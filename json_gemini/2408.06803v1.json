[{"title": "Integrating Saliency Ranking and Reinforcement Learning for Enhanced Object Detection", "authors": ["Matthias Bartolo", "Dylan Seychell", "Josef Bajada"], "abstract": "With the ever-growing variety of object detection approaches, this study explores a series of experiments that combine reinforcement learning (RL)-based visual attention methods with saliency ranking techniques to investigate transparent and sustainable solutions. By integrating saliency ranking for initial bounding box prediction and subsequently applying RL techniques to refine these predictions through a finite set of actions over multiple time steps, this study aims to enhance RL object detection accuracy. Presented as a series of experiments, this research investigates the use of various image feature extraction methods and explores diverse Deep Q-Network (DQN) architectural variations for deep reinforcement learning-based localisation agent training. Additionally, we focus on optimising the detection pipeline at every step by prioritising lightweight and faster models, while also incorporating the capability to classify detected objects, a feature absent in previous RL approaches. We show that by evaluating the performance of these trained agents using the Pascal VOC 2007 dataset, faster and more optimised models were developed. Notably, the best mean Average Precision (mAP) achieved in this study was 51.4, surpassing benchmarks set by RL-based single object detectors in the literature.", "sections": [{"title": "I. INTRODUCTION", "content": "Object detection, within the area of computer vision, is a critical problem in recognising and localising objects inside an image or video [1]. At first sight, the challenge appears simple: identify objects in visual scenes. However, the difficulty stems from the sheer variation of objects in terms of size, shape, orientation, occlusion, and lighting. Furthermore, the context in which these objects occur complicates the process even more, mandating models that can generalise across varied contexts and views.\nWhile substantial progress has been made in advancing this field with technologies such as YOLOv10 [2], which utilise various computer algorithms to overcome these issues, further experimentation is required to achieve human-like competency. To this end, grasping the intricacies of human visual perception [3]-[5] and understanding how humans locate objects are crucial aspects of this effort.\nIn [6], T. Boger and T. Ullman conducted several tests to investigate individuals' capacity to locate objects. Their findings show that, regardless of object realism, judgements based on physical reasoning, namely centre of mass, are consistently the most significant element in perceived object location. Successfully showing a dependence on physical qualities to determine object location. In a broader sense, this study provides an excellent justification for investigating a fresh paradigm in the modelling of spatial perception inside visual frameworks, such as human perception in object detection [6].\nThus, this study aims to investigate whether the integration of reinforcement learning and saliency ranking techniques, which mimic human perception, can effectively address the problem of object detection. We also explore the individual components of reinforcement learning-based object detectors with the goal of developing faster and more optimised models, while also examining the accuracy of the proposed architecture in comparison to the literature. Moreover, this research investigates whether adopting a reinforcement learning framework facilitates more transparent detection by enabling monitoring of the object detection training process.\nThe first part of this paper consists of background information on methodologies employed by saliency ranking and reinforcement learning, as well as a comprehensive review of past reinforcement learning techniques for object detection. Chapter IV provides a full overview of the solution architecture and its modules. This is followed by a series of experiments, and a thorough comparative analysis in Chapter V. The paper summarises the findings in Chapter VII and discusses proposed directions for future work in Chapter VIII."}, {"title": "II. BACKGROUND", "content": "This chapter presents background knowledge that readers must familiarise themselves with before proceeding to the subsequent chapters, which served as the foundation for this endeavour. A brief synopsis of feature and classification learning, saliency ranking, and reinforcement learning is provided."}, {"title": "A. Saliency Ranking", "content": "Saliency Ranking is a fundamental process aimed at discerning the most visually significant features within an image. In [7], [8], the authors introduce a technique for separating images into segments, and ranking their saliency. Using a grid-based methodology, this technique successfully locates salient objects, that allows for automatic selection of objects without the need of training a model. SaRa operates in two separate stages. It first processes texture and, if available, depth maps with a defined grid G with a size of kxk, to generate a saliency map using Itti's model [4], as implemented in [9]. Moreover, the presented saliency model [4], recognises features using a constructed \"feature integration theory\" method inspired by the design and behaviour of the primate visual system. Subsequently, each segment inside grid G is scored separately, taking into consideration criteria such as proximity to the image centre, accounting for centre-bias (CB), and entropy (H) computed using Equation 1 together with a depth score (DS). Figure 1 illustrates the aforementioned architecture.\n\\(H(X) = -\\sum_{i=1}^{t}P(x_i)\\log_2(P(x_i))\\)\n\nThe grid G consisting of n = k2 segments, each yielding a corresponding saliency ranking score S is calculated as depicted in Equation 2. These scores are then used to rank the segments of the input image, whereby a greater value of S indicates a more salient segment in image. According to extensive testing and experimentation described in [7], the ideal grid size for G is for k = 9, which successfully covers the region of prominent items while minimising total segment area utilisation.\n\\(S_n = H_n + CB_n + DS_n\\)"}, {"title": "B. Feature and Classification Learning", "content": "Feature Learning, particularly critical in image classification, entails extracting vital features from images to aid in accurate object and pattern detection, most notably through the use of CNNs [10]. CNNs accomplish this through convolutional layers, which extract features using kernels or filters, successfully detecting patterns in images by striding across and computing dot products to generate feature maps, while pooling layers down-sample these maps to reduce parameter size. These networks have revolutionised the area of computer vision by allowing feature learning without the need to train massive complicated neuron architectures streamlining the process. To this extent, popular CNN architectures designed for image classification include: ResNet50 [11], VGG16 [12], Inception [13], Xception [14], MobileNet [15], and EfficientNet [16], each characterised by its architectural design and feature learning methods. Earlier architectures include VGG16, which emphasises depth in networks by employing lower filter sizes [12], and ResNet50, which uses residual connections to develop deeper and more expressive representations [11]. These architectures demonstrate the varied methodologies employed in designing CNNs for feature and classification learning. On the other hand, Inception improves feature learning performance by using parallel filters of varying sizes [13], [17]. Xception takes the Inception concept to its extreme by employing depth-wise separable convolutions to efficiently capture intricate features across varying spatial scales [14], whereas MobileNet focuses on efficient feature learning using lightweight depth-wise separable convolutions [15]. With compound scaling, EfficientNet [16] balances model depth, breadth, and resolution for optimal feature learning while retaining computational efficiency. Moreover, a common utilisation approach for representation learning involves leveraging these architectures pre-trained on the ImageNet dataset [18], [19]."}, {"title": "C. Reinforcement Learning", "content": "Reinforcement Learning is the process of learning what to do-how to translate states into actions in order to maximise a numerical reward signal [20]. In RL, the learner is not taught which actions to perform but instead must experiment to determine which actions provide the highest return. Additionally, RL is distinguished by two key characteristics: reliance on trial-and-error search, and the capacity to maximise cumulative rewards over extended periods rather than seeking immediate satisfaction. The RL framework is often modelled as an interaction loop, as can be seen in Figure 2. In the loop, the RL agent performs actions in an environment based on a policy that transitions its current state to a new one while collecting rewards and observations in the process [20].\nThe policy, which is frequently represented as \u03c0, serves as a function that guides the agent's decision-making process in an environment. It maps each state st to an action At, determining the agent's movements from the current state st to the next state st+1 at a given time step t. Rt represents the numerical reward that the agent obtains as a result of each action in a state. These rewards are critical in refining the agent's policy, assisting it in determining the appropriate action to take in various states in order to achieve its ultimate goal: the optimal policy \u03c0*. Maximising the predicted cumulative reward, which is the sum of all rewards collected over multiple time steps, is required to achieve this optimum policy [21]. The Environment refers to the external system or context with which an agent interacts and learns. It encompasses everything outside the agent's control, presenting states and rewarding the agent's actions, which are critical for the agent's observance, learning, and decision-making in a dynamic environment.\n1) Markov Decision Process: Formally, RL can be expressed as a MDP [22]. An MDP is a mathematical framework for modelling SDMP in which the present state of the system, potential actions, and probability of transitioning to the next state are known [23]. Moreover, an MDP can be modelled as the following tuple: S, A, P, R, \u03b3), whereby:\n\u2022 S is a set of possible states st \u2208 S observed by the agent at time step t.\n\u2022 A is a set of possible actions at \u2208 A that the agent can execute at time step t.\n\u2022 P is the transition dynamics p(St+1|St, at) that determine the probability of moving to st+1 after taking action at in state st.\n\u2022 R is the immediate reward function r(st, at, St+1) that provides the reward obtained after transitioning from st to St+1 through action at.\n\u2022 \u03b3 is the discount factor (\u03b3\u2208 [0,1]) that regulates the balance between immediate and future rewards, and shapes the agent's decision-making.\n2) Deep Reinforcement Learning: Employing techniques such as bootstrapping, dynamic programming, monte carlo, temporal-difference, and tabular search, RL methods face the issue of handling and generalising large-scale data in broad environments. A solution to this problem pertains to the use of Deep Reinforcement Learning (DRL). DRL combines the concept of RL methods with deep learning, connecting their strengths to address shortcomings in classic tabular techniques. As described in [22], DRL effectively navigates away from the confines of manually built state representations by combining complex neural network topologies such as ANNs and CNNs. It excels at exploring huge and complicated search spaces, allowing RL agents to generalise their learning across different settings. Deep RL uses Neural Networks (NN) to estimate value functions or rules, as opposed to traditional tabular techniques, which suffer from scalability in contexts with exponentially vast state spaces. This in turn allows agents to learn, and take decisions in high-dimensional and continuous state spaces, enabling them to display flexible behaviour across a wide range of complex situations [22].\n3) Deep Q-Network: Deep Q-Network (DQN) is an extension of the Q-learning technique but replaces the tabular Q-values with a deep neural network. In Q-learning, the Q-value represents the mapping that assigns a numerical value to each state-action pair, while indicates the expected cumulative reward when taking action a in state s. Moreover, Q-learning is an off-policy temporal-difference control algorithm whereby after every time step, Q-values are updated using Equation 3.\n\\(Q(s, a) = Q(s, a) + \\alpha\\cdot (R + \\gamma \\max_{a_{t+1}}Q(s_{t+1}, a_{t+1}) - Q(s, a))\\)\n\nDQN attempts to increase generalisation capabilities, particularly in cases with huge state spaces, which traditional Q-learning and memory techniques fail to control [22]. As illustrated in Equation 4 the DQN utilises a deep neural network in the form of a new parameter \u03b8 which symbolises the deep neural network's weights, in order to approximate the optimal value function. To achieve this, Equation 5 defines the expected loss Li(\u03b8i), where U(D) denotes the uniform distribution over samples stored in the replay buffer D, ensuring a random selection of experiences for training, and i represents the iteration index for the parameters \u03b8i. Furthermore, DQN leverages this loss function to optimise the state-action value function [24].\n\\(Q(s, a; \\theta) \\approx Q^*(s, a)\\)\n\\(L_i(\\theta_i) = E_{(s, a, r, s', a') \\sim U(D)} \\left[\\left(r + \\gamma \\max_{a'} Q(s', a'; \\theta_i) - Q(s, a; \\theta_i)\\right)^2\\right]\\)\n\nThe deep Q-learning algorithm employed in DQN utilises both a policy network and a target network to facilitate efficient and stable learning. The policy network, or Q-network, continually approximates Q-values for state-action pairings in deep Q-learning to guide action selection. Similarly to normal Q-learning, the policy network updates the Q-values through the temporal-difference error and gradient descent. Subsequently, the target network, a separate network with fixed parameters updated from the policy network on a regular basis, stabilises learning by providing consistent Q-value objectives during training [24]. As previously hinted in Equation 5, DQN also employs an experience Replay Buffer RB, which can be used to sample through transition data during training in order to facilitate more stable learning. These transitions consist of a tuple of five elements containing: state, action, reward, done, and after state. Transitions in the environment are added to a RB, and the loss and gradient are calculated using a batch of transitions sampled from the RB rather than utilising the most recent transition while training [20], [24]. Considering that DQN is an extension of Q-learning, it also employs the \u03f5-Greedy policy to select an action. Thus, ensuring a balance between agent exploration and exploitation, whereby the optimal action is chosen with a probability of 1-\u03f5 otherwise, a random action is selected [20], [22].\n4) Double Deep Q-Network: The Double Deep Q-Network (DDQN) algorithm is an improvement on the traditional DQN approach, designed to address overestimation difficulties in Q-learning methods and reducing maximisation bias [25]. DDQN works by separating action selection from action value estimation using two independent NN. In DDQN, the network with the highest Q-value determines the ideal action, whereas the alternate network evaluates the value of that action. The difference is apparent when comparing the target value functions in Equations 6 and 7. In these equations, Y represents the target value at time step t, used in the update rule, and composed of the immediate reward Rt+1 and a discounted Q-value. In Equation 6, the Q-value is maximised using the same network parameters \u03b8t for both action selection and value estimation. Conversely, in Equation 7, a separate network with target network parameters \u03b8\u2212t is utilised for value estimation. Significantly, this improves the DDQN algorithm's stability and learning efficiency by limiting the overestimation bias found in single-network Q-learning techniques [22].\n\\(Y^{DQN} = R_{t+1} + \\gamma Q\\left(s_{t+1}, \\underset{a}{\\operatorname{argmax}} Q(s_{t+1}, a; \\theta_t) ; \\theta_t\\right)\\)\n\\(Y^{DoubleDQN} = R_{t+1} + \\gamma Q\\left(s_{t+1}, \\underset{a}{\\operatorname{argmax}} \\left( Q(s_{t+1}, a; \\theta_t) \\right); \\theta^-_t\\right)\\)\n\n5) Dueling Deep Q-Network: The Dueling Deep Q-Network (Dueling DQN) architecture presents a substantial improvement on the traditional DQN. It improves efficient learning whilst allowing the model a more precise identification of the crucial state values, and advantageous actions [22]. Moreover, Dueling DQN presents a revolutionary design that separates the estimation of state values from the assessment of each action taken, independently of the remaining state value in a specific state [26].\n6) Double Dueling Deep Q-Network: The Double Dueling Deep Q-Network (D3QN) architecture represents a significant advancement over both traditional DQN and Dueling DQN models, by enhancing learning efficiency and decision-making accuracy in DRL tasks [20], [21]. Through the integration of DDQN and Dueling DQN, D3QN presents a dual-network architecture that separates the estimation of state values, and action advantages. Additionally, double Q-learning techniques are utilised to reduce overestimation biases [20], [21]. This novel technique not only increases the model's capacity to discover critical state-action values, but also makes it much more resilient to sub-optimal action selection, rendering it ideal for complex and dynamic contexts."}, {"title": "III. LITERATURE REVIEW", "content": "This chapter will focus on the literature review necessary for understanding the subsequent chapters. It will identify key architectures and methodologies previously developed in reinforcement learning object detectors.\nPioneering reinforcement learning-based object detectors, and appearing around the same time RCNN was released (2015), Caicedo et al. present \u201cActive Object Localization with Deep Reinforcement Learning\u201d. In their study [27], DRL was utilised for active object localisation, whereby the object detection problem was transformed into the MDP framework, in an attempt to solve it. In their implementation, the authors took into consideration eight distinct actions (up, down, left, right, bigger, smaller, fatter, and taller) to enhance the bounding box's fit around the item and an extra action (trigger) to indicate that an object is correctly localised in the current box. The trigger also modifies the environment by using a black cross to mark the region covered by the box, which acts as an Inhibition of Return (IoR) mechanism to prevent re-attention to the currently attended region, a widely used strategy in visual attention models to suppress continuous attraction to highly salient stimuli [5]. Furthermore, in their approach, the authors tackled multiple object detection by allowing the RL agent to run for a maximum of 200 time steps, evaluating 200 regions for each given image. Caicedo et al. encoded the state as a tuple of feature vectors and action histories, while also employing shift in Intersection over Union (IoU) as a binary reward strucuture ([-1,+1]) across actions, and a subsequent reward for trigger actions. The authors also employed a guided exploration strategy based on apprenticeship learning principles [28]\u2013[30], which uses expert demonstrations to inform the agent's actions, particularly in determining positive and negative rewards based on IoU with the current bounding box, allowing the agent to choose positive actions at random during exploration.\nAn enhancement to the algorithm presented in [27] was put forth by [31], whose authors also treated the object detection problem as an MDP and employed a hierarchical technique for object detection. The RL agent's objective in their approach consisted of first identifying a Region of Interest (ROI) in the image, then slowly reducing the area of interest to obtain smaller regions, thus generating a hierarchy. Similar to [27], Bellver et al. [31] additionally represented the reward function as an IoU change in between actions. According to their paper, they utilised DQN as the agent's architecture while also employing Image-zooms and Pool45-crops with VGG16 [12] networks as the image feature extraction backbone. In addition to a memory vector that stores the last four actions, the extracted image feature vector from the aforementioned networks was incorporated into the DQN state.\nK. Simonyan et al. [32], use the sequential search technique to present a DRL-based object identification system. The model was trained by using a collection of image regions, where the agent returned fixate actions at each time step, identifying an area in the image that the actor should investigate next. The state was composed of three pieces in total: the focus history Ft, the selected evidence region history Et, and the observed region history Ht. The fixate action was composed of three elements: the image coordinate of the subsequent fixate zt, the index of the evidence region et, and the fixate action itself. As presented in their paper, K. Simonyan et al. specified the reward function to be responsive to the detection location, the confidence of the final state, and the imposition of a penalty for each region evaluation.\nAs described in [33], Z. Jie et al. proposed a tree-structured RL agent (Tree-RL) for object localisation in order to map the inter-dependencies among the various items. By treating the problem as an MDP, the authors took into account two different kinds of actions: translation and scaling. The former consisted of five actions, while the latter involved eight. In addition, the agent's defined state consists of a concatenation of the extracted feature vectors from the current window, the entire image, and the history of actions taken. The feature vector was constructed from a pre-trained VGG16 [12] model on the ImageNet [18] [19] dataset, whilst also incorporating IoU shifts between actions as a reward function. The proposed Tree-RL agent architecture uses a top-down tree search, which starts with the entire image and proceeds to iteratively select the optimal action from each action group, thereby generating two new windows. This process is performed recursively in order to successfully locate the object in the image.\nContrary to previous techniques, [34] suggested a multitask learning policy that uses DRL for object localisation. Y. Wang et al. also viewed the problem as a MDP, wherein the agent needed to execute a series of actions to manipulate the bounding box in various ways. A total of eight actions for bounding box transformation (left, right, up, down, bigger, smaller, fatter, and taller) were employed. Similar to the previously mentioned methods, the authors encoded the states as a fusion of historical actions and feature vectors while also employing shift in the IoU as a reward between actions. An increase in IoU would result in a reward of 0, while a decrease to this value would result in a reward of -1. On the other hand, the reward for the terminal action was set to 8 if the IoU was larger than 0.5 and -8 otherwise. Moreover, in their paper, Y. Wang et al. separated the eight transformation actions and the terminal action into two networks and trained them together using DQN with multitask learning for object localisation.\nIncorporating RL, A. Pirinen and C. Sminchisescu in [35] suggested an improvement to Regional Proposal Networks (RPN) that greedily chooses ROIs. In this research, the authors utilised a two-stage detector comparable to Fast RCNN [36] and Faster RCNN [37], whilst also employing RL in the decision-making process. They also utilised the normalised shift in IoU as the reward.\nRather than learning a policy from a huge quantity of data, [38] presented a bounding box refinement (BAR) technique based on RL. In their publication, M. Ayle et al. utilised the BAR algorithm to anticipate a sequence of operations for bounding box refinement for imprecise bounding boxes predicted by some algorithm. Similarly to previous methods, they examined eight different bounding box transformation actions (up, down, left, right, broader, taller, fatter, and thinner) and categorised the problem as a SDMP. They suggested two methods: BAR-DRL, which is an offline algorithm, and BAR-CB, which is an online algorithm and requires training on every image. The developers of BAR-DRL trained a DQN over the states using a history vector of ten actions and features taken from ResNet50 [11] [17], which were pre-trained on ImageNet [18] [19]. The encoded reward for the BAR-DRL agent after executing an action consisted of 1 in the case of an IoU increase and -3 otherwise. To record the shape and boundaries of the object of interest, M. Ayle et al. modified the LinUCB [39] method for BAR-CB and took into consideration the Histogram of Oriented Gradients (HOG) for the state. Additionally, the online technique (BAR-CB) followed the same steps as the offline method, with a 1 reward for a positive shift in IoU, and a 0 reward otherwise. Moreover, the authors regarded \u1e9e as the terminal IoU for both implementations.\nA structure including two modules, coarse and fine-level search, was proposed by [40] as an enhancement to the sequential search approach of [32]. In their study, B. Uzken et al. claim that their approach is effective for detecting objects in huge images with dimensions exceeding 3000 pixels. The authors initiated their process with a broad-scale examination of a large image, identifying a set of patches. These patches were subsequently employed in a more detailed analysis to locate sub-patches. A two-step episodic MDP was used for both fine and coarse levels, with the policy network's job being to return the probability distribution of each action. Additionally, the authors represented the action space using a binary array format ([0,1]), where a value of 1 denotes the agent's decision to explore obtaining sub-patches for a specific patch, and 0 indicates otherwise. As described in their paper, B. Uzken et al. applied the linear combination of Racc (detection recall) and Rcost (image acquisition cost plus run-time performance reward) in their design, accounting for a total of 16 patches and 4 sub-patches, respectively.\nM. Zhou et al. presented ReinforceNet [41], a DRL framework for object detection. Formulating the object detection challenge as an MDP, the authors suggested an agent that iteratively alters an initial bounding box by executing actions such as translate, scale, and aspect ratio change. In addition, actions are parameterised using a deep policy network based on the VGG16 [12] feature extractor network. The encoded state was made up of features from ROI-pooling CNN layers as well as a history vector. The reward was designed as a change in IoU after each action, and the episode ends when the IoU exceeds a certain threshold. The REINFORCE algorithm was used to train an end-to-end policy network using a learned value function baseline. The authors also introduced a \"completeness\" metric in their paper to evaluate the extent to which the agent explores the state space during training. Low completeness shows that the agent converges prematurely without sufficient exploration. M. Zhou et al. identified inadequate exploration as a major difficulty in applying RL for object localisation, as well as implementing a curriculum learning technique with the aim of improving completeness and exploration.\nM. Samiei and R. Li [42] propose two DRL methods to actively solve the object detection problem. Similarly to [31], the authors proposed a hierarchical method that enables an RL agent to select in a timely manner, one out of five sub-regions of the image, with the aim of obtaining smaller regions. Additionally, the proposed dynamic method allows the agent to transform the bounding box at each time step by using actions like translate, scale, and deform. M. Samiei and R. Li also formalised the problem as an MDP and encoded the state as a combination of the current region features and action history. In their study, the authors also experimented with different reward functions, examining both IoU improvements and recall as measurements for this function. In a comparable manner to [31], the authors estimated Q-values using the DQN architecture with an experience RB. As an image feature extraction backbone, the VGG16 [12] was also used. Unfortunately, the proposed method has limitations such as class-specific training and struggles with multiple objects.\nRecent advancements on the approach discussed in [27], however tackling single object detection, encompass innovations highlighted in [43], [44]. In the study presented in [43], a novel approach utilising Q-learning with CNNs is introduced to devise a strategy for object detection via visual attention. The study showcases innovative experiments that dismiss pre-trained networks for representation learning. In addition, it demonstrates the network's capability to generalise within and across super-categories, detect objects of any type, and achieve comparable results to previous research. This master's thesis investigates the inclusion of two split-view actions and evaluates an incremental training technique. Additionally, it tests whether fine-tuning the model for one category and then bootstrapping training for the next category is better than training on a shuffled dataset involving all categories. On the other hand, in the implementation discussed in [44], the author addresses the challenge of single object detection and explores the utilisation of the VGG16 network [12] for representation learning, deviating from the 6-layer CNN specified in the original paper [27], yet achieving somewhat comparable outcomes."}, {"title": "IV. METHODOLOGY", "content": "To identify the most relevant object inside an image", "SaR-LVision\" system architecture, explaining the design decisions and their justifications. The approach and techniques used in creating the system architecture are also described, drawing on knowledge gained from thorough research of pertinent literature. Moreover, all code utilized is made available on GitHub\u00b9.\nA. Reinforcement Learning\nIn alignment with the primary goal, an RL framework was developed to achieve object localisation within images. To this extent, the developed system was built via the gymnasium\u00b2 API, which provided a platform for the problem formulation, inspired by the existing literature. Subsequently, DRL techniques were applied to approximate the object detection problem.\n1) States": "Similar to methodologies presented in several prior studies [27", "34": [43], "44": "the state representation in this work comprises a tuple (o", "27": [43], "weights)": "VGG16"}, {"34": [44], "17": "and Xception [14", "43": "."}, {"43": "showed that utilising a pre-trained CNN achieved comparable results. Additionally", "27": "the history vector h is a binary vector encoding the ten most recent actions taken", "Actions": "In a manner akin to the methodologies described in [27", "34": [44], "subsets": "horizontal and vertical box movements", "y2": "during interactions with the environment.\nEach transformation action induces a discrete change to the box's size relative to its current dimensions"}, {"43": "introduces two additional split view actions", "27": [43], "44": "the parameter \u03b1 is established at 0.2", "5": "similar to [27"}, {"43": [44], "Rewards": "The reward function (R) quantifies the agent's progress in localising an object following a specific action", "27": [34]}, {"43": [44], "27": "."}, {"27": "to encourage better localisation. An enhancement to the pipeline in [27", "43": [44]}, "pisodes", 0, 27], "43": [44], "Architecture": "The DQN architecture", "27": [44]}, {"27": "employs a 3-layer Dense Layer DQN representation", "44": "extends this architecture by incorporating Dropout Layers. Our proposed approach", "43": ".", "26": "."}, {"27": "adopts a strategy of employing class-specific DQN agents", "43": "investigates retraining class-specific DQNs on additional categories. However"}, {"43": ".", "Parameters": "Similar to the parameter choices in [27"}]