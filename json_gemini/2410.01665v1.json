{"title": "Towards a vision foundation model for comprehensive assessment of Cardiac MRI", "authors": ["Athira J Jacob", "Indraneel Borgohain", "Teodora Chitiboi", "Puneet Sharma", "Dorin Comaniciu", "Daniel Rueckert"], "abstract": "Cardiac magnetic resonance imaging (CMR), considered the gold standard for\nnoninvasive cardiac assessment, is a diverse and complex modality requiring a wide\nvariety of image processing tasks for comprehensive assessment of cardiac\nmorphology and function. Advances in deep learning have enabled the development\nof state-of-the-art (SoTA) models for these tasks. However, model training is\nchallenging due to data and label scarcity, especially in the less common imaging\nsequences. Moreover, each model is often trained for a specific task, with no\nconnection between related tasks. In this work, we introduce a vision foundation\nmodel trained for CMR assessment, that is trained in a self-supervised fashion on 36\nmillion CMR images. We then finetune the model in supervised way for 9 clinical tasks\ntypical to a CMR workflow, across classification, segmentation, landmark localization,\nand pathology detection. We demonstrate improved accuracy and robustness across\nall tasks, over a range of available labeled dataset sizes. We also demonstrate improved\nfew-shot learning with fewer labeled samples, a common challenge in medical image\nanalyses. We achieve an out-of-box performance comparable to SoTA for most clinical\ntasks. The proposed method thus presents a resource-efficient, unified framework for\nCMR assessment, with the potential to accelerate the development of deep learning-\nbased solutions for image analysis tasks, even with few annotated data available.", "sections": [{"title": "1. Introduction", "content": "Cardiac Magnetic Resonance (CMR) Imaging is considered the\nreference standard for non-invasively assessing cardiac\nstructure, function, and viability. However, a typical CMR study\nis complex and consists of different sequences, requiring a wide\nvariety of image analysis tasks. For example, segmentation of\nthe left and right ventricles (LV, RV) and left and right atria (LA,\nRA) on cine images throughout the cardiac cycle are required for\nassessing ejection fraction, and other functional metrics.\nSegmentation of the left ventricle myocardium in delayed\ngadolinium enhancement CMR (LGE) and in tissue mapping\nimages (T1 mapping pre- or post- contrast, T2 mapping, ECV\netc.) is required for myocardial tissue characterization. In\naddition, the essential step of pre-filtering and selecting the\nappropriate cardiac sequences and views for each of these\nanalyses, is also required. Reporting of myocardial analyses is\nstandardized clinically through the AHA segment model\n(Cerqueira et al., 2002), which additionally requires the\ndetection of key points (landmarks) in the cardiac region.\nIntegration of information from multiple techniques is required\nfor diagnosing cardiac pathologies. The insights gathered from\nsuch an analysis could also influence study acquisition, where\nthe measured cardiac parameters influence the scan plan during\nthe acquisition process. However, assessment of CMR images\nrequires high clinical expertise and is time-consuming, due to\nvariability in the images from differing scan protocols, scanners,\nnormal and pathological anatomical variations, image artifacts,\nand the lack of quantitative standards. Several studies have\nexplored each of these tasks individually or together, and deep\nlearning (DL) techniques have emerged as the state-of-the-art\n(SoTA) across the various tasks. However, many of these studies\nare on very small cohorts or are limited to cine MRI on relatively\nhealthy subjects. An additional consequence of the fragmented\nlandscape is that, in general, each of the cardiac image analysis\ntasks, though highly related, are treated independently with no\nin-formation sharing between the trained DL networks.\nOn the other hand, foundational models (FM) represent the\nlatest generation of Al models that promise generalizability\nacross data distributions and across different tasks. These are\nlarge models, trained on massive, diverse datasets. As a result,\nthey have achieved SoTA results and excellent zero-shot and\nfew-shot performance and generalizability in natural image\nprocessing (Oquab et al., 2024; Kirillov et al., 2023). FM are often\nvision transformers, which by being able to better capture global\nrelationships in images, can overcome certain shortcomings of\nfully convolutional networks(Willemink et al., 2022). However,\nthey require very large amounts of training data to reach their\nfull potential. Thus, in the field of medical image analysis, task-\nspecific models are still mainly used, especially for obtaining\nclinical metrics or disease diagnoses. While generalist FM show\nimpressive transferability to medical domains (Baharoon et al.,\n2023), they still often lag behind fully supervised training in\nterms of task specific accuracies (Zhang and Metaxas, 2023).\nCompared to natural images, medical images offer different, but\nsignificant challenges due to its diverse modalities, contrast\nproperties, different dimensionality and reconstruction\nmethods which can lead to subtle and fine-grained differences\nbetween images, varying resolutions, and the long tail\ndistribution of abnormalities. Most medical vision tasks are also\nlow-data tasks, with few labeled examples, due to the high cost\n(expertise, time etc.) required to create annotations. We\nhypothesize that a modality-specific or organ-specific FM could\nprovide stronger image representations relevant to the\ndownstream tasks. This could in turn lead to better accuracies\nand robustness for related clinical tasks, reduce the need for\nlabeled data, and offer an efficient path to model deployment\nfor new tasks.\nIn this study, we propose a foundation model trained in a task-\nagnostic way on 36 million CMR images, the first such FM for\nCMR to our knowledge. We show that this model can be\nfinetuned for classification, segmentation, regression, and\ndetection tasks typical to a CMR image analyses workflow and\ndemonstrate performance improvements relative to a\ncomparable model, with no other changes in the pipeline. We\nalso conduct ablation studies to demonstrate the importance of\npretraining on relevant medical images, as opposed to natural\nimages. In addition, we demonstrate the data efficiency of such\na targeted FM by exploring its few-shot learning capabilities."}, {"title": "2. Related Works", "content": "Many studies explore CMR imaging tasks separately. Chauhan\net al. (2022) report an accuracy of 0.90-0.95 for distinguishing\nbetween short-axis and 3 long-axis views. The study trained a\nconvolutional network on a dataset of 2000 images. The dataset\nwas actively augmented with views of complex anatomy and\nbalanced across classes to achieve the best results. In the case\nof segmentation, studies report best Dice scores of 0.90 to 0.95\nfor LV(Bernard et al., 2018; Suinesiaputra et al., 2022; Schilling\net al., 2024; Bai et al., 2018), 0.90-0.92 for RV (Bernard et al.,\n2018; Schilling et al., 2024; \u00c5kesson et al., 2023) and 0.88 to 0.91\nfor myocardium (Bernard et al., 2018; Suinesiaputra et al., 2022;\nSchilling et al., 2024; Bai et al., 2018) in short axis (SAX) cine MRI.\nMany of these studies were done on public datasets, made\navailable as part of challenges. \"Automatic Cardiac Diagnosis\nChallenge\" dataset (ACDC)(Bernard et al., 2018) is one such\npublicly available and fully annotated dataset for the purpose of\n(SAX, cine) MRI assessment. It consists of 150 patients with\nground truth for LV, myocardium and RV segmentations in ED\nand ES, as well as an expert-assigned disease label. Isensee et al.\nachieved the overall top-performing method in the challenge by\nusing an ensemble of 2D and 3D U-Net architectures to perform\nthe segmentation. A random forest classifier was then used on\ntop of clinical metrics handcrafted from the segmentations to\npredict disease classes. In long axis (LAX) views, Bai et al. (2018)\ntrained a DL network on a large scale dataset of 4875 subjects\nfrom the UKBB cohort (Sudlow et al., 2015) and reported dice\nscores 0.94 for LV, 0.88 for myocardium, 0.95 for LA and 0.96 for\nRA. In the case of LGE images, some studies report dice scores\nof 0.84 to 0.88 for the short-axis myocardium (Zhuang et al.,\n2022; Zhang, 2021). EMIDEC (Lalande et al., 2020) dataset\nprovides LGE images and myocardial contours for 100 patients."}, {"title": "2.1. Cardiac MR imaging tasks"}, {"title": "2.2. Foundation Models", "content": "Foundation models (FM) can be supervised (such as\nCLIP, Radford et al. (2021)), weakly supervised (SAMKirillov et\nal. (2023)) or self-supervised (DINO, Caron et al. (2021)). Self-\nsupervised methods provide a promising option in medical\nimaging where labeled data is scarce. Baharoon et al. (2023)\nexplored DINOv2(Oquab et al., 2024) pretrained models on\nclassification and segmentation in radiology tasks, and report\npromising results. However, performance varies depending\non the complexity of the task, the anatomy involved, the\namount of training data available, and the finetuning method\nused. Ghesu et al. (2022) train a medical FM with 100 million\nimages, and report a significant increase across multiple\ndetection tasks in chest radiographs and brain MRI. Tang et\nal. pretrained a UNetr model 3D computed tomography\nimages (CT) and obtained SoTA results on various\nsegmentation tasks in CT images. Lu et al. (2024) trained a\nvision-language FM on 1.17M histopathological images, and\nshowed SoTA results on various related downstream tasks\nwith or without finetuning."}, {"title": "3. Methods", "content": "We pretrain a ViT-S/8 (Touvron et al., 2021) model in a\nselfsupervised manner using DINO(Caron et al., 2021) on 36\nmillion cardiac MRI images from 27,524 subjects from 3\nsources: two clinical centers (henceforth referred to as\nCenters 1 and 2) and the UK Biobank (UKBB) (Sudlow et al.,\n2015) cohort. Datasets from Centers 1 and 2 are composed of\ncomprehensive CMR studies including cine, LGE, T1 and T2\nweighted imaging, flow imaging, black blood sequences,\nlocalizers, etc., from both healthy and diseased subjects.\nMore information is given in Appendix 6.1. The images focus\non cardiac views and anatomies, with a large proportion\nbeing short-axis images, followed by long-axis (2, 3, and 4\nchambers). Since the region of interest in a typical DICOM\nimage from a cardiac study is limited, relative to the entire\nimage size, all images were preprocessed by rescaling to a 1\nmm x 1 mm resolution, and center-cropping to a size of\n224x224. The model was trained for 7 days on 8 Nvidia Tesla\nH100 (80GB) GPUs, with a batch size of 1024 and patch size\nof 8 for the ViT."}, {"title": "3.1. Foundation model for cardiac MR", "content": "We finetune and test the model separately for 9 tasks in\nclassification, segmentation, landmark localization, and\npathology detection, across cine, LGE, and mapping images,\nfrom multiple datasets. The tasks detailed below are\nillustrated in Figure 2.1. Data used for each task is summarized\nin Table 1. Public datasets and labels were used when available.\nIn other cases, clinical data from three centers (Centers 1-3, 1\nand 2 as in the previous section), were annotated by trained\nCMR experts. More information about the private datasets is\ngiven in Appendix 6.1. For each task, the available data was\ndivided into training, validation, and testing split, on the patient\nlevel. Image type classification (task 1) aims to recognize\ndifferent CMR acquisition protocols that lead to different tissue\ncontrasts. Each image type can cover diverse views of the heart\n(short, long axis etc.). In this case, we classify between cine (a\nbalanced steadystate free precession - bSSFP), T1 (a MOLLI\nacquisition for T1 pre- and post-contrast, including both the\nmaps and the different T1-weighted images), T2 (Single-shot T2\nprepared bSSFP, both the maps and the different T2-weighted"}, {"title": "3.2. Downstream tasks", "content": "images) and LGE (single-short or segmented PSIR acquisition).\nCine view classification (task 2) is a fundamental task for\nautomated CMR analyses, and attempts to choose relevant cine\nviews for further analyses. While SAX and LAX views are typically\nstudied, a CMR study can include many diverse views. This is\nrepresented in our study through the more uncommon Aorta\nfocussed view and the \"Others\" class which includes other views\nas well as non-diagnostic images from the existing named\nclasses. Tasks 1 and 2 use data from Center 3.\nFor CMR segmentation, in task 3 we segment the LV blood pool,\nmyocardium and RV blood pool in SAX cine bSSFP images from\nthe ACDC challenge. In task 4 we segment all four cardiac\nchambers and the myocardium in LAX 4-chamber view images\nfrom the Kaggle dataset. We present two experiments for the\nmyocardium segmentation in LGE SAX images (tasks 5A and 5B).\nTo explore the effect of the size of the datasets on the same task,\nwe compare the segmentation performance on only the EMIDEC\ndataset for task 5A vs. adding more clinical data from Center 1\nand Center 2 amounting to a 10-fold increase for task 5B. We\nnote that this is an annotated subset of the larger dataset used\nfrom Centers 1 and 2 in the pretraining task, making task 5B an\nin-distribution downstream task. Mapping segmentation (task\n6) includes T1 maps (pre- and post-contrast) and T2 maps. For\ntask 7, landmark localization is performed on the UKBB dataset\nfor the anterior and inferior right ventricular insertion points\n(ant. RVIP and inf. RVIP) in short-axis cine bSSFP images. While\nonly the ant. RVIP is typically used clinically for AHA segment\ninitialization, we also include the inf. RVIP. While tasks 8A and 8B\nboth involve the detection of enhancement from LGE images,\nthe GT (presence of LGE) in task 8A is extracted automatically\nfrom clinical reports, while task 8B used the publicly provided\nannotations (scar masks) to infer the same. Both tasks use 3\nimage slices extracted from the image stack at basal, mid, and\napical levels as the input, and predict a binary label. Task 9 uses\n3 frames from the cardiac cycle (ED, halfway between ED and ES,\nand ES), extracted at the basal level from the image stack as the\ninput, and predicts the patient as normal (NORM), or assigns a\ndisease label dilated cardiomyopathy (DCM), hypertrophic\ncardiomyopathy (HCM), abnormal RV (RV) and previous\nmyocardial infarction (MINF).\nSince the focus of the study was to evaluate the pretraining\nstrategy, no further optimization was done in terms of\nincorporating multiple spatial or temporal information,\nthough that could potentially further improve the\nperformance. Out of the 9 tasks, 6 are out-of-distribution,\nusing datasets that were not seen during pretraining. The\nremaining 3 tasks (5B, 7 and 8A) are in-distribution, where we\nuse a small subset of the pretraining data with annotations.\nIt is to be noted that GT annotations were not used in the\npretraining stage."}, {"title": "3.3. Supervised fine-tuning", "content": "For every task, we consider two experiments: a) Baseline:\nResNet50 (He et al., 2016) initialized with ImageNet\npretrained weights (23M parameters) b) The ViT-S encoder\npre-trained on cardiac MR images (21M parameters).\nResNet50 was chosen for its similar number of parameters.\nBoth networks were trained on labeled data available for\neach task, in an identical training setup.\nFor classification and detection tasks, a single linear layer\nwas added on top of the features from the respective\nencoders, similar to the linear evaluation in this study (Caron\net al., 2021). Freezing/finetuning of the encoder was treated\nas a hyperparameter, and chosen based on performance on\nthe respective validation split for each experiment. For the\nResNet50 encoder, the feature vector was extracted and\naveraged along the spatial dimension to get a feature vector\nof dimension 2048. For the ViT-S, we concatenate the [CLS]\ntoken of the last 4 blocks, along with the mean of the patch\ntokens from the last block, to get a feature vector of\ndimension 1920. Both models were trained with Cross\nEntropy loss.\nFor segmentation and landmark tasks, the trained\nResNet50 and ViT-S/8 encoders were used with UNet decoder\nto create a ResNet50-Unet (lakubovskii, 2019) or a\nUNETR(Hatamizadeh et al., 2021) respectively. The models\nwere trained with Jaccard loss(Bertels et al., 2019), with\nSoftmax activation on the last layer. Prior studies (Baharoon et\nal., 2023) as well as our empirical experiments demonstrated\nhigher accuracies for end-to-end finetuning, so both models\nwere finetuned end-to-end for the segmentation tasks. The\nlandmark localization was treated as a regression task, with\nthe network trained to predict a Gaussian heatmap centered\non the landmark, similar to the set up in Xue et al. (2021).\nIn all tasks (except for the detection tasks, as noted in Section\n3.2) the model works on a single image, repeated in the channel\ndimension to create a 3-channel input. All images were\nresampled to 1 mm x 1 mm resolution, and center cropped to\n224 x 224. Intensities were capped at 98 percentile and then\nnormalized to 1 to 1. For each task, the available data was\ndivided into training, validation, and testing split, on the patient\nlevel. The best model was chosen based on the validation\nmetric, averaged over classes, on the specific validation split.\nThe metrics chosen were the Dice coefficient for segmentation,\nper-class accuracy for classification (number of correct\npredictions/total number of samples per class) and detection\ntasks, and the absolute Euclidean distance error in mm for\nlandmark tasks."}, {"title": "3.4. Effect of pretraining on CMR data", "content": "An alternative to pretraining on CMR data is to pretrain the\nFM on natural images. To evaluate the benefit of training\nspecifically on CMR data, we obtain the weights of a ViT-S/8\npretrained on natural images (Caron et al., 2021), and finetune\nit in the same framework. We choose three representative tasks\nfor the comparison: cine view classification (task 2), LGE SAX\nsegmentation (task 5A) and mapping SAX segmentation (task 6)."}, {"title": "3.5. Few-shot performance", "content": "To simulate a low annotated data task often encountered in\nmedical image analysis settings, we evaluated the few-shot\nperformance of both the baseline and the proposed model. The\namount of training data was varied systematically between 1512\nsamples per class, as available, in two representative tasks - cine\nview classification (task 1), and mapping SAX segmentation (task\n6). The encoder was kept frozen, to prevent overfitting."}, {"title": "4. Results", "content": "The results are shown in Table 2. The proposed model\noutperforms the baseline model in the majority of the cases.\nBoth models perform well in distinguishing different CMR image\ntypes, with minor improvements from the SSL pretrained model,\nespecially for the \"Others\" class. For the cine view classification\ntask (task 2), we observe an increase of 6.8 percentage points\n(pp) over all classes, ranging from 4.7 pp in SAX to 27.2 pp for\nthe \"2CH\" class. While the proposed model achieves excellent\naccuracies for the more common views (short and long-axis),\nboth models face challenges with the \"Others\" class, which is a\nheterogenous class, consisting of other views (such as LVOT,\n5CH, etc.) as well as non-diagnostic images from regular views\n(such as chamber not visible). For the segmentation tasks, we\nobtain improvements from 0.1 to 1.8 pp, depending on the class\nand task. It is interesting to note that we get greater\nimprovements in task 5A, relative to task 5B for LGE SAX\nsegmentation. This could be related to the smaller finetuning\ndataset in task 5A, where the learned features of the CMR SSL\npre-trained model provides a greater benefit. For the landmark\ndetection task, we achieve mixed results, with the proposed\nmodel outperforming the baseline for one landmark, and vice\nversa for the other. In both segmentation and landmark\ndetection, we also observe generally lower standard deviations\nin the metrics for the proposed method. In the pathology\ndetection tasks, the proposed model obtains higher average\naccuracies in all tasks. We observed improvements of 6.7 and\n13.7 pp for the LGE detection tasks, with the greater\nimprovement in task 8A, with the greater amount of data. The\nresults from the CMR-SSL model are presented against\nperformance reported in prior studies (where available) in Table\n3. Performance from prior studies is presented as a range due to\nthe fragmented nature of the field, with evaluations on private\ndatasets being the norm. The out-of-box performance of the\nproposed model (fine-tuned without any task-specific\noptimization of method or hyperparameters), is comparable or\nsuperior in many tasks across classification, segmentation, and\nlandmark localization.\nFor the disease detection (task 11), the proposed model\noutperforms the baseline by 14 pp. Recognizing\ncardiomyopathies is a complex task, and more careful study\ndesign such as incorporating multiple spatial locations and\ntemporal dynamics through metrics like ejection fraction will\nalmost certainly improve performance for both models, similar\nto the SoTA (Table 3). However, the scope of the current study is\nto assess the information content of the raw features obtained\nfrom both the encoders. We see that the SSL pretrained model\nextracts more relevant features, as evidenced by the higher\n\"out-of-box\" accuracy when compared to the baseline method."}, {"title": "4.1. Downstream tasks", "content": "In most settings (Table 4), the CMR pretrained model\noutperforms the ViT-S/8 model pretrained on natural images\n(NI-VITS). Interestingly, the NI-VITS achieves competitive\nperformance in the classification task on some classes, while\nlagging behind both the proposed and the baseline training\nin segmentation tasks, with a wider gap in the task with the\nsmaller finetuning dataset (task 5A). This trend is also\nobserved in the cardiac segmentation task in this\nstudy(Baharoon et al., 2023). This might be explained by the\nfact that classification can be done on more global features\nwhile segmentation requires fine-grained features at a\nsmaller scale. Pre-training on CMR images provides more\nrelevant representations at the finer scale."}, {"title": "4.2. Effect of pretraining on CMR data", "content": "Figure 5 shows the test metric averaged across classes for cine\nview classification, and Mapping segmentation when using very\nfew labeled samples for training. The proposed model\noutperforms the baseline model in all configurations. It also\nreaches convergence accuracy faster than the baseline model."}, {"title": "4.3. Few-shot performance", "content": "In this study, we evaluated a vision foundation model trained\nin a self-supervised manner, on large amounts of CMR data on a\nwide variety of downstream tasks relevant to a clinical workflow.\nWe achieve performance comparable to the stateof-the-art for\nclassification, segmentation, and landmark localization tasks,\nand promising results for disease detection, with no task-\nspecific optimization. We compare against a comparable fully\nsupervised DL network, without large-scale pretraining and\ndemonstrate that targeted SSL pretraining can benefit all tasks\nin terms of accuracy and robustness, across a wide range of\nresource (labeled data) settings. The proposed method thus\npresents a resource-efficient, unified framework to tackle a\ncardiac MR imaging workflow, providing opportunities for faster\ntime to deployment in real world settings. While pretraining the\nFM is a computationally intensive exercise, it is a onetime\nactivity, providing an encoder that can then be used across\ndifferent tasks. This study explored the feasibility of such a\nframework in very basic settings. Further task-specific\noptimization, motivated by clinical knowledge of the task can\npotentially improve the results, especially in more complex\ntasks such as disease detection. Parameter efficient training\nmethods such as LoRA(Hu et al., 2021) could provide faster\npretraining times. Model distillation methods can be used to\nobtain a more lightweight model for deployment. Another\navenue of research is to create a unified, cardiac-specific FM\ntrained on images from multiple modalities such as cardiac\ncomputed tomography and echocardiography. Such a model\ncould implicitly utilize the strengths of various imaging\nmodalities to provide stronger image representations. In\naddition, weakly supervised training can enable learning from\naccompanying clinical reports to create multi-modal foundation\nmodels."}, {"title": "5. Conclusions", "content": "Acknowledgements: This research was conducted using the\nUK Biobank Resource under application number 30769.\nDisclaimer: The concepts and information presented in this\npaper are based on research results that are not commercially\navailable. Future commercial availability cannot be guaranteed."}, {"title": "6. Appendix", "content": "Center 1: Data from this center constitutes of CMR studies\nperformed on 1.5T scanners (MAGNETOM Aera, Siemens\nHealthineers AG, Erlangen, Germany). Long-axis and short axis\nviews covering the entire LV were obtained using balanced\nsteady-state free-precession sequence (b-SSFP). LGE images\nwere acquired after injection of a bolus of gadolinium-based\ncontrast agent (Dotarem, Guerbet, France, 0.1 mmol/kg). Stress\nperfusion imaging was performed using a saturation-prepared\nb-SSFP sequence. A series of six slices (four short-axis views, in\naddition to 2- and 4-chamber views) were acquired every other\nheartbeat. Single-breath-hold 3D T1-weighted inversion\nrecovery gradient-echo sequence was acquired with the same\nprescriptions to detect LGE. The inversion time was individually\nadjusted to null normal myocardium. Other images in the\ndataset include those acquired with T2 weighted sequences\nsuch as HASTE and STIR, T2 star weighted, compressed sensing\ntechniques, localizer scans, etc.\nCenter 2: The datasets from this center were acquired on 1.5T\nand 3T MRI systems (MAGNETOM Avanto and Skyra, Siemens\nHealthineers AG, Erlangen, Germany). CMR studies included"}, {"title": "6.1. Private datasets", "content": "cine bSSFP images in 2-, 3-, and 4-chamber LAX, and a stack of\nSAX slices. The protocol also included cine images focussed on\nthe LV outflow track and aorta, at various planes. LGE images,\nboth single-shot and segmented, were acquired as single-\nbreath-hold T1-weighted inversion-recovery gradientecho\nimages acquired 10 minutes after contrast injection. The dataset\nalso included first-pass myocardial perfusion imaging at rest\nand adenosine stress. The patient cohort includes both normal\npatients, as well as patients with cardiomyopathies such as\ndilated cardiomyopathy, hypertrophic cardiomyopathy, ischemic\nheart disease, and myocarditis.\nCenter 3: The data from this center includes 144 clinical\nsubjects (52 normal, 49 myocarditis, 20 sarcoidosis, 23 systemic\ndisease) were scanned on a 1.5T MRI system (MAGNETOM Aera,\nSiemens Healthineers AG, Erlangen, Germany). CMR studies\nincluded cine bSSFP images in 2-, 3-, and 4chamber LAX, and a\nstack of SAX slices. The cine imaging included views of the LV\noutflow track and aorta. LGE imaging consisted of Inversion\nRecovery FLASH acquired 10 minutes after contrast injection in\nseveral short-axis planes. Native\nand post-contrast T1 Modified Look-Locker Inversion recovery\n(MOLLI) and T2 prepared fast-low-angle shot maps were\nacquired. This dataset was not used in the pretraining stage."}]}