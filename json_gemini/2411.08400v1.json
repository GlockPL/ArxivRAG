{"title": "\u0412\u0410\u041c\u0410\u0425: Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning", "authors": ["Geetansh Kalra", "Amit Patel", "Atul Chaudhari", "Divye Singh"], "abstract": "Autonomous robots collaboratively exploring an unknown environment is still an open problem. The problem has its roots in coordination among non-stationary agents, each with only a partial view of information. The problem is compounded when the multiple robots must completely explore the environment. In this paper, we introduce Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (BAMAX), a method for collaborative exploration in multi-agent systems which attempts to explore an entire virtual environment. As in the name, BAMAX leverages backtrack assistance to enhance the performance of agents in exploration tasks. To evaluate BAMAX against traditional approaches, we present the results of experiments conducted across multiple hexagonal shaped grids sizes, ranging from 10x10 to 60x60. The results demonstrate that BAMAX outperforms other methods in terms of faster coverage and less backtracking across these environments.", "sections": [{"title": "1 Introduction", "content": "Autonomous exploration by robots in unknown environments has diverse applications such as search and rescue, environmental monitoring, and disaster management, and is still an open challenge [7]. Moreover, individual robots often struggle with limitations in coverage, efficiency, reliability, resiliency, and adaptability when operating in complex and dynamic environments.\nTo overcome these challenges, multi-agent collaborative systems have gained attention [19]. By leveraging collective knowledge and coordinating actions, these multiple agents can explore the environment more effectively, leading to improved coverage, robustness, and information exchange [17,3]. However, collaborative strategies may encounter challenges such as navigating local extrema or overcoming dead ends [14].\nIn this paper we present an approach called Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (BAMAX, for short). Multi-agent reinforcement learning is chosen as the foundation for our approach due to its capability to enable agents to learn optimal behaviors through interactions with the environment, leveraging rewards and penalties to enhance decision-making abilities. Our method allows multiple robots to autonomously explore and construct"}, {"title": "2 Related Work", "content": "The traditional methods for autonomous exploration in unknown environments identify frontier cells as boundaries between known and unknown areas. In an approach discussed in the work [18], each robot maintains a global evidence grid, integrating its local grid with the global map at the frontiers. By summing log odds probabilities, a collaborative and decentralized system is achieved. Another approach [21] utilizes a bidding protocol for assigning regions with high unknown areas to robots. The work discussed in [2] proposed a navigation approach with safety regions and a Next-Best-View algorithm.\nWhile traditional methods have shown success, incorporating high-level knowledge like structural patterns remains a challenge [5]. To address this challenge and to improve exploration strategies in unknown environments, recent advancements in Deep Reinforcement Learning (DRL, for short) offer potential. A novel approach introduced in the work [15] leverages raw sensor data from an RGB-D sensor to develop a cognitive exploration strategy through end-to-end DRL. Similarly, the D3QN algorithm discussed in the work [13] enables mobile robots to gradually learn about their environment and autonomously navigate to target destinations using only a camera, avoiding obstacles.\nIn dynamic environments, [8] employed a deep neural network with long short-term memory and a reinforcement learning algorithm for effective robot navigation. Another study by [16] utilized environment information as input to a neural network and trained agents using the asynchronous deep deterministic policy gradient algorithm. To extract and utilize structural patterns, [20] intro-"}, {"title": "3 Our Approach", "content": "In this section, we will discuss the hexagonal grid environment used in our experimentation and the setup of reinforcement learning for our problem. We will also explain the architecture and how data flows in our method, B\u0410\u041c\u0410\u0425."}, {"title": "3.1 Environment", "content": "The environment used in this paper is a two-dimensional grid maze, Ga, with d number of hexagonal cells across height and width. The hexagonal shape was selected for the unit cell in our environment because out of all regular polygons, hexagon is the shape with the highest number of edges which creates a regular tiling in the euclidean plane (other being square and triangle). This would facilitate consistency while also providing a higher complexity in terms of choice of directions available to move from one cell to another. Moreover, to allow for distinct ingress and egress points for each cell, we ensure that more than two sides are left open while creating the maze. An illustration of our hexagonal grid maze (G10) can be seen in figure 1."}, {"title": "3.2 Building A Reinforcement Learning Model", "content": "In our study, we utilize the DQN algorithm [10] to approximate the Q-function, which represents the optimal action-value function using a deep neural network architecture. By taking the agent's observations as input, the DQN architecture generates Q-values for various actions. These Q-values indicate the expected cumulative rewards that the agent can attain by taking specific actions in a given state. The agent's objective is to select actions with higher Q-values, thereby maximizing long-term rewards and making optimal decisions in the environment (Eq 1).\n$Q(s,a) = E [r+max Q(s', a') \\s, a]$\nState Space To capture the information contained in the environment effectively, we extracted the following six distinct sub-states.\n1. Sexp: Image representing the area of the map that has been explored.\n2. Sunexp: Image corresponds to the area of the map that is yet to be explored.\n3. Sagents-pos: Image indicating the location of the agent in the map.\n4. Sother-agents-pos: Image showing the positions of other agents in the map.\n5. Swalls: Image containing the information about the walls present in the explored area.\n6. Slocal-obs: This component provides a local observation for each agent, capturing essential information such as the presence of walls, the presence of predators, and whether neighboring cells have already been explored in each direction relative to the agent's current position.\nThis decomposition allows us to utilize relevant features from different aspects of the environment to enhance the learning and decision-making capabilities of our method. These sub-states together form a single state and the entire state space, S for our method is represented as 2.\nState space, S = {sk = (Sexp-map, Sunexp-map, Sagents-own-pos,\nSother-agents-pos, Swalls-map, Slocal-obs) | k \u2208 {agents}}\nAction Space The action space A consists of six distinct actions, denoted by a\u017c available to the agent. The actions correspond to the movement of the agent along the six sides of a hexagon cell.\nReward In our specific problem, we aimed our agent to consider increasing rewards through exploration and incentivize moving toward unexplored regions to maximize overall exploration. In order to capture this, we formulated the reward as comprising of two components as seen in equation 3\n$r_t = r_{immediate,t} + r_{surrounding,t}$"}, {"title": "3.3 Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (\u0412\u0410\u041c\u0410\u0425)", "content": "In this section, we discuss the architecture of BAMAX as shown in the figure 2. In our approach, we decompose the environment into six parts to enhance representation, as described in section 3.2.\nImage Reshape Layer receives the five of these six parts as the input, and transforms the state space images of varying sizes into a standardized fixed size. By incorporating the Image Reshape Layer, our architecture maintains flexibility and adaptability to different environment sizes."}, {"title": "4 Experiments and Results", "content": "In this section, we present the experimentation conducted to evaluate the effectiveness of our proposed approach. We describe the setup and metrics used to measure the performance of our method."}, {"title": "4.1 Experiments", "content": "To evaluate the effectiveness of our method, we conducted experiments on various environment sizes, including G10, G20, G40, and 960. We compared the performance of our approach, BAMAX, with traditional methods such as Depth-First Search (DFS) [11] and Breadth-First Search (BFS) [12]. To enable collaborative exploration, we extended the Distributed DFS algorithm [9] and BFS algorithm [1] to create Collaborative DFS and Collaborative BFS, respectively. These adaptations allow multiple agents to explore the grid map together. In our experiments, we generated 100 environments for each size, and in each scenario, we deployed 4 agents to explore the environment. Each agent started from a different random point, while ensuring a common starting point for all 4 agents across different methods, ensuring a fair comparison. We analyzed the results of the experiments based on following two key metrics.\n1. Backtrack Count quantifies the frequency of backtracking performed by all agents, counting each transition from one point to another as a single backtrack step.\n2. Simulation Steps measures the count of timesteps taken collectively by all agents to explore the entire maze. Each simulation step represents a single step taken by all agents."}, {"title": "4.2 Results and Discussion", "content": "The performance of different methods across diverse environments is summarized in Table 2. The experimental results clearly demonstrate that our proposed method, BAMAX, outperforms traditional algorithms in terms of achieving faster 100 percent grid coverage with the minimum number of steps. The difference in the number of steps between BAMAX and the traditional algorithms is significant across various grid sizes. This could be attributed to the fact that algorithms like DFS and BFS are greedy approaches, which could prioritize certain paths without considering potentially better alternatives. In contrast, BAMAX understands the importance of effectively collaborating with other agents to optimize exploration. This significant difference arises from BA-MAX's ability to intelligently select the most promising branches to explore,\nresulting in fewer steps required to cover the entire grid."}, {"title": "5 Conclusion", "content": "This paper presents Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (BAMAX) for collaborative exploration in hexagonal environments. Through extensive experimentation in environments of varying sizes, we compared BAMAX with traditional approaches. The results demonstrated BA-MAX's capability to explore hexagonal grids consistent performance in exploration efficiency. Further, the experimentation also showcases BAMAX's ability to translate its learning from training on a grid size of 10x10 to efficient explore hexagonal grids of different sizes. This showcases good robustness and scalability across different environment sizes. Currently, BAMAX is able to extend its learning to grids of different sizes with only hexagonal cells. As part of future work, we would expand BAMAX's ability to handle grids with different sizes and also shapes."}]}