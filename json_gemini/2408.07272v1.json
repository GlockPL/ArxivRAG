{"title": "NL2OR: Solve Complex Operations Research Problems Using Natural Language Inputs", "authors": ["Junxuan Li", "Ryan Wickman", "Sahil Bhatnagar", "Raj Kumar Maity", "Arko Mukherjee"], "abstract": "Operations research (OR) uses mathematical models to enhance decision-making, but developing these models requires expert knowledge and can be time-consuming. Automated mathematical programming (AMP) has emerged to simplify this process, but existing systems have limitations. This paper introduces a novel methodology that uses recent advances in Large Language Model (LLM) to create and edit OR solutions from non-expert user queries expressed using Natural Language. This reduces the need for domain expertise and the time to formulate a problem. The paper presents an end-to-end pipeline, named NL2OR, that generates solutions to OR problems from natural language input, and shares experimental results on several important OR problems.", "sections": [{"title": "1 Introduction", "content": "Operations research (OR) is a field that has been widely used to enhance decision-making by creating and using mathematical models of real-world situations to find solutions [9]. A major stream of OR solution is to use mathematical programs (MP) which consist mathematical representations of decision variables, optimization objectives, and decision constraints. For instance, in the area of e-commerce order fulfillment, a resource allocation MP aims to minimize total fulfillment cost, utilizing limited inventories located across the merchant's warehouse/retailing network. The feasible fulfillment decisions are required to satisfy operational constraints, e.g. channel restrictions, region restrictions, logistic restrictions, and etc.. Another OR problem example, in the area of resource scheduling optimization, a scheduling planner aims to optimize the utilization of human agent resources to accomplish tasks with skill set specifications. The feasible scheduling decisions are required to satisfy operational constraints, e.g. staffing requirements, shift scheduling, skill matching and etc.. For physical tasks,"}, {"title": "2", "content": "the scheduling planner will also need to minimize agent traveling, and making sure agent traveling schedules satisfy time windows for both task reservations and agent shifts. In both examples, the MP aims to optimize a certain objective function, subject to a set of operational constraints.\nHowever, in the related industries, modeling an OR problem and developing an MP solution will require specialized knowledge set (e.g. mathematical modeling and operations research algorithm design training). For instance, in the e-commerce order fulfillment example, one needs to have knowledge in inventory optimization, transportation optimization, and logistics optimization. Therefore, the development of MP solutions is often left to a small group of domain experts. Moreover, developing and deploying a computer-based solution may take months. This long development cycle can be a barrier to OR solution adoption, especially in the context of small and medium-sized enterprises.\nTo overcome this barrier, the field of automated mathematical programming (AMP) has emerged, aiming to automate the process of developing MP solutions. AMP aims to enable non-experts to develop and deploy MP solutions with little or no human intervention. AMP systems can be divided into two categories: closed-box and open-box systems.\nClosed-box AMP systems use machine learning techniques to learn the mapping between the input data and the optimal solution [23,11]. These systems require no prior knowledge of the mathematical structures. However, the solutions produced by black-box systems are often sub-optimal and cannot be guaranteed to satisfy operational constraints.\nOpen-box AMP systems, on the other hand, are based on mathematical programming techniques and require the user to specify the mathematical model of the problem [10,13,21]. These systems start from detailed business logics and a problem data set, and generate a solver-specific script (also known as a concrete model) for user to review. After the generated model being reviewed and validated, it is able to guarantee that the solution satisfies the operational constraints, and can often produce optimal solutions. However, the user needs to have a good understanding of the mathematical programming problem and how to interface with a variety of solvers/libraries (e.g. Gurobi, CPLEX, ORtools, AMPL).\nTo mitigate the limitations these drawbacks of existing closed- and open-box AMP systems, we propose a novel methodology that create new OR solutions and edit existing OR solutions using natural language (NL) queries. Recent advances in natural language processing (NLP) have led to the development of large language models (LLMs) that can be used for human language text generation. These models have been used for various tasks, such as language translation, question answering, and text summarization [6,24,5,4,1,16]. We propose to use LLMs to generate markup language passages for OR problems, which can then be converted into programming language passages. This technique entails obtaining user input for an OR problem, formulating an NL prompt grounded in the user input, generating a domain-specific language (DSL) that captures the OR problem, transforming the DSL into an executable script (with a suitable"}, {"title": "3", "content": "solver triaging), and producing a solution by executing the script. Our proposed method can significantly reduce the time and effort required to model and solve complex OR problems, which 1) reduces the time required to formulate a problem in a solver-specific format, and 2) provides a framework for the development of an interactive OR problem-solving tool that can be used by non-experts.\n1.1 Contributions and Organization\nOur paper distinguishes itself from existing work on the intersection of natural language processing and automated mathematical programming in the following ways:\n1.  We are (to the best of our knowledge) the first to handle abstract OR model creation and editing, allowing for operations on a model contract that can be instantiated into multiple concrete models when parameter data sets are provided. Our approach empowers LLMs to create and edit an abstraction of a class of OR models, such as order fulfillment models. This abstraction is particularly important for industrial solution providers, as it enables them to work with the model abstraction class instead of having to work with each individual concrete OR models.\n2.  We are also the first to handle automatic solver triage. In an OR problem-solving cycle, selecting a proper solver is as important as building a mathematical program. All existing MP interfaces require developers to manually decide which solver to choose, while our approach handles solver triage through a solver interface unification. This innovation streamlines the problem-solving process and saves time for developers.\nWe have implemented the proposed methodology as an end-to-end pipeline, named NL2OR, that can 1) take natural language input and generate an abstract OR model; 2) resolve data mapping and triage solvers to provide a solution to the generated OR problem; 3) edit the generated OR model for what-if analysis. In this paper, we describe the details of our proposed method, implementation details and present experimental results on several OR problems. The rest of this paper is organized as follows. Section 2 provides an overview of related work. Section 3 describes our proposed method in detail. Section 4 presents experimental results on several OR problems. Finally, Section 5 concludes the paper and provides directions for future work.\n2 Related Works\n2.1 Large Language Models\nA large language model (LLM) [25] is a sequence-to-sequence model designed for predicting sequences of text-based tokens. This model has undergone training on a substantial volume of text data to enhance its ability to generate accurate predictions for sequences of tokens in a given context. In recent years, the"}, {"title": "4", "content": "Transformer architecture [17] has revolutionized the field by encouraging efficient parallelization of training and scalability. There have been many foundational LLM models such as BERT [6], OPT [24], PaLM [5], GPT-3 [4], GPT-4 [1], LLAMA [16], Gemini [15], and many more.\nPrompt engineering is a vital step in generating quality responses from a language model. A popular approach is to perform in-context learning [4,19,3], which is an emergent ability of LLMs where it can perform complex tasks by conditioning it on a description of the problem and few-shot examples. There are many approaches that have been shown to dramatically increase the model inference accuracy such as Chain of Thought (CoT) [20], Self-consistency with COT (COT-SC) [18], Tree of Thoughts (ToT) [22] and Graph of Thoughts (GOT) [2].\n2.2 Automated Mathematical Programming\nAn automated mathematical programming (AMP) system aims to automate the process of developing MP solutions with little of no human intervention. The first research stream of AMP focuses on developing an inference system to derive solutions based on historical data. [23] surveys ML-enhanced approaches that solve mix-integer programming and combinatorial optimizations. For example, researchers have found a close-box system using neural network approximations significantly improves the efficiency of modeling and solving the class of SCIP problems in the OR space. The other stream of AMP research considers the formulation of the MP problem to be articulated in natural language and subsequently translated into MP solutions utilizing LLMs [10,13,21]. Recent studies have demonstrated the efficacy of leveraging pretrained LLMs for AMP tasks [10,13,21,12]. Due to their capacity to obviate the need for custom model training or fine-tuning, pretrained LLMs emerge as an appealing solution for AMP endeavors. Our work differentiates from existing research, as we focus on creating and editing abstract OR models, and our approach is solver-agnostic.\n3 Methodology\nThe main objective is of NL2OR is to convert a natural language description of an OR problem into an OR model, and then generate executable artifacts. These executables are known as abstract models, which have data input contracts. By providing data following an input contract, a concrete model instance can be solved by a selected solver. By solving the concrete model, the NL2OR pipeline can generate problem solutions with reports. This takes considerable engineering efforts to develop various parts of the pipeline. In this section, we will go through the architecture of our system in-depth. An overview of the architecture is given in Figure 1.\nThe NL2OR pipeline is composed of four major components: Domain Specific Language (DSL) Generator, Framework for OR Analytics (FORA) Builder, FORA Executor and Report Generator. The pipeline starts by admitting a user"}, {"title": "5", "content": "query to identify the user intention, whether the query is asking to generate a new OR model or edit an existing OR model. The DSL Generator is responsible for converting the natural language query into a formal representation of the optimization problem. The FORA Builder takes the formal representation and generates a FORA model that can be executed by the FORA Executor. The FORA Executor takes the FORA model and user specified parameter data sets, and executes it, returning an optimal solution if the concrete model is feasible. Finally, the Report Generator takes the executor's output and generates a report that can be easily understood by the user. To illustrate the pipeline, we use the example of optimizing a food purchasing plan to satisfy nutrition constraints while minimizing food purchasing cost.\n3.1 DSL Generator\nThe input constract of the DSL generator variates based on a job type, i.e. creation or edition. A creation job only requires a user query, while a edition job also requires providing an original model YAML file.\nGiven a user input, the first step in the DSL generator is to construct a DSL generation prompt. Prompt engineering is directly interlinked with the quality of the LLM output. We streamline this process by automating prompt construction within our system. A prompt builder module is designed to automatically generate a prompt for the LLM by amalgamating various components, including instructions, syntax overview for the (OR) model YAML, few-shot learning instances, and the user's specific problem statement. Subsequently, this constructed prompt is inputted into the LLM to facilitate the generation of the DSL, for a new OR model or an updated model.\nAn illustrative example of a model generation prompt is depicted in Figure 2. From the user's perspective, the complexities involved in OR model YAML and LLM prompts are abstracted away. Users must only provide a query elucidating the OR problem they aim to address. By concealing these technical intricacies, we empower users to focus solely on articulating and engaging with solutions to their OR problems without being burdened by the internal mechanics of the system. See Figure 2 for an illustration of building a food purchasing planning optimization problem.\nFollowing the generation of the OR model YAML, i.e. the DSL, it undergoes a series of validation, correction, and processing steps facilitated by error detection mechanisms to ensure its integrity as a valid OR model. Initially, the YAML is subjected to syntax error rectification, which involves automatically correcting property names, validating correctly labeled constraints, and rectifying various Python expression errors. These corrections aim to preempt the need for YAML regeneration if syntax errors are the only remaining issues.\nSubsequently, beyond syntax correction, early error detection mechanisms are deployed to identify irreparable errors. These mechanisms include schema validation, detection of redefined variable declarations, and identification of undefined variables. Upon detection of any of these errors, the system throws an error and logs it; as such, errors cannot be rectified programmatically."}, {"title": "6", "content": "Validation of the YAML involves employing a custom JSON schema and defining permissible properties and valid formats for each property. For detecting redefined variable declarations, the input and decision variables are scanned to identify any duplicated names. Upon the absence of duplicates, these variables are cached as the defined set of variables, which aids in detecting undefined variables. Detection of undefined variables involves transforming Python expressions into abstract syntax trees (ASTs), enabling the extraction of non-control loop"}, {"title": "7", "content": "variables and subsequent verification of their prior definition as input or decision variables.\nAfter completing these post-processing steps and confirming the absence of detectable errors, the system may still contend with the possibility of runtime errors. Such errors may stem from either a malformed OR model or mismatches between user data and the OR model specifications. In cases where the OR model is malformed, regeneration of the model is necessary, necessitating a restart of the entire process. An output of the food purchase planning optimization model creation is depicted in Figure 3.\nA model edition prompt has the same structure with the creation prompt, with additional Original YAML file sections in few shots examples and user prompt guides. An illustrative example is depicted in Figure 4. An edition of OR model YAML file output, after DSL post processing, is illustrated in Figure 5. The model YAML file editions are highlighted.\n3.2 FORA Builder and Executor\nAfter the LLM post-processing step, assuming successful completion, a model YAML file is transpiled into an FORA model. This transpile step primarily involves the instantiation of entities within the internal FORA library, with metadata declared in the InputData property of the YAML. The InputData property"}, {"title": "8", "content": "is a contractual agreement between the user and the OR model, delineating the requisite data for the solver to optimize the OR model. Consequently, it falls"}, {"title": "9", "content": "upon the user to furnish the specified data accordingly. Notably, the method of providing data to our system is agnostic to application specifics and, therefore, lies outside the primary scope of this work.\nAn illustration of this process is provided in Figure 6 for the objective. Upon loading the YAML, the objective of the OR model is instantiated by extracting the construction definition that delineates the objective. Subsequently, the associated Python expression is executed, incorporating the specified sense parameter, following which the objective is incorporated into the OR model. This"}, {"title": "10", "content": "procedural pattern remains consistent for the remaining properties in the OR model YAML.\nobj = Objective (\n constructor=exec(or-yaml [\"constructor\"]),\n sense=or_yaml [\"sense\"]\n)\nmodel.add_obj(obj)\nThe generated YAML file also indicates solver specific properties, so that a generated abstract model is triaged with a proper solver during the DSL generation phase. The generated artifacts from the FORA builder is an abstract model program.\nIf the user furnishs input data sets, the FORA executor is then able to instantiate a concreate model. By executing the optimize method of a concreate model with the triaged solver, the FORA executor then either generates an optimal solution for feasible concreate models or returns a status flag with logs indicating infeasibility, early stop or suboptimality."}, {"title": "11", "content": "3.3 Report Generator\nOnce FORA is employed to execute a concrete model, yielding a solution or status log, the Report Generator facilitate further analysis of the results, storing this solution in a database may be necessary. The responsibility of constructing the database schema for the solution falls under the purview of the report summarization step. The report prompt builder plays a vital role in this process by assimilating details about the OR problem and its solution, abstracting any information related to sensitive user data, and generating a report prompt tailored for the LLM. Utilizing this prompt, the LLM generates a database schema designed explicitly for the solution to the OR model. An illustrative example of the resulting database schema is presented in Figure 7.\nAfter establishing this database schema, the report YAML undergoes processing, and the OR solution is transformed into an intermediate data format conducive to being seamlessly written as rows to a database. This transformative step prepares the OR solution data for efficient storage and retrieval in a structured database environment.\n4 Experiments\n4.1 OR Model Creation\nFor initial experimentation aimed at assessing the capabilities of NL2OR, we evaluate its proficiency in generating a valid OR model that can be executed from a specified user query. The experimentation involved assessing NL2OR across a spectrum of 30 distinct OR problem instances, spanning various scenarios encountered in practical applications. A comprehensive overview of these problem instances is presented in Table 1. Additionally, we explored the performance of NL2OR using two different LLM models, gpt-35-turbo-16k and gpt-4-32k, each"}, {"title": "12", "content": "subjected to varied temperature values. The models were evaluated based on two key metrics: Valid@k and Latency. Valid@k is the average number of successes generating a valid OR model when given k attempts. At the same time, Latency is the amount of time, in seconds, required for NL2OR to generate and execute the OR model. The results are given in Table 2.\nAs one might expect, gpt-4-32k has a higher latency compared to gpt-35-turbo-16k due to it having a more significant number of parameters. Additionally, we see that gpt-4-32k outperforms gpt-35-turbo-16k w.r.t. the Valid@k metric across all temperature values, showcasing its superior performance.\nBoth models performed similarly in producing valid YAML outputs of the OR model for well explained real world scenarios such as inventory assignment, network flow optimization or resource scheduling. One of the major differences between the gpt-35-turbo-16k and gpt-4-32k models was in the handling of more ambiguous mathematical optimization such as optimizing the Knapsack problem or graph based algorithm solving with linear programming, where the gpt-35-turbo-16k consistently failed to produce a YAML describing a valid OR solution."}, {"title": "13", "content": "4.2 OR Model Edit\nAn essential practical application of NL2OR is editing existing OR models. We characterize OR model edits as introducing alterations or augmentations to the properties outlined in the OR model YAML. Such refinement proves invaluable when addressing minor adjustments necessary to accommodate evolving customer or business demands or validating prospective alterations to the foundational system upon which the model relies. This is also known as What-if analysis. For instance, this functionality proves invaluable when integrating a new product into a resource allocation problem or assessing the impact of updated material constraints on the resultant outcomes.\nThe test data is constructed as follows. We select 15 validated OR model YAML as original YAMLs, and we use GPT-4 to create possible model update"}, {"title": "14", "content": "queries. For each YAML file, four edition queries are generated, and thus 60 different edit scenarios are prepared. The test over 60 different edit scenarios where the results are given in Table 3\nLike the generation experiments, gpt-4-32k exhibits superior performance over gpt-35-turbo-16k concerning the Valid@k metric across all temperature settings, albeit with an associated increase in Latency. Across the entire spectrum of temperature values, NL2OR consistently demonstrates its capability to incorporate user-specified modifications into the OR model accurately. A comparative analysis with the creation experiments underscores the relative simplicity of modifying an existing OR model compared to generating a novel one. Intuitively, this makes sense, as translating user intentions into components of an OR model is much easier when the prior underlying system is given."}, {"title": "15 Conclusion and Future Work", "content": "In this paper, we propose a novel methodology that creates new OR solutions and edits existing OR solutions using natural language (NL) queries. Our proposed method can significantly reduce the time and effort required to model and solve complex OR problems, which reduces the time required to formulate a problem in a solver-specific format and provides a framework for the development of an interactive OR problem-solving tool that can be used by non-experts. We have implemented the proposed methodology as an end-to-end pipeline, named NL2OR, that can 1) take natural language input and generate an abstract OR model; 2) resolve data mapping and triage solvers to provide a solution to the generated OR problem; 3) edit the generated OR model for what-if analysis. We have evaluated NL2OR across a spectrum of more than 30 distinct OR problem instances, spanning various scenarios encountered in practical applications. The results demonstrate that NL2OR can generate valid OR models with high accuracy and low latency. Additionally, we have shown that NL2OR can effectively edit existing OR models. In future work, we plan plan to explore the use of other LLM/SLM models on the performance of NL2OR. Additionally, we plan to explore the use of reinforcement learning to improve the performance of NL2OR. Finally, we plan to evaluate the performance of NL2OR on a larger dataset of OR problems and to compare its performance with existing AMP systems."}]}