{"title": "FLARE: FAITHFUL LOGIC-AIDED REASONING AND EXPLORATION", "authors": ["Erik Arakelyan", "Pasquale Minervini", "Pat Verga", "Patrick Lewis", "Isabelle Augenstein"], "abstract": "Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce Faithful Logic-Aided Reasoning and Exploration (FLARE), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on 7 out of 9 diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that FLARE allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.", "sections": [{"title": "1 INTRODUCTION", "content": "Complex Reasoning in natural Question Answering (QA) tasks assumes the capability to explore the problem space of the designated query with a formalised set of facts, relations, commonsense knowledge and logical implications. In line with this, LLMs have been enhanced with CoT (Wei et al., 2022) prompting, which supplements the QA process by generating intermediate reasoning chains given a set of in-context examples (Brown et al., 2020a), as shown in Figure 1. This allowed for advancement in commonsense (Madaan et al., 2022), symbolic (Wang et al., 2022; Sprague et al., 2024) and mathematical (Jie et al., 2023) reasoning. Although CoT allows for a problem exploration in natural language steps, such an approach has been shown to cause performance degradation for nuanced reasoning tasks involving multi-hop planning (Valmeekam et al., 2022; Suzgun et al., 2023), problem exploration (Yao et al., 2022) and arithmetic (Hendrycks et al., 2021b; Madaan & Yazdanbakhsh, 2022b). These discrepancies arise as CoT suffers from a limited ability to decompose, search, verify and backtrack using intermediate rationale chains (Yao et al., 2022), cascading hallucinations and errors (Ling et al., 2023) and that natural language might not be an optimal representation for describing the reasoning process (Li et al., 2024). Simultaneously, LLM output has been shown to be unfaithful and inconsistent w.r.t. the intermediate CoT rationale (Jacovi et al., 2024; Lanham et al., 2023b; Turpin et al., 2023). To circumvent the problem of CoT faithfulness"}, {"title": "2 FAITHFUL LOGIC-AIDED REASONING AND EXPLORATION", "content": ""}, {"title": "2.1 LLM SIMULATED SEARCH", "content": "FLARE comprises three modules for generating a plan, code and simulated search for answering a natural language query Q = {T\u2081 . . . T|Q|}, where each T\u2081 is a token."}, {"title": "Generating A Plan", "content": "For each query Q, given an LLM M, we initially prompt it to generate a plan, P, which should be comprised of task explanation, analysis and a plan for further formalising the query. An example of this can be seen in the plan section in Figure 1. We use in-context few shot examples Ep of such plan generations along with greedy decoding for obtaining the final plan.\nPi~arg max p\u043c(\u0422\u0456 | \u0415\u0440, Q) \t\t(1)"}, {"title": "Generating Code", "content": "After generating the plan, we prompt the LLM M to generate a Prolog code C, an example of which can be seen in Figure 1. We append executable code generation samples Csample to the previous in-context examples Ep and obtain few-shot code generation demonstrations Ec = [Ep; Csample]\nCi ~ arg max p\u043c(\u0422\u0456 | \u0415\u0441, Q,P)\nFcode, Rcode, G code = EXTRACT(Ci)\n(2)"}, {"title": "Benefits of Prolog", "content": "Prolog is a symbolic logic-programming engine (Bowen, 1979) used for heuristic search over Horn Clauses (Chandra & Harel, 1985). It is a declarative programming paradigm (Lloyd, 1994), meaning that the code is expressed as the logic of computation. In particular, this logic is formalised as a set of facts F and relations R forming our problem space, while the final goal G is a first-order logic combination of them. As a default, Prolog uses a depth-first search (DFS) strategy (Bowen, 1979) for sub-goal decomposition and feasible traversal of the problem space that satisfies the goal G. Such a traversal is referred to as the trace. At each trace step, the program can either confirm or invalidate the sub-goal using the feasibility of fact and relation combinations, expand the search tree or retry satisfying a failed sub-goal with new combinations. An example of such a search can be observed in Figure 1. It is possible to complete an exhaustive search, exploring all possible paths that do or do not satisfy the goal. These characteristics are beneficial as we can explicitly access and segment the facts and relations that form the problem space and the search strategy used for query formalisation. As Prolog is declarative, it is sufficient to use a regexp heuristic for the segmentation, which is referred to as EXTRACT in Equation (2) and Equation (3). Furthermore, including exhaustive traversal traces in-context allows the LLM to simulate sub-goal decomposition, backtracking, intermediate goal invalidation, etc. We discuss this in more depth in the next paragraph."}, {"title": "Simulating Search", "content": "After generating the logic-programming code, we want to simulate program execution by generating a problem space traversal trace with our LLM M. We update our in-context samples by appending search traces Ssample constructed from Prolog execution of sample codes Csample, i.e. Es = [Ec; Ssample].\nSi ~ arg max p\u043c(\u0422\u0456 | Es, Q, P, C)\nAsearch, Fsearch, Rsearch = EXTRACT(Si) \t\t(3)\nDuring iterative problem space traversal, we can segment the facts Fsearch, relations Rsearch, completed and backtracked paths with their answers Asearch used during the search simulation. To get the"}, {"title": "2.2 DETECTING REASONING INCONSISTENCIES", "content": "For each query Q given the code C and the simulated search S along with the extracted facts Fcode, Fsearch and relations Rcode, Rsearch from each designated module, we aim to detect the inconsistencies during the reasoning process of the LLM. We use exact string matching between all these facts and relations in code and simulated search.\nVi, j such that Foode = Fsearch \t\t(5)\n\nVi, j such that Rcode = Rsearch \t\t(6)\nWith this framework in mind, we define two reasoning failure modes. In the first failure mode, given that some fact or relation was used in the simulated search but did not exist in the generated code, i.e. \u2203j such that Fi Fsearch & Fcode, we claim that the LLM has hallucinated. We postulate that the model either produced incomplete knowledge during formalisation to code or created a piece of non-existing information during the search. We do not consider facts that emerged during a direct inference step within the simulated search during our calculation. For example, if we are dealing with a mathematical query 4 (5+6) =?, the search would involve separately evaluating the expression 5 + 6 = 11. In this case, 11 will not be treated as a hallucinated fact within the search but rather as an emergent fact obtained from direct inference. The second failure mode is the reciprocal case, where a fact or relation present in the code is not used during the search. We refer to this phenomenon as sub-optimal reasoning as it shows that the LLM could not explore the problem space completely or injected unsuitable knowledge during formalisation into code."}, {"title": "2.3 MEASURING FAITHFULNESS", "content": "We propose a method to measure the faithfulness of the LLM reasoning process when using FLARE. As mentioned in Section 2.1, for each query in a dataset D = [Q1, ..., Q|D|], we generate a set of codes \u03a6 = [C1,...,C|4|] and simulated problem space searches \u03a8 = [S1,...,S[4]|]. We use the Prolog engine to execute all of the codes I and obtain a set of correctly written programs \u03a6' and exact search paths \u03a8'. As we do not require explicit programmatic correctness during inference in FLARE for any code Ci, some Prolog executions resulting in an error are filtered out in \u03a8'. To assess model reasoning faithfulness towards code formalisations, we compare the search paths \u03a6' obtained from Prolog execution with their designated counterparts \u03a6'en generated by the LLM from the same"}, {"title": "3 EXPERIMENTAL SETUP", "content": ""}, {"title": "3.1 DATASETS", "content": "To evaluate FLARE, we use a benchmark of 9 tasks that cover Math Word Problems (MWP), multi-hop QA and relation inference. For testing numeric and mathematical reasoning, we follow CoT (Wei et al., 2022) by including GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), Multi-Arith (Roy & Roth, 2015), ASDiv (Miao et al., 2020) and AQUA (Ling et al., 2017). Among these, GSM8K, SVAMP, MultiArith and ASDiv cover elementary and middle school arithmetic word prob-lems with a set of integers or decimals as the answer. AQuA is a multiple-choice numerical, symbolic reasoning dataset where each answer is a mathematical expression containing notations, values and expressions not defined in the query. We also test FLARE using three multi-hop QA datasets. We use StrategyQA (Geva et al., 2021), which is a boolean QA task that requires sub-goal decomposi-tion and a multi-hop reasoning strategy to answer. The example \"Do all parts of the aloe vera plant taste good?\" used in Figure 1, is taken from StratedyQA. The multi-hop QA testing also includes Date and Sports Understanding, subsets of BIG-Bench (bench authors, 2023). The tasks involve inferring an exact date given some calculations in the relative time period and understanding if an artificially created sports statement is feasible. Finally, we assess FLARE on Relational Inference using CLUTRR (Sinha et al., 2019), which involves inferring the familial relation between two en-tities mentioned in a natural language description of the partial family graph. We adopt the same in-context samples used in (Xu et al., 2024; Wei et al., 2022). The complete descriptions, statistics and examples for each dataset can be found in Table 7 in Appendix A."}, {"title": "3.2 BENCHMARKS", "content": "We compare FLARE with CoT (Wei et al., 2022) as a prompting method that reasons using natural language chains and with F-CoT (Xu et al., 2024) that formalises the query into a code and offload the reasoning to an external symbolic solver. We use Llama3.1 (8B) (Dubey et al., 2024), CmDR"}, {"title": "4 RESULTS", "content": ""}, {"title": "4.1 MAIN RESULTS", "content": "To evaluate FLARE, we use a set of models of varying sizes on diverse benchmarks, as defined in Section 3. We compare the performance of each model while using FLARE, CoT and F-CoT prompting. The results for F-CoT and CoT on all the models are computed using the codebase of the original study (Xu et al., 2024).\nLLMs for general reasoning Our results, presented in Table 1, show that using FLARE allows the LLMs to achieve state-of-the-art results on 7 out of 9 datasets, with an average 28% increase over CoT. We can see a clear trend that FLARE increases the performance compared to CoT and F-CoT for all the models of varying scales. We also see that LLMs that are not explicitly tuned for coding suffer massive degeneracies when using F-CoT. We postulate that they are unable to consistently produce executable programs that satisfy a predefined scheme in F-CoT, thus resulting in an error during execution. This further highlights the value of simulating program execution using an LLM instead of using external solvers. The results show that using FLARE yields more benefit on datasets that require longer chains of multi-hop and symbolic reasoning, like AQuA and StrategyQA."}, {"title": "4.2 IS SIMULATING SEARCH USEFUL?", "content": "To understand if simulating a search over the problem space is useful, we compare the performance of FLARE where we only generate the plan without the subsequent code or search components. We refer to this framework setup as plan-only, which can be seen in Figure 1 if we were to use only the plan for answer generation. We completed this ablation using CmDR, CmDR+, and GPT-3.5, and we used GSM8K, AQuA, and StrategyQA as our baselines. The results in Table 2 confirm that all of the models suffer massive performance degradation from 61.1 \u2192 49.9 when omitting the code and the search components of FLARE. We hypothesise that this is caused by insufficient problem space exploration when using the plan-only setting. Furthermore, we have already seen in Table 1 that in methods, like F-CoT, that do not use simulated problem space exploration for soft-reasoning and only rely on plan and code, the performance also deteriorates even resulting in a complete breakdown of reasoning over the designated datasets. This can be viewed as a constrained version of FLARE with code-only execution. Consequently, our results show that simulating problem space traversal is highly beneficial as it avoids the pitfalls posed by plan-only and code-only modes by exploring the problem space more rigorously and soft-reasoning during that traversal instead of using external solvers."}, {"title": "4.3 FAITHFUL REASONING IMPROVES PERFORMANCE", "content": "As described in Section 2, using FLARE allows us to measure the faithfulness of the LLM reasoning process by comparing the simulated problem space traversals een with actual traces \u03a6' produced from a symbolic Prolog solver. To do this, we initially compute the percentage of syntactically correct executable code each LLM produces. We can see from the right part of Figure 3 that all of the models are capable of producing correct executable Prolog code in 67% of cases on average and > 50% of cases at the very least. This shows that the simulated searches 'ren can be considered a representative sample that will be further used to accurately measure the faithfulness of the simulated search w.r.t. the generated code. After measuring the reasoning faithfulness for each model, we want to understand what impact it has on the performance of the LLM. In Figure 2, we segment the models"}, {"title": "4.4 WHAT IS IMPORTANT DURING THE SEARCH?", "content": "We expand the analysis of the simulated search traces to detect the reasons which can lead to optimal reasoning within an LLM. For this purpose, we calculate several statistics, like the average number of explored paths, average and total hops and failures per path, for each model during the simulated traversal. The failure in a path is an invalidation of a solution for a sub-goal explored during the search, which is used for backtracking, as explained in Section 2. Calculating these statistics is simple as the search component of FLARE, seen in Figure 1, is a structured simulation of a Prolog trace, where each line contains a hop of reasoning inference. We split these statistics for the reasoning paths that lead to correct or incorrect outcomes. Our results in Table 3 show that LLM performance and reasoning optimally are not directly connected to the amount of explored paths or multi-hop inferences per path. We also see that traces that lead to incorrect answers have a higher number of failures per path and in total. We explain this phenomenon with the hypothesis that LLMs with traces that were optimal for reasoning and led to correct answers could skip exploring degenerate solutions due to strong commonsense reasoning capabilities. Further analyses focus on identifying inconsistencies and failure modes (Section 2.2). By comparing relations in code with those in search traces, we measure emergent hallucinations and unused relations, highlighting areas of sub-optimal reasoning. Additionally, we assess the uniqueness of emergent facts per inference hop, which indicates the extent of problem-space exploration (Table 4). The results in Table 4 show consistently over each model that, on average, traces that lead to correct answers had a higher percentage of unique emergent facts and overlap in the relations used between the code and search, while the portion of underutilized relations was lower. This means that optimal reasoning with an LLM requires a great degree of problem-space exploration with fewer relation hallucinations during the search and more relation utilization from the defined code. This aligns with our prior discoveries, which show a strong correlation between simulated search faithfulness towards the formalised code and model performance. Our framework FLARE has these reasoning patterns ingrained within its inference pipeline."}, {"title": "4.5 THE EFFECT OF SCALE", "content": "We want to assess the impact of the number of parameters in the model on the overall performance and faithfulness. The results in Figure 4 show no precise relation between model scale, performance and faithfulness. However, scaled models from the same family, i.e. CmDR (30B) and CmDR+ (100B), show improvements in reasoning faithfulness and model performance. We can also see in Table 5 that as the model size increases, the average number of hops and the portion of hallucinations and unutilised knowledge decreases. This further confirms our prior assumptions that models with strong commonsense soft-reasoning capabilities can skip steps during the search while maintaining the knowledge and structure of the traversal strategy outlined in the code."}, {"title": "5 RELATED WORK", "content": "Reasoning in Natural Language Few-shot prompting (Brown et al., 2020b) has been shown to be an effective approach for increasing the reasoning capabilities of LLMs in natural language gen-eration (Gehrmann et al., 2021; Reif et al., 2022; Sanh et al., 2022). LLM reasoning can be further enhanced with prompting techniques such as CoT (Wei et al., 2022), which attempts to segment reasoning into explicitly written intermediate steps. Concurrent work has also proposed that models \"think step by step\" (Kojima et al., 2022), or divide the problem into subtasks before the solu-tion (Zhou et al., 2023, Least-to-Most). These approaches have been shown to suffer from arith-metic inaccuracies (Lewkowycz et al., 2022; Hendrycks et al., 2021a) and reasoning inconsistencies (Madaan & Yazdanbakhsh, 2022a). Further attempts have been made to add a planning stage before reasoning by dividing the process into recursive plan formulation and execution steps (Yao et al., 2023b; Wang et al., 2023a). The plan generation step in FLARE is a hybrid technique inspired by these methods but focused on generating a natural language strategy for formalising the query into code."}, {"title": "Reasoning with Search", "content": "Several lines of work propose using techniques to expand the reason-ing paths over the problem space. Self-consistency decoding (Wang et al., 2023b) is an approach used to sample many natural language reasoning paths and take a majority vote for an answer. Another popular approach is Tree-of-Thoughts (ToT, Yao et al. (2023a)), which proposes to explore the problem space with reasoning similar to a tree traversal, where each state is created and evaluated using an LLM. Similar techniques try to adapt symbolic search approaches akin to DFS, BFS (Besta et al., 2024), A* (Lehnert et al., 2024) or other combinations (Gandhi et al., 2024) with direct tuning (Lehnert et al., 2024), imitation training (Yang et al., 2022) or few-shot prompting (Zhang et al., 2024). It must be noted that all of these techniques have only been tested in constrained mathe-matical puzzle-solving and algorithmic domains like the 24 Game (Yang et al., 2022), Countdown (Wikipedia, 2024), Sorting (Besta et al., 2024), maze solving (Yang et al., 2022), Sokoban (Lehnert et al., 2024) and others. Although the search component of FLARE has some similarities to these techniques, we argue that our method allows for generalistic reasoning with interpretable multi-hop search through iterative logic-based problem space exploration."}, {"title": "Reasoning with Formalisation", "content": "Another line of research has tried formalising natural language queries into code (Gao et al., 2023; Li et al., 2024) or pseudo-code (Chae et al., 2024; Gandhi et al., 2024). This allows the translation of the query into a strict structure and offloads the reasoning and search components to deterministic solvers like Python Chen et al. (2023), PDDL Xu et al. (2024); Liu et al. (2023), DataLog Xu et al. (2024) and others. While models are reasonably capable of synthesising programs (Austin et al., 2021; Nijkamp et al., 2023) and benefit from the use of code in numerical and algorithmic reasoning settings (Chen et al., 2023; Gao et al., 2023), the usage of code for general QA has not been rigorously explored. The reasons are that formalisation from natural language into a strict and executable code is challenging (Wu et al., 2022), following the exact syn-tactic constraints of the programming language not abundantly used during pre-training is onerous (Liu et al., 2024) and can require models explicitly tuned for coding (Chen et al., 2021). Using an external solver for reasoning also limits the capability for soft reasoning in commonsense knowl-edge and implications. Although we formalise the natural language query into a logic programming Prolog program during the code generation part of FLARE, we do not explicitly require the code to be executable and do not use external solvers during inference. This allows for the further use of the LLM for soft-reasoning to simulate code execution in a logic-based problem space traversal similar to Prolog while circumventing the need for code tuning a generalist model."}, {"title": "Reasoning Faithfulness", "content": "An explanation is considered faithful if it explicitly and accurately de-scribes the reasoning process of the model during inference (Gilpin et al., 2018; Jacovi & Goldberg, 2020). In the context of prompting techniques such as CoT, we are interested in the faithfulness of the intermediate reasoning chains towards the final output. Faithful intermediate reasoning chains should not just look plausible (Herman, 2017) but have exact reflections of the problem exploration and reasoning used to arrive at the final answer. Natural language reasoning chains prevalent in CoT and similar methods are shown to be unfaithful, either masking the reasoning biases (Turpin et al., 2023) of the model or outright ignoring the intermediate reasoning (Lanham et al., 2023a). In FLARE, we introduce a method to seamlessly measure the faithfulness of the final outcome w.r.t. completed search."}, {"title": "6 CONCLUSION", "content": "This work introduces FLARE, a novel approach for logic-aided interpretable formalisation and rea-soning with simulated search over the problem space. We show that models of varying scales obtain state-of-the-art results compared to prompting paradigms like CoT and F-CoT. We further pinpoint that using FLARE allows us to perform soft-reasoning with simulated search, making it flexible for diverse reasoning benchmarks. We introduce a method to measure model reasoning faithfulness w.r.t. the problem formalization ingrained within FLARE. Our results show that model performance is positively correlated with the faithfulness of the reasoning process. The systematic studies of the method show the benefits of using simulated search compared to natural language reasoning and external symbolic solvers. We further show that using FLARE allows us to interpretably and rigorously detect hallucinations and sub-optimal and inconsistent reasoning patterns."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 LLM PROMPTS", "content": "We define straight-forward prompts for generating plan, code and search simulation in FLARE, which can be observed in Appendix A.2."}, {"title": "A.2 DATASET STATISTICS", "content": "The datasets used in this study encompass a variety of domains, specifically targeting the perfor-mance of the models in interpreting Math Word Problems, multi-hop question answering, and re-lational inference. Table 7 provides a detailed breakdown of each dataset, including the number of few-shot in-context samples (shots), the number of test samples, and representative examples from each dataset. The datasets provide a comprehensive basis for evaluating the models' abilities to han-dle complex tasks across different domains, facilitating an in-depth analysis of model performance under few-shot conditions."}]}