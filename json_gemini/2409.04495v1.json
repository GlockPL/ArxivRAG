{"title": "Learning to Solve Combinatorial Optimization under Positive Linear Constraints via Non-Autoregressive Neural Networks", "authors": ["Runzhong WANG", "Yang LI", "Junchi YAN", "Xiaokang YANG"], "abstract": "Combinatorial optimization (CO) is the fundamental problem at the intersection of computer science, applied mathematics, etc. The inherent hardness in CO problems brings up challenge for solving CO exactly, making deep-neural-network-based solvers a research frontier. In this paper, we design a family of non-autoregressive neural networks to solve CO problems under positive linear constraints with the following merits. First, the positive linear constraint covers a wide range of CO problems, indicating that our approach breaks the generality bottleneck of existing non-autoregressive networks. Second, compared to existing autoregressive neural network solvers, our non-autoregressive networks have the advantages of higher efficiency and preserving permutation invariance. Third, our offline unsupervised learning has lower demand on high-quality labels, getting rid of the demand of optimal labels in supervised learning. Fourth, our online differentiable search method significantly improves the generalizability of our neural network solver to unseen problems. We validate the effectiveness of this framework in solving representative CO problems including facility location, max-set covering, and traveling salesman problem. Our non-autoregressive neural solvers are competitive to and can be even superior to state-of-the-art solvers such as SCIP and Gurobi, especially when both efficiency and efficacy are considered.", "sections": [{"title": "1 Introduction", "content": "Combinatorial optimization (CO) refers to a class of non-convex optimization problems with discrete decision spaces, named for its decision space often \"exploding combinatorially\" with increasing parameters. As a shared problem in computer science, applied mathematics, and management science, CO has a long research history, e.g., Euler [1] in the 18th century initiated the study of graph theory, and the pioneering big names such as Birkhoff[2], von Neumann [3], and Dantzig[4] systematically studied CO in the 1940s-1950s. Most researchers agree that apart from a few \"simple\" problems, most CO problems are NP-hard and thus cannot be solved exactly in polynomial time.\nNonetheless, with the growth of computational power and machine learning advancements, improving the performance boundaries of current solvers remains an active research direction."}, {"title": "2 Related Work", "content": "We follow the survey[18] to categorize neural CO solvers into autoregressive and non-autoregressive.\nAutoregressive Neural Solvers for Combinatorial Optimization. Autoregressive neural networks are widely used in sequence learning, where the output at time (t+1) depends on the output at time t. In the context of CO, autoregressive neural solvers gradually construct a complete solution step by step. Thus, for problems where decision variables have a dimension of l, autoregressive neural networks need O(l) steps. The advantage of autoregressive networks lies in their ability to restrict the action space at each step, eliminating invalid solutions. This paradigm was first applied to CO in [10]. Later, Khalil et al. [11] showed that this multi-step decision process could naturally be modeled as a Markov decision process, allowing reinforcement learning algorithms to train the network [19]. Due to their flexibility, the \"autoregressive network + reinforcement learning\" paradigm has become a mainstream research direction, with applications in scheduling [20],"}, {"title": "3 Non-Autoregressive Neural Solvers for Combinatorial Optimization", "content": ""}, {"title": "3.1 Problem Formulation", "content": "Unless otherwise stated, lowercase bold letters mean vectors, and uppercase bold letters mean matri- ces. The non-autoregressive network framework proposed in this paper solves binary combinatorial optimization problems with the following form of positive linear constraints:\n$\\min J(x, w),$ (2a)\n$\\text { s.t. } A x<b, C x>d, E x=f, x \\in\\{0,1\\}^{l}.$ (2b)\nHere, x represents the decision variables, also known as the solution; w represents the problem parameters; J(x, w) is the objective function (for maximization problems, we minimize the negative objective); the elements in A, b, C, d, E, f are non-negative, and the three sets of constraints may not all exist simultaneously. The input to a CO problem is w and the constraints, and the output is a solution x that (as much as possible) minimizes J(x, w) within a given time."}, {"title": "3.2 Framework Building Blocks", "content": "This section describes the building blocks of the non-autoregressive CO solving framework shown in Figure 1.\nGraph Modeling: Neural networks struggle to process inputs directly in the mathematical form of (2), so a general solution is to model the problem parameters as a graph. Different problems have different graph modeling methods, but there is a general best practice: decision variables must correspond to node classification or edge classification tasks on the graph. For single-graph"}, {"title": "3.3 Further Discussions", "content": ""}, {"title": "3.3.1 Expressive Power of Graph Neural Networks for Combinatorial Opti- mization", "content": "GNNs were initially developed to process \"common\" graph structures such as social networks. However, current theoretical studies on GNN expressivity often begin with graph isomorphism problems, which are a form of CO. The mainstream view is that the expressivity of GNNs is comparable to the Weisfeiler-Lehman (WL) graph isomorphism test [46,38]. Although the graph isomorphism problem is also a CO problem, it should be noted that the WL test is an approximate algorithm, a sufficient but not necessary condition for graph isomorphism. Whether GNNs have the expressivity to solve any CO problem remains an open question. However, practical experience in machine learning for CO shows that GNNs are up to the task [11,33,25]. Furthermore, recent theoretical research has shown positive results. For example, Chen et al. [47,15] proved that GNNs have enough expressivity to predict feasibility, optimal objective values, and optimal solutions for linear and mixed-integer programming problems. Hence, this paper chooses GNNs as the primary neural network architecture, though the framework is flexible enough to support more expressive neural networks."}, {"title": "3.3.2 Advantages of Gumbel Reparameterization", "content": "This paper adopts Gumbel reparameterization [17] to enable differentiable sampling in (nearly) discrete spaces, offering two main advantages."}, {"title": "3.3.3 Revisiting Deep Learning Frameworks", "content": "The success of deep learning in recent years is largely due to the development of GPU computing and efficient open-source deep learning frameworks (especially the highly optimized low-level implementations). One reason for the success of the proposed non-autoregressive framework is its ability to leverage these high-efficiency deep learning frameworks. With good support for GPUs and efficient low-level code, operations like Gumbel sampling and objective function estimation for multiple quasi-discrete solutions can be performed in parallel on GPUs. Additionally, deep learning frameworks provide highly efficient large-scale gradient optimization algorithms. Our framework reuses the automatic differentiation capabilities of deep learning frameworks, especially during inference, where automatic differentiation is used to update latent code, enabling efficient online gradient-based search."}, {"title": "3.3.4 A Meta-Learning Perspective on Combinatorial Optimization", "content": "Qiu et al. [27] were the first to propose a meta-learning perspective (more precisely, model-agnostic meta-learning [50]) for CO. In the pretraining stage, the neural network learns a set of initial weights that are effective across the entire dataset. In specific CO problems, the network weights can be further updated to obtain better solutions. Our framework can also be understood from a similar meta-learning perspective, but here the meta-learning object shifts from the neural network weights to the lighter latent code: in the pretraining stage, the network outputs learn to map to a set of latent codes that are effective across the entire dataset; for specific CO problems, the latent code can be further optimized via gradient descent to obtain better solutions."}, {"title": "4 Experiments and Analysis", "content": "This paper demonstrates the effectiveness of non-autoregressive networks on facility location, max- set covering, and traveling salesman problems, comparing our approach to existing neural network methods [25-26] and traditional MILP solvers [51-52]. In the facility location and max-set covering experiments, the primary goal is to compare the performance of non-autoregressive neural networks with traditional solvers (integer programming solvers like SCIP and Gurobi); in the traveling salesman problem, the focus is on comparing our framework with other neural network-based solvers. Facility location and max-set covering experiments were run on a workstation with an i7-9700K CPU, 16GB RAM, and an RTX 2080Ti GPU, while the traveling salesman problem experiments were conducted on a workstation with an AMD 3970X CPU, 32GB RAM, and an RTX 3090 GPU."}, {"title": "4.1 Facility Location Problem", "content": "The facility location problem (FLP) is formulated as follows: Given m locations, we are to choose k locations to build new facilities, where each facility will serve other locations based on proximity (subject to capacity limits). The objective is to minimize the sum of distances from each location to the nearest facility. In the simplified version without capacity limits, the problem is formulated as:\n$\\min \\sum_{j=1}^{m} \\min \\left(\\left{\\Delta_{i, j} | \\forall x_{i}=1\\right\\right), \\text { s.t. } x \\in\\{0,1\\}^{m}, \\sum_{i=1}^{m} x_{i} \\leq k,$ (9)\nwhere x represents the decision variables, and \u2206i,j means the distance between locations i and j. For this problem, the implementation of the non-autoregressive neural network is as follows:\n\u2022 Graph Modeling: The problem is modeled as a graph, where each location is a node, and edges are defined between locations with a distance less than 2% of the total area diameter. The facility location problem is equivalent to node classification on this graph.\n\u2022 Graph Neural Network: A spline convolutional network [53] is used, with 3 layers and a hidden dimension of 16 and 5 spline kernels.\n\u2022 Gumbel Reparameterization and LinSAT Layer: These steps ensure that the network output is projected into the feasible region with constraints from (9).\n\u2022 Objective Function Estimation: Since the min operator in (9) truncates gradients, we use a smooth softmin operator (i.e., applying softmax after negating the input). Specifically, we replace"}, {"title": "4.2 Max-Set Covering Problem", "content": "The max-set covering problem (MCP) is formulated as follows: Given m sets and n items, each set covers some items from the universe. We are to select k sets such that the total value of the items covered by the selected sets is maximized. A typical case of MCP arises in social networks,"}, {"title": "4.3 Traveling Salesman Problem", "content": "The traveling salesman problem (TSP) is formulated as follows: Given m cities with known coordinates, a traveling salesman needs to visit all cities and return to the starting city, minimizing the total travel distance. The mathematical formulation is:\n$\\min \\frac{1}{m} \\operatorname{tr}\\left(D^{T} X\\right), \\text { s.t. } X \\in\\{0,1\\}^{m \\times m} \\forall j: \\sum_{i=1}^{m} X_{i, j}=2, \\forall i: \\sum_{j=1}^{m} X_{i, j}=2, X \\in H,$\nwhere D \u2208 Rmxm represents the pairwise distances between cities, Xi,j = 1 indicates that the salesman travels between cities i and j, and X \u2208 H represents a Hamiltonian cycle. Our non- autoregressive solver for TSP is implemented as follows:\n\u2022 Graph Modeling: The cities' coordinates are used as node features, and pairwise distances are used as edge features to construct a fully connected graph. For the larger TSP-500 task, the graph is sparsified using a k-nearest neighbors approach (k = 50).\n\u2022 Graph Neural Network: We use an anisotropic graph neural network [58], which, unlike traditional GNNs [35], encodes both node and edge features and performs message passing between nodes and edges.\n\u2022 LinSAT Layer: The Hamiltonian cycle constraint cannot be directly handled by LinSAT, but its relaxed form $\\forall j: \\sum_{i=1}^{m} X_{i, j}=2, \\forall i: \\sum_{j=1}^{m} X_{i, j}=2$ is a positive linear constraint that can be handled by LinSAT. Gumbel reparameterization is not used in this experiment due to conflicts with the existing TSP framework.\n\u2022 Supervised Network Training: To ensure a fair comparison with other neural solvers for TSP, we follow supervised training practices from the literature [59-60], unlike the unsupervised learning used in other experiments in this paper.\n\u2022 Neighborhood Search in Inference: For TSP, we apply greedy output combined with 2-Opt search and Monte Carlo tree search (MCTS) strategies, which are commonly used in solving TSP.\nAs shown in Table 4, we validate the effectiveness of our method on TSP instances with m = 50,100,500. Following common practice in the field, the city coordinates are randomly generated in a 2D uniform distribution in [0, 1], and the Euclidean distance between each pair of cities is computed. The TSP-50 and TSP-100 datasets each have 1280 test instances, while TSP-500 has 128 test instances. The reported times in Table 4 are the total runtime across the entire test set. In terms of both solution quality and runtime, our non-autoregressive method performs on par or better than existing neural network solvers [27,59-60]. TSP is one of the most well-researched CO problems in machine learning [10-11], and the primary purpose of this experiment is to compare"}, {"title": "5 Conclusion and Outlook", "content": "The success of neural networks has been complemented by advancements in deep learning frameworks and GPU hardware. Given the fundamental importance of CO, leveraging the computational power of neural networks (especially deep learning frameworks and GPUs) for CO is becoming a cutting- edge research direction. This paper first summarizes existing autoregressive neural solvers, which can better handle constraints and train using reinforcement learning. However, autoregressive networks face challenges like error accumulation, sparse rewards, reduced efficiency, and the inability to model permutation invariance in large-scale problems. The development of techniques like differentiable constraint encoding, Gumbel reparameterization, and unsupervised learning makes non-autoregressive networks a promising research direction.\nIn pursuit of a general-purpose non-autoregressive neural solver for CO, this paper proposes a framework, including building blocks such as graph modeling, GNNs, latent code (latent variables), Gumbel reparameterization, LinSAT layers, and (differentiable) objective function estimation. Our approach can handle all CO problems with positive linear constraints, which is more general than other non-autoregressive methods. Our framework supports offline unsupervised pretraining, reducing the need for labeled data and making it more practical. We also present an online gradient search technique that enhances the network's generalization ability. Our method was validated on (capacitated/un-capacitated) facility location, max-set covering, and TSP problems, surpassing existing non-autoregressive networks in terms of applicability and performance. For TSP, our LinSAT-augmented non-autoregressive network performed on par with other state-of-the-art neural methods; for facility location and max-set covering, our method achieved comparable performance to commercial solvers like Gurobi and even outperformed them on certain problem instances.\nWe believe that the advantage of neural solvers (especially non-autoregressive networks) lies in offering optimization solvers whereby the user has full control of. All building blocks introduced in this paper are open-sourced. In real-world applications, data distributions are often specific and limited, so sacrificing some generality for better performance within a particular distribution is usually acceptable. Finally, this paper's experiments have not fully explored the design space of neural networks, customized search algorithms, or just-in-time Python compilation techniques, and we believe there is room for further improvement in the performance of neural solvers. The research into non-autoregressive networks for CO optimization is still in its infancy, with many opportunities to improve generality, efficiency, and explore more applications."}]}