{"title": "Learning to Solve Combinatorial Optimization under Positive Linear Constraints via Non-Autoregressive Neural Networks", "authors": ["Runzhong WANG", "Yang LI", "Junchi YAN", "Xiaokang YANG"], "abstract": "Combinatorial optimization (CO) is the fundamental problem at the intersection of computer science, applied mathematics, etc. The inherent hardness in CO problems brings up challenge for solving CO exactly, making deep-neural-network-based solvers a research frontier. In this paper, we design a family of non-autoregressive neural networks to solve CO problems under positive linear constraints with the following merits. First, the positive linear constraint covers a wide range of CO problems, indicating that our approach breaks the generality bottleneck of existing non-autoregressive networks. Second, compared to existing autoregressive neural network solvers, our non-autoregressive networks have the advantages of higher efficiency and preserving permutation invariance. Third, our offline unsupervised learning has lower demand on high-quality labels, getting rid of the demand of optimal labels in supervised learning. Fourth, our online differentiable search method significantly improves the generalizability of our neural network solver to unseen problems. We validate the effectiveness of this framework in solving representative CO problems including facility location, max-set covering, and traveling salesman problem. Our non-autoregressive neural solvers are competitive to and can be even superior to state-of-the-art solvers such as SCIP and Gurobi, especially when both efficiency and efficacy are considered. Code is available at https://github.com/Thinklab-SJTU/NAR-CO-Solver", "sections": [{"title": "Introduction", "content": "Combinatorial optimization (CO) refers to a class of non-convex optimization problems with discrete decision spaces, named for its decision space often \"exploding combinatorially\" with increasing parameters. As a shared problem in computer science, applied mathematics, and management science, CO has a long research history, e.g., Euler [1] in the 18th century initiated the study of graph theory, and the pioneering big names such as Birkhoff[2], von Neumann [3], and Dantzig[4] systematically studied CO in the 1940s-1950s. Most researchers agree that apart from a few \"simple\" problems, most CO problems are NP-hard and thus cannot be solved exactly in polynomial time.\nNonetheless, with the growth of computational power and machine learning advancements, improving the performance boundaries of current solvers remains an active research direction.\nThis is the English version of the paper published on Scientia Sinica Informationis. Please cite this paper as \"Wang et al, Learning to Solve Combinatorial Optimization under Positive Linear Constraints via Non-Autoregressive Neural Networks. Scientia Sinica Informationis (2024)\". Submitted 2023-09-17; Accepted 2024-06-17; English version 2024-09-05.\nCorresponding to: Junchi YAN (yanjunchi@sjtu.edu.cn)\nIn particular, Turing Award laureate Prof. Yoshua Bengio noted that fitting and solving CO problems under a specific distribution is often easier than developing a general-purpose solver [5] . Compared to traditional solvers that typically run on CPUs [6-7], neural network algorithms running on GPUs can fully exploit the power of parallelism. Moreover, due to their ability to fit data distributions, neural networks hold significant potential as CO solvers. However, how to design a neural CO solver with the expected performance improvement remains an open problem [8-9], with the primary challenge being ensuring that the network output satisfies specific discrete constraints. A common solution is to build autoregressive neural networks, which output solutions step- by-step, applying rules at each step to ensure the final solution lies in the feasible domain [10-11]. While autoregressive networks have gained wide application in CO due to their generality, they face challenges such as error accumulation, large action spaces, sparse reward signals, and the difficulty in modeling the permutation invariance of the problem space.\nThis paper introduces a non-autoregressive neural network architecture as a CO solver. The non-autoregressive network outputs all decision variables in one forward pass, avoiding the issues of autoregressive models. Additionally, non-autoregressive networks are a mature architecture with proven efficiency and accuracy in fields such as computer vision [12-14] . The latest theoretical research also shows that neural networks (especially graph neural networks that break symmetry with random features) can solve CO problems, such as mixed-integer programming [15]. To tackle the technical challenges (i.e., neural network outputs are typically unconstrained, while CO problems require the network to output constrained solutions), this paper introduces the Linear Satisfiability Network (LinSATNet) [16], which projects network outputs into the feasible domain defined by positive linear constraints:\n$Ax < b, Cx > d, Ex = f, where A, b, C, D, E, f > 0, x \u2208 [0, 1]^l.$\nThis covers many common CO problems, such as the knapsack problem, where constraints w\u00afx \u2264 m can be handled as \"positive linear constraints.\""}, {"title": "Related Work", "content": "We follow the survey[18] to categorize neural CO solvers into autoregressive and non-autoregressive.\nAutoregressive Neural Solvers for Combinatorial Optimization. Autoregressive neural networks are widely used in sequence learning, where the output at time (t+1) depends on the output at time t. In the context of CO, autoregressive neural solvers gradually construct a complete solution step by step. Thus, for problems where decision variables have a dimension of l, autoregressive neural networks need O(l) steps. The advantage of autoregressive networks lies in their ability to restrict the action space at each step, eliminating invalid solutions. This paradigm was first applied to CO in [10]. Later, Khalil et al. [11] showed that this multi-step decision process could naturally be modeled as a Markov decision process, allowing reinforcement learning algorithms to train the network [19]. Due to their flexibility, the \"autoregressive network + reinforcement learning\" paradigm has become a mainstream research direction, with applications in scheduling [20],"}, {"title": "Non-Autoregressive Neural Solvers for Combinatorial Optimization", "content": "3.1 Problem Formulation\nUnless otherwise stated, lowercase bold letters mean vectors, and uppercase bold letters mean matri- ces. The non-autoregressive network framework proposed in this paper solves binary combinatorial optimization problems with the following form of positive linear constraints:\nmin J(x, w),\ns.t. Ax < b, Cx > d, Ex = f, x \u2208 {0,1}^l.\nHere, x represents the decision variables, also known as the solution; w represents the problem parameters; J(x, w) is the objective function (for maximization problems, we minimize the negative objective); the elements in A, b, C, d, E, f are non-negative, and the three sets of constraints may not all exist simultaneously. The input to a CO problem is w and the constraints, and the output is a solution x that (as much as possible) minimizes J(x, w) within a given time.\n3.2 Framework Building Blocks\nThis section describes the building blocks of the non-autoregressive CO solving framework shown in Figure 1.\nGraph Modeling: Neural networks struggle to process inputs directly in the mathematical form of (2), so a general solution is to model the problem parameters as a graph. Different problems have different graph modeling methods, but there is a general best practice: decision variables must correspond to node classification or edge classification tasks on the graph. For single-graph\nCO problems (e.g., graph cuts, node covers, TSP), the modeling process is relatively straightfor- ward[11,29-30]; if there are two graphs (e.g., graph matching), an auxiliary graph that integrates the structures of both graphs can be constructed [31]; for Boolean satisfiability problems, there is also a corresponding graph modeling method, where the conjunctive normal form is represented as a bipartite graph [32]; for general matrix-form problems (e.g., linear integer programming), a common approach is to construct a bipartite graph with Ax < b constraints[33] . In summary, for most CO problems, there exists a reasonable graph modeling method. Readers are encouraged to refer to the literature mentioned above for specific problem modeling methods.\nGraph Neural Networks (GNNs): Node classification or edge classification problems on graphs are naturally handled by GNNs [34]. There are many GNN variants, but node classification GNNs typically follow the general framework shown in Figure 3[35]: in each layer of a GNN, when node 0 needs to be updated, it only considers its neighboring nodes (a, b, c, d). For each neighbor, a message function ($f_{msg}$, typically a multi-layer neural network) transforms its current node feature into a message (e.g., msga). Next, an aggregation function ($f_{agg}$, which could be mean, sum, or attention mechanism [36]) aggregates the messages from all neighbors into the current node. In every GNN layer, the weights of $f_{msg}$ and $f_{agg}$ are shared across all nodes. The choice of $f_{msg}$ and $f_{agg}$ determines the GNN variant used, with common choices including GCN [34], GraphSage [37], and GIN[38]. For edge classification problems, features can be propagated to edges from neighboring nodes, or node classification can be performed on the dual graph.\nLatent Code: For problems modeled as node classification, if the decision variable dimension is l, a GNN with a Sigmoid activation function will output a vector of dimension I with values in [0, 1], referred to as latent code. The l-dimensional real-valued space forms a latent space, where each point corresponds to a distribution of feasible solutions. This mapping is achieved using Gumbel reparameterization and the LinSAT constraint layer. Importantly, the latent space is continuous; by searching and optimizing within the latent space, better decision variables can be found.\nGumbel Reparameterization: The purpose of this step is to treat the latent code as a distribution over feasible solutions and sample from this distribution to obtain feasible solutions. However, discrete sampling is not differentiable, so we use Gumbel reparameterization [17,39-40] to approximate discrete sampling. Given the Gumbel distribution\n$g_\u03c3 (u) = -\u03c3log(-log(u)),$\nwhere o controls variance and u is sampled from a uniform (0,1) distribution, we apply this to the GNN's output y \u2208 R':\n$\u1ef9 = [y_1 + g_\u03c3(u)_1, y_2 + g_\u03c3(u)_2,\u2026, y_l + g_\u03c3(u)_l],$"}, {"title": "Further Discussions", "content": "3.3.1 Expressive Power of Graph Neural Networks for Combinatorial Opti- mization\nGNNs were initially developed to process \"common\" graph structures such as social networks. However, current theoretical studies on GNN expressivity often begin with graph isomorphism problems, which are a form of CO. The mainstream view is that the expressivity of GNNs is comparable to the Weisfeiler-Lehman (WL) graph isomorphism test [46,38]. Although the graph isomorphism problem is also a CO problem, it should be noted that the WL test is an approximate algorithm, a sufficient but not necessary condition for graph isomorphism. Whether GNNs have the expressivity to solve any CO problem remains an open question. However, practical experience in machine learning for CO shows that GNNs are up to the task [11,33,25]. Furthermore, recent theoretical research has shown positive results. For example, Chen et al. [47,15] proved that GNNs have enough expressivity to predict feasibility, optimal objective values, and optimal solutions for linear and mixed-integer programming problems. Hence, this paper chooses GNNs as the primary neural network architecture, though the framework is flexible enough to support more expressive neural networks.\n3.3.2 Advantages of Gumbel Reparameterization\nThis paper adopts Gumbel reparameterization [17] to enable differentiable sampling in (nearly) discrete spaces, offering two main advantages."}]}