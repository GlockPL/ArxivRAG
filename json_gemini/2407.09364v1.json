{"title": "Is Contrasting All You Need? Contrastive Learning for\nthe Detection and Attribution of AI-generated Text", "authors": ["Lucio La Cava", "Davide Costa", "Andrea Tagarelli"], "abstract": "The significant progress in the development of Large\nLanguage Models has contributed to blurring the distinction between\nhuman and AI-generated text. The increasing pervasiveness of AI-\ngenerated text and the difficulty in detecting it poses new challenges\nfor our society. In this paper, we tackle the problem of detecting\nand attributing AI-generated text by proposing WhosAI, a triplet-\nnetwork contrastive learning framework designed to predict whether\na given input text has been generated by humans or AI and to un-\nveil the authorship of the text. Unlike most existing approaches, our\nproposed framework is conceived to learn semantic similarity rep-\nresentations from multiple generators at once, thus equally handling\nboth detection and attribution tasks. Furthermore, WhosAI is model-\nagnostic and scalable to the release of new AI text-generation models\nby incorporating their generated instances into the embedding space\nlearned by our framework. Experimental results on the TuringBench\nbenchmark of 200K news articles show that our proposed framework\nachieves outstanding results in both the Turing Test and Authorship\nAttribution tasks, outperforming all the methods listed in the Turing-\nBench benchmark leaderboards.", "sections": [{"title": "Introduction", "content": "In recent years, advancements in artificial intelligence (AI) have\nrevolutionized various domains, including natural language process-\nsing (NLP), leading to the emergence of sophisticated text genera-\ntion models. These AI-powered systems are capable of generating\nhuman-like text, ranging from simple sentences to complex narra-\ntives, with remarkable fluency and coherence [20]. Such advance-\nments have attracted attention from the research community as well\nas industry and society at large, offering opportunities for enhancing\ncommunication, creativity, and productivity [7].\nHowever, a pressing challenge comes alongside these advance-\nments: distinguishing between AI-generated text and human-written\ntext. As AI text generation models continue to improve in sophisti-\ncation and realism, the ability to differentiate between AI-generated\nand human-generated content becomes increasingly crucial [37]. The\nimplications of failing to discern between these two sources of text\nare profound and multifaceted, spanning various aspects of society,\nsuch as the preservation of truth, authenticity, and trustworthiness in\nonline communication. With the proliferation of AI-generated con-\ntent on social media, news platforms, and other digital channels,\nthere is a growing risk of misinformation, manipulation, and decep-\ntion [51, 9]. Without effective means of distinguishing between AI-\ngenerated and human-generated text, users may unwittingly consume\nand propagate false or misleading information, undermining the in-\ntegrity of public discourse and decision-making processes.\nFurthermore, the rise of AI text generation brings ethical and so-\ncietal concerns about authorship, intellectual property rights, and ac-\ncountability. As AI systems become increasingly proficient at mim-\nicking human language and creativity [8], questions arise regarding\nthe ownership and attribution of AI-generated content. Without clear\nguidelines and mechanisms for identifying the origin of text, issues\nmight also arise about plagiarism, copyright infringement, and legal\nresponsibility, posing challenges to established norms in intellectual\nproperty law and digital content creation.\nRelated work. The remarkable boost in human-like text generation\nperformances achieved by Large Language Models (LLMs) in re-\ncent years has determined a rising challenge in detecting whether\nand to what extent texts have been generated by humans or ma-\nchines [21, 49, 39]. In this context, the \"watermarking\" paradigm\nrapidly gained attention [25, 53, 28, 50], as it allows embedding\nspecific signals into generated texts that remain invisible to hu-\nmans but are algorithmically detectable. Statistical learning methods\nalso offer advanced solutions for detecting the authorship of texts.\nThese include probabilistic models [31, 1, 47, 14], log rank informa-\ntion [38], perplexity [44], discourse motifs [24], and other statistical\napproaches [16, 40, 45]\nMore recently, we have witnessed the emergence of deep learning\nto detect or attribute AI-generated content, which stands as a promis-\ning body of research. Researchers have been exploiting LLMs to de-\ntect generated text [18, 46], using ChatGPT itself as a detector [2],\nor combining LLMs with topological aspects [43].\nA very recent trend involves leveraging contrastive learning to\nhandle textual information. Indeed, despite its origins in the com-\nputer vision domain, contrastive representation learning has been\nproven particularly effective in NLP contexts to improve research\non semantic similarity related problems, such as text classifica-\ntion [33, 10], spotting hate-speech [23], unveiling intents [55], and\neventually detecting AI-generated text through domain adaptation [3]\nor domain adversarial training [4].\nDespite the advancements in research on detection of AI text gen-\neration, each of the above mentioned approaches faces significant\nchallenges. Watermarking approaches are conditioned by the con-"}, {"title": "Problem Statement", "content": "We are given a set of discrete labels (categories) C = {C}=1, with\nM\u2265 2, and a collection of text data objects D = {D}=1, such\nthat each text object in D is assigned to one of the categories in C.\nThe semantics of such categories refer to information on the origi-\nnator of a written text, which is assumed to be either a human or a\nmachine, i.e., an AI model for text generation; hence, in this setting,\nthe authorships of the texts in D are a priori known.\nThe problem we are interested in is generally to learn a model,\nsupervisedly trained on (D, C), that can predict the category from C\nfor any given text data whose authorship is unknown. Specifically,\nwe address two supervised learning problems: (i) A binary classifi-\ncation task, known as Turing Test (TT), which requires to predict\nwhether the author of a text is a human or an AI text-generator, and"}, {"title": "Background", "content": "Transformer-based Pre-trained Language Models (PLMs) are\nthe well-established NLP tools to build deeply contextualized text-\nrepresentation learning models. Given a text data Di \u2208 D, a token\nsequence T\u2081 = [Ti,1,..., Ti,|T\u00bf|] is produced as initial representa-\ntion of Di through a tokenization process typically associated with\na PLM. Each token sequence is deeply contextualized by mapping it\nonto a dense, relatively low dimensional space of size f, based on the\nPLM. The resulting output is the token embeddings of Di, denoted\nas PLM(T\u2081) \u2208 R^{f\u00d7|Ti|}. Eventually, a pooling function pooling(\u00b7) is\napplied to the token embeddings of each object Di to yield a single\nembedding vector hi of size f:\nhi = pooling(PLM(Ti)) \u2208 Rf. (1)\nTypically, this pooled output is an average embedding over all to-\nken embeddings of a data object. The embeddings h\u2081 are commonly\nreferred to as sentence embeddings.\nSimilarity Learning. The deeply contextualized representations\nproduced by a PLM lend themselves particularly suited to enable\nsemantic comparisons between the input text objects. In this respect,\nwe want to explicitly model and leverage the similarity space induced\nfrom the sentence embeddings. Similarity learning aims to train a\nmodel to distinguish between similar and dissimilar pairs of objects.\nMore specifically, if we consider objects whose relative similarity\nfollows a predefined order \u2013 i.e., for any triplet of objects, the first\nobject is assumed to be more similar to the second object than to the\nthird object - the goal becomes to learn a contrastive loss function,\nso that it favors small distances between pairs of objects labeled as\nsimilar, and large distances for pairs labeled as dissimilar. This is"}, {"title": "The WhosAl Framework", "content": "Overview. We propose WhosAl, a deep learning framework for the\ndetection and attribution of open-ended texts generated by AI models\nvs. human-written texts. Figure 1 shows a schematic illustration of\nthe main components and data flows of WhosAl.\nWhosAl is conceived to be trained on text data with associated la-\nbels expressing authorship as either human or an AI text-generation\nmodel. The framework is comprised of three key elements: (i) a\nPLM, which is charge of learning deeply contextualized represen-\ntations (embeddings) of the text data, in an unsupervised fashion, (ii)\na Triplet Network architecture, which is designed to perform con-\ntrastive learning to induce a similarity space of the PLM embeddings,\nand (iii) a nearest centroid classification model, which is in charge of\npredicting the authorship category for any query text.\nDuring the training phase, WhosAl builds a deep semantic repre-\nsentation space whereby different regions correspond to features of\nhuman-written texts as well as distinct AI text-generators. The con-\ntrastive learning strategy allows for capturing the underlying simi-\nlarity structure and relations within the data objects, such that the\ndeeply contextualized embeddings produced by a PLM encoder will\nbe grouped together when they correspond to the same author and\nwill be kept separated when they correspond to different authors.\nMoreover, as a byproduct, the similarity learned space facilitates the\nlearning of the decision boundary for our classification objective of\ndetermining the class of previously unseen texts; in this setting, our\nchoice of a nearest centroid classifier turns out to be a highly efficient\nyet effective way to perform authorship prediction.\nWhosAl is designed to be versatile and modular. Versatility\nmainly refers to the possibility of choosing alternative PLMs as core\ncomponent of the Triplet Network, variants of the Triplet Network\narchitecture, and alternative (instance-based) classification models.\nMoreover, WhosAl is modular in that enhanced methods are consid-\nered to improve specific aspects of the framework. In particular, these\nenhancements include (i) improving the efficiency and generalization\ncapabilities of the contrastive learning component, (ii) refining the"}, {"title": "Optimizations", "content": "We discuss here a set optimization techniques as enhancements of\nkey components in WhosAl, namely improved triplet mining, dy-\nnamic margin scheduling, and data corruption."}, {"title": "Experimental Methodology", "content": "We used the publicly available benchmark dataset TuringBench\n[42, 41], which contains 200K news articles, where 10K are human-\nwritten and the other ones are machine-generated news articles\nequally distributed over 19 different AI text-generation models. From\nthe human-written articles, originally collected from sources like\nCNN and with typical length of 200-400 words, the titles were used\nto prompt the 19 AI text-generators to generate 10K articles each.\nTable 1 summarizes the main characteristics of the dataset, providing\ndetails for the various subsets of data associated with the human cat-\negory and each of the AI text-generator categories. It should be noted\nthat TuringBench comes with a pre-defined split into train, validation\nand test sets. We will follow this setting, so as to fully compare with\nprevious and future evaluation studies on TuringBench."}, {"title": "Assessment Criteria and Model Settings", "content": "To validate the performance of WhosAl in detecting and attributing\nAI-generated text, we resort to standard statistics based on the con-\nfusion matrices derived from testing WhosAl predictions w.r.t. the"}, {"title": "Turing Test", "content": "We start with evaluating WhosAl on the binary classification task,\ni.e., TT, aimed at recognizing whether a given piece of text origi-\nnates from a human or any AI text-generator. As reported in Fig-\nure 3, the official TuringBench leaderboard presents the F\u2081-scores\nfor the TT under a One-vs-One approach, whereby one side of the\ncomparison denotes \u201chuman\u201d and the other one corresponds to each\nof the available AI-generators in TuringBench. It can be noticed that\nsome generators are more easily detectable than others, resulting in\nsubstantial disparities in terms of average weighted F\u2081-scores.\nBy contrast, WhosAl is able to learn a deep semantic space for the\nwhole set of generators at once. As a major result, WhosAl achieves\nan impressive F1-score of 0.999 on the whole TT test set supplied by\nthe TuringBench benchmark, setting a new best performance on the\nTuring Test. Our remarkable F\u2081-score is further corroborated by a\nqualitative analysis based on the visualization provided in Figure 4:5\nwhile at the beginning of the training the semantic representation di-"}, {"title": "Authorship Attribution", "content": "We discuss our evaluation of WhosAl on the Author Attribution\n(AA) task, aimed at deciding the authorship of a text, being a hu-\nman or one of the AI text-generators in TuringBench.\nOur first remarkable finding derives from a comparison between\nWhosAl results against those reported on the TuringBench leader-\nboard for the AA task, whose top-5 best-performing models are\nshown in Table 2. ROBERTa [29] with a multi-class classification set-\nting turns out to be the best model in the leaderboard for the AA task,\nwith a F1 score of 0.811, followed by other BERT-based approaches,\nas well as the official OpenAI detector and machine learning-based\nmodels. The winner method from the leaderboard is however out-\nperformed by WhosAl, which achieves a striking average weighted\nF\u2081 score, precision and recall of 0.990, thus demonstrating almost\nperfect capabilities of authorship prediction.\nTable 3 offers insights into the prediction performance of WhosAl\nw.r.t. each of the generator categories corresponding to the largest-\nsize versions of the AI models. Results show extremely robustness\nof WhosAl, as it achieves F\u2081 score at least 0.960, and above 0.99 in\n7 out of 10 cases. It should also be noticed that precision and recall\nare always comparable or very close to each other, thus indicating an\nequal capability of avoiding both types of statistical errors.\nAs previously found for the Turing Test task, the striking F\u2081 scores\nachieved by WhosAl couple with an evidence of highest cohesive-\nness and separation of the subspaces associated with the various text\nauthorships, as visually shown in Fig. 5 (left); quantitatively, this cor-\nresponds to an average pairwise similarity between embeddings of\nobjects sharing the same category, resp. belonging to different cate-\ngories, of 0.938, resp. -0.012 (cf. Table 5).\nIt is worth noting that the outstanding performance by WhosAl in\nthe AA task is not paired by a state-of-the-art sentence-embedding\nmethod for semantic-similarity-related tasks like SBERT [36], based\non a Siamese network using BERT at its core: indeed, as shown in\nFig. 5 (right), the intra-class cohesiveness and inter-class separation\nof the semantic space learned by SBERT are clearly worse than those\nachieved by WhosAl."}, {"title": "Sensitivity Analysis", "content": "Turing Test. Table 4 reports the results achieved by WhosAl on the\nTT task by varying the framework settings according to the various\noptimizations discussed in Section 4.1.\nAt first glance, we notice that WhosAl can solve the TT task al-\nmost perfectly - with P, R, F\u2081 always above 0.996 \u2013 regardless of\nspecific optimizations. This remarkable finding, coupled with the vi-\nsual evidence of the semantic space representation displayed in Fig-\nure 4, indicate that WhosAl excels in distinguishing between human\nauthors and AI text-generators, even when equipped with the sim-\nplest configuration.\nWhile the classification performance criteria have indeed only\nslight fluctuations by varying the framework settings, different be-\nhaviors of WhosAl appear to be more evident in terms of com-\npactness (intra) and, especially, separation (inter), with the former\nconsistently above 0.846 and the latter that can vary from 0.35 to\n-0.81. In particular, applying data corruption techniques can affect\nthe distance-based criteria: in fact, by using either token deletion and\nspan cropping, we notice a worse (i.e., higher) similarity between\nembeddings of objects pertaining to different categories, whereas the\nsimilarity between embeddings of objects from the same category re-\nmains coherent. This suggests that corrupting the data in input to the\nTT predictor might impact particularly on some of the tokens that are\ndiscriminative of the text-generators, being human or AI models.\nMore importantly for our TT evaluation, we assessed the effect on\nthe WhosAl performance due to the presence of texts that were gen-\nerated by the same AI architecture yet with different parameter sizes\n(cf. Table 1). To this aim, we focused on comparing the performance\nof WhosAl when keeping all instances generated by the same AI\ntext-generation architecture, and only the subsets corresponding to\neither the largest model or the smallest model of that AI architecture\navailable in TuringBench.\nAs reported in Table 4, the variation of the model subset mainly\nimpacts on the separation between embeddings of objects pertaining\nto different classes: keeping all instances from differently sized mod-"}, {"title": "Conclusions and Future Work", "content": "We tackled the challenge of detecting and attributing AI-generated\ntext through WhosAl, a novel PLM-based framework that leverages\ncontrastive learning to induce a semantic similarity space texts writ-\nten by humans or AI text-generation models. This similarity space is\nefficiently exploited at inference time by means of a nearest centroid\nclassifier to predict the authorship of unlabeled texts. Extensive ex-\nperimentation on the well-known TuringBench dataset has revealed\nstate-of-the-art performances of WhosAl on both TT and AA tasks.\nFurthermore, WhosAl comes with several key advantages: (i) it can\nbe applied straightforwardly without altering texts or accessing mod-\nels' internals, (ii) it can be adapted to a number of AI text-generators\nwithout needing model-specific adjustments, and (iii) it is model-\nagnostic and scalable for easy integration of novel AI text-generators.\nRemarkably, such empirical evidence of outstanding performance of\nWhosAl holds despite our choice of PLM in the experimental eval-\nuation refers to the baseline BERT model.\nThere are important directions to explore. Particularly, we will\nevaluate WhosAl on other types of written texts than those avail-\nable in TuringBench. We aim to compare WhosAl with advanced yet\ncommercially licensed AI detection tools (e.g., GPTZero). Also, we\nwill investigate explainability aspects of WhosAl in order to unveil\nwhich features are determinant to characterize and which to discrim-\ninate text originators.\nAs a supplemen-\ntary analysis, we investigated the environmental impact of WhosAl.\nBased on our selected reference PLM, which has 109.48M parame-\nters, we estimate an inference cost of 290.17 GFLOPS for a single-\nelement batch with a sequence length of 512, and a training cost of\n835.8 PFLOPS assuming 32 batch size, 512 sequence length, and\n30K training steps. With an average training time for all WhosAl\nconfigurations of ~8 hours on a 350W Nvidia GeForce RTX 3090\nGPU, we estimate an energy consumption of 2.8 kWh per training\nrun. With an average carbon efficiency factor of 0.432 kg/kWh, a sin-\ngle training run is associated with 1.21 kg of CO2 emissions, which\nextends to 50.4 kWh of consumed energy and 21.78 kg of CO2 equiv-"}]}