{"title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization", "authors": ["Jiayi Wu", "Hengyi Cai", "Lingyong Yan", "Hao Sun", "Xiang Li", "Shuaiqiang Wang", "Dawei Yin", "Ming Gao"], "abstract": "The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the RAG generator to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators.", "sections": [{"title": "1 Introduction", "content": "Pre-trained large language models (LLMs) demonstrate impressive question-answering capabilities but also reveal certain drawbacks, such as generating outdated or hallucinatory information. The emergence of retrieval-augmented generation (RAG) has alleviated this issue by incorporating retrieved documents relevant to the question into the prompt, thereby providing the LLMs with information they may not know.\nHowever, while RAG systems help improve factual grounding, the role of the generator in previous work is usually far from well-aligned. Despite their impressive pre-training, off-the-shelf LLMs are not optimally suited for the specific requirements of RAG tasks without proper task alignment. 1) Helpfulness for RAG application (response informativeness): The generator should possess a refined ability to identify and utilize valuable information from the provided context. When presented with a mix of high-quality and less relevant information, it should actively and effectively leverage the valuable content while disregarding the rest. 2) Harmlessness for RAG application (response robustness): A well-aligned generator should demonstrate robust noise resistance, capable of filtering out misleading information without being adversely affected. This ability ensures the generated responses remain accurate and coherent even when the input context is imperfect. 3) Honesty for RAG application (citation quality): The generator should be capable of producing responses that are firmly rooted in the provided documents while maintaining appropriate attribution. This involves explicitly citing the source documents when necessary, and ensuring that the generated content is not only factually accurate but also traceable to the retrieved documents.\nTo satisfy the aforementioned RAG requirements, existing RAG systems mainly fall into two categories: end-to-end architecture and pipeline architecture. These approaches attempt to bridge the gap between off-the-shelf LLMs and the specific demands of RAG, but both face significant limitations. End-to-end architectures primarily rely on supervised fine-tuning (SFT) of general LLMs. These approaches focus on constructing high-quality responses in RAG scenarios. However, through further analysis, we found that the requirements for the generators in RAG tasks are highly context-dependent and often interwoven. These varying and intertwined demands make it challenging for a model to meet all RAG objectives through standard SFT alone, as it does not incorporate the necessary preference information required for adapting to different retrieval scenarios. On the other hand, pipeline architectures introduce additional steps beyond generation, such as re-ranking retrieved documents, filtering irrelevant information, or employing post-hoc verification to ensure that citations support the claims. However, these additional steps can only satisfy certain RAG requirements and still have a considerable gap in aligning with the global RAG requirements. Moreover, these additional steps introduce extra computational costs and time consumption, making it less practical for large-scale deployment. This further inspires us to think: Is it possible to fully align the generator with the diverse RAG requirements while retaining the simplicity and efficiency of an end-to-end architecture?\nTo this end, in this paper, we propose Multi-Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), aiming to optimize the generator of RAG systems to align comprehensively with specific RAG requirements. As illustrated in Figure 1, PA-RAG maintains the end-to-end architecture of the generator while enabling it to learn multi-perspective preference information. The training of PA-RAG is divided into two phases. The first phase is foundational capability training, where the generator acquires the basic ability to utilize and cite documents through instruction fine-tuning. To construct high-quality instruction fine-tuning data, we utilize ChatGPT to generate complete and correct answers and employ a citation rewrite mechanism to ensure citation quality. The second phase is the multi-perspective preference optimization phase, in which the generator is trained using Direct Preference Optimization (DPO) to learn preference information from different perspectives. This phase encompasses three sub-stages, sequentially enhancing the generator's response informativeness, response robustness, and citation quality. To construct high-quality preference data, we sample responses of varying quality from the generator across scenarios with different document qualities in prompt to build preference data for informativeness and robustness. Additionally, we employ the citation rewrite mechanism to construct preference data for citation quality.\nWe conducted extensive experiments on four QA datasets and three LLMs, demonstrating that PA-RAG significantly enhances the generator's performance. The improvement achieved by PA-RAG far surpasses the performance gains using only SFT or additional steps. LLMs trained with PA-RAG show an average absolute improvement of 13.97% in correctness, 49.77% in citation recall, and 39.58% in citation precision. Our contributions can be summarized as follows:\n\u2022 We propose PA-RAG, which achieves comprehensive alignment of LLMs with specific RAG requirements through a multi-stage, multi-perspective training while preserving its end-to-end architecture.\n\u2022 We publicly release our training data, which includes 58.9k instruction fine-tuning data and 48.7k preference optimization data.\n\u2022 Through extensive experiments, we demonstrate the effectiveness of PA-RAG, showing that the preference optimization from each perspective is effective."}, {"title": "2 Preliminaries", "content": "In this section, we describe the optimization objectives of the RAG generator, the motivation for multi-perspective optimization, and the rationale for choosing preference optimization over mere instruction fine-tuning."}, {"title": "2.1 Optimization Objectives of the RAG Generator", "content": "As illustrated in Figure 1, when answering a question, the RAG generator outputs the corresponding response after receiving the question and multiple relevant documents retrieved by the retriever. Our optimization goal is to enable the generator to fully utilize valuable documents and accurately cite the references corresponding to claims.\nThe ability to fully utilize valuable documents corresponds to the correctness of the response, meaning that all answers contained within the documents should be included in the generator's output. Formally, let G represent the generator, x denotes the input, $D = \\{d_1,d_2,..., d_n\\}$ represent the documents retrieved by the retriever, $A = \\{a_1, a_2, ..., a_n\\}$ represent the short answers contained in the documents, and y denote the response generated by the generator. The correct response by the generator can be expressed as:\n$y = G(x, D)$\ns.t.$ \\forall a_i \\in A, C(y, a_i) = True$\nwhere C(y, ai) = True implies that the answer ai is included in y.\nBased on the generator's ability to correctly answer questions, it is necessary to enhance its capability to accurately cite documents corresponding to claims, referred to as citation quality. Each response y contains multiple statements comprising a claim and multiple citations. We aim for each claim to be fully supported by the cited documents, while avoiding irrelevant citations. Formally, we denote the statement as s, the claim as c, and the citation as t. The correct citation of references corresponding to claims can be expressed as follows:\n$y = \\{s_1, s_2, ..., s_n\\}$\n$s_i = \\{\\text{``claim''}:c_i, \\text{``citation''}:t_i = [t_{i1}, t_{i2}, ..., t_{in}]\\}$\n$\\forall s_i \\in y, \\phi(\\text{concat}(t_i), c_i) = 1$\n$\\forall s_i \\in y, \\forall t_{ij} \\in t_i,$\n$\\phi(t_{ij}, c_i) = 1 \\lor \\phi(\\text{concat}(t_i \\setminus t_{ij}), c_i) = 0$\nHere, concat(ti) denotes the concatenation of all cited documents, and $(t, c) = 1$ indicates that the claim c is fully supported by the citation t. Formula 4 indicates that all claims can be fully supported by their citations, while Formula 5 ensures that no irrelevant documents are cited."}, {"title": "2.2 Preference Optimization Under Different Document Qualities", "content": "As described in \u00a7 2.1, enhancing the correctness of the RAG generator is the primary objective. However, optimizing correctness is highly complex. Specifically, the probability of a model correctly answering a question is P(y|x). In the RAG framework, this probability depends on the distribution of the retrieved documents and can be formulated as $P(y|x) = \\sum_D[P(D|x) \\cdot P(y|D, x)]$. However, the retriever in a RAG system is not perfect, leading to variability in document quality under different circumstances, which results in a non-uniform distribution of P(D|x). Therefore, it is essential to optimize the generator under different document quality conditions.\nWe categorize document quality into two types. The first type involves scenarios where all documents contain relevant answers. In this scenario, we expect the generator to fully utilize the valuable documents and produce complete answers. Therefore, when the documents are of high-quality, we need to focus on optimizing response informativeness. The second type includes scenarios where the document set contains noisy documents. In this case, we expect the generator to avoid the interference of noisy documents and still maintain the completeness of its answers. Thus, when dealing with low-quality documents, the emphasis shifts to optimizing response robustness. However, optimizing for informativeness and robustness pulls the generator in different directions. The former encourages referencing more documents, while the latter demands ignoring more. Standard instruction fine-tuning is insufficient to teach the model how to balance these competing demands (documents encouraged to be referenced are valuable, while those that should be ignored are considered noisy, see \u00a7 4.5 for details), making it difficult to optimize the generator effectively. To address this challenge, we introduce preference information and perform multi-perspective preference optimization to guide the generator in focusing on valuable documents while ignoring noisy ones."}, {"title": "3 Methodology", "content": "After defining the optimization objectives, we trained the generator in two phases. The first phase enabled a general LLM to acquire basic RAG capabilities. The method for constructing the training data for this phase is detailed in \u00a7 3.1. The second phase involved multi-perspective preference optimization, further enhancing the generator's response informativeness, response robustness, and citation quality. The method for constructing the training data for this phase is described in \u00a7 3.2."}, {"title": "3.1 Instruction Fine-tuning for Basic RAG Capabilities", "content": "During the instruction fine-tuning phase, we aimed to equip the generator with fundamental abilities to utilize and cite documents following the optimization objectives outlined in \u00a7 2.1. When constructing the training data, we employ ChatGPT-3.5 (GPT-3.5-Turbo-1106) and introduce a citation rewrite mechanism to create near-perfect responses. Firstly, we need to acquire high-quality documents after obtaining the questions and short answers provided by existing datasets. We use the RAG retriever to retrieve the top 100 most relevant documents from the retrieval corpus, and then filter to retain the golden documents that contain the short answer. Subsequently, we select up to five documents from all the golden documents as prompt documents, ensuring that all short answers are included in the prompt documents.\nSecondly, we need to construct responses that include all short answers and exhibit high citation quality. Specifically, we use ChatGPT-3.5 to generate responses. To enhance the quality of the responses, we include instructions and short answer hints in the ChatGPT prompt. Although ChatGPT-3.5 is quite powerful and the prompt includes short answer hints, the quality of the citations in the answers is unsatisfactory. Therefore, we introduce a citation rewrite mechanism to improve the quality of substandard citations, which is divided into the following steps: First, verify the citation. We use a Natural Language Inference (NLI) model to validate whether the document corresponding to each citation (premise) in the statement can satisfy the claim (hypothesis). Second, construct the citation. If the citation in the first step cannot support the claim, we traverse the power set of all prompt documents as citations to explore a feasible citation scheme. Third, simplify the citation. If the citation can support the claim, we check one by one whether the citation is irrelevant to the claim and remove irrelevant citations.\nAfter generating answers with ChatGPT-3.5 and performing citation rewrite, we filter to retain responses that include all short answers and contain only accurate citations to serve as the training data for the instruction fine-tuning of the generator, which is then subsequently fine-tuned."}, {"title": "3.2 Preference Optimization of the Generator through DPO", "content": "Once the generator has acquired the fundamental ability to utilize and reference documents, we sequentially optimize the generator from the perspectives of response informativeness, response robustness, and citation quality during the preference optimization phase. Preference optimization requires constructing data with preference information, including an input, a superior answer (chosen output), and an inferior answer (rejected output). We will progressively explain the methods for constructing preference data from each perspective."}, {"title": "3.2.1 Response Informativeness", "content": "Response informativeness refers to the completeness of the answer in the response. The optimization objective from this perspective is to make the generator fully utilize the document that contains the short answer.\nInput Construction: Similar to constructing the input component of the instruction fine-tuning data, the input includes the instruction, the question, and the high-quality prompt documents containing all short answers from up to 5 golden documents.\nChosen Output Construction: The construction method is consistent with the output component of the instruction fine-tuning data, using ChatGPT-3.5 and the citation rewrite mechanism to construct a response that includes all short answers and the accurate citation.\nRejected Output Construction: To simulate the scenario where the generator ignores parts of the golden documents, we delete some golden documents from the prompt, generate responses with the generator, and filter to retain inferior answers that are incomplete."}, {"title": "3.2.2 Response Robustness", "content": "Response robustness refers to the ability of the generator to resist interference. The optimization objective from this perspective is to enable the generator to avoid interference from noisy documents.\nNoisy document construction: We categorize noisy documents into two types. The first type is documents related to the question but does not contain the answer, from which we randomly select two documents that do not contain short answers from the top 100 relevant retrieved documents. The second type is irrelevant documents to the question, from which we randomly select two documents that do not contain short answers from the retrieval documents of other questions.\nInput construction: The input includes the instruction, the question, and the low-quality prompt documents that mix up to five golden documents and four noisy documents.\nChosen output construction: To simulate a scenario where the generator ignores all noisy documents, we use the chosen output generated in the absence of noisy documents in \u00a7 3.2.1.\nRejected output construction: Use inputs containing low-quality documents, generate responses with the generator, and filter to retain inferior answers that are incomplete."}, {"title": "3.2.3 Citation Quality", "content": "Citation quality refers to the generator's ability to cite documents correctly. The optimization objective from this perspective is to enable the generator to cite documents related to the claim correctly and to avoid citing irrelevant documents.\nData construction is divided into two steps: First, we use the generator to generate responses and filter to retain those containing all short answers. Second, we use the citation rewrite mechanism to identify incorrect citations that fail the NLI model verification or cite irrelevant documents as rejected output and than correct them as chosen output.\nConstruction of input: The input includes the instruction, the questions, the prompt documents, and the response up to the incorrect citation.\nConstruction of chosen output: Correct citation after citation rewrite.\nConstruction of rejected output: Incorrect citation that fails the NLI model verification or cites irrelevant documents."}, {"title": "3.2.4 Staged Preference Optimization", "content": "Given the substantial discrepancies in preference information from different perspectives, we divide the preference optimization process into multiple sub-stages to independently optimize the RAG preferences from each perspective. Specifically, the first and second sub-stages optimize response informativeness and response robustness, respectively. The rejected outputs in these stages are generated by the generator that has undergone instruction fine-tuning. The third sub-stage optimizes citation quality, with preference data generated by the generator that has undergone optimization in the first and second sub-stages."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Experimental Setup", "content": null}, {"title": "4.1.1 Dataset and Evaluation Methodology", "content": "Datasets The questions used for constructing our training data are sourced from the ASQA, WebQuestions, and Natural Questions training splits. The evaluation data is sourced from the test splits of the three datasets mentioned above, along with TriviaQA, which serves as an unseen dataset to assess the out-of-domain generalizability of the generator.\nEvaluation Metrics To align with the optimization objectives discussed in \u00a7 2.1, we evaluate the generator's performance regarding correctness and citation quality. Following ALCE and VTG, for correctness, we assess whether the short answers (provided by the dataset) are exact substrings of the generation to calculate the exact match (EM) score. For citation quality, we use citation recall to evaluate whether the output is fully supported by the cited documents and citation precision to assess whether irrelevant documents are cited."}, {"title": "4.1.2 Implementation Details", "content": "We selected three general LLMs as the base RAG generator: LLAMA2-7B-CHAT, LLAMA2-13B-CHAT, and LLAMA3-8B-INSTRUCT. For the NLI model of citation rewrite and citation quality evaluation, we follow the works of Gao et al. (2023a), Sun et al. (2023), and Huang et al. (2024), utilizing the NLI model TRUE (Honovich et al., 2022), a T5-11B model that is fine-tuned on a collection of NLI datasets. Meanwhile, ALCE has verified through extensive human evaluations that this NLI model correlates with human judgment. For retrieval, we employed the Wikipedia dump from December 20, 2018, as our retrieval corpus and used GTR as our dense retriever.\nWe performed full fine-tuning on the generators, with the training hyperparameters detailed in Appendix A. All generators were trained on the same dataset, in which the RAG generator used to generate preference data was the LLAMA2-7B-CHAT at different stages of fine-tuning."}, {"title": "4.1.3 Baselines", "content": "We adopt the following baseline systems. 1) Base generator, the general LLM without further optimization. 2) RetRobust-13B (Yoran et al., 2024b), a method that enhances the robustness of the RAG generator through SFT. 3) Self-RAG-13B (Asai et al., 2024), a method that improves the performance of the RAG generator through a rank-then-generate pipeline. 4) SFT on chosen, to compare preference optimization and SFT, we use the chosen output of preference data for SFT on the LLAMA2-7B-CHAT. All the methods above were evaluated using the same prompt documents used for PA-RAG to facilitate a fair comparison."}, {"title": "4.2 Main Results", "content": "The evaluation results of PA-RAG and the baseline across four QA datasets are presented in Table 1. We have made the following observations: First, PA-RAG consistently and significantly improves the performance of various LLMs in RAG scenarios, with an average increase of 13.97% in EM score, 49.77% in citation recall, and 39.58% in citation precision. This demonstrates that our training data is applicable to different LLMs, showcasing strong generalizability. Meanwhile, PA-RAG does not compromise the fluency of the generator's outputs. Second, PA-RAG significantly outperforms the baselines, indicating that compared to end-to-end generators trained via SFT and pipeline generators, the end-to-end generator trained through preference optimization aligns better with RAG preferences, resulting in superior performance."}, {"title": "4.3 Ablation Study", "content": "The evaluation results for each training phase are presented in Table 2. We observed that each training phase generally contributes to improvements in both correctness and citation quality, indicating that optimization of RAG requirements in each perspective is necessary. Furthermore, correctness and citation quality are complementary to each other."}, {"title": "4.4 Impact of Preference Optimization Order", "content": "In the main experiment, we first optimized response informativeness, followed by response robustness. We further investigated the impact of altering the optimization order on performance, with results shown in Table 3. Our findings are as follows: First, optimizing response robustness before response informativeness resulted in lower performance than the main experimental setup. Second, skipping informativeness and directly optimizing robustness or merging the data to optimize simultaneously can lead to a decline in performance. We believe this is because optimizing informativeness enables the generator to learn how to effectively utilize retrieved documents, which is the fundamental capability in the RAG system. In contrast, optimizing robustness requires the generator to learn to reject irrelevant documents, which is a more advanced skill. Learning skills at different levels requires a reasonable arrangement in learning order, which is similar to curriculum learning (Bengio et al., 2009). Neglecting the foundational skills or mixing different skill levels during learning may hurt the model's original capabilities."}, {"title": "4.5 Further Comparison of DPO and SFT", "content": "As mentioned in \u00a7 2.2, there is a significant disparity in the optimization directions of response informativeness and robustness. We further explore the differences between DPO and SFT when optimizing these two directions sequentially. As shown in Table 4, we observe that, when using SFT for training, optimizing response informativeness first can enhance the generator's performance. However, subsequent optimization for response robustness leato a significant performance decline, exposing SFT's vulnerability to catastrophic forgetting (French and Chater, 2002) when undergoing two relatively different optimizations consecutively. In contrast, DPO handles this situation well, consistently improving the generator's performance."}, {"title": "5 Related Work", "content": null}, {"title": "5.1 Retrieval-Augmented Generator", "content": "To satisfy the requirements of RAG system, existing retrieval-augmented generators are primarily categorized into end-to-end and pipeline frameworks. In end-to-end frameworks, models such as RetRobust and RAAT enhance the robustness of the generator by performing SFT on high-quality data. In pipeline frameworks, models like DPA-RAG, REAR , Self-RAG , and RankRAG enhance the robustness of the generator by explicitly re-ranking the retrieved documents. Furthermore, VTG improves citation quality by introducing explicit citation verification and modification. However, these methods still struggle to align the generator with the RAG requirements fully."}, {"title": "5.2 Fine-tuning Approaches for LMs", "content": "The mainstream approaches for fine-tuning language models in generative tasks include SFT and reinforcement learning from human feedback (RLHF). SFT involves constructing input-output pairs to teach the model how to complete tasks. RLHF methods like DPO incorporate training data that includes both superior and inferior outputs, allowing the model to learn preference information. RLHF is more effective than SFT in aligning the model with task preferences."}, {"title": "6 Conclusion", "content": "In this work, we propose PA-RAG, a method for optimizing the generator of RAG systems to align with specific RAG requirements comprehensively. The training process includes instruction fine-tuning and multi-perspective preference optimization. We conducted extensive experiments on four QA benchmarks and three LLMs, demonstrating that PA-RAG can significantly enhance the generator's response informativeness, response robustness, and citation quality."}, {"title": "Limitations", "content": "Our method requires fine-tuning in four stages, including one instruction fine-tuning stage and three preference optimization stages. This results in a cumbersome search for the optimal hyperparameter settings during training. We have presented the hyperparameter details in Appendix A to prevent other researchers from duplicating the search for the best hyperparameter settings."}, {"title": "Ethics Statement", "content": "This work was conducted in strict compliance with the ACL Ethics Policy. All datasets and models used for experiment are publicly available. Furthermore, our work aims to explore a RAG generator training method. We do not foresee any negative ethical impacts arising from our work."}, {"title": "A Training Hyperparameters", "content": "We utilized full fine-tuning for all training stages and employed the same hyperparameter settings for all models.\nDuring the instruction fine-tuning phase, we set the batch size to 128, the learning rate to 2e-5, and trained for one epoch.\nIn the preference optimization phase, we set the batch size to 64 and trained for one epoch for all stages. For the optimization stages of response informativeness and response robustness, the learning rate is 2e-6. In the citation quality optimization stage, the learning rate is 2e-7."}, {"title": "B Compute Resources", "content": "Our experiments were conducted on a server equipped with 1TB of memory and 4 Nvidia A800 80G GPUs."}, {"title": "C Training Data Statisics", "content": "Detailed information on the data size and composition for each training stage can be found in Table 5."}, {"title": "C.1 Distribution of Citation Complexity in Instruction Fine-tuning", "content": "In the instruction fine-tuning training data, the distribution of the number of citations per claim is shown in Table 6."}, {"title": "C.2 The Disparity Between \u201cChosen\u201d and \"Rejected\" in Preference Data.", "content": null}, {"title": "D Prompt", "content": "Prompt for ChatGPT-3.5\nInstruction: Write an accurate, engaging, and concise answer for the given question using only the provided search results (some of which might be irrelevant) and cite them properly. Use an unbiased and journalistic tone. Always cite for any factual claim. When citing several search results, use [1][2][3]. Cite at least one document and at most three documents in each sentence. If multiple documents support the sentence, only cite a minimum sufficient subset of the documents.\nQustion: {Question}\nThe final answer should contain the following short answers: {Short answers}\nDocuments: {Documents}\nAnswer:"}, {"title": "E Examples of Adjusting the Citation Number", "content": null}, {"title": "E.1 Rejected Output for Response Informativeness", "content": "When constructing the DPO training data for response informativeness, a question and five golden documents were given, numbered [1][2][3][4][5]. When constructing the rejected output, we would remove some golden documents, such as documents [1] and [3], leaving documents [2], [4], and [5]. To maintain a unified input format, the documents will be renumbered starting from 1."}, {"title": "E.2 Chosen Output for Response Robustness", "content": "When constructing the training data for response robustness, given a question and documents numbered [1][2][3][4][5][6][7][8], where [1][3][4][7] are golden documents and [2][5][6][8] are noisy documents. When constructing the chosen output, we will remove all the noisy documents."}, {"title": "F Detailed Analysis of Why Rejecting Irrelevant Documents is a More Advanced Skill", "content": "As shown in Table 3, in our experiments, skipping the training for informativeness (learning to utilize relevant documents) and directly conducting the training for robustness (learning to ignore irrelevant documents) leads to a decline in model performance. This is because the model may not realize that the documents it needs to ignore are the irrelevant ones, resulting in indiscriminate neglect of most documents. In contrast, performing training for informativeness first and then training for robustness improves performance. This is because the model has already developed a basic ability to utilize documents and can recognize that it should ignore only the irrelevant documents. Therefore, ignoring irrelevant documents is a more advanced skill than fully utilizing every relevant document."}, {"title": "G Detailed Analysis of Why Optimizing Citation Quality Might Have a Negative Impact on citation Recall.", "content": "It is challenging to simultaneously improve both the precision and recall of citation quality. While enhancing recall, the model may become stricter in assessing the relevance of documents, which can lead to a decrease in precision. Therefore, we introduced the F1 score to evaluate citation quality from a more balanced perspective. Our experimental results indicate that optimizing response informativeness and response robustness can enhance answer correctness, while optimizing citation quality can improve the citation F1 score."}, {"title": "H Further analysis of whether PA-RAG affects the fluency of responses", "content": "We follow ALCE , using MAUVE to evaluate the fluency of the model's responses."}, {"title": "I Specific Criteria and Examples of Inferior Answers that are Incomplete", "content": "Inferior answers refer to responses that do not meet the criteria for correct answers. The criteria for the correct answers are shown in \u00a7 2.1. Transform these into criteria for inferior answers is as follows: Let G represent the generator, x denotes the input, $D = \\{d_1, d_2, ..., d_n\\}$ represent the documents retrieved by the retriever, $A = \\{a_1, a_2, ..., a_n\\}$ represent the short answers contained in the documents, and y denote the response generated by the generator. The correct response by the generator can be expressed as:\n$y = G(x, D), \\exists a_i \\in A,C(y, a_i) = False$"}, {"title": "J LLM Evaluation", "content": "As an accuracy evaluation metric, Exact Match (EM) cannot directly reflect the response informativeness and response robustness. We follow AlpacaEval and use the LLM (GPT-3.5-Turbo) evaluation to qualitatively judge whether the response informativeness and response robustness increase after RAG preference alignment."}, {"title": "K Case Study", "content": "To more intuitively demonstrate the improvements of preference optimization on response informativeness, response robustness, and citation quality, we randomly sampled several outputs from the LLAMA2-13B-CHAT before and after preference alignment (PA) for comparison. The examples are as follows.\nTake Case 1 as an example, we found that the generator without PA-RAG failed to utilize the document containing the answer and was misled by the noisy document, mistakenly identifying Erika Flores as the actress who played Ingrid, resulting in the incorrect citation. In contrast, the generator with PA-RAG training correctly used the document containing the answer and provided a correct response, demonstrating better response informativeness, response robustness, and citation quality."}]}