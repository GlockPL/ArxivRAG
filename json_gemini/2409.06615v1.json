{"title": "One-Shot Imitation under Mismatched Execution", "authors": ["Kushal Kedia", "Prithwish Dan", "Sanjiban Choudhury"], "abstract": "Human demonstrations as prompts are a powerful way to program robots to do long-horizon manipulation tasks. However, directly translating such demonstrations into robot-executable actions poses significant challenges due to execution mismatches, such as different movement styles and physical capabilities. Existing methods either rely on robot-demonstrator paired data, which is infeasible to scale, or overly rely on frame-level visual similarities, which fail to hold. To address these challenges, we propose RHYME, a novel framework that automatically establishes task execution correspondences between the robot and the demonstrator by using optimal transport costs. Given long-horizon robot demonstrations, RHYME synthesizes semantically equivalent human demonstrations by retrieving and composing similar short-horizon human clips, facilitating effective policy training without the need for paired data. We show that RHYME outperforms a range of baselines across various cross-embodiment datasets on all degrees of mismatches. Through detailed analysis, we uncover insights for learning and leveraging cross-embodiment visual representations.", "sections": [{"title": "I. INTRODUCTION", "content": "Human demonstrations as prompts are a powerful way to program robots to do long-horizon manipulation tasks. Compared to language instructions, demonstrations are grounded in the task environment and are rich with cues that detail what steps to follow, which objects to interact with, and how to interact with them.\nWe can view this as a translation problem where a human video must be translated into a series of robot actions [1], [2]. However, in the most naive setting, training such policies requires paired data of human and robot demonstrations, which are infeasible to collect at a large scale for long-horizon tasks. While there exists internet-scale human demonstration videos (e.g. YouTube) and increasingly large robot datasets [3], [4], these datasets are unpaired and cannot directly be used to learn the translation.\nPrior works attempt to leverage unpaired human and robot demonstrations to learn an aligned visual representation that map both human and robot images to a common embedding [5], [6]. A policy is then trained to generate actions conditioned on robot video embeddings, and directly transferred at test time to work with embeddings from a human prompt video. However, a key assumption these works rely on is that the demonstrator and robot perform tasks with matched execution, i.e. the human executes tasks in a visually similar way to that of the robot (e.g. slowly moving one arm with a simple grasp). Oftentimes, this is not the case: it is natural for humans to act more swiftly, use both hands to manipulate objects, or even execute two separate tasks simultaneously."}, {"title": "II. RELATED WORK", "content": "Demonstrations have been utilized to guide robot manipulation policies in several different ways. We place our work relative to each cluster of related research.\nTracking reference motion. The imitation challenge is reduced to motion tracking when the robot receives the demonstrator's motion as input. Robots with human-like joint configurations can directly mimic human trajectories, a method applied to humanoid robots with similar morphologies [7]-[12] and robotic hands [13]-[20]. When the execution capabilities are mismatched, either human models are simplified [7], [9], [11] or robot trajectories [8], [10] are optimized to approximately match the reference trajectory. [12] shows a method to extract human poses from videos and use that as a reference for training reinforcement learning agents. Similarly, human demonstrations have been used to guide reinforcement learning for robotic hands [13]-[16]. More recently, large-scale video datasets of humans on the internet have been used to extract hand positions and adapted for robot manipulation [17]\u2013[20]. Distinct from these works, we focus on learning robot manipulation tasks directly from demonstrators' RGB videos without explicit motion input.\nLearning reward functions from observations. These methods tackle the problem of matching the demonstrator when their motion cannot be simply mimicked. Still, their videos contain useful task information and can be used to learn reward functions for reinforcement learning on the robot. A general method in this line of research [21]\u2013[24] is to extract reward functions that encourage the robot to manipulate objects in the same way as the video. For example, [21] self-supervises the robot by making it learn to match the demonstrations of a demonstrator perturbing a rope. GraphIRL [24] extracts sequences of object pose movement from the demonstrator and enforces temporal cyclic consistency with the robot's execution. Other approaches frame the problem as task matching [25], [26], where the robot is rewarded when it is deemed to perform the same skill as the demonstrator. While these papers tackle the problem of mismatched execution directly, they require reinforcement learning to train robot policies, which can be challenging for complex tasks.\nLearning aligned robot-demonstrator representations. Another strategy for addressing this challenge is to train representations for both the robot and the demonstrator that are indistinguishable when performing the same task. This approach often frames the task as a video translation challenge, where the demonstrator's video is converted into a robot's perspective to simplify task mimicry [27]\u2013[29]. Additionally, methods like WHIRL [30] align videos by effectively masking out both the robot and the demonstrator, creating a neutral visual field. Embeddings can be aligned using datasets that either directly pair human and demonstrator actions or utilize human preference datasets to rank image frames, as shown in X-IRL [31] and RAPL [32]. Unlike these methods, our approach does not depend on labeled correspondences between robots and demonstrators.\nOne-shot visual imitation from demonstration videos. We tackle this problem setting in our work where the robot imitates actions from demonstrator videos in a one-shot setting, i.e., the robot uses a prompt video as a guide, aiming to replicate the demonstrated actions after viewing them once [1], [2], [5], [6], [33]-[35]. If a paired dataset of robot and demonstrator videos executing the same task exists, the robot can learn to translate a prompt video into actions directly [1], [2]. The closest to our work is the setting without paired data of demonstrator and robot skills. Prior works [5], [6] train policies conditioning on robot videos and zero-shot transfer to a prompt demonstration at test time using aligned visual embeddings. For example, XSkill [5] uses a self-supervised clustering algorithm based on visual similarity to align representations of demonstrations and robot videos. However, such an approach can falter when there are significant mismatches in execution. We address this issue by posing the visual imitation problem as one of train-time retrieval. During training, we match robot videos to the closest demo videos from an unpaired play dataset to imagine synthetic demo videos. Training robot policies conditioned on these synthetic videos enable the robot to translate demonstration videos into robot actions at test time."}, {"title": "III. PROBLEM FORMULATION", "content": "Test Time: Translate Demonstration Video to Robot Actions. The robot's goal is to replicate a series of tasks as shown by the demonstrator using a policy $\\pi(A_R|S_R, V_H)$ that translates a demonstration video to predict the robot's action $a_R$ at a robot state $S_R$. The demonstration video is a sequence of images $v_H = \\{v_1^H, v_2^H, ..., v_T^H\\}$, where $T$ is the length of the video.\nTrain Time: Learning from Unpaired Demonstrator and Robot Data. Training the policy is straightforward given paired robot and demonstrator data. But, this is infeasible to collect at scale. Instead, we setup the problem as learning aligned embeddings from unpaired data that enable transferring policies trained on robot data embeddings to demonstration embeddings.\nDatasets. We assume access to two distinct datasets: a robot dataset of long-horizon manipulation tasks, denoted as $D_{robot}$, and a play dataset, denoted as $D_{play}$, consisting of short-horizon demonstrator (e.g., a human) video clips that show how to interact with different objects and the environment. The robot dataset, $D_{robot} = \\{(E_R, V_R)\\}$, comprises pairs of state-action trajectories and robot videos. Each robot trajectory, $E_R = \\{(s_1^R, a_1^R), (s_2^R, a_2^R), ..., (s_T^R, a_T^R)\\}$, represents the sequence of robot states and actions throughout an episode. Correspondingly, the video $V_R = \\{v_1^R, v_2^R, ..., v_T^R\\}$ is a sequence of images of the robot executing the task. The play dataset, $D_{play} = \\{V_H\\}$, consists of demonstrator videos without any direct correspondence to the robot's dataset. At test-time, the demonstrator's video consists of a set of"}, {"title": "IV. APPROACH", "content": "We present RHYME, a one-shot imitation learning algorithm that translates demonstrations to robot actions, without paired data. Before policy training, we first train a video encoder on using a dataset of unpaired robot and demonstrator videos (Section IV-A). Then, this trained video-encoder is frozen and utilized for retrieval during policy training (Section IV-B). At train time, given just a robot trajectory, RHYME imagines a corresponding demonstration by retrieving and composing corresponding demonstrator snippets. It then trains a policy to condition on this imagined demonstration to predict robot actions. We discuss details of the retrieval, training process, and video embeddings below.\nA. Representation Learning of Visual Embeddings\nWe align embeddings of the robot and the demonstrator videos without trajectory-level correspondences in three different ways: visually, temporally and at the task-level. We employ unsupervised losses for visual and temporal alignment following prior works [36], [37] (see Appendix for details), and introduce an optional task alignment loss.\nTask Alignment. The task-level alignment is applied when there exists a small dataset of short-horizon snippets of the robot and demonstrator executing the same task. In contrast to the other frame-level representation learning methods used by prior methods, this method focuses on aligning video embeddings of the robot $Z_R$ and the demonstrator $Z_H$. To do so, we use the optimal transport distance function defined in Section IV-B, $d(Z_R, Z_H)$ to measures the matching between two sequence of video embeddings that may differ in length. Our alignment attempts to pull embedding sequences of paired human-demonstrator tasks closer to each other while pushing those of different tasks away from each other. We achieve this result by applying the INFO-NCE [38] contrastive learning objective similarly as before. In a batch $B$, consisting of paired demonstrator and robot video snippets $\\{Z_R^i, Z_H^i\\}_{i=1}^B$, we generate negative pairs by matching video embeddings from distinct pairs within the batch. Then, we construct the following loss function:\n$L_{task}(\\phi) = - \\frac{exp(-d(Z_R^i, Z_H^i))}{\\sum_{j=1}^B exp(-d(Z_R^i, Z_H^i)) + exp(-(Z_R^i, Z_H^j))}$"}, {"title": "Algorithm 1 Imagine-Demo: Retrieving Matched Demonstrator Embeddings", "content": "Input: Robot Embeddings ZR, Demonstrator Play Dataset\nDplay, Video Encoder (z|v), Segment Length K, Dis-tance Function d\nOutput: Imagined Demo 2H\nInitialize empty demo 2H \u2190 {}\n//Divide long-horizon robot sequence into\nshort-horizon clips\nZR = {ZK, Z 1:KK+1:2K\nR\n2\n,...,z-K+1:T}\nfor robot segment R +K in ZR do\n// Find closest short-horizon clip\nembedding in play dataset\n2play \u2190 arg min d(Zplay, zi+K)\n// Extend imagined embedding sequence with\nretrieved demo\n2H.extend(2play)\nReturn Imagined Demo H"}, {"title": "Algorithm 2 RHYME: Retrieval for Hybrid imitation under Mismatched Execution", "content": "Input: Robot Dataset Drobot, Demonstrator Play Dataset\nDplay, Video Encoder (z|v)\nOutput: Trained Robot Policy \u03c0\u03bf(als, z)\nInitialize Robot Policy \u03c0\u03bf\nwhile not converged do\nGet robot video and actions R, VR ~ Drobot\nGenerate robot embeddings ZR = \u03c6(VR)\n// Retrieve demonstrator embeddings from\nplay data\n2H \u2190 Imagine-Demo (ZR, Dplay)\n// Hybrid Training\nfor (st, at) in \u00c9R do\n//Condition on imagined demo\n\u03c0\u03bf \u2190 Update-Policy(at, \u03c0\u03bf(St, 2H))\n//Condition on robot video\n\u03c0\u03bf \u2190 Update-Policy(at,\u03c0\u03bf(St, ZR))\nReturn Trained Robot Policy \u03c0"}, {"title": "B. RHYME: Retrieval for Hybrid imitation under Mismatched Execution", "content": "Training Overview. Algorithm 2 provides an overview\nof our hybrid learning approach to train a robot policy that\nlearns to condition on mismatched demonstration videos.\nThe policy is trained on robot data that has robot trajectory\nER and the robot video VR. The policy is trained in a\nhybrid fashion over two modes: (1) on robot video and 2)"}, {"title": "Method 1: Computing Distance via Optimal Transport.", "content": "In this formulation, we define the distance we attempt to find matching between the sequence between robot and demonstrator video embeddings. We do this by calculating the total Wasserstein distance between the videos, i.e., the cost of the optimal transport plan that transfers one sequence of video embeddings into another. The robot's embedding distribution is defined as $P_R = \\{1/T, 1/T, . . .,1/T\\}$, and the demonstrator's embedding distribution is defined as $P_H = \\{1/T',1/T',...,1/T'\\}$, where $T$ and $T'$ are the lengths of the video sequences respectively. The cost function for the transport is $C \\in \\mathbb{R}^{T\\times T'}$ where $C_{ij}$ is the cosine distance between the robot embedding $z_i^R$ and the demonstration embedding $z_j^H$. Our goal is to find the optimal assignment $M \\in \\mathbb{R}^{T\\times T'}$ that transports the distribution from $P_R$ to $P_H$ while minimizing the cost of the plan. Formally, we need to find $M^* = arg \\min \\sum_i \\sum_j C_{ij} M_{ij}$. After solving the optimal transport assignment, the distance function is the cost of the plan, i.e., $d(Z_R, Z_H) = \\sum_i\\sum_j C_{ij} M^*_{ij}$. In practice, we optimize an entropy-regularized version of this problem to find an approximate solution efficiently using the Sinkhorn-Knopp algorithm [39]. More details are provided in the appendix."}, {"title": "Method 2: Computing Distance via Temporal-Cyclic Consistency (TCC).", "content": "Originally proposed by [40], the TCC loss function can be used to match videos of a robot and its demonstrator [24], [31]. The loss function enforces cyclic consistency between robot video embedding are $Z_R = \\{z_1^R, ..., z_T^R\\}$ and its demonstrator $Z_H = \\{z_1^H, z_2^H, ..., z_{T'}^H\\}$. Starting from a robot frame $z_t^R$, the TCC loss first computes $\\alpha$, a similarity distribution of $z_t^R$ with respect to the demonstrator's embeddings, cycling to its soft-nearest neighbor $\\bar{z_t^H} = \\sum_{t'=1}^{T'} \\alpha_{t'} z_{t'}^H$. Then, $\\bar{z_t^H}$ cycles back to the robot video by again computing its similarity distribution $\\beta$ with respect to robot video embeddings to get its soft-nearest neighbor $\\bar{z_t^R} = \\sum_{t=1}^{T} \\beta_t z_t^R$. Formally, the cycling process is described as:\n$\\alpha_t = \\frac{e^{-||z_t^R - z_{t'}^H||^2}}{\\sum_{t'=1}^{T'} e^{-||z_t^R - z_{t'}^H||^2}}$\n$\\bar{z_t^H} = \\sum_{t'=1}^{T'} \\alpha_t z_{t'}^H$\n$\\beta_t = \\frac{e^{-||z_t^H - z_t^R||^2}}{\\sum_{t=1}^T e^{-||z_t^H - z_t^R||^2}}$\n$\\bar{z_t^R} = \\sum_{t=1}^{T} \\beta_t z_t^R$\nThe TCC loss function for a robot frame $z_t^R$ is the mean square error with its cycled-back frame $\\bar{z_t^R}$ as $l_{tcc} = ||z_t^R - \\bar{z_t^R}||^2$. We define the video-level TCC distance function by summing over the frame-level losses $d(Z_R, Z_H) = \\sum_{t=1}^T l_{tcc}(z_t^R)$.\nUsing the optimal transport formulation, we measure distance by matching the distribution of two video embeddings. In contrast, the TCC distance attempts to establish strong one-to-one correspondences between the frames of two videos. We hypothesize that video retrieval using TCC distance can be inaccurate in two scenarios: 1) when the embeddings from robot and demonstrator tasks are not closely aligned, and 2) when multiple robot embeddings"}, {"title": "V. EXPERIMENTS", "content": "Simulation Setup and Datasets. We test our approach on the Franka Kitchen simulator [41], in which a 7-DOF Franka arm can perform 7 different tasks. 580 robot trajectories, each executing 4 tasks sequentially, are used to train our models. Datasets of cross-embodiment videos are created in three different ways, progressively increasing the embodiment gap. Introduced by XSkill [5], The SPHERE-EASY dataset replaces the robot model's visual rendering in the simulator with a sphere that follows the gripper's position, creating only a visual gap. To create mismatches in manipulation styles, we construct the SPHERE-MEDIUM dataset by handcrafting randomized motion primitives to complete tasks. For example, the robot drags the kettle object while the demonstrating agent lifts and carries it. Finally, the SPHERE-HARD dataset further challenges the robot as the demonstrator performs two tasks simultaneously, mimicking how humans use two hands. Fig 2 (left) illustrates the simulator and the cross-embodiment datasets. Before training policies, all methods utilize a database of unpaired robot and demonstrator videos to pre-train visual representations. All policies are trained using Diffusion Policy [42]. More details are provided in the appendix.\nBaselines. XSkill [5] simply conditions on robot videos during train-time, and uses its shared representation space to generalize zero-shot to inputs of demonstrator videos at test-time. OraclePairing [1], [2] is the gold-standard approach, assuming pairing between a demonstrator and each robot trajectory, enabling conditioning on the demonstrator at train time. Our approach, RHYME, finds a middle ground. Without pairing, it imagines demonstrator trajectories that perform the same tasks as a robot trajectory (Section IV-B). We compare two variants of our algorithm, RHYME-TCC and RHYME-OT, which differ in their distance functions used for retrieval. We also show how vision representations can be improved using short-horizon robot-demonstrator task pairs.\nEvaluation and Success Metrics. At test-time, we provide the robot policy with demonstrator videos consisting of a composition of 4 tasks unseen at train-time. We measure Task Recall, which measures recall by counting the successfully completed tasks shown in the demonstrator's video, and Task Imprecision, which measures imprecision and reports the percentage of tasks the robot attempts incorrectly\u2014those not specified in the demonstrator's video.\nQ1. How does performance vary across different levels of execution mismatch?\nFig 2 reports task recall and imprecision rates across datasets. As the demonstrator's actions visually and physically deviate further from those of the robot, policies trained with our framework RHYME consistently outperform XSkill. OraclePairing baseline consistently achieves greater than 90% task recall without almost any task im-precision. The easiest dataset, SPHERE-EASY, only exhibits a cosmetic gap between embodiments, keeping robot and demonstrator object movements the same. All methods have high task recall while maintaining precision. Still, we see that RHYME-OT has greater than 90% task recall compared to XSkill's 82%. This gap increases on the SPHERE-MEDIUM dataset (72% vs 42%) and becomes larger on the hardest dataset, SPHERE-HARD.\nFig 3 (left) investigates this trend by probing the visual representations of the video encoder \u03c6, common across policies. We plot the image embeddings of the robot and demonstrator across three different tasks using a t-SNE plot. SPHERE-EASY dataset, embeddings generated by both the robot and demonstrator executing the same tasks are clustered very closely. This explains why the XSkill policy can zero-shot to demonstration embeddings at test time. However, as soon as there are slight deviations between the robot and demonstrator, as seen in SPHERE-MEDIUM, its performance quickly deteriorates. Still, the embeddings of the same task are close enough for both RHYME algorithms to successfully retrieve the correct demonstration"}, {"title": "Q2. How does video retrieval using Optimal Transport and TCC impact policies at test-time?", "content": "In the SPHERE-EASY setting, where task embeddings are clustered well (Fig 3), the TCC distance is low between matching robot and demonstrator videos since embeddings cycle to and from other embeddings within the same cluster. In the harder settings where clusters are inherently ambiguous due to execution mismatches, mistakes in cycling-back are more frequent. Fig 3 (right) demonstrates a common failure mode of TCC in SPHERE-HARD: both clips complete the same two tasks, but TCC yields high distance since the robot frame moving the kettle cycles to the demonstrator multi-tasking with the kettle and light switch, thus having a significant probability of incorrectly cycling back to the robot turning on the light switch. RHYME-TCC ultimately shows some improvements in task recall compared to XSkill on these dataset, but also has increased imprecision due to incorrect retrievals (Fig 2). Notably, RHYME-OT outperforms RHYME-TCC in both metrics across all datasets.\nThe key reason for this performance difference is that optimal transport computes distances by matching videos across a sequence of embeddings. Fig 4 visualizes the cost of the optimal transport plan between prompt robot clips and demonstration videos in the hard SPHERE-HARD dataset. Comparing a robot clip doing two tasks (e.g. kettle and light), the transport cost across assignments is minimum only when compared to the demonstrator performing those same two tasks. TCC, on the other hand, attempts to establish one-to-"}, {"title": "Q3. Does fine-tuning representations with short-horizon task pairs help?", "content": "In this experiment on the SPHERE-HARD dataset, we use a small dataset of short-horizon task pairs and attempt to fine-tune the representation space to pull together sequences of embeddings (ZR, ZH) that encode the same tasks. We employ the contrastive optimal transport-based task alignment loss outlined in Section IV-A, and find that fine-tuning with this objective improves vision representations across all baselines. We report results in Fig 5 for XSkill (fine-tuned) and RHYME-OT (fine-tuned). XSkill, which only trains its policy on robot embeddings, reaps more than a 25% task recall gain, up from 0%, when supplied with the fine-tuned representation space. However, RHyME (0% fine-tuned) still significantly outperforms all XSkill (fine-tuned) policies, as the embeddings seen across human and robot videos still differ in length and values. Ultimately, we find that comparing induced distributions over embeddings with optimal transport is a beneficial design choice for matching clips at a task-level in the face of execution mismatches. As the vision representations are fine-tuned on increasingly more task pairs, RHYME-OT's task recall linearly increases. This is because task alignment helps retrieve video snippets more accurately for policy training."}, {"title": "VI. DISCUSSION AND LIMITATIONS", "content": "This work tackles the problem of one-shot imitation from a demonstrator with mismatched execution. We introduce RHYME, a novel framework that leverages task-level correspondences to overcome frame-level visual gaps between the robot and a demonstrator, and learns a video-conditioned policy without access to any paired demonstrations.\nLimitations. Although the exact composition of test-time tasks isn't seen at train-time, transitions between every pair of tasks in the unseen composition must exist in the robot dataset to learn transition actions. This prohibits our method from learning brand-new task sequences in a one-shot fashion."}]}