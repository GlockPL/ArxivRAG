{"title": "Sub-DM: Subspace Diffusion Model with Orthogonal Decomposition for MRI Reconstruction", "authors": ["Yu Guan", "Qinrong Cai", "Wei Li", "Qiuyun Fan", "Dong Liang", "Qiegen Liu"], "abstract": "Diffusion model-based approaches recently achieved remarkable success in MRI reconstruction, but integration into clinical routine remains challenging due to its time-consuming convergence. This phenomenon is particularly notable when directly apply conventional diffusion process to k-space data without considering the inherent properties of k-space sampling, limiting k-space learning efficiency and image reconstruction quality. To tackle these challenges, we introduce subspace diffusion model with orthogonal decomposition, a method (referred to as Sub-DM) that restrict the diffusion process via projections onto subspace as the k-space data distribution evolves toward noise. Particularly, the subspace diffusion model circumvents the inference challenges posed by the complex and high-dimensional characteristics of k-space data, so the highly compact subspace ensures that diffusion process requires only a few simple iterations to produce accurate prior information. Furthermore, the orthogonal decomposition strategy based on wavelet transform hinders the information loss during the migration of the vanilla diffusion process to the subspace. Considering the strategy is approximately reversible, such that the entire process can be reversed. As a result, it allows the diffusion processes in different spaces to refine models through a mutual feedback mechanism, enabling the learning of accurate prior even when dealing with complex k-space data. Comprehensive experiments on different datasets clearly demonstrate that the superiority of Sub-DM against state-of-the-art methods in terms of reconstruction speed and quality.", "sections": [{"title": "I. INTRODUCTION", "content": "Magnetic Resonance Imaging (MRI) is one of the most widely used imaging modalities due to its excellent soft tissue contrast, but it has prolonged and costly scan sessions [1], [2]. This limitation has galvanized innovations to accelerate the MRI process, all with the common goal of drastically reducing scan time without compromising image quality. A fundamental solution is to shorten scan times by under-sampled k-space data and solve an ill-posed inverse problem to reconstruct images. From sparsity-driven compressed sensing [3]-[5] to deep-learning-based model [6]-[8], significant progress has been made in MRI reconstruction. However, many existing methods are limited by suboptimal capture of the data distribution and reliance on fully-sampled acquisitions for model training.\nThe desire for more robust and efficient techniques in MRI reconstruction has led to the development of pioneering approaches, among which diffusion models (DMs) [9]\u2013[12] have recently shown to be promising. As a promising surrogate, DMs provide a more accurate representation of the data distribution and shows great potential to reconstruct MR images. One of the notable advancements in this area was initially proposed by Jalal et al. [13], where they trained the DMs using Langevin dynamics without making any assumptions on the measurement system, yielding competitive reconstruction results for both in-distribution and out-of-distribution data. Inspired by this innovative work, Song et al. [14] were committed to expanding the theoretical framework of DMs, who successfully extended them to medical image reconstruction tasks. Furthermore, Chung et al. [15] demonstrated that score-based diffusion models trained solely on magnitude images can be utilized for reconstructing complex-valued data. Luo et al. [16] described a comprehensive approach using data-driven Markov chains for MRI reconstruction which not only facilitates efficient image reconstruction across variable sampling schemes, but also enables the generation of uncertainty maps.\nDespite demonstrating high-quality sampling in MRI reconstruction, DMs suffer from slow sampling processes and high computational burden due to the need for hundreds of reverse steps for image generation [17], [18]. To overcome the inherent drawbacks caused by DMs, researchers turned the focus to exploring optimization techniques for DMs to decrease sampling time. Peng et al. [19] proposed a representative work that considered rescaling the diffusion step size during inference to accelerate image sampling, but this can potentially reduce the accuracy of reverse diffusion steps. Instead of directly altering the network structure to minimize iteration time, recent innovative studies have reengineered the diffusion process to unfold in the latent space, consequently reducing memory consumption. A straightforward extension of the latent space method proposed by Li et al. [20], which considered compressing the latent features in the DM into a low-dimensional latent space to reduce computational complexity and the number of iteration steps. While the potential is undeniably promising, these methodologies concentrate on the model optimization while neglecting the intricate characteristics of k-space data, thereby inadvertently increasing the difficulty of achieving fast convergence.\nReevaluating the essence of MRI reconstruction reveals that the main difference between MR imaging and other medical imaging modalities is the control over the k-space data acquisition and how it can be managed to yield an adequate reconstructed image [21], [22]. To effectively optimize the sampling process of the DMs for this task, a comprehensive analysis of the characteristics of k-space data is essential. Peng et al. [23] have considered this by conducting Hankel transformations on the k-space data and extracting relevant blocks for the phase of training. This approach enables them to generate samples from a training set that can be as minimal as one k-space data. Moreover, an optimized subspace formed by frequency separation of k-space data is proposed by Cao et al. [24], which ensured determinism in the fully-sampled low-frequency regions and accelerated the sampling procedure of reverse diffusion. The success of above methods stems from their ability to retain valid information in k-space data while diminishing its complexity. Hence, it is crucial to prevent the diffusion process from occurring in the space of complete data, as this can lead to refined training coverage and improved performance.\nDrawing inspiration from this methodology, this work proposed a pioneering Subspace Diffusion Model for MRI reconstruction (Sub-DM) that integrates orthogonal decomposition for dimensionality modification. One of the innovative aspects of Sub-DM is its emphasis on the role of adjusting the diffusion process to optimize model inference runtimes. Specifically, we employ orthogonal decomposition to extract feature from high-dimensional k-space data as the noise perturbations increase, thereby migrating the diffusion process to a lower-dimensional subspace and avoiding continuous sampling in the intricate full-space. This strategy ultimately enables the sampling process to be implemented within a few diffusion steps and significantly enhances the speed of image reconstruction, as shown in Fig. 1. Moreover, to prevent the loss of information during the diffusion process across different spaces, we employ orthogonal wavelet transforms, capitalizing on their invertibility, as the decomposition operator for dimensionality reduction of k-space data. In this way, it not only reduces computational complexity but also ensures effective learning during the migration process, thereby enabling rapid and accurate MRI reconstruction. The main contributions of this work are summarized as follows:\n\u2022\tTo bridge the existing gap between sampling acceleration and generation quality in DMs, we adopt a different perspective to accelerate the diffusion process by reducing the dimensionality of the original signal through orthogonal decomposition, thereby improving the efficiency and obtaining better reconstruction performance.\n\u2022\tAnalyzing the distinctive attributes of k-space data, we employ wavelet transforms to decompose it into multiple orthogonal components. This strategy effectively extracts relevant features and concurrently optimizes the solution space for the reverse diffusion process.\n\u2022\tComprehensive experiments on different datasets demonstrate that Sub-DM achieves faster convergence speed and preserves high reconstruction accuracy under highly un-der-sampling rates (i.e., 10\u00d7, 12\u00d7).\nThe following sections of the paper are organized as follows: Section II provides a brief overview of related works. Section III presents the core concept of the proposed method. Section IV details the experimental settings and results. Section V offers a succinct discussion and Section VI concludes the work."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Problem Formulation", "content": "MRI reconstruction is a challenging inverse problem due to the under-sampling operation in k-space. Its objective is to recover the original k-space signal $k \\in C^d$ from the complex-valued measurement $f \\in C^d$. Mathematically, the forward model of this task can be expressed using the following formulation:\n$f = Ak + \\eta$,\t\t(1)\nwhere $A = PFS$ is the imaging operator that captures the influence of the k-space under-sampling pattern $P$ and coil sensitivity maps $S$, $\\mathcal{I}\\mathcal{F}$ denotes the Fourier transform matrix and $\\eta$ represents the measurement noise.\nDue to the imaging operator A is often rank-deficient, making the recovery of k in Eq. (1) an ill-posed problem. Hence, a regularization prior $R(k)$ is routinely incorporated into the data consistency term to constrain the solution space as follows:\n$k^* = \\underset{k}{min} \\{||Ak \u2013 f||_2 + \\lambda R(k)\\}$,\t\t(2)\nwhere $k^*$ is the reconstruction and $ \\lambda$ is a prior knowledge-guided regularization parameter. A growing body of work demonstrates that DMs have recently become the predominant tools for prior extraction. In this work, we aim to optimize the representation of k-space data to enhance its suitability for model training, thereby improving the DM\u2019s convergence rate and reconstruction efficiency."}, {"title": "B. Strategy for Optimizing K-space Data", "content": "Considering the distinct underlying information between k-space data and other medical imaging data, the optimization strategies used in k-space data plays an important role in MRI reconstruction [25], [26]. Such strategies reduce the complexity of k-space data through preprocessing, thereby facilitating better integration with model structures. A growing body of work indicate that they are suitable for improving the performance of MRI reconstruction and are flexible in adapting to various models. One most typically employed procedure involves constraining k-space data using artificial masks. For example, Xie et al."}, {"title": "III. METHOD", "content": ""}, {"title": "A. Motivation", "content": "Unlike other generative models, the distinctive feature of DMs lies in their generative principle, which learns distributions through a more circuitous manner. Specifically, they first define a forward diffusion stage where the input data is gradually perturbed by adding noise, and then learns to reverse the diffusion process to retrieve the desired noise-free data from noisy data samples. Fig. 2 visualizes the two distinct stages of data transfer to noise and generation of new samples in the diffusion process. High image quality has typically been reported with diffusion-based MRI reconstruction. Nevertheless, the above pipeline does not change the dimension of the data throughout the entire diffusion process [11], [31]. It thus requires the reverse process to map a high-dimensional input to a high-dimensional output at every single step, causing heavy computation overheads. Given such a fact, acceleration techniques have recently been considered to speed up the characteristically slow sampling process in regular diffusion models.\nA novel strategy rethought the role of Gaussian noise and generalizes the diffusion process using different kinds of degradation approaches. Huang et al. [32] proposed a methodology that transitions the degradation operation from the Gaussian white noise to under-sampled k-space data, also designing strategies for starting points and data consistency conditioning to effectively guide and accelerate the reverse process. In contrast to initiate sampling based on under-sampled k-space data, Qiao et al. [33] constructed an alternative degradation operation that obtains preliminary guidance images through a separate reconstruction method. While promising, this approach involves implementation of a second reconstruction procedure. Another powerful technology is to train DMs with small step size and to rescale to large step sizes during inference. Gungor et al. [34] considered rescaling the diffusion step size via adversarial mapping over large reverse diffusion steps to accelerate image sampling, but reverse diffusion steps can potentially have suboptimal accuracy. Besides varying diffusion in image space, a"}, {"title": "B. Migrating Subspace to Diffusion Process", "content": "Diffusion Process: Suppose we are given a k-space MR dataset $k$ in the score-based diffusion model, where each datapoint is independently drawn from an underlying data distribution $p(k)$. With a sequence of positive noise scales $\\sigma_{min} = \\sigma_0 < \u2026 < \\sigma_t <\u2026\\sigma_T = \\sigma_{max}$, the forward diffusion process for $t = 0,..., T$ is defined by an Ito stochastic differential equation (SDE) which iteratively adds the Gaussian white noise and progressively maps the distribution of data $p(k)$ into the normal distribution. In a general case, it is associated with a noising process $k_t$, with $k_0$ distributed according to the data distribution and satisfying:\n$dk_t = f(k, t)dt + g(t)du_t$.\t\t(3)\nA concrete formulation of a score-based diffusion model requires a choice of forward diffusion process, specified by $f(k, t)$ and $g(t)$. Almost always, these are chosen to be isotropic of the form:\n$f(k, t) = f(t)k, g(t) = g(t)I_d$,\t\t(4)\nwhere $I_d$ is an identity matrix and d is the data dimensionality. For example, the variance exploding (VE) SDE has $f(t) = 0$ and $g(t) = \\sqrt{d[\\sigma^2(t)]/dt}$ so that it can be parameterized as:\n$dk_t = \\sqrt{d[\\sigma^2(t)]/dt} I_d du_t$,\t\t(5)\nwhere $\u03c3^2(t)$ is the variance of the perturbation kernel at time t. Accordingly, the target of score-based diffusion model is to learn how to reverse the chain and restore the data distribution $p(k)$. Under mild assumptions, for any T \u2265 0, the reverse diffusion process satisfies:\n$dk_t = [\\frac{d[\u03c3^2(t)]}{dt} \\nabla_{k_t}log p_{ot} (k_t)] dt + \\frac{d[\u03c3^2(t)]}{dt} d\\bar{u_t}$.\t\t(6)\nOnce an accurate approximation of the Stein score $\\nabla_{k_t}log p_{ot} (k_t)$ is estimated for all t, we can derive the reverse diffusion process and simulate it to sample from $p(k)$.\nSubspace Diffusion: Considering that in practical MR imaging applications, the target k-space data typically resides near the linear sub-region, such that under isotropic forward diffusion, the components of the data orthogonal to the space become Gaussian significantly compared to the general components. For the reason, we thus propose that at a certain point in time, the diffusion process is migrated to an isotropic subspace S. Specifically, the forward diffusion begins in the full space, but is projected and restricted to smaller subspace as time goes on. For any diffusion process with a form similar to the Eq. (4), we define the corresponding subspace diffusion as follows. Divide the diffusion time (0, T) into individual subintervals with m as the time point, i.e., $(t_0, ...,t_m)$, $(t_{m+1},t_r)$. Subsquently, diffusion process in subspace can be redefined as follows for each interval $t_0 < t_m < t_r$:\n$g(t) = g(t)Q_mQ_m^*$,\t\t(7)\nwhere $Q_m \u2208 C^{d\u00d7d}$ is the orthogonal matrix and its orthonormal columns vectors span the subspace S, $Q_m^*$ is the transposed matrix which can satisfy $Q_m^*Q_m = I_d$.\nMathematically, these definitions clarify that diffusion process is coupled or constrained to occur in smaller subspaces defined by $Q_m$ in the interval $(t_m, t_r)$. Turning to drift coefficient $f(k, t)$, it can be expressed as:\n$f(k, t) = f(t)k + \\sum_{t=m+1}^T \u03b4(t \u2212 t_m)(Q_mQ_m^* - I_d)k$,\t\t(8)\nwhere $\u03b4(\u00b7)$ is the Dirac delta. Eq. (8) states that at time $t_m$, k is projected onto the subspace. Fig. 3 illustrates the high-level concept of subspace diffusion, along with additional properties elaborated in Appendix A. On this basis, we migrate the diffusion process into the subspace, which decreases the dimensionality as time evolves. Learning to match a lower-dimensional score function may lead to refined training coverage and further improved performance.\nOrthogonal Decomposition: To limit the loss of the information during the dimensionality reduction process, we employ"}, {"title": "A. Score Matching in Subspace Diffusion Model", "content": "As previously discussed, the k-space data is first diffused in the full-space according to Eq. (3). To generate samples, we need to learn the function $\u2207_{k} log p_t (k)$ as usual. However, the analytical form of $\u2207_{k} log p_t (k)$ is generally intractable, and hence we learn a score model $s_{\u03b8} (k, \u03c3_t)$ parameterized by the network to estimate its values:\n$\u03b8^* = \\underset{\u03b8}{min} E_{t}\\{\u03bb_t E_{k(0)} E_{k(t)|k(0)} [||s_{\u03b8}(k(t), t) \u2212 \u2207_{k(t)} log p_t (k(t)|k(0))||]\\}$.\t\t(11)\nInstead of conducting the score matching on the full-space, DWT is used to reduce the dimension of k-space data and then the diffusion process is restricted to the subspace in the interval $t\u2208 (t_m, t_r)$. This means that wavelet components K as diffusion elements more accurately formulate the subspace. To learn the lower-dimensional diffusion process in subspace, we leverage the fact that the subspace components K of the k-space data diffuse under an SDE with the same f(t) and g(t) as the full-space, independent of the orthogonal components. Consider the case that we only use one proper subspace. Then the diffusion process in the subspace can be described as:\ndK = f(t)Kdt + g(t)du.\t\t(12)\nAs a result, the perturbation kernels in the subspace have the"}, {"title": "B. Iterative Sampling via Subspace Diffusion Prior", "content": "Regularization Constraint: To generate the samples, score models $s_{\u03b8} (k, t)$ and $s_{\u03b8s} (K, t)$ in the corresponding interval $(t_m, t_r)$ are used to solve the reverse diffusion. Generically, the reverse diffusion process is equipped with the Predictor-Corrector sampler [14] in our implementation for its superior performance. Predictor is the SDE solver first gives an estimate of the reconstruction results at the next time step, while the Corrector is the score-based Markov Chain Monte Carlo approach refines the marginal distribution of the estimated results. Formally, the Predictor updates the estimate results in the full-space with the trained diffusion model $s_{\u03b8} (k, t)$ from the above section as:\n$k_{t\u22121} = k_t + (\u03c3_t^2 \u2212 \u03c3_{t\u22121}^2)s_{\u03b8}(k_t, t) + \\sqrt{\u03c3_t^2 \u2212 \u03c3_{t\u22121}^2}Z$,\t\t(14)\nwhere $\u03c3_t^2$ represents a monotonically increasing function with respect to time $t\u2208 (0, t_m)$. The variable $Z\u223cN(0,1)$ follows a Gaussian distribution representing random noise. For each updated $k_{t\u22121}$, the following Corrector step is performed multiple times to refine it:\n$k_{t\u22121} = k_{t\u22121} + \u03b5_{t\u22121}s_{\u03b8} (k_{t\u22121}, t) + \\sqrt{2\u03b5_{t\u22121}}Z$,\t\t(15)\nwhere $\u03b5_{t\u22121}$ is the step size at time t - 1. Similar to the alternative sample strategy, data in the subspace can also be reconstructed by alternatively updating Predictor-Corrector samplers. Due to the reconstruction step is performed within the subspace, the original k-space data k should first undergo orthogonal decomposition with the wavelet transform to produce K. Hereafter, the Predictor step is conducted with the trained diffusion model $s_{\u03b8s} (K, t)$ from the above section as follows:\n$K_{t\u22121} = K_t + (\u03c3_t^2 \u2212 \u03c3_{t\u22121}^2)s_{\u03b8s} (K_t, t) + \\sqrt{\u03c3_t^2 \u2212 \u03c3_{t\u22121}^2}Z$.\t\t(16)\nSubsequently, the Corrector step is executed on K as follows:\n$K_{t\u22121} = K_{t\u22121} + \u03b5_{t\u22121}s_{\u03b8s} (K_{t\u22121}, t) + \\sqrt{2\u03b5_{t\u22121}}Z$.\t\t(17)\nPredictor and Corrector samplers are alternatively performed for several times between the full-space and the subspace to reach convergence.\nLatent Consistency Module: To preserve the accuracy of intricate appearance features in the reconstructed result, we further incorporate the data consistency between the reconstructed image and the measurement data within the Predictor-Corrector sampler. Considering the practical significance, the latent consistency module ensures that the intermediate solution remains in the feasible region of the data. However, since the diffused variables in the subspace undergo orthogonal decomposition"}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Experimental Setup", "content": "Datasets: In the main experiments, especially the training of the score function, were performed with the brain dataset SIAT, which are provided by Shenzhen Institute of Advanced Technology, the Chinese Academy of Sciences and informed consent was obtained from the imaging object in compliance with the IRB policy. The fully-sampled SIAT data are acquired on a 3.0T Siemens Trio Tim MRI scanner, consisting of 12-channel complex-valued MR images with 256 \u00d7 256, and combined into single-channel data by coil compression. A subset of 500 samples is selected to form the training dataset. Subsequently, the dataset is expanded to 4000 single-coil images through flip and rotation, thereby ensuring a more comprehensive training to enhance model performance.\nNoted that our Sub-DM is a one-for-all model. This means that once Sub-DM is trained, it can be reused for a diverse range of datasets with different sequences during the testing phase. One dataset was the T1-GE Brain, including 8-channel complex-valued obtained by 3.0T GE. As well as the T1-weighted Brain, obtained with an 8-channel joint-only coil using 1.5T GE. Additionally, the fastMRI+ dataset [39] also is used to test"}, {"title": "B. Reconstruction Experiments", "content": "Comparisons with State-of-the-arts: To evaluate the effectiveness of our proposed method, we compare its performance with other methods, including traditional methods SAKE [40] and ESPIRIT [41], as well as deep learning methods EBMRec [42] and HKGM [23] on two different datasets. For under-sampled MRI reconstruction under \u00d78, \u00d710, and \u00d712 acceleration factors across Poisson, Radial, Random, and Uniform sampling patterns, the PSNR, SSIM, and MSE values from the brain dataset are summarized in Table I. The quantitative results demonstrate that Sub-DM outperforms other reconstruction methods. Obviously, training within a subspace framework effectively learns profound information, which in turn facilitates the diffusion of effective features and enhances the robustness of the model. Even under higher acceleration factors, Sub-DM is capable of reconstructing more realistic details.\nIt is evident that the visual effects of Sub-DM are consistent with the quantitative results, compared to other methods under same acceleration factors in Fig. 4. Specifically, SAKE and ESPIRIT reconstructions exhibit noise, aliasing, and blurriness, resulting in degraded image quality and loss of fine structures. In contrast, EBMRec and HKGM, as the deep learning techniques, demonstrate substantial improvements in recovering the prominent structures and edges. Nevertheless, upon close examination, it becomes apparent that EBMRec inaccurately reconstruct structures, failing to capture the fine details, even suffers from heavy noise and artifacts under high acceleration factors. While the HKGM approach eliminates some artifacts, it results in a loss of high-frequency details, obtained unsatisfactory effects for noise suppression and structural detail preservation under high acceleration factors. Remarkably, Sub-DM achieves visual results with the most texture details and the least"}, {"title": "Necessity of Orthogonal Decomposition", "content": "Experiments are also conducted using WKGM [30] to compare different k-space optimization methods. As evident from the quantitative results in Fig. 9, Sub-DM consistently outperforms WKGM across all cases. Furthermore, as the acceleration factor is increased, WKGM exhibits a corresponding decrease in reconstruction performance, which is consistent with the observations in Fig. 10. Concretely, some noise still remains in the results of WKGM, hindering the full preservation of structural details. In contrast, Sub-DM can easily identify over-smoothing and distortion, while reconstructing accurate texture details with less noise. Such results show that although the k-space weighting technique effectively optimizes the network reconstruction, it does not capture the difference between high and low frequencies. While Sub-DM decomposes the k-space into low-dimensional orthogonal components with DWT, effectively learns the distinctive characteristics of both high and low-frequency priors, reducing the training computational cost and achieving high-quality reconstructions."}, {"title": "C. Convergence Analysis and Computational Cost", "content": "Convergence Analysis: In this subsection, comparison with DM-based MRI reconstruction methods on fastMRI+ dataset was conducted. The convergence plot in Fig. 1 highlights the performance of Sub-DM in comparison to other MRI reconstruction methods, demonstrating its clear superiority. Sub-DM not only achieves the highest PSNR, but also has significantly fewer iteration steps, around 50, surpassing other methods in both speed and image quality. In contrast, the other models, including HKGM, WKGM, and HFS-SDE, require more iterations to reach lower PSNR levels. CSGM-MRI and Score-MRI are the slowest to converge, needing over 1500 and 2000 iterations, respectively, and still fall short in terms of PSNR. The specific computational cost is shown in Table II. Consequently, it can be concluded that migrating the diffusion process to the subspace effectively promotes the capture of high-frequency information and accelerates convergence. Moreover, the correlation between different orthogonal components increases the diversity of the prior distribution, facilitating the learning of a"}, {"title": "Practical Considerations on Iteration Time", "content": "Given the evident constraint of image reconstruction for using diffusion models is the sampling time, we further explored the diffusion time in full-space and subspace method under practical experimental conditions. According to the results in Table II, it can be seen that the algorithm Sub-DM, where the diffusion process is migrated in the subspace, rapidly achieves convergence with relatively low time consumption. Meanwhile, limited by the inherent characteristics of the diffusion model in the entire space, score-MRI and WKGM forward process occurs in the entire environment space of the data distribution, and its high dimensionality further increases the computational cost and increases sampling time. The integration of subspace and orthogonal decomposition strategies undoubtedly reduces runtime, and maximizing their synergy further balances reconstruction performance with iteration time."}, {"title": "V. DISCUSSION", "content": "The previous section has been proved that the strategy of diffusing k-space data in the subspace via orthogonal decomposition can significantly enhance both the reconstruction quality and convergence speed, while also reducing the time required for sampling without compromising the reconstruction quality. However, certain aspects of our proposed model still require further discussion and improvement.\nOut-of-Distribution Performance: Assessing the practical generalizability of deep learning techniques is of considerable importance in MRI reconstruction. This focus can lead to degraded performance when training and testing data distributions diverge and impedes efficient feature extraction for out-of-distribution samples. However, the subspace approach that promotes the diffusion of informative features demonstrates resilience and flexibility across diverse reconstruction challenges, independent of superficial data patterns. To validate this hypothesis, the experimental design was trained on the SIAT brain dataset, followed by reconstruction on the fastMRI knee dataset. The results, as presented in Fig. 11, demonstrate that the proposed method has good generalization capabilities, accurately"}, {"title": "Low-rank Optimization", "content": "It is also significant to analyze the influence of the optimization module shown in Fig. 3 on the reconstruction results. Low-rank matrix completion strategy leverages the redundancy within image data to mitigate noise and augment image quality, proving particularly effective in scenarios where data is incomplete or corrupted. Therefore, we performed a quantitative evaluation of the low-rank optimization module, with the corresponding metrics results presented in Table III. Obviously, the metrics listed in Table III illustrate the contribution of this optimized strategy to overall performance enhancement, and the impact of the optimization components on the reconstruction process is found to be constrained. At the same time, even in the absence of the low-rank module, Sub-DM, with its robust generative capabilities, demonstrates excellent performance under high acceleration factors. Hence, combining the two can complement each other, leading to an overall improvement in reconstruction performance."}, {"title": "VI. CONCLUSION", "content": "In summary, the primary focus of the current study was a subspace diffusion model for MRI reconstruction that combined orthogonal decomposition to simplify complex distributions, aiming to enhance reconstruction accuracy while increasing reconstruction rate. Specifically, we migrated the diffusion process to a subspace and decomposes the k-space data into multiple orthogonal subcomponents. This method facilitated the learning of effective information within complex distributions, addressing the inherent high-dimensional extrapolation challenges and reducing the computational cost associated with diffusion models. Extensive theoretical analysis and rigorous experiments have shown that Sub-DM is not only competitive in in-distribution reconstruction tasks, but also achieves better reconstruction in OOD. Therefore, it remains important future work to explore the potential benefits of adopting subspace operations in MRI reconstruction."}]}