{"title": "Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing", "authors": ["Bingliang Zhang", "Chenlin Meng", "Wenda Chu", "Anima Anandkumar", "Julius Berner", "Yang Song"], "abstract": "Diffusion models have recently achieved success in solving Bayesian inverse problems with learned data priors. Current methods build on top of the diffusion sampling process, where each denoising step makes small modifications to samples from the previous step. However, this process struggles to correct errors from earlier sampling steps, leading to worse performance in complicated nonlinear inverse problems, such as phase retrieval. To address this challenge, we propose a new method called Decoupled Annealing Posterior Sampling (DAPS) that relies on a novel noise annealing process. Specifically, we decouple consecutive steps in a diffusion sampling trajectory, allowing them to vary considerably from one another while ensuring their time-marginals anneal to the true posterior as we reduce noise levels. This approach enables the exploration of a larger solution space, improving the success rate for accurate reconstructions. We demonstrate that DAPS significantly improves sample quality and stability across multiple image restoration tasks, particularly in complicated nonlinear inverse problems. For example, we achieve a PSNR of 30.72dB on the FFHQ 256 dataset for phase retrieval, which is an improvement of 9.12dB compared to existing methods. Our code is available at the GitHub repository DAPS.", "sections": [{"title": "1 Introduction", "content": "Inverse problems are ubiquitous in science and engineering, with applications ranging from image restoration [1-6], medical imaging [7-12] to astrophotography [13-16]. Solving an inverse problem involves finding the underlying signal x0 from its partial, noisy measurement y. Since the measurement process is typically noisy and many-to-one, inverse problems do not have a unique solution; instead, multiple solutions may exist that are consistent with the observed measurement. In the Bayesian inverse problem framework, the solution space is characterized by the posterior distribution p(xo | y)\u221dp(y | xo)p(x0), where p(y | xo) represents the noisy measurement process, and p(x0) is the prior distribution. In this work, we aim to solve Bayesian inverse problems where the measurement process p(y | x0) is known, and the prior distribution p(x0) is captured by a deep generative model trained on a corresponding dataset.\nAs score-based diffusion models [1, 17-21] have risen to dominance in modeling high-dimensional data distributions like images, audio, and video, they have become the leading method for estimating the prior distribution p(x0) in Bayesian inverse problems. A diffusion model generates a sample x0 by smoothly removing noise from an unstructured initial noise sample xT through solving stochastic differential equations (SDEs). In particular, each step of the sampling process recursively converts a noisy sample xt+\u2206t, where \u2206t > 0 denotes the step size, to a slightly less noisy sample xt until t = 0. This iterative structure in the diffusion sampling process can be leveraged to facilitate Bayesian"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Diffusion Models", "content": "Diffusion models [1, 17-20] generate data by reversing a predefined noising process. Let the data distribution be p(x0). We can define a series of noisy data distributions p(x; \u03c3) by adding Gaussian noise with a standard deviation of \u03c3 to the data. These form the time-marginals of a stochastic differential equation (SDE) [20], given by\n$$dxt = \\sqrt{2\\sigma \\dot{\\sigma}_t}dw_t,$$\n(1)\nwhere \u03c3t is the predefined noise schedule with \u03c30 = 0 and \u03c3T = \u03c3max, \u03c3t is the time derivative of \u03c3t, and wt is a standard Wiener process. We use xt interchangeably with x\u03c3t. For a sufficiently large \u03c3max, the distribution p(x; \u03c3max) converges to pure Gaussian noise N (0, \u03c32maxI).\nTo sample from data distribution p(x0), we first draw an initial sample from N(0, \u03c32maxI), then solve the reverse SDE\n$$dxt = -2\\sigma \\dot{\\sigma}_t\\nabla_{x_t} \\log p(x_t; \\sigma_t)dt + \\sqrt{2\\sigma \\dot{\\sigma}_t}dw_t.$$\n(2)\nwhere \u2207xt log p(xt; \u03c3t) is the time-dependent score function [1, 17]. Here \u2207xt log p(xt; \u03c3t) is unknown, but can be approximated by training a diffusion model s\u03b8(xt, \u03c3t) such that s\u03b8(xt, \u03c3t) \u2248 \u2207xt log p(xt; \u03c3t)."}, {"title": "2.2 Bayesian Inverse Problems with Diffusion Priors", "content": "Inverse problems aim to recover data from partial, potentially noisy measurements. Formally, solving an inverse problem involves finding the inversion to a forward model that describes the measurement process. In general, a forward model takes the form of\ny = A(x0) + n,\n(3)\nwhere A is the measurement function, x0 represents the original data, y is the observed measurement, and n symbolizes the noise in the measurement process, often modeled as n \u223c N(0, \u03b2I). In a Bayesian framework, x0 comes from the posterior distribution p(xo | y) \u221d p(xo)p(y | xo). Here p(x0) is a prior distribution that can be estimated from a given dataset, and p(y | xo) = N(A(x0), \u03b2I) models the noisy measurement process.\nWhen the prior p(x0) is modeled by a pre-trained diffusion model, we can modify Eq. (2) to approximately sample from the posterior distribution following Bayes' rule, i.e.,\n$$dxt = -2\\sigma_t\\dot{\\sigma}_t (\\nabla_{x_t} \\log p(x_t; \\sigma_t) + \\nabla_{x_t} \\log p(y | x_t))dt + \\sqrt{2\\sigma \\dot{\\sigma}_t}dw_t.$$\n(4)\nHere, the noisy likelihood \u2207xt log p(y | xt) is generally intractable. Multiple methods have been proposed to estimate the noisy likelihood [6, 8, 22, 23, 29\u201332]. One predominant approach is the DPS algorithm [3], which estimates p(y | xt) \u2248 p(y | x0 = E[x0 | xt]). Another line of work [2, 31, 33] solves linear inverse problems by running the reverse diffusion process in the spectral domain via singular value decomposition (SVD). Other approaches bypass direct computation of this likelihood by interleaving optimization [25, 34-37] or projection [2, 7, 9, 31] steps with normal diffusion sampling steps.\nDespite promising empirical success, we find that this line of approaches faces challenges in solving more difficult inverse problems when the forward model is highly nonlinear. Accurately solving the reverse SDE in Eq. (4) requires the solver to take a very small step size \u2206t > 0, causing xt and xt+\u2206t to be very close to each other. Consequently, xt can only correct minor errors in xt+\u2206t, but oftentimes fails to address larger, global errors that require substantial changes to xt+\u2206t. One such failure case is given in Fig. 4."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Posterior Sampling with Decoupled Noise Annealing", "content": "Instead of solving the reverse-time SDE in Eq. (4), we propose a new noise annealing process that reduces the dependency between samples at consecutive time steps, as illustrated in Figs. 2, 3a and 3b. Unlike previous methods, we ensure xt and xt+\u2206t are conditionally independent given x0. To generate sample x0|y \u223c p(xo | xt+\u2206t, y), and (2) sampling xt \u223c N(x0|y, \u03c32t1I). We repeat this process, gradually reducing noise until x0 is sampled. We call this process decoupled noise annealing, which is justified by the proposition below.\nProposition 1. Suppose xt1 is sampled from the time-marginal p(xt1 | y), then\nxt2 \u223c Exo\u223cp(xo|xt1,y) [N(x0, \u03c32t2I)]\n(5)"}, {"title": "3.2 Discussion and Connection with Existing Methods", "content": ""}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "We evaluate our method using both pixel-space and latent diffusion models. For pixel-based diffusion experiments, we leverage the pre-trained diffusion models trained by [3] on the FFHQ dataset and the pre-trained model from [39] on the ImageNet dataset. For latent diffusion models, we use the same pre-trained models as [25]: the unconditional LDM-VQ4 trained on FFHQ and ImageNet by [40]. The autoencoder's downsample factor is 4. We use the same time step discretization and noise schedule as EDM [20].\nAs mentioned in Section 3, we implement a few-step Euler ODE solver to compute x0(xt), while maintaining the same number of neural function evaluations (NFE) for different noise levels. In our experiments, we use DAPS-1K for all linear tasks and DAPS-4K for all nonlinear tasks. DAPS-1K uses 4 ODE solver NFE and 250 annealing scheduling steps, while DAPS-4K uses 10 ODE solver NFE and 400 annealing scheduling steps. Ablation studies on their effects are shown in Section 4.3. The corresponding latent version, LatentDAPS, follows the same settings but performs sampling in the latent space of VAEs. We use 100 Langevin steps per denoising iteration and tune learning rates separately for each task. Further details on model configurations, samplers, and other hyperparameters are provided in Appendix D, along with more discussion on sampling efficiency in Appendix C.1.\nDatasets and metrics. Adopting the previous convention, we test our method on two image datasets, FFHQ 256 \u00d7 256 [41] and ImageNet 256 \u00d7 256 [42]. To evaluate our method, we use 100 images from the validation set for both FFHQ and ImageNet. We include Learned Perceptual Image Patch Similarity (LPIPS) score [43] and peak signal-to-noise-ratio (PSNR) as our main evaluation metrics. For both our method and baselines, we use the versions implemented in piq [44] with all images normalized to the range [0, 1]. The replace-pooling option is enabled for LPIPS evaluation.\nInverse problems. We evaluate our method with a series of linear and nonlinear tasks. For linear inverse problems, we consider (1) super-resolution, (2) Gaussian deblurring, (3) motion deblurring, (4) inpainting (with a box mask), and (5) inpainting (with a 70% random mask). For Gaussian and motion deblurring, kernels of size 61\u00d761 with standard deviations of 3.0 and 0.5, respectively, are used. In the super-resolution task, a bicubic resizer downscales images by a factor of 4. The box inpainting task uses a random box of size 128\u00d7128 to mask the original images, while random"}, {"title": "4.3 Ablation Studies", "content": "Effectiveness of the number of function evaluations. To better understand how the number of function evaluations (NFE) of the diffusion model influences the performance, we evaluate the performance of DAPS with different configurations. Recall that we use an ODE sampler in each inner loop to compute x0(xt), the total NFE for DAPS is the number of inner ODE steps times the number of noise annealing steps. We evaluate DAPS using NFE ranging from 50 to 4k, with configurations as specified in Appendix C.1. As indicated by Fig. 7, DAPS achieves relatively decent performance even with small NFE.\nMore Discussion on Phase Retrieval. Compared to the baselines, DAPS exhibits significantly better sample quality and stability in the phase retrieval task. Unlike other selected tasks, phase retrieval is more ill-posed, meaning that images with the same measurements can appear quite different perceptually. A demonstration of this is shown in Appendix F.1. To understand how this ill-posedness affects performance, we present results on 20 images from FFHQ"}, {"title": "5 Conclusion", "content": "In summary, we propose Decoupled Annealing Posterior Sampling (DAPS) for solving inverse problems, particularly those with complex nonlinear measurement processes such as phase retrieval. Our method decouples consecutive sample points in a diffusion sampling trajectory, allowing them to vary considerably, thereby enabling DAPS to explore a larger solution space. Empirically, we demonstrate that DAPS generates samples with better visual quality and stability compared to existing methods when solving a wide range of challenging inverse problems.\nHowever, DAPS has its limitation and could be further improved in future work. For example, our method approximates p(xo | xt) with a Gaussian distribution. This inaccurate approximation might lead to irrecoverable error in posterior sampling. Moreover, our method calls forward models multiple times while running Langevin dynamics, which increases the computation overhead when the forward"}, {"title": "A Sampling with Latent Diffusion Models", "content": "Latent diffusion models (LDMs) [40] operate the denoising process not directly on the pixel space, but in a low-dimensional latent space. LDMs have been known for their superior performance and computational efficiency in high-dimensional data synthesis. In this section, we show that our method can be naturally extended to sampling with latent diffusion models.\nLet E : Rn \u2192 Rk and D : Rk \u2192 Rn be a pair of encoder and decoder. Let z0 = E(x0) where x0 \u223c p(x0), and p(z; \u03c3) be the noisy distribution of latent vector z by adding Gaussian noises of variance \u03c32 to the latent code of clean data. We have the following Proposition according to the factor graph in Fig. 3b.\nProposition 2. Suppose zt1 is sampled from the measurement conditioned time-marginal p(zt1 | y), then\n$$zt2 \\sim \\mathbb{E}_{x_0\\sim p(x_0|z_{t_1},y)} [N(E(x_0), \\sigma_{t_2}^2I)]$$\n(10)\nsatisfies the measurement conditioned time-marginal p(zt2 | y). Moreover,\n$$zt2\\sim \\mathbb{E}_{z_0\\sim p(z_0|z_{t_1},y)} [N(z_0, \\sigma_{t_2}^2I)].$$\n(11)\nalso satisfies the measurement conditioned time-marginal p(zt2 | y).\nRemark. We can efficiently sample from p(xo | zt1, y) using similar strategies as in Section 3, i.e.,\nx(j+1)\n$$x_0^{(j+1)} = x_0^{(j)} + \\eta \\cdot (\\nabla_{x_0^{(j)}} \\log p(x_0^{(j)} | z_{t_1}) + \\nabla_{x_0^{(j)}} \\log p(y | x_0^{(j)})) + \\sqrt{2\\eta}\\epsilon_j.$$\n(12)\nWe further approximate p(x0) p(x0 | zt1) by N(x0; D(z0 (zt1)), r2I), where z0(zt1) is computed by solving the (unconditional) probability flow ODE with a latent diffusion model s\u03b8 starting at zt1. The Langevin dynamics can then be rewritten as\nx(j+1)\n$$x_0^{(j+1)} = x_0^{(j)} - \\eta \\cdot (\\nabla_{x_0^{(j)}} \\frac{||x_0^{(j)} - D(z_0 (z_{t_1}))||^2}{2r^2}+ \\frac{||A(x_0^{(j)}) - y||^2}{2\\beta}) + \\sqrt{2\\eta}\\epsilon_j.$$\n(13)\nOn the other hand, we can also decompose p(zo | zt1, y) \u2248 p(zo | zt1)p(y | zo) and run Langevin dynamics directly on the latent space,\nz(j+1)\n$$z_0^{(j+1)} = z_0^{(j)} + \\eta \\cdot (\\nabla_{z_0^{(j)}} (\\log p(z_0^{(j)} | z_{t_1}) + \\nabla_{z_0^{(j)}} \\log p(y | z_0^{(j)})) + \\sqrt{2\\eta}\\epsilon_j.$$\n(14)\nAssuming p(zo | zt1) by derive another Langevin MCMC updating rule in the latent space,\nz(j+1)\n$$z_0^{(j+1)} = z_0^{(j)} - \\eta \\cdot (\\nabla_{z_0^{(j)}} (\\frac{||z_0^{(j)} - z_0 (z_{t_1})||^2}{2r^2}+ \\frac{||A(D(z_0^{(j)})) - y||^2}{2\\beta}) + \\sqrt{2\\eta}\\epsilon_j.$$\n(15)\nBoth approaches are applicable for our posterior sampling algorithm. We summarize DAPS with latent diffusion models in Algorithm 2."}, {"title": "B Proof for Propositions", "content": "Proposition 3 (Restated). Suppose xt1 is sampled from the measurement conditioned time-marginal p(xt1 | y), then\n$$xt2 \\sim \\mathbb{E}_{x_0\\sim p(x_0|x_{t_1},y)}[N(x_0, \\sigma_{t_2}^2I)]$$\n(18)\nsatisfies the measurement conditioned time-marginal p(xt2 | y).\nProof. We first factorize the measurement conditioned time-marginal p(xt2 | y) by\n$$p(x_{t_2} | y) = \\int\\int p(x_{t_2}, x_0, x_{t_1} | y)dx_0dx_{t_1}$$\n(19)\n$$= \\int\\int p(x_{t_1} | y)p(x_0 | x_{t_1}, y)p(x_{t_2} | x_0, x_{t_1}, y)dx_0dx_{t_1}.$$\n(20)\nRecall the probabilistic graphical model in Fig. 3a. xt2 is independent of xt1 and y given x0. Therefore,\n$$p(x_{t_2} | x_0, x_{t_1}, y) = p(x_{t_2} | x_0).$$\n(21)\nAs a result,\n$$p(x_{t_2} | y) = \\int\\int p(x_{t_1} | y)p(x_0 | x_{t_1}, y)p(x_{t_2} | x_0)dx_0dx_{t_1}$$\n(22)\n$$= \\mathbb{E}_{x_{t_1}\\sim p(x_{t_1}|y)} \\mathbb{E}_{x_0\\sim p(x_0|x_{t_1},y)} p(x_{t_2} | x_0)$$\n(23)\n$$= \\mathbb{E}_{x_0\\sim p(x_0|x_{t_1},y)}N(x_{t_2}; x_0, \\sigma_{t_2}^2I),$$\n(24)\ngiven xt1 is drawn from the measurement conditioned time-marginal p(xt1 | y).\nProposition 4 (Restated). Suppose zt1 is sampled from the measurement conditioned time-marginal p(zt1 | y), then\n$$zt2\\sim Ezo\\sim p(zo/Zt1,y)N(E(x0), \u03c3\u03b5 I)$$\n(25)"}, {"title": "C Discussions", "content": ""}, {"title": "C.1 Sampling Efficiency", "content": "The sampling efficiency is a crucial aspect of inverse problem solvers. The time cost of diffusion model-based methods is highly dependent on the number of neural function evaluations (NFE). Here in Table 6 we show the NFE of the default setting of some pixel space baseline methods and DAPS with different configurations. In Fig. 7, we show the quantitative evaluation of DAPS with different NFE. As we can see, DAPS can achieve relatively much better performance than baselines with small NFE."}, {"title": "C.2 Limitations and Future Extension", "content": "Though DAPS achieves significantly better performance on inverse problems like phase retrieval, there are still some limitations.\nFirst, we only adopt a very naive implementation of the latent diffusion model with DAPS, referred to as LatentDAPS. However, some recent techniques [25, 27] have been proposed to improve the performance of posterior sampling with latent diffusion models. Specifically, one main challenge is that x0|y obtained by Langevin dynamics in pixel space might not lie in the manifold of clean images. This could further lead to a sub-optimal performance for autoencoders in diffusion models since they are only trained with clean data manifold.\nFurthermore, we only implement DAPS with a decreasing annealing scheduler, but the DAPS framework can support any scheduler function \u03c3t as long as \u03c3\\dot\u03c3t = 0. A non-monotonic scheduler has the potential of providing DAPS with more power to explore the solution space.\nFinally, we utilize fixed NFE for the ODE solver. However, one could adjust it automatically. For example, less ODE solver NFE for smaller t in later sampling steps. We would leave the discussions above as possible future extensions."}, {"title": "C.3 Broader Impacts", "content": "We anticipate that DAPS can offer a new paradigm for addressing challenging real-world inverse problems using diffusion models. DAPS tackles these problems by employing a diffusion model as a general denoiser, which learns to model a powerful prior data distribution. This approach could significantly enrich the array of methods available to the inverse problem-solving community. However, it is important to note that DAPS might generate biased samples if the diffusion model is trained on biased data. Therefore, caution should be exercised when using DAPS in bias-sensitive scenarios."}, {"title": "D Experimental Details", "content": ""}, {"title": "D.1 Inverse Problem Setup", "content": "Most inverse problems are implemented in the same way as introduced in [3]. However, for inpainting with random pixel masks, motion deblurring, and nonlinear deblurring, we fix a certain realization for fair comparison by using the same random seeds for mask generation and blurring kernels. Moreover, for phase retrieval, we adopt a slightly different version as follows:\n$$y \\sim N(|FP(0.5 \\times x_0 + 0.5)|, \\beta I),$$\n(37)\nwhich normalize the data to lies in range [0, 1] first. Here F and P are discrete Fourier transformation matrices and oversampling matrices with ratio k/n. Same as [3], we use an oversampling factor k = 2 and n = 8. We normalize input x0 by shifting its data range from [-1, 1] to [0, 1] to better fit practical settings, where the measured signals are usually non-negative.\nThe measurement for high dynamic range reconstruction is defined as\ny \u223c N(clip(\u03b1x0, \u22121, 1), \u03b2I),\n(38)\nwhere the scale \u03b1 controls the distortion strength. We set \u03b1 = 2 in our experiments."}, {"title": "D.2 DAPS Implementation Details", "content": "Euler ODE Solver For any given increasing and differentiable noisy scheduler \u03c3t and any initial data distribution p(x0), we consider the forward diffusion SDE dxt = \u221a2\u03c3\\dot\u03c3t dwt, where \u03c3t denotes the time derivative of \u03c3t and dwt represents the standard Wiener process. This SDE induces a probability path of the marginal distribution xt, denoted as p(xt; \u03c3t). As demonstrated in [1, 20], the probability flow ODE for the above process is given by:\n$$dxt = -\\sigma \\dot{\\sigma}_t \\nabla_{x_t} \\log p(x_t; \\sigma_t) dt.$$\n(39)\nBy employing the appropriate preconditioning introduced in [41], we can transform the pre-trained diffusion model with parameter \u03b8 to approximate the score function of the above probability path: s\u03b8(x, t) \u2248 \u2207x log p(xt; \u03c3t). In DAPS, we compute x0(xt) by solving the ODE given xt and time t as initial values.\nNumerically, we use scheduler \u03c3t = t and implement an Euler solver [20], which evaluates xt0+ (t - tmin)()p. We use \u03c1 = 7 and tmin = 0.02 throughout all experiments.\nAnnealing Scheduler To sample from the posterior distribution p(xo | y), DAPS adopts a noise annealing process to sample xt from measurement conditioned time-marginals p(xt | y), where xt is defined by noisy perturbation of x0: xt = x0 + \u03c3tet, \u03b5 \u223c N(0, I), where \u03c3t is the annealing scheduler. In practice, we start from time T, assuming p(xT | y) \u2248 N(0, \u03c32maxI), with \u03c3max = \u03c3T. For simplicity, we adopt \u03c3t = t and the same polynomial interpolation in Eq. (40) between \u03c30 and \u03c3T for total NA steps.\nDAPS with Latent Diffusion Model As shown in Appendix A, the Langevin dynamics can be performed both in pixel space in Eq. (25) and in latent space Eq. (26) by adopting different assumptions. However, the computation cost of latent space Langevin dynamics is much more costly than pixel space one. Thus we perform pixel space Langevin dynamics in early annealing time steps and perform latent space Langevin dynamics later. We use a hyperparamter R to decide the proportion of total annealing steps using latent pixel space Langevin dynamics.\nHyperparameters Overview The hyperparameters of DAPS can be categorized into the following three categories."}, {"title": "E Experiments on Synthetic Data Distributions", "content": "Fig. 4 shows the sampling trajectories and predicted posterior distribution of DPS and DAPS on a synthetic data distribution. Specifically, we create a 2D Gaussian mixture as the prior distribution, i.e., p(xo) = 12(N(xo; c1, \u03a31) + N(xo; c2, \u03a32)). Let c1 = (-0.3, -0.4) and c2 = (0.6, 0.5), \u03a31 = \u03a32 = diag(0.01, 0.04). We draw 1000 samples from this prior distribution to create a small dataset, from which we can compute a closed-form empirical Stein score function at any noise level \u03c3.\nMoreover, we consider the simplest measurement function that contains two modes, i.e., y = 12 (exp (\u2212||\n||x||2\n))+ n, where n ~ N(0, \u03b2I) with \u03b2y = 0.3. Let y = 0, so that the likelihood p(y | xo) has two modes at (0.5,0.5) and (0,0). Since the prior distribution is large only at (0.5,0.5), the posterior distribution is single-mode, as illustrated in Fig. 8.\nWe run both DPS and DAPS for 200 steps and 100 independent samples on this synthetic dataset. However, as shown in Fig. 8, both SDE and ODE versions of DPS converge to two different modes. This is because DPS suffers from large errors in estimating likelihood p(xt | y), especially in the early stages. These errors can hardly be corrected and are propagated along the SDE/ODE trajectory. DAPS, on the other hand, samples from a time-marginal distribution at each time step, and is able to recover the posterior distribution more accurately.\nWe further investigate the performance of pos-terior estimation by computing the Wasserstein"}, {"title": "F Additional Results", "content": ""}, {"title": "F.1 More Ablation Study", "content": "Effectiveness of ODE number of function evaluation. Recall that we use an ODE sampler to compute x0(xt), the estimated mean of the approximated distribution p(xo | xt). We use the same number of function evaluations in our ODE sampler throughout the entire algorithm. To test how the number of function evaluations (NFE) in the ODE sampler influences the performance, we try different NFE on two linear tasks and one nonlinear task. As shown in Fig. 10, increasing NFE in the ODE sampler consistently improves the overall image perceptual quality. In particular, when NFE is 1, the ODE sampler is equivalent to computing E[x0 | xt] via Tweedie's formula.\nEffectiveness of annealing noise scheduling step. To better understand how the scheduling of sigma influences performance, we also evaluate the effects of sampling with varying noise scheduling steps. A larger number of scheduling steps implies a denser discretization grid between \u03c3max and \u03c3min. The quantitative results are shown in Fig. 11. The performance of DAPS on linear tasks slightly increases as the number of annealing noise scheduling steps increases, while its performance on nonlinear tasks (e.g., phase retrieval) increases dramatically with the number of scheduling steps. However, DAPS achieves a near-optimal sample quality, when the number of noise scheduling steps is larger than 200.\nDifferent Measurement Noisy Level When subjected to varying levels of measurement noise, the quality of solutions to inverse problems can differ significantly. To evaluate the performance of DAPS under different noise conditions, we present the results in Fig. 12. DAPS is robust to small noise levels (\u03c3 < 0.05) and degrades almost linearly as \u03c3 continues to increase."}, {"title": "F.2 More Discussion on Phase Retrieval", "content": "Phase retrieval is inherently a harder problem than other tasks considered in this paper. There are multiple disjoint modes with exactly the same measurement for phase retrieval. This is completely different from other tasks such as super-resolution and deblurring, for which the subset of images with low measurement error is a continuous set. We show in Fig. 13 eight images with disparate perceptual features but with exactly the same measurement in phase retrieval."}, {"title": "F.3 More Analysis on Sampling Trajectory", "content": "Here we show a longer trajectory of phase retrieval in Figs. 14 to 16. The x0(xt) evolves from unconditional samples from model to the posterior samples while x0|y evolves from a noisy conditioned samples to the posterior samples. These two trajectories converge to the same sample as noise annealing down."}, {"title": "F.4 More Qualitative Samples", "content": "We show a full stack of phase retrieval samples in 4 runs without manual post-selection in Figs. 17 and 18. More samples for other tasks are shown in Figs. 19 and 20. The more diverse samples from box inpainting of size 192 \u00d7 192 and super-resolution of factor 16 are shown in Figs. 21 and 22."}]}