{"title": "NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface Reconstruction", "authors": ["Yifan Wang", "Di Huang", "Weicai Ye", "Guofeng Zhang", "Wanli Ouyang", "Tong He"], "abstract": "Signed Distance Function (SDF)-based volume rendering has demonstrated significant capabilities in surface reconstruction. Although promising, SDF-based methods often fail to capture detailed geometric structures, resulting in visible defects. By comparing SDF-based volume rendering to density-based volume rendering, we identify two main factors within the SDF-based approach that degrade surface quality: SDF-to-density representation and geometric regularization. These factors introduce challenges that hinder the optimization of the SDF field. To address these issues, we introduce NeuRodin, a novel two-stage neural surface reconstruction framework that not only achieves high-fidelity surface reconstruction but also retains the flexible optimization characteristics of density-based methods. NeuRodin incorporates innovative strategies that facilitate transformation of arbitrary topologies and reduce artifacts associated with density bias. Extensive evaluations on the Tanks and Temples and ScanNet++ datasets demonstrate the superiority of NeuRodin, showing strong reconstruction capabilities for both indoor and outdoor environments using solely posed RGB captures.", "sections": [{"title": "1 Introduction", "content": "3D surface reconstruction [34, 16, 35, 28, 8, 24, 26, 41, 39, 14, 12, 2, 40, 38, 25] is a long-standing research topic in the field of computer vision. This process involves using images to recover the underlying 3D geometry, typically represented as meshes. These reconstructed meshes find diverse applications in various domains, including video games and augmented/virtual reality systems. In this paper, we specifically address the challenge of reconstructing 3D surfaces from posed RGB images.\nInspired by the density-based representation [15] for task of novel view synthesis, recent works for neural surface reconstruction commonly introduce signed distance functions (SDF) [28, 35] to recover high-quality geometry.\nHowever, incorporating SDF to the density function is nontrivial and often fails to intricate geometric details. We illustrate this by comparing two methods for reconstructing the same scene: Instant-NGP [18], which employs density-based volume rendering, and Neuralangelo [13], which utilizes SDF-based volume rendering. Both methods use a similar multi-resolution hash table representation. As illustrated in Figure 2, Instant-NGP reconstructs surfaces with accurate localization albeit with a certain roughness, while Neuralangelo produces smoother surfaces yet encounters issues in correctly positioning portions of the surface. This disparity underscores the limitations and highlights the need for improved modeling capabilities in SDF-based surface reconstruction methods.\nWhy do SDF-based surface reconstruction methods face challenges in accurately capturing intricate geometric details, and how can these methods be improved? In this paper, we thoroughly analyze the reconstruction pipeline and identify two primary factors in current SDF-based pipelines that contribute to suboptimal surface reconstruction:\n\u2022 SDF-to-density conversion: SDF-based volume rendering requires a conversion function to relate the SDF field with the density field. Existing methods use a conversion function that assigns uniform density across the same level sets, which restricts the representation of arbitrary non-negative density values. Additionally, there is no assurance that the geometric representation within a volume rendering framework will align perfectly with the implicit surface. This misalignment often results in accurate visual renderings on incorrect surfaces, due to inherent biases.\n\u2022 Geometric regularization: Regularization constraints imposed on the implicit surface can limit topological changes during optimization. These constraints often introduce biases, complicating the convergence of the model and hindering its ability to accurately represent complex geometries.\nTo tackle these challenges, we introduce NeuRodin (Figure 1), a high-fidelity 3D surface reconstruction method that innovatively overcomes the limitations previously outlined. Firstly, we refine the SDF-to-density conversion by transitioning from a global scale parameter to a local adaptive parameter. Unlike previous methods that enforce the same densities for points with identical SDF values, our approach enhances the flexibility and effectiveness of the SDF function by allowing adaptive density values. Secondly, we implement a novel loss function designed to align the maximum probability distance with the zero-level set in volume rendering, improving the alignment of geometric representations. Additionally, We incorporates above innovations within a two-stage optimization framework to tackle the over-regularization imposed by geometric constraints. Initially, we employ a coarse optimization stage in which the SDF field operates similarly to a density field,"}, {"title": "2 Related Work", "content": "Multi-view 3D reconstruction. In traditional 3D surface reconstruction, methods based on Multi-View Stereo (MVS) have long been prevalent, serving as a foundational approach for mining sparse geometric data from multiple views and generate detailed 3D models by comparing and analyzing the disparities across multiple camera perspectives. The traditional MVS methods [24], while effective in texture-rich domains, often stumble upon the challenge of processing ambiguous observations. The point clouds produced by traditional MVS methods suffer from noise, undermining the reliability of the surface triangle meshes reconstructed from these point clouds. For learning-based MVS methods [34, 45, 33, 32], the generated point clouds are still plagued by noise, leading to consistently incomplete reconstructions.\nNeural surface reconstruction. NeRF [16, 37, 10, 17] pioneers the use of neural network to represent neural radiance fields for novel view synthesis and optimizes these scenes through differentiable volume rendering. Following NeRF, subsequent research has combined implicit surfaces with differentiable volume rendering [19, 28, 35]. These methods typically represent implicit surfaces as SDF and use the zero-level set of SDFs to describe geometry, achieving high-quality reconstruction on individual objects. Various improvements have been made based on this foundation, including the incorporation of different positional encoding to enhance representational capabilities [22, 29, 48, 49] and the introduction of additional priors to deal with surfaces that exhibit specular highlights or have low textures [44, 27]. Several studies refine the modeling of SDF-to-density conversion [47, 31] to address bias issues in density. Meanwhile, other works employ patch-match techniques to improve multi-view consistency [5, 7]. Neuralangelo [13] enhances the network's representational capability by introducing hash encoding. Additionally, it proposes numerical gradients and coarse-to-fine optimization strategies to enhance the quality of surface reconstruction."}, {"title": "3 Study on SDF-based Volume Rendering", "content": "3.1 Preliminary\nDensity-based volume rendering Density-based volume rendering methods model a 3D scene as a volume density field. Given a camera position o and view direction d, the ray emitted from o in direction d is denoted as {r(t) = o + td|t > 0}. A set of n points is sampled along this ray. The predicted density \\(\\sigma(r(t))\\) and geometry features \\(z(r(t))\\) of the point \\(r(t)\\) are obtained from a geometry network \\(@geo\\). the density is parameterized by an activation function, such as ReLU, softplus, or the exp function, prior to being output by the network.\nA color network \\(color\\) predicts the color \\(c(r(t), d)\\), taking as inputs the geometry feature \\(z(r(t))\\), and the viewing direction d. The rendered color of this ray can be calculated as:\n\\(\n\\hat{C} (r) = \\int_0^{+\u221e} T(t) \\sigma(r(t)) c(r(t)), T(t) = exp \\left(- \\int_0^t \\sigma(r(u)) du\\right).\n\\)\n3.2 Challenges in Previous SDF-Based Volume Rendering\nState-of-the-art SDF-based volume rendering techniques frequently fail to reconstruct surfaces with accuracy in scenarios where density-based methods manage to renders realistic novel views. This disparity highlights the inherent limitations of SDF-based volume rendering approaches. To further elucidate these issues, we explore the fundamental distinctions between SDF-based and density-based volume rendering. Please refer to the appendix for a more in-depth analysis.\nUnsuitable assumption for SDF-to-density conversion. SDF-based volume rendering methods typically employ a predefined function \\(\\Psi\\) and a global scale parameter \\(s\\) to convert SDF values into density, as described by Equation (3). These methods often result in uniform density values for points sharing identical SDF values. Such a global scaling mechanism restricts the representation capability of the density field derived from the SDF field. Intuitively, previous top-performing methods for novel view synthesis can generate arbitrary non-negative density values within \\((0, +\u221e)\\). In contrast, incorporating SDF representation with a global scaling factor for surface reconstruction can only result in density values within \\((0,1]\\).\nBias of the density. When applying an Eikonal constraint or any form of smooth regularization to an SDF, the geometric representation within the rendering framework must align with that of the SDF. Unfortunately, current SDF-based methods often fail to ensure this alignment, particularly at larger-scale parameters \\(s\\) as explored mathematically in [47] to analyze this issue. Recent studies [31, 47, 3] have attempted to tackle this issue, proposing designs for SDF to density conversion that aim to minimize bias. Despite these advancements, these solutions still exhibit inherent biases. Additionally, the introduction of geometric regularization often exacerbates this bias, complicating model convergence and resulting in the creation of inaccurate surfaces. A more detailed analysis of this issue is provided in Section 4.2 and Appendix B to Appendix D.\nOver-regularization of Geometry. To maintain a high-quality surface, previous methods often introduce geometric constraints, such as Eikonal loss or smoothness constraints. However, these global constraints result in excessive smoothing across all regions, both flat and intricate, leading to a loss of fine details. Moreover, in the framework of SDF-based volume rendering, the prediction of color typically necessitates being conditioned on normals following IDR [36], a characteristic that distinctly sets it apart from the density-based volume rendering approach. When optimizing the"}, {"title": "4 Method", "content": "4.1 Uniform SDF, Diverse Densities\nTo deal with the representation limitations of SDF-transformed density, instead of using a global scale \\(s\\) for the transformation from SDF to density, we have employed a strategy akin to that of [30], which utilizes a non-linear mapping to obtain the unique scale \\(s\\) associated with a given point \\(r(t)\\). More precisely, it is defined as follows:\n\\(\n(f(r(t)), s(r(t)), z(r(t))) = \\@geo(r(t)), \u03c3(r(t)) = \\Psi_{s(r(t))} (f(r(t))) .\n\\)\nWith this particular design, the density is not identical within the same SDF level set and can achieve any non-negative value through the continuous representation that maps an input coordinate to its corresponding scale.\nThis approach ensures that densities within the same SDF level set are no longer uniformly identical. Instead, they can vary, achieving any non-negative value through a continuous representation that maps input coordinates to their corresponding scales. This design greatly enhances the flexibility and accuracy of our density modeling, enabling more realistic and detailed reconstructions. More detailed analysis regarding the local scale can be found in Appendix A.\n4.2 Explicit Bias Correction\nThe issue of bias represents a critical concern frequently addressed within SDF-based volume rendering. As demonstrated in Figure 3, it is necessary to align the geometric representation under the volume rendering framework with that of the implicit surface. For the volume rendering framework, the most intuitive way to represent geometry is through the rendered distance:\n\\(\nD_{rendered} (r) = \\int_0^{+8} T(t) \u03c3(r(t)) t dt.\n\\)\nWe can also consider the position where \\(w(t)\\) is maximized - that is, the probability that the light ray arrives and collides is the greatest, or in other words, the location that contributes the most to the color - as the geometric representation within the volume rendering framework:\n\\(\nD_{prob} (r) = arg max w(t) = arg max T(t) \u03c3(r(t)).\n\\)\nWe shall refer to \\(D_{prob}(r)\\) as the maximum probability distance. For an implicit surface, the zero level set offers a direct geometric representation. In an ideal scenario, irrespective of whether convergence has been achieved, the geometric representations of volume rendering (i.e., rendered distance and maximum probability distance) and the geometric representation of the implicit surface (i.e., zero level set) should be aligned, as illustrated in Figure 3 (a). However, in the practical optimization process, conflicts such as those depicted in Figure 3 (b) may arise, leading to misalignment between the two representations.\nMultiple past methods have broached this topic, offering various solutions. For example, in TUVR [47], an unbiased model is proposed and"}, {"title": "4.3 Two-Stage Optimization to Tackle Geometry Over-Regularization", "content": "Previous methods often produce incorrect surfaces due to over-regularization of geometry as shown in Figure 2 (a). However, we have discovered that methods based on density are not constrained by changes in topology as shown in Figure 2 (b), prompting us to question whether the SDF field can be first optimized as freely as density field, then refine to a smooth surface by geometry regularization. We now propose a novel two-stage optimization approach. This approach allows the optimization process to initially mimic density-based behavior in the first stage and subsequently refines to a smooth surface in the second stage.\nFor the first stage, our objective is to tackle the over-regularization issue. An intuitive solution might be to eliminate or downweight any geometric constraints and avoid conditioning the color on the predicted normal, but this approach often results in an unnatural zero level set [9, 28, 35, 36]. We experimentally validated in the Appendix H.6.\nWe have identified a simple but effective method to preserve the natural level sets of large-scale structures while allowing the formation of complex structures to be unimpeded by geometric regularization. Instead of applying geometric regularization directly to the gradient \\(\\nabla f (r(t))\\), we elect to impose them upon an estimated gradient \\(\\hat{\\nabla} f(r(t))\\), to which we introduce uncertainty through a specific design. Specifically, the x-component of the estimated gradient is\n\\(\n\\frac{\\partial}{\\partial x} f(r(t)) = \\frac{f (r(t) + \\epsilon_x) - f (r(t) - \\epsilon_x)}{2\\epsilon}\n\\),\nwhere \\(\\epsilon_x = (\\epsilon, 0, 0)\\) and \\(\\epsilon \\sim U(0, \\epsilon_{max})\\).\nUsing this technique, we observed that estimated larger-scale normals have smaller variance, while fine details exhibit larger variance, as depicted in Figure 4. This introduces uncertainty in geometric regularization, ensuring stability for large features and flexibility for complex details. We provide further explanation in Appendix E.\nDuring the initial stage, our goal is to reconstruct the approximate, coarse structure of the 3D content. This is primarily addressed by tackling the issues of over-regularization and the bias in density estimation that we previously mentioned. We employ the stochastic-step numerical gradient estimation along with the explicit bias correction to address the initial reconstruction of the coarse"}, {"title": "5 Experiments", "content": "Experimental setup. We carry out experimental evaluations on two benchmark datasets: Tanks and Temples [11] and ScanNet++ [42]. We include several baselines for comparisons: VolSDF [35], NeuralWarp [5], COLMAP [23], NeuS [28], Geo-NeuS [7], Neuralangelo [13] and MonoSDF [44]. We extract mesh through marching cube algorithm with a resolution of 2048 applied across all scenes and report the F-score for suface evaluation. More details are provided in the supplementary materials. Please refer to Appendix H for additional experimental results.\n5.1 Tanks and Temples\nNeuRodin outperforms previous state-of-the-art methods in terms of the average F-score. Owing to our explicit bias correction technique, the barn's roof maintains its structural integrity without"}, {"title": "5.2 ScanNet++ Benchmark", "content": "Since no public results are available for the ScanNet++ dataset, we randomly selected 8 scenes to construct a benchmark. For more details and results on our ScanNet++ benchmark, please refer to the supplementary materials. Quantitative results are shown in Table 3. We surpassed the methods we compared against in most scenes and achieved comparable results to those with prior knowledge in terms of F-score. We provide more visual result on ScanNet++ dataset in the supplementary."}, {"title": "5.3 Analysis", "content": "Ablation Study. To validate the efficacy of the proposed techniques, we performed an ablation study on scene Meetingroom from Tanks and Temples dataset. As illustrated in Figure 7 (a), applying a global scale for SDF-to-density conversion results in inaccurate surfaces, primarily due to the"}, {"title": "6 Conclusion", "content": "This paper proposes NeuRodin, a two-stage framework for high-fidelity neural surface reconstruction with intricate details. It introduces key designs to tackle SDF-based rendering challenges, notably the local scale adjustment for SDF-to-density conversion, which enables any non-negative value to be achieved, facilitating accurate density derivation from SDF. Additionally, an explicit bias correction method is employed to ensure the geometry of the volume rendering scheme coherently aligns with that of the implicit surface, thereby preventing the emergence of incorrect surfaces. Finally, a two-stage optimization strategy effectively resolves the issue of over-regularization imposed by geometric constraints. Comprehensive experiments demonstrate that NeuRodin simultaneously delivers superior quality."}, {"title": "A Analysis of Local Scale in SDF-to-Density Transformation", "content": "We further illustrate the importance of the local scale in Figure A, using a simple single plane scenario for explanation. This plane has a low-texture region on the left and a rich-texture region on the right.\nWithout special constraints, the rendering weight should converge to a Dirac delta function at the surface in the richly textured region and form a scattered distribution in the weakly textured region.\nHowever, under the assumption of a global scale factor, all areas on the plane follow the same distribution which will be derived in the following sections. This means that both richly textured and weakly textured regions share the same density bias, preventing the surface from converging correctly to the richly textured surface with higher certainty.\nBy introducing the local scale factor, the most significant difference is that the distribution of rendering weights along the ray is no longer uniform. The network can adaptively converge in the richly textured regions, and the density bias in these areas is no longer affected by the low-texture regions.\nIn Adaptive shells, their focus is primarily on rendering quality and they did not evaluate surface reconstruction metrics. A straightforward application to surface reconstruction can lead to issues such as increased density bias (as w(t) is also a function of the scale factor s(t)). We tackle this by implementing special designs to ensure effectiveness, such as gradually scheduling the lower bound of the scale factor to correct relatively small biases and the explicit bias correction."}, {"title": "B Analysis of Density Bias", "content": "In this section, we analyze the density bias and discuss the main disadvantage of previous work (NeuS [28], VolSDF [35] and TUVR [47]). According to Equation (8), the rendering weight maximum point t* should satisfy\n\\(\nt^* = arg max T(t)\u03c3(r(t)).\n\\)\nThe derivation of rendering weight w(t) = T(t)\u03c3(r(t)) respected to t* should equal to zero:\n\\(\n\\frac{dw(t)}{\u2202t} \\big| _{t=t^*} = \\frac{\u2202(T(t)\u03c3(r(t)))}{\u2202t} \\big| _{t=t^*}\n\\)\n\\(\n= \\frac{\u2202T(t)}{\u2202t} \\big| _{t=t^*} \u03c3(r(t^*)) + T(t^*)  \\frac{\u2202\u03c3(r(t))}{\u2202t} \\big| _{t=t^*}\n\\)\n\\(\n= ((\\frac{\u2202T(t)}{\u2202t}) - ((\\frac{\u2202\u03c3(r(t))}{\u2202t})) exp(-\\int_0^t \u03c3(r(u))du) ) T(t^*)\n\\)\n\\(\n= 0.\n\\)\nThen, we have\n\\(\n\u03c3^2(r(t^*)) =  \\frac{\u2202\u03c3(r(t))}{\u2202t} \\big| _{t=t^*}\n\\)\nNeuS [28] and TUVR [47] seek a modeling approach for \u03c3(r(t)) such that it also satisfies the above equation at the point where the SDF value is zero f(r(t\u00b0)) = 0. NeuS only satisfies this under the"}, {"title": "D Analysis of TUVR", "content": "Previously, we mentioned that TUVR only proves t\u00ba as a local maximum for rendering weights, not a global one. Here, we present a scenario where bias exists in the modeling of TUVR. Consider a wall composed of three planes in space, with light passing through it, as shown in Figure 13 (a). In this situation, the rendering weights, as illustrated in Figure 13 (b), exhibit a bias in TUVR; although TUVR ensures a local peak at the SDF zero crossing point, the rendering weight is greater at a previous location. In this scenario, the bias in TUVR may even be greater than that in VolSDF.\nNext, we analyze the unbiasedness of TUVR under the assumption of a local scale factor. To simplify the equations, we will henceforth abbreviate all r(t) as t and all as '(t). When using a local scale factor, the density represented under TUVR modeling is:\n\\(\n\u03c3(t) =\n\\begin{cases}\n\\frac{1}{s(t)} exp (-\\frac{f(t)}{s(t)|f'(t)|}) & \\text{if } f(t) \u2265 0,  \\\\              (1-exp (\\frac{f(t)}{s(t)|f'(t)|})  & \\text{if } f(t) < 0.\n\\end{cases}\n\\)\nWhen f(t) \u2265 0, the left-hand side of Equation (15) is:\n\\(\n\u03c3'(t) =  \\frac{s'(t)}{s^2 (t)} exp ( \\frac{1}{+}  \\frac{fts'(t)}{s(t)|f'(t)s(t)} ) exp (\\frac{f(t)}{s(t)|f'(t)|})\n\\)\nWhen f(t) = 0, the above expression can be simplified to:\n\\(\n\u03c3'(t) =  \\frac{s'(t)}{s^2 (t)} -  \\frac{f'(t)}{s^2 (t)}  \\frac{s'(t)}{s(t) f'(t)|}\n\\)\nAnd we only consider the scenario where the light ray enters the plane, namely f'(t) < 0. So we have\n\\(\n\u03c3'(t) =   \\frac{-s'(t) + 1}{s^2(t)}\n\\)\nAt this point, \u03c3\u00b2(t) = \\frac{1}{s^2(t)}, so Equation (15) is only satisfied when s'(t) = 0, meaning the scale factor is a constant. Therefore, under the assumption of a local scale factor, TUVR's local unbiasedness (ensuring the SDF zero crossing point is a local maximum in rendering weights) cannot be achieved. When f(t) < 0, we can also arrive at a similar conclusion."}, {"title": "E Explanation of Stochastic Gradients", "content": "Our stochastic gradient estimation introduces some uncertainty into the true normals. For large-scale features, the Eikonal loss with varying step sizes can still be successfully minimized (since SDF near"}, {"title": "F Impact of Color Conditioning on Normal", "content": "In Figure 14, we experimentally observed that in indoor scenes, when color conditioning on normals is applied, the optimization process becomes very slow and adversely affects the optimization results (both surface and rendering quality). However, this issue is nearly absent in outdoor scenes. We believe this is primarily due to the convergence behavior of the scale factor. Indoor scenes often have many weakly textured areas, leading to larger scale factors and a greater scatter distribution. Additionally, color conditioning on normals, which is intended for geometry disentanglement as mentioned in IDR and resonable for surface points, results in many details being optimized to incorrect positions before convergence is achieved.\nHowever, our use of stochastic normals helps mitigate these erroneous surfaces. For detailed regions, the estimated normals have greater variance, which alleviates the impact of incorrect surfaces. This is also demonstrated in our ablation experiments, as shown in the Table 4. Cases D and E are as follows: Case D: Excludes the stage 1 Eikonal loss but incorporates color conditioning based on the estimated normal. Case E: Excludes the stage 1 Eikonal loss but includes color conditioning based on the analytical normal."}, {"title": "G Experimental Details", "content": "G.1 Datasets\nWe carry out experimental evaluations on two benchmark datasets: Tanks and Temples [11] and ScanNet++ [42]. The Tanks and Temples dataset is characterized by its large-scale, diverse real-world scenes, both indoors and outdoors. For our experiments, we utilize six scenes from the training subset, consistent with the scenes employed in Neuralangelo, to maintain comparability. Additionally, we extend our validation to four expansive indoor scenes from the advanced subset to further assess the robustness of our method. Turning to the ScanNet++ dataset, it is distinguished by its high-quality indoor scenes, supplemented with DSLR-quality images. From this dataset, we have selected eight scenes for our analysis.\nG.2 Baselines\nFor the Tanks and Temples dataset, our methodology is compared against several prominent methods, including: NeuralWarp [5], COLMAP [23], NeuS [28], Geo-NeuS [7], and Neuralangelo [13]. In the context of the ScanNet++ dataset, our approach is contrasted with methods lacking prior knowledge, such as VolSDF [35], NeuS [28], and Neuralangelo [13]. Additionally, we evaluate our approach against methods incorporating pretrained prior information, notably MonoSDF [44]. It should be noted that our efforts to reproduce Neuralangelo for indoor scenes were unsuccessful. Instead, we employed the implementation of Bakedangelo from [43], which serves as an enhanced version of Neuralangelo. Bakedangelo utilizes the same proposal network as our setup. We discovered that manually adjusting the global scale of SDF-to-density conversion in Bakedangelo significantly"}, {"title": "G.4 Implementation Details", "content": "NeuRodin utilizes a multi-resolution hash grid for encoding, spanning from 25 to 211 across 16 levels. Each hash entry possesses a channel size of 2 for room-level scenes such as ScanNet++, which is adjusted to 8 for larger-scale scenes like Tanks and Temples. The maximum number of hash entries for each resolution is set at 219. We incorporate per-image appearance encoding in the style of NeRF-W [15] while employing a proposal network [1] based on a compact hash grid. For the outdoor scene, we model the background using an additional network [46] with the hash grid. For the ScanNet++ datasets, we sample 512 pixels per iteration. In the case of the Tanks and Temples dataset, we sample 1024 pixels during the first stage and escalate to 8192 pixels for the second stage. We set the weights Aeik and Asmooth to be 0.01 and 0.005, respectively. For outdoor scenes, we set Abias to 0.1. For indoor scenes, we progressively increase Abias from 0.001 to 0.05 over the first 10,000 iterations through an exponential adjustment. Bakedangelo samples 8192 pixels per iteration in large indoor scenes of Tanks and Temples, aligning with the settings employed by Neuralangelo. In the context of room-level scenarios within ScanNet++, the batch size is adjusted to 1024 pixels per iteration. Our implementation of the method employs PyTorch [20] and utilizes the Adam optimizer, with a learning rate of 0.001 applied to both the hash grid and the network, alongside a weight decay set at 0.01. For the background model, we set the learning rate to 0.01. The total training steps amount to 300k, with the learning rate for the foreground model being decayed by a factor of 10 at 160k and 240k steps. For the background model, we employ an exponential schedule for the learning rate, reducing it to 0.0001. For the proposal network, the learning rate is decayed by a factor of 3 at steps 150k, 225k, and 270k. All our experiments were conducted on an A100 40G GPU. We roughly require 7 GPU hours to complete the reconstruction of an indoor scene from the ScanNet++ dataset. For large-scale scenes, the reconstruction takes approximately 18 GPU hours."}, {"title": "G.5 Implementation of the Explicit Bias Correction", "content": "In the actual implementation of explicit bias correction, when inferring the SDF at Ebias after the point r(t*) along the ray in Equation (8), we also infer the SDF at Ebias mask beyond r(t*) along the ray as a mask. If f(r(t* + \u20acbias mask)) is less than zero, we do not apply the bias loss to this particular ray. We have found that this approach effectively prevents incorrect alignments that may be caused by our approximate estimation of t*. For outdoor scenes, we simply determine whether a ray is cast towards the background by checking if there exists a negative value of SDF at any sampled point along the ray. If so, we similarly refrain from performing bias correction. We have configured Ebias mask to 0.001 for large-scale scenes, such as the Tanks and Temples dataset. For room-level scenes, such as the ScanNet++ dataset, we have set it to 0.01."}, {"title": "G.6 Implementation Details of the Two-Stage Optimization", "content": "The local scale modeling in Equation (5) can impede the model's convergence to the surface to some extent. Therefore, we manually adjust the lower bound Scoarse for the scale to prevent the ambiguity that arises from excessively small scales. Analogous to the manual adjustments made in the first phase, our objective in this stage is to facilitate convergence from volume rendering to surface rendering, thereby aligning the implicit surface with volume rendering completely. To achieve this, we exponentially increase the lower bound of the scale sfine to a substantial value. In all experiments, we set the value of Scoarse to 100 and Sfine to 3000."}, {"title": "H More Experimental Results", "content": "H.1 Experimental Results on the Scannet++ Benchmark\nFor every scene, we use high-quality DSLR camera images from all frames for our experiments. For methods that do not use prior knowledge, we downsample the images from 1752 \u00d7 1168 to 876 \u00d7 584 for training. For methods that do use prior knowledge, we do something similar to MonoSDF. We first crop the images to 1152 \u00d7 1152, then downsample them to 384 \u00d7 384 before feeding them into the Omnidata model [6] to predict geometry cues. First, we scale the pose to be centered within a bounding sphere of radius equal to 1. Subsequently, we scale the camera pose by 0.8 to ensure that all scene boundaries are contained within the bounding sphere. We apply the marching cubes algorithm to a cube with dimensions -1 to 1 at a resolution of 2048. Then, we evaluate the metrics: Accuracy (Acc), Completeness (Comp), Precision (Pred), Recall (Rec), and F-score. The final result is shown in Table 5. Here, we also present some visual results in Figure 15.\nH.2 Experimental Results on the DTU Benckmark\nAlthough our design is not specifically tailored for single-object datasets, we validated our method on the DTU Benchmark. The results are shown in the Table 7. We found that, even without special parameter tuning, our method achieves results comparable to Neuralangelo and surpasses other baseline methods.\nH.3 Experimental Results on the Tanks and Temples Advance Subset\nIn this section, we present the individual results for each scene within the advanced subset of the Tanks and Temples dataset, as shown in Table 6. Except for the Courtroom scene, our method significantly outperforms comparative approaches in the other three large-scale indoor scenes, demonstrating the effectiveness of our approach. We present a portion of the mesh for the museum data in Figure 16 and more results in Figure 17, highlighting our method's significant capability in capturing details.\nH.4 Comparison with TUVR"}, {"title": "I Limitation", "content": "Though NeuRodin has capabilities in reconstruction, it falls short in certain areas. Specifically, it struggles to faithfully reconstruct the correct surface in areas that are textureless and less observed. NeuRodin is also incapable of handling situations with strong ambiguity. Additionally, density is not guaranteed to be unbiased, hence, bias will always exist though the SDF-to-density conversion."}, {"title": "J Societal Impact", "content": "Our model achieves high-fidelity 3D reconstruction. The societal impact of this development is multifaceted. On one hand, it enables significant advancements in fields such as architecture and augmented reality, improving professional practices and potentially benefiting the public by enhancing the precision and interactivity of digital models. On the other hand, the increase in computational power demands may lead to greater energy consumption, which poses environmental considerations. Moreover, there could be privacy concerns if such technology is applied to reconstruct environments from personal data without consent. Overall, while this technology presents opportunities for progress and innovation, it also requires careful consideration of ethical and environmental implications."}]}