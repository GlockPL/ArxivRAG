{"title": "A Depression Detection Method Based on\nMulti-Modal Feature Fusion Using Cross-Attention", "authors": ["Shengjie Li", "Yinhao Xiao"], "abstract": "Depression, a prevalent and serious mental health\nissue, affects approximately 3.8% of the global population. De-\nspite the existence of effective treatments, over 75% of individuals\nin low- and middle-income countries remain untreated, partly\ndue to the challenge in accurately diagnosing depression in its\nearly stages. This paper introduces a novel method for detecting\ndepression based on multi-modal feature fusion utilizing cross-\nattention. By employing MacBERT as a pre-training model to\nextract lexical features from text and incorporating an additional\nTransformer module to refine task-specific contextual under-\nstanding, the model's adaptability to the targeted task is en-\nhanced. Diverging from previous practices of simply concatenat-\ning multimodal features, this approach leverages cross-attention\nfor feature integration, significantly improving the accuracy in\ndepression detection and enabling a more comprehensive and\nprecise analysis of user emotions and behaviors. Furthermore, a\nMulti-Modal Feature Fusion Network based on Cross-Attention\n(MFFNC) is constructed, demonstrating exceptional performance\nin the task of depression identification. The experimental results\nindicate that our method achieves an accuracy of 0.9495 on the\ntest dataset, marking a substantial improvement over existing\napproaches. Moreover, it outlines a promising methodology for\nother social media platforms and tasks involving multi-modal\nprocessing. Timely identification and intervention for individuals\nwith depression are crucial for saving lives, highlighting the\nimmense potential of technology in facilitating early intervention\nfor mental health issues.", "sections": [{"title": "I. INTRODUCTION", "content": "Depression(major depressive disorder) is a common and\nserious mental disorder that negatively affects how you feel,\nthink, act, and perceive the world [1].According to estimates\nfrom the World Health Organization (WHO), around 3.8% of\nthe global population is affected by depression, including 5%\nof adults-comprising 4% of men and 6% of women-and\n5.7% of adults aged 60 years and older. Approximately 280\nmillion people worldwide suffer from depression, with women\nhaving a roughly 50% higher chance of developing the disor-\nder compared to men. More than 10% of pregnant women and\nthose who have recently given birth globally also experience\ndepression. Each year, over 700,000 individuals die by suicide,\nmaking it the fourth leading cause of death among individuals\naged 15 to 29. Despite the availability of effective treatments\nfor mental illnesses, over 75% of individuals in low- and\nmiddle-income countries do not receive any form of treatment\n[2]. The challenge in making accurate diagnoses during the\nearly stages of depression contributes to a substantial number\nof patients being unable to access timely diagnosis and care.\nWith the widespread adoption of the internet and social\nmedia, these platforms have gradually evolved into a new\nwindow for studying mental health, particularly in identifying\nearly signs of depression [3]. The digital footprints users leave\nonline, such as posted content, comments, liking behavior,\nfrequency and nature of online interactions, serve as vital data\nsources for analyzing their psychological states. Research has\nuncovered correlations between specific behavioral patterns\nand linguistic habits on social media and depression, includ-\ning frequent posting of negative content, increased nighttime\nactivity, and reduced social engagement, all of which may be\nindicative of depressive symptoms [4].For example, a study\ncould analyze users' word choices in their texts, looking\nfor frequent occurrences of negative emotion vocabulary like\n\"loneliness,\" \"fatigue,\" or \"despair\" as indicators of a depres-\nsive tendency [3]. Moreover, the application of algorithms\nand machine learning technologies enables the processing\nand analysis of large-scale data, identifying more complex\nand subtle patterns that can facilitate earlier recognition of\nindividuals likely experiencing depression by mental health\nprofessionals.In China, Sina Weibo, with its massive young\nuser base, naturally emerges as a priceless resource for this\nkind of research. By scrutinizing the online behaviors of these\nusers, researchers can not only track trends in mental health\nbut also develop tools to facilitate early interventions.\nBuilding upon prior work, in this paper, we aim to ac-\ncomplish depression detection through user modeling with\nimproved performance over previous efforts. To this end, we\nconstruct a new deep neural network classification model, the\nMulti-Modal Feature Fusion Model Using Cross-Attention.\nThis model employs MacBERT [5] as the pre-training model\nto extract word features from text and incorporates an ad-\nditional Transformer module to further refine context under-\nstanding specific to the task at hand, thereby enhancing adapt-\nability to the targeted task. Departing from past practices, our\napproach utilizes Cross-Attention [6] for multimodal feature\nintegration instead of simply concatenating multiple features.\nExperimental proofs that our method effectively boosts the\naccuracy in detecting depression.\nOur Contributions. The major contributions are listed as\nfollows:\n\u2022 We employ the Cross-Attention mechanism for fusing\nmultimodal features, deviating from the common practice\nin prior works that typically resort to straightforward con-\ncatenation of multimodal attributes. The cross-attention\nstrategy effectively captures and integrates these comple-\nmentary pieces of information, enabling the model to con-\nduct a more comprehensive and precise analysis of user\nemotions and behaviors. By calculating attention weights\nbetween features across different modalities, it highlights\nthe most pertinent information from each modality. This\ncapability of capturing associations is instrumental in\nenhancing the model's understanding and interpretation\nof relationships between distinct multimodal features.\n\u2022 We have constructed a deep neural network classification\nnetwork, multimodal feature fusion network based on\ncross-attention(MFFNC), specifically designed for de-\npression detection, capable of handling multimodal fea-\nture inputs. This model demonstrates exceptional per-\nformance in the task of depression identification. Our\nexperiments have shown that compared to other prevalent\nclassification models, ours achieves the highest level of\naccuracy and exhibits the greatest robustness.\nPaper Organization. The remainder of this paper is or-\nganized as follows. Section II introduces the background\nknowledge and elaborates on our research problem. Section III\npresents a detailed design of our model framework. Section IV\ndemonstrates the implementation of our model. Section V\nreports the evaluation results of our model in predicting\ndepression from social media content, along with comparisons\nto recent popular models. Section V summarizes the most\nrelevant prior works. The final section, Section VII, provides\na conclusion for the entire paper."}, {"title": "II. BACKGROUND", "content": "In this section, we introduce the dataset utilized in this paper\nas well as the relevant technologies employed.\nA. Weibo-User-Depression-Detection-Dataset(WU3D)\nThe dataset employed in our research is Weibo User De-\npression Detection Dataset (hereafter referred to as WU3D),\ncompiled by Wang et al. [7]. Specifically, within the Weibo\nplatform, each user possesses a unique ID, and the WU3D\ndataset accesses the homepage of each user through web\ncrawling techniques to gather information. With a focus on\nensuring authority and high reliability, the dataset underwent\ndual scrutiny by psychologists and psychiatrists for labeling\naccuracy. The information collected for each user is illustrated\nin Fig. 1.\nB. Attention\nAttention is a mechanism, which has found widespread ap-\nplication in Natural Language Processing (NLP) and computer\nvision tasks. At its core, the idea is to enable models to\nselectively focus on the most crucial parts of the input informa-\ntion during processing, rather than treating all input elements\nequally or averaging their importance. This mechanism mimics\nthe way humans concentrate their attention on salient details\nwhen processing complex information, thereby enhancing the\nefficiency and performance of models.\nTransformer: Transformer is a groundbreaking sequence\ntransduction model, first proposed by Vaswani et al. in their\nseminal paper 'Attention Is All You Need' in 2017 [8]. It has\nutterly transformed the field of Natural Language Processing\n(NLP), reshaping the landscape dominated by Recurrent Neu-\nral Networks (RNNs) and their variants, such as Long Short-\nTerm Memory (LSTM) and Gated Recurrent Units (GRUs).\nThese models traditionally struggled with computational ineffi-\nciency when handling long sequences. Vaswani and colleagues\naddressed this by abandon the loop structure altogether, rely-\ning exclusively on self-attention mechanisms. This innovation\ndemonstrated superior performance compared to the best RNN\nmodels of the time in machine translation tasks, while also\nsignificantly accelerating training speeds.\nCross Attention: Cross Attention is a specialized form of\nattention mechanism that primarily deals with dependencies\nbetween two different sequences [9]. Unlike self-attention,\nwhich focuses on interdependencies among elements within\nthe same sequence, cross attention operates through a 'query-\nanswer' paradigm, enabling one sequence to adjust its repre-\nsentation based on the content of another sequence. In this\npaper, to uncover the relationships between word features and\nstatistical characteristics, the cross attention mechanism has\nbeen employed.\nC. Pre-trained Model\nLarge-scale pre-trained models (PTMs), benefiting from in-\ntricate pre-training objectives and substantial parameter counts,\nexcel at extracting knowledge from vast amounts of both\nlabeled and unlabeled data. They encapsulate this abundant\nknowledge within their extensive parameters, which, when\nfined-tuned for particular tasks, can greatly enhance a wide"}, {"title": "III. MULTIMODAL FEATURE FUSION NETWORK BASED ON\nCROSS-ATTENTION", "content": "This section describes the architecture of multimodal feature\nfusion network based on cross-attention(MFFNC), which com-\nprises four fundamental components: word vector extracting,\nstatistical feature extracting, feature fusion and multilayer per-\nceptron(MLP), as shown in Fig. 2. The workflow commences\nby concatenating the user's nickname, profile, and tweets into\na single, extended text sequence. This concatenated sequence\nis then fed into the pre-training model of MacBERT [5],\nyielding embedded word vectors which serve as input 1.\nConcurrently, the text features, social behavioral features, and\npicture features are encoded separately to generate input 2.\nSubsequently, both input 1 and input 2 are channeled into a\ncross-attention module to perform cross-attention mechanisms,\nthereby yielding a fused feature representation. This fused fea-\nture is then propagated through a feedforward neural network\nfor the detection of tendencies indicative of depression.\nA. Word Vector Extraction\nThis module specializes in processing user text informa-\ntion originating from social media platforms, encompassing\nmultiple dimensions of users, including nicknames, personal\nprofiles, and posted tweets, among others. The process begins\nby integrating these various categories of information, con-\ncatenating them to form a continuous long-text sequence. This\ndesign is intended to capture and preserve the full picture of\nusers' online behavior, as each component may harbor clues\nabout their personality traits, emotional states, or social habits.\nThe constructed long-text sequence is then fed into a pre-\ntrained MacBERT model. As an enhanced version of the\nBERT model, MacBERT introduces error-correcting masked\nlanguage modeling tasks during pre-training, optimizing the\nmodel and reducing inconsistencies between pre-training and\ndownstream tasks. This process enables not only the learning\nof lexical meanings but also the understanding of contextual\nnuances, extracting profound semantic information embedded\nwithin the text. Through MacBERT's processing, the raw\ntext information is transformed into high-dimensional word\nembedding vectors. These vectors carry rich semantic and con-\ntextual information, furnishing high-quality input data, referred\nto as input 1, for subsequent natural language processing tasks.\nThis input 1, derived from deep learning model transfor-\nmation, embodies a comprehensive understanding and parsing\nof user textual expressions. It not only facilitates the identifi-\ncation of key terms but also delves into the text to uncover\nemotional tendencies, underlying themes, and intricate patterns\nof interpersonal interactions. In applications such as depression\ndetection, this nuanced text analysis capability is particularly\ncrucial, enabling the recognition of subtle linguistic cues\nassociated with depressive sentiments and providing robust\ndata support for mental health assessments.\nB. Statistical Feature Extraction\nThis module is responsible for extracting statistical informa-\ntion about users. Based on the research of previous work [7]\n[11] [12] [13] [14] [15] [16], we have adopted six statistical\nfeatures: the proportion of negative emotional tweets, the\nproportion of original tweets, the ratio of posts made during\nlate-night hours, the frequency of posts per week, the standard\ndeviation of posting times, and the proportion of posts that\ninclude images. The detailed descriptions of these statistical\nfeatures are provided in Table I. Detailed calculations for each\nof these statistical features along with their comprehensive de-\nscriptions will be elaborated upon in Section IV-B. Following\nthe manual extraction of these statistical features, they are fed\ninto a fully connected network for encoding, serving as input 2,\ntailored to conform to the dimensional specifications required\nby the subsequent cross-attention mechanism module.\nC. Cross Attention\nCross-attention [17] is a mechanism predominantly used in\nTransformer models, calculating attention across two distinct\nsequences to manage semantic relationships between them. It\nfinds applications in tasks such as machine translation, image\ncaptioning, and video-text alignment. This mechanism expands\nupon self-attention, enabling the model to dynamically aggre-\ngate information at each position of one sequence (the query\nsequence) based on the content of another sequence (the key-\nvalue sequence).\nThe cross-attention mechanism is a specialized form of\nmulti-head attention, where the input tensor is divided into\ntwo parts, $X_1 \\in R^{n \\times d_1}$ and $X_2 \\in R^{n \\times d_2}$.One part serves\nas the set of queries, while the other part acts as the key-\nvalue set.Its output is a tensor of size n x d2, where for\neach query vector, there are attention weights assigned to all\nthe key vectors. Fig. 3 illustrates the computation process\nof cross-attention, where through matrix operations and the\nsoftmax function, the model learns to selectively gather perti-\nnent information from the key-value sequences based on each\nelement in the query sequence. This enhances the model's\ncapability in handling cross-sequence dependencies, thereby\nboosting its performance. Such a mechanism is particularly\ncrucial in dealing with multimodal tasks, as it enables the\nmodel to flexibly integrate features from disparate modalities.\nFor instance, in the joint analysis of image and text, image\nfeatures can serve as the key-value sequences, while the textual\ndescriptions act as the query sequence. The cross-attention\nmechanism then facilitates precise alignment and information\nfusion between the two, thereby enhancing the interpretability\nand accuracy of the combined multimodal analysis.\nSpecifically, let $Q = X_1 W^Q$ and $K = V = X_2 W^K$, the\ncomputation of the cross-attention is as follows:\n$CrossAttention(X_1, X_2) = Softmax(\\frac{Q K^T}{\\sqrt{d_2}})$\nWhere $W^Q \\in R^{d_1 \\times d_k}$ and $W^K \\in R^{d_2 \\times d_k}$ represent learned\nprojection matrices, de denotes the dimensionality of the key-\nvalue set (which is also the dimensionality of the query set).\nBelow, we will elaborate on the computation process of\ncross-attention, as depicted in Figure 10086. This figure illus-\ntrates a mechanism known as \"Cross-attention,\" a commonly\nemployed technique in many modern Natural Language Pro-\ncessing (NLP) tasks. Within this process, V, K, and Q represent\nthree matrices that collectively determine the attention scores.\nHere is a concise explanation of each component:\n\u2022 V Matrix (Value): The Value matrix encapsulates infor-\nmation from the input sequence, which has been encoded\nfor the purpose of computing attention scores. Typically\ngenerated by an embedding layer of the input sequence,\nthis layer is responsible for transforming raw text into\nnumerical representations.\n\u2022 K Matrix (Key): Similarly derived from the input se-\nquence, the Key matrix serves to ascertain which po-\nsitions are more significant during the calculation of\nattention scores. Elements in the K matrix are compared\nagainst those in the Q matrix to decide which positions\nought to receive heightened attention.\n\u2022 Q Matrix (Query): The Query matrix, another derivation\nfrom the input sequence, is utilized to inquire about the\nmost relevant positions within the K matrix. Through\nsimilarity matching between elements in the Q matrix\nand those in the K matrix, it facilitates the determination\nof attention scores.\n\u2022 Attention Scores: Attention scores are the outcome of\nthe interaction between the V, K, and Q matrices. By\nassessing the similarity between the Q and K matrices,\nattention scores are computed. These scores are subse-\nquently employed to weight and combine values from\nthe V matrix, yielding the final output.\nIn practical applications, the cross-attention mechanism\ntypically involves the following steps, as exemplified by For-\nmula 1:\n1) Linear Transformation: Initially, the V, K, and Q matri-\nces undergo linear transformations, often through multi-\nplication with weight matrices, to ensure they all possess\nthe same dimensionality. This preprocessing aligns them\nfor the subsequent computations.\n2) Inner Product Operation: Subsequently, each element\nin the Q matrix is paired with every element in the\nK matrix through an inner product operation, yielding\na scalar value. These scalars quantify the degree of\nsimilarity between each element in the Q matrix and its\ncorresponding elements in the K matrix.\n3) Softmax Function: The outcomes from the inner prod-\nucts are then fed into a softmax function. This operation\nscales the results such that each position's attention\nscore falls within the interval [0, 1], with the sum of all\nscores equaling 1. Consequently, a probability distribution\nis obtained, reflecting the relative importance of each\nposition.\n4) Weighted Summation: Finally, the values in the V\nmatrix are aggregated via a weighted summation process,\nwhere the weights are determined by the attention scores.\nThis means that positions with higher attention scores\ncontribute more significantly to the resultant output.\nThe cross-attention mechanism is particularly valuable in\nneural network architectures like the Transformer, as it en-\nables the model to focus on different segments of the input\nsequence. This is crucial for comprehending complex contex-\ntual dependencies and long-range relationships. Consequently,\nincorporating cross-attention as our multimodal feature fusion\nmodule proves highly efficient; it empowers the model to al-\nlocate attention resources flexibly, thereby achieving enhanced\nperformance.\nD. Multi-Layer Perceptron\nThe Multilayer Perceptron (MLP) [18], short for Multilayer\nPerceptron, is a type of feedforward artificial neural network\nmodel that serves as an extended version of the single-layer\nperceptron. By incorporating one or more hidden layers, MLP\nenhances the learning capacity and expressive power of the\nnetwork. This structural enhancement enables MLPs to fit\ncomplex nonlinear functional relationships, thereby excelling\nin solving many practical problems, particularly in classifica-\ntion and regression tasks.\nIn this paper, MLP is employed as the final classification\nhead module, comprising two essential components: linear\ntransformation layers and activation functions. Specifically, the\nMLP consists of two layers of linear transformations, with\na Rectified Linear Unit (ReLU) activation function inserted\nbetween them. Renowned for its simplicity and efficacy, the\nReLU activation function effectively alleviates the vanishing\ngradient problem, facilitating the training of deeper networks,\nas defined by the following equation:\n$MLP(x) = ReLU(W_2(ReLU(W_1x+b_1))+b_2)$\nwhere MLP() represents the multilayer perceptron (MLP), x\nrepresents the input feature vector, $W_1$ and $W_2$ are the weight\nmatrices of the two linear transformation layers, and b\u2081 and\nb2 respectively denote the bias terms for these two layers. The\nReLU function is defined as f(x) = max(0,x), which maps\nall negative inputs to zero while preserving positive values\nunchanged. This approach maintains the nonlinearity of the\nnetwork while circumventing the issue of gradient saturation\nin the negative value region.\nEquipped with this architecture, the MLP is capable of\nlearning high-level abstract features from the input data and,\nthrough its final output layer, provides category predictions. In\nthe context of depression detection methodologies, the MLP\nserves as the decision layer, integrating multimodal features\nfused via the cross-attention mechanism. Through nonlinear\ntransformations, these features are mapped onto a classification\nprobability space, thereby facilitating accurate judgment re-\ngarding whether a user is experiencing depression. This design\nnot only enhances the model's ability to recognize complex\nemotional and behavioral patterns but also ensures that the\nmodel exhibits good generalization performance, capable of\nmaking accurate predictions on unseen data.\nE. Psychological Analysis\nIn this section, we delve into the inner world and behavioral\nmanifestations of individuals with depression from a psycho-\nlogical perspective, to present readers with a vivid mental\nhealth landscape. Depression, a prevalent mental health issue\nworldwide [19], not only impacts an individual's emotional\nexperiences but also significantly alters their cognitive func-\ntions, social behaviors, and even physical health [20]. Our\naim, through meticulous data analysis, is to illuminate the\ndistinctions between depression and other mental health states,\nthereby enhancing public comprehension of this complex\npsychological condition [21].\n1) Analysis of Characteristics in Individuals with Depres-\nsion: The subjective experience of individuals with de-\npression is often characterized by pervasive negative emo-\ntions, including sadness, hopelessness, self-deprecation,\nand loss of interest [22]. These emotions are not confined\nto specific situations but permeate throughout their daily\nlives, markedly diminishing their quality of life and sense"}, {"title": "IV. IMPLEMENTATION", "content": "In this section, we provide a detailed exposition on the\nimplementation of the Multimodal Feature Fusion Network\nbased on Cross-Attention (MFFNC), which was introduced in\nSection III. The overall architecture of the implementation is\ndepicted in Fig. 2.\nA. Implementation of Word Vector Extraction\nIn our study, to conduct a comprehensive analysis of users'\nemotional states, we integrate various pieces of information\nleft by users on social media platforms, including their user-\nnames, profile descriptions, and posts or retweets, forming a\nlong text sequence. Notably, these posts comprise not only\noriginal content created by users but also retweets that shed\nlight on their interests and emotional expressions. Through\nsuch integration, the constructed long text sequence transcends\nthe limitations of individual posts, establishing strong con-\ntextual links across multiple tweets. Users may sequentially\npost tweets at different times to articulate their experiences\nbattling depression or seek solace and express inner distress\non social media. Consequently, aggregating this information is\ncrucial for discerning whether a user is undergoing depression."}, {"title": "V. EVALUATION", "content": "V. EVALUATION\nA. Experiment Setup\n1) Performance Metrics: True Positive (TP)", "performance": "n$Accuracy = \\frac{TP+TN"}, {"FN}$\nAccuracy": "Accuracy represents the proportion of all pre-\ndictions that are correctly predicted.\n$Recall = \\frac{TP"}, {"FN}$\nRecall": "Recall measures the model's ability to identify\nall true positive instances", "FP}$\nPrecision": "Precision denotes the proportion of samples\npredicted as positive by the model that are actually positive", "Recall}$\nF1-Score": "F1 Score ranges from 1 to 0", "Dataset": "For the datasets setup", "groups": "Depressed' and\n'Normal", "8": 2, "D1": "D2 as 16", "000": "n4", "Configuration": "The computational environ-\nment for this study is anchored within Alibaba Cloud's robust\ninfrastructure-the Platform for Artificial Intelligence (PAI)", "7": "We reproduced their approach"}, {"7": "to establish our baseline. They adopted\nthe widely-used pre-trained model", "28": "for ex-\ntracting text-based word features. Subsequently", "29": "LERT is a pre-training model that", "30": "PERT is an innovative pre-training model that\ndoes not rely on the use of masked tokens ([MASK", "outcomes": "n1) Case 1: Table IV presents the profile information and\ntweets of a user named 'Director Ren Youbing' who iden-\ntifies himself as the 'Director of the Depression Research\nInstitute.' Through his profile description and the content\nof the illustrated tweets", "Director Ren Youbing": "xhibits\na degree of self-awareness", "question": "What are some self-healing\nmethods for mild depression?' This demonstrates not\nonly his awareness of his condition but also a proactive\napproach in seeking remedies through social media plat-\nforms.\nFor users like 'Director Ren Youbing', whose profiles\nand tweets frequently incorporate negative terms such\nas 'depression,' our model demonstrates a high level\nof sensitivity and precision. It is capable of accurately\nidentifying depressive tendencies and swiftly converging\non predictions, highlighting the efficacy and reliability of\nour model in assessing mental health indicators within\nonline contexts.\n2) Case 2"}]}