{"title": "Aligning Audio-Visual Joint Representations with an Agentic Workflow", "authors": ["Shentong Mo", "Yibing Song"], "abstract": "Visual content and accompanied audio signals naturally formulate a joint repre-\nsentation to improve audio-visual (AV) related applications. While studies de-\nvelop various AV representation learning frameworks, the importance of AV data\nalignment is usually undermined for achieving high-quality representation. We\nobserve that an audio signal may contain background noise interference. Also, non-\nsynchronization may appear between audio and video streams. These non-strict\ndata alignment limits representation quality and downgrade application perfor-\nmance. In this paper, we propose to improve AV joint representations from a\ndata-centric perspective by aligning audio signals to visual data. Our alignment is\nconducted in an agentic workflow controlled by an LLM-based assistant named\nAVAgent. For each input AV data pair, our AVAgent uses a multi-modal LLM\nto convert audio and visual data into language descriptions separately (i.e., tool\nuse). Then, AVAgent reasons whether this paired data is aligned well and plans to\nedit the audio signal if needed (i.e., planning). The audio editing is executed by\npredefined actions that filter noise or augment data. Moreover, we use a VLM to\nevaluate how modified audio signals match the visual content and provide feedback\nto AVAgent (i.e., reflection). The tool use, planning, and reflection steps oper-\nate cyclically to become an agentic workflow where audio signals are gradually\naligned to visual content. To this end, existing methods can directly leverage the\naligned AV data via our agentic workflow to improve AV joint representations. The\nexperimental results comprehensively demonstrate the state-of-the-art performance\nof the proposed approach against previous baselines in diverse downstream tasks.", "sections": [{"title": "Introduction", "content": "Video stream is usually captured with sound recording. Intuitively, the audio signal supplements video\ndata to formulate a joint AV representation. Compared to a single visual or audio representation, the\njoint representation benefits both single and cross-modality applications such as automatic captioning,\ncontent retrieval, and human-computer interaction. Learning audio-visual representations has been\nheavily investigated in the audio-visual recognition [1, 2, 3], sound source separation [4, 5, 6],\nand self-supervised form [7, 8]. These studies design various representation learning framework\nto leverage existing AV data pairs to obtain joint representations, which are further applied into\ndownstream audio and visual related scenarios.\nThe audio and visual pairs may not align well in practice during data capturing. We observe there\nare two main issues. First, the audio signal may contain background noise interference. In the real\ncapturing scene, the microphone may record sound unrelated to the visual content. Second, the\nrecorded sound may not correspond to the video frames temporally. This may be because of the"}, {"title": "Related works", "content": "In this section, we provide an overview of the landscape of prior research in audio-visual representa-\ntion learning and discuss how current innovations in the use of Large Language Models (LLMs) as\nagents contribute to advancements in this field.\nAudio-Visual Representation Learning Audio-visual representation learning has long been a\nfocus of multimedia research [1, 2, 3, 9, 10, 4, 5, 6, 7, 8, 11, 12, 13, 14], aiming to establish effective\ncross-modal correlations between audio and visual data. Pioneering works such as SoundNet [1]\nand the approaches by Owens et al. [2] and Arandjelovic et al. [3] have laid the foundation for\nunderstanding these modalities as intertwined rather than separate. These studies have shown that\nsynchronizing audio with visual input enhances machine perception and can be pivotal for tasks\nsuch as event localization [15, 16, 17, 18] and audio-visual spatialization [19, 20, 21]. Recent\nadvancements have also explored complex scenarios like audio-visual navigation [21, 22, 23] and\nparsing [24, 25, 26, 27], highlighting the depth and versatility of audio-visual data integration. Our\nfocus on improving data quality through intelligent adjustments sets our work apart from existing\nmethods, positioning it as a significant contribution to the field of AV representation learning.\nLLM-based Agents The integration of LLMs [28, 29] as decision-making agents represents a\nsignificant leap in multimedia processing. For instance, the AesopAgent [30] and VideoAgent\nprojects [31] utilize LLMs to drive long-form video understanding and story-to-video production,\nshowcasing the potential of LLMs in generating and refining multimedia content. However, these\napplications primarily focus on generating or interpreting content rather than enhancing the quality"}, {"title": "Proposed method", "content": "We propose to enhance the alignment of audio-visual (AV) paired data for joint representation\nimprovement. Our alignment is fulfilled in an agentic workflow where audio signals are gradually\naligned to visual data. Figure 2 shows an overview of our workflow controlled by an LLM-based\nassistant named AVAGENT In the following, we revisit the AV representation learning, illustrate our\nAVAGENT and analyze our aligned AV paired data."}, {"title": "Revisiting Audio-Visual Representation Learning", "content": "Audio-visual (AV) representation learning aims to fuse information from both audio and visual\nmodalities to create a unified representation that captures the intrinsic correlations between these\ntwo streams. This integration is foundational for enhancing the performance of various multimedia\napplications, including speech recognition, event detection, and content-based retrieval. Traditional\nframeworks in AV representation learning, such as those introduced in SoundNet [1] and the works\nby Arandjelovic et al. [3], typically generate feature embeddings for each modality, which are\nthen merged to form joint representations. The effectiveness of these representations hinges on the\nassumption that the paired audio and visual data are well-aligned and synchronized. In practice,\nAV representation learning models directly consume raw or minimally processed AV pair data to\nconstruct these joint representations. While effective, this direct approach generally overlooks the\npotential misalignments and asynchronies inherent in the source data. Misalignments, whether\ntemporal or contextual, can degrade the quality of the learned representations, thereby reducing the\noverall performance of the system on downstream tasks.\nOur method addresses these limitations by introducing a data-centric approach that first assesses and\ncorrects any misalignment between the audio and visual data before they are used for representation"}, {"title": "Agentic Workflow", "content": "We design an automatic workflow to adjust audio signals in accordance with visual data. It consists\nof tool use, planning, and reflection steps. Taking the raw AV pair data as input, our workflow\nstill outputs AV pair data where the audio signal is well aligned with visual data. As we focus\non data processing, our refined pair data can be widely utilized in various scenarios related to AV\nrepresentation learning.\nTool use We leverage multi-modal LLMs [33, 34] to map audio and video streams into a unified\nspace where both data can be described in the language form. We transform the audio and video\nseparately to obtain two independent language descriptions, which our AVAGENT further analyzes\nfor planning. The separate transformation benefits alignment identifications. For example, when a\nperson is speaking in a noisy market, the video content depicts 'A person talking in a market' while\nthe audio signal may be dominated by background noise interference. Such AV discrepancy will\nbe reflected in separate transformations to language description while the discrepancy may not be\nnoticed if transferred jointly. Meanwhile, transferring AV pair into language form will benefit our\nLLM-based AVAGENT to identify and plan actions accordingly.\nPlanning Our AVAGENT reasons the given AV pair data in language form and plans for the upcoming\nactions. We have predefined 8 actions in advance as shown in Figure 3. These actions are defined\nbased on our observation that background noise interference and non-synchronization mostly limit the\nAV joint representations. As such, we introduce 4 noise-filtering operations to remove background\ninterference, and 4 audio coordination operations to correspond audio signals to video streams. In\nthe planning step, AVAGENT plans one action to execute. To train AVAGENT for action planning, we\nprepare paired data where there are video streams, audio signals, and actions. Each pair is annotated\nwith a context description and a corresponding action that should be taken for better video and audio\nalignment. Our AVAGENT is based on Vicuna-v1.5-7b [35] and we adopt LoRA [36] tuning so as not\nto affect the original reasoning ability of LLM.\nReflection We evaluate how the modified audio signals match visual data after executing actions.\nThe ImageBind [37] measures the similarity between different modalities of data, which we adopt\nto compute the AV alignment score and temporal synchronization score. These two scores provide\nfeedback to the AVAGENT for planning in the next cycle. If our scores are relatively low, the action\nplanned in the next cycle will be put on the original AV pair, rather than the repaired AV data in the\ncurrent round because of avoiding accumulated errors."}, {"title": "Data Analysis", "content": "In this section, we present the data analysis procedures used to validate the effectiveness of our\nproposed method for improving audio-visual data alignment. Our approach involves quantitatively\nassessing the matching score between audio and video streams before and after applying our method,\nhighlighting the significant enhancements achieved through our adaptive synchronization techniques.\nTo measure the alignment of audio-visual data, we introduce the visual alignment and temporal\nalignment score, which quantitatively evaluates how well the audio stream corresponds with the\nvisual content in terms of timing and context auditory-visual congruence. The visual alignment and\ntemporal alignment scores are computed using X-CLIP [38] that analyzes the coherence between the\ngenerated audio and the synchronization accuracy. Specifically, we perform simulation experiments\nby computing the metrics from modified (true) audio-video pairs and original (false) pairs.\nVisual Alignment For visual alignment, we compute the alignment (Alignment) score between\nvideo and audio features by using 50k modified (true) video-audio pairs and 50k randomly selected\noriginal (false) pairs from VGGSound [39]. The quantitative results are reported in Table 1a. With\nthe increase in the number of false pairs with noisy visual information, all alignment metrics decrease.\nAdding 50k true pairs to [50k, 50k] cases further increases all metrics. These results validate the\neffectiveness of the proposed agent in removing audio data noises to improve visual alignment.\nTemporal Synchronization For temporal synchronization, we compute the temporal alignment\n(T-Alignment) score between video and audio features by using 50k modified (true) video-audio pairs\nand 50k randomly selected original (false) pairs averaged across all time steps. Table 1b shows the\ncomparison results on our dataset in terms of T-Alignment scores. As can be seen, the T-Alignment\nscore decreases with the increase in the number of false pairs without synchronized audio with the\noriginal videos, although they share the same visual information. Adding 50k additional true pairs\nto [50k, 50k] cases also increases T-Alignment scores, which further shows the importance of the\nproposed agent in increasing temporal synchronization together with visual alignment.\nThese analyses not only substantiate the effectiveness of our proposed adjustments but also illustrate\nthe practical implications of our method in real-world scenarios. The improvement in matching\nscores post-intervention validates our approach, confirming that our method significantly enhances\nthe synchronization and overall perceptual quality of audio-visual content. This section underscores\nthe transformative potential of integrating large language models with multimodal large language\nmodels to achieve superior audio-visual data alignment, setting a new benchmark in the field of\naudio-visual representation learning."}, {"title": "Experiments", "content": "In this section, we provide the detailed experimental setup and evaluation protocols used to assess the\nperformance of our proposed method on various audio-visual representation learning tasks. These\nexperiments are designed to validate the effectiveness of our approach, highlighting its advantages\nover existing state-of-the-art methods."}, {"title": "Experimental Setup", "content": "Our experiments cover a range of audio-visual tasks, each chosen to demonstrate the robustness\nand versatility of our method in enhancing audio-visual data quality and alignment. The tasks"}, {"title": "Comparison to Prior Work", "content": "In this work, we propose a novel agentic workflow for aligning audio-visual joint representations.\nTo demonstrate the effectiveness of the proposed AVAGENT, we comprehensively compare it to prior\nwork on linear-prob/fine-tune, audio-visual sound source localization, separation, and segmentation.\nThe results from these experiments are crucial in validating the effectiveness of our data-centric\napproach. By focusing on enhancing data quality through intelligent audio-visual synchronization,\nour method not only improves the accuracy of specific tasks but also enhances the overall utility of\nAV systems in diverse applications.\nAudio-visual classification. To validate the effectiveness of the proposed AVAGENT on audio-visual\nclassification, we compare to the following prior baselines: 1) MAE [43]: a masked autoencoder with"}, {"title": "Experimental Analysis", "content": "In this subsection, we provide a detailed analysis of the experiments conducted to assess the effec-\ntiveness of our approach. The experimental results are analyzed to understand the impact of our\nmethod on audio-visual representation learning, particularly focusing on the improvements over naive\napproaches using random actions.\nTo demonstrate the effectiveness of LLMs in our AVAGENT, we further compare the performance\nof our method against a naive approach where random audio modifications are applied without the\nguidance of LLMs in Table 5. This comparison is crucial to demonstrate the necessity and efficiency\nof our intelligent, context-aware synchronization strategy. For the naive baseline, random actions\nsuch as arbitrary pitch adjustments, random noise addition, or unsystematic volume changes are\napplied to the audio-visual data. The audio-visual-visual data modified using our LLM-guided\nmethod significantly outperforms the randomly modified data in all evaluation metrics. For instance,\nin audio-visual classification tasks, our method achieves a higher accuracy rate, demonstrating the\nrelevance and precision of the audio adjustments suggested by the LLM. The poor performance of\nthe naive approach underscores the importance of precise, context-driven modifications, which are\nonly possible through the understanding and analysis capabilities of our LLMs."}, {"title": "Conclusion", "content": "In this work, we introduced AVAGENT, a novel data-centric approach for enhancing audio-visual\nrepresentation learning by utilizing Large Language Models (LLMs) as agents to achieve precise\naudio-visual synchronization and alignment. Our method diverges from traditional method-centric\napproaches, which primarily focus on algorithmic enhancements, and instead emphasizes the critical\nrole of data quality in audio-visual learning processes. Our methodology leverages both Multimodal\nLLMs and advanced LLM techniques, including LoRA tuning, to analyze and adaptively modify the\naudio stream in alignment with the corresponding video content. Through a series of well-structured\nexperiments across various audio-visual tasks such as classification, source localization, retrieval,\nquestion-answering, segmentation, and source separation, our approach has demonstrated significant\nimprovements over existing methods. The success of our method confirms the importance of focusing\non data quality and intelligent data manipulation in audio-visual representation learning. By ensuring\nthat the audio and video streams are well-aligned and contextually synchronized, we can significantly\nenhance the effectiveness of audio-visual learning models, thereby improving their applicability in\nreal-world scenarios.\nLimitations. While our approach significantly advances the field of audio-visual representation\nlearning, there are some limitations that merit further investigation. Our method's effectiveness is\ncontingent on the initial quality of the audio and visual inputs. In scenarios where inputs are of\npoor quality or excessively noisy, the performance of our LLM and VLM might be compromised,"}, {"title": "Appendix", "content": "In this appendix, we provide the following material:\n\u2022 addition implementation and datasets details in Section A,\n\u2022 algorithm for our AVAGENT in Section B,\n\u2022 additional experimental analyses in Section C,\n\u2022 additional example results from our AVAGENT in Section D,\n\u2022 additional discussions on limitations and broader impact in Section E."}, {"title": "Implementation & Dataset Details", "content": "In this section, we provide more implementation and dataset details.\nAudio-visual classification. For linear probing, we follow the prior work [43, 44] and extract frozen\naudio-visual representations from our AVAGENT pre-trained audio-visual masked autoencoder. Then\nwe attach a linear layer as a head to the frozen features for training with the audio-visual classes.\nDuring training, we only fine-tune the linear head to evaluate the quality of pre-trained features.\nThe models are trained for 50 epochs using the Adam optimizer [49] with a learning rate of le - 4\nand a batch size of 128. For fine-tuning, we use the same optimizer and batch size settings, but all\nparameters are learnable.\nSound source localization and segmentation. For sound source localization, we train all base-\nlines [48, 41, 40] using the same backbone (i.e., ViT-Base) for audio/visual encoder with different\nproposed objectives in their original papers. The final localization map is generated through bilinear\ninterpolation of the similarity map between audio/visual features from the last self-attention layer.\nThe models are trained for 30 epochs using the Adam optimizer [49] with a learning rate of le - 4\nand a batch size of 128. For segmentation, we follow the prior work [42], and apply an upsampling\ndecoder on features from the last self-attention layer to generate the final segmentation mask. We use\nthe binary cross entropy (BCE) loss between the prediction and ground-truth masks for training. The\nmodels are trained for 20 epochs using the Adam optimizer [49] with a learning rate of 1e - 4 and a\nbatch size of 128.\nSound source separation. For sound source separation, we follow the previous method [4, 32] and\nattach an audio U-Net decoder to our pre-trained audio-visual encoders for separating sounds from the\nmixture. The decoder depth for self-attention layers is 8, and the decoder receives the representations\nof the audio mixture and the visual embeddings. We also apply multiple transposed convolutions and\nan output head to predict a time-frequency separation mask. This separation mask is then used to\nmultiply the input mixture STFT to separate the audio. Similarly to [4], the target masks refer to the\ntime-frequency bins where the source is the most dominant component in the mixture. The sound\nsource separation is achieved by optimizing a binary cross-entropy loss over these binary targets. The\nmodel is trained for 20 epochs using the Adam optimizer [49] with a learning rate of 1e - 4 and a\nbatch size of 128.\nDataset Details. We evaluated our method using several prominent audio-visual datasets:\n\u2022 Flick-SoundNet [10]: a dataset consisting of natural soundscapes with associated Flickr\nimages with 4,500 audio-visual pairs for training and testing the model on 250 audio-visual\npairs of sounding objects and extended 250 non-sounding objects;\n\u2022 VGG-Instruments [40]: contains video clips of musical instrument performances, with 32k\nvideo clips of 10s lengths from 36 musical instrument classes, a subset of VGG-Sound [39],\nand each video only has one single instrument class;\n\u2022 MUSIC [4]: consists of 448 untrimmed YouTube music videos of solos and duets from 11\ninstrument categories;\n\u2022 VGG-Music [32]: a dataset that features a collection of music videos with annotations\nrelated to the genre and instruments present;\n\u2022 VGGSound [39]: a comprehensive dataset that includes a wide variety of sound categories\nand corresponding visual scenes, which contains categories, such as animals, instruments,\nvehicles, people, etc;"}, {"title": "Algorithm for AVAgent", "content": "Our approach is formalized through the following algorithmic steps:\n1. Input an audio-visual pair.\n2. Use the multimodal LLM to independently generate text descriptions for both audio and\nvisual data.\n3. Analyze the descriptions to detect discrepancies and assess alignment using a trained LLM\nmodel.\n4. Plan and apply necessary audio modifications to enhance alignment, such as noise filtering\nand temporal adjustments.\n5. Use a VLM to assess the effectiveness of the modifications and provide a feedback score.\n6. Iterate the process until the audio-video alignment meets the predefined threshold or maxi-\nmizes the synchronization score.\nAlgorithm 1 outlines the steps followed by our AVAgent in processing and synchronizing audio-\nvisual data using a data-centric approach. The algorithm utilizes a multimodal large language model\n(mLLM), large language model (LLM), and a vision-language model (VLM) to refine audio to better\nalign with the visual content iteratively. This algorithm encapsulates the cyclic process of audio\nsignal refinement, leveraging both machine learning models and heuristic analysis to ensure that\nthe audio accurately reflects the visual context. Each step of the process is designed to enhance the\nsynchronization between the audio and visual modalities iteratively."}, {"title": "Additional Experimental Analyses", "content": "In this section, we conduct additional experimental analyses to explore the impact of LLM (Low-Rank\nAdaptation) tuning in our AVAGENT. The results are reported in Table 6. LoRA tuning enhances\nour LLM's memory and context retention capabilities, enabling it to make more informed decisions\nregarding AV synchronization. We implement LoRA tuning on the LLM to fine-tune it specifically\nfor tasks that require a deep understanding of the audio-visual context. This involves adjusting the\nLLM's parameters to remember better and integrate lengthy audio-visual sequences. The application\nof LoRA tuning leads to noticeable improvements in tasks such as audio-visual source localization\nand segmentation, where the precision of temporal and spatial alignments is crucial. Metrics such\nas mIoU and F1 score show substantial enhancements. These improvements can be attributed to\nthe LLM's enhanced ability to maintain context over extended interactions, proving essential in\ncomplex scenarios where long-term dependencies are critical. These analyses highlight the efficacy\nof our adaptive, LLM-driven approach in achieving high-quality AV synchronization. By leveraging\nsophisticated LLM tuning methods and moving away from naive, random modifications, our method\nsets a new standard in AV data processing, paving the way for more accurate and effective multimedia\napplications."}, {"title": "Additional Examples", "content": "In this section, we present detailed examples of the agent workflow in action, demonstrating the\niterative process of aligning audio-visual content through the use of tool use, planning, and reflection\nphases. Each example includes inputs, model outputs, action plans, and reflection scores, showcasing\nthe method's effectiveness in improving synchronization and alignment."}, {"title": "Example 1: Lecture in a Large Hall", "content": "Figure 5 provides a full example of our agent workflow for lectures in a large hall."}, {"title": "Example 2: Dog Barking in Park", "content": "Figure 6 provides a full example of our agent workflow for dog barking in the park."}, {"title": "Example 3: Waterfall Scene", "content": "Figure 7 provides a full example of our agent workflow for the waterfall scene."}, {"title": "More Discussions", "content": ""}, {"title": "Limitations", "content": "Our approach, while advancing the state-of-the-art in audio-visual synchronization, is subject to\nseveral limitations that should be considered:\n\u2022 Dependency on Data Quality: The performance of our method heavily relies on the quality\nof the input data. Poor quality video or audio, such as low-resolution video or highly\ndistorted audio, can significantly impair the ability of the LLM and VLM to accurately\ngenerate useful captions and subsequent synchronization actions."}, {"title": "Broader Impact", "content": "The broader impact of our research is multifaceted, with potential implications for numerous fields:\n\u2022 Media and Entertainment: Our method can significantly enhance the quality of multimedia\ncontent by ensuring better synchronization between audio and visual elements, leading to\nmore immersive experiences in film, television, and virtual reality.\n\u2022 Accessibility Enhancements: Improved audio-visual synchronization can also benefit\naccessibility technologies, such as developing more accurate closed captioning and audio\ndescriptions for the hearing or visually impaired.\n\u2022 Educational Tools: In educational settings, our method can be used to create more engaging\nand comprehensible instructional videos, where precise alignment of audio explanations\nwith visual demonstrations is crucial.\n\u2022 Surveillance and Security: Enhanced synchronization capabilities may improve the relia-\nbility of surveillance systems where audio cues are critical to understanding visual footage.\n\u2022 Ethical Considerations: While our technology has many positive applications, it is essential\nto develop guidelines to prevent its misuse, particularly in contexts where audio-visual\nmisalignment could be used to deceive or mislead viewers."}]}