{"title": "Model Human Learners:\nComputational Models to Guide Instructional Design", "authors": ["Christopher J. MacLellan"], "abstract": "Instructional designers face an overwhelming array of design choices, making it challenging to identify the most effective interventions. To address this issue, I propose the concept of a Model Human Learner, a unified computational model of learning that can aid designers in evaluating candidate interventions. This paper presents the first successful demonstration of this concept, showing that a computational model can accurately predict the outcomes of two human A/B experiments-one testing a problem sequencing intervention and the other testing an item design intervention. It also demonstrates that such a model can generate learning curves without requiring human data and provide theoretical insights into why an instructional intervention is effective. These findings lay the groundwork for future Model Human Learners that integrate cognitive and learning theories to support instructional design across diverse tasks and interventions.", "sections": [{"title": "Introduction", "content": "In their seminal paper, Card, Moran, and Newell (1986) propose the concept of a Model Human Processor, a unified information-processing model that codifies psychological theories to predict human performance in user interfaces. They argue that interface designers could use this kind of model to make more informed design decisions that improve usability. In this paper, I propose an analogous concept: a Model Human Learner, a unified computational model that codifies cognitive and learning theories to predict human learning in instructional systems. I argue that instructional designers could use this kind of model to make more informed instructional design decisions that improve pedagogical effectiveness.\nAn analysis by Koedinger, Booth, and Klahr (2013) suggests that instructional designers face a crisis of choice-design spaces are combinatorial, containing trillions of possible configurations. How can instructional designer determine the best choices? Currently, they rely on A/B experiments to navigate this vast space, but human studies are costly and time consuming. Moreover, each A/B experiment provides only a single bit of information, meaning instructional designers are essentially playing a losing game of 20 questions with nature (Newell, 1973).\nComputational models of learning, such as those proposed by MacLellan (2017) and Weitekamp and Koedinger (2023), offer a potential solution. Unlike abstract, mathematical models of learning, such as the Additive Factors Model (Cen, 2009) or Bayesian Knowledge Tracing (Corbett & Anderson, 1995), which fit functions to performance data, computational models of learning are mechanistic. Similar to cognitive architectures (Langley, Laird, & Rogers, 2009), these models use artificial intelligence and machine learning to simulate how knowledge is updated in response to practice and how performance evolves over time. I argue that these models can realize the Model Human Learner concept, letting instructional designers simulate A/B experiments to test alternative interventions and identify the most promising ones before conducting costly human studies.\nPrior research has explored several applications of computational models of learning. For example, Li, Stampfer, Cohen, and Koedinger (2013) investigated their use for discovering cognitive models, while Matsuda, Yarzebinski, Keiser, Cohen, and Koedinger (2011) explored how they can promote \"learning-by-teaching,\" where students learn by instructing simulated students. MacLellan and Koedinger (2022) and Weitekamp, Harpstead, and Koedinger (2020) examined their applications for tutor authoring. Lastly, others have studied their utility for theory testing (Rachatasumrit, Carvalho, Li, & Koedinger, 2023; Matsuda, Lee, Cohen, & Koedinger, 2009; Li, Matsuda, Cohen, & Koedinger, 2010).\nMore recently, researchers have proposed using these models to predict how different instructional design choices will impact learning (MacLellan, Harpstead, Patel, & Koedinger, 2016; MacLellan, Stowers, & Brady, 2023). While these studies have generated reasonable predictions about experimental effects, none have yet been compared with human outcomes. This critical next step is the focus of this paper. Specifically, I apply a computational model of learning to predict the outcomes of two human A/B experiments. By comparing these predictions to previously collected human data, I demonstrate that computational models of learning can:\n\u2022 Successfully predict the main effects of two human experiments-one evaluating a problem sequencing intervention and the other testing an item design intervention;\n\u2022 Generate learning curve predictions that closely align with human learning curve trends, without training on human data first; and\n\u2022 Provide theoretical insights into why specific interventions work, challenging a prior hypothesis by Lee, Betts, and Anderson (2015) and suggesting a novel explanation."}, {"title": "The Computational Model", "content": "This study employs a computational model from the Apprentice Learner Architecture (MacLellan, 2017; Weitekamp et al., 2020; MacLellan & Koedinger, 2022), which integrates mechanisms from prior models of human learning, including ACM (Langley & Ohlsson, 1984), CASCADE (VanLehn, Jones, & Chi, 1991), STEPS (Ur & VanLehn, 1995), and SimStudent (Li et al., 2013).\nSpecifically, I use the Trestle model,\u00b9 which provides an account for how skills are incrementally acquired from a mixed combination of worked examples and correctness feedback. When presented with a problem, such as the fraction arithmetic problem shown in Figure 1, the model matches previously learned skills against the current state. If any skills match, it executes the one with the highest utility to generate a step. If no skills match, which is common early in learning, then the model requests a demonstration from the tutor. For example, in Figure 1, the tutor might demonstrate placing a 6 in the lower left denominator conversion box. The model, equipped with basic arithmetic primitives for adding, subtracting, multiplying, and dividing, searches for a sequence of mental operations (i.e., a procedure) that explains this demonstration. It might identify that multiplying the two given fraction denominators produces the demonstrated value. The agent then generalizes this procedure, removing specific values to form a reusable skill. It then applies separate learning mechanisms to identify the conditions for applying the skill and to update its utility. When the agent uses this skill on subsequent problems and receives feedback from the tutor, it further refines the skill's conditions and utility."}, {"title": "Study 1: Fraction Arithmetic Tutor", "content": "To evaluate this model's ability to predict human learning outcomes, I used data from the Fraction Addition and Multiplication dataset accessed via DataShop (Koedinger et al., 2010).2 Patel, Liu, and Koedinger (2016) collected these data during an experiment to examine whether blocking or interleaving fraction arithmetic problems leads to better learning. The dataset includes tutor data from 79 students solving 24 fraction addition problems (ten with same denominators and 14 with different denominators) and 24 fraction multiplication problems using the tutor interface shown in Figure 1. Students had to check the \"I need to convert these fractions before solving\u201d box to reveal the conversion fields. For fraction addition requiring conversion, only the cross multiplication strategy (multiplying denominators) was accepted; alternative strategies were marked incorrect. After training, students took a posttest within the tutor, which consisted of four fraction multiplication and four fraction addition problems (two with same denominators and two with different). No hints or feedback were provided during the posttest.\nStudents were randomly assigned to one of two conditions. In the blocked condition, problems were presented in three sequential blocks: fraction addition (same denominators), fraction addition (different denominators), and finally, fraction multiplication. The order of the problems within each block was randomized. In the interleaved condition, problems were presented in a fully randomized order. This experiment aimed to test whether interleaving enhances learning compared to the blocking approach recommended by the Common Core State Standards. Patel et al. (2016) found that students in the blocked condition performed better during training, while those in the interleaved condition performed better on the posttest. This suggests that although interleaved practice results in errors during training, it ultimately leads to greater long-term learning. The goal of this study is to determine whether the computational model can accurately predict this main experimental effect."}, {"title": "Simulation and Analysis Methods", "content": "To simulate human behavior, I created an instance of the model (an agent) for each student and connected it to the machine-readable version of the tutor, shown on the right in Figure 1. This isomorphic tutor, developed using Cognitive Tutor Authoring Tools (Aleven, McLaren, Sewall, & Koedinger, 2006), provided each agent with the same sequence of problems that its corresponding human student received. Importantly, agents were not constrained to take the same actions as their human counterparts. Thus, a tutor was necessary to provide the agents with appropriate hints and feedback during training, as the log data alone was insufficient for conducting the simulation.\nOne key technical limitation of the isomorphic tutor was that it did not support hidden fields. In the human tutor, the fields for converting fractions remained hidden until the student checked the \"I need to convert these fractions before solving\" box, whereas in the machine-readable tutor, these fields were always visible. As a result, simulated students could make errors such as attempting to convert fractions before selecting the conversion box-that were impossible in the human tutor. However, preliminary simulations showed that agents rarely made such errors.\nAfter training, the human students took a posttest to assess their fraction arithmetic knowledge. This assessment was administered directly within the tutor with hints and correctness feedback disabled. Students could enter any values in the text"}, {"title": "Results", "content": "The overall tutor and posttest\nperformance for both humans and agents are shown in Figure 2. The results of the regression analysis are shown in Table 1. Mirroring the prior findings of Patel et al. (2016), \u0406\nfind that while humans have lower tutor performance in the\ninterleaved condition (odds ratio: 0.50, p < 0.05), they have\nhigher posttest performance (odds ratio: 2.35, p < 0.05). My\nanalyses shows that agents exhibit this same effect. Agents in\nthe interleaved condition have lower tutor performance (odds\nratio: 0.40, p < 0.05), but higher posttest performance (odds\nratio: 7.75, p < 0.05). Beyond the main effect of condition, the model exhibits most of the other effects, and direction of\neffects, that are present in the human data. There are two\nexceptions: the effect of practice on the add same problem\nperformance and the intercept on the posttest. However, in\nboth situations either the human or model regression is not\nsignificant, suggesting that more data is needed."}, {"title": "Learning Curves", "content": "I plotted the errors on each problem, averaging across students within condition. The resulting learning curves are shown in Figure 3. Unlike models that generate predictions by fitting a function to the student data, such as the Additive Factors Model (Cen, Koedinger, & Junker, 2006; MacLellan, Liu, & Koedinger, 2015), these learning curves are parameter-free (Weitekamp, Harpstead, Rachatasumrit, Maclellan, & Koedinger, 2019) predictions based solely on task structure. There is a large difference between the models and humans on earlier opportunities. This suggests many students have prior fraction knowledge\u2014their average first problem error was 53.8%. In contrast, the agents have zero prior fraction knowledge-they have an average first problem error of 100%. Despite this difference, the model does surprisingly well at qualitatively capturing the main effects. For example, both humans and agents exhibit high error rates when transitioning from one problem type to another in the blocked condition and have similar asymptotic error."}, {"title": "Discussion", "content": "These results show that the model can successfully predict the main experimental effects of a fraction arithmetic problem ordering manipulation. They also show that the model can generate reasonable parameter-free learning curve predictions. These results are a clear example of how tutor A/B experiment results can be predicted in a completely theory-driven way using a computational model of learning.\nOne caveat is that the model only qualitatively predicts the experimental effects. It does not accurately predict the absolute tutor and posttest scores for each student (or their average) because it does not currently account for prior fractions knowledge. This is particularly noticeable in earlier practice opportunities, where the model predicts that human performance should be much worse than what is observed. As previously mentioned, these differences are due to prior fraction arithmetic skills that students bring to the tutoring system, which are not accounted for in the current model. The model is essentially predicting what human performance would look like if the students did not have any prior fraction arithmetic skills. While these exaggerated error rates might be useful for detecting transitions between skills, such as when using learning curve analysis to develop knowledge-component models (Corbett & Anderson, 1995), they also suggest an opportunity for improvement.\nSome researchers have started to explore different ways to account for this knowledge. Weitekamp et al. (2019) propose statistically estimating how much previous practice each student has had with each type of problem and pretraining agents on a comparable number of problems to initialize prior knowledge. Alternatively, MacLellan et al. (2023) suggest iteratively adjusting each agent's prior fraction knowledge to minimize the differences between agent and human learning trajectories. Both approaches require human data, limiting their use when testing novel designs; more research is needed to better account for prior knowledge in this case."}, {"title": "Study 2: Box and Arrows Tutor", "content": "As a second test of whether a computational model can predict the results of a human experiment, I used the Box and Arrow Tutor Data (Turk Study) dataset accessed via DataShop.3\nLee et al. (2015) collected these data from Mechanical Turk as part of an experiment to investigate how different instructional choices effect student learning of problem-solving rules. This experiment used the box and arrows tutor shown"}, {"title": "Simulation and Analysis Method", "content": "I used the same simulation procedure as study 1, creating an agent for each student and connecting it to a machine-readable version of the box and arrows tutor, shown in Figure 4 to simulate learning. An analysis of the human data showed that the majority of human students solved the easy problems correctly on the first attempt. In contrast, only a few students got the hard problems correct on the first try because they are specifically designed to prevent prior knowledge use. Thus, for my analysis, I trained the agents on all the problems, but only analyzed their performance on the hard problems, because my model does not take into account prior"}, {"title": "Result", "content": "Similar to the\nfractions study, it shows the model and human performance\nare qualitatively similar. I find that Lee et al. (2015)'s findings\napply to just the hard problems: students in the unconstrained\ncondition are less likely to be correct (odds ratio: 0.17, 95%\nCI: [0.08, 0.37], p < 0.05). I find that agents exhibit the\nsame effect: those trained with unconstrained problems are\nalso less likely to be correct (odds ratio: 0.46, 95% CI: [0.37,\n0.56], p < 0.05).\nThis task is also uniquely well suited for comparing the\nquantitative performance between agents and humans. In\ncontrast to the fractions task, where humans could apply\nprior knowledge, this task was intentionally designed to in-\nhibit prior knowledge use. Thus, the agents and humans\nstart with the same initial conditions. Figure 5 shows that\nhumans have a constrained accuracy of 19.1% (SD = 0.39)\nand agents have a similar constrained accuracy of 19.6%\n(SD = 0.40). Also, humans have an unconstrained accuracy\nof 8.8% (SD = 0.28) while agents have an unconstrained ac-\ncuracy of 10% (SD = 0.30)."}, {"title": "Learning Curves", "content": "shows the learning trajectories\nof the agents and humans, averaging error across students\nwithin each condition at each problem. In contrast to the\nfractions results, we see a much shallower rates of learning\nfor both the agents and humans, suggesting that the model\nis able to capture general trends in learning (rapid learning\nin fractions, but more limited learning here). This is notable\nbecause the model is not trained on or fit to the human data;\nthese predictions are generated based entirely on the struc-\nture of the task and the sequence of the items. They could\nhave been generated prior to collecting any human data."}, {"title": "Discussion", "content": "These findings provide a second demonstration of the model\naccurately predicting the main effect of a human experiment.\nThey also showcase again its ability to generate learning\ncurves that reasonably align with human learning trajecto-\nries. Although the study 1 and 2 tasks are both math related,\nthey employ different instructional manipulations-a prob-\nlem ordering manipulation and an item design manipulation.\nThis provides some evidence for the model's general ability to\nguide instructional design for varying kinds of interventions.\nThe model can also facilitate testing of different learning\ntheories and hypotheses. My model directly instantiates Lee\net al. (2015)'s theory of learning as search through a space of\nrules; its ability to generate data that matches the human data\nprovides additional evidence in support of this theory. Sur-\nprisingly, however, the results provide evidence against Lee et\nal. (2015)'s hypothesis that constrained problems aid learning\nby making the correct procedures easier to compute. Comput-\ning with whole or fractional numbers is equally difficult for\nagents, but they still exhibits the main effect, so constrained\nproblems must aid agent learning in another way.\nI hypothesize that the benefit of constrained problems is\nless about ease of computation and more about lower pro-\ncedural ambiguity. To satisfy the property that only correct\nprocedures yield whole-number solutions, constrained prob-\nlems often have only a single (correct) candidate procedure\nthat is consistent with the correct answer. In contrast, uncon-\nstrained problems often have multiple candidate procedures\n(some incorrect) that are consistent. As a result, both the\nmodel and humans are more likely to select the correct proce-\ndure on constrained problems, which aids their learning and\nperformance. More research is needed to test this hypothesis,\nbut if true, it would suggest a distinctively different strategy\nto designing problems to promote learning-items should be\ndesigned so there is only a single (correct) candidate proce-\ndure that is consistent with the correct answer."}, {"title": "Conclusion", "content": "To my knowledge, this paper presents the first evidence that a\ncomputational model of learning can successfully predict the\nmain effects observed in multiple human A/B experiments.\nIt also demonstrates that this models can generate reasonable\npredictions of human learning trajectories and offer theoreti-\ncal insights into the effectiveness of specific instructional in-\nterventions. These findings suggest that computational mod-\nels of learning could operate as Model Human Learners, sup-\nporting instructional designers in evaluating and identifying\nthe most pedagogically effective instructional designs. More\nresearch is needed to expand the Model Human Learner con-\ncept to additional tasks and interventions, but this work repre-\nsents a preliminary proof of concept that lays the groundwork\nfor a broader research program."}]}