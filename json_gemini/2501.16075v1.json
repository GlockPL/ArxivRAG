{"title": "PISCO: Pretty Simple Compression for Retrieval-Augmented Generation", "authors": ["Maxime Louis", "Herv\u00e9 D\u00e9jean", "St\u00e9phane Clinchant"], "abstract": "Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models (LLMs) by retrieving relevant documents, but they face scalability issues due to high inference costs and limited context size. Document compression is a practical solution, but current soft compression methods suffer from accuracy losses and require extensive pretraining. In this paper, we introduce PISCO\u00b9, a novel method that achieves a 16x compression rate with minimal accuracy loss (0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing approaches, PISCO requires no pretraining or annotated data, relying solely on sequence-level knowledge distillation from document-based questions. With the ability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers a highly efficient and scalable solution. We present comprehensive experiments showing that PISCO outperforms existing compression models by 8% in accuracy.", "sections": [{"title": "1. Introduction", "content": "Retrieval-Augmented Generation (RAG) [1, 9, 22] pipelines have become a crucial component in address-ing various natural language tasks. By incorporating documents retrieved from a selected collection, RAG en-hances Large Language Models (LLMs) enabling them to provide more accurate, current, and domain-specific responses.\n\nThe primary drawback is the increased inference cost, which scales quadratically with the number of tokens and, consequently, with the number of retrieved doc-uments when using transformer-based architectures. In addition to inference costs, the limitations on LLM context size restrict the number of documents\u2014and thus the amount of information\u2014that can be utilized. This constrains the potential scaling of inference time [44].\n\nCompressing documents is a practical way to reduce the computational burden of processing large contexts. Hard compression techniques focus on altering the sur-face structure of the documents, such as by pruning [5,23,29,38] or summarization [40]. These methods are easily interpretable and can typically be applied to any LLM without requiring modifications. However, the compression rate is limited by the amount of infor-mation that can be effectively conveyed through text tokens, usually achieving a reduction of 2x-5x.\n\nSoft compression techniques aim to condense docu-ments into vector representations [39]. They may also involve attention key-value pairs that the model at-tends to during generation, either through self-attention [2,32] or dedicated cross-attention mechanisms [43]. These methods trade off interpretability for efficiency, achieving higher compression rates while maintaining some performance levels. Most existing soft compres-sion approaches for RAG follow a similar pipeline [2,3, 7,24,31,33,43]. Typically, a pretraining task (such as auto-encoding and/or causal language modeling) on unlabeled data is used to train an initial compressor, followed by fine-tuning for RAG question answering (QA) to optimize the embeddings for QA tasks.\n\nCurrently, all existing soft methods experience signifi-"}, {"title": "2. Related Work", "content": "Table 1 compares key methods for soft compression, which, although not always explicitly designed for this purpose, can be applied in RAG (retrieval-augmented generation) applications. [37] presents a more thor-ough survey or introduction to context compression."}, {"title": "2.1. Dealing with long contexts via compres-sion", "content": "In [3], the authors present the Autocompressor, a recur-sive context compression method trained on a language modeling task. By appending compression tokens to the context and extracting the hidden states, this ap-proach supports longer contexts and can be applied to document compression in RAG-QA. The in-context auto-encoder (ICAE) [7] simplifies this by freezing the de-coder, removing recursion, and pretraining through doc-ument auto-encoding. In [43], multiple contexts are en-coded in parallel, with cross-attention layers introduced"}, {"title": "2.2. Compression specific to RAG-QA", "content": "In [32], the authors specifically address the RAG-QA problem. After large-scale auto-encoding and language modeling pretraining, they fine-tune their decoder mod-els to handle multiple documents simultaneously. Al-though this approach enhances the usability and perfor-mance of the RAG pipeline, there remains a significant performance drop (~8%) between uncompressed and x16 compressed models. The x500 Compressor [24] is similar to COCOM except that the document embed-dings consist directly of the K/V values obtained on the memory tokens during forward pass. This saves decod-ing computations but substantially increases the storage size of the embeddings. The xRAG method [2] pro-poses leveraging existing document embeddings-such as those used for retrieval\u2014to reduce the storage and computational costs of generating additional embed-dings. To achieve this, they train a small adapter to map the retrieval embeddings into the input space of a frozen decoder LLM. Similar to [43], xRAG also utilizes token-level distillation for fine-tuning in QA tasks.\n\nAll current compression methods [2, 3, 7, 24, 31, 33, 43] rely on large-scale pretraining tasks and require annotated labels. Despite their advancements, these methods still fall short of achieving the QA performance of their uncompressed LLM backbones (see 2)."}, {"title": "3. Methods", "content": ""}, {"title": "3.1. Retrieval-Augmented Generation", "content": "In RAG, each query q is augmented with a set of rele-vant documents $(d_1, d_2, ..., d_k)$ retrieved from a large database of documents D. For improved performance, this process typically involves two steps: first, a re-triever identifies an initial pool of relevant documents, and then a re-ranker refines and prioritizes the k most relevant ones. The final response r is generated by prompting an language model F with both the query and the set of retrieved documents.\n\nIn general, the accuracy of the generated response im-proves as the number of documents increases. However, since documents tend to be longer than queries and the computational complexity of transformer-based models scales quadratically with the context length, this can make generation computationally expensive and cause delays. A soft compression model addresses this by mapping each document $d_i$ into a shorter set of embeddings or a key-value (K/V) cache, $c_i$. The generation process is then conditioned on these compressed repre-sentations: $r \\sim F(. | q, c_1, c_2, ..., c_k)$."}, {"title": "3.2. PISCO", "content": "PISCO adopts a standard architecture, involving a com-pressor and decoder model, detailed in the following section. The main difference lies in its training task. The method is described on Figure 2.\n\nCompression is performed following the approach in [3,7,33], utilizing the language model F with LoRA [13] adapters $\\theta_c$. Specifically, a set of l memory tokens $(m_1, ..., m_l)$ is appended to each document $d_i$: the cor-responding prompt $(d_i; m_1, ..., m_l)$ is passed through $F_{\\theta_c}$. The l final hidden states, corresponding to the memory tokens, are extracted to form the document"}, {"title": "Distillation objective", "content": "While the architecture of PISCO is similar to existing approaches, its training principle is fundamentally different. The motivation for using a distillation approach stems from an invari-ance principle: language models should give the same answers whether their input is compressed or not. To achieve this, we propose to use Sequence-level Knowl-edge Distillation (SKD) [18]: generating labels with a teacher model rather than token-level distillation based on existing labels as done in previous works.\n\nSpecifically, given a query q, let $a_1, ..., a_r$ represent the tokens generated by the teacher based on the docu-ments and query and $a_{<i} = (a_1, ..., a_{i-1})$:\n\n$a_i \\sim T(. | d_1, ..., d_k, q, a_{<i})$.\n\nThe training objective on the parameters $\\theta_c$ and $\\theta_d$ is the cross-entropy loss computed on the decoder condi-tioned on the compressed documents and the query:\n\n$c_i = (c_i)_{s=1,...,l} = F_{\\theta_c} (d_i, m_1, ..., m_l)$\n\n$\\mathcal{L}(\\theta_c, \\theta_d) = - \\sum_{i=1}^{r} log F_{\\theta_d} (a_i | q, c_1, ..., c_k, a_{<i})$\n\nFurther details on this process are provided in Appendix A. Note that the teacher-generated labels can be pre-computed and re-used across different training runs.\n\nNote that in xRAG [2], the authors minimize the Kullback-Leibler (KL) divergence between the logits of the teacher and student models, with both models being teacher-forced on a reference answer. Similarly,"}, {"title": "4. Experiments", "content": "Our experiments aim to measure the performance of PISCO models \u00a74.2, then to analyse the importance of training data, distillation and pretraining \u00a74.3. Further-more, we evaluate PISCO models generalization \u00a74.4 to out-of-domain, multilingual data and large number of documents. Finally, we investigate how information is stored within the document embeddings \u00a74.5."}, {"title": "4.1. Experimental details", "content": "We run experiments using Mistral-7B-instruct [15]\u00b3, LLama-3.1-8B-instruct \u2074 and SOLAR-10.7B-Instruct \u2075 as different backbones for PISCO. Our training set of questions \u2076 is taken from [33]: it consists of 453k ques-tions based on documents from Wikipedia-KILT [30] which we preprocess in chunks of 128 tokens and use as our database collection \u2077. For each question, we search the first top-k documents and feed them to a teacher LLM to obtain the silver label used for distillation. Dur-ing training, the number of retrieved documents k is set to 5. Each document is compressed into l embed-ding vectors where l is fixed for each PISCO model. PISCO models with compression rate 16 use 8 memory embeddings per document.\n\nAll experiment details, including the choice of the re-triever, reranker and prompts are provided in Appendix C and Appendix B. Trainings and evaluations were per-formed using the Bergen [32] library."}, {"title": "4.2. Main results", "content": "After training, we first evaluate the PISCO models on general knowledge QA tasks: Natural Questions [20], TriviaQA [17], HotpotQA [42], ASQA [36], and PopQA [26] datasets. Our main evaluation metric is the accu-racy -also called match in the QA context- which we define as 1 if the normalized label is found within the normalized prediction and 0 otherwise, as detailed in Appendix J."}, {"title": "Computational efficiency", "content": "For Mistral-7B and PISCO-Mistral, we measure FLOPS, maximum batch size and inference time on an A100 gpu. We perform these mea-sures on a 128-token query and 5 128-token documents, forcing the generation of a 32-token answer. Results"}, {"title": "4.3. Analysis of Training Data and Tasks for Compression", "content": "In this section, we aim to give evidences justifying the design choices of the PISCO approach."}, {"title": "Pretraining has little benefits", "content": "A potential way to im-prove compression models was by improving pretrain-ing with new or refined pretraining tasks. To explore this, we conducted experiments using pretraining on 10B tokens extracted from FineWeb\u00ae with a variety of tasks. These tasks included auto-encoding [7,33], text continuation from compressed representations [2,33], and a novel task of text continuation from a sequence of keywords within a compressed segment\u2014enabling access to information embedded in the learned rep-resentations, aimed at mitigating a potential \"lost-in-the-middle\" effect. Additionally, we tested continua-tion from multiple documents, where the model was prompted to continue text either from within or follow-ing a designated document among several compressed ones (see Appendix I). Figure 4 illustrates that, across all preliminary experiments, there is a weak correla-tion between success in pretraining tasks and perfor-mance in QA. Notably, training on auto-encoding often achieves near-perfect Rouge-L scores (>0.99) without any significant improvement in QA performance.\n\nTo analyze in detail the impact of the adopted fine-tuning strategy, we ran experiments with variable num-ber of fine-tuning samples. We compare performances when fine-tuning is applied to a pretrained model or from scratch. Results are shown on Figure 5. Pretrain-ing benefits final performances for low fine-tuning sam-ple size, but it is not useful at 450k samples."}, {"title": "Needle-in-a-Haystack analysis", "content": "Secondly, we ana-lyzed the different model behaviors on a needle-in-a-haystack test [8], an accessible proxy for RAG-QA that effectively measures the model's retrieval and lo-calization capabilities, crucial for accurate question-answering on large datasets. Interestingly, while pretraining enables some success on the needle-in-a-haystack task, fine-tuning on the raw labels used in [2, 33] diminishes this capability, as illustrated in Figure 6. This result highlighted the need for a better fine-tuning approach. Higher-quality labels from a teacher LLM emerged as a promising solution: providing more informative signals during fine-tuning [12,34]. Early experiments shown on Figure 6 and main results shown on Table 2 indeed confirm the benefits."}, {"title": "Impact of Teacher and Labels Quality", "content": "To under-stand the impact of the teacher LLM we train PISCO models with varying teachers and without distillation. Average accuracy in general domain QA for each ob-tained model are shown on Figure 7. Interestingly, the best-performing teachers are generally Mistral-7B or Solar-10B models, not necessarily the stronger teach-ers, as found in [41]. A manual analysis of the labels for each teacher suggests that these models often in-clude justifications for their answers based on the given"}, {"title": "4.4. Generalization Evaluation", "content": "Increasing the number of documents. To evaluate the PISCO models' ability to handle a large volume of documents, we conduct inference on PISCO - Solar with document sets ranging from 1 to 50 on NQ and HotpotQA. Results are presented in Figure 8. For com-parison, we include the base SOLAR model, which can process only up to about 20 documents due to its 4096-token context length limit. PISCO's performance aligns closely with the base model, with a gradual decrease beyond 20 documents, a common trend in RAG models [16], which could be addressed by increasing k during fine-tuning (all of our experiments train with k = 5 documents)."}, {"title": "Out-of-domain", "content": "We evaluate whether PISCO models generalize to unseen domains using a diverse set of datasets (details in Table 7 in the appendix). The results are shown in Table 4. Results in Table 4 show that PISCO models generalize nearly as effectively as their backbone decoders, with some performance drops (-3-10%) on the ParaphraseRC task. This indicates robust compression capabilities, showing PISCO models do not rely on memorizing the general knowledge in the KILT collection."}, {"title": "Multilinguality", "content": "We evaluate whether PISCO models generalize to unseen languages using the MKQA dataset [25]. The experiments use the bge-m3 retriever and the recall-3gram metric, more resilient to language varia-tion [4]. We choose a latin language (french) as well as Korean and Russian. Note that the PISCO backbones Llama and Mistral are not strong multilingual models, but these experiments serve mostly to analyze the com-pression behavior. Results are shown on Table 4 (right). PISCO models seem to generalize fairly well to other languages, with still a small drop compared to their backbones. Further analysis is needed to determine whether the drop is due to compression or language generation limitations."}, {"title": "4.5. Document embeddings analysis", "content": "To better understand how compression works, we com-pute, on a set of documents, the cosine similarity be-tween the l embeddings and each document token. Interestingly, Figure 9 shows there is a spatial special-ization of the embeddings, each attending preferably to some part of the text. This specialization does not occur when the decoder is kept frozen. Then, we used"}, {"title": "5. Conclusion", "content": "We proposed PISCO, the first compression method for RAG which enables large compression rates with little to no accuracy loss. Our analysis and ablations revealed the ineffective transfer between pretraining and the question answering task for compression models. We also showed the importance of training labels for com-pression: using an appropriate teacher LLM for distil-lation is key. Given the strong evidences of robustness and accuracy, PISCO models may be used as drop-in replacement for existing RAG systems currently relying on their uncompressed backbones. Adopting PISCO would reduce inference costs and latency with minimal performance impact."}, {"title": "A. Implementation details", "content": "Both for teacher-label generation and student evalua-tion, generation is done using greedy decoding, limited to a maximum of 128 tokens. Both for training and evaluation, documents are retrieved using Splade-v3 [21] and Debertav3 [11]. They are prompted from most relevant to less relevant according to the rerank-ing scores."}, {"title": "B. Training Hyper-parameters", "content": "Table 5 gives the hyper-parameters we used for training. Note that on top of the LoRA adapters, the embeddings of the memory tokens given as input to the encoder (as in [3,33]) are also optimized, adding an extra $l \\times$ model_hidden_size trainable parameters to each PISCO model (much less than the number of parameters in the adapters)."}, {"title": "C. Main prompt", "content": "Below is the prompt used in our experiments. <DOC> is replaced by the corresponding document compressed embeddings before the generation while <QUESTION> is replaced with the query q. It is formatted as an in-struction prompt to the instruction-tuned models."}, {"title": "D. Effect of prompts on PISCO models", "content": "We evaluate the robustness of PISCO models to prompt variations by testing with modified versions of the prompt shown in C. The results in Table 6 show minimal performance differences, indicating that the models are stable with different prompts and do not overfit to the specific prompt used during training. Notably, Prompt 3 provides no guidance to the model beyond the infor-mation in the documents."}, {"title": "E. Pairwise comparison using gpt-40", "content": "To compare answers generated by different methods in a more precise way that using the accuracy metric, we use gpt-4o with the following prompt, inspired from Alpaca-eval [6]. Evaluations were run using gpt-4o-2024-11-20. \u03a4o limit costs, only a 1000 samples were used for each dataset. Answer positions in the prompt were randomly switched to prevent position bias."}, {"title": "F. Out-of-domain datasets", "content": "Table 7 provides details on the out-of-domain datasets used and the primary evaluation metric for each. We use the F1 score for RobustQA test suites, given the extended format of the reference answers."}, {"title": "G. PISCO with frozen decoder", "content": "Freezing the decoder for compression models is appeal-ing: it would enable to use compressed representations without any major change to the decoding pipeline of an existing system, as in [2]. To that end, we ran fine-tuning (with and without pre-training) of PISCO models with frozen decoder. Table 8 shows the difference in performance is huge. In fact, a look at the loss curves 11 seems to show that fitting only the compressor does not offer nearly enough flexibility for learning."}, {"title": "H. Embeddings analysis", "content": "To better understand how information is compressed within the document embeddings, we apply the logit lens [28] to each embedding. This allows us to identify the top 10 tokens by mapping the embeddings to logits space using the LLM head. An example of the results is provided in Table 9. Most top tokens correspond or are close to some token in the compressed text. We also recover the spatial specialization shown on Figure 9."}, {"title": "I. Attempted pretraining tasks", "content": "As described in \u00a74.3, our initial approach focused on designing more complex pretraining tasks. Pretrain-ing followed the lines and configurations of [33]. The pretraining task we implemented consisted in:\n\n\u2022 Auto-encoding (AE): the compressed representa-tion of a single document as well as a task-specific token <AE> is prompted to the decoder during pretraining: the labels is the plain text document.\n\n\u2022 Text Continuation (TC): as for general LM training, this task prompts the decoder with the compressed representation of the document and its task is to generate the following text.\n\n\u2022 Keyword-Based Text Continuation (KBTC): A po-tential concern -especially since auto-encoding works so well even with high compression rates\u2013 was that accessing information within the mid-dle of texts while working on their compressed"}]}