{"title": "Evaluating Evidential Reliability In Pattern Recognition Based On Intuitionistic Fuzzy Sets", "authors": ["Juntao Xu", "Tianxiang Zhan", "Yong Deng"], "abstract": "Determining the reliability of evidence sources is a crucial topic in Dempster-Shafer theory (DST). Previous approaches have addressed high conflicts between evidence sources using discounting methods, but these methods may not ensure the high efficiency of classification models. In this paper, we consider the combination of DS theory and Intuitionistic Fuzzy Sets (IFS) and propose an algorithm for quantifying the reliability of evidence sources, called Fuzzy Reliability Index (FRI). The FRI algorithm is based on decision quantification rules derived from IFS, defining the contribution of different BPAs to correct decisions and deriving the evidential reliability from these contributions. The proposed method effectively enhances the rationality of reliability estimation for evidence sources, making it particularly suitable for classification decision problems in complex scenarios. Subsequent comparisons with DST-based algorithms and classical machine learning algorithms demonstrate the superiority and generalizability of the FRI algorithm. The FRI algorithm provides a new perspective for future decision probability conversion and reliability analysis of evidence sources.", "sections": [{"title": "1. Introduction", "content": "Evidence theory, also referred to as Dempster-Shafer theory (DST) [1, 2], has been extensively applied across multiple fields, including decision-making [3, 4], fault diagnosis [5, 6], clustering [7, 8] and pattern recognition [9, 10]. The ability of DST to effectively manage uncertain, imprecise, and conflicting information [11, 12] has made it a favored tool among both researchers and practitioners. Consequently, it has been further developed into various areas, such as complex evidence theory [13, 14] and generalized quantum evidence theory [15]. In recent studies, advancements in evidence theory have been linked with the exploration of information fractal dimensions to assess the complexity of mass functions [16, 17], as well as new entropy measures such as Deng entropy [18], generalized information entropy [17], Tsallis extropy [19] and Deng extropy [20].\nHowever, despite its advantages, DST has several notable limitations. One of the primary issues is its tendency to produce counter-intuitive results [21], particularly when dealing with highly conflicting evidence [22, 23], as highlighted by the Zadeh paradox [24]. This paradox illustrates how the combination of seemingly reliable evidence can lead to absurd conclusions, challenging the robustness and reliability of DST in practical applications. The robustness of DST is another concern, as it often struggles to maintain consistent performance in the face of varying data quality and conflicting sources. Researchers have identified that the reliability of the body of evidence (BoE) can significantly impact the outcomes of DST-based analyses [25, 26]. As a result, numerous studies have focused on developing methods to improve the robustness and reliability of DST by addressing the quality and reliability of evidence sources. Historically, scholars have approached these challenges from various angles. Early works by Dempster and Shafer laid the foundation for DST, providing a formal framework for combining evidence from different sources [1, 2]. Later, researchers like Yager and Smets proposed modifications to the combination rules to handle conflicting evidence better [27, 28]. Yager introduced an alternative combination rule that accommodates conflict by redistributing it among the evidence, while Smets developed an unnormalized combination rule to preserve certain desirable properties. Further con-"}, {"title": "2. Preliminaries", "content": "The challenge of modeling the uncertainty and dynamics of systems [? 40] has garnered significant attention. Dempster-Shafer evidence theory (DST), also referred to as the theory of belief functions, presents an intriguing alternative to conventional probability theory for managing uncertainty. It allows us to assign belief to sets of possibilities, making it incredibly useful when dealing with incomplete or ambiguous information. The following sections will provide an overview of DST."}, {"title": "2.1.1. Basic Conceptions", "content": "The frame of discernment (FOD) is one of the fundamental concepts of DST. Here, represents the exhaustive set of all possible values of a random variable."}, {"title": "(1) Frame of Discernment (FOD) [1]", "content": "= {A1, A2, ..., An} \t\t                                  \t\t\t\t\t\t\t                                                       (1)"}, {"title": "(2) Basic Probability Assignment (BPA) [1]", "content": "For each element in 2\u00ba, the basic probability assignment (BPA) function is a mapping 2\u00ba \u2192 [0, 1], which satisfies the following two conditions:\nm(0) = 0                                                   (3)\n\u03a3(A) = 1\t\t                                                      (4)\n\u0391\u0395\u0398\nLet m be the basic probability assignment (BPA) function on A. For any subset A of, m(A) represents the basic probability mass associated with A. When A Cand A \u2260 0, m(A) reflects the degree of belief in the proposition A. When A = 0, m(A) signifies the uncertainty regarding the distribution of the probability mass. In the belief framework, since the empty set contains no elements, it is not assigned any support, which implies m(0) = 0."}, {"title": "(3) Belief (Bel) and Plausibility (Pl) Function [2]", "content": "Given a FOD \u0398, the belief function (Bel) and the plausibility (Pl) function can be defined as follows:\nBel(A) = \u2211m(B) \t\t \u2200A\u2286\u0398                                      (5)\nBEA\nPl(A) = \u2211 m(B) \t\t \u2200A \u2286 \u0398                                       (6)\nBOA\u22600"}, {"title": "2.1.2. Dempster's Combination Rules", "content": "For two BPAs m\u2081 and m2 defined on FOD, Dempster's Combination Rules [1] are defined as:\n(m1m2) (A) = \u2211 m\u2081(B)m2(C),                               (9)\n\t\t\t\t\t\t\t                                                      1\n                                                            1-K\t B,CCO,BOC=A\nwhere K represents the conflict between m\u2081 and m2:\nK = \u2211\u2211 m\u2081(B)m2(C)                                  (10)\nB,CCO, B\u2229C=0"}, {"title": "2.1.3. Pignistic Probability Transfomation (PPT)", "content": "Pignistic Probability Transformation (PPT) [41] is a method used to apply the BPA framework in decision-making contexts. Its principle involves averaging the uncertainty associated with each proposition across all its elements, assuming no additional external information is available. This ensures fairness in decision-making. Specifically, it can be expressed as:\nBetP(A) =\t  1 m(B)                                 (11)\n                                                            \u03a3 \u0392\u0395\u0398.\u0391\u0395\u0392 B 1-m(0)'\nwhere B is the number of elements in B."}, {"title": "2.1.4. Discounting Rules", "content": "Because the reliability of different sources of evidence varies, it is necessary in the decision-making process to discount them to different extents based on their reliability [42]. If the reliability of a source of evidence is denoted as \u03bb \u2208 [0, 1], one of the classic discounting operations proposed by Shafer [2] is defined as:\nm'(A) = \u03bbm(A), A \u2286 \u0398\n                                                     (12)\nm'(\u0398) = 1 \u2212 \u03bb + \u03bbm(\u0398)\nwhere A represents the evidential reliavility. As the reliability of an evidence decreases, the mass allocated to increases. When the reliability of this evidence is 0, the BPA becomes m(0) = 0, which means all degrees of belief are completely uncertain."}, {"title": "2.2. Atanassov's Intuitionistic Fuzzy Sets (IFS)", "content": "Atanassov's Intuitionistic Fuzzy Sets (IFS), introduced by K. Atanassov [38, 39] in 1986. While fuzzy sets only account for the degree to which an element belongs to a set, IFS additionally quantify the degree of non-belonging and the uncertainty or hesitation regarding the membership. This enhanced framework allows IFS to better handle and represent uncertainty."}, {"title": "2.2.1. Basic conceptions", "content": null}, {"title": "(1) Membership, Non-membership and Hesitation Degree [38, 39]", "content": "Let X = {X1, X2, ..., xn} be a universe of discourse. According to the definition, each A in X can be expressed as:\nA = {(x, \u03bc\u2081(x), v\u2081(x)) | x \u2208 X}\t\t                                 (13)\nwhere \u00b5\u2081(x) : X \u2192 [0,1] and v\u2081(x) : X \u2192 [0, 1] represent the membership and non-membership degrees of x to A, respectively. The relationship between \u00b5A(x) and VA(x) satisfies the following:\n0 \u2264 \u03bc(x) + va(x) \u2264 1\t\t\t\t\t\t\t                                          (14)"}, {"title": "2.2.2. Ranking method of IFVs", "content": "In decision-making problems, the ranking of IFVs is crucial because it determines how to make the most rational judgments when variables have multiple scenarios and IFVs associated with them."}, {"title": "(1) Normal Ranking [39]", "content": "Suppose there are two intuitionistic fuzzy values A = <\u03bc\u0104, VA) and B = <\u03bc\u03b2, VB). It can be easily determined the priority order between A and B in the following scenarios.\n\u03bc\u03b1(x) \u2265 \u03bc\u03b2(x)\t\t\t\t                                                  \nVA(X) \u2264 VB(X) \t\t\t                                                  A > B                                                                          (16)"}, {"title": "(2) Score Function and Accuracy Function [45]", "content": "Given a IFV A = <\u00b5A, VA). Then the score function S and accuracy function H of A are defined as:\nSA(x) = \u03bc\u2081(x) - VA(X)\t\t\t\t\t\t                                (17)\nHA(x) = \u03bc\u2081(x) + va(x) = 1 \u2212 \u043f\u0430(\u0445)                                 (18)\nThe score function SA(x) can be understood as the degree of support for determining x belongs to the IFS-A. The larger SA(x), the greater the support for x belonging to A. The accuracy function reflects the degree of certainty in making this determination, which is related to the hesitation degree (\u03c0\u2081(x)) in IFS. The larger HA(x), the greater the certainty that x belongs to IFS-A. When the hesitation degree is 1, HA(x) becomes 0, indicating that the determination is entirely baseless.\nBased on the mentioned score function and accuracy function, when faced with the problem of ranking multiple IFVs, the following rules can be applied.\nLet A = (\u03bc\u0391, VA) and B = <\u03bcB, VB) be two IFVs, and SA(x), HA(x), SB(x), HB(\u0445) represent their score functions and accuracy functions respectively, in order to get their rankings in decision-making problem.\n(1) if SA(x) > SB(x), then x is more likely to belong to A, denoted as A > B.\n(2) if S A(x) = S B(x), then\n(i) if HA(x) > HB(x), x is more likely to belong to A, denoted as A > B.\n(ii) if HA(x) < HB(x), x is more likely to belong to B, denoted as A < B.\n(iii) if HA(x) < HB(x), A and B have the same likelihood, denoted as A = B."}, {"title": "2.3. Triangular Fuzzy Numbers (TFN)", "content": "Triangular Fuzzy Numbers [46] are an effective tool for representing and handling uncertainty. TFNs are represented by a triplet (a, b, c), where a \u2264 b \u2264 c correspond to"}, {"title": "3. Proposed Method: Fuzzy Reliability Index Based on IFS", "content": "In the following method, we assess the evidential reliability based on the similarities between DST and IFS, which is called Fuzzy Reliability Index (FRI). By leveraging the strengths of both frameworks, we can estimate the reliability of evidence more effectively."}, {"title": "3.1. BPAs generated from TFNs", "content": "The generation of BPAs is achieved through triangular fuzzy numbers, following the strategy outlined below:\n1. When a sample intersects the TFN model for a proposition, the y-coordinate of the intersection is recorded to represent the degree of membership supporting that proposition.\n2. When a sample intersects multiple TFN models for different propositions, the highest y-coordinate is recorded to represent the degree of membership supporting that proposition, while the lowest y-coordinate is used to represent the membership degree for the proposition encompassing multiple subsets.\n3. When a sample is within the TFN models of multiple propositions, the highest y-coordinate is recorded as the membership degree supporting each single proposition,"}, {"title": "Example 1. Assume there is an object with three possible classes A, B, and C. Their triangular fuzzy numbers are A = (1,2,4), B = (0,1,3), and C = (1,3,5), respectively.", "content": "\u03bc\u03bb(x1) = 0.5 \u03bc\u03b2(x1) = 0 \u03bc\u03b5(x1) = 0\t                                (20)\nThen,\nm(A) =\t0.5\t = 1 m(B) =\t0\t = 0 m(C) = 0 (21)\n  0.5 +0 +0 0.5 +0 +0 0.5 + 0 + 0"}, {"title": "3.2. Similarities between DST and IFS", "content": "In 2.1.1 (3), Bel(A) and Pl(A) respectively represent the degree to which A is supported and the degree to which A is not contradicted. In 2.2.1 (1), \u03bc\u2081(x) and VA(x) indicate the degree of x belonging and not belonging to A.\nIn decision-making problems, when assessing the relationship between x and A, a higher degree of support for A implies that x is more likely to belong to A. Conversely, if A is highly contradicted, it indicates a greater non-membership degree of x to A. Therefore, due to the similarity in objectives, DST and IFS can be connected, leading to the following relationships:\nBel(A) = \u03bc\u2081(x)\t\t\t\t\t\t                                (29)"}, {"title": "3.3. Explanation of Representation Methods in Classification Problems", "content": "For classification problems, it is essential to first clarify certain notations before introducing the methods.\n\u2022 = {A1, A2, A3 Ak} represents the frame of discernment (FOD), indicating the different types that a variable may take.\n\u2022 O = {01, 02,..., On} represents the objects being evaluated.\n\u2022 S = {S1, S2, . . ., S j} represents a set of evidence sources, which in classification problems could correspond to different features of an object.\n\u2022 mj {On} (Ak) represents the support degree with which evidence source S j judges object On to belong to Ak, expressed in the form of BPA.\n\u2022 m* {On} (A*) represents the true class of object On as A*. Note that in supervised learning processes, the true class of objects is known. Therefore, during training, for VAkebut Ak \u2260 A*, m* = 0."}, {"title": "3.4. Decision Confidence", "content": "In 2.2.2 (2), the score function S is designated to resolve the ranking of different IFVs, and its calculation method is shown in Eq. 18. Similarly, based on the transformation relationships given by Eq.20 and 21, the decision confidence in DST, denoted as:\ndcj{On} (Ak) = jk - Vjk = (Bel + Pl)j{On} (Ak) \u2013 1                                                                                                 (31)\nwhere j indicates different sources and k indicates different classes that object belongs to."}, {"title": "3.5. Correct Decision Contribution", "content": "For an evidence source Sj, there may be different judgments regarding the classification of object On and the corresponding decision confidence. The contribution of this evidence source to the correct classification result is defined as follows:\nCjn = dc j {On} (A*) \u2013 \u2211\u0391\u0395\u0398-\u0391*} dcj {On} (Ak)                                              (32)\n                                                            |- \u0391* |\nwhere | - A* | denotes the number of elements in {O \u2013 A*}.\nThe correct decision contribution is the sum of positive and negative contributions. If an evidence source incorrectly supports an incorrect result, it increases its negative contribution to the correct decision. Since the final decision will only result in one outcome, to ensure that negative contributions are not overcounted, it is necessary to average the negative contributions across all incorrect classifications."}, {"title": "3.6. Source Reliability", "content": "An evidence source's reliability is defined by its contribution to the correct result. Evaluating each object's contribution {01, 02, ..., On} allows for the calculation of the reliability R; of the evidence source S j:\nRj = \u2211 C ji                                                                                                                                (33)\ni=1"}, {"title": "3.7. Source Weights", "content": "Given the different reliability of evidence sources to correct decisions Rj, their weights in subsequent decision stages are defined as follows:\nWj = Rj - min{R}R\u2208 {R1, R2... Rj                                      (34)\n                                                            max{R} \u2013 min{R}'\nConsidering that evidence sources may have negative reliability towards correct decisions, meaning they strongly support a class other than the correct one, the calculation of evidence source weights is based on relative reliability to avoid negative weights. When an evidence source has higher reliability, it carries more weight in subsequent classification decisions. Conversely, when an evidence source has minimal reliability, its weight is zero, implying it is disregarded entirely in decision-making."}, {"title": "Example 2. Let O\u2081 be an object to be identified and S1, S2, S3, S4 be three evidence sources in a FOD \u2299 = {A, B, C}, and assume m* {On} (B) = 1.", "content": "m\u2081{01}(A) = 0.4 m\u2081{01}(B) = 0.2 m\u2081 {01} (C) = 0.2 m\u2081{01}(B, C) = 0.2                 (35)\nm2{01}(A) = 0.1 m2{01}(B) = 0.5 m2 {01} (C) = 0.1 m2{01}(A, C) = 0.3          (36)\nm3{01}(A) = 0.15 m3{01}(B) = 0.55 m3 {01} (A, C) = 0.2 m3{01}(0) = 0.1     (37)\nm4{01}(A) = 0.3 m4{01}(B) = 0.3 m4 {01} (C) = 0.3 m4{01}(0) = 0.1            (38)\nFirst, compute the Bel and Pl functions for S, representing them as [Bel, Pl]:\nBased on the transformation relationship between DST and IFS, the decision confidence for different evidence sources making different classification decisions can be calculated as:\ndc =\t S1 -0.2 -0.4\t-0.4\n   S2 -0.5   0-0.5\t\t                                                                                   (40)\n   S3\t-0.4  0.2 -0.7\n   S4 -0.3 -0.3 -0.301\nBy comparing with the correct classification result, the classification contributions of the three evidence sources can be determined as:\nC11 = (-0.4) \u2013 (-0.2) + (-0.4)\t = -0.1 (41)\n2\nC21 = 0 (-0.5) + (-0.5)\t = 0.5 (42)\n2\nC31 = 0.2 (-0.4) + (-0.7)\t = 0.75 (43)\n                                                            2"}, {"title": "Example 3. Let 01, 02, 03 be three objects to be identified and S1, S2, S3 be three evidence sources with their decision contribution C1, C2, C3 shown as follows:", "content": "01 02 03\nC1 0.5 0.6 0.3\nC =\t       (45)\nC2\t0.2 -0.25 -0.15\nC3\t0.25 0.4 0.1\nThen,\nC\u2081 = 0.5+0.6+0.3 = 1.4\t\t                                C2 = 0.2+(-0.25)+(-0.15) = \u22120.2 C3 = 0.25+0.4+0.1 = 0.75\t\t   (46)\nAccording to the relative weight calculation method:\nW\u2081 =\t 1.4 \u2013 (-0.2)\t = 1\t\t       W2 =\t -0.2 - (-0.2)\t = 0\t\t      W3 =\t 0.75- (-0.2)\t = 0.59375  (47)\n                                                            1.4-(-0.2) 1.4-(-0.2) 1.4-(-0.2)\nNote that when calculating relative weights, the evidence source with the highest contribution will be fully retained, whereas the source with the lowest contribution will be disregarded, assigning it a weight of 0. This helps prevent decision-making from being influenced by incorrect information sources."}, {"title": "4. Experiment and Analysis", "content": "In recent years, pattern recognition has garnered significant attention due to its applications across various domains such as healthcare, finance, and robotics. This section aims to evaluate the reliability and superiority of the developed model in pattern recognition. The goal is to apply the model to various classification problems and assess its performance rigorously. By comparing its outcomes across different datasets and conditions, this study seeks to provide insights into its effectiveness and suitability for practical applications."}, {"title": "4.1. Datasets", "content": "This experiment incorporates five datasets: Iris, Parkinsons, Connectionist Bench (CB), Fertility and Algerian Forest Fires (AFF), all sourced from the UC Irvine (UCI). The UC Irvine repository is renowned for its comprehensive collection of datasets, which span various domains and serve as standard benchmarks in the field of machine learning. Researchers worldwide rely on these datasets to develop, validate, and compare machine learning algorithms across diverse applications. The information regarding their categories, sample sizes, features, and missing values is provided in Table 1."}, {"title": "4.2. Comparative Models", "content": "To further illustrate the accuracy of the FRI model in classification performance, a series of methods are used for comparison. These methods are generally classified into"}, {"title": "4.2.1. Algorithms based on DS theory", "content": "1. Murphy's method [29]: Murphy, in his study, proposed an idea of averaging the BPAs. By traversing every focal element within the existing evidence framework, he formulated a new evidence framework that encompasses all existing focal elements. The resulting BPA is an averaged structure. In Murphy's approach, the reliability of the evidence sources does not have a direct outcome, but it can be inferred by calculating the similarity between them and other evidence sources. Frequently occurring focal elements often result in a higher degree of BPA, reflecting this concept.\n2. Deng's method [30]: Deng's research incorporates Murphy's averaging concept by also creating a new evidence framework that encompasses the known BPAs. However, Deng's approach to averaging BPAs is not an absolute average but is based on the similarity between evidence sources. Similarity among evidence sources is evaluated through the pignistic distance calculation, which, in theory, underscores the relevance of more dependable evidence sources.\n3. PCA [47]: Principal Component Analysis (PCA) is a widely used technique for reducing dimensionality, aimed at converting high-dimensional datasets into lower dimensions while preserving the most critical features of the data. When applied to evidence analysis, PCA tends to retain evidence sources that capture the core characteristics of the evidence framework, while less relevant evidence sources, which do not align with the primary assessment focus, are given less weight."}, {"title": "4.2.2. Classical Machine Learning Methods", "content": "1. SVM [48]: Support Vector Machines (SVMs) are a class of supervised learning algorithms distinguished by their principle of maximizing the margin between classes. SVMs identify a hyperplane within the feature space that serves to separate different categories, relying heavily on the support vectors, which are the data points closest to the decision boundary. Additionally, SVMs address non-linear classification issues through the use of kernel tricks, which map the data into a higher-dimensional space"}, {"title": "4.3. A Detailed Application Sample in Pattern Classification", "content": "In this section, the proposed FRI method will be applied to the classification of the iris dataset, and its effectiveness will be explained in detail through comparisons with different algorithms."}, {"title": "4.3.1. Dataset Description", "content": "The Iris dataset from the UCI Machine Learning Repository is a well-known dataset in pattern recognition. It consists of 150 samples representing three species of iris flowers: Iris setosa (Se), Iris versicolor (Ve), and Iris virginica (Vi). Each sample includes four attributes: sepal length, sepal width, petal length, and petal width. Detailed information regarding these attributes for the different iris species can be found in Figure 3."}, {"title": "4.3.2. Implementation", "content": "Step 1: Partition the Iris dataset such that 70% of the data is used for training and 30% is allocated for testing.\nStep 2: Iterate through all training samples, generate BPAs through triangular fuzzy numbers, and then fuse them to obtain new BPAS.\nStep 3: Calculate the reliability among different evidence sources according to the proposed method.\nStep 4: Process the test set by applying reliability-based discounting to the evidence sources, then determine the classification accuracy after this processing step."}, {"title": "4.3.3. Discussion", "content": "The maximum, average, and minimum values for different features of each Iris species are shown in Table 2. Based on this, corresponding triangular fuzzy numbers and membership function graphs can be generated, as shown in Figure 4.\nFrom Figure 2, it can be observed that the similarity between the three Iris species varies for different features. In terms of petal length (Fig. 2(c)) and petal width (Fig. 2(d)), the differences between the three Iris species are relatively pronounced, resulting in higher recognizability. Conversely, for sepal length (Fig. 2(a)) and sepal width (Fig. 2(b)), the similarity among the species is higher, which reduces their distinguishability.\nDuring the training phase, since classical DST, Murphy, and Deng's methods do not directly define the reliability of evidence sources, the proposed FRI and PCA methods will be compared regarding evidence source reliability. Note that the reliability calculated by the proposed FRI is based on the contribution to the correct result, whereas PCA is derived solely from the similarity between evidence sources. The reliability of the features calculated by both methods is shown in Table 3. It can be observed that there are differences between PCA and FRI: for Petal Length (PL) and Petal Width"}, {"title": "4.4. Other Application Samples", "content": "In this section, the generalizability of FRI across different datasets will be discussed. Since the previous section covered the specific processing and operations of FRI, this section will directly present results to compare FRI with other DS-based methods and some classical machine learning algorithms. This comparison aims to demonstrate the reliability and generalizability of FRI in classification tasks.\nThe datasets and algorithms used for this comparison have been introduced in Section 4.2. The performance of different algorithms on these datasets is shown in Figure 4, with their specific accuracies detailed in Table 6.\nThe box plot shows that the FRI algorithm generally performs the best, with a median accuracy close to 0.90 and a relatively concentrated accuracy distribution, indicating consistently high accuracy in most experiments. Additionally, the high lower bound of accuracy without significant outliers suggests good stability of the algorithm. The average accuracy in Table 6 further demonstrates the superiority of FRI. Murphy and Deng's methods exhibit outstanding upper limit performance on some datasets but have a broader distribution range. Among classical machine learning models, SVM displays the best stability."}, {"title": "4.5. Discussion", "content": "The superior stability and accuracy of the FRI can be attributed to the following factors:\n1. The reliability of evidence sources in FRI is determined by a result-oriented approach rather than based on the similarity between evidence sources. This means that evidence sources that contribute positively to correct decisions are given higher reliability, while those with minimal or negative contributions are assigned lower reliability. This approach is more direct and efficient, eliminating the influence of highly similar but counterproductive evidence sources on correct decision-making.\n2. The contribution based on intuitionistic fuzzy set is closely related to decision-making. This contribution not only comes from the BPA of correct decisions but is also related to the BPA of incorrect decisions. The interpretation of negative contributions makes this theory more comprehensive.\n3. The connection with intuitionistic fuzzy set. The method mentioned in this experiment refers to the intuitionistic fuzzy set's definition in decision-making, which distinguishes it from other methods such as PPT. This provides a new perspective for decision-making in the DS theory and offers new ideas for future developments in DS-based decision-making processes.\nAlthough the FRI algorithm proposed in this paper effectively addresses classification problems, its reliability calculation is based on the contribution to correct results. Therefore, FRI requires a supervised learning process during training, which means its accuracy is affected by the sample size. It does not involve the analysis of similarity between evidence sources, so its accuracy might decrease when the number of known samples is small. Additionally, FRI assigns a reliability of 0 to the evidence source with the smallest contribution, treating it as completely unknown. This makes FRI particularly suitable for recognition problems with a large number of features. When the features include those that negatively impact the correct result, FRI can demonstrate better performance."}, {"title": "5. Conclusion", "content": "This paper introduces a novel method for assessing the reliability of evidence sources, termed the Fuzzy Reliability Index (FRI). The FRI approach leverages intuitionistic fuzzy set theory. Initially, a set of transformation rules is established by exploring the similarities between IFS and DST. Subsequently, the method defines and quantifies the contribution to accurate decision-making, considering both positive and negative impacts. These contributions affect the assessed reliability of evidence sources. In cases where negative contributions are present, the reliability is calculated based on the relative magnitude of these contributions, with normalization adjusted according to the size of the difference intervals.\nIn the experimental section, several information fusion algorithms based on DS theory and some classic machine learning algorithms were compared with FRI to demonstrate its reliability and superiority in pattern classification problems. The experiment first conducted a detailed analysis using the iris dataset, including the generation of BPAs, determination of reliability, discounting processing, and final result analysis. To verify the generalizability of FRI, additional datasets from different fields were used for a comprehensive comparison. The results ultimately demonstrated that the proposed FRI has advantages across various datasets and is more effective than other known methods.\nThe primary contribution of this paper is the development of a decision quantification rule and a contribution measurement method rooted in intuitionistic fuzzy sets, distinguishing it from the PPT algorithm. The proposed FRI fusion algorithm is particularly suited for supervised learning with large datasets. It demonstrates exceptional performance in cases where there are many features and high overlap in feature values among samples, making differentiation challenging. In the future, FRI will focus on considering the similarity between different evidence sources, rather than solely being result-oriented. Additionally, more reasonable standards will be established in the calculation of reliability from contributions to enhance the applicability of the FRI fusion algorithm."}]}