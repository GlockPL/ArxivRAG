{"title": "ROBUST WATERMARKING USING GENERATIVE PRIORS AGAINST IMAGE EDITING: FROM BENCHMARKING TO ADVANCES", "authors": ["Shilin Lu", "Zihan Zhou", "Jiayou Lu", "Yuanzhi Zhu", "Adams Wai-Kin Kong"], "abstract": "Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first comprehensive benchmark designed to evaluate the robustness of watermarking methods against a wide range of image editing techniques, including image regeneration, global editing, local editing, and image-to-video generation. Through extensive evaluations of eleven representative watermarking methods against prevalent editing techniques, we demonstrate that most methods fail to detect watermarks after such edits. To address this limitation, we propose VINE, a watermarking method that significantly enhances robustness against various image editing techniques while maintaining high image quality. Our approach involves two key innovations: (1) we analyze the frequency characteristics of image editing and identify that blurring distortions exhibit similar frequency properties, which allows us to use them as surrogate attacks during training to bolster watermark robustness; (2) we leverage a large-scale pretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to achieve more imperceptible and robust watermark embedding. Experimental results show that our method achieves outstanding watermarking performance under various image editing techniques, outperforming existing methods in both image quality and robustness.", "sections": [{"title": "1 INTRODUCTION", "content": "The primary function of an image watermark is to assert copyright or verify authenticity. A key aspect of watermark design is ensuring its robustness against various image manipulations. Prior deep learning-based watermarking methods (Bui et al., 2023; Tancik et al., 2020; Zhu, 2018) have proven effective at withstanding classical transformations (e.g., compression, noising, scaling, and cropping). However, recent advances in large scale text-to-image (T2I) models (Chang et al., 2023; Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022) have significantly enhanced image editing capabilities, offering a wide array of user-friendly manipulation tools (Brooks et al., 2023; Zhang et al., 2024b). These T2I-based editing methods produce highly realistic modifications, rendering the watermark nearly undetectable in the edited versions. This poses challenges for copyright and intellectual property protection, as malicious users can easily alter an artist's or photographer's work, even with embedded watermarks, to create new content without proper attribution.\nIn this work, we present W-Bench, the first holistic benchmark that incorporates four types of image editing techniques to assess the robustness of watermarking methods, as shown in Figure 1(a). Eleven representative watermarking methods are evaluated on W-Bench. The benchmark encompasses image regeneration, global editing, local editing, and image-to-video generation (I2V). (1) Image regeneration involves perturbing an image into a noisy version and then reconstructing it, which can be categorized as either stochastic (Meng et al., 2021; Zhao et al., 2023b) or deterministic (also known as image inversion) (Mokady et al., 2022; Song et al., 2020a). (2) For global editing, we use models such as Instruct-Pix2Pix (Brooks et al., 2023) and MagicBrush (Zhang et al., 2024b), which take the image and a text prompt as inputs to edit images. (3) For local editing, we employ models like ControlNet-Inpainting (Zhang et al., 2023) and UltraEdit (Zhao et al., 2024c), which allow an additional mask input to specify the region to be modified. (4) Additionally, we evaluate watermarking models in the context of image-to-video generation using Stable Video Diffusion (SVD) (Blattmann et al., 2023) to determine whether the watermark remains detectable in the resultant video frames. Although this is not a conventional image editing approach, we consider it a special case that allows us to identify if the generated videos use copyrighted images. Experimental results (Figure 1(b)) reveal that most previous watermarking models struggle to extract watermarks after images are edited by these methods. StegaStamp (Tancik et al., 2020) and MBRS (Jia et al., 2021) manage to retain watermarks in certain cases, but at the expense of image quality.\nTo this end, we propose VINE, an invisible watermarking model designed to be robust against image editing. Our improvements focus on two key components: the noise layers and the watermark encoder. For the noise layers, a straightforward way to train a watermarking model robust to image editing would be to incorporate editing processes into the training pipeline. However, this is nearly infeasible for large scale T2I model-based image editing because it requires backpropagating through the entire sampling process, which can lead to memory issues (Salman et al., 2023). Instead, we seek surrogate attacks by analyzing image editing from a frequency perspective. The key insight is that image editing tends to remove patterns embedded in high-frequency bands, while those in low-frequency bands are less affected. This property is also observed in blurring distortions (e.g., pixelation and defocus blur). The experiments show that incorporating various blurring distortions into the noise layers can enhance the robustness of the watermarking against image editing.\nHowever, this robustness comes at the cost of watermarked image quality, which is limited by the capability of the watermark encoder. To address this, we leverage a large-scale pretrained generative model, such as SDXL-Turbo (Sauer et al., 2023), as a powerful generative prior, adapting it specifically for the watermarking task. In this context, the watermark encoder functions as a conditional generative model, taking original images and watermarks as inputs and generating watermarked images with a distinct distribution that can be reliably recognized by the corresponding decoder. By utilizing this strong generative prior, the watermark is embedded more effectively, resulting in both improved perceptual image quality and enhanced robustness.\nOur contributions are summarized as follows:\n1. We present W-Bench, the first comprehensive benchmark designed to evaluate eleven representative watermarking models across various image editing methods: image regeneration,"}, {"title": "2 RELATED WORK", "content": "Watermarking benchmark. To the best of our knowledge, WAVES (An et al., 2024) is currently the only comprehensive benchmark for evaluating the robustness of deep learning-based watermarking methods against image manipulations driven by large-scale generative models. However, WAVES considers only image regeneration (Zhao et al., 2023b) among prevalent image editing techniques and does not include other T2I-based editing models. In contrast, W-Bench encompasses not only image regeneration but also global editing (Brooks et al., 2023), local editing (Zhang et al., 2023), and image-to-video generation (Blattmann et al., 2023), thereby broadening the scope of our assessment of image editing methods. Furthermore, WAVES evaluates only three watermarking methods-StegaStamp (Tancik et al., 2020), Stable Signature (Fernandez et al., 2023), and Tree-Ring (Wen et al., 2023). Notably, Stable Signature and Tree-Ring are limited to generated images and cannot be applied to real ones. In contrast, W-Bench is designed to evaluate watermarking models that work with any type of image, thereby enhancing their effectiveness for copyright protection.\nRobust watermarking. Image watermarking has long been studied for purposes such as tracking and protecting intellectual property (Al-Haj, 2007; Cox et al., 2007; Navas et al., 2008). Recently, deep learning-based methods (Bui et al., 2023; Chen & Li, 2024; Fang et al., 2022; 2023; Jia et al., 2021; Kishore et al., 2021; Luo et al., 2020; 2024; Ma et al., 2022; Tancik et al., 2020; Wu et al., 2023; Zhu, 2018; Zhang et al., 2019; 2021) have demonstrated competitive robustness against a wide range of transformations. However, these methods remain vulnerable to image editing powered by large-scale generative models. Three recent studies-EditGuard (Zhang et al., 2024d), Robust-Wide (Hu et al., 2024), and JigMark (Pan et al., 2024)-have begun to develop watermarking models that are robust to such image editing. However, although EditGuard, which replaces the unedited regions of an edited image with corresponding areas of the unedited watermarked version, demonstrates good performance, its application in real-world scenarios could be ineffective if the detector has no access to the unedited watermarked image. In our benchmark, we assume that the detector has access only to the edited image without any additional information. JigMark employs contrastive learning to train a watermark encoder and a classifier that determines if an image is watermarked, but it does not support decoding free-form messages. Robust-Wide incorporates Instruct-Pix2Pix into the noise layer using gradient truncation, but its generalization to other editing models could be less effective. Moreover, neither JigMark nor Robust-Wide has been open-sourced."}, {"title": "3 METHOD", "content": "Given an original image $x_o$ and a watermark $w$, our goal is to imperceptibly embed the watermark into the image by an encoder $E(\\cdot)$ to obtain a watermarked image $x_w = E(x_o, w)$. The watermark should be accurately extracted by a corresponding decoder $D(\\cdot)$ from $x_w$, i.e., $w' = D(x_w)$, even when $x_w$ undergoes image editing $\\epsilon(\\cdot)$.\nIn Section 3.1, we examine the frequency properties of various image editing methods and identify surrogate attacks that enhance the robustness of watermarking against them. In Section 3.2, we further improve both the robustness and quality of the watermarked image by adapting a one-step"}, {"title": "3.1 FREQUENCY NATURE OF IMAGE EDITING", "content": "To develop a robust watermarking model against image editing, a straightforward way is to integrate image editing models in the noise layers between the encoder and decoder during training. However, many prevalent image editing methods are based on diffusion models, which typically involve multiple sampling steps to produce edited images. This can lead to memory issues when backpropagating through the denoising process. Alternative methods, such as gradient truncation (Hu et al., 2024; Yuan et al., 2024), achieve subpar results, and the straight-through estimator (Bengio et al., 2013) fails to converge when training from scratch. Thus, we seek surrogate attacks during training.\nWe start by examining how image editing methods influence the spectrum of an image. Specifically, we conduct three sets of experiments in which symmetric patterns are inserted into the low-, mid-, and high-frequency bands. Figure 2 illustrates the analysis process for a pattern inserted into the low-frequency band. In this procedure, a ring-shaped pattern, $w$, with a constant value is embedded in the low-frequency region of the Fourier spectra of the RGB channels of the original image, $x_o$, i.e., $F(x_o) + w$. The inverse Fourier transform is then applied to obtain the watermarked one, $x_w$. The image editing model, denoted as $\\epsilon(\\cdot)$, is applied to both the original image $x_o$ and the watermarked one $x_w$, producing edited versions $\\epsilon(x_o)$ and $\\epsilon(x_w)$, respectively. Finally, we compute the difference between their Fourier spectra, $|F(\\epsilon(x_w)) - F(\\epsilon(x_o))|$, to assess how the inserted pattern is affected by the editing process. The details of the used image editing methods are provided in Section 4.1.\nFigure 3 illustrates that image editing methods typically remove patterns in the mid and high-frequency bands, while low-frequency patterns remain relatively unaffected. This suggests that T2I-based image editing methods often fail to reproduce intricate mid and high-frequency details. We infer that this occurs because T2I models are trained to prioritize capturing the overall semantic content and structure of images (i.e., primarily the low-frequency components) for aligning with the text prompts. As a result, high-frequency patterns are smoothed out during the generation process.\nTo develop a robust watermarking model against image editing, it should learn to embed information into the low-frequency bands. To identify effective surrogate attacks, we explore various image distortions, denoted as $T(\\cdot)$, that resemble image editing to some extent. Both image editing and distortion methods preserve the overall image layout and most of the content, although image distortions typically result in lower perceptual quality. Notably, as shown in Figure 3, certain blurring distortions (e.g., pixelation and defocus blur) exhibit a similar trend to image editing, where patterns in the low-frequency bands are less affected than those in the mid- and high-frequency bands. In contrast, widely used distortions, such as JPEG compression and saturation, do not display this behavior. Since these blurring distortions are computationally efficient, we incorporate them with varying severity levels into our noise layers during training. This encourages the model to embed in-"}, {"title": "3.2 GENERATIVE PRIOR FOR WATERMARK ENCODING", "content": "Although incorporating image distortions into the noise layers can enhance robustness against image editing, this improvement comes at the expense of watermarked image quality, which is constrained by the capabilities of the watermark encoder. The watermark encoder can be viewed as a conditional generative model, where the conditions include both a watermark and a detailed image, rather than simpler representations like depth maps, Canny edges, or scribbles. We hypothesize that a powerful generative prior can facilitate embedding information more invisibly while enhancing robustness. Thus, we aim to adapt a large-scale T2I model as a watermark encoder. There are two types of large scale T2I models: multi-step and one-step. Multi-step T2I models complicate the backpropagation of watermark extraction loss and suffer from slow inference speeds. Thus, we use a one-step pretrained text-to-image model, SDXL-Turbo (Sauer et al., 2023).\nTo convert SDXL-Turbo into a watermark encoder, an effective strategy to incorporate both the input image and the watermark into the model is essential. A common strategy for integrating conditions into diffusion models is to introduce additional adapter branches (Mou et al., 2024; Zhang et al., 2023). However, in the one-step generative model, the noise map\u2014the input to the UNet\u2014directly determines the final layout of the generated images (Sauer et al., 2023). This contrasts with multi-step diffusion models, where the image layout is gradually established during the early sampling stages. Adding an extra conditional branch to the one-step model causes the UNet to receive two sets of residual features, each representing distinct structures. This makes the training process more challenging and results in subpar performance, as demonstrated by the ablation study in Table 2. Instead, as illustrated in Figure 4, we employ a condition adaptor to fuse the information from the input image and the watermark (the architecture of the condition adaptor is shown in Figure 9). This fused data is then fed into the VAE encoder to obtain latent features, which are subsequently input into the UNet and VAE decoder to generate the final watermarked image. We also attempted to input the watermark through the text prompt and finetune the text encoder simultaneously, but this approach failed to converge. Thus, during training, the text prompt is set to a null prompt.\nDespite the general effectiveness of SDXL-Turbo's VAE, its architecture is not ideally suited for watermarking tasks. The VAE is designed to balance reconstructive and compressive capabilities,"}, {"title": "3.3 OBJECTIVE FUNCTION AND TRAINING STRATEGY", "content": "Objective function. We follow the standard training scheme, which balances the quality of the watermarked image with the effectiveness of watermark extraction under various image manipulations. The total loss function is as follows:\n$\\mathcal{L}_{ALL} = \\mathcal{L}_{IMG} (x_o, x_w) + \\alpha \\mathcal{L}_{BCE} (w, w'), \\quad\\quad (1)$\nwhere $\\alpha$ is a trade-off hyperparameter, $\\mathcal{L}_{BCE}$ is the standard binary cross-entropy loss calculated between the extracted watermark and the ground truth. The image quality loss $\\mathcal{L}_{IMG}$ is defined as:\n$\\mathcal{L}_{IMG} = \\beta_{MSE} \\mathcal{L}_{MSE} (\\gamma(x_o), \\gamma(x_w)) + \\beta_{LPIPS} \\mathcal{L}_{LPIPS} (x, x_w) + \\beta_{GAN} \\mathcal{L}_{GAN} (x_o, x_w), \\quad (2)$\nwhere $\\beta_{MSE}$, $\\beta_{LPIPS}$, and $\\beta_{GAN}$ are the weights of the respective loss terms. Here, $\\gamma(\\cdot)$ is a differentiable, non-parametric mapping that transforms the input image from RGB color space into the more perceptually uniform YUV color space, $\\mathcal{L}_{LPIPS} (x, x_w)$ is the perceptual loss, and $\\mathcal{L}_{GAN} (x_o, x_w)$ is a standard adversarial loss from a GAN discriminator $D_{disc}$:\n$\\mathcal{L}_{GAN} = E_{x_o} [log D_{disc} (x_o)] + E_{x_o,w} [log (1 - D_{disc}(E(x_o, w)))].\\quad\\quad (3)$\nTraining strategy. In the first training stage, we prioritize the watermark extraction loss by setting $\\alpha$ to 10 and $\\beta_{MSE}$, $\\beta_{LPIPS}$, $\\beta_{GAN}$ each to 0.01. To preserve the generative prior, the UNet and VAE decoder of the SDXL-Turbo, along with the added zero-convolution layers, are frozen. Once the bit accuracy exceeds 0.85, we transition to the second stage, unfreezing all parameters for further training. At this point, the loss weighting factors are adjusted to $\\alpha = 1.5$, $\\beta_{MSE} = 2.0$, $\\beta_{LPIPS} = 1.5$, and $\\beta_{GAN} = 0.5$. The model after the first two stages serves as our base model, dubbed VINE-B. We then fine-tune VINE-B in the third stage by incorporating Instruct-Pix2Pix (Brooks et al., 2023), a"}, {"title": "4 EXPERIMENTS", "content": "In W-Bench, we assess the robustness of eleven representative watermarking models against a variety of image editing methods, including image regeneration, global editing, local editing, and image-to-video generation. Section 4.1 and Section 4.2 outline the employed image editing methods and the benchmark setup, respectively. Section 4.3 analyzes the benchmarking results. In Section 4.4, we conduct ablation studies to understand the impact of the key components."}, {"title": "4.1 IMAGE EDITING METHODS", "content": "Image regeneration. Image regeneration involves perturbing an image into a noisy version and then reconstructing it. The perturbing process can be either stochastic or deterministic. In the stochastic method (Meng et al., 2021; Nie et al., 2022; Zhao et al., 2023b), random Gaussian noise is introduced to the image, with the noise level typically controlled by a timestep $t$, using common noise schedulers such as VP (Ho et al., 2020), VE (Song & Ermon, 2019; Song et al., 2020b), FM (Lipman et al., 2022; Liu et al., 2022a), and EDM (Karras et al., 2022). The diffusion model then denoises the noisy image starting from timestep $t_s$ to produce a clean image. In contrast, the deterministic method (Mokady et al., 2022; Song et al., 2020a; Wallace et al., 2022), also known as image inversion, utilizes a diffusion model to deterministicly invert a clean image into a noisy version through multiple sampling steps $n_d$. The image is then reconstructed by applying the same sampling methods over the same number of sampling steps $n_d$. We employ the widely used VP scheduler for stochastic regeneration, testing noise timesteps $t_s$ from 60 to 240 in increments of 20. For deterministic regeneration, we utilize the fast sampler DPM-solver (Lu et al., 2022a;b) and evaluate sampling steps $n_d$ of 15, 25, 35, and 45. See Appendix F.1 for the effects of image regeneration on images.\nGlobal and local editing. Although global editing typically involves stylization, we also consider editing methods guided solely by text prompts. In these cases, unintended background changes frequently occur, regardless of the requested edit\u2014adding, replacing, or removing objects; altering actions; changing colors; modifying text or patterns; or adjusting object quantities. Even though the edited background often appears perceptually similar to the original, these unintended alterations can compromise the embedded watermark. In contrast, local editing refers to editing models that use region masks as input, ensuring that the area outside the mask remains unchanged in the edited image. We employ Instruct-Pix2Pix (Brooks et al., 2023), MagicBrush (Zhang et al., 2024b), and UltraEdit (Zhao et al., 2024c) for global editing, while ControlNet-Inpainting (Zhang et al., 2023) and UltraEdit are used for local editing. Notably, UltraEdit can accept a region mask or operate without it, allowing us to utilize this model for both global and local editing. We use each model's default sampler and perform 50 sampling steps to generate edited images. The difficulty of global editing is controlled by the classifier-free guidance scale of text prompts (Ho & Salimans, 2022), which ranges from 5 to 9, while the image guidance is fixed at 1.5. For local editing, difficulty is determined by the percentage of the edited region with respect to the entire image (i.e., the size of the region mask), with intervals set at 10-20%, 20-30%, 30-40%, 40-50%, and 50-60%. In all cases of local editing, the image and text guidance values are consistently set to 1.5 and 7.5, respectively.\nImage-to-video generation. In the experiments, we utilize SVD (Blattmann et al., 2023) to generate a video from a single image. We assess whether the watermark remains detectable in the resulting"}, {"title": "4.2 EXPERIMENTAL SETUP", "content": "Datasets. We train VINE using the OpenImage dataset (Kuznetsova et al., 2020) at a resolution of 256 \u00d7 256. The training details are provided in Appendix E. For evaluation, we randomly sample 10,000 instances from the UltraEdit dataset (Zhao et al., 2024c), each containing a source image, an editing prompt, and a region mask. The images in UltraEdit dataset are photographs sourced from datasets such as COCO (Lin et al., 2014), Flickr (Young et al., 2014), and ShareGPT4V (Chen et al., 2023). Of these 10,000 samples, 1,000 are allocated for stochastic regeneration, another 1,000 for deterministic regeneration, and 1,000 for global editing. 5,000 samples are designated for local editing. These consist of five sets, each containing 1,000 images, which are edited using mask sizes covering 10-20%, 20\u201330%, 30\u201340%, 40\u201350%, and 50-60% of the total image area. Additionally, we include 1,000 samples for image-to-video generation and 1,000 for testing conventional distortion, thereby completing the total evaluation set.\nBaselines. We compare VINE with eleven watermark baselines, all utilizing their officially released checkpoints. These baselines include MBRS (Jia et al., 2021), CIN (Ma et al., 2022), PIMOG (Fang et al., 2022), RivaGAN (Zhang et al., 2019), SepMark (Wu et al., 2023), TrustMark (Bui et al., 2023), DWTDCT (Al-Haj, 2007), DWTDCTSVD (Navas et al., 2008), SSL (Fernandez et al., 2022), Ste-gaStamp (Tancik et al., 2020), and EditGuard (Zhang et al., 2024d). Although the baselines were trained at different fixed resolutions (as detailed in Appendix C), we apply resolution scaling (described in Section 3.3) to standardize all of them to a uniform resolution of 512 \u00d7 512. This standardization does not compromise their robustness, as demonstrated in Appendix C.3.\nMetrics. We evaluate the imperceptibility of watermarking models using standard metrics, including PSNR, SSIM, LPIPS (Zhang et al., 2018), and FID (Parmar et al., 2022). For watermark extraction, it is essential to strictly control the false positive rate (FPR), as incorrectly labeling non-watermarked images as watermarked can be detrimental\u2014a concern often overlooked in previous studies. Neither high bit accuracy nor AUROC alone guarantees a high true positive rate (TPR) at low FPR levels. Thus, we primarily focus on TPR@0.1%FPR and TPR@1%FPR as our main metrics. Accordingly, both the watermarked and original images are fed into watermark decoders for evaluation. Additionally, we also provide bit accuracy and AUROC for reference. Note that all reported baseline bit accuracies do not include error correction methods, such as BCH (Bose & Ray-Chaudhuri, 1960), which can be applied to all watermarking models."}, {"title": "4.3 BENCHMARKING RESULTS AND ANALYSIS", "content": "Table 1 summarizes the overall evaluation results. As discussed in Section 4.2, we report TPR@0.1%FPR as the primary metric, with additional metrics provided in Figure 5. Each reported"}, {"title": "4.4 ABLATION STUDY", "content": "In this section, we showcase the effectiveness of our designs through an extensive ablation study summarized in Table 2. We start with Config A, a baseline utilizing a simple UNet as the watermark encoder and incorporating only the common distortions outlined in Section 3.1. Building upon this, Config B introduces blurring distortions to the noise layer, which significantly enhances robustness against image editing but compromises image quality. Config C further refines Config B by using a straight-through estimator to fine-tune with Instruct-Pix2Pix. This enhances robustness, albeit at the cost of some image quality. Config D replaces the UNet backbone with the pretrained SDXL-Turbo and integrates image and watermark conditions via ControlNet (Zhang et al., 2023), boosting robustness while degrading image quality due to conflicts from the additional branch. Config E substitutes ControlNet with our condition adaptor, restoring image quality to the level comparable with Config B while maintaining the robustness of Config D. Config F (VINE-B) enhances Config E by introducing skip connections and zero-convolution layers, further improving both image quality and robustness. Config G (VINE-R) fine-tunes Config F using a straight-through estimator with Instruct-Pix2Pix, which increases robustness but reduces image quality. Notably, compared to Config C, Config G leverages a larger model and a powerful generative prior, resulting in significant improvements in image quality and modest gains in robustness. Finally, Config H is trained with randomly initialized weights instead of pretrained ones while retaining all other settings from Config G, leading to lower image quality (particularly on the FID metric) but no change in robustness."}, {"title": "5 CONCLUSION", "content": "In this work, we introduce W-Bench, the first comprehensive benchmark that incorporates four types of image editing powered by large-scale generative models to evaluate the robustness of watermarking models. Eleven representative watermarking methods are selected and tested on W-Bench. We demonstrate how image editing commonly affects the Fourier spectrum of the image and identify an effective and efficient surrogate to simulate these effects during training. Our model, VINE, achieves outstanding watermarking performance against various image editing techniques, outperforming prior methods in both image quality and robustness. These results suggest that one-step pre-trained models can serve as strong and versatile backbones for watermarking, and that a powerful generative prior enhances information embedding in a more imperceptible and robust manner.\nLimitations. While our method delivers exceptional performance against common image editing tasks powered by generative models, its effectiveness in I2V generation remains limited. Moreover, our model is larger than the baseline models, leading to increased memory requirements and slightly slower inference speeds, as detailed in Table 6."}, {"title": "G INFERENCE SPEED AND GPU MEMORY EVALUATION", "content": "We evaluate the inference speed and GPU memory usage of watermarking methods on an NVIDIA Quadro RTX6000 GPU. The results are averaged over 1,000 images. SSL employs instance-based iterative optimization, resulting in longer processing times. EditGuard utilizes an inverse neural network to encode secret images and bits simultaneously, thereby also requiring additional running time. Although our model demands more GPU memory and longer inference times, these requirements remain within acceptable ranges."}, {"title": "H QUALITATIVE COMPARISON", "content": "Figure 15 showcases qualitative examples of 512\u00d7512 images encoded using the watermarking methods evaluated in this study. The residuals are calculated by normalizing the absolute difference $|x_w - x_o|$. For a more detailed examination, please zoom in on the images. It can be observed that the images watermarked by MBRS and PIMoG exhibit slight color distortions, whereas the StegaStamp watermarked images display black artifacts. Other methods produce watermarked images that, to the human eye, appear identical to the original images."}]}