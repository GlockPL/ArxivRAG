{"title": "CodeXEmbed: A Generalist Embedding Model Family\nfor Multiligual and Multi-task Code Retrieval", "authors": ["Ye Liu", "Rui Meng", "Shafiq Jot", "Silvio Savarese", "Caiming Xiong", "Yingbo Zhou", "Semih Yavuz"], "abstract": "Despite the success of text retrieval in many\nNLP tasks, code retrieval remains a largely un-\nderexplored area. Most text retrieval systems\nare tailored for natural language queries, often\nneglecting the specific challenges of retriev-\ning code. This gap leaves existing models un-\nable to effectively capture the diversity of pro-\ngramming languages and tasks across differ-\nent domains, highlighting the need for more\nfocused research in code retrieval. To address\nthis, we introduce CODEXEMBED, a family\nof large-scale code embedding models rang-\ning from 400M to 7B parameters. Our novel\ntraining pipeline unifies multiple programming\nlanguages and transforms various code-related\ntasks into a common retrieval framework, en-\nhancing model generalizability and retrieval\nperformance. Our 7B model sets a new state-of-\nthe-art (SOTA) in code retrieval, outperforming\nthe previous leading model, Voyage-Code, by\nover 20% on CoIR benchmark. In addition to\nexcelling in code retrieval, our models demon-\nstrate competitive performance on the widely\nadopted BeIR text retrieval benchmark, offer-\ning versatility across domains. Experimental\nresults demonstrate that improving retrieval\nperformance significantly enhances end-to-end\nRetrieval-Augmented Generation (RAG) per-\nformance for code-related tasks.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demon-\nstrated exceptional performance across numerous\nNatural Language Processing (NLP) tasks. How-\never, they often struggle to produce faithful an-\nswers and may lack up-to-date or domain-specific\nknowledge. To bridge this gap, retrieval-augmented\ngeneration (RAG) (Cai et al., 2022; Cheng et al.,\n2024) techniques have gained prominence, integrat-\ning Information Retrieval (IR) systems with LLMs\nto enhance their access to relevant external informa-\ntion. This synergy has drawn significant attention\nrecently, leading to the development of various re-\ntrieval models (Wang et al., 2022; Chen et al., 2024)\nbased on BERT (Kenton and Toutanova, 2019) and\nother LLMs with sizes exceeding 1 billion parame-\nters (Wang et al., 2023; Moreira et al., 2024; Meng\net al., 2024). Despite these advancements, standard\nIR methods, while effective in text-based retrieval,\noften fall short in specialized domains such as code\nretrieval (Husain et al., 2019).\nCode retrieval is critical for accelerating devel-\nopment processes and improving code quality. Un-\nlike general text retrieval, code retrieval enables\ndevelopers to quickly locate relevant code snippets,\nexplanations, bug analyses, summaries, and simi-\nlar instances. Effective code retrieval systems are\nnow integrated into commercial products like VS\nCode (Del Sole, 2021) and GitHub Copilot (Wer-\nmelinger, 2023; Yeti\u015ftiren et al., 2023), enhanc-\ning productivity. Code-RAG systems (Parvez et al.,\n2021; Liu et al.; Jimenez et al., 2024; Wang et al.,\n2024) also leverage code retrieval to minimize hal-\nlucinations in generated code from LLMs, ensur-\ning more accurate outputs. However, traditional\ntext retrieval models often struggle with code be-\ncause they focus on linguistic patterns, while code\nretrieval must handle elements like syntax, vari-\nable dependencies, control flow, and API usage.\nDespite the importance of code-specific models,\nexisting ones like CodeBERT (Feng et al., 2020),\nCodeGPT (Lu et al.), and UniXcoder (Guo et al.,\n2022) are based on smaller BERT models (Kenton\nand Toutanova, 2019). While large-scale models\nfor retrieval have become popular, only Voyage-\nCode has followed this approach for code retrieval,\nbut it remains a closed model, leaving a gap for\nopen-source, large-scale code retrieval models.\nIn this work, we introduce CODEXEMBED,\na family of open-source embedding models tai-\nlored for both code and text, available in sizes of\n400 million, 2 billion, and 7 billion parameters.\nCODEXEMBED introduces a generalizable training"}, {"title": "Method", "content": "We transform general code-related tasks into a uni-\nfied retrieval framework by categorizing them into\ndistinct retrieval settings. As shown in Figure 1,\neach setting serves a specific purpose in mapping\nbetween code and text, enhancing the model's abil-\nity to generalize across various retrieval tasks.\nText-to-Code Retrieval. The Text-to-Code re-\ntrieval setting focuses on retrieving relevant code\ndocuments based on a given textual query. This\nsetup is particularly useful in scenarios where nat-\nural language descriptions are mapped to code so-\nlutions. Several code generation tasks can be trans-\nformed to fit into this retrieval paradigm.\nOne such task is code contest generation (Billah\net al., 2024; Kadir et al., 2024), where a natural\nlanguage description of a programming problem\nis provided, and the goal is to retrieve or generate\na code snippet that correctly solves the problem.\nThis setting closely aligns with the Text-to-Code\nretrieval framework as it involves finding code so-\nlutions based on a question or problem statement.\nAnother example is the Text-to-SQL task (Finegan-\nDollak et al., 2018; Li et al., 2024a), where the aim\nis to generate or retrieve a SQL query based on a\nuser's natural language question. The task fits well\nwithin the Text-to-Code category, as it requires un-\nderstanding the intent of the query and retrieving or\ngenerating the appropriate SQL code that matches\nthe given description.\nCode-to-Text Retrieval. The Code-to-Text re-\ntrieval setting is designed to retrieve relevant tex-\ntual documents or descriptions based on a given\ncode query. This setup is particularly valuable in\nscenarios where developers or systems seek to un-\nderstand or explain code by mapping it to human-\nreadable documentation or summaries.\nOne such task is code summarization (Sontakke\net al., 2022; Sun et al., 2024), where a code file, or\neven an entire repository, is provided as input, and\nthe goal is to generate a concise textual summary\nof the code's functionality. This task aligns with\nthe Code-to-Text retrieval setting, as it involves\ntaking potentially complex code and producing a\nhuman-readable description or explanation, which\nis critical for documentation, knowledge sharing,\nand onboarding in software projects.\nCode-to-Code Retrieval. The Code-to-Code re-\ntrieval setting focuses on retrieving relevant code\ndocuments based on a code query. This retrieval\nsetting is particularly useful for tasks that involve\nunderstanding, transforming, or complementing\ncode snippets. Several tasks naturally align with\nthis framework, making it an essential part of the\nbroader code retrieval ecosystem.\nOne such task is code translation (Pan et al.,\n2024; Karanjai et al., 2024), where the goal is to\ntranslate code snippets from one programming lan-\nguage to another. For instance, a developer might\nprovide a code snippet in Python and seek to trans-\nlate it into C++ or Java. This task fits well into the\nCode-to-Code retrieval setting, as it involves find-\ning the equivalent functionality in a different pro-\ngramming language, ensuring the translated code\nmaintains the same logic and structure.\nAnother important task is code completion (Ding"}, {"title": "Experiments", "content": "Evaluation Benchmarks. We mainly use two\nbenchmarks to evaluate code and text retrieval\nperformance. COIR (Li et al., 2024b) is a com-\nprehensive benchmark specifically designed for\ncode retrieval tasks. COIR covers a wide range\nof retrieval challenges, including 8 fine-grained re-\ntrieval subtasks, spanning 14 major programming\nlanguages. The dataset is composed of 10 distinct\ndatasets, with a total corpus exceeding 2 million\nentries. BEIR (Thakur et al.) is a widely-adopted\nbenchmark designed for text retrieval tasks. BEIR\nencompasses a diverse set of retrieval challenges,\ncovering 9 distinct tasks across various domains\nsuch as question answering, duplicate detection,\nfact-checking, and more. It supports retrieval over\na wide range of datasets and provides a standard-\nized benchmark for evaluating text retrieval models\nacross different domains.\nImplementation Details. We conduct general train-\ning on our proposed code and text pair dataset us-\ning three model sizes: 400M, 2B, and 7B. For the\nCodeXEmbed400M, we use the base model Alibaba-\nNLP/gte-large-en-v1.5 (Li et al., 2023b), applying\nfull model fine-tuning. For the CodeXEmbed2B,\nwe initialize our embedding model from the gen-\neration model google/gemma-2-2b-it (Team et al.,\n2024), using low-rank adaption(LoRA) (Hu et al.,\n2022) with a rank of 8. For the CodeXEmbed7B,\nwe initialize our embedding model from the gen-\neration model mistralai/Mistral-7B-Instruct-v0.3,\nalso using LoRA with a rank of 8. Follow-\ning prior work (Meng et al., 2024), we apply:\n(i) last token pooling for CodeXEmbed2B and\nCodeXEmbed7\u00df, and (ii) beginning token pooling\nfor CodeXEmbed400M to generate semantic vector\nrepresentations (Li et al., 2023b). Cosine similarity\nis used to compute the similarity between query\nand corpus for ranking. The batch size is set to\n1024 across all three model sizes, with 7 hard neg-\natives. The learning rate is set to 5e-5, and the\nend learning rate to 5e-6, with linear decay and\na 50-step warmup. To improve training efficiency\nand reduce GPU memory usage, we adopt gradient\ncaching (Gao et al., 2021). The more implementa-\ntion details can be found in Appendix A.2.\nBaseline Models. For code-domain-specific mod-\nels, we included UniXcoder (Guo et al., 2022) and\nVoyage-Code-002\u00b9, both of which are pre-trained\non code data, making them strong baselines for\ncomparison. For general retrieval models, we eval-\nuated E5-Base (Wang et al., 2022), GTE-Base (Li\net al., 2023b), BGE-Base (Xiao et al., 2023), Con-\ntriever (Izacard et al., 2023), E5-Mistral (Wang\net al., 2023), BGE-M3 (Chen et al., 2024), NV-\nEmbed-V2 (Moreira et al., 2024), SFR-V2 (Meng*\net al., 2024) and OpenAI-Ada-002\u00b2.\nEvaluation Metrics. In code retrieval, selecting\nthe right evaluation metric is key for assessing both\nranking sensitivity and relevance. Building on prior\nwork (Wang et al., 2013), Normalized Discounted"}, {"title": "General Training Evaluation", "content": "In the General Training block of Table 1, we\npresent the results of models trained exclusively\non our proposed general training data, without\nusing any CoIR in-domain data. When averaged\nover all 10 datasets in the CoIR benchmark,\nCodeXEmbed7B model achieves the best results,\nexceeding the SOTA code-domain specific model\nVoyage-Code-002 by over 20%, which shows our\ngeneral code and text training stage significantly\nimproves model performance on code tasks.\nAs shown in Table 1, CodeXEmbed400M and\nCodeXEmbed2B also provide a significant improve-\nment over Voyage-Code-002 and offer a great al-\nternative to the 7B model with substantial practi-\ncal advantages on the latency and cost. Moreover,\ntheir success further validates the transferability\nand generalizability of our proposed training recipe\nfor code embedding models."}, {"title": "In-domain Training Evaluation", "content": "We further trained the model on the CoIR in-\ndomain dataset. As shown in the In-domain\nTraining block of Table 1, further training\non in-domain data results in consistent perfor-\nmance improvements across all model sizes.\nSpecifically, CodeXEmbed400M improves by 6.5\npoints, CodeXEmbed2B by 8.24 points, and\nCodeXEmbed7\u00df by 7.74 points on average across\nall 10 datasets. In Figure 2, the top of each bar\nrepresents the improvement from in-domain train-\ning. Among all categories, the Code-to-Text cate-\ngory shows the largest improvement, even outper-\nforming Voyage-Code-002. In other categories, the\nmodel also achieves over a 5-point improvement."}, {"title": "Unified Text and Code Retrieval", "content": "To evaluate the text and code retrieval capabilities\nwithin a single embedding model, we also present\nthe BEIR text retrieval performance of CODEX-\nEMBED across various sizes. As shown in Table 2,\nour 7B model achieves an average score of over 60\nacross 15 datasets, placing it among the top tier on\nthe MTEB leaderboard\u00b3."}, {"title": "Retrieval-Augmented Code Generation", "content": "In this section, we explore how different retriev-\ners influence the final code completion and issue\nresolution performance in repository-level tasks."}, {"title": "RepoEval", "content": "To address this, we utilize RepoEval (Zhang et al.,\n2023) for repository-level code completion. While\nRepoEval consists of three splits (function, class,\nand module), we report results only on the function\nsplit, as it is the only one that supports execution-\nbased evaluation. We adopt Pass@1 as our evalua-\ntion metric, which measures the accuracy of the top-\n1 generated code passing the provided test cases.\nFor code generation, we supply the top-5 re-\ntrieved code snippets as input to the GPT-3.5-turbo\ngeneration model. As shown in Table 4, all sizes\nof CODEXEMBED outperform the canonical setup.\nWhile some files may not contain direct solutions,\nas in the canonical documents, they often include\nvaluable function definitions or usage examples\nthat improve code generation outcomes. This sug-\ngests that our embeddings effectively capture the\nrepository structure and retrieve contexts that im-\nplicitly support problem-solving."}, {"title": "SWE-Bench-Lite", "content": "In our experiments, we use SWE-bench-Lite\u2074, a\ncurated subset of 300 problems from the original\nSWE-bench benchmark. It focuses on resolving\nGitHub issues by requiring models to modify mul-\ntiple files to pass test cases, providing a manageable\nand reproducible dataset. SWE-bench-Lite also in-"}, {"title": "Impact of the Base Models", "content": "To understand the base model's impact, we exam-\nine: (1) if training from a text retrieval model out-\nperforms a generation model, and (2) if a code-\nspecific generation model offers more advantages\nthan a general language model."}, {"title": "Embedding Models v.s. LLMs", "content": "As shown in Table 5, the text retrieval model offers\na stronger starting point, and additional training\nwith our approach enhances both its text and code\nretrieval capabilities. For instance, gte-Qwen27's\nCOIR performance improves from 62.96 to 68.52,\nwhile its text performance increases from 58.29\nto 59.12. In contrast, the text generation model re-\nquires more extensive fine-tuning to reach similar\nperformance. However, the advantage of text re-\ntrieval models can sometimes hinder code retrieval\nperformance, as seen with SFR-V2 (Meng* et al.,\n2024) underperforming compared to Mistral in spe-\ncific tasks. This suggests that prior knowledge from\ntext-focused models may not always transfer well\nto code-specific scenarios. To pursue a more gen-\neral training approach, we chose to train using a\ngeneration model rather than a text retrieval model."}, {"title": "Code-Specific LLMs v.s. General LLMs", "content": "We evaluate whether to choose code-specific mod-\nels (Lozhkov et al., 2024; Guo et al., 2024) or gen-\neral LLMs (Jiang et al., 2023; Team et al., 2024).\nAs shown in Table 6, code-specific LLMs excel in\ncode tasks but underperform in text tasks, while\ngeneral LLMs perform well in both. This suggests\nthat recent advancements in general LLMs have\nintegrated code data into their training (Team et al.,\n2024), and this capability can be effectively trans-\nferred to code retrieval. This finding highlights the\nversatility of general LLMs, making them viable\nfor both text and code retrieval without the need\nfor specialized models."}, {"title": "Programming Language Transferability", "content": "We aim to explore the diversity of programming\nlanguages and their unique features. The details of\nour code training dataset, including language cov-\nerage, are provided in Appendix A.1. Our dataset\ncomprises 12 programming languages, with Python\nrepresenting the highest percentage of the data. For\ntesting, we selected Python and Java due to their\ndistinct programming paradigms: Python is known\nfor its scripting capabilities and Java for its strong\nobject-oriented design. This selection allows us to\nevaluate our model's performance across a range of\nprogramming styles, reflecting the versatility and\nadaptability of the embedding model.\nAs shown in Table 7, training with all 12 pro-\ngramming languages yields the best average per-\nformance across 7 target languages, compared to\ntraining with a single language. However, train-\ning on Java-only consistently achieves the highest\nperformance for Java and delivers comparable re-"}, {"title": "Limitations", "content": "While CODEXEMBED achieves state-of-the-art re-\nsults in code and text retrieval, there are certain\nlimitations to consider. First, the model's large size,\nparticularly the 7B parameter variant, leads to sig-\nnificant storage and computational costs. With em-\nbeddings of 4096 dimensions, the model is more\nresource-intensive compared to smaller or more ef-\nficient representations. Although embedding size\nreduction techniques, such as Matryoshka represen-\ntation learning (Kusupati et al., 2022), have shown\npromise in maintaining performance with lower di-\nmensions, further research is needed to apply these\napproaches effectively to code retrieval without\ncompromising performance. Additionally, while\nCODEXEMBED generalizes well across different\nprogramming languages and text retrieval tasks, it\nmay still struggle with niche programming domains\nor highly specialized languages that were underrep-\nresented in the training data. Future work could\nexplore adapting the model for these underrepre-\nsented languages and improving efficiency without\nsacrificing accuracy."}, {"title": "Appendix", "content": "A.1 Dataset Details\nOur training dataset contains a total of 3.36M train-\ning pairs, covering 12 different programming lan-\nguages. As shown in Figure 3, the distribution of\nprogramming languages is imbalanced, with the\nmajority of the data concentrated in a few popular\nlanguages. Python represents the largest portion of\nthe dataset at 27.1%, followed by Go with 25.2%,\nand JavaScript and PHP at 17.0% and 17.2%, re-\nspectively. The remaining languages, including\nJava, SQL, Ruby, and others, account for smaller\nproportions, with Rust, Kotlin, and C# making up\nthe smallest shares of the dataset.\nA.2 Implementation Details\nWe summarize the hyperparameters in Table 8. For\nthe code training data, we prepend a prompt to the\nquery in the format: \"Instruct: Given Code or Text,\nretrieve relevant content. Query:\"."}, {"title": "Retrieval Training", "content": "To train a unified code and\ntext retrieval model, we transform all the aforemen-\ntioned tasks into a query-and-answer format, where\neither the query or the answer can be a text or code\ndocument. This unified format allows us to handle\na variety of tasks such as Text-to-Code, Code-to-\nText, and Code-to-Code retrieval within the same\ntraining framework. The training loss is designed\nas a contrastive loss (Liu et al., 2021), which aims\nto maximize the similarity between the query and\nthe correct answer while minimizing the similar-\nity to the negative samples. The loss function is\nrepresented as:\n$\\mathcal{L}_{C}=-\\sum_{i=1}^{N}\\log \\frac{\\exp \\left(q_{i} d_{i}^{+}\\right)}{\\exp \\left(q_{i} d_{i}^{+}\\right)+\\sum_{j=1}^{K} \\exp \\left(q_{i} d_{j}^{-}\\right)}$\nwhere $q_i$ represents the query, which can be either\ntext or code. The term $d_{i}^{+}$ refers to the correspond-\ning positive document, which could also be in the\nform of text or code. On the other hand, $d_{j}^{-}$ de-\nnotes a set of hard negative documents, which are\nselected to be similar in context to the query but in-\ncorrect, making the retrieval task more challenging.\nThe similarity between the query and a document is\nmeasured by $\\operatorname{sim}(q, d)$. The variable $N$ represents\nthe total number of training samples, indicating the\nnumber of queries used during training, while $K$\ndefines the number of hard negatives associated\nwith each query."}]}