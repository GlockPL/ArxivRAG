{"title": "AMAES: Augmented Masked Autoencoder Pretraining on Public Brain MRI Data for 3D-Native Segmentation", "authors": ["Asbj\u00f8rn Munk", "Jakob Ambsdorf", "Sebastian Llambias", "Mads Nielsen"], "abstract": "This study investigates the impact of self-supervised pre- training of 3D semantic segmentation models on a large-scale, domain- specific dataset. We introduce BRAINS-45K, a dataset of 44,756 brain MRI volumes from public sources, the largest public dataset available, and revisit a number of design choices for pretraining modern segmenta- tion architectures by simplifying and optimizing state-of-the-art meth- ods, and combining them with a novel augmentation strategy. The result- ing AMAES framework is based on masked-image-modeling and intensity- based augmentation reversal and balances memory usage, runtime, and finetuning performance. Using the popular U-Net and the recent Med- Next architecture as backbones, we evaluate the effect of pretraining on three challenging downstream tasks, covering single-sequence, low- resource settings, and out-of-domain generalization. The results highlight that pretraining on the proposed dataset with AMAES significantly im- proves segmentation performance in the majority of evaluated cases, and that it is beneficial to pretrain the model with augmentations, despite pretraing on a large-scale dataset. Code and model checkpoints for re- producing results, as well as the BRAINS-45K dataset are available at https://github.com/asbjrnmunk/amaes.", "sections": [{"title": "1 Introduction", "content": "Large-scale self-supervised pretraining promises immense potential for medical image segmentation. Works in computer vision from the natural image domain demonstrate that state-of-the-art results are achieved by scaling model capac- ity and pretraining on large datasets, enabling increased generalization perfor- mance, few- or zero-shot predictions, and highly semantic embeddings [9]. To en- able self-supervised representation learning in vision, various methods have been proposed. Early works involved reversing image degradations, such as denoising images [24], solving jigzaw puzzles [18], or re-colorizing images [26]. Masked- Image-Modeling (MIM) proved to be an especially successful and scalable vari- ant, where patches of the input image are masked and learned to be reconstructed either directly or indirectly by predicting an embedding [1,9]. A Masked Autoen- coder (MAE) is an encoder-decoder architecture that reconstructs the entire im- age from a masked version [9]. Another family of approaches applied contrastive learning, where semantically similar transformed inputs are mapped closely in a latent space, while dissimilar inputs are pushed apart [3]. Self-distillation is based on a related idea, enforcing similar embeddings for semantically close in- puts for a student and teacher network [6]. Recent approaches combined these image-level contrastive or self-distillation objectives with patch-level MIM to enhance the spatial representation performance for downstream segmentation tasks [1,23]. Evidence from medical vision indicates that the benefits of pretraining are transferable to biomedical image segmentation [2]. Yet, the current method of choice for solving segmentation tasks is still a fully-supervised paradigm [11]. Due to a heavily optimized data augmentation pipeline, the fully-supervised approach can be effective even with limited labeled data [4,10]. However, to fully leverage the potential of large-scale models in medical image segmentation as witnessed in computer vision, more training data is required, and the supervised approach is inherently limited by labeling cost. Large-scale unlabeled medical imaging datasets could resolve this need, but they are not readily available to the public, hindering progress for methodological research on how to harness unlabeled data for downstream segmentation most effectively. To address the need for pretraining datasets in the brain MRI domain, we propose BRAINS-"}, {"title": "2 BRAINS-45K dataset", "content": "We propose BRAINS-45K a dataset of 44.756 Brain MRI volumes collected from public sources, featuring a diverse set of acquisition parameters and patient"}, {"title": "3 Method", "content": "In this section we briefly describe AMAES. Our approach is highlighted in Fig- ure 2. The overall goal of the framework is to provide an effective and efficient"}, {"title": "3.1 Pretraining", "content": "AMAES is based on a Masked Autoencoder (MAE) architecture with a light weight decoder and a specific augmentation scheme. Importantly, the architec- ture assumes no specific choice of encoder and can be used with any encoder that is compatible with the skip connections of the U-Net decoder.\nPre-text task. Prior work has centered around 2D natural images [25,9], however, previous work showed that a Masked Autoencoder works well in 3D by applying it to 3D CT Brains using the SwinUNETR architecture [23]. The SwinUNETR pretraining objective further included both a rotation and a con- trastive loss. However we find that the rotation task was near-trivial for the models to solve and thus provided no tangible utility, while the contrastive loss resulted in the memory footprint during pretraining to be doubled, without sig- nificant performance gains (see Appendix D). Threfore, we reduce the pre-text task for AMAES to masked reconstruction, and instead increase the batch size and double the number of training epochs. For each 3D input patch, the input is divided into subpatches of size 4 \u00d7 4 \u00d7 4 voxels, and each subpatch is masked with probability 0.6, following prior work [25].\nDecoder architecture. Since the decoder is discarded after pretraining, it is desirable to use as small a decoder as possible. This not only decreases training time and memory usage, but also forces the network to learn expressive representations in the encoder. We find that a light weight version of the standard U-Net decoder provides a good balance between flexibility, memory efficiency and parameter count.\nSkip connections. While skip connections are used during finetuning, our proposed encoder-decoder architecture does not include skip connections during pretraining. We found that using skip connections during pretraining tended to slightly adversely affect downstream performance, with a finetuning dice score of"}, {"title": "3.2 Finetuning", "content": "For finetuning, we transfer the backbone encoder weights from pretraining and attach a new standard U-Net decoder with skip connections between the encoder and the decoder. We only use spatial augmentations (rotation, scaling) during finetuning, since this setup performed better empirically. Upon closer inspection, the learning curves showed signs of catastrophic forgetting during the second half of training when finetuned with the full augmentation package."}, {"title": "4 Experimental Design", "content": ""}, {"title": "4.1 Datasets", "content": "We evaluate AMAES on three different segmentation tasks covering tumor (BraTS21 [7]), ischemic stroke lesion (ISLES22 [20]) and white matter hyperin- tensity (WMH [14]). To test the models few-shot learning ability, we evaluate the models in a low resource setting: we only use $n_{train}$ = 20 labeled exam- ples for finetuning, while we evaluate the models on the remaining data. We choose $n_{train}$ = 20 to realistically model what one medical expert can manually"}, {"title": "4.2 Configurations", "content": "Our baselines, pretraining and finetuning are implemented using PyTorch, Py- Torch Lightning, and the Yucca library for medical image analysis [16] as a back-bone. All models are trained using 16-bit mixed-precision to limit mem- ory footprint. Models were trained patch-based, using an input patch size of 128\u00d7128\u00d7128. Models are pretrained for 100 epochs (one epoch is one patch from each volume) and trained supervised for 2500 steps on downstream datasets. The exact hyperparameters used for each model is given in Appendix A. All models are pretrained on two H100 GPUs, and fine-tuned on a single H100 GPU."}, {"title": "4.3 Baselines", "content": "We evaluate AMAES along two dimensions: (i) How does AMAES compare to the pretrained SwinUNETR?, and (ii) What is the impact of pretraining on downstream performance?\nTo evaluate (i), we pretrain SwinUNETR with the exact configuration given in [23] on BRAINS-45K, which includes both a contrastive loss, a rotation loss and a reconstruction loss, as well as a different choice of masking ratio and mask size. To ensure a fair comparison, SwinUNETR is pretrained for 100 epochs, similar to AMAES, and is pretrained with patch size 1283, instead of patch size 963 and 90 epochs. To address (ii), we apply AMAES to a set of convolutional backbones and evaluate the difference between training from scratch and fine- tuning the pretrained model. The backbone models include a U-Net in two sizes: XL (90M parameters) and B. Further, we explore using the modernized U-Net architecture Med- NeXt [21], which uses depth-wise seperable convolution to introduce compound scaleable medical segmentation models."}, {"title": "5 Results and Discussion", "content": "The results of our experiments are given in Table 4. Both U-Net B, U-Net XL and MedNeXt models benefit from pretraining in all scenarios. MedNeXt M does not benefit from pretraining on BraTS21 and ISLES22, however exhibits very good performance on WMH both in and out of domain. We believe this is mainly due to strong baseline performance, before pretraining is added. Note that models do not use the full augementation pipeline during finetuning, while the baselines from scratch use all augmentations. All models trained with AMAES outperform the pretrained SwinUNETR, except for the standard U-Net on WMH OOD.\nGenerally, model size seems to be correlated with performance, with two notable exceptions: The SwinUNETR model seems to be worse than both MedNeXt M model and the standard U-Net. The results on WMH OOD demonstrate that pre- training also increases performance when applied to out-of-domain data even with respect to the pretraining domain.\nLimitations. This study only considers single sequence and low resource settings. We have not applied AMAES to the multi-sequence versions of the dataset and neither have we applied it to datasets with more datapoints than $N_{train}$ = 20. How to leverage pretraining for multi-sequence tasks is a relevant avenue for future work. This study neither tests the SwinUNETR framework using different backbones and nor tests AMAES using a SwinUNETR backbone, since it is not readily compatible with a U-Net decoder."}, {"title": "6 Conclusion", "content": "We present AMAES, a framework based on Masked Image Modeling to pretrain 3D-native segmentation models which balances memory usage, runtime, and fine-tuning performance. We further introduce BRAINS-45K, the largest dataset of brain MRI for pretraining to date. We pretrain AMAES on BRAINS-45K and evaluate the models on three challenging datasets. We compare the framework to models trained from scratch as well as the pretrained SwinUNETR model. Our results demonstrate that when AMAES is used to pretrain U-Net models or MedNeXt models, the models outperform similar models trained from scratch, as well as SwinUNETR models, both pretrained and not in the challenging tasks evaluated."}]}