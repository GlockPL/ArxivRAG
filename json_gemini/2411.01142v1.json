{"title": "NEO: SAVING GPU MEMORY CRISIS WITH CPU OFFLOADING FOR ONLINE LLM INFERENCE", "authors": ["Xuanlin Jiang", "Yang Zhou", "Shiyi Cao", "Ion Stoica", "Minlan Yu"], "abstract": "Online LLM inference powers many exciting applications such as intelligent chatbots and autonomous agents. Modern LLM inference engines widely rely on request batching to improve inference throughput, aiming to make it cost-efficient when running on expensive GPU accelerators. However, the limited GPU memory has largely limited the batch size achieved in practice, leaving significant GPU compute resources wasted.\nWe present NEO, an online LLM inference system that offloads part of attention compute and KV cache states from the GPU to the local host CPU, effectively increasing the GPU batch size and thus inference throughput. To this end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling to balance GPU and CPU loads and fully utilize their compute and memory resources. We evaluate NEO on a wide range of workloads (i.e., code generation, text summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B, 70B). NEO achieves up to 7.5\u00d7, 26%, and 14% higher throughput compared to GPU-only approach on T4, A10G, and H100 GPUs, respectively, while maintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3% throughput gain on A10G GPU.", "sections": [{"title": "1 INTRODUCTION", "content": "The advent of auto-regressive transformer-based large language models (LLMs) has significantly reshaped existing technologies such as search engines and chatbots and empowered various new ones, such as autonomous agents and programming assistants. In these online scenarios, LLM inference is directly user-facing and thus requires low latency for immersive interaction; it also desires high throughput, typically via request batching, to efficiently leverage expensive hardware accelerators like GPUs.\nHowever, to achieve large batch sizes for high throughput, online LLM inference requires huge GPU memory, but GPUs have limited memory resources. Nowadays LLM models have billions of parameters (e.g., 70B LLaMa-3.1 model) that occupy dozens to hundreds of GB GPU memory; modern LLM inference engines like VLLM additionally store KV cache in the GPU memory to reuse previous computations, whose size increases linearly with prompt and output length. As a result, the memory-bounded LLM inference workloads have created the GPU memory crisis where people demand expensive high-end GPUs with large memory sizes.\nPrior work has recognized this problem and proposed various solutions. One line of work is on model quantization to reduce model memory consumption. However, they come at the cost of lower accuracy. Another line of work is offloading model weights, KV cache, and compute to the CPU, such as Flex-Gen, PowerInfer, TwinPilots, HeteGen, and FastDecode. Memory offloading could increase request batch sizes on the GPU, potentially increasing overall inference throughput; compute offloading avoids repetitively swapping the KV cache between the GPU and CPU, thus preventing PCIe bandwidth from becoming the bottleneck. Unfortunately, most of these work trades inference latency for throughput by using huge GPU batch sizes and layer-by-layer swapping, thus not suitable for online inference; the exception is Fast-Decode, but it uses 8 32-core AMD Epyc CPUs in remote servers to handle the offloaded compute of only one A10G GPU, where the CPUs cost much more than the GPU.\nThis paper aims to achieve higher throughput for online LLM inference without compromising accuracy or latency in a cost-efficient way-only using local host CPUs as the offloading target that comes with the GPU \u201cas free\u201d. Achieving this goal faces two main challenges. First, within each inference iteration, how to balance the compute happening on GPU and CPU to fully utilize their compute and memory resources, while not overloading them? This is challeng-"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "2.1 LLM Inference and Performance Bottleneck\nAuto-regressive transformer-based LLMs perceive tokens as the basic elements of languages, with the main task of predicting the next token of the given sequence. Formally, given $[t_1, t_2,..., t_n]$ as input, for each token $x$ in the vocabulary, an LLM needs to return the probability that $x$ is the next token of the sequence, i.e. $Pr[t_{n+1} = x | t_1,..., t_n]$. To do this prediction, as illustrated in Figure 1, the transformer-based LLM first converts tokens to embedding vectors. These vectors are then passed through a series of primary blocks called transformer layers, leveraging the attention mechanism. The embedding vectors remain the same shape but become more precise and context-aware after passing through each transformer layer, before finally getting through a final fully connected layer that converts embedding vectors to corresponding probabilities for each possible token.\nOne inference request typically consists of prefilling and decoding stages and heavily involves the KV cache on GPU memory to reuse previous computations. The prefilling stage generates the initial KV cache after consuming all input tokens, while the decoding stage repetitively read-and-appends the KV cache and auto-regressively generates output tokens until an EOS (End Of Sequence). To optimize this process, modern LLM inference engines like VLLM leverage iteration-level scheduling to accommodate various input/output lengths of requests, selective batching to increase performance by batching matrix multiplications, and paged attention to efficiently manage GPU memory.\nThe throughput of LLM inference highly depends on the batch size the engine can achieve, which is essentially bounded by the GPU memory size due to the large KV cache. Prior work has demonstrated that the throughput would increase almost linearly as the batch size increases up to hundreds on modern A10, V100, and H100 GPUs; meanwhile, unfortunately, these GPUs fail to accommodate hundreds or even dozens of batch size, leaving significant GPU compute resources unutilized.\n2.2 CPU Memory as a Possible Rescue\nOne approach to increase batch size is storing the overflowing KV cache in the main memory of the CPU, transferring it to the GPU when needed, and transferring it back when not needed. However, such repetitive KV cache swap-ins and -outs make the system severely bounded by GPU-CPU PCIe bandwidth, as shown by prior work. Fortunately, only the decoding-phase attention operation relies on the KV cache, therefore offloading this part of computation to the CPU would help avoid repetitively transferring the KV cache between the GPU and CPU. Moreover, this operation only takes a tiny proportion of computation (compared to other parts in the transformer architecture) and doesn't require loading model weights.\nThe decoding attention operation is memory-bandwidth-bounded on both GPU and CPU due to low arithmetic intensity (i.e., FLOP per memory load). The memory bandwidth gap between GPUs and CPUs is much smaller than their compute gap. For example, an A10G GPU features 600 GB/s memory bandwidth and 125 TFLOPS, while a modern x86 server has around 200 GB/s memory bandwidth with 1.2 TFLOPS; modern ARM processors like AWS Graviton4 offer 537.6 GB/s memory bandwidth per socket. Therefore, although there may seem to be a huge compute gap between GPUs and CPUs (i.e., 125 vs. 1.2 TFLOPS), the actual performance gap for the decoding attention operation is relatively small because their memory bandwidths are closer (i.e., 600 vs. 200 GB/s).\n2.3 Challenges\nDespite being promising, there are several challenges when building an efficient LLM inference system that offloads decoding attention compute and memory to CPUs. These challenges stem from the fundamentally different capabilities between GPUs and CPUs (in terms of memory and compute power), and get amplified in real-world dynamically-changing inference workloads (e.g., various input/output lengths).\nChallenge #1: How to efficiently overlap the GPU and CPU within each inference iteration? GPUs have more compute and memory bandwidth but are limited in memory size, while CPUs have more memory and a decent amount of memory bandwidth, but lack strong compute power. Therefore, we must carefully restructure the pipeline of LLM inference to fit different pipeline modules into the right hardware, while not overloading any hardware. Such restructuring also needs to take care of the complex inter- and intra-transformer-layer data dependencies, without breaking transformer semantics.\nChallenge #2: How to schedule inference requests to the GPU and CPU across inference iterations to maintain high performance in dynamically-changing workloads? Prior work like FastDecode, FlexGen, and more only consider idealistic settings where request input and output lengths are fixed, and adopt a static scheduling policy (e.g., obtained from offline profiling) to assign requests across the GPU and CPU. However, in real-world dynamic settings, vastly different input/output lengths would make static scheduling no longer work efficiently; instead, it would require an adaptive scheduling policy to determine the best request assignments at the per-iteration level."}, {"title": "3 NEO DESIGN", "content": "Figure 2 shows the high-level architecture of NEO. NEO consists of a request scheduler running on the CPU that maintains a prefilling waitqueue, a GPU decoding runqueue, and a CPU decoding runqueue; this scheduler makes iteration-level adaptive scheduling decisions on whether to run the incoming requests on GPU or CPU. NEO features two key techniques: 1) asymmetric pipelining to fully leverage the compute resource of both GPU and CPU without\n3.1 Asymmetric Pipelining\nNEO proposes asymmetric pipelining to address the GPU-CPU overlapping challenge in \u00a72.3. This design offloads decoding attention (both its compute and KV cache) of a selected portion of inference requests to the CPU. It forms two sub-batches of requests-one mostly runs on the GPU while another runs across the GPU and CPU, and overlaps these two sub-batches to balance the load of GPU and CPU.\nTo motivate this design, we first explore a simple strawman called simple offloading that offloads compute and KV cache to the CPU but leaves the GPU idle. Next, we examine a more intricate strawman called symmetric pipelining used by prior work that tends to leave the GPU idle. Finally, we will arrive at our design of asymmetric pipelining, and show how it effectively addresses the issues in the simple offloading and symmetric pipelining designs.\nStrawman #1: simple offloading. As shown in Figure 3, this design extracts the decoding attention and offloads its compute and KV cache to the CPU, while leaving the rest to the GPU. The rest includes prefilling attention and token-wise independent operations\u2014referred to as the linear operations that mainly involve matrix multiplications. However, during these linear operations, the CPU always remains idle.\nAs a result, this design fails to leverage the compute and memory resources of CPUs.\nStrawman #2: symmetric pipelining. A straightforward approach to reducing the CPU idle time is to evenly split a single decoding batch into two sub-batches, and overlap their linear operations (on the GPU) and attention operations (on the CPU), as shown in Figure 4. For the prefilling batch, symmetric pipelining just runs it on the GPU without offloading, as the prefilling stage requires high computation for matrix multiplication while not consuming much memory. The output of the symmetric pipelining, i.e., the KV cache, will be swapped out to the CPU for offloading. This technique has been used by prior work like FastDecode and more. Nevertheless, seemingly efficient, this design suffers from three major issues.\n\u2022 Firstly, this design results in significant underutilization of GPU memory. In this scheme, the GPU solely retains the model weights and runtime activations, while the rest of the memory-which stores the KV cache in non-offloading settings-remains unused.\n\u2022 Secondly, this design fails to achieve balanced GPU-CPU overlapping. This is because 1) it entirely overlooks the prefilling stage and KV cache swap-out time, during which the CPU stays idle; 2) the linear stage of a decoding sub-batch on the GPU is typically much shorter than the attention stage on the CPU, due to the attention's auto-regressive nature and high memory bandwidth consumption. As a result, the duration of the attention stage will likely exceed that of the linear stage, causing the CPU to become the bottleneck.\n\u2022 Finally, it is challenging to split two batches evenly in practice. In real-world workloads, it is nearly impossible to ensure that a single batch can be divided into two identical sub-batches, due to different input lengths and unpredictable output lengths. The discrepancies between the batches would likely result in a significant number of idle periods or \"bubbles\" in the pipeline.\nAsymmetric pipelining, as shown in Figure 5, offers a solution to the aforementioned problems. To fully utilize GPU memory, NEO does partial offloading. The KV cache system of NEO is divided into two separate components: the \"GPU-cache\" located in the GPU's HBM, and the \"CPU-cache\" located in the CPU main memory. For any request that has already been prefilled in the system, its KV cache will either reside entirely in the GPU-cache-designated as a \"GPU-request\"\u2014or entirely in the CPU-cache-designated as a \"CPU-request\". Requests are prioritized for storage in the GPU-cache to maximize GPU memory utilization.\nTo achieve full GPU-CPU overlapping, NEO integrates the prefilling stage computation into the GPU decoding sub-batch, so that the prefilling stage computation (in GPU) also happens in parallel with the CPU attention computation. This shares a similar spirit as the selective batching technique used in Orca. In our context, this selective batching largely extends the duration of the GPU compute, allowing for a longer overall CPU computing time and enabling more CPU-requests to be incorporated into the batch. Further, NEO leverages the layer-wise swapping technique to facilitate the overlapping of KV value transmission with computation. Given that the KV values of newly prefilled requests are computed layer by layer, we can initiate PCIe transmission immediately after each layer's KV value is computed, rather than deferring this process until the end of the entire iteration.\nTo simplify the batch division scheme and minimize idle periods, NEO introduces asymmetric batch division. Instead of attempting to create two identical batches, we consolidate all prefilling requests and GPU decoding requests with a few CPU decoding requests to batch-0, while dispatching the majority of CPU decoding requests to batch-1. The two batches are complementary: batch-0 features a long linear stage with little CPU attention computation, whereas batch-1 includes a lengthy attention stage with a very short linear stage. This arrangement not only simplifies system implementation, but also facilitates effective overlapping between the two batches, resulting in an inference iteration characterized by alternative \u201clong-stages\u201d and \u201cshort-stages\", rather than uniform stages.\nAsymmetric pipelining offers even more benefits. As shown in Figure 5, the non-overlapping segments at the beginning and end are minimized due to this intentional asymmetry. Furthermore, this approach reduces GPU kernel launching overhead, which can be substantial in Python due to its limited multi-threading parallelism, consuming considerable CPU time. In asymmetric pipelining, the GPU attention kernel is only invoked once per iteration, as only one sub-batch will need the GPU attention, compared to twice in symmetric pipelining."}, {"title": "3.2 Load-Aware Scheduling", "content": "Real-world inference workloads are complex, irregular, and dynamically changing, e.g., different input/output lengths across requests. NEO scheduler considers asymmetric pipelining and adaptively determines whether the incoming request should be placed on the GPU or CPU, to keep both busy while not overloading anyone.\nNEO faces a more challenging problem than prior GPU-only inference engines like vLLM. First, NEO needs to form two sub-batches in each iteration, which means we need to consider batch selection and batch splitting at the same time. Secondly, blindly putting as many requests as possible into the CPU decoding runqueue does not work well. This is because too many CPU requests might overload the CPU capacity either in memory bandwidth or compute. Finally, the two-batch asymmetric pipelining does not always work better than GPU-only inference. This is because in asymmetric pipelining, the GPU memory needs to hold two concurrently running sub-batches, and each sub-batch can only achieve half of the maximum batch size; if NEO cannot offload enough KV caches, the sub-batch size might be even smaller than the GPU-only inference, yielding lower throughput.\nTo address these challenges, NEO follows several principles:\n\u2022 Greedy: At the beginning of each iteration, NEO's scheduler would make a GPU-only inference schedule and a two-batch asymmetric pipelining schedule, and would choose the one with higher estimated throughput as the final decision.\n\u2022 Balancing: For asymmetric pipelining, the scheduler should minimize pipeline \u201cbubbles\". That is, the estimated CPU busy time and GPU busy time should be as close as possible.\n\u2022 Hiding CPU: For asymmetric pipelining, there shouldn't be cases when the CPU is busy but the GPU is idle.\n\u2022 Maximizing GPU: For asymmetric pipelining, in each iteration, the scheduler should pick as many requests as possible from the prefilling waitqueue, and GPU and CPU decoding runqueues (that hold requests running on GPU and CPU respectively).\nConcretely, denote each iteration time as $T$ and batch size as $x$, NEO's goal is to maximize $T/x$. As shown in Figure 5, $T$ consists of the following components: pre-layer time $T_{prl}$, transformer layer time $T_{tr}$, post-layer time $T_{pol}$, while $T_{tr}$ is the major part that usually takes more than 95% of time in each iteration. Therefore, we can estimate $T$ as below:\n$T \u2248 T_{tr} = L \u00d7 (max\\{T_{po0} + T_{pr0}, T_{ca1}\\} + max\\{T_{po1} + T_{pr1} + T_{ga0}, T_{ca0}\\})$\nwhere $T_{pon}, T_{prx}, T_{gax}, and T_{cax}$ denote time for post-projection, pre-projection, GPU attention, and CPU attention time for batch-x, respectively. For simplicity, we define $T_{lx} = T_{pox} + T_{prx}$, which stands for time mainly consumed by multiplying activations with model weights in one transformer layer. To estimate $T_{l}$, $T_{ga}$, and $T_{ca}$, NEO does offline profiling for typical input/output lengths and uses linear interpolation to approximate the values for other lengths. Overall, to stick to principles of \"Balancing\" and \u201cHiding GPU\u201d, we would like to guarantee $T_{lo} > T_{ca1}$ and $T_{l1} + T_{ga0} \u2265 T_{cao}$, and minimize the gap between the left-hand side and right-hand side of both inequations.\nBased on the analysis above, we present our load-aware scheduler, with the following procedures on each iteration:\n1. Initialization. Initialize two empty batch schedules: batch-0 would mostly run on the GPU and contain requests in both prefilling and decoding phases; batch-1 would mostly run on the CPU and only contain requests that calculate decoding attention.\n2. Schedule GPU decoding requests. Try to put every request from the GPU decoding runqueue into batch-0. Then swap out requests until GPU memory can hold all new KV cache, or swap in requests if there is ample space on GPU (Maximizing GPU).\n3. Schedule prefilling requests. Pop the prefilling waitqueue and put requests into batch-0: keep the generated KV cache on GPU if there is enough GPU memory, otherwise, swap out the generated KV cache. Do this repeatedly until the GPU cannot hold the activations in the batch (Maximizing GPU).\n4. Schedule CPU decoding requests. Scan the CPU decoding runqueue and put requests into either batch-0 or batch-1, maintaining $T_{cao} \u2264 T_{l1} +T_{gao}$ and $T_{ca1} \u2264 T_{lo}$. If putting a request into either batch would violate the inequations, skip this request and leave it for the next iteration (Balancing and hiding CPU).\n5. Reduce prefilling requests. Remove any prefilling request from batch-0 that would require swapping out the generated KV cache, as long as the above inequations hold. This is to avoid the CPU being idle (Balancing).\n6. Make decisions. Now the two-batch schedule is made; we make the GPU-only schedule by taking batch-0 and excluding all the CPU decoding requests added in step 4. Finally, we compare their estimated $T_{tr}/x$ values and pick the schedule with a larger one (Greed)."}, {"title": "4 IMPLEMENTATION", "content": "We implement NEO based on SwiftLLM\u2014a tiny yet powerful LLM inference system for research purposes. SwiftLLM achieves vLLM-equivalent (single GPU, default scheduling policy) performance with around 2K lines of code. Our NEO implementation consists of 4K lines of Python and 1.5K lines of C++.\nEfficient CPU Kernels. We implement a custom C++ torch extension library called Paged-Attention-for-CPU (PACPU) and plug it into SwiftLLM. The PyTorch runtime will directly call a multi-threaded procedure written in C++. Under the hood, PACPU calls yet another extension library written in ISPC\u2014a language for writing SPMD programs on CPUs. ISPC code can be compiled to targets with different sets of vectorized instructions, such as AVX2, AVX512, and ARM Neon, making it perfectly portable between different types of CPUs. PACPU utilizes a paged KV cache similar to vLLM with the Paged Attention algorithm to mitigate memory fragmentation.\nNEO optimizes memory access performance for decoding attention on CPU, as the decoding attention is heavily bounded by memory bandwidth (even with GQA). 1) Within a single CPU core, we utilize SIMD memory load/store instructions (features provided by ISPC) to minimize instruction overhead and enhance memory throughput. Additionally, we carefully organize the memory access order to ensure its contiguity. 2) Across different CPU cores, we use a parallelism strategy similar to Flash Decoding. For each request, we partition its computation into individual tasks along the request dimension, allowing each task to access unique and continuous memory at block granularity. These tasks are evenly dispatched to all threads with each thread having an equal number of blocks to process. Finally, we aggregate the partial outputs of each request to obtain the final result.\nReducing Kernel Launching Overhead. Due to Python's poor multi-threaded execution performance incurred by its global interpreter lock (GIL), the data plane CPU kernels could not run in parallel with the control plane CUDA kernel launching calls, making kernel launching blocking on the critical path. To reduce kernel launching overhead, we replaced most of SwiftLLM's Triton-JIT kernels with kernels written in CUDA C++. This can effectively reduce the additional kernel selection and launching overhead brought by Triton-JIT.\nMulti-GPU inference. We redesigned SwiftLLM's architecture to support model sharding and tensor parallelism, as it originally only supported single-GPU inference. We utilize Ray actors to hold shards of the model and use PyTorch's pre-built communication library (based on NCCL) to handle cross-GPU communication. The underlying mechanism of splitting tensors and collecting results across GPUs is the same as vLLM. As a standard approach, each Ray actor also has its partition of CPU KV cache, each responsible for a portion of KV heads to avoid cross-CPU communication."}, {"title": "5 EVALUATION", "content": "5.1 Experiment Setup\nTestbeds. We run our experiments on multiple types of single-GPU instances on the AWS EC2 public cloud, including g5.2/4/8/16xlarge with an A10G GPU and g4dn.4xlarge with a T4 GPU. By default, we use g5.4xlarge for all A10G experiments. We also run on an 8\u00d7H100 SXM local server to test multi-GPU performance. The specifications of hardware are listed in Table 1. Note that the HGX machine has 4 CPU NUMA nodes. We confine our system to running on a single NUMA node (thus 1/4th of the total memory size and bandwidth) when running 2-GPU experiments.\nModels. We evaluate NEO on the representative LLaMa models including LLaMa-3.1-8B, LLaMa-3.1-70B and LLaMa-2-7B.\nBaselines. We consider three baselines in our evaluation:\n\u2022 vLLM is a popular state-of-the-art LLM inference system. vLLM's default scheduling policy doesn't include selective batching; so we set the --enable-chunked-prefill flag to enable it.\n\u2022 FastDecode is an LLM inference system that offloads full decoding attention. Since the original work didn't consider the prefilling stage or implement an end-to-end system, we implemented our own version of FastDecode+. It leverages NEO's asymmetric pipelining and load-aware scheduling, but offloads all requests' decoding attention to the host CPU.\n\u2022 SwiftLLM is a simplified version of vLLM with similar Pythonic implementation and without offloading, upon which NEO is built. We also make simple modifications to SwiftLLM to support multi-GPU inference. It achieves comparable performance with VLLM on a single GPU, and slightly lower performance than vLLM in 2-GPU settings (\u00a75.5).\nWorkloads. We use real-world workloads as well as synthetic ones to evaluate our system.\n\u2022 Azure LLM inference trace for coding (AC) is a LLM coding trace collected from Azure cloud's production environment.\n\u2022 OpenAI summarization comparison (OSC) is an open dataset of input text, chosen summary, and rejected summary produced in real-world human chatbot interactions.\n\u2022 Synthetic workloads with various input and output lengths. For a pair of input length $l_i$ and output length $l_o$, we synthesize requests with input and output lengths sampled independently and uniformly from $[0.9l_i, 1.1l_i]$ and $[0.9l_o, 1.1l_o]$, respectively.\nWe use the AC trace with relatively longer requests on the H100 and A10G GPUs, while using the OSC trace with shorter requests on the lower-end T4 GPU."}, {"title": "5.2 Online Latency vs. Load", "content": "We evaluate the online latency of NEO under various request rates. We sample request arrival timestamps following the Poisson process. As Figure 6 shows, NEO sustains higher loads than vLLM in all listed hardware/model settings while providing comparable latencies at low rates. NEO achieves 14.3% higher throughput on H100 (at 2 sec latency), 6.40% higher on A10G (at 2 sec latency), and 563% higher on T4 (at 1 sec latency). In the T4+LLaMa-2-7B setting, we achieve nearly 6\u00d7 throughput gains over vLLM; this is because the T4 GPU has an extremely constrained memory budget for the KV cache, severely limiting vLLM batch size, while NEO could offload the KV cache to the CPU, achieving a much larger batch size.\nAt extremely low request rates, NEO's latency behaves exactly the same as vLLM. As the request rate grows, NEO's latency gets slightly higher for the following reasons: 1) vLLM is heavily optimized, while NEO features less engineering optimizations for simplicity and performance clarity-this is especially true in multi-GPU settings. 2) NEO actively seeks opportunities to offload requests, even though offloading may not help, which consequently leads to more system-level overheads in scheduling and swapping.\nFigure 7 further compares the latency distributions of NEO and vLLM, and shows that our throughput gains do not come at the cost of latency. Our inference latency is comparable to vLLM's at all percentages."}, {"title": "5.3 Comparison with FastDecode+", "content": "We compare NEO and FastDecode+ in terms of both latency and throughput. The results are shown in Figure 8. FastDecode+'s higher latency is caused by its inflexibility to tackle irregular workloads. When there are many requests in the CPU decoding runqueue but few or no requests in the prefilling waitqueue, FastDecode+ would have no choice but to launch CPU batches to make progress, hindering overall performance, while NEO could simply launch GPU batches to utilize GPU resources.\nFurthermore, NEO is always better in throughput than baseline because its scheduler can always decide to fall back to GPU-only mode. However, as output length grows, FastDecode+ becomes CPU bounded, and its performance drops quickly to less than 60% of baseline's performance."}, {"title": "5.4 Varying Input/Output Lengths", "content": "We further examine NEO's performance over various workloads with different input/output lengths. As shown in Figure 9, where we fix input length and tweak output length, NEO achieve up to 14%, 26%, and 750% throughput gains on H100, A10G, and T4, respectively. When the output length is short, NEO may perform slightly worse than the baseline because NEO still attempts to put some requests onto the CPU and swap them back in later, incurring slight overheads. As the output length increases, NEO's gains first grow to the maximum point, where GPU and CPU times are exactly balanced, then gradually drop as the system launches a larger proportion of GPU-only batches. NEO performance would be close to the baseline when output length gets large enough, sometimes slightly worse due to suboptimal scheduling decisions caused by the inevitable inaccuracy of the offline performance profiling."}, {"title": "5.5 Sensitivity Study", "content": "Impact of CPU capacity. We first study the impact of CPU capacity on throughput gains. To examine the impact of CPU memory bandwidth, we use 4 kinds of AWS EC2 instances, namely g5.2xlarge, g5.4xlarge (what we used in previous experiments), g5.8xlarge, and g5.16xlarge. These instances all have 1 A10G GPU and thus the same baseline (non-offloading) performance. However, their offloading performance varies due to different CPU cores, memory sizes, and memory bandwidths. A g5.nxlarge generally has 2n CPU cores (i.e., 4n hyperthreads) and 16n GB CPU memory; g5.2xlarge has nearly the same peak memory bandwidth as g5.4xlarge, while g5.8xlarge has about twice the bandwidth of g5.4xlarge, and g5.16xlarge has about twice the bandwidth of g5.8xlarge. In experiments, we set the CPU KV cache size proportionally to the number of cores. Figure 10a shows the results. NEO achieves up to 12.2%, 13.3%, 29.7%, and 79.3% higher throughput over the baseline under different CPU capacities. When the output length is short, these instances have nearly the same throughput gains as the workload mainly runs on the GPU. As output length increases, the instances with less CPU memory bandwidth start to drop performance earlier. The peak throughput gain is positively related to the CPU memory bandwidth. This supports the fact that the memory bandwidth, rather than computing power (i.e., number of cores), is the factor that determines the performance of attention operation on CPUs.\nSwiftLLM vs. vLLM. We now examine the gap between our baseline system and vLLM. We feed the Azure Code trace all at once to both systems and examine the GPU token throughput, i.e., the total time elapsed divided by the total number of tokens processed (input length + output length). We evaluate in both single-GPU (A10G + LLaMa-3.1-8B) and multi-GPU (2xH100 + LLaMa-3.1-80B) settings.\nFigure 10b shows the results. As SwiftLLM is initially targeted at single-GPU inference and mimics vLLM implementation, it achieves comparable throughput with vLLM in"}, {"title": "6 DISCUSSION", "content": "Compare to chunked-prefill. NEO's performance benefits are essentially two-fold: 1) larger GPU batch size, and 2) shifting some unbatchable decoding attention operations to CPU, thus making up room on GPU for other batchable operations. The second benefit source shares a similar spirit as the chunked-prefill technique in Sarathi-Serve, which breaks a prompt into multiple chunks to launch more prefill-decode mixed batches and thus less decoding-only batches (that contain unbatchable decoding attention operations).\nHowever, chunked-prefill suffers from several drawbacks that NEO does not have. First, chunked-prefill consumes significantly more GPU memory bandwidth, because the KV cache of all previous chunks needs to be loaded repetitively to compute for the subsequent chunk. Second, chunking-prefill would not work well on memory-constrained GPUs, because the resulting small batch size would limit the opportunity of piggybacking decode on prefill chunks while saturating the GPU. In contrast, NEO relies on large CPU memory to ensure throughput gains. Nevertheless, we believe NEO could integrate with chunked-prefill, providing a larger design space. We leave it for future work.\nOffloading other parts to CPU. NEO implicitly assumes putting all model weights on the GPU and only offloading attention computation to the CPU is the most efficient way to balance GPU and GPU loads. However, in prior work like and , people offload components other than attention to the CPU. This is not only useful when the GPU's memory is too constrained to hold all model weights, but also potentially beneficial even if the GPU could hold all the model weights. For example, when requests in the workload have too few output tokens, NEO would be bounded by GPU computation, while the CPU is mostly idle. Therefore, offloading some of the dense operations to the CPU could alleviate the GPU's pressure in these extreme workloads. Nevertheless, the actual gain needs to be validated by further exploration.\nNEO usage scenarios. NEO works best in scenarios where the GPU memory is constrained such that it limits the batch size and underutilizes the GPU compute. These scenarios will likely hold for a long time, as the GPU compute capacity continues growing while its memory size stays relatively stagnant, e.g., H100 triples the compute of A100 but with the same 80GB memory size. NEO will degrade to non-offloading mode when the GPU has enough memory to reach a batch size that can saturate the GPU. Another usage scenario of NEO is the economic serving of LLM models by leveraging cheap and abundant CPU resources that already exist in current datacenters.\nUsing remote CPUs. NEO focuses on improving inference throughput by only using the host CPU. In order to gain more throughput, NEO could be extended to support remote CPU workers. However, CPU memory bandwidth in current commercial clouds is still expensive, and as shows, cross-machine transfer latency could be yet another bottleneck. We leave this for future work."}, {"title": "7 RELATED WORK", "content": "GPU-efficient LLM inference. There is a line of work on optimizing the efficiency of LLM inference on GPUs, including general inference systems from Orca, vLLM, SGLang, FastServe, Sarathi-Serve, NanoFlow, DistServe, and more , and low-level GPU kernel optimizations from FlashDecoding , FlashDecoding++, and FlashInfer. NEO leverages several techniques from these work such as selective batching and paged attention , and could be used in parallel with others, e.g., leveraging NEO to optimize the decoding phase in Dist-Serve. Another line of work leverages sparcification and quantization techniques to trade accuracy for performance, including AWQ , SparseGPT , AlphaTuning, GPT3.int8() , GPTQ , ZeroQuant , SmoothQuant , StreamingLLM , and more . Different from them, NEO does not compromise accuracy.\nOffloading for LLM serving. Many existing work offloads LLM models, activations, KV cache, or computations to the CPU for offline scenarios that trade latency for throughput, such as FlexGen , HeteGen, PowerInfer , and TwinPilots. InstInfer further offloads to computational SSD to lower the inference cost. FastDecode targets similar online scenarios as NEO, but lacks critical designs to address the load imbalance between the GPU and CPU (see \u00a71)."}, {"title": "8 CONCLUSION", "content": "NEO is a CPU offloading system for online LLM inference to increase GPU batch sizes and improve inference throughput. It features asymmetric pipelining and load-aware scheduling to fully leverage both GPU and CPU resources without overloading them. NEO achieves up to 14%-7.5\u00d7 (depending on GPUs) higher throughput than GPU-only inference systems across a variety of workloads and model sizes, while maintaining the same latency. We will open source NEO codebase to encourage more research on cost-efficient LLM inference."}]}