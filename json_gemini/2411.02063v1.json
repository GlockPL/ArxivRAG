{"title": "Scalable Efficient Training of Large Language Models with\nLow-dimensional Projected Attention", "authors": ["Xingtai Lv", "Ning Ding", "Kaiyan Zhang", "Ermo Hua", "Ganqu Cui", "Bowen Zhou"], "abstract": "Improving the effectiveness and efficiency of\nlarge language models (LLMs) simultaneously\nis a critical yet challenging research goal. In\nthis paper, we find that low-rank pre-training,\nnormally considered as efficient methods that\nwill compromise performance, can be scal-\nably effective when reduced parameters are\nprecisely targeted. Specifically, applying the\nlow-dimensional module only to the attention\nlayer - resolves this issue and enhances both\neffectiveness and efficiency. We refer to this\nstructure as Low-dimensional Projected Atten-\ntion (LPA) and provide an explanatory analysis.\nThrough extensive experimentation at parame-\nter scales of 130M, 370M, and scaling up to 3B,\nwe have validated the effectiveness and scala-\nbility of LPA. Our results show that LPA model\ncan save up to 12.4% in time while achieving an\napproximate 5% improvement in test perplexity\n(ppl) and on downstream tasks compared with\nthe vanilla Transformer.", "sections": [{"title": "1 Introduction", "content": "Improving large language models' (LLMs) (Bom-\nmasani et al., 2021; Han et al., 2021; Brown\net al., 2020; Touvron et al., 2023; Zhou and\nDing, 2024) effectiveness and efficiency simultane-\nously presents challenges due to inherent trade-\noffs, which remains a critical research goal in\nthe research field. Among series methods pro-\nposed to alleviate this issue, parameter-efficient\nfine-tuning (Houlsby et al., 2019; Li and Liang,\n2021; Zaken et al., 2021; Ding et al., 2023b) of-\nfer valuable insights. Notably, low-rank or low-\ndimension techniques such as LoRA (Hu et al.,\n2021) demonstrate on-par or even enhanced perfor-\nmance over traditional full-parameter fine-tuning\nwith reduced computational resources.\nIntuitively, besides the fine-tuning phase, adapt-\ning LoRA's principles to the pre-training phase\nthrough low-rank decomposition is both viable and\npromising, which can yield substantial benefits if\neffectiveness is maintained. However, existing stud-\nies have found that the direct low-rank pre-training\noften compromises the effectiveness. To reduce\nsuch effects, strategies such as iteratively accumu-\nlating low-rank updates (Lialin et al., 2023) or in-\nte grating low-rank decomposition directly into the\ngradient (Zhao et al., 2024) have been suggested.\nWhether it's the original LoRA or these improved\nmethods, they all involve performing low-rank de-\ncomposition and updates on \"amounts of change\"\n(weights or gradients), and do not reduce the num-\nber of parameters in the model itself, which face\nobstacles in maintaining efficiency during subse-\nquent inference and fine-tuning stages. Therefore,\nan ideal scenario would be permanently reducing\nthe number of parameters (computational load)\nthrough efficient methods, without compromising\nor even enhancing the performance of pre-trained\nmodels.\nTo achieve this goal, is it feasible to directly\nperform low-rank decomposition on the matrices\nin the model itself, rather than on the changes?\nCurrent limited research suggests that existing low-\nrank pre-training methods experience performance\nlosses and uncertainties (Lialin et al., 2023; Zhao\net al., 2024), with even fewer studies exploring\nmore direct approaches. However, in this paper, we\ndemonstrate that such direct low-rank pre-training\nis feasible, provided that the parameters to be re-\nduced are more precisely targeted. Specifically, we\ndescribe the reduction of parameters as replacing\nthe original matrices with low-dimensional mod-\nules. We find that using low-dimensional modules\nin the feed-forward neural (FFN) layers or across\nall layers negatively impacts the model's effective-\nness. However, we observe that employing them in\nthe attention layers consistently allows the model\nto outperform the original Transformer. We refer\nto this structure as Low-dimensional Projected At-"}, {"title": "2 Related Work", "content": "Low-rank Parameter-efficient Fine-tuning.\nParameter-efficient fine-tuning optimize only a tiny\nportion of parameters while keeping the majority\nof the neural network frozen (Houlsby et al., 2019;\nLi and Liang, 2021; Lester et al., 2021; Hu et al.,\n2021; Zaken et al., 2021; Ding et al., 2023a),\nsaving significant time and computational costs\nand achieving performance comparable to full\nparameter fine-tuning on many tasks (Ding et al.,\n2023b). Low-rank adaptation (LoRA) is one of the\nmost effective and influential parameter-efficient\nfine-tuning methods, having found widespread\napplication (Dettmers et al., 2023). The LoRA\nmethod involves freezing the weights W0 of the\npre-trained model while training two low-rank\ndecomposition matrices Wu and Wd, resulting in\nthe output of the LoRA module being represented\nas z Wox + WuWax. We drew inspiration\nfrom LoRA and its improvement works, adapting\nthem to the pre-training process to enhance\neffectiveness and efficiency of the model.\nLow-rank Pre-training for Neural Network.\nSome efforts have focused on making pre-training\nmore efficient by reducing the number of train-\nable parameters (Lin et al., 2020; Yuan et al.,\n2020), and after finding that modules with low-\ndimension often yield poor results (Bhojanapalli\net al., 2020), many works have concentrated on\ncombining two low-rank matrices to reduce the pa-\nrameter count while keeping the module dimension-\nality constant (Schotth\u00f6fer et al., 2022; Idelbayev\nand Carreira-Perpin\u00e1n, 2020; Zhao et al., 2023;\nThangarasa et al., 2023). Current research has\npredominantly emphasized refining pre-training\nmethods for CNN networks (Sui et al., 2024; Jader-\nberg et al., 2014) or employing smaller language\nmodels (Kamalakara et al., 2022). However, some\nstudies have found that low-rank pre-training can\nnegatively impact model performance and train-\ning effectiveness, leading to the use of low-rank\nupdates to train high-rank networks or the intro-\nduction of low-rank decomposition in gradient for\noptimization (Lialin et al., 2023; Zhao et al., 2024).\nAdditionally, Liu et al. 2024 introduces low-rank\nlatent states in the attention layer, successfully op-\ntimizing the KV cache.\nWe discover that the unsatisfactory performance\nof the direct low-rank pre-training stems from\nthe lack of precise parameter reduction placement.\nThis insight guides our further exploration into the\nimpact of low-dimensional modules and their ap-\nplications at various locations within the model on\nboth effectiveness and efficiency."}, {"title": "3 Low-dimensional Projected Attention", "content": "We use a low-dimensional module for replacing\nthe original weight matrix, and observe the vary-\ning effects of incorporating the low-dimensional\nstructure in different modules. We provide an ex-\nplanatory analysis of these findings and propose\nthe Low-dimensional Projected Attention (LPA).\nAdditionally, we examine the efficiency of this ap-\nproach."}, {"title": "3.1 Low-dimensional Module", "content": "The low-dimensional module is constructed by se-\nquentially connecting two low-dimensional matri-\nces. Specifically, given a predetermined hyper-\nparameter r, which is typically less than din dout\nthe low-dimensional module comprises two ma-\ntrices WA \u2208 Rdin\u00d7r and WB \u2208 Rrxdout, where\ndin and dout represent the input and output di-\nmensions of the parameter matrix, respectively.\nThe input data x \u2208 RL\u00d7din passes through WA\nand WB sequentially, and the forward propaga-\ntion of the low-dimensional module is expressed\nas z \u2190 WB (WA(x)). The low-dimensional\nmodule is employed to displace the weight met-\nric W\u2208 Rdin\u00d7dout in linear layers of the original\nmodel, such as the weight in the Query sublayer of\nthe attention layer.\nFor the classic Transformer architecture, the for-"}, {"title": "3.2 Position Optimization of Low-dimensional\nModule", "content": "The model performance may be influenced by the\nposition of the low-dimensional module within the\nmodel, a phenomenon akin to what has been widely\nobserved in the field of parameter-efficient finetun-\ning (Zaken et al., 2021; Hu et al., 2022; Zhang et al.,\n2023; Ding et al., 2023a). In order to validate this\ninfluence and ascertain the appropriate position, we\napply the low-dimensional module separately in the\nattention layers, FFN layers, and across all layers.\nThe resulting models are based on the 135M and\n369M Transformers, and we adjust the hyperparam-\neter r to ensure that the parameter count of these\nmodels remains approximately consistent across\nthese three position settings.\nTo confirm the robustness of the optimal low-\ndimensional module position, we apply it in two\ndifferent Transformer model settings, each con-\ntaining only decoders. The Model Setting 1 em-\nploys the Layer Normalization (Ba et al., 2016)\nand the \"ATTN(FFN)-Norm-Add\" regularization\nprocess, with ReLU (Fukushima, 1975) as the acti-\nvation function. The corresponding models are pre-\ntrained on the WikiText-103 dataset (Merity et al.,\n2016), which contains 0.1B tokens. The Model Set-\nting 2 uses RMS Normalization and the same FFN\nlayer as in LLaMA (Touvron et al., 2023), along\nwith the \"Norm-ATTN(FFN)-Add\" regularization\nprocess. The corresponding models are pre-trained\non the Pile dataset (Gao et al., 2020), using 2.6B\ntokens for the 130M parameter model and 6.8B\ntokens for the 370M parameter model.\nThe perplexities of these pre-trained models on"}, {"title": "3.3 Explanation for Position Optimization", "content": "Our preliminary experiments indicate that the op-\ntimal position for low-dimensional modules in the\nTransformer architecture is the attention layer. Fur-\nther detailed observations reveal that applying low-\ndimensional modules to the FFN layers diminishes\nthe model's effectiveness compared to the original\nTransformer model, whereas applying them to the\nattention layers enhances the model's performance,\nparticularly in the 370M parameter setting.\nOn one hand, according to Lemma 1 and\nLemma 2, the attention layer cannot independently\nmap individual tokens, whereas the FFN layer per-\nforms computations for each input token indepen-\ndently. On the other hand, the FFN layer typi-\ncally projects inputs to a high-dimensional space,\nwhile the attention layer does not engage in simi-\nlar operations. We posit these differences are the\nprimary reasons for the positive effect of applying\nlow-dimensional modules within the attention layer,\ncontrasted with their negative impact in the FFN\nlayer. Detailed empirical explanations are provided\nin Appendix B, based on the perspective of viewing\nthe introduction of low-dimensional modules as a\ntwo-step projection.\nLemma 1. In the attention layer, for the input vec-\ntor xi \u2208 R1\u00d7din of the i-th input token, the corre-\nsponding output zi \u2208 R1\u00d7dout satisfies\nzi\u2190 S\n(\nxWQWXT\n\u221ad\n);\nxWvWo, (5)\nindicating that zi is dependent on all the vectors in\nthe input x, especially for the computation in the\nKey, Value layers.\nLemma 2. In the FFN layer, the output zi \u2208\nR1\u00d7dout corresponding to xi \u2208 R1\u00d7din satisfies\nzi \u2190 d (xiWU)WD, (6)\nimplying that zi is only dependent on xi instead of\nother vectors in the input x.\nHowever, when the original model has a low\nparameter count, applying low-dimensional mod-\nules to the attention layer degrades the effect of\nprojection, leading to a noticeable decline in the\nmodel's capacity to fit the data. As a result, this\nmethod is effective only for models with a larger\nparameter count, with a critical threshold between\n130M and 370M parameters, as identified in our\npre-experiments in Section 3.2\nTherefore, applying low-dimensional modules to\nthe attention layer is the optimal strategy in Trans-\nformer models. This essentially involves two-step\nprojection through a low-dimensional space within\nthe attention layer, and we term this model architec-\nture Low-dimensional Projected Attention (LPA)."}, {"title": "3.4 Methodological Efficiency", "content": "The core architecture of the LPA model is com-\nposed of low-dimensional modules. Because of\nthe lower parameter number in these modules,\npre-training LPA model reduces memory con-\nsumption and is more conducive to large-scale"}, {"title": "4 Experiments", "content": "Extensive experiments are conducted to validate\nthe effectiveness of LPA across models of various\nscales, particularly emphasizing its efficacy with\nthe 3.23B models. Furthermore, we investigate the\nimpact of hyperparameter r on LPA, whether apply-\ning the low-dimensional module to all sublayers in\nthe attention layer is necessary, and the allocation\nof surplus parameters."}, {"title": "4.1 Effectiveness of LPA", "content": "Experimental Settings. To validate the effective-\nness and robustness of the LPA architecture, we\nconduct experiments with two model settings in-\ntroduced in Section 3.2, pre-training models with\nparameter sizes of 130M and 370M. For Model\nSetting 1, we use the WikiText-103 dataset (Mer-\nity et al., 2016), consisting of 0.1B tokens, and\nset r of LPA to 256. For Model Setting 2, we pre-\ntrain the models using 2.6B tokens from the Pile\ndataset (Gao et al., 2020) for the 130M parameter\nmodel and 6.8B tokens for the 370M parameter\nmodel, with the LPA architecture r set to 128 or\n256. Detailed model configurations and training\nhyperparameters are provided in Table 9 in Ap-\npendix A. For the implementation of our models,\nwe leverage the Huggingface Transformers (Wolf\net al., 2020) and PyTorch (Paszke et al., 2019)\nframeworks. Our computational infrastructure is\npowered by the NVIDIA GeForce RTX 3090 (max-\nimum GPU memory=24GB), NVIDIA A800 (max-\nimum GPU memory=80GB), and NVIDIA A6000\n(maximum GPU memory=48GB).\nAs indicated in Table 9, the parameter count\nof the LPA model typically ranges from 75% to\n90% of the corresponding Transformer, referred\nto as the Same-Dim Transformer. To compare the\nperformance of the LPA and Transformer models\nunder the same parameter settings, we also pre-\ntrain Transformer models with parameter counts\nnearly equal to those of LPA models. For each\nmodel, repeated pre-training with 3 random seeds is\nperformed, and following pre-training, we evaluate\nthe models on test datasets, using perplexity (ppl)\nas the performance metric.\nResults and analysis. The mean test perplexity\nand standard deviation for each model are pre-"}, {"title": "4.2 Scaling up to 3.23B", "content": "In this section, experiments are conducted on the\n3B-scale models, including the pre-training of\na 2.43B LPA model, a 3.23B Same-Dim Trans-\nformer, and a 2.49B Transformer with nearly the\nsame parameter count as the LPA model. Inspired\nby LLaMA (Touvron et al., 2023), we adopt the\npre-normalization for these large models. Com-\npared to pre-training smaller models, we utilize a\nlarger dataset, specifically 13% of the Pile dataset,\namounting to 51B tokens, without data repetition\nduring pre-training. Additional hyperparameters\nfor the model architecture and training settings are\ndetailed in Table 10 in Appendix A."}, {"title": "4.3\nDownstream Tasks Performance", "content": "To further demonstrate the superiority of the\nLPA model over the Transformer, in addition\nto comparing test perplexities, we also evalu-\nate the performance of the pre-trained 369M\nTransformer and the 319M LPA model with\nModel Setting 1 on downstream tasks. Us-\ning the GLUE benchmark (Wang et al.), which"}, {"title": "4.4 Apply LPA with Different r", "content": "For the LPA, r is the most critical hyperparame-\nter, and it is essential to investigate the impact of\ndifferent r on the performance of the LPA mod-\nels. We pre-train a 369M Transformer with Model\nSetting 1 and the corresponding LPA models with\nr set to 256, 128, 64, and 32, followed by con-\nducting repeated experiments with 3 random seeds\nand computing the average test perplexity for each\nconfiguration.\nFigure 3 shows the training loss curves of these\nmodels, and Table 6 presents the test perplexity re-"}, {"title": "4.5 Apply Low-dimensional Module to\nDifferent Sublayers in Attention", "content": "In the aforementioned experiments, we apply the\nlow-dimensional module to all sublayers of the\nattention layer, including the Query, Key, Value,\nand Output layers. In this section, we explore\nwhether applying the low-dimensional module to\nonly some sublayers can achieve better results. We\ndesign combinations of sublayers to which the low-\ndimensional module is applied based on the func-\ntional characteristics of them. Specifically, accord-\ning to Lemma 1, the computations in the Key and\nValue layers require all the vectors in the input\nx. Additionally, the Query, Key, and Value lay-\ners collectively handle the computation of the re-\nlationships between the input tokens. Therefore,\nwe consider two configurations in the experiments:\napplying the low-dimensional module to the Key\nand Value layers, and applying it to the Query, Key,\nand Value layers, which are denoted as LPAK,V\nand LPAQ, K,V, respectively."}, {"title": "4.6 Allocating Surplus Parameters across\nModules", "content": "The reduced parameter of the LPA model compared\nto the Same-Dim Transformer presents an oppor-\ntunity to allocate the saved parameters to other\nmodules of the model, which is a worthwhile av-\nenue to explore for further enhancing the model's\neffectiveness. Building upon the LPA model, we\nrespectively allocate the parameters in three ways:\n(1) Attn Dim. Increasing the output dimensions\nof WQ, WK, Wy and the input dimensions of\nWo in attention layers. (2) FFN Dim. Expand-\ning the output dimensions of the up-project matrix\nWu and the input dimensions of the down-project\nmatrix WD in the FFN layers. (3) Layer Num.\nEnlarging the number of layers in LPA model. We\nconduct repeated experiments with Model Setting\n1, using the same training settings and 3 random\nseeds for the Transformer and LPA model, and the\naverage test perplexities are presented in Table 8."}, {"title": "5 Conclusion", "content": "This paper demonstrates that low-rank pre-training\ncan enhance both the effectiveness and efficiency\nof LLMs when reduced parameters are precisely\ntargeted. By incorporating low-dimensional mod-\nules specifically in the attention layers, we develop\nthe Low-dimensional Projected Attention (LPA),\nwhich outperforms Transformers without the effi-\nciency compromises. Our empirical analysis and\nexperiments show that LPA maintains its effective-"}, {"title": "Limitations", "content": "Despite the encouraging results demonstrated by\nthis paper, certain limitations in our current study\nare worth acknowledging. First of all, our expla-\nnation in Section 3.3 is empirical rather than a rig-\norous theoretical explanation with mathematical\nderivation. Furthermore, due to computational re-\nsource limitations, we conduct experiments with a\n3B parameter scale on only one Transformer model\nsetting and don't verify the effectiveness of LPA\nat larger parameter scales. Last, we find that the\nefficiency of LPA during the pre-training phase is\nnot very apparent, which may require the introduc-\ntion of KV cache because LPA has the potential to\nreduce KV cache, but we don't explore this further."}, {"title": "Acknowledgements", "content": "This work is supported by the National Science\nand Technology Major Project (2023ZD0121403),\nYoung Elite Scientists Sponsorship Program by\nCAST (2023QNRC001), National Natural Science\nFoundation of China (No. 62406165)."}, {"title": "B Explanatory Analyses for Phenomena\nDescribed in Section 3.2", "content": "There are two primary empirical explanations\nfor the different effects when applying the low-\ndimensional modules to the attention layer and\nFFN layer. First, the parameter matrix with low-\ndimensional modules can be viewed as a two-step\nprojection, which involves first mapping the in-\nput data into a low-dimensional space and then\nback into the target space. Typically, the FFN\nlayer projects the input into a high-dimensional\nspace via Wu, processes it with the non-linear\nactivation function, and then maps it back to\nthe original space via WD. The heavy reliance\non the high-dimensional space of the FFN lay-\ners means that introducing low-dimensional space\nthrough low-dimensional modules negatively im-\npacts it. Additionally, for each token in the in-\nput consisting of L tokens, considering Lemma 1\nand SxWQWxT\n\u2208 R1XL, the softmax com-\nputation in the attention layer results in one-\ndimensional weight data for L tokens, indicating\nthat the attention layer is less sensitive to the dimen-\nsionality of the input space. Hence, introducing a\nlow-dimensional space has a minimal negative im-\npact on the attention layer.\nSecondly, for the input data which comprises\nL tokens, based on Lemma 2, the projection of\nthese L tokens in the FFN layer is independent, ef-\nfectively processing them sequentially. In contrast,\nbased on Lemma 1, the computation in the attention\nlayer involves the relationships between each input\ntoken and all L tokens. Theoretically, since the\nprojection can be optimized to any possible choice,\nprojecting data into a low-dimensional space be-\nfore mapping it back to the target space should\nnot affect the size of the output space. However,\nin practice, this operation tends to concentrate the\noutput in several subspaces within the target space,\nreducing the output space size, which constrains\nthe possible output values and makes it harder to\nidentify the optimal weight point.\nThis negative impact is substantial for the FFN\nlayer, but for the attention layer, the reduced out-\nput space implies that the data points for input\ntokens are closer together, making their relation-\nships easier to capture. Consequently, applying the\nlow-dimensional module to the attention layers can\nenhance the model's effectiveness."}]}