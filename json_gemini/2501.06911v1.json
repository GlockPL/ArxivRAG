{"title": "Risk-Averse Fine-tuning of Large Language Models", "authors": ["Sapana Chaudhary", "Ujwal Dinesha Dileep", "Kalathil Srinivas Shakkottai"], "abstract": "We consider the challenge of mitigating the generation of negative or toxic content by the Large Language Models (LLMs) in response to certain prompts. We propose integrating risk-averse principles into LLM fine-tuning to minimize the occurrence of harmful outputs, particularly rare but significant events. By optimizing the risk measure of Conditional Value at Risk (CVaR), our methodology trains LLMs to exhibit superior performance in avoiding toxic outputs while maintaining effectiveness in generative tasks. Empirical evaluations on sentiment modification and toxicity mitigation tasks demonstrate the efficacy of risk-averse reinforcement learning with human feedback (RLHF) in promoting a safer and more constructive online discourse environment. Trigger Warning: This paper contains prompts and model outputs that can be offensive in nature.", "sections": [{"title": "Introduction", "content": "The deployment of large language models (LLMs) is witnessing remarkable growth across both personal and professional domains [Nakano et al., 2021, Touvron et al., 2023]. While a majority of users utilize LLMs via relatively innocuous prompts, a minority might do so with negative or toxic prompts, leading to the generation of content that violates acceptable norms [Bai et al., 2022a, Ganguli et al., 2022, Bai et al., 2022b], restricting LLM usage in innovative applications with broad societal impacts. In this work, we aim to answer \"Can LLMs be fine-tuned to avoid such outputs?\".\nThe key idea that we explore in this work is to bring the notion of risk-averseness into the realm of LLMs. Unlike the traditional fine-tuning approach of Reinforcement Learning from Human Feedback (RLHF), which seeks to maximize the expected reward in a risk-neutral manner, we seek to optimize a risk measure of the generated trajectories. The specific measure that we use follows Conditional Value at Risk (CVaR), which minimizes the expected cost, conditioned on it being greater than a certain quantile value a [Tamar et al., 2015, Greenberg et al., 2022]. In other words, we seek to minimize the toxicity or negativity, specifically of rare but high-stakes events that might occur. This is in contrast to the existing approach of safety-constrained RLHF [Dai et al., 2023], which constrains the expected harmfulness score of the output within limits. Constraining expectation means that the scores of positive trajectories can offset those of negative trajectories, rather than explicitly constraining the probability of toxic outputs. Additionally, this approach necessitates learning two separate reward/preference models.\nOur objective is to develop a risk-averse RLHF (RA-RLHF) algorithm to utilize pre-collected prompts and their associated responses, which have varying levels of negativity or toxicity, to fine-tune an LLM to be risk-averse. Several ideas need to come together to realize such an approach. The two elements that must be considered during each policy optimization step are the risk-level quantile that we train against in that step, and the batch size of data to be used in that step. We use a soft-risk"}, {"title": "Related Work", "content": "Alignment: LLMs have shown remarkable proficiency in text/language generation tasks [Vaswani et al., 2017, Radford et al., 2019, Brown et al., 2020, Devlin et al., 2018, Bubeck et al., 2023]. Despite their inherent capabilities, optimizing these models for specific downstream tasks necessitates additional strategies. One approach involves adapting the language model training to be multi-task oriented, as exemplified by the T5 family of instruction-tuned models [Raffel et al., 2020]. Alternatively, aligning these models with downstream task data through specialized techniques can be effective. Specialized techniques such as retrieval augmented generation (RAG) [Lewis et al., 2020], supervised fine-tuning (SFT) [Howard and Ruder, 2018], and fine-tuning via human feedback (RLHF) [Christiano et al., 2017, Ziegler et al., 2019, Stiennon et al., 2020, Ouyang et al., 2022] or AI feedback (RLAIF) [Lee et al., 2023] represent pivotal methods for enhancing downstream task performance in large language models. Among these, RLHF has shown notable success in aligning LLMs with human preferences, making it a focal point of study in this paper.\nSafety and risk considerations: LLMs are typically trained on vast datasets sourced from the internet, encompassing a wide spectrum of content ranging from positive and neutral to negative and potentially toxic. Consequently, unaligned versions of LLMs have been documented to generate harmful content, as evidenced by recent studies [Sheng et al., 2019, Wallace et al., 2019] which highlight the risks associated with uncurated training data. Furthermore, even aligned versions of LLMs are not immune to exploitation. The aligned models can still be prompted or \u2018red-teamed' to produce harmful content under certain conditions [Gehman et al., 2020, Weidinger et al., 2021, Ganguli et al., 2022, Deshpande et al., 2023]. This underscores the complexity of mitigating risks in LLM deployment and the necessity for robust, ethical alignment strategies. Algorithmically including safety in LLM generations is a budding area of research. Recent works have tackled safe generation by means of learning appropriate preference models [Bai et al., 2022a, Ganguli et al., 2022, Dai et al., 2023], finetuning on curated data [Solaiman and Dennison, 2021, Lu et al., 2022], and expert assisted or rule based decoding [Krause et al., 2020, Liu et al., 2021, Liang et al., 2021, Cao et al., 2023]. These methods either require additional human/expert feedback [Bai et al., 2022a, Ganguli et al., 2022, Dai et al., 2023, Solaiman and Dennison, 2021] or correct for token level toxicity/bias at the expense of overall model performance. In both Bai et al. [2022a], Ganguli et al. [2022], safety is induced in LLMs by finetuning using a single reward or preference model (helpfulness and harmlessness (HH) model), as is the case in our work.\nRisk averseness in RL: In the RL community, risk averseness to ensure safe policy execution has been studied using various risk criteria. Examples of these criteria include mean-variance, entropic and distortion risk measures [Sato et al., 2001, La and Ghavamzadeh, 2013, Prashanth and Ghavamzadeh, 2016, Xie et al., 2018, Vijayan et al., 2021]. A more studied criterion is Conditional"}, {"title": "Preliminaries", "content": "In this work, we frame the problem of generative language modeling as a token-level Markov decision process (MDP) [Ramamurthy et al., 2022]. An MDP is the fundamental mathematical framework used to study sequential decision-making problems in reinforcement learning (RL). Our MDP comprises of the tuple < S, A, r, \u03b3, P, po >. Here, S denotes the state space. Each st \u2208 S at time step t is a sequence of language tokens (X1, X2, X3, ..., xt) generated until the current time step. Each token xt comes from a finite vocabulary or action space A. At any time step t, action at \u2208 A is the next token Xt+1 predicted by the language model. The probability of landing in a state St+1 \u2208 S after taking an action at \u2208 A in the state st \u2208 S is given by the transition probability distribution P(St+1|St,at) : S \u00d7 A \u2192 \u2206(S). In the case of language modeling, Xt+1 = at making P(st+1 = (X1, X2, .., Xt,at)|St,at) = 1. Once the language model finishes generating a sentence of length T, it is rewarded with r(sT\u22121, \u0430\u0442\u22121) where r(s, a) : S \u00d7 A \u2192 R is the reward function, and T is also called the horizon or episode length. This reward function is sparse with r(st, at) = 0 t = 1, .., T \u2013 2, and quantifies the desirability of an entire generated sentence. The reward can be based on various factors like fluency, coherence, relevance to a prompt, and adherence to grammatical rules, or can even be derived from human preferences.\nA policy \u03c0 : \u0391 \u2192 \u0394(S) is a strategy that the LLM follows to choose the next token (action) given the current sequence (state). Each sentence generated by the LLM policy is termed a trajectory/episode T = (81, A1, 82, a2, . . . ), where s\u2081 is sampled from the starting state distribution po, and at ~ \u03c0(\u00b7|st). An episode in this context ends when the model generates a special end-of-sequence token or reaches a predefined maximum length. Return of a trajectory 7 is given by $R(+) = \\sum_{t=1}^T \\vtr (st, at)$, where y is the discount factor. The state st can be assigned a value under this policy given by the value function $V^\\pi (st) = \u0395_{\\pi}[\\sum_{t=t}^Tr(st, at)]$. Similarly, an (st, at) pair can be assigned a value given by the state-action value function $Q^\\pi (s, a) = r(st, at) + ^V^\\pi(St+1)$. The advantage function A\u2122 is defined as $A^\\pi (st, at) = Q^\\pi (st, at) \u2013 V^\\pi (st)$. The advantage function encodes the relative advantage of taking a particular action in a particular state compared to the typical or average action that would be taken in that state. An LLM policy can be learned via reinforcement learning by maximizing the expected discounted reward defined as $J(\\pi) = E_{\\tau} [R(\\tau)] = E_{s1~\\rho_0} [V^\\pi (s1)]$. In LLM fine-tuning, 81 is drawn from a fixed dataset of prompts, Din.\nRLHF is the technique used to align LLMs with human preferences. Alignment via RLHF is a three- step process. The first step is the supervised fine-tuning (SFT) where a pretrained LLM is fine-tuned w.r.t. the cross entropy loss using the alignment dataset of the form (x1, x2,...) ~ DSFT, resulting in a modified LLM, denoted as #SFT. In the second step, the SFT model is prompted with prompts x = (x1,...,xt) to produce completions $Yi ~ \\pi_{\\$FT}(\\cdot|x),i = 1,2$, where $Yi = (Xt+1, ..., XT)$ is generated in an autoregressive way. The completions (y1, y2) are then presented to human annotators who rank them as y1 > Y2 or y2 > Y1, where > denotes the annotator's preference. It is assumed that the ranking is obtained w.r.t an unknown reward function r* according to the the Bradley-Terry (BT) model [Bradley and Terry, 1952], given by\n$P^*(Y_1>Y_2|x) = \\frac{exp(r^*(x, y1))}{exp(r^*(x, y1)) + exp(r^*(x, y2))}$.\nWe denote the preferred response as yw, the other response as y\u0131, and the preference data as D$ = $(Xi, Y_{i,l}, Y_{i,t})_{i=1}^n$. The reward function r$ is then estimated by treating this as a binary classification problem with negative log-likelihood loss as\n$L(r) = \u2212E_{(x,yw,w)~D_{\\$}} [logp^{\\phi}(Yw > Y\u0131)|x)],$"}, {"title": "Risk-Averse RLHF for LLM Fine-tuning", "content": "In this section, we present our algorithm for the risk-averse fine-tuning of LLMs. The key idea is to adopt the RARL approach [Tamar et al., 2015, Greenberg et al., 2022] to RLHF by optimizing a risk measure of the return, instead of maximizing the expected value as in the standard RLHF. In particular, we adapt soft-risk scheduling [Greenberg et al., 2022] to the standard RLHF pipeline to fine-tune an LLM such that toxic content generation, even with challenging or adversarial prompts, is reduced.\nThere are two critical aspects to consider in learning risk-averse policies through RL:\nA. Recognition of positive episodes: It is crucial that during the early stages of training, the policy recognizes and learns from positive episodes. In the context of language generation, this involves the ability of the model to transform challenging prompts into appropriate responses. To address this, we implement two strategies:\n(a) We initiate the RLHF process with a baseline model already fine-tuned on positive data. This base model is predisposed to generate outputs that are more aligned with desired outcomes, such as content resembling \u2018IMDB reviews' or 'Wikipedia comments', and is more likely to produce positive and non-toxic content (see the performance improvement supervised finetuning (SFT) only on positive (prompts + completions) data brings over the base GPT-2 models in Tables 2 and 3).\n(b) During the initial phase of fine-tuning, we introduce risk-aversion only gradually. This means that for a set number of iterations at the beginning, we utilize the entire batch of episodes for training without utilizing any risk-averse filtering, ensuring a high exposure to both positive and negative scenarios.\nB. Inclusion of challenging scenarios: To foster risk-aversion, it is essential to include a sufficient number of challenging or 'worst-case' episodes in each training batch. This ensures that the model is consistently exposed to and learns from scenarios that require heightened risk management.\nWe incorporate both the aspects above in our proposed Risk-Averse RLHF (RA-RLHF) algorithm by carefully balancing the exposure to both positive and risk-laden episodes during the training process. Thus, RA-RLHF learns policies that are adept at handling complex and adverse scenarios, while maintaining the capacity to generate beneficial and appropriate responses.\nWe implement our RA-RLHF algorithm in the following manner. In each iteration i, we generate B trajectories (episodes), $(T_j)_{j=1}^B$, by first sampling the prompt 81,j ~ Din and then generating the completion according to the current model \u03c0\u03c1. Using the fixed reward model, we then calculate the return for each of these trajectories R(tj), 1 \u2264 j \u2264 B. Ideally, we should then calculate the empirical quantile qa using these returns for given risk level a, and then select only the trajectories with returns below this qa for policy updates (c.f. (6)). However, we will use a simplified approach similar to (7) where we will select Bo trajectories with the lowest returns and use these trajectories for policy updates. Since the original RLHF update is equivalent to performing the standard RL update with the equivalent reward given in (5), our equivalent RA-RLHF can be expressed as\n$max_{\u03c0_\u03b8} E_{\u03c4j,Bo,j=(sj,t,aj,t)=1}[\\sum_{t=1}^T \\frac{\u03c0_\u03b8(aj,t Sj,t)}{\u03c0_{ref}(aj,t Sj,t)}]$.\nSelecting Bo is nontrivial because of the issues of 'recognition of positive episodes' and 'inclusion of challenging scenarios' as we pointed out above. To accommodate this, we implement soft-risk scheduling by changing the value of Bo as the training progresses. In particular, for the first io training iterations, we use the full batch of B trajectories for policy updates. We then gradually decrease the value of B0. The specific procedure is given as follows.\nLet M be the maximum number of policy finetuning iterations and let a be the risk level, then:\nA. For iterations i < io, we use the entire batch, and select Bo = B.\nB. For iterations i, i \u2265 [pM], where p is a hyperparamater, we select B\u2081 = [aB].\nC. For iterations i, io \u2264 i \u2264 [pM], we select\n$B_0 = [B*max(a, 1 \u2013 K(m \u2013 io))], K = \\frac{1-\u03b1}$ where K determines the constant rate at which the trajectories are dropped. The step A above ensures recognition of positive episodes, and B and C together ensure balanced inclusion of challenging episodes. We update the parameter \u1e9e in each iteration using the data from Bo trajectories, according to (4).\nOur practical implementation to solve (8) is by using the Proximal Policy Optimization (PPO) algorithm [Schulman et al., 2017], as now standard in RLHF implementations [Ziegler et al., 2019, Ramamurthy et al., 2022]. The actor in PPO is the base transformer extended with a language modeling head and the critic is the same base transformer extended with a value function head. Critic is updated per training iteration to estimate the current policy returns.\nOur RA-RLHF pseudo-code is included in Algorithm 1. Our codebase is available on the linked Github repository 2, and further implementation details are included in Appendix E. Our algorithm has the same computational complexity as that of RLHF during the first io iterations. Once the soft risk scheduling kicks in, our algorithm introduces an additional computational complexity of O(B + Bo log(B)). The space complexity remains the same as that of RLHF."}, {"title": "Experimental Evaluation", "content": "Through our experimental evaluation, we aim to answer the following questions:\nA. How does the reward distribution of the generated responses vary across different baseline algorithms? Can RA-RLHF induce risk-averse behavior in language generation tasks?\nB. How stable is the RA-RLHF policy fine-tuning process?\nC. Do the fine-tuned policies yield high-quality text generations? This includes an evaluation of both the coherence of the generated text and the appropriateness of sentence length.\nD. How sensitive is RA-RLHF to the variations in hyperparameters?\nBaselines: We compare the performance of the RA-RLHF algorithm against the following baselines.\n1. Base LLM: the base pretained LLM, and in our case GPT-2 or GPT-J\n2. Prompted base LLM (\u2018Prompted'): We add a prefix \u2018generate positive sentiment' and \u2018generate non-toxic text' to sampled prompts from the respective datasets.\n3. DExperts [Liu et al., 2021]: This is a test-time decoding method that uses additional expert and anti-expert language models to update probabilities of generated tokens.\n4. SFT: We fine-tune the base LLM using supervised learning with the respective data sets.\n5. RLHF: We fine-tune the SFT model using the standard RL approach.\n6. Quark [Lu et al., 2022] - SoTA fine-tuning method that induces 'unlearning' of undesirable behavior using selective fine-tuning.\nFor DExperts, as suggested in [Liu et al., 2021], we use GPT-2 as the expert and the author provided GPT-2 anti-expert checkpoint. For Quark, we use the finetuned toxicity-free GPT-2 Large (762M parameters) model to obtain generations on RealToxicityPrompts-Gen and Jigsaw-Gen. We used the GPT-2 Large sentiment steering model [Lu et al., 2022] to obtain generations on IMDB-Gen.\nTasks and Models: We work with generative versions of three established classification tasks: IMDB sentiment classification, Jigsaw toxicity classification, and RealToxicityPrompts classification. IMDB-Gen, adapted from Ramamurthy et al. [2022], tasks an LLM with completing a movie review to maximize positive sentiment. We consider two additional tasks, Jigsaw-Gen and RealToxicityPrompts- Gen, where the goal is to generate text in the least toxic manner. In IMDB-Gen, the LLM is prompted with up to 64 tokens to generate up to 48 tokens; for Jigsaw-Gen, it is prompted with up to 8 tokens to generate up to 32; and for RealToxicityPrompts-Gen it is expected to generate 32 tokens when prompted with up to 32 tokens. We include results for GPT-2 (117M) and GPT-J (6B) models. Extended experiments are included in Appendix F.\nEvaluation Metrics: We evaluate various algorithms using: 1) The standard task performance scores sentiment scores returned by lvwerra/distilbert-imdb for IMDB-Gen and toxicity scores returned by unitary/toxic-bert for Jigsaw-Gen and RealToxicityPrompts-Gen, 2) Per- plexity - a metric that gauges linguistic coherence. Whenever included, and unless stated otherwise, perplexity scores are obtained exclusively on positive class samples, and 3) Distinct-n (Dist-n) -"}, {"title": "Results on Risk-Aversion", "content": "Prompt distribution shift and quantile plots: We set out with the goal of improving LLM per- formance under challenging input prompts. To measure our performance on that goal, we generate two types of plots: the distribution shift plots and the quantile plot (see Fig. 1 and 2)). We analyze reward distributions for input prompts and generated continuations from SFT, RLHF, and RA-RLHF models using IMDB-Gen and Jigsaw-Gen test datasets (see first four columns in Fig. 1 and 2). For IMDB-Gen, we observe that SFT shifts rewards for both the positive and the negative classes by a small amount. Here, positive (/negative) class means the entire review was marked as having positive (/negative) sentiment in the original IMDB dataset. RLHF brings greater reward distribution shift than SFT. The largest shift is observed in RA-RLHF. Jigsaw-Gen shows similar trends, despite having a higher variance reward distribution over prompts. Overall, RA-RLHF performed the best in shifting input prompts towards positive sentiment/non-toxicity for both datasets.\nAdditionally, we include an average reward vs quantile plot, where the x-axis is the quantile wrt to the prompt rewards and y-axis is the average reward for the prompt completions for the various models (see column 5 in Figs.1 and 2). We observe that our RA-RLHF model brings about the maximum reward shift for input prompts. To qualitatively assess performance on tail prompts, we also include two sample generations from RLHF and RA-RLHF models for each task belonging to the tail prompts in Table 1.\nQuantitative performance on test data: Performance metrics on the test datasets across tasks are presented in Tables 2,3. The numbers are reported over the worst case prompts from randomly sampled dataset of 5k test prompts. The RA-RLHF model outperforms all the other baselines on average reward for these least favorable prompts sampled from the prompt (reward distribution) tail. For IMDB-Gen, the tail average reward corresponding to prompts with a score of < -2.5 is the greatest as compared to the other baselines. We observe a similar trend for the Jigsaw-Gen and RealToxicityPrompts-Gen tasks, where tail is below the score of < +5 for both. Across datasets, we observe RA-RLHF enjoying amongst the highest text diversity as measured by Dist-1, Dist-2 and Dist-3 metrics. For IMDB-Gen, we include model perplexities, demonstrating that the text generated by RA-RLHF is coherent. We observe a marginal increase in model perplexity for RA-RLHF, likely attributed to the model undertaking more aggressive adjustments to satisfy the goals of sentiment modification and toxicity mitigation. Tail score results for RLHF and RA-RLHF are reported over models trained over three different seeds and evaluated on one test seed - the standard deviations included in subscript. SFT training code adapted from Huggingface TRL repository had a faulty"}, {"title": "Training Stability", "content": "Next, we study the effects of inducing risk-averseness on the overall training stability in terms of both the average return using and the environment rewards r during training. We observe that RA-RLHF model gradually diverges towards positive environment rewards after we start inducing risk-averseness, more so in IMDB-Gen than in Jigsaw-Gen (see Fig. 4 (a) and (c)). The average return per token follows an expected trend where the average for RA-RLHF drops as compared to RLHF (see Fig. 4 (b) and (d)). This is because of a reduction in high return episodes per batch for RA-RLHF as the training progresses.\nAs seen in Fig. 5, we also observe that throughout the training process, RA-RLHF consistently generates almost equal or more tokens than RLHF, and does not resort to potentially high rewarding sub-optimal policies that just repeatedly generate a positive word like \"great great great ....\" to counter the negative sentiment/toxicity in the initial prompt.\""}, {"title": "RA-RLHF Hyperparameter Analysis", "content": "To study the effect of various hyperparameters on our algorithm, we run RA-RLHF on various risk schedules included in Fig. 12 in Appendix. As seen in Table 5, a trade-off between reward and perplexity seems to emerge: too aggressive of a risk-aversion, characterized by low n, low a, and high p results in high reward at the expense of higher perplexity."}, {"title": "Conclusion", "content": "This paper introduced a novel approach for fine-tuning LLMs by integrating risk-averse principles, aiming to mitigate the generation of toxic content in response to prompts. By optimizing the CVaR risk measure and employing RLHF, the proposed method demonstrates superior performance in avoiding harmful outputs while ensuring effectiveness in generative tasks. Empirical evaluations on sentiment modification and toxicity mitigation tasks underscore the effectiveness of the approach. These findings highlight the potential of risk-averse RLHF to enhance the responsible deployment of LLMs across various applications, thereby contributing to a more constructive digital interaction landscape."}, {"title": "Limitations and Future Work", "content": "The effectiveness of the risk-averse fine-tuning strategy may vary across different domains and languages, necessitating further investigation and adaptation. In our work, we primarily focussed on generative tasks, and not the Question-Answer (Q&A) format. However, by focusing on IMDB-Gen and Jigsaw-Gen tasks, we aim to establish a solid foundation upon which more complex applications, such as conversational AI, can be built. This is a standard practice in the field, allowing for focused analysis before extending to broader contexts. IMDB-Gen and Jigsaw-Gen tasks while specific to generation, are critically relevant for assessing the fundamental capabilities of LLMs in generating content that is both non-toxic and contextually appropriate. Additionally, while we emphasize the importance of promoting a safer online discourse environment, ethical considerations regarding the potential biases and unintended consequences of LLMs remain paramount and warrant continued attention in future research efforts."}, {"title": "Broader Impact and Ethics", "content": "Unaligned versions of LLMs have been documented to generate harmful content, as evidenced by recent studies Sheng et al. [2019], Wallace et al. [2019] which highlight the risks associated with uncurated training data. Furthermore, even aligned versions of LLMs are not immune to exploitation. The aligned models can still be prompted or 'red-teamed' to produce harmful content under certain conditions Gehman et al. [2020], Weidinger et al. [2021], Ganguli et al. [2022], Deshpande et al. [2023]. This underscores the complexity of mitigating risks in LLM deployment and the necessity for robust, ethical alignment strategies. In response to these challenges, our research introduces a novel approach to instill a predisposition against harmful prompts in an LLM, employing a modified Reinforcement Learning from Human Feedback (RLHF) mechanism. Our aim is to cultivate a framework that supports positive and respectful discourse in online environments. It is important to note that our methodology did not involve direct human experimentation but instead relied on the application of pre-existing preference and reward models.\nWe would also like to point out that \"safety\" can take different representations in different applications. We optimize for performance on rare high stake events, making our approach of wider use in applications employing LLMs, beyond the tasks of safe text generation considered in our work.\nWhile we recognize that any alignment strategy, including the one we propose, can potentially be reversed to engineer an LLM to produce content with elevated levels of toxicity or negative sentiment, we believe addressing the regulation of LLM outputs in response to malicious prompts is a critical area of inquiry. Our hope is that our contributions will positively impact the collective effort towards enhancing the quality of online interactions for the broader community."}, {"title": "Related Work - Extended", "content": "LLM Alignment. Large language models (LLMs), utilizing transformer architectures, have shown remarkable proficiency in advanced language generation tasks Vaswani et al. [2017], Radford et al. [2019], Brown et al. [2020], Devlin et al. [2018], Bubeck et al. [2023]. Despite their inherent capabilities, optimizing these models for specific downstream tasks necessitates additional strategies. One approach involves adapting the language model training to be multi-task oriented, as exemplified by the T5 family of instruction-tuned models Raffel et al. [2020]. Alternatively, aligning these models with downstream task data through specialized techniques can be effective. Specialized techniques such as Retrieval Augmented Generation (RAG) Lewis et al. [2020], Supervised Fine-Tuning (SFT) Howard and Ruder [2018], and Fine-Tuning via Reinforcement Learning with Human Feedback (RLHF) Christiano et al. [2017], Ziegler et al. [2019], Stiennon et al. [2020], Ouyang et al. [2022] or AI Feedback (RLAIF) Lee et al. [2023] represent pivotal methods for enhancing downstream task performance in large language models. Each technique offers a unique approach to optimizing model proficiency: RAG integrates external knowledge sources during generation knowledge-intensive tasks like question answering, SFT adapts models to specific tasks through targeted training, and RLHF/RLAIF employs feedback-driven learning for iterative improvement. Among these, RLHF has shown notable success in aligning LLMs with human preferences, making it a focal point of study in this paper."}, {"title": "Data Analysis", "content": "D.1 Datasets\nFor IMDB-Gen, we make use of the IMDB dataset which contains a large collection of movie reviews. These reviews are labeled as either positive or negative. There are a total of 25k train and test reviews each. The dataset used for Jigsaw-Gen originates from a 2017 Kaggle competition focused on classifying Wikipedia talk page comments. Specifically, the data consists of human-labeled samples from a corpus compiled by Jigsaw (a subsidiary of Alphabet Inc.) and partners, where human raters identified multiple dimensions of toxicity including toxic, severely toxic, obscene, identity hate, threat, and insult. For constructing the task dataset, we sampled the original data to create a training set distribution of 70% non-toxic and 30% toxic data points and a test set containing 50% toxic and non-toxic points. Although the original corpus includes six hierarchical toxicity labels, the current study focuses solely on the presence or absence of the broad toxic class. The resulting dataset consists of 36, 973 training and 7, 708 test samples.\nD.2 Motivation for the choice of tasks\nIn addition to requiring deeper level of language understanding and generation capability, transforming classification tasks into generative tasks makes them potentially more powerful and versatile in their applications. The model now needs to not only analyze and understand the input text, but, also creatively generate appropriate and contextually relevant content while maintaining the original message or sentiment. This could be used to understand how a review might evolve based on its beginning, or to generate examples of different types of sentiment expressions for training or analysis purposes. This can have practical applications in enhancing user experience and safety on various digital platforms.\nD.3 IMDB\nD.3.1 Scores (Environment rewards) distribution\nAnalysis of test dataset. Here, full reviews that are assigned positive sentiment in the dataset belong to Class 1. Similarly, full reviews that are marked as having negative sentiment belong to Class 0. Only 16 of the prompts belonging to the true Class 1 were scored below -2.8. A total of 1806 of Class 0 prompts were below a score of -2.8.\nD.3.2 Critical prompt clusters\nWe perform k-means cluster analysis on the embedding for prompts from the previous section that get a score less than -2.8. We use a total of 167 (150 from Class 0 and 17 from Class 1) prompts for this analysis. We use EleutherAI/gpt-j-6b model available on Huggingface model repository to generate embeddings. We then group these embeddings into 8 clusters using"}, {"title": "Jigsaw", "content": "D.4.1 Scores (Environment rewards) distribution\nEnvironment reward distribution for Jigsaw train and test datasets is included in Fig. 9 and Fig. 10.\nD.4.2 Critical prompt clusters\nWe perform clustering on the critical prompts from the Jigsaw dataset, similar to the analysis done for IMDB. We observe that two out of the three sampled prompts from Cluster-0 seem to be referring to Wikepedia. Cluster-1 seems to have some clutural and pop references like the dance form 'dabke' and the word 'nerd'. Cluster-2 has prompts where posters seem to take pride in being able to post negative content irrespective of possible censorship by specific platform. There seem to be generic toxic prompts in Cluster-3. The prompts in Cluster-4 seem to have negative sexual connotation. Cluster-5 prompts seem to have toxicity towards certain social and political groups. Cluster-6 seems to have toxicity towards certain social groups like Jews and Blacks. Cluster-7 prompts, again, have toxicity towards social groups like Blacks."}, {"title": "RA-RLHF Implementation", "content": "E.1 Models and Compute\nFor our experiments with both the datasets, we use the model GPT2 (117 million parame- ters, 0.5 GigaBytes (GB)) as our LLM policy. We adapt the RLHF and SFT implementa- tions available on the Hugging Face transfomer reinforcement learning (TRL) repository to our datasets. We implement RA-RLHF starting from this existing RLHF implementation. The AutoModelForCausalLMWithValueHead class provides functionality to attach a Value head to the GPT2 model with an existing LMHeadModel (see Listing 2 in the Appendix). The vocabulary size A = 50257. The tokenizer (GPT2TokenizerFast) specifications for GPT2 model are included in Listing 3. For IMDB task, we use lvwerra/distilbert-imdb as the reward model. It is available on Hugging Face model repository. The model specifications and corresponding tokenizer (DistilBertTokenizerFast) specifications are included in Listings 4 and 5, respectively. For Jigsaw-Gen we use (unitary/toxic-bert) as the reward model; also available on Hugging Face model repository. This model achieves an AUC metric of 0.98 on the Kaggle Challenge. Speccifica- tions of this reward model and it's tokenizer are included in Listings 6 and 7 respectively. Our codes were run on machines with GPU configurations of NVIDIA Tesla V100 SXM2 32 GB, and NVIDIA A100 80 GB. Average run time across algorithms is 52 minutes."}, {"title": "Proximal Policy Optimization", "content": "Consider a batch of three episodes, i.e., three pairs of input prompts and output generations.\nInput prompt Generation\nX11 X12 X14 X15 X16\nbatch= X21 X22 X23 X24 X25 X26\nX31 X3"}]}