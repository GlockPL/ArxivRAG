{"title": "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "authors": ["Daehoon Gwak", "Junwoo Park", "Minho Park", "Chaehun Park", "Hyunchan Lee", "Edward Choi", "Jaegul Choo"], "abstract": "Predicting future international events from textual information, such as news articles, has tremendous potential for applications in global policy, strategic decision-making, and geopolitics. However, existing datasets available for this task are often limited in quality, hindering the progress of related research. In this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction), a novel dataset designed to address these limitations by leveraging the advanced reasoning capabilities of large-language models (LLMs). Our dataset features high-quality scoring labels generated through advanced prompt modeling and rigorously validated by domain experts in political science. We showcase the quality and utility of WORLDREP for real-world event prediction tasks, demonstrating its effectiveness through extensive experiments and analysis. Furthermore, we publicly release our dataset along with the full automation source code for data collection, labeling, and benchmarking, aiming to support and advance research in text-based event prediction.", "sections": [{"title": "1 Introduction", "content": "Accurate prediction of future international events is essential for effective decision-making in international relations, global strategy, and security policy (Goldstein and Pevehouse, 2011; O'Brien, 2010). However, the dynamic nature of international relations, with its evolving conflicts and cooperation between multiple countries, presents a significant challenge for accurate prediction (Mellers et al., 2014). Recent global events, such as ongoing regional tensions and shifting economic dynamics among various countries, further complicate these challenges for policymakers and analysts. These complexities highlight the importance of developing reliable machine learning models capable of predicting changes in international relationships and the subsequent events they could trigger.\nExisting approaches for text-based international events prediction like Shi et al. (2024) have heavily relied on datasets like the Global Database of Events, Language, and Tone (GDELT) (Leetaru and Schrodt, 2013) due to their comprehensive coverage and ease of use. By providing a comprehensive archive of global news and extracting relationships between subjects like countries, these datasets can be leveraged by machine learning models to tackle predictive tasks (Xing et al., 2018). However, they have notable limitations:\n1.  Overlooked Multilateral Relations: GDELT frequently fails to capture complex interactions involving multiple countries, typically focusing on bilateral relationships. This simplification overlooks the intricate multilateral relations that are critical for understanding international dynamics. As analyzed in our study (detailed in Figure 2(a) in Section 3.1), this limitation becomes evident, highlighting the need for more comprehensive datasets.\n2.  Inaccurate Labeling: GDELT uses rule-based methods and basic machine learning techniques in some parts of the process (Saz-Carranza et al., 2020), resulting in frequent mislabeling. Furthermore, the binary categorization into conflict or cooperation fails to capture the nuanced nature of international relations, lacking information to reflect the intensity of relationships and an 'Unknown' category for indeterminate relationships. This oversimplification can lead to significant gaps in understanding and predicting complex international dynamics.\nDespite its significant contributions as a seminal work in the field, these limitations highlight the need for a more reliable and extensive dataset.\nIn this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction), a new dataset designed to overcome existing limitations by leveraging the advanced reasoning capabilities of large-language models (LLMs) (Brown et al., 2020; Radford et al., 2018). To ensure reliable and efficient results, we developed a structured scratchpad (Nye et al., 2021) with a self-correcting mechanism, incorporating a verification-correction pattern for annotations. This designed scratchpad allows for step-by-step reasoning, which helps in accurately identifying countries involved in multilateral events. The advanced contextual understanding of these models also ensures precise labeling of international relationships.\nTo validate the quality of WORLDREP, we conducted extensive experiments involving multiple steps. First, we obtained annotations from domain experts in international relations and political diplomacy. We then compared the number of extracted countries in our dataset to those identified by the experts, finding a high level of agreement. We also measured the alignment of our labels with the experts' labels, ensuring the consistency and reliability of our annotations. Next, we trained predictive models on WORLDREP and evaluated their performance using the expert-provided labels as the test set, achieving strong accuracy and F1 score. Leveraging these results confirming the reliability of our annotation process, we automated the labeling process and created a benchmark for predicting future international relations.\nIn summary, we present a comprehensive and reliable dataset that captures complex multilateral interactions. Using advanced LLMs, we accurately annotate relationship labels with a structured prompt design. We make both our dataset and the full automation source code publicly available to sustainably support further research and development in text-based event prediction and international relations. We believe this contribution will significantly advance the field by providing a solid foundation for future studies and applications."}, {"title": "2 Dataset Construction", "content": "Constructing a reliable dataset for research to predict future geopolitical events involves addressing several key challenges due to its complexity and ambiguity. We focus on two main enhancements compared to existing datasets such as GDELT: 1) capturing multilateral relations and 2) improving the accuracy of relationship labeling.\nOur dataset construction process is designed to reflect the complexity of multilateral international relations more accurately. After collecting data, the process involves two main stages, Multi-Subject Extraction and Relationship Score Labeling, as illustrated in Figure 1. By following these steps, WORLDREP captures the intricate nature of global events and provides high-quality labels for research in predicting text-based international relations. The details of each step are explained below."}, {"title": "2.1 Data Collection", "content": "We collected a diverse set of news articles that could influence future international relationships between countries, resulting in a corpus of about 44,706 articles, including their dates and times. Following the approach taken by previous studies on future event prediction (Shi et al., 2024), we treat each news article as a single event with its corresponding occurrence time. Detailed data collection procedures are provided in the Appendix."}, {"title": "2.2 Stage 1: Multi-Subject Extraction", "content": "Identification of all relevant countries in international events is important for a comprehensive understanding of multilateral interactions. To extract multiple countries accurately, we incorporate a self-correcting mechanism within our LLM prompt design. As shown in Figure 1, this design consists of three tasks within a single prompt, allowing the LLM to extract, verify, and correct its output in single inference. Specifically, our prompt instructs the LLM to perform the following tasks:\n1.  Extract important countries in the article.\n2.  Verify the accuracy of the extracted countries.\n3.  Provide corrections if necessary.\nBy doing so, we achieved significant improvement in the identification of all important countries compared to not using the last two tasks, verification and self-correction. The effects of verification and self-correction steps will be detailed in Section 3 comparing with extraction results by domain experts. The complete scratchpad is available in Appendix Figure 6."}, {"title": "2.3 Stage 2: Relationship Score Labeling", "content": "Accurate labeling of relationships is crucial for constructing a reliable dataset related to future geopolitical events. Given the inherent ambiguity and uncertainty in international relations, it is impractical to categorize interactions strictly as conflict or cooperation. Instead, we adopted a nuanced scoring system that reflects the complexities of these relationships. This scoring approach allows us to assign an 'Unknown' category for cases where relationships cannot be clearly defined, providing a more accurate and comprehensive representation of international interactions.\nOur scoring process includes the following steps:\n1.  Pair Generation: For each news article, identified subjects are paired to create possible relationships based on the article's content. For example, if four subjects are extracted in one news article with Stage 1, six pairs are generated.\n2.  Scoring Relationships: Each pair is evaluated and scored using a designed prompt that enables the LLM to:\n1)  Determine if there is evidence in the given article to predict a relationship between two countries.\n2)  If evidence is found, describe it and score the relationship on a scale from 0.0 (full cooperation) to 1.0 (full conflict). If no evidence is found, assign 'Unknown' to the relationship.\n3)  For the pairs that have a score, verify whether the assigned score is biased or overly aggressive. Correct the score if necessary to ensure that it accurately reflects the relation.\nThe full content of this designed prompt is illustrated in Appendix Figure 7. To increase the consistency and reduce the variance in the scoring process, we performed five separate scoring inferences for each relationship pair using the LLM. By averaging these five scores, we obtained a more stable and reliable relationship score. The statistics of our constructed dataset are detailed in Table 1.\nIn the following section, we will validate the reliability of these labels through domain expert evaluations and various experiments. In addition, we will demonstrate the effectiveness of our prompt design, particularly the self-correcting mechanisms."}, {"title": "3 Dataset Quality Evaluation", "content": "To demonstrate the quality of WORLDREP, we conducted several experiments, including an evaluation against domain expert labels. Our evaluations in this section focused on the reliability of the labels and the validity of the designed prompting system."}, {"title": "3.1 Domain Expert Annotation", "content": "To establish a ground truth for our evaluation and to ensure the quality of our dataset, we engaged domain experts in political science and international relations. A group consisting of a professor and several graduates from a leading political science department, manually labeled the relationships in subset of 1,030 selected articles that have fully annotated samples in both GDELT and our dataset. These specific articles were chosen due to the limited number of fully annotated samples in GDELT, enabling a direct quality comparison between GDELT and WORLDREP. The experts labeled the relationships as either conflict, cooperation, or unknown if the relationship was indeterminable. The labeling process was supervised by the professor to ensure consistency, and detailed guidelines, example samples, and other relevant information about the labeling process are provided in Appendix. The domain expert labels for the selected articles are also made publicly available to facilitate future research.\nLabel Alignment with Domain Experts We compared the alignment of our labels with those from domain experts. To facilitate this comparison, we categorized our numerical scores into three classes: scores of 0.0-0.25 as cooperation, 0.75-1.0 as conflict, and 0.25-0.75 as unknown. Table 2 shows the alignment rates for conflict, cooperation, and unknown categories, as well as the overall alignment rate.\nAs shown in Table 2, our dataset achieves alignment rates of 84.8% for conflict, 70.6% for cooperation, and 73.7% for unknown, with an overall alignment rate of 77.4%. In comparison, GDELT achieves 48.8% for conflict, 21.0% for cooperation, and does not include an \"unknown\" label, resulting in an overall alignment rate of 30.6%.\nThese results demonstrate that WORLDREP not only has a higher overall alignment but also outperforms GDELT in both conflict and cooperation categories. Additionally, Figure 2 (a) shows that the number of key countries identified by WORLDREP aligns more closely with domain experts compared to GDELT, which tends to identify fewer key countries. Figure 2 (b) illustrates that WORLDREP's label distribution more closely matches the domain experts' label distribution, further validating the accuracy and reliability of our dataset. The significant use of the \"unknown\" label by domain experts highlights the necessity of this category to accurately capture the complexity of international relationships.\nEffectiveness of Self-correcting Mechanisms Our annotation process includes a self-correcting mechanism to enhance the accuracy and reliability of our labels. This mechanism allows the model to verify and correct its initial responses based on a comprehensive analysis of the article's content.\nFigure 3 illustrates examples of our self-correcting process in both key country extraction and relationship labeling. In the key country extraction example (Figure 3a), the initial list of countries extracted from the article is verified and corrected. Initially, the list included 'CHN' (China) due to a sea name reference, which was later removed to provide a more accurate list of relevant countries. In the relationship labeling example (Figure 3b), the model initially scored the relationship between 'FRA' (France) and \u2018RUS' (Russia) as conflict with a score of 0.7. However, after verifying the context, the score was corrected to 0.5, reflecting a more balanced view of the cooperative and conflictual elements in the article.\nTable 3 quantifies the improvements achieved through self-correcting process. The average number of countries identified increased from 4.06 to 5.60, bringing it closer to domain experts' average of 5.20. Furthermore, the F1 score, which measures the alignment with domain expert labels, improved from 0.825 to 0.963 after correction, demonstrating a significant enhancement in label accuracy.\nThese results highlight the effectiveness of our self-correcting mechanism. By allowing the model to refine its initial outputs, we achieve a higher level of precision and reliability in our dataset. This approach not only captures a more comprehensive set of relevant countries, but also aligns more closely with the nuanced judgments of domain experts.\nFurthermore, we confirm that the latest LLM efficiently processes our scratchpads with numerous conditional instructions and format guidelines, resulting in few logical inconsistencies in its responses, as shown in Appendix Table 6."}, {"title": "3.2 Document Classification Experiments", "content": "The purpose of this subsection is to evaluate the reliability of our annotated labels by training various models on WORLDREP and comparing their performance against models trained on the GDELT dataset. Through these experiments, we aim to demonstrate the superiority of our labeling approach in providing consistent and accurate information about international relationship."}, {"title": "3.2.1 Experimental Setup", "content": "To evaluate the quality of our dataset, we designed a document classification task. The goal of this task is to classify the relationships between pairs of countries mentioned in news articles as either cooperative or conflictual. Specifically, each input consists of a document and two countries, and the task is to predict the nature of the relationship between these two countries based on the content of the news article.\nThe models were trained using labels from both WORLDREP and the GDELT dataset, but the evaluation of all models was performed using a test dataset labeled by domain experts. This approach ensures a consistent and unbiased comparison of the labeling quality across different datasets and allows us to benchmark our dataset against the highest standard of labeling quality.\nTo ensure a fair comparison, we focused on the samples where both our labels and the domain expert labels were not classified as \"unknown.\" This setup allowed us to conduct a two-class classification task, making the comparison with GDELT more equitable. The models used in our evaluation include BERT (Devlin et al., 2018), ROBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), XLNet (Yang et al., 2019), DistilBERT (Sanh et al., 2019), and ERNIE (Zhang et al., 2019). These pre-trained language models are widely recognized for their effectiveness in text classification tasks and are easy to fine-tune, making them ideal for evaluating the quality of our training labels. Details about the hyperparameters such as epochs, learning rates, and input instructions are provided in the Appendix."}, {"title": "3.2.2 Experimental Results", "content": "Using the document classification task, we evaluated the performance of various models trained on the same set of documents but with different labels: one set with our annotated labels and the other with GDELT. In addition, all models were evaluated using domain expert labels as ground truth.\nAs shown in Table 4, our labels consistently outperformed the GDELT labels in all models. Specifically, models trained on our labels achieved significantly higher accuracy and F1 scores compared to those trained on the GDELT labels. For example, the BERT model trained on our labels achieved an accuracy of 0.875 and an F1 score of 0.817, while the same model trained on GDELT labels achieved an accuracy of 0.536 and an F1 score of 0.528.\nThese results clearly demonstrate the superior quality of WORLDREP's labels. Models trained on our labels not only achieve higher overall performance, but also show consistent improvements across all evaluated models. Furthermore, the strong performance of models trained on our labels when evaluated with domain expert labels highlights the high alignment between our labels and those of the domain experts. This alignment is consistent with previous class distribution and consistency rate results, further validating the accuracy and reliability of our dataset. This highlights the effectiveness of our labeling approach in providing accurate and reliable relationship classifications, which are crucial for understanding international dynamics and predicting future events."}, {"title": "4 Future Event Prediction", "content": "In this section, we provide a benchmark for text-based prediction of future relationships between countries. Predicting future international relations based on text data is a highly challenging task due to the complexities involved in modeling such events and the high cost of gathering quality-labeled datasets. In the previous section, we demonstrated the superiority of our labeling approach through comparisons with domain expert labels. Building on this foundation, we now leverage our automated labeling system to efficiently label new data, replacing the prohibitively expensive expert labeling process. This approach allows for flexible and timely updates of WORLDREP while maintaining high data quality. Based on the results of this labeling process, we provide a benchmark using the latest large models capable of performing this task. We hope that this benchmark will serve as a starting point for further modeling advancements in predicting future geopolitical events."}, {"title": "4.1 Experimental Settings", "content": "Task Description The task involves predicting the relationship between two countries at a future date based on past news articles. The goal is to determine whether the relationship on the target date will be characterized by cooperation, conflict, or remain indeterminate. For example, consider the task of predicting the relationship between Country A and Country B on June 1, 2025. Given past articles, the task involves predicting whether the relationship on June 1, 2025, will be characterized by cooperation, conflict, or remain indeterminate. The prediction relies on the historical context provided by the past articles to assess the relationship between the two countries.\nFor the basic experiment, the model is provided with a fixed number of the most recent 15 articles up to the prediction date, under the assumption that recent events are more likely to influence future relationships. This context information provides an overview of the most recent interactions and events involving the countries. The articles are summarized to retain core information within the model's input limits. For this experiment, we used data from a two-week period, from May 15, 2024, to May 28, 2024, comprising a total of 353 articles.\nThe target setting specifies the relationship to be predicted between the two countries on the given future date. For this, we use the same categorization method described in the document classification section to convert our scoring system into categorical labels. This ensures consistency in the labeling process and allows for a clear definition of the relationship categories. We note that methods for news article selection and label categorization represent just a straightforward strategy, and more advanced techniques could certainly be employed in the future research.\nBaseline Models Successfully predicting future international relationships from text data requires the ability to understand and interpret context-rich complex information. Large Language Models (LLMs) are particularly suited for this task, as their ability to capture nuanced and complex contexts and generate insightful predictions has been extensively validated across various applications (Dhingra et al., 2022; Shi et al., 2024). Additionally, LLMs offer the potential to not only predict relationships but also generate detailed event descriptions, providing richer insights for future event prediction. Therefore, by leveraging both API-based and open-source LLMs, we aim to provide a comprehensive evaluation of their performance in this challenging predictive task. While LLMs are a focus of this benchmark, the benchmark experiments can be applied to other suitable models, allowing for broader exploration and evaluation in future event prediction tasks. The models include:\n\u2022 LLM API: GPT-4-Turbo, GPT-40 (Achiam et al., 2023), Claude-3-Opus (Anthropic, 2024), Gemini-1.5-Pro (Team et al., 2023), GPT-3.5-Turbo (Brown et al., 2020)\n\u2022 Open-source LLM: Llama3-8B (Meta, 2024), Llama2-13B (Touvron et al., 2023), Phi3-mini-4k (Abdin et al., 2024), Mistral-7B (Jiang et al., 2024)\nThese models are evaluated in a zero-shot setting, relying on their pre-trained knowledge and the provided context for predictions. The designed prompt and detailed model settings used for these evaluations are detailed in Appendix. Note that this experiment uses data in May 2024, which is beyond the knowledge cutoff dates of all these models."}, {"title": "4.2 Experimental Results", "content": "The performance of the models on the future event prediction task is summarized in Table 5. We report accuracy and macro F1 scores for each model. The range of accuracy across models is approximately 45-61%, demonstrating that the models can make meaningful predictions even in a zero-shot setting. Additionally, API-based models generally exhibited better performance compared to open-source models. These results suggest that while the models show potential in predicting international event outcomes, there is significant room for improvement in their accuracy. These results suggest that the models are capable of predicting international event outcomes to a certain extent, but there is significant room for refining their accuracy.\nQualitative Results To illustrate the practical applications of our prediction framework, we present a case study involving the relationship between Israel and Egypt. As shown in Figure 4, the framework uses past news articles to predict the nature of the relationship on a future date. The context includes articles detailing recent interactions between the two countries, such as military operations and diplomatic tensions.\nUsing this context, the model predicts that the relationship between Israel and Egypt on May 25, 2024, will be characterized by conflict. The model's output includes not only the prediction but also a rationale for its decision. For example, the model identifies recent military actions and diplomatic tensions as key factors contributing to the conflict prediction.\nThis case demonstrates the model's ability to leverage historical context to make informed predictions about future international relationships, providing both the prediction and an explanation grounded in the provided news articles. Such detailed outputs underscore the potential of LLMs to assist in strategic decision-making and geopolitical analysis. Finally, for a detailed discussion on potential strategies to improve future relation prediction using LLMs, please refer to the Appendix."}, {"title": "5 Conclusion", "content": "In this paper, we introduced WORLDREP, a novel dataset designed to predict future international events from textual information, leveraging the advanced reasoning capabilities of large-language models (LLMs). WORLDREP addresses the limitations of existing datasets by capturing complex multilateral relations and providing high-quality labels validated by domain experts. Extensive experiments demonstrate the reliability and effectiveness of WORLDREP in real-world event prediction tasks. Furthermore, we established a benchmark for future event prediction, highlighting the potential and challenges of using LLMs for this purpose. We believe that WORLDREP and the automated labeling system will advance research in text-based event prediction and international relations, providing a solid foundation for future studies and applications."}, {"title": "Limitations", "content": "Despite the promising results, our work has several limitations. Firstly, while WORLDREP covers a broad range of international events, it may still miss less-reported but significant incidents. Secondly, the predictive models used in our experiments, although demonstrating meaningful performance, are not yet fully optimized for capturing the nuances of international relationships. This indicates potential for further improvement in model selection and fine-tuning. Lastly, our approach heavily depends on the capabilities of current LLMs, which may introduce biases inherent in their training data. Future work should aim to diversify data sources, enhance model accuracy, and address these biases through improved training methodologies."}, {"title": "Ethical Considerations", "content": "The development and application of predictive models in international relations come with ethical responsibilities. It is crucial to ensure that our models do not reinforce existing biases or contribute to misinformation. The data used in our models should be transparently sourced and carefully annotated to avoid misrepresentation. Furthermore, predictions made by our models should be interpreted with caution, recognizing the inherent uncertainties in forecasting international events. Policymakers and analysts using these models should complement the insights gained with human judgment and expertise to make informed decisions. We are committed to continually assessing and mitigating ethical risks associated with our research and its applications."}, {"title": "A Related Works", "content": "Predicting international affairs involves a global scope and multiple intertwined interests, which makes it much more complex than forecasting individual social activities such as shopping, social media use, travel, or hospital visits, which are common in event prediction benchmarks (Xue et al., 2023). In international affairs, relying solely on the occurrence and types of past events is insufficient for accurate predictions. An accurate prediction must incorporate the contextual information surrounding international events. Consequently, existing studies (Leetaru and Schrodt, 2013; Zou et al., 2022) have attempted to collect and process a vast amount of global news, treating news as an event in itself. Despite these efforts, the extensive range of collection and the need for expert knowledge in international affairs make relevant data more limited compared to social event datasets. Most international affairs prediction problems rely on the GDELT project2, which has created a huge news corpus by collecting news from around the world. However, the accuracy of mapping events to news through labeling is still lacking.\nRecently, another type of dataset has been proposed for solving international affairs prediction problems using a QA-based approach, leveraging advancements in LLMs (Zou et al., 2022; Yan et al., 2023; Halawi et al., 2024). As LLMs have significantly improved the performance of natural language interfaces, QA-based approaches, and event QA datasets have been introduced. Notably, AutoCast (Zou et al., 2022) includes queries about which events might occur at a future point, providing answers such as occurrence, numerical values, or selections from multiple choices. While the natural language interface allows for diverse questions, the predictions are often somewhat inaccurate, and the reliability of the answers is low, considering the difficulty of future prediction. Additionally, creating QA pairs is costly, resulting in fewer than 10K pairs, with even fewer questions related to international affairs."}, {"title": "A.2 LLM-based Dataset Labeling and Generation", "content": "In recent years, the use of LLMs for dataset generation and labeling has emerged as a significant trend in the ML community (Wang et al., 2021; Yoo et al., 2021; Liu et al., 2022; Lee et al., 2023; Honovich et al., 2023; Tan et al., 2024). Various studies show that LLMs can substantially reduce labeling costs and improve dataset quality. For instance, research by Wang et al. (2021) suggests that GPT-3 can significantly lower labeling expenses. In another study, Yoo et al. (2021) explore methods to enhance dataset diversity and richness through text augmentation. Additionally, Liu et al. (2022) propose a collaborative approach between human workers and AI to create natural language inference datasets, demonstrating how such synergy can enhance dataset quality.\nThe utility of LLMs has also been proven in specific application domains. Research by Goyal et al. (2022) shows that summaries generated by GPT-3 receive higher evaluations from human annotators compared to those created by models fine-tuned on summarization datasets. Furthermore, Wadhwa et al. (2023) confirm the effectiveness of LLMs in extraction tasks, highlighting their ability to accurately identify relationships between subjects. Lastly, Lee et al. (2023) address the use of LLMs to generate datasets for multi-choice question answering. These studies collectively demonstrate that LLMs are becoming innovative tools for dataset generation and labeling, opening new possibilities in the field of data science."}, {"title": "A.3 Discussion about Improvement Strategies for Predicting Future Relations", "content": "Based on the results of our experiments, we discuss potential strategies for improving future relation prediction using LLMs. We aim to understand how different factors influence model performance and identify ways to enhance predictive accuracy. Here, we outline two key strategies for improving the performance of LLMs in predicting future international relationships.\nEnhanced Context Retrieval: Improving the methods for selecting context articles can significantly boost prediction accuracy. This includes leveraging more sophisticated retrieval algorithms and exploring more advanced embedding techniques. Advanced techniques to capture and represent context can ensure the consistency and relevance of context over extended prediction horizons, making retrieval more effective and accurate.\nDomain-Specific Fine-Tuning: Fine-tuning LLMs on domain-specific datasets can help them incorporate more relevant knowledge, enhancing their predictive capabilities in specific contexts such as geopolitical events. For example, models might fail to make accurate predictions if the provided context does not include sufficient information. However, fine-tuning on a dataset specific to geopolitical events can improve the model's understanding and accuracy, even with minimal context. This suggests that fine-tuning can further refine the model's predictions by providing it with a more focused and relevant knowledge base.\nThese observations indicate that future research should focus on developing more advanced context retrieval techniques and fine-tuning strategies to improve model performance further. Enhanced context retrieval ensures that the most relevant information is used for predictions, while domain-specific fine-tuning can help models better understand and predict complex international relationships. By addressing these areas, the predictive capabilities of LLMs in forecasting future geopolitical events can be significantly improved."}, {"title": "A.4 Future Applications of WORLDREP", "content": "In this paper, we demonstrated that WORLDREP can serve as a reliable benchmark than existing datasets and can be effectively used to forecast text-based relationships between countries. Furthermore, we highlight that WORLDREP offers not only more accurate labels compared to other datasets but also provides significant value for traditional research areas, such as event prediction and time series forecasting.\nFirstly, traditional event prediction models (Zuo et al., 2020; Chen et al., 2021), including those based on Temporal Point Processes (TPP), have primarily relied on numerical or categorical data to predict next event by understanding frequency patterns or transitions between events. These models are typically benchmarked on datasets (Xue et al., 2023) with simple event dependencies and dynamics. However, such datasets are insufficient for guaranteeing whether a model can learn the complex dynamics involved in applicable evnts in real-world such as international relations and economic events. In contrast, our dataset incorporates the rich semantic information embedded in news text, allowing for more sophisticated and reliable event prediction benchmark. By leveraging textual data, our model moves beyond simple predictions based on the numerical and categorical features, offering a new features for tackling more complex event prediction problems that require deeper insights into event dependencies.\nSecondly, in time series forecasting (Zhou et al., 2021), our dataset enables models to go beyond simple periodic pattern recognition and account for the actual impact of international events on time series data, resulting in more accurate predictions. Events in international relations have a significant effect on economic uncertainty. This has a potential to improve time-series models. Previous studies (Park et al., 2023; Wang et al., 2024) have shown that much of the error in traditional time series forecasting models stems from unforeseen events. Also, even though AutoCon (Park et al., 2024) used to learn long-term correlation to achieve accurate forecasting, there are still rooms for improvement. Using our data set, we can validate time series models to see whether they can integrate semantic information from news events to improve prediction accuracy, not just learn periodic patterns. This enables a deeper analysis of how political and economic events influence time series patterns, providing a crucial foundation for reducing forecasting uncertainty."}, {"title": "B Dataset Details", "content": "To accurately evaluate LLM-based annotations, reliable ground truth labels are essential. We employed human labels from international politics experts, and their involvement is crucial for obtaining trustworthy annotations from articles.\nWe engaged three graduate students who majored in international politics under the supervision of a professor of international politics to label the relationships between major countries that appeared in articles. They worked under the guidance of a professor to ensure the quality of the labels.\nThe process of identifying major countries went beyond simply checking if a country name appeared on a list; it involved understanding the context of the news articles to select key countries based on specific guidelines as follows:\n1.  Countries directly related to the major events or topics covered in the article.\n2.  Countries that are major actors or play specific roles in the article.\nNext, we extracted major countries from the article and then asked an expert to create possible country pairs from those countries and label the relationship between the two countries. The guidelines provided were as follows:\n1.  Choose between 'Conflict' or 'Cooperation' to describe the relationship between countries.\n2.  If it is impossible or ambiguous to choose based on a given article, 'Unknown' can be selected.\nUsing these guidelines, the experts carried out the annotations following the questions shown in Figure 5."}, {"title": "B.2 Scratchpads Details", "content": "In our dataset construction process, we employed structured scratchpads to ensure the accurate extraction of important countries and the labeling of relationships in news articles. The scratchpads guide the model through a series of tasks to verify and correct its outputs, enhancing reliability.\nFigures 6 and 7 illustrate the detailed prompts used for extracting important countries and relationship labeling, respectively. The extraction scratchpad (Figure 6) involves tasks such as extracting country codes, summarizing articles, and verifying accuracy. This process ensures that all relevant countries are identified and listed accurately.\nThe relationship labeling scratchpad (Figure 7) focuses on identifying relationships between countries, summarizing the context, and correcting inaccuracies. This method allows for precise and comprehensive labeling of international relationships.\nThese structured prompts facilitate detailed and accurate annotations, critical for constructing a high-quality dataset for future event prediction."}, {"title": "B.3 Logical Consistency in Scratchpads", "content": "Annotation using LLMs has been widely adopted, showing increasingly successful results as LLM performance improves. Unlike existing works (Tan et al., 2024; Goel et al., 2023) that provide guidelines and a few examples for a single task, our scratchpads have more questions and strict format guidelines. Specifically, as shown in Figures 6 and 7, our two scratchpads have a variety of conditional instructions and detailed format guidelines for each instruction's outcome. Despite this difficulty, we found that the LLM produces results consistent with the guidelines. Although they include many instructions and long prompts, the LLM manages them efficiently as demonstrated in Table 6. Consequently, there are few logical inconsistencies in its responses."}, {"title": "B.5 Detailed Dataset Construction", "content": "To collect comprehensive information describing international political events, we need to create a dataset that includes reliable, high-quality news articles and detailed annotations of international relations. Our dataset construction process involves three main steps: Data Collection, Subject Extraction, and Score Labeling. We have automated this entire process and the dataset will be publicly available. Additionally, we will release the code used for this automated process. By making this code publicly available, we aim to support the analysis of international relations and various other text-based event analysis tasks across different domains."}, {"title": "B.5.1 Challenges and Requirements", "content": "Creating a high-quality dataset for international political events presents several key challenges:\n1.  Capturing Multiple Subjects: Ensuring that all relevant countries and their relationships are captured in each event. A single news article or event often involves multiple countries with complex interactions. It is essential to identify and label all significant subjects to provide a comprehensive understanding of the event dynamics.\n2.  Scoring Relationship Labels: Representing relationships with numerical scores to capture the nuance of interactions. For instance", "good\" or \"bad\" can miss the subtleties of international dynamics. Numerical scores allow for a more nuanced representation, reflecting varying degrees of cooperation or conflict.\n3.  Handling Unknown Relationships": "Identifying and labeling relationships that cannot be determined. In many cases", "unknown\" to avoid misleading conclusions.\n4.  Ensuring Consistency and Reliability": "Achieving consistency and reliability in the information extracted from articles. Variations in"}]}