{"title": "Real-Time AI-Driven People Tracking and Counting Using Overhead Cameras", "authors": ["Ishrath Ahamed", "Chamith Dilshan Ranathunga", "Dinuka Sandun Udayantha", "Benny Kai Kiat Ng", "Chau Yuen"], "abstract": "Accurate people counting in smart buildings and intelligent transportation systems is crucial for energy management, safety protocols, and resource allocation. This is especially critical during emergencies, where precise occupant counts are vital for safe evacuation. Existing methods struggle with large crowds, often losing accuracy with even a few additional people. To address this limitation, this study proposes a novel approach combining a new object tracking algorithm, a novel counting algorithm, and a fine-tuned object detection model. This method achieves 97% accuracy in real-time people counting with a frame rate of 20-27 FPS on a low-power edge computer.", "sections": [{"title": "I. INTRODUCTION", "content": "Keeping track of the number of people who have entered, exited, and are remaining inside a building or public transport is essential for crowd, facility, and safety management. Even though recent advancements in technology have paved the path for smart buildings and intelligent public transportation systems, it remains challenging to develop economical, efficient, and accurate systems to count people in real-time. Several works have been conducted since the early 2000s for real-time people counting using signal processing and deep learning techniques. Regardless of the technology they use, the reliability, accuracy of these, and cost to implement them are questionable. Either the highly accurate systems are extremely expensive or low-cost systems are not accurate and reliable.\nThe use of time-of-flight concept sensors such as laser beam, thermal, and ultrasound sensors tends to fail when two or more people need to be counted. Jae Hoon et al. [1] attempted using laser range finders. The system requires two sensors to function properly, and these sensors need specific placement depending on the environment. This customization makes it difficult to implement in various settings. In order to address these issues Jeong Woo et al. [2] proposed a new method using two ultra-wide band radar sensors. Even in this approach, the placement of two sensors is crucial, placing them very close leads to undercounting, and placing them far apart leads to miscalculations such as double counting. Yanni et al. [3] and Tiang et al. [4] have done studies employing WiFi and analyzing the phase, but they are not ideal as WiFi easily get distorted by other objects leading the system for inaccurate results.\nIn this deep learning and image processing era, several works have been carried out to count people. In [5], they introduced an image processing-based method employing background subtraction and blob detection to track objects, but it suffered from sensitivity to illumination changes and false positives caused by non-human objects in the frame. Li Guangqin et al. [6] proposed a depth camera-based solution to mitigate illumination issues but still faced challenges with false positives, while [7], inspired by [6], used depth cameras for background subtraction and 3D reconstruction for person detection, yet struggled with detecting individuals outside predefined human model dimensions. In response to challenges with traditional image processing methods, researchers have explored deep learning approaches for person detection and tracking. Guojin et al. [8] utilizes a convolutional neural network (CNN) for head detection and a spatio-temporal context tracking algorithm but faces issues with computational efficiency and sensitivity to head rotations, while [9] proposes \"cluster pruning\" to enhance real-time performance, achieving improved frames per second (FPS) values for people counting tasks but still falling short for monitoring door crossings due to low FPS rates.\nTherefore, a low-cost yet reliable system with a high FPS rate is needed for real-time people counting on edge devices. To this end, we propose a new efficient tracking algorithm, a counting algorithm, and a fine-tuned model for object detection in any complex environment. With these proposed methods we obtain higher FPS rates even when there are more than two people in the frame in both good and low lighting conditions. This achieves an overall accuracy of 97% in real-time video testing which is a 2% improvement compared to the best state-of-the-art and high frame rate: 20-27 FPS on average."}, {"title": "II. METHODOLOGY", "content": "This section is divided into four subsections. In the subsection II-A, we introduce the the proposed object detection model. Subsections II-B and II-C are dedicated to discuss the newly introduced feature extraction method and tracking algorithm respectively. Finally in subsection II-D we introduce the method employed in this proposed algorithm to count people."}, {"title": "A. Head Detection", "content": "Our video input originates from an overhead camera positioned above a doorway. We initially consider background subtraction and blob detection for head identification, as explored by prior researchers. However, this method suffers from accuracy limitations, further exacerbated by complexities introduced by the opening and closing of the door. Additionally, these methods present significant challenges in distinguishing heads from potential distractions such as bags, chairs, and trolleys.\nConsequently, we opt for object detection using deep learning techniques to identify human heads. We identify that the SSD Mobilenet model offers a compelling combination of accuracy, speed, and adaptability for our requirements. [10] Our head detection with SSD Mobilenet employs two distinct, self-trained models:\n1) Performs object detection under daylight conditions.\n2) Performs object detection in low-light/night light conditions.\nThe camera we use ((Tapo C100 WiFi Camera) is equipped with infrared (IR) mode to automatically discern between daytime and nighttime conditions. During nighttime, the camera switches to IR mode, resulting in grayscale images where the pixel values across all three channels (R, G, and B) remain constant as in Eq. 1. Therefore, we employ this characteristic of IR mode to determine nighttime conditions. When the camera operates in IR mode, indicating nighttime, the second model is utilized for object detection, ensuring optimal performance in low-light environments. One such instance is visualized in Fig. 1c\n$pixels (x, y); \\ R(x,y) = G(x,y) = B(x, y)$ (1)\nOur efforts focus on mitigating the influence of potential distractions, including chairs, trolleys, and bags. To achieve this, we fine-tune the SSD Mobilenet model to not only identify human heads captured by the overhead camera but also to distinguish them from these three aforementioned distractions. In Fig. 1, such an instance with a chair is visualized."}, {"title": "B. Feature Extraction", "content": "To enable head tracking across video frames, we extract features from the detected heads in each frame. These features provide a robust representation of the head, facilitating its identification in subsequent frames. We employ the MobileNetV2 [11] model for feature extraction due to its advantageous combination of lightweight architecture, accuracy, and mobile device compatibility. Notably, MobileNetV2 is pre-trained on the vast ImageNet dataset, offering a strong foundation for image matching tasks in our application.\nWe identify the feature extraction block as the system's computational bottleneck. To expedite the feature extraction process, we implement a two-pronged optimization strategies:\n\u2022 Model Compression: We compressed the MobileNetV2 model into a \".tflite\" format. This significantly reduces the model size, leading to faster loading and execution times.\n\u2022 Input Downsampling: We downsampled the input image fed to the feature extraction model to a resolution of (120, 120, 3). This technique effectively reduces the computational burden associated with feature extraction.\nConsequently, the input provided to the MobileNetV2 model is a cropped head image of size (120, 120, 3), and the model outputs an embedding vector of size (1, 1024)."}, {"title": "C. Tracking", "content": "Following head detection and feature extraction in each video frame, we implement a custom-designed tracking algorithm to facilitate person counting by monitoring head movement across subsequent frames. This algorithm functions by assigning a unique identifier (ID) to each newly detected head object. In subsequent frames, the algorithm compares the extracted features of newly detected head objects with those associated with existing head objects. If the features exhibit significant similarity, the newly detected head object is assigned the same ID as the corresponding existing head object. Conversely, if the features demonstrate a low degree of similarity, a new ID is assigned to the newly detected head object.\nThe object tracking algorithm is extensively outlined in Algorithm 1. M is the distance matrix that contains the feature distances between newly detected objects in the current frame and already registered objects in the program. Matrix element $M_{i,j}$ denotes the feature distance between the $i^{th}$ registered object and the $j^{th}$ newly detected object. Here, $i \\le m$ and $j \\leq n$, where m and n are the numbers of already registered objects and newly detected objects, respectively. Now, the object tracking problem is reduced to an assignment problem with the objective of minimizing the feature distance cost. The number of maximum possible assignments and assignments done is denoted by A and a, respectively, where $A = min(m, n)$. The threshold for feature distance and spatial distance is denoted by T and D, respectively. Let us also define the spatial distance matrix N. We also allow some degree of detection misses by setting a property called eCount for the objects. Let us denote the threshold for the number of consecutive detection misses for a particular object by E."}, {"title": "D. Counting Algorithm", "content": "The field of view (FOV) obtained by the overhead camera is divided into 3 regions, separated by 2 horizontal lines:\n\u2022 A: Totally outside region\n\u2022 B: Critical region\n\u2022 C: Totally inside region\nTo facilitate person counting, we incorporate a region history attribute for each \u201cperson-object.\u201d Dividing the field of view into three distinct regions surpasses the limitations of two-region systems that are susceptible to disturbing oscillations when people stay near the central boundary line. This region history attribute maintains a record of the object's traversal path. When a person-object moves from Region A to Region C, it is registered as entering the designated space (room). Conversely, movement from Region C to Region A signifies exiting the space."}, {"title": "III. RESULTS AND DISCUSSION", "content": "Our model is successfully implemented on an Intel\u00ae NUC 12 Pro Mini PC, the NUC12WSHi7 model. No GPU is used in the testing stage. The processor has 12 cores, 16 threads, and a max turbo frequency of up to 4.7 GHz in burst mode. The frame rate remains consistently above 23 frames per second (FPS) under standard conditions. Even with an increased number of individuals present in the frame, there is only a slight drop in FPS, which remains at an acceptable level for real-time applications."}, {"title": "IV. CONCLUSION", "content": "Despite the advancements in deep learning and image processing, accurately counting people from an overhead camera with low computational power has remained a challenge. In response, we have developed a new algorithm that leverages deep learning and image processing techniques to achieve state-of-the-art results in real-time people counting, regardless of lighting conditions. This algorithm can achieve impressive FPS ranging from 20 to 27 due to a highly optimized object tracking algorithm and fine-tuned object detection model. Future work includes improving the FPS rate for more than 3 people by optimizing the feature extraction method."}]}