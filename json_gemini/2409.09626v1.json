{"title": "Understanding Simplicity Bias towards Compositional Mappings via Learning Dynamics", "authors": ["Yi Ren", "Danica J. Sutherland"], "abstract": "Obtaining compositional mappings is important for the model to generalize well compositionally. To better understand when and how to encourage the model to learn such mappings, we study their uniqueness through different perspectives. Specifically, we first show that the compositional mappings are the simplest bijections through the lens of coding length (i.e., an upper bound of their Kolmogorov complexity). This property explains why models having such mappings can generalize well. We further show that the simplicity bias is usually an intrinsic property of neural network training via gradient descent. That partially explains why some models spontaneously generalize well when they are trained appropriately.", "sections": [{"title": "1 Introduction", "content": "There is a general belief that having more compositional representations is the key to improving compositional generalization [13]. Although there are many specifically designed algorithms (e.g., [17]) and network structures (e.g., [12, 22]) with this goal, methods to reliably obtain such representations in a variety of settings remain elusive. On the other hand, it has been repeatedly shown that compositional generalization ability can spontaneously emerge from standard supervised learning tasks [e.g. 16] or under repeated self-distillation training [e.g. 20]. In a recent position paper, Huh et al. [9] argued based on a variety of previous results that deep networks naturally adhere to Occam's razor, implicitly favoring simple solutions that fit the data.\nTo help understand the relationships between compositional mappings and the network's inherent bias, we first argue that compositional mappings are generally the simplest bijections to learn. Specifically, assuming the data-generating process is compositional, compositional mappings are the simplest (i.e. lowest-complexity). Next, we demonstrate experimentally that neural networks naturally favor simpler mappings through the process of gradient descent. This simplicity bias can be intuitively explained by the mutual influence of learning different samples, building on prior analyses [5, 21]. Although our experiments and discussions are restricted to a simplified setting, we believe the framework of this paper can help pave the way towards analyzing why some networks naturally achieve great compositional generalization ability under suitable learning tasks, and hopefully help inspire more effective algorithms to exploit this simplicity bias for more effective generalization."}, {"title": "2 Compositional Mappings are the Simplest Bijections", "content": "In this section, we first specify that the mapping from the ground-truth characters of the input signal to the learned representation is the key to analyzing the compositional generalization problem. We then describe the uniqueness of compositional mappings by analyzing their Kolmogorov complexity.\nIn short, we show that compositional mappings are simpler than non-compositional bijections."}, {"title": "3 Simpler Mappings are Learned Faster", "content": "The analysis above links the concepts of compositionally, simplicity, and Kolmogorov complexity under an idealized setting, which also aligns well with many related works. For example, Huh et al. [9] claim that simplicity bias is the key for the models in different modalities converging to a shared representation space that is similar to the ground truth. Goldblum et al. [4] also link Kolmogorov complexity to PAC-Bayes generalization bounds. This supports the idea that having more compositional mappings greatly benefits the model's generalization ability. This section further demonstrates that a neural network naturally favors such simpler mappings. We will first verify this claim by experiments under manual settings, and then provide a detailed explanation of why such a tendency exists using learning dynamics.\nSpecifically, we claim that simpler mappings are learned faster by a neural network trained using GD. To verify this, we consider a multi-label classification problem and create 256 different datasets (each only contains 4 examples) for each M in our \u201cToy256\u201d setting. For example, the dataset for the mapping in Figure 1-(c) should be {(blue box, 00), (blue circle, 01), (red box, 10), (red circle, 11)}, where the label \"01\" means Y\u2081 = 0 and Y2 = 1. We then randomly initialize a neural network as our ho and concatenate two Sigmoid functions as our go. With the same initialization and all hyper-parameters, we train the network to convergence for each dataset. We also consider different input signals (images and dense random vectors), network structures (MLP and ResNet), loss functions (cross-entropy and mean square error), and optimizers (standard SGD and Adam).\nFigure 2-(a) shows the training curves of 256 runs in our default setting. Since the only difference among these runs is the dataset generated by different mappings, it is safe to conclude that the difference in their learning speed is caused by the inherent bias of the model's learning behavior on this problem. From this figure, we see compositional mappings are learned faster than holistic ones. However, some mappings are learned even faster, which makes sense because those non-bijection mappings contain degenerate components, i.e., two or more objects are mapped to the same z, which means they are simpler. That also explains why the four degenerate mappings, which map all four objects to the same z, are learned fastest among all 256 mappings.\nTo further verify this, we quantify the learning speed using the concept of \"convergence time,\" i.e., the area under the learning curve. A smaller convergence time means the mapping is learned faster. This metric is similar to the C-score of Jiang et al. [10], which describes a training example's difficulty."}, {"title": "4 Conlusion", "content": "This paper first shows that compositional mappings are the simplest bijections in terms of Kolmogorov complexity or coding length. Then, using experiments and analysis from the learning dynamics perspective, the paper claims that the simplicity bias (in terms of coding length) is inherent in neural network training for typical architectures using gradient descent. Although the settings in the paper are simple, the theoretical formulation and analysis have the potential to be extended to more practical problems."}, {"title": "A Compositional Representation and Platonic Representation Hypothesis", "content": "This appendix tries to uncover the implicit relationship between compositional representation learning (usually studied in a manually toyish setting) and the Platonic representation hypothesis (proposed in [9], experimentally verified on many SOTA large vision and language models). Specifically, we focus on the following three aspects: 1.) the underlying assumption of the existence of G; 2.) the measuring metrics; 3.) the converging pressures. Our analysis hints that more advanced compositional generalization ability could also be achieved if we design appropriate learning systems following the fundamental principles demonstrated in Huh et al. [9]."}, {"title": "A.1 The Underlying Assumption of the Ground-truth Generating Mechanism", "content": "The main claim of Huh et al. [9] is that there exists a unique ground-truth idealized world (i.e., the G in our paper), from which, all observations in different modalities are its projections. A deep learning system, which learns from these projections and then generalizes to related tasks, are trying to uncover such ground truth. As the models in different modalities become stronger, their representations (i.e., z \u2208 Z in our paper) are more aligned, because they all tend to converge to the ground truth G.\nIn our paper, we also assume the existence of G and consider both input signal x and labels y are determined by it. By treating the mapping from G \u2192 Y as a special projection for a specific modality, our Figure 1-(a) becomes the upper part of the Figure 1 in [9]. The goal of a compositional representation learning task is to recover a good representation space that is similar to the ground truth, and hence generalize well to unseen combinations of attributes. This also aligns with the claim that \"models generalize better on different modalities align better to the ground-truth\" in [9]."}, {"title": "A.2 Measuring Metrics: Kernel Alignment, Disentanglement, and Topological Similarity", "content": "To mathematically describe the representation's convergence, Huh et al. [9] use three steps to define a metric called Kernel Alignment to measure the similarity between two representation spaces.\n1. A representation, which maps the input signal to a dense representation space, is a function f : X \u2192 Rn. Note that our ho(x) plays a similar role;\n2. A kernel, K : X x X \u2192 R, characterize the similarity between two elements in X. In a dense representation case, the inner product is usually applied, i.e., K(xi,Xj) = (f(xi), f(xj)), K \u2208 K. Our paper consider Hamming distance, because our G and z are all categorical variables;\n3. A kernel-alignment metric m : K \u00d7 K \u2192 R, measures the similarity between two kernels.\nFor the third-level measurement, Huh et al. [9] use Centered Kernel Distance (CKA), a kernel-alignment metric throughout their paper. Actually, many related works in compositional generalization also have similar measurements, e.g., the topological similarity proposed in [2]:\nTopsim(G, z) = Corr (dG(G(i), G(i)), dz(z(i), z(i))),\nThis definition also follows three steps: z are representations generated by feeding x to ho, dG and dz are distance measurements (or kernel in the second step above) for space G and Z, and Corr(, ) is the Spearman's correlation measuring the relationship between the output of two functions (kernels). In short, higher topological similarity means similar objects in G are mapped to similar positions in Z. If we consider G as another modality of the projected ground truth, the topological similarity is just a special kernel-alignment metric used in [9]. Also, some other measurements of compositionality like TRE (Tree Reconstruction Error, [1]), representation disentanglement [8], etc., also follow this principle generally. In summary, since the main measurement of the Platonic representation hypothesis and the compositional representation learning are essentially identical, we can draw more parallels between these two seemingly distinct fields in the future."}, {"title": "A.3 The Converging Pressures", "content": "Section 3 of [9] proposes three pressures that lead the model's representations to converge to the ground truth. We could also find some counterparts in the field of compositional representation"}, {"title": "B Coding Length and Topological Similarity for the Mappings in Toy256", "content": "The main target of this paper is to show that the simplicity bias is inherent in neural network training. Inspired by many related works on compositional generalization, we believe the Kolmogorov complexity is a perfect measurement for the simplicity of a mapping. However, it is well known that Kolmogorov complexity is usually hard to calculate and people typically use the minimum description length (MDL) under specific constraints as its approximation [14].\nTo experimentally show the correlation between learning speed and simplicity, we use the coding method provided in Kirby et al. [11] to calculate the coding length for all 256 mappings. Note that such a coding mechanism might not be optimal (i.e., its length is not the MDL). Hence we only call this measurement coding length (CL) throughout the paper.\nThe calculation of CL involves three steps. First, we convert all mappings using a compressed CFG, as illustrated in Figure 3-(a). As all the attributes studied here are categorical variables, we use b, x, r, c to represent blue, box, red, circle, respectively. Then, we delete all the redundant characters and generate the unique coding sequence for each CFG, as in Figure 3-(b). The special characters \"S\" and \";\" denote the starting and ending of one piece of rule and \",\" is used to separate different objects sharing the same message. Finally, the coding length in bits of a mapping Mis"}, {"title": "C Experimental Settings", "content": "We consider various settings for the Toy256 examples to verify that the simplicity bias discussed in this paper is general enough. For the input signals, we first consider two types of one-hot concatenation vectors. One is the concatenation of two 2-dim vectors (OHT2 for short). For example, blue box and red circle are encoded as [0101]\u00b7W4\u00d7d and [1010]\u00b7W4\u00d7d, respectively, where W4xd is a randomly initialized matrix fixed for all 256 mappings. Another setting is 0HT3, which considers a redundant dimension for each attribute, where blue box and red circle are encoded as [010010]\u00b7W6\u00d7d and [100100]\u00b7W6\u00d7d, respectively. We also consider the vision input, where the image is sampled and colored from the dSprite dataset, as illustrated in Figure 1.\nWe consider different network structures for different input modalities. For the one-hot input, we use an MLP with three hidden layers with a width of 128. For the image input, we consider both a 3-layer MLP and a ResNet9 [7] with narrower hidden layers. The task heads for all the networks are identical: we add two separate linear projection layers with size h \u00d7 2 on the output of the backbone. After that, we take Softmax on each of these outputs to generate probabilistic predictions. When calculating the loss function, we consider both cross-entropy (CE) loss and a mean square error (L2) loss, where the latter is calculated between the predicting probability vector and a one-hot distribution of the ground truth labels. When optimizing the network, we consider both stochastic gradient descent (SGD) and Adam. Unless otherwise stated, the learning rate is set to 10-3, weight decay is 5 * 10-4, and all other parameters are set to be the default values. Note that all hyper-parameters (including the initialization of the network) are shared for all 256 experiments in each group."}, {"title": "D More Experimental Results", "content": "To visualize the relationship between learning speed and the simplicity of each mapping, we provide three types of visualizations in Figure 4 and Figure 5. The first one is the learning curves of all 256 mappings. It is clear that under most settings, the blue curves (i.e., those for compositional mappings) decay faster than the red ones (the non-compositional bijections). The second one is the scatter plots showing the correlation between the converging time (i.e., the integral under the learning curve) and CL in Equation (4). The third one is the scatter plots showing the correlation between the converging time and topological similarity defined in Equation (3). We also calculate the Pearson correlation in the latter two cases in Table 1. It is clear that in most cases, simpler mappings are indeed learned faster under different settings. One exceptional case is training a ResNet with image input using Adam optimizer. The simplicity bias is even reversed compared with the results using SGD. This phenomenon hints that the bias in DNN's learning is also influenced by the inherent bias of specific network structures and optimizers. We left this for our future work."}]}