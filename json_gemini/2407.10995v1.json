{"title": "LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content", "authors": ["Jessica Foo", "Shaun Khoo"], "abstract": "As large language models (LLMs) become increasingly prevalent in a wide variety of applications, concerns about the safety of their outputs have become more significant. Most efforts at safety-tuning or moderation today take on a predominantly Western-centric view of safety, especially for toxic, hateful, or violent speech. In this paper, we describe LionGuard, a Singapore-contextualized moderation classifier that can serve as guardrails against unsafe LLM outputs. When assessed on Singlish data, LionGuard outperforms existing widely-used moderation APIs, which are not finetuned for the Singapore context, by 14% (binary) and up to 51% (multi-label). Our work highlights the benefits of localization for moderation classifiers and presents a practical and scalable approach for low-resource languages.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have led to a breakthrough in the generative abilities of conversational AI agents, achieving unprecedented levels of linguistic fluency and generalizability. Due to their strong conversational abilities, LLMs have been deployed to a range of domains such as workplace productivity, education, and customer service.\nGiven how frequently and directly users interact with these systems, moderation guardrails have been proposed to safeguard against the risk of LLMs generating harmful content. This is especially crucial for LLMs which may not have been sufficiently safety-tuned, as they can be easily instructed to generate hateful, toxic and offensive material at scale. In addition, moderation classifiers can also be used to automatically generate adversarial data (Perez et al., 2022) for safety alignment in LLM model training, score outputs from red-teaming, and benchmark LLMs in unsafe content generation. Hence, a robust and accurate moderation classifier can help in building safe LLMs at both the output (as guardrails) and input (augmenting training data) level.\nThe most widely used content moderation classifiers today include OpenAI's Moderation API\u00b9, Jigsaw's Perspective API,2 and Meta's LlamaGuard.\u00b3 While these classifiers are continually updated and have gradually incorporated multilingual capabilities (Lees et al., 2022), they have not been tested rigorously on low-resource languages. Singlish, an English creole (i.e. a variant of English) is widely used by people residing in Singapore, with a population of close to 5.5 million. As a creole language, Singlish has acquired its own unique phonology, lexicon and syntax (Ningsih and Rahman, 2023). As such, the linguistic shift between English and Singlish is significant enough such that existing moderation classifiers that perform well on English are unlikely to perform well on Singlish.\nWe present a practical and scalable approach to localizing moderation, which can be applied to any low-resource language. In this work, we make the following contributions:\n\u2022 Defining a safety risk taxonomy aligned to the local context. We constructed our safety risk taxonomy by combining existing taxonomies across various commercial providers and aligning these categories with relevant Singaporean legislation and guidelines, such as the Singapore Code of Internet Practice.\u2074\n\u2022 Creating a new large-scale dataset of Singlish texts for training moderation classifiers. We collected Singlish texts from various online forums, conducted automated labelling using safety-tuned LLMs using our safety risk taxonomy, resulting in a novel dataset of 138k Singlish texts that can be used for safety-tuning or benchmarking LLMs, or developing moderation classifiers.\n\u2022 Contextualized moderation classifier outperforms generalist classifiers. We finetuned a range of classification models on our automatically labelled dataset, and our best performing models outperformed Moderation API, Perspective API and LlamaGuard, while being faster and cheaper to run than using safety-tuned LLMs as guardrails. LionGuard is available on Hugging Face Hub.\u2076"}, {"title": "Singlish, an English Creole", "content": "Singlish is mainly influenced by non-English languages like Chinese, Malay, Tamil and Chinese dialects (e.g., Hokkien). While based on English, different languages are often combined within single utterances. To illustrate with the example below, the phrase \"chionging\" is derived from the Chinese romanized word \"chong\", which means \"to rush\"; the \"-ing\" indicates the progressive verb tense from English grammar; \"lao\" is the romanization of the Chinese word that means \"old\"; \"liao\" is a Singlish particle that means \"already\".\nSinglish also contains content-specific terminology. For example, \"ceca\", the Singlish racial slur which describes people of Indian nationality, is a derogatory synecdoche. It refers to the Comprehensive Economic Cooperation Agreement (CECA), a free-trade agreement signed between Singapore and India which has faced scrutiny in recent years.\u2077 Furthermore, new vocabulary has emerged in the online domain, such as the word \"sinkie\", which is a self-derogatory term referring to Singaporeans. Such lexicons are unlikely to be understood by Western-centric language models, unless they have been specifically trained or instructed to."}, {"title": "Related Work", "content": "The importance of content moderation has led to a plethora of works focused on the detection of toxic and abusive content (Nobata et al., 2016; de Gibert et al., 2018; Chakravartula, 2019; Mozafari et al., 2020; Vidgen and Yasseri, 2020). Bidirectional Encoder Representations from Transformers (BERTs) (Devlin et al., 2018) first emerged as powerful word embeddings that could be fine-tuned for downstream tasks like hate speech detection. Lee et al. (2021) used BERT embeddings in the Hateful Memes Challenge, while Liu et al. (2019) combined BERT and a Long-Short Term Memory model on the OffensEval dataset (Zampieri et al., 2019). Caselli et al. (2021) retrained BERT on offensive Reddit comments, building a shifted BERT model, HateBERT, that outperformed general BERT in hate speech detection.\nModeration APIs have become increasingly popular due to the ease at which they can be integrated into applications. Such APIs aim to be universally applicable to different languages and domains. Jigsaw (2017) developed Perspective API, which uses multilingual BERT-based models that are then distilled into single-language Convolutional Neural Networks (CNNs) for each language supported. Markov et al. (2023) developed OpenAI's Moderation API, which uses a lightweight transformer decoder model with a multi-layer perceptron head for each toxicity category. However, one concern amidst the increasing adoption of moderation APIs is how strikingly different toxicity triggers are across the Western and Eastern contexts (Chong and Kwak, 2022), underscoring the importance of localized content moderation."}, {"title": "Low-resource language adaptation for moderation", "content": "Adapting toxicity detection to Singlish, Zou (2022) used a CNN to detect hate speech from Twitter data. Haber et al. (2023) curated a multilingual dataset of Reddit comments in Singlish, Malay and Indonesian and found that domain adaption of mBERT (Devlin et al., 2018) and XLM-R (Conneau et al., 2020) models improved F1 performance in detecting toxic comments. Prakash et al. (2023) analyzed multimodal Singlish hate speech by creating a dataset of offensive memes. Our work contributes to this space by establishing a more systematic approach to detecting unsafe content with automated labelling and by developing a contextualized moderation classifier which outperforms existing generalized moderation APIs."}, {"title": "Automated labelling", "content": "Despite requiring more time and resources, human labelling has frequently been used to generate gold standard labels for toxic speech, particularly via crowdsourcing (Davidson et al., 2017; Parrish et al., 2022). However, Waseem (2016) found that amateur annotators were more likely than expert annotators to label items as hate speech, causing poorer data quality. Considering the scale of data required for building safe LLMs, automated labelling has emerged as an alternative to human labelling. Bai et al. (2022) used Constitutional AI to automatically perform evaluations with Claude, and then trained a preference model using the dataset of AI preferences. Chiu and Alexander (2021) found that with few-shot learning, GPT-3 can be used to detect sexist or racist text. Plaza-del arco et al. (2023) also found that zero-shot prompting of FLAN-T5 produced favorable results on several hate speech benchmarks. Inan et al. (2023) proposed an LLM-based input-output safeguard model, LlamaGuard, which classifies text inputs based on specific safety risks as defined by prompts. Unlike existing works that rely on a single model for automated labelling, we combined several LLMs to provide more accurate and reliable labels, leveraging the collective wisdom and knowledge of several safety-tuned LLMs."}, {"title": "Methodology", "content": "To develop a robust moderation classifier that is sensitive to Singlish and Singapore's context, we adopted a 4-step methodology as seen in Figure 1."}, {"title": "Data Collection", "content": "To build a dataset of Singlish texts, we collected comments from HardwareZone's Eat-Drink-Man-Woman online forum and selected subreddits from Reddit on Singapore.\u2079 The former is notorious in Singapore as a hotspot of misogynistic, xenophobic, and toxic comments,\u00b9\u2070 while the latter is a popular online forum for Singapore-specific issues. We collected comments on all threads between 2020 and 2023 from both forums, resulting in a dataset of approximately 8.9 million comments.\nHowever, upon manual inspection of the data, only a small minority of the comments were unsafe. Both forums have a wide range of topics which are not always controversial or harmful, and forum moderators often remove the most toxic comments.\nTo ensure sufficient unsafe texts for a balanced dataset, we used entire threads that discussed controversial topics in Singapore or contained offensive words (see Appendix A), which were more likely to be unsafe. This resulted in approximately 400,000 texts, which we randomly subsampled to a smaller set of 69,000 potentially unsafe texts. We then randomly sampled another 69,000 texts from the remaining dataset that had not been identified as likely to be unsafe, for greater heterogeneity in topics and language, resulting in a final training dataset of 138,000 texts (examples in Appendix B)."}, {"title": "Safety Risk Taxonomy", "content": "Next, we referenced the moderation categories defined in OpenAI's Moderation API, Jigsaw's Perspective API and Meta's LlamaGuard, and took into consideration Singapore's Code of Internet Practice and Code of Practice for Online Safety.\u00b9\u00b9"}, {"title": "Automated Labelling", "content": "We then automatically labelled our Singlish dataset according to our safety risk categories using LLMs. Automated labelling with LLMs is increasingly popular given vast improvements in instruction-following with recent LLMs (Ouyang et al., 2022; Muennighoff et al., 2023; Xu et al., 2024).\nTo verify the accuracy of our automated labelling, we internally labelled 200 texts that served as our expert-labelled dataset. The dataset was handpicked by our team with a focus on selecting particularly challenging texts that were likely to be mislabelled. This consisted of 143 unsafe texts (71.5%) and 57 safe texts (28.5%)."}, {"title": "Engineering the labelling prompt", "content": "We incorporated the following prompt engineering methods for our automated labelling:\n1. Context prompting (OpenAI, 2023): We specified that the text to be evaluated is in Singlish and that the evaluation needs to consider Singapore's socio-cultural context. We also provided examples and definitions of common offensive Singlish slang.\n2. Few-shot prompting (Brown et al., 2020): We gave examples of Singlish texts (that included Singlish slang and Singaporean references) and associated safety risk labels.\n3. Chain-of-Thought (CoT) prompting (Wei et al., 2023): We specified each step that the LLM should take in evaluating the text, asking it to consider whether the text fulfils any of the seven criteria, and to provide a \"yes/no\" label along with a reason for its decision.\nTo determine the effectiveness of these prompt engineering techniques, we conducted an ablation study that compared the performance of the:\n(a) Full prompt (combining all three methods)\n(b) Full prompt less context prompting (no Singlish examples)\n(c) Full prompt less few-shot prompting\n(d) Full prompt less CoT prompting\nWe measured how effective the prompts were in terms of their F1 score (i.e. taking into account precision and recall of detecting unsafe content with respect to our expert-labelled dataset)\u00b9\u00b3 and agreement (i.e. how frequently the LLMs concurred). F1 scores were chosen as our evaluation metric as the expert-labelled dataset was slightly skewed to unsafe text (71.5%)."}, {"title": "LLM Selection", "content": "We started with four candidate LLMs: OpenAI's GPT-3.5-turbo (version 0613) (Brockman et al., 2023), Anthropic's Claude 2.0 (Anthropic, 2023), Google's PaLM 2 (text-bison-002) (Anil et al., 2023), and Meta's Llama 2 Chat 70b (Touvron et al., 2023). These LLMs were chosen as they were the top-performing LLMs at the time and had also been safety-tuned.\nWe assessed each LLM's accuracy by comparing their F1 scores in labelling texts against the expert-labelled dataset. We ran all four prompts detailed in subsection 4.3.1 for each of the candidate LLMs. \u00b9\u2074\nAs seen in Figure 3, Llama 2 underperformed by a clear margin compared to the other three candidate LLMs when the full prompt was used. Upon closer inspection, we found that Llama 2 predicted nearly every text as unsafe,\u00b9\u2075 and this behaviour persisted despite additional changes to the prompt. Through error analysis (see Appendix E), we found that Llama 2 was overly conservative and provided incorrect justifications for classifying safe text as unsafe. As such, Llama 2 was dropped to avoid distorting the labels for our classification dataset."}, {"title": "Determining the Threshold for Safety", "content": "After determining the best prompt and set of LLMs for labelling, we considered two thresholds for determining unsafe content: majority vote (i.e. at least two of three LLMs label the text as unsafe) or consensus (i.e. all 3 LLMs label the text as unsafe). We compared the F1 scores and agreement for these two threshold levels, as seen in Figure 4.\nAs we were assembling a new dataset to build a contextualized moderation classifier from scratch, we determined that the priority was labelling accuracy. As such, we chose the consensus approach for our training (see subsection 4.4), which had higher accuracy even though the agreement rate is lower."}, {"title": "Compiling the dataset", "content": "The final dataset consisted of 138,000 labelled texts. The breakdown of the number of positive labels in the dataset can be found in Table 2. Note the severe imbalance of data for most categories, which made our model training process challenging. The dataset was split into train (70%), validation (15%), and test (15%) sets. Texts from the same threads were allocated to the same split. All experimental results in section 5.2 are reported using the test set."}, {"title": "Moderation Classifier", "content": "Architecture: LionGuard, our moderation classifier, comprises two components: an embedding and classifier model. The embedding model generates a vector representation of the text, which the classifier model uses as inputs to generate a moderation score. This simple architecture enables us to test different embedding and classifier models to find the best-performing combination for LionGuard.\nEmbedding model: Our approach compared general embedding models against finetuned models. We chose BAAI General Embedding (BGE) (Xiao et al., 2023) given its strong performance on Hugging Face's leaderboard for embeddings,\u00b9\u2076 HateBERT (Caselli et al., 2021), as well as SingBERT (Lim, 2023). We also experimented with masked language modelling (MLM) on these embedding models on a separate sample of 500,000 texts from our initial dataset of 8.9m texts for 30 epochs. Ablation studies were also conducted with BGE-small, BERT-base and BERT-large embedding models.\nClassifier model: We selected our classifier models based on different levels of model complexity to reveal any differences in performance due to the number of parameters. In order of complexity, we chose a ridge regression classifier, XGBoost classifier, and a neural network (consisting of one hidden and one dropout layer). We carried out hyperparameter tuning for the XGBoost and neural network classifier. More details on the hyperparameter search and the final set of hyperparameters are provided in Appendix F.\nTraining: We developed two versions of LionGuard: a binary classifier (to detect if a text is safe or unsafe), and a multi-label classifier (to detect if a text fulfills any category in our safety risk taxonomy defined in 4.2).\nFor the binary classifier, we limited the training data to texts where there was consensus among the LLMs on the label (i.e., unsafe or safe). This resulted in a smaller dataset of 99,597 texts (72.2%). For the multi-label classifier, we trained a dedicated classifier model for each category. We included texts where there was a consensus for that category, which enabled us to maximize the use of our limited number of positive labels. Apart from the toxic category, there was consensus on over 96% of the labels for each of the other categories.\u00b9\u2077\nEvaluation: Due to the heavily imbalanced dataset, we chose the Precision-Recall AUC (PR-AUC) as our evaluation metric as it can better represent the classifier's ability to detect unsafe content across all score thresholds. PR-AUC was also used by OpenAI (Markov et al., 2023) and LlamaGuard (Inan et al., 2023) in their evaluations.\nBenchmarking: After identifying the best classifier, we compared LionGuard with Moderation API, Perspective API and LlamaGuard. Both APIs provided a score, while LlamaGuard provided the probability of the first (classification) token, which we used to calculate the PR-AUC. We benchmarked them on both the binary and multi-label experiments."}, {"title": "Results", "content": "We compared labels provided by the LLMs with labels annotated by crowdsourced human labellers to further validate the accuracy of LLM labels. We worked with TicTag, a Singapore-based annotation company, to crowdsource for workers who could understand Singlish. These workers accessed the labelling task via TicTag's mobile application (see Appendix G), and were provided extensive instructions, including the safety risk taxonomy and examples. They could choose \"I Don't Know\" if they did not understand the text. 95 workers labelled 11,997 unique texts randomly drawn from our final dataset (see subsection 4.3.4), with each text labelled by 3 different workers. The demographic profile of the workers were reflective of Singapore's population characteristics (see Appendix G.1).\nOf the 11,997 texts, we found that crowdsourced human labellers had low concurrence (i.e., inter-rater agreement). As seen in Table 3, human labellers only had full concurrence on binary labels 52.9% of the time. Concurrence on binary labels was significantly lower than individual categories as we found that human labellers tended to label more texts unsafe, resulting in lower concurrence on generally safe texts. On less contentious categories like self-harm, public harm, sexual and violence, concurrence occurred more than 85% of the time. In contrast, toxic and hateful categories had less than 75% concurrence. Even with detailed instructions and strong quality control measures, the inherent subjectivity of labelling harmful content makes it challenging to achieve consensus among non-expert human labellers.\nFor sentences with concurrence among all human labellers and all LLM labellers respectively, we found that the human labels generally have high concurrence with LLM labels (see Table 3), with the concurrence rate exceeding 90% for all categories. This suggested that where human labels were consistent, LLMs were relatively accurate in providing labels aligned with human judgment. However, in contentious and ambiguous cases where human labels are inconsistent, evaluating the accuracy and concurrence of LLM labels vis-\u00e0-vis human labels is an area for future work."}, {"title": "Classifier Results", "content": "Model experimentation results (see Table 4): On the binary label, we found that the classifiers which used BGE Large performed significantly better than those which used HateBERT and SingBERT. Based on our ablation study with BERT-base, BERT-large and BGE-small models (see Appendix H), which all performed poorly, we posit that the number of parameters and type of pre-training embeddings are critical in improving performance. As for the classifier model, the ridge classifier performed comparably to XGBoost and the neural network for all embedding models despite its relative simplicity. We also found that MLM finetuning on the embedding models had a negligible effect on performance (see Appendix H).\nFor the multi-label classifiers, we similarly found that the classifiers which used BGE Large were the best performers by a large margin. Likewise, the ridge classifier performed best, indicating that a simple classification layer is sufficient for good performance, given a complex embedding model.\nOverall, the best performing combination was the BGE model combined with the Ridge classifier. We used this combination for LionGuard, our moderation classifier.\nBenchmarking results (see Table 4): We found that LionGuard significantly outperformed Moderation API, Perspective API and LlamaGuard.\nOn the binary label experiments, LionGuard's PR-AUC score of 0.819 is higher than OpenAI's 0.675, Perspective's 0.588 and LlamaGuard's 0.459.\u00b9\u2078 Likewise, for multi-label classification, LionGuard outperformed on all categories. The difference in performance is especially clear for the harassment, sexual, toxic and violent categories, with the performance more than doubled."}, {"title": "Discussion", "content": "Importance of localization: Our work suggests a clear need for contextualized moderation classifiers to detect localized slang and dysphemisms that are not offensive elsewhere. In our error analysis of a few examples where Moderation API, Perspective API and LlamaGuard failed to provide accurate labels (see Table 8 in Appendix I), LionGuard was able to understand Singapore-specific slang and references like \"ceca\", \"kkj\" and \"AMDK\" and provide the correct label. In contrast, Moderation API, Perspective API and LlamaGuard seemed to perform better in examples where only offensive English words or references (e.g. \"fuck\", \"cuck\", \"scum\") were present. Hence, while Moderation API, Perspective API and LlamaGuard are well-adapted to Western-centric toxicity, LionGuard is able to perform better on Singlish texts.\nHowever, LionGuard may not generalize well to other domains and languages, as it was trained specifically to detect harmful content in the Singapore context. Nonetheless, our approach can be adapted to any low-resource languages which require localization. Future work can use LionGuard to generate adversarial Singlish-centric data (Perez et al., 2022) to augment moderation training data and refine generalist moderation classifiers (Markov et al., 2023) for better performance even on low-resource languages.\nBenefits of automated LLM labelling: While crowdsourced labelling works well with simple tasks with an objective truth, we found that it may have limited mileage for subjective tasks like assessing toxicity or harassment. Each person has a different understanding of what is unsafe, and aligning on these definitions is challenging. On the other hand, automated LLM labelling, with the right prompt, can achieve higher labelling accuracy and consistency. More importantly, this approach can be adapted to other low-resource languages, and easily updated as the language evolves. While our work adopted the consensus approach for automated labelling, future work can explore other methods for synthesizing varying LLM output labels, including self-reflection."}, {"title": "Conclusion", "content": "We highlighted the importance of low-resource language localization for moderation by showing that our finetuned classifier, LionGuard, outperformed existing widely-used moderation APIs. We evaluated the best prompt and LLMs for automatic labelling, and presented a practical and scalable approach to automatically generating labels for low-resource language moderation data. We hope our work encourages more to build moderation tools that excel in both general and localized contexts."}, {"title": "Limitations", "content": "Dataset. As our dataset is a static, albeit up-to-date, snapshot of the online discourse in Singapore, our model may become less effective as the linguistic features of Singlish inevitably change over time. However, our approach simply requires a change in the prompt used for labelling in order to generate accurate labels for a new dataset. Moreover, active learning can be used to continually learn from production data (Markov et al., 2023) and ensure that the moderation classifier performs well over time. Future work can incorporate our methods (e.g., automated labelling) into an end-to-end pipeline to ensure a robust detection model for real-world applications.\nExperiments. As the focus of our work was to highlight the importance of localized content moderation, we did not perform extensive experiments on varying model hyperparameters. However, our results found that the ridge classifier, which only depended on one hyperparameter, could outperform generalist classifiers. For future work, we hope to experiment with finetuning encoder-decoder transformer language models with a classification output layer, which may perform even better than our relatively simple classifier models.\nLLM Labelling Accuracy. While we have attempted to validate our LLM labels both with our expert-labelled dataset and crowd-sourced labels, we cannot completely guarantee the accuracy of our LLM labels. However, our work aims to demonstrate the potential of LLM labelling with prompt engineering, as an alternative to manual human-labelling. Future work can explore more advanced LLM labelling techniques to increase accuracy."}, {"title": "Ethical Considerations", "content": "Labeller Wellbeing. Workers were informed about the nature of the task before commencing their work. They completed their work in batches, on their own schedules, and could decide to withdraw at any point in time. Trigger warnings were placed in the task description and mental health resources were made available by TicTag to the workers. Workers were compensated at a rate of SG$0.20 per text annotated. TicTag shared that the workers annotated approximately 80 texts per half an hour, which adds up to SG$32 per hour, well above the living wage in Singapore. No identifiable information was provided to us about our workers.\nData Privacy and Terms of Use. Reddit data was collected via the Pushshift API (Baumgartner et al., 2020). We collected Hardwarezone data that was publicly available, in a manner that is permissible pursuant to the Singapore Copyright Act 2021, which allows for the use of copyrighted works for computational data analysis (i.e., machine learning).\nModel Terms of Use. We used LLMs commercially licensed by OpenAI, Anthropic and Google and abided by their Terms of Use. We also accessed Llama 2 via Hugging Face, licensed by Meta. We accepted and abided by Meta's license terms and acceptable use policy. We accessed BGE, SingBERT and HateBERT via Hugging Face Hub and abided by their Terms of Use. Our moderation classifier, LionGuard, will be made available on Hugging Face for research and public interest purposes only.\nEnvironmental Impact. We only trained lightweight models in our main experiments, such as a ridge classifier, XGBoost and a simple neural network. The most significant training required was unsupervised MLM fine-tuning of the embedding models, which took approximately three days on two NVIDIA Tesla V100s. Compared to the environmental costs of pre-training LLMs, the environmental impact of our work is relatively small."}, {"title": "Examples of predictions on Singlish texts", "content": "See Table 8 for examples of predictions on Singlish texts from LionGuard, Moderation API, Perspective API and LlamaGuard. The categories are with reference to our safety risk categories. Ticks represent the model correctly predicted the text as fulfilling the definition of that category, and crosses indicate the model did not detect the text as belonging to that category."}]}