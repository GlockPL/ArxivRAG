{"title": "LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content", "authors": ["Jessica Foo", "Shaun Khoo"], "abstract": "As large language models (LLMs) become increasingly prevalent in a wide variety of applications, concerns about the safety of their outputs have become more significant. Most efforts at safety-tuning or moderation today take on a predominantly Western-centric view of safety, especially for toxic, hateful, or violent speech. In this paper, we describe LionGuard, a Singapore-contextualized moderation classifier that can serve as guardrails against unsafe LLM outputs. When assessed on Singlish data, LionGuard outperforms existing widely-used moderation APIs, which are not finetuned for the Singapore context, by 14% (binary) and up to 51% (multi-label). Our work highlights the benefits of localization for moderation classifiers and presents a practical and scalable approach for low-resource languages.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have led to a break-through in the generative abilities of conversational AI agents, achieving unprecedented levels of linguistic fluency and generalizability. Due to their strong conversational abilities, LLMs have been deployed to a range of domains such as workplace productivity, education, and customer service.\nGiven how frequently and directly users interact with these systems, moderation guardrails have been proposed to safeguard against the risk of LLMs generating harmful content. This is especially crucial for LLMs which may not have been sufficiently safety-tuned, as they can be easily instructed to generate hateful, toxic and offensive material at scale. In addition, moderation classifiers can also be used to automatically generate adversarial data (Perez et al., 2022) for safety alignment in LLM model training, score outputs from red-teaming, and benchmark LLMs in unsafe content generation. Hence, a robust and accurate moderation classifier can help in building safe LLMs at both the output (as guardrails) and input (augment-ing training data) level.\nThe most widely used content moderation classifiers today include OpenAI's Moderation API\u00b9, Jig-saw's Perspective API,2 and Meta's LlamaGuard.3 While these classifiers are continually updated and have gradually incorporated multilingual capabilities (Lees et al., 2022), they have not been tested rigorously on low-resource languages. Singlish, an English creole (i.e. a variant of English) is widely used by people residing in Singapore, with a population of close to 5.5 million. As a creole language, Singlish has acquired its own unique phonology, lexicon and syntax (Ningsih and Rahman, 2023). As such, the linguistic shift between English and Singlish is significant enough such that existing moderation classifiers that perform well on English are unlikely to perform well on Singlish.\nWe present a practical and scalable approach to localizing moderation, which can be applied to any low-resource language. In this work, we make the following contributions:\n\u2022 Defining a safety risk taxonomy aligned to the local context. We constructed our safety risk taxonomy by combining existing tax-onomies across various commercial providers and aligning these categories with relevant Singaporean legislation and guidelines, such as the Singapore Code of Internet Practice.4\n\u2022 Creating a new large-scale dataset of Singlish"}, {"title": "2 Singlish, an English Creole", "content": "Singlish is mainly influenced by non-English lan-guages like Chinese, Malay, Tamil and Chinese dialects (e.g., Hokkien). While based on English, different languages are often combined within sin-gle utterances. To illustrate with the example below, the phrase \"chionging\" is derived from the Chinese romanized word \"chong\", which means \"to rush\"; the \"-ing\" indicates the progressive verb tense from English grammar; \"lao\" is the romanization of the Chinese word that means \"old\"; \"liao\" is a Singlish particle that means \"already\".\nSinglish also contains content-specific terminol-ogy. For example, \"ceca\", the Singlish racial slur which describes people of Indian nationality, is a derogatory synecdoche. It refers to the Comprehen-sive Economic Cooperation Agreement (CECA), a free-trade agreement signed between Singapore and India which has faced scrutiny in recent years. Furthermore, new vocabulary has emerged in the online domain, such as the word \"sinkie\", which is a self-derogatory term referring to Singapore-ans. Such lexicons are unlikely to be understood by Western-centric language models, unless they have been specifically trained or instructed to.\nSeveral works have emerged to tackle Singlish for various Natural Language Processing (NLP) tasks, including sentiment analysis (Lo et al., 2016; Bajpai et al., 2018; Ho et al., 2018), parts-of-speech tagging (Wang et al., 2019) and neural machine translation (Sandaruwan et al., 2021). Hsieh et al. (2022) trained a Singlish BERT model to identify Singlish sentences, while Lim (2023) fine-tuned BERT on a colloquial Singlish and Manglish corpus (SingBERT). Such efforts highlight the significant linguistic differences between English and Singlish and the need for Singlish-focused content moderation."}, {"title": "3 Related Work", "content": "The importance of content moderation has led to a plethora of works focused on the detection of toxic and abusive content (Nobata et al., 2016; de Gib-ert et al., 2018; Chakravartula, 2019; Mozafari et al., 2020; Vidgen and Yasseri, 2020). Bidirec-tional Encoder Representations from Transformers (BERTS) (Devlin et al., 2018) first emerged as pow-erful word embeddings that could be fine-tuned for downstream tasks like hate speech detection. Lee et al. (2021) used BERT embeddings in the Hateful Memes Challenge (Kiela et al., 2021), while Liu et al. (2019) combined BERT and a Long-Short Term Memory model on the OffensEval dataset (Zampieri et al., 2019). Caselli et al. (2021) re-trained BERT on offensive Reddit comments, build-ing a shifted BERT model, HateBERT, that outper-formed general BERT in hate speech detection.\nModeration APIs have become increasingly pop-ular due to the ease at which they can be integrated into applications. Such APIs aim to be universally applicable to different languages and domains. Jig-saw (2017) developed Perspective API, which uses multilingual BERT-based models that are then dis-tilled into single-language Convolutional Neural"}, {"title": "3.2 Low-resource language adaptation for moderation", "content": "Adapting toxicity detection to Singlish, Zou (2022) used a CNN to detect hate speech from Twitter data. Haber et al. (2023) curated a multilingual dataset of Reddit comments in Singlish, Malay and Indonesian and found that domain adaption of mBERT (Devlin et al., 2018) and XLM-R (Conneau et al., 2020) models improved F1 performance in detecting toxic comments. Prakash et al. (2023) analyzed multimodal Singlish hate speech by creating a dataset of offensive memes. Our work contributes to this space by establishing a more systematic approach to detecting unsafe content with automated labelling and by developing a contextualized moderation classifier which outperforms existing generalized moderation APIs."}, {"title": "3.3 Automated labelling", "content": "Despite requiring more time and resources, human labelling has frequently been used to generate gold standard labels for toxic speech, particularly via crowdsourcing (Davidson et al., 2017; Parrish et al., 2022). However, Waseem (2016) found that amateur annotators were more likely than expert anno-tators to label items as hate speech, causing poorer data quality. Considering the scale of data required for building safe LLMs, automated labelling has emerged as an alternative to human labelling. Bai et al. (2022) used Constitutional AI to automatically perform evaluations with Claude, and then trained a preference model using the dataset of AI preferences. Chiu and Alexander (2021) found that with few-shot learning, GPT-3 can be used to detect sexist or racist text. Plaza-del arco et al. (2023) also found that zero-shot prompting of FLAN-T5 produced favorable results on several hate speech benchmarks. Inan et al. (2023) proposed an LLM-based input-output safeguard model, LlamaGuard, which classifies text inputs based on specific safety risks as defined by prompts. Unlike existing works that rely on a single model for automated labelling, we combined several LLMs to provide more accurate and reliable labels, leveraging the collective wisdom and knowledge of several safety-tuned LLMs."}, {"title": "4 Methodology", "content": "To develop a robust moderation classifier that is sensitive to Singlish and Singapore's context, we adopted a 4-step methodology as seen in Figure 1."}, {"title": "4.1 Data Collection", "content": "To build a dataset of Singlish texts, we collected comments from HardwareZone's Eat-Drink-Man-Woman online forum and selected subreddits from Reddit on Singapore. The former is notorious in Singapore as a hotspot of misogynistic, xenophobic, and toxic comments, while the latter is a popular online forum for Singapore-specific issues. We collected comments on all threads between 2020 and 2023 from both forums, resulting in a dataset of approximately 8.9 million comments.\nHowever, upon manual inspection of the data, only a small minority of the comments were unsafe. Both forums have a wide range of topics which are not always controversial or harmful, and forum moderators often remove the most toxic comments.\nTo ensure sufficient unsafe texts for a balanced dataset, we used entire threads that discussed controversial topics in Singapore or contained offensive words (see Appendix A), which were more likely to be unsafe. This resulted in approximately 400,000 texts, which we randomly subsampled to a smaller set of 69,000 potentially unsafe texts. We then randomly sampled another 69,000 texts from the remaining dataset that had not been identified as likely to be unsafe, for greater heterogeneity in topics and language, resulting in a final training dataset of 138,000 texts (examples in Appendix B)."}, {"title": "4.2 Safety Risk Taxonomy", "content": "Next, we referenced the moderation categories defined in OpenAI's Moderation API, Jigsaw's Perspective API and Meta's LlamaGuard, and took into consideration Singapore's Code of Internet Practice and Code of Practice for Online Safety."}, {"title": "4.3 Automated Labelling", "content": "We then automatically labelled our Singlish dataset according to our safety risk categories using LLMs. Automated labelling with LLMs is increasingly popular given vast improvements in instruction-"}, {"title": "4.3.1 Engineering the labelling prompt", "content": "We incorporated the following prompt engineering methods for our automated labelling:\n1. Context prompting (OpenAI, 2023): We specified that the text to be evaluated is in Singlish and that the evaluation needs to consider Singapore's socio-cultural context. We also provided examples and definitions of"}, {"title": "4.3.2 LLM Selection", "content": "We started with four candidate LLMs: OpenAI's GPT-3.5-turbo (version 0613) (Brockman et al., 2023), Anthropic's Claude 2.0 (Anthropic, 2023), Google's PaLM 2 (text-bison-002) (Anil et al., 2023), and Meta's Llama 2 Chat 70b (Touvron et al., 2023). These LLMs were chosen as they were the top-performing LLMs at the time and had also been safety-tuned.\nWe assessed each LLM's accuracy by comparing their F1 scores in labelling texts against the expert-labelled dataset. We ran all four prompts detailed in subsection 4.3.1 for each of the candidate LLMs."}, {"title": "4.3.3 Determining the Threshold for Safety", "content": "After determining the best prompt and set of LLMs for labelling, we considered two thresholds for de-termining unsafe content: majority vote (i.e. at least two of three LLMs label the text as unsafe) or consensus (i.e. all 3 LLMs label the text as un-safe). We compared the F1 scores and agreement for these two threshold levels, as seen in Figure 4."}, {"title": "4.3.4 Compiling the dataset", "content": "The final dataset consisted of 138,000 labelled texts. The breakdown of the number of positive labels in the dataset can be found in Table 2. Note the severe imbalance of data for most categories, which made our model training process challenging. The dataset was split into train (70%), validation (15%), and test (15%) sets. Texts from the same threads were allocated to the same split. All experimental results in section 5.2 are reported using the test set."}, {"title": "4.4 Moderation Classifier", "content": "LionGuard, our moderation classifier, comprises two components: an embedding and classifier model. The embedding model generates a vector representation of the text, which the classi-fier model uses as inputs to generate a moderation score. This simple architecture enables us to test different embedding and classifier models to find the best-performing combination for LionGuard."}, {"title": "5 Results", "content": "We compared labels provided by the LLMs with labels annotated by crowdsourced human labellers to further validate the accuracy of LLM labels. We worked with TicTag, a Singapore-based annotation company, to crowdsource for workers who could understand Singlish. These workers accessed the labelling task via TicTag's mobile application (see Appendix G), and were provided extensive instructions, including the safety risk taxonomy and examples. They could choose \"I Don't Know\" if they did not understand the text. 95 workers labelled 11,997 unique texts randomly drawn from our final dataset (see subsection 4.3.4), with each text labelled by 3 different workers. The demographic profile of the workers were reflective of Singapore's population characteristics (see Appendix G.1).\nOf the 11,997 texts, we found that crowdsourced human labellers had low concurrence (i.e., inter-rater agreement). As seen in Table 3, human la-bellers only had full concurrence on binary labels 52.9% of the time. Concurrence on binary labels was significantly lower than individual categories as we found that human labellers tended to label"}, {"title": "5.2 Classifier Results", "content": "On the binary label, we found that the classifiers which used BGE Large performed significantly better than those which used HateBERT and SingBERT. Based on our ablation study with BERT-base, BERT-large and BGE-small models (see Appendix H), which all performed poorly, we posit that the number of parameters and type of"}, {"title": "6 Discussion", "content": "Our work suggests a clear need for contextualized moderation classifiers to detect localized slang and dysphemisms that are not offensive elsewhere. In our error analysis of a few examples where Moderation API, Perspective API and LlamaGuard failed to provide accurate labels (see Table 8 in Appendix I), LionGuard was able to understand Singapore-specific slang and references like \"ceca\", \"kkj\" and \"AMDK\" and provide the correct label. In contrast, Moderation API, Perspective API and LlamaGuard seemed to perform better in examples where only offensive English words or references (e.g. \"fuck\", \"cuck\", \"scum\") were present. Hence, while Moderation API, Perspective API and LlamaGuard are well-adapted to Western-centric toxicity, LionGuard is able to perform better on Singlish texts.\nHowever, LionGuard may not generalize well to other domains and languages, as it was trained specifically to detect harmful content in the Singapore context. Nonetheless, our approach can be adapted to any low-resource languages which require localization. Future work can use Li-onGuard to generate adversarial Singlish-centric"}, {"title": "7 Conclusion", "content": "We highlighted the importance of low-resource language localization for moderation by showing that our finetuned classifier, LionGuard, outperformed existing widely-used moderation APIs. We evaluated the best prompt and LLMs for automatic labelling, and presented a practical and scalable approach to automatically generating labels for low-resource language moderation data. We hope our work encourages more to build moderation tools that excel in both general and localized contexts."}, {"title": "8 Limitations", "content": "As our dataset is a static, albeit up-to-date, snapshot of the online discourse in Singapore, our model may become less effective as the linguistic features of Singlish inevitably change over time. However, our approach simply requires a change in the prompt used for labelling in order to generate accurate labels for a new dataset. Moreover, active learning can be used to continually learn from production data (Markov et al., 2023) and ensure that the moderation classifier performs well over time. Future work can incorporate our methods (e.g., au-tomated labelling) into an end-to-end pipeline to ensure a robust detection model for real-world ap-plications.\nAs the focus of our work was to highlight the importance of localized content moderation, we did not perform extensive experiments on varying model hyperparameters. However, our results found that the ridge classifier, which only de-pended on one hyperparameter, could outperform generalist classifiers. For future work, we hope to experiment with finetuning encoder-decoder trans-former language models with a classification out-put layer, which may perform even better than our relatively simple classifier models.\nWhile we have at-tempted to validate our LLM labels both with our expert-labelled dataset and crowd-sourced labels, we cannot completely guarantee the accuracy of our LLM labels. However, our work aims to demon-strate the potential of LLM labelling with prompt engineering, as an alternative to manual human-labelling. Future work can explore more advanced LLM labelling techniques to increase accuracy."}, {"title": "9 Ethical Considerations", "content": "Workers were informed about the nature of the task before commencing their work. They completed their work in batches, on their own schedules, and could decide to with-draw at any point in time. Trigger warnings were placed in the task description and mental health resources were made available by TicTag to the workers. Workers were compensated at a rate of SG$0.20 per text annotated. TicTag shared that the workers annotated approximately 80 texts per half an hour, which adds up to SG$32 per hour, well above the living wage in Singapore. No identifiable information was provided to us about our workers."}, {"title": "A List of Controversial Topics and Words", "content": "\"ceca\", \"ghey\", \"tiong\", \"abnn\", \"amdl\", \"amdk\",\n\"pinoy\", \"jiuhu\", \"prc\", \"indian\", \"filipino\", \"for-eign\", \"angmo\", \"spg\", \"atb\", \"chennai\", \"****\",\n\"bbm\", \"ft\", \"fw\", \"transformer\", \"chink\", \"bangla\",\n\"yalam\", \"curry\", \"piak\", \"syt\", \"fap\", \"pcc\",\n\"nnp\", \"pika\", \" kkj\", \"abalone\", \"asgm\", \"btss\",\n\"hmv\", \"humsup\", \"milf\", \"nekkid\", \"nsfw\", \"ocb\",\n\"okt\", \"pcc\", \"perbird\", \"tps\", \"vpl\", \"parang\",\n\"slash\", \"punch\", \"kick\", \"shoot\", \"buibui\", \"bbfa\",\n\"cheesepie\", \"gcp\", \"diu lei\", \"ccb\", \"siao\", \"cheese\npie\", \"knn\", \"ccb\", \"pcb\", \"smlj\", \"tiu\", \"rcp\",\n\"asw\", \"bus3rd\", \"digger\", \"gcp\", \"vape\", \"weed\",\n\"drug\", \"launder\", \"wash money\", \"377a\", \"raeesah\nkhan\", \"oxley\", \"halimah\", \"brownface\", \"chi-nese privilege\", \"presidential election\", \"ashlee\",\n\"wuhan\", \"mahathir\", \"pink dot\", \"egg freezing\",\n\"kong hee\", \"schooling\", \"amos yee\", \"kurt tay\""}, {"title": "B Examples of Singlish texts in our dataset", "content": "\u2022 \"Hahahah now i dont play nice liao and take\nlonger liao but my personality cannot sia ha-hahaha dont like things to be left hanging\nthere. Now not my daiji i will throw to others\nhais but company cut cost so i got extra job\ntoo\" (Comment from Reddit, posted on Oct\n2020)\n\u2022 \"walao, never share with us, that's why you\nkena caught lah\" (Comment from Reddit,\nposted on May 2021)\n\u2022 \"wah lao i jio then u say cut down. knn jin\nhurt my heart. price is ok i will share with\nmy friends\" (Comment from HardwareZone,\nposted on Sep 2022)\n\u2022 \"siao bo, write such a long WOT to \"prove\"\nSAP ppl are mostly jiak kantangs. WTF\"\n(Comment from HardwareZone, posted on\nFeb 2023)"}, {"title": "C Toxicity Categories", "content": "1. Hateful: Content that expresses, incites, or\npromotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, dis-ability status, or caste. Hateful content aimed\nat non-protected groups (e.g., chess players)\nis harrassment. Hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnic-ity, religion, nationality, sexual orientation, disability status, or caste.\n2. Harassment: Content that expresses, incites, or promotes harassing language towards any\ntarget/individual. Without mention of vio-lence or harm, content that results in pro-longed mental or emotional suffering for the\ntarget, enduring for more than an hour. Any\nharassment content that includes violence or\nserious harm towards any target/individual.\n3. Encouraging public harm: Content that pro-motes, facilitates, or encourages harmful pub-lic acts, vice or organized crime.\n4. Encouraging self-harm: CContent that pro-motes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating dis-orders. Content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.\n5. Sexual: Content meant to arouse sexual excitement, such as the description of sexual\nactivity, or that promotes sexual services (ex-cluding sex education and wellness). Sexual content that includes an individual who is un-der 18 years old.\n6. Toxic: Content that is rude, disrespectful, or profane, including the use of slurs. A very hateful, aggressive, disrespectful comment or otherwise very likely to make a user leave a discussion or give up on sharing their perspective.\n7. Violent: Content that depicts death, violence, or physical injury."}, {"title": "D Labelling prompt", "content": "We share the full prompt used for automated labelling below, highlighting in different colors which sections correspond to context prompting, few-shot prompting, and Chain-of-Thought prompting respectively.\nContext Prompting\nContext: You are helping to identify\nif a given Singlish comment is toxic by\nunderstanding its meaning and evaluating\nit on seven criteria.\nSome examples of Singaporean slang\nand their definitions are provided\nbelow.\nghey: male homosexual, usually in a\nderogatory way\ntiong: people from China, may be in a\nderogatory way\nceca: people from India, usually in a derogatory way\nbbfa: people who are overweight and resigned to being alone\nyalam: people who are ethnically Malay\nfap: masturbation\npiak: sexual intercourse\nsyt: sexy young thing\nZG or zao geng: accidentally showing underwear in public, usually for women\nnnp: nipples\namdk: sarcastic reference to Caucasians being superior\nsiao: crazy\nknn: vulgar word\nkns: vulgar word\nsot: Singlish for \"shot\", meaning no longer functional\nTask: Iterate through the list of seven criteria below and determine whether the Singlish comment below, which includes Singaporean terms, fulfil any of the seven criteria below. You should be a more liberal labeller if something has a 50% chance of being toxic, you should label it as such. You also need to take into account the Singlish terms, vulgarities, and unique racial, religious, and cosmopolitan context.\na. hateful: Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harrassment. Hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.\nb. harassment: Content that expresses, incites, or promotes harassing language towards any target/individual. Harassment content that also includes violence or serious harm towards any target/individual.\nc. encouraging public harm: Content that"}, {"title": "I Examples of predictions on Singlish texts", "content": "See Table 8 for examples of predictions on Singlish texts from LionGuard, Moderation API, Perspec-tive API and LlamaGuard. The categories are with reference to our safety risk categories. Ticks rep-resent the model correctly predicted the text as fulfilling the definition of that category, and crosses indicate the model did not detect the text as belong-ing to that category."}]}