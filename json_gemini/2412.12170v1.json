{"title": "PickLLM: Context-Aware RL-Assisted Large Language Model Routing", "authors": ["Dimitrios Sikeridis", "Dennis Ramdass", "Pranay Pareek"], "abstract": "Recently, the number of off-the-shelf Large Language Models (LLMs) has exploded with many open-source options. This creates a diverse landscape regarding both serving options (e.g., inference on local hardware vs remote LLM APIs) and model heterogeneous expertise. However, it is hard for the user to efficiently optimize considering operational cost (pricing structures, expensive LLMs-as-a-service for large querying volumes), efficiency, or even per-case specific measures such as response accuracy, bias, or toxicity. Also, existing LLM routing solutions focus mainly on cost reduction, with response accuracy optimizations relying on non-generalizable supervised training, and ensemble approaches necessitating output computation for every considered LLM candidate. In this work, we tackle the challenge of selecting the optimal LLM from a model pool for specific queries with customizable objectives. We propose PickLLM, a lightweight framework that relies on Reinforcement Learning (RL) to route on-the-fly queries to available models. We introduce a weighted reward function that considers per-query cost, inference latency, and model response accuracy by a customizable scoring function. Regarding the learning algorithms, we explore two alternatives: PickLLM router acting as a learning automaton that utilizes gradient ascent to select a specific LLM, or utilizing stateless Q-learning to explore the set of LLMs and perform selection with a e-greedy approach. The algorithm converges to a single LLM for the remaining session queries. To evaluate, we utilize a pool of four LLMs and benchmark prompt-response datasets with different contexts. A separate scoring function is assessing response accuracy during the experiment. We demonstrate the speed of convergence for different learning rates and improvement in hard metrics such as cost per querying session and overall response latency.", "sections": [{"title": "1 Introduction", "content": "Recently, generative Large Language Models (LLMs) have emerged as a clear trend for solving a diverse set of problems such as assistive dialogue, summarization, text classification and coding aid [2, 24]. In addition, they are becoming more and more capable in terms of understanding complex contexts and providing factual accuracy due to larger training datasets, and an increase in model sizes [27]. Thus, their applicability for problem-solving across diverse applications and industry domains has led to the explosion of their number with new LLMs being released daily by academia and industry alike\u00b9, both in open-source fashion (e.g., Meta's Llama [25]) but also as fully supported LLMs-as-a-service(e.g., OpenAI's GPT models [1]).\nWhile the observed influx of proprietary and open-source LLMs adds diversity and a plethora of options for practitioners and newly flocked users, there are no obvious selection criteria for picking between the different LLM offerings [12]. Indeed, open-source LLMs tend to exhibit diverse weaknesses, strengths, and heterogeneous expertise in various domains mainly due to variations in architecture, training data input, and parameter tuning [2, 34]. Still, interested users are left with a couple of options to adjust LLMs to their use cases including fine-tuning, and even training their LLM from scratch. The latter can be an expensive process in terms of computation and large volume data gathering, and even challenging to even perform due to the scarcity of GPUs in the market. Fine-tuning on the other hand is resource friendlier, but requires a specific level of expertise among engineering teams. In addition, modern LLM offerings often restrict any direct model optimization through weight adjustments, providing just access to black-box APIs.\nFor all the reasons above, more and more practitioners are now relying on off-the-shelf LLMs for building their applications. However, relevant concerns are surfacing regarding the practical aspects of running them, namely rising costs, availability, inference latency, and average accuracy depending on the use case [3, 16, 32]. Indeed, the latest state-of-the-art models comprise billions of parameters leading to excessive computing power needs and even environmental impact, especially in cases where the application demands high-throughput delivery. A case in point is OpenAI's ChatGPT where the same query (4K-token context) is \u224820 times cheaper when GPT-3.5 is used in comparison to the newer GPT-4 model (8K-token context)2. Interestingly, the estimated accumulated cost of running a small business's customer support service in GPT4 can exceed $21K per month [3]."}, {"title": "Contributions", "content": "While such cutting-edge models can indeed handle the most demanding text generation tasks, the reality is that many use cases could be handled more than adequately by less-capable and thus less-expensive models. In addition, depending on the use case, the use of local open-source models or even quantized models running on CPUs can eliminate network latency and availability concerns where accuracy degradation can be tolerated. Given the observed variations in heterogeneous cost and quality, the efficient capitalization and real-time evaluation of all the available LLM solutions is quite understudied. There is, thus, the opportunity to identify the optimal LLM for a specific task (i.e., family of queries) and within a desired context (e.g., availability, latency, cost) in a lightweight, real-time manner that can also be adaptable to changes in query themes on-the-fly.\nContributions. In this paper, we tackle the challenge of selecting the optimal LLM from a model pool for specific queries with customizable objectives. Our contributions are summarized as follows:\n\u2022 We propose PickLLM, a lightweight framework that relies on Reinforcement Learning (RL) to route on-the-fly queries to available models.\n\u2022 We introduce a weighted reward function that considers per-query cost, inference latency, and model response accuracy by a scoring function.\n\u2022 We explore two alternatives: PickLLM router acting as a learning automaton that utilizes gradient ascent to select a specific LLM, or PickLLM utilizing stateless Q-learning to explore the set of LLMs and perform selection with a e-greedy approach.\n\u2022 We demonstrate the speed of convergence for different learning rates, and the effect of variations on the reward weights that result in selecting the optimal LLM in terms of cost, and latency with no accuracy degradation.\nStructure. Section 2 reviews related work on LLM selectors and routing. Section 3 analyses the PickLLM framework, and the underlying RL mechanisms. Section 4 describes our experimental setup, and presents our findings. Finally, Section 5 discussed future directions, while Section 6 concludes this paper."}, {"title": "2 Related Work", "content": "Existing solutions attempt optimizing LLM selection based on either strict monetary cost, or answer accuracy, rarely for both, and certainly without taking into account other throughput performance metrics. In [3], the authors propose the sequential use of different increasingly expensive LLMs in a cascade manner until the output of a dedicated scoring model (specifically DistilBERT) is acceptable. Similarly, AutoMix [16] utilizes small-size LLMs to produce an approximate correctness metric for outputs before strategically routing the queries further to larger LLMs. LLM cascades are also used in [31], where the authors utilize the \"answer consistency\" [26] of the weaker LLM as the signal of the query difficulty that will drive the decision to route towards a more capable, and more expensive LLM. The requirement of the above solutions falls on potentially utilizing multiple models per input before yielding the optimal result.\nAnother family of solutions relies only on computationally heavy pre-training of supervised reward models or uses generic datasets for the supervised training which does not provide generalization for different user cases. For instance, the authors in [21] utilize a regression model that is pre-trained on pairs of language model queries and related output scores. The work in [22] utilizes existing benchmark datasets to train an LLM router model through a collection of binary classification tasks. In [15] the authors deploy reward model ranking on a query-answer training set to obtain an estimation of expertise between open-source LLMs. The normalized rewards from the training phase are subsequently used to train a routing function that will make the final routing decision of a new query in the online phase.\nFinally, prior work on LLM selection relies on the selection of the LLM that generates the optimal output for the given input after producing all the possible outcomes. The authors in [14], and [20] propose the training of specific ranking and scoring models after considering outputs from every examined LLM. The work in [10] introduces a pairwise comparison to evaluate differences between candidate LLM outputs from the same query. Following that, a fusing mechanism merges the candidate responses that ranked higher to create the final improved output. However, ranking between answer pairs, although effective, becomes infeasible for real-time applications as the LLM option space expands as each candidate has to be compared with all the rest.\nRecently, a promising solution to the LLM routing problem has been proposed in [19]. The framework involves learning to dynamically route between a stronger model and a weaker model, thereby minimizing costs while achieving a specific performance target. The authors utilize human preference data and data augmentation techniques to enhance performance, and demonstrate significant cost savings without compromising response quality. This work highlights the potential of LLM routing to provide a cost-effective yet high-performance solution for deploying LLMs."}, {"title": "3 PickLLM", "content": "In this section, we describe the basic mechanisms behind PickLLM, which specifically addresses the problem of selecting the optimal LLM for a single use case by taking into account contextual optimization choices such as running cost, or response latency. We consider a pool of |M| available LLMs (set M = {1, ..., n, ..., |M|}) ready to receive user queries. The LLM set can be diverse, consisting of a mix of"}, {"title": "3.1 Overall Framework", "content": "In this section, we describe the basic mechanisms behind PickLLM, which specifically addresses the problem of selecting the optimal LLM for a single use case by taking into account contextual optimization choices such as running cost, or response latency. We consider a pool of |M| available LLMs (set M = {1, ..., n, ..., |M|}) ready to receive user queries. The LLM set can be diverse, consisting of a mix of"}, {"title": "3.2 Reward Function Formulation", "content": "Regarding the reward function, we considered a small set of hard performance-related parameters that are critical for the user's experience and perceived satisfaction. For each model m, we define, (a) the cost per query $c_m$, associated with the cost of running the model either locally or on the cloud, (b) the inference latency $l_m$, namely the observed inference runtime (can also account for each model's service options, variations in utilized hardware, software optimizations, and requests spending times in queues), and (c) a response accuracy metric $a_m$, defined as the average correctness of the LLM's answer based either on human feedback or on a mixture of scoring functions. Given that, he reward function $R_m$ is defined as follows:\n$R_m (a_m, c_m, l_m) = \\frac{w_a A_m - w_c \\cdot C_m}{(\\log_{10}(l_m))^{t_{scaling}}} $  (1)\nwhere $w_a$, $w_c$, and $w_l$ are the weights assigned to accuracy, cost, and latency, and $t_{scaling}$ is a constant used to scale latency depending on the unit of time used. Evidently, the reward reflects the \"competitiveness\" of each LLM n as formed by response performance and user preferences. Note, that the proposed reward function is easily extensible to accommodate other optimization targets e.g., bias, toxicity,"}, {"title": "3.3 Gradient Ascent Learning", "content": "First, we examine a gradient ascent learning approach where PickLLM acts as a stochastic learning automaton (SLA) the learns the environment by performing updates of the perceived reward [18]. PickLLM maintains an action probability vector $P[i] = [P^{[i]}_1, ...,p^{[i]}_n, ..., P^{[i]}_{|M|}]$, where $P^{[i]}_m$ represents the probability of PickLLM router selecting LLM m for handling the upcoming query at iteration i. The adjustment of action probabilities follows the Linear Reward-Inaction (LRI) algorithm: when the current selection strategy is o = m, the probability of continuing using the same LLM m is updated as:\n$P^{[i]}_m = P^{[i-1]}_m + \\beta R^{[i-1]}_m \\cdot (1 - P^{[i-1]}_m)$\nwhile the probabilities of each one of the rest LLMs m' is:\n$p^{[i]}_{m'} = p^{[i-1]}_{m'} - \\beta \\cdot R^{[i-1]}_m \\cdot p^{[i-1]}_{m'}, \\forall m' \\in M, m' \\neq m$ (3)\nwhere \u03b2 \u2208 (0, 1] denotes the learning rate, and controls the convergence of the process. PickLLM naturally converges to a single LLM for the session of queries when the action probabilities vector delta is smaller a predefined threshold e: i.e., max |AP[i]| < \u0454 for all strategies/choices m, where e is a small positive value indicating the threshold for negligible updates, ensuring that the decision-making process has stabilized. Fig. 1 presents an overview of the proposed framework."}, {"title": "3.4 Stateless e-greedy Q-Learning", "content": "In addition, we study stateless e-greedy Q-Learning approach as an alternative mechanism for PickLLM [6, 23, 28]. Under this scenario PickLLM maintains an action-value vector"}, {"title": "4 Performance Evaluation", "content": "The experimental testbed consisted of a local host and remote execution for the LLMs utilizing an internal in-house LLM API Service. The local client running PickLLM was equipped with an Intel i9 with eight cores at 2.4 GHz each, and 32 GB of RAM.\nWe used four models to construct our set |M|, and specifically Mistral-7B3 [8], WizardLM-70B4 [29], and two versions of Llama-2 [25], one with 13 Billion parameters, and one with 70 Billion. For our experiments' dataset we utilize the HC3 dataset [5] that consists of a mix of questions of varying topics and their corresponding human answers. HC3 is a collection of different individual datasets, with domains that include open-questions [4, 30], finance [17], and medicine [7]."}, {"title": "4.1 Experimental Setup", "content": "The experimental testbed consisted of a local host and remote execution for the LLMs utilizing an internal in-house LLM API Service. The local client running PickLLM was equipped with an Intel i9 with eight cores at 2.4 GHz each, and 32 GB of RAM.\nWe used four models to construct our set |M|, and specifically Mistral-7B3 [8], WizardLM-70B4 [29], and two versions of Llama-2 [25], one with 13 Billion parameters, and one with 70 Billion. For our experiments' dataset we utilize the HC3 dataset [5] that consists of a mix of questions of varying topics and their corresponding human answers. HC3 is a collection of different individual datasets, with domains that include open-questions [4, 30], finance [17], and medicine [7]."}, {"title": "4.2 Evaluation and Discussion", "content": "First, we consider how PickLLM learning rates impact the action probability convergence speed and quality of learning. Our experiment utilizes the ELI5 [4] portion of the test dataset, fixes the reward weights to wa = 0.5, wc = 0.25, w\u2081 = 0.25, and averages 10-runs for different values of the SLA learning rate \u03b2. Fig. 2 shows the average convergence time in terms of querying rounds for increasing values of \u03b2. For larger values of \u03b2 the convergence time is lower with a reduction of \u2248 82% when moving from 0.3 to 0.9. However, for small \u03b2 values the system usually concludes with better choices. In this context, Fig. 2 also shows (a) the average normalized reward gathered throughout single runs and (b) the average normalized LLM-Blender score yielded from 200 queries after the convergence of PickLLM to the"}, {"title": "$Q^{[i]} = [Q^{[i]}_1,...,Q^{[i]}_m, ...Q^{[i]}_{|M|}]$", "content": "$Q^{[i]} = [Q^{[i]}_1,...,Q^{[i]}_m, ...Q^{[i]}_{|M|}]$ approximates the utility of selecting LLM m up to iteration i, symbolizing the expected reward $R^{[i]}_m$ given the selection of LLM m, namely strategy o = m: $Q^{[i]} \\approx E[R^{[i]}_m]|o = m[i]$. The adaptation of action values follows the Q-Learning update rule:\n$Q^{[i]}_m = Q^{[i-1]}_m + \\theta \\cdot (R^{[i-1]}_m - Q^{[i-1]}_m)$ (4)\nwhere \u03b8\u2208 (0, 1] is the learning rate, influencing the magnitude of updates. For selecting LLMs, we employ an e-greedy approach, where PickLLM chooses the strategy o that maximizes the expected reward as follows:\n$\\sigma^{[i]} = {\\arg \\max_{m \\in M} Q^{[i-1]}_m, \\quad \\text{with probability } 1 \u2013 \\epsilon \\\\ \\text{random LLM } m \\text{ from } M, \\quad \\text{with probability } \\epsilon $,\nallowing PickLLM to explore non-maximal strategies with a probability e, thereby balancing exploration and exploitation."}, {"title": "5 Future Work", "content": "PickLLM aims for a simple way to guide agnostic users to an LLM choice that fits their data, and operational requirements. In the next steps, we consider the move from a model-free RL mechanism to a model-based RL approach that constructs a model of the environment to predict future states and rewards. For instance, model-based contextual multi-armed bandits, and statistical learners, such as neural networks, can be used to learn and generalize value functions to predict total returns (e.g., outcome from LLM choice) given a state and action pair. This will allow us to consider user context/conversation history towards steering users to models that are best suited for their type/style of questions. In addition, the reward function could be easily extended to include any factors of importance for the user e.g., GPU memory, environmental impact, licensing or other LLM usage constrains."}, {"title": "6 Conclusion", "content": "In this work, we underline the challenge of VCF customers to select the best-performing LLMs for their application given certain optimization contexts and resource constraints they may face. To tackle this issue, we propose PickLLM, a reinforcement-learning-based mechanism that examines the performance of the available LLMs while guiding queries to eventually the optimal model in terms of cost per query, overall latency, and response quality. Our approach is easily extendable to cover additional contextual optimization goals, while also being extremely computing-resource efficient in comparison to alternatives. Results indicate the adaptability of the proposed framework towards optimizing user querying sessions in terms of hard metrics with improvements of up to \u2248 50-60% for overall session cost and average latency."}, {"title": "A Prompt templates", "content": "We list the prompt template used for ChatGPT 3.5 Turbo when acting as an LLM judge [33]:\n[System]\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below.\nWe provide also a human generated response to use as guidance for your scoring. Rate the response on a scale of 0 to 1 with three decimal accuracy by strictly returning just one number, for example: \"0.345\".\n[User Question]\nquestion\n[AI Response]\nai_response\n[Human Response:]\nhuman_response"}]}