{"title": "LICM: Effective and Efficient Long Interest Chain Modeling for News Recommendation", "authors": ["Zhen Yang", "Wenhui Wang", "Tao Qi", "Peng Zhang", "Tianyun Zhang", "Ru Zhang", "Jianyi Liu", "Yongfeng Huang"], "abstract": "Accurately recommending personalized candidate news articles to users has always been the core challenge of news recommendation system. News recommendations often require modeling of user interests to match candidate news. Recent efforts have primarily focused on extract local subgraph information, the lack of a comprehensive global news graph extraction has hindered the ability to utilize global news information collaboratively among similar users. To overcome these limitations, we propose an effective and efficient Long Interest Chain Modeling for News Recommendation(LICM), which combines neighbor interest with long-chain interest distilled from a global news click graph based on the collaborative of similar users to enhance news recommendation. For a global news graph based on the click history of all users, long chain interest generated from it can better utilize the high-dimensional information within it, enhancing the effectiveness of collaborative recommendations. We therefore design a comprehensive selection mechanism and interest encoder to obtain long-chain interest from the global graph. Finally, we use a gated network to integrate long-chain information with neighbor information to achieve the final user representation. Experiment results on real-world datasets validate the effectiveness and efficiency of our model to improve the performance of news recommendation.", "sections": [{"title": "1 Introduction", "content": "News recommendation is crucial technique to provide people with the news which satisfies their personalized reading interests (Das et al., 2007). Due to the vast amount of news generated each day, It's essential to model user interests for better personalized interests (Wang et al., 2018)\nIn news recommendation, local modeling typically focuses on extracting semantic information from the news clicked by users to model user interest representations. This is achieved through deep learning techniques such as attention mechanisms, CNN networks, and LSTM networks, using local user information and news click sequences for personalized recommendations (Wu et al., 2019b).\nMethods based on local user interest representations have reached a bottleneck in personalized recommendation. Therefore, graph neural network-based recommendations have emerged, enhancing news data by incorporating external knowledge graphs to capture implicit interests. For example, KIM (Qi et al., 2021a) utilizing knowledge graphs to improve the knowledge-aware representations of a user's clicked news and the candidate news.\nNonetheless, these methods struggle to make collaborative recommendations through the interests of similar users, thus failing to capture some common interest characteristics among global users. Recent work on graph neural networks has concentrated on constructing local subgraph based on all users click sequence global graph such as figure 1, GLORY (Yang et al., 2023) build an overall click global graph using the click information from all users, and obtain a global view of news click to enhance the recommendation. However, the common strategy for global graphs is to construct local graphs for individual users and distill information through various graph neural networks (Velickovic et al., 2017; Chen et al., 2020b; Li et al., 2015) which constrains modeling the short-range interactions among clicked news. Thus, these methods have difficulty capturing high-dimensional collaborative information on a global interest graph, thereby overlooking potential underlying interest connections.\nWe have investigated the barriers to the full utilization of the global graph. Firstly, the process of distilling information from the complete graph, based on users' click history, often results in a graph that is excessively large, typically incurring significant efficiency costs in terms of memory and computational speed. Secondly, graph neural networks (Velickovic et al., 2017; Chen et al., 2020b; Li et al., 2015), which utilize subgraphs based on short-range hop neighbors, exhibit limited expressive capabilities and struggle to capture long-chain information. This limitation makes it challenging to effectively distill information from the entire graph based on the current user's activities.\nTo address the above issues, in this work, we propose a long interest chain modeling framework for news recommendation (LICM). This framework combines global and local perspectives, enhancing the personalization and temporal modeling accuracy of news recommendations by incorporating long-chain interest constructed with the collaborative of other users with similar interests, revealing latent common interests within the global news click graph. Our main contributions are:\n\u2022 Designing a long-chain selection mechanism based on popularity, category and semantics to pick long-chain, which facilitates better classification and distillation of long-chain features to capture latent connection among collaborative users. As far as we know, this is the first paper to use this approach in the global news graph;\n\u2022 We adopt long-chain attention network to capture the long-chain information at the news level and user level, subsequently utilize a gated network to combine long-chain and neighbor information.\n\u2022 Extensive experiments on real world datasets demonstrate that LICM significantly improves news recommendation effectiveness, outperforming other baseline methods."}, {"title": "2 Related Work", "content": "A common approach is to study local news semantic representation through user's reading history with some deep learning technique. (Okura et al., 2017a; An et al., 2019a; Qi et al., 2021b) several approaches have been refined to better model user interests based on their news interaction histories. EBNR (Okura et al., 2017b) and RA-DSSM (Kumar et al., 2017) utilize GRU and Bi-LSTM networks respectively to capture user behaviors from news click sequences, focusing on immediate browsing contexts. LSTUR (An et al., 2019b) combines long-term interests represented by a user's ID with short interests derived from recent activities, while DAN (Zhu et al., 2019) enhances user profile accuracy through attention-enhanced LSTM structures. When the effectiveness of local semantic representation reaches its limitations, improving personalized recommendation systems can be accomplished by creating an extensive news graph that leverages the rich, multidimensional data within it, User-as-graph (Wu et al., 2021) and DIGAT (Mao et al., 2022) employ graph-based approaches to capture higher-dimensional information by leveraging user behavior patterns and dual graph interactions. KIM (Qi et al., 2021a) uses a co-encoding method with a knowledge graph to understand the interactions between clicked and candidate news, enhancing the personalization of recommendations. Hi-eRec (Qi et al., 2021c) advances this approach by modeling user interests from subtopics to a comprehensive synthesis within a hierarchical interest framework. This method reflects a trend towards a more nuanced and layered understanding of user preferences in news recommendation systems.\nBased on the techniques that use user-news graphs for news recommendation, besides, graph modeling is proved effectiveness for recommendation systems (Chen et al., 2020a). Ge et al. (2020) and Hu et al. (2020b) formulate news and users jointly in a bipartite graph to model news-user interaction. Some methods also incorporate a global news graph constructed from user click history to enrich collaborative information. GNewsRec (Hu et al., 2020a) uses a hybrid approach that combines user and news graph data with news topics, utilizing DAN's architecture for text-based news representation and a two-layer GNN for graph-based insights from a heterogeneous user-news-topic graph, enhancing news modeling by exploiting higher-order graph information. GLORY (Yang et al., 2023) in"}, {"title": "3 Approach", "content": "Problem Formulation. The click history sequence of a user u can be denoted as $H_u = [d_1, d_2, ..., d_H]$, where H is the number of historical news articles. Each news article d\u2081 has a title, which contains a text sequence $T_i = [W_1, W_2, ..., W_t]$ consisting of T word tokens, and an entity sequence, which is denoted by $E_i = [e_1, e_2, ..., e_E]$ consisting of E entities. The objective is to predict the level of interest $s_{u,c}$ for a given candidate news article $d_c$ and user u, which reflects the likelihood of a clicking action occurring between them. The task of recommending involves ordering various potential news articles by their respective probability scores."}, {"title": "3.1 Category-enhanced news Representation", "content": "To model user behaviors, we obtain local news and entity representations from the news text and associated entities. We limit our analysis to the last $L_{his}$ clicked news in user u's history $H_u$ and the candidate news $d_c$. Only the first $L_{title}$ words of each news title are considered for input.\nNews Encoder. We utilize a local news encoder, initiating with a word embedding layer using pre-trained GloVe (Pennington et al., 2014) embeddings to convert news text into embedding vectors $x_n$. These are processed by a multi-head self-attention (MSA) layer to produce word representations $X_n$:\n$X_n = MSA(x_n)$ (1)\nWe enhance long-chain selection by incorporating title semantics to select news with similar content and by distilling category information to improve click sequence detection within the same categories.\nThus, by integrating category, subcategory, and title semantic information through a straightforward linear layer, the model achieves greater computational speed and efficiency. The title representation is combined with news category and subcategory using concatenation and trainable parameters $W_{mix}$ and $b_{mix}$:\n$C_n = (X_n \\oplus C_n \\oplus SC_n) \\cdot W_{mix} + b_{mix}$ (2)\nLocal News encoder. A text attention layer aggregates word representations to form local news representation $h_n^{ln}$, using the following attention mechanism:\n$h^{ln} = \\sum_{i=1}^{T} \\alpha_{i}^{n} C_{i}^{n}$ (3)\n$\\alpha_{i}^{n} = \\frac{exp(q_{\\epsilon}^{T} tanh(W_{w} \\cdot C_{i}^{n}))}{\\sum_{i=1}^{T} exp(q_{\\epsilon}^{T} tanh(W_{w} \\cdot C_{i}^{n}))}$ (4)\nwhere $q_{\\epsilon}$ is a query vector.\nFor entity representation, we use an entity embedding layer with pre-trained TransE (Bordes et al., 2013) embeddings from WikiData. This is followed by an entity self-attention network to form $X_e$, and an attention network aggregates these into $h_e^{le}$"}, {"title": "3.2 Long-Chain Encoder", "content": "The Global Click Graph represents the relationships between news articles, constructed from the temporal sequence of click histories of all users. Since each user possesses their own reading history, the Global News Graph and Entity Graph are subsequently constructed to facilitate recommendations.\nAlternative Long-chain Picking. For user i within the range from $u_1$ to $u_n$, a sequence {$n_A,n_B, n_C,...$} is formed in click timestamp sequence. For any two adjacent news items in the sequence, a directed pair (na,nB) can be represented within a global graph. For each directed"}, {"title": "3.2.1 Long-Chain Selection mechanism", "content": "pair ($n_A$,$n_B$), when the click sequence of user i {$N_{A,i}$, $N_{B,i}$, $N_{C,i}$, . . .} contains ($n_A$, $n_B$), the click frequency of user i can be defined as $f_i(n_A \\rightarrow n_B)$ = 1; otherwise, $f_i(n_A \\rightarrow n_B)$ = 0.\nOur third long-chain selecting strategy utilizes the temporal popularity of news clicks, making news directed pairs with more clicks more likely to be captured. Therefore, to accurately utilize the collaborative relationships among different users to match user interests, for the global graph, different users may have a non-zero click frequency for the same directed pair. Thus, a click weight for a directed pair can be defined as follows:\n$w(n_A \\rightarrow n_B) = \\sum_{i=u_1}^{U_N} f_i(n_A \\rightarrow n_B)$\nWe apply a similarity-based method to find the most relevant neighbors for each news item in a global news graph G. For each neighbor $v \\in N$, where N is the set of click neighbors, we select the top n articles from Sv, the set of news associated with v. We use a weight function w : $S_v$ \u2192 R+ to assign click weights to these articles. The selection process is represented by:\n$Top_n(S_v) = {s \\in S_v | s are the top n}$ (5)\nAlternative Long-chain Pruning. After selecting the articles with the highest weights from $S_v$ as Topn (Sv), we compare their similarity to the current news C. We calculate sim(C, s) for each s in Topn (Sv) and select the most similar article s*:\n$s^* = arg \\underset{s \\in Top_n(S_v)}{max} sim(C, s)$ (6)\nThis iterative selection process creates a detailed long-chain for each clicked news item, which is refined based on semantics, category, and popularity to enhance the computation of similarities.\nLoop-chain. The chain selection inherently includes recursive loops, such as nA = nB, resulting in sequences like nA \u2192 nB \u2192 nA \u2192 nB. These loops are critical for identifying key nodes in complex networks. While avoiding loops could reduce redundancy, it may also limit the use of vital temporal information, potentially diminishing model performance. Therefore, we intentionally allow such cycles."}, {"title": "3.2.2 Long-Chain Interest Encoder", "content": "In order to distill long chain information accurately, effectively and quickly, we use the long-chain attention network to model long-chain information. At the news level, we model the long-chain representations of user's some clicked news. At the user level, we continue to use the attention network to learn the vector representation of the long chain representation of all click news. The formula is as follows:\n$1_i = \\sum_{i=1}^{T} \\beta C_i^{c} \\rightarrow$ (7)\n$\\beta = \\frac{exp(q_{\\epsilon}^{T} tanh(W_{w} \\cdot C_{i}^{1}))}{\\sum_{i=1}^{T} exp(q_{\\epsilon}^{T} tanh(W_{w} \\cdot C_{i}^{1})))}$ (8)\n$L = \\sum_{i=1}^{T} \\nu^{t} l_i \\rightarrow$ (9)\n$\\nu = \\frac{exp(q_{\\epsilon}^{T} tanh(W_{w} \\cdot L_{i}^{1}))}{\\sum_{i=1}^{T} exp(q_{\\epsilon}^{T} tanh(W_{w} \\cdot L_{i}^{1}))}$ (10)"}, {"title": "3.3 Neighbor Interest Encoder", "content": "For each user history, we construct a subgraph $G_{ub} = (V_{sub}, E_{ub})$ from the global news graph"}, {"title": "3.4 Candidate Neighbor entity Encoder", "content": "Inspired by (Yang et al., 2023), this section details an encoder that leverages a global entity graph for candidate news representation. The graph $G_e = (V_e, E_e)$ is constructed from entity occurrences in news, with entities and edges denoted by Ve and Ee, respectively.\nEntity encoding begins with selecting the top Me neighbors for each entity in candidate news $d_c$, based on edge weights. The local-view entity encoder's embedding layer is then used to derive xe, which is refined by a multi-scale attention mechanism MSA(xc) to yield the global entity representation $h_e^{ge}$. The candidate news representation embcand is synthesized by pooling the local news $h_n^{ln}$, local entity $h_e^{le}$, and global entity $h_e^{ge}$ representations through an attention network."}, {"title": "3.5 Click Prediction and Model Training", "content": "Following prior work (Wu et al., 2019a), we use negative sampling in training. In each session of the training set, clicked news are positive samples nt, and $K_{neg}$ non-clicked news are [$n_1^{neg}, n_2^{neg}, ..., n_k^{neg}, ..., n_k^{neg}$]. We calculate the click score \u0177i for each news item by taking the inner product of the user embedding $emb_{user}$ and the candidate embeddings $emb_{cand}$, which includes one positive and $K_{neg}$ negative samples. We then optimize the log-likelihood loss $L_{NCE}$ for positive samples during training:\n$\\hat{y_i} = softmax(emb_{user} \\cdot emb_{cand})$ (15)\n$L = -log(\\sum_{i=1}^{D} \\frac{e^{\\hat{y}_{t}}}{\\sum_{j=1}^{K} e^{{\\hat{y}_{t}}}})$ (16)"}, {"title": "4 Experiment Setup", "content": "Based on (Wu et al., 2019a), we define experimental parameters for user interaction and news representation. Each user's history is capped at Lhis = 50 most recent news articles, with each title limited to Ltitle = 30 words. We utilize up to Lentity = 5 entities per news article. The news neighbor subgraph considers 10 neighboring news and 2 hops, whereas adjacency entities involve 10 neighboring entities. We employ a 2-layer GGNN (Li et al., 2015) for graph neighbor node processing. We initialize with 300-dimensional GloVe and 100-dimensional TransE embeddings from the MIND dataset, configuring news representations at 400 dimensions. Each news long chain involves selecting one of top 3 neighbors in a loop, extending up to a length of 8 hop within one clicked news loop. Adam (Kingma and Ba, 2015) optimizes our model"}, {"title": "5.1 Main Comparison Results", "content": "Table 2 shows the performance comparison results. Our model, LICM, consistently delivers the highest performance across all evaluation metrics. Generally, the recommended performance of sequence-based modeling is much lower than that of most graph neural networks. This is because the graph neural network contains a lot of high-dimensional information, which can collect more higher-order representation than the ordinary local temporal information model.\nAmong all measures evaluated, our model LICM outperformed all comparison methods. This performance improvement comes from the fact that our model not only utilizes the neighbor information in the global graph, but also uses the user long-chain interest in global news click graph to build"}, {"title": "5.2 Ablation Analysis", "content": "To verify the validity of our module components, we conducted three sets of experiments to verify the validity of our long-chain selecting mechanism and fusion mechanism, and we conducted four experiments: (1)only Neighbor: No long chain distillation and only use neighbor news/entity graph (2)w/o Selection: No selection mechanism and select only by weights (3)w/o Fusion: no gated network fusion (4)Full model. All other Settings remain the same as in the full model, with only one component being replaced or removed. We evaluated these experiments on a MIND-small scale. The experimental results are shown in Figure 3."}, {"title": "5.3 Analysis on Hyperparameters", "content": "In this study, we explored critical hyperparameters, particularly the number of hop in the long chain, to assess their impact on model performance. Experimental results from the MIND-small dataset, as illustrated in Figure 4, indicate that the model attains optimal recommendation performance when the number of hops is configured to eight. This finding suggests that a lower number of hops may be inadequate for effectively distilling essential long-chain information, while a higher number may introduce excessive noise, thereby degrading the performance of the model."}, {"title": "5.4 Impact of Graph Encoder", "content": "In this study, we explored the performance of various graph encoders. By comparing our model with three popular graph models\u2014Gate Graph Neural Networks (Li et al., 2015), Graph Convolutional Networks (Chen et al., 2020b), and Graph Attention Networks (Velickovic et al., 2017)\u2014over subgraphs ranging from 1-hop to 3-hop as shown in Table 4, we observed significant increases in time and space costs with increasing hops. Firstly, the capture of"}, {"title": "5.5 Case Study", "content": "In our case study, we use a global click news graph structure to evaluate LICM and compare it with GLORY, which is currently the best on the global news graph, to demonstrate the advantages of our news modeling approach. From the MIND-small dataset, we observed significant differences in recommendation results: users actually clicked on football-related articles, with LICM recommending a football-related news article first, whereas GLORY initially recommended a financial news article because it could only model within a limited hop range, thus failing to effectively suggest football news.\nSpecifically, LICM uses a model that combines weights and similarities to distill long-chain sequences related to the user's reading interests. Carefully selected reading content includes NFL controversies, upcoming matches, and player injuries, showing a strong user interest in football. In contrast, GLORY did not leverage long-chain user interests in the global graph, limiting its ability to capture a broader range of user interests, especially since other news reading interests in the global graph were at the fifth hop(traditional GNN is hard to capture), posing a challenge for traditional graph neural networks. This detailed interest analysis enables LICM to closely align its recommendation (a controversial football topic) with the user's preferences, demonstrating its exceptional user interest modeling capability in real-world scenarios."}, {"title": "6 Conclusion", "content": "This paper presents LICM, a novel news recommendation system that utilizes a global news graph to capture users' latent long-chain interests. We first design alternative long-chain picking with news click popularity, and then prune the alternative long-chain based on enhanced news semantics and category representation. The model incorporates a neighbor and long-chain interest encoder, distilling long-chain and neighbor interests. Tests on two datasets demonstrate that LICM surpasses existing models, validating the effectiveness and efficiency of its components."}, {"title": "7 Limitations", "content": "In this section, we explore the limitations of our methodology in depth. Our approach is limited by"}]}