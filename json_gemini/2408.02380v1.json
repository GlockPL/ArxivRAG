{"title": "Perfect Information Monte Carlo with Postponing Reasoning", "authors": ["J\u00e9r\u00f4me Arjonilla", "Abdallah Saffidine", "Tristan Cazenave"], "abstract": "Imperfect information games, such as Bridge and Skat, present challenges due to state-space explosion and hidden information, posing formidable obstacles for search algorithms. Determinization-based algorithms offer a resolution by sampling hidden information and solving the game in a perfect information setting, facilitating rapid and effective action estimation. However, transitioning to perfect information introduces challenges, notably one called strategy fusion. This research introduces 'Extended Perfect Information Monte Carlo' (EPIMC), an online algorithm inspired by the state-of-the-art determinization-based approach Perfect Information Monte Carlo (PIMC). EPIMC enhances the capabilities of PIMC by postponing the perfect information resolution, reducing alleviating issues related to strategy fusion. However, the decision to postpone the leaf evaluator introduces novel considerations, such as the interplay between prior levels of reasoning and the newly deferred resolution. In our empirical analysis, we investigate the performance of EPIMC across a range of games, with a particular focus on those characterized by varying degrees of strategy fusion. Our results demonstrate notable performance enhancements, particularly in games where strategy fusion significantly impacts gameplay. Furthermore, our research contributes to the theoretical foundation of determinization-based algorithms addressing challenges associated with strategy fusion.", "sections": [{"title": "I. INTRODUCTION", "content": "Search algorithms in artificial intelligence have significantly evolved, demonstrating superhuman performance in games such as Chess, Go [1], Poker [2], Skat [3], and Contract Bridge [4]. Perfect information games, like Chess and Go, where all information is available, have been extensively studied, allowing algorithms to surpass human professionals [1], [5], [6]. In contrast, imperfect information games, including Poker, Skat, and Bridge, where some information is hidden, have been less studied, with only a few algorithms capable of outperforming professional human players [2].\nIn imperfect information games, two commonly used search methods are regret-based approaches, which excel in Poker and are theoretically convergent but slower [7]\u2013[9], and determinization-based methods, considered state-of-the-art in various trick-taking card games, offering scalability but lacking theoretical guarantees [3], [10]\u2013[12]. In recent years, both methods have incorporated neural networks to enhance performance and facilitate scalability on large games [2], [13]\u2013[16].\nDeterminization-based algorithms, like PIMC, operate by sampling hidden information based on current knowledge and using a perfect information leaf evaluator to predict game outcomes under perfect information assumptions. These algorithms achieve state-of-the-art performance because solving problems with perfect information is inherently simpler than dealing with imperfect information. Despite their state-of-the-art performance, determinization-based algorithms face challenges, notably encountering 'strategy fusion' [10], [17]. This challenge arises from the use of the perfect information leaf evaluator, which independently solves each possible world without considering the uncertainties induced by games with imperfect information.\nWithin determinization-based algorithms, Perfect Information Monte Carlo (PIMC) [10] is particularly susceptible to strategy fusion due to its early usage on the perfect information leaf evaluator in decision-making. Our study addresses this challenge by postponing the leaf evaluator until a depth d, which mitigates the impact of strategy fusion. The act of postponing the perfect information leaf evaluator at a depth of d introduces new considerations, specifically, it prompts the need for alternative strategies to reason from step 1 to step d. We formally define the problem of strategy fusion, demonstrating that, in the worst case, increasing depth d does not exacerbate strategy fusion, and in every case, there exists a depth d that strictly reduces it. For finite games, there is a depth d that eliminates strategy fusion entirely.\nSection II covers notation and a detailed explanation of determinization-based algorithms, specifically addressing PIMC and the strategy fusion challenge. Section III introduces Extended PIMC, incorporating our idea of postponing the leaf evaluator at depth d, which operates online without initial costs, making it suitable for diverse games or General Game Playing [18]. It is noteworthy that, like other modern determinization algorithms, there exists the potential to integrate neural networks for enhanced performance. Section IV presents the theoretical results of increasing depth d in determinization-based algorithms. Section V showcases experimental findings across various games, highlighting cases with significant strategy fusion where deeper reasoning improves performance beyond other state-of-the-art methods. Section VI reviews related research, and Section VII summarizes our contributions and future research directions."}, {"title": "II. NOTATION AND BACKGROUND", "content": "Throughout the paper, we use the formalism of Factored-Observation Stochastic Games (FOSG) [19] and utilize Figure 1, a variant of 'Rock-Paper-Scissors' to present the notations. Our notation employs subscripts to denote players.\nA game, denoted as G, involves N players, initiating at $w_{init}$ and progressing through successor world states represented by $w \\in W$. Players make joint actions, denoted as $a = (a_1,...,a_N) \\in A(w)$, in each world state w, and the game continues until a terminal state is reached. Upon choosing a joint action a, players observe rewards, and the next world state $w'$ is determined probabilistically. During this transition from w to w', each player receives an observation denoted as $o_i \\in O_i(w,a,w')$, where $O_i(w,a,w')$ represents the set of possible observations for player i.\nIn Figure 1, the first player faces the choice between 'Leave' with a reward of -0.6 or engaging in the game. If it opts to play, the standard rules apply: Rock beats Scissors, Scissors beats Paper, and Paper beats Rock, with wins yielding 1, losses resulting in -1, and ties providing 0. The game encompasses four possible world states denoted as $W = {w^a, w^b, w^c, w^d}$, where $w^a$ is the initial state and $w^b$, $w^c$, and $w^d$ follow the first action.\nThe first player has four possible actions in $w^a$ and a null action in other states\u2014specifically, $A_1(w^a) = \\{Leave, Rock, Paper, Scissors\\}$ and $A_1(w^b) = A_1(w^c) = A_1(w^d) = noop$. The second player has a null action in $w^a$ and three actions in other states\u2014namely, $A_2(w^a) = \\emptyset$ and $A_2(w^b) = A_2(w^c) = A_2(w^d) = \\{Rock, Paper, Scissors\\}$. In this game, the second player receives the observation Play if the first player plays Paper, Scissors, or Rock. Rewards for both players are obtained at the game's conclusion."}, {"title": "A. Notation", "content": "A history is a finite sequence of world states and legal actions denoted as $h_t = (w^0, a^0, w^1, a^1,...,w^t)$. An infostate $s_i$ is a sequence of an agent's observations and actions, denoted as $s_i = (a_i^0, o_i^0,...,a_i^{t-1}, o_i^{t-1})$. For each history, there exists a unique infostate noted as $s_i(h)$, and for each infostate, there is a set of histories that match the sequence denoted as $H(s_i)$. $I(G)$ represents the set of possible infostates in the game G, and $\\Pi(G)$ is the set of possible policies in the game G, where a policy is a function mapping every history h to a probability distribution over actions.\nIn Figure 1, if we examine the history $h^1 = (w^0, Rock, w^b)$, considering it from the second player's perspective, the infostate becomes $s_2 = (noop, Play)$ since no action was taken by the second player, and the observation Play was noted. The set of histories that correspond to $s_2^h$ includes $\\{(w^a, Rock, w^b); (w^0, Paper, w^c); (w^a, Scissors, w^d)\\}$."}, {"title": "B. Determinization-based algorithm", "content": "Each determinization-based algorithm has its own characteristics, nevertheless, they share some common features such as (i) samples a history h according to a probability distribution over the current infostate s; (ii) uses a perfect information leaf evaluator for estimating the value of the sampled world state. In the description of the Algorithm 1 and 2 (i) is noted Sampling(s) and (ii) is noted PerfectAlgo(h).\nA perfect information leaf evaluator is an algorithm used in games with perfect information to estimate the value of a history h. It predicts the outcome of a game from a specific position. This evaluator can be exhaustive methods like Minimax with Alpha-Beta pruning [20], heuristics methods like Random Rollout (also called playouts [21]) or neural network [1], [5], [6].\nDeterminization-based algorithms are simple and, in practice, achieve great results. Yet, certain problems are encountered such as (i) non-locality and strategy-fusion [10], [17]; (ii) revealing private hidden information [17], [22]; (iii) no theoretical guarantees.\nIn Figure 1, when conducting sampling from the infostate $s_h$, the available options include sampling $w^b$, $w^c$, or $w^d$. Utilizing a perfect information leaf evaluator, such as Minimax, on the world state $w^b$ would yield a value of -1, as the second player optimally plays Paper to maximize their score i.e., minimizes the value of the first player."}, {"title": "C. Strategy fusion", "content": "In imperfect information games, histories stemming from the same infostate must be approached with the same strategy, as players cannot distinguish between them. Formally, $\\forall s \\in I(G), \\forall h, h' \\in H(s), \\pi(\\cdot|h) = \\pi(\\cdot|h').$"}, {"title": "III. EXTENDED PIMC", "content": "However, determinization-based algorithms deviate from this principle. They employ a perfect information leaf evaluator algorithm to estimate the value at each sampled history. In other words, each history originating from the same infostate is solved using a strategy tailored to that specific history. Formally, $\\forall s \\in I(G), \\forall h, h' \\in H(s), \\pi(\\cdot|h) \\neq \\pi(\\cdot|h').$\nIn Figure 1, the strategies in $w^b$, $w^c$, and $w^d$ must be identical since they originate from the same infostate $s_h$. For the second player, the optimal strategy results in an average score of 0 (playing with the same probability for all three actions). By back-propagating, the first player opts for Play, leading to an average score of 0, instead of choosing Leave, which results in -0.6.\nFrom the perspective of a determinization-based algorithm, a policy is tailored to the sampled history. The best policy for the second player is Paper in $w^b$, Scissors in $w^c$, and Rock in $w^d$. In $w^b$, $w^c$, and $w^d$, playing the best policy yields -1. By back-propagating, the first player concludes that opting for Leave to obtain -0.6 is preferable compared to playing and receiving -1.\nPerfect Information Monte Carlo (PIMC) is a determinization-based algorithm that is the state-of-the-art of many imperfect information games.\nAs presented, employing the perfect information leaf evaluator results in strategy fusion. In the case of PIMC, this evaluator is utilized after the first action is played. However, there are no inherent constraints preventing the resolution after the first action, and it is not difficult to believe that postponing the use of the perfect information leaf evaluator could mitigate the issue of strategy fusion.\nIn the following, we introduce a novel algorithm that embraces this straightforward concept of postponing the leaf evaluator's utilization. The algorithm, termed 'EPIMC' (Extended Perfect Information Monte Carlo), extends the PIMC paradigm by incorporating Extended reasoning at a depth of d, where the instance of d = 1 corresponds to PIMC.\nIn the original PIMC, world state evaluations occur a maximum of A times during each sampling phase, with A denoting the highest feasible action count. However, applying the same approach at depth d could potentially lead to evaluating $|A|^d$ world states. Therefore, careful consideration of the number of actions per step becomes imperative. In EPIMC, our chosen strategy is to explore one action per sampling iteration, resulting in a singular world state evaluation per sampling instance, however, other choices could have been envisaged."}, {"title": "D. Perfect Information Monte Carlo", "content": "While theoretically, any depth d could be employed, practical considerations necessitate a prudent choice of d. Selecting a depth d that is too small can exacerbate strategy fusion issues, as observed in approaches like PIMC. On the other hand, opting for a depth d that is too large demands significant sampling efforts to accurately estimate the subgame U. Moreover, depending on the algorithm employed to solve the subgame U, a larger depth can lead to increased computational costs.\nBy postponing the application of the perfect information leaf evaluator until a depth of d, alternative reasoning methods must be employed for steps 1 through d. In EPIMC, the subgame U of size d is built to approximate the real game by encapsulating various elements, including infostates, world states, post-action dynamics, and more. In particular, the leaves of the subgame U are average scores obtained from the leaf evaluator.\nAfter the budget is finished, the subgame U is solved using an algorithm that does not create strategy fusion. In other words, one needs an algorithm that works on infostates instead of world states. This choice of algorithm, although potentially resource-intensive, carries a reduced computational burden when applied to a subgame U of size d. One can think of using information set search [23], [24] which operates on infostates according to a minimax rule or CFR/CFR+ [7], [8] that have theoretical guarantees for two players."}, {"title": "Exploration Strategy", "content": "In the following, we present the theoretical foundation for determinization-based algorithms that suffer from strategy fusion. We formally (i) define the condition to create strategy fusion; (ii) define the quantity of strategy fusion; (iii) prove that, in the worst case, increasing the depth does not increase the strategy fusion, and in every case, there exists a depth d such that the strategy fusion is strictly reduced and in a finite game, there exists a depth d such as the that the strategy fusion is removed.\nFor any game G, a policy $\\pi \\in \\Pi(G)$ and an infostate $s \\in I(G)$ creates strategy fusion if there are $h,h' \\in H(s)$ such that $\\pi(h) \\neq \\pi(h')$. For any game G, a policy $\\pi \\in \\Pi(G)$ creates strategy fusion if there is an infostate $s \\in I(G)$ such that $ \\pi$ creates strategy fusion in s.\nTo evaluate the quantity of strategy fusion in $\\pi \\in \\Pi(G)$, we propose the following measure $SF(\\pi) = |\\{s \\text{ such that } \\forall s \\in S(G), \\pi \\text{ creates strategy fusion in s }\\}|$. In other, we count the number of infostate that create strategy fusion. $SF(\\pi) = 0$ implies that there is no strategy fusion.\nIn the subsequent discussions, for the sake of simplicity, we presume adherence to the policy provided by EPIMC, denoted as $\\Pi_E^d(G)$ when executing EPIMC at a depth of d. Although alternative choices could have been considered, opting for the EPIMC policy is more straightforward, as influenced by the uniform growth of the EPIMC policy across the entire depth d space."}, {"title": "IV. THEORETICAL FOUNDATION", "content": "$\\forall G, \\forall d \\in [0,T \u2013 1] \\text{ where } T = \\{T \\text{ if G has a finite horizon T; else } \\infty\\}, \\forall \\pi \\in \\Pi_E^0(G)$, then $\\forall \\pi' \\in \\Pi_E^{d+1}(G), SF(\\pi, G) \\geq SF(\\pi', G)$\nFor any s \u2208 I(G), s does not create strategy fusion in $\\pi^{1:d}$. Increasing the depth by 1, extends the non-inducing region. Two possibilities arise: (i) if at least ones at d + 1 create strategy fusion, it can no longer do so, i.e., $SF(\\pi, G) > SF(\\pi', G)$; (ii) if all s at d + 1 do not create strategy fusion, increasing the depth does not reduce SF, i.e., $SF(\\pi, G) \\geq SF(\\pi', G)$. We conclude $SF(\\pi, G) \\geq SF(\\pi', G)$.\n$\\forall G, \\forall d \\in [0,T] \\text{ where } T = \\{T \\text{ if G has a finite horizon T; else } \\infty\\}, \\pi \\in \\Pi_E^d(G)$, if $SF(\\pi, G) > 0$, then $\\exists d' \\in [1,T\u2212d], \\forall \\pi' \\in \\Pi_E^{d+d'}(G)$ such that $SF(\\pi, G) > SF(\\pi', G)$.\nLeveraging the rationale from Proposition 1, increasing the depth by d' extends the non-inducing region. Given the presence of strategy fusion (SF(\u03c0,G) > 0), at least one infostate creates strategy fusion. Extending the reasoning up to this infostate, located d' away from the original depth d, effectively diminishes strategy fusion. Hence, we establish SF(\u03c0,G) > SF(\u03c0', G).\nWe extend Proposition 2 by showing that, in a finite game, there is a depth at which there is no longer strategy fusion.\nProposition 3. G such that G has a finite horizon T,\n\u2200d \u2208 [1,T \u2212 1],\u2200\u03c0 \u2208 \u03a0\u0395\u00baG if SF(\u03c0,G) > 0 then \u2203d' \u2208\n[1,T \u2212 d], \u2200\u03c0' \u2208 \u03a0\u0395d+d' G such that SF(\u03c0', G) = 0.\nProof. Setting d' = T \u2212 d, i.e., until the end of the game,\nensures that no further infostate can induce strategy fusion, as\nthere are no infostates remaining."}, {"title": "Proposition 1.", "content": "Our experiment set involved testing five games: Card Game,\nBattleship, Dark Chess, Phantom Tic-Tac-Toe, and Dark Hex.\nEach of them is considered a large game, is described below\nand is implemented in OpenSpiel [25], a collection of envi-\nronments and algorithms for research in general reinforcement\nlearning and search/planning in games. The benchmarks were\nchosen to show the strengths and weaknesses of the algorithm,\nespecially, by choosing games with public observations (Card\ngame, Battleship) or with private observations (Phantom Tic-\nTac-Toe, Dark Chess, and Dark Hex).\nPublic observations refer to information that is visible to\nall players, while private observations are exclusive to a\nplayer. This distinction significantly influences the dynamics\nof strategy fusion. Private observations amplify the number\nof potential world states within a single infostate, thereby"}, {"title": "Proposition 2.", "content": "For all the experiments, the budget ranged from 0.1 seconds\nto 100 seconds for Card Game, Battleship and Phantom Tic-\nTac-Toe, and from 0.1 seconds to 1000 seconds for Dark Hex\nand Dark Chess. Each experiment was conducted over 500\ngames, in which the games were evenly split between playing\nin the first and second positions. The opponent is PIMC with\na fixed one-second budget.\nAll experiments were executed on a single CPU Intel(R)\nXeon(R) Gold 5218. EPIMC and PIMC use random rollout\nas the perfect information leaf evaluator, depth at 3 and\nInformation Set Search for the subgame resolution. In practice,\nPIMC is often tested with minimax as the perfect information\nleaf evaluator, yet using it may be slow in large benchmarks\nor require using a handmade heuristic. As a reminder, EPIMC\nat depth 1 is PIMC. To reduce EPIMC/PIMC costs, multiple\nCPUs could be utilized, but for fairness across algorithms, this\napproach was not employed.\nOther online algorithms: The other algorithms com-\npared are Information Set MCTS (IS-MCTS) [11], Online\nOutcome Sampling (OOS) [26], Recursive PIMC (IIMC) [3],\nand a random agent (Random). IS-MCTS is a determinization-\nbased algorithm that employs Monte Carlo Tree Search on\ninfostates. OOS is a regret-based algorithm that converges to\nthe Nash equilibrium with increasing search time. IIMC is a\ndeterminization-based algorithm rooted in PIMC, estimating\naction values by recursively calling PIMC until game comple-\ntion.\nFor IS-MCTS, the exploration constant was chosen from\n0.6, 1, 1.5, 2 and set at 1. In OOS, the target was selected\nbetween Information Set and Public Subgame Targeting, and\nit was set at Information Set. Regarding IIMC, the number of\nsamplings at level 2 was chosen from 2, 5, 10 and set at 5."}, {"title": "V. RESULTS", "content": "increasing the likelihood of strategy fusion. As our method reduces strategy fusion, we expect EPIMC to be more effective in games where observations are private.\na) Card game: The game is played with two players, 22 cards are taken from a pack of 52, and known by all, 6 are hidden and 8 is given to each player. The playing phase is decomposed into tricks, the player starting the trick is the one who won the previous trick. The starting player of a trick can play any card in his hand, but the other player must follow the suit of the first player. If he can not, he can play any card he wants without possibly winning the trick. The winner of the trick is the one with the highest-ranking card. At the end of the game, a player wins if he has at least half of the number of tricks won.\nb) Battleship: Battleship is a two-player strategy-type guessing game. Each player possesses a grid. In the beginning, each player secretly places a set of ships S on their grid. After placement, turn after turn, each player tries to fire at other players' ships. The game ends when all the ships of a player have been destroyed. The payoff of each player is computed as the sum of the opponent's ships that were destroyed, minus the sum of ships that the player lost. The grid is fixed to 3 \u00d7 3, with 2 ships, one of size 1 \u00d7 1, and the second of size 2 \u00d7 1.\nc) Dark Chess: Dark chess is a chess variant with incomplete information. In chess, there are two players, white and black, each controlling a set of chess pieces of their respective colors. The goal of the game is to checkmate the opponent's king. In Dark chess, the incomplete information comes from the fact that each player sees his own pieces, but only sees his opponent's pieces if they are reachable by one of his pieces. Furthermore, in his variant, the purpose is to capture the king (not to checkmate it), however, a player must be wary as he is not told if their king is in check. The size of the board is fixed to 4 x 4.\nd) Phantom Tic-Tac-Toe: Phantom Tic-Tac-Toe is a variant of the game of Tic-Tac-Toe with imperfect information. In Tic-Tac-Toe, the goal is to claim three cells along the same row, column, or diagonal. With imperfect information, the players do not observe the other player's pieces, only a referee knows the world state of the board. When it is a player's turn, the player selects a move and indicates it to the referee. The referee informs the player's whether the action is 'legal' or 'illegal'. If the move is 'illegal', the player must choose a new move until they find a legal one.\ne) Dark Hex: Dark Hex is an imperfect information version of the classic game of Hex. The objective of the game is to create a connection between opposite sides of a rhombus-shaped board. In Dark Hex, players are not exposed to opposite sides' pieces of information. Only a referee has the full information of the board and when a move fails due to collision/rejection the player gets some information of the cell and is allowed to make another move until success. The size of the board is fixed to 4 x 4."}, {"title": "A. Games", "content": "In our experiments, we aimed (i) to analyze the hyperparameters of EPIMC (depth, subgame resolution, perfect leaf evaluator) and (ii) to compare its performance against other online algorithms.\na) Postponing leaf evaluator: In Figure 2, we examine the performance of EPIMC according to various depths varying from 1 to 3. As expected, for Card Game and Battleship, increasing the depth does not lead to any improvement as both games have a majority of their observation public. Conversely, for games where the observations are private, we observe an important increase in performance At 100 seconds for Dark Chess, we achieve 80%/65%/45% winning rates at depths 3/2/1. More than that, at 1000 seconds, EPIMC at depth 3 wins close to 100% for both Dark Hex and Dark Chess. Interestingly, in Dark Hex, at 1000 seconds, similar performances are observed at depth 2 and 1. Indeed reducing the strategy fusion does not necessarily mean that the strategy produced at the end will be changed.\nIn the following, the experiments are reduced to Dark Chess, Phantom Tic-Tac-Toe, and Dark Hex, as our methods work when games have private information, yet, the extended"}, {"title": "B. Experimental Information", "content": "MCTS across all benchmarks considered. Furthermore, it is\nnoteworthy that even at a depth of 2, EPIMC demonstrated\ncomparable or even superior performance to IS-MCTS in\ngames like Dark Chess and Phantom Tic-Tac-Toe."}, {"title": "C. Experimental Results", "content": "As we have seen and as was expected, increasing the depth\nhas a significant impact on games with a private observation\n(Dark Hex / Dark Chess / Phantom Tic-Tac-Toe) and much\nless impact on games without much private information (Card\nGame / Battleship). On the domains tested and given a suffi-\ncient budget, augmenting the depth consistently outperforms\nthe basic version. In addition, a depth of 2 is sufficient, in\nmost cases, to beat the state-of-the-art online algorithms. With\nregard to the other hyperparameters, we recommend using\nInformation Set Search and Random Rollout as they require\nless computing time and therefore can be computed on larger\ngames. However, customization could be beneficial in order\nto increase performance or the need to maintain theoretical\nproperties in the subgame. Especially, using algorithms such\nas CFR+ allows to obtain the properties of CFR+ (in the sub-\ngame) such as the convergence towards the Nash equilibrium\nin two players."}, {"title": "D. Discussion", "content": "In determinization-based algorithms, both IIMC [3] and IS-MCTS [11] extend reasoning beyond a depth of one. IS-MCTS increases depth with the budget, and IIMC recur-"}, {"title": "VI. RELATED WORK", "content": "In this paper, we introduce a novel online algorithm 'Extended PIMC' for games with imperfect information. Building upon the foundation of PIMC, our approach postpones the perfect leaf evaluator to a deeper depth. Thanks to that, we have been able to successfully reduce past problems of PIMC and beat other online algorithms on multiple benchmarks. Especially, when benchmarks have hidden observation, significant performance improvements are observed. Furthermore, we conducted an in-depth analysis of various hyperparameters to provide a comprehensive understanding of their impact.\nWe enhance our research by presenting theoretical foundations for determinization-based algorithms that suffer from strategy fusion. We demonstrate that, in the worst case, increasing the depth does not increase the strategy fusion and in every case, there exists a depth d such that the strategy fusion is strictly reduced.\nAs our algorithm is online, it has the advantage of being tested in a short period of time, especially in comparison to recent algorithms which need a domain-specific abstraction or a very high initialization cost due to neural networks. Even though, improving our algorithm by using deep learning could have led to superior performance. In particular, this could provide a better and faster approximation to the leaf or being able to remove the problem of non-locality by adding an inference system [29].\nIn future research, it would be intriguing to assess our algorithm in conjunction with other determination algorithms. Particularly, exploring a trade-off between our approach, which ensures non-strategy fusion at a certain depth thanks to uniform sampling, and other algorithms that may explore the state space more efficiently but currently encounter issues with strategy fusion during exploration, could be beneficial."}, {"title": "VII. CONCLUSION AND FUTURE WORKS", "content": "In the following, we expand upon the main paper's experiments by: (i) adding Card Game and Battleship; (ii) replicating the experiment that compares the depth against IS-MCTS instead of PIMC.\nAs in the main paper, the default settings for EPIMC include a fixed depth of 3, information set search for subgame resolution, and random rollouts as the perfect information leaf evaluator. The opponent uses one second of reasoning, while the tested algorithms use between 0.1 to 100 seconds.\nAs discussed in the main paper, Card Game and Battleship primarily involve public actions, and therefore as limited impact on our algorithms."}, {"title": "APPENDIX", "content": "Dark Hex, and Phantom Tic-Tac-Toe involve private actions, where changing the depth significantly impacts performance. Increasing the depth to 3 notably improves performance in these three games.\nInterestingly, in Dark Chess, we observe that performance at depth 2 is lower than that of PIMC. This can be attributed to the fact that while increasing depth reduces strategy fusion, the fused strategy at depth 1 may have been advantageous against IS-MCTS. Thus, moving to depth 2 removed this beneficial fusion, leading to a reduction in performance."}, {"title": "A. Postponing leaf evaluator", "content": "In Figure 7, we analyze the impact of using CFR+ instead of Information Set Search (ISS) in the subgame resolution. CFR+ is used with 1000 iterations. Compared with the main paper, Card Game and Battleship have been added. As the game have public actions, the performance of the two methods is similar."}, {"title": "B. Subgame Resolution", "content": "In Figure 8, we analyze the impact of using Minimax with Alpha-Beta instead of using Random Rollout for the perfect information leaf evaluator. Compared with the main paper, Card Game and Battleship have been added. As the game have public actions, the performance of the two methods is similar."}, {"title": "C. Leaf Evaluator", "content": "In Figure 9, we add the game Battleship and Card Game against other online algorithms. As in the main paper, we observe that EPIMC and IS-MCTS obtain the best performance against other online algorithms and, OOS and RecPIMC obtain poorer performance. However, RecPIMC achieves performance close to EPIMC and IS-MCTS in Battleship."}, {"title": "D. Against other online algorithms", "content": null}]}