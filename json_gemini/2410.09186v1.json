{"title": "Learning Algorithms Made Simple", "authors": ["Noorbakhsh Amiri Golilarz", "Elias Hossain", "Abdoljalil Addeh", "Keyan Alexander Rahimi"], "abstract": "In this paper, we discuss learning algorithms and their importance in different types of applications which includes training to identify important patterns and features in a straightforward, easy-to-understand manner. We will review the main concepts of artificial intelligence (AI), machine learning (ML), deep learning (DL), and hybrid models. Some important subsets of Machine Learning algorithms such as supervised, unsupervised, and reinforcement learning are also discussed in this paper. These techniques can be used for some important tasks like prediction, classification, and segmentation. Convolutional Neural Networks (CNNs) are used for image and video processing and many more applications. We dive into the architecture of CNNs and how to integrate CNNs with ML algorithms to build hybrid models. This paper explores the vulnerability of learning algorithms to noise, leading to misclassification. We further discuss the integration of learning algorithms with Large Language Models (LLM) to generate coherent responses applicable to many domains such as healthcare, marketing, and finance by learning important patterns from large volumes of data. Furthermore, we discuss the next generation of learning algorithms and how we may have an unified Adaptive and Dynamic Network to perform important tasks. Overall, this article provides brief overview of learning algorithms, exploring their current state, applications and future direction.", "sections": [{"title": "I. INTRODUCTION", "content": "The definition of artificial intelligence is to use machines based on human intelligence to perform a task, such as decision-making or object recognition. Nowadays, AI technology and its applications are becoming very common in society, and are penetrating people's lives in many ways. Its fast development is due to the rapid progress in science and technology [1]. We can claim that AI and its multidisciplinary applications will be categorized among the hottest subjects in the future. AI technologies can be seen everywhere, and in every technology-based institution such as Google, YouTube, Amazon, etc. It has vast applications in all fields of interest such as disease diagnosis, early prediction of tumors, recommended systems, image and signal processing, computer vision, etc. Experts can solve their problems more efficiently by employing AI compared to applying traditional methods and approaches. This technology is based on extensive computer programming for training machines with human beings' behaviors for performing some important cognition-based tasks [2]. This is a full integration between humans and computer-based technologies [1]. Prediction and classification are categorized among the important tasks of learning-based systems. To better predict and classify, we are in need of suitable algorithms mimicking human behaviors, skills, and abilities to carry out and process these tasks [3]. As can be seen from Fig. 1, Machine Learning (ML) is a sub-field of AI, and Deep Learning (DL) is a sub-field of ML which we explained briefly in the following.\nMachine learning (ML) is a branch of artificial intelligence that creates and studies statistical algorithms that can learn from data and generalize to previously unknown data, allowing them to execute tasks without explicit instructions [5]. Training data is the sample data that a machine learning algorithm uses to build a mathematical model. This allows the system to execute future actions based on past information. Machine learning is widely utilized in many domains, such as computer vision, natural language processing, and robotics. It is essential in data mining, analysis, classification, and prediction. This technology is making our lives easier; for example, when we give a voice command on the phone or search for an image on the internet, it provides the results we need. Machine learning has numerous applications, including speech recognition, image recognition, and prediction. In speech recognition, software applications listen to human speech and transform it into text, simplifying many everyday tasks. Furthermore, the development of different algorithms has improved image recognition efficiency, allowing the system to comprehend an unknown image and deliver comprehensive information about it. In addition, the healthcare industry has emerged as a data-driven solution, enabling medical professionals to detect and categorize complicated diseases or find important patterns associated with a certain medical condition that would not be achievable through manual analysis."}, {"title": "II. MACHINE LEARNING", "content": "Machine learning is categorized as a subset of artificial intelligence which deals with learning from the data to act like human beings, and to analyze and process the given tasks efficiently. It has a wide variety of applications in biomedical, engineering, science, agriculture, animal sciences, neuroscience, remote sensing, etc. Those equipped with ML can run decision-making and performance analysis much faster than they ever could have before. Segmentation, detection, and recognition are among the most important real-world problems. Segmentation is one of the critical steps to be done prior to further image analysis and processing. The segmented features are supposed to be fed to the algorithm for recognition and classification. Some published research has shown that ML can process these tasks properly [6]. In this section, we discuss some important concepts and techniques in ML."}, {"title": "A. Supervised Learning", "content": "Supervised learning is a subset of ML in which the training can be processed on the labeled dataset. In this case, the algorithm can learn from the labeled input data. If it is provided with a new set of examples, then based on the learned features from the training stage, the machine can predict the outcome accurately. This characteristic allows supervised learning to solve various problems with high accuracy. The two sub-categories of supervised learning are classification (SVM and neural nets) and regression (logistic and linear regression).\nAs mentioned above, SVM is a supervised classification algorithm. According to Fig. 3, let's consider a training dataset of n points as $(x_1,y_1),..., (x_n, y_n)$ and also consider $w^T x + b = 0$ as the hyperplane separating two classes linearly [8].\nWe can write the optimization problem as:\n$\\mathop {\\min }\\limits_{w,b} {1 \\over 2}{||w||^2}$ subject to\n${y_i}({w^T}{x_i} + b) \\ge 1$,\nwhere ${y_i} \\in \\{  - 1,1\\} $ is the class label, w is the weight, and b is the bias.\nOn the other hand, let's imagine the data points cannot be separated linearly or without misclassification. In this case, we will add one more constraint [8]. We are dealing with minimizing the misclassification error. The new optimization problem can be written as:\n$\\mathop {\\min }\\limits_{w,b} {1 \\over 2}{||w||^2} + C\\sum\\limits_{i = 1}^n {{\\varphi _i}}$ subject to\n${y_i}({w^T}{x_i} + b) \\ge 1 - {\\varphi _i},{\rm{ }}{\\varphi _i} \\ge 0,{\rm{ }}i = 1,...,n,$\nwhere C is the regularization parameter and ${\\varphi _i}$ is the slack variable.\nK-NN is categorized as a supervised algorithm that can be utilized for both classification and regression. Consider several categories (sets of classified points [9]) and the number of k neighbors (three) for an unclassified data point q1. Assume one neighbor belongs to class O, while the other two belong to class X. In this circumstance, q1 will be classified as class X because the majority of our neighbors are members of that class. This classification can be accomplished with simple majority vote or distance-weighted voting [10]."}, {"title": "B. Unsupervised Learning", "content": "Unsupervised learning deals with input data that are not labeled at all. The input is unsorted information or data and we do not have any idea, clue or guidance about the input characteristics and features. In this case, grouping can be processed based on the shape, differences, similarities and different patterns of the data. In terms of complexity, it is a computationally complex model. Unlike supervised learning, the number of classes is unknown. Unsupervised learning can be used for clustering, feature learning, anomaly detection, and dimensionality reduction [6]. Clustering is one of the important types of un-supervised machine learning algorithms which group and categorize unlabeled data. It is similar to classification, but the only difference is in the dataset. In classification we are dealing with labeled data but in clustering we are working with un-labeled dataset. To provide a clearer understanding of this concept, the general framework of clustering is shown in Fig. 7 (b).\nK-means clustering is a type of unsupervised learning algorithm which is working based on randomly initializing the centroids, computing the distances among given data points and centroids, and assigning the sample points to the new clusters. The main steps of the k-means clustering are depicted in Fig. 5 [12].\nIn Fig. 6, we try to find the best place to separate the image after filtering with k-means clustering method. The images have been collected from Kaggle [13]. In this method as briefly described in [14], we can use the final image in the form of a matrix. We select several random starting points in the matrix. Using the k-means method, we divide the points inside the image into several clusters [14]. The center point of each cluster can also be considered as the center of each character."}, {"title": "C. Reinforcement Learning", "content": "Reinforcement learning (RL) is another subset of machine learning in which the agents are allowed to learn from their own experiences and errors in an environment leading to maximized total cumulative reward and making a balance between exploration and exploitation which are the main goals of RL [15]. As it is mentioned above, the main aim of unsupervised learning is to deal with unsorted data, differences, and similarities among them. Also, the main target in supervised learning is dealing with labeled data for better prediction and classification. The framework of supervised, unsupervised, and reinforcement learning are depicted in Fig. 7."}, {"title": "D. Semi-supervised Learning", "content": "Some drawbacks in supervised and unsupervised learning algorithms like manually labeling the data and limited spectrum of applications, respectively [16] are the main motivations of developing semi-supervised learning.\nIn this type of learning, we are dealing with both labeled and unlabeled data. As seen in Fig. 8, the initial step here is clustering based on the similarity in the input data. It means that unsupervised learning is used first to cluster similar data and use it to train the model. Then this information can be used to label the remaining unlabeled data [6]. This process is called pseudo-labeling. Next, train the model based on these combined labeled and pseudo-labeled data [6][16] which results in prediction with better accuracy."}, {"title": "E. Federated Learning", "content": "Federated Learning (FL) offers a revolutionary training strategy for creating individualized models while preserving user privacy [17] [18]. With the introduction of artificial intelligence chipsets, client devices' processing resources have increased, gradually shifting model training from a central server to terminal devices. Additionally, this approach provides a privacy protection mechanism that leverages the processing capabilities of terminal devices to train models, preventing private information from being leaked during data transmission. It also fully utilizes the vast dataset resources available from smartphones and other devices [17].\nNotably, we demonstrated how federate learning operates in Fig. 9, where several models are trained on various data sources while preserving security [19]. It is demonstrated that each model is trained locally on the relevant data source, and after that, it sends its updated parameters to a central server, which aggregates them to create a global model. After that, the individual models receive further training from this global model, which enhances their functionality and ability to generalize. This collaborative learning is made possible through this iterative technique without disclosing private information.\nAdditionally, a major function of this training paradigm is to ensure user privacy, distinguishing it significantly from typical privacy protection techniques used in big data, such as differential privacy and k-anonymity; moreover, federated learning primarily safeguards privacy by transmitting encrypted processing parameters, preventing attackers from obtaining source data. These measures ensure data-level privacy and compliance with the General Data Protection Regulation (GDPR) and other regulations [17].\nDepending on data distribution, FL can be classified as horizontal, vertical, or federated transfer learning. Horizontal federated learning is appropriate when user features of two datasets overlap significantly but the users overlap minimally [17]. Vertical federated learning is applicable when user features overlap slightly, but users overlap significantly. When users and features rarely overlap, transfer learning compensates for the lack of data or tags [17]. Moreover, this technique is comparable to multiparty computing and distributed machine learning, which includes distributed model outcomes, distributed data storage, and distributed computing activities [17]. In distributed machine learning, the parameter server helps increase training speed by distributing data across working nodes and managing resources via a trusted central server. Furthermore, it allows each worker node to retain its own data while participating in model training.\nOn the other hand, a critical aspect of this paradigm in ensuring privacy is allowing users complete control over their local data, emphasizing data owners' privacy [17]. There are two types of privacy protection measures used in a federated environment: encryption algorithms such as homomorphic encryption, and secure aggregation techniques [17]. Another common approach is incorporating the noise of differential privacy into model parameters. For instance, Google proposes a method using a combination of secure convergence and differential privacy [20]. Other studies have utilized solely homomorphic encryption to ensure privacy [21]."}, {"title": "F. Feature Learning", "content": "Feature learning is an approach that allows machines to learn the characteristics and representations required for feature selection, recognition, and classification [22], [23], resulting in much better representations, making feature extraction easier and more accurate [22]. In contrast, manually extracting features may not yield an optimal feature set or accurate predictions [6]. Consequently, the following methods serve as a replacement for manual feature extraction: autoencoders, principal component analysis (PCA), bag of words (BoW), term frequency-inverse document frequency (TF-IDF), and image processing techniques.\nFirstly, the autoencoder concept is based on learning from the coding of the original data sets to create new and more powerful features. It accomplishes this by training a neural network to replicate its input, forcing it to identify and exploit data structures [24]. It minimizes dimensionality and extracts key features from data, resulting in more effective machine-learning models. Secondly, large data sets' dimensionality is decreased while maintaining as much information as possible using the PCA-based feature extraction technique. Principal Component Analysis highlights variation and identifies significant trends and connections among the dataset's variables [24]. Next, BoW is a useful method in Natural Language Processing (NLP) that allows words (also known as features) to be retrieved from a text and categorized based on how often they are used. Every document is represented by a vector of word counts, and the word count is sent into machine learning algorithms as input [24].\nThen, TF-IDF, an NLP-based feature extraction technique, is an extension of BoW that uses a numerical statistic to represent a word's importance to a document inside a collection or corpus [24]. In contrast to BoW, it takes into account all of the other texts in the corpus in addition to a word's frequency in a single document. This makes up for the fact that some terms are used more frequently than others overall. Finally, to recognize and isolate important features or patterns in a picture, image processing algorithms analyze raw data [24]. The process could be extracting properties such as color, texture, and shape, or it could be recognizing corners and edges. Then, tasks like object detection, image segmentation, and image classification can be performed using these features [24]. For example, as can be seen in Fig. 10, detection of key points using SURF (Speeded-Up Robust Features) technique can provide us with important features in images. The dataset is available in [25].\nWe illustrated the feature learning paradigm in machine learning in Fig. 11. Under the paradigm shown in Fig.11, an initial (often sparse) feature set or raw data are input to learn implicit feature representations using a variety of techniques [26]. As a result, the feature representation is richer and frequently lower dimensional, which might improve performance when applied as the input to more specialized learning tasks [26]."}, {"title": "G. Transfer Learning", "content": "In machine learning, transfer learning is a method where knowledge acquired from one task or dataset is applied to enhance model performance on a different but related task or dataset [27]. Stated differently, this method improves generalization in a different context by applying knowledge gained in a previous context [27]. It can be used for many tasks, including deep learning model training and regression problem-solving. For situations involving deep neural networks, which need a lot of data to function well, this method is particularly appealing [27].\nConventional learning procedures use the available labeled data to create a new model for every task. This is because, even when trying to complete a task that is similar to the first model, users must retrain a newer model from the beginning if the data distribution changes or the trained model is utilized for a completely new dataset [27]. This happens because traditional machine learning algorithms assume that training and test data originate from the same feature space. On the other hand, this method starts with models or networks that have previously been trained. To handle a new but related target task or dataset, like classifying song reviews, the model leverages the information acquired from a source task or dataset, e.g., categorizing movie reviews [27].\nFurthermore, transfer learning has numerous advantages, including inexpensive processing costs, the flexibility to use huge pre-trained datasets, and broad application across a variety of tasks. First, it reduces the computing costs associated with developing models for new issues. By reusing pre-trained models or networks for other applications, users can save time, data, processing units, and other computing resources [27]. For instance, reaching the necessary learning rate may necessitate fewer epochs, thereby accelerating and simplifying model training procedures. Second, it considerably minimizes the challenges involved in collecting large datasets. Large language models, for example, need a massive quantity of training data to attain optimal performance.\nFinally, while this method aids in model optimization, it can also improve a model's generalizability. Since transfer learning involves retraining an existing model with a new dataset, the retrained model will be familiar with multiple datasets and may perform better on a wider variety of data compared to the original base model, which was trained on only one type of dataset [27]."}, {"title": "H. Ensemble Learning", "content": "Ensemble learning is a machine learning technique that combines two or more learners (such as neural networks or regression models) to generate better predictions [28]. In other words, an ensemble model makes predictions that are more accurate than those of a single model by combining multiple models [28]. There are two types of ensemble learning approaches in machine learning: parallel and sequential. In parallel techniques, each base learner is trained independently of the others, while sequential approaches train a new base learner to minimize errors made by the previous model trained in the preceding stage [29].\nParallel methods are further subdivided into homogeneous and heterogeneous methods [28]. To generate all of the component base learners in homogeneous parallel ensembles, the same base learning algorithm is used. In contrast, heterogeneous parallel ensembles use different algorithms to generate base learners [30]. Additionally, some methods, such as stacking, can combine base learners into a final learner by separating the algorithms required to train an ensemble learner from the base learners [28]. Voting, specifically majority voting, is a typical method for condensing base learner predictions.\nThe final forecast produced by majority voting is based on the predictions made by the majority of learners, considering the predictions made by each base learner for a particular data instance [28]. For example, in a binary classification task, the majority vote utilizes the final prediction for a specific data instance, combining predictions from each base classifier [28]. A variation of this method is weighted majority voting, which prioritizes the predictions of some learners over others [31].\nThe most prevalent ensemble learning strategies are bagging, boosting, and stacking. The differences between sequential, parallel, homogeneous, and heterogeneous varieties of ensemble methods are best illustrated by combining them. A homogeneous parallel technique known as \"bootstrap aggregating\" is bagging [28]. To train multiple base learners with the same training method, it takes advantage of modified replicas of a particular training data set [32]. Specifically, bagging trains multiple base learners by extracting several new datasets from a single initial training dataset using a process known as bootstrap resampling.\nStacking, also known as a layered generalization, is a heterogeneous parallel approach that exhibits meta-learning [33]. Meta-learning is the process of training a meta-learner using the outputs of many base learners. Stacking trains several base learners from the same dataset using a unique training procedure for each learner [28]. Each base learner produces predictions on a previously unseen dataset. These initial model predictions are then combined and utilized to train the final model, which is the meta-model [31].\nBoosting algorithms employ a sequential ensemble technique and it trains a learner on an initial dataset, and the resulting learner is often ineffective, misclassifying a large number of samples in the dataset. Like bagging, boosting takes instances from the initial dataset and creates a new dataset (d2) [28]. However, unlike bagging, boosting prioritizes mis-classified data instances from the initial model or learner. A new learner is then trained on this fresh dataset, d2 [28]. A third dataset (d3) is subsequently created from d1 and d2, prioritizing the second learner's misclassified samples, as well as instances in which d1 and d2 disagree. This technique is repeated n times to produce n learners [28]. Boosting then integrates and weights all of the learners together to generate final predictions [33]."}, {"title": "III. DEEP LEARNING", "content": "This section discusses frequently used deep learning models, including convolutional neural networks (CNN), recurrent neural networks (RNN), long short-term memory (LSTM), and generative adversarial networks (GANs). Furthermore, we cover backpropagation, a crucial training procedure for neural network optimization. In the following subsections, we provide a more detailed explanation of the operational principles of these designs."}, {"title": "A. Convolutional Neural Network (CNN)", "content": "Neural Network is a subset of machine learning. This network contains various nodes and layers; and there are lots of connections between nodes or neurons. These nodes are connected to each other associated with weight and threshold. The main structure of a neural network contains an input layer, one or more hidden layers, and the output layer. Once the output of each node is higher than the threshold value, then the information will be passed to the next layer. Convolutional neural networks are one of the important types of neural networks which are mainly used for classification, object detection and segmentation.\nIn CNN, each convolutional layer can be followed by additional convolutional (Conv) layers, pooling layers, flatten, and fully connected (FC) layers. These layers are designed for better understanding and processing of data, features, and patterns. The input data passes through these layers to get precise output. During training, convolutional neural nets are learned to extract important and relevant features from data. Conv layers extract meaningful features like edges, textures and patterns from raw data with the aim of detection, classification, and prediction. Kernels are small filters that slide over these inputs and create feature maps. As the network deepens, the convolutional layers learn increasingly abstract representations of the input, making CNNs highly effective in recognizing complex structures within the data. We should be very careful about the complexity of the models as using deeper layers may cause complexity and slower processing. However, advanced CNN architectures such as ResNet and DenseNet have introduced innovations like residual connections and dense blocks, enabling the training of deeper networks without suffering from gradient degradation.\nPooling layers down sample these features to reduce dimensionality issues, ensuring that the most prominent features are retained while unnecessary details are discarded which allows the network to focus on key aspects of the data. Following the conv and pooling layers, these feature maps go through flatten layer, reshaping and converting the multidimensional tensor to 1D vector. Then to produce the final output, all previous neurons need to be connected, and this process is done in fully connected layer. The total number of neurons in the flatten layer equals the total number of components and elements in the previous layer. If the previous layer is a 3-dimensional tensor, then the number of neurons in flatten layer is the product of these dimensions. Additionally, the capacity of the network is dependent on the number of neurons in fully connected layer. As we have more neurons in FC layer, the model can learn more complex representations, but we should be careful about increasing the risk of computational cost and overfitting."}, {"title": "B. Recurrent Neural Network (RNN)", "content": "An RNN, or recurrent neural network, is a type of deep neural network trained on time series or sequential data to create a machine learning model that can produce sequential outputs in the form of predictions [34]. Like conventional neural networks, including feedforward and convolutional neural networks (CNNs), recurrent networks learn from training data. However, what sets them apart is their ability to use information from previous inputs to affect the present input and output [34]. They rely on the results of earlier parts in the sequence, while standard deep learning networks assume that inputs and outputs are independent. Although the outcome of a given sequence could also be influenced by future occurrences, unidirectional RNNs cannot incorporate these events into their predictions [34].\nTo provide context for understanding RNNs, consider the expression \"feeling under the weather,\u201d which is often used to describe someone who is sick. The idiom must be expressed in that precise order to make sense. Thus, RNNs must consider each word's position within the idiom and use this knowledge to predict the next word in the sequence [34]. The words in the phrase \"feeling under the weather\" are not in any particular order; rather, they are part of a sequence [34]. Maintaining a hidden state at each time step allows the RNN to track the context. Moving the hidden state from one time step to the next creates a feedback loop. Information about prior inputs is stored in this hidden state, functioning as a kind of memory [34]. At each time step, the RNN processes the hidden state from the previous time step and the current input (such as a word in a phrase). As a result, it can remember past data points and apply that knowledge to affect the present output [34].\nRecurrent networks are also distinguished by their parameter sharing across all network layers [34]. Unlike feedforward networks, which have distinct weights for each node, RNNs use the same weight parameter throughout all layers. Nevertheless, these weights are still modified using gradient descent and backpropagation techniques to support reinforcement learning [34].\nTo find the gradients (or derivatives), RNNs employ forward propagation and backpropagation through time (BPTT) methods. This approach differs slightly from standard backpropagation because it is tailored to sequence data [34]. The fundamentals of BPTT are similar to those of conventional backpropagation, where the model learns by estimating errors from its input layer to its output layer [34]. These computations enable us to suitably adjust the model's parameters. The key difference with BPTT is that, while feedforward networks do not share parameters across layers, backpropagation through time (BPTT ) must accumulate errors at each time step [34]."}, {"title": "C. Long Short-Term Memory (LSTM)", "content": "The LSTM was introduced in [35] to address the vanishing gradient problem that arises with ordinary RNN and prevents the learning of long-term dependencies. The vanishing-gradient problem presents a situation in which the weights of RNN cease to change throughout the training process [36]. As a result, prioritizing current knowledge may result in the disregard of past occurrences. As a result, long-term dependencies are unable to be effectively learned. The LSTM is designed to regulate the whole information flow among neurons. To do this, a gating system is employed that governs the process of adding and removing information from an iteratively propagated cell state [36]. Thus, the forgetting process can be regulated, and a specified memory behavior is accomplished to simulate both short-term and long-term dependency.\nIn contrast to typical RNN, the neuron's output h(t) is determined by the cell state z(t), rather than the inputs x(t) and prior outputs h(t-1) [36]. On the other hand, the LSTM gating mechanism determines the cell state. Vectors f(t), the output of the forget gate, and n(t), the output of the add gate, iteratively alter the cell state to govern memory behavior [36]. In the LSTM architecture, an inverse link between these two gates is employed to restrict memory capacity to some extent. As a result, each cycle adds and removes information from the cell state [36].\nThis approach is driven by the fact that no memory is unlimited, and the human memory, as a role model, has a finite capacity. The output gate calculates the intended output by inferring the updated cell state. Hence, the most recent inputs do not always dominate the creation of output signals since the cell state includes a reduced and weighted representation of historical input information. This data is projected onto the output [36]. Therefore, the effect of important historical events, for example, is included in the output projection, and it is possible to ignore current inputs with low information density [36]. The gates are built using current inputs and past outputs. LSTM cells' adaptability has led to their use in a wide range of neural network architectures, each designed to address a particular set of issues [36].\nIn other words, because LSTM networks are able to learn temporal associations and capture them in a low-dimensional state representation, they are destined to identify contextual anomalies [36]. These relationships deal with short- and long-term dependencies, as well as stationary and nonstationary dynamics. Multivariate time series and time-variant systems can be effectively modeled by LSTM networks [36]. As demonstrated in [37], which proposes a stacked LSTM architecture to detect abnormalities within time series data, LSTM-based techniques have demonstrated strong anomaly identification capacity. An LSTM network is used in [38] to forecast regular system dynamics, and a support vector machine is used to classify anomalies in order to provide a self-learning detection mechanism. In [39], a method for detecting collective abnormalities using LSTM networks is proposed, which involves analyzing numerous one-step forward prediction errors. A real-time detection technique is implemented in [40] using two LSTM networks, one of which models short-term properties and the other of which controls the detection by means of long-term thresholds."}, {"title": "D. Generative Adversarial Networks (GANS)", "content": "Goodfellow initially introduced GANs in 2014 [41]. One simple concept is to have two models combat each other [42]. One model is generator G, while the other is discriminator D. Then it must provide the G with a random noise z that follows the prior probability, and the G will output the data as G(z). Finally, the discriminator is updated using the G(z) and P(datax) values [42]. Following training, the D network determines whether the input data is real or created by the generator. The D improves its discriminating capacity through continual learning, whereas the G makes its data more realistic through continuous learning. To trick the discriminator, G and D combat each other, strengthening their abilities and eventually reaching a stable state. The discriminator is unable to distinguish the generator's data, resulting in falsification [42].\nThere are some issues with the original GANs [42]. First, there is instability in the training. Ensuring the synergy between G and D is a challenge. Second, there is little variation in the fake photos. Lastly, there isn't a consistent effective criterion for the resulting image's quality. As a result, several GAN enhancements have lately been suggested. As an example, DCGAN (deep convolutional GAN) was proposed by Radford et al. [43]. The network structure of the original GANs is primarily enhanced by DCGAN. It substitutes two convolutional neural networks for the generator and discriminator, increasing network stability but failing to address the core issue [42]."}, {"title": "E. Transformer Architecture", "content": "The transformer model was first introduced in [52] for machine translation work. Since then, many other models have been created to meet a range of problems in many fields, drawing inspiration from the original transformer model [52]. The task and performance of transformer-based models can therefore differ according to the particular architecture used. However, self-attention is a crucial and frequently utilized part of transformer models, and it is necessary for their functionality [52]. The self-attention mechanism and multi-head attention, which normally comprise the architecture's first learning layer, are used by all transformer-based models [52].\nThe attention mechanism has become more popular because of its ability to focus on important information [53]. Certain visual regions were discovered to be more relevant than others while processing images. As a result, the attention mechanism was proposed as a novel approach to computer vision tasks, to emphasize key components based on their contextual relevance inside the application. When applied to computer vision, this technique produced remarkable results, promoting its widespread use in a variety of other domains, including language processing.\nTo overcome the shortcomings of other neural networks, such as recurrent neural networks (RNN) in recording long-range dependencies in sequences, particularly in language translation tasks, a unique attention-based neural network known as \"Transformer\" was presented in 2017 [52] [54]. By lowering the dependence on outside data and better capturing local features, the transformer model's self-attention mechanism enhanced the attention mechanism's performance [52]. The \"Scaled Dot Product Attention\" in the original transformer design is used to execute the attention approach [52]. It is based on three main parameter matrices: query (Q), key (K), and value (V) [52]. Every matrix in the series has an encoded representation of every input [52]. The scaled dot product attention function can be calculated mathematically in the following way:\n$Attention(Q,K,V) = softm\u0430\u0445 (\\frac{QK^T}{\\sqrt{d_k}})V$\nThe value vectors are represented by matrix V, whereas the query and key vectors, with a dimension of dk, are represented by matrices Q and K, respectively.\nTo extract the maximal interdependence between various segments in the input sequence, the multi-head attention module must apply the scaled dot-product attention function simultaneously [52]. The attention mechanism is carried out by each head, represented by the letter k, using its unique learnable weights, $W_k^Q, W_k^K$ and $W_k^V$. According to Vaswani et al. [52], the attention outputs computed by each head are then concatenated and linearly transformed into a single matrix with the anticipated dimension.\nThe neural network can learn and capture many features of the sequential input data more easily when multi-head attention is used [52]. As a result, this improves the representation of the input contexts by combining data from various attention mechanism aspects within a given range\u2014which may be brief or long. Better network performance is the outcome of this strategy since it permits the attention mechanism to operate cooperatively [52]. The auto-regressive sequence transduction model served as the foundation for the development of the original transformer architecture, which consisted of the encoder and detector modules [52]. Each module integrates the attention mechanism over multiple layers. Specifically, the transformer architecture's many concurrent executions of the attention mechanism account for the existence of multiple \"Attention Heads\" [52]."}, {"title": "F. Backpropagation", "content": "The backpropagation algorithm calculates the gradient of a neural network's loss function in terms of weights and biases [55", "55": "calculate the cost", "sum": "n${z^L"}, {"55": "n$\\frac{\\partial C_k"}, {"55": "across data points is computed as:\n$\\frac{\\partial C}{\\partial w^L} = \\frac{1}{n"}]}