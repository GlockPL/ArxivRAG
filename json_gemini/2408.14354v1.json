{"title": "SWE-BENCH-JAVA: A GITHUB ISSUE RESOLVING\nBENCHMARK FOR JAVA", "authors": ["Daoguang Zan", "Zhirong Huang", "Ailun Yu", "Shaoxin Lin", "Yifan Shi", "Wei Liu", "Dong Chen", "Zongshuai Qi", "Hao Yu", "Lei Yu", "Dezhi Ran", "Muhan Zeng", "Bo Shen", "Pan Bian", "Guangtai Liang", "Bei Guan", "Pengjie Huang", "Tao Xie", "Yongji Wang", "Qianxiang Wang"], "abstract": "GitHub issue resolving is a critical task in software engineering, recently gaining significant attention\nin both industry and academia. Within this task, SWE-bench [1] has been released to evaluate\nissue resolving capabilities of large language models (LLMs), but has so far only focused on\nPython version. However, supporting more programming languages is also important, as there\nis a strong demand in industry. As a first step toward multilingual support, we have developed\na Java version of SWE-bench, called SWE-bench-java-verified. We have publicly released\nthe dataset, along with the corresponding Docker-based evaluation environment and leaderboard,\nwhich will be continuously maintained and updated in the coming months. To verify the reliability\nof SWE-bench-java-verified, we implement a classic method SWE-agent [2] and test several\npowerful LLMs on it. As is well known, developing a high-quality multi-lingual benchmark is time-\nconsuming and labor-intensive, so we welcome contributions through pull requests or collaboration\nto accelerate its iteration and refinement, paving the way for fully automated programming.", "sections": [{"title": "1 Introduction", "content": "Automating software engineering tasks with large language models (LLMs) has gained considerable attention [3, 4, 5].\nBeyond code generation, the issue resolving task proposed by SWE-bench [1] changes the role of LLMs from code\nassistants to fully autonomous AI programmers. SWE-bench contains 2, 294 issues from 12 widely-used open-sourced\nPython libraries. LLMs are tasked to generate a patch based on the issue description along with the buggy code repository.\nWithin less than one year, the resolving rate on SWE-bench lite increased from 0.33% [1] (for RAG+GPT3.5) to\n43.00% [6] (for CodeStory Aide+Mixed Models). SWE-bench lite is a subset of 300 issues selected from SWE-bench,\nchosen for their relatively clear descriptions and simple fix solutions.\nSWE-bench [1] is centered on Python, which confined its evaluation to LLMs in Python-related fields such as\ndata processing and artificial intelligence. However, it does not cover other common and necessary fields like web\napplications, mobile applications, and system programming, which rely on other programming languages [7, 8].\nTherefore, as the first step in moving towards a multi-lingual issue resolving benchmark, we choose to develop a Java\nversion of SWE-bench for the following two reasons:\n1. Popularity. Java enjoys widespread popularity, making it one of the most widely adopted programming lan-\nguages in the industry, particularly in fields like finance, cloud services, and Android application development.\nAccording to the TIOBE index for August 2024, Java ranks among the top 4 languages, following Python, C,\nC++. With an active developer community, Java continues to grow, as evidenced by Oracle's data showing\n120 million developers worldwide, with over 1 million new developers added annually."}, {"title": "2. Platform Independence.", "content": "Java programs run on a virtual machine, which automatically manages memory\nsimilar to Python, and compiles to Java byte-code that is interpreted by the virtual machine. While C and\nC++ are preferred for system programming due to their performance and memory efficiency, we believe that\nlanguage models are not primarily designed to address performance issues. Therefore, we chose Java over the\nmore performance-focused C/C++ as the first step.\nThis paper proposes a Java version SWE-bench, named SWE-bench-java-verified. We describe the details of\ndataset construction, the main challenges, and potential problems. With SWE-bench-java-verified, we also evaluate\nthe performance of SWE-agent [2] with the state-of-the-art models including GPT-40 [9], GPT-40-mini [10], DeepSeek-\nV2 [11], DeepSeekCoder-V2 [12], Doubao-pro [13]. Overall, the contributions of this paper are as follows:\n\u2022 We meticulously created and manually verified the SWE-bench-java-verified benchmark, marking the\nfirst step in establishing a multilingual GitHub issue-resolving benchmark with a focus on Java. We plan to\nsupport more programming languages and make continuous improvements in the coming months. We also\nencourage the community to contribute by submitting pull requests to collaborate in advancing this field.\n\u2022 We open-sourced the dataset, along with a comprehensive evaluation Docker environment and a leaderboard,\nto advance further research in this field.\n\u2022 We implemented SWE-Agent [2] on SWE-bench-java-verified and derived several insightful findings that\nenhance our understanding of issue resolving in Java projects."}, {"title": "2 Multi-SWE-bench", "content": ""}, {"title": "2.1 Benchmark Construction", "content": ""}, {"title": "2.1.1 Workflow Overview", "content": "We construct SWE-bench-java-verified by following the work of SWE-bench [1]. Specifically, the workflow of\nconstructing this benchmark comprises five phases:\n1. Candidate repository collection. We collect candidate repositories for SWE-bench-java-verified con-\nstruction from two sources: (1) Popular Java repositories on GitHub, which are collected by requesting a\nlist of Java repositories sorted by their stars via GitHub API; It is worth noting that we rule out repositories\nwhere Java is not the main language. (2) Repositories included in the Defects4j [7] database, which is a dataset\ncollecting reproducible bugs across multiple Java repositories. As a result, we collect 53 repositories from\nGitHub and 17 repositories from Defect4J, obtaining a total of 70 candidate Java repositories. After careful\nmanual selection and filtering, we narrowed down the list to 19 open-source Java repositories: 10 from source\n(1) and 9 from source (2).\n2. Issue instance crawling. The issue instance crawling process was conducted in the following three steps: (1)\nWe crawled all pull requests for the 19 selected repositories. (2) We filtered these pull requests by retaining\nonly those that were associated with at least one issue and involved changes to test files. (3) For each pull\nrequest that met our selection criteria, we further crawled its detailed information, including \u201cinstance ID\u201d,\n\"patch\", \"repository name\u201d, \u201cbase commit\u201d, \u201chints text\", \"creation date\", \"test patch\", \"issue statement\u201d,\n\u201cenvironment setup commit\u201d, and \u201cfail-to-pass\u201d. In total, we crawled 1,979 issue instances for 19 repositories.\n3. Runtime environment determination. We determine the runtime environment of each issue through code\nreading and trial runs. To be specific, we determine the build tool, JDK version and compilation commands\nused in the target issue. Given the code repository associated with the target issue, we carry out three steps:\n(1) We identify the build tool type (maven or Gradle) from the repository structure; (2) We determine the\nJDK version used through reviewing the build configuration; (3) We compile the repository to establish its\ncompilation commands. In this phase, we also filter out issues where the associated repository fails to be\ncompiled, e.g. the repositories depending on additional development environments like Android SDK. As a\nresult, we primarily derive a collection of 308 issue instances from the pool of 1,979, which are verified to\ncompile successfully under the determined runtime environment.\n4. Fail-to-pass test extraction. We extract fail-to-pass tests for each issue by comparing test results before and\nafter applying the ground-truth patch. In detail, given an issue instance, we build two different containers"}, {"title": "5. Questionnaire-based manual verification.", "content": "To ensure the reliability of our benchmark in evaluating the LLMs'\nabilities in the issue resolving task, we conduct a comprehensive manual verification process. Following\nthe recently published SWE-bench-verified annotation guidelines, we invite 10 software developers\nexperienced in Java, to screen the above 137 issue instances. The verification process involved answering\nthe following three questions of each issue: (Q1) the clarity of the issue description, rated on a scale from 0\nto 3, where lower scores indicate greater clarity; (Q2) the comprehensiveness of test coverage in evaluating\npotential solutions, also rated from 0 to 3, where lower scores reflect better coverage; and (Q3) the presence of\nany major flaws that might necessitate exclusion from the dataset, with 0 indicating the absence of such flaws\nand 1 indicating their presence. Based on these annotations, we retained issues that satisfied the following\ncriteria: \"(Q1 is 0 or Q1 is 1) and (Q2 is 0 or Q2 is 1) and (Q3 is 0)\". This stringent filtering resulted in a final\ndataset of 91 high-quality issue instances, covering 6 repositories."}, {"title": "2.1.2 Troubleshooting", "content": "During benchmark construction for SWE-bench-java-verified, we spot and fix a few issues which negatively\naffect the benchmark quality and evaluation efficiency. Although SWE-bench has established a complete and easy-to-\nuse pipeline for mining issues and automatically evaluating solutions of issue resolving in Python projects, we still\nencountered some difficulties when migrating to Java. In this section, we will present the identified issues and discuss\nour solution for troubleshooting.\nBase commit crawling errors in the original SWE-bench script. We found that the original SWE-bench script\nsometimes crawls the incorrect base commit for pull requests, mainly because it indiscriminately uses the previous\ncommit without considering branch differences. We have fixed this bug using \u201cgit commit graph\" to distinguish\nbetween different branches, making the script more reliable and ready for use.\nRedundant downloads of repositories and dependencies. We found it a burden to repeatedly download repositories\nand dependencies when running evaluations on different issue instances. To be specific, SWE-bench builds a separate\ndocker container for each issue instance, where the corresponding repository will be pulled online to initialize the\nworking directory; in addition, the dependencies will be installed to construct the runtime environment. However, some\nof the issues share the same code repository and their dependencies may overlap, which means redundant downloads\noccur. To reduce the redundancy of repository downloads, we pre-download all the selected repository locally all at\nonce, and then replicate the downloaded repositories from local storage into containers. We also plan to similarly\nmaintain a local cache for dependencies and directly mount it to the built container to avoid repeated downloads.\nPossible compilation broken during incremental compilation. When pre-compiling dependencies to improve\ncaching, applying incremental patches can sometimes cause compilation failures. Those failures may occur when the\npatch disrupt the project's build process, especially in projects with complex dependency structures. To address\nthis, we carefully analyzed the source code and project architecture, identifying potential conflicts. We then crafted\nspecific test commands tailored to each project's unique setup, ensuring that the build process remained stable despite\nthe applied changes. This thorough process helped us maintain consistent build integrity across different environments."}, {"title": "2.2 Data Statistics", "content": "As illustrated in Figure 1, the SWE-bench-java-verified benchmark includes a total of 91 issues across\n6 popular GitHub repositories. The distribution of issues varies among these repositories, with the high-\nest concentration found in \u201cfasterxml/jackson-databind\" containing 49 issues, while \"apache/dubbo\" has\nthe fewest, with only 4 issues. These 6 repositories span various domains, including data serialization (e.g.,\n\"fasterxml/jackson-core\", \"fasterxml/jackson-dataformat-xml\"), web services (e.g., \"apache/dubbo\"),\ndata formats (e.g., \u201cgoogle/gson\u201d), and container tools (e.g., \u201cgooglecontainertools/jib\"), demonstrating the\ndataset's broad coverage. This diversity underscores the dataset's representativeness, providing a wide-ranging testbed\nfor evaluating LLMs' performance in automated issue resolving within the Java ecosystem.\""}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Setup", "content": ""}, {"title": "3.1.1 Evaluation Metrics", "content": "Following SWE-bench [1], we adopt the Resolved Rate (%) as our evaluation metric. This metric indicates the\nproportion of issues in SWE-bench-java-verified that are successfully resolved. An issue is considered resolved"}, {"title": "3.1.2 Evaluation Approaches and Models", "content": "To assess the reliability of SWE-bench-java-verified, we use SWE-agent [2], a classic approach that enhances LLM\nagents in software engineering tasks through a custom agent-computer interface (ACI). This interface empowers agents\nto autonomously create and edit code, navigate repositories, and execute tests. With its proven superior performance on\nbenchmarks like SWE-bench [1], SWE-agent is well-suited for evaluating the robustness and practical value of our\nnewly created SWE-bench-java-verified.\nNote that we implemented SWE-agent rather hastily, without configuring the runtime environment for all Java issues in\nSWE-bench-java-verified. This will affect the agent's ability to accurately reproduce issues, potentially leading to\nlower results (Section 3.2) compared to SWE-agent's normal performance.\nBased on SWE-agent, we employ several popular and powerful models, including GPT-40-2024-08-06, GPT-40-mini-\n2024-07-18, DeepSeek-Coder-V2-0724, DeepSeek-V2-0628, and Doubao-pro-128k."}, {"title": "3.2 Results", "content": "Table 2 demonstrates the varying capabilities of different models in solving SWE-bench-java-verified tasks. Com-\npared to GPT and Doubao models, DeepSeek exhibits superior problem-solving abilities. The performance of GPT-40\nsignificantly surpasses that of GPT-40-mini, demonstrating the effectiveness of the model's comprehensive capabili-\nties in problem-solving and highlighting the discriminative power of SWE-bench-java-verified in differentiating\nmodels. Overall, a greater amount of descriptive language in the issues leads to better problem-solving outcomes.\nWe observe that the more detailed the task description, the higher the requirement for the model's natural lan-\nguage understanding capability. As observed in Table 3, DeepSeek-V2 significantly outperforms DeepSeekCoder-\nV2 on the \"GoogleContainerTools/jib\" repository, which happens to have the most extensive natural lan-\nguage description among the six repositories (refer to Table1). Similar results are also reflected in the\n\"FasterXML/jackson-dataformat-xml\" repository. Conversely, in the \"FasterXML/jackson-core\" repository,\nwhich has the least textual content, DeepSeekCoder-V2 performs notably better than DeepSeek-V2. This further\ncorroborates the positive correlation between the level of detail in task descriptions and the required natural language\nunderstanding ability of the models.\nThe diverse performance of different models across SWE-bench-java-verified's various repositories underscores\nthe diversity of this benchmark. Moreover, the significant performance disparities among models indicate that the dataset\neffectively distinguishes the capability differences between models, demonstrating good sensitivity and discrimination.\nFurthermore, considering the scores achieved by the models, most tasks have not reached perfect or high scores,\nsuggesting that SWE-bench-java-verified presents significant challenges to the work in related fields while also\nproviding valuable guidance for future research. In summary, SWE-bench-java-verified reveals the limitations and\nstrengths of models from multiple perspectives, holding great significance for advancing progress in relevant domains."}, {"title": "4 Related Works", "content": "The development of benchmarks for code generation has gained significant attention in recent years due to the increasing\nprominence of LLMs in programming tasks [3, 5, 14, 15, 16]. These benchmarks serve as critical tools for evaluating the\ncapabilities of LLMs in understanding, generating, and refining code. Early efforts in this domain focused on primarily\nevaluating models in monolingual settings [17, 18, 19, 20, 21, 22]. For example, the HumanEval benchmark [19]\noffers a range of Python programming problems of varying difficulty to evaluate the functional correctness of code\ngenerated by LLMs. Similarly, the MBPP benchmark [20] is widely used to assess LLMs' performance on basic\nPython programming problems. As LLMs advanced, benchmarks became more sophisticated, evolving to better reflect\nreal-world software engineering scenarios, such as library-oriented code generation, repository-level code completion,\nand issue resolving. In the field of library-oriented code generation, several Python-specific benchmarks have emerged,\nincluding PandasEval [23], NumpyEval [23], and TorchDataEval [24]. These benchmarks assess the ability of LLMs\nto generate code that interacts effectively with popular libraries. For repository-level code generation, early works\nlike CoCoMIC [25] and RepoEval [26, 27] crafted datasets that require cross-file context to complete code, focusing\nexclusively on Python repositories. These benchmarks challenge LLMs to understand and complete code across\nmultiple files within a repository. In the field of repository-level issue resolving, SWE-bench [28] was created, including\n2, 294 software engineering problems derived from GitHub issues and corresponding pull requests across 12 Python\nrepositories. Resolving issues in SWE-bench often demands a deep understanding of the repository and the ability to\ncoordinate changes across multiple functions, classes, and files simultaneously [29, 30].\nIn addition to monolingual benchmarks, there has been a growing interest in multilingual benchmarks to assess\nthe performance of LLMs across different programming languages. For example, Multilingual-HumanEval [31]\nand HumanEval-X [32] extends HumanEval [33] by providing equivalent tasks in multiple programming languages.\nSimilarly, MBXP [34] extends the MBPP to include multilingual scenarios for more comprehensive evaluation.\nMultiPL-E [35] creates multilingual benchmarks based on HumanEval [33] and MBPP [36] in a translation-based\nway: it translates the two unit test-driven benchmarks to 18 additional programming languages. This shift towards\nmultilingualism reflects the global nature of software development and the need for LLMs to perform well across\nvarious programming contexts. Moreover, recent work has emphasized the importance of benchmarking in real-world\nsoftware engineering scenarios. The CoderEval [37] benchmark, for instance, is a context-aware benchmark for Java\nand Python that includes six levels of context dependency for code generation, such as class and file dependencies.\nAdditionally, RepoBench [38] and CrossCodeEval [39] focus on more complex, real-world, multi-file programming\ntasks and thus serve as multilingual replacements of RepoEval [26].\nDespite these advancements, aligning benchmarks with real-world programming needs remains challenging. SWE-\nbench [28] provides a benchmark for evaluation in a realistic software engineering setting for Python repositories,\nwhich is monolingual. To address this, we extend SWE-bench to include Java, creating a multilingual benchmark to\nbetter evaluate LLMs' coding abilities in real-world scenarios."}, {"title": "5 Conclusion and Future Works", "content": "This paper presents SWE-bench-java-verified, an evaluation benchmark specifically designed for resolving issues\nin Java projects. We detailed the construction process and conducted a comprehensive statistical analysis of the dataset.\nAdditionally, we have open-sourced the dataset, evaluation Docker environment, and leaderboard. In the future, we plan\nto create benchmarks for more programming languages such as Go, Rust, C, and C++, while also continuing to improve\nthe quality and coverage of the existing Java and Python datasets."}]}