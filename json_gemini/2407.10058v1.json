{"title": "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs", "authors": ["Zhenhua Liu", "Tong Zhu", "Chuanyuan Tan", "Wenliang Chen"], "abstract": "Large language models (LLMs) exhibit remarkable capabilities in understanding and generating natural language. However, these models can inadvertently memorize private information, posing significant privacy risks. This study addresses the challenge of enabling LLMs to protect specific individuals' private data without the need for complete retraining. We propose RETURN, a Real-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from Wikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods for protecting personal data in a realistic scenario. Additionally, we introduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection, which enables the model to learn which individuals' information should be protected without affecting its ability to answer questions related to other unrelated individuals. Our extensive experiments demonstrate that NAUF achieves a state-of-the-art average unlearning score, surpassing the best baseline method by 5.65 points, effectively protecting target individuals' personal data while maintaining the model's general capabilities.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) demonstrate extraordinary abilities to understand and generate natural languages following instructions, attributing to the massive amounts of parameters and training data (Brown et al., 2020; Anil et al., 2023). However, these models sometimes memorize about private contents since there are personally identifiable information in the pre-training corpus (Carlini et al., 2021; Huang et al., 2022). This presents a significant privacy concern, as an adversary can prompt the model to extract an individual's name, email address, phone number, or other sensitive information for malicious purposes, as shown in Figure 1. The General Data Protection Regulation (European Parliament and Council of the European Union, 2016) gives individuals Right To Be Forgotten (RTBF), which can limit the direct and indirect commercial use of their personal information. This situation leads us to the question: How can we enable LLMs to protect specific individual's private data to mitigate privacy risks?\nWith the costly training process of LLMs, removing all private information from the training data and retraining it from scratch is not a practical solution (Lison et al., 2021; Kandpal et al., 2022; Liu et al., 2024a). Therefore, researchers have attempted to adopt machine unlearning (MU) as an alternative, which aims to eliminate the influence of undesirable data and associated model capabilities without retraining (Cao and Yang, 2015; Bourtoule et al., 2021; Jang et al., 2022; Si et al., 2023; Zhang et al., 2023a; Maini et al., 2024; Liu et al., 2024a). To evaluate the performance of MU methods, some studies have experimented with question-answering datasets (Patil et al., 2023), fictitious biographies (Maini et al., 2024), and copyrighted contents (Eldan and Russinovich, 2023). However, there is a lack of evaluation of MU methods for protecting personal privacy data in real-world scenarios, where the target individuals exist in reality and have been memorized by LLMs.\nConsidering these problems, we propose RETURN, a Real-world personal daTa UnleaRNing dataset. As illustrated in Figure 2, we collect extensive background information on celebrities from Wikipedia and use GPT-4 (Achiam et al., 2023) to generate 20\u00d7QA pairs for each individual. After manual and automated validation, we obtain a dataset of 2,492 individuals, each with a (Name, 20\u00d7QA pairs) data instance. Next, we could select a base model to evaluate the MU methods on RETURN. In this work, we take LLaMA-3-8B-Instruct (AI@Meta, 2024) as an example. We first identify individuals with deep memorization in the model and then divide them into the forget set and the retain set. Our goal is for the model to protect the information of individuals in the forget set, ensuring that questions related to these individuals are not answered correctly, while maintaining the model's performance on the retain set.\nExisting MU methods often suffer from sensitivity to hyperparameter selection or the inability to effectively distinguish between the forget set and the retain set. To mitigate the drawbacks of existing methods, we propose a simple yet novel unlearning method: Name-Aware Unlearning Framework (NAUF) for privacy protection. The framework comprises two key components: Name-Aware Refusal Answer and Contrastive Data Augmentation. The Name-Aware Refusal Answer is designed to help the model learn which individuals' information should be protected, and the Contrastive Data Augmentation aims to expand the distribution of both the forget set and the retain set for enhancing the generalization of our method. We evaluate the effectiveness of our proposed method on RETURN and compare it with the baseline methods, and the results show that our proposed NAUF achieves a state-of-the-art average unlearning score, outperforming the best baseline method by 5.65 points.\nOur contributions can be summarized as follows:\n\u2022 We propose RETURN, which consists of 2,492 real individual names and 20\u00d7QA pairs for each individual. As far as we know, this is the first dataset for evaluating MU methods for protecting personal data in a real-world scenario.\n\u2022 We propose a simple yet novel method NAUF for privacy protection. This method could help the model protect the privacy of individuals in the forget set while maintaining the model's performance on the retain set.\n\u2022 We conduct extensive experiments on RETURN to evaluate the effectiveness of our proposed method and compare it with the baseline methods. The results show that our proposed NAUF achieves a state-of-the-art average unlearning score, outperforming the best baseline method by 5.65 points. Through comprehensive experimental analysis, we demonstrate the effectiveness of our proposed method in protecting the privacy of individuals in the forget set while maintaining the model's performance on the retain set."}, {"title": "Related Work", "content": "Memorization and Privacy Risks of LLMs.\nPrevious works show that LLMs can memorize sensitive information from the training data (Thakkar et al., 2021; Carlini et al., 2021; Huang et al., 2022). Adversaries can utilize membership inference attacks to infer whether a specific data point was in the LLMs' training set (Shi et al., 2023; Liu et al., 2024b). They can also recover the training data by powerful data extraction attacks (Carlini et al., 2021; Nasr et al., 2023). These privacy risks can be mitigated by removing the sensitive information from the LLMs. However, retraining the LLMs from scratch is impractical due to the high cost of training (Lison et al., 2021; Kandpal et al., 2022; Liu et al., 2024a). One approach to minimizing the memorization of sensitive information is to apply differential privacy techniques in model training (Dwork et al., 2006; Shokri and Shmatikov, 2015; McMahan et al., 2017). Unfortunately, these methods often reduce the accuracy and increase the training time, making them less common in practice (Jayaraman and Evans, 2019).\nMachine Unlearning for LLMs. Machine unlearning (MU) aims to eliminate the influence of undesirable data and remove associated model capabilities while preserving model performance for other data (Cao and Yang, 2015; Bourtoule et al., 2021; Jang et al., 2022; Si et al., 2023; Zhang et al., 2023a; Maini et al., 2024; Liu et al., 2024a). The study of MU methods encompasses diverse domains, such as image classification (Ginart et al., 2019; Golatkar et al., 2020; Sekhari et al., 2021; Fan et al., 2023), text-to-image generation (Kumari et al., 2023; Zhang et al., 2023b; Fan et al., 2023), and federated learning (Wang et al., 2022; Liu et al., 2023; Che et al., 2023).\nSpecifically in the era of LLMs, MU has been applied to addressing trustworthiness concerns, such as toxicity (Lu et al., 2022), copyright (Eldan and Russinovich, 2023), and privacy (Jang et al., 2022; Patil et al., 2023; Maini et al., 2024). We find that these studies have tested MU methods on question-answering datasets (Jang et al., 2022; Patil et al., 2023), fictitious biographies (Maini et al., 2024), and copyrighted contents (Eldan and Russinovich, 2023), but have not yet evaluated the methods for protecting personal privacy data in real-world scenarios. Considering the practical applications, we propose RETURN to evaluate MU methods when an individual practices his/her RTBT in a black-box setting, where adversaries can only interact with the model through API query.\nJang et al. (2022) shows that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances. Maini et al. (2024) tries to unlearn the memorized information in LLMs by relabeling the target data with uninformed answers such as \"I don't know\". We believe that these methods have their drawbacks: gradient ascent is sensitive to hyperparameters and could easily cause model training to crash; simply allowing the model to learn to respond with uninformed answers could easily affect the model's performance on the retain set. Therefore, we propose Name-Aware Unlearning Framework, to mitigate these issues and achieve a better balance between privacy protection and model performance."}, {"title": "RETURN: Real-world pErsonal daTa UnleaRNing", "content": "In order to evaluate various MU methods in a practical scenario, we propose RETURN, a Real-world personal data UnleaRNing dataset. We take Llama-3-8B-Instruct (AI@Meta, 2024) as an example to demonstrate how to use the dataset to evaluate MU methods. It is worth noting that we could use any LLM to replace Llama-3-8B-Instruct as the base model for evaluation.", "sections": [{"title": "Data Construction", "content": "We begin by leveraging PopQA (Mallen et al., 2022) to collect a large set of names of individuals. PopQA is a large-scale open-domain question-answering (QA) dataset constructed by Mallen et al. (2022), consisting of 14k entity-centric QA pairs. Each pair comes with the original [subject entity, relationship type, object entity] annotation, as well as Wikipedia monthly page views. Specifically, for the data in PopQA, we collect \"subject entity\" if the \"relationship type\" is within [ occupation, place of birth, father, mother]; and we collect \"object entity\" if the \"relationship type\" is within [ producer, director, screenwriter, composer, author].\nAfter gathering these names, we retrieve their corresponding Wikipedia pages and extract the abstracts from these pages as background information. We then filter the background information to retain only those whose word count falls between 100 and 500 words. Through this process, we ultimately obtain 2,516 records consisting of (Name, Background Information). Next, given each pair of name and the background information, we use a prompt to generate 20\u00d7QA pairs with GPT4 (Achiam et al., 2023). The prompt template is shown in Table 3 in the Appendix.\nAs shown in Table 1, after manually verifying and filtering out data with content or formatting errors, we finally obtain RETURN consisting of 2,492 (Name, 20\u00d7QA pairs). Next, we will demonstrate how to use the dataset to evaluate MU methods with LLaMA-3-8B-Instruct (AI@Meta, 2024)."}, {"title": "Identifying Individuals with Deep Memorization", "content": "To perform unlearning on LLaMA-3, we first need to identify which individuals the model has deeply memorized. We ask the model to answer the questions for each individual in RETURN, then calculate the average accuracy by comparing the model's predicted answers with the gold answers using a Natural Language Inference (NLI) model. If the prediction is \"entailment\" or \"neutral,\" we consider the model's answer correct; if the NLI model's prediction is \"contradiction,\" we consider the model's answer incorrect. The accuracy distribution of LLaMA-3 on RETURN is shown in Figure 3. The higher the accuracy, the more deeply the model memorizes the individual's information. Finally, we take 466 individuals with accuracy \u2265 0.8 as individuals with deep memorization for the subsequent unlearning experiments."}, {"title": "Evaluation Setup", "content": "We split the 466 individuals into 2 sets in a ratio of 1:9: forget set DF and retain set DR. We mark the original model as Mo and the unlearned model as Mu. We want the model to learn to protect the privacy of individuals in the forget set, ensuring that questions related to these individuals are not answered correctly, while not affecting the performance on the retain set and other tasks. Specifically, we aim for the following:\n1. For questions regarding individuals in DF, the model should not answer correctly, or refuse to respond to protect their privacy.\n2. For questions regarding individuals in DR, the model should respond normally.\n3. Meanwhile, MU methods should not affect the model's general capabilities on other tasks."}, {"title": "Evaluation Metrics", "content": "We measure MU methods' comprehensive performance using the following metrics:\nForget Score. To quantify the model's ability to protect the privacy of individuals in the forget set, we propose the Forget Score. It is calculated as the relative decrease in accuracy on DF after unlearning compared to the original model's accuracy on DF:\n$ForgetScore = \\frac{Acc_{M_O}(D_F) - Acc_{M_U}(D_F)}{Acc_{M_O}(D_F)} = 1 - \\frac{Acc_{M_U}(D_F)}{Acc_{M_O}(D_F)}$   (1)\nRetain Score. To quantify the model's ability to retain the performance on the retain set after unlearning, we propose the Retain Score. It is calculated as the ratio of the unlearned model's accuracy on DR to the original model's accuracy on DR:"}]}, {"title": "Machine Unlearning for LLMs", "content": "$RetainScore = \\frac{Acc_{M_U}(D_R)}{Acc_{M_O}(D_R)}$   (2)\nDownstream Task Accuracy. To quantify the influence of unlearning on the model's general capabilities, we evaluate the model on 5 downstream natural language processing tasks: WinoGrande (Sakaguchi et al., 2021), PIQA (Bisk et al., 2020), LogiQA (Liu et al., 2020), LAMBADA (Paperno et al., 2016), and ARC-c (Clark et al., 2018). We use the accuracy of the downstream tasks as the evaluation metric."}, {"title": "Machine Unlearning for LLMs", "content": "In this section, we introduce MU baselines. Then we propose a novel method for mitigating privacy risks in LLMs: Name-Aware Unlearning Framework for Privacy Protection.", "sections": [{"title": "Baseline Methods", "content": "A typical MU method often have two parts: unlearning on the forget set and regularization on the retain set. These two types of loss can be used in any combination."}, {"title": "Unlearning on Forget Set", "content": "Gradient Ascent. Gradient ascent (GA) stands as the most straightforward method for unlearning, which is simply performing gradient ascent on the loss over forget set. GA is to minimize the likelihood of correct predictions on the forget set, denoted as:\n$L_{GA}(D_F,M_U) = \u2013 E_{(x,y)\u223cD_F} [-log(M_U(y|x))] = E_{(x,y)\u223cD_F} [log(M_U(y|x))]$   (3)\nNegative Preference Optimization. Zhang et al. (2024) proposed Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. The loss function of NPO is defined as:\n$L_{NPO}(D_F,M_U,M_O) = E_{(x,y)\u223cD_F} [log(1 + (\\frac{M_O(y|x)}{M_U(y|x)})^2)]$   (4)\nRelabeled Gradient Descent. A variant of GA is to transform it into a gradient descent approach, which aims to maximize the likelihood of predictions on relabeled forget set. Following Maini et al. (2024), we relabel the question in the forget set with an uninformed answer like \"I don't know.\" (or any one of 100 versions of this response, we name the uninformed answer set as Didk). The loss function of Relabeled Gradient Descent (RGD) is defined as:\n$L_{RGD}(D_F,M_U) = -E_{(x,y)\u223cD_F,y_{idk}\u223cDidk} [log(M_U(y_{idk}|x))]$   (5)\nRelabeled Direct Preference Optimization. Direct Preference Optimization (DPO) seeks to fine-tune the model with human preferences (Rafailov et al., 2024). We take the uninformed answer from Didk as preferred answer, the gold answer as the dispreferred answer. The loss function of Relabeled Direct Preference Optimization (RDPO) is defined as:\n$L_{RDPO}(D_F, M_U, M_O) = -E_{(x,y)\u223cD_F, y_{idk}\u223cDidk} [log \u03c3(\u03b2 log \\frac{M_U(y|x)}{M_O(y|x)} - \u03b2 log \\frac{M_U(y_{idk}|x)}{M_O(y_{idk}|x)})]$   (6)"}, {"title": "Regularization on Retain Set", "content": "MU methods should not only protect the privacy of individuals in the forget set but also maintain the model's performance on the retain set. Regularization methods are designed to achieve this goal. If we only fine-tune the model to maximize the likelihood of the uninformed answer on the forget set, the model may also refuse to answer the questions on the retain set. To achieve a balance between the forget set and the retain set, there are two regularization methods:\nGradient Descent Regularization. Simply performing gradient descent (GD) on the loss over the retain set. The loss function is defined as:\n$L_{GD}(D_R,M_U) = -E_{(x,y)\u223cD_R} [log(M_U(y|x))]$   (7)\nKullback-Leibler Divergence Regularization. Minimizing the Kullback-Leibler divergence (KLD) between the predictions on the retain set of the original model and the unlearned model. The loss function is defined as:\n$L_{KL}(D_R,M_U,M_O) = E_{(x,y)\u223cD_R} [KL(M_O(y|x)||M_U(y|x))]$   (8)\nConsidering a computing budget that scales with the size of the forget set, we randomly sample an example from DR every time we see an example from DF to stay within the constraints following Maini et al. (2024)."}]}, {"title": "Name-Aware Unlearning Framework", "content": "In our pilot study, we find that RGD achieves the comparatively best performance in protecting the privacy of individuals in the forget set. However, the model's performance on the retain set is significantly affected. The model tends to refuse to answer the questions on the retain set, which is not desirable. To address this issue, we propose a novel method: Name-Aware Unlearning Framework (NAUF) for privacy protection. The framework comprises two key components: Name-Aware Refusal Answer and Contrastive Data Augmentation.\nName-Aware Refusal Answer. First, we relabel the questions in the forget set with a name-aware refusal answer, such as \"I'm afraid I can't help with inquiries about NAME.\" Then we could perform gradient ascent on the loss over the relabeled forget set. The name-aware refusal answer is designed to help the model learn which individuals' information should be protected. We curate 100 name-aware refusal answer templates $D_{refuse}$ using GPT-4, and some examples are shown in Table 4 in the Appendix.\nContrastive Data Augmentation. In addition, given the limited number of QA pairs for each individual, we propose contrastive data augmentation (CDA) to enhance the generalization of unlearning. Specifically:\n\u2022 For each individual in the forget set, we randomly sample questions from other individuals in the forget set and replace the name with the target individual's name. Then relabel the questions with the name-aware refusal answer.\n\u2022 For each individual in the retain set, we also randomly sample questions from other individuals in the forget set and replace the name with the target individual's name, but we use the original model's prediction for that question as the relabeled answer.\nThis contrastive data augmentation strategy expands the distribution of both the forget set and the retain set. For simplicity, we expand the forget set and the retain set by doubling the amount of data."}, {"title": "Experiments", "content": "Implementation Details\nDue to the limited training data available for unlearning, we aim to use this limited data to teach the model to protect all privacy information of the target individuals, which places stricter requirements on the generalization capability of the MU methods. Considering this situation, we divide the QA pairs for each individual in the forget set and retain set into train and test sets in a ratio of 1:1, as well as $D^{train}_F, D^{test}_F, D^{train}_R, and D^{test}_R$. We use $D^{train}_F and D^{train}_R$ to perform unlearning on the model and then evaluate each MU method on $D^{test}_F and D^{test}_R$.\nThe $\u03b2$ for NPO and RDPO is set to 0.1. We use the AdamW optimizer with a learning rate of 1e-5 for all experiments. We set the batch size to 32 and train the model for 5 epochs. Considering the computational budget, we constraint that the number of samples used from the retain set is equal to the number of the entire forget set in each epoch. All experiments are conducted with 2 NVIDIA A100 (40GB) GPUs."}, {"title": "Main Results", "content": "We present the main results of the experiments in Table 2. We report the average unlearning score and average downstream task accuracy to evaluate the overall performance of the model.\nThe results show that our proposed NAUF with KLD regularization achieves a state-of-the-art average unlearning score, outperforming the best baseline method (RGD with GD regularization) by 5.65 points. The GA method performs the worst on our dataset, and the unlearned model generates meaningless predictions for questions in the forget set and significantly impacts the retain score and the performance on downstream tasks. The decline in the retain score and the performance on downstream tasks is mitigated to some extent only when using GD regularization."}, {"title": "Analysis", "content": "Importance of Regularization on Retain Set.\nWithout regularization on retain set, the average unlearning score of all methods except GA is around 50 points, and the average downstream task accuracy is also affected to varying degrees. With any regularization, the unlearned model performs well on downstream tasks with any MU method, showing performance close to the original model. This indicates that regularization on the retain set can effectively protect the model's general capabilities.\nThe experimental results indicate that our method, when using GD regularization, achieves similar forget and retain scores, with a difference of only 5 points between them. In contrast, when using KLD regularization, the forget score reaches 93.69, but the retain score is only 67.82, resulting in a difference of 26 points. This demonstrates that GD regularization can achieve a better balance between unlearning metrics.\nImportance of Contrastive Data Augmentation. To analyze the importance of CDA, we evaluate the performance of our unlearning framework without this component. The results are presented in Table 2. We find that without regularization, CDA has almost no effect. However, it can improve our method's forget score by 10 points when using the GD regularization. With the KLD regularization, it can increase the retain score by 4 points while maintaining a similar forget score. Notably, our method without CDA also achieves a competitive (with GD regularization) or better (with KLD regularization) average unlearning score compared to the baseline methods, which demonstrates the effectiveness of the name-aware refusal answer. These findings indicate that CDA can enhance performance on the forget set or retain set depending on the regularization method used, thereby enhancing the generalization of our proposed unlearning framework.\nUnlearning Performance across Different Numbers of Epochs. We investigate the impact of the number of unlearning epochs on the performance of MU methods. Specifically, We evaluate RGD and NAUF with 1, 3, 5, and 10 epochs, and the results are shown in Figure 4. For the Forget Score, our method with KLD regularization demonstrates relatively stable performance across different epochs. With GD regularization, the Forget Score improves as the number of epochs increases. Conversely, for the Retain Score, our method with GD regularization shows little variation across epochs, while KLD regularization leads to a gradual improvement in the Retain Score with increasing epochs. Our method's average unlearning score improves with an increasing number of epochs, while RGD shows little to no improvement from the 5 to the 10 epoch, which indicates our method still has room for further optimization.\nAverage Unlearning Score vs Average Downstream Task Accuracy across Different Numbers of Epochs. We analyze the relationship between the average unlearning score and the average downstream task accuracy across different numbers of epochs. We choose RGD and NAUF with KLD regularization for this analysis, and the results are shown in Figure 5. We observe that as the number of epochs increases, both the average unlearning score and the average downstream task accuracy increase proportionally. However, our method surpasses RGD in all aspects after just 3 epochs. Additionally, from the 5 to the 10 epoch, RGD shows a decline in average downstream task accuracy without any significant improvement in the average unlearning score. In contrast, our method continues to achieve higher average unlearning scores at the 10 epoch while maintaining stable average downstream task accuracy."}, {"title": "Conclusion", "content": "In this work, we introduce RETURN, a novel benchmark designed to evaluate MU methods for protecting personal data in a real-world scenario. We also present the Name-Aware Unlearning Framework (NAUF), which integrates Name-Aware Refusal Answer and Contrastive Data Augmentation to enhance the generalization of unlearning methods. Our experimental results show that NAUF not only effectively protects the privacy of individuals in the forget set but also maintains the performance of the model on the retain set, achieving an average unlearning score that outperforms the best baseline method by 5.65 points. These findings underscore the potential of NAUF to advance privacy protection in large language models."}, {"title": "Limitations", "content": "The proposed NAUF method is designed for individual-level privacy protection. Our goal is to protect all information about an individual, ensuring that the model refuses to answer any questions related to that individual. However, this method does not provide fine-grained protection of the target individual's information. In other words, it cannot distinguish between questions that can be answered and those that are too sensitive to answer. Future work will explore how to align the model with human judgment, enabling it to discern which personal information can be publicly discussed and which information, potentially susceptible to malicious use, should be protected."}]}