{"title": "CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization", "authors": ["Yang Zhao", "Di Huang", "Chongxiao Li", "Pengwei Jin", "Ziyuan Nan", "Tianyun Ma", "Lei Qi", "Yansong Pan", "Zhenxing Zhang", "Rui Zhang", "Xishan Zhang", "Zidong Du", "Qi Guo", "Xing Hu", "Yunji Chen"], "abstract": "The increasing complexity and high costs associated with modern processor design have led to a surge in demand for processor design automation. Instruction-tuned large language models (LLMs) have demonstrated remarkable performance in automatically generating code for general-purpose programming languages like Python. However, these methods fail on hardware description languages (HDLs) like Verilog due to the scarcity of high-quality instruction tuning data, as even advanced LLMs like GPT-3.5 exhibit limited performance on Verilog generation. Regarding this issue, we observe that (1) Verilog code collected from the real world has higher quality than those generated by LLMs. (2) LLMs like GPT-3.5 excel in summarizing Verilog code rather than generating it. Based on these observations, this paper introduces CodeV, a series of open-source instruction-tuned Verilog generation LLMs. Instead of generating descriptions first and then getting the corresponding code from advanced LLMs, we prompt the LLM with Verilog code and let the LLM generate the corresponding natural language description by multi-level summarization. Experimental results show that CodeV relatively surpasses the previous open-source SOTA by 14.4% (BetterV in VerilogEval) and 11.3% (RTLCoder in RTLLM) respectively, and also relatively outperforms previous commercial SOTA GPT-4 by 22.1% in VerilogEval.", "sections": [{"title": "Introduction", "content": "The design flow of processors is expensive and complex, especially the coding of the hardware description language (HDL) like Verilog. Researchers have therefore explored ways to automate this process. Traditional methods include agile hardware development like Chisel [3], and high-level synthesis (HLS), which compiles high-level programming languages such as C/C++ to Verilog [8, 20]. Recently, the rise of large language models (LLMs) has achieved excellent performance in text-to-code tasks for general-purpose programming languages (e.g., Python) [29, 31, 33, 34, 51, 53, 56] and shows a new perspective on automatic Verilog generation from text or natural language descriptions.\nA promising approach for adapting LLMs to Verilog generation is instruction tuning, which has been widely adopted in code generation [10, 29, 31, 41, 51, 53, 56]. Instruction tuning constructs high-quality datasets consisting of instruction-response (i.e. description-code) pairs and then fine-tunes LLMs on them. These datasets are constructed either from human-crafted resources or"}, {"title": "Related Work", "content": null}, {"title": "LLM-based Verilog Generation", "content": "Models for verilog generation. Thakur et al. [43] collected approximately 400MB of Verilog files from GitHub and textbooks to fine-tune CodeGen-16B [32]. However, the dataset was not refined, resulting in a blend of text and code, with some Verilog code potentially containing syntax errors. ChipNemo [23] used LLaMA2 [46] as its base model, performing domain-adaptive pre-training on public datasets and NVIDIA's internal chip design files (including HDL code, EDA scripts, and configuration files, etc.), and fine-tuning on public instruction datasets and an expert-crafted domain dataset with 1430 examples. However, likely due to the dataset not being refined for Verilog generation tasks, the 70B version model of ChipNemo even underperforms its base model on the VerilogEval-Human benchmark. RTLCoder [25] uses GPT to synthesize instruction-code pairs, starting by generating instructions based on a keywords pool and a source code pool, then mutating these instructions to increase their number and diversity, and finally having GPT generate Verilog code based on these synthesized instructions to form a fine-tuning dataset. This approach may lead to unrealistic instructions, and the code quality is limited by GPT's generation ability. BetterV [35] collects Verilog code from GitHub and expands its dataset by translating Verilog code into C to create C-Verilog pairs, leveraging the base model's knowledge in C to enhance Verilog generation capabilities. However, they did not validate the effectiveness of this approach by experiments. In contrast to these methods, CodeV employs a novel dataset construction approach, utilizing multi-level summarization to generate high-quality instruction tuning datasets from high-quality Verilog code, achieving SOTA results.\nFrameworks on verilog generation. Instead of training specific models, some work focuses on designing generation frameworks for Verilog tasks to improve general-purpose LLMs' performance."}, {"title": "Instruction Tuning", "content": "Instruction tuning has emerged as a powerful strategy to enhance the capabilities of LLMs by fine-tuning them with a mixture of instructions and corresponding responses. Initially explored through models like T5 [36], which was trained on a diverse array of public NLP tasks, this approach has seen significant advancements over time. Notable developments include FLAN [50], ExT5 [1], TO [38], and UnifiedQA [19], which expanded the range of tasks to improve cross-task generalization abilities of LLMs.\nUsing human annotators, OpenAI has developed models such as InstructGPT [34] and ChatGPT by training on a large corpus of human-generated instructions, aligning the model outputs more closely with real-world user intentions. This dataset, however, remains proprietary and not publicly accessible. Meanwhile, Vicuna [12] has explored using user-shared conversations from platforms like ShareGPT.com for training, illustrating a community-driven approach to data collection.\nWithout human annotators, SELF-INSTRUCT [47] utilize foundation LLMs like GPT-3 [7] to generate synthetic instruction-response pairs. This methodology allowed the same LLM to be instruction-tuned on its self-generated data, thereby distilling knowledge from advanced LLMs without external inputs. Following this, models like Alpaca [42] and Code Alpaca [10] applied SELF-INSTRUCT to fine-tune Llama [44] using ChatGPT-generated instructions. Furthermore, WizardLM [52] introduced Evol-Instruct to utilize heuristic prompts to guide ChatGPT, creating more complex and diverse datasets.\nFor code generation, WizardCoder [29] introduced Code Evol-Instruct to adapt Evol-Instruct to coding problems. Codellama-Instruct [37] are finetuned from Codellama using SELF-INSTRUCT by prompting Llama 2 [45] for coding problems and then ask Codellama for unit tests filtering and code solutions. OctoPack [31] collects open-source commit information as a data source for instruction tuning. Taking into account the scenario of modifying code through multiple rounds of interaction, OpenCodeInterpreter [56] constructs a multi-turn instruction tuning dataset by using GPT-3.5/GPT-4 to simulate human feedback and incorporating execution feedback. Magicoder [51] integrates real-world code snippets into the instruction generation process to enhance the controllability, quality, and creativity in synthetic coding tasks.\nHowever, due to the high cost of annotation and the generally poor performance of advanced LLMs like GPT-4, these methods fail on the task of Verilog generation. Our work aims to fill this gap using a reverse instruction method."}, {"title": "Code Generation Models for General-purposed Language", "content": "LLMs trained on extensive codebases have proven to be highly effective in code generation. Among them, closed-source models including Codex by OpenAI [11], PaLM-Coder by Google [13], and Code-Davinci by Microsoft [30] have demonstrated remarkable capabilities in popular code generation benchmarks like HumanEval [11] and MBPP [2]. On the other hand, there are also several notable open-source works like InCoder [15], CodeGen series [32], CodeT5 series [48, 49], CodeGeeX [55], StarCoder series [21, 27], CodeLlama-Series [37], PanGu-Coder series [14, 40], AIXCoder [17], Deepseek-Coder [16], CodeQwen [4]. Although these models perform well with general-purpose programming languages, they perform poorly with HDL like Verilog due to a lack of training data. To address this issue, our work proposes an instruction tuning method for fine-tuning these models on Verilog generation and introduces CodeV, a series of SOTA Verilog generation models."}, {"title": "Methods", "content": null}, {"title": "High-quality Verilog Code Collection", "content": "High-quality real-world Verilog code is the foundation of our approach. We first crawl Verilog and SystemVerilog files from GitHub, ensuring each file contains a complete module, i.e. contains at least one \"module\" and \"endmodule\" pair. Files with external references, i.e. code containing \"include\" or \"import\" keywords, are excluded. This is because reconstructing these reference relationships from the crawled files is challenging, and we do not want LLM to generate these non-self-contained Verilog codes after training. Then we deal with the comments interspersed throughout the files. These comments can be categorized into two types: those related to Verilog implementation, such as functional descriptions, test cases, etc.; and those unrelated to implementation, such as licenses, company and author information, logs, etc. The former is essential for LLMs to understand the code and should be kept, while the latter should be avoided. Therefore, we use regular expressions to delete these implementation-unrelated comments. Next, files exceeding 2096 characters are deleted, as longer files would be truncated during training, resulting in incomplete file content and degrading the dataset quality. Following Thakur et al. [43], we also use MinHash and Jaccard similarity to filter out duplicate code. Finally, we run syntax checks on all Verilog code and remove any that fail the check, to further enhance the quality of the dataset.\nThrough the above-mentioned effort, we obtain high-quality real-world Verilog code. Next, we will construct requirement descriptions for them."}, {"title": "Multi-level Code Summarization", "content": "Manual annotation is prohibitively time-consuming and costly. Hence, we employed GPT-3.5 to generate high-level summaries for each Verilog module as its requirement description. As analyzed in VerilogEval [24], when required for summarising, LLMs often produce verbose descriptions, preferring line-by-line explanations over high-level summaries. To address this issue, we introduce a multi-level summarization method, employing few-shot learning [6] to guide GPT-3.5 in first producing detailed descriptions and then abstracting high-level summaries."}, {"title": "Data Decontamination", "content": "To ensure a fair evaluation, we conduct decontamination for the training data to remove sequences similar to benchmarks. Specifically, we use Rouge-L, a widely used similarity metric, to measure the similarity between data. Rouge-L \u2208 [0, 1], with higher values indicating greater similarity. Following RTLCoder [26], we remove data from the training set that have a Rouge-L score greater than 0.5 with any of the benchmarks, thereby preventing test data leakage."}, {"title": "Supervised Finetuning", "content": "Finally, we finetune base models on our dataset. Given a natural language description, the LLM with parameter \\(\\theta\\) generates code repeatedly drawing from \\(p_{\\theta}(x_t|x_{<t})\\) and using \\(x_t\\) as part of the input for the subsequent prediction. Following prior work, the LLM is designed to reduce the negative log-likelihood across the dataset \\(D = \\{x_1,...,x_{|D|}\\}\\) [18]:\n\\[\\mathcal{L} = - \\frac{1}{|D|} \\sum_{i=1}^{|D|} \\sum_{t=1}^{n} \\log p_{\\theta}(x_t | x_{<t})\\]."}, {"title": "Experiments", "content": "We conduct comprehensive experiments demonstrating that CodeV achieves SOTA results across diverse Verilog generation tasks, and we further analyze CodeV's performance as detailed below."}, {"title": "Experimental Setup", "content": null}, {"title": "Instruction-tuning", "content": "We used CodeLlama-7b-Instruct [37], Deepseek-Coder-6.7b-Instruct [16], and CodeQwen1.5-7B-Chat [4] as our base models. We fine-tuned these models for 3 epochs using PyTorch's Distributed Data Parallel (DDP) on 4 NVIDIA A100-80GB SMX GPUs. We employed the Adafactor optimizer [39] with a linear learning rate scheduler, setting the initial learning rate at \\(5e - 5\\) with 15 warm-up steps. We set global batch size 256, with max sequence length 2048."}, {"title": "Benchmarks and Metrics", "content": "We conduct comprehensive evaluations on the VerilogEval [24] and RTLLM [28] benchmarks.\nVerilogEval includes two parts: VerilogEval-machine and VerilogEval-human, with 143 GPT-generated and 156 hand-crafted Verilog generation problems, respectively. We follow the VerilogEval paper, using the pass@k metric to measure the Verilog generation accuracy. The pass@k metric estimates the proportion of problems that can be solved at least once in k attempts:\n\\[pass@k := E_{\\text{problems}} \\left[1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}}\\right]\\,\nwhere \\(n \\geq k\\) represents the total number of trials for each problem, and \\(c\\) represents the number of trials that pass the functional check. We set \\(n = 20\\) according to the original settings."}, {"title": "Baseline Methods", "content": "In our experiment, we compare our results with baseline methods, including: (1) foundation models not tailored for Verilog generation tasks, including closed-source commercial LLMs such as GPT-3.5 and GPT-4, and four open-source models specialized in code generation tasks: Starcoder [21], CodeLlama-7b-Instruct [37], DeepSeek-Coder-6.7b-Instruct [16], and CodeQwen1.5-7B-Chat [4]; (2) models fine-tuned for Verilog generation tasks, including Thakur et al. [43], RTLCoder [25] and BetterV [35]."}, {"title": "Main Results", "content": "Table 1 compares the main results of our CodeV with baseline methods on the VerilogEval and RTLLM benchmarks. We test CodeLlama, DeepSeek-Coder, and CodeQwen on RTLLM, while other baseline results are sourced from RTLCoder [26] or BetterV [35] paper. For a fair comparison, we also evaluate our models trained on comparable-size datasets against RTLCoder. The results show that:\nCodeV achieves SOTA on both benchmarks. In the VerilogEval benchmark, CodeV outperforms all previous baseline methods across all metrics, particularly in the most challenging pass@1 metric where CodeV-CodeLlama achieves 78.1% in VerilogEval-machine, and CodeV-CodeQwen achieves 53.1% in VerilogEval-human, significantly outperforming both GPT-4 and BetterV-CodeQwen. In the RTLLM v1.1 benchmark, CodeV-CodeLlama nearly matches GPT-4's functional check success rate, failing in just 1 more case out of 29 circuits compared to GPT-4, and significantly outperforms all other models. These results demonstrate that, contributed by a high-quality instruction tuning dataset, CodeV exhibits significant superiority in Verilog generation tasks."}, {"title": "Ablation Study", "content": "In this section, we measured the effectiveness of multi-level summarization. Due to budget constraints, we conducted this experiment only on a 10K dataset. We sampled 10K data from the multi-level summarization dataset and generated single-level summarization (i.e. summarize directly) for the same Verilog code. The results in Table 3 show that the instruction data obtained through multi-level summarization can effectively enhance model performance which is consistent with our expectations."}, {"title": "Further Analysis", "content": null}, {"title": "Evaluating RTLLM with pass@k metric", "content": "As shown in Eq. 2, the pass@k metric provides more unbiased estimations. In this section, we set \\(n = 20\\) to further evaluate our models on RTTLM using pass@k metrics and conduct varying comparisons with baselines. It is noteworthy that, even with 5 trials for each problem, the success rate metric in original RTLLM settings does not equal pass@5 metric, hence one cannot directly"}, {"title": "Impact of Data Size Scaling", "content": "We randomly sampled subsets of various sizes (27K, 50K, 100K) from the complete 165K dataset and fine-tuned all three baseline models, reporting their evaluation results on VerilogEval as shown in Figure 5. Except for CodeV-CodeQwen on VerilogEval-machine, which peaks on 100K-size, we observed that model performance generally improves as training set size increases, but with diminishing returns. This suggests that for 7B models, the benefits of further increasing the training set size may be limited, and future efforts should focus more on enhancing the quality of the dataset."}, {"title": "Conclusion", "content": "In this paper, we present CodeV, a series of Verilog generation LLMs. By collecting Verilog code from the real world and utilizing multi-level summarization to generate high-level descriptions, we successfully construct a high-quality dataset and train multiple CodeV models based on it. Our experimental results have shown CodeV's effectiveness, showcasing superior performance over both previous open-source and commercial state-of-the-art models by significant margins. Specifically, CodeV relatively exceeds GPT-4 in VerilogEval by 22.1%. This highlights its practical utility in the domain of circuit design automation."}, {"title": "Limitations", "content": "Our work has two main limitations: first, CodeV is unable to directly generate complex circuits without the assistance of a framework, such as the one proposed by Lu et al. [28]; second, CodeV"}, {"title": "Broader Impacts", "content": "Compared to existing models, CodeV has the highest accuracy in Verilog generation tasks. This demonstrates that our proposed instruction tuning dataset construction method and CodeV can further push the boundaries of circuit design automation and lower the barrier to circuit design.\nHowever, like other code models, CodeV may generate code that does not align with user intentions and could be misused. On this point, we refer readers to the comprehensively discussed broader impacts and hazard analysis in Chen et al. [11]. Due to the lower usage threshold of code LLMs compared to programming languages, there may be certain security issues. Therefore, we recommend users conduct a thorough functionality verification and security review of the code generated by CodeV to avoid potential losses."}, {"title": "Additional Experimental Results", "content": null}, {"title": "Impact of Generation Temperature", "content": "Figure 6 further analyzes the VerilogEval results of our models when setting on different generation temperatures. Generally, as the temperature increases, the pass@1 metric decreases while pass@5 and pass @ 10 increase. This is because when the temperature rises, the model tends to generate more uncertain and diverse responses, which has two effects: on the one hand, uncertain responses are"}, {"title": "Instructions token count distributions with and without multi-level summarization", "content": "Figure 7 illustrates the token count distributions of 10K descriptions generated by GPT-3.5, with or without multi-level summarization, and that of the corresponding code. It shows that employing multi-level summarization results in descriptions with fewer tokens, exhibiting a more left-skewed distribution compared to those without summarization, which aligns better with actual Verilog design tasks, demonstrating the effectiveness of our approach."}, {"title": "The field distribution of our dataset.", "content": "We sampled 27K data and utilized GPT to analyze the distribution of Verilog code within our dataset and depict the statistics in Figure 8. It is evident that the Verilog code we collected covers a wide range of fields, including signal processing circuits, and arithmetic circuits, among others, reflecting the high quality of our data set."}, {"title": "Dataset Examples From the Study", "content": "There are some examples given by CodeV in Table 5."}]}