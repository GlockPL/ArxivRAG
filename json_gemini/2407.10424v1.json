{"title": "CodeV: Empowering LLMs for Verilog Generation\nthrough Multi-Level Summarization", "authors": ["Yang Zhao", "Di Huang", "Chongxiao Li", "Pengwei Jin", "Ziyuan Nan", "Tianyun Ma", "Lei Qi", "Yansong Pan", "Zhenxing Zhang", "Rui Zhang", "Xishan Zhang", "Zidong Du", "Qi Guo", "Xing Hu", "Yunji Chen"], "abstract": "The increasing complexity and high costs associated with modern processor design\nhave led to a surge in demand for processor design automation. Instruction-tuned\nlarge language models (LLMs) have demonstrated remarkable performance in\nautomatically generating code for general-purpose programming languages like\nPython. However, these methods fail on hardware description languages (HDLs)\nlike Verilog due to the scarcity of high-quality instruction tuning data, as even\nadvanced LLMs like GPT-3.5 exhibit limited performance on Verilog generation.\nRegarding this issue, we observe that (1) Verilog code collected from the real world\nhas higher quality than those generated by LLMs. (2) LLMs like GPT-3.5 excel in\nsummarizing Verilog code rather than generating it. Based on these observations,\nthis paper introduces CodeV, a series of open-source instruction-tuned Verilog\ngeneration LLMs. Instead of generating descriptions first and then getting the\ncorresponding code from advanced LLMs, we prompt the LLM with Verilog code\nand let the LLM generate the corresponding natural language description by multi-\nlevel summarization. Experimental results show that CodeV relatively surpasses\nthe previous open-source SOTA by 14.4% (BetterV in VerilogEval) and 11.3%\n(RTLCoder in RTLLM) respectively, and also relatively outperforms previous\ncommercial SOTA GPT-4 by 22.1% in VerilogEval.", "sections": [{"title": "Introduction", "content": "The design flow of processors is expensive and complex, especially the coding of the hardware\ndescription language (HDL) like Verilog. Researchers have therefore explored ways to automate this\nprocess. Traditional methods include agile hardware development like Chisel [3], and high-level\nsynthesis (HLS), which compiles high-level programming languages such as C/C++ to Verilog [8, 20].\nRecently, the rise of large language models (LLMs) has achieved excellent performance in text-to-\ncode tasks for general-purpose programming languages (e.g., Python) [29, 31, 33, 34, 51, 53, 56] and\nshows a new perspective on automatic Verilog generation from text or natural language descriptions.\nA promising approach for adapting LLMs to Verilog generation is instruction tuning, which has\nbeen widely adopted in code generation [10, 29, 31, 41, 51, 53, 56]. Instruction tuning constructs\nhigh-quality datasets consisting of instruction-response (i.e. description-code) pairs and then fine-\ntunes LLMs on them. These datasets are constructed either from human-crafted resources or"}, {"title": "Related Work", "content": ""}, {"title": "LLM-based Verilog Generation", "content": "Models for verilog generation. Thakur et al. [43] collected approximately 400MB of Verilog files\nfrom GitHub and textbooks to fine-tune CodeGen-16B [32]. However, the dataset was not refined,\nresulting in a blend of text and code, with some Verilog code potentially containing syntax errors.\nChipNemo [23] used LLaMA2 [46] as its base model, performing domain-adaptive pre-training\non public datasets and NVIDIA's internal chip design files (including HDL code, EDA scripts,\nand configuration files, etc.), and fine-tuning on public instruction datasets and an expert-crafted\ndomain dataset with 1430 examples. However, likely due to the dataset not being refined for Verilog\ngeneration tasks, the 70B version model of ChipNemo even underperforms its base model on the\nVerilogEval-Human benchmark. RTLCoder [25] uses GPT to synthesize instruction-code pairs,\nstarting by generating instructions based on a keywords pool and a source code pool, then mutating\nthese instructions to increase their number and diversity, and finally having GPT generate Verilog\ncode based on these synthesized instructions to form a fine-tuning dataset. This approach may lead\nto unrealistic instructions, and the code quality is limited by GPT's generation ability. BetterV [35]\ncollects Verilog code from GitHub and expands its dataset by translating Verilog code into C to\ncreate C-Verilog pairs, leveraging the base model's knowledge in C to enhance Verilog generation\ncapabilities. However, they did not validate the effectiveness of this approach by experiments. In\ncontrast to these methods, CodeV employs a novel dataset construction approach, utilizing multi-level\nsummarization to generate high-quality instruction tuning datasets from high-quality Verilog code,\nachieving SOTA results."}, {"title": "Frameworks on verilog generation", "content": "Instead of training specific models, some work focuses on\ndesigning generation frameworks for Verilog tasks to improve general-purpose LLMs' performance."}, {"title": "Instruction Tuning", "content": "Instruction tuning has emerged as a powerful strategy to enhance the capabilities of LLMs by fine-\ntuning them with a mixture of instructions and corresponding responses. Initially explored through\nmodels like T5 [36], which was trained on a diverse array of public NLP tasks, this approach has seen\nsignificant advancements over time. Notable developments include FLAN [50], ExT5 [1], \u03a4\u039f [38],\nand UnifiedQA [19], which expanded the range of tasks to improve cross-task generalization abilities\nof LLMs.\nUsing human annotators, OpenAI has developed models such as InstructGPT [34] and ChatGPT\nby training on a large corpus of human-generated instructions, aligning the model outputs more\nclosely with real-world user intentions. This dataset, however, remains proprietary and not publicly\naccessible. Meanwhile, Vicuna [12] has explored using user-shared conversations from platforms\nlike ShareGPT.com for training, illustrating a community-driven approach to data collection.\nWithout human annotators, SELF-INSTRUCT [47] utilize foundation LLMs like GPT-3 [7] to generate\nsynthetic instruction-response pairs. This methodology allowed the same LLM to be instruction-\ntuned on its self-generated data, thereby distilling knowledge from advanced LLMs without external\ninputs. Following this, models like Alpaca [42] and Code Alpaca [10] applied SELF-INSTRUCT to\nfine-tune Llama [44] using ChatGPT-generated instructions. Furthermore, WizardLM [52] introduced\nEvol-Instruct to utilize heuristic prompts to guide ChatGPT, creating more complex and diverse\ndatasets.\nFor code generation, WizardCoder [29] introduced Code Evol-Instruct to adapt Evol-Instruct to\ncoding problems. Codellama-Instruct [37] are finetuned from Codellama using SELF-INSTRUCT by\nprompting Llama 2 [45] for coding problems and then ask Codellama for unit tests filtering and code\nsolutions. OctoPack [31] collects open-source commit information as a data source for instruction\ntuning. Taking into account the scenario of modifying code through multiple rounds of interaction,\nOpenCodeInterpreter [56] constructs a multi-turn instruction tuning dataset by using GPT-3.5/GPT-4\nto simulate human feedback and incorporating execution feedback. Magicoder [51] integrates real-\nworld code snippets into the instruction generation process to enhance the controllability, quality, and\ncreativity in synthetic coding tasks.\nHowever, due to the high cost of annotation and the generally poor performance of advanced LLMs\nlike GPT-4, these methods fail on the task of Verilog generation. Our work aims to fill this gap using\na reverse instruction method."}, {"title": "Code Generation Models for General-purposed Language", "content": "LLMs trained on extensive codebases have proven to be highly effective in code generation. Among\nthem, closed-source models including Codex by OpenAI [11], PaLM-Coder by Google [13], and\nCode-Davinci by Microsoft [30] have demonstrated remarkable capabilities in popular code genera-\ntion benchmarks like HumanEval [11] and MBPP [2]. On the other hand, there are also several notable\nopen-source works like InCoder [15], CodeGen series [32], CodeT5 series [48, 49], CodeGeeX [55],\nStarCoder series [21, 27], CodeLlama-Series [37], PanGu-Coder series [14, 40], AIXCoder [17],\nDeepseek-Coder [16], CodeQwen [4]. Although these models perform well with general-purpose\nprogramming languages, they perform poorly with HDL like Verilog due to a lack of training data.\nTo address this issue, our work proposes an instruction tuning method for fine-tuning these models\non Verilog generation and introduces CodeV, a series of SOTA Verilog generation models."}, {"title": "Methods", "content": ""}, {"title": "High-quality Verilog Code Collection", "content": "High-quality real-world Verilog code is the foundation of our approach. We first crawl Verilog and\nSystemVerilog files from GitHub, ensuring each file contains a complete module, i.e. contains at least\none \"module\" and \"endmodule\" pair. Files with external references, i.e. code containing \"include\"\nor \"import\" keywords, are excluded. This is because reconstructing these reference relationships\nfrom the crawled files is challenging, and we do not want LLM to generate these non-self-contained\nVerilog codes after training. Then we deal with the comments interspersed throughout the files.\nThese comments can be categorized into two types: those related to Verilog implementation, such\nas functional descriptions, test cases, etc.; and those unrelated to implementation, such as licenses,\ncompany and author information, logs, etc. The former is essential for LLMs to understand the code\nand should be kept, while the latter should be avoided. Therefore, we use regular expressions to\ndelete these implementation-unrelated comments. Next, files exceeding 2096 characters are deleted,\nas longer files would be truncated during training, resulting in incomplete file content and degrading\nthe dataset quality. Following Thakur et al. [43], we also use MinHash and Jaccard similarity to filter\nout duplicate code. Finally, we run syntax checks on all Verilog code and remove any that fail the\ncheck, to further enhance the quality of the dataset.\nThrough the above-mentioned effort, we obtain high-quality real-world Verilog code. Next, we will\nconstruct requirement descriptions for them."}, {"title": "Multi-level Code Summarization", "content": "Manual annotation is prohibitively time-consuming and\ncostly. Hence, we employed GPT-3.5 to generate high-\nlevel summaries for each Verilog module as its require-\nment description. As analyzed in VerilogEval [24], when\nrequired for summarising, LLMs often produce verbose de-\nscriptions, preferring line-by-line explanations over high-\nlevel summaries. To address this issue, we introduce a\nmulti-level summarization method, employing few-shot\nlearning [6] to guide GPT-3.5 in first producing detailed\ndescriptions and then abstracting high-level summaries."}, {"title": "Data Decontamination", "content": "To ensure a fair evaluation, we conduct decontamination for the training data to remove sequences\nsimilar to benchmarks. Specifically, we use Rouge-L, a widely used similarity metric, to measure the\nsimilarity between data. Rouge-L \u2208 [0, 1], with higher values indicating greater similarity. Following\nRTLCoder [26], we remove data from the training set that have a Rouge-L score greater than 0.5 with\nany of the benchmarks, thereby preventing test data leakage."}, {"title": "Supervised Finetuning", "content": "Finally, we finetune base models on our dataset. Given a natural language description, the LLM with\nparameter \u03b8 generates code repeatedly drawing from p\u03b8(xt|x<t) and using xt as part of the input\nfor the subsequent prediction. Following prior work, the LLM is designed to reduce the negative\nlog-likelihood across the dataset D = {x1,...,x|D|} [18]:\nL =\n1\n|D|\n|D|\n\u2211i=1\nn\n1-2\n\u2211t=1log p\u03b8 (xt|x<t).\n(1)"}, {"title": "Experiments", "content": "We conduct comprehensive experiments demonstrating that CodeV achieves SOTA results across\ndiverse Verilog generation tasks, and we further analyze CodeV's performance as detailed below."}, {"title": "Experimental Setup", "content": ""}, {"title": "Instruction-tuning", "content": "We used CodeLlama-7b-Instruct [37], Deepseek-Coder-6.7b-Instruct [16], and CodeQwen1.5-7B-\nChat [4] as our base models. We fine-tuned these models for 3 epochs using PyTorch's Distributed\nData Parallel (DDP) on 4 NVIDIA A100-80GB SMX GPUs. We employed the Adafactor opti-\nmizer [39] with a linear learning rate scheduler, setting the initial learning rate at 5e - 5 with 15\nwarm-up steps. We set global batch size 256, with max sequence length 2048."}, {"title": "Benchmarks and Metrics", "content": "We conduct comprehensive evaluations on the VerilogEval [24] and RTLLM [28] benchmarks.\nVerilogEval includes two parts: VerilogEval-machine and VerilogEval-human, with 143 GPT-\ngenerated and 156 hand-crafted Verilog generation problems, respectively. We follow the VerilogEval\npaper, using the pass@k metric to measure the Verilog generation accuracy. The pass@k metric\nestimates the proportion of problems that can be solved at least once in k attempts:\npass@k :=\n1\nproblems\nE[\n(n\u2212c\nk)\n(n\nk)\n],\n(2)\nwhere n \u2265 k represents the total number of trials for each problem, and c represents the number of\ntrials that pass the functional check. We set n = 20 according to the original settings."}, {"title": "Baseline Methods", "content": "In our experiment, we compare our results with baseline methods, including: (1) foundation models\nnot tailored for Verilog generation tasks, including closed-source commercial LLMs such as GPT-\n3.5 and GPT-4, and four open-source models specialized in code generation tasks: Starcoder [21],\nCodeLlama-7b-Instruct [37], DeepSeek-Coder-6.7b-Instruct [16], and CodeQwen1.5-7B-Chat [4];\n(2) models fine-tuned for Verilog generation tasks, including Thakur et al. [43], RTLCoder [25] and\nBetterV [35]."}, {"title": "Main Results", "content": "Table 1 compares the main results of our CodeV with baseline methods on the VerilogEval and\nRTLLM benchmarks. We test CodeLlama, DeepSeek-Coder, and CodeQwen on RTLLM, while other\nbaseline results are sourced from RTLCoder [26] or BetterV [35] paper. For a fair comparison, we\nalso evaluate our models trained on comparable-size datasets against RTLCoder. The results show\nthat:\nCodeV achieves SOTA on both benchmarks. In the VerilogEval benchmark, CodeV outperforms\nall previous baseline methods across all metrics, particularly in the most challenging pass@1 metric\nwhere CodeV-CodeLlama achieves 78.1% in VerilogEval-machine, and CodeV-CodeQwen achieves\n53.1% in VerilogEval-human, significantly outperforming both GPT-4 and BetterV-CodeQwen. In\nthe RTLLM v1.1 benchmark, CodeV-CodeLlama nearly matches GPT-4's functional check success\nrate, failing in just 1 more case out of 29 circuits compared to GPT-4, and significantly outperforms\nall other models. These results demonstrate that, contributed by a high-quality instruction tuning\ndataset, CodeV exhibits significant superiority in Verilog generation tasks."}, {"title": "Ablation Study", "content": "In this section, we measured the effectiveness of multi-level summarization. Due to budget constraints,\nwe conducted this experiment only on a 10K dataset. We sampled 10K data from the multi-level\nsummarization dataset and generated single-level summarization (i.e. summarize directly) for the\nsame Verilog code. The results in Table 3 show that the instruction data obtained through multi-level\nsummarization can effectively enhance model performance which is consistent with our expectations."}, {"title": "Further Analysis", "content": ""}, {"title": "Evaluating RTLLM with pass@k metric", "content": "As shown in Eq. 2, the pass@k metric provides more unbiased estimations. In this section, we\nset n = 20 to further evaluate our models on RTTLM using pass@k metrics and conduct varying\ncomparisons with baselines. It is noteworthy that, even with 5 trials for each problem, the success\nrate metric in original RTLLM settings does not equal pass@5 metric, hence one cannot directly"}, {"title": "Impact of Data Size Scaling", "content": "We randomly sampled subsets of various sizes (27K, 50K, 100K) from the complete 165K dataset\nand fine-tuned all three baseline models, reporting their evaluation results on VerilogEval as shown\nin Figure 5. Except for CodeV-CodeQwen on VerilogEval-machine, which peaks on 100K-size,\nwe observed that model performance generally improves as training set size increases, but with\ndiminishing returns. This suggests that for 7B models, the benefits of further increasing the training\nset size may be limited, and future efforts should focus more on enhancing the quality of the dataset."}, {"title": "Conclusion", "content": "In this paper, we present CodeV, a series of Verilog generation LLMs. By collecting Verilog code\nfrom the real world and utilizing multi-level summarization to generate high-level descriptions, we\nsuccessfully construct a high-quality dataset and train multiple CodeV models based on it. Our\nexperimental results have shown CodeV's effectiveness, showcasing superior performance over both\nprevious open-source and commercial state-of-the-art models by significant margins. Specifically,\nCodeV relatively exceeds GPT-4 in VerilogEval by 22.1%. This highlights its practical utility in the\ndomain of circuit design automation."}, {"title": "Limitations", "content": "Our work has two main limitations: first, CodeV is unable to directly generate complex circuits\nwithout the assistance of a framework, such as the one proposed by Lu et al. [28]; second, CodeV"}, {"title": "Broader Impacts", "content": "Compared to existing models, CodeV has the highest accuracy in Verilog generation tasks. This\ndemonstrates that our proposed instruction tuning dataset construction method and CodeV can further\npush the boundaries of circuit design automation and lower the barrier to circuit design.\nHowever, like other code models, CodeV may generate code that does not align with user intentions\nand could be misused. On this point, we refer readers to the comprehensively discussed broader\nimpacts and hazard analysis in Chen et al. [11]. Due to the lower usage threshold of code LLMs\ncompared to programming languages, there may be certain security issues. Therefore, we recommend\nusers conduct a thorough functionality verification and security review of the code generated by\nCodeV to avoid potential losses."}, {"title": "Additional Experimental Results", "content": ""}, {"title": "Impact of Generation Temperature", "content": "Figure 6 further analyzes the VerilogEval results of our models when setting on different generation\ntemperatures. Generally, as the temperature increases, the pass@1 metric decreases while pass@5\nand pass @ 10 increase. This is because when the temperature rises, the model tends to generate more\nuncertain and diverse responses, which has two effects: on the one hand, uncertain responses are"}]}