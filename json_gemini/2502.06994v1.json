{"title": "SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering", "authors": ["Xuehang Guo", "Xingyao Wang", "Yangyi Chen", "Sha Li", "Chi Han", "Manling Li", "Heng Ji"], "abstract": "Software engineering (SE) is increasingly collaborative, with developers working together on shared complex codebases. Effective collaboration in shared environments requires participants-whether humans or AI agents\u2014to stay on the same page as their environment evolves. When a collaborator's understanding diverges from the current state-what we term the out-of-sync challenge the collaborator's actions may fail, leading to integration issues. In this work, we introduce SyncMind, a framework that systematically defines the out-of-sync problem faced by large language model (LLM) agents in collaborative software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE derived from 21 popular GitHub repositories with executable verification tests. Experiments on SyncBench uncover critical insights into existing LLM agents' capabilities and limitations. Besides substantial performance gaps among agents (from Llama-3.1 agents < 3.33% to Claude-3.5-Sonnet > 28.18%), their consistently low collaboration willingness (< 4.86%) suggests fundamental limitations of existing LLM in CSE. However, when collaboration occurs, it positively correlates with out-of-sync recovery success. Minimal performance differences in agents' resource-aware out-of-sync recoveries further reveal their significant lack of resource awareness and adaptability, shedding light on future resource-efficient collaborative systems. Code and data are openly available.", "sections": [{"title": "1. Introduction", "content": "Collaborative systems-whether involving humans, AI agents, or both boost efficiency and capabilities by combining complementary strengths. Recent advances have demonstrated impressive capabilities of AI agents in collaborative tasks (Wang et al., 2024c), from conversational AI assistants, like ChatGPT (OpenAI, 2022), Claude (Anthropic, 2023), that effectively assist users in daily problem-solving, to coding agents, like Devin (Cognition AI, 2024), Open-Hands (Wang et al., 2024a), that can actively collaborate with humans on software development.\n\nThese collaborative coding agents are typically designed and evaluated in static environments where the workspace remains fixed throughout task execution (Jimenez et al., 2023; Yang et al., 2024a). However, real-world collaborative software engineering (CSE) fundamentally operates in dynamic environments, where effective teamwork depends on team members maintaining synchronized awareness of workspace states\u2014a core challenge in the field (Yang et al., 2024b). While version control systems (Torvalds, 2005) can detect surface-level code conflicts, they cannot identify semantic inconsistencies that require manual resolution. This includes scenarios where agents must resolve dependency updates, modify existing functions to align with newly imported modules, and so on (Fig. 2)."}, {"title": "2. SyncMind: Agent Out-of-Sync Recovery", "content": "In this work, we introduce SyncMind (\u00a72), a framework that systematically defines the agent out-of-sync problem in CSE (Fig. 3), where multiple collaborators frequently modify and update shared codebases. This occurs when a collaborator's belief state (Bk) deviates from the actual world state (Sk) at time Tk, resulting in collaboration failures due to outdated information. Consider a human-AI collaboration scenario in Fig. 1: while an Agent implements changes based on its understanding at time Ti, the Human modifies the codebase at Tj (Ti < Tj < Tk). The Agent's subsequent update at Tk becomes incompatible with the current state Sk due to its outdated belief state Bk. This raises the key challenge: How can collaborators effectively recognize their belief being out-of-sync (Bk \u2260 Sk), diagnose the root causes, and recover their belief Bk to match the world state Sk?\n\nSyncMind facilitates multi-dimensional evaluation of collaborative coding agents:\n\n\u2022 Out-of-sync recovery effectiveness (\u00a74.2): We evaluate how agents detect and resolve state misalignments via exploring the environment and consulting fellow developers, enabling them to understand system changes and resynchronize after failures.\n\n\u2022 Collaborative tendency and effectiveness (\u00a74.5): We measure agents' tendency to engage in productive interactions with collaborators, a critical problem in CSE. By analyzing the assistance seeking rate and the performance difference in independent and collaborative working settings, we measure agents' recovery effectiveness in CSE.\n\n\u2022 Environmental awareness and resource allocation (\u00a74.7): We examine how agents balance independent problem-solving (i.e., exploring environment) with collaborative assistance. While excessive self-reliance in debugging can strain computational resources, over-dependence on peer support can burden collaborators through repetitive cycles of assistance requests, revisions, and testing. We evaluate resource allocation strategies in out-of-sync recovery by analyzing recovery efficiency, considering computing time and expense budget."}, {"title": "2.1. Definition of Agent Out-of-Sync", "content": "In collaborative environments, a state of 'out-of-sync' arises when a collaborator's belief state deviates from the project's state due to missed updates from other team members (Fig. 1). We propose the formal definition of \u2018out-of-sync' state, which applies to both human and AI agents.\n\nLet Si be the true world state at time Ti, and Bi be an agent's belief state. Starting from Ti when the agent begins a task, the agent becomes out-of-sync at Tk (Ti < Tk) if any of the following conditions are satisfied:\n\n(1) Knowledge gap: \u2203 Update U at time Tj (Ti < Tj < Tk) where the agent lacks knowledge of U.\n\n(2) State mismatch: Bk \u2260 Sk.\n\n(3) Task failure: Task completion based on Bk fails to achieve intended outcomes in Sk.\n\nRecovery from an out-of-sync state therefore requires:"}, {"title": "2.2. Agent Out-of-Sync Recovery", "content": "In SyncMind (Fig. 3), an agent updates its out-of-sync belief state to attain Bn = Sn through two types of recovery:\n\n\u2022 Independent Recovery. Operating autonomously, independent agents update their world beliefs through interacting with Environment (Env) and proposing solutions, besides their reflection on prior experience and feedback.\n\n\u2022 Collaborative Recovery. Collaborative agents can also take advantage of collaborator assistance to update their belief states by interacting with other collaborative agents."}, {"title": "2.3. Resource-Aware Recovery", "content": "To reflect real-world resource constraints in collaborative environments, we integrate a resource awareness module into SyncMind (Fig. 4). This module tracks and constrains two dimensions of resources: (1) recovery time measured as the number of turns taken for an agent to recover, and (2) hypothetical cost that quantify financial resources consumed through the course of recovery (e.g., computing resources for debugging and testing, Human's time and effort to answer Agent's questions). This resource-aware out-of-sync recovery framework measures how agents utilize and adapt their strategies under different resource constraints, enabling comparisons of efficiency between successful and failed recovery attempts across agentic systems."}, {"title": "3. SyncBench: Agent Out-of-Sync Benchmark", "content": "Aligning with real-world out-of-sync scenarios, our benchmark construction method is applicable to Python-based GitHub repositories with existing unit tests. SyncBench leverages 21 popular GitHub repositories and can be expanded to include additional repositories following our benchmark construction methodology (\u00a7B.3). In accordance with the definition of agent out-of-sync (\u00a72.1), our benchmark construction implements a systematic pipeline that takes all three conditions into consideration:\n\nEnv Configuration. We employ Docker (Founadi et al., 2013) to configure isolated, reproducible, and executable testing environments tailored for our out-of-sync recovery tasks. Each source repository is packaged into a dedicated Docker image with complete codebase, dependencies, and validation infrastructure for unit test execution. Each execution verification (\u00a73.4) automatically creates an isolated container instance with auto-removal upon completion, ensuring consistent and clean testing environments for reliable recovery evaluation.\n\nOut-of-Sync Simulation. We first extract Python functions and class methods (hereafter collectively referred to as func-"}, {"title": "3.1. Benchmark Construction", "content": "tions) from source repositories. For each extracted function, we employ its up-to-date state as ground truth (S2), while obtaining the out-of-sync belief state (B2) by tracing its Git history reversely until identifying a commit (B2) where execution fails (B2 \u2260 S2). In this way, Caller and Callee are constructed through simulating unit test out-of-sync and tested dependency out-of-sync, respectively: (1) Caller: We roll back the testing function until it becomes out-of-sync; (2) Callee: We roll back imported dependency for tested module out-of-sync, thereby presenting higher task complexity-agents need to understand dependency relationships and localize the problematic imported modules.\n\nMulti-level Quality Filtering. For each out-of-sync instance, we execute unit tests before and after out-of-sync happens and use the parsed test outputs to filter for high-quality instances. Our parsing-based execution testing (\u00a73.4) requires the pass-to-fail state divergence (B2 \u2260 S2): (1) updated repository (S1) passes the test to demonstrate ground truth validity, and (2) repository with the out-of-sync function (B2) fails the test to allow the out-of-sync scenario to take shape. To enhance data quality, we additionally apply a filter that retains only instances with their execution outputs comprising: (1) at least one execution error or unit test failure in B2, (2) more than one passing test in S1, (3) identical parsing result between S1 and Sn.\n\nWeighted Downsampling. In constructing our evaluation subset with 300 representative instances\u00b9 across 21 repositories, we downsample each repository's data to less than 15 instances while maintaining the original patch distribution over all sampled data, thereby applying the same task complexity distribution to all downsampled instances."}, {"title": "3.2. Benchmark Datasets", "content": "Constructing SyncBench with two complementary datasets-Caller and Callee (Fig. 5), our initial extraction yields 24,332 instances (Tab. B2). Pruning the raw dataset to 8,461 instances via multi-level filtering, the evaluation subset is further reduced via weighted downsampling. As such, we finalize our evaluation samples as 300 instances\u00b9 with"}, {"title": "3.3. LLM-Simulated Collaborators", "content": "We leverage LLMs to simulate both agents (who enter out-of-sync states B2) and know-everything collaborators (S2).\n\nAgent Out-of-Sync. We employ LLMs to power AI agents in out-of-sync states, which allows belief states to become tractable and controllable throughout the recovery process. Meanwhile, this also supports the precise measurement of an agent's resource consumption and the systematic evaluation of an agent's recovery patterns.\n\nSimulating Know-Everything Collaborators. Validated by single-turn experiments (\u00a74.4), LLM-simulated know-everything collaborators are furnished with: (1) complete task context, including both Bo - Bt and So - St (where the agent seek assistance at Tt, 2 < t < n), (2) ground-truth solution to reach Bn = Sn, (3) update history (U at T\u2081), and (4) task-specific response protocols (\u00a7D.5)."}, {"title": "3.4. Evaluation Metrics", "content": "We propose five complementary metrics tailored for comprehensively evaluating agent out-of-sync recovery:\n\nSuccess Rate (SR). We evaluate recovery success (Eq. 1) through a two-stage validation process: (1) Execution Test: Execution success can be reached only if an agent's updated repository passes the test without errors (i.e., command exit code of 0). (2) Parsing Validation: We compare the parsed test execution outputs of an agent's proposed solution against that of the ground-truth state (i.e., the original commit without issue). Recovery success requires all parsed output of test cases for the agent-proposed solution to exactly match the ground-truth values.\n\nSR = \\frac{\\sum_{m \\in M} 1(SR_m = 1)}{\\sum_{m \\in M} 1}\n\nwhere M represents the task space, SRm \u2208 {0,1} suggests whether task m achieves recovery success, 1(.) is the indicator function that returns 1 when the condition is met and 0 otherwise.\n\nLocalization Accuracy (LA). We evaluate an agent's ability to localize an out-of-sync function at two levels: (a) file (LAfile): accurately identifying the Python file containing the out-of-sync function; and (b) function (LAfunc): accu-"}, {"title": "4. Experiments", "content": "4.1. Setup\n\nRecovery Protocol. For baselines, each agent is allowed up to 30 turns to achieve Bn = Sn, which is then extended to 50 turns to assess agents' temporal resource awareness and exploitation. Financial resources are mapped similarly to each resource-aware recovery task. Provided with different action options-interacting with Env, proposing a solution, or proactively seeking collaborator assistance (\u00a72.2)-both independent and collaborative agents take each of their moves autonomously.\n\nEnv Space. We employ OpenHands (Wang et al., 2024a) to empower agents to autonomously explore and inspect the codebase environment by executing various commands."}, {"title": "4.5. Quality and Strategy of Communication Are Crucial for Recovery Success", "content": "The quality and strategy of communication prove crucial for recovery success, with several key patterns emerging:\n\nAgents with More High-Quality Questions Achieve Better Performance. Depending on whether the question asked by the agent can lead to recovery success (\u00a7C.4), we rate the quality of each query as low (resulting in recovery failure) or high (resulting in recovery success), which can be further classified into two general categories: localization queries closely related to localizing out-of-sync causes and solution queries seeking guidance on out-of-sync resolution. Despite no significant correlation between query volume and recovery success, agents with a larger proportion of high-quality questions achieve higher performance (Fig. 7).\n\nStrategic Early Exploration Facilitates Recovery Success. We compute each agent's communication timing distribution respectively for its success and failure cases (Fig. C1). Results reveal that the proportion of assistance seeking in agents' first half of recovery time is substantially larger in success cases (85.71% \u2013 100.00%) than in recovery failures (55.76% \u2013 97.93%). As top-performing agents exhibit distinct communication strategies with front-load queries, random or back-loaded assistance-seeking demonstrates less effective improvements on agents' performance. Compared with solution proposal timing that shows trivial differences between success and failure cases (averagely 2.79 turns delayed in successful recoveries), collaborative agents bene-"}, {"title": "4.6. More Challenging Tasks Decrease Performance While Better Manifest Collaboration Benefits", "content": "We observe a large negative influence of increased task complexity on agents' recoveries. Callee's additional dependency tracing allows it to serve more challenging out-of-sync tasks (\u00a73.1). Comparing agents' performance between Caller and Callee (Tab. 1 & C3), Claude-3.5-Sonnet's performance gains (+\u25b3complexity) demonstrate its superior technical capabilities in resolving complicated out-of-sync tasks. Nevertheless, Callee, presenting higher task complexity, in general undermines agents' performance (-\u2206complexity).\n\nLeveraging dissimilar complexity levels of 21 source repositories (Fig. C8), our repository-wise evaluation reflects consonant patterns between task complexity and recovery success. While the repository 11-whisper proposes the least recovery difficulty (SR: Independent 33.33%, Collaborative 22.22%), the lowest performance delivered on the repository 13-sphinx (SR: Independent 0.88%, Collaborative 4.70%) serves more challenging tasks.\n\nAlthough repository complexity manifests negative correlations with recovery success, the effectiveness of collaborator assistance increases on more challenging tasks, comparing to its trivial or negative influence on agents' recoveries in simpler out-of-sync scenarios (e.g., 11-whisper with Acollaborator = -11.11%, in contrast to 13-sphinx with Acollaborator = +3.82%). This holds in Callee where agents notably gain more benefits from collaborator assistance (Tab. 1), despite higher task complexity."}, {"title": "4.7. Agents' Significant Lack of Resource Awareness", "content": "We systematically vary resource constraints (Tables C5-C8) to investigate LLM-based agents' resource awareness in two key dimensions (\u00a72.3): (1) Time Resources, through comparing the standard 30-turn recovery with extended 50-turn performance; (2) Financial Resources, through varying initial budgets from $1000 (insufficient for 30-turn costs) to $3000 (adequate for any 30-turn action taking patterns), and halving or doubling the cost of collaborator assistance. Our experiments reveal critical limitations in agents' resource awareness and adaptive resource utilization capabilities.\n\nMore Recovery Time Can Not Guarantee Performance Gains. Extended time produces divergent effects (Fig. C3 & Tab. C5): diminishing returns on Llama-3.1-8B (SR: Independent -0.33%, Collaborative -1.00%), while notable improvements on Llama-3.1-70B (SR: Independent +3.67%, Collaborative +4.67%). This observation suggests that extending the recovery time limit alone is insufficient for improved performance, while LLM agents' com-"}, {"title": "A. Discussions and Limitations", "content": "The evolution of collaborative software engineering introduces complex challenges in maintaining synchronization among collaborators, whether humans or AI agents. Our investigation into the agent out-of-sync challenge reveals fundamental insights into how collaborators detect, respond to, and recover from belief state divergence (Bk \u2260 Sk at time Tk). This section elaborates on our key findings, discusses their broader implications, and acknowledges important limitations that suggest potential directions for future research.\n\nKey Findings and Implications. By introducing our evaluation framework (SyncMind \u00a72) and benchmark (SyncBench \u00a73) built upon real-world GitHub repositories, our experiments (\u00a74) illuminate critical aspects of agent out-of-sync:\n\n(1) Technical Capabilities and Collaborative Effectiveness. Through either independent or collaborative recovery (\u00a72.2), the stark performance variations among LLM agents (Tables 1-C1 & C2-C3) demonstrate that technical proficiency alone is insufficient for a successful out-of-sync recovery (\u00a74.2-4.3 & \u00a7C.1). Similar recovery trajectories of high-performance agents (e.g., Claude-3.5-Sonnet with SR: Independent 28.18%, Collaborative 33.70%), especially with respect to early-stage exploration (\u00a7C.2) and proactive collaborative initiative (\u00a74.4-4.5), suggest that both independent problem-solving and collaborative communication capabilities are crucial for maintaining synchronization in real-world dynamic collaborative environments.\n\n(2) Collaboration Patterns and Communication Quality. Experiment results reveal the positive correlation between LLM agents' collaboration willingness and recovery success. Benefiting from proactive assistance seeking, the influence of collaborator assistance (Acollaborator \u2208 [0.33, 5.52]%) remains beneficial among different LLM agents. However, its effectiveness varies significantly as affected by communication timing (85.71% \u2013 100.00% early-stage assistance among successful recoveries), question quality (rather than volume), and proactive collaboration initiative (ASR < 4.86%) (\u00a74.4-4.5).\n\n(3) Task Complexity and Recovery Strategies. Agents' performance gaps between Caller and Callee tasks (Tab. 1) highlight how different types of out-of-sync scenarios require distinct recovery strategies (\u00a74.5). While Callee tasks requiring dependency tracing show wider performance variations (0.67% \u2013 38.67%) and degraded performance among most agents, complex tasks generally benefit more from collaborative assistance (\u00a74.6). Leveraging the dissimilar complexity levels of different source repositories, our repository-wise analysis (\u00a7C.7) lends further evidence to the negative correlation between increased task complexity and recovery success.\n\n(4) Resource Awareness and Efficiency. Our experiments with varying resource constraints (\u00a74.7 & C.5) unveil critical insights with regard to LLM agents' resource awareness and adaptive resource utilization. While their underlying technical capabilities significantly affect their time resource utilization, LLM agents' recovery performance demonstrates their notably low awareness of both temporal and financial resources. As early-stage resource allocation proves crucial for attaining recovery success, strategic action planning and resource estimation are highlighted for high-performing and resource-efficient out-of-sync recoveries.\n\nBroader Implications for Collaborative Systems. Our findings reveal meaningful implications for the future development of collaborative systems, especially in real-world scenarios with dynamic environments and intricate task contexts. From system design perspectives, the agent out-of-sync challenge in real-world collaboration scenarios emphasizes the importance of state monitoring and divergence detection that are able to provide collaboration with effective recognition of the state mismatch Bk \u2260 Sk taking place at time Tk (\u00a72.1). The low ASR (\u2264 4.86%) among all LLM agents also underscores the value of stronger collaboration initiative and communication capabilities for effective collaboration. Our implementation of resource-aware out-of-sync recovery demonstrates the necessity of intelligent resource allocation and estimation strategies, illuminating both the importance of early-stage investment in environmental understanding and the need for adaptive resource utilization based on task complexity and resource availability. In designing effective collaboration protocols, our work elaborates the benefits of collaborator assistance, meanwhile highlighting the value of quality-focused rather than quantity-focused communication.\n\nLimitations and Future Work. While our study provides meaningful insights, several limitations present potential directions for our future work:\n\n(1) Benchmark Limitations. Although our benchmark construction method is applicable to diverse GitHub repositories and therefore can be further expanded to larger sizes to accommodate custom use (e.g., large-scale training), our SyncBench"}, {"title": "Impact Statement", "content": "This work aims to provide meaningful insights into advancing collaborative AI systems and their application in software engineering, with potential societal implications in several areas. The framework and findings serve to improve the reliability and efficiency of collaborative software engineering, potentially reducing costly errors and development delays. However, there are important considerations valuable to take into account. First, while enhanced collaboration capabilities of AI agents could improve software quality and developer productivity, they may also impact human developers' jobs and require careful integration into existing workflows. Second, our resource-aware framework highlights the need to consider computational and environmental costs in deploying collaborative AI systems at scale. Additionally, as AI agents become more capable of detecting and recovering from synchronization issues, it is of significance to ensure that human developers maintain meaningful oversight and understanding of system changes. We believe these considerations should be actively discussed as the field moves toward more sophisticated collaborative AI systems in software engineering."}]}