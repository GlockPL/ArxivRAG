{"title": "EMERGING PIXEL GROUNDING IN LARGE MULTI-MODAL MODELS Without GROUNDING SUPERVISION", "authors": ["Shengcao Cao", "Liang-Yan Gui", "Yu-Xiong Wang"], "abstract": "Current large multimodal models (LMMs) face challenges in grounding, which requires the model to relate language components to visual entities. Contrary to the common practice that fine-tunes LMMs with additional grounding super- vision, we find that the grounding ability can in fact emerge in LMMs trained without explicit grounding supervision. To reveal this emerging grounding, we introduce an \"attend-and-segment\u201d method which leverages attention maps from standard LMMs to perform pixel-level segmentation. Furthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM utilizing a diffusion-based visual encoder, as opposed to the standard CLIP visual encoder, and trained with the same weak supervision. Without being constrained by the biases and limited scale of grounding-specific supervision data, our approach is more generalizable and scalable. We achieve competitive performance on both grounding-specific and general visual question answering benchmarks, compared with grounding LMMs and generalist LMMs, respectively. Notably, we achieve a 44.2 grounding mask recall on grounded conversation generation without any grounding super- vision, outperforming the extensively supervised model GLaMM. Project page: https://groundLMM.github.io.", "sections": [{"title": "INTRODUCTION", "content": "Large multimodal models (LMMs) (Liu et al., 2023; Zhu et al., 2024; Dai et al., 2023) have brought the new opportunity of solving vision-language tasks in a general-purpose manner, which are typically built by connecting a visual encoder and a large language model (LLM) and fine-tuned by visual instructions. Currently, one major challenge faced by LMMs is grounding\u2014the key ability of relating language components (e.g., noun phrases) to visual entities (e.g., objects) in a given image (Yu et al., 2016; Krishna et al., 2017). With the grounding ability, LMMs can lift the constraint of text-only responses and address more vision-language tasks in the real world.\nTo equip LMMs with the grounding ability, the common belief is that additional supervision for grounding is necessary, and corresponding architectural modifications need to be introduced. For instance, recent efforts extend the output modality from pure text to bounding boxes (Chen et al., 2023b; Peng et al., 2024), trace points (Xu et al., 2024), or segmentation masks (Lai et al., 2024; Rasheed et al., 2024), by 1) attaching additional modules to the vanilla LMM architecture, and 2) fine-tuning the LMM with grounding supervision. The grounding supervision originates from either re-purposing existing datasets that contain human-labeled object-level annotations or automatically annotating images using other models.\nHowever, such reliance on strong supervision brings more undesired constraints: 1) Scalability: The current scale of image datasets with high-quality object-level annotations (at most millions of images (Shao et al., 2019; Kuznetsova et al., 2020)) is significantly smaller than those with only coarse image-text pairs (up to billions of images (Schuhmann et al., 2022)), so re-purposing such object-level annotations can only result in a limited scale of visual instruction data. Meanwhile, if the object-level annotations are produced by automated models, such annotations are noisier and less reliable than human-labeled ones (Rasheed et al., 2024). 2) Supervision bias: Changing the data focus to grounding tasks can lead to catastrophic forgetting (French, 1999) and hurt LMMs' general conversation capabilities. Furthermore, whether the grounding data are manually annotated (Lin et al., 2014) or pseudo-labeled by other models (Rasheed et al., 2024), they are biased by the annotators' or models' knowledge and may fail to align with general human preferences, as these fine-grained annotations can vary significantly among different annotators or models. 3) Generalizability: The grounding supervision is constrained within the visual concepts from either the existing datasets or other models, which contradicts with the ultimate goal of developing a general-purpose assistant for solving open-world problems (Bendale & Boult, 2015). Consequently, the resulting LMMs may be hard to scale, biased by the grounding supervision data, and generalize poorly to novel visual concepts and domains. Figures 1 and 4 show illustrative examples of these limitations.\nTo avoid such limitations, the question worth rethinking then arises: Is there an approach to grounding LMMs other than strong supervision? In fact, in this work, we reveal a critical yet previously overlooked fact: LMMs have inherently obtained the grounding ability through the weakly supervised visual instruction tuning. In other words, the grounding ability can emerge in LMMs without grounding supervision. Echoing prior observations of traditional convolutional neural networks (Zhou et al., 2015; 2016), we find that LMMs learn to detect visual entities and relate them with the language implicitly, during the progress of vision-language learning at the image level.\nWe therefore propose a simple and effective \u201cattend-and-segment\u201d strategy to transform this emerging grounding ability into pixel-level segmentation masks. Intuitively, the attention mechanism (Vaswani et al., 2017) in LMMs reveals where the LMM is looking at, and thus provides clues for visual grounding. We start with a base LMM trained with standard visual instruction tuning (Liu et al., 2023) but without grounding supervision, and acquire its attention maps corresponding to the visual input as it generates output tokens. Then, the attention maps are further refined into pixel-level segmentation masks. With this attend-and-segment method, we enable vision-language tasks that directly rely on the grounding capability (e.g., grounded conversation generation (Rasheed et al., 2024)). Remarkably, attend-and-segment does not require explicit grounding supervision like prior work does; in contrast, weak supervision from standard visual instruction tuning data is sufficient to achieve performance comparable with or even higher than previous grounding-supervised models. Furthermore, as a general approach, attend-and-segment can be readily integrated with recent generalist LMMs (Li et al., 2024a; Tong et al., 2024a), and benefit from their stronger vision-language capabilities.\nFurthermore, we introduce a simple solution to enhance the emerging grounding ability of LMMs. Previously, CLIP (Radford et al., 2021) plays a dominant role as the visual encoder of LMMs,"}, {"title": "RELATED WORK", "content": "Large multimodal models (LMMs). Pioneering work in LMMs, such as LLaVA (Liu et al., 2023; Sun et al., 2024; Liu et al., 2024a;b), MiniGPT-4 (Zhu et al., 2024; Chen et al., 2023a), and InstructBLIP (Dai et al., 2023; Li et al., 2023a), enables visual inputs for large language models (LLMs) via vision-language feature alignment (Radford et al., 2021) and instruction tuning (Wei et al., 2022). To equip LMMs with the grounding ability, a series of methods have been proposed to produce model outputs of bounding boxes (Peng et al., 2024; Chen et al., 2023b; Wang et al., 2023; Pi et al., 2023; You et al., 2024; Li et al., 2024b), traces of points (Xu et al., 2024), or segmentation masks (Lai et al., 2024; Rasheed et al., 2024; Zhang et al., 2024; Ren et al., 2024), by adding region-specific tokens or decoders. These methods require further grounding supervision, so image datasets with fine-grained annotations (Lin et al., 2014; Yu et al., 2016; Zhou et al., 2017) are usually repurposed for the visual instruction tuning. Unlike these supervised methods, our approach, attend-and-segment, does not change the LMM architecture or require any grounding supervision data.\nA concurrent work F-LMM (Wu et al., 2024a) shows a similar method to exploit attention maps in frozen LMMs for visual grounding, but we differ from it mainly in two aspects: 1) F-LMM still follows the supervised learning paradigm and uses grounded data to learn the additional modules, while our attend-and-segment requires zero supervision. For the first time, we reveal LMMs' emerging grounding capabilities without explicit supervision. 2) F-LMM examines existing LMMs without changing their visual encoding. In contrast, based on our systematic analysis on visual representations and their grounding ability, we propose DIFFLMM to further enhance the implicit grounding.\nDiffusion models (DMs) as visual feature extractors. DMs (Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021; Karras et al., 2022; Nichol & Dhariwal, 2021; Rombach et al., 2022) have become a prevalent paradigm in visual generation, and intermediate features from DMs are explored for applications beyond generative tasks. For example, DDPM-Seg (Baranchuk et al., 2022), ODISE (Xu et al., 2023), and EmerDiff (Namekata et al., 2024) utilize DM features for various segmentation tasks. Features from DMs can also establish point- or pixel-level correspondences between images (Tang et al., 2023; Luo et al., 2023; Zhang et al., 2023; Hedlin et al., 2023). For the first time, we show DMs can be utilized for learning a general-purpose LMM with strong grounding capabilities."}, {"title": "APPROACH", "content": "In this section, we first introduce the common architecture design of LMMs (Section 3.1). Then, we discuss attend-and-segment, which transforms the implicitly learned grounding ability into segmentation masks (Section 3.2). Based on the standard LMM and attend-and-segment, we propose DIFFLMM, to further enhance the grounding ability without additional supervision (Section 3.3)."}, {"title": "PRELIMINARY: META-ARCHITECTURE OF LARGE MULTIMODAL MODELS (LMMS)", "content": "Most LMMs (Liu et al., 2023; Zhu et al., 2024; Dai et al., 2023) share a common meta-architecture which consists of a visual encoder $M_v$, a vision-to-language feature projector $M_{V\\rightarrow L}$, and a large language model (LLM) $M_L$, as illustrated in Figure 2. Given an image $I$ of resolution $H \\times W$, the visual encoder $M_v$ (e.g., CLIP (Radford et al., 2021)) is employed to extract visual features $V = M_v(I) \\in [R^{h\\times w \\times c_v}$, where $h \\times w$ represents the feature map size, and $c_v$ is the visual feature dimension. Then, the visual feature map is considered as a sequence of $hw$ elements, and projected element-wise into the language feature space by the projector $M_{V\\leftrightarrow L}$. The projector can be implemented as a learnable lightweight multilayer perceptron (MLP). The $k$-th projected visual token is computed as $v_k = M_{V\\leftrightarrow L}(V_k) \\in R^{c_L}$, where $c_L$ is the feature dimension in the LLM. The visual tokens, concatenated with other language tokens, form the input sequence $S_{input}$:\n$S_{input} = \\{t_1,..., t_p, v_1,..., v_{hw}, t_{p+1},...,t_{p+q}\\},\\qquad(1)$\nwhere $\\{v_1,..., v_{hw}\\}$ are the $hw$ visual tokens projected from the visual feature map, $t_1, ..., t_p$ are the $p$ language tokens before the visual tokens, and $\\{t_{p+1},...,t_{p+q}\\}$ are the $q$ language tokens after the visual tokens.\nThe LLM is usually a decoder-only Transformer model, which is capable of next-token prediction. Given the input sequence $S_{input}$, the output sequence $S_{output} = \\{o_1,...,o_r\\}$ is generated in an auto-regressive manner, where the $i$-th token is predicted as:\n$o_i = M_L(S_{input}, o_1,..., o_{i-1}).\\qquad(2)$\nThe generation is terminated when the last predicted token $o_r$ is a special \u201cend-of-sequence\" token."}, {"title": "Attend-and-Segment: GROUNDING LMMS WITHOUT GROUNDING SUPERVISION", "content": "Prior efforts towards grounding LMM attach a detection or segmentation module to the LMM archi- tecture, and specialize the LMM training procedure with grounding supervision, i.e., visual instruction data augmented by object-level annotations, such that the LMM learns to predict connections between the text response and the image contents in the form of localized bounding boxes or segmentation masks. In contrast to these strongly supervised methods, we propose attend-and-segment, a sim- ple and effective method for grounding LMMs without changing their architecture or providing additional grounding supervision. We investigate the attention maps inside the transformer-based language model when generating tokens, and observe strong interpretablity associated with the attention maps. Intuitively, the attention maps can provide information about where the model is looking at when producing outputs.\nFormally, we consider the input token sequence $S_{input}$ as detailed in Section 3.1. When predicting an output token $o_r$, we capture the raw attention maps $A^{raw} \\in [0,1]^{n_{layer} \\times n_{head} \\times (p+hw+q+i-1)}$ inside the transformer-based LLM $M_L$, where $n_{layer}$ is the number of layers in the LLM, $n_{head}$ is the number of heads per layer, and $p + hw+q+i-1$ is the number of tokens before the $i$-th output token $o_r$. We only use the attention maps associated with the $hw$ visual tokens, and reduce the dimensions by averaging over $n_{layer}$ layers and $n_{head}$ heads per layer. This operation returns an attention matrix $A^{reduced} \\in [0,1]^{h\\times w}$, with the same spatial dimension as the visual feature map.\nThe attention between the output token and the visual tokens can provide interpretable grounding signals already. To further amplify the grounding signals and reduce the noise, we apply normalization across the whole output sequence:\n$A^{norm} = A^{reduced} - \\frac{1}{r} \\sum_{j=1}^r A_j^{reduced},\\qquad(3)$\nwhere $r$ is the output sequence length."}, {"title": "DIFFLMM: ENHANCED GROUNDING WITH DIFFUSION-BASED LMM", "content": "Most LMMs employ CLIP (Radford et al., 2021) as the visual encoder because it has been pre-trained to align vision and language representations, but CLIP is known to be sub-optimal in tasks that require precise localization (e.g., object detection, image segmentation) (Zhou et al., 2022; Ghiasi et al., 2022; Li et al., 2022). To enhance the grounding ability of LMMs, a direct choice may be replacing CLIP with better localized pure-vision backbones like DINO (Caron et al., 2021; Oquab et al., 2024). However, the lack of alignment with language representations can hurt vision-language task performance (Jiang et al., 2023; Tong et al., 2024b).\nCompared with vision-language models with image-level alignment (e.g., CLIP) and pure-vision models (e.g., DINO), visual representations from diffusion models (DMs) strike a better balance: 1) DMs learn to generate high-fidelity images, for which well-localized visual features are necessary. Consequently, they are better than CLIP at localization. 2) DMs are trained to perform text-to- image generation, and in this procedure, they acquire alignment with language instructions, which is lacking in pure-vision models like DINO. Therefore, we propose diffusion-based LMM (DIFFLMM, illustrated in Figure 3), which strengthens the visual encoder with a pre-trained DM.\nTo extract visual features for a given input image $I$, we simulate one denoising step in the diffusion process. The image is tokenized by a vector quantized (VQ) encoder, added with a random noise, and fed into the U-Net model of a DM (Ho et al., 2020; Rombach et al., 2022). We extract the visual feature map from the second upsampling block in the U-Net, which best preserves visual semantics (Tang et al., 2023). Text conditioning can enhance the visual feature extraction in the DM, but the image caption is usually unavailable. We employ the implicit captioning mechanism (Xu et al., 2023), which simulates text conditioning by the CLIP visual encoder. Specifically, the CLIP visual"}, {"title": "EXPERIMENTS", "content": "In this section, we first present comprehensive empirical results to evaluate our proposed attend- and-segment and DIFFLMM on both grounding-specific tasks (Sections 4.1 and 4.2) and general visual question answering tasks (Section 4.3). Then, we examine our module designs (Section 4.4) and show qualitative results (Section 4.5). Due to limited space, we include implementation details and further results in the appendix. It is worth noting that attend-and-segment and DIFFLMM are general approaches for LMMs, but considering the computation limitations, we focus on grounding and enhancing LMMs with 7B or 8B-scale language models (Chiang et al., 2023; Meta, 2024)."}, {"title": "PILOT STUDY: INSTANCE SEGMENTATION", "content": "We start by conducting an analytical study via instance segmentation (He et al., 2017) on MS- COCO (Lin et al., 2014) to demonstrate the emergence of grounding ability in LMMs and how different visual encoders impact this ability. Different from vision-language entangled benchmarks (which will be tested in later sections), the vision-centric instance segmentation task 1) directly focuses on relating image regions (represented as segmentation masks) with visual concepts (object categories), which is exactly the objective of grounding, and 2) does not evaluate based on language generation, making it more convenient to directly compare grounding abilities in different models.\nLMMs are not originally designed for instance segmentation. Therefore, for evaluation purposes, we ask LMMs to generate a detailed description of a given image, and leverage attend-and-segment to produce pairs of noun phrases and segmentation masks from the LMM response. Then we find the best-matching category label for each noun phrase by computing their embedding similarities"}, {"title": "GROUNDED CONVERSATION GENERATION", "content": "The pilot study on instance segmentation shows that LMMs trained without explicit grounding supervision already implicitly acquires grounding ability, which can be used to produce pixel-level segmentation masks. Following the discussion above, we examine LMMs' grounding ability on a more comprehensive benchmark, grounded conversation generation (GCG) (Rasheed et al., 2024). The objective of GCG is to understand visual entities in an image, and organize them into a localized description. To be specific, the GCG task requires the LMM to generate a detailed caption for a given image, in which phrases are related to their corresponding segmentation masks in the image.\nSince the GCG task requires model abilities in both captioning and segmentation, three types of metrics are considered: 1) To measure the caption quality, the text-only metric, METEOR (Banerjee & Lavie, 2005), compares the generated captions with the human-annotated reference captions. 2) To assess the segmentation mask quality, the mask-only metric, mean intersection-over-union (mIoU), quantifies the similarity between ground-truth masks and their matched predicted masks. 3) The grounding mask recall (Rasheed et al., 2024) is an integrated metric for region-specific grounding, which considers both the mask IoU and the textual similarities between the predictions and the ground truth. Therefore, the grounding mask recall is mainly considered when comparing different models."}, {"title": "VISUAL QUESTION ANSWERING", "content": "When enhancing the grounding ability of LMMs, we do not want LMMs to lose their general vision- language abilities. To assess such general abilities, we evaluate DIFFLMM on a wide range of visual question answering (VQA) benchmarks, including VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), Vizwiz (Gurari et al., 2018), ScienceQA-IMG (Lu et al., 2022), and TextVQA (Singh et al., 2019). We also consider more comprehensive LMM benchmarks, including POPE (Li et al., 2023b), MMBench (Liu et al., 2024c), and LLaVA-Bench (Liu et al., 2023).\nIt is worth noting that previous grounding LMMs (e.g., LISA (Lai et al., 2024), GLaMM (Rasheed et al., 2024)) are not usually evaluated on these general-purpose VQA benchmarks. For example, POPE is designed for quantifying object hallucination in LMMs by asking questions like \u201cIs there an [object] in the image?\u201d but the queried object often does not exist. However, we find that GLaMM almost always answers \u201cSure, it is [seg].\u201d and provides an incorrect segmentation mask (see examples in Figure 4). Such loss of capabilities in answering general questions is due to supervision bias\u2014these LMMs are fine-tuned for grounding tasks and they forget how to answer general visual questions without grounding. Therefore, grounding LMMs like GLaMM have extremely low scores on these benchmarks, and we choose to compare with stronger generalist LMMs that are not designed for grounding tasks on VQA benchmarks.\nWhen compared with state-of-the-art LMMs of the same scale (fine-tuned from a 7B LLM), including InstructBLIP (Dai et al., 2023), IDEFICS (HuggingFace, 2023), Qwen-VL-Chat (Bai et al., 2023), and LLaVA-1.5 (Liu et al., 2024a), DIFFLMM ranks 1st on 3 benchmarks, and 2nd on 4 benchmarks."}, {"title": "ABLATION STUDY", "content": "In this section, we examine the designs in attend-and-segment and DIFFLMM. We include further analysis of the attention maps in Appendix B.\nOur attend-and-segment applies normalization across the sequence of attention maps (Equation 3), which significantly reduces noises in the maps (Figure 6). From the attention map, we select the single point with the highest attention value to prompt SAM, instead of providing the entire map as a mask prompt. Empirically, we find that attention maps are sparse, tending to focus on a few key points within objects rather than the entire objects, so point prompts are more effective. Quantitative comparisons are summarized in Table 4.\nIn DIFFLMM (Figure 3), we employ a few modules to enhance the visual feature extraction and encourage better alignment with the language model. Specifically, we 1) add learnable positional encodings (Vaswani et al., 2017) to the visual features, and 2) use the implicit captioner (Xu et al., 2023) to simulate text conditioning with CLIP visual features. Due to limited computation, we cannot retrain models with the full dataset of LLaVA-1.5 (Liu et al., 2024a) and run thorough evaluation as in the previous sections. Instead, we examine the modules' effects with respect to the optimization"}, {"title": "QUALITATIVE RESULTS", "content": "In Figure 4 we present qualitative results of DIFFLMM + attend-and-segment for more challenging visual questions that are different from the training data, in comparison with GLaMM (Rasheed et al., 2024). First, when the questions are not formulated as usual, GLaMM tends to interpret these questions as image captioning or referring segmentation tasks, while DIFFLMM can still follow the user's instructions and accurately answer the questions. Meanwhile, attend-and-segment provides well-grounded responses that connects text phrases and visual entities. Furthermore, our approach shows better generalizability to unfamiliar question types, visual concepts, and image domains."}, {"title": "CONCLUSION", "content": "In this work, we reveal a previously overlooked yet critical fact that LMMs possess grounding capabilities even if they are trained without grounding supervision. We propose attend-and-segment to convert this implicit grounding ability into segmentation masks, and we introduce DIFFLMM to further enhance the grounding ability. Our approach is more scalable and generalizable compared with supervised methods. Moreover, extensive evaluation results demonstrate strong performance on both grounding-specific and general vision-language benchmarks, even surpassing grounding LMMs trained with extensive supervision on the challenging grounded conversation generation task."}, {"title": "IMPLEMENTATION DETAILS", "content": "In this section, we provide the implementation details of this work to ensure reproducibility of our experiments.\nAttend-and-Segment. We first collect the attention maps for the visual tokens, and aggregate the attention maps by averaging over all layers and heads. Then we apply normalization across the output token sequence to remove noisy points, and upsample the normalized attention map to the original image resolution. During mask refinement, we find the coordinate where the normalized attention value is maximized, and use it as a prompt to the ViT-H SAM model (Kirillov et al., 2023) for producing the pixel-level segmentation map. In the instance segmentation and grounded conversation generation tasks, we parse the model response into noun phrases using spacy (Honnibal et al., 2020), and for each phrase, we average the normalized attention maps for the tokens that compose the central noun of the phrase.\nDIFFLMM. Our development of DIFFLMM is based on the codebase and dataset of LLaVA-1.5 (Liu et al., 2024a). We employ the Stable Diffusion v1.5 (Rombach et al., 2022) model as our visual backbone. In the denoising step, we add a random noise at the 100 timestep, and extract features from the second upsampling block, following the practice of DIFT (Tang et al., 2023). We also provide additional ablation study on the choice of the noise level and feature block in Table 6. In the implicit captioner (Xu et al., 2023), we employ the visual encoder of CLIP-ViT-L-336px (Radford et al., 2021), which is the same CLIP model in the original LLaVA-1.5. The model is trained with LORA (Hu et al., 2022), and the same training recipe as LLaVA-1.5. The training data are also the same as LLaVA-1.5. The included datasets and their licenses are listed below.\n\u2022 LAION (Schuhmann et al., 2022): MIT License.\n\u2022 CC (Changpinyo et al., 2021): \"The dataset may be freely used for any purpose, although acknowledgement of Google LLC (\u201cGoogle\u201d) as the data source would be appreciated.\"\n\u2022 SBU (Ordonez et al., 2011): Unknown license.\n\u2022 MS-COCO (Lin et al., 2014): Creative Commons Attribution 4.0 International License.\n\u2022 GQA (Hudson & Manning, 2019): Creative Commons Attribution 4.0 International License.\n\u2022 OCR-VQA (Mishra et al., 2019): Unknown license.\n\u2022 TextVQA (Singh et al., 2019): Creative Commons Attribution 4.0 International License.\n\u2022 VisualGenome (Krishna et al., 2017): Creative Commons Attribution 4.0 International License."}, {"title": "ADDITIONAL ANALYSIS OF ATTENTION MAPS", "content": "In attend-and-segment, we aggregate the attention values between each generated token and the visual tokens into a 2D map. In this section, we provide more in-depth analysis of the attention maps."}, {"title": "Attention in each head and layer", "content": "Instead of averaging the attention values over $n_{layer}$ layers and $n_{head}$ heads per layer in the LLM, we first inspect the individual attention values in each head and layer. Figure 5 visualizes the attention between one generated token \"cat\" and the input visual tokens. Consistent with some recent observations (Wu et al., 2024b), a few heads in the intermediate layers show stronger activation with respect to the visual object in the image. Also, attention maps in intermediate layers are more localized. However, it is infeasible to build direct connections between attention heads and visual concepts, given the absence of grounding annotations."}, {"title": "Attention normalization", "content": "After reducing the attention maps into one 2D map for each generated token, we observe some noisy patterns in the attention maps (Figure 6-top). Some seemingly uninformative visual tokens (usually in the background) attracts more attention from the generated token than other visual tokens. A recent work (Darcet et al., 2024) shows similar observations, and explains that such less informative tokens are \u201crepurposed for internal computations.\u201d To remove such artifacts, they propose to provide additional tokens to the Vision Transformer as registers.\nHowever, in our setting, we cannot retrain the visual backbone or the language model due to limited data and computes. Instead, we simply normalize the attention maps by subtracting the mean attention map averaged over the output sequence (Section 3.2). Although the noisy attention patterns exist, we observe that these patterns are relatively stable (Figure 6-top), so the mean attention map, aggregated over the output sequence, can capture the undesired attention patterns and allow us to remove them.\nAfter the attention normalization, we observe clearer patterns (Figure 6-bottom) which leads to accurate pixel grounding. Quantitatively, attention normalization improves the GCG mask recall from 44.1 to 46.6 (Table 4). In addition to noun phrases, other words reveal relations or comparisons between visual entities, and could be helpful for more vision-language tasks. We leave this investigation for future research."}]}