{"title": "Observation Interference in Partially Observable Assistance Games", "authors": ["Scott Emmons", "Vincent Conitzer", "Caspar Oesterheld", "Stuart Russell"], "abstract": "We study partially observable assistance games (POAGs), a model of the human-AI value alignment problem which allows the human and the AI assistant to have partial observations. Motivated by concerns of AI deception, we study a qualitatively new phenomenon made possible by partial observability: would an AI assistant ever have an incentive to interfere with the human's observations? First, we prove that sometimes an optimal assistant must take observation-interfering actions, even when the human is playing optimally, and even when there are otherwise-equivalent actions available that do not interfere with observations. Though this result seems to contradict the classic theorem from single-agent decision making that the value of perfect information is nonnegative, we resolve this seeming contradiction by developing a notion of interference defined on entire policies. This can be viewed as an extension of the classic result that the value of perfect information is nonnegative into the cooperative multiagent setting. Second, we prove that if the human is simply making decisions based on their immediate outcomes, the assistant might need to interfere with observations as a way to query the human's preferences. We show that this incentive for interference goes away if the human is playing optimally, or if we introduce a communication channel for the human to communicate their preferences to the assistant. Third, we show that if the human acts according to the Boltzmann model of irrationality, this can create an incentive for the assistant to interfere with observations. Finally, we use an experimental model to analyze tradeoffs faced by the AI assistant in practice when considering whether or not to take observation-interfering actions.", "sections": [{"title": "1 Introduction", "content": "Assistance games provide a formalization of the human-AI value alignment problem [31]. They are based on Hidden Goal MDPs [12] and Cooperative Inverse Reinforcement Learning (CIRL) [15], an extension of Inverse Reinforcement Learning (IRL) [26, 1]. In assistance games, a single human and a single AI assistant share the same reward function, but this reward function is only known to the human; the assistant must learn it. In assistance games, desirable properties, such as teaching by the human and learning by the assistant, emerge as optimal solutions to the game [31]. (This contrasts with prior work on algorithms where teaching is an explicit objective [8, 14, 3].) For example, Woodward et al. [38] find that deep neural networks solving an assistance game invent strategies that involve information sharing, information seeking, and question answering."}, {"title": "2 Preliminaries / setup", "content": ""}, {"title": "2.1 Partially Observable Assistance Games", "content": "We study partially observable assistance games [31]:\nDefinition 2.1. A partially observable assistance game (POAG) $M$ is a two-player DecPOMDP with a human or principal, $H$, and an Al assistant, $A$. The game is described by a tuple,\n$M = (S, {A_H, A_A},T(\\cdot | \\cdot, \\cdot, \\cdot), {\\Theta, R(\\cdot, \\cdot, \\cdot; \\cdot)}, {\\Omega^H, \\Omega^A }, O(\\cdot, \\cdot | \\cdot, \\cdot, \\cdot), P_0(\\cdot, \\cdot), \\gamma)$, with the following definitions:\n$S$ a set of world states: $s \\in S$.\n$A_H$ a set of actions for $H$: $a_H \\in A_H$.\n$A_A$ a set of actions for $A$: $a_A \\in A_A$.\n$T(\\cdot|\\cdot,\\cdot,\\cdot)$ a conditional distribution on the next world state, given previous state and action for both players: $T(s' | s,a_H, a_A)$.\n$\\Theta$ a set of possible static reward parameter values, only observed by $H$: $\\theta \\in \\Theta$.\n$R(\\cdot,\\cdot,\\cdot;\\cdot)$ a parameterized reward function that maps world states, joint actions, and reward parameters to real numbers. $R : S \\times A_H \\times A_A \\times \\Theta \\rightarrow \\mathbb{R}$.\n$\\Omega^H$ a set of observations for $H$: $o_H \\in \\Omega^H$.\n$\\Omega^A$ a set of observations for $A$: $o_A \\in \\Omega^A$.\n$O(\\cdot,\\cdot|\\cdot,\\cdot,\\cdot)$ a conditional distribution on the observations, given the next world state and action of both players: $O(o_H, o_A | s', a_H, a_A)$.\n$P_0(\\cdot,\\cdot)$ a distribution over the initial state, represented as tuples: $P_0(s_0, \\theta)$.\n$\\gamma$ a discount factor: $\\gamma \\in [0,1]$.\nWe denote H's and A's marginal observation distributions as $O^H(o_H | s', a_H, a_A) = \\sum_{o_A} O(o_H, o_A | s', a_H, a_A)$ and $O^A(o_A | s', a_H, a_A) = \\sum_{o_H} O(o_H, o_A | s', a_H, a_A)$. We consider H policies $\\pi_H$ which, at timestep $t$, take as input the full history of H's observations and actions $h^H \\in (\\Omega^H \\times A_H)^t$ and map to a distribution over actions $\\Delta^{A_H}$. A's policy $\\pi_A : (\\Omega^A \\times A_A)^+ \\rightarrow \\Delta^{A_A}$ is analogous. We call $\\pi_H$ a best response to $\\pi_A$ when $\\pi_H$ maximizes expected discounted reward given $\\pi_A$, i.e., $\\pi_H \\in arg\\,\\underset{\\pi_H}{max} \\mathbb{E}_{(\\pi_H, \\pi_A)}[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a^A_t, a^H_t|\\theta)]$, where the expectation is taken over trajectories induced by the policies $(\\pi_H, \\pi_A)$ and initial distribution $P_0$. The best response for A is defined analogously. A policy pair $(\\pi_H, \\pi_A)$ is optimal if it maximizes the expected discounted reward in the POAG: $(\\pi_H, \\pi_A) = arg\\,\\underset{\\pi_H, \\pi_A}{max} \\mathbb{E}_{(\\pi_H, \\pi_A)}[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a^A_t, a^H_t|\\theta)]$.\nNote that optimal policy pairs are in particular Nash equilibria. Computationally, POAGs are equivalent to 2-player DecPOMDPs. Thus, finding optimal policy pairs for POAGs is NEXP-hard in general [5] [cf. 30]. A POAG may have multiple distinct optimal policy pairs. For instance, H and A may have multiple ways of resolving coordination problems, or choose different ways of communicating."}, {"title": "2.2 Beliefs and calibration of beliefs", "content": "We are motivated to study observation interference because of its potential impact on H's belief about the state of the world. If A interferes with observations, could this cause H to have false beliefs?\nTo address this question, we apply known techniques to establish what information H needs to form calibrated beliefs in a POAG. The key idea is that if H knows A's policy, H can treat A like any other part of the environment. Forming beliefs then reduces to POMDP inference.\nThe simplest case of H knowing A's policy is when A is playing a fixed policy:\nProposition 2.2. Suppose A is playing a fixed policy. If H knows A's policy along with the POAG specification M, then H can form calibrated beliefs about the world state. For any timestep t and state $s_t$, H can form $P(s_t | o_t^{H})$, the probability of $s_t$ given H\u2019s observation history $o_t^{H}$.\nIn an iterated setting where A updates its policy between iterations, H can form beliefs if H additionally knows the policy update rule.\nProposition 2.3. Suppose A is updating its policy each iteration of the game. Knowledge of the game dynamics, of A's initial policy, and of A's update rule is sufficient for H to form calibrated beliefs about A's future policy and of the world state.\nRemark 2.4. Propositions 2.2 and 2.3 hold even if A is interfering with observations (Definition 3.2).\nRemark 2.5. Proposition 2.2 and Proposition 2.3 continue to hold if H only knows a prior over A's policy. H can form a posterior using Bayes' rule; the posterior is calibrated if the prior is calibrated.\nIn Section 4, we study when observation interference occurs in optimal policy pairs, i.e., when H and A are each playing a best response to the other. By design, this solution concept assumes that H knows whatever information is needed about A's policy to compute a best response. In such a case where H knows A's policy, the preceding results show that H can form calibrated beliefs about the world, even when A is interfering with observations. Observation interference increases H's uncertainty, but it doesn't break the calibration of H's beliefs. Because H can still form calibrated beliefs in this setting, our work uses the concept of \u201cinterference\" rather than the concept of \"deception.\""}, {"title": "3 Defining observation interference", "content": "Observation interference First, we define what interference means. Intuitively, interference is taking action so that the human receives a less informative signal about the state. In particular, the human receives, in some sense, a subset of the information. We formalize this by saying one signal is less informative than another about the state if (without knowing the state) we could generate one signal from the other [cf. 7, 6, 10].\nDefinition 3.1. Let $(P(\\cdot | s))_{s \\in S}$ and $(\\tilde{P}(\\cdot | s))_{s \\in S}$ be families of probability distributions over $\\Omega$. We say that $\\tilde{P}$ is at most as informative as $P$ if there exists a stochastic function $F: \\Omega \\rightarrow \\tilde{\\Omega}$ s.t. for all states $s$ we have $F(X) \\sim \\tilde{P}(\\cdot | s)$ if $X \\sim P(\\cdot | s)$. We say that $P$ is (strictly) more informative than $\\tilde{P}$ if $P$ is at least as informative as $\\tilde{P}$ but not vice versa.\nWhy do we include the condition \"for all states s\" in Definition 3.1? Intuitively, we want it always to be possible to use the stochastic function F to reconstruct the less informative signal from the more informative signal. Since our setting is partially observable, the \"for all states s\" condition allows a player of the game to do this reconstruction in any scenario, even if their observations don't enable them to infer the state.\nNote that this definition induces only a partial order on probability distributions. For instance, different signals may provide information about different aspects of s, and it may not be possible to generate either distribution from the other.\nWith this definition in hand, we define an observation-interfering action as one that results in the human's observation being less informative about the state than the observation distribution resulting from another assistant action. We additionally require that this other action has the same effects on the state and immediate reward. After all, it is clear that sometimes A has to trade off providing information to H with optimizing its effect on the environment. Formally:"}, {"title": "4 Communicating private information is an incentive for observation interference", "content": ""}, {"title": "4.1 Revealing errors can emerge as an optimal POAG solution", "content": "Past work has shown how RLHF can cause misleading [35] and deceptive [37, 22] behaviors. Specifically, Lang et al. [22] show that in order to get better human feedback, RLHF can have an incentive to hide error messages. In contrast, we show with the following example that revealing error messages can emerge in POAG solutions.\nExample 4.1. First, A is executing on a remote machine where logging has been disabled by default. A takes one of two actions: (1) Attempt to install cuda. The installation succeeds with 50% probability. An empty observation is produced (since logging is disabled). (2) Re-enable logging and attempt to install cuda. The installation succeeds with 50% probability. An observation is produced containing a success or failure message.\nThen, H takes one of two actions: (1) Run an experiment. If cuda is installed successfully, this yields +1 reward. Otherwise, it yields -2 reward. (2) Don't run an experiment. This always yields 0 reward.\nIn the optimal policy pair, A reenables logging; this reveals errors to H!\nWhy does RLHF have an incentive to hide error messages, while the POAG solution has an incentive to reveal the errors? In RLHF, the agent is merely maximizing the feedback it receives from the human, rather than the human's true reward function. If an RLHF agent can deceive the human to get better feedback, it has an incentive to do so. In contrast, optimal POAG agents only care about the human's true reward and will reveal errors when that information is useful to the human.\nIn fact, if A has no private information, then it never needs to take observation-interfering actions for an optimal solution!\nTheorem 4.2. Let M be any POAG. Let A have no private information. Then there is an optimal policy pair $(\\pi_H, \\pi_A)$ for M in which $\\pi_A$ does not interfere with observations at the action level (and $\\pi_H$ observes naively)."}, {"title": "4.2 Communicating private information is an incentive for observation interference at the action level", "content": "Intuitively, one might hope that A would never take observation-interfering actions. After all, classic theory tells us that when H is in a single-agent setting, the value of perfect information is nonnegative: more informative observations never lead to worse solutions. But as it turns out, when H and A interact in a POAG, there are cases in which all optimal policy pairs require A to take observation-interfering actions. The main reason for A to take observation-interfering actions is to communicate its own private information to H. Consider the following example.\nExample 4.3. H has typed apt list -a cuda to see the list of cuda versions available to be installed. Out of 10 total versions, only a (non-empty) subset are available. And of these available versions, only a subset are compatible with the other environment software.\nFirst, A takes an action. For each of the 10 total cuda versions, A can choose to or not to suppress it from the list of available packages. This gives A $2^{10}$ total actions, where 1 action is non-observation interference (suppressing nothing), and the remaining $2^{10} \u2013 1$ actions interfere with observations.\nSecond, H takes an action. H has 10 possible actions which try to install the corresponding version of cuda if it appears in the version list. If an available cuda version that is compatible with the other environment software is installed, it yields +1 reward. Otherwise, it yields 0 reward."}, {"title": "4.3 Optimal policy pairs never require observation interference at the policy level", "content": "In Definition 3.2, we first define observation interference as a feature of actions. We then say in Definition 3.3 that a policy interferes with observations at the action level if and only if it ever takes an observation-interfering action.\nBecause the definition is ultimately about actions, it doesn't consider how $\\pi_A$ might choose to take observation-interfering actions in a way that depends on A's observations. To account for $\\pi_A$'s dependence on its observation, we define an alternative notion of what it means for a policy to interfere with observations.\nLet $P_O$ be the distribution over human observations at time $t$. Further, let $L_t(\\pi_H, \\pi_A)$ be the set of possible states at time $t$.\nDefinition 4.6. Let M be a POAG. We say that A's policy $\\pi^A$ interferes with observations at the policy level if there exists some other partial policy $\\pi'_A$ for time step t s.t. $\\pi'_t \\neq \\pi^A_t$ and $\\pi'_A$ and $\\pi^A$ have the same effect on state transitions and immediate rewards, but for all $\\pi_H$ we have that $P^H_\\pi (o^{H} | s_{t+1}, \\pi^H, \\pi^A_{0:t}, s_{t+1} \\in L_{t+1}(\\pi_H, \\pi'_A))$ is less informative than the corresponding distribution if we replace $\\pi^A$ with $(\\pi^H_{0:t-1}, \\pi'_A)$.\nCompared to our previous action-level notion of observation interference (Definition 3.2), this new policy-level notion (Definition 4.6) differs in how it treats H's inference process. Whereas the action-level notion models inference about isolated observations, the policy-level notion allows H to make inferences in the context of A's overall strategy. In this broader framework, cases which appear to destroy information when viewed at the action level may actually provide new information when viewed at the policy level. In fact, we show in the following theorem that it's never strictly necessary to interfere with observations at the policy level.\nTheorem 4.7. Let M be any POAG. Then there exists an optimal policy pair $(\\pi_H, \\pi_A)$ for M s.t. $\\pi_A$ does not interfere with observations at the policy level."}, {"title": "5 Querying H's preferences is an incentive for observation interference", "content": "We now study a second reason A can have for interfering with observations. We have already shown (Theorems 4.2, 4.5 and 4.7) that even if H has private information and no communication channel, there's always an optimal policy pair in which A does not interfere, as long as A doesn't have private information. So, if H plays a best response to A's policy, then A can choose a non-interference policy without loss of utility. However, if H does not play a best response to A, then reasons for interference emerge that are more subtle than those in the A \u2192 H case.\nIntuitively, A might need to interfere with observations to elicit H \u2192 A communication. Suppose A needs some information from H, but H is acting naively (see Definition 3.7) in a way that does not reveal her private information. By changing H's observation, A can make H's naive response communicate useful information to A. The following example illustrates this phenomenon.\nExample 5.1. \u0397 would like to schedule a job on a cluster. She can choose between two nodes. By default, she receives a signal from the environment about the two nodes' specifications. Each node may be either GPU-optimized or CPU-optimized. Also, the CPUs may be either AMD or Intel.\nH has a strong preference between GPU-optimized and CPU-optimized nodes. She has a weak preference between AMD and Intel. These preferences are unknown to A.\nA can interfere with H's observation about the available nodes. In particular, A can make it so that a choice between two CPU-optimized nodes appears as a choice between a GPU-optimized and CPU-optimized node. A observes H's choice. Later, A is charged with scheduling a job for H and has to choose between a CPU- and a GPU-optimized node on H's behalf.\nIf H chooses naively upon seeing only CPU-optimized nodes (simply choosing her favorite), then A's best response interferes with observations at both the action and policy levels. Interfering with observations allows A to learn H's preference about GPU- vs CPU-optimized nodes.\nAt first sight, this may appear to be a counterexample to Theorem 4.2. However, note that Example 5.1 actually does have optimal policy pairs in which A doesn't interfere. In particular, even if A does not interfere and the two available nodes are CPU-optimized, H may simply communicate her CPU-versus-GPU preference anyway! That is, when facing a choice between CPU-optimized node 1 and 2, she may choose, say, 1 if she favors GPU-optimized nodes and 2 if she favors CPU-optimized nodes. However, this type of human strategy seems implausible, as it would require H and A to have settled on some communication strategy that overrides H's immediate preferences about the machines that H can in fact choose between.\nIn Example 5.1, one might ask why A can't just ask H each time A makes a decision. Simply asking H's preference is reasonable when A has only one decision to make. However, we are motivated by cases where A has many decisions to make, and asking H's preferences each time would be cumbersome."}, {"title": "6 Human irrationality is an incentive for observation interference", "content": "Finally we consider a third reason for observation interference: human irrationality or bounded rationality. Roughly, reducing the amount of information supplied to the human may simplify the human's decision problem and thus improve her decision making. Importantly, this motivation for observation interference may exist even if neither H nor A has any private information.\nAs our model of human decision making, we adopt Boltzmann rationality [24, 25], which has recently been used in (C)IRL [21, 29, 39]. We define Boltzmann rationality as follows:\nDefinition 6.1. Let M be a POAG. Let $\\pi_A$ be A's policy in M. We say that H's policy $\\pi_H$ is a Boltzmann-rational response to $\\pi_A$ if there exists some $\\beta > 0$ s.t. for every human observation history $h$ that arises with positive probability in M under $(\\pi_A, \\pi_H)$ we have that $\\pi_H(a | h) \\propto exp(\\beta \\mathbb{E} [\\sum_{t=0}^{\\infty} R(s_t, a_A^\\uparrow, a_H) | \\pi_H, \\pi_A, h])$.\nMathematically speaking, a Boltzmann-rational agent at each time step computes the expected utilities of each of the available actions and then randomizes according to the softmax of the expected utilities.\nThe central feature of the Boltzmann rationality model is that it postulates that agents are more likely to get decisions right if the differences in expected utility of the options are large. It's easy to see that if the human observes naively (and thus doesn't have calibrated beliefs), A sometimes prefers observation interference. Roughly, A wants to make H always believe that the difference in utilities between her actions is high.\nHowever, it turns out that even if the Boltzmann-rational human has calibrated beliefs, A's optimal policy sometimes interferes with observations, even if neither A nor H has private information. Intuitively, providing more information may sometimes result in less clear-cut decisions, i.e., decision situations with a smaller difference between the correct and incorrect option. To illustrate this phenomenon, consider the following example.\nExample 6.2. H is running a terminal command and is unsure whether to run the command with flag 1 or flag 2. With equal probability, either flag 1 or flag 2 is better, and how good the flags are differs by either a little or a lot. Thus, H is uniformly at random in one of four states. A has two"}, {"title": "7 Experiments", "content": "In the previous sections, we explored why AI assistants might take observation-interfering actions. Section 4 showed that sometimes they interfere with observations at the action level in order to communicate other, more important information at the policy level. Section 6 showed that sometimes they interfere with observations to make decisions easier for humans. Now, we develop a model game to analyze these behaviors. We run experiments to answer the following questions within our model:\n1. How does the amount of H's irrationality affect A's incentive to take observation-interfering actions?\n2. How does the amount of A's private information affect A's incentive to take observation-interfering actions?"}, {"title": "7.1 Experiment details", "content": "We study a game where selecting the best action requires combining private observations known only to H and private observations known only to A. The game presents A with a tradeoff: A can interfere with observations to communicate information that only A observes, but interfering also destroys information that only H observes.\nConcretely, the game has $d$ products. Each product $i$ has two attributes, $H_i$ and $R_i$, drawn i.i.d. from Unif(0, 1). Each product's utility is the sum of its attributes, $U_i = H_i + R_i$. The game consists of two moves. First, A sees $R_i$ for $i = 1, . . ., k$ where $k$ is the number of A's private observations. A chooses a set of products to interfere with. For the products A interfered with, H sees $\\hat{H_i} = -\\infty$; for the remaining products, H sees $\\hat{H_i} = H_i$. Second, H chooses a product $a_i$. Both H and A receive a common payoff of the chosen product's utility, $U_i$.\nWe assume the human's product selection policy is Boltzmann rational over their observed values $\\hat{H_i}$:\nDefinition 7.1. H's Boltzmann selection policy chooses products by a Boltzmann distribution over $\\hat{H_i}$, the observed product values: $\\pi_H(a_i) \\propto exp(\\beta \\hat{H_i})$. The parameter $\\beta$ controls H's rationality.\nWe consider A policies that always interfere with $k$ observations for some fixed $k$. Call these policies k-interference. We study the optimal such policies, characterized by the following result:\nProposition 7.2. Consider A policies that always interfere with k observations for some fixed k. Among the k-interference policies for a given k, A's best response to H's straightforward product selection policy is as follows. A interferes with the k smallest $R_i$ values where $\\tilde{R_i} = R_i$ if A observes $R_i$, and $\\tilde{R_i} = 0.5$ otherwise."}, {"title": "8 Conclusion", "content": "Even when the AI assistant and the human have perfect value alignment, we show how observation interference can emerge from several distinct incentives. As we focus on optimal assistants- analyzing optimal policy pairs and best responses\u2014all of the incentives for observation interference that we consider are done for the human's benefit. This creates a nuanced picture, suggesting that not all observation interference is inherently bad. In practice, we expect that AI assistants will exhibit observation interference for a mix of good and bad reasons. With this theory, our goal is to lay a foundation for understanding the causes of observation interference and helping to disentangle them in practice.\nLimitations and future work We choose to study optimal solutions, such as optimal policy pairs and best responses. This has the advantage of providing general insight into the underlying game structure that is independent of any particular learning algorithm. However, this independence is also a drawback; if algorithms fail to find optimal solutions, they might break down in unexpected ways not captured by our theory.\nMoreover, we find that some optimal policy pairs in our examples, such as Example 5.1, require H and A to have a shared communication protocol. It would be interesting to study additional solution concepts, such as correlated equilibria and communication equilibria, to handle this sort of communication [13]. While we consider only a single human and single assistant, it would also be interesting to study scenarios with multiple humans and multiple assistants. Lastly, while we run experiments in one model of a POAG, it would be interesting to see if and how our experimental trends generalize to other POAGS."}]}