{"title": "LLMER: Crafting Interactive Extended Reality Worlds with JSON Data Generated by Large Language Models", "authors": ["Jiangong Chen", "Xiaoyi Wu", "Tian Lan", "Bin Li"], "abstract": "The integration of Large Language Models (LLMs) like GPT-4 with Extended Reality (XR) technologies offers the potential to build truly immersive XR environments that interact with human users through natural language, e.g., generating and animating 3D scenes from audio inputs. However, the complexity of XR environments makes it difficult to accurately extract relevant contextual data and scene/object parameters from an overwhelming volume of XR artifacts. It leads to not only increased costs with pay-per-use models, but also elevated levels of generation errors. Moreover, existing approaches focusing on coding script generation are often prone to generation errors, resulting in flawed or invalid scripts, application crashes, and ultimately a degraded user experience. To overcome these challenges, we introduce LLMER, a novel framework that creates interactive XR worlds using JSON data generated by LLMs. Unlike prior approaches focusing on coding script generation, LLMER translates natural language inputs into JSON data, significantly reducing the likelihood of application crashes and processing latency. It employs a multi-stage strategy to supply only the essential contextual information adapted to the user's request and features multiple modules designed for various XR tasks. Our preliminary user study reveals the effectiveness of the proposed system, with over 80% reduction in consumed tokens and around 60% reduction in task completion time compared to state-of-the-art approaches. The analysis of users' feedback also illuminates a series of directions for further optimization.", "sections": [{"title": "INTRODUCTION", "content": "The rapid advancement of generative artificial intelligence (GenAI) has revolutionized the creation of various types of content, such as text, scripts, audio, images, and videos. Within this realm, Large Language Models (LLMs) such as GPT-4 have emerged as powerful tools, particularly adept at generating coherent and contextually relevant text and script data. On the other hand, Extended Reality (XR) (see [5], an umbrella term encapsulating Augmented Reality (AR), Virtual Reality (VR), Mixed Reality (MR), and everything in between) facilitates interactions among human users, virtual content, and real-world environments. The fusion of LLM with XR technologies (see [3, 6, 9]) seems a natural approach for enhancing such interactions, providing a more effective method than standard hand gestures or pre-defined menus. For example, users can potentially create virtual objects that adapt to the real-world environment or generate specific animations that interact with their limbs, all by simply articulating their needs through natural language.\nDespite these promises, integrating LLM into XR systems to enhance the immersive experience presents several challenges: i) XR environments often encompass thousands of artifacts and parameters which could easily overwhelm LLM during generation, in terms of both hindering the accurate capture of useful contextual information and increasing the costs associated with pay-per-use Application Programming Interfaces (APIs); For instance, in a scene with hundreds of objects and each with tens of properties, LLMs may create floating cups not adhering to the surface of a table. ii) Existing approaches often rely on generating and compiling source codes to achieve desired functionalities, but the codes generated by LLMs are prone to errors, leading to compilation issues and application crashes during runtime; iii) To mitigate code generation errors, prior solutions apply multiple iterations to produce high-quality scripts, which leads to significant generation latency and reduces system responsiveness.\nThis leads to several critical research questions: (i) How can we efficiently identify a minimum set of essential contextual information from user requests? (ii) How can we mitigate the degradation of user experience caused by AI \"hallucinations\" in code generation? (iii) How can we design a system that creates various XR interactions via natural languages without a time-consuming iteration process? Previous studies (see [6, 9, 16]) have explored integrating LLM-generated code into XR environments but faced challenges with responsiveness and high error rates due to LLM's hallucination issues. Other researchers (see [3, 8, 14, 27]) have developed methods to generate controlling data along with scripts for real-time animation of virtual objects or robotic arms, yet these solutions did not dive deep into the fusion with the XR environments and thus cannot deliver seamless and intuitive interactions with human users.\nIn this paper, we introduce LLMER (Fig. 1), the first system designed to leverage LLMs to generate JSON data for crafting Extended Reality worlds. Specifically, LLMER facilitates user interaction with a virtual agent through natural language, which enables an intuitive and efficient creation of virtual objects and animations within XR environments. LLMER accepts natural language inputs initiated by hand gestures and communicates with the remote LLM server through a multi-stage strategy, selectively adding contextual information as needed to avoid overwhelming LLMs. Unlike previous approaches that rely on script generation and runtime compilation, LLMER utilizes LLMs to directly produce JSON data, which is then used for executing various XR tasks by dedicated modules. The combination of structured JSON data and dedicated modules mitigates the complexity of code generation \u2013 by projecting the generation problem into constrained, compact subspaces (defined with respect to the context-relevant structured JSON data), performing generation within these subspaces, and mapping and applying the results back to the original XR space. Compared to direct script generation, focusing on JSON data enables a well-grounded approach that not only enables smooth XR interactions but also prevents scripting errors and application crashes. To the benefit of the structured design, our proposed system can operate without fine-tuning models or coding expertise from users.\nThe key design aspects of LLMER are: First, LLMER employs a novel multi-stage process to minimize distractions from irrelevant data and reduces costs associated with pay-per-use APIs. It integrates a Context Library and an LLM Wrapper to systematically process each user request in two stages. Initially, the LLM analyzes user requests and categorizes them based on contextual properties. Subsequently, the LLM Wrapper leverages these properties to retrieve essential contextual information, which is then incorporated into a new prompt sent back to the LLM. Second, the system requests structured JSON data as output, rather than script generation, to confine the process within constrained subspaces, enhancing responsiveness and avoiding scripting errors. By providing proper JSON schemas to LLMs, LLMER obtains high-quality structured JSON data without the need for model fine-tuning. Lastly, LLMER introduces three modules targeted for executing different XR tasks, expanding the system's capabilities while maintaining efficiency. Each module contains distinct, minimal executable units that can be combined to handle a wide variety of XR tasks.\nThe main contributions of our paper are summarized as follows:\n\u2022 We propose a novel system, LLMER, which accepts natural language inputs from users and leverages LLMs to craft interactive XR worlds through JSON data generation.\n\u2022 We showcase the implementation details of LLMER running as a Unity application in both link-free and linked modes and deploy it on commercial off-the-shelf XR headsets. We open source our implementation for academic and/or non-commercial use\u00b9.\n\u2022 We conduct a preliminary user study to assess the usability and robustness of LLMER, which reveals its powerful functionality and indicates opportunities for further optimization.\n\u2022 We perform a thorough analysis of the numerical results and interview feedback from the user study. The results indicate over 80% reduction in consumed tokens and around 60% reduction in task completion time compared to state-of-the-art approaches."}, {"title": "RELATED WORK", "content": ""}, {"title": "LLM Agents", "content": "To the benefit of the versatility of LLMs, many researchers (see [12, 18, 29]) utilized LLMs as the foundation to build AI agents that adapt to diverse scenarios. Treating LLMs as universal approximate knowledge sources with reasonable randomness, some researchers (see [4, 11, 15, 26]) investigated the way to leverage LLMs to facilitate the planning or reasoning tasks. Other researchers (see [25]) tried to integrate LLMs into gaming scenarios, mimicking human players to interact with virtual worlds with sufficient environmental understanding. Besides single agents, some researchers (see [1, 10, 22]) considered the collaboration of multiple agents powered by LLMs, targeting believable simulations of human behavior with proposed frameworks. However, these are not designed for XR environments that are characterized by natural interactions among human users, virtual content, and real-world environments."}, {"title": "LLM in XR", "content": "The fusion of LLM with XR environments significantly facilitates human interaction within XR environments. Researchers have raised interest in how to provide seamless integration with more convenient and powerful interactions compared with specific hand gestures or menus. Authors in [8] simplified robotic programming by enabling LLM for prompt processing and AR for visualizing the generated waypoints. DreamCodeVR [9], powered by LLM, assists users in programming VR environments by translating spoken language into code, simplifying VR development, and enhancing accessibility for users of varying technical skills. LLMR [6] facilitates the real-time creation and modification of interactive XR experiences, leveraging novel strategies and the Unity game engine to handle complex scenarios with limited ideal training data. PaLM-E [7] incorporated real-world continuous sensor modalities into language models. MagicItem [16] integrated cluster scripts into LLMs to accelerate the generation. However, all of them are code-based designs, which usually suffer from the hallucination issue of LLMs and potentially lead to crashes of applications. Authors in [14] utilized LLM to generate and control real-time animations created by parsing structured strings that define joint movements demonstrated across various models and motions. However, they did not consider the deep fusion with the XR environments while our work provides a seamless integration for better interactions with human users."}, {"title": "3D Objects and Interactive Environment", "content": "Users in XR environments often require the creation of 3D objects for an enhanced immersive experience. Recent studies have put efforts into how to dynamically create 3D objects based on users' requests. DreamFusion [23] generated high-fidelity coherent 3D objects and scenes for a diverse set of user-provided text prompts. Subsequently, Magic3D [19] was introduced to create high-quality 3D models faster than previous methods such as DreamFusion. In addition to generating objects, the creation of interactive 3D environments has been further explored. Chat-3D [28] is the dialogue system for 3D scenes, which combines LLM and 3D representations to comprehend diverse instructions for 3D scenes. 3D-LLM [13] performed better on capturing 3D spatial information after introducing a 3D localization mechanism to training. However, those approaches usually pose significant generation delays and are difficult to integrate into real-time systems."}, {"title": "SYSTEM OVERVIEW", "content": "In this section, we provide an overview of LLMER, a system designed to craft interactive XR worlds based on users' audio commands. Utilizing state-of-the-art LLMs like GPT-4 from a cloud server, LLMER generates high-quality and structured JSON data and leverages backend modules to execute these data in accordance with user requirements. It also generates synthetic audio to emulate conversations among humans. An overview of the system architecture is presented in Fig. 2.\nUnlike systems that generate scripts and compile them at runtime, LLMER employs pre-defined modules to handle various tasks and leverages JSON data to enable real-time interactions. The system comprises three primary modules to execute the generated JSON data for various XR tasks: i) Virtual Object Creator, which can generate new virtual objects using either Unity primitives or existing local prefab resources; ii) Animation Library, which produces a variety of animations in response to user requests; iii) Reality Fusion Engine, which enables the interaction between human user and virtual objects overlayed on the real-world environments. By leveraging clearly defined JSON data, LLMER avoids the error-prone script generation and accelerates the processing time compared with images or 3D meshes creation.\nTo minimize the distraction from the complex XR contexts and enhance the quality of the generated JSON data, we introduce two essential components: the LLM Wrapper and the Context Library. Each user request undergoes two main phases within the LLM Wrapper: initial stage and the refined stage. Initially, LLMER submits the raw user request, encapsulated in an initial prompt that includes a JSON schema, to the cloud server. The server classifies the request and identifies necessary contextual categories as the initial response. Following this, LLMER consults the Context Library to extract necessary contexts from complex XR contexts space and integrate them into refined prompts. These refined prompts guide the generation of refined responses, which consist of JSON data that complies with the specified JSON schema and is then utilized by aforementioned modules for executing XR tasks. For more complex inquiries, the initial stage decomposes the original request into simpler segments, producing multiple refined prompts. Taking the example as shown in Fig. 1, LLMER first analyzes the initial prompt and splits the user request into an object creation task and an animation generation task in the initial response. Then, it generates two different refined prompts with necessary contextual information. The refined responses from the LLMs will include structured JSON data that can be converted to desired behavior in the XR world. Different from prior iterative approaches (e.g., the Builder-Inspector structure in LLMR [6]) focusing on inspecting and regenerating results, which often leads to indeterminate iterations with significant delays, our multi-stage process operates in a more controlled and structured manner. Moreover, this design effectively excludes unrelated contextual information and thus alleviates the distraction of LLMs.\nTo mimic natural human communication, LLMER includes a virtual avatar representing an AI agent in the XR environment. This avatar accepts raw audio inputs from the user and responds with synthesized speech of concise descriptions. User voice recordings are activated by hand gestures and transcribed into text using OpenAI's Whisper model [24]. Meanwhile, selected text responses from the LLM are processed through OpenAI's text-to-speech (TTS) model and delivered in artificial speech together with other interactions. In the next section, we will present the detailed design principles for our system."}, {"title": "SYSTEM DESIGN", "content": "In this section, we will showcase the core design of LLMER. We begin with the description of the Context Library, focusing on its roles in generating high-quality JSON data while maintaining low cost. This is followed by the presentation on the three task modules, detailing what kind of JSON data is required and how they convert the JSON data into real-time interactions. Lastly, we introduce the LLM Wrapper, which connects all those components."}, {"title": "Context Library", "content": "Contextual understanding is crucial for seamlessly integrating LLM into XR scenes. However, complex contexts can distract LLM models from accurately extracting useful information and increase the operational costs of using pay-per-use models like the GPT-4 API. LLMR (see [6]) addresses this challenge with a Scene Analyzer module that condenses complex scene context into a succinct summary. While effective, this approach introduces additional processing latency and consumes significant token inputs. In contrast, LLMER utilizes a Context Library that supplies only the essential information needed based on the user's request, enhancing the system's ability to interpret vague commands and interact with XR environments without consuming excessive tokens.\nWithin LLMER, the Context Library is accessed via the LLM Wrapper, where each user request is integrated into an initial prompt that includes a specified JSON schema. This schema directs the AI model to generate JSON data that retrieves contextual information from the Context Library. We categorize XR scene context into several crucial areas: resources for environment creation, existing scene data (distinguishing between virtual objects and real-world contexts), ongoing animations, and user-related contexts (such as tracking head and hand poses). Each category has associated properties like position, orientation, scale, and size. The LLM model dynamically selects the relevant categories and properties based on the user's request and includes them in the initial response as structured JSON data."}, {"title": "Virtual Object Creator", "content": "One core functionality of LLMER is to add virtual objects to the XR scene. By default, LLMER supports creating objects from two primary sources: existing 3D models stored within the project resources and Unity primitives. It is also feasible to obtain online free models in real-time, as demonstrated in Section 4 of LLMR (see [6]). As the extension deviates from the main contribution of this paper, we left it as an optional module for future development.\nAs discussed in Section 4.1, the refined prompt includes contextual data, such as the names of available prefab resources in the project. By analyzing this data alongside the user's request, the LLM model determines the source for object creation. Typically, it prefers using available prefabs unless otherwise specified by the user. If no appropriate local resource is available, the model generates JSON data to construct the target object using Unity primitives. The refined response for such requests typically includes the prefab name, the created object's name, and other standard properties like position, orientation, color, scale, etc., most of which are optional. The Virtual Object Creator uses this JSON data to execute backend scripts that add virtual content to the scene. An example of creating a simple cube using the Virtual Object Creator is illustrated in Fig. 4. Besides creating simple objects from Unity primitives, we also provide examples of complex objects, e.g., the sports car and the table tennis table, using local resources, as shown in Fig. 5. Note that the object name serves as the primary identifier for most interactions within LLMER. Therefore, we prompt the LLM to generate a descriptive name that incorporates distinct information, facilitating easier identification and interaction.\nTo mitigate the 'hallucination' issue often associated with LLMs, especially in objects composed of multiple parts (e.g., creating a car using Unity primitives may involve multiple cubes and spheres in various sizes), we introduce some special properties. For instance, the 'parent' property allows the LLM to determine dependencies between objects and generate positions in local coordinates relative to the parent. This setup also facilitates rendering checks against physical constraints, with a post-processing step to adjust positions if violations (e.g., an office supply item floating on air or placed outside the table's boundary) are detected. Notably, this check excludes empty placeholder objects used solely for coordinate referencing.\nAdditionally, we integrate Unity's physics engine through an optional 'physics' property. Activating this property adds a rigid body to the object, if absent, and enables the use of 'gravity' feature to prevent unnatural phenomena like floating objects. This property can be deactivated, particularly for objects created from Unity primitives where physical interactions are not expected. The LLM model dynamically manages these properties based on user requests to enhance the immersive experience in the XR environment."}, {"title": "Animation Library", "content": "Static environment deviates from the immersive experience pursued in most XR applications. Thus, another fundamental functionality of LLMER is to dynamically add animations to objects within the scene. Users can request actions such as movement or rotation of specified objects. LLMER supports these interactions through its Animation Library, which maintains a series of predefined animation units that can be combined to create complex animations.\nThe Animation Library functions similarly to the Virtual Object Creator, transforming JSON data from refined responses into real-time animations within the XR scene. Our integration with the Unity animation system allows the following animation units:\n\u2022 Translate: Move an object to a target position.\n\u2022 Rotate: Change an object's orientation or rotate it around an axis by a specified degree.\n\u2022 Gaze: Let an object's orientation change with the position of another object.\n\u2022 Orbit: Rotate an object around another.\n\u2022 Scaling: Increase or decrease an object's size.\n\u2022 Coloring: Transit an object's color towards the target.\n\u2022 Attach: Designate an object as a child to another, recording the previous parent for potential detachment.\n\u2022 Detach: Revert an object's parent to its previous state or set it to null.\n\u2022 Catch: A complex sequence combining translation, rotation, attach, and detach, designed for the virtual avatar to mimic interactions like bringing objects somewhere.\n\u2022 Stop: Cease a running animation.\n\u2022 Destroy: Remove an object from the scene.\nNote that these animation units can be used individually or flexibly combined by LLMER to create a vast range of interactions based on user requests. For example, in response to a request to simulate a solar system, LLMER automatically selects and applies animation units such as Rotate, Orbit, Scaling, and Coloring to meet the specified requirements. Users are not required to have any prior knowledge of XR application development or adhere to certain rules when creating animations; they simply express their requests intuitively, and LLMER analyzes and handles the remaining workflow. Additional animation units can be easily integrated into the library to accommodate different game engines or development environments. Through extensive testing, we have found that the existing animation units adequately cover the most commonly used animations in XR environments.\nIn addition to specifying the action type, the LLM provides additional properties for more personalized animations, such as the object's name, target position, orientation, color, and attributes like speed and duration. The refined response needs to include these details, which are drawn from the context provided by the Context Library. The LLM's ability to recognize objects through vague commands, like \u201cthe closest cube to my position,\" is also enabled by the Context Library, as discussed in Section 4.1. Fig. 6 illustrates an example of calling the Animation Library during runtime.\""}, {"title": "Reality Fusion Engine", "content": "Previous work in integrating LLM with XR lacks deep fusion with real-world objects, including contextual understanding of the real-world surroundings and interaction with the user's body. LLMER explicitly addresses these limitations by introducing the Reality Fusion Engine, which seamlessly integrates real-world objects into virtual contexts and leads to more immersive interactions than traditional setups, as showcased in Fig. 7. While the current implementation relies on Meta XR All-in-One SDK, it is designed to be adaptable to other platforms, such as Apple's RoomPlan API.\nUtilizing the Scene API from the Meta XR All-in-One SDK, we assume that the user has previously scanned their environment, allowing for real-time matching with the room model. In this setup, real-world objects such as tables and storage units are represented by invisible objects equipped with precise colliders that facilitate interaction with our virtual models. These objects are assigned generic names (e.g., invisible plane, invisible volume), which prevent them from precise identification via names. However, they are tagged with descriptive labels that, along with their positions, orientations, and sizes, are cataloged in the Context Library to support immersive interactions.\nImplementing XR-specific actions like picking up or throwing objects using the user's hands poses significant challenges for an LLM in generating bug-free scripts quickly. To address this, we create prefabs for the building blocks, like grabbability, which can be instantiated and attached to virtual objects to enable interaction during runtime through specific JSON data.\nFurthermore, we incorporate the hand-tracking results into the contextual information, which enhances user interaction by allowing for more precise and immersive engagements with the user's body. For instance, we can place objects directly in the user's palm, with their position dynamically adjusting to the movement of the user's hand. The contextual information also provides detailed tracking for each finger's bone, enabling fine-grained operations with the fingers."}, {"title": "LLM Wrapper", "content": "To seamlessly integrate various modules with the LLM, we propose the LLM Wrapper, which serves as a connector for communicating between the remote AI server and the designed modules.\nAs outlined in Section 3, our system utilizes three distinct models for handling various types of input and output: the Whisper model for audio transcription, the GPT-4 model for generating responses given the provided prompts, and the TTS model for text-to-speech processing. The Whisper model operates independently, processing only audio recordings. The responses from the GPT-4 model, consisting of a mix of plain text and JSON data, are segmented by punctuation to isolate plain texts, which are then forwarded to the TTS model for vocalization.\nUpon receiving transcribed texts from the user's audio inputs, the LLM wrapper orchestrates the workflow through two main phases for each request: the initial and the refined stages. In each stage, it encapsulates the user's request and contextual information into predefined prompts. These prompts include a JSON schema that directs the LLM to produce appropriate JSON data. The JSON data generated by the LLM are parsed within the wrapper and utilized to trigger corresponding modules, facilitating immersive interactions in the XR environment. The parsing process can be treated as a validation process on whether the generated JSON output meets the required structure and semantics. Invalid JSON data will lead to a log of warnings and no response to the user's request, to some extent degrading the user's experience. To the benefit of our well-structured prompts and the powerful functionality of the GPT-4 model, the generation of invalid JSON data is pretty rare throughout our testing. Moreover, the latest LLMs, such as GPT-4 Omni and OpenAI 01, support structured outputs [21], which ensure responses adhere to the provided JSON schema."}, {"title": "SYSTEM IMPLEMENTATION", "content": "LLMER is built upon the Meta XR All-in-One SDK in Unity, tailored for the commercial off-the-shelf XR headsets, e.g., Meta Quest 3. We offer two operational modes for users: one allows running the application directly from the Unity Editor by connecting the headset to a computer via cables, and the other supports deploying it to the mobile device for a link-free experience. In both configurations, users can interact with the real world and access all functionalities outlined in Sections 3 and 4. For user studies, we opt for the linked mode to facilitate real-time monitoring of the user status, as we will be able to provide timely help when needed.\nIn addition to the main application written in C# within Unity, we have developed a forward server in Python. This server acts as an intermediary to handle communications between the user and OpenAI's cloud server via the official API. It processes prompts generated by the LLM wrapper, sends them to the cloud, and relays responses back to the user. The communication between the forward server and the user is established through socket programming using the Transmission Control Protocol (TCP).\nFor the user study, LLMER runs on a Dell workstation equipped with an Intel Core i7-11700K CPU @ 3.60GHz, an NVIDIA GeForce RTX 3070 Graphics Card, 32 GB memory, 1 TB of disk space, and Windows 11 Enterprise. The forward server is also hosted on this machine, ensuring robust and efficient handling of data transmission via the local network."}, {"title": "USER STUDY", "content": "We have recruited eight graduate students (three females, five males; mean age: 26.5 years) for our user study\u00b2, following guidelines for \"debugging\" tests as recommended in prior work (see [2]). Those students are all recruited at the Pennsylvania State University through email propaganda. The participants represent diverse academic backgrounds, including computer science, electrical engineering, and materials science. Their familiarity with XR technology varies: three are novices, who are aware of XR concepts but have no practical experience; two have intermediate experience, being familiar with XR and having used XR products; and three are experts, possessing in-depth knowledge of state-of-the-art advancements in XR. This participant profile is selected to align with the target users of our proposed framework-individuals innovating in education and gaming who may have varying levels of familiarity with XR and a diverse range of backgrounds. The primary purpose of our user study is to reveal the feasibility of leveraging LLM-generated JSON data to execute various XR tasks with simple natural language input. The study may not evaluate every facet of the system but will explore the potential and indicate the opportunities for future optimization. We will also analyze the insensitive statistics, e.g., the consumed tokens and response generation time, to demonstrate the efficiency of the proposed system.\nDuring the study, participants can sit down or stand wearing a Meta Quest 3 HMD. They enter an XR world that maps the real-world environments. They can see a green canvas to initiate communication with LLMER, along with a virtual agent with which they can interact. We define seven tasks for each participant to finish as follows:\n1. Car creation. Create a small car and adjust the size.\n2. Office supplies placement. Place multiple office supplies on top of a real-world table.\n3. Cube movement. Create a cube in some color and let the cube move towards a specific position.\n4. Solar system. Create a simple dynamic solar system, enabling orbiting and self-rotation.\n5. Virtual agent delivery. Let the agent catch something for the user.\n6. Get agent's attention. Create a grabbable cube and move it to the table while keeping the agent's gaze on it. Observe how the agent's head follows the cube's movement.\n7. Recognizable drawing. Create a whiteboard, perform a 2D drawing on it, and let the agent recognize the drawing. The user may change the color of the tip during drawing or clear the drawing by both voice and eraser.\nThose tasks cover a range of topics with various difficulty levels. Tasks 1 and Task 2 are both 3D creations, where Task 1 creates complex objects, and Task 2 involves understanding the real world. Task 3 and Task 4 are 3D animations in different categories. Task 5 and Task 6 showcase interaction with the virtual agent. Task 7 is more advanced and involves the vision-based recognition of various drawings. Note that compared with tasks defined in [9], our tasks share some commonality but involve more complex components, aiming to evaluate the interactivity and the capability of LLM to understand real-world environments.\nAt the beginning of the user study, participants are provided with a concise system overview. This is followed by a 10-minute training session, allowing them to communicate with the virtual agent to execute simple tasks such as creating a cube, coloring it, and moving it. Subsequently, participants are requested to complete each task in order."}, {"title": "RESULTS AND ANALYSIS", "content": "In this section, we perform a comprehensive analysis based on non-sensitive data collected during the user study. Specifically, we analyze the consumed tokens and various task completion metrics as quantitative indicators for comparison with other approaches. Additionally, we examine questionnaire scores and interview responses to assess the usability of the proposed system and explore opportunities for further optimization."}, {"title": "Cost Analysis", "content": "Recall that we collect the token consumption and response generation time for each user request in LLMER. Note that users may submit multiple requests per task until satisfied; however, for a fair comparison with existing works, we present single-request metrics here, with task-level metrics discussed in subsequent subsections. On the other hand, LLMER does not compile code during runtime, resulting in a zero error rate as defined in [6] and [9]. Thus, no error rate measurements are necessary for LLMER, as it inherently eliminates runtime compilation errors typical in similar technologies.\nOn average, LLMER consumes about 3,200 input tokens and 80 output tokens per request, totaling less than 3,300 tokens. This results in over 80% reduction compared to around 30,000 tokens required per request by running similar requests on the open-sourced version of LLMR ([6]). The reduced token consumption in LLMER can be attributed to the strategic design of the Context Library and the use of JSON data generation rather than script creation.\nAdditionally, the average response generation time for tasks in LLMER is only 10.35 seconds, substantially quicker than the 20 to 90 seconds reported for various baseline models in LLMR ([6]). This improved response time is likely due to the lower token count and the compact subspace of generating JSON data over coding scripts. Although the requests compared may not be identical, they are similar enough at a high level to validate the significant performance enhancements achieved by LLMER."}, {"title": "Subjective Usability Metrics", "content": "Task Completion Time. The task completion time is measured from the moment the participant begins the task to when they indicate its completion. The average value and standard deviation of all participants for each task are shown in Table 1. Intuitively, it will be much longer than the response generation time of LLMs as it also includes the user's actions and thinking time. Besides, the deviation is mainly caused by the variations in user behavior. For instance, when the task is complicated, some users tend to use one sentence to describe their request, while some users like to split the request into several small requests, thus resulting in more rounds for our system to finish the task. Recall that we design these distinct tasks to encompass a range of XR activities and demonstrate the efficiency and versatility of LLMER."}, {"title": "Questionnaire", "content": "As depicted in Fig. 11, we can observe that all participants provided positive feedback in the questionnaire regarding usability, immersion, interactivity, future use, and overall experience (specifically, the average score of each question \u2265 5.25). Users are particularly impressed by the system's interactivity and immersion. They are very satisfied with the overall XR experience. In terms of usability aspects like responsiveness, ease of operation, and accuracy, the feedback is moderately positive, indicating that there is still potential for improvement to enhance user satisfaction."}, {"title": "Semi-Structured Interview", "content": ""}, {"title": "User Satisfaction and Surprise", "content": "Participants value the intuitive controls and the realistic interactions, which enhance their sense of presence within the XR environment. Many report a deep sense of immersion. Novice XR users find the experience particularly surprising and new, especially enjoying the engaging Tasks 5 to 7. One user remarks, \u201cI love this system's imagination and possibilities. In Task 7, although my drawing wasn't great, the system impressively recognized and accurately transformed it into an object.\u201d. Another user highlights his surprise to interact with the virtual agent (virtual robot) set in XR and comments,\u201d It's fascinating to interact with the robot so immersively, especially being able to control where the robot focuses its attention.\u201d."}, {"title": "User Critiques and Areas for Improvement", "content": "Many users have criticized the system for lacking the capacity to learn about context and sometimes being memoryless. One user points out \u201cThis system can't remember the previous request. For example, if I asked this system to create a sphere. Then, if we want to manipulate this sphere again, if I just use 'it' to represent the sphere created before, the system could fail to understand that 'it' represents the sphere\u201d. Another limitation of our system is that 3D objects created by our system can be too simple and abstract. It is a consensus from three of the participants who all think the car created in Task 1 is abstract. This problem is caused by the limitation of assembling simple 3D objects such as cubes and spheres to create items requested by users. Additionally, the gesture to control the voice input can be ineffective for some users. One user comments \u201cMy gestures are not easily captured by the system\u201d. It is time-consuming for those users whose gestures are not easily detected by the system to finish each task. Furthermore, the current system also can't fully understand the real-world environment. For example, in Task 2, many users complain it's hard to find the table accurately in the real world. Even if our system has its current limitations, our participants still have a positive attitude toward the future use of our system. One user comments \u201cIf the system can achieve better performance on understanding context, the 3D creation and animation in XR can be applied to factory assembly.\u201d."}, {"title": "User Study Insights", "content": "Overall, the subjective usability metrics provide further positive feedback, with most participants rating the system highly across dimensions such as immersion, interactivity, and overall satisfaction. The average scores across all questions are consistently above 5.25 on a 7-point Likert scale. Besides, the user study yields a high task fulfillment ratio and a low number of help requests. All of those indicate the effectiveness of the proposed system, even for users without much programming knowledge or XR experience. Moreover, the statistics analysis reveals an over 80% reduction of consumed tokens for each user request and around 60% reduction in task completion time compared with other state-of-the-art works.\nThe semi-structured interviews with users also highlight satisfaction with the system's intuitive controls and immersive experience, particularly among novice users. However, we also identify some opportunities for further optimization, including the system's inability to retain contextual memory across requests and the undesirable appearance of generated 3D objects. Additionally, gesture-based inputs are noted as occasionally ineffective, impacting task completion for certain users. Despite these challenges, participants express optimism about the future potential of LLMER, particularly in real-world applications such as factory assembly, provided further improvements in contextual understanding and object creation are made."}, {"title": "LIMITATIONS AND FUTURE WORK", "content": "In this section, we discuss several unresolved aspects of our current work and outline potential directions for future research.\nImpact of different LLMs. Previous studies on code generation (see [6, 9"}]}