{"title": "Improving LLM Reasoning with Multi-Agent Tree-of-Thought Validator Agent", "authors": ["Fatemeh Haji", "Maryam Tabar", "Anthony Rios", "Mazal Bethany", "Jason Chiang", "Peyman Najafirad"], "abstract": "Multi-agent strategies have emerged as a promising approach to enhance the reasoning abilities of Large Language Models (LLMs) by assigning specialized roles in the problem-solving process. Concurrently, Tree of Thoughts (ToT) methods have shown potential in improving reasoning for complex question-answering tasks by exploring diverse reasoning paths. A critical limitation in multi-agent reasoning is the 'Reasoner' agent's shallow exploration of reasoning paths. While ToT strategies could help mitigate this problem, they may generate flawed reasoning branches, which could harm the trustworthiness of the final answer. To leverage the strengths of both multi-agent reasoning and ToT strategies, we introduce a novel approach combining ToT-based Reasoner agents with a Thought Validator agent. Multiple Reasoner agents operate in parallel, employing ToT to explore diverse reasoning paths. The Thought Validator then scrutinizes these paths, considering a Reasoner's conclusion only if its reasoning is valid. This method enables a more robust voting strategy by discarding faulty reasoning paths, enhancing the system's ability to tackle tasks requiring systematic and trustworthy reasoning. Our method demonstrates superior performance compared to existing techniques when evaluated on the GSM8K dataset, outperforming the standard ToT strategy by an average 5.6% across four LLMs.", "sections": [{"title": "Introduction", "content": "LLMs have demonstrated remarkable capabilities across various tasks, yet they often struggle with complex reasoning, particularly in situations where human-like reasoning capabilities are crucial Zelikman et al. [2023]. To address this, multi-agent strategies have emerged as a promising method to enhance LLM reasoning. Using multi-agent strategies, multiple specialized agents collaborate, with each agent assigned distinct roles in the problem-solving process. By allowing different agents to tackle various aspects of a task, we they are able to utilize specialized expertise to each phase of the task to improve performance Guo et al. [2024]. This has been shown to improve the quality of answers in reasoning-intensive domains. However, despite the promise of multi-agent reasoning, one"}, {"title": "Background", "content": "critical limitation remains: Reasoner agents often explore reasoning paths shallowly, failing to fully consider the complexity of the problem space. Tree of Thoughts (ToT) methods offer a potential solution to this limitation by encouraging a more systematic exploration of multiple reasoning paths. TOT allows LLMs to simulate human-like thought processes by branching out and examining various possibilities before converging on a solution Yao et al. [2023]. By enabling LLMs to consider diverse reasoning pathways, ToT can mitigate the shallow exploration issue seen in some other multi-agent systems. However, while ToT encourages exploration, it also introduces a new challenge: the risk of generating flawed reasoning branches. Without proper validation, these erroneous paths could lower the overall trustworthiness of the final answer.\nTo address these challenges, we propose a novel approach that combines the strengths of multi-agent reasoning with ToT while introducing a critical validation mechanism. In our framework, multiple Reasoner agents operate in parallel, each employing ToT to explore different reasoning paths. These Reasoner agents are supported by a Thought Validator agent, which evaluates the proposed reasoning branches produced by the Reasoners. The Validator discards faulty reasoning branches, ensuring that only logically sound paths contribute to the final decision. This approach allows for both exploration of the problem space and increased reliability of the answers by eliminating flawed reasoning paths before they can impact the outcome. Our proposed approach is evaluated on the GSM8K dataset Cobbe et al., a benchmark known for its challenging arithmetic reasoning tasks. Results show that our method outperforms existing techniques, demonstrating improved accuracy and trustworthiness in solving complex reasoning problems.\nOur key contributions are as follows:\n\u2022 The integration of ToT into a multi-agent reasoning framework.\n\u2022 The introduction of a novel Thought Validator agent that evaluates and filters reasoning branches produced by Reasoner agents.\n\u2022 Experimental results on the GSM8K dataset demonstrating improved accuracy and performance in complex arithmetic reasoning tasks compared to existing techniques."}, {"title": "2.1 Multi-agent Systems for Enhancing LLM Reasoning", "content": "By distributing tasks among multiple agents, multi-agent systems aim to improve performance on reasoning-based tasks Du et al., Talebirad and Nadiri. For example, CausalGPT Tang et al. introduces evaluative layers to verify the reasoning branches produced by LLMs, while the Counterfactual Multi-Agent Debate (CFMAD) framework Fang et al. provides an innovative method to mitigate the potentially biased reasoning branches of LLMs by assigning agents to fixed roles to generate justifications from specific perspectives, and a third-party judge evaluates these arguments to decide the most rational outcome. Despite these advancements, current methods still suffer from shallow sampling of reasoning paths or majority vote schemes. These techniques can overlook critical inferential errors and are especially prone to early-stage errors, which can propagate through multiple rounds of reasoning. This limitation is especially problematic in complex scenarios where systematically evaluating and eliminating incorrect options is crucial. Recent research has demonstrated that LLMs can effectively identify both factual and inferential mistakes Li et al., making the integration of a dedicated verification component in multi-agent systems particularly beneficial for assessing the faithfulness and reliability of generated solutions."}, {"title": "2.2 The Role of the 'Reasoner' Agent in Multi-Agent Frameworks", "content": "Within multi-agent architectures, the Reasoner agent plays a pivotal role. It serves as the system's core decision-maker, ensuring that valid conclusions are derived from the reasoning process. However, Reasoner agents in current frameworks often struggle to systematically evaluate and eliminate incorrect reasoning paths, particularly in more challenging problem spaces. This bottleneck highlights the need for more advanced reasoning strategies to be integrated into the Reasoner agent. CFMAD has also shown that checking all available options can enhance the overall ability of the multi-agent systems Fang et al.."}, {"title": "3 Method", "content": "We propose a novel multi-agent reasoning framework that integrates the ToT strategy with a robust validation mechanism to enhance complex problem-solving. Our approach employs multiple concurrent Reasoner agents, each using ToT to explore diverse reasoning paths. At each tree level, a state evaluation agent scores the generated reasoning, with the highest-scored reasoning expanded in the subsequent level. Upon reaching the final tree level, each Reasoner agent produces a proposed reasoning chain composed of the chain of the highest-scored reasoning in the tree. These reasoning branches are then independently assessed by a Thought Validator agent to either validate or invalidate the proposed reasoning. We then use a consensus-based voting mechanism, where verified reasoning paths contribute to the vote, and invalidated ones are abstained. If consensus is not reached, we initiate a new reasoning round, incorporating feedback from the Thought Validator on the reasoning branch to refine the next reasoning round. Our proposed framework is illustrated in Figure 1."}, {"title": "3.1 Reasoner Agent", "content": "The Reasoner agents in our multi-agent architecture employ the ToT strategy, which enables structured exploration of reasoning paths in parallel. ToT improves upon Chain of Thought (CoT) prompting Wei et al. by enabling parallel exploration and dynamic path evaluation. While CoT follows a single, linear path, ToT actively explores and evaluates multiple reasoning paths, making it better suited for complex problems that benefit from diverse thought exploration Yao et al.. We formalize the reasoning process as a search over a tree of states. Let $Q$ denote the input prompt or query, and each Reasoner agent $R_i$ constructs a Tree of Thoughts $T_i(Q)$, where each node represents a state $s_t$, which is a distinct point along a reasoning path. A state $s_t$ consists of the problem $Q$ and a sequence of intermediate reasoning steps up to that point $z_1, z_2, ..., z_t$, with each step $z_j$ being a coherent unit of reasoning generated by the language model.\n$s_t = [Q, z_1, z_2, ..., z_t]$"}, {"title": "Step 1: Decomposition and Generation of Thought Paths", "content": "The process is decomposed into intermediate thought steps using LLM prompting. For each state $s_t$, the next potential thought $z_{t+1}$ is generated by the Thought Generator $G(p_e, s_t, k)$, where $p_e$ denotes the language model. The Reasoner agents explore multiple branches from any given state $s_t$, corresponding to different continuations of the reasoning process. This approach ensures that the exploration process covers a diverse range of possible solutions, avoiding the linearity of CoT and allowing reconsideration of earlier steps."}, {"title": "Step 2: State Evaluation and Path Selection", "content": "To evaluate each state $s_t$, we introduce a state evaluation agent that assigns a score to the generated reasoning. This evaluation can be implemented through prompting, where the state evaluation agent assesses the quality and potential of each reasoning step. At each tree level, the highest-scored reasoning is selected for expansion in the subsequent level. This process continues until the final tree"}, {"title": "Step 3: Reasoning Branch Construction", "content": "level is reached. The selection mechanism can be formalized as:\n$s_{t+1}^* = \\arg \\max_{s_{t+1}} V(p_e, s_{t+1})$\nwhere $V(p_e, s_{t+1})$ is the evaluation score assigned by the state evaluation agent.\nUpon reaching the final tree level, each Reasoner agent constructs a proposed reasoning chain. This chain is composed of the highest-scored reasoning steps from each level of the tree. Formally, the reasoning branch $C_i$ for Reasoner Agent $R_i$ can be represented as:\n$C_i = [z_1^*, z_2^*, ..., z_T^*]$\nwhere $z_t^*$ is the highest-scored reasoning step at level $t$ of the tree."}, {"title": "3.2 Thought Validator Agent", "content": "The Thought Validator agent, inspired by the role of a teacher providing feedback to students, plays a crucial role in assessing the validity of the reasoning branches produced by the Reasoner agents. Much like a teacher helping students refine their answers, this agent independently evaluates each proposed reasoning branch to either validate or invalidate it. For each reasoning branch $C_i$, the Thought Validator agent performs several key steps. It begins with a logical consistency check to evaluate the internal logic and coherence of the reasoning chain, similar to how a teacher might assess a student's argument. This is followed by a factual accuracy assessment to verify any factual claims made within the reasoning, akin to a teacher fact-checking a student's work. Finally, the agent conducts a completeness evaluation to ensure that the reasoning branch adequately addresses all aspects of the original query, much as a teacher would ensure a student's response fully answers the question. Through this comprehensive process, the Thought Validator agent ensures the robustness and reliability of the reasoning branches, ultimately helping to improve the quality of the final output. Based on these assessments, the Thought Validator assigns a binary validation status $V_i$ to each reasoning chain:\n$V_i = \\begin{cases} 1 & \\text{if } C_i \\text{ is validated} \\\\ 0 & \\text{if } C_i \\text{ is invalidated} \\end{cases}$"}, {"title": "Consensus-Based Voting Mechanism", "content": "After the validation process, we employ a consensus-based voting mechanism to determine the final outcome. Only validated reasoning branches contribute to the vote, while invalidated ones are abstained. The consensus solution $S^*$ can be represented as:\n$S^* = \\arg \\max_S \\sum_{i=1}^N V_i \\cdot \\delta(S = S_i)$\nWhere $S_i$ represents the solution derived from reasoning branch $C_i$, $V_i$ is the validation status of $C_i$, $\\delta$ is an indicator function that returns 1 if the solutions match and 0 otherwise, and $N$ is the total number of Reasoner agents."}, {"title": "3.3 Iterative Refinement", "content": "If consensus is not reached (i.e., no solution receives a majority of validated votes), we initiate a new reasoning round. This refinement process incorporates feedback from the Thought Validator on the reasoning branches to guide the next iteration. This iterative process continues until consensus is reached or a predefined maximum number of iterations is exceeded."}, {"title": "4 Experiments", "content": "Dataset: GSM8K Cobbe et al. is a dataset of 8.5K high-quality linguistically diverse grade school math word problems created by human problem writers. GSM8K is widely recognized as a benchmark for testing arithmetic reasoning in LLMs. The dataset comprises complex, multi-step mathematical word problems that challenge both the reasoning and computation capabilities of LLMs. Our"}, {"title": "5 Limitations and Conclusion", "content": "While the ToT approach has shown promise in enhancing reasoning capabilities, our observations of the outputs and reasoning trees revealed several limitations that warrant further investigation. A key challenge we observed is the lack of dynamic exploration in the search space. The ToT method proposed by Yao et al. [2023] employs a fixed width and depth for the tree structure, which our analysis showed can lead to suboptimal performance in certain scenarios. For instance, when examining the reasoning trees for problems that could be solved efficiently without extensive reasoning, we found that the predetermined depth of exploration often introduced unnecessary complexity, potentially leading to errors or confusion in the reasoning process. Conversely, for problems requiring more in-depth analysis, we observed that the fixed depth proved insufficient, limiting the model's ability to fully explore complex reasoning paths. Additionally, our proposed approach, while addressing some"}, {"title": "6 Social Impact Statement", "content": "of these limitations, is computationally expensive due to the use of the ToT method, which requires significant resources for generating and evaluating multiple thought paths.\nIn conclusion, we have presented a novel approach that combines the ToT strategy with a multi-agent reasoning framework enhanced by a Thought Validator agent. Our method addresses key limitations in existing reasoning strategies for LLMs by enabling a more systematic exploration of reasoning paths while simultaneously improving the reliability of generated solutions. Experimental results on the GSM8K dataset demonstrate that our approach outperforms state-of-the-art methods, particularly for complex arithmetic reasoning tasks.\nBy improving the depth of reasoning and enabling more systematic option elimination, our approach could lead to more trustworthy AI applications. However, these advancements also raise ethical considerations regarding the deployment of highly autonomous reasoning systems, particularly in high-stakes domains. It is essential to carefully manage the use of such systems to avoid over-reliance on AI, ensuring that human oversight and accountability remain integral to decision-making processes. Additionally, the broader societal implications must be monitored to prevent unintended consequences, such as biases being amplified through algorithmic decision-making or the replacement of human expertise in fields where nuanced judgment is required."}, {"title": "Appendix", "content": "Experiment Prompts\nIn our experiments, we designed a number of carefully crafted prompts to guide language models during reasoning tasks. Here are the key prompts used and their purposes:"}, {"title": "Standard Input-Output (IO) Prompt", "content": "The standard IO prompt is used as a baseline approach:\nAnswer the following math problem. Your response should\nconclude with \"the answer is n\", where n is a number:\n{input}\nThis prompt directly asks the model to solve the math problem and provide the answer in a specific format."}, {"title": "Chain of Thought (CoT) Prompt", "content": "The CoT prompt encourages the model to show its reasoning:\nAnswer the following question: {input}\nMake a strategy, then write. Your output should be in\nthe following format:\nStrategy:\nYour strategy about how to answer the question.\nAnswer:\nYour answer to the question. It should end with\n\"the answer is n\", where n is a number.\nThis prompt explicitly asks the model to formulate a strategy before providing an answer, leading to a more structured thought process."}, {"title": "Tree of Thoughts (ToT) Prompt", "content": "Our implementation of ToT is inspired by the approach described by Yao et al. [2023] but with specific modifications tailored to our multi-agent framework. The ToT method uses the CoT prompt as a base and applies it iteratively, allowing for branching and exploration of multiple reasoning paths. Our implementation includes the following components:\n1. Thought Generation: We use the 'sample' method for generating thoughts. This method uses the CoT prompt as a base but applies it iteratively, allowing for branching and exploration of multiple reasoning paths. The prompt flow for ToT includes:\nAnswer the following question: {input}\nMake a strategy, then write. Your output should be in\nthe following format:\nStrategy:\nYour strategy about how to answer the question.\nAnswer:\nYour answer to the question. It should end with\n\"the answer is n\", where n is a number.\n2. State Evaluation: For evaluating the generated thoughts, we employ the 'vote' method. This involves using a prompt to assign votes to different reasoning paths:\nGiven an instruction and several choices, decide which\nchoice is most promising. Analyze each choice in detail,\nthen conclude in the last line \"The best choice is {s}\",\nwhere s the integer id of the choice.\n3. Path Selection: We use the 'greedy' method for selecting the most promising paths to expand further. This doesn't involve a specific prompt but rather selection of the highest-scored paths from the evaluation step.\nEach step involves multiple API calls to the language model, with the generated thoughts and their evaluations guiding the exploration of the reasoning space. This approach allows for a dynamic and adaptive exploration of potential solution paths, enhancing the model's ability to tackle complex reasoning tasks."}, {"title": "Verifier Prompt", "content": "A crucial component of our approach is the Thought Validator agent, which uses the following prompt:\nAs a critical mathematical reasoning verifier, evaluate\nthe following thought process, which builds upon previous\nsteps to reach a final conclusion. Focus on:\n1. **Question Relevance**:\nEnsure the entire reasoning process directly\naddresses the original question.\nCheck if the final answer actually solves what\nwas asked.\n2. **Reasoning Progression**:\nAssess logical flow and consistency, especially\nin final steps.\nVerify mathematical operations' correctness and\nappropriateness.\nIdentify logical fallacies or unjustified leaps."}, {"title": "Examples", "content": "3. **Factual Accuracy**:\nCheck accuracy and relevance of facts and numbers,\nparticularly in final calculations.\nSpot any misuse of mathematical concepts.\n4. **Completeness**:\nEnsure all necessary aspects are addressed,\nparticularly in concluding thoughts.\nIdentify significant omissions that could affect\nthe result.\n5. **Critical Assessment**:\nActively seek potential errors or weak points.\nDon't hesitate to invalidate reasoning if\nsignificant issues are found.\nProvide a holistic evaluation of the entire reasoning\nprocess, from start to finish. Conclude with\n\"Reasoning is Valid\" only if the entire process is\nrelevant, logically sound, and error-free. Otherwise,\nconclude with \"Reasoning is Invalid\" and briefly\nexplain why.\nThis comprehensive prompt guides the Verifier in thoroughly assessing the validity of the reasoning process, ensuring that the final answer is not only correct but also logically sound and relevant to the original question.\nTo demonstrate the effectiveness of our approach, we show a challenging example from the GSM8K dataset using the gpt-3.5-turbo model. Using this example, we can see how the Thought Validator Agent prevents incorrect reasoning from the ToT Reasoner agents from leading to errors in the final answer.\nProblem 1: Last month, Tasha made $80 from selling lemonade and mowing lawns. The first week, she mowed Kamala's lawn three times as many times as Joe's. The following week, she mowed Alba's lawn five times as Joe's. If Joe paid Tasha $6 for her work, how much did she make from lemonade sales? Answer: 26.\nWe have three rounds, each involving three Reasoner agents (R1, R2, and R3). After each round, the Thought Validator Agent evaluates their reasoning."}]}