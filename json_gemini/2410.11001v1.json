{"title": "GRAPH OF RECORDS: BOOSTING RETRIEVAL AUGMENTED GENERATION FOR LONG-CONTEXT SUMMARIZATION WITH GRAPHS", "authors": ["Haozhen Zhang", "Tao Feng", "Jiaxuan You"], "abstract": "Retrieval-augmented generation (RAG) has revitalized Large Language Models (LLMs) by injecting non-parametric factual knowledge. Compared with long-context LLMS, RAG is considered an effective summarization tool in a more concise and lightweight manner, which can interact with LLMs multiple times using diverse queries to get comprehensive responses. However, the LLM-generated historical responses, which contain potentially insightful information, are largely neglected and discarded by existing approaches, leading to suboptimal results. In this paper, we propose graph of records (GoR), which leverages historical responses generated by LLMs to enhance RAG for long-context global summarization. Inspired by the retrieve-then-generate paradigm of RAG, we construct a graph by establishing an edge between the retrieved text chunks and the corresponding LLM-generated response. To further uncover the intricate correlations between them, GoR further features a graph neural network and an elaborately designed BERTScore-based objective for self-supervised model training, enabling seamless supervision signal backpropagation between reference summaries and node embeddings. We comprehensively compare GoR with 12 baselines across four long-context summarization datasets, and the results indicate that our proposed method reaches the best performance (e.g., 15%, 8%, and 19% improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP dataset). Extensive experiments further demonstrate the effectiveness of GoR.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have recently achieved remarkable performance across sorts of language modeling tasks Achiam et al. (2023); AI@Meta (2024). Among them, the long-context global summarization task is of great importance, which requires ultra-long context understanding capabilities of LLMs Li et al. (2024a); Liu et al. (2024b). Current attempts to accomplish this task mainly include long-context LLMs Touvron et al. (2023); GLM et al. (2024); Li* et al. (2023); Tworkowski et al. (2023) and retrieval-augmented generation (RAG) Ram et al. (2023); Yu et al. (2023); Trivedi et al. (2022); Jiang et al. (2023b); Asai et al. (2023). In comparison with long-context LLMs that expand their context window to accommodate long-context inputs, RAG performs a cost-effective retrieve-then-generate paradigm and provides a few retrieved short text chunks from a long document to LLMs. In a running RAG system (Figure 1), there are usually a large number of historical user queries and LLM-generated responses for a long document. Nevertheless, these historical responses, which contain informative task-related content, are mostly neglected without sufficient utilization by current RAG approaches.\nUnfortunately, utilizing LLM historical responses for long-context global summarization presents two major challenges. (1) Sophisticated yet implicit correlations between historical responses and text. Given a long document, there will inevitably be complicated correlations among plentiful user queries (e.g., logical correlations), which are further inherited by LLM-generated responses and the"}, {"title": "2 GRAPH OF RECORDS", "content": "In this section, we first present some necessary backgrounds in Section 2.1. Then, we describe our proposed method sequentially through two sections, i.e., Graph Construction (Section 2.2) and BERTScore-based Objective for Self-supervised Training (Section 2.3)."}, {"title": "2.1 PRELIMINARIES", "content": "Retrieval-augmented Generation. Retrieval-augmented Generation (RAG) Ram et al. (2023) can typically be summarized into the following two processes. (1) Retrieval. Give a long document which consists of several split text chunks $C = \\{c_i\\}_{i=1}^I$ as retrieval corpus, RAG first employs a retriever (e.g., Contriever Izacard et al. (2021)) to retrieve K text chunks that are most relevant to a given query q based on semantic similarity. The retriever typically embeds the query q and a text chunk c from C using a query encoder $E_q(\\cdot)$ and a context encoder $E_c(\\cdot)$, respectively, and quantify their semantic similarity by the dot product operation described as $Sim(q, c) = E_q(q)^T \\cdot E_c(c)$.\n(2) Generation. The retrieved text chunks are fed into LLMs with the query q to obtain the final response r. The whole process can be described as:\n$r = Generation(q, \\{c_1,\\dots, c_K\\}), \\{C_1,\\dots,C_K\\} = Retrieval(q|C)$                                                  (1)\nGraph Neural Networks. Graph Neural Networks (GNNs) Kipf & Welling (2016) stand out for their excellent representation learning ability on graph data. GNNs update node embeddings iteratively by aggregating messages from their neighboring nodes. Generally, the l-th layer of GNNs can be formalized as:\n$h_v^{(l)} = AGG^{(l)}(h_v^{(l-1)}, MSG^{(l)} (\\{h_u^{(l-1)}, u \\in N(v)\\} ; \\Theta_m^{(l)}) ; \\Theta_a^{(l)})$                                                                                                                                         (2)\nwhere $h_u^{(l)} \\in \\mathbb{R}^{d_l}$ is the embedding vector of nodes u in layer l and the embedding dimension is $d_l$. $MSG^{(l)} (\\cdot)$ is a message computation function parameterized by $\\Theta_m^{(l)}$ and $AGG^{(l)} (\\cdot)$ is a message aggregation function parameterized by $\\Theta_a^{(l)}$ in layer l."}, {"title": "2.2 GRAPH CONSTRUCTION", "content": "In this section, we describe how to organize LLM historical responses into a graph of records by simulating user queries.\nQuery Simulation. User queries play a very critical role in the design of GoR since LLM historical responses generated by lots of repetitive, nonsense, or meaningless questions are inherently not beneficial for summarization. One solution is to use doc2query Nogueira et al. (2019) to simulate queries for a long document, but the generated results inevitably suffer from simplicity and rigidity due to the limited text generation capabilities of T5 Raffel et al. (2020). To this end, we directly turn to LLMs for query simulation with temperature sampling instead of greedy decoding for generating meaningful, insightful, and diverse questions. Specifically, we split a long document into several text chunks C following the standard procedure of RAG and prompt LLMs to generate a query $q_s^i$ based on a randomly selected text chunk $c_s^i$. We repeat the above process until a certain number of non-duplicate queries are generated, which are gathered in pairs with the corresponding text chunks to form a corpus $T = \\{(q_s^i, c_s^i)\\}_{i=1}^T$ for further model training (Section 2.3). The prompt for query simulation can be found in Appendix B.\nOrganize LLM Historical Responses into A Graph. After obtaining simulated queries, we utilize them to perform RAG on the long document. LLM-generated responses during this process include informative and valuable understanding, summarizing, and answering of retrieved text chunks in the long document. Moreover, since there may exist sophisticated correlations among simulated queries, the text chunks and responses can inherit these features and potentially assist in answering a more comprehensive query, especially global summarization that needs to be understood from a holistic perspective. Nevertheless, it is a significant challenge to find correlations among complex and massive text at the linguistic level and the embeddings from language models (e.g., SBERT Reimers & Gurevych (2019)) or retrievers Karpukhin et al. (2020) focus on semantic similarity, which also suffers from poor performance in this case. To this end, we propose to break out of this dilemma by organizing these historical responses into a graph."}, {"title": "2.3 BERTSCORE-BASED OBJECTIVE FOR SELF-SUPERVISED TRAINING", "content": "So far, we have constructed a graph using LLM-generated historical responses during RAG given the simulated queries. The key in this section lies in designing a reasonable and effective objective function for model optimization. Considering that some random walk Grover & Leskovec (2016) or propagation-based Zhu & Ghahramani (2002) algorithms are not differentiable, we turn to graph neural networks (GNNs) for learning node embeddings, which are backpropagation-friendly. Intuitively, given a global summarization query q, our ultimate optimization goal is to make the learned node embeddings adaptively reflect the similarity with the query embedding $E_q(q)$ by taking the complicated correlations among nodes into account. However, in global summarization tasks, there are essentially no text chunk indices as labels to indicate which nodes are most relevant for a query since it needs to consider the long document as a whole. Another naive solution is to use global reference summaries as labels, but there is a gap in supervision signal backpropagation between them and node embeddings because we still need to find out which nodes are most relevant to them.\nTherefore, inspired by BERTScore Zhang et al. (2019), which measures the semantic similarity between the reference and the generated text, we propose to use it to rank all nodes based on the similarity with reference summaries. By this means, BERTScore fills the gap in the backpropagation"}, {"title": "Contrastive Loss Driven by BERTScore", "content": "Based on the above observations, we directly reuse the simulated queries $T = \\{(q_s^i, c_s^i)\\}_{i=1}^T$ to serve as self-supervised training data, in which the text chunk c is highly relevant to the query q and has more focused content. Given node embeddings output by the last L-th layer of GNNs, for the i-th query $q_s^i$, we rank them according to the similarity with the i-th text chunk $c_s^i$ and obtain a node embedding ranking list $M_i$, which can be described as:\n$M_i = [h_{c_s^i}^{(L)}, h_1^{(L)},..., h_{|C|-1}^{(L)}]$                                                                                      (4)\nwhere $h_{c_s^i}^{(L)}$ stands for the node embedding with highest similarity. Note that we utilize the context encoder $E_c(.)$ from the retriever to initialize node embeddings for simplicity. Then, we regard $h_{c_s^i}^{(L)}$ as the positive while the rest in $M_i$ as negative samples to conduct contrastive learning using InfoNCE van den Oord et al. (2018), which brings the query q and the positive sample $h_{c_s^i}^{(L)}$ closer in the semantic embedding space. We formulate the contrastive training objective as follows:\n$\\mathcal{L}_{CL} = - \\frac{1}{|T|} \\sum_{i=1}^{|T|} \\log \\frac{\\exp (E_q(q_s^i)^T h_{c_s^i}^{(L)} / \\tau)}{\\exp (E_q(q_s^i)^T h_{c_s^i}^{(L)} / \\tau) + \\sum_{h_j^{(L)} \\in M_i, j \\neq c_s^i} \\exp (E_q(q_s^i)^T h_{j}^{(L)} / \\tau)}$                                                                                                                      (5)\nwhere $\\tau$ is the temperature coefficient. Note that in the optimization pipeline of GoR, we conduct mini-batch training on the graph level, and each graph is associated with an independent self-supervised training dataset T. We also leverage in-batch negatives from other graphs since the nodes in them are completely irrelevant content from other long documents (it is not shown in Formula 5 for brevity)."}, {"title": "Auxiliary Pair-wise Ranking Loss", "content": "In the above-described contrastive loss $\\mathcal{L}_{CL}$, although we impose constraints on positive and negative samples, the ranking of negative samples themselves is not well utilized. Inspired by LambdaRank Burges (2010), we further introduce an auxiliary pair-wise ranking loss on the ranking list $M_i$, which can be formulated as:\n$\\mathcal{L}_{RANK} = \\frac{1}{|T|} \\sum_{i=1}^{|T|} \\sum_{h_j^{(L)}, h_k^{(L)} \\in M_i, r(h_j^{(L)}) > r(h_k^{(L)})} - \\log \\big(1 + e^{E_q(q_s^i)^T h_j^{(L)} - E_q(q_s^i)^T h_k^{(L)}}\\big)$                                                (6)\nwhere $r(\\cdot)$ denotes the ranking index (e.g., $r(h_j^{(L)}) < r(h_k^{(L)})$). Given $h_j^{(L)}, h_k^{(L)} \\in M_i$ that satisfies $r(h_j^{(L)}) > r(h_k^{(L)})$, the pair-wise ranking loss will explicitly optimize in the direction of $E_q(q_s^i)^T h_j^{(L)} < E_q(q_s^i)^T h_k^{(L)}$, thus imposing stricter constraints to the pair-wise ranking."}, {"title": "Overall Training Objective", "content": "To sum up, the overall training objective can be formulated as:\n$\\mathcal{L} = \\mathcal{L}_{CL} + \\alpha \\cdot \\mathcal{L}_{RANK}$                                                                                                                                                                                                                                                                                              (7)\nwhere $\\alpha \\in [0, 1]$ is a hyper-parameter. It is worth noting that GoR's training costs are lightweight since the only trainable module is GNNs, and no human-crafted labels are needed."}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 EXPERIMENTAL SETUP", "content": "Datasets. We evaluate our proposed method on four long-context summarization datasets, i.e., AcademicEval Feng et al. (2024), QMSum Zhong et al. (2021), WCEP Gholipour Ghalandari et al. (2020), and BookSum Kry\u015bci\u0144ski et al. (2021). Among them, AcademicEval collects scientific papers from arXiv for abstract writing, given the long inputs of its main body. QMSum is a query-based summarization dataset, and we only use \"general queries\" for evaluating global summarization. WCEP is a multi-document summarization dataset about news events, while BookSum"}, {"title": "3.2 MAIN RESULTS", "content": "We conduct comprehensive experiments on QMSum, AcademicEval, WCEP, and BookSum datasets compared with dozens of baselines to evaluate the long-context global summarization capabilities of our proposed method. The results are shown in Table 1.\nGoR consistently outperforms retriever-based methods. From Table 1, our proposed GoR beats sparse retrievers, dense retrievers, and hybrid retrievers in every aspect. Thanks to the constructed graph, which integrates text chunks from long documents and LLM historical responses into a whole, node embeddings can better reflect the complicated correlations with given queries, thus significantly improving the retrieval performance of GoR. Moreover, the informative content of historical responses can also potentially enhance the summarization task.\nGoR shows superiority over long-context LLMs. We compare Gemma-8K and Mistral-8K with a longer context window to accommodate long-context inputs. However, longer inputs may contain minor information, and long-context LLMs struggle with this situation. In contrast, GoR can effectively differentiate key and topic-related content in long texts using learned node embeddings and achieve better results with shorter input lengths.\nAdditional Findings. (1) Node2Vec produces unsatisfactory results, and the node embeddings cannot be optimized effectively since it is based on a non-differentiable algorithm. (2) Although Thought Retriever demonstrates competitive results, it is still inferior to GoR due to the lack of exploration of the correlations between retrieved text chunks and LLM-generated responses. (3) Since"}, {"title": "3.3 ABLATION STUDY", "content": "To investigate how each component of GoR contributes to its performance, we conduct an ablation experiment, and the results are shown in Table 2.\nFrom Table 2, we can draw several conclusions. (1) Directly using the text embeddings from the retriever without training leads to degraded performance (i.e., w/o train), highlighting the effectiveness of the learned node embeddings. (2) Both the contrastive loss $\\mathcal{L}_{CL}$ and pair-wise ranking loss $\\mathcal{L}_{RANK}$ significantly improve performance. The pair-wise ranking loss imposes stricter ranking constraints on node embeddings, making effective use of the indirect supervision signal from the self-supervised reference summaries. (3) In-batch negatives are crucial to the performance of contrastive learning. Removing in-batch negatives (i.e., w/o in-b neg) leads to a significant drop in results, especially on the WCEP and BookSum datasets. (4) Compared with self-supervised train-"}, {"title": "3.4 DISCUSSIONS", "content": "Impact of GNN Architectures. GNNs play a vital role in learning node embeddings. we explore various GNN architectures to study their impact on learning node embeddings, including GCN Kipf & Welling (2016), SGC Wu et al. (2019), GIN Xu et al. (2019), and GraphSAGE Hamilton et al. (2017). Our findings, illustrated in Figure 3, show that GAT outperforms the other architectures. This is because GAT considers the significance of neighboring nodes when updating node embeddings, allowing the model to effectively capture essential information from the nodes. Among the other architectures, GraphSAGE performs poorly due to its unstable neighbor sampling mechanism.\nOverall, GAT reaches the best results, which shows that considering the importance of neighboring nodes is effective in mining complicated correlations and is critical to improving performance.\nImpact of the Number of Simulated Queries During Training. Query Simulation is a crucial stage in our method design, and we will discuss the impact of the number of simulated queries used during training on learning performance. In particular, we explore this effect by gradually increasing the number of simulated queries used in training. We present the results in Figure 4. From a holistic perspective, R-L shows an upward trend as the number of simulated queries increases. Nevertheless, since fewer queries cover less relevant content from long documents, the curves of each dataset have some fluctuations, indicating the occurrence of underfitting.\nIn general, 30 simulated queries can optimize the model well across these four datasets, which indicates that our proposed GoR is cost-effective. Nevertheless, increasing the number of simulated queries may still potentially further improve the performance of the model. Due to budget constraints, we will leave this for future work.\nSupervised Training on Global Summarization Queries. To dive deeper into the differences between self-supervised and supervised training, we carry out additional experiments using global"}, {"title": "4 RELATED WORK", "content": "Long-context Summarization using LLMs. In recent years, LLMs have demonstrated their excellent capabilities in long-context modeling Achiam et al. (2023); AI@Meta (2024); Team et al. (2024); Jiang et al. (2024). Current approaches for summarizing long documents using LLMs are mainly divided into two categories: retrieval-augmented generation (RAG) Ram et al. (2023); Yu et al. (2023) and long-context LLMs GLM et al. (2024); Li* et al. (2023); Tworkowski et al. (2023). Long-context LLMs feature a large context window length to accommodate long-context inputs. However, they may suffer from severe performance degradation when accessing some local key details in the middle of long contexts Liu et al. (2024b). In contrast, RAG emerges as a promising approach for cost-effective long-context summarization. By equipping with a retriever Karpukhin et al. (2020); Robertson et al. (2009), RAG can first perform a relevance search based on user queries and then feed the retrieved text into LLMs for summary. For a recent example, GraphRAG Edge et al. (2024) conducts query-focused summarization by setting up a graph index and detecting graph communities for summary generation.\nNevertheless, most current RAG approaches still focus on enhancing LLMs' reasoning and question-answering capabilities, which only require retrieving locally relevant information Trivedi et al. (2022); Jiang et al. (2023b); Asai et al. (2023); Li et al. (2023); Zheng et al. (2023). In comparison, our proposed method stands out from these methods by focusing on LLMs' global summarization capability of long-context inputs.\nHistorical Response Utilization of LLMs. Little work has been done on this under-explored topic. Thought-retriever Feng et al. (2024) saves the historical responses of each user-LLM interaction as high-level and informative thoughts to expand the retrieval corpus for future user queries. However, the intricate correlations among thoughts are neglected, leaving room for further improvement.\nAnother line of work is the Chain-of-Thought (CoT), which is similar to our approach in terms of utilizing LLM historical responses and has been regarded as an effective means to enhance the reasoning ability of LLM during inference time in recent years. Few-shot CoT Wei et al. (2022) and zero-shot CoT Kojima et al. (2022) elicit intermediate reasoning paths by prompting LLMs with"}, {"title": "5 CONCLUSION", "content": "In this work, we introduce a method named graph of records (GoR) to improve long-context global summarization in retrieval-augmented generation (RAG) by utilizing LLM-generated historical responses. Intuitively, we establish connections between text chunks retrieved from long documents and LLM-generated historical responses to create a graph of records. To uncover complex correlations between these connections, we use a graph neural network and develop a BERTScore-based objective for self-supervised training. This allows for seamless supervision signal backpropagation between self-supervised reference summaries and node embeddings. In our experiments, we assess our proposed method on four long-context summarization datasets, and the results consistently demonstrate that GoR outperforms numerous baselines by a significant margin, highlighting its effectiveness.\nDespite the superiority of our proposed method, GoR has some limitations. (1) Due to a limited budget, we only simulate and generate a small number of user queries, which may cause a bottleneck in further model optimization. (2) The simulated queries may not accurately reflect the real-world distribution, as they do not account for the possibility of users asking many meaningless questions. Therefore, a filtering process may be necessary, which we leave for future work."}, {"title": "ETHICS STATEMENT", "content": "The security of Large Language Models (LLMs) has always been a concern. Unfortunately, current LLMs sometimes produce harmful and biased information unexpectedly. Our proposed method uses LLMs to generate simulated queries and summary responses, which are only used to construct a graph of records and connect text chunks from long documents. However, more work is needed in real-world applications to ensure that LLMs' responses are reliable and harmless, so that they do not harm users."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "In the experimental setup phase 3.1, we clearly describe the open-source LLMs used in this paper (Mixtral-8x7B-Instruct-v0.1 Jiang et al. (2024) and LLaMA-2-7b-chat Touvron et al. (2023)), the data preprocessing details (including RAG-related chunk size settings and dataset descriptions, etc.), and the training hyper-parameters. Moreover, we provide all the prompts used in this work in Appendix B. More detailed information can be found in the Appendix. The above description ensures the reproducibility of this work."}, {"title": "BROADER IMPACTS", "content": "In the era of LLMs, countless interactions take place between users and these models on a daily basis, resulting in the generation of a vast amount of historical responses. Our proposed method demonstrates that these historical responses hold significant potential and can be effectively leveraged to further improve the quality of future responses generated by LLMs. By analyzing and reusing these past outputs, we can not only refine and enhance the overall performance of the models but also reduce computational overhead. This approach highlights the untapped value of historical data in optimizing response generation while making the process more efficient and resource-friendly."}, {"title": "A EXPERIMENTAL DETAILS", "content": ""}, {"title": "A.1 DATASET", "content": "We present dataset statistics in Table 3. Due to the limited budget, we randomly select training and test samples for the training and test set and calculate the average input and output token lengths using the LLaMA-2 tokenizer Touvron et al. (2023) (samples with short input lengths are filtered out).\nWe evaluate our proposed method on four long-context summarization datasets, i.e., AcademicEval Feng et al. (2024), QMSum Zhong et al. (2021), WCEP Gholipour Ghalandari et al. (2020), and BookSum Kry\u015bci\u0144ski et al. (2021).\n\u2022 QMSum Zhong et al. (2021). QMSum is a query-based summarization dataset that features lengthy meeting transcripts, specific queries, and general queries. Specific queries focus on query-based summarization, and general queries are questions that summarize the entire meeting transcript, such as \u201cSummarize the whole meeting.\u201d We only use \"general queries\" for evaluating global summarization.\n\u2022 AcademicEval Feng et al. (2024). AcademicEval collects scientific papers from arXiv for abstract and related work writing. We use the abstract writing subset, which provides the main body of a paper as input and generates the predicted abstract.\n\u2022 WCEP Gholipour Ghalandari et al. (2020). WCEP is a multi-document summarization dataset about news events, which requires comprehensive consideration of the contents of multiple documents.\n\u2022 BookSum Kry\u015bci\u0144ski et al. (2021). BookSum features long-form narrative summarization, which covers source documents from the literature domain and includes highly abstractive human-written summaries."}, {"title": "A.2 BASELINES", "content": "We present detailed descriptions of adopted baselines.\n\u2022 Node2Vec Grover & Leskovec (2016). Node2Vec generates node embeddings for graphs by simulating biased random walks to capture both local and global structural properties of nodes.\n\u2022 BM25 Robertson et al. (2009), TF-IDF Ramos et al. (2003). BM25 ranks documents based on term frequency, inverse document frequency, and document length normalization, while TF-IDF evaluates the importance of a term in a document relative to a corpus by combining term frequency and inverse document frequency.\n\u2022 Contriever Izacard et al. (2021), DPR Karpukhin et al. (2020), Dragon Lin et al. (2023), SBERT Reimers & Gurevych (2019). Contriever is a self-supervised dense retriever that learns unsupervised document embeddings for information retrieval, DPR (Dense Passage Retriever) is a bi-encoder model that retrieves relevant passages by training on question-passage pairs, Dragon is a dense retrieval model optimized through diverse augmentation for generalizable dense retrieval, and SBERT (Sentence-BERT) is a modification of BERT that generates semantically meaningful sentence embeddings for tasks like similarity and clustering using a siamese network structure."}, {"title": "A.3 ADDITIONAL IMPLEMENTATION DETAILS", "content": "In the stage of graph construction, due to the number and randomness of the simulated queries, there may be some isolated nodes, and we just keep them in the graph with self-loop edges. During model optimization, BERTScore is pre-computed for efficient training. We present hyper-parameters on QMSum, AcademicEval, WCEP, and BookSum datasets in Table 4.\nFor metrics, we adopt Rouge-1 (R-1), Rouge-2 (R-2), and Rouge-L (R-L) Lin (2004) to assess the text alignment between the reference summaries and the predicted content generated by our proposed method. If a global summarization query has multiple reference summaries, we calculate the Rouge-L/1/2 of the predicted summary and all references, respectively, and take the maximum value as the final evaluation result. We follow this setting in all experiments, including the baseline evaluation."}, {"title": "A.4 ADDITIONAL EXPERIMENTAL RESULTS", "content": "We conduct extensive experiments on GovReport Huang et al. (2021) and SQUALITY Wang et al. (2022a) datasets, and the results are shown in Table 5, which demonstrate our proposed GoR is still competitive among baselines on these two datasets."}, {"title": "B LLM PROMPTS", "content": "In this section, we present LLM prompts used in GoR, including user query simulation and RAG prompts."}, {"title": "B.1 LLM PROMPTS FOR USER QUERY SIMULATION", "content": "Prompt for User Query Simulation\nYou are a great questioner of any text, and are adept at asking valuable and insightful questions. Your goal is to generate 1 summary question for the text provided below. The generated summary question should try to simulate the tone of human questions as much as possible, and make sure that the generated question must be interrogative sentences and a summary question. Important! Please make sure this text must be a complete and non-redundant answer to the generated summary question. Please directly output the generated summary question, do not output irrelevant text.\nDOCUMENT:\n{document}"}, {"title": "B.2 LLM PROMPTS FOR RAG", "content": "RAG Prompt\nRefer to the following supporting materials and answer the question with brief but complete explanations.\nSUPPORTING MATERIALS:\n{materials}\nQUESTION:\n{question}"}]}