{"title": "SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex Reasoning over Knowledge Graphs", "authors": ["Ben Liu", "Jihai Zhang", "Fangquan Lin", "Cheng Yang", "Min Peng", "Wotao Yin"], "abstract": "Recent advancements have highlighted that Large Language Models (LLMs) are prone to hallucinations when solving complex reasoning problems, leading to erroneous results. To tackle this issue, researchers incorporate Knowledge Graphs (KGs) to improve the reasoning ability of LLMs. However, existing methods face two limitations: 1) they typically assume that all answers to the questions are contained in KGs, neglecting the incompleteness issue of KGs, and 2) they treat the KG as a static repository and overlook the implicit logical reasoning structures inherent in KGs. In this paper, we introduce SymAgent, an innovative neural-symbolic agent framework that achieves collaborative augmentation between KGs and LLMs. We conceptualize KGs as dynamic environments and transform complex reasoning tasks into a multi-step interactive process, enabling KGs to participate deeply in the reasoning process. SymAgent consists of two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages LLM's inductive reasoning capability to extract symbolic rules from KGs, guiding efficient question decomposition. The Agent-Executor autonomously invokes predefined action tools to integrate information from KGs and external documents, addressing the issues of KG incompleteness. Furthermore, we design a self-learning framework comprising online exploration and offline iterative policy updating phases, enabling the agent to automatically synthesize reasoning trajectories and improve performance. Experimental results demonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields better or comparable performance compared to various strong baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "Knowledge Graphs (KGs) store massive factual triples in a graph-structured format, providing critical supportive information to various semantic web technologies [9, 11, 21, 40]. Recently, Large Language Models (LLMs) have demonstrated impressive capabilities in language understanding and information integration across diverse domains [51]. However, they are limited by the lack of precise knowledge and are prone to hallucinations in their responses [44]. Given that KGs encapsulate the essence of data interconnectivity, providing explicit and explainable knowledge, integrating LLMs and KGs has garnered significant research interest. This integration facilitates a wide range of web-based applications, including search engine recommendation [18, 50], fake news detection [26], and social networks [48].\nExisting work mainly adopts retrieval-augmented [6, 25, 27, 34] or semantic-parsing [1, 23, 47] methods to enhance the complex reasoning performance of LLMs with KG data. The former approaches rely on vector embeddings to retrieve and serialize the relevant subgraph as input prompt for LLMs, while the latter employs LLMs to perform a structured search on KGs (e.g., SPARQL) to obtain answers. Despite their success, these methods share significant limitations. Firstly, they treat KGs merely as static knowledge repositories, overlooking the inherent reasoning patterns embedded in the symbolic structure of KGs. These patterns could substantially aid LLMs in decomposing complex problems and aligning the semantic"}, {"title": "2 RELATED WORKS", "content": "Complex Reasoning over Knowledge Graph. Complex Reasoning over Knowledge graph aims to provide answers to multi-hop natural language questions using knowledge graphs as their pri-mary source of information [1, 18, 23]. Existing methods can be broadly categorized into semantic-parsing and retrieval-augmented"}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 Symbolic Rules", "content": "A knowledge graph is a collection of factual triples, denoted as G = {(e, r, e')|e, e' \u2208 &,r \u2208 R}, where & and R represent the sets of entities and relations, respectively. Symbolic rules in KGs are typically expressed as first-order logic formulae:\n$r_h(x, y) \\leftarrow r_1(x, z_1) \\land r_2(z_1, z_2) \\land ... r_n(z_{n-1}, y)$,                                        (1)\nwhere the left-hand side denotes the rule head with relation rh that can be induced by (\u2190) the right-hand rule body, the rule body forms a closed chain, with successive relations sharing intermediate variables (e.g., zi), represented by the conjunction () of body relations. A KG can be regarded as groundings of symbolic rules by substituting all variables x, y, z with specific entities. For example, given the triples (Sam, workFor, OpenAI), (OpenAI locatedIn SF), and (Sam liveIn, SF), a grounding of the length-2 symbolic rule is liveIn(Sam, SF) \u2190 workFor(Sam, OpenAI) locatedIn(OpenAI, SF)."}, {"title": "3.2 Task Formulation", "content": "In this paper, we transform the reasoning task on KG into an LLM-based agent task, where the KG serves as an environment providing execution feedback rather than merely acting as a knowledge base. The reasoning process can thus be viewed as a multi-step interaction with partial observations from the KG. This interactive process can be formalized as a Partially Observable Markov Decision Process (POMDP): (Q, S, A, O, T) with question space Q, state space S, action space A, observation space O, and state transition function T : S \u00d7 A \u2192 S. Note that in our language agent scenario, Q, A, and O are subspaces of the natural language space, and the transition function T is determined by the environment.\nGiven a question q \u2208 Q and the KG G, the LLM agent generates the action a0 \u223c \u03c0\u03bf(\u00b7|q, G) \u2208 A based on its policy \u03c0\u03bf. This action leads to a state transition, and the agent receives execution feedback as observation o0 \u2208 O. The agent then continues to explore the environment until an appropriate answer is found or another stop condition is met. The historical trajectory Hn at step n, consisting of a sequence of actions and observations, can be represented as:\n$H_n = (q, G, a_0, o_0, ..., a_{n-1}, o_{n-1}) \\sim \\pi_\\theta(H_n|q, G)$,\n$\\pi_\\theta (H_n|q, G) = \\prod_{j=1}^n \\pi_\\theta(a_j|q, G, a_0, o_0, ..., o_{j-1})$,                                                (2)\nwhere n is the total interaction steps. Finally, the final reward r(q, Hn) \u2208 [0, 1] is computed, with 1 indicating a correct answer."}, {"title": "4 METHODOLOGY", "content": "In this section, we present SymAgent, a framework that combines knowledge graphs (KGs) with LLMs to autonomously solve complex reasoning tasks. SymAgent includes an Agent-Planner, which extracts symbolic rules from the KG to decompose questions and plan reasoning steps (Section 4.1), and an Agent-Executor, which synthesizes insights from reflection and environment feedback to answer questions (Section 4.2). To address the lack of annotated reasoning data, we introduce a self-learning framework for collaborative improvement through autonomous interaction (Section 4.3). The overall architecture is shown in Figure 2."}, {"title": "4.1 Agent-Planner Module", "content": "The Agent-Planner functions as a high-level planner, leveraging LLM's reasoning capability to decompose questions into executable reasoning chains. However, we observed that merely prompting"}, {"title": "4.2 Agent-Executor Module", "content": "Building upon the generated symbolic rules from KG G, the Agent-Executor engages in a cyclical paradigm of observation, thought, and action to navigate the autonomous reasoning process. In contrast to existing methods that retrieve information from the KG, potentially introducing large amounts of irrelevant data, the Agent-Executor leverages expert feedback from the KG structure to dynamically adjust the reasoning process. This approach enables KGs, which store a wealth of informative and symbolic facts, to deeply partici-pate in the reasoning process together with LLMs rather than being merely treated as a static repository of information."}, {"title": "4.2.1 Action Space", "content": "Given that LLMs cannot directly process the structured data in KGs, and considering the need to rely on external unstructured documents during the reasoning process to address the issue of incomplete information in KGs, we define the agent's action space as a set of functional tools. By leveraging the"}, {"title": "4.2.2 Interactive Process", "content": "Treating the KG as the environment and the results of action executions as observations, the entire reasoning process becomes a sequence of agent action calls and corresponding observations. We adopt a react-style approach [45], which generates a chain-of-thought rationale before taking actions, reflecting on the current state of the environment. Formally, we extend the Equation 2, and the interaction trajectory at step n can be further represented as:\n$H_n = (q, G, \\rho, \\tau_0, a_0, o_0, ..., \\tau_{n-1}, a_{n-1}, o_{n-1})$,                                                    (4)\nwhere \u03c4 is the internal thought of the agent by reflecting on the historical trajectory, a is an action selected from the tool set defined above, and o is the observation determined by executing an action. Based on this historical trajectory, the process for generating the subsequent thought \u03c4n and action an can be formulated as:\n$\\pi_\\theta(\\tau_n|H_n) = \\prod_{i=1}^{|\\tau_n|} \\pi_\\theta(\\tau_n^i|H_n, \\tau_{<i}^n)$,\n$\\pi_\\theta(a_n|H_n, \\tau_n) = \\prod_{j=1}^{|a_n|} \\pi_\\theta(a_n^j|H_n, \\tau_n, a_{<j}^n),$                                          (5)\nwhere \u03c4n and |\u03c4n| represent the i-th token and the total length of \u03c4n, an and |an| represent the j-th token and the total length of an. The agent loop continues until either the finish() action is invoked or it reaches the predefined maximum iterative steps."}, {"title": "4.3 Self-learning", "content": "Given that the initial dataset comprises only question-answer pairs without well-annotated step-by-step interaction data, we propose a self-learning framework. Rather than distilling reasoning chains from more capable models (e.g., GPT-4 [28]), our approach enables weak policy LLM \u03c0\u03b8 to interact with the environment adequately, thereby improving through self-training. The self-learning process consists of two primary phases: online exploration and offline iterative policy updating."}, {"title": "4.3.1 Online Exploration", "content": "In this phase, the base agent \u03c0\u03b8 interacts with the environment autonomously through a thought-action-observation loop according to Section 4.2.2, synthesizing a set of initial trajectories U0 = {\u00b51, \u00b52, ..., \u00b5N }. For each trajectory \u00b5i, we employ an outcome-based reward mechanism, defining the reward as the final answer's recall value:\n$r(\\mu_i) = Recall(A_{\\mu_i}, A_{gt}) = \\frac{A_{\\mu_i} \\cap A_{gt}}{A_{gt}}$                                                          (6)\nwhere A\u00b5i is the set of answer entities extracted from the final action of trajectory \u00b5i, and Agt is the set of ground truth answer entities. This process yields a collection of self-explored trajectories D0 = {(\u00b5i, r(\u00b5i))}N1.\nTo address the potential errors in agent action invocation (e.g., incorrect tool invocation formats) that may impair exploration effectiveness, we leverage the LLM's self-reflection capability to refine the trajectories. Using D0 as reference, the policy LLM \u03c0\u03c1\u03bf regenerate new refined trajectories, formulated as {\u00b5\u0302i}N1 \u223c \u03c0\u03c1\u03bf(\u00b7|\u00b5i, r(\u00b5i)). After applying the same reward mechanism, we can get a refined trajectory collection D\u03020 = {(\u00b5\u0302i, r(\u00b5\u0302i))}N1.\nAfter self-exploration and self-reflection, we obtain two trajectory collections of equal size: D0 and D\u03020. To enhance the quality of candidate trajectories, we employ a heuristic method to merge these two collections, resulting in an optimized trajectory set. Following the principle of final answer consistency, we obtain the merged trajectory collection D1 = {(\u00b5\u2217i, r(\u00b5\u2217i))}N1 :\n$D(i) = \\begin{cases} (\\mu_i, r(\\mu_i)), & \\text{if } r(\\mu_i) > r(\\hat{\\mu}_i), \\\\ (\\hat{\\mu}_i, r(\\hat{\\mu}_i)), & \\text{if } r(\\mu_i) < r(\\hat{\\mu}_i), \\\\ (t, r(t)), & \\text{filtered, if } r(\\mu_i) = r(\\hat{\\mu}_i) > 0, \\\\ & \\text{if } r(\\mu_i) = r(\\hat{\\mu}_i) = 0. \\end{cases}$                                                                        (7)\nIn this equation, t = arg mins\u2208 {\u00b5i,\u00b5\u0302i} |s| denotes that we select the trajectory with the shorter length when the rewards are equal and non-zero."}, {"title": "4.3.2 Offline Iterative Policy Updating", "content": "Given the merged trajectories D1, an intuitive way to improve the performance of the agent is fine-tuning with these trajectories. Under an auto-regressive manner, the loss of the agent model can be formulated as:\n$L_{SFT} = -E_{\\mu \\sim D^*}[\\pi_\\theta(\\mu|q)]$,\n$\\pi_\\theta(\\mu/q) = - \\sum_{j=1}^X (1(x_j \\in A) \\times log \\pi_\\theta(x_j|q, x_{<j})),$                                               (8)\nwhere 1(xj \u2208 A) is the indicator function about whether xj is a token belonging to thoughts or actions generated by the agent."}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate SymAgent on widely used datasets. We conduct extensive experiments to show the effectiveness of our method by answering the following research questions:\n\u2022 RQ1: How does SymAgent perform compared to state-of-the-art (SOTA) baselines across various complex reasoning datasets?\n\u2022 RQ2: What is the contribution of each key module in our SymAgent framework to the overall performance?\n\u2022 RQ3: How effective is the proposed self-learning framework compared to distillation from teacher models?\n\u2022 RQ4: To what extent can SymAgent enhance KGs by identifying missing triples and facilitating automatic KG completion?"}, {"title": "5.1 Experimental Setup", "content": "We adopt three popular knowledge graph question answer datasets: WebQuestionSP (WebQSP) [46], Complex Web Questions (CWQ) [36], and MetaQA-3hop [10] for evaluation. WebQSP and CWQ datasets are constructed from commonsense KG Freebase [3], which contain up to 4-hop questions. MetaQA-3hop is based on a domain-specific movie KG, and we specifically select this dataset to evaluate the zero-shot reasoning performance of our model in a specific domain scenario. This means we only train on CWQ and WebQSP and then perform in-context reasoning on MetaQA-3hop to assess the model's generalization capabilities to specific domain. To further simulate incomplete KGs, we adopt a breadth-first search method to extract paths from the question entity to the answer entity and then randomly remove some triples following the setting of [42]. In this scenario, semantic parsing methods fail to obtain the correct answers due to unexecutable formal expressions. The detailed construction process can refer to Appendix A.1. To better evaluate model performance on complex reasoning tasks, we sample a sub-set from the test sets that specifically require multi-hop reasoning to solve the questions. The statistics of the resulting datasets are presented in Table 1. For the baselines and implementation detail can refer to Appendix A.2 and Appendix A.3."}, {"title": "5.2 Performance Comparison with SOTA (RQ1)", "content": "The experimental results are presented in Table 2. The overall results demonstrate that SymAgent consistently achieves superior performance across all datasets, validating the effectiveness of our approach. First, SymAgent demonstrates consistent improvement across all LLM backbones compared to both prompt-based and fine-tuned methods, which underscores SymAgent's adaptability and robustness. In particular, SymAgent, with Qwen2-7B backbone, achieves the best performance, outperforming GPT-4 across all three datasets with average improvements of 37.19% in Hits@1, 16.87% in Accuracy, and 30.17% in F1 score. The superior performance can be attributed to the better function-calling capabilities of Qwen2 compared to the other two backbones, which often encounter action tool calling errors (e.g., extra arguments). This demonstrates that our method can effectively leverage the strengths of more advanced LLM, enhancing the overall performance in complex reasoning tasks.\nMoreover, GPT-4 performance between CoT and retrieval reveals that direct document retrieval for complex questions can harm performance, especially in domain-specific tasks. For instance, in MetaQA-3hop, the F1 score degrades by 10.25 (from 22.86 to 12.61) when using retrieval augmentation. The potential reason is that shallow vector retrieval introduces semantically similar but irrelevant noisy information [49]. A similar trend is observed with weaker LLMs. Interestingly, when the base model has adequate instruction-following capabilities (e.g., Qwen2-7B), ToG outperforms the fine-tuned RoG. The reason is that the explore-and-exploit strategy can leverage the LLM's inherent knowledge to address the incompleteness issue of KG, whereas RoG relies heavily on path retrieval and struggles in such a scenario. In contrast, our SymAgent can fully utilize the advantages of both KG and LLM, effectively decomposing problems and achieving excellent performance.\nFinally, by comparing the performance of SymAgent across different datasets, we observe that SymAgent shows a larger improvement ratio on the more challenging CWQ dataset, demonstrating its capability to handle complex reasoning. Furthermore, from the results on MetaQA-3hop, we can observe that LLMs lacking domain knowledge perform worse, while our SymAgent can significantly enhance the backbone's capabilities. This improvement is particularly notable in the zero-shot setting, where SymAgent achieves a remarkable 6\u00d7 increase in F1 score compared to the base LLM, highlighting its ability to generalize and reason effectively in specific domain. In the following ablation and further analysis experiments, unless otherwise specified, we adopt Qwen2-7B as the backbone of SymAgent due to its superior performance."}, {"title": "5.3 Ablation Study (RQ2)", "content": "In this section, we conduct a series of ablation experiments to analyze the contribution of each component in SymAgent. To validate the planner module (PM), executor module (EM), and self-learning framework (SL), we systematically remove these components to create variants for comparison. Ablation results in Table 3 reveal that all components are essential because their absence has a detrimental effect on performance. Specifically, we argue that deriving symbolic rules from the KG is vital, which can be demonstrated by comparing the experimental results between SymAgent and"}, {"title": "5.4 Analysis on Self-learning Framework (RQ3)", "content": "The Number of Iterations. Figure 3 presents a comparative analysis of the effects of the number of iterations during the self-learning phase. In the initial stages of iterative training, we observe a rapid improvement in model performance, validating the effectiveness of our self-refine and heuristic merge methods in acquiring substantial trajectory data. This iterative approach enables the model to"}, {"title": "5.5 Quality of Extracted Triples & Error Type Analysis (RQ4)", "content": "Quality of Extracted Triples. Armed with a comprehensive action tool set, our SymAgent addresses KG incompleteness by leveraging both structured and unstructured data. The WikiSearch action triggers an extract action to identify missing triples from retrieved texts, effectively aligning and enriching the KG with external information. To validate this approach, we augment the KG with SymAgent-identified triples and test a retrieval-augmented generation model RoG on this enhanced KG. As shown in Figure 4, the results demonstrated a significant improvement in the performance of RoG, providing empirical evidence that the quality of triples identified by our method is sufficient for integration into existing KGs. This finding not only validates our approach but also suggests a potential synergistic enhancement between LLM and KG through our SymAgent.\nError Analysis. To gain deeper insights into our model's performance, we conducted an error analysis by categorizing the failure cases into four types: 1) Invalid Action (IA), where the model invokes an action not defined in the action tool set, 2) Error in Arguments (EA), where insufficient or excessive arguments are provided, 3) Exceeding Maximum Steps (EMS), where the reasoning steps exceed the predefined maximum number of steps, and 4) Reasoning Error (RE), where the final answer is incorrect despite valid actions and steps. Table 5 presents the distribution of these error types across WebQSP, CWQ, and MetaQA-3hop datasets. WebQSP errors are predominantly RE (94.34%), while CWQ and MetaQA-3hop show more diverse distributions with significant EMS errors, indicating potential areas for targeted improvements in the future."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce SymAgent, an automatic agent framework that synergizes LLM with structured knowledge to conduct complex reasoning over KG. Our method involves utilizing symbolic rules in KG to guide question decomposition, automatically invoking action tools to address the incompleteness issue of KG, and employing a self-learning framework for trajectory synthesis and continuous improvement. This multifaceted approach not only enhances the planning abilities of the agent but also proves effective in complex reasoning scenarios. Extensive experiments demonstrate the superiority of SymAgent, showcasing the potential to foster mutual enhancement between KG and LLM."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 Construction Process of Dataset", "content": "To construct datasets that simulate real-world incomplete KGs, we extract query entity-centric subgraphs from existing datasets and strategically remove certain triples that connect query entities to their corresponding answer entities. This process allows us to create realistic scenarios of missing information in KGs. As shown in Algorithm 1, we demonstrate the detailed process of identifying and selecting potential triples associated with each question to construct our training datasets. Specifically, we employ Breadth-First Search (BFS) to discover reasoning paths within the KG and randomly remove selected triples to simulate the incomplete scenario."}, {"title": "A.2 Baselines", "content": "We evaluate the performance of SymAgent with three different LLM backbones: (i) Mistral-7B [14] (Mistral-7B-Instruct-v0.2 version), (ii) LLaMA2-7B [37] (Meta-LLaMA-2-7B-Chat version), and (iii) Qwen2-7B [43] (Qwen2-7B-Instruct version). Our method is compared against two prompt-based baselines: CoT [38] and Re-Act [45]. Additionally, we include two strong baselines, ToG [35] and RoG [25]. ToG employs an explore-and-exploit strategy, while RoG adopts a retrieval-augmented approach, effectively coupling KG and LLM to achieve state-of-the-art performance. Notably, we have not included semantic parsing methods in our comparisons. This is because, in the incomplete KG scenario, the formal expressions generated by these methods are often unexecutable, rendering them ineffective for this task. To provide a comprehensive evaluation, we also incorporate comparisons with GPT-4 (gpt-4-32K-0613) using document retrieval augmentation. All prompt-based baselines are tested under one-shot settings, while the fine-tuning-based baselines are trained using LoRA [12]. For detailed prompts used in our experiments, please refer to Appendix A.4. Following the previous setting, we adopt Accuracy, Hits@1, and F1 scores as metrics."}, {"title": "A.3 Implementation Details", "content": "We fine-tune the proposed approach with LoRA. The initial learning rate is 2e-5, and the sequence is 4096 for all the backbone models. The training epoch is 3, and the batch size is 4. We adopt the AdamW optimizer [22] with a cosine learning scheduler. During the inference, we adopt vLLM [19] to accelerate the reasoning process. All the training and inference experiments are conducted on 4 NVIDIA A800 80G GPUs. Detailed hyperparameters used in our experiments are displayed in Table 6."}, {"title": "A.4 Prompt for SymAgent", "content": "In this section, we present the prompt template and detail how SymAgent handles various scenarios during its operation. The prompt consists of core components that guide the agent's reasoning and interaction process with the knowledge graph. As shown in Figure 5, our agent functions as a specialized knowledge graph question-answering system that follows a structured interaction format using interleaved Thought, Action, and Observation steps. To"}, {"title": "A.5 Case Study", "content": "To illustrate how SymAgent operates in practice, we present a detailed case study demonstrating its reasoning process when handling complex queries. As shown in Figure 7, the example showcases how SymAgent systematically processes a question about a movie character, utilizing both knowledge graph information and external resources. Through this case, we can observe how the agent effectively employs its various components - from initial reasoning path planning, through knowledge graph querying, to external information retrieval when necessary. The example demonstrates SymAgent's ability to handle incomplete knowledge scenarios and successfully integrate information from multiple sources to arrive at accurate answers."}]}