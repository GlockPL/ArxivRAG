{"title": "AGENT SECURITY BENCH (ASB):\nFORMALIZING AND BENCHMARKING ATTACKS AND\nDEFENSES IN LLM-BASED AGENTS", "authors": ["Hanrong Zhang", "Jingyuan Huang", "Kai Mei", "Yifei Yao", "Zhenting Wang", "Chenlu Zhan", "Hongwei Wang", "Yongfeng Zhang"], "abstract": "Although LLM-based agents, powered by Large Language Models (LLMs), can\nuse external tools and memory mechanisms to solve complex real-world tasks,\nthey may also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against LLM-\nbased agents. To address this, we introduce Agent Security Bench (ASB), a com-\nprehensive framework designed to formalize, benchmark, and evaluate the attacks\nand defenses of LLM-based agents, including 10 scenarios (e.g., e-commerce, au-\ntonomous driving, finance), 10 agents targeting the scenarios, over 400 tools, 23\ndifferent types of attack/defense methods, and 8 evaluation metrics. Based on\nASB, we benchmark 10 prompt injection attacks, a memory poisoning attack,\na novel Plan-of-Thought backdoor attack, a mixed attack, and 10 correspond-\ning defenses across 13 LLM backbones with nearly 90,000 testing cases in to-\ntal. Our benchmark results reveal critical vulnerabilities in different stages of\nagent operation, including system prompt, user prompt handling, tool usage, and\nmemory retrieval, with the highest average attack success rate of 84.30%, but\nlimited effectiveness shown in current defenses, unveiling important works to be\ndone in terms of agent security for the community.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have rapidly advanced in their capabilities, enabling them to per-\nform tasks such as content generation, question answering, tool calling, coding and many others.\nThis has paved the way for developing AI agents that combine\nLLMs with tools and memory mechanisms capable of interacting with broader environments.\nThese LLM-based agents have the potential to be deployed in various roles, such as\nsafety-critical domains like financial services, medical care and autonomous driving.\nAs shown in Fig. 1, an LLM-based\nagent based on ReAct framework usually operates through several key steps when\nsolving a task: \u2460 Defining roles and behaviors via a system prompt. Receiving user instructions\nand task details. Retrieving relevant information from a memory database. Planning based on\nthe retrieved information and prior context. \u2464 Executing actions using external tools.\nAlthough recent research on LLM agents and advanced frameworks has made significant progress,\nthe primary emphasis has been on their effectiveness and generalization with their trustworthiness remaining largely under-investigated.\nSpecifically, while each of these steps mentioned above enables the agent to perform highly\ncomplex tasks, they also provide attackers with multiple points of access to compromise the agent\nsystem. Each stage is vulnerable to different types of adversarial attacks. Although several bench-\nmarks have been proposed to evaluate the security of LLM agents, such as InjecAgent and AgentDojo they are often limited by their scope, assessing\neither a single type of attack, i.e., indirect prompt injection, or operating in only a few scenarios,"}, {"title": "RELATED WORK", "content": "Prompt Injections Attacks. Prompt injection adds special instructions to the original input, and at-\ntackers can manipulate the model's understanding and induce unexpected outputs. The prompt injection can target the user prompt directly or indirectly in-\nfluence the agent's behavior by manipulating its accessible external environment.\nevaluate the per-\nformance of prompt injection attacks toward agents, but they are limited to indirect prompt injection\nattacks. ASB examines prompt injection attacks on the agent and integrates multiple attacks across\nvarious stages of the agent's operation.\nAgent Memory Poisoning. Memory poisoning involves injecting malicious or misleading data into\na database (a memory unit or a RAG knowledge base) so that when this data is retrieved and pro-\ncessed later, it causes the agents to perform malicious actions. have ex-\nclusively examined the effects of poisoning on LLMs and RAG, without considering the impact of\nsuch poisoning on the overall agent framework. investigates\ndirect memory poisoning of the LLM agent but is constrained to scenarios where the database's in-\nternal structure is known. ASB analyzes the impact of poisoning on the agent framework and treats\nmemory or RAG base as a black box for memory poisoning without knowing the internal structure.\nBackdoor Attacks in LLM and LLM Agent. Backdoor attacks embed triggers into the LLMs to\ngenerate noxious outputs. has engineered specific trigger words designed\nto disrupt the Chain-of-Thought (CoT) reasoning of LLMs. utilizes trigger words to disrupt the contextual learning process. Researchers have recently targeted\nLLM agents for backdoor attacks. contaminates task data for fine-tuning\nLLM agents, enabling attackers to introduce a threat model. In contrast, the PoT backdoor attack\nproposed in the paper is a training-free backdoor attack on the LLM agent."}, {"title": "DEFINITIONS TO BASIC CONCEPTS AND THREAT MODEL", "content": "3.1 DEFINING BASIC CONCEPTS\nLLM Agent with Knowledge Bases. We consider LLM agents utilizing knowledge bases, such as\nRAG for corpus retrieval. For a user query q and its tool list, the agent retrieves relevant memory\nfrom a database D = {(k\u2081, v\u2081),..., (k|D|, v|D|)}. LLM agents use an encoder Eq to map both the query and keys into a shared embedding space. A\nsubset Ek(qT, D) \u2282 D is retrieved, containing the K most relevant keys and values based on the\nsimilarity between q\u2295T and the database keys. Formally, an agent using RAG aims to maximize:\n$Eq~{\\pi}q [1 (Agent (LLM (psys, q, O, T,Ek(q{\\oplus}T,D))) = a_b)],$ \nwhere \u03c0q denotes the distribution of user queries, LLM is the backbone, and 1(\u00b7) is an indicator\nfunction. The input to the agent is the task plan from the LLM, and the output is a tool-using action\nduring execution. Here, psys is the system prompt, O = (0\u2081,\u2026, om) is a set of observations from\nthe task trajectory, and T = (T\u2081,\uff65\uff65\uff65, Tn) is the available tool list. a\u044c is the labeled benign action.\nWe define a target tool list Tt = (\u0442\u2081,\u2026, \u0442\u0456) \u0441 T. If the agent successfully uses all tools in\nTt, it achieves a\u044c. \u0395\u03ba(q,T,D) refers to K retrieved memories serving as in-context examples for\nthe LLM, such as prior plans. The backbone LLM decomposes the task and generates action plans\nP = (p\u2081,\uff65\uff65\uff65,Pr), which the agent follows for each step.\nTarget task: A task is composed of an instruction, tool list and data. When a user seeks to complete\na task, it is referred to as the target task. We denote the target task as t, its target instruction as qt,\nits tool list as Tt = (T\u2081,\u2026\u2026,7), and its target data as dt. Each tool + includes the tool name,\na description of its functionality, and its parameter settings. The user employs an LLM agent to\naccomplish the target task. The agent accepts a combination of an instruction prompt qt, the tool list\nT and data dt in a certain format f as input, which denotes as f (qt, T,dt)."}, {"title": "THREAT MODEL", "content": "Adversarial Goal. Generally, the attacker aims to mislead the LLM agent into using a specified tool,\ncompromising its decision-making in Direct Prompt Injections (DPI), Observation Prompt Injections\n(OPI), Memory Poisoning, and Mixed Attacks. The Adversarial goal is to maximize:\n$Eq~{\\pi}q [1 (Agent(q, malicious) = am)],$ \nwhere the adversary aims to maximize the expected probability that the agent when influenced by\nadversarial modifications @malicious, performs a malicious action am for a given input query q. Apart\nfrom this, a Plan-of-Thought (PoT) backdoor attack should keep benign actions for clean queries.\nOther notations are the same as those in Eq. 1. The Adversarial goal is to maximize:\n$Eq~{\\pi}q [1 (Agent(q, Obenign) = ab)],$\nwhere the agent behaves correctly on clean, unaltered inputs. The agent, under benign conditions\nObenign, is expected to perform a benign action as for input queries q from the distribution \u03c0q.\nAdversary's Background Knowledge and Capabilities. \u2460 Tools. The attacker knows every detail\nof the attack tools, such as their name and functionality. Moreover, the attacker can integrate their\nattack tools into the agent's toolkit, such as manipulating third-party API platforms to add mali-\ncious tools. \u2461 Backbone LLM. The attacker lacks knowl-\nedge about the agent's backbone LLM, including architecture, training data, and model parameters.\nThe agent interacts with the LLM solely through API access, without the ability to manipulate the\nLLM's internal components. \u2462 System Prompts. The attacker can also craft and insert prompts into\nthe agent's system prompt Psys to deploy the prompt as a new agent, like through ChatGPT plu- gins. \u2463 User Prompts. We adopt the common assumption from prior backdoor\nattacks on LLMS, which posits that the attacker has access\nto the user's prompt and can manipulate it, such as by embedding a trigger. This assumption is\nrealistic in scenarios where users rely on third-party prompt engineering services, which could be\nmalicious, or when a man-in-the-middle attacker intercepts the user's prompt by\ncompromising the chatbot or the input formatting tools. \u2464 Knowledge Database. Unlike previous\nscenarios with white-box access to RAG databases and RAG embedders, the attacker has black-box access to RAG databases and embedders."}, {"title": "FORMALIZING ATTACKS AND DEFENSES IN LLM AGENTS", "content": "As shown in Fig. 1, the LLM agent handles tasks involving system prompts, user prompts, memory\nretrieval, and tool usage, all of which are vulnerable to attacks. An intuitive method is direct prompt\nmanipulation during the user prompt step, where attackers design malicious prompts to directly\ncall the attack tools (Sec. 4.1.1 DPI Attacks). Tool usage is also at risk due to reliance on third- party platforms that may contain malicious instructions (Sec. 4.1.2 OPI Attacks). Additionally, the\nmemory module can be compromised (Sec. 4.2 Memory Poisoning Attacks), and the hidden system\nprompt is another attack target, where we propose a PoT-based backdoor attack (Sec. 4.3). These\nattacks can also be combined into mixed attacks (Sec. 4.3 Mixed Attacks). After that, we define the\ndefenses to the attacks above in Sec. 4.5. Finally, we provide attacking examples in App. A.2.2."}, {"title": "FORMALIZING PROMPT INJECTION ATTACKS", "content": "Next, we introduce prompt injection attacks, including DPI, which directly manipulates the agent\nvia user prompts, and OPI, which embeds malicious instructions in tool responses."}, {"title": "DIRECT PROMPT INJECTION ATTACKS", "content": "Detailed Adversarial Goal. We define the DPI (Direct Prompt Injection) of an agent as follows:"}, {"title": "OBSERVATION PROMPT INJECTION ATTACKS", "content": "Detailed Adversarial Goal. We define the OPI (Observation Prompt Injection) attack as follows:\nDefinition 2 - Observation Prompt Injection Attack : Considering an LLM agent provided with a\ntarget instruction prompt qt, a tool list of all available tools T, a target tool list Tt CT for a target\ntask t, it obtains an observation set O = (0\u2081,\u2026, 0m) from the agent's task execution trajectory.\nAn OPI attack injects an injected instruction xe of an injected task e to any step i of O, denoted as\nO\u2295 xe = (01,\uff65\uff65\uff65, O\u00bf \u2295 xe,\uff65\uff65\uff65,0m), and injects an attack tool list Te to T, such that the agent\nperforms the injected task apart from the intended target task.\nFormally, the adversarial goal is to maximize\n$Eqt~\\pi ngt [1 (Agent (LLM (psys, q\u207a, O {\\oplus} x\u00ae, T+ T\u00ba)) = am)],$\nwhere other notations are the same as those in Eq. 1 and Eq. 4."}, {"title": "ATTACK FRAMEWORK FOR DIFFERENT PROMPT INJECTION WAYS", "content": "Based on Definitions in Sec. 4.1.1 and Sec. 4.1.2, an adversary injects harmful content into the data\nxt, leading the LLM agent to execute an unintended task xe using an attacker-specific tool. For a\nDPI attack, xt is the target instruction prompt qt. For an OPI attack, xt is an observation result\nOi \u2208 O, such as a response of an API tool called by the agent in the task execution process. We refer\nto the data containing this malicious content as compromised data, denoted by 2. Various prompt\ninjection attacks employ different methods to generate the compromised data \u00ee, using the original\ntarget data xt, injected instruction xe of the malicious task. For simplicity, we represent a prompt\ninjection attack with P. Formally, the process to generate \u00ee can be described as follows:\n$\\hat{x}= P(x_t, x_e).$"}, {"title": "FORMALIZING MEMORY POISONING ATTACK", "content": "4.2.1 DETAILED ADVERSARIAL GOAL\nWe define the memory poisoning attack of an agent as follows:\nDefinition 3 - Memory Poisoning Attack : Considering an LLM agent provided with a target\ninstruction prompt qt, a tool list of all available tools T, a target tool list Tt CT for a target task\nt, an attacker conducts a memory poisoning attack by providing the agent a poisoned RAG database\nDpoison, and injecting an attack tool list Te to T, such that the agent performs the injected task\napart from the intended target task.\nFormally, the adversarial goal is to maximize\n$Eqt~{\\pi} ngt [1 (Agent (LLM (psys, q\u00b2, O, T +T\u00ae,Ek(q+T+T\u00ae, Dpoison ))) = am)],$\nwhere Ek (qT\u2295Te, Dpoison) represents K demonstrations retrieved from the poisoned database\nfor the user query q and the tool list T\u2295T. The poisoned memory database is defined as Dpoison =\nDclean UA, where A = {(k1(91), 21),..., (k\\A\\|(q\\A\\), \\A\\)} is the set of adversarial key-value pairs\nintroduced by the attacker. In this set, each key is a user query and its tool list information and each\nvalue is a poisoned plan. Other notations follow Eq. 1 and Eq. 4."}, {"title": "ATTACK FRAMEWORK", "content": "Recall that the attacker has black-box access to RAG databases and embedders. We consider that the\nagent saves the task execution history to the memory database after a task operation. Specifically,\nthe content saved to the database is shown below:"}, {"title": "FORMALIZING PLAN-OF-THOUGHT BACKDOOR ATTACK", "content": "4.3.1 DETAILED ADVERSARIAL GOAL\nWe first define a PoT prompt for an LLM agent as an initial query qo along with a set of demonstra-\ntions X = (d\u2081,.,di,...,d|x|). Different from the CoT prompt definition for an LLM in Xiang\net al. (2024b), we define a demonstration di = [qi, P1, P2, ..., Pr, ai], where qi is a demonstrative\ntask, pr refers to the r-th step of a plan to the task, and a\u017c is the (correct) action. PoT backdoor at-\ntack first poisons a subset of these plan demonstrations denoted as X. The poisoned demonstration\nis denoted as d\u2081 = [\u011fi, P1,P2,...,Pr,p*, \u0101i], where p* and \u00e3\u00bf is the backdoored planing step and\nthe adversarial target action. Then it injects a backdoor trigger & into the query prompt q, forming\nthe backdoored prompt q\u2295 d. Then we define the PoT backdoor attack on an LLM agent as follows:\nDefinition 4 - PoT Backdoor Attack : Considering an LLM agent provided with a target instruction\nprompt qt, a tool list of all available tools T, a target tool list Tt < T for a target task t, an attacker\nconducts a PoT backdoor attack by injecting backdoored PoT demonstrations X to system prompt\nPsys, embedding a backdoor trigger & into the query prompt qt, and injecting an attack tool list Te\nto T, such that the agent performs the injected task apart from the intended target task."}, {"title": "ATTACK FRAMEWORK", "content": "To embed an effective backdoor in an LLM agent, the key challenge is contaminating the demon-\nstrations, as agents often struggle to connect the backdoor trigger in the query with the adversarial\ntarget action. However, In-Context Learning (ICL) can help the agent generalize from a few exam-\nples, improving its ability to associate the backdoor trigger with the target action. The importance\nof demonstrations in ICL has been extensively studied, show- ing that LLMs possess inherent reasoning capabilities, particularly in complex tasks like arithmetic\nreasoning. These reasoning skills can be used to manipulate the model's response. For instance,\nBadChain exploits LLMs' reasoning by embedding a backdoor reasoning step,\naltering the final output when a trigger is present. As the core of an LLM agent, the LLM handles\nunderstanding, generating, and reasoning with user inputs, giving the agent strong reasoning abili- ties for complex tasks. Like the CoT approach, the agent develops step-by-step plans to tackle tasks,\nbbreaking them into manageable steps for improved accuracy and coherence in the final solution.\nAttacking Procedures: Building on the previous intuition, we construct a backdoored Plan-of-\nThought (PoT) demonstration that utilizes the planning capabilities of LLM agents by incorporating\nthe plan reasoning step as a link between the user prompting process and the adversarial target action\nof the agent, such as utilizing a specific attacker tool. Specifically, we design the PoT backdoor\nattack for user tasks through the following steps: 1) embedding a backdoor trigger in the user prompt\nfor a task, 2) introducing a carefully designed backdoor planning step during PoT prompting, and\n3) providing an adversarial target action accordingly. Formally, a backdoored demonstration is\nrepresented as d\u2081 = [\u011fi, P1, P2, ..., pr, p*, \u0101i], where p* and \u00e3\u00bf is the backdoored planing step and\nthe adversarial target action.\nBackdoor Triggers Design: A backdoor trigger should have minimal semantic relevance to the\ncontext to strengthen its association with the adversarial target. Therefore, we propose two types\nof triggers: non-word-based triggers and phrase-based triggers. In our experiments, we use simple\nnon-word tokens, like special characters or random letters, such as '@_@' to represent a face or ':)' to represent a smile. Since spell-checkers may flag non-\nword triggers, we use phrase-based triggers generated by querying an LLM like GPT-40, following\nXiang et al. (2024b). The LLM is used to optimize a phrase trigger with weak semantic correlation\nto the context, constrained by phrase length,"}, {"title": "FORMALIZING MIXED ATTACKS", "content": "We defined four attacks targeting different steps of an LLM agent: DPI in user prompting, OPI in\ntool use, and memory poisoning in memory retrieval. These can combine as mixed attacks across\nsteps. PoT backdoor prompts, embedded in the system prompt and not recorded in the database, are\nexcluded from mixed attacks. Formally, the adversarial goal is to maximize\n$Eqt~{\\pi}ngt [1 (Agent (LLM (psys, q\u207a {\\oplus} x\u00ae, O{\\oplus}x\u00ae,T+T\u00b0,Ex(q{\\oplus} T{\\oplus}T\u00ae, Dpoison ))) = am)],$\nwhere other notations are the same as those in Eq. 1, Eq. 4 and Eq. 7."}, {"title": "FORMALIZING DEFENSES FOR OUR ATTACK FRAMEWORK", "content": "This section presents defenses against the four individual attacks summarized in Tab. 2. We elaborate\non and formalize each defense method in App. A.3. Except for PPL and LLM-based detection, all"}, {"title": "EVALUATION RESULTS ON AGENT SECURITY BENCH (ASB)", "content": "5.1 INTRODUCTION TO ASB\nASB is a comprehensive benchmarking framework designed to evaluate various adversarial attacks\nand defenses of LLM-based agents. Compared to other benchmarks, ASB's key advantages lie in\nits inclusion of multiple types of attacks and defense mechanisms across diverse scenarios. This not\nonly allows the framework to test agents under more realistic conditions but also to cover a broader\nspectrum of vulnerabilities and protective strategies. We summarize the statistics of ASB in Tab. 3.\nWe conduct all the experiments on the ASB.\n5.2 EXPERIMENTAL SETUP\nEvaluation Metrics. We introduce the evaluation metrics in Tab. 4. Generally, a higher ASR indi-\ncates a more effective attack. A lower ASR-d indicates a more effective defense. The refuse rate is\nmeasured to assess how agents recognize and reject unsafe user requests, ensuring safe and policy-\ncompliant actions. Our benchmark includes both aggressive and non-aggressive tasks to evaluate\nthis ability. Higher RR indicates more refusal of aggressive tasks by the agent. Moreover, if PNA-t\nis close to PNA, the defense has little negative impact on the agent's normal performance. If BP is\nclose to PNA, it indicates that the agent's actions for clean queries are unaffected by the attack. In\naddition, lower FPR and FNR indicate a more successful detection defense."}, {"title": "BENCHMARKING ATTACKS", "content": "Tab. 5 shows the average ASR and Refuse rate of different attacks and LLM backbones. We can draw\nthe following conclusions that all five attacks are effective: \u2460 Mixed Attack is the Most Effective.\nMixed Attack which combines multiple vulnerabilities achieves the highest average ASR of 84.30%\nand the lowest average Refuse Rate of 3.22%. Models like Qwen2-72B and GPT-40 are completely\nvulnerable, with ASRs nearly reaching 100%. \u2461 DPI is Widely Effective. DPI achieves an average\nASR of 72.68%. Models like GPT-3.5 Turbo and Gemma2-27B are particularly vulnerable, with\nASRs of 98.40% and 96.75%, respectively. DPI's ability to manipulate prompts makes it a major\nthreat across various models. \u2462 OPI Shows Moderate Effectiveness. OPI has a lower average\nASR of 27.55%, but models like GPT-40 are more susceptible (ASR 62.45%). Also, models such\nas Claude3.5 Sonnet demonstrate strong resistance, refusing up to 25.50% of OPI instructions. \u2463\nMemory Poisoning is the Least Effective. Memory Poisoning has an average ASR of 7.92%. Most\nmodels, like GPT-40, show minimal vulnerability, with ASRs below 10%, though LLaMA3.1-8B\nhas a higher ASR of 25.65%. \u2464 PoT Backdoor Targets Advanced Models. PoT Backdoor has\na moderate average ASR of 42.12%, but it is highly effective against advanced models like GPT-\n4o and GPT-40-mini, with ASRs of 100% and 95.50%, respectively. This indicates that advanced\nmodels may be more susceptible to backdoor attacks, making it a critical concern. \u2465 Partial Refusal\nof Aggressive Instructions. Agents with different LLM backbones exhibit some refusal to execute\naggressive instructions, which suggests that models actively filter out unsafe requests in certain\ncases. For example, GPT-40 has a refusal rate of 20.05% under DPI.\nWe also compare the attacking results between different LLM backbones, we can draw the following\nconclusions: \u2460 Larger Models Tend to be More Fragile."}, {"title": "BENCHMARKING DEFENSES", "content": "We show the defense results for DPI and OPI in Tab. 6\nand Tab. 7. It illustrates that current prevention-based de-\nfenses are inadequate: they are ineffective at preventing\nattacks and often cause some utility losses in the primary\ntasks when there are no attacks (see App. D.2). Notably,\neven though the average ASR under Paraphrasing defense\nin DPI decreases compared to no defense, it remains high,\nwith an average ASR-d of 56.87%.\nWe show the defense results for DPI and OPI in Tab. 6\nand Tab. 7. It illustrates that current prevention-based de-\nfenses are inadequate: they are ineffective at preventing\nattacks and often cause some utility losses in the primary\ntasks when there are no attacks (see App. D.2). Notably,\neven though the average ASR under Paraphrasing defense\nin DPI decreases compared to no defense, it remains high,\nwith an average ASR-d of 56.87%."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We introduce ASB, a benchmark for evaluating the security of LLM agents under various attacks\nand defenses. ASB reveals key vulnerabilities of LLM-based agents in every operational step. ASB"}, {"title": "DETAILS FOR ATTACK AND DEFENSE METHODS", "content": "A.1 LLM-BASED AGENT FRAMEWORK - REACT\nIn this paper, we use the ReAct framework as our LLM agent framework . At time\nt, the agent receives feedback ot \u2208 O, executes action at \u2208 A, and follows policy \u03c0(at|ct) based on\nthe current context ct = (01, 01, 02, A2, ..., Ot\u22121, At\u22121, Ot). ReAct extends the agent's action space\nto A = AUL, where L is the language space. An action \u00e2t \u2208 L, known as a thought, is used\nto generate reasoning over ct, updating the context to Ct+1 = (ct, \u00e2t), aiding further reasoning or\naction, like task decomposition or action planning.\nA.2 ATTACKING DETAILS\nA.2.1 PROMPT INJECTION METHODS\nTab. 1 outlines five types of prompt injection attacks and provides descriptions and examples for\neach. They are also used in DPI, OPI, and Memory Poisoning Attacks. PoT Backdoor Attacks and\nMixed Attacks only utilize Combined Prompt Injection Attacks. Next, we introduce and formalize\nthese five types of attacks as follows.\nNaive Attack: This attack directly appends the in-\njected instruction xe to the prompt xt, forming compromised data to manipulate system behavior.\nFormally: $\\hat{x}= x^+ {\\oplus} x^e$, where denotes string concatenation.\nEscape Characters Attack: In this method, special characters c (e.g., newline \\n\nor tab \\t) are placed between xt and xe, tricking the system into treating the injected task as part of\nthe input. Formally: $\\hat{x}= x^+ {\\oplus} c{\\oplus} x^e$.\nContext Ignoring Attack: This attack uses\nphrases i to make the system ignore xt and focus on xe, altering the task context. Formally: $\\hat{x}=$\nx\u2295 i\u2295 xe, where i is a task-ignoring phrase, such as \u201cignore previous instructions\u201d.\nFake Completion Attack: In this approach, a fake response r is added to xt,\ntricking the system into thinking the task is complete, prompting xe instead. Formally: $\\hat{x}= x^+\nr{\\oplus}x^e$, where r denotes a fake response, such as \u201cTask complete.\"\nCombined Attack: This attack combines escape character c, context-ignoring text\ni, and fake response r to enhance xe's success, making more effective and harder to detect. The\ncharacter c is used twice to distinguish between r and i, with \"Task complete\" as a placeholder for\nr. Formally: $\\hat{x}= x^+ {\\oplus} c{\\oplus} r{\\oplus} c{\\oplus} i{\\oplus}x\u00ae$.\nA.2.2 ATTACKING EXAMPLES\nFig. 3 illustrates four attacks on the LLM agent. Initially, the user intended to employ sys- tem_admin_agent to identify and patch vulnerabilities in outdated software. The benign sys- tem_admin_agent should first use the system monitor for security testing to ensure system integrity\nand then use the update manager to manage software updates and prevent vulnerabilities.\nDPI Attack Example. In DPI scenario, a third-party attacker appends a malicious command to the\nuser's prompt, instructing the Data Export tool to leak the latest financial reports. When the LLM\nprocesses the combined user prompt and the injected attack command, it interferes with the reason- ing process of the system_admin_agent, altering its internal logic. The manipulated plan generated\nby the LLM enables the export and leak of sensitive financial data. As a result, by following the\nmanipulated plan, the agent utilizes the Data Export tool to fulfill the attacker's intent.\nOPI Attack Example. In OPI scenario, after the agent completes step 1 (calling the System Mon- itor tool correctly), a third-party attacker injects a malicious instruction into the observation, e.g.,\nappending the attack instruction to the normal tool response. This altered observation affects the\nrethinking process of the system_admin_agent during the second step. The result is that the agent,\ninfluenced by the manipulated observation, incorrectly uses the Data Export tool, leaking the latest\nfinancial reports in the next step.\""}, {"title": "DEFENSE DETAILS", "content": "A.3.1 DEFENSES FOR DIRECT PROMPT INJECTION ATTACK\nParaphrasing. We defend against DPI attacks by paraphrasing the user query\nwith injected instructions 2 to disrupt special characters, task-ignoring text, fake responses, and\ninjected instructions using the backbone LLM. This may reduce the effectiveness of prompt injection\nattacks. The agent then executes based on the paraphrased query.\nFormally, the user query q\u2295 d (where \u03b4 represents the injected malicious instruction) is transformed\ninto a paraphrased query q' = fp(q\u2295 d) using a paraphrasing function fp. This disruption may\nweaken the connection between the malicious instruction and the adversarial target action am. The\ngoal is to make the probability of executing the malicious action after paraphrasing significantly\nsmaller than without the defense:\n$Eq~ng [P(Agent(fp(q{\\oplus} d)) = am)] \\ll Eq\u223cng [P(Agent(q{\\oplus} d) = am)]$.\nHere, \u03c0q represents the distribution of possible user queries. By paraphrasing, the malicious effect\nof the injection may be reduced, leading to safer agent behavior.\nDelimiters. DPI attacks exploit the agent's inability to distinguish between user and attacker instructions, lead- ing it to follow the injected prompt. To counter this, we enclose the user instruction within <start>\nand <end> delimiters, ensuring the agent prioritizes the user input and ignores the attacker's in- structions.\nFormally, the user's instruction q is encapsulated within delimiters such as (start) and (end), en- suring that the agent processes only the content within the delimiters and ignores any malicious\ninstruction & outside. This ensures that the agent prioritizes the correct task:\n$Eq~{\\pi}q [P(Agent((start)q(end) {\\oplus} \u03b4) = \u03b1\u2081)] \\gg Eq~\u03c0, [P(Agent(q {\\oplus} \u03b4) = \u0430\u0442)]$.\nThis ensures that on average, the agent follows the user's legitimate instruction more often.\nInstructional Prevention. This defense modifies the instruction prompt\nto explicitly direct the agent to disregard any additional instructions beyond user instruction.\nFormally, this method modifies the instruction prompt I(q) to explicitly direct the agent to follow\nonly the user's instruction and ignore any external commands. The probability of the agent execut- ing the correct action as under instructional prevention should be much higher than executing the\nmalicious action:\n$Eq~\u03c0\u03b1 [P(Agent(I(q) {\\oplus} \u03b4) = a\u044c)] \\gg Eq~\u03c0\u03b1 [P(Agent(q + d) = am)]$.\nThe purpose of this defense is to ensure the agent strictly follows the legitimate instructions and\ndismisses any additional injected content.\nA.3.2 DEFENSES FOR OBSERVATION PROMPT INJECTION ATTACK\nDelimiters. Like DPI attacks, OPI attacks exploit the agent's inability to distinguish between the\ntool response and the attacker's instruction, leading it to follow the injected instruction instead of the\nintended prompt. Therefore, we use the same delimiters as the DPI's to defend against OPI attacks.\nThe difference is that in OPI the malicious instruction is appended to the observation rather than the\nuser query. Formally, it is defined as:\n$Eq~\u03c0\u03b1 [P(Agent((start)q(end)) = ab)] > Eq~ng [P(Agent(q{\\oplus} d) = am)]$.\nInstructional Prevention. This defense is the same as the one in DPI\nthat modifies the instruction prompt to direct the agent to ignore external instructions as defined in\nEq. 13.\nSandwich Prevention. Since the attack instruction is injected by the tool\nresponse during the execution in OPI, the defense method creates an additional prompt and attaches\nit to the tool response. This can reinforce the agent's focus on the intended task and redirect its\ncontext back, should injected instructions in compromised data have altered it."}, {"title": "DEFENSE FOR MEMORY POISONING ATTACK", "content": "PPL detection. Perplexity-based detection (PPL detec-\ntion) was first used to identify jailbreaking prompts by assessing their perplexity, which indicates\ntext quality. A high perplexity suggests compromised plans due to injected instructions/data. If\nperplexity exceeds a set threshold, the plan is flagged as compromised. However, previous works\nlacked a systematic threshold selection. To address this, we evaluate the FNR and FPR at different\nthresholds to assess the detection effectiveness.\nLLM-based detection. This approach employs the backbone LLM\nto identify compromised plans, which can also utilize FNR and FPR as evaluation metrics."}, {"title": "DEFENSES FOR POT BACKDOOR ATTACK", "content": "Shuffle. Inspired by that random-\nize inputs or shuffle reasoning steps, we propose a post-training defense against PoT backdoor at-\ntacks that disrupts the link between the backdoor planning step and the adversarial target action.\nThe defense randomly rearranges the planning steps within each PoT demonstration. Formally,\nfor a given"}]}