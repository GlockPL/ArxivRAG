{"title": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs", "authors": ["Shuirong Cao", "Ruoxi Cheng", "Zhiqiang Wang"], "abstract": "LLMs can exhibit age biases, resulting in unequal treatment of individuals across age groups. While much research has addressed racial and gender biases, age bias remains little explored. The scarcity of instruction-tuning and preference datasets for age bias hampers its detection and measurement, and existing fine-tuning methods seldom address age-related fairness. In this paper, we construct age bias preference datasets and instruction-tuning datasets for RLHF. We introduce ARG, an age fairness reward to reduce differences in the response quality of LLMs across different age groups. Extensive experiments demonstrate that this reward significantly improves response accuracy and reduces performance disparities across age groups. Our source code and datasets are available at the anonymous link.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) used in various fields can perpetuate age biases, affecting career opportunities and healthcare [1]. Unlike fixed gender and racial biases, age bias is continuous and evolving. Additionally, common fine-tuning methods for LLMs include instruction-based fine-tuning [11] and reinforcement learning with human feedback [12]. However, no instruction-based datasets address age bias, and these methods do not target social biases, leading to potential performance discrepancies across age groups.\nTo address this challenge, we revised and expanded BBQ [13] and ISB [14] datasets and manually annotated them to create age preference and instruction fine-tuning datasets for age bias. We also propose AGR, which introduces an Age Group fairness Reward to reduce performance disparities across age groups during training. \nIn summary, our contributions are as follows:\n\u2022\tWe construct age bias preference and instruction fine-tuning datasets for bias evaluation in LLMs.\n\u2022\tWe introduce AGR, which employ a fairness reward to reduce performance disparities across age groups, showing improvement on BBQ and age bias instruction fine-tuning dataset.\n\u2022\tExperiments across various LLMs prove AGR's effectiveness in age bias mitigation, surpassing existing related methods."}, {"title": "II. GROUP-FAIRNESS-BASED AGE BIAS MITIGATION", "content": ""}, {"title": "A. Task Overview and Formalization", "content": "Let M be an LLM parameterized by \u03b8, which takes a text sequence $x = (x_1,...,x_m) \\in X$ as input and produces an output $y \\in Y$, where $y = M(X;\\theta)$ and the form of y depends on the specific task. The input can come from a labeled dataset $D = {\\{(x^{(1)},y^{(1)}),..., (x^{(N)}, y^{(N)})\\}}$, or an unlabeled dataset of sentence continuations and prompt completions $D = {\\{x^{(1)},...,x^{(N)}\\}}$.\nAge debiasing in LLMs can be framed as ensuring that the model treats all age groups fairly. Specifically, for a model M and its output $y = M(x; \\theta)$, given a set of age groups g, age group fairness requires that the statistical measures $M_y(g)$ of the model's output for all different age groups $g \\in G$ are approximately equal, i.e.:\n$|M_y(g) \u2013 M_y(g')| \\leq \\epsilon$\nwhere the choice of M specifies a fairness constraint, and M could be accuracy, true positive rate, etc."}, {"title": "B. Construction of Age Bias Preference Datasets", "content": "We extract samples related to age bias from BBQ [13] question-answering dataset and ISB [14] dataset to construct two preference datasets: Age Bias Mitigation for Behavior (ABMB) and Age Bias Mitigation for Attribute (ABMA). Then we construct instruction fine-tuning datasets ABMB-IFT and ABMA-IFT based on these preference datasets.\n1) Response Generation: Based on the context, question, and each candidate answer, GPT-3.5 Turbo rewrites the answers to create a modified dataset.\n2) Response Adjustment and Evaluation: We adjust the responses provided by GPT-3.5-Turbo and recruit five annotators to evaluate each response based on the following three criteria:\n\u2022\tCommunication Effectiveness (CE) measures fluency and grammar, scoring 1 to 3. Higher scores indicate more natural language.\n\u2022\tLogical Soundness (LS) assesses logical coherence, scoring 1 to 3. Scores higher, logic better.\n\u2022\tAge-related Bias (AB) evaluates age bias, scoring 1 to 3. Higher scores indicate less bias.\nFinal score for each dimension is the most common annotation score. Total quality score for each response is the sum of scores across all three dimensions. \n3) Response Ranking: Due to varied annotator values, quality scores are noisy. Ranking responses standardizes comparisons among models. We use a dataset format like Nakano et al. [15] and Bai et al. [16], where each item has a query with two responses, ranked by quality. Invalid pairs with identical scores are discarded."}, {"title": "C. Instruction Fine-Tuning Dataset Construction", "content": "To further test age bias in LLMs across different age groups within the same context, we construct the instruction fine-tuning datasets ABMB-IFT and ABMA-IFT based on the original BBQ and ISB datasets. The process includes:\n\u2022\tQuestion Rewriting: Extract age groups from the context and answers of each sample, then rewrite the questions using each age group.\n\u2022\tResponse Generation: Determine tag category (\"Yes\" or \"No\") for rewritten questions based on labeled answers. Use GPT-3.5-Turbo to expand tags and add explanations based on context.\nAge group classifications vary by country, culture, and field and can change over time. For simplicity, we define age groups as: 10-29 years (young adults), 30-59 (middle-aged), and 60+ (elderly)."}, {"title": "D. Age Group Fairness Reward", "content": "RLHF directly uses the output of a trained preference model as the reward value in the reinforcement learning phase, without considering fairness in response quality across different age groups under prompt paradigms. We propose an age group fairness reward signal.\nGiven a LLM M parameterized by \u03b8, and inputs for different age groups $a \\in {\\{X_{young}, X_{middle}, X_{old}\\}}$, and their corresponding outputs $y_a \\in {\\{Y_{young}, Y_{middle}, Y_{old}\\}}$, we define a reward signal R to train the preference model P, aligning the LLM with human preferences and mitigating age-related bias. For a set of age groups $A = {\\{young, middle, old\\}}$, we calculate the quality score of the model output for each age group $a \\in A$, denoted as $Q(Y_a | x_a)$. The quality score Q measures whether the model's output meets predefined fairness requirements."}, {"title": "E. Training Process of AGR", "content": "We propose AGR, which uses R\u03b8 to train the preference model and leverage it in the reinforcement learning phase to optimize model parameters and reduce age bias.\nAGR employs a three-stage process, similar to RLHF, to fine-tune the base model for age bias mitigation, as illustrated in Figure 3.\n1) Supervised Fine-Tuning: The LLM is fine-tuned based on the conditional probability distribution $y \\sim P(y | x; \\theta)$, where \u03b8 represents the initialization parameters. We perform supervised fine-tuning of the LLM using ABMB-IFT and ABMA-IFT datasets, injecting age bias mitigation knowledge into the pre-trained base LLM. This process aims to enhance response to specific contextual questions and accelerate the convergence speed of the reinforcement learning phase.\n2) Training the Preference Model: Formally, a preference model [17] or reward model [18] can be represented as a parameterized mapping function $R : X \u00d7 Y \u2192 R$, which provides a real-valued reward (or preference) score $R_\\theta(x, y)$. We use the proposed age group fairness reward, which quantifies the fluency, logical soundness, and age bias of textual responses corresponding to input prompts $x = (x_1,x_2,\\dots,X_N) \\in X$ and text responses $y = (Y_1, Y_2,\\dots,Y_M) \\in Y$. Given an input x and a pair of responses ($y_{good}$, $y_{bad}$), where $y_{good}$ represents a high-quality response and $y_{bad}$ represents a low-quality response, the reward model Rd should establish a preference for $y_{good}$, i.e., $R_\\theta(x, y_{good}) > R_\\theta(x, y_{bad})$.\nTherefore, given the preference data tuple $D = {\\{(x, y_{good}, y_{bad})\\}}$, we train the reward model by increasing the gap between $R_\\theta (x, y_{good})$ and $R_\\theta (x, y_{bad})$.\nBased on this idea, this chapter adopts the binary ranking loss function to measure the accuracy of the preference model's ranking:\n$L_{Ranking} = -E_{(x,y_{good}, y_{bad})\\sim D} log \\sigma(\\Delta R_\\theta)$,\nwhere $\\Delta R_\\theta = R_\\theta (x, y_{good}) \u2013 R_\\theta (x, y_{bad})$ and $\\sigma()$ is the Sigmoid function.\n3) Reinforcement Learning Fine-Tuning with Preference Model: AGR updates LLM parameters using the group fairness reward Rd provided by the preference model to guide the LLM in generating outputs with lower bias. We use Remax algorithm [19] to optimize the supervised fine-tuned base model using the preference model trained in the second step. The objective function is as follows:\n$J(\\phi) = E_{y\\sim\\pi_{\\phi}(\\cdot|x)} [R_\\theta(x, y)] \u2013 \\beta D_{KL} (\\pi_{\\phi}||SFT)$\nwhere $\\pi_\\phi$ is the learned policy, $\\pi_{SFT}$ is the supervised fine-tuned model, $D_{KL}$ is the KL divergence, and $\\beta$ is a constant coefficient. This objective function uses the policy gradient method to learn the optimal policy $\\pi_\\phi$ that maximizes $J(\\phi)$."}, {"title": "III. EXPERIMENTS", "content": ""}, {"title": "A. Baseline", "content": "We test four open-source models\u2014Llama2-7B-base [20], Qwen1.5-7B-base\u00b9, ChaGLM3-6B-base\u00b2, and Baichuan2-7B [21] for supervised learning. Qwen1.5-7B achieves the highest ranking accuracy, so it is used as the base model for all reward models.\nWe empirically compare AGR with the following SOTA bias mitigation methods.\n\u2022\tDePrompt [22] uses debias-prompt like \u201cNote that the answer does not rely on stereotypes.\u201d directly.\n\u2022\tKG-Debias [23] collects relevant nouns and obtains structured knowledge, which is then converted into sentences and applied to LLMs.\n\u2022\tSFT-LORA [24] freezes pre-trained model weights and introduces trainable low-rank decomposition matrices in each layer of transformer to reduce parameters number for downstream tasks."}, {"title": "B. Metrics", "content": "Following previous works [13], [14], we use question-answering accuracy to compare bias levels in BBQ-Age, ABMB-IFT, and ABMA-IFT test sets. Tag accuracy measures \u201cYes\u201d or \u201cNo\u201d response accuracy, while content accuracy checks alignment with reference explanations. Higher values indicate lower age bias."}, {"title": "C. Settings", "content": "Experiments are conducted on four NVIDIA V100 GPUs (32GB each). For supervised fine-tuning, the learning rate is 5 \u00d7 10-5 with a batch size of 8 per GPU and 3 epochs. Preference model training uses a learning rate of 3 \u00d7 10-4, batch size of 8, and 1 epoch. Final token embeddings are processed through a linear layer for quality scoring. Reinforcement learning fine-tuning employs a learning rate of 1 \u00d7 10\u20136, batch size of 2, and 1 epoch, with a cosine annealing scheduler [25] and a maximum text length of 512. The fairness reward coefficient \u03bb is 0.5 for ABMA-IFT and 0.7 for ABMB-IFT. Models use FP16 during reinforcement learning. Preference and reference models have a zero-stage of 3 and are loaded into GPU memory only during inference, while the actor model has a zero-stage of 2."}, {"title": "D. Results", "content": "Table I shows that base versions of the four 7B-parameter LLMs perform better on tag and content accuracy in the ABMA-IFT test set compared to the ABMB-IFT test set, indicating lower bias in age attributes than age behavior. Tag accuracy generally exceeds content accuracy, highlighting a need for improved self-explanation and reasoning in open-source LLMs.\nAGR with age group fairness rewards significantly enhances content and combined tag/content accuracy over RLHF. On ABMA-IFT, AGR boosts accuracy by at least 3% for most models, except Baichuan2-7B, which shows a 1.7% improvement. On ABMB-IFT, it increases tag/content accuracy by at least 2.9%, with Qwen1.5-7B improving by 5.4%. Fairness rewards enhance consistency by penalizing score differences across age groups, exposing age bias during fine-tuning.\nTable II shows that AGR improves Tag&Content accuracy across age groups compared to baseline methods. Qwen1.5-7B, for example, increases accuracy by 2.7%, 4.1%, and 4.9% for Young, Middle-aged, and Old groups on the ABMA-IFT dataset, and by 4.2%, 5.5%, and 6.5% on the ABMB-IFT dataset. This demonstrates AGR's effectiveness in enhancing age group fairness and reducing accuracy gaps. For Qwen1.5-7B on the ABMA-IFT dataset, the accuracy gap between elderly and young, and middle-aged groups was reduced from 2.8% and 4.7% to 2% and 2.5%."}, {"title": "IV. CONCLUSION", "content": "We developed ABMA and ABMB preference datasets and ABMA-IFT and ABMB-IFT instruction fine-tuning datasets to address age bias in LLMs under prompt-based paradigms. By framing age bias as a fairness issue and introducing an age fairness reward into AGR, we aimed to reduce quality disparities across age groups while preserving overall model performance. Experiments show that AGR significantly improves accuracy and reduces age-related performance gaps compared to existing methods."}]}