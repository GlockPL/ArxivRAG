{"title": "Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method", "authors": ["Jiayi Lin", "Chenyang Zhang", "Haibo Tong", "Dongyu Zhang", "Qingqing Hong", "Bingxuan Hou", "Junli Wang"], "abstract": "Multi-Span Question Answering (MSQA) requires models to extract one or multiple answer spans from a given context to answer a question. Prior work mainly focuses on designing specific methods or applying heuristic strategies to encourage models to predict more correct predictions. However, these models are trained on gold answers and fail to consider the incorrect predictions. Through a statistical analysis, we observe that models with stronger abilities do not predict less incorrect predictions compared with other models. In this work, we propose Answering-Classifying-Correcting (ACC) framework, which employs a post-processing strategy to handle incorrect predictions. Specifically, the ACC framework first introduces a classifier to classify the predictions into three types and exclude \"wrong predictions\", then introduces a corrector to modify \"partially correct predictions\". Experiments on several MSQA datasets show that ACC framework significantly improves the Exact Match (EM) scores, and further analysis demostrates that ACC framework efficiently reduces the number of incorrect predictions, improving the quality of predictions.", "sections": [{"title": "1 Introduction", "content": "Machine Reading Comprehension (MRC) requires models to answer a question based on a given context (Rajpurkar et al., 2018; Kwiatkowski et al., 2019; Lai et al., 2017). In a real-world scenario, a single question typically corresponds to multiple answers. To this end, Multi-Span Question Answering (MSQA) has been proposed (Ju et al., 2022; Li et al., 2022; Yue et al., 2023). Different from the traditional Single-Span Question Answering (SSQA) task, the goal of MSQA is to extract one or multiple non-overlapped spans from the given context. Taking Figure 1 as an instance, the question \"Who made Don't Hug Me I'm Scared?\" has two answers: \"Becky Sloan\" and \"Joseph Pelling\". Recent MSQA work integrates various approaches. Yang et al. (2021); Hu et al. (2019) incorporate heuristic strategies based on traditional pointer models (Vinyals et al., 2015) to extract multiple answers; Segal et al. (2020); Li et al. (2022) convert MSQA task into a sequence-tagging task to mark answers; Huang et al. (2023a); Zhang et al. (2024) enumerate all candidate answers and select the final answers with a learnable threshold, and Huang et al. (2023b); Zhang et al. (2023) utilize Large Language Models (LLMs) to handle MSQA tasks with few shot prompts.\nPrior work mainly focus on specific methods or heuristic strategies for more correct predictions. However, these models are trained on gold answers, and fail to consider the incorrect predictions. To further investigate the incorrect predictions, we categorize predictions into \"correct predictions\", \"partially correct predictions\" and \"wrong predictions\" based on whether they should be modified or excluded. Then we conduct a statistical analysis on several MSQA models (details in Section 2.3), and observe that stronger MSQA models \u00b9 do not predict less incorrect predictions compared with other models. Consequently, performance of the MSQA models can be further improved on the basis of reducing incorrect predictions.\nIn this work, we propose Answer-Classify-Correct (ACC) framework, which employs a post-processing strategy to handle with incorrect predictions. The ACC framework simulates humans strategy in realword examinations: listing candidate answers, reviewing and modifying. Specifically, we design the classifier to categorize candidate answers into \"correct predictions\", \"partially correct predictions\" or \"wrong predictions\", then we design the corrector to modify \"partially correct predictions\", finally we exclude \"wrong predictions\" and obtain final predictions. To train the classifier and the corrector, we also apply an automatic annotation approach which samples incorrect predictions from the training datasets and constructs the silver-labeled datasets.\nWe conduct experiments on four MSQA datasets. Experiment results show that the ACC framework significantly improves the performance. After applying the ACC framework, the EM F1 score increases from 69.05% to 72.26% for Tagger-ROBERTa (Li et al., 2022) and from 65.57% to 76.31% for BART-base (Lewis et al., 2020) on the MultiSpanQA dataset (Li et al., 2022). Further analysis on the predictions indicates that the ACC framework effectively reduces the number of incorrect predictions and obtains more correct predictions, enhancing the qualities of predictions. In addition, a pilot study with GPT-3.5 \u00b2 is conducted, exhibiting extensive application of ACC framework for LLMs in a Chain-of-Thought (CoT) manner (Wei et al., 2022; Kojima et al., 2022).\nOur contributions are summarized as follows:\n\u2022 We develop a three-fold taxonomy for the MSQA predictions based on whether a prediction should be modified or excluded. Then, we conduct a statistical analysis, revealing distributions over the three categories."}, {"title": "2 Taxonomy of MSQA Predictions", "content": "Given a question Q and its corresponding context C, the goal of MSQA is to train a model M to extract a set of m answer spans P = {P1, P2, \u2026, Pm} from the given context, shown as Eq. 1.\n$$P = M(C, Q)$$\nIntuitively, the predictions can be categorized as correct or incorrect predictions. However, some of incorrect predictions should be modified while others should be excluded. For instances in Figure 1, \"Sloan\" and \"DHMIS\" are both incorrect predictions. However, \"Sloan\" is similar to the gold answer \"Bercy Sloan\" but \"DHMIS\" is totally wrong. Therefore, we further categorize incorrect predictions into \"partially correct predictions\" and \"wrong predictions\".\nBased on above analysis, we category the prediction pi \u2208 P into one of the following three types: \"correct prediction\", \"partially correct prediction\" and \"wrong prediction\".\nCorrect prediction. If the prediction pi is one of the gold answers, which means pi \u2208 A, pi is regarded as a correct prediction.\nPartially correct prediction. We utilize Word Overlap (WO) and BERTScore (BS) (Zhang et al., 2020) to define partially correct predictions. Word Overlap considers the overlap between two spans in word level, while BERTScore computes semantic similarity of two spans in the manner of cosine similarity. Details of Word Overlap and BERTScore are shown in Appendix A.\nFor a prediction pi, if there exists aj \u2208 A which satisfies WO(pi,aj) \u2265 a and BS(pi,aj) \u2265 \u03b2,"}, {"title": "3 Method", "content": "In this section, we describe the ACC framework, which is designed to handle with partially correct predictions and wrong predictions. The architecture of the ACC framework is shown in Figure 3.\nSimilar to the humans' strategies, the post-processing procedure of the ACC framework consists of three steps: The first step is answering, where we employ a reader to obtain initial predictions P; The second step is classifying, where we employ a classifier to categorize each prediction pi into one of the three classes: correct prediction, partially correct prediction and wrong prediction; The last step is correcting, where we employ a corrector to modify the partially correct predictions. We reserve correct predictions predicted by the classifier and the modified predictions from the corrector as the final predictions.\nNext, we will provide more details of the reader, the classifier and the corrector. We will also introduce an automatic annotation approach which samples incorrect predictions and constructs training data for the classifier and the corrector.\nThe main function of the reader is to extract several text spans from context based on a given question. This process can be described as:\n$$P = Reader(Q, C)$$\nThe predictions of the reader may include partially correct predictions or wrong predictions (men-tioned in Section 2.2). To this end, we design the classifier to classify them and exclude wrong predictions. Given the candidate predictions P, the classifier splits them into correct predictions Pc, partially correct predictions Pp and wrong predictions Pw. This process can be described as:\n$$Pc, Pp, Pw = Classifier(P,Q, C')$$\nSpecifically, the classifier consists of a transformer (Vaswani et al., 2017) encoder and a classification head. The classification head includes an MLP layer to obtain probability of each class. Inspired by Zhu et al. (2022), we also add a cross-attention layer in the classification head. The cross-attention layer calculates the attention scores between the question and the context to enhance the representations of them.\nThe classifier is able to exclude wrong predictions, however, there may still contain partially correct predictions which are imperfect and should be modified. Hence, we design the corrector to modify those partially correct predictions. This process can be described as:\n$$Pp = Corrector(Pp, Q, C)$$\nwhere Pp are the predictions modified by corrector. We adopt traditional pointer model (Vinyals et al., 2015) to predict the start and end probabilities st and ed. During the inference stage, for the text span starting at i-th token and ending at j-th token, we calculate its confidence score scoreij = sti + edj and obtain the best index pair (i, j) which maximizes scoreij, then extract its corresponding span as the modified prediction.\nThe final outputs of the ACC framework P consist of the correct predictions Pe predicted by the classifier and the modified predictions Pp from the corrector, described as:\n$$P = P_c \\cup P_p$$\nTo train the classifier and the corrector, we need both correct predictions and incorrect predictions. However, most MSQA datasets do not contain incorrect predictions. Inspired by Gangi Reddy et al. (2020), we adopt an automatical sampling method similar to K-fold cross-validation, to collect incorrect predictions from the MSQA datasets and construct our silver-labeled datasets.\nFirst, we randomly divide the training data D into K equal subsets: D1, D2, ..., Dk. We perform K iterations, in the i-th iteration we initialize a MSQA model M (i.e. reader mentioned in Section 3.1) and train it with all training data except Di, then sampling the predictions of Di with M. After K iterations, we utilize the gold answers from training dataset D to annotate all predictions, and construct the silver-labeled dataset ."}, {"title": "4 Experiments", "content": "Four MSQA datasets are integrated in experiments: MultiSpanQA (Li et al., 2022), MultiSpanQA-Expand (Li et al., 2022), MAMRC (Yue et al., 2023) and an additional synthetic dataset MAMRC-Multi. Details of these datasets are shown in Appendix B.1.\nMSQA models We set both discriminative models and generative models as readers. For discriminative models, we set MTMSN (Hu et al., 2019), MUSST (Yang et al., 2021), Tagger (Li et al., 2022) and SpanQualifier (Huang et al., 2023a); For generative models, we set BART (Lewis et al., 2020), T5 (Raffel et al., 2020) and GPT-3.5. Details of these models are shown in Appendix B.2.\nEvaluation Metrics We use Exact Match Precision/Recall/F1 (EM P/R/F1) (Li et al., 2022) as the main metrics in our experiments. EM assign a score of 1 when a prediction fully matches one of the gold answers and 0 otherwise.\nImplementation Details For the classifier and corrector in the ACC framework, we use ROBERTa-base (Zhuang et al., 2021) as encoder. For discriminative MSQA models, we use both BERT-base (Devlin et al., 2019) and RoBERTa-base as encoder. For the hyper parameters mentioned in Section 2.2, based on the average Word Overlap and BERTScore of the sampled data, we set \u03b1 = 0.25 and \u03b2 = 0.6 to obtain balanced training data. See more training and inference details in Appendix B.3."}, {"title": "4.2 Main Results", "content": "Table 1 shows the main results on four MSQA datasets. Discriminative models perform better than generative models on the MSQA task, especially on MultiSpanQA-Expand and MAMRC where questions may contain only one answer or no answer. The reason may be that discriminative models are suited for extracting text spans from a given context, whereas generative models are suited for text generation.\nAfter applying the ACC framework, both discriminative models and generative models gain improvements. For instances, the EM F1 score of Tagger (RoBERTa-base) increases from 69.05% to 72.26% and the EM F1 score of BART increases from 65.57% to 67.31% on MultiSpanQA. For most settings, presicion scores show significant improvements while some recall scores show slight declines, the reason may be that while the classifier successfully identifies some wrong predictions, it also mistakenly classifies some correct predictions as wrong, leading to the exclusion of some correct predictions and thereby lowering the recall scores. In Section 5.2, we will analyze the classification results of the classifier to verify this point.\nWe also evaluate the ACC framework with Partial Match P/R/F1 (PM P/R/F1), which considers the overlap between the predictions and gold answers. Results are shown in Appendix C.1."}, {"title": "5 Discussions", "content": "ACC framework uses the \"answer-classify-correct\" procedure with the classifier and the corrector. To investigate whether there exists better post-processing procedure, we conduct an ablation study by: 1. only employing the classifier or corrector (cls \\ cor only); 2. changing the order of classifier and corrector (cor & cls); 3. modifying both correct predictions and partially correct predictions (binary cls & cor).\nTable 2 shows the results of the ablation study on the dev set of MultiSpanQA. The performance of \"cls only\" and \"cor only\" lags behind ACC framework, demonstrating the significance of the classifier and corrector. Changing the order between classifier and corrector also shows decline, the reason may be that using corrector first may lead to conceal wrong predictions, thereby the classifier may fail to categorize them as wrong predictions. We also observe that modifying both correct predictions and partially correct predictions does not achieve improvements, demostrating the necessity of distinguishing correct predictions and partially correct predictions solely.\nframework uses a classifier with a cross-attention layer and a corrector based on the pointer model. However, ACC framework can also opt for alternative type of classifiers or correctors. To this end, we replace the classifier and the corrector with other models and compare their performance.\nTable 3 shows the results of the comparison between different model combinations on the dev set of MultiSpanQA. After replacing the classifier or the corrector, ACC framework shows declines, especially when applying a generative model, ACC framework lag behind other settings. This indicates that the generative models are less capable than traditional pointer models in correcting predictions."}, {"title": "5.2 Analysis on the Predictions", "content": "To analyze the capability of the classifier, we conduct a statistical analysis on its classification results. Table 4 shows the accuracy of the classifier on the dev set of MultiSpanQA. The classifier achieves an high accuracy on the correct predictions (95.82% for Tagger-BERT and 95.45% for Tagger-RoBERTa), demonstrating that the ACC framework reserves most correct predictions. On the other hand, the classifier exclude about 1/3 wrong predictions, contributing to the imporvements on EM F1 scores, while the accuracies on the partially true predictions and the wrong predictions can be further improved.\nTo analyze the capability of the corrector, we also conduct a statistical analysis on how many prediction has been changed. Table 5 shows the changes of the partially correct predictions on the dev set of MultiSpanQA. The corrector changes 30.77% of the answers for Tagger-BERT and 27% for Tagger-RoBERTa, respectively. For Tagger-BERT, 27.47% of the not-correct predictions are modified to the correct predictions, while 3.3% of the correct predictions are modified to the not-correct predictions. Furthermore, among all the partially correct predictions derived from the classifier, over 60% of the incorrect predictions remain incorrect, indicatisng a significant room for improvements."}, {"title": "5.3 Analysis on the Quality of the Predictions", "content": "Previous experiment indicates that the ACC framework imporves EM scores. However, the ACC framework may overfit to the annotation boundaries rather than enhancing the quality of the predictions. To this end, we utilize other metrics such as the Word Overlap and BERTScore (mentioned in Section 2.2) and compare the changes of these metrics after applying the ACC framework.\nFigure 4 shows the comparison results. Both Word Overlaps and BERTScores raise after applying the ACC framework, with the most significant enhancement in the Tagger where Word Overlap increases by 7% and BERTScore increases by 4%. This indicates that the ACC framework enhances the quality of the predictions, rather than overfit to the annotation boundaries."}, {"title": "5.4 Case Study", "content": "We conduct a case study to demostrate that the ACC framework effectively excludes incorrect predictions and corrects some partially correct predictions. We select a real example where the predictions exactly match the gold answers (i.e. EM F1 = 100%), shown in Figure 5. In this example, the MTMSN presents five predictions: \"the London Studios in 2000\", \"and then relocated to Grosvenor House\", \"London\", \"Dorchester Hotel\" and \"Daily Mirror\". The classifier identifies \"Dorchester Hotel\" as a correct prediction and \"Daily Mirror\" as a wrong prediction. The others three predictions contain irrelevant information or lack specific details, so they are identified as partially correct predictions and modifed by the corrector. This example demostrates that our ACC framework effectively enhance the quality of the predictions."}, {"title": "5.5 Pilot Study with LLM", "content": "ACC framework utilizes a fine-tuned ROBERTa encoder as the backbone. To investigate whether our proposed method works on larger models, we conduct a pilot study by replacing the classifier or corrector with a prompted LLM. The implementation details and prompts are shown in Appendix C.4."}, {"title": "5.6 Model Size and Inference Time", "content": "We analyze the model size and the inference time of the ACC framework. Results and analysis are shown in Appendix C.3."}, {"title": "6 Related Work", "content": "Recently, a series of MSQA benchmarks (Ju et al., 2022; Li et al., 2022; Yue et al., 2023) have been proposed to faclitate research on QA tasks that are closer to real-world scenarios. MSQA tasks require models to extract one or multiple answer spans from a given context. Therefore, traditional SSQA models (Seo et al., 2017; Yu et al., 2018) are not sufficient to handle multi-span questions.\nExisting MSQA methods can be categorized into four categories: (1) pointer-network-based methods. MTMSN (Hu et al., 2019) predicts the number of answers, then extracts non-overlapped answer spans; MUSST (Yang et al., 2021) uses an autogressive approach to iteratively extract multiple answers. (2) sequence-tagging-based methods. Segal et al. (2020) first convert MSQA task to a sequence-tagging task and utilize BIO tags to mark answer spans; Furthermore, Li et al. (2022) introduce multi-task learning and achieve better performance. (3) span-enumeration-based methods. SpanQualifier (Huang et al., 2023a) utilizes Multi-Layer Perceptron (MLP) to obtain confidence scores for each candidate span and applies a learnable threshold to select answer spans; Similarly, CSS (Zhang et al., 2024) compares each candidate span with its corresponding question after scoring to obtain answers more similar to the question. (4) LLM-based methods. With the emergence of LLMs like ChatGPT and GPT-4, generative pre-trained language models have been widely applied to various NLP tasks. Zhang et al. (2023) employ CoT strategies to prompt LLM, and Huang et al. (2023b) add negative examples in the few-shot demonstrations.\nExisting methods mainly focus on predicting more correct predictions, while the ACC framework takes a post-processing strategy which aims to reduce the number of incorrect predictions. By excluding or modifying incorrect predictions, the ACC framework achieves better performance."}, {"title": "6.2 Post-Processing Methods", "content": "The post-processing method refers to modifying the original of the model to obtain better predictions. Existing post-processing methods can be categorized into two types: rule-based methods and model-based methods.\nRuled-based methods typically involve mannually designed rules such as voting to process the outputs from models (Campos and Couto, 2021; Wang et al., 2023). On the other hand, model-based methods utilize additional models to modify the hidden states or outputs of the original model, which have been widely applied in Controlled Text Generation (CTG) (Yang and Klein, 2021; Krause et al., 2021; Kim and Cho, 2023). In addition to CTG methods, GRACE (Khalifa et al., 2023) applies a fine-tuned discriminator to guide language model towards correct multi-step solutions; Ohashi and Higashinaka (2023) utilize a generative model to rewrite the output from a dialogue system and optimize it with Reinforcement Learning (RL) algorithms (Stiennon et al., 2020).\nThe work most similar to ours is (Gangi Reddy et al., 2020), which utilizes a corrector to modify the outputs of the SSQA model. However, they only focus on partial matches in single-span questions. In constrast, we consider the correctness of multiple predictions in MSQA and additionally employ a classifier to exclude incorrect predictions."}, {"title": "7 Conclusion", "content": "In this work, we primarily focus on incorrect predictions of the MSQA models. Through a statistical analysis, we observe that models with better performance do not predict less incorrect predictions compared with other models. To this end, we propose ACC framework, which employ a post-processing strategy to exclude wrong predictions and modify partially correct predictions. Experiments and analysis show that the ACC framework significantly improving the performance by reducing the number of incorrect predictions and obtaining more correct predictions, enhancing the quality of the MSQA predictions."}, {"title": "8 Limitations and Future Work", "content": "In this work, we categorize incorrect predictions into \"partially correct predictions\" and \"wrong predictions\", based on whether the answer should be modified or excluded. However, for \"partially correct predictions\", there exists more complicated conditions, for example, an incorrect prediction may responses to multiple gold answers. However, the ACC framework can only obtain one modified prediction. In addition, we do not consider the gold answers that MSQA models fail to predict (i.e., \"missing predictions\"), although the SOTA model still miss 1/3 gold answers. As for future work, we will design more effectively models to handle \"partially correct predictions\" and \"wrong predictions\". we will also explore strategies to handle \"missing predictions\"."}, {"title": "A Details of Word Overlap and BERTScore", "content": "Assuming that a prediction pi contains k words {Pi1, Pi2, \u2026, Pik} and a gold answer aj contains l words {aj\u0131,aj2, ..., ajl}, the Word Overlap is defined as Equation 6:\n$$WO(p_i, a_j) = \\frac{|P_i \\cap a_j|}{\\max(k, l)}$$\nwhere |A| denotes the number of element in the set A.\nBERTScore primarily calculates the semantic similarity between the candidate text and the reference text using cosine similarity. Given candicate text X with m tokens {X1, X2, ..., Xm} and reference text Y with n tokens {Y1, Y2, \u2026\u2026\u2026, Yn }, BERTScore first computes the cosine similarity sij between each pair of token vectors xi and yj. Then, it maximizes the similarity score using a greedy matching approach to calculate the precision score P(X, Y) and the recall score R(X, Y). Finally, it computes the harmonic mean of these two scores (i.e., the F1 score) to obtain the final BERTScore. The above process can be represented by Equation 7-10. 7\n$$S_{ij} = \\frac{H_{x_i} \\cdot H_{y_j}}{|| H_{x_i} || || H_{y_j} ||}$$\n$$P(X,Y) = \\frac{1}{m} \\sum_{i=1}^{m} \\max_{j} s_{ij}$$\n$$R(X,Y) = \\frac{1}{m} \\sum_{j=1}^{m} \\max_{i} s_{ij}$$\n$$BS(X,Y) = 2 \\cdot \\frac{P(X, Y) \\cdot R(X, Y)}{P(X,Y) + R(X, Y)}$$\nwhere Hi and Hyj are the representations of xi and yj from a Pre-trained Language Model, ||a|| denotes the length of the vector a."}]}