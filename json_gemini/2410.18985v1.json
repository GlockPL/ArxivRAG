{"title": "rECGnition_v1.0: Arrhythmia detection using cardiologist-inspired multi-modal architecture incorporating demographic attributes in ECG", "authors": ["Shreya Srivastava", "Durgesh Kumar", "Jatin Bedi", "Sandeep Seth", "Deepak Sharma"], "abstract": "Objective: A substantial amount of variability in ECG manifested due to patient characteristics hinders the adoption of automated analysis algorithms in clinical practice. None of the ECG annotators developed till-date consider the characteristics of the patients in a multi-modal architecture. Methods: We employed the XGBoost model to analyze the UCI Arrhythmia dataset, linking patient characteristics to ECG morphological changes. The model accurately classified patient gender using discriminative ECG features with 87.75% confidence. We propose a novel multi-modal methodology for ECG analysis and arrhythmia classification that can help defy the variability in ECG related to patient-specific conditions. This deep learning algorithm, named rECGnition_v1.0 (robust ECG abnormality detection version 1), fuses Beat Morphology with Patient Characteristics to create a discriminative feature map that understands the internal correlation between both modalities. A Squeeze and Excitation based Patient characteristic Encoding Network (SEPcEnet) has been introduced, considering the patient's demographics. Results: The trained model outperformed the various existing algorithms by achieving the overall F1-score of 0.986 for the ten arrhythmia class classification in the MITDB and achieved near-perfect prediction scores of ~0.99 for LBBB, RBBB, Premature ventricular contraction beat, Atrial premature beat and Paced beat. Subsequently, the methodology was validated across INCARTDB, EDB and different class groups of MITDB using transfer learning. The generalizability test provided F1-scores of 0.980, 0.946, 0.977, and 0.980 for INCARTDB, EDB, MITDB AAMI, and MITDB Normal vs. Abnormal Classification, respectively. Conclusion: Therefore, with a more enhanced and comprehensive understanding of the patient being examined and their ECG for diverse CVD manifestations, the proposed rECGnition_v1.0 algorithm paves the way for its deployment in clinics.", "sections": [{"title": "Introduction", "content": "Over the years, cardiovascular diseases (CVDs) have increased significantly, almost doubling from 271 million in 1990 to 523 million in 2019 [1]. In addition, CVD-related deaths have also risen steadily, with numbers going up from 12.1 million in 1990 to 20.5 million in 2021 [2]. This steady rise in cardiovascular diseases poses a diagnostic burden on the healthcare infrastructure, primarily in developing countries like those in Africa, with higher concentrations and rates of increase in CVDs due to the unaffordability of healthcare services [3, 4]. The detection of CVDs has become eminent considering its statistical trends, but it has long been a challenging task, plagued by several hurdles. Traditional diagnostic methods often lack sensitivity and specificity, leading to false- positive (FP) or false-negative (FN) results [5]. Additionally, manual ECG signal interpretation is time-consuming and requires expert knowledge. Hence, in the near future medical informatics will play a pivotal role in improving the healthcare [6-9]. Machine learning and deep learning techniques have shown promise in improving the accuracy of CVD detection by analyzing the ECG signals [10-12]. Various methods have been developed for ECG analysis, like linear discriminator using RR interval based approach for arrhythmia categorization [13, 14], fuzzy Neural Network model and Hermite function for the feature extraction [15, 16], CNN-LSTM Fusion networks for temporal information of ECG signal [17, 18], non-linear decomposition methods and Support Vector Machine (SVM) [19-21], Random Forest (RF) [22, 23], k-Nearest Neighbour (kNN) [24, 25], and CNN-based Deep Neural Network [26-28]. These approaches can automate interpretation, reduce human error and enable real-time patient monitoring. Despite achieving near-perfect prediction capability for abnormal heart conditions like arrhythmias, real- life implications of published algorithms remain low. This is due to the inability to generalize over a diverse population as their validation was done on a very restricted and smaller data sample, the failure to correlate patients' characteristics (Pc) with the ECG morphology (Em), and the lack of comprehensiveness in the prediction outcomes [29]. In real diagnosis scenarios, an experienced cardiologist considers the multitude of information about the patient being examined along with ECG test results and has a wider array of abnormalities in mind while making diagnostic decisions about the patient's health from the ECG report [30]. Our study improves the real-life medical adoption of automatic ECG analysis algorithm by delivering on the aforementioned problems. We have developed rECGnition_v1.0 (robust ECG abnormality detection version 1) algorithm that simulates the procedural structure followed by cardiologists for inspecting ECG. It fuses the Po with Em and builds a correlation map to identify Po-specific patterns, for instance, smaller QRS"}, {"title": "Literature Survey", "content": "Incorporating patient demographic and medical data is extremely valuable when analyzing CVDs. A more thorough comprehension of potential anomalies can be attained by adding patient-specific information, such as age, gender, medical history, genetic variations, medication, and other pertinent aspects. As humans' environmental and social conditions are so rapidly changing [37, 38] that even cardiologists have difficulty segregating disease-induced and external parameter- induced changes, this remains a critical problem for automatic mechanisms as well. From the early-mid 20th century, various studies have been carried out across the globe to understand the distinctive pattern in beat morphology. Differences in QRS duration have been reported based on"}, {"title": "Material and methods", "content": "This study comprises three segments: Firstly, we validated the importance of incorporating the demographic features of the patient for understanding and determining the distinctive changes in heartbeats due to those features using UCIDB. Secondly, we developed the rECGnition_v1.0 algorithm using the conventional MITDB. Towards this end, we prepared and processed the dataset, which involved collecting, cleaning, and transforming the data to make it compatible with the model requirements. Subsequently, we designed the model architecture, which included choosing and configuring neural network layers, optimization algorithms, and regularization techniques. The architecture was carefully designed to incorporate the Pc in ECG signals relevant to arrhythmia classification. Finally, we trained the model using the prepared dataset and evaluated its performance on unseen data. This iterative process involved adjusting hyperparameters, optimizing model weights, and monitoring metrics to enhance the model's accuracy, reliability, and robustness. Thirdly, a comprehensive evaluation was conducted across multiple datasets, including INCARTDB, EDB and various class groups of MITDB, using transfer learning to justify the model's performance and generalization capabilities."}, {"title": "ECG datasets", "content": "The heart's electrical activity was recorded using either two or twelve leads in the ECG datasets utilized in this study (Table 2). The recordings were annotated at both the heartbeat and cardiac rhythm levels, and certain ECGs had been marked explicitly for particular conditions or syndromes (Suppl. Figure S1). The UCIDB dataset is noteworthy because it offered characteristics that were extracted from 12-lead ECG recordings and annotated for the classification of arrhythmias using the extracted feature set. The MITDB dataset was primarily used for the main experimentation and model training. To enhance the assessment of the model's generalizability and usefulness outside of the primary dataset, other datasets like INCARTDB and EDB were used."}, {"title": "Data processing", "content": "The definition of heartbeat used for this study is given by the Eq.1 which defines heartbeat as a temporal entity starting from the time instance (TQRSpeakn \u2013 x) and end at time instance (TQRSpeakn + x). It persists for 2x time delta in the continuous vector of potential values of the heart's electrical activity. The occurrence of heartbeat is deterministic and repeats itself after a certain time interval called RR interval. In all ECG datasets used, since the instance of all occurring QRS peaks were annotated, we segmented out the ECG signal VTn. The only unknown to the equation (x) was determined based on the sampling frequency of the dataset being examined and the average RR interval duration. For MITDB, the value of x being used is ~280ms.\n\n3.2.2 ECG signal pre-processing for standardized image representation\nThe 2D image representation of the signal is immune to various limitations of 1D signal representation, like non-normalized potential values, irregular sampling frequencies across datasets, etc. Nonetheless, there are some imperfections like uneven baseline activity and noise in potential values of acquired ECG signal, which degrades the quality of sample and negatively affects the model's training. Hence, before translating signal to image, we minimized the noise using a convolution based smoothing technique and corrected the baseline using statistical shift algorithm."}, {"title": "Signal smoothening using convolution and baseline correction", "content": "We applied Hanning window [67], for smoothening of ECG signal (Eq. 2 to Eq. 7). When applied in conjunction with convolution, it effectively reduced spectral leakage and provided better frequency resolution. The segmented signal v(n) was multiplied pointwise with the Hann window function leading to a smoother transition between adjacent segments. The transient part or edge effects at the beginning and the end of the convolved signal V which introduced unwanted artifacts and distortions, were handled using reverse reflection mechanism described by Eq. 5. Simple edge trimming process was used to obtain the initial length (n) of the signal; the final signal obtained after Eq. 7 had N more data points."}, {"title": "rECGnition_v1.0 architecture and fusion strategy for ECG Analysis", "content": "The rECGnition_v1.0 architecture proposed in this study was developed to effectively classify heartbeat anomalies by considering both the heartbeat image (X\u2081) and Pc (X2) [Figure 2]. The input X consisted of a 2D vector X\u2081 and a 1D vector X2. This combined information played a crucial role in establishing correlations and its mapping to the ground truth labels. The architecture was specifically tailored to holistically process the entire input, X. The heartbeat feature extraction unit, referred to as the heartbeat encoder, employed a CNN-based approach inspired by the seminal work of LeCun et al. [71]. CNNs have demonstrated exceptional performance in spatial-based classification tasks. To leverage these advancements, we adopted the efficient net [72] model as the backbone for the Heartbeat Feature Extractor network (HbFEnet). This model processed fixed size 224x224 heart images through a series of convolutional layers with different filter sizes, followed by activation functions and pooling layers. This process resulted in extracting high-level"}, {"title": "Loss function and optimization strategy", "content": "We combined the Adam optimizer Field [73] and the cosine decay with a linear warm-up learning rate scheduler to minimize cross-entropy loss during model training [74]. The Adam optimizer, which is renowned for its effectiveness in optimizing deep learning models, efficiently alters the model's parameters throughout training. A learning rate scheduler was also utilized to modify the learning rate dynamically. This scheduler decreased the learning rate progressively according to a cosine-shaped decay function (Suppl. Figure S2). The linear warmup phase at the beginning of training helps in stabilizing the learning process by progressively accelerating the learning rate. The final updated parameter is given by the last Eq. 15."}, {"title": "Hyperparameter optimization and k-fold validation", "content": "Deep learning relies greatly on hyperparameter optimization, substantially improving models' accuracy and practical applicability. Determining the optimal hyperparameter combination using conventional methods can be exhausting and computationally intensive. Grid Search, Random Search [75] and Bayesian Search are among the popular techniques proposed for determining the optimal hyperparameters for Al models. Bayesian Search [76], which employs probabilistic models to narrow down the search space intelligently, was utilized in this study. Depending on hardware specifications, input size, epochs, learning rate constants and feature extraction backbone, the length of each hyperparameter optimization run varied from 25 minutes to 3.5 hours. Notably, the preponderance of hyperparameter scans was performed only on MITDB-generated dataset splits. The complete process of obtaining the best performing model is represented in Suppl. Figure S4. Further, we utilized k-fold training [77], which resamples the training dataset into k groups. In our study, the training dataset was divided into k = 9 groups, also known as validation-folds. This approach allowed us to perform separate training on 8 groups while using the remaining group for evaluation purposes (Suppl. Figure S5). Before conducting the k-fold training, the stratified splits from the dataset were generated based on age, gender and heartbeat annotation."}, {"title": "Performance metrics", "content": "Metrics for measuring the effectiveness of deep learning models in multi-class classification tasks are essential. These metrics provide information about the model's precision, recall, accuracy, and F1-score to assess how well the model performs the task. Several metrics are frequently used for multi-class classification. A fundamental metric for gauging how accurate predictions are made overall is accuracy. Out of all positive predictions for a given class, precision is the percentage of true positive predictions. The ratio of correctly identified positive instances out of all actually positive instances for a given class is calculated as recall, also known as true positive rate or sensitivity. An accurate evaluation of a model's performance is provided by the F1-score, which is a harmonic mean of precision and recall."}, {"title": "Results and Discussion", "content": "By using the pattern-finding capability of machine learning, we first tested our hypothesis on a dataset having Em features; to do so, we utilized the UCIDB. 245 Normal beat characteristics were"}, {"title": "Analysis of UCIDB for ascertaining patient characteristics", "content": "By using the pattern-finding capability of machine learning, we first tested our hypothesis on a dataset having $E_m$ features; to do so, we utilized the UCIDB. 245 Normal beat characteristics were"}, {"title": "Performance of rECGnition_v1.0 on MITDB (10 class classification)", "content": "We validated the trained model for classifying 10 different types of heartbeats on MITDB (Experiment Eo) [Figure 4, Table 4(a)]. rECGnition_v1.0 achieved an overall F1-score of 0.9855 with a prediction accuracy of 98.56% (Table 5). Among all 10 anomaly classes, LBBB (L) [Se: 0.9950; Pr: 0.9950] and RBBB (R) [Se: 1.00; Pr: 0.9932] predictions were off the chart as model displayed a greater understanding of patient-specific changes from arrhythmia induced changes. In addition, high F1-scores of 0.9915, 0.9860 and 0.9861 were obtained for the identification of Paced beat (/), Premature ventricular contraction beat (V) and Atrial premature beat (A), respectively. Given the limited sample size of beats like Fusion of ventricular and normal beat (F), Fusion of paced and normal beat (f), Nodal (junctional) escape beat (j), Aberrated atrial premature beat (a), prediction scores were not as compelling as of other beats; despite having such imbalance rECGnition v1.0 attained an F1-score of >0.85 for all these classes."}, {"title": "Transferability test of rECGnition_v1.0 on INCARTDB, EDB and MITDB (AAMI and N vs. AB)", "content": "Inference from the MITDB was our primary experimental setup (Eo). However, to prove the robustness, generalizability and transferability of the model, we performed several experiments (E1, E2, E3, E4 and E5) by changing the datasets and their characteristics. Usually, the deep learning setups follow the strategy of independent train/test splits to build inference metrics; however, the"}, {"title": "Comparative Study", "content": "The objective of our study was not limited to attaining marginal enhancements in test scores on the test dataset. Instead, we focused on introducing a cutting-edge approach that establishes a framework for advancing CVD prediction by utilizing Artificial Intelligence (AI). To achieve our objective, we meticulously curated the most optimal models available till-date and compared them with rECGnition_v1.0 algorithm (Table 5). Our main aim was to offer a comprehensive comparison of the proposed methodology and its possible ramifications in the field of CVD detection instead of concentrating solely on quantitative metrics. It is acknowledged that substantially feature-engineered and heavily tuned algorithms have demonstrated remarkable outcomes. For instance, Houssein et al. [80] reported an overall accuracy of 99.33% and an F1- score of 0.9865 due to extensive feature engineering and hyperparameter tuning for demonstrating their improved Marine Predator algorithm's hyperparameter search capabilities. In contrast, rECGnition_v1.0 was only optimized for 10-class MITDB classification and transferred to other"}, {"title": "Conclusion", "content": "Our study employed the MITDB dataset and sought to improve ECG analysis and the precision of arrhythmia categorization by integrating patient characteristics variables and segmented ECG heartbeats. The rECGnition_v1.0 algorithm demonstrated a noteworthy overall accuracy of"}, {"title": "Summary Table", "content": "What was already known on the topic:\n\u2022\nDeep learning architectures are very effective when it comes to finding correlations among different modalities, and hence, by their incorporation, automatic diagnostic methodologies have achieved specialist-level accuracy.\n\u2022\nPatient demographics and physical parameters are known to influence physiological parameters to the extent that ECG can also be used to identify individuals uniquely.\nWhat this study added to our knowledge:\n\u2022\nThis work uses machine learning and public datasets to document physical parameter- induced ECG morphological variation.\n\u2022\nOur unique multimodal technique (rECGnition_v1.0) can fully interpret ECG data for different patients and improve classification and generalization."}, {"title": "CRediT author statement", "content": "Shreya Srivastava: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Visualization, Writing - original draft. Durgesh Kumar: Software, Validation, Formal analysis, Investigation, Data curation, Visualization, Writing - original draft. Jatin Bedi: Formal analysis, Writing - reviewing and editing. Sandeep Seth: Conceptualization, Writing reviewing and editing. Deepak Sharma: Conceptualization, Data curation, Visualization, Writing - reviewing and editing, Resources, Project administration, Funding acquisition, Supervision."}]}