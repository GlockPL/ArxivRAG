{"title": "Data Quality Awareness: A Journey from Traditional Data Management to Data Science Systems", "authors": ["Sijie Dong", "Soror Sahri", "Themis Palpanas"], "abstract": "Artificial intelligence (AI) has transformed various fields, significantly impacting our daily lives. A major factor in AI's success is high-quality data. In this paper, we present a comprehensive review of the evolution of data quality (DQ) awareness from traditional data management systems to modern data-driven AI systems, which are integral to data science. We synthesize the existing literature, highlighting the quality challenges and techniques that have evolved from traditional data management to data science including big data and ML fields. As data science systems support a wide range of activities, our focus in this paper lies specifically in the analytics aspect driven by machine learning. We use the cause-effect connection between the quality challenges of ML and those of big data to allow a more thorough understanding of emerging DQ challenges and the related quality awareness techniques in data science systems. To the best of our knowledge, our paper is the first to provide a review of DQ awareness spanning traditional and emergent data science systems. We hope that readers will find this journey through the evolution of data quality awareness insightful and valuable.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid emergence of data-centric technologies, particularly in the big data and ML field, has increased attention to the challenge of data quality, prompting a need for DQ awareness in emergent data science systems. Although recent research work still considers the importance of adapting existing data quality characteristics and frameworks from traditional data management practices, it is still somewhat unclear how to adapt them to improve DQ awareness in these systems. Data-driven Al systems, which are integral to data science, emphasize the integration of adaptive and scalable quality measures capable of dynamically responding to evolving data landscapes and user requirements. This is crucial for ensuring that data inconsistencies do not compromise the accuracy and reliability of AI outputs. Effective data quality management in these systems involves real-time integration of quality measures that adapt to changes in data and user needs.\nOur journey from traditional data management to data science, including big data and machine learning (ML), reflects an evolving landscape of data quality challenges. The singular focus of traditional data management systems on meeting the needs of immediate users often oversimplified the complexity of data quality issues. In a big data context, data quality issues are multiplying due to (i) large datasets that grow to the range of Tera- and Peta-Bytes, (ii) different types of data that existing data quality dimensions and evaluation methods cannot cope with, and (iii) real-time and time-evolving data, which may alter the data characteristics and consequently the insight derived from it. In [105], Saha and Srivastava present two main areas where data quality management challenges arise in the big data environment: (i) discovering data quality semantics and data repairing; and (ii) the trade-off between accuracy and efficiency using various computing models. This emphasizes the big data quality dimensions are more complex which leads to new techniques to assess data quality such as using big data platforms.\nThe ability to extract value from big data depends on data analytics which includes business intelligence, data visualization, machine learning, and statistical analysis. In this paper, we focus on machine learning which is considered the core of the big data revolution [19, 96]. This synergy between big data and machine learning allows data science to transform big data into insights, decisions, and predictions. However, the complex configurations of the datasets used in data science systems and the diverse backgrounds of ML practitioners introduce new data quality challenges.\nDespite the rich literature on data quality in both big data [18, 19] and machine learning [99] fields, as well as on the challenges of ML in conjunction with big data [83, 96], there is a gap in the literature that specifically explores the interplay between data quality issues and the unique challenges that arise from linking ML and big data within the scope of data science systems. Motivated by the need for data quality awareness in emergent data science systems, our paper aims to take readers on an enlightening journey through the evolution of data quality awareness from traditional to data science systems. We particularly focus on ML pipelines as a key component of data science systems. We emphasize the essential baggage from big data and traditional data management, including various quality awareness techniques, for effectively navigating the emerging data quality challenges in ML pipelines. By connecting these techniques with those of machine learning, we gain a deeper understanding of the challenges at hand.\nThe rest of the paper provides an overview of fundamental data quality concepts in Section 2. Section 3 presents the main DQ techniques in traditional systems. Section 4 explores the DQ issues in big data systems, including new dimensions and techniques. Section 5 discusses quality awareness in ML pipelines. Finally, Section 6 summarizes the key findings and suggests future research directions to advance data quality awareness in modern data science systems."}, {"title": "2 FOUNDATIONAL CONCEPTS OF DATA QUALITY", "content": "Data quality refers to the extent to which data is suitable for a specific task, emphasizing its actual utility and relevance to the context in which it is used. Data quality is often measured by its \"fitness for use\" in supporting operations, decision-making, and planning in various contexts [101]. To evaluate and ensure data quality, two main ingredients are used: DQ Dimensions and DQ Metrics. DQ Dimensions represent various aspects of data that determine its overall quality, including attributes such as accuracy, completeness, consistency, and timeliness. These dimensions provide a framework to assess how well data meets its intended purpose [8, 127]. DQ Metrics are quantifiable measures that evaluate these dimensions by assigning scores or percentages, helping to objectively assess data quality. Metrics enable comparison between datasets or across time, aiding in the continuous monitoring and improvement of data quality [94, 127].\nData Quality Awareness involves an organization's recognition of essential quality dimensions and metrics as a foundation for high-quality data. However, awareness alone cannot achieve or sustain these standards. Batini et al. [6] describe a data quality methodology as a structured framework guiding the selection and application of tools and techniques tailored to specific needs, thus translating awareness into practical steps for maintaining data quality. Supported by such a methodology, DQ awareness extends beyond understanding metrics to foster a structured approach for implementing and sustaining standards. Continuous improvement and assessment efforts are then essential to enhance source quality and meet or exceed user expectations [6, 18].\nData Quality Assessment is the process of systematically evaluating data against relevant quality dimensions to determine its adequacy for a given purpose. This involves identifying data anomalies, errors, and inconsistencies using predefined metrics and tests. Effective data quality assessment not only pinpoints defects but also elucidates their causes, enabling targeted interventions to bolster data quality [7, 116].\nData Quality Improvement aims to elevate the standards of data quality to meet or exceed new targets through the implementation of corrective measures derived from assessment results. Improvement strategies are typically categorized into two main approaches: data-driven and process-driven. Data-driven techniques directly update data values, like data cleansing. Process-driven techniques focus on redesigning data creation or modification processes, such as adding data format validation steps."}, {"title": "3 DATA QUALITY AWARENESS IN TRADITIONAL DATA MANAGEMENT", "content": "Traditional data management systems incorporate various techniques to provide data quality awareness. Literature shows that most of these techniques are designed from the perspective of data producers or data sources, focusing on modeling and measuring the quality of data at its origin. However, to effectively address data quality challenges, it is crucial to also consider the requirements of data consumers as well, incorporating techniques for modeling user requirements on data quality and ensuring that the data meets these expectations. Following, we emphasize the most commonly used techniques in our categorization, exploring them from both data and user perspectives."}, {"title": "3.1 Quality Awareness from Data Perspective", "content": "The data perspective for quality awareness focuses on data characterization. Dimensions are used to characterize various data properties [114]. Literature categorizes the most common DQ dimensions into intrinsic, accessibility, contextual, and representational quality [128]. The intrinsic dimensions are context-free and related to internal properties of data (e.g., accuracy); the accessibility dimensions are related to data access (e.g., availability); the representational dimensions are related to the design of the data (e.g., consistency); and the contextual ones, as their names indicate, are context-dependent (e.g., timeliness). In [64, 65], quality dimensions were divided into two types: (i) those with a declarative perspective to explain services (e.g., accuracy, completeness, and timeliness); (ii) and those with a perceptual perspective, which is convenient for perceptual judgment of service usage (consistency, accessibility, and responsiveness). To understand data characteristics and measure their data quality, data profiling techniques are mostly used, at various levels including: (i) attribute/tuple level such as missing value and domain violation; (ii) single relation such as violation of business rules; (iii) multiple relations as referential integrity violation; and (iv) multiple sources as inconsistent duplicate tuples. Existing data quality profiling techniques include defining new data quality rules (e.g., Functional Dependencies) and identifying data quality problems (e.g., inconsistent data) [1, 23, 62, 90, 93].\nThe concept of data quality profiling in [11] is achieved by associating quality contracts with data sources. A set of contracts related to one or more data sources forms a quality profile. These profiles are used to negotiate quality requirements with the data source's wrapper, ensuring that the query processing framework selects sources based on their quality characteristics.\nConditional DQ profiling, as proposed in [134], associates the quality of specific data attributes with conditions specified in user queries. For example, the quality of the Price attribute is evaluated only when the Brand is specified as Sony in a query, as presented in [134]. This method allows for targeted assessments of data quality based on user-defined relevance, rather than a blanket evaluation of all attributes. In addition, when quality metadata is unavailable, broader quality metadata may be used to estimate attribute completeness. For instance, if the Price attribute lacks completeness data, the overall dataset completeness can serve as a surrogate. However, this method can introduce significant errors; for example, a dataset might be 50% complete overall while the Price attribute could be only 25% complete, leading to discrepancies in data quality assessments."}, {"title": "3.2 Quality Awareness from User perspective", "content": "Quality-aware query processing techniques are largely used to ensure that the quality of data meets user requirements and preferences, this mainly involves query language extensions and adapting query processing."}, {"title": "3.2.1 Query language extensions.", "content": "They enable the integration of data quality considerations into query processing. They allow the expression of quality metrics and constraints across various quality dimensions in a simple and declarative manner. In [134], an SQL extension was proposed to model user preferences related to data quality through hierarchical prioritization. Additionally, a framework referred to as DQAQS was introduced to enhance user satisfaction by considering these preferences in query results. In [39], an extension to XQuery incorporates domain-specific quality constraints, referred to as quality views (QV), during query processing. This work extends previous works of the same author that addressed completeness [106] and timeliness dimensions [33].\nIn [11], the quality-extended query language XQual, based on a negotiation strategy, was introduced for selecting dynamic sources. XQual extends SQL with a Qwith operator to specify quality constraints through quality contracts and profiles, adapting query processing to meet defined quality dimensions (e.g., dataAge and lastUpdate). This approach was later applied to online skyline queries, incorporating graph methods to optimize quality-based results using the nearest neighbor search [12]. Recently, XQual was used to enforce data quality thresholds for training data in machine learning, ensuring that only data meeting specified quality standards is used [27]."}, {"title": "3.2.2 Adaptive query processing.", "content": "It allows to dynamically handle data quality issues and provide more reliable query results. It involves creating multiple query execution plans that can be switched dynamically during runtime based on the quality of the data. In [89, 91], a distributed query planning (DQP) algorithm discards low-quality sources, ordering plans by completeness. System P [103] uses a completeness-driven approach where peers rank local plans by potential result size and prune based on user-set budget thresholds, balancing completeness and cost. Similarly, [134] incorporates planning and optimization to assess each plan's utility based on expected data quality, ensuring efficient handling of diverse data sources."}, {"title": "4 DATA QUALITY AWARENESS IN BIG DATA", "content": "As data-centric technologies advanced, particularly with the advent of big data, the scope of data quality expanded. Following, we present the quality dimensions related to new data quality challenges and the impacts of big data characteristics on the big data dimensions."}, {"title": "4.1 Big Data Quality Dimensions", "content": "Traditional quality dimensions (e.g., accuracy, completeness, consistency, etc.) are not sufficient to assess big data. In the context of big data, the meaning and calculation methods of these traditional dimensions undergo significant changes.\nThis section presents the common quality dimensions relevant to big data. We classify them according to the existing literature [18, 19, 42, 46], into source-specific and users' perspective dimensions."}, {"title": "4.1.1 Source' Perspective dimensions.", "content": "The UNEC (United Nations Economic Commission for Europe) classification identified three main types of data sources from the most structured to the least structured ones: process-mediated, machine-generated, and human-sourced [46]. Big data quality dimensions, characterized as source' perspective, are then categorized according to these source types as follows:\n\u2022 Process-mediated data sources, usually correspond to relational databases that provide structured data (e.g., business data with customer records). The main quality issues concern the values provided for records' attributes, e.g., incorrect values, duplicates, incomplete values, etc; and the related quality dimensions to assess the data are then: consistency, accuracy, freshness, etc.\n\u2022 Machine-generated information sources, use sensors and machines to measure and record the events and situations in the physical world and produce machine-generated data that is often well-structured. Still, its size and speed can become increasingly important. The main quality issues consider the environment of such measures and records (e.g., the noise problems of machines, the environmental effects, etc). The related quality dimensions are mainly: accuracy, completeness, consistency, trustworthiness, and freshness.\n\u2022 Human-sourced information sources correspond particularly to social networks that store human experiences, photographs, audio, videos, etc. In addition to some of the quality dimensions mentioned above, the ambiguity related to short text understanding can also be considered."}, {"title": "4.1.2 Users' perspective dimensions.", "content": "To better understand big data applications, quality dimensions are defined from the users' perspective. Two main data quality assessment approaches are considered, effective and context-dependent, to define the corresponding DQ dimensions. The effective data quality assessment, emphasizes evaluating quality dimensions relevant to user interactions with data. Key quality dimensions include reliability, availability, usability, relevance, and presentation quality, as noted in [18]. These dimensions reflect users' ease of accessing data, its usefulness, trustworthiness, alignment with users' expectations, and the overall improvement of their satisfaction. Structured hierarchically, they consist of sub-elements and indicators that refine their meaning. The context-dependent quality assessment, highlighted in [4], emphasizes the adaptation of quality dimensions based on the specific context of data assessment, encompassing levels such as data source, data type, and intended application. For instance, accuracy in batch data differs from that in sensor streams. In structured datasets, completeness may refer to non-null values, while for image data, it could mean image clarity. In [85], the adaptive quality model is extended by considering user requirements related to execution time and performance constraints, introducing the confidence dimension to address potential accuracy issues, and measuring data trustworthiness. The importance of context is further illustrated by specific domain applications [43, 110]. Social media platforms prioritize timeliness and accuracy for sentiment analysis, while online news platforms value credibility to ensure that reported information is trustworthy [37, 38, 120]. In healthcare, accuracy, completeness, and privacy are critical for ensuring the reliability of clinical data and the success of predictive analytics used in disease forecasting [73, 78]. Similarly, in financial data analysis, accuracy and timeliness are crucial for robust market assessments and detecting fraud in real time [6, 121]."}, {"title": "4.2 Impact of Big Data Characteristics on Big Data Quality", "content": "The works presented above, and much of the previous work in the area characterize big data quality by (i) investigating the relation between big data characteristics and data quality dimensions, (ii) identifying specific data quality dimensions, and (iii) analyzing how to evaluate them to constitute the basis of a big data quality assessment. However, there is still a gap between big data characteristics and data quality dimensions, and their impact on better understanding big data applications. This is due to the impact of data quality on the insight derived from it, which makes the data quality and its value conceptually different, despite their correlation [2]. Furthermore, data quality affects the whole big data pipeline for an application, including data acquisition, analysis, and different interpretations [104]. Hence, assessing the overall quality of big data for a given application (e.g. analytic task) remains a grand challenge.\nTo fill the gap between Big Data Characteristics (BDC) and quality dimensions, previous work investigated the correlation between big data and data quality specifically for financial service organizations. [126] suggested that, in the financial domain where multiple data sources are used, data variety is the most important BDC that affects most DQ dimensions, e.g. accuracy, consistency, security timeliness, and completeness. It was also suggested that velocity is often correlated to timeliness due to the correspondence between the rapidity of generating and processing data and the timely use of data.\n[48] studied the effect of data volume on DQ dimensions which in turn influence the overall effectiveness and adoption of big data analytics in business contexts. The DQ dimensions considered in this study are less common and are data diagnosticity, accessibility, security, and task complexity. Data diagnosticity corresponds to the valuable insight from data; data accessibility measures the easiness of data availability; data security is related to security issues when aggregating and analyzing big data; and task complexity is related to data processing tasks. Based on the theory of valence, used in economics and psychology to explain the relationships between individuals' behavior and the perceptions of risks and benefits, the authors studied the positive and negative roles of DQ dimensions to develop guidelines for business data practices. Their findings show that BDCs and in particular the data volume improve data diagnosticity and accessibility, positively impacting big data analysis. However, BDCs also increase data security concerns and task complexity, negatively affecting analysis. Indeed, the large volume of data raises security issues, and using frameworks like Hadoop for data aggregation in distributed environments poses additional risks [13].\nThe findings indicate that contextual data quality (DQ) dimensions, particularly timeliness and accessibility, are most closely correlated with Big Data Characteristics (BDCs) for user applications. However, there are several limitations in existing studies. For instance, DQ dimensions beyond data security and accessibility can also affect big data analytics usage. While BDCs often focus on data volume, the impacts of other dimensions like variety and velocity on data security and accessibility, and their influence on analytics usage, remain unclear. Additionally, most research is limited to the financial sector, lacking broader applications in other domains and types of big data applications. This narrow focus hinders generalization, as user validation is insufficient to broadly apply findings across various big data contexts.\nEven if some limitations can be pointed out in the above studies, their findings are useful for understanding the relationship between BDCs and DQ dimensions throughout user applications. Based on the above studies, we summarize the BDCs and their impact on DQ dimensions in Figure 1."}, {"title": "4.3 Quality Awareness Techniques for Big Data", "content": "In this section, we present quality awareness techniques in a big data context, according to the V's characteristics, focusing on sampling, parallel processing, and incremental techniques."}, {"title": "4.3.1 Parallel Computing.", "content": "Distributed frameworks such as Apache Hadoop and Apache Spark help manage data quality across large datasets by parallelizing the profiling and assessment tasks. We showed in the section above, how parallel computing can resolve the problem that samples are not always representative of the whole large dataset, which is important to compute data quality measures. Parallel processing frameworks for big data can also be used for quality assessment that is not based on sampling. Several studies assess the performance and scalability of data quality (DQ) assessment tasks by implementing their solutions on big data computing frameworks. The main contributions indicate that using Spark can significantly enhance computational efficiency for very large data volumes [19, 26, 108]. For instance, Cisneros et al. [26] investigate the performance of combining Spark and Python Pandas for data quality calculations compared to using them separately on different data sizes."}, {"title": "4.3.2 Sampling and sketch techniques.", "content": "Sampling techniques help reduce the time required to calculate data quality by enabling the approximation of results. Commonly used methods include simple random sampling, systematic sampling, stratified sampling, cluster sampling, and reservoir sampling [14, 57, 81, 113, 124]. These techniques help determine sample sizes and select samples to effectively evaluate quality metadata considering the data source's features and the relevant quality dimensions. For instance, in [19], sampling improves calculation accuracy within the constraints of time or optimizes calculation time when accuracy meets user requirements. [81] discusses the use of simple random and systematic sampling to assess data quality dimensions like completeness, accuracy, and timeliness, demonstrating that systematic sampling is preferable for accuracy and completeness, while simple random sampling suits timeliness assessment best. Further, [118] explores bootstrap sampling as part of a Big Data Quality (BDQ) evaluation scheme, which uses sampled data to profile the dataset and choose suitable quality metrics, particularly for accuracy, completeness, and consistency. The subsequent work in [119] applies the Bag of Little Bootstrap (BLB) technique to enhance efficiency in processing unstructured data.\nSampling heterogeneous data requires preparation to ensure it is suitable for quality assessment. Taleb et al. [119] proposed methods for preparing unstructured data like text, images, and videos for quality assessment. For instance, text data undergoes text mining, while video data requires feature extraction. This preparation identifies useful information that informs subsequent quality assessments by enabling the selection of key features. For remote sensing images, Wang et al. [129] introduced a multi-level non-uniform spatial sampling method that improves accuracy assessment by considering spatial auto-correlation and heterogeneity. Additionally, other methods like multi-layer spatial sampling and BLB sampling are used, while random sampling is effective for faster evaluations with smaller datasets.\nOverall, sampling is integrated into big data frameworks to accommodate large datasets, often necessitating innovative approaches like block-based sampling methods that utilize MapReduce to manage data scale and distribution effectively [56].\nSketch Techniques, compared to sampling techniques, also greatly reduce the size of an input dataset. The difference is that sketching generates an approximate, compact data summary, which retains properties of interest. In contrast, sampling does not guarantee the preservation of such properties when selecting a subset of the data [29]. Sketch techniques are considered fast, easy to parallelize, and can provide high approximation accuracy [28, 32]. They are often used to efficiently find approximate answers to some online analytical processing queries, supported on big data (e.g., Pig, Hive, Spark SQL), with some reasonable guarantees on the quality. This is the case for many applications with online requirements that want to get results fast, as data-stream processing applications [76]."}, {"title": "4.3.3 Data fusion and integration Techniques.", "content": "These techniques involve combining data from various sources to create a coherent dataset. In the big data environment, data can originate from multiple sources such as social media, internal enterprise databases, and IoT devices, and these data can vary in structure, format, and quality [70]. The goal of data fusion and integration is to resolve inconsistencies between these datasets, such as mismatches in timestamps, differences in data formats, and potential data duplication or conflicts [35]. A large body of research has focused on data fusion and integration techniques, using methods such as data mapping, normalization, and transformation algorithms to ensure accuracy and consistency [16]. Metadata is often used to align the structure and semantics of different sources [54], and quality assessments are often integrated to meet standards like integrity and accuracy. Readers can refer to existing studies for more in-depth insights [34, 35, 40]."}, {"title": "4.3.4 Incremental Big Data Quality Assessment.", "content": "The pace at which data is growing makes some data outdated, and consequently, the DQ assessment including profiling, very challenging. Indeed, data profiling methods should efficiently process such data growing, and without profiling the whole dataset again. Moreover, data quality metrics should be updated continuously. To improve the computation of the updated data, [1] suggested performing incremental and continuous profiling. Incremental profiling updates data based on periodic changes from previous results, while continuous profiling updates data on the fly as data is entered. DQ metrics can be updated in three ways: on-demand, periodically or event-driven [19, 107]. In addition, interactive profiling, including online profiling [90], improves user satisfaction by considering user quality requirements and displaying intermediate results from the interaction between users and applications. This allows users to make decisions based on early profiling results. Additionally, [136] proposed a visual data profiling interface to enable user interaction for data cleaning, error detection, and transformations."}, {"title": "5 QUALITY AWARENESS IN ML PIPELINES", "content": "Data analytics, a cornerstone of big data, relies heavily on the quality of data to deliver accurate and actionable insights. Machine learning, as a core approach within data analytics, leverages these insights to develop predictive models and automate complex tasks, making it integral to modern data science systems. These emergent systems demand an iterative and integrated approach to data quality, addressing issues such as label quality, data drift, data imbalance, and incorrect data entries throughout the entire ML pipeline."}, {"title": "5.1 ML-based quality dimensions", "content": "DQ dimensions tailored to ML pipelines cover the entire lifecycle, from data preparation to model training and monitoring. In this section, we review the literature on data quality dimensions in machine learning (ML) pipelines, based on various factors [31, 87, 92, 97]. While these studies offer valuable classifications of data quality dimensions, their frameworks lack to emphasize the cause-effect connection between the quality challenges of ML and those of big data to allow a more thorough understanding of emerging data quality challenges in data science systems and particularly in ML pipelines. To address this, we synthesized the existing techniques in Section 5.2, based on the following five main categories that consolidate and expand upon previous quality frameworks, in particular those in [30, 87, 99, 141]. Table 1 presents these categories alongside their related quality-aware techniques."}, {"title": "5.1.1 Data-based Dimensions.", "content": "The data considered here is used as input for each ML component. [117] distinguishes between training data and serving data. The training data refers to the dataset used during the development phase to train an ML component, while the serving data refers to the dataset used during the deployment phase to make predictions in real time.\nTraditional data quality dimensions [65], such as representativeness, correctness, completeness, currentness, and intra-consistency, are essential to ensuring that the data accurately reflects real-world scenarios, is free from errors, and is both comprehensive and up-to-date. In addition to these conventional dimensions, machine learning introduces several unique data quality dimensions that are critical to ensuring the integrity and fairness of the model's performance. For example, Train/Test Independence is a key consideration, where it is imperative that the training and testing datasets remain independent of each other. This prevents data leakage, which could otherwise artificially inflate performance metrics and lead to misleading conclusions about the model's effectiveness. Another critical dimension is Balancedness, which refers to the need for a balanced distribution of classes or categories within the dataset. An imbalanced dataset, where certain classes are overrepresented, can result in a biased model that performs poorly on underrepresented classes. Ensuring balanced data helps maintain fairness and accuracy across all predictions. Lastly, the Absence of Bias addresses the need to ensure that the dataset does not contain any inherent biases that could lead to unfair or unethical outcomes."}, {"title": "5.1.2 Model-based Dimensions.", "content": "The quality of an ML model is influenced by several factors, including the specific task being addressed (e.g., classification, clustering, regression, anomaly detection, dimensionality reduction), the type of model (neural network, decision tree, etc.), the data used for building the model (i.e., training), and evaluating the developed artifacts, as well as the manner in which the data is separated for training and validation [115, 125]. Dimensions like performance, robustness, scalability, model complexity, and resource demand [65] help ensure that the model operates effectively across various scenarios and environments. In addition to these conventional metrics, Fairness and Explainability are increasingly important for building models that are not only technically sound but also ethically responsible and trustworthy. Fairness ensures that the model's decisions are unbiased and do not systematically disadvantage any particular group or individual. Addressing fairness allows for preventing discriminatory outcomes and promoting ethical AI use. Explainability refers to the model's ability to provide clear and understandable reasons for its decisions, which is important for gaining user trust and ensuring transparency."}, {"title": "5.1.3 Process-based Dimensions.", "content": "The quality of machine learning systems depends not only on the data and models but also on the robustness of the processes used to develop, deploy, and maintain these systems. Effective process management ensures that the system remains reliable, efficient, and secure throughout its lifecycle. Key dimensions in this category, as highlighted by [87] under the system facet, include recoverability, portability, efficiency, transparency, traceability, cost, accessibility, ease of manipulation, and security. For example, traceability ensures that every step in the system's lifecycle can be audited, making it easier to identify and resolve issues, while efficiency focuses on optimizing resource usage such as computational power and time, ensuring the system runs smoothly and effectively. These dimensions are vital for ensuring the robustness, adaptability, and security of machine learning systems, particularly as they scale or evolve."}, {"title": "5.1.4 Use case and context-based Dimensions.", "content": "Modeling quality requirements in ML pipelines based on specific use cases and application contexts is presented in [115, 125] to emphasize the importance of tailoring quality dimensions to the particular needs of ML applications. For instance, healthcare applications require stringent data privacy measures and high model accuracy. However, if the data used in these applications lacks contextual relevance, such as using a generic dataset for specialized medical diagnoses, this can lead to significant performance issues and may compromise patient safety. Addressing these issues helps ensure that ML models are effectively aligned with their intended applications, taking into account dimensions such as value, contextual relevance, and use case specificity. Such alignment leverages trust in ML applications, highlighting the importance of continuous assessment and adaptation of quality requirements throughout the ML pipeline, as presented in [99]. Understanding these dimensions helps to identify various factors that can impact the quality of ML components (e.g., classification). Indeed, based on existing use cases, the various categories of quality dimensions identified can ensure that the ML pipeline effectively meets its intended objectives [115]."}, {"title": "5.1.5 Stakeholders-based Dimensions.", "content": "Data quality challenges related to stakeholders in ML pipelines are mainly presented in [96, 99]. The important role of ML practitioners, including data engineers, data scientists, and domain experts, should be emphasized to ensure both effectiveness and ethical responsibility in ML pipelines. This allows the data to align with specific use case requirements, mitigating the risk of using data in ways that may be ethically misaligned with the original intent of data curators. Ethical alignment is then a key quality dimension allowing data usage to adhere to the ethical standards set by the data curators and comply with legal and social norms. For instance, in contexts where data can perpetuate biases, all stakeholders must ensure that their work supports fairness and accountability. In addition, all stakeholders from data curators to end-users, must have a clear understanding of how data is processed and how model decisions are made. For this aim, transparency is another key dimension to build trust and facilitate collaboration across diverse users including developers, business analysts, and policymakers."}, {"title": "5.2 Quality Awareness Techniques for ML Pipelines", "content": "Following, we present the ML quality techniques and emphasize their connection to big data challenges."}, {"title": "5.2.1 Data Validation and Cleaning.", "content": "Data Validation techniques verify that data conforms to certain standards [53] and detect anomalies before they impact model performance. One fundamental technique is the use of descriptive statistics, which involves calculating metrics such as mean, median, variance, and standard deviation to summarize the central tendency, dispersion, and shape of the data distribution [59]. Recent advancements in this area include robust statistical methods that improve the detection of outliers and anomalies in large-scale datasets [112]. Schema validation ensures that the data adheres to a predefined schema, checking for consistency in data types, ranges, and formats [3]. Recent schema evolution techniques automatically adapt schemas to handle changes in data formats over time [9]. Anomaly detection techniques, such as Isolation Forests [80] and One-Class SVM [109], are used to identify data points that deviate significantly from the norm, indicating potential errors or outliers in the data. More recent methods, like Deep Anomaly Detection using Autoencoders and LSTM-based models [20], provide enhanced detection capabilities for complex, high-dimensional data.\nAutomated validation tools are increasingly used to maintain data quality in large-scale ML systems. Schelter et al. [108] introduced Deequ, a validation tool for extensive datasets using Apache Spark, which allows user-defined quality constraints and provides an API for real-time monitoring through incremental assessments (cf. Section 4.4). Deequ includes ML techniques for automating tasks like outlier detection and value prediction. Data Linter [60] provides automated suggestions for data cleaning by analyzing data schemas, statistics, distributions, and data type inconsistencies that could impact model training. Google's TensorFlow Extended (TFX) [98], deployed in Google Play's recommendation systems, includes schema inspection and anomaly detection tools to ensure data quality throughout the training and deployment phases.\nData cleaning techniques address other challenges such as noise, outliers, and duplicate records during validation. While there is a large literature on data cleaning [72, 77, 102], not all the cleaning techniques directly benefit ML accuracy. Cleaning techniques are more effective when they target improving model accuracy and making training more robust to noise. Indeed, data noise is considered adversarial when it contains malicious poisoning. In such cases, cleaning involves data sanitization techniques aimed at removing malicious inputs, along with outlier detection methods [17] like Isolation Forests and deduplication [86, 132] processes to eliminate anomalies and duplicate records, thereby further enhancing the robustness of machine learning models.\nData imputation fills gaps caused by missing data, thereby enhancing the completeness and consistency of training data. Techniques such as Multiple Imputation by Chained Equations (MICE)[123] perform multiple imputations to generate several complete datasets, while k-NN [138] iteratively imputes values by calculating distances for both numerical and categorical attributes. Generative adversarial networks (GANs) [135], adapted for imputation, generate synthetic values that accurately fill data gaps, while autoencoder-based models [84] leverage latent variable techniques to impute data missing at random. Additionally, recent adaptive frameworks automatically select optimal models and hyperparameters, further improving imputation quality [63]."}, {"title": "5.2.2 Data Augmentation.", "content": "Data augmentation techniques allow for improving the robustness of models, particularly for datasets that are small or imbalanced. They expand training data to increase their diversity, mitigate overfitting, and maintain model performance. For imbalanced datasets, techniques such as SMOTE [21", "21": "are commonly used. SMOTE works by creating synthetic samples for underrepresented classes, while Mixup involves mixing pairs of examples from different classes to generate new training samples. Deep learning-based techniques like Variational Autoencoders (VAEs) [71", "49": "models such as StyleGAN [69"}, {"21": "attempt to mitigate this by automating augmentation strategies and improving the balance between realistic and diverse data. Finally, quality assessment metrics such as signal-to-noise"}]}