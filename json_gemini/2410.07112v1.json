{"title": "VHELM: A Holistic Evaluation of Vision Language Models", "authors": ["Tony Lee", "Haoqin Tu", "Chi Heem Wong", "Wenhao Zheng", "Yiyang Zhou", "Yifan Mai", "Josselin Somerville Roberts", "Michihiro Yasunaga", "Huaxiu Yao", "Cihang Xie", "Percy Liang"], "abstract": "Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website at https://crfm.stanford.edu/helm/vhelm/v2.0.1. VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.", "sections": [{"title": "1 Introduction", "content": "Vision-language models (VLMs) models that take both text and images as a prompt and produce text as output-have seen rapid growth and deployment in the past year. They are used in visual question answering [35], text-driven image creation and alteration [26], image captioning [7], and robotics [49]. Despite their prevalence, much remains unknown regarding their capabilities, limitations, and risks, particularly in the areas of contextual understanding, bias [10], ethics [40], and safety [28].\nCurrent benchmarks for VLMs assess the models only on a limited number of factors, often related to their perception or problem-solving capabilities. Other factors, such as the ability to generate contextually relevant and unbiased content, their performance across diverse linguistic and cultural contexts, or their environmental impact, are less frequently studied. We refer readers to Table A1 for a comparison of the factors that the benchmarks assess."}, {"title": "2 Related Work", "content": "VLM benchmarks There exists a wide range of benchmarks that measure the various capabilities of VLMs. We summarize the ones that we incorporate into VHELM. A common method of testing VLM is visual question answering (VQA). It presents the VLMs with an image and an associated question that the models are expected to answer. VQA tasks can vary significantly, depending on the image type (e.g., real-world photographs [12, 14, 34, 44], artworks [44], sketches [39]), domains (e.g., celebrity, landmark) or subjects [47] (e.g., literature), or languages [12,34]. Other methods of probing VLMs include captioning [38], generating codes, or simply text-generation [28]. The benchmarking community has focused most of its effort on quantifying the knowledge, visual perception, and reasoning capabilities of VLMs.\nWe summarize which aspects the benchmarks study in Table A1.\nHolistic evaluation The concept of holistic evaluation has gained traction as developers and researchers alike strive to understand the multifaceted capabilities and limitations of foundation models [4]. Notable efforts in this direction include comprehensive assessments of LLMs with Holistic Evaluation of Language Models (HELM) [24] and text-to-image models with Holistic Evaluation of Text-to-Image Models (HEIM) [21], which aim to evaluate these systems across a range of dimensions beyond their primary function. These studies underscore the importance of a multi-dimensional approach to evaluation, highlighting that the true potential and challenges of foundation models can only be fully understood by considering a variety of factors.\nDespite these advancements, this holistic approach has yet to be extensively applied to vision- language models. Previous studies within the field have often concentrated on single aspects of model performance. For instance, the work by Lin et al. [25] primarily focuses on evaluating models"}, {"title": "3 The VHELM Framework", "content": "VHELM focuses on vision-language models that take in interleaved images and text input as prompts to produce text completions 1 (see Figure A1). The VHELM evaluation process consists of 4 main components: aspect, scenario, adaptation, and metric (see Figure 2).\nAn aspect is a specific evaluative dimension that contributes to assessing the overall performance. The aspects considered in VHELM are bias, fairness, knowledge, multilinguality, reasoning, robustness, toxicity, and visual perception (details are in Sec. Section 3.1). Aspects are evaluated by computing metrics over scenarios.\nA scenario represents a use case for a VLM and is identified by a task (e.g., question answering, code generation, and captioning) and a usage category such as the domain, origin, language, or subject. An example scenario is \"visual question answering on medical images\" where the task is visual question answering and the usage category is medical images. We consider a wide range of scenarios, with tasks ranging from visual question answering to captioning and usage categories consisting of multiple languages, subjects, and image types. The scenarios used in VHELM are listed in Table 3.\nA dataset is a set of instances-defined as a pair of prompt and reference that can be used for evaluating the model performance on one or more scenarios. A dataset can power multiple scenarios, such as in the case of Bingo [6], where the 'region bias' or 'OCR bias' subsets assess visual question answering of images from different geographic locations (used to test fairness) and visual question answering of images with text in various languages (used to test multilinguality), respectively. A dataset is sometimes synonymous with the scenario, especially in the context of model evaluation. For example, we may state \u201cMMMU (Accounting)\" as a scenario with the understanding that the accounting subset of MMMU tests visual question answering in the domain of accounting. VHELM compiles a total of 21 existing datasets (see Table 3).\nAn adaptation is a specific procedure for invoking a model. Adaptation strategies include zero-shot prompting, k-shots prompting, and chain-of-thought prompting. In this study, we use only zero-shot prompting as it is the most common strategy used by the layperson.\nA metric quantifies how well a VLM performs on a scenario. Some examples of metrics are exact match or using either a human or a model to score on a scale of 1 to 5."}, {"title": "3.1 Aspects & Scenarios", "content": "VHELM considers 9 aspects that are crucial for developing capable, safe, and reliable VLMs (see Table 2). These include fundamental capabilities, such as visual perception, knowledge, and reasoning, and behavior relating to society and ethics, such as bias, fairness, multilinguality, robustness, toxicity, and safety.\nVLMs that produce images as output are currently not covered in this study."}, {"title": "5 Results and Analysis", "content": "In this section, we present some of our key empirical findings and while encouraging readers to refer to our interactive website at https://crfm.stanford.edu/helm/vhelm/v2.0.1, where they can view the result groups and sort them by their desired column. We also display the prompts, predictions, and scores for every model and instance there."}, {"title": "6 Discussion", "content": "6.1 Limitations\nThe choice of metrics can affect the evaluation of the models and we have opted to use automatic metrics in order to reduce cost and speed up evaluations. We simplify the scenarios, such as making the questions multiple-choice ones, in order to reduce the variance in the output. Furthermore, we use Prometheus-Vision, which has been shown to emulate human evaluators [20]. Despite our best efforts, these metrics are not perfect, as can be seen from Figures A4 and A5. We will continue to refine the metrics and update our benchmark as better ones become available.\nOur benchmark currently measures 9 aspects that we believe are important to VLMs; there may be other aspects that are equally important that we may have missed, and we encourage readers to provide feedback. We acknowledge that the coverage for some of the aspects (e.g., toxicity or safety) is thin, and we would like to develop or integrate more scenarios for them. Additionally, identifying an aspect of a scenario is not exact, as there are overlaps between the aspects. For example, fairness and robustness are interchangeable when the language of the inputs is perturbed (i.e., AAE perturbation is both fairness and robustness). We envision VHELM as a living benchmark and will continuously strive to add more models and scenarios over time.\nBenchmark results are technical objects that are only useful if they are contextualized. Further work has to be done to understand the nuances of the scores and quantify the correlation between the scores and real-world impact.\nWe are also cognizant that our benchmark, like others before us, can be 'gamed'. We hope to integrate scenarios that will pull fresh, real-world data at execution time so that models are always evaluated on data that is unseen during training."}, {"title": "6.2 Broader Impact", "content": "VHELM evaluates VLMs on a standardized set of prompts, scenarios, and metrics, allowing stake- holders, including researchers, developers, and policymakers, to better understand and compare the performance of different VLMs. Our evaluations can quickly highlight the strengths and flaws of each model across the various aspects, thereby encouraging VLM developers to iterate toward better models."}, {"title": "7 Conclusion", "content": "VHELM assesses 9 important aspects for 22 well-known VLMs, which we hope will contribute to the ongoing development and refinement of VLMs, making them more reliable, fair, and useful across a"}]}