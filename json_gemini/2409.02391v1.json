{"title": "Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Translation", "authors": ["Ali Merali"], "abstract": "This paper derives 'scaling laws' - empirical relationships between the amount of training compute used for a Large Language Model (LLM) and their performance- for economic outcomes. In a preregistered online experiment, 300 professional translators completed 1800 tasks with access to one of thirteen LLMs with differing model training compute sizes (or a control). Our results show that model scaling substantially raises productivity: for every 10x increase in model compute, translators completed tasks 12.3% quicker, received 0.18 s.d. higher grades, and earned 16.1% more per minute (including bonus payments). Further, the gains from model scaling are much higher for lower-skilled workers who gain a 4x larger improvement in task completion speed. These results imply further frontier model scaling-which is currently estimated at 4x increase per year- may have significant economic implications.", "sections": [{"title": "1 Introduction", "content": "The amount of training compute used by frontier large language models (LLMs) increased by 5000x between the release of GPT-2 in 2019 and GPT-4 in 2023 and estimates from Epoch AI suggest a similar increase over the next six years. How does this massive increase in model training compute map onto performance? The empirical machine learning literature has derived remarkably consistent \u2018scaling laws\u2019 suggesting a strong relationship between a model's training compute and model perplexity, a measure of model loss, across more than seven orders of magnitude. But there is so far a very limited understanding of how this reduction in perplexity affects key economic and social outcomes.\nThis paper aims to offer the first experimental evidence on this question by conducting a randomized controlled trial (RCT) involving 300 professional translators conducting 1800 tasks of varying complexities. The participants were randomly assigned to either treatment groups where they could utilize one of thirteen LLMs of differing model training compute to help them complete their task or to a control group where they completed tasks without any AI assistance. Participants face high-powered incentives with significant bonus payments for high-quality tasks as evaluated by three experienced professionals in the field. The key outcome variables, therefore, were how translator's time taken, quality of tasks completed, and earnings per minute (inclusive of bonuses) varied by model training compute.\nThe results were stark. For every 10x increase in model training compute, translator's managed to complete a task 12.3% quicker (p=0.001). Converted into 'GPT-jumps', denoting the 70x difference in model training compute between successively numbered GPT models, this equates to a 22.7% decrease in time per model jump. Not only were responses completed more quickly, however, but they also received higher grades. For every 10x increase in model compute grades improved by 0.25 points on a 7-point scale or by 0.18 standard deviations (p=0.000). This means that the translator's overall productivity improvements (measured in earnings per minute, inclusive of bonuses) was even greater than the reduction in time taken alone with a 16.1% increase in earnings per 10x increase in model compute or a 29.7% increase per 'GPT-jump' (p=0.001)."}, {"title": "3 Results", "content": "Section 3.1: Productivity and Quality Impacts of any AI Usage\nAs a preliminary analysis, we study the impacts of participants receiving any Al model versus the control. All thirteen AI models of differing compute sizes are therefore pooled into a single treatment group. As depicted by Figure 1 below, the average time taken by participants to conduct a task without access to any AI model was almost exactly ten minutes (600.7 seconds). With access to any AI model this was reduced to 413.8 seconds (p=0.000), a statistically significant reduction in time of 31.1%.\nNot only were participants much quicker in completing tasks with access to AI models, however, but their tasks were also associated with higher quality scores although this result was not statistically significant (p=0.148) as shown in Figure 1. The average grade without access to an AI model was 4.51. When participants were able to use an AI model this increased to 4.71, an increase of 0.2 points on a seven-point scale. This equates to an increase of 0.14 standard deviations in the average grade. Participants therefore completed tasks both significantly quicker and with higher quality when given access to any AI model. Both of these results are reported in regression tables with and without controls as Table 1 in Appendix A.\nThese gains were not split evenly between all translators. All participants completed a baseline task without any AI assistance offering a metric for translator skill. Splitting the translator's into two groups based on whether their first task was completed in above or below the median time the impact of increased model compute differed significantly. Those with \u2018high-skill' reduced their time taken per task by 4.9% whilst those with \u2018low-skill' saw a more than 4x larger reduction of 21.1% (p=0.017). These results have potentially significant implications for the degree to which future AI improvements are skill-biased and their subsequent impacts on wage inequalities.\nThe rest of the paper is organized as follows. Below, connections to four existing academic literatures are drawn. In Section Two, an overview of the experimental design is offered alongside some summary statistics on how the translator's viewed the experimental design. This includes the degree to which they considered the tasks as similar to ones they experienced as professionals and their previous familiarity with AI tools. In Section Three, all the results are presented. Finally, in Section Four the paper concludes with discussion on the results.\nThere's already a wide range of existing economic evidence suggesting that current frontier LLMs are able to lead to significant productivity enhancements across varied tasks. For instance, Noy and Zhang (2023) found that ChatGPT can lead to productivity improvements of 37% for a range of professional writing tasks including data analysis and marketing. These are similar estimates to the double-digit productivity improvements found in code completions with GitHub Copilot (Kalliamvakou et al 2022) and legal tasks (Choi et al 2023) with GPT-4. Research using BCG consultants to perform professional tasks, however, highlights the potential for significant variation with GPT-4 yielding double-digit productivity enhancements on some common consulting tasks whilst offering no such benefits on others. These papers, alongside many others, show the impact of workers being offered an AI model or not. This paper aims to extend this literature by plotting out the trajectory of economic impacts as AI model sizes grow and automation capabilities increase.\nSecondly, the relationship between how model capabilities increase as the amount of compute used to train the model increases is best understood through 'scaling laws' such as those most prominently derived by Kaplan et"}, {"title": "4 Discussion", "content": "Previous research has already highlighted the potential for currently available LLMs to offer double-digit productivity improvements in a wide range of settings including legal tasks, consulting, software engineering, and a variety of professional tasks. Combined with forecasts from Epoch AI that frontier LLMs may use 10,000x more training compute in six years time than they currently do this experimental evidence suggests that future generations of LLMs may have significant productivity improvements. These improvements may be heterogenous with respect to ability, however, with the lowest-skilled workers reaping the greatest gains.\nThere are many limitations to this study. This paper focused on a single professional skill (translation) and only tested participants on short tasks. Further, the results were only derived on a range of just over two orders of magnitude of compute.\nWhether these economic scaling laws generalize to other domains and for greater model training compute sizes is a question for further research. For now, the evidence provided suggests future frontier LLMs may have significant economic implications."}, {"title": "6 Appendix B", "content": "The following section offers the text description of all six tasks completed by participants in the experiment.\nTask One:\nOur calculations indicate that currently proposed U.S. policies to reduce pharmaceutical prices, though particularly beneficial for low-income and elderly populations, could dramatically reduce firms' investment in highly welfare-improving R&D. The U.S. subsidizes the worldwide pharmaceutical market. One reason is U.S. prices are higher than elsewhere. If each drug had a single international price across the highest-income OECD countries, and total pharmaceutical firm profits were held fixed, then U.S. prices would fall by half and every other country's prices would increase (by 28 to 300%). International prices would maintain firms' R&D incentives and more equitably share the costs of pharmaceutical research.\nTask Two:\nOne has only to look over the hedges of eastern England to agree with those who are predicting the worst harvest in living memory. What assessment has the Secretary of State made of the impact that will have on the wider rural economy\u2014in particular, the availability and price of straw, which is vital for the livestock sector, and important commodities such as potatoes, which are likely to be under great pressure in terms of supply and price this autumn?\nTask Three:\nAt the time, college coaches handled their own endorsement deals separately from the universities they coached for, which often led to conflicts of interests between sponsors. When Jerry Claiborne became the Kentucky football head coach he signed a deal with Pepsi that included sideline rights in the stadium. Host attended a game that season with the largest Coca-Cola distributor in the state of Kentucky who noticed the Pepsi cups on the team's bench and grew confused.\nTask Four:\nWe provide evidence on the role of fairness for tax compliance: households are willing to pay more in taxes if they believe that other households are contributing their fair share. We conducted an information-disclosure natural field experiment in the context of property taxes in the United States. We induced exogenous shocks to households' perceptions about the average tax rate paid by other households. We find that a higher perceived average tax rate decreases the probability of filing a tax appeal. Translating our estimates into a money metric, we find that for each additional $1 contributed by the average household, a taxpayer is willing to pay an extra $0.43 in his or her own taxes.\nTask Five:\nWhen I write my book of windows, I want to leave all the scenes open, so the wind can blow through. At the same time, I want to explain things. I want to explain those years when Mark and I were invisible to each other. When Mark disappeared into his relationship with Olivia, and I disappeared into my work-I was a fashion model, for a boutique agency in the city-and now we never talk about it, that period of our lives\u2013 one day we started referring to Mark's relationship and my work in the past tense and that was that.\nTask Six:\nThis paper uses the responses to questions about charitable contributions from the Survey of Consumer Finances (SCF) between 1992 and 2022 to consider the rates of US households contributing money or time to charitable organizations. The fraction donating $500 or more remained relatively constant over this period, with about 47% answering they had donated in both 1991 and 2021. The fraction of households volunteering time declined consistently after 2005 from 34% to 26%."}]}