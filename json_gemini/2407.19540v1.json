{"title": "OVERCOMING UNCERTAIN INCOMPLETENESS FOR ROBUST MULTIMODAL SEQUENTIAL DIAGNOSIS PREDICTION VIA KNOWLEDGE DISTILLATION AND RANDOM DATA ERASING", "authors": ["Heejoon Koo"], "abstract": "In this paper, we present NECHO v2, a novel framework designed to enhance the predictive accuracy of multimodal sequential patient diagnoses under uncertain missing visit sequences, a common challenge in clinical settings. Firstly, we modify NECHO to handle uncertain modality representation dominance under the imperfect data. Next, we develop a systematic knowledge distillation by employing the modified NECHO as both teacher and student. It encompasses a modality-wise contrastive and hierarchical distillation, transformer representation random distillation, along with other distillations to align representations tightly and effectively. We also utilise random erasing on individual data points within sequences during both training and distillation of teacher to lightly simulate scenario with missing visit information to foster effective knowledge transfer. As a result, NECHO v2 verifies itself by showing superiority in multimodal sequential diagnosis prediction on both balanced and imbalanced incomplete settings on multimodal healthcare data.", "sections": [{"title": "1. INTRODUCTION", "content": "Predicting future patient diagnoses, a.k.a. sequential diagnosis prediction (SDP), based upon clinical records is crucial for enhancing healthcare decision-making [1-5]. Recent advances in multimodal learning, which integrate diverse modalities such as clinical notes and demographics, have significantly improved SDP accuracy [4, 5]. Nevertheless, most previous studies assume the availability of all data, which is often impractical due to privacy, equipment failures, and other uncertain factors [5]. Encountering such situation presents a significant challenge to healthcare analytics.\nMissing data, a common issue in reality, exacerbates model performance. Basic approaches, such as imputation by mean or excluding incomplete data instances, often fail to preserve true data distribution and result in information loss [6]. Advanced statistical techniques, including Multiple Imputation by Chained Equations (MICE) [7], show better efficacy, but their application in complex multimodal scenarios still remains challenging. Therefore, deep learning approaches such as reconstruction [8-10], which impute missing features, and knowledge distillation (KD) [11], which transfers teacher's knowledge on complete data to a student learning with incomplete data [12, 13], have gained popularity.\nKD has proven effective in model compression [14-16] and other applications, such as tackling missing data. Wang et al. [12] modality-specialised teachers transfer knowledge to a multimodal student. MissModal [13] employs geometric multimodal contrastive loss [17] and distribution alignment loss on combination of modality representations. However, no existing methodologies have taken into account the fixed dominance of specific modalities under complete data and the fluctuating modalities importance under incomplete data, leading to sub-optimal performance. Furthermore, there is a research gap in systematically leveraging KD to alleviate the representation discrepancy in teacher-student in missing data contexts.\nMeanwhile, data augmentation (DA) is widely used to enhance model performance [18]. Some studies have explored its impact on KD [19]. However, there is still lack of research on effectively applying DA in the presence of incomplete data.\nTo this end, we propose NECHO v2, not only overcoming the challenges in multimodal SDP with imperfection for the first time but also tackling the aforementioned limitations. First, we modify the original NECHO to manage uncertain modality representation dominance in the presence of missing data. Second, we establish a systematic KD pipeline, including modality-wise contrastive and hierarchical distillation, followed by transformer random representation distillation, penultimate layer distillation, and dual-level logit distillation, to fully transfer teacher's semantic knowledge from perfect data. We also employ a random data point erasing on visit sequences to simulate missing visit to reduce the data distributional gap and facilitate representation transfer. By doing so, NECHO v2 demonstrates robust superiority under both balanced and imbalanced imperfect data scenarios on MIMIC-III data [20]."}, {"title": "2. METHODOLOGIES", "content": "2.1. Problem Statement\nMultimodal EHR Data A clinical record is a time-ordered sequence of visits V1, ..., VT, where T represents the total number of visits for any given patient P. Each visit Ve at the t-th admission consists of three components: Ht, demographics; Nt, a clinical note; and Dt, a set of diagnosis codes. Specifically, a medical ontology G is utilised to structure diagnosis codes into three hierarchical levels: unique medical codes, category codes, and disease-typing codes, from leaf to node. The input and output is unique medical codes and category codes, respectively.\nMissing Visit Sequences To simulate real-world missingness, we randomly discard aforementioned components in each visit sequence, creating an m-modal dataset with 2m missing patterns. Missing probabilities are balanced or imbalanced across modalities and kept the same during both training and inference phases.\nSequential Diagnosis Prediction Given a patient's multimodal clinical records for the past T visits under incompleteness, the objective is to predict diagnoses codes that will appear in the (T + 1)-th visit."}, {"title": "2.2. NECHO v2", "content": "In this section, we present the KD-based NECHO v2 framework. For a comprehensive flow detailing the process from input to prediction, refer to the original NECHO [5].\n2.2.1. Modification of NECHO\nNECHO [5] achieves state-of-the-art performance in SDP by integrating demographics, clinical notes, and medical codes using a medical code-centric framework with bi-modal contrastive loss and a centric multimodal adaptation gate (CMAG) for alignment and fusion. Each modality-specific encoder predicts at the parental level of target medical codes (disease-typing codes) to enhance training.\nHowever, it confronts two issues: 1) under-performance in incomplete data despite outstanding performance in complete data and 2) adoption of a pre-trained BioWord2Vec [21], limiting performance on non-MIMIC-III datasets [20]. To mitigate these concerns, we modify by: 1) replacing the demo \u2192 code cross-modal transformer (CMT) with a demo \u2192 note CMT to relieve bias from medical codes and 2) utilisation of clinical TinyBERT [22] as a text encoder to potentially facilitate adaptability to other datasets.\n2.2.2. Systematic Knowledge Distillation Framework\nTeacher-Student Network Configuration In our KD pipeline, we adopt the modified NECHO as both teacher and student. In addition, the teacher leverages CMAG [5] to consider modality representation dominance when learning with the full data, while the student adopts MAG [23] that adjusts significant representations flexibly, considering fluctuating dominant feature under missing conditions. We avoid using the original NECHO as the teacher to reduce architectural heterogeneity, thereby fostering the KD [24]. We adopt offline distillation [11] where the teacher is trained, then frozen during distillation. Additionally, the teacher is absent during student's inference.\nModality-wise Contrastive and Hierarchical Distillation We begin our KD process by distilling modality-wise representations from the teacher to the student, using contrastive learning [25] and L2 distance measures (Mean Squared Error, MSE). First, contrastive learning identifies and amplifies both similarities and discrepancies between the representations [17, 25]. When utilised in KD, it encourages the student's representations to be similar to those of the teacher's for corresponding samples, while also distinguishing between different samples. MSE further tightens this alignment, reducing deviations and promoting consistency.\nUnlike previous method [13], we explicitly distill modality-specific semantic distributions. We utilise a contrastive loss [25] with symmetrical losses to promote stable and effective distillation in a modality-wise fashion. Let teacher and student representations with the same data as positive sample pairs (Hm, , Gm, ), with m\u2208 (H, N, D), and teacher and student are H and G, respectively. Then, modality-wise contrastive distillation $\\mathcal{L}_{MWCD}$ is:\n$\\mathcal{L}_{CH\\rightarrow G,m}^{LMWCD} = -log \\frac{exp((H_{m,,}, \\hat{G}_{m,,})/\\tau)}{\\sum_{k=1}^{N} exp((H_{m,,}, \\hat{G}_{m,k})/\\tau)}$ (1)\n$\\mathcal{L}_{CWCH,m}^{LMWCD} = -log \\frac{exp((\\hat{G}_{m,,}, H_{m,,})/\\tau)}{\\sum_{k=1}^{N} exp((\\hat{G}_{m,,}, \\hat{H}_{m,k})/\\tau)}$ (2)\n$\\mathcal{L}_{MWCD} = \\sum_{m\\in{H,N,D}} {\\alpha \\mathcal{L}_{CH\\rightarrow G,m}^{LMWCD} + (1 - \\alpha)\\mathcal{L}_{CWCH,m}^{LMWCD}}$ (3)\nwhere (, ) denotes cosine similarity and the temperature $\\tau \\in \\mathbb{R}^+$ is a parameter that controls the distribution concentration and the gradient of the Softmax function. Next, with $\\mathbb{H}_{tm}$ and $\\hat{G}_{tm}$, modality-specific teacher and student feature at t-th visit, modality-wise hierarchical distillation $\\mathcal{L}_{MWHD}$ Via MSE $\\left | \\left | \\right | \\right |_2$ is:\n$\\mathcal{L}_{MWHD} = \\sum_{m\\in{H,N,D}} \\left | \\left | H_{tm} - \\hat{G}_{tm} \\right | \\right |_2^2$. (4)\nAccordingly, the modality-wise contrastive and hierarchical distillation $\\mathcal{L}_{MWD}$ is formulated as the sum of the above two loss terms:\n$\\mathcal{L}_{MWD} = \\mathcal{L}_{MWCD} + \\mathcal{L}_{MWHD}$. (5)\nTransformer Representation Random Distillation Previous research have explored intermediate layer distillation (ILD) between transformer layers for compression [15, 22, 26]. Meanwhile, NECHO has cross-modal (CMT) and self-attention transformer encoders (SAT) to align and merge inter- and intra-modality representations. Considering its multiple transformers, layer-wise distillation is computationally expensive. Hence, we distill teacher's randomly selected final transformer features, reducing computational burden and avoiding overfitting.\nDenote representations from two CMTs as $\\mathbb{R}^{M,H\\rightarrow N}$ and $\\mathbb{R}^{M,D\\rightarrow N}$, and those from three SATs are $\\mathbb{S}^{M,H\\rightarrow N}$, $\\mathbb{S}^{M,D\\rightarrow N}$ and $\\mathbb{S}^{M,D}$, where M is either teacher or student. Then, for the randomly selected transformer representations, the proposed distillation $\\mathcal{L}_{TR2D}$ using MSE for both CMT ($\\mathcal{L}_{CMTD}$) and SAT ($\\mathcal{L}_{SATD}$) are:\n$\\mathcal{L}_{TR2D} = \\mathcal{L}_{CMTD} + \\mathcal{L}_{SATD}$, (6)\nwhere $\\mathcal{L}_{CMTD} = \\sum_{m\\in{H,D}} \\left | \\left | \\mathbb{R}^{teacher, m\\rightarrow N} - \\mathbb{R}^{student, m\\rightarrow N} \\right | \\right |_2$, (7)\n$\\mathcal{L}_{SATD} = \\sum_{m\\in{H,D}} \\left | \\left | \\mathbb{S}^{teacher, m\\rightarrow N} - \\mathbb{S}^{student, m\\rightarrow N} \\right | \\right |_2 + \\left | \\left | \\mathbb{S}^{teacher, D} - \\mathbb{S}^{student, D} \\right | \\right |_2$. (8)\nMAG Distillation To ensure the student model further mimics the teacher, we introduce MAG (penultimate layer) distillation. Its importance is also highlighted due to its rich, informative features [27]. MAG representations from teacher and student be $MAG^{teacher}$ and $MAG^{student}$, respectively. Then, the regarding loss is:\n$\\mathcal{L}_{MAGD} = \\left | \\left | MAG^{teacher} - MAG^{student} \\right | \\right |_2$. (9)\nDual Logit Distillation NECHO predicts target codes, as well as parental-level codes (disease typing codes) at the modality-specific encoders. Accordingly, we transfer both teacher predictions to the corresponding student predictions. Previous work [16] argues that MSE outperforms Kullback-Leibler (KL) divergence for logit distillation, without requiring hyper-parameter tuning. Hence, MSE is applied to both distillations.\nThe final prediction and modality-specific parental-level prediction are $\\hat{y}_{t+1}^M$ and $\\hat{O}_{t+1}^{ M,m}$. Then, the dual logit distillation loss $\\mathcal{L}_{DualLD}$ is written as:\n$\\mathcal{L}_{DualLD} = \\mathcal{L}_{LLD} + \\mathcal{L}_{hrchyLD}$, (10)\nwhere $\\mathcal{L}_{LLD} = \\left | \\left | \\hat{y}_{teacher}^M - \\hat{y}_{student}^M \\right | \\right |^2$, (11)\n$\\mathcal{L}_{hrchyLD} = \\sum_{m\\in{H,N,D}} \\left | \\left | \\hat{O}_{teacher, m} - \\hat{O}_{student, m} \\right | \\right |^2$ (12)\nwhere $\\mathcal{L}_{LLD}$ and $\\mathcal{L}_{hrchyLD}$ are final logit distillation and modality-specific hierarchical logit distillation, respectively.\nModel Optimisation The student model is also optimised using a pair of task loss $\\mathcal{L}_{DualCE}$ (CE stands for Cross Entropy), which consists of two components: one for the target level $\\mathcal{L}_{CE}$ and another for the parental level $\\mathcal{L}_{hrchyCE}$, in accordance with NECHO.\nBy integrating the task losses with the above distillation losses with the corresponding constant $\\lambda$, the full optimisation objective is formulated as:\n$\\mathcal{L}_{total} = \\lambda_{MWD} \\mathcal{L}_{MWD} + \\lambda_{TR2D} \\mathcal{L}_{TR2D} +\\lambda_{MAGD} \\mathcal{L}_{MAGD} + \\lambda_{DualLDL} \\mathcal{L}_{DualLD} + \\lambda_{DualCE} \\mathcal{L}_{DualCE}$. (13)\n2.2.3. Single Data Point Erasing Guided Curriculum Learning\nIt is desirable to acquire a teacher that transfers better representations to a student learning with uncertain missing data. Prior studies show that large discrepancies in data distribution between teacher and student can hinder KD [19]. Therefore, we propose random single-point data erasing [28] to both training and distillation of the teacher, a minimalistic approach to mimic missing sequences and alleviate the data distribution gap, thereby further improving KD and the performance of resultant student. Note that, DA is not applied to the student during distillation.\nFirstly, the teacher is trained using data erasing guided curriculum learning [29], starting with easier samples and gradually progressing to more difficult ones. Each modality is assigned a missing probability of 0.0 or 0.1 with equal probability until specific epoch e1, after which the probability of 0.2 is added. Next, during the distillation, the unified teacher trained in the previous manner, complete data representations are transferred until epoch e2, after which training continues with either no missing data or a 0.1 missing ratio."}, {"title": "3. EXPERIMENTS", "content": "3.1. Experimental Setup\nDataset and Pre-processing We evaluate on MIMIC-III data [20], following the pre-processing steps from previous works [2, 5] but with a more rigorous patient selection criteria by removing records 1) with a length of stay of zero or negative and 2) who died within 30 days post-discharge. We also leverage only discharge summaries for clinical notes. Detailed statistics upon pre-processing are in Table 1.\nTo handle missing data, we assign a value beyond the existing range for erased or missing points in demographics and codes. For instance, if the total number of medical codes is 3882, the missing value is assigned as 3883. We also replace missing tokens in notes with the UNK token.\nTraining and Evaluation Details We mostly follow the implementation details from previous study [5]. We set the hidden dimension to 128 and the dropout rate to 0.1. The transformer encoders have 4 heads and 3 layers. We set the temperature $\\tau$ to 0.1 and the alpha $\\alpha$ to 0.25 for the contrastive distillation. The coefficients for loss terms are set to 1, except for the $\\mathcal{L}_{hrchyCE}$ which is 0.1.\nOptimisation is performed via AdamW [30], with a constant learning rate of 2e-5 for the parameters of clinical TinyBERT and 1e-4 for all other parameters. We train with a batch size of 4 for up to 100 epochs, stopping early if no improvement in validation set for 5 consecutive epochs. For curriculum learning, e1 and e2 are set to 5 and 10, respectively.\nNECHO v2 is evaluated against joint learning methods (MulT [23] and three NECHO variations: original, teacher, and student [5]) and KD methods (UnimodalKD [12] and MissModal [13]). KD methods use the same teacher and student for fair comparison. Evaluation uses top-k accuracy with k values of 10 and 20, following [1, 3, 5]. Experiments are implemented using PyTorch [31] and trained on a single NVIDIA RTX A6000."}, {"title": "3.2. Experimental Results", "content": "3.2.1. Main Results\nAs shown in Table 2, NECHO v2 demonstrates remarkable performance across various missing scenarios on MIMIC-III dataset. Specifically, it outperforms the original NECHO by 1.52%, its teacher by 3.32%, its student by 1.45%, and UnimodalKD by 1.83% in top-10 accuracy at the balanced missingness of 0.5. Similar trends are observed in other settings. On the other hand, NECHO performs well when medical codes are mostly present (0.2) but predicts poorly in scenarios where codes are highly missing (0.8). This highlights the robust and superior performance of NECHO v2.\nThis performance gain is attributed to: 1) modifying NECHO for both teacher and student to address varying modality significance, 2) implementing systematic KD, including modality-wise contrastive and hierarchical distillation and transformer representation random distillation and others, to comprehensively mimic teacher at various representation levels, and 3) simulating random visit information by single point data erasing to minimise data distribution gaps and improve KD. These enables student to imitate the teacher in varied incompleteness, ensuring robust performance effectively.\n3.2.2. Ablation Studies\nWe first assess the effectiveness of KD. While $\\mathcal{L}_{MWCD}$ occasionally performs better without it, its consistent use generally enhances performance. The absence of $\\mathcal{L}_{TR2D}$ and $\\mathcal{L}_{MAGD}$ during distillation significantly deteriorates the performance, highlighting the importance of intermediate supervision. Additionally, $\\mathcal{L}_{hrchyLD}$ is beneficial, all of which emphasise systematic KD's importance to align student's semantic knowledge to that of the teacher. We also evaluate the efficacy of DA in three scenarios: only distillation, only teacher training, and not at all. Overall performance significantly improves, highlighting the significance of the proposed random single data point erasing under missing visit information.\n3.2.3. Comparative Studies\nAs shown in Table 4, we compare NECHO v2 with various combinations of teacher and student (original to original, original to modified, adopted MAG) and transformer full distillation under the same settings with the ablation studies. Our proposed methodologies achieves the best overall performance, highlighting the importance of: 1) carefully pairing teacher and student to address varying representation dominance and minimise architectural heterogeneity, and 2) introducing randomness into KD to prevent overfitting and foster effective knowledge transfer."}, {"title": "4. CONCLUSION", "content": "We tackle uncertain missing sequences for robust multimodal SDP with NECHO v2. Firstly, we modify NECHO to dynamically adjust dominant representations under varying missingness. Next, we design a systematic KD pipeline, adopting the modified NECHO and leveraging multiple distillations including modality-wise contrastive and hierarchical distillation, to comprehensively learn teacher representations from complete data. We also randomly erase single data points per visit to reduce the data distribution discrepancy and enhance KD process. Extensive experiments show the efficacy of NECHO v2 over the existing methodologies. The code will be released upon publication."}]}