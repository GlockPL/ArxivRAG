{"title": "Scoring with ChatGPT", "authors": ["Mark D. Shermis"], "abstract": "This study aimed to determine if ChatGPT's large language models could match the scoring accuracy of human and machine scores from the ASAP competition. The investigation focused on various prediction models, including linear regression, random forest, gradient boost, and boost. ChatGPT's performance was evaluated against human raters using quadratic weighted kappa (QWK) metrics. Results indicated that while ChatGPT's gradient boost model achieved QWKs close to human raters for some data sets, its overall performance was inconsistent and often lower than human scores. The study highlighted the need for further refinement, particularly in handling biases and ensuring scoring fairness. Despite these challenges, ChatGPT demonstrated potential for scoring efficiency, especially with domain-specific fine-tuning. The study concludes that ChatGPT can complement human scoring but requires additional development to be reliable for high-stakes assessments. Future research should improve model accuracy, address ethical considerations, and explore hybrid models combining ChatGPT with empirical methods.", "sections": [{"title": "Introduction", "content": "Twelve years ago, the Hewlett Foundation sponsored a competition (Automated Student Assessment Prize; ASAP\u2014also referred to as the Hewlett Trials) that demonstrated the capabilities of automated essay scoring (both essays and short-form constructed responses) in an open, transparent, and replicable platform (Shermis & Hamner, 2013). The competition, which had one prize of $100,000 and another for $90,000, was divided into three parts: (1) an anonymous vendor demonstration consisting of nine commercial vendors and one university participant who scored responses from eight high-stakes essays across six different U.S. states and three grade-levels (no prize money), an open competition that consisted of 159 data-science teams from around the world who created models and scored an anonymized version of the same essays, and an open competition that consisted of 189 teams who focused on responses from ten short answer questions/prompts. All three components were administered on the Kaggle platform (http://kaggle.com), a web-based platform for data science competitions. The competition aimed to explore whether or not high-stakes essays and short-form constructed responses processed through machine scoring algorithms predicted scores similar to those assigned by human raters. At the time, the U. S. Congress had funded assessment efforts in support of The Common Core State Standards Initiative, an effort to upgrade the K-12 curriculum in the U.S. through the use of \"deeper learning\" (referred to as \"Race-to-the-Top\"). The concern was that many of the assessments would require a substantial amount of student writing to meet their objectives. Grading this material would present considerable work and cost challenges. These assessments were led by two assessment consortia, Smarter Balanced and the Partnership for Assessment of Readiness for College and Careers (PARCC), who recruited the"}, {"title": "Method", "content": "Participants\nStudy 1\nThe publicly available ASAP essays were used in this study to assess essays and short-form constructed responses. Student essays (N = 22,029) were collected for eight different prompts representing six PARCC and SBAC) states (three PARCC states and three SBAC states) that were part of the two Race-to-the-Top assessment consortia (2015) during the 2010-2011 school year. The two consortia made the initial contact with the participating states, and ASAP began a series of negotiations that screened for their availability for participation, the type of essay, grade levels, the presence of multiple ratings, and the likelihood that the essays could be processed and validated in time for the launch of the vendor demonstration. Many states do not employ writing as part of their high-stakes assessment programs, and it obviously could not be included in the sample. The states that were chosen and cooperated had the endorsement of the consortia and asked that they not be identified in the report. In formulating the sample, an attempt was made to ensure a range of ethnicities and gender representation of students in the sample. However, these were not targeted to be representative samples for any particular population of students. Efforts were also undertaken to ensure that the types of prompts represented a range of typical writing tasks through prompts that might be similar to those anticipated in the new assessments. Responses represented a purposely heterogeneous mix in length of response. The sample was composed of essays from volunteer states and, therefore, cannot be assumed to be a genuinely representative sample of state practice. Senior representatives from both consortia indicated that the participating states were appropriate representatives of the other states in the respective consortia. However, due to the sample"}, {"title": "Models", "content": "The models used in this study are commonly employed for predictions based on empirical data. These include linear regression, random forest, gradient boost, and xgboost. Others could have been investigated (i.e., support vector machines, neural networks, etc.) and may be utilized in the future.\nLinear Regression\nA linear regression model is a simple and commonly used statistical technique for modeling the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship, represented by the equation $Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon$, where Y is the dependent variable, X1, X2, ..., Xn are the independent variables, $\\beta_0$ is the intercept, $\\beta_1$, $\\beta_2$, ..., $\\beta_n$ are the coefficients representing the weights of the independent variables, and $\\epsilon$ is the error term. The goal is to find the best-fitting line through the data points that minimizes the sum of the squared differences between observed and predicted values (Draper & Smith, 1998; Montgomery et al., 2021; Seber & Lee, 2012)."}, {"title": "Results", "content": "Study 1\nTable 7 shows the quadratic weighted kappa results for ChatGPT's scoring of essays for all four prediction models. For five of the nine data sets, QWKs were lower for the ChatGPT models than for human raters, sometimes appreciably. For example, the H1H2 agreement for data set 2a was .80, but the highest prediction model for ChatGPT was only .63. However, on four of the data sets, ChatGPT met or exceeded the QWKs for human raters. On data set five, the H1H2 QWK was 0.74, but the gradient boost model for ChatGPT achieved a QWK of .80. Except for data set 5, the linear regression models did not work particularly well. The random forest models performed slightly better. The model that worked the best overall was the gradient boost model, followed by xgboost. Xgboost might have worked better, but ChatGPT had difficulty converging on prediction solutions with the standard resources allocated to creating the models. The models eventually produced predictions when parameters or resources were reduced (e.g., from 100 estimators to 50 or 25; smaller training sample sizes, etc.). Figure 1 illustrates the performance of human raters and all four ChatGPT prediction models across the eight essay data sets."}, {"title": "Discussion", "content": "Most assessment procedures aim to generate a valid score indicating current ability or performance level or generating feedback to improve performance. In the realm of writing assessment, the assessment enterprise can be expensive if it requires empirical models for the automated scoring of essays or short-form constructed responses since these currently incorporate relatively large samples upon which the scoring models are based. With the introduction of large language models, researchers have been exploring ways to employ artificial intelligence as a substitute for empirical models with varying degrees of success. For example, Xiao et al. (2024) utilized GPT-4 and GPT-3.5-turbo for scoring essays with various approaches, including zero-shot (i.e., one example), few-shot, and fine-tuning methods. The QWK scores for different configurations ranged from 0.67 to 0.80, indicating good agreement with human raters. The study highlighted the effectiveness of prompt engineering and retrieval-based approaches to enhance scoring accuracy, and these results are likely good enough for some types of formative feedback (the topic of that study). Yoon (2023) developed an automated short answer grading (ASAG) model that provided analytic and final holistic scores. The study used one-shot prompting and a text similarity scoring model with domain adaptation using small manually annotated datasets with large language models to develop predictions. Four short-answer questions were drawn from the ASAP data files (data sets 1, 2, 5, & 6) to test out the model. Quadratic weighted kappa ranged from 0.67 to 0.71, which was lower than that obtained in the second part of the present study. The advantage of this approach was that significantly less training data was involved in creating a model."}]}