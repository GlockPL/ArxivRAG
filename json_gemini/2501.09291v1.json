{"title": "LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport", "authors": ["Kyeongha Rho", "Hyeongkeun Lee", "Valentio Iverson", "Joon Son Chung"], "abstract": "Automated audio captioning is a task that generates textual descriptions for audio content, and recent studies have explored using visual information to enhance captioning quality. However, current methods often fail to effectively fuse audio and visual data, missing important semantic cues from each modality. To address this, we introduce LAVCap, a large language model (LLM)-based audio-visual captioning framework that effectively integrates visual information with audio to improve audio captioning performance. LAVCap employs an optimal transport-based alignment loss to bridge the modality gap between audio and visual features, enabling more effective semantic extraction. Additionally, we propose an optimal transport attention module that enhances audio-visual fusion using an optimal transport assignment map. Combined with the optimal training strategy, experimental results demonstrate that each component of our framework is effective. LAVCap outperforms existing state-of-the-art methods on the AudioCaps dataset, without relying on large datasets or post-processing. Code is available at https://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap.", "sections": [{"title": "I. INTRODUCTION", "content": "Automated audio captioning (AAC) is a task that aims to generate textual descriptions of a given audio input. In contrast to audio tagging and sound event classification tasks that focus on understanding low-level sound characteristics, AAC also requires a comprehension of high-level semantics contained in the audio. It has recently garnered significant attention from the deep learning community due to its wide range of applications, such as providing audio descriptions for broadcasts and movies, text query-based audio retrieval, and developing more human-like conversational AI systems [1].\nMost recent works [2]\u2013[9] for AAC adopt the encoder-decoder framework and have explored different architectures. They utilize various audio encoders [10]\u2013[14] to extract semantically rich acoustic features. Pre-trained language models are used as text decoders because of their sequence modeling and text generation capabilities. In particular, some recent works [4], [8] leverage LLMs for their strong ability to understand contexts and generate text sequences.\nRecently, several studies [15], [16] have incorporated visual modality to improve audio captioning. Visual information helps distinguish sounds in complex scenes. For example, when a man speaks while drilling, visual cues clarify the presence of both elements, allowing for generating more accurate captions.\nTo this end, we introduce LAVCap, an LLM-based Audio-Visual Captioning framework. This work focuses on exploring various audio-visual fusion methods and identifying the optimal training strategy for the LLM-based audio-visual captioning framework. First, we find out that naively combining visual features with audio features does not enable the LLM to take full advantage of the visual information. We attribute this shortfall to the significant modality gap between audio and visual feature spaces.\nTo bridge the modality gap, we consider the alignment of audio and visual tokens as an optimal transport (OT) problem. While previous work [17] applies OT algorithms to align video and text, this study pioneers their application to audio-visual alignment. Specifically, we introduce an optimal transport-based alignment loss (OT loss) tailored for this purpose.\nOT loss encourages the encoders to extract features that are rich in semantics while ensuring that they are well-aligned. In addition, our experiment reveals that the existing cross-attention mechanism [15] struggles to integrate cross-modal features effectively. To tackle this, we propose an optimal transport attention module (OT-Att) that leverages the OT assignment map as attention weights for audio-visual fusion. This approach provides more effective fusion compared to other audio-visual fusion methods.\nExperimental results demonstrate that LAVCap outperforms previous state-of-the-art methods on the AudioCaps benchmark, without pre-training the model on large datasets or applying post-processing to generated captions. It is noteworthy that our approach achieves such high performance although the ground-truth captions in AudioCaps are audio-centric. Furthermore, our user study shows that LAVCap obtains mean opinion scores (MOS) even higher than the ground-truth captions. This underscores the importance of utilizing the visual modality to distinguish the various sounds and understand the scene.\nThe main contributions of this work are summarized as follows:\n\u2022 We present LAVCap, an LLM-based audio-visual captioning framework that leverages visual information to complement audio modality in audio captioning tasks.\n\u2022 We employ optimal transport for effectively bridging the modality gap and fusing audio-visual features.\n\u2022 Our approach outperforms existing state-of-the-art methods on AudioCaps without the need for pre-training the model on large datasets or applying post-processing."}, {"title": "II. METHOD", "content": "The overall architecture of LAVCap is illustrated in Fig. 1. Each component of our framework and the training objectives are explained in detail in the following sections."}, {"title": "A. Audio-Visual Encoding", "content": "Given an audio-visual input pair (xa, xv), the audio encoder Ea and the visual encoder Ev encode them into modality-specific features with C dimensions as follows:\n$h_a = E_a(x_a), h_v = E_v(x_v)$ (1)\nwhere $h_a \u2208 \\mathbb{R}^{Na\u00d7C}$, $h_v \u2208 \\mathbb{R}^{Nv\u00d7C}$ represent the audio and visual features, respectively, with Na and Nv denoting the number of tokens for audio-visual features."}, {"title": "B. Audio-Visual Alignment Based on Optimal Transport", "content": "To effectively bridge the modality gap, we apply OT loss to align the audio-visual features. First of all, we compute the similarity matrix $S \u2208 \\mathbb{R}^{Na\u00d7Nv}$ of audio and visual features. Similar to the prior work [17], we optimize the audio-visual assignment map $Q \u2208 \\mathbb{R}^{Na\u00d7Nv}$ to maximize the global similarity between the cross-modal features:\n$\\max_{Q \\in \\mathcal{Q}} tr(Q^T S) + \\epsilon H(Q)$\ns.t. $Q = \\{Q | Q1_{N_v} = \\frac{1}{N_a} 1_{N_a}, Q1_{N_a} = \\frac{1}{N_v}1_{N_v}\\}$ (2)\nwhere $1_{Na}, 1_{Nv}$ denotes the vector of ones in dimension Na and Nv. $H(Q)$ is an entropy regularization term with $\\epsilon$ controlling the smoothness. Then the optimal solution Q* is obtained by iterative Sinkhorn-Knopp algorithm [18]:\n$Q^* = Diag(\\mu)exp(S/\\epsilon)Diag(\\nu)   (\\mu \\in \\mathbb{R}^{N_a}, \\nu \\in \\mathbb{R}^{N_v})$ (3)\nwhere \u03bc and \u03bd are the non-negative scaling vectors.\nBased on the optimal solution Q* and the similarity matrix S, OT loss is computed as follows:\n$L_{OT} = \\sum_{m \\in \\{a,v\\}} \\frac{1}{N_m} \\sum_{i=1}^{N_m} -\\log \\frac{exp((q^m_i, s^m_i)/\\tau)}{\\sum_{j=1}^{N_m} exp((q^m_j, s^m_j)/\\tau)}$ (4)\nwhere $\\{q^m_i\\}^N_{i=1}$ and $\\{s^m_i\\}^N_{i=1}$ denote the row vectors of Q and S when m = a, and the column vectors when m = v. OT loss encourages maximizing the similarity between audio-visual token sequences within a sample, rather than between averaged features in a mini-batch. This provides more fine-grained supervision and thus a more effective way to align the cross-modal features."}, {"title": "C. Audio-Visual Fusion and Projection", "content": "In addition to using the optimal transport assignment map for loss computation, we also employ it for the fusion of audio-visual features. We refer to this fusion module as OT-Att module in Eq. 5). OT-Att module operates similarly to the conventional cross-attention but utilizes the optimal transport assignment map as the attention weight. The visually-attended audio feature and audio-attended visual feature are computed as follows:\n$\\hat{h}_a = OT-Att(h_a, Q^*, h_v) = h_a + Q^*h_v$\n$\\hat{h}_v = OT-Att(h_v, Q^{*T}, h_a) = h_v + Q^{*T}h_a$ (5)\nThen these two features are concatenated token-wise and then projected to the LLM latent space through the linear projector:\n$h_{av} = W_{av\u2192t} Concat(\\hat{h}_a, \\hat{h}_v)$ (6)"}, {"title": "D. Text Decoding", "content": "Since we leverage an LLM as the text decoder, the text instruction prompt $x_t$ needs to be transformed into text embeddings through the LLM tokenizer $E_t$. Then the fused audio-visual features and text embeddings are concatenated and fed into the LLM decoder $D_t$ to produce the output $z_t$:\n$z_t = D_t (Concat(h_{av}, E_t(x_t)), M_{att})$ (7)\nwhere $M_{att}$ denotes attention masks for reflecting the auto-regressive property of LLM."}, {"title": "E. Training Objectives", "content": "As well as OT loss, the conventional autoregressive cross-entropy loss is also used for training:\n$L_{CE} = -\\frac{1}{T} \\sum_{i=1}^{T} \\log p(y_i | y_{1:i-1}, h_{av}, h_t)$ (8)\nwhere $y_i$ is a i-th text token. The final training objective is the weighted sum of two losses:\n$L = \\lambda_{CE}L_{CE} + \\lambda_{OT} L_{OT}$ (9)"}, {"title": "III. EXPERIMENTS", "content": "A. Experimental settings\n1) Datasets: We utilize the AudioCaps dataset [19] for training and evaluation, where each 10-second clip is annotated based on its audio component. Due to the limited accessibility of some YouTube links, we acquired 48,595 clips for the training and 944 clips for the test set. For audio pre-processing, we applied Short-Time Fourier Transform using a 25-ms Hanning window with a 10-ms hop size to each 10-second waveform sampled at 16 kHz, resulting in a 1024 \u00d7 64 spectrogram. For visual input, 20 frames are uniformly selected from a 10-second clip at 2 FPS, then center-cropped to 224 x 224 pixels and normalized.\n2) Implementation Details and Metrics: During training, we utilize the AdamW optimizer with $\u03b2_1$ = 0.9, $\u03b2_2$ = 0.999, and a weight decay of le-6. For the first two epochs, out of a total of 100, the learning rate warms up to 5e-6, and then it gradually decreases following a cosine annealing strategy. We adopt a pre-trained Consistent Ensemble Distillation model [20] as an audio encoder, and a pre-trained CLIP ViT-L/14 model [21] as a visual encoder. For text decoding, Llama 2 [22] with 7B parameters is employed. Both the audio encoder and text decoder are fine-tuned using low-rank adaptation (LoRA) [23], while the visual encoder is kept frozen. When evaluating our methods, we use the metrics commonly employed for AAC, including BLEU [24], ROUGE-L [25], METEOR [26], CIDEr [27], SPICE [28], and SPIDEr [29]. Experiments are conducted using Intel Gaudi 2 AI Accelerator."}, {"title": "B. Main Results", "content": "The audio captioning performance of LAVCap on the AudioCaps dataset is shown in Table I. LAVCap not only outperforms previous works in closely matching the lexical content of ground truth captions but also shows enhanced semantic relevance and informativeness. It is impressive that our framework achieves high performance although the ground-truth captions in AudioCaps are annotated based solely on audio. Notably, our model still achieves comparable performance to the concurrent works [8], [9] that utilize multiple and additional datasets for pre-training. The results demonstrate that using an OT mapping between audio and visual modalities enables the model to train more effectively on semantically aligned features across these modalities, compared to previous methods such as cross-attention and concatenation. A further analysis of the results can be found in Section III-C."}, {"title": "C. Ablation Studies", "content": "1) Visual Modality and OT Loss: We conduct an ablation study to evaluate the effectiveness of leveraging visual modality and OT loss. As shown in Table. II, the performance improvement from simply adding visual modality is marginal without OT loss. This demonstrates that bridging the modality gap through OT loss is crucial for enabling LLM to comprehend multi-modal contexts and process audio-visual features.\n2) Audio-Visual Fusion Methods: Based on the utilization of visual modality and OT loss, we explore various audio-visual fusion methods. The results in Table. III show that the cross-modal fusion methods employed by the previous works are not effective in our setting. We infer that this is due to the lack of training data to optimize the learnable parameters of these fusion methods and simultaneously align the fused audio-visual features to the LLM latent space. On the other hand, the proposed OT attention module does not need any learnable parameters and leverages an optimal transport assignment map as an attention weight, thus providing effective audio-visual fusion in a data-efficient way.\n3) Encoder-Decoder Training Strategy: After deciding on the optimal network architecture, we seek the best training strategy for our framework. As shown in Table. IV, training the encoder with LoRA instead of fine-tuning the entire parameters is much more efficient under data-limited circumstances. In addition, freezing the weights of the LLM decoder severely degrades the performance, indicating that the LLM decoder should be adapted to the target dataset.\n4) Instruction Prompts: To ensure that the LLM text decoder understands the input tokens properly, we explore various instruction prompts detailed in Table V. While prompts I and II handle audio and visual tokens together, prompt III is specifically designed to understand them separately. Since AAC focuses primarily on audio-based captioning, prompt I performs slightly better than the others, as shown in Table VI."}, {"title": "D. Qualitative Results", "content": "In Fig. 2 we visualize the captions generated by models trained with audio-only, visual-only, and combined audio-visual data. While using a single modality may capture incorrect elements, utilizing both audio and visual inputs provides a more detailed view of the video. Furthermore, we conduct a user study with 20 participants, using MOS to rate each of the 35 generated captions from 1 to 5 based on how well they describe the video. The results in Table VII show that the captions generated by LAVCap concretely represent the audio content. Remarkably, MOS of LAVCap is even higher than the ground-truth captions. This highlights the significant benefit of incorporating the visual modality to better distinguish the various sounds and comprehend the scene context."}, {"title": "IV. CONCLUSION", "content": "In this work, we propose LAVCap, an LLM-based audio-visual captioning framework designed to incorporate visual information into the audio modality using an optimal transport-based strategy. Specifically, OT loss and OT-Att are introduced to align the modality gap and promote effective fusion of audio and visual features. The proposed model outperforms previous captioning methods on the AudioCaps dataset, without the need for pre-training on large datasets or post-processing, highlighting its promise in designing new audio-visual fusion methods."}]}