{"title": "VLM2VEC: TRAINING VISION-LANGUAGE MODELS\nFOR MASSIVE MULTIMODAL EMBEDDING TASKS", "authors": ["Ziyan Jiang", "Rui Meng", "Xinyi Yang", "Semih Yavuz", "Yingbo Zhou", "Wenhu Chen"], "abstract": "Embedding models have been crucial in enabling various downstream tasks such\nas semantic similarity, information retrieval, and clustering. Recently, there has\nbeen a surge of interest in developing universal text embedding models that can\ngeneralize across tasks (e.g., MTEB). However, progress in learning universal\nmultimodal embedding models has been relatively slow despite their importance.\nIn this work, we aim to explore the potential for building universal embeddings\ncapable of handling a wide range of downstream tasks. Our contributions are\ntwofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers\n4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval,\nand visual grounding) and 36 datasets, including 20 training and 16 evaluation\ndatasets, and (2) VLM2VEC (Vision-Language Model \u2192 Vector), a contrastive\ntraining framework that converts any state-of-the-art vision-language model into\nan embedding model via training on MMEB. Unlike previous models such as\nCLIP and BLIP, VLM2VEC can process any combination of images and text to\ngenerate a fixed-dimensional vector based on task instructions. We build a series\nof VLM2VEC models on Phi-3.5-V and evaluate them on MMEB's evaluation split.\nOur results show that VLM2VEC achieves an absolute average improvement of\n10% to 20% over existing multimodal embedding models on both in-distribution\nand out-of-distribution datasets in MMEB.", "sections": [{"title": "1 INTRODUCTION", "content": "Embeddings, or distributed representations, encode inputs (whether text or images) as\nfixed-dimensional vectors, enabling a range of downstream tasks. Since the advent of\nWord2Vec (Mikolov, 2013) and GloVe (Pennington et al., 2014), substantial research efforts have\nfocused on learning textual embeddings (Kiros et al., 2015; Conneau et al., 2017) and image em-\nbeddings (Radford et al., 2021; Li et al., 2022; Jia et al., 2021; Yu et al., 2022). These embeddings\nfacilitate a variety of applications, including textual and visual semantic similarity (Agirre et al.,\n2012; Marelli et al., 2014; Chechik et al., 2010; Cer et al., 2017), information retrieval (Mitra et al.,\n2017; Karpukhin et al., 2020; Lin et al., 2014), automatic evaluation (Zhang et al., 2020; Sellam\net al., 2020), prompt retrieval for in-context learning (Liu et al., 2022; Rubin et al., 2022; Hongjin\net al., 2022), and retrieval-augmented generation (Lewis et al., 2020; Guu et al., 2020; Izacard &\nGrave, 2020). A recent shift in research has focused on developing universal embeddings that can\ngeneralize across a wide range of tasks. For instance, Muennighoff et al. (2023) introduced \u039c\u03a4\u0395\u0392\n(Massive Text Embedding Benchmark) to comprehensively assess text embeddings across tasks such\nas classification and clustering. MTEB has become the standard for evaluating universal text em-\nbeddings. Recent works (Wang et al., 2022a; Su et al., 2023; Wang et al., 2024; Springer et al.,\n2024; BehnamGhader et al., 2024) have demonstrated promising results on the MTEB benchmark.\nHowever, progress in multimodal embeddings has been relatively slower. Despite advancements\nin text embeddings, the lack of both benchmarks and methodologies in the multimodal embedding\ndomain remains a challenge."}, {"title": "2 MMEB: A BENCHMARK FOR MULTIMODAL EMBEDDINGS", "content": ""}, {"title": "2.1 DATASET OVERVIEW", "content": "We present MMEB (Massive Multimodal Embedding Benchmark), a comprehensive benchmark\ndesigned to evaluate multimodal embeddings across a diverse set of tasks. MMEB consists of 36\ndatasets organized into four meta-tasks: classification, visual question answering, retrieval, and\nvisual grounding. Each task is reformulated as a ranking problem, where the model is provided with\nan instruction and a query (which may consist of text, images, or both) and is tasked with selecting\nthe correct answer from a set of candidates. These candidates could be text, images, or additional\ninstructions. The datasets are divided into two categories: 20 in-distribution datasets for training and\n16 out-of-distribution datasets for evaluation. We report performance metrics across all 36 tasks. An\noverview of MMEB is provided in Figure 2 and the dataset statistics are provided in Table 1.\nThe embedding models are supposed to compress the query side into a vector and the target can-\ndidates into a set of vectors. The candidate with the highest dot-product will be selected as the\nprediction for evaluation. We measure the Precision@1 to reflect the percentage of top candidate\nmatching the groundtruth. To ensure the task difficulty, we introduce a large amount of candidates.\nMMEB offers a wide range of tasks from various domains, such as common, news, Wikipedia,\nweb, and fashion. The benchmark incorporates diverse combinations of modalities for both queries\nand targets, including text, images, and text-image pairs. Additionally, tasks are designed to follow\ndifferent types of instructions. For instance, tasks may involve object recognition (e.g., \u201cIdentify the\nobject shown in the image.\"), retrieval (e.g., \"Find an image that matches the given caption.\"), or\nvisual grounding (e.g., \"Select the portion of the image that answers the question.\"). Examples for\neach dataset in MMEB are provided in Tables 6, 7, 8 and 9. The diversity in MMEB makes it an\nideal testbed for universal embeddings.\""}, {"title": "2.2 \u039c\u0395\u03a4A-TASK AND DATASET DESIGN", "content": "MMEB is organized into four primary meta-task categories:\nClassification This category comprises 5 in-distribution and 5 out-of-distribution datasets. Queries\nconsist of instructions and images, optionally accompanied by related text. Targets are class labels,\nand the number of class labels corresponds to the number of classes in the dataset.\nVisual Question Answering This category includes 6 in-distribution and 4 out-of-distribution\ndatasets. The query consists of an instruction, an image, and a piece of text as the question, while\nthe target is the answer. Each query has 1,000 target candidates: 1 ground truth and 999 distractors.\nInformation Retrieval This category contains 8 in-distribution and 4 out-of-distribution datasets.\nBoth the query and target sides can involve a combination of text, images, and instructions. Similar\nto the VQA task, each query has 1,000 candidates, with 1 ground truth and 999 distractors.\nVisual Grounding This category includes 1 in-distribution and 3 out-of-distribution datasets, which\nare adapted from object detection tasks. Queries consist of an instruction, an image, and text re-\nferring to a specific region or object within the image. The target may include a cropped image of\nthe object or text describing the same region. Each query includes 1,000 candidates: 1 ground truth\nand 999 distractors. These distractors may include hard negatives from the same object class, other\nobjects in the image, or random objects from different images.\nThis task evaluates the model's ability to recognize and represent the same object or concept across\ndifferent modalities (image or text) and from varying perspectives. The task involves correctly iden-\ntifying or referring to specific regions or objects within an image based on the provided instructions\nor language expressions."}, {"title": "3 VLM2VEC: TRANSFORMING LVMS TO EMBEDDERS", "content": ""}, {"title": "3.1 CONTRASTIVE TRAINING", "content": "We develop VLM2VEC, a contrastive training framework designed to convert any state-of-the-art\nvision-language model into an embedding model, as illustrated in Figure 3. A relevant query-target\npair is denoted as (q, t+). Both q and t+ could be either single image, text or single image + text.\nWe define q : (qt, qi) and t+ : (t+,).\nWe then apply the instruction to the original query q to generate a new one qinst:\nQinst = [IMAGE_TOKEN] Instruct: {task_definition} \\n Query: {q}  (1)\nwhere \"{task_definition}\" is a placeholder for a one-sentence description of the embedding task.\nTo enhance the embedding model's generalizability by better understanding instructions, we have\ncrafted task-specific instructions, as shown in Tables 6, 7, 8 and 9."}, {"title": "3.2 INCREASING BATCH SIZE THROUGH GRADCACHE", "content": "Since hard negatives are often difficult or ambiguous to collect for most multimodal datasets, using\nlarger batch sizes becomes crucial. This increases the number of in-batch random negatives, which\nin turn helps improve the performance of the embedding model.\nA bottleneck lies in the GPU memory that limits us from increasing the batch size and the number\nof in-batch random negatives during training, as each training instance may include one image (ei-\nther from the query or target side) or multiple images (from both query and target sides), resulting in\nsubstantial memory consumption. We apply GradCache (Gao et al., 2021a), a gradient caching tech-\nnique that decouples backpropagation between contrastive loss and the encoder, removing encoder\nbackward pass data dependency along the batch dimension.\nMathematically, supposed we have a large batch of queries Q, and we divide it into a set of sub-\nbatches, each of which can fit into memory for gradient computation: Q = {Q1, Q2, . . . }. There\nare two major steps: \u201cRepresentation Gradient Computation and Caching\u201d and \u201cSub-batch Gradient\nAccumulation\". First, gradient tensors within each subbatch is calculated and stored: u\u1d62 = . Then gradients are accumulated for encoder parameters across all sub-batches:\n\\frac{\\partial \\mathcal{L}}{\\partial \\Theta} = \\sum_{Q_j \\in Q} \\sum_{q_i \\in Q_j} \\frac{\\partial \\mathcal{L}}{\\partial f(q_i)} \\frac{\\partial f(q_i)}{\\partial \\Theta} = \\sum_{Q_j \\in Q} \\sum_{q_i \\in Q_j} u\u1d62 (4)"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENT SETTINGS", "content": "In this paper, we adopt Phi-3.5-V (Abdin et al., 2024) as our backbone VLM, with training conducted\nvia either full model fine-tuning or LoRA. The temperature is set to 0.02. Multiple experimental\nconfigurations are explored to assess the impact of key hyperparameters, such as batch size and input\nlength. In the basic setting, we use a batch size of 256, 2,000 training steps, 4 image crops, and a\nmaximum text length of 256 tokens. Detailed ablation studies on these parameters are discussed in\nSection 4.4.\nFor the 20 training datasets, if a dataset contains more than 50K samples, we randomly select 50K\nfor consistency, resulting in a total training set of 662K data points. When using GradCache, we set\na sub-batch size of 4 to enable full model tuning, with the total batch size accumulated to 1,024. All\nexperiments were run on 8 H100 GPUs."}, {"title": "4.2 BASELINES", "content": "Four groups of baselines are reported in this study.\nCLIP-family: We utilize vision/language encoders such as CLIP (Radford et al., 2021), Open-\nCLIP (Cherti et al., 2023), SigLIP (Zhai et al., 2023), and BLIP2 (Li et al., 2023a) as our baseline.\nDue to the length limitations of the text encoder, some queries or target text in certain tasks may\nbe truncated. We apply score-level fusion by combining multimodal features using element-wise\naddition with equal weights (W\u2081 = W2 = 1). As found in UniIR (Wei et al., 2023), we do not use\ninstructions, as this was observed to potentially degrade performance.\nUniIR: UniIR (Wei et al., 2023) is a unified, instruction-guided multimodal retriever designed to\nhandle eight different retrieval tasks across multiple modalities. The model builds on CLIP and\nBLIP, employing shallow fusion techniques such as score-level and feature-level fusion to integrate\nmodalities. In this study, we use the BLIP_FF variation as a baseline.\nMagicLens: MagicLens (Zhang et al., 2024) is a self-supervised image retrieval model capable of\nhandling open-ended instructions. It utilizes a dual-encoder architecture with shared parameters,\ninitializing the vision and language encoders with either CoCa or CLIP. The model uses a multi-\nhead attention pooler to unify multimodal inputs into a single embedding. For this study, we report\nresults using the CLIP-Large backbone. Since MagicLens requires image inputs, we represent pure\ntext inputs by using the output vector of the text encoder."}, {"title": "4.3 MAIN RESULT", "content": "From Table 2, the LoRA version of VLM2VEC is the best variant, achieving an average precision@1\nof 60.1% across all 36 datasets from MMEB. Additionally, it maintains an average precision@1\nof 52.0% on 16 out-of-distribution tasks in zero-shot evaluation, suggesting strong generalization\nability. This indicates that our model, when well-trained on datasets from diverse task categories,\ndomains, and modality combinations, can effectively follow instructions to align the visual and text\nspaces and generalize well to unseen tasks. The full fine-tuning variation achieves slightly lower\nscores than the LoRA version. For a detailed discussion comparing full fine-tuning and LoRA,\nplease refer to Section 4.4.1.\nCompared to other baseline models, we observe consistent improvements in our model across all\nmeta-task categories. Notably, our model achieves a 17.3-point improvement (from 42.8 to 60.1)\nacross all 36 MMEB datasets and a 11.6-point increase (from 40.4 to 52.0) on 16 out-of-distribution\ndatasets for zero-shot evaluation. Additionally, unlike the baseline models, which fail to demonstrate\nreasonable performance across all different task categories, VLM2VEC achieves relatively strong per-\nformance (at least 50%) across all four meta-task categories. This highlights its capability to handle\na wide range of multimodal embedding tasks effectively. It's worth noting that our contemporary\nwork E5-V (Wang et al., 2022a), which is also based on vision-language models, performs much\nworse than VLM2VEC. This is due to the fact that E5-V has been trained exclusively on text data\nwithout using multimodal data."}, {"title": "4.4 RESULT ANALYSIS", "content": "To train an effective and generalizable multimodal embedding, various factors need to be considered,\nranging from the data to the training setup. In this section, we present detailed ablation studies on\nthese factors. We will discuss two training setups: Full Fine-Tuning vs. LoRA, along with Training"}, {"title": "4.4.1 FULL FINE-TUNING VS. LORA", "content": "When fine-tuning the VLMs, a key decision is whether to conduct full fine-tuning, which updates\nall parameters in the model, or to use a parameter-efficient method such as LoRA. We compare\nthe performance of fully fine-tuned VLM2VEC with its LoRA variants at different ranks. The train-\ning and data setups are kept consistent across all models. We observe that LoRA achieves better\nperformance when the rank is appropriately configured."}, {"title": "4.4.2 TRAINING PARAMETERS", "content": "During our experiments, we identified three key parameters that significantly impact the perfor-\nmance of VLM2VEC: training batch size, the number of sub-image crops, and the number of training\nsteps. In Figure 4, we observe that the final performance gradually improves as we increase the\nbatch size, training step size, and number of sub-image crops. We particularly want to highlight\nthe impact of batch size. Due to the lack of hard negatives, using a large batch size with plenty\nof random negatives, supported by the GradCache technique, plays a crucial role in enhancing the\nperformance of VLM2VEC, as discussed in Section 3.2."}, {"title": "4.4.3 \u039c\u0395TA-TASK GENERALIZATION", "content": "We have demonstrated that VLM2VEC has the potential to transfer to out-of-distribution datasets\nafter being trained on a diverse range of in-distribution datasets, with the instruction-following set-\ntings. An interesting question arises as to whether focusing on a specific meta-task can enhance the\nmodel's overall generalizability. We have trained three models, each focused solely on one meta-task\n(classification, visual question answering, and retrieval). Visual grounding was not included due to"}, {"title": "4.4.4 IMPACT OF INSTRUCTIONS", "content": "Previous studies have shown the influence of instructions on addressing various tasks. VLM2VEC,\nwhich leverages a VLM as its backbone and is trained on large-scale datasets with instructions, is\nexpected to better generalize across tasks and improve performance in multimodal embedding tasks.\nIn this section, we evaluate the performance of VLM2VEC with and without task-specific instructions\nto quantify the impact of incorporating instructions into the embedding process. As shown in Table\n4, excluding instructions leads to an average performance drop of around 30%, highlighting the\nimportance of instruction-guided embeddings."}, {"title": "5 RELATED WORK", "content": ""}, {"title": "5.1 TEXT EMBEDDING", "content": "Text embeddings have demonstrated significant potential in powering downstream applications such\nas information retrieval (Karpukhin et al., 2020; Xiong et al., 2020), text similarity (Gao et al.,\n2021b), prompt retrieval for in-context learning (Hongjin et al., 2022), and classification (Lo-\ngeswaran & Lee, 2018; Reimers & Gurevych, 2019). Early work focused on creating effective"}, {"title": "5.2 MULTIMODAL EMBEDDINGS", "content": "Multimodal embeddings have long been a significant research challenge. Early works like\nCLIP (Radford et al., 2021), BLIP (Li et al., 2022; 2023a), Align (Jia et al., 2021), SigLIP (Zhai\net al., 2023), SimVLM Wang et al. (2022b) and CoCa (Yu et al., 2022) primarily focused on learn-\ning universal representations from large-scale, weakly supervised image-text pairs. These models\ngenerally encode images and text separately, projecting them into a shared space. This approach has\nlaid the groundwork for more recent multimodal models like LLaVA (Liu et al., 2024).\nMost research on universal multimodal embeddings involves fine-tuning models like CLIP or BLIP,\ntypically using simple fusion mechanisms to combine visual and language information. For in-\nstance, UniIR (Wei et al., 2023) creates multimodal embeddings by simply adding text and visual\nfeatures, while MagicLens (Zhang et al., 2024) employs shallow self-attention layers to integrate\nthese features more effectively. The study most similar to ours is E5-V (Jiang et al., 2024), which\nconverts a multimodal generative model into an embedding model. We compare our approach with\nE5-V and find that our models achieve significant improvements in performance."}, {"title": "5.3 EMBEDDING BENCHMARKS", "content": "Significant efforts have been made to develop benchmarks for evaluating retrieval systems. For text\nretrieval models, MS MARCO (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al.,\n2019b) are two of the most widely used benchmarks in general domains. To broaden the evaluation\nacross more diverse domains, BEIR (Thakur et al.) was introduced, incorporating 18 datasets from\nvarious fields. Building on this, MTEB (Muennighoff et al., 2023) further expands BEIR's scope by\nadding more tasks, such as classification, clustering, and semantic textual similarity (STS), to assess\nthe generalization capabilities of embedding models.\nFor multimodal retrieval, several benchmarks have been introduced to evaluate model performance\nacross different modalities. MBEIR (Wei et al., 2023) includes 8 tasks and 16 datasets, designed\nto test models' ability to retrieve information based on various forms of queries (text, image, or a\ncombination) and instructions that span multiple modalities."}, {"title": "6 CONCLUSION", "content": "In this paper, we aim to build the first large-scale multimodal embedding framework, comprising\ntwo main components: MMEB and VLM2VEC. MMEB includes 36 datasets across four meta-task\ncategories, providing a comprehensive and diverse framework for training and evaluating embed-\nding models. VLM2VEC leverages VLMs as a backbone to deeply fuse visual and textual spaces,\nenhancing generalization to unseen tasks through instruction following."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 DATASET DETAILS", "content": ""}, {"title": "A.1.1 CLASSIFICATION", "content": "There are a total of 10 datasets for classification tasks.\nImageNet-1K (Deng et al., 2009) The dataset is s large-scale dataset commonly used in image\nclassification, consisting of over 1 million images across 1K different classes.\nImageNet-A (Hendrycks et al., 2021b) The dataset contains images from a distribution unlike the\nImageNet training distribution. ImageNet-A examples belong to ImageNet classes, but the exam-\nples are harder and can cause mistakes across various models. They cause consistent classification\nmistakes due to scene complications encountered in the long tail of scene configurations and by\nexploiting classifier blind spots.\nImageNet-R (Hendrycks et al., 2021a) The dataset contains set of images labeled with ImageNet\nlabels obtained by collecting art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paint-\nings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game\nrenditions of ImageNet classes.\nVOC2007 (Everingham et al., 2014) The dataset focuses on recognizing objects in realistic scenarios\nand contains 20 object classes.\nN24News (Wang et al., 2021) The dataset is sourced from the New York Times and consists of 24\ncategories, with each news article containing both text and image information. The task is to classify\nthe given news image and its accompanying text into one of the 24 categories.\nHatefulMemes (Kiela et al., 2020) The dataset proposes a new challenge set for multimodal classi-\nfication, focusing on detecting hate speech in multimodal memes.\nPlace365 (Zhou et al., 2017) The dataset is a repository of 10 million scene photographs, labeled\nwith scene semantic categories, comprising a large and diverse list of the types of environments\nencountered in the world.\nSUN397 (Xiao et al., 2010) The dataset is a dataset for scene recognition consisting of 397 cate-\ngories.\nObjectNet (Barbu et al., 2019) The dataset is a crowd-sourced test set of 50K images featuring\nobjects in unusual poses and cluttered scenes, designed to challenge recognition performance. It\nincludes controls for rotation, background, and viewpoint, and covers 313 object classes.\nCountry-211 (Radford et al., 2021) The dataset is designed to assess the geolocation capability of\nvisual representations. It filters the YFCC100M dataset to find 211 countries that have at least 300\nphotos with GPS coordinates."}, {"title": "A.1.2 VISUAL QUESTION ANSWERING (VQA)", "content": "There are a total of 10 datasets for VQA tasks.\nOK-VQA (Marino et al., 2019) The dataset includes questions that require external resources for\nanswers.\nA-OKVQA (Schwenk et al., 2022) The dataset is an augmented successor of OK-VQA, requiring\na broad base of commonsense and world knowledge to answer. The questions generally cannot be\nanswered by simply querying a knowledge base, and instead require some form of commonsense\nreasoning about the scene depicted in the image.\nDocVQA (Mathew et al., 2021) The dataset contains questions for document analysis and recogni-\ntion over document images of various types and content.\nInfographicsVQA (Mathew et al., 2022) The dataset comprises a diverse collection of infographics\naccompanied by natural language question and answer annotations. The questions require methods\ncapable of jointly reasoning over the document layout, textual content, graphical elements, and data\nvisualizations."}, {"title": "A.1.3 RETRIEVAL", "content": "There are a total of 12 datasets for retrieval tasks.\nVisDial (Das et al., 2017) The dataset features dialogues created by two Amazon Mechanical Turk\nworkers. One worker takes the role of the \u201cquestioner\u201d, who only sees the text description of an\nimage, while the other plays the \u201canswerer\u201d, who has access to the image. They engage in a 10-\nround Q&A session about the image. We repurpose this dataset as a retrieval task, where the goal is\nto retrieve the image based on the given dialogue.\nCIRR (Liu et al., 2021) The dataset is designed for the task of composed image retrieval. It consists\nof pairs of real-life reference and target images, along with a modification sentence that describes\nthe changes made between the two images.\nFashionIQ (Wu et al., 2021) The dataset contains images of fashion products with crowd-sourced\ndescriptions highlighting the differences between these products. Similar to CIRR, FashionIQ can\nalso be used for the task of composed image retrieval, where each test case consists of a pair of\nreference and target images, along with a modification sentence that describes the changes between\nthe two images.\nVisualNews (Liu et al., 2020) The dataset contains publicly available news image paired with cap-\ntions. We split this task into two setups: \u201cVisualNews_i2t\u201d, which retrieves the caption given the\nnews image and \u201cVisualNews_t2i\u201d, which retrieves the news image given the caption.\nMSCOCO (Lin et al., 2014) The dataset is a well-known image caption dataset. Similar to Visu-\nalNews, WE split this task into two setups: \u201cMSCOCO_i2t', which retrieves the caption given the\nimage and \"MSCOCO_t2i\u201d, which retrieves the image given the caption.\nWebQA (Chang et al., 2022) The dataset is a multihop, multimodal QA dataset that requires re-\ntrieving a Wikipedia page to answer a given question. We use the Wikipedia page's image and text\ndescriptions as the candidates for retrieval.\nNIGHTS (Fu et al., 2023) The dataset contains human similarity judgments on image pairs that are\nalike in various ways. The original dataset consists of triplets: a reference image and two perturbed\nversions, along with human judgments indicating which version is most similar to the reference.\nFollowing M-BEIR (Wei et al., 2023), we refactor this dataset into a retrieval task to match pairwise\nimages, where the reference image serves as the query, and the perturbed version that aligns with\nhuman judgment is the target."}, {"title": "A.1.4 VISUAL GROUNDING", "content": "There are a total of 4 datasets for visual grounding tasks.\nMSCOCO (Lin et al., 2014) The dataset includes an object detection task, which involves recogniz-\ning an object from a given class in an image. We have repurposed this task into a ranking problem\nwithin the MMEB format. The query consists of the image and the object name, while the target\nis the cropped image of the specified object. We gather distractors from other objects in the same\nimage as well as from different images. We discard test cases where the object is too small.\nRefCOCO (Kazemzadeh et al., 2014) The dataset includes an object detection task that requires\nmore reasoning than MSCOCO. Unlike simply identifying the object class, the RefCOCO dataset\nuses language expressions to refer to specific objects within an image. In our MMEB, we have two\ntasks related to RefCOCO: \u201cRefCOCO\u201d and \u201cRefCOCO-Matching\u201d. In \u201cRefCOCO\u201d, the query\nconsists of the image and the language expressions referring to a specific object, while the target is\nthe cropped image of that object. In \u201cRefCOCO-Matching\u201d, both the query and the target contain\nthe image and the language expressions referring to a specific object, where the two objects are\nidentical.\nVisual7W-pointing (Zhu et al., 2016) The dataset establishes a semantic link between textual de-\nscriptions and image regions through object-level grounding. It has two types of questions: \"telling\"\nand \"pointing\". It leverages the six W questions (what, where, when, who, why, and how) to sys-\ntematically examine a model's capability for visual understanding through telling questions. Addi-\ntionally, a seventh \u201cwhich\u201d question is appended for visual answers as pointing questions. We use\n\"Visual7W-telling\" in our VQA category and \u201cVisual7W-pointing\u201d in our visual grounding category."}]}