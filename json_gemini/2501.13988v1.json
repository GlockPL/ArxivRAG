{"title": "MCRL4OR: Multimodal Contrastive Representation Learning for Off-Road Environmental Perception", "authors": ["Yi Yang", "Zhang Zhang", "Liang Wang"], "abstract": "Most studies on environmental perception for autonomous vehicles (AVs) focus on urban traffic environments, where the objects/stuff to be perceived are mainly from man-made scenes and scalable datasets with dense annotations can be used to train supervised learning models. By contrast, it is hard to densely annotate a large-scale off-road driving dataset manually due to the inherently unstructured nature of off-road environments. In this paper, we propose a Multimodal Contrastive Representation Learning approach for Off-Road environmental perception, namely MCRL4OR. This approach aims to jointly learn three encoders for processing visual images, locomotion states, and control actions by aligning the locomotion states with the fused features of visual images and control actions within a contrastive learning framework. The causation behind this alignment strategy is that the inertial locomotion state is the result of taking a certain control action under the current landform/terrain condition perceived by visual sensors. In experiments, we pre-train the MCRL4OR with a large-scale off-road driving dataset and adopt the learned multimodal representations for various downstream perception tasks in off-road driving scenarios. The superior performance in downstream tasks demonstrates the advantages of the pre-trained multimodal representations.", "sections": [{"title": "Introduction", "content": "In the realm of autonomous driving, it is critical to accurately perceive the surrounding environment for safe and comfortable navigation. The majority of researches focuses on urban traffic environments, where the detection and segmentation of traffic participants (e.g., pedestrians, vehicles) or traffic elements (e.g., road surface, lane lines, and traffic signs) have been widely studied. Remarkable improvements have been achieved due to the advance of deep learning technologies and the availability of large-scale and densely annotated datasets (e.g., nuScenes (Caesar et al. 2020)). By contrast, environmental perception in off-road driving scenarios remains a big challenge due to the unstructured and open-world nature. Accurate perception of off-road environments is crucial for autonomous vehicles (AVs) or robots to robustly and safely operate in natural (non-man-made) environments, such as harvesting in agricultural lands and Mars exploration. It is essential to investigate the issue of off-road environmental perception.\nHowever, the task settings of off-road environmental perception vary with different object categories of interests and granularity of annotations. For example, Off-Road Freespace Detection (ORFD) (Min et al. 2022) aims to classify the off-road visual scene into traversable areas, non-traversable areas, and unreachable areas. Nevertheless, for a harvesting robot, the above coarse-grained taxonomy is not enough to distinguish different vegetation of agricultural lands. Moreover, the definition of traversable areas is also ambiguous depending on different task requirements. Due to the huge diversity and complexity, it is challenging to collect a unified scable dataset with dense manual annotations of all-inclusive information in off-road driving scenarios. The difficulty hinders the learning of generalized representations of off-road scenes based on traditional supervised learning methods. Whereas current AVs can record the off-road driving trajectories automatically with a rich source of multimodal data (such as cameras, Lidars and inertial sensors) as well as data loggers for control actions like braking and accelerating. Thus, it is highly valuable to adopt self-supervised pre-training methods to explore the intrinsic patterns in these multimodal trajectories and learn transferable multimodal representations for various perception tasks in off-road driving scenarios.\nInspired by the success of multimodal vision-language models, we propose a multimodal contrastive representation learning approach for off-road environmental perception tasks, namely MCRL4OR, to jointly learn informative and transferable representations for visual perception, control actions, and locomotion states during off-road driving. As shown in Fig.1, the MCRL4OR consists of three branches including a visual observation encoder, a control action encoder, and a locomotion state encoder, respectively, The objective of contrastive learning is to predict the correct correspondences between different modalities within a batch of multimodal training examples. In particular, the vision branch and control branch are early fused before the alignment with the locomotion branch. This alignment strategy indicated as (observation + action) \u2194 locomotion can be explained by the causal relationship that the locomotion state is the result of taking a certain control action (such as accelerating or steering) under the current landform/terrain condition (e.g., rough or smooth road surface) perceived by visual sensors. For example, when traversing an unpaved rough road with holes and cracks, the accelerating operation can lead to the ego-vehicle shaking or vibrating over the road.\nIn experiments, we firstly pre-train the MCRL4OR model on the TartanDrive dataset (Triest et al. 2022), which is the largest real-world, multimodal, off-road driving dataset. Then, a set of down-stream evaluation tasks are performed to demonstrate the advantages of the pre-trained representations, namely cross-modal retrieval, dynamics prediction, and scene segmentation. Experimental results demonstrate the effectiveness of the alignment strategy of (observation+action) \u2192 locomotion. Moreover, the pre-trained MCRL4OR model can consistently improve all three down-stream tasks, which validates the generalization capability of the learned multimodal representations.\nThe contributions can be summarized as follows.\n(1) We propose MCRL4OR, a multimodal contrastive representation learning approach for off-road environmental perception, which jointly learns the encoders of visual images, control actions and locomotion states with a multimodal alignment paradigm.\n(2) We perform in-depth investigations into the MCRL4OR model architecture and design an optimal alignment strategy based on the intrinsic dynamics of vehicle traversal across various landform and terrain conditions.\n(3) Extensive experiments are conducted to validate the advantages of MCRL4OR, including the use of the largest multimodal off-road dataset for model pre-training and evaluating the learned encoders with three down-stream tasks such as cross-modal retrieval, dynamics prediction and scene segmentation."}, {"title": "Related Work", "content": "Numerous studies have been conducted for autonomous driving in urban environments. A number of large-scale and densely annotated datasets, such as KITTI (Geiger, Lenz, and Urtasun 2012), Waymo (Sun et al. 2020), Argoverse (Chang et al. 2019; Wilson et al.), nuScenes (Caesar et al. 2020) and BLVD (Xue et al. 2019), have been released for the scalable training of supervised learning models. Significant advancements are achieved in various tasks such as 3D object detection (Li et al. 2022), scene segmentation (Ng et al. 2020), and occupancy prediction (Li et al. 2023), within urban traffic scenes.\nDue to the unstructured nature and wide diversity of off-road environments, current studies on off-road environmental perception are largely based on self-collected datasets for specific tasks, such as object detection or traversable region segmentation. For instance, NERC (Pezzementi et al. 2018) claims that the success of person detection in urban scenes cannot be directly transferred to agricultural environments and thus collects a specific dataset on person detection in agricultural scenes. Besides that, several datasets on off-road scene segmentation such as DeepScene (Valada et al. 2017), YOCR (Maturana et al. 2018), RUGD (Wigness et al. 2019), and RELLIS-3D (Jiang et al. 2021), have been proposed, where various modalities, from RGB images to multispectral images and 3D point clouds, have been exploited to deal with the difficulties in off-road environments. However all the above segmentation datasets are not compatible for each other due to the different annotation granularities. For example, the ORFD (Min et al. 2022) dataset is proposed for traversability analysis, which segments a visual scene into three coarse-grained categories, i.e., traversable, non-traversable, and unreachable areas. While the RELLIS-3D (Jiang et al. 2021) annotates 20 categories of objects and terrain classes, e.g., mud, grass, and rubble piles. Thus, designing a unified annotation ontology that meets the requirements of various applications in off-road scenarios is challenging.\nRecently some researchers have also attempted to learn off-road perception models, guided by the decision-making and planning goals of AVs. Kahn et al. (Kahn, Abbeel, and Levine 2021) propose a model-based reinforcement learning method to enable a wheeled robot to complete navigation tasks on off-road terrain, which leverages hours of multimodal sensor data collected through random explorations to train a perception module that understands the traversability of various types of terrain. Triest et al. (Triest et al. 2022) collect a large-scale multimodal dataset, termed TartanDrive, to learn a dynamics prediction model for future trajectory planning of an all-terrain vehicle (ATV). Their work proves that the dynamics of terrain is essential for robust and safe driving in off-road environments. Seo et al. (Seo, Sim, and Shim 2023) leverage historical driving data to autonomously generate labels indicating high traversability for areas traversed by a vehicle. These self-generated labels serve as the foundation for training a neural network to identify safe-to-navigate terrains through a singular-class classification task. Ye, et al. (Ye, Mei, and Hu 2023) design a proficient multimodal learning network, named M2F2-Net, aiming to detect free space within off-road environments. Tremblay, et al. (Tremblay et al. 2021) exploit a multimodal recurrent state-space model to integrate multi-modal learning with latent time-series prediction."}, {"title": "Multimodal contrastive learning", "content": "Contrastive learning has recently become a popular self-supervised representation learning method in computer vision (CV) community, which aims at encoding augmented versions of the same sample close to each other while pushing away those embeddings from different instances. The current dominant contrastive learning methods include SimCLR (Chen et al. 2020a,b) and MoCo (He et al. 2020; Chen et al. 2020c).\nContrastive learning can be naturally extended for multimodal representation learning, where each unimodal component is considered as one augmented version of a multimodal sample. For example, CLIP (Radford et al. 2021) has been proposed for scalable vision-language pre-training, which jointly learns an image encoder and a text encoder to predict the correct alignment relationships in a batch of image and text training pairs. The similar idea has been adopted to other modalities such as video (Xu et al. 2021), audio (Guzhov et al. 2022), 3D point cloud (Zhang et al. 2022), and inertial measurement unit (IMU) (Moon et al. 2022). In this work, we also adopt a CLIP-like multimodal contrastive alignment paradigm to learn the representations of visual images, control actions, and locomotion states for off-road environmental perception. Meanwhile, the contrastive learning has become a popular way for multi-modal learning in autonomous systems. For example, Ma, et al. (Ma et al. 2022) propose a multimodal pre-training approach that needs to optimize three contrastive losses based on spatial connections, temporal connections, and spatiotemporal connections between spatial and temporal modalities. Zhang, et al. (Zhang, Peng, and Zhou 2022) present an action-conditioned contrastive learning method to jointly learn visual features and policy features. However, it fails to consider the interactions between the visual observations, actions, and locomotion states, which are crucial in off-road driving scenarios."}, {"title": "Methods", "content": "We assume an off-road AV (ego-agent) has been mounted with multiple perception sensors (e.g., camera and IMU) to perceive its surroundings and data loggers to record control actions (throttle and steering). Then the multimodal trajectories gathered by these sensors and data loggers are categorized into three groups: observations, locomotion states, and control actions.\n\u2022 Observations denoted as O are the visual/depth data captured by exteroceptive sensors, e.g., cameras or lidars, which describe the exteroceptive situation of the ego-agent in the off-road environment. In our work, observations consist of visual images captured by a stereo camera recording terrain information ahead of the ego agent.\n\u2022 Locomotion states denoted as S are the proprioceptive states of the ego-agent, including various locomotion measurements such as wheeled odometers, acceleration, angular velocity, and force transducer. The locomotion states reflect the ego-agent's perception of traversing different terrains in the off-road environment. In this work, each locomotion state contains 1) ego-pose, which is the position vector $p = (x, y, z)$ and quaternion orientation $q = (q_x,q_y,q_z, q_w)$; 2) ego-motion, which includes the inertial data (angular velocity and linear acceleration), shock travel and wheel revolutions per minute (RPM). The channel width of a locomotion state is 27. Detailed information about each channel can be found in the Supplementary Material.\n\u2022 Control actions denoted as C are the driver's actions to control vehicles moving in off-road environments, which are represented as a two-dimensional vector $(\\mu^1, \\mu^3)$ corresponding to throttle and steering, respectively. We adhere to the definitions in TartanDrive (Triest et al. 2022), where the throttle action $u^t$ ranges from 0 to 1, with 1 corresponding to full throttle. The steering action takes a value between -1 and 1, with -1 representing a hard left turn and 1 representing a hard right turn.\nBy the above definitions, the ego-agent's perception state in the current off-road environment can be represented by a pair of exteroceptive state (O) and proprioceptive state (S)."}, {"title": "Data Preparation", "content": "Before introducing the details of MCRL4OR, the procedure for data preparation is outlined here. This process synchronizes all sensors to a unified frequency, and then splits the trajectories of O, S, C to produce a set of triplet samples for training the MCRL4OR.\n\u2022 Frequency synchronization. Various sensors often work at different frequencies, for instance, in the TartanDrive (Triest et al. 2022), the IMU operates at 200Hz (400Hz in their released dataset), while the wheel RPM sensor reads at 50Hz, and 10Hz for the stereo camera. As previously mentioned, the locomotion state consists of multiple high-frequency sensor data stream such as IMU and wheel RPM readings. Working with raw data would entail a significant amount of time spent on aligning the timestamps from different sensors for the purpose of loading training samples. To mitigate this issue, we perform an explicit synchronization process, where all locomotion sensors and the control actions are upsampled to a common frequency of 400Hz for a subtle temporal alignment. Then, an average downsampling operation is performed with a unified frequency of 40Hz to reduce the data redundancies and noises. Finally, the frequency of the stereo camera observations is preserved at 10Hz.\n\u2022 Construction of triplet samples. We need to select an appropriate granularity to split the continuous trajectories into a scalable and diverse set of triplet samples for training the MCRL4OR model. Unlike urban traffic scenes, vehicles in off-road environments often move at relative slower speeds. To ensure that the vehicle can drive through the off-road area observed by the camera, we sample the trajectories of O, S, C with a temporal window of 6 seconds to construct a triple sample < $O_i$, $S_i$, $C_i$ >, where i is the sample index. For each sample, $O_i$ is chosen as the first image frame in the sampled temporal window from trajectory O. $s_i$ corresponds to all 240 frames in the 6 seconds from trajectory S with a channel width of 27, i.e., a 240 \u00d7 27 tensor. $c_i$ also contains 240 frames with 2 channels, i.e., a 240 \u00d7 2 tensor. Finally, we sample the triplet samples every 2 seconds, ensuring that two adjacent samples have no more than 4 seconds overlapping period."}, {"title": "MCRL4OR Framework", "content": "As presented in Fig. 1, the proposed MCRL4OR framework is inspired by the success of CLIP framework (Radford et al. 2021) for vision-language pre-training. Here, for a mini-batch of n training samples < $O_i$, $S_i$, $C_i$ >=1, the MCRL4OR adopts three encoders to extract the features of each input triplet, i.e., $v_i^o = f^O(o_i)$, $v_i^s = f^S(s_i)$, $v_i^c = f^C(c_i)$. Specifically, the observation encoder is based on the Swin-T (Liu et al. 2021), where the classification head is replaced by a 2-layer multilayer perceptron (MLP). As for the locomotion state encoder $f^S(*)$ and the control action encoder $f^C(*)$, the sequence encoder proposed in IMU2CLIP (Moon et al. 2022) is adopted, which is a stacked 1D-CNN with GroupNorms (Wu and He 2018).\nBefore the multimodal alignment between the observations and the locomotion states, the features of observation $v_i^o$ and control action $v_i^c$ are fused by a 2-layer MLP-based fusion module to acquire a joint feature $v_i^m = mlp(v_i^o, v_i^c)$. The reason behind this early fusion strategy is as follows. The locomotion state is the proprioceptive sense of the ego-agent when traversing across the current terrain (depicted by the visual observation) with a certain control action (e.g., accelerating or steering). The alignment strategy denoted as (observation+action) \u2194 locomotion can be explained as the learning of visual affordance (Gibson 1979) in off-road environments, i.e., the ego-agent learns to predict the possible proprioceptive sense based on the given control action and the observation of the current terrain.\nThen, for each pair of features $v_i^s$ and $v_i^m$, the similarity is calculated using cosine similarity.\n$sim(v_i^s,v_j^m) = \\frac{{v_i^s}^T v_j^m}{||v_i^s||_2 ||v_j^m||_2}$    (1)\nwhere i, j \u2208 {1...n}.\nFinally, a n x n similarity matrix is obtained to calculate the contrastive alignment loss for each mini-batch of training samples."}, {"title": "Contrastive Loss", "content": "Similar to the loss function in the CLIP (Radford et al. 2021), the goal is to contrast the paired features $v_i^s$ and $v_i^m$ from the same source with that from different sources in the current mini-batch. For a locomotion state feature $v_i^s$, the training loss function can be summarized as:\n$l(v_i^s, v_i^m) = -log \\frac{exp(sim(v_i^s, v_i^m)/\\tau)}{\\sum_{j=1}^n exp(sim(v_i^s, v_j^m)/\\tau)}$   (2)\nwhere \u03c4 is a learnable temperature parameter.\nSince the alignment relationship is symmetric, there exists a symmetric loss function for a given joint feature $v_i^m$:\n$l(v_i^m, v_i^s) = -log \\frac{exp(sim(v_i^m, v_i^s)/\\tau)}{\\sum_{j=1}^n exp(sim(v_i^m, v_j^s)/\\tau)}$     (3)\nThe overall loss function is defined as follows.\n$L = \\sum_{i=1}^n (l(v_i^s, v_i^m) + l(v_i^m, v_i^s))$    (4)"}, {"title": "Datasets and Evaluation Tasks", "content": "Two large-scale off-road datasets, i.e., TartanDrive (Triest et al. 2022) and ORFD (Min et al. 2022), are adopted to validate the effectiveness of the proposed MCRL4OR.\nTartanDrive dataset (Triest et al. 2022) is built for the task of off-road dynamics prediction, which collects 630 trajectories with roughly 200,000 driving interactions on a modified Yamaha Viking ATV with 7 unique sensing modalities in a variety of terrain including tall grass, rocks, and mud. This is the current largest real-world multimodal off-road driving dataset, both in terms of the number of interactions and sensing modalities. Following the training/test dataset splits in (Triest et al. 2022), we use 46101 triplets sampled from the trajectories in the training set to pre-train the MCRL4OR model. Then, two evaluation tasks, i.e., cross-modal retrieval and off-road dynamics prediction, are performed on the test set.\nORFD dataset (Min et al. 2022) is proposed for traversability analysis in off-road environments. It collects 30 sequences covering a distance of about 3km in off-road environments with various scenarios and weather conditions. A total of 12198 Lidar point cloud and RGB images are annotated at pixel level with three classes, i.e., traversable, non-traversable, and unreachable area. In this work, we utilize the ORFD dataset to showcase the generalization capabilities of the pre-trained MCRL4OR model in the task of off-road semantic segmentation."}, {"title": "Evaluation Tasks", "content": "As mentioned above, three tasks are adopted to evaluate the advantages of the pre-trained MCRL4OR.\nTask 1: Cross-modal retrieval is adopted to evaluate the effects of multimodal alignments of the pre-trained MCRL4OR, which has been commonly used in the studies of multimodal self-supervised learning (Moon et al. 2022; Radford et al. 2021; Xu et al. 2021). Here, we conduct the evaluation task on the test set of TartanDrive. For each triplet sample, bi-directional retrievals i.e., locomotion \u2192 (observation + action) and (observation + action) \u2192 locomotion are performed. The evaluation metric is the ranked accuracy (rank-n Acc.), which measures the percentage of targets retrieved within the top-k ranked results over all queries. Notably, for each query, there is only one target from the same triplet sample.\nIn this task, two baselines are tested for comparison. The first is a random retrieval, which serves as a lower-bound performance. The second one is to use the simple alignment strategy locomotion \u2192 observation for pre-training MCRL4OR, which ignores the intervention of control actions. In this case, the input of control action input is padded with zeros to train the MCRL4OR without the input of control actions. This baseline is adopted to investigate alternative architecture for the MCRL4OR model.\nTask 2: Off-road dynamics prediction is to predict the vehicle's trajectory and orientation given the planned control actions and history trajectories. TartanDrive (Triest et al. 2022) proves that in unstructured environments, the dynamics of the terrain is a significant factor in affecting the ego-vehicle's trajectory following driving actions. We adhere to the same dynamics prediction setup as in TartanDrive and construct two dynamics prediction models using two backbones, i.e., Gate Recurrent Unit (GRU) (Cho et al. 2014) which has been used in (Triest et al. 2022) and Informer (Zhou et al. 2021) which is a more advanced transformer based predictor. For both backbones, the pre-trained locomotion states encoder is fixed to extract locomotion features. The evaluation metric is rooted mean square error (RMSE) of the predicted locomotion state (x-y-z coordinates and quaternion).\nTask 3: Off-road semantic segmentation aims to segment the traversable areas in off-road environments, which is used to demonstrate the generalization capability of the pre-trained MCRL4OR model in the ORFD dataset (Min et al. 2022). ORFD treats segmentation as a pixel-level classification task, using accuracy (acc), precision, recall, f1 and mIoU to evaluate model's performance. Concretely, we replace the image encoder of the OFF-Net (Min et al. 2022), with the observation encoder of the pre-trained MCRL4OR model. Then, we follow the train/validation/test set split in the ORFD to fine-tune the OFF-Net. Since the observation encoder in the MCRL4OR model, i.e., Swin-T, is more advanced than that in OFF-Net, we conduct an additional MCRL4OR pre-training with the image encoder in OFF-Net for a fair comparison."}, {"title": "Experiments", "content": "In this section, we will introduce the implementation details in experiments, and the results of various evaluation tasks and the ablation studies."}, {"title": "Implementation Details", "content": "As for pre-training MCRL4OR, Adam (Kingma and Ba 2014) optimizer with recommended hyper-parameters is used. The total training epochs are 20. The learning rate is linearly increased from 10-4 to 10\u20133 in the first 10 epochs, then kept unchanged for the last 10 epochs. The batch size is set to 64 samples per GPU. The output feature dimensions for all encoders are 128. Following the studies (Chen et al. 2020a,b; He et al. 2020; Chen et al. 2020c), a two-layer MLP is set to project encoder features to a common alignment feature space. Additionally, a two-layer MLP is used for fusing observation features and control action features before the alignment with locomotion state features. To enforce the model to focus on the ground, we crop the images to use only the lower half as input. The MCRL4OR is pre-trained on a server (Ubuntu 22.04) with 6 GPUs (Nvidia Titan X, 12GB Memory).\nFor the task of cross-modal retrieval, we report the rank-1, rank-10, and rank-50 accuracies. A retrieval is deemed successful if the positive paired samples lies in the top-k ranking results.\nFor the task of off-road dynamics prediction, Adam (Kingma and Ba 2014) optimizer is used for training the dynamic prediction model. The batch size in fine-tunning is set to 64. Instance normalization (Ulyanov, Vedaldi, and Lempitsky 2016) is used to normalize the input of locomotion states. To address the non-stationary nature of the locomotion state, we predict the trajectory's differentiation between two adjacent frames instead of absolute trajectory. During evaluation, we accumulate the differentiations to obtain the predicted positions. The pre-trained locomotion state encoder is frozen and extracts historical locomotion state features as the initial hidden state of GRU, or as the input tokens of the encoder of Informer. The train/test split keeps the same as the setting in (Triest et al. 2022).\nFor the task of off-road semantic segmentation, the input image size is set to 640 \u00d7 352 and the batch size is set to 12. All other settings are kept the same settings as the ORFD (Min et al. 2022). The performance of the last model after 30 training epochs is reported."}, {"title": "Results", "content": "Cross-Modal Retrieval The rank-n accuracy of cross-model retrieval are reported in Tab. 1. Compared to the proposed alignment strategy (ours), the simple alignment strategy (mask a) has a drastic drop in retrieval performance on the test set, e.g., 41.73% vs. 5.31% in the rank-50 group. This exhibits a severe over-fitting of representations learned by the simple strategy, although the observation and locomotion state can be well-aligned in training phase.\nDynamics Prediction The results of dynamic prediction are shown in Table 2, where the RMSE denotes the errors of the predicted position and orientation.\nOff-Road Semantic Segmentation The evaluation results of semantic segmentation on the ORFD dataset (Min et al. 2022) are presented in Table 3."}, {"title": "Conclusion", "content": "In this paper, we propose MCRL4OR, a general-purpose pre-training approach to learning multimodal representations for supporting a variety of off-road perception tasks. The proposed alignment strategy aims to learn the intrinsic relationships between observations, locomotion states and control actions. Three downstream evaluation tasks have been performed comprehensively to validate the advantages of the pre-trained representations learned by the MCRL4OR. Experimental results show that the MCRL4OR can capture the dynamics of various terrains in off-road environments and improve the perception capability over different tasks."}, {"title": "Appendices", "content": "Following the data description in TartanDrive's github\nrepository, https://github.com/castacks/tartan_drive, and the\npaper of TartanDrive(Triest et al. 2022), the data registered\nin robot operating system (ROS) bag can be categorized into:\n\u2022 Action: robot action, 2-dimensional vectors corresponding to the throttle and steering positions;\n\u2022 State: robot pose, 7-dimensional vectors corresponding\nto x-y-z cartesian coordinate and quaternion;\n\u2022 RGB Image: images of terrain ahead of the car;\n\u2022 RGB Map: images of terrain ahead of the car generated\nfrom BEV view;\n\u2022 Height Map: depth information of RGB Map;\n\u2022 IMU: 6-dimensional vectors including angular velocity\naround 3 axis and linear acceleration in 3 axis;\n\u2022 Shock Pos: a 4 dimensional vector indicating the vibration/shock information of four wheels;\n\u2022 Wheel RPM: a 4 dimensional vector indicating the revolutions per minute of four wheels.;\n\u2022 Pedals: intervention data of brake pedal, including a brake pedal position, and a boolean intervention signal that indicated when the brakes exceeded a threshold."}, {"title": "Experiments Illustration", "content": "Given the diverse focus of off-road autonomous driving and the novel modality we use, vision & IMU sensors. It's not easy to directly conduct our experiments based on other's work. So we mainly develop based on the code of CLIP(Radford et al. 2021), then replace the encoders we need into the framework of CLIP.\nWe will clarify the settings of experiments in this section by the encoders we use and the aim of our experiments.\nThere are four pretraining experiments for our various downstream task and ablations. The expriments are shown as in Tab.5. All the pretraining experiments share the same hyper-parameters and converge during training."}, {"title": "Cross-Modal Retrieval", "content": "More cross-modal retrieval cases are show in Fig.5. In general, the variance of linear acceleration is correlated to the roughness of terrain, yet the velocity and throttle also affect the driving state."}, {"title": "Dynamics Prediction", "content": "The way we use locomotion state encoder in dynamics prediction is illustrated in Fig.6.\nGRU is an auto-regressive RNN-based model, also the baseline model in TartanDrive. It takes the action and the historical location motion state as the inputs and outputs the future position and orientation results. The locomotion state encoder in the pre-trained MCRL4OR model is adopted to generate the initial hidden state embedding of GRU from the past locomotion state.\nInformer is a transformer-based encoder-decoder architecture. It also takes the past state and action as the encoder's input, and outputs all positions and orientations at once. The locomotion state encoder in the pre-trained MCRL4OR model is adopted to transfer the past locomotion state into a state embedding. Then the embedding is further added to the tokens generated from both the past state and control action."}, {"title": "Semantic Segmentation", "content": "Because Swin-T (Liu et al. 2021) is adopted as the observation encoder in MCLR4OR pre-training, which is larger and more powerful backbone compared to the visual encoder in OFF-net, we further conduct ablations with only the vision modality."}, {"title": "Ablation on models with only RGB as input", "content": "We can find that the MCRL4OR pre-training on TartanDrive consistently improves the Swin-T for both initialization methods, except for a slight drop in the Acc. when using random initialization. Meanwhile, the sole MCLR4OR pre-training on TartanDrive with randomly initialized Swin-T does not outperform the OFF-Net baseline. This indicates that the pre-training on larger-scale datasets such as ImageNet is essential when employing the advanced transformer-based models (e.g., Swin-T) with a large parameter capacity.\nThe OFF-Net from ORFD(Min et al. 2022) is illustrated in Fig.7."}, {"title": "Limitation and Discussion", "content": "It is crucial to recognize the limitations of the current study and suggest promising avenues for future research.\nFirstly, the scalability of the TartanDrive (Triest et al. 2022) used for MCRL4OR pre-training are still limited, which might hinder the generalization capability of the learned representations. Thus, it is necessary to collect larger-scale and more diverse off-road driving datasets for more convincing model comparison.\nSecondly, the diversity of evaluation tasks remains limited. Performance saturation is observed in the tasks of dynamic prediction and semantic segmentation. It would be beneficial to delve into the capabilities of the MCRL4OR model for off-road navigation tasks, where MCRL4OR could function as a world model for a reinforcement learning (RL) agent. The agent could utilize the model to image the future locomotion states based on different actions taken when navigating off-road environments.\nFinally, high-fidelity driving simulators, e.g., Carla(Dosovitskiy et al. 2017), have been widely used for developing autonomous driving agents within a closed-loop training and test environment. Thus, researchers would profit from an off-road driving simulator that incorporates high-fidelity visual sensing and a physics engine to model dynamics when AVs traversing diverse terrain conditions."}]}