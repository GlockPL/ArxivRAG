{"title": "Using GPT-4 to guide causal machine learning", "authors": ["Anthony C. Constantinou", "Neville K. Kitson", "Alessio Zanga"], "abstract": "Since its introduction to the public, ChatGPT has had an unprecedented impact. While some experts praised AI advancements and highlighted their potential risks, others have been critical about the accuracy and usefulness of Large Language Models (LLMs). In this paper, we are interested in the ability of LLMs to identify causal relationships. We focus on the well-established GPT-4 (Turbo) and evaluate its performance under the most restrictive conditions, by isolating its ability to infer causal relationships based solely on the variable labels without being given any context, demonstrating the minimum level of effectiveness one can expect when it is provided with label-only information. We show that questionnaire participants judge the GPT-4 graphs as the most accurate in the evaluated categories, closely followed by knowledge graphs constructed by domain experts, with causal Machine Learning (ML) far behind. We use these results to highlight the important limitation of causal ML, which often produces causal graphs that violate common sense, affecting trust in them. However, we show that pairing GPT-4 with causal ML overcomes this limitation, resulting in graphical structures learnt from real data that align more closely with those identified by domain experts, compared to structures learnt by causal ML alone. Overall, our findings suggest that despite GPT-4 not being explicitly designed to reason causally, it can still be a valuable tool for causal representation, as it improves the causal discovery process of causal ML algorithms that are designed to do just that.", "sections": [{"title": "1. Introduction", "content": "Causal discovery moves beyond mere correlations to uncover the underlying causal mechanisms that drive observed phenomena. Determining a causal graph enables the parameterisation of causal models, such as a Causal Bayesian Network (CBN), which can then be used for causal inference and optimal decision-making under uncertainty through simulation of hypothetical interventions. A CBN is a probabilistic graphical model represented by a Directed Acyclic Graph (DAG), where nodes represent variables, and directed edges indicate causal relationships between these variables. Each node in a CBN is described by a Conditional Probability Distribution (CPD) that quantifies the effect of its parent nodes. This structure allows for the representation of complex causal relationships and the computation of joint conditional and marginal probability distributions.\n\nA CBN supports both backward and forward inference. For example, predicting effects such as symptoms given a cause such as disease, or inferring the most likely disease cause given observed symptoms. More importantly, causal models enable the simulation of hypothetical interventions and estimation of their effects before real-world implementation, which is crucial for decision support. For a comprehensive review of"}, {"title": "2. Methodology and experimental setup", "content": "Five case studies were selected from diverse domains for a more comprehensive evaluation. These are described in Table 1. We avoided selecting case studies incorporating hundreds of variables to ensure that a) the case studies are simple enough to enable questionnaire participants to review them, and b) the number of the variable labels can be processed by GPT-4, since there is a limit to the number of characters an input to LLMs can have, which varies with platforms and implementation versions."}, {"title": "a. Input preparation for GPT-4:", "content": "For each case study, we provide the labels of the variables as input to GPT-4, and ask GPT-4 to identify causal relationships between the labels. Specifically, GPT-4 was asked to specify a set of directed edges representing causal links between the input variables. No additional context or data was given to GPT-4, isolating its ability to infer causal relationships based solely on the labels. Moreover, because the way a question is posed to GPT- 4 may influence its output, we repeated this process 10 times for each case study using different prompts generated by GPT-4, as shown in Table 2."}, {"title": "2.2. Questionnaire", "content": "A questionnaire was designed for human participants to evaluate the different causal graphs produced by GPT-4 based on variable labels, causal ML based on data samples, and domain experts based on their subjective causal knowledge. A sample of the questionnaire is shown in Figure A.1, showing the first causal graph of the first case study. Participants were free to complete one or up to all five case studies. It was completely up to them to decide how many, and which, of the case studies they completed. This option was necessary to ensure that we did not force participants to complete case studies they were not be able to judge reasonably well. Moreover, we estimated that each case study required an average of 6 minutes to complete, which makes for a total of 30 minutes for those who decide to complete the questionnaire in full.\nThe questionnaire involved three causal graphs for each case study in Table 1, for a total of 15 causal graphs. The three graphs for each case study represent the following:"}, {"title": "a. Knowledge graphs:", "content": "These are the causal graphs elicited from domain experts. They are taken from the Bayesys repository (Constantinou, 2020), and are based on the knowledge graphs as published in the original studies."}, {"title": "b. Causal ML graphs:", "content": "These are causal graphs learnt with causal ML algorithms from real case study data. For the Diarrhoea and COVID-19 case studies, we took the learnt graphs from the original studies. The other three studies did not employ causal ML, so these graphs were not available. We, therefore, learnt the structures using and a set of algorithms spanning different classes of learning; i.e., score-based HC, Tabu, GES and MAHC, constraint-based PC-Stable, and hybrid MMHC and SaiyanH.\nHowever, because our aim here was to obtain a single DAG structure representative of causal ML, we performed model-averaging on the set of causal ML graphs learnt for each case study. We use a model-averaging process similar"}, {"title": "c. LLM graphs:", "content": "These are the causal graphs obtained by GPT-4 as described in subsection 2.1. Because we obtained 10 graphs per case study, we applied the same model-averaging process described in (b) above in order to retrieve a single DAG structure for each case study that is representative of the GPT-4 output.\n\nParticipants were shown the causal graphs and asked to specify whether they had been produced from domain knowledge, causal ML, or LLM. They were also asked to judge the accuracy of each graph. Answering these questions involved selecting one of four possible responses:"}, {"title": "3.2. Using GPT-4 to guide causal ML", "content": "As described in Section 2.3, we also test the usefulness of GPT-4 in terms of using its output as causal constraints to restrict or guide the search space of graphs explored by causal ML algorithms. The results presented here are based on four case studies involving real (not synthetic) datasets, as shown in Table 3. These results consider three different confidence levels of constraints, also described in Table 3, and three types of constraints: required edges, temporal order, and initial graph, as described in Section 2.3. Additionally, eight algorithms from different classes of learning, which support some or all of these types of constraints, are utilised as detailed in Table 4.\nFigure 3 presents the overall impact of GPT-4 constraints on structure learning. Specifically, the results measure the relative impact on the graphical structures learnt by the causal ML algorithms with real data, comparing scenarios with and without GPT-4 constraints, and with reference to the knowledge graph for each case study as determined by domain experts. Each sub-chart summarises the results using the different metrics of F1, BSF, SHD, and BIC scores, for each rate and type of constraint across all algorithms and case studies.\n\nThe F1, BSF, and SHD scores represent graphical metrics that measure the distance between two graphical structures. With reference to the confusion matrix, the SHD score considers the false positive and false negative edges between the two graphs, the F1 score includes those considered by SHD plus the true positive edges, and the BSF score further includes those considered by F1 plus the true negative edges. Note that because SHD does not account for true positive nor true negative edges, it is known to be biased in favour of sparser graphs. However, the SHD score is widely used in the literature, and while we present the SHD scores to enable cross-comparisons between studies, most of our focus will be on the F1 and BSF metrics. Lastly, in contrast to the graphical metrics, the BIC score is a model-selection function that estimates how well the learnt model, balances between data fitting and model dimensionality.\n\nThe results presented in Figure 3 show that all three graphical metrics agree that the GPT-4 constraints help the algorithms output a graphical structure that is closer to those produced by domain experts, compared to the corresponding graphical structures learnt without GPT-4 constraints. The results also indicate that, amongst the different types of constraints, the GPT-4 constraints are most effective when employed as required edge constraints, irrespective of the rate of constraints. The initial graph constraints do generate a positive effect too, but not as strong and consistent as required edge constraints, whereas the temporal constraints produce mixed results.\nFor required edge constraints, both the F1 and BSF scores show that the results are stronger at a 33% rate of constraints, implying that the constraints are more beneficial when extracted from the set of edges that appear in at least a third of the 10 GPT-4 prompts. This goes against our initial expectation, which expected the results to be stronger at a 67% rate of constraints, where the edges constrained are restricted to those that appear in at least two-thirds of the 10 GPT-4 prompts, thereby increasing the confidence in the set of constraints due to larger agreement between GPT-4 prompts. On the other hand, we observe the reverse effect for initial graph constraint, with mixed effect in other cases, and so this observation does not seem to be consistent across all types of constraints and metrics."}, {"title": "4. Concluding remarks", "content": "LLMs transform data and user input into numerical representations known as tokens. These tokens capture the sematic meaning of the words, and the trained models appear to understand context through layers of neural-network transformations. This process helps LLMs generate coherent and relevant response. Therefore, while LLMs are not designed to disentangle correlation from causation, they often produce output that appears to be causally valid due to their ability to recognise sophisticated patterns. This can create the impression that the models understand causality, but it important to highlight that their apparent causal reasoning is a byproduct of their training process rather than a true comprehension of causal relationships.\n\nStill, because the output of LLMs is now perceived to be much more causally valid than we would expect from an associational model, the role of causality in LLMs is becoming an area of significant debate. This study adds to this emerging field by exploring the usefulness of GPT-4 outputs in terms of causal reasoning, and comparing them to those derived from domain experts and those learnt from data using causal ML algorithms.\n\nWe first designed a questionnaire that asked participants to predict whether a presented graph was drawn by causal ML, LLM, or domain experts, and to judge the causal accuracy of the graph. The results (refer to Table 6) show that participants correctly identified causal ML graphs, but misclassified some LLM graphs as knowledge graphs and vice versa. Causal ML graphs were the easiest to classify, and this observation is attributed to counterintuitive edges that we would not expect a domain expert nor an LLM to produce (refer to Table 7). Moreover, participants consistently rated LLM graphs as being fairly more accurate than knowledge graphs elicited from domain experts, and much more accurate than causal ML graphs (refer to Table 5).\n\nGPT-4 has shown to be able to generate outputs for the case studies tested that are indistinguishable from, and often were judged by questionnaire participants as being more accurate than, those from domain experts. This might be because LLMs effectively summarise targeted human knowledge from sources that are assumed to be credible. This suggests that LLM outputs are likely to be valid, generating responses that are, or appear to be, well-informed. While some case studies tested in this paper might be part of GPT-4's training data, this cannot be confirmed. Regardless, this is not expected to impact performance, as LLMs like GPT-4 produce well-generalised outputs with an element of randomness from vast amounts of related examples and hence, it is not unreasonable to assume that removing a single example from its training process is unlikely to lead to significant changes in its output.\n\nWe also tested the usefulness of GPT-4 in terms of using its output as causal constraints to restrict the search space of graphs explored by causal ML algorithms. Through an extensive set of empirical experiments involving multiple case studies, causal ML algorithms, types of constraints, and quantities of constraints, the results show that GPT-4 consistently helps causal ML to produce graphical structures that are closer to those produced by domain experts, compared to the corresponding graphical structures learnt without GPT-4 constraints.\n\nOverall, our findings suggest that even though GPT-4 is not explicitly designed to reason causally, it can still be a valuable tool for causal representation. This is despite the fact that GPT-4 was provided with no domain context; it was given just a set of variable labels and asked to connect them causally. Note that the variable labels are meaningful to LLMs, but meaningless to causal ML since learn from data in an unsupervised manner. Therefore, these results potentially highlight the lowest possible performance one could expect from GPT-4 in terms of causal reasoning. Nonetheless, the results of this study suggest that GPT-4 potentially enhances current solutions to causal discovery. Despite these positive findings in favour of LLMs, and somewhat negative ones for causal ML, the latter is expected to be more effective in tackling previously unexplored problems where LLMs may struggle to generalise effectively.\n\nThis study comes with some limitations. Firstly, the questionnaire results, despite producing reasonably clear patterns, are based on 32 responses. This limited participation was partly due to the questionnaire not offering any payment and partly due"}]}