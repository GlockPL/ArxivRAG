{"title": "Using GPT-4 to guide causal machine learning", "authors": ["Anthony C. Constantinou", "Neville K. Kitson", "Alessio Zanga"], "abstract": "Since its introduction to the public, ChatGPT has had an unprecedented impact. While some experts praised AI advancements and highlighted their potential risks, others have been critical about the accuracy and usefulness of Large Language Models (LLMs). In this paper, we are interested in the ability of LLMs to identify causal relationships. We focus on the well-established GPT-4 (Turbo) and evaluate its performance under the most restrictive conditions, by isolating its ability to infer causal relationships based solely on the variable labels without being given any context, demonstrating the minimum level of effectiveness one can expect when it is provided with label-only information. We show that questionnaire participants judge the GPT-4 graphs as the most accurate in the evaluated categories, closely followed by knowledge graphs constructed by domain experts, with causal Machine Learning (ML) far behind. We use these results to highlight the important limitation of causal ML, which often produces causal graphs that violate common sense, affecting trust in them. However, we show that pairing GPT-4 with causal ML overcomes this limitation, resulting in graphical structures learnt from real data that align more closely with those identified by domain experts, compared to structures learnt by causal ML alone. Overall, our findings suggest that despite GPT-4 not being explicitly designed to reason causally, it can still be a valuable tool for causal representation, as it improves the causal discovery process of causal ML algorithms that are designed to do just that.", "sections": [{"title": "1. Introduction", "content": "Causal discovery moves beyond mere correlations to uncover the underlying causal mechanisms that drive observed phenomena. Determining a causal graph enables the parameterisation of causal models, such as a Causal Bayesian Network (CBN), which can then be used for causal inference and optimal decision-making under uncertainty through simulation of hypothetical interventions. A CBN is a probabilistic graphical model represented by a Directed Acyclic Graph (DAG), where nodes represent variables, and directed edges indicate causal relationships between these variables. Each node in a CBN is described by a Conditional Probability Distribution (CPD) that quantifies the effect of its parent nodes. This structure allows for the representation of complex causal relationships and the computation of joint conditional and marginal probability distributions.\nA CBN supports both backward and forward inference. For example, predicting effects such as symptoms given a cause such as disease, or inferring the most likely disease cause given observed symptoms. More importantly, causal models enable the simulation of hypothetical interventions and estimation of their effects before real-world implementation, which is crucial for decision support. For a comprehensive review of causal Machine Learning (ML) algorithms, we direct readers to Kitson et al. (2023) and Zanga et al. (2022).\nDespite their utility, causal ML algorithms face significant challenges that necessitate combining these algorithms with domain knowledge or interventional data. Two key limitations that are relevant to this study are:\na. Uncertainty in the number of edges: Causal ML algorithms often face significant challenges in accurately recovering the correct number of edges in a causal graph. One major limitation is their tendency to underestimate the number of edges when the sample size is low. This occurs because limited data can obscure subtle dependencies and causal relationships, leading to an unreasonably sparse graph. Conversely, when the sample size is high, these algorithms may overestimate the number of edges, often due to overfitting issues or inability to disentangle all spurious relationships from causal relationships. Consequently, the reliability of these algorithms can vary significantly with the sample size, impacting their effectiveness in accurately modelling causal structures.\nb. Incomplete orientation of edges: Causal ML algorithms typically do not orientate all the edges they discover. This limitation arises because observational data alone is generally insufficient to distinguish between different causal graphs, often requiring either interventional (also refer to as experimental) data for complete causal discovery, or additional strong assumptions which force edge orientations from observational data. In the absence of additional assumptions that force edge orientations irrespective of the input data, a causal ML algorithm typically employs an objective function that is score-equivalent, allocating the same score to any two DAG structures that are part of the same Markov Equivalence Class (MEC). A MEC of DAGs is a set of DAGs that entail the same conditional independencies, and each MEC is represented by a Completed Partially DAG (CPDAG). A CPDAG contains both directed and undirected edges, where a directed edge indicates that all of the DAGs within the MEC have the same orientation for that specific edge, whereas an undirected edge indicates a directional inconsistency between those DAGs.\nc. Irrational orientation of edges: Even when edges are orientated, some may be wrongly-orientated, and may even appear completely irrational to a human in that they disobey the fundamental tenets of causality. For instance, an algorithm might incorrectly suggest that Dance moves cause Music, or that Rainbow causes Rain. This is partly due to causal ML algorithms not being provided with key temporal information about the input variables; i.e., data indicating that event A occurs after observing B, and hence the constraint that B cannot cause A. While it has been argued that objective temporal information should form part of observational data in causal discovery (Constantinou, 2021), it is generally viewed as a form of optional subjective information that is overlooked, contributing to these orientational inaccuracies."}, {"title": "Large Language Models (LLMs)", "content": "Large Language Models (LLMs) represent a class of artificial intelligence models designed to understand and generate human-like text. These models are built on deep learning architectures, particularly transformers, which enable them to process and produce natural language text with remarkable accuracy and fluency. The most well- known example is OpenAl's ChatGPT, with iterations like GPT-2, GPT-3, and beyond setting new benchmarks for generating coherent and contextually relevant text. LLMs are not designed to disentangle correlation from causation, and some argue that it is crucial for LLMs to reason causally in order to generate logical inferences. Because LLMs do not reason causally by design, significant debate remains as to whether they merely generate restructured memorised information or go beyond that and towards some form of causal reasoning (Bubeck et al., 2023; Zhong et al., 2023; Zhou et al., 2024).\nSince the public release of GPT-3, there has been a growing interest in utilising LLMs for causal discovery, with studies highlighting conflicting conclusions about their causal reasoning capabilities. We begin with the papers that conclude that LLMs are mostly inadequate in terms of causal reasoning. These include Jin et al. (2024) who evaluated 17 LLMs on causal inference skills and found that these models \"achieve almost close to random performance\u201d. Zhou et al. (2024) explored criteria for benchmarking the causal learning capabilities of LLMs and concluded that \u201ceven the most advanced LLMs do not yet match the performance of classic and SOTA methods in causal learning\u201d. They illustrated that while LLMs can compete with state-of-the-art (SOTA) methods when the problem relies on small datasets, their effectiveness diminishes with larger datasets. Long et al. (2024) showed that the accuracy of GPT-3 in causal discovery depends on the language used by the user to describe the relationship between two events, concluding that \u201cthe use of LLMs to build DAGs should be, at present, only conducted with expert verification\u201d. Zhang et al. (2023) suggested that while LLMs can answer causal questions based on existing knowledge, they are still incapable of providing satisfactory answers to problems involving new knowledge. Pawlowski et al. (2024) demonstrated that neither context-augmented LLMs, that are given the non- parameterised causal graph, nor API-augmented LLMs that are given the parameterised causal model, can correctly solve causal question-answering tasks. Tu et al. (2023) tested the ability of ChatGPT to answer causal discovery questions about a neuropathic pain diagnosis case study, and showed that while ChatGPT is good at correctly discovering true positives, it is poor at correctly identifying false negative causal relationships. Lastly, Ze\u010devi\u0107 et al. (2023) focused on experiments with Structural Causal Models (SCMs) to illustrate and argue that LLMs not only cannot reason causally, but are also weak 'causal parrots'.\nIn contrast to the above studies that highlight the inability of LLMs to reason causally, other research indicates that LLMs are adequate in producing causal graphs. For instance, Kiciman et al. (2023) studied the capabilities of LLMs on various causal reasoning tasks and found that algorithms based on GPT-3.5 and GPT-4 \u201coutperform state-of-the-art causal algorithms in graph discovery and counterfactual inference\u201d. Lyu et al. (2022) explored the capability of LLMs in establishing causality between two variables at a time, which is not generally feasible for causal discovery algorithms that do not make additional assumptions to force orientations (i.e., some algorithms claim to be able to orientate all edges, but this requires strong assumptions about the nature of noise in the data), and showed that LLMs are effective in distinguishing cause from effect. Long et al. (2023) demonstrated that LLMs can serve as imperfect domain experts, helping causal discovery algorithms to select the correct DAG from a MEC. Zhang et al. (2024) showed that pairing LLMs with Retrieval Augmented-Generation (RAG) solutions enables them to recover causal graphs that are more accurate than those learnt by causal ML. Lastly, Antonucci et al. (2023) found that LLMs are competitive in inferring causal relationships with traditional natural language processing and deep learning techniques.\nIn this paper, we use a questionnaire to gather data from human participants on their ability to identify whether LLMs, causal ML, or domain experts constructed the presented causal graphs, and to evaluate and comment on their accuracy. Additionally, we investigate whether the causal relationships extracted from GPT-4 (Turbo) can address some of the current limitations in causal ML, with a focus on the two limitations discussed above in incomplete and irrational edge orientations. The paper is structured as follows: Section 2 describes the methodology and experimental setup, Section 3 presents and discusses the results, and Section 4 provides our concluding remarks, highlighting limitations and future research directions."}, {"title": "2. Methodology and experimental setup", "content": "Five case studies were selected from diverse domains for a more comprehensive evaluation. These are described in Table 1. We avoided selecting case studies incorporating hundreds of variables to ensure that a) the case studies are simple enough to enable questionnaire participants to review them, and b) the number of the variable labels can be processed by GPT-4, since there is a limit to the number of characters an input to LLMs can have, which varies with platforms and implementation versions."}, {"title": "2.1. Case studies as input to GPT-4 (Turbo)", "content": "a. Input preparation for GPT-4: For each case study, we provide the labels of the variables as input to GPT-4, and ask GPT-4 to identify causal relationships between the labels. Specifically, GPT-4 was asked to specify a set of directed edges representing causal links between the input variables. No additional context or data was given to GPT-4, isolating its ability to infer causal relationships based solely on the labels. Moreover, because the way a question is posed to GPT-4 may influence its output, we repeated this process 10 times for each case study using different prompts generated by GPT-4, as shown in Table 2."}, {"title": "2.2. Questionnaire", "content": "A questionnaire was designed for human participants to evaluate the different causal graphs produced by GPT-4 based on variable labels, causal ML based on data samples, and domain experts based on their subjective causal knowledge. A sample of the questionnaire is shown in Figure A.1, showing the first causal graph of the first case study. Participants were free to complete one or up to all five case studies. It was completely up to them to decide how many, and which, of the case studies they completed. This option was necessary to ensure that we did not force participants to complete case studies they were not be able to judge reasonably well. Moreover, we estimated that each case study required an average of 6 minutes to complete, which makes for a total of 30 minutes for those who decide to complete the questionnaire in full.\nThe questionnaire involved three causal graphs for each case study in Table 1, for a total of 15 causal graphs. The three graphs for each case study represent the following:\na. Knowledge graphs: These are the causal graphs elicited from domain experts. They are taken from the Bayesys repository (Constantinou, 2020), and are based on the knowledge graphs as published in the original studies.\nb. Causal ML graphs: These are causal graphs learnt with causal ML algorithms from real case study data. For the Diarrhoea and COVID-19 case studies, we took the learnt graphs from the original studies. The other three studies did not employ causal ML, so these graphs were not available. We, therefore, learnt the structures using and a set of algorithms spanning different classes of learning; i.e., score-based HC, Tabu, GES and MAHC, constraint-based PC-Stable, and hybrid MMHC and SaiyanH.\nHowever, because our aim here was to obtain a single DAG structure representative of causal ML, we performed model-averaging on the set of causal ML graphs learnt for each case study. We use a model-averaging process similar to (Petrungaro et al., 2024; Zahoor et al., 2024; Constantinou et al., 2023), where the average graph contains all the edges that appear in at least two thirds of the graphs in the input set of learnt graphs, as long as an edge added to the average graph - starting from the directed edges that appear the most times within the set of graphs - does not produce a cycle.\nc. LLM graphs: These are the causal graphs obtained by GPT-4 as described in subsection 2.1. Because we obtained 10 graphs per case study, we applied the same model-averaging process described in (b) above in order to retrieve a single DAG structure for each case study that is representative of the GPT-4 output.\nParticipants were shown the causal graphs and asked to specify whether they had been produced from domain knowledge, causal ML, or LLM. They were also asked to judge the accuracy of each graph. Answering these questions involved selecting one of four possible responses:\na. Very Likely, Likely, Unlikely, and Very Unlikely, in determining whether a graph was constructed by human experts, causal ML, or LLM;\nb. Very Accurate, Mostly Accurate, Mostly Inaccurate, and Very Inaccurate, in judging the accuracy of a causal graph.\nThe participants were also given the option to comment on each graph presented to them. Key comments left by participants are presented in Table 7 and are discussed in Section 3."}, {"title": "2.3. Using LLMs to guide causal ML", "content": "Unlike the common practice of evaluating causal ML algorithms with synthetic data due to the absence of real-world ground truth graphs, this paper investigates whether LLMs can assist causal ML in learning graphs that more closely align with those constructed by domain experts. Specifically, we investigate the usefulness of the causal relationships generated by GPT-4 in terms of guiding causal ML algorithms when learning from real data. We employ a systematic approach that involves multiple algorithms across different classes of structure learning, and test those algorithms on real case-study data with and without GPT-4 constraints, with different quantities of constraints.\nWe begin by describing how we convert GPT-4 outputs into constraints. As discussed in Section 2.1, the variable labels are provided as input to GPT-4 using 10 different prompts, leading to 10 GPT-4 outputs. We take those 10 outputs, for each case study, and record the edges that appear in at least a third (33%), a half (50%), and two thirds (67%) of those 10 outputs. These differing number of edges are reflected by the different numbers of constraints, so that we assess the robustness and consistency of the causal relationships proposed by GPT-4 across different levels of confidence. Table 3 presents the results by repeating this across all five case studies, leading to 15 different sets of edges. Note that for case study ForMed, there was no edge that appeared in at least two thirds of the 10 outputs and hence, no edge-set is generated for that case.\nWe then take each set of edges specified in Table 3, and convert it into three different types of constraints that could be used to guide structure learning algorithms. These are:\na. Required edges: explicitly define the directionality of causal links between variables, where the search space of graphs is restricted to structures containing the specified directed edges.\nb. Initial graph: also known as starting graph, represents the starting point for exploration in the search-space of graphs. For most algorithms, the starting point is typically an empty, a fully connected, or a random structure. When the set of constraints is given as an initial graph, then the starting point in the search space is the structure specified in the set of constraints.\nc. Temporal order: also known as temporal edge tiers, ensures that the temporal order of events was respected, preventing causal directions that contradict temporal sequences. Specifically, the search space of graphs explored is restricted to graphical structures that satisfy the temporal constraints, converted from the set of required edge constraints. For example, if A \u2192 B and B \u2192 C appear in a set of constraints, these two edges alone would produce multiple temporal constraints; i.e., B cannot a parent nor an ancestor of A (although not all implementations impose restrictions on ancestral relationships), and C cannot be a parent nor an ancestor of neither A nor B. In this case, the search-space of graphical structures is restricted to DAGs that do not violate any of the temporal orderings implied by the set of required edge constraints.\nTo enforce these constraints, we selected algorithmic implementations that support structure learning with constraints on discrete data. Table 4 lists these algorithms, their class of learning and implementation details. We used the constraint- based PC-Stable algorithm, the score-based Fast Greedy Equivalence Search (FGES), Hill-Climbing (HC), TABU, and Model-Averaging Hill-Climbing (MAHC) algorithms, and the hybrid Max-Min Hill Climbing (MMHC) and SaiyanH algorithms."}, {"title": "3. Results and Discussion", "content": "The results are separated into two subsections. The first part focuses on the questionnaire outcomes, while the second part focuses on structure learning outcomes."}, {"title": "3.1. Questionnaire outcomes", "content": "We invited approximately 200 MSc students, 300 PhD students, and 1,000 LinkedIn connections to complete the questionnaire. The 200 MSc students invited were enrolled in the post-graduate Data Analytics course at Queen Mary University of London (QMUL), where 40% of the material is based on causal ML. The 300 PhD students invited were enrolled in the School of Electronic Engineering and Computer Science at QMUL. The 1,000 LinkedIn connections invited included academics and industry professionals across different disciplines.\nWe received 32 responses from 11\u00b9 different universities or organisations, resulting in a response rate of approximately 2.13%. The rather low response rate may be partly explained by the fact that this questionnaire was unfunded and so the respondents were not offered any payment for their participation. Additionally, the questionnaire took a relatively long time to complete, with an estimated six minutes per case study, resulting in a maximum total of 30 minutes for those who chose to complete all five case studies. Figure 1 shows the distribution of the responses by the participants' expertise or knowledge in a pre-determined set of domains relevant to the case studies.\nDespite the rather limited number of responses, consistent patterns emerged across all five case studies. As presented in Table 5 and detailed in Figure 2, the questionnaire responses suggest that GPT-4 is the most reliable method for achieving higher accuracy in the evaluated categories, closely followed by knowledge graphs, with causal ML far behind. Specifically, as shown in Table 5, GPT-4 was consistently judged by participants as the highest for accuracy scores, with knowledge graphs generally close to those of GPT-4. The graphs learnt by causal ML, however, received the lowest accuracy across all categories.\nThese results do not necessarily suggest that causal ML is less effective than the other two methods in this context. Instead, they highlight and support the important limitation of causal ML discussed in the introduction, in that they often produce causal relationships that are counterintuitive, which is not something we would expect from a domain expert or LLM. As shown in Table 7, which presents some of the key optional comments provided by participants, most of them commented negatively on the graphs learnt by causal ML, and say that the graphical structures tend to be sparse or too simplistic, some relationships seem counterintuitive or wrong, some edge orientations appear to be incorrect, and some key relationships are missed.\nPresumably, these observations helped the questionnaire participants to more accurately identify the graphical structures generated by causal ML. As shown in Table 6, all five causal ML graphs were correctly identified by the average participant. On the other hand, most of the knowledge graphs were incorrectly identified as LLM graphs, whereas most LLM graphs were incorrectly identified as knowledge graphs. Overall, the results suggest that the participants were accurate in identifying graphical structures learnt with causal ML, but they were partly correct in identifying knowledge graphs and LLM graphs, often confusing a knowledge graph as an LLM graph and vice-versa."}, {"title": "3.2. Using GPT-4 to guide causal ML", "content": "As described in Section 2.3, we also test the usefulness of GPT-4 in terms of using its output as causal constraints to restrict or guide the search space of graphs explored by causal ML algorithms. The results presented here are based on four case studies involving real (not synthetic) datasets, as shown in Table 3. These results consider three different confidence levels of constraints, also described in Table 3, and three types of constraints: required edges, temporal order, and initial graph, as described in Section 2.3. Additionally, eight algorithms from different classes of learning, which support some or all of these types of constraints, are utilised as detailed in Table 4.\nFigure 3 presents the overall impact of GPT-4 constraints on structure learning. Specifically, the results measure the relative impact on the graphical structures learnt by the causal ML algorithms with real data, comparing scenarios with and without GPT-4 constraints, and with reference to the knowledge graph for each case study as determined by domain experts. Each sub-chart summarises the results using the different metrics of F1, BSF, SHD, and BIC scores, for each rate and type of constraint across all algorithms and case studies.\nThe F1, BSF, and SHD scores represent graphical metrics that measure the distance between two graphical structures. With reference to the confusion matrix, the SHD score considers the false positive and false negative edges between the two graphs, the F1 score includes those considered by SHD plus the true positive edges, and the BSF score further includes those considered by F1 plus the true negative edges. Note that because SHD does not account for true positive nor true negative edges, it is known to be biased in favour of sparser graphs. However, the SHD score is widely used in the literature, and while we present the SHD scores to enable cross-comparisons between studies, most of our focus will be on the F1 and BSF metrics. Lastly, in contrast to the graphical metrics, the BIC score is a model-selection function that estimates how well the learnt model, balances between data fitting and model dimensionality.\nThe results presented in Figure 3 show that all three graphical metrics agree that the GPT-4 constraints help the algorithms output a graphical structure that is closer to those produced by domain experts, compared to the corresponding graphical structures learnt without GPT-4 constraints. The results also indicate that, amongst the different types of constraints, the GPT-4 constraints are most effective when employed as required edge constraints, irrespective of the rate of constraints. The initial graph constraints do generate a positive effect too, but not as strong and consistent as required edge constraints, whereas the temporal constraints produce mixed results.\nFor required edge constraints, both the F1 and BSF scores show that the results are stronger at a 33% rate of constraints, implying that the constraints are more beneficial when extracted from the set of edges that appear in at least a third of the 10 GPT-4 prompts. This goes against our initial expectation, which expected the results to be stronger at a 67% rate of constraints, where the edges constrained are restricted to those that appear in at least two-thirds of the 10 GPT-4 prompts, thereby increasing the confidence in the set of constraints due to larger agreement between GPT-4 prompts. On the other hand, we observe the reverse effect for initial graph constraint, with mixed effect in other cases, and so this observation does not seem to be consistent across all types of constraints and metrics.\nThe BIC score, on the other hand, decreases across all cases. This is not necessarily surprising since the score-based algorithms are designed to find optimal or close-to-optimal structures that maximise the BIC objective score. This means that the added constraints prohibit the algorithms from exploring parts of the search space that may contain a higher objective score. For example. notice how the higher numbers of constraints, in the 33% and 50% cases, tend to decrease the BIC score faster than when the quantity of constraints is lower, as in the 67% case. While it may be counterintuitive for constraints to increase graphical scores but decrease model-selection scores, it is consistent with previous studies that show that the graphs constructed by domain experts often yield BIC scores that are distant from the optimal graph \u2013 as judged by BIC within the search space of graphical structures. This discrepancy arises because knowledge-based graphs tend to overlook model dimensionality, and this study further highlights the weaknesses of traditional objective functions in recovering graphical structures that align with expertly-constructed causal graphs.\nFigure 4 presents the range of graphical scores produced across the different structure learning settings using box-plots. Unlike Figure 3 which contradicted our initial expectations \u2013 that fewer, more \u2018certain' GPT-4 constraints (at the 67% threshold) would be more effective than more, less \u2018certain' constraints - Figure 4 partly supports this expectation. This is because it shows that the fewer constraints generated at 67% threshold effectively limit the number of low graphical scores. However, these fewer constraints do not lead to the higher graphical scores observed at 33% and 50% thresholds, explaining why Figure 3 supports these lower thresholds. Overall, the 67% threshold seems to reduce the variability of the results, effectively avoiding the lowest scores but also failing to reach the highest scores."}, {"title": "4. Concluding remarks", "content": "LLMs transform data and user input into numerical representations known as tokens. These tokens capture the sematic meaning of the words, and the trained models appear to understand context through layers of neural-network transformations. This process helps LLMs generate coherent and relevant response. Therefore, while LLMs are not designed to disentangle correlation from causation, they often produce output that appears to be causally valid due to their ability to recognise sophisticated patterns. This can create the impression that the models understand causality, but it important to highlight that their apparent causal reasoning is a byproduct of their training process rather than a true comprehension of causal relationships.\nStill, because the output of LLMs is now perceived to be much more causally valid than we would expect from an associational model, the role of causality in LLMs is becoming an area of significant debate. This study adds to this emerging field by exploring the usefulness of GPT-4 outputs in terms of causal reasoning, and comparing them to those derived from domain experts and those learnt from data using causal ML algorithms.\nWe first designed a questionnaire that asked participants to predict whether a presented graph was drawn by causal ML, LLM, or domain experts, and to judge the causal accuracy of the graph. The results (refer to Table 6) show that participants correctly identified causal ML graphs, but misclassified some LLM graphs as knowledge graphs and vice versa. Causal ML graphs were the easiest to classify, and this observation is attributed to counterintuitive edges that we would not expect a domain expert nor an LLM to produce (refer to Table 7). Moreover, participants consistently rated LLM graphs as being fairly more accurate than knowledge graphs elicited from domain experts, and much more accurate than causal ML graphs (refer to Table 5).\nGPT-4 has shown to be able to generate outputs for the case studies tested that are indistinguishable from, and often were judged by questionnaire participants as being more accurate than, those from domain experts. This might be because LLMs effectively summarise targeted human knowledge from sources that are assumed to be credible. This suggests that LLM outputs are likely to be valid, generating responses that are, or appear to be, well-informed. While some case studies tested in this paper might be part of GPT-4's training data, this cannot be confirmed. Regardless, this is not expected to impact performance, as LLMs like GPT-4 produce well-generalised outputs with an element of randomness from vast amounts of related examples and hence, it is not unreasonable to assume that removing a single example from its training process is unlikely to lead to significant changes in its output.\nWe also tested the usefulness of GPT-4 in terms of using its output as causal constraints to restrict the search space of graphs explored by causal ML algorithms. Through an extensive set of empirical experiments involving multiple case studies, causal ML algorithms, types of constraints, and quantities of constraints, the results show that GPT-4 consistently helps causal ML to produce graphical structures that are closer to those produced by domain experts, compared to the corresponding graphical structures learnt without GPT-4 constraints.\nOverall, our findings suggest that even though GPT-4 is not explicitly designed to reason causally, it can still be a valuable tool for causal representation. This is despite the fact that GPT-4 was provided with no domain context; it was given just a set of variable labels and asked to connect them causally. Note that the variable labels are meaningful to LLMs, but meaningless to causal ML since learn from data in an unsupervised manner. Therefore, these results potentially highlight the lowest possible performance one could expect from GPT-4 in terms of causal reasoning. Nonetheless, the results of this study suggest that GPT-4 potentially enhances current solutions to causal discovery. Despite these positive findings in favour of LLMs, and somewhat negative ones for causal ML, the latter is expected to be more effective in tackling previously unexplored problems where LLMs may struggle to generalise effectively.\nThis study comes with some limitations. Firstly, the questionnaire results, despite producing reasonably clear patterns, are based on 32 responses. This limited participation was partly due to the questionnaire not offering any payment and partly due to the length of the questionnaire, which required approximately 30 minutes for those who chose to complete in full. Secondly, the empirical experiments are restricted to case studies of small to moderate complexity, containing up to 56 variables. This limitation was necessary for the questionnaire to be readable and for the GPT-4 prompts and outputs to handle the number of variables reasonably well. Therefore, the results presented in this paper may or may not extend to problems of higher complexity."}]}