{"title": "An Effective Dynamic Gradient Calibration Method for Continual Learning", "authors": ["Weichen Lin", "Jiaxiang Chen", "Ruomin Huang", "Hu Ding"], "abstract": "Continual learning (CL) is a fundamental topic in machine learning, where the goal is to train a model with continuously incoming data and tasks. Due to the memory limit, we cannot store all the historical data, and therefore confront the \"catastrophic forgetting\" problem, i.e., the performance on the previous tasks can substantially decrease because of the missing information in the latter period. Though a number of elegant methods have been proposed, the catastrophic forgetting phenomenon still cannot be well avoided in practice. In this paper, we study the problem from the gradient perspective, where our aim is to develop an effective algorithm to calibrate the gradient in each updating step of the model; namely, our goal is to guide the model to be updated in the right direction under the situation that a large amount of historical data are unavailable. Our idea is partly inspired by the seminal stochastic variance reduction methods (e.g., SVRG and SAGA) for reducing the variance of gradient estimation in stochastic gradient descent algorithms. Another benefit is that our approach can be used as a general tool, which is able to be incorporated with several existing popular CL methods to achieve better performance. We also conduct a set of experiments on several benchmark datasets to evaluate the performance in practice.", "sections": [{"title": "1. Introduction", "content": "In the past years, Deep Neural Networks (DNNs) demonstrate remarkable performance for many different tasks in artificial intelligence, such as image generation, classification, and pattern recognition. Usually we assume that the whole training data is stored in our facility and the DNN models can be trained offline by using Stochastic Gradient Descent (SGD) algorithms. However, real-world applications often require us to consider training lifelong models, where the tasks and data are accumulated in a streaming fashion. For example, with the popularity of smart devices, a large amount of new data is generated every day. A model needs to make full use of these new data to improve its performance while keeping old knowledge from being forgotten. Those applications motivate us to study the problem of continual learning (CL), where its goal is to develop effective method for gleaning insights from current data while retaining information from prior training data.\nA significant challenge that CL encounters is \"catastrophic forgetting\", wherein the exclusive focus on the current set of examples could result in a dramatic deterioration in the performance on previously learned data. This phenomenon is primarily attributed to limited storage and computational resources during the training process; otherwise, one could directly train the model from scratch using all the saved data. To address this issue, we need to develop efficient algorithm for training neural networks from a continuous stream of non-i.i.d. samples, with the goal of mitigating catastrophic forgetting while effectively managing computational costs and memory footprint.\nA number of elegant CL methods have been proposed to alleviate the catastrophic forgetting issue. One representative CL approach is referred to as \u201cExperience Replay (ER)\u201d, which has shown promising performance in several continual learning scenarios. Roughly speaking, the ER method utilizes reservoir sampling to maintain historical data in the buffer, then extract new incoming training data with random samplings for learning the current task. Though the intuition is simple, the ER method currently is one of the most popular CL approaches that incurs moderate computational and storage demands. Moreover, several recently proposed approaches suggest that the ER method can be combined with knowledge distillation to further improve the performance; for example, the methods of DER/DER++ and X-DER preserve previous training samples alongside their logits in the model as the additional prior knowledge. Besides the ER methods, there are also several other types of CL techniques proposed in recent years, and please refer to Section 1.2 for a detailed introduction."}, {"title": "1.1. Our Main Ideas and Contributions", "content": "Though existing CL methods can alleviate the catastrophic forgetting issue from various aspects, the practical performances in some scenarios are still not quite satisfying. In this paper, we study the continual learning problem from the gradient perspective, and the rationality behind is as follows. In essence, an approach for avoiding the catastrophic forgetting issue in CL, e.g., the replay mechanisms or the regularization strategies, ultimately manifests its influence on the gradient directions during model updating. If all the historical data are available, one could compute the gradient by using the stochastic gradient descent method and obviously the catastrophic forgetting phenomenon cannot happen. The previous methodologies aim to approximate the gradient by preserving additional information and incorporating it as a constraint to model updates, thereby retaining historical knowledge. However, the replay-based methods in practice are often limited by storage capacity, which leads to a substantial loss of historical data information and inaccurate estimation of historical gradients. Therefore, our goal is to develop a more accurate gradient calibration algorithm in each step of the continual learning procedure, which can directly enhance the training quality.\nWe are aware of several existing CL methods that also take into account of the gradients, but our idea proposed here is fundamentally different. We draw inspiration from the seminal stochastic variance reduction methods (e.g., SVRG from and SAGA from), which are originally designed to reduce the gradient variance so that the estimated gradient can closely align with the true full gradient over the entire dataset (including the current and historical data). These variance reduction methods have been extensively studied in the line of the research on stochastic gradient descent method; their key idea is to leverage the additional saved full gradient information to calibrate the gradient in the current training step, which leads to significantly reduced gradient variance comparing with the standard SGD method. This intuition also inspires us to handle the CL problem. In a standard SGD method, the variance between the obtained gradient and the full gradient is due to the \"batch size limit\" (if the batch size has no bound, we can simply compute the full gradient). Recall that the challenge of CL is due to the \u201cbuffer size limit\u201d, which impedes the use of full historical data (this is similar with the dilemma encountered by SGD with the \"batch size limit\"). So an interesting question is\nCan the calibration idea for \"batch size limit\" be modified to handle \"buffer size limit\"? Specifically, is it possible to develop an effecitve method to compute a SVRG (or SAGA)-like calibration for the gradient in CL scenarios?\nObviously, it is challenging to directly implement the SVRG or SAGA algorithms in continual learning because of the missing historical data. Note that proposed a streaming SVRG (SSVRG) method that realizes the SVRG method within a given fixed buffer, but unfortunately it does not perform quite well in the CL scenarios (as shown in our experimental section). One possible reason is that SSVRG can only leverage information within the buffer and fails to utilize all historical information.\nIn this paper, we aim to apply the intuition of SVRG to handle the \"buffer size limit\" in CL, and our contributions can be summarized as follows:\n\u2022 First, we propose a novel two-level dynamic algorithm, named Dynamic Gradient Calibration (DGC), to maintain a gradient calibration in continual learning. DGC can effectively tackle the storage limit and leverage historical data to calibrate the gradient of the model at each current stage. Moreover, our theoretical analysis shows that our DGC based CL algorithm can achieve a linear convergence rate.\n\u2022 Second, our method can be conveniently integrated with most existing reservoir sampling-based continual learning approaches (e.g., ER, DER/DER++, XDER, and Dynamic ER), where this hybrid framework can induce a significant enhancement to the overall model performance. Note that storing the gradient calibrator can cause an extra memory footprint; so a key question is whether such an extra memory footprint can yield a larger marginal benefit than simply taking more samples to fill the extra memory? In Fig 1, we illustrate a brief example to answer this question in the affirmative; more detailed evaluations are shown in the experimental part.\n\u2022 Finally, we conduct a set of comprehensive experiments on the popular datasets S-CIFAR10, S-CIFAR100 and S-TinyImageNet; the experimental results suggest that our method can improve the final Average Incremental Accuracy (FAIA) in several CL scenarios by more than 6%. Moreover, our improvement for a larger number of tasks is more significant than that for a smaller number. Furthermore, our adoption of the SVRG-inspired DGC calibration method leads to enhanced stability in minimizing the loss function throughout the parameter optimization process."}, {"title": "1.2. Related Work", "content": "We briefly overview existing important continual learning approaches (except for the ones mentioned before). We also refer the reader to the recent surveys for more details.\nA large number of CL methods are replay based, where they often keep a part of previous data through approaches like reservoir sampling. Several more advanced data selection strategies focus on optimizing the factors like the sample diversity of parameter gradients or the similarity to previous gradients on passed data, e.g., GSS and GCR. Experience replay can be effectively combined with knowledge distillation. For example, proposed to distill colliding effects from the features for new coming tasks, and ICARL proposed to take account of the data representation trained on old data. MOCA improves replay-based methods by diversifying representations in the space. Another replay-based approach is based on generative replay, which obtains replay data by generative models.\nAnother way for solving continual learning is through some deliberately designed optimization procedures. For example, the methods GEM, AGEM, and MER restrict parameter updates to align with the experience replay direction, and thereby preserve the previous input and gradient space with old training samples. Different from saving old training samples, proposed to adapt parameter updates in the orthogonal direction of the previously saved gradient. The method AOP projects the gradient in the direction orthogonal to the subspace spanned by all previous task inputs, therefore it only keeps an orthogonal projector rather than storing previous data.\nTo mitigate the problem of forgetting, we can also augment the model capacity for learning new tasks. tried to enhance model performance by employing meta-learning techniques when dynamically extending the model. The method ANCL proposes to utilize an auxiliary network to achieve a trade-off between plasticity and stability. Dynamic ER introduces a novel two-stage learning method that employs a dynamically expandable representation for learning knowledge incrementally."}, {"title": "2. Preliminaries", "content": "We consider the task of training a soft-classification function f(\u00b7; \u03b8): X \u2192 Y, where X and Y respectively represent the space of data and the set of labels, and \u03b8 is the parameter to optimize. Without loss of generality, we assume y = {1,2,\u2026\u2026\u2026, K}. So f(\u00b7; \u03b8) maps each x \u2208 X to some f(x;\u03b8) \u2208 RK. To find an appropriate \u03b8, the classification function f is usually equipped with a loss function l(f(x; \u03b8), y), which is differentiable for the variables x and \u03b8 (e.g., cross-entropy loss). To simplify the notation, we use l(x, y, \u03b8) to denote l(f(x; \u03b8), y). Given a set of data P = {(xi, yi) | 1 \u2264 i \u2264 n} = Xp \u00d7 Yp \u2282 X \u00d7 Y, the training process is to find a \u03b8 such that the empirical risk of l(x, y, \u03b8), i.e., \u03a3i=1n l(xi, yi, \u03b8), is minimized. We define the full gradient of l(x, y, \u03b8) on P as\n$\\begin{equation}\\label{eq:fullgradient}\\mathcal{G}(\\mathcal{P},\\theta) \\triangleq \\nabla_\\theta L(\\mathcal{X}_\\mathcal{P}, \\mathcal{Y}_\\mathcal{P}, \\theta) = \\frac{1}{n} \\sum_{i=1}^n \\nabla_\\theta l(x^i,y^i,\\theta). \\\\ \\end{equation}$"}, {"title": "2.1. Continual Learning Models", "content": "In this paper, we focus on two popular CL models: Class-Incremental Learning (CIL) and Task-Free Continual Learning (TFCL).\nIn the setting of CIL, the training tasks come in a sequence {T1, T2,\u2026, TT} with disjoint label space; each time spot t\u2208 {1,2,\u2026,T} corresponds to the task Tt with a training dataset {(xi, yi) | 1 \u2264 i \u2264 nt}. With a slight abuse of notations, we also use Tt to denote its training dataset. Also, we use Vt to denote the corresponding set of labels {y | 1 \u2264 i \u2264 nt}. Although the training data for individual tasks Tt is independently and identically distributed (i.i.d.), it is worth noting that the overall task stream {T1, T2,\u2026, TT} does not adhere to the i.i.d. assumption due to the evolving label space over time. We define the overall risk under this CL setting at the current time spot t as follows:\n$\\begin{equation}\\label{eq:cl_risk}\\mathcal{R}_{t}^{C L}(\\theta) \\triangleq \\sum_{c=1}^{t} \\underset{(x,y)\\sim T_c}{\\mathbb{E}}[l(x, y, \\theta)].\\\\ \\end{equation}$\nThe setting of TFCL is similar to CIL, where the major difference is that the task identities are not provided in neither the training nor testing procedures. So the TFCL setting is more challenging than CIL because the algorithm is unaware of task changes and the current task identity. In the main part of our paper, we present our results in CIL; in our supplement, we explain how to extend our results to TFCL."}, {"title": "2.2. Variance Reduction Methods", "content": "A comprehensive introduction to variance reduction methods is provided in. Here we particularly introduce one representative variance reduction method SVRG, which is closely related to our proposed CL approach.\nThe high-level idea of SVRG is to construct a calibration term to reduce the variance in the gradient estimate. The complete optimization process can be segmented into a sequence of stages. We denote the training data as P = {(xi, yi)}i=1n \u2282 X \u00d7 Y, and denote the parameter at the beginning of each stage as \u03b8. The key part of SVRG is to minimize the variance in SGD optimization by computing an additional term \u03bc:\n$\\begin{equation}\\label{eq:auxiliary}\\mu \\triangleq \\frac{1}{n} \\sum_{i=1}^n \\nabla_\\theta l(x^i,y^i, \\widetilde{\\theta}).\\\\ \\end{equation}$\nIn each stage, SVRG applies the standard SGD with the term \u03bc to update the parameter \u03b8: let\n$\\begin{equation}\\label{eq:variance}\\nu^{k} = \\nabla_\\theta l(x^{i},y^{i}, \\theta^k) - (\\nabla_\\theta l(x^{i},y^{i}, \\widetilde{\\theta}) - \\mu),\\\\ \\end{equation}$\n$\\begin{equation}\\label{eq:parameter_update}\\theta^{k+1} = \\theta^{k} - \\eta \\nu^{k},\\\\ \\end{equation}$\nwhere (xi, yi) \u2208 P is the sampled training data, \u03b8k represents the parameter at the k-th step of SGD, and \u03b7 is the learning rate. Since E[\u2207\u03b8l(xi, yi, \u03b8k)] = \u03bc, \u03bdk is an unbiased estimate of the gradient G(P, \u03b8k). Subsequently, the term \u201c\u2207\u03b8l(xi, yi, \u03b8) \u2212 \u03bc\u201d in Eq (4) can be regarded as a calibrator to reduce the variance of gradient estimation and achieve a linear convergence rate, which is faster than directly using \u2207\u03b8l(xi, yi, \u03b8k)."}, {"title": "3. Our Proposed Method", "content": "In this section, we propose the Dynamic Gradient Calibration (DGC) approach which maintains a gradient calibration during the learning process. A highlight of DGC is that it utilizes the whole historical information to obtain a more precise gradient estimation, and consequently relieves the negative impact of catastrophic forgetting. In Section 3.1, we introduce ER and analyze the obstacle if we directly combine ER with SVRG. In Section 3.2, we present our DGC method for addressing the issues discussed in Section 3.1. In Section 3.3, we explain how to integrate DGC with other CL techniques."}, {"title": "3.1. Experience Replay Revisited and SVRG", "content": "First, we overview the classical ER method as a baseline for the CIL setting. ER employs the reservoir sampling algorithm to dynamically manage a buffer (denoted as Mt) at time t, which serves to store historical data. At each time spot t (and assume the current updating step number of the optimization is k), ER updates the model parameter \u03b8kt following the standard gradient descent method:\n$\\begin{equation}\\label{eq:er_normal}\\theta_{t}^{k+1} = \\theta_{t}^{k} - \\eta \\cdot v_{t}^{k},\\\\ \\end{equation}$\nwhere \u03b7 is the learning rate and vtk is the calculated gradient. If not using any replay strategy, vtk is usually calculated on a randomly sampled training data (x, y) \u2208 Tt. It is easy to see that this simple strategy can cause the forgetting issue for shifting data stream since it does not contain any information from the previous data. Hence the classical ER algorithm takes a random sample (x, y) from the aforementioned buffer Mt (who contains a subset of historical data via reservoir sampling), and computes the gradient:\n$\\begin{equation}\\label{eq:er}\\nu_t^k \\triangleq \\frac{T_t}{\\sum_{c=1}^{t} T_c} \\nabla_\\theta l(x_{t},y_{t}, \\theta_t^k)+\\frac{1}{\\sum_{c=1}^{t} T_c} \\nabla_\\theta l(\\widetilde{x}_t,\\widetilde{y}_t, \\theta_t^k).\\\\ \\end{equation}$\nRemark 3.1. (1) For simplicity, we assume that the data sets of all the tasks have the same size. So the obtained vtk in (7) is an unbiased estimation of the full gradient G(Uc=1t Tc, \u03b8) at the current time spot t. If they have different sizes, we can simply replace the coefficients \u201cTt\u201d and \u201c1\u201d by \u201cTt\u201d and \u201cTc\u201d, respectively. (2) Also, we assume that the batch sizes of the random samples from Tt and Mt are both \u201c1\u201d, i.e., we only take single item (x, y) and (x, y) from each of them. Actually, we can also take larger batch sizes and then the Eq (7) can be modified correspondingly by taking their average gradients.\nNow we attempt to apply the SVRG method to Eq (7). Our objective is to identify a more accurate unbiased estimate of the gradient at the current time spot t so as to determine the updating direction. At first glance, one possible solution is to adapt the streaming SVRG method to the CL scenario. We treat Mt as the static data set in our memory, and apply the SVRG technique to calibrate the gradient \u201c\u2207\u03b8l(xt, yt, \u03b8)\u201d and \u201c\u2207\u03b8l(xt, yt, \u03b8)\u201d in Eq (7). Similar to the procedure introduced in Section 2.2, we denote the parameter at the beginning of the current stage s as \u03b8t,s. For simplicity, when we consider the update within stage s, we just use \u03b8t to denote \u03b8t,s. Similar with Eq (3), we define the terms\n$\\begin{equation}\\label{eq:svrghat}\\mu\\triangleq G(\\mathcal{M}_t,\\widetilde{\\theta}_t),\\;\\;\\;\\widetilde{v}\\triangleq G(\\mathcal{T}_t,\\widetilde{\\theta}_t).\\\\ \\end{equation}$\nThen we can calibrate the gradient by \u2207\u03b8l(xt, yt, \u03b8t) and \u2207\u03b8l(xt, yt, \u03b8t) (which serves as the similar role of \u2207\u03b8l(xi, yi, \u03b8 in (4)); the new form of vtk becomes\n$\\begin{equation}\\label{eq:erhat}\\nu_t^k = \\frac{1}{\\sum_{c=1}^{t} T_c} (\\nabla_\\theta l(x_{t},y_{t}, \\theta_t^k) - (\\nabla_\\theta l(x_{t},y_{t}, \\widetilde{\\theta}_t) + \\widetilde{v}))+\\frac{1}{\\sum_{c=1}^{t} T_c} (\\nabla_\\theta l(\\widetilde{x}_t,\\widetilde{y}_t, \\theta_t^k) - (\\nabla_\\theta l(\\widetilde{x}_t,\\widetilde{y}_t, \\widetilde{\\theta}_t) + \\mu)).\\\\ \\end{equation}$\nObviously, if the buffer Mt contains the whole historical data (denote by T[1:t) = Uc=1t Tc), the above approach is exactly the standard SVRG. However, because Mt only takes a small subset of T[1:t), this approach still cannot avoid information loss for the previous tasks. In next section, we propose a novel two-level dynamic algorithm to record more useful information from T[1:t), and thereby reduce the information loss induced by Mt. We also take the approach of Eq (9) as a baseline in Section 4 to illustrate the advantage of our proposed approach."}, {"title": "3.2. Dynamic Gradient Calibration", "content": "To tackle the issue discussed in Section 3.1, we propose a novel two-level update approach \u201cDynamic Gradient Calibration (DGC)\u201d to maintain our calibration term. Our focus is designing a method to incrementally update an unbiased estimation for G(T[1:t), \u03b8t). To illustrate our idea clearly, we decompose our analysis to two levels: (1) update the parameter during the training within each time spot t; (2) update the parameter at the transition from time spot t to t + 1 (i.e., the moment that the task Tt has just been completed and the task Tt+1 is just coming).\n(1) How to update the parameter during the training within each time spot t. We follow the setting of the streaming SVRG as discussed in Section 3.1: the training process at the current time spot t is divided into a sequence of stages; the model parameter at the beginning of each stage is recorded as \u03b8t. To illustrate our idea for calibrating the gradient vtk in Eq (9), we begin by considering an \"imaginary\" approach: we let\n$\\begin{equation}\\label{eq:DGC_imaginary}\\nu_{t} \\triangleq \\frac{1}{\\sum_{c=1}^{t} T_c} (\\nabla_\\theta l(x_{t},y_{t}, \\theta_t^k) - \\nabla_\\theta l(x_{t},y_{t}, \\widetilde{\\theta}_t) + G(\\mathcal{T}_t, \\widetilde{\\theta}_t)) + \\frac{1}{\\sum_{c=1}^{t} T_c} \\Gamma_t,\\\\ \\end{equation}$\nwhere\n$\\begin{equation}\\label{eq:Gamma}\\Gamma_t = \\nabla_\\theta l(\\widetilde{x}_t,\\widetilde{y}_t, \\theta_t^k) - (\\nabla_\\theta l(\\widetilde{x}_t,\\widetilde{y}_t, \\widetilde{\\theta}_t) - G(\\mathcal{T}_{[1:t)}, \\theta_t)).\\\\ \\end{equation}$\nDifferent from Eq (9), we compute vtk based on the full historical data T[1:t), which follows the same manner of SVRG. However, a major obstacle here is that we cannot obtain the exact \u0393t since T[1:t) is not available. This motivates us to design a relaxed form of (10). We define a surrogate function to approximate \u0393t, which can be computed through recursion. Suppose each training stage has m \u2265 1 steps, then we define\n$\\begin{equation}\\label{eq:GDGC1}\\Gamma_{\\text{DGC}}(t, k) = \\nabla_\\theta l(\\widetilde{x}_t,\\widetilde{y}_t, \\theta_t^k) - (\\nabla_\\theta l(\\widetilde{x}_t,\\widetilde{y}_t, \\widetilde{\\theta}_t) - \\Gamma_{\\text{DGC}}(t)),\\\\ \\end{equation}$\n$\\begin{equation}\\label{eq:GDGC2}\\Gamma_{\\text{DGC}}(t) = \\Gamma_{\\text{DGC}}(t, m+1),\\\\ \\end{equation}$\nin the stage. Note that the term \u201c\u2207\u03b8l(xt, yt, \u03b8t)\u201d in (11) can be computed by the previous parameter \u03b8t during training, so we do not need to store it in buffer. For the initial t = 1 case (i.e., when we just encounter the first task), we can directly set \u0393DGC(1) = 0. We update the function \u0393DGC(t) at the end of each training stage in (12), and use the function \u0393DGC(t, k) to approximate \u0393 in (10). Comparing with the original formulation of \u0393 in (10), we only replace the term \u201cG(T[1:t), \u03b8t)\u201d by \u201c\u0393DGC(t)\u201d. Also, we have the following lemma to support this replacement. The detailed proof of lemma 3.2 is provided in our supplement.\nLemma 3.2.\n$\\begin{equation}\\label{eq:lemma}\\\\mathbb{E} [\\Gamma_{\\text{DGC}}(t)] = G(\\mathcal{T}_{[1:t)}, \\theta_t)\\\\ \\end{equation}$\nWe utilize the term \u201c\u2207\u03b8l(xt, yt, \u03b8t) \u2013 \u0393DGC(t)\u201d of (11) as the calibrator for each updating step, thereby preserving the unbiased nature of the gradient estimator and reducing the variance of gradient estimation.\n(2) How to update the parameter at the transition from time spot t to t + 1. At the end of time spot t, we update the recorded \u03b8t to \u03b8m+1, and the data Tt from time t should be integrated into the historical data. In this context, it is essential to update the calibrated gradient accordingly:\n$\\begin{equation}\\label{eq:GDGC3}\\Gamma_{\\text{DGC}}(t + 1) = \\frac{1}{t} ((t-1) \\cdot \\Gamma_{\\text{DGC}}(t) + G(\\mathcal{T}_{t}, \\theta_t)).\\\\ \\end{equation}$\nThe complete algorithm is presented in Algorithm 1. Compared with the conventional reservoir sampling based approaches, we only require the additional storage for keeping \u0393DGC(t), and so that the gradient \u0393DGC(t, k) in (11) can be effectively updated by the recursion. Moreover, our method can also conveniently adapt to TFCL where the task boundaries are not predetermined. In such a setting, we can simply treat each batch data (during the SGD) as a \u201cmicro\u201d task at the time point and then update the gradient estimation via Eq (14). The detailed algorithm for TFCL is placed in our appendix.\nSimilar to the theoretical analysis of SVRG, under mild assumptions, the optimization procedure of our DGC method at each time spot t can also achieve linear convergence. We denote the optimal parameter at time spot t as \u03b8\u2217 \u225c arg min\u03b8\u2208X R tCL(\u03b8). Then we have the following theorem.\nTheorem 3.3. Assume that f(x;\u03b8) is L-smooth and \u03b3-strongly convex; the parameters m > 10\u03b7L2 and \u03b7 = 10\u03b7L. Then we have a linear convergence in expectation for the DGC procedure at time t:\n$\\begin{equation}\\label{eq:convergence_rate}\\\\mathbb{E} [||\\theta_{t,s+1}-\\theta^*||^2] \\leq (1 - \\frac{1}{10L^2}m)^s \\mathbb{E}[||\\theta_{t,1} -\\theta^*||^2],\\\\ \\end{equation}$\nwhere \u03b8t,s represents the initialization parameter at the beginning of the s-th stage at time spot t.\nThe proof of Theorem 3.3 is provided in appendix. This theorem indicates that the gradient calibrated by our DGC method shares the similar advantages with SVRG. For instance, when updating each task Tt, the loss function has a smoother decrease (we validate this property in Section 4.3)."}, {"title": "3.3. Combine DGC with Other CL Methods", "content": "Our proposed DGC approach can be also efficiently combined with other CL methods. As discussed in Section 1, a number of popular CL methods rely on the reservoir sampling technique to preserve historical data in buffer. For these methods, such as DER and XDER, we can conveniently combine the conventional batch gradient descent with the DGC calibrated gradient estimator \u0393DGC (t, k) defined in Section 3.2, so as to obtain a more precise gradient estimator with reduced variance based on Eq (10):\n$\\begin{equation}\\label{eq:finalv}\\nu_{t} = \\frac{1}{\\sum_{c=1}^{t} T_c} (\\nabla_\\theta l(x_{t},y_{t}, \\theta_t^k) - \\nabla_\\theta l(\\widetilde{x}_{t},y_{t}, \\widetilde{\\theta}_t) + G(\\mathcal{T}_t, \\widetilde{\\theta}_t)) + \\frac{1}{\\sum_{c=1}^{t} T_c} [\\alpha \\Gamma_{\\text{DGC}}(t, k) + (1 - \\alpha) \\nabla_\\theta l(\\widetilde{x}_{t},y_{t}, \\theta_t^k)],\\\\ \\end{equation}$\nwhere \u03b1 is a given parameter to control the proportion of the two unbiased estimations \u0393DGC(t,k) and \u2207\u03b8l(xt, yt, \u03b8) of the gradient G(T[1:t), \u03b8). According to the theoretical analysis in SSVRG, the selection of \u03b1 should be related to 1/L, where L is the smoothness coefficient of the model f (x; \u03b8), \u0456.\u0435.,\n$\\begin{equation}\\label{eq:smoothness}L = \\underset{\\theta_1,\\theta_2 \\in \\Theta}{\\text{max}} \\frac{|\\nabla_\\theta f(x; \\theta_1) - \\nabla_\\theta f(x; \\theta_2)|}{\\theta_1 - \\theta_2},\\\\ \\end{equation}$\nwhere \u0398 represents the parameter space. The experimental study on the impact of \u03b1 is placed in our supplement. In Section 4, we show that the amalgamation of the CL method and our DGC calibration procedure can yield a more precise update direction, and consequently enhance the ultimate model performance."}, {"title": "4. Experiments", "content": "We conduct the experiments to compare with various baseline methods across different datasets. We consider both the CIL and TFCL models.\nDatasets We carry out the experiments on three widely employed datasets S(Split)-CIFAR10, S-CIFAR100, and S-TinyImageNet. S-CIFAR10 is the split dataset by partitioning CIFAR10 into 5 tasks, each containing two categories; similarly, S-CIFAR100 and S-TinyImageNet are the datasets by respectively partitioning CIFAR100 and TinyImageNet\nBaseline methods We consider the following baselines. (1) Replay-based methods: ER, DER++, XDER, MOCA, GSS, GCR, HAL, and ICARL. (2) Optimization-based methods: AGEM, AOP, and SSVRG. (3) Dynamic architecture method: Dynamic ER. We integrate DGC with ER, DER++, XDER, and Dynamic ER, and assess their performances in CIL. For TFCL, we consider its combination with ER and DER++. For convenience, we use \"DGC-Y\" to denote the combination of DGC with a CL method \"Y\". For example, DGC-ER denotes the method combining ER and DGC methods.\nEvaluation metrics We employ the Average Accuracy (AA) and the final Average Incremental Accuracy (FAIA) to assess the performance. These two metrics are both widely used for continual learning. Let ak,j \u2208 [0,1](k > j) denote the classification accuracy evaluated on the testing set of the task Tj after learning Tk. The value AA at time spot i is defined as AAi = \u03a3j=1i ai,j. In particular, we name the value AAT as the final Average Accuracy (FAA). The final AIA is defined as FAIA = 1 T \u03a3i=1T AAi. We also use Final Forgetting (FF) from to measure the forgetting of the model throughout the learning process (the formal definition of FF and its numerical results are placed in the supplement). Each instance of our experiments is repeated by 10 times."}, {"title": "4.1. Results in CIL", "content": "Hyper-parameters selection In our implementation, we fixed the values of epoch and batch size, which implies that the total number of optimization steps (i.e., the value s \u00d7 m) is also fixed. In our experiments, we set the value m = 200, so the value of s in Algorithm 1 is also determined.\nExcept for the Dynamic ER (which is dynamically expanded), all other testing methods have constant storage limits. The results shown in Table 1 reveal that our DGC method can bring improvements to the combined methods on the testing benchmarks. For the sake of clarity, we also underline the best-performing method except Dynamic ER and DGC-Dynamic ER in Table 1. Among the methods with constant storage limits, DGC-ER achieves the best results on S-CIFAR10 when the buffer size is 2000, and DGC-XDER achieves the best results on S-CIFAR100 and S-TinyImageNet. It is worth noting that our DGC method can also be conveniently integrated with existing advanced dynamic expansion representation techniques, such as Dynamic ER, which demonstrates the improvements to certain extent, e.g., it achieves an improvement more than 6% on S-CIFAR10/100 with buffer size 2000. We also record the training time of these baseline methods in our supplement.\nIn the subsequent experiment, we investigate the performance of DGC compared to other baseline methods with varying the number of tasks. Throughout the experiment, we maintain a constant buffer size of 2000. As outlined in Table 2, our results demonstrate that DGC can bring certain improvements to ER, DER++, and XDER with setting the number of tasks to be 5 and 20. Moreover, similar to the previous research in, the results shown in Table 2 also imply that increasing the number of tasks could exacerbate the catastrophic forgetting issue. This phenomenon occurs because the model faces a reduced volume of data on each specific task, and thereby necessitates the capability of retaining the information of historical data to guide the model updates. As can be seen, the improvement obtained by DGC for the case of 20 tasks usually is more significant compared with the case of 5 tasks. For example, DGC-DER++ achieves a 3.94% improvement with 20 tasks versus a 1.18% improvement with 5 tasks, while DGC-XDER exhibits a 3.08% improvement with 20 tasks and only 0.42% with 5 tasks. These results highlight the advantage of DGC for mitigating catastrophic forgetting by effectively utilizing historical data through gradient-based calibration."}, {"title": "4.2. Results in TFCL", "content": "We then conduct the experiments in TFCL. The curves shown in Figure 2 depict the average AA t evolutions with varying the \"implicit\u201d task number t on S-CIFAR100. We call it \"implicit\" since the value t is not given during the training, which means that the design of the algorithm cannot rely on task boundaries or task identities. Therefore, we only compare those baseline methods that are applicable to TFCL model. The results suggest that DGC can bring improvements to ER and DER++ on AA t for almost all the ts; in particular, DGC-DER++ achieves the best performance and also with small variances. Through approximating the full gradient, the GCR and SSVRG methods can relieve the catastrophic forgetting issue to a certain extent, but they still suffer from the issue of storage limit, which affects their effectiveness for estimating the full gradient. Consequently, these methods exhibit performance downgrade and larger variance in Figure 2. Comparing with them, the approaches integrated with DGC illustrate more consistent and stable improvements. More detailed results for TFCL are available in our supplement."}, {"title": "4.3. Smoothness of Training with"}]}