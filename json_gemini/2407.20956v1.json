{"title": "An Effective Dynamic Gradient Calibration Method for Continual Learning", "authors": ["Weichen Lin", "Jiaxiang Chen", "Ruomin Huang", "Hu Ding"], "abstract": "Continual learning (CL) is a fundamental topic in machine learning, where the goal is to train a model with continuously incoming data and tasks. Due to the memory limit, we cannot store all the historical data, and therefore confront the \"catastrophic forgetting\" problem, i.e., the performance on the previous tasks can substantially decrease because of the missing information in the latter period. Though a number of elegant methods have been proposed, the catastrophic forgetting phenomenon still cannot be well avoided in practice. In this paper, we study the problem from the gradient perspective, where our aim is to develop an effective algorithm to calibrate the gradient in each updating step of the model; namely, our goal is to guide the model to be updated in the right direction under the situation that a large amount of historical data are unavailable. Our idea is partly inspired by the seminal stochastic variance reduction methods (e.g., SVRG and SAGA) for reducing the variance of gradient estimation in stochastic gradient descent algorithms. Another benefit is that our approach can be used as a general tool, which is able to be incorporated with several existing popular CL methods to achieve better performance. We also conduct a set of experiments on several benchmark datasets to evaluate the performance in practice.", "sections": [{"title": "1. Introduction", "content": "In the past years, Deep Neural Networks (DNNs) demonstrate remarkable performance for many different tasks in artificial intelligence, such as image generation, classification, and pattern recognition. Usually we assume that the whole training data is stored in our facility and the DNN models can be trained offline by using Stochastic Gradient Descent (SGD) algorithms. However, real-world applications often require us to consider training lifelong models, where the tasks and data are accumulated in a streaming fashion. For example, with the popularity of smart devices, a large amount of new data is generated every day. A model needs to make full use of these new data to improve its performance while keeping old knowledge from being forgotten. Those applications motivate us to study the problem of continual learning (CL), where its goal is to develop effective method for gleaning insights from current data while retaining information from prior training data.\n\nA significant challenge that CL encounters is \"catastrophic forgetting\", wherein the exclusive focus on the current set of examples could result in a dramatic deterioration in the performance on previously learned data. This phenomenon is primarily attributed to limited storage and computational resources during the training process; otherwise, one could directly train the model from scratch using all the saved data. To address this issue, we need to develop efficient algorithm for training neural networks from a continuous stream of non-i.i.d. samples, with the goal of mitigating catastrophic forgetting while effectively managing computational costs and memory footprint.\n\nA number of elegant CL methods have been proposed to alleviate the catastrophic forgetting issue. One representative CL approach is referred to as \u201cExperience Replay (ER)\u201d, which has shown promising performance in several continual learning scenarios. Roughly speaking, the ER method utilizes reservoir sampling to maintain historical data in the buffer, then extract new incoming training data with random samplings for learning the current task. Though the intuition is simple, the ER method currently is one of the most popular CL approaches that incurs moderate computational and storage demands. Moreover, several recently proposed approaches suggest that the ER method can be combined with knowledge distillation to further improve the performance; for example, the methods of DER/DER++ and X-DER preserve previous training samples alongside their logits in the model as the additional prior knowledge. Besides the ER methods, there are also several other types of CL techniques proposed in recent years, and please refer to Section 1.2 for a detailed introduction."}, {"title": "1.1. Our Main Ideas and Contributions", "content": "Though existing CL methods can alleviate the catastrophic forgetting issue from various aspects, the practical performances in some scenarios are still not quite satisfying. In this paper, we study the continual learning problem from the gradient perspective, and the rationality behind is as follows. In essence, an approach for avoiding the catastrophic forgetting issue in CL, e.g., the replay mechanisms or the regularization strategies, ultimately manifests its influence on the gradient directions during model updating. If all the historical data are available, one could compute the gradient by using the stochastic gradient descent method and obviously the catastrophic forgetting phenomenon cannot happen. The previous methodologies aim to approximate the gradient by preserving additional information and incorporating it as a constraint to model updates, thereby retaining historical knowledge. However, the replay-based methods in practice are often limited by storage capacity, which leads to a substantial loss of historical data information and inaccurate estimation of historical gradients. Therefore, our goal is to develop a more accurate gradient calibration algorithm in each step of the continual learning procedure, which can directly enhance the training quality.\n\nWe are aware of several existing CL methods that also take into account of the gradients , but our idea proposed here is fundamentally different. We draw inspiration from the seminal stochastic variance reduction methods (e.g., SVRG from and SAGA from), which are originally designed to reduce the gradient variance so that the estimated gradient can closely align with the true full gradient over the entire dataset (including the current and historical data). These variance reduction methods have been extensively studied in the line of the research on stochastic gradient descent method ; their key idea is to leverage the additional saved full gradient information to calibrate the gradient in the current training step, which leads to significantly reduced gradient variance comparing with the standard SGD method. This intuition also inspires us to handle the CL problem. In a standard SGD method, the variance between the obtained gradient and the full gradient is due to the \"batch size limit\" (if the batch size has no bound, we can simply compute the full gradient). Recall that the challenge of CL is due to the \u201cbuffer size limit\u201d, which impedes the use of full historical data (this is similar with the dilemma encountered by SGD with the \"batch size limit\"). So an interesting question is\n\nCan the calibration idea for \"batch size limit\" be modified to handle \"buffer size limit\"? Specifically, is it possible to develop an effecitve method to compute a SVRG (or SAGA)-like calibration for the gradient in CL scenarios?\n\nObviously, it is challenging to directly implement the SVRG or SAGA algorithms in continual learning because of the missing historical data. Note that proposed a streaming SVRG (SSVRG) method that realizes the SVRG method within a given fixed buffer, but unfortunately it does not perform quite well in the CL scenarios (as shown in our experimental section). One possible reason is that SSVRG can only leverage information within the buffer and fails to utilize all historical information.\n\nIn this paper, we aim to apply the intuition of SVRG to handle the \"buffer size limit\" in CL, and our contributions can be summarized as follows:\n\n\u2022 First, we propose a novel two-level dynamic algorithm, named Dynamic Gradient Calibration (DGC), to maintain a gradient calibration in continual learning. DGC can effectively tackle the storage limit and leverage historical data to calibrate the gradient of the model at each current stage. Moreover, our theoretical analysis shows that our DGC based CL algorithm can achieve a linear convergence rate.\n\n\u2022 Second, our method can be conveniently integrated with most existing reservoir sampling-based continual learning approaches (e.g., ER , DER/DER++, XDER and Dynamic ER ), where this hybrid framework can induce a significant enhancement to the overall model performance. Note that storing the gradient calibrator can cause an extra memory footprint; so a key question is whether such an extra memory footprint can yield a larger marginal benefit than simply taking more samples to fill the extra memory? In Fig 1, we illustrate a brief example to answer this question in the affirmative; more detailed evaluations are shown in the experimental part.\n\n\u2022 Finally, we conduct a set of comprehensive experiments on the popular datasets S-CIFAR10, S-CIFAR100 and S-TinyImageNet; the experimental results suggest that our method can improve the final Average Incremental Accuracy (FAIA) in several CL"}, {"title": "1.2. Related Work", "content": "We briefly overview existing important continual learning approaches (except for the ones mentioned before). We also refer the reader to the recent surveys  for more details.\n\nA large number of CL methods are replay based, where they often keep a part of previous data through approaches like reservoir sampling. Several more advanced data selection strategies focus on optimizing the factors like the sample diversity of parameter gradients or the similarity to previous gradients on passed data, e.g., GSS and GCR. Experience replay can be effectively combined with knowledge distillation. For example, proposed to distill colliding effects from the features for new coming tasks, and ICARL proposed to take account of the data representation trained on old data. MOCA improves replay-based methods by diversifying representations in the space. Another replay-based approach is based on generative replay, which obtains replay data by generative models .\n\nAnother way for solving continual learning is through some deliberately designed optimization procedures. For example, the methods GEM, AGEM and MER restrict parameter updates to align with the experience replay direction, and thereby preserve the previous input and gradient space with old training samples. Different from saving old training samples, proposed to adapt parameter updates in the orthogonal direction of the previously saved gradient. The method AOP projects the gradient in the direction orthogonal to the subspace spanned by all previous task inputs, therefore it only keeps an orthogonal projector rather than storing previous data.\n\nTo mitigate the problem of forgetting, we can also augment the model capacity for learning new tasks. tried to enhance model performance by employing meta-learning techniques when dynamically extending the model. The method ANCL proposes to utilize an auxiliary network to achieve a trade-off between plasticity and stability. Dynamic ER introduces a novel two-stage learning method that employs a dynamically expandable representation for learning knowledge incrementally."}, {"title": "2. Preliminaries", "content": "We consider the task of training a soft-classification function \\(f(\\cdot; \\theta): \\mathcal{X} \\rightarrow \\mathcal{Y}\\), where \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) respectively represent the space of data and the set of labels, and \\(\\theta\\) is the parameter to optimize. Without loss of generality, we assume \\(\\mathcal{Y} = \\{1,2,\\ldots, K\\}\\). So \\(f(\\cdot; \\theta)\\) maps each \\(x \\in \\mathcal{X}\\) to some \\(f(x;\\theta) \\in \\mathbb{R}^K\\). To find an appropriate \\(\\theta\\), the classification function \\(f\\) is usually equipped with a loss function \\(l(f(x; \\theta), y)\\), which is differentiable for the variables \\(x\\) and \\(\\theta\\) (e.g., cross-entropy loss). To simplify the notation, we use \\(l(x, y, \\theta)\\) to denote \\(l(f(x; \\theta), y)\\). Given a set of data \\(\\mathcal{P} = \\{(x^i, y^i) | 1 \\leq i \\leq n\\} = \\mathcal{X}_\\mathcal{P} \\times \\mathcal{Y}_\\mathcal{P} \\subset \\mathcal{X} \\times \\mathcal{Y}\\), the training process is to find a \\(\\theta\\) such that the empirical risk of \\(l(x, y, \\theta)\\), i.e., \\(\\Sigma_{i=1}^n l(x^i, y^i, \\theta)\\), is minimized. We define the full gradient of \\(l(x, y, \\theta)\\) on \\(\\mathcal{P}\\) as\n\n\\(\\mathcal{G}(\\mathcal{P},\\theta) \\triangleq \\nabla_\\theta l(\\mathcal{X}_\\mathcal{P}, \\mathcal{Y}_\\mathcal{P}, \\theta) = \\frac{1}{n} \\Sigma_{i=1}^n \\nabla_\\theta l(x^i, y^i, \\theta)\\).\n\n2.1. Continual Learning Models\n\nIn this paper, we focus on two popular CL models: Class-Incremental Learning (CIL) and Task-Free Continual Learning (TFCL).\n\nIn the setting of CIL, the training tasks come in a sequence \\(\\{T_1, T_2,\\ldots, T_T\\}\\) with disjoint label space; each time spot \\(t \\in \\{1,2,\\ldots,T\\}\\) corresponds to the task \\(T_t\\) with a training dataset \\(\\{(x^i, y^i) | 1 \\leq i \\leq n_t\\}\\). With a slight abuse of notations, we also use \\(T_t\\) to denote its training dataset. Also, we use \\(\\mathcal{V}_t\\) to denote the corresponding set of labels \\(\\{y | 1 \\leq i \\leq n_t\\}\\). Although the training data for individual tasks \\(T_t\\) is independently and identically distributed (i.i.d.), it is worth noting that the overall task stream \\(\\{T_1, T_2,\\ldots, T_T\\}\\) does not adhere to the i.i.d. assumption due to the evolving label space over time. We define the overall risk under this CL setting at the current time spot \\(t\\) as follows:\n\n\\(\\mathcal{R}_{CL}^t(\\theta) \\triangleq \\Sigma_{c=1}^t \\Sigma_{(x,y) \\sim T_c} \\mathbb{E}[l(x, y, \\theta)]\\).\n\nThe setting of TFCL is similar to CIL, where the major difference is that the task identities are not provided in neither the training nor testing procedures. So the TFCL setting is more challenging than CIL because the algorithm is unaware of task changes and the current task identity. In the main part of our paper, we present our results in CIL; in our supplement, we explain how to extend our results to TFCL."}, {"title": "2.2. Variance Reduction Methods", "content": "A comprehensive introduction to variance reduction methods is provided in. Here we particularly introduce one representative variance reduction method SVRG, which is closely related to our proposed CL approach.\n\nThe high-level idea of SVRG is to construct a calibration term to reduce the variance in the gradient estimate. The complete optimization process can be segmented into a sequence of stages. We denote the training data as \\(\\mathcal{P} = \\{(x^i, y^i)\\}_{i=1}^n \\subset \\mathcal{X} \\times \\mathcal{Y}\\), and denote the parameter at the beginning of each stage as \\(\\theta\\). The key part of SVRG is to minimize the variance in SGD optimization by computing an additional term \\(\\mu\\):\n\n\\(\\mu \\triangleq \\mathcal{G}(\\mathcal{P}, \\tilde{\\theta}) = \\frac{1}{n} \\Sigma_{i=1}^n \\nabla_\\theta l(x^i, y^i, \\tilde{\\theta})\\).\n\nIn each stage, SVRG applies the standard SGD with the term \\(\\mu\\) to update the parameter \\(\\theta\\): let\n\n\\begin{aligned}\n\\upsilon^k &= \\nabla_\\theta l(x^i, y^i, \\theta^k) - (\\nabla_\\theta l(x^i, y^i, \\tilde{\\theta}) - \\mu), \\\\\n\\theta^{k+1} &= \\theta^k - \\eta \\upsilon^k,\n\\end{aligned}\n\nwhere \\((x^i, y^i) \\in \\mathcal{P}\\) is the sampled training data, \\(\\theta^k\\) represents the parameter at the \\(k\\)-th step of SGD, and \\(\\eta\\) is the learning rate. Since \\(\\mathbb{E}[\\nabla_\\theta l(x^i, y^i, \\theta^k)] = \\mu\\), \\(\\upsilon^k\\) is an unbiased estimate of the gradient \\(\\mathcal{G}(\\mathcal{P}, \\theta^k)\\). Subsequently, the term \u201c\\(\\nabla_\\theta l(x^i, y^i, \\tilde{\\theta}) - \\mu\\)\u201d can be regarded as a calibrator to reduce the variance of gradient estimation and achieve a linear convergence rate, which is faster than directly using \\(\\nabla_\\theta l(x^i, y^i, \\theta^k)\\)."}, {"title": "3. Our Proposed Method", "content": "In this section, we propose the Dynamic Gradient Calibration (DGC) approach which maintains a gradient calibration during the learning process. A highlight of DGC is that it utilizes the whole historical information to obtain a more precise gradient estimation, and consequently relieves the negative impact of catastrophic forgetting. In Section 3.1, we introduce ER and analyze the obstacle if we directly combine ER with SVRG. In Section 3.2, we present our DGC method for addressing the issues discussed in Section 3.1. In Section 3.3, we explain how to integrate DGC with other CL techniques.\n\n3.1. Experience Replay Revisited and SVRG\n\nFirst, we overview the classical ER method as a baseline for the CIL setting. ER employs the reservoir sampling algorithm to dynamically manage a buffer (denoted as \\(M_t\\)) at time \\(t\\), which serves to store historical data. At each time spot \\(t\\) (and assume the current updating step number of the optimization is \\(k\\)), ER updates the model parameter \\(\\theta_t\\) following the standard gradient descent method:\n\n\\(\\theta_t^{k+1} = \\theta_t^{k} - \\eta \\cdot \\upsilon_t^k,\n\nwhere \\(\\eta\\) is the learning rate and \\(\\upsilon_t^k\\) is the calculated gradient. If not using any replay strategy, \\(\\upsilon_t^k\\) is usually calculated on a randomly sampled training data \\((x, y) \\in T_t\\). It is easy to see that this simple strategy can cause the forgetting issue for shifting data stream since it does not contain any information from the previous data. Hence the classical ER algorithm takes a random sample \\((\\bar{x}, \\bar{y}) \\in M_t\\) from the aforementioned buffer \\(M_t\\) (who contains a subset of historical data via reservoir sampling), and computes the gradient:\n\n\\(\\upsilon_t^k = \\frac{1}{t} (\\nabla_\\theta l(x, y, \\theta_t^k) + \\nabla_\\theta l(\\bar{x}, \\bar{y}, \\theta_t^k)).\n\nRemark 3.1. (1) For simplicity, we assume that the data sets of all the tasks have the same size. So the obtained \\(\\upsilon_t^k\\) in (7) is an unbiased estimation of the full gradient \\(\\mathcal{G}(\\cup_{c=1}^t T_c, \\theta)\\) at the current time spot \\(t\\). If they have different sizes, we can simply replace the coefficients \u201c\\(\\frac{1}{t}\\)\u201d and \u201c\\(\\frac{1}{t}\\)\u201d by \u201c\\(\\frac{T_t}{\\Sigma_{c=1}^t T_c}\\)\u201d and \u201c\\(\\frac{\\Sigma_{c=1}^{t-1} T_c}{\\Sigma_{c=1}^t T_c}\\)\u201d, respectively. (2) Also, we assume that the batch sizes of the random samples from \\(T_t\\) and \\(M_t\\) are both \u201c1\u201d, i.e., we only take single item \\((x, y)\\) and \\((\\bar{x}, \\bar{y})\\) from each of them. Actually, we can also take larger batch sizes and then the Eq (7) can be modified correspondingly by taking their average gradients.\n\nNow we attempt to apply the SVRG method to Eq (7). Our objective is to identify a more accurate unbiased estimate of the gradient at the current time spot \\(t\\) so as to determine the updating direction. At first glance, one possible solution is to adapt the streaming SVRG method to the CL scenario. We treat \\(M_t\\) as the static data set in our memory, and apply the SVRG technique to calibrate the gradient \u201c\\(\\nabla_\\theta l(x, y, \\theta)\\)\u201d and \u201c\\(\\nabla_\\theta l(\\bar{x}, \\bar{y}, \\theta)\\)\u201d in Eq (7). Similar to the procedure introduced in Section 2.2, we denote the parameter at the beginning of the current stage \\(s\\) as \\(\\theta_{t,s}\\). For simplicity, when we consider the update within stage \\(s\\), we just use \\(\\tilde{\\theta}_t\\) to denote \\(\\theta_{t,s}\\). Similar with Eq (3), we define the terms\n\n\\begin{aligned}\n\\mu \\triangleq \\mathcal{G}(M_t,\\tilde{\\theta}_t), \\quad \\tilde{\\upsilon} \\triangleq \\mathcal{G}(T_t, \\tilde{\\theta}_t).\n\\end{aligned}\n\nThen we can calibrate the gradient by \\(\\nabla_\\theta l(x_t, y_t, \\tilde{\\theta}_t)\\) and \\(\\nabla_\\theta l(\\bar{x}_t, \\bar{y}_t, \\tilde{\\theta}_t)\\) (which serves as the similar role of \\(\\nabla_\\theta l(x^i, y^i, \\tilde{\\theta})\\) in (4)); the new form of \\(\\upsilon_t^k\\) becomes\n\n\\upsilon_t^k = \\frac{1}{t} ((\\nabla_\\theta l(x_t, y_t, \\theta_t^k) - \\nabla_\\theta l(x_t, y_t, \\tilde{\\theta}_t) + \\tilde{\\upsilon}) + (\\nabla_\\theta l(\\bar{x}_t, \\bar{y}_t, \\theta_t^k) - \\nabla_\\theta l(\\bar{x}_t, \\bar{y}_t, \\tilde{\\theta}_t) + \\mu)).\n\nObviously, if the buffer \\(M_t\\) contains the whole historical data (denote by \\(T_{[1:t)} = \\cup_{c=1}^{t-1} T_c\\)), the above approach is exactly the standard SVRG. However, because \\(M_t\\) only takes a small subset of \\(T_{[1:t)}\\), this approach still cannot avoid information loss for the previous tasks. In next section, we propose a novel two-level dynamic algorithm to record more useful information from \\(T_{[1:t)}\\), and thereby reduce the information loss induced by \\(M_t\\). We also take the approach of Eq (9) as a baseline in Section 4 to illustrate the advantage of our proposed approach.\n\n3.2. Dynamic Gradient Calibration\n\nTo tackle the issue discussed in Section 3.1, we propose a novel two-level update approach \u201cDynamic Gradient Calibration (DGC)\u201d to maintain our calibration term. Our focus is designing a method to incrementally update an unbiased estimation for \\(\\mathcal{G}(T_{[1:t)}, \\theta_t)\\). To illustrate our idea clearly, we decompose our analysis to two levels: (1) update the parameter during the training within each time spot \\(t\\); (2) update the parameter at the transition from time spot \\(t\\) to \\(t + 1\\) (i.e., the moment that the task \\(T_t\\) has just been completed and the task \\(T_{t+1}\\) is just coming).\n\n(1) How to update the parameter during the training within each time spot \\(t\\). We follow the setting of the streaming SVRG as discussed in Section 3.1: the training process at the current time spot \\(t\\) is divided into a sequence of stages; the model parameter at the beginning of each stage is recorded as \\(\\theta_t\\). To illustrate our idea for calibrating the gradient \\(\\upsilon_t^k\\) in Eq (9), we begin by considering an \"imaginary\" approach: we let\n\n\\upsilon_t = \\frac{1}{t} ((\\nabla_\\theta l(x_t, y_t, \\theta_t^k) - \\nabla_\\theta l(x_t, y_t, \\theta_t) + \\mathcal{G}(T_t, \\theta_t)) + \\Gamma,\n\nwhere\n\n\\(\\Gamma = \\nabla_\\theta l(\\bar{x}_t, \\bar{y}_t, \\theta_t^k) - (\\nabla_\\theta l(\\bar{x}_t, \\bar{y}_t, \\theta_t) - \\mathcal{G}(T_{[1:t)}, \\theta_t))\n\nCalibration from the previous parameter \\(\\theta_t\\)\n\nDifferent from Eq (9), we compute \\(\\upsilon_t^k\\) based on the full historical data \\(T_{[1:t)}\\), which follows the same manner of SVRG. However, a major obstacle here is that we cannot obtain the exact \\(\\Gamma\\) since \\(T_{[1:t)}\\) is not available. This motivates us to design a relaxed form of (10). We define a surrogate function to approximate \\(\\Gamma\\), which can be computed through recursion. Suppose each training stage has \\(m \\geq 1\\) steps, then we define\n\n\\begin{aligned}\n\\Gamma_{DGC}(t, k) &= \\nabla_\\theta l(x_t, y_t, \\theta) - (\\nabla_\\theta l(x_t, y_t, \\tilde{\\theta}_t) - \\Gamma_{DGC}(t)),\\\\\n\\Gamma_{DGC}(t) &= \\Gamma_{DGC}(t, m + 1),\n\\end{aligned}\n\nin the stage. Note that the term \u201c\\(\\nabla_\\theta l(x_t, y_t, \\tilde{\\theta}_t)\\)\u201d in (11) can be computed by the previous parameter \\(\\theta_t\\) during training, so we do not need to store it in buffer. For the initial \\(t = 1\\) case (i.e., when we just encounter the first task), we can directly set \\(\\Gamma_{DGC}(1) = 0\\). We update the function \\(\\Gamma_{DGC}(t)\\) at the end of each training stage in (12), and use the function \\(\\Gamma_{DGC}(t, k)\\) to approximate \\(\\Gamma\\) in (10). Comparing with the original formulation of \\(\\Gamma\\) in (10), we only replace the term \u201c\\(\\mathcal{G}(T_{[1:t)}, \\theta_t)\\)\u201d by \u201c\\(\\Gamma_{DGC}(t)\\)\u201d. Also, we have the following lemma to support this replacement. The detailed proof of lemma 3.2 is provided in our supplement.\n\nLemma 3.2.\n\n\\(\\mathbb{E} [\\Gamma_{DGC}(t)] = \\mathcal{G}(T_{[1:t)}, \\theta_t)\\)\n\nWe utilize the term \u201c\\(\\nabla_\\theta l(x_t, y_t, \\theta_t^k) - \\Gamma_{DGC}(t)\\)\u201d of (11) as the calibrator for each updating step, thereby preserving the unbiased nature of the gradient estimator and reducing the variance of gradient estimation.\n\n(2) How to update the parameter at the transition from time spot \\(t\\) to \\(t + 1\\). At the end of time spot \\(t\\), we update the recorded \\(\\theta_t\\) to \\(\\theta_t^{m+1}\\), and the data \\(T_t\\) from time \\(t\\) should be integrated into the historical data. In this context, it is essential to update the calibrated gradient accordingly:\n\n\\(\\Gamma_{DGC}(t + 1) = \\frac{t - 1}{t} (\\Gamma_{DGC}(t) + \\frac{1}{t} \\mathcal{G}(T_t, \\theta_t)).\n\nThe complete algorithm is presented in Algorithm 1. Compared with the conventional reservoir sampling based approaches, we only require the additional storage for keeping \\(\\Gamma_{DGC}(t)\\), and so that the gradient \\(\\Gamma_{DGC}(t, k)\\) in (11) can be effectively updated by the recursion. Moreover, our method can also conveniently adapt to TFCL where the task boundaries are not predetermined. In such a setting, we can simply treat each batch data (during the SGD) as a \u201cmicro\u201d task at the time point and then update the gradient estimation via Eq (14). The detailed algorithm for TFCL is placed in our appendix.\n\nSimilar to the theoretical analysis of SVRG, under mild assumptions, the optimization procedure of our DGC method at each time spot \\(t\\) can also achieve linear convergence. We denote the optimal parameter at time spot \\(t\\) as \\(\\theta^* \\triangleq arg min_\\theta \\mathcal{R}_{CL}^t(\\theta)\\). Then we have the following theorem.\n\nTheorem 3.3. Assume that \\(f(x;\\theta)\\) is \\(L\\)-smooth and \\(\\gamma\\)-strongly convex; the parameters \\(m \\geq \\frac{10L^2}{\\gamma^2}\\) and \\(\\eta = \\frac{\\gamma}{10L^2}\\). Then we have a linear convergence in expectation for the DGC procedure at time \\(t\\):\n\n\\mathbb{E}[||\\theta_{t,s+1} - \\theta^*||^2] \\leq \\frac{1}{2^s} \\mathbb{E}[||\\theta_{t,s} - \\theta^*||^2],\n\nwhere \\(\\theta_{t,s}\\) represents the initialization parameter at the beginning of the \\(s\\)-th stage at time spot \\(t\\).\n\nThe proof of Theorem 3.3 is provided in appendix. This theorem indicates that the gradient calibrated by our DGC method shares the similar advantages with SVRG. For instance, when updating each task \\(T_t\\), the loss function has a smoother decrease (we validate this property in Section 4.3).\n\n3.3. Combine DGC with Other CL Methods\n\nOur proposed DGC approach can be also efficiently combined with other CL methods. As discussed in Section 1, a number of popular CL methods rely on the reservoir sampling technique to preserve historical data in buffer. For these methods, such as DER and XDER, we can conveniently combine the conventional batch gradient descent with the DGC calibrated gradient estimator \\(\\Gamma_{DGC}(t, k)\\) defined in Section 3.2, so as to obtain a more precise gradient estimator with reduced variance based on Eq (10):\n\n\\upsilon_t^k = \\frac{1}{t} ((\\nabla_\\theta l(x_t, y_t, \\theta) - \\nabla_\\theta l(x_t, y_t, \\tilde{\\theta}_t) + \\mathcal{G}(T_t, \\tilde{\\theta}_t)) + \\alpha \\Gamma_{DGC}(t, k)+(1 - \\alpha)\\nabla_\\theta l(x_t, y_t, \\theta)),\n\nwhere \\(\\alpha\\) is a given parameter to control the proportion of the two unbiased estimations \\(\\Gamma_{DGC}(t, k)\\) and \\(\\nabla_\\theta l(x_t, y_t, \\theta)\\) of the gradient \\(\\mathcal{G}(T_{[1:t)}, \\theta)\\). According to the theoretical analysis in SSVRG, the selection of \\(\\alpha\\) should be related to \\(1/L\\), where \\(L\\) is the smoothness"}, {"title": "4. Experiments", "content": "We conduct the experiments to compare with various baseline methods across different datasets. We consider both the CIL and TFCL models.\n\nDatasets We carry out the experiments on three widely employed datasets S(Split)-CIFAR10, S-CIFAR100 , and S-TinyImageNet. S-CIFAR10 is the split dataset by partitioning CIFAR10 into 5 tasks, each containing two categories; similarly, S-CIFAR100 and S-TinyImageNet are the datasets by respectively partitioning CIFAR100 and TinyImageNet into 10 tasks, each containing 10 (S-CIFAR100) and 20 (S-TinyImageNet) categories.\n\nBaseline methods We consider the following baselines. (1) Replay-based methods: ER , DER++ , XDER , MOCA , GSS, GCR, HAL, and ICARL. (2) Optimization-based"}, {"title": "5. Conclusion", "content": "In this paper, we revisit the experience replay method and aim to utilize historical information to derive a more accurate gradient for alleviating catastrophic forgetting. Inspired by the variance reduction methods for SGD, we introduce a new approach \"DGC\" to dynamically manage a gradient calibration term in CL training. Our approach can be conveniently integrated with several existing continual learning methods, contributing to a substantial improvement in both CIL and TFCL. Moreover, the improved stability of training loss reduction can also ease our practical implementation."}]}