{"title": "Semantic Decomposition and Selective Context Filtering", "authors": ["Karl John Villardar"], "abstract": "In this paper, we present two techniques for use in context-aware systems: Semantic Decomposition, which sequentially decomposes input prompts into a structured and hierarchal information schema in which systems can parse and process easily, and Selective Context Filtering, which enables systems to systematically filter out specific irrelevant sections of contextual information that is fed through a system's NLP-based pipeline. We will explore how context-aware systems and applications can utilize these two techniques in order to implement dynamic LLM-to-system interfaces, improve an LLM's ability to generate more contextually cohesive user-facing responses, and optimize complex automated workflows and pipelines.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in Large Language Models (LLMs) have led to them becoming increasingly more capable and versatile via various scaling laws related to model parameter count and training dataset size [10, 18]. These developments have significantly improved the models' abilities to understand context, generate coherent responses, and adapt to a wide range of tasks without extensive retraining. As a result, LLMs are now better equipped to mimic human-like language comprehension and generation, enabling them to perform complex analytical tasks, such as summarizing lengthy documents, generating creative content, and providing insightful predictions [18]. Furthermore, their ability to learn and generalize from diverse datasets has opened new avenues for innovative applications, driving further research and deployment in various sectors.\nConsequently, there is a growing need for effectively capturing the nuances of natural language inputs and transforming them into formats that systems can easily parse and process [3]. This is because natural language is notoriously difficult to work with and design systems around due to it's free-form and fluid nature. For domains that deal with large amounts of complex natural language data such as Customer Service, Healthcare, Education, Finance, [2, 21], or any industry that requires constant processing of new documents [3], LLMs are becoming more desirable for integration into automated systems within these domains to enhance their decision-making capabilities, streamline redundant processes, and reduce operational costs in some specific sub-areas of these domains [21].\nIn order to enable a more cohesive integration of LLMs and implement a more scalable and a better structured natural language data processing schema for these systems, we propose two new techniques: Semantic Decomposition and Selective Context Filtering. These techniques induces a form of synthetic, controlled, and human-interpretable \"systematic reasoning\" within these systems, and can be integrated with other existing LLM-based techniques as well to improve an LLM-based agent's text generation to be more coherent with respect to its integrated system's context."}, {"title": "1.1 Context-Aware Systems", "content": "We define Context-Aware Systems as those that leverage LLM-based pipelines to dynamically adapt and respond to a wide range of contextual inputs. These systems are designed to understand, incorporate, and react to various forms of input data, allowing them to perform tasks with a high degree of relevance and precision. Integrating context-awareness in systems using LLM-based pipelines enhances their functionality, adaptability, and user-centricity across varied application domains.\nOne such example can be found in customer service platforms, where LLMs analyze customer inquiries and provide real-time assistance by drawing on context from prior interactions, customer history, and specific queries to offer personalized responses [2, 17]. In the healthcare domain, context-aware systems powered by LLMs can synthesize patient data and medical records to assist healthcare professionals in diagnosing conditions or recommending treatment plans. By utilizing specific patient histories and current symptomatology, these systems ensure that their outputs are tailored to the unique needs of each patient, thereby enhancing decision support processes [19].\nEducational technologies also benefit from context-aware systems, where LLMs personalize learning experiences by adapting to a student's progress and comprehension level. Such systems take into account a student's previous interactions, learning pace, and areas of difficulty to curate educational content and suggest resources that best suit the student's current understanding, promoting an individualized learning journey [2, 14].\nIn the realm of finance, context-aware systems utilize LLMs to analyze market trends, economic data, and investor behavior, providing financial analysts with insights and forecasts that are highly contextualized. By integrating real-time data streams and historical market information, these systems support timely and informed investment decisions [15]."}, {"title": "2 LIMITATIONS OF LARGE LANGUAGE MODELS", "content": "Context in LLMs is the set of specific input data that is used to generate an output response. More specifically, it is the set of text tokens that is fed into an LLM's transformer network, which combined with the transformer's attention mechanism, generates another set of output tokens that is a plausibly valid and contextually coherent sequence continuation of the set of input tokens. This is because the attention mechanism within transformers takes \"important\" tokens within the current context and assigns high relevance values for them [22], so your output sequence will be coherent with respect to your input.\nHowever, the transformer attention mechanism has a very inherent limitation, and that is its limited context window size [7]. This limitation comes from the fact that the attention mechanism requires a quadratic ($O(n^2)$) scaling in computational requirements as the input sequence length grows [22]. This effectively means that the longer your LLM generates an output sequence, the more likely that the head of the sequence will lose coherence with respect to the original input context.\nRecent advances have explored various methods to extend the context window by altering the attention mechanism to be more efficient in order to improve the LLM's ability to handle longer sequences [1, 24]. However, system designers and engineers aiming to integrate these LLMs within their systems still need to work around this context window size limitation because most use-cases can easily surpass the context size of even the largest transformer models in the current market\u00b9."}, {"title": "2.1 Context Window Size", "content": "Representational memory refers to the model's capability to retain and utilize information across different contexts and tasks. LLMS primarily rely on their weights to store and retrieve information. These weights encode vast amounts of knowledge during training. For an LLM, the ability to store unique information as memory is dictated by the effective parameter size of the model, meanwhile the ability to generate unique output responses to inputs is dictated by the size of the training dataset distribution [10].\nHowever, while LLMs have shown remarkable prowess in capturing short-term dependencies, they often struggle with long-term information retention and retrieval, which is crucial for tasks requiring sustained reasoning or recalling information presented earlier in a dialogue or document. Moreover, during inference, the model lacks explicit mechanisms to access or update this knowledge dynamically based on new contexts. As a result, LLMs may produce outputs that seem disconnected from prior established facts or exhibit inconsistency across an extended output sequences."}, {"title": "2.2 Representational Memory Limits", "content": "Hallucination in LLMs is a phenomenon wherein the generated output sequence is plausible, coherent, and grammatically correct, but is factually incorrect or logically nonsensical at times. The previously mentioned limitations largely contribute to hallucination phenomenon. Output sequences becoming incoherent with respect to the original context the longer it grows, or the model producing outputs that seem inconsistent from earlier priors are all forms of hallucination [9].\nDespite the representational complexity within LLMs, they are ultimately autoregressive next-token prediction models [22]. As long as the output sequence is probabilistically plausible according to the LLM's training data distribution, there will be a likelihood that hallucination will occur. Therefore, external methodologies are necessary in order for our LLM agents to produce coherent output consistently."}, {"title": "2.3 Hallucination", "content": "Retrieval Augmented Generation (RAG) enhances the capabilities of Large Language Models (LLMs) by integrating information retrieval mechanisms into the text generation framework [12]. This approach is particularly effective in improving the accuracy and relevance of the output by accessing and utilizing external information from pre-existing corpora or knowledge bases, which may not be directly encoded in the LLM's training data [6]."}, {"title": "3 EXISTING TECHNIQUES FOR ENHANCING LLM OUTPUTS", "content": "RAG is typically implemented using vector embeddings, where both the query (input context or prompt) and the documents in the knowledge base are transformed into fixed-size vector representations [12]. These vector embeddings are generated using pre-trained models, such as BERT or other transformer-based architectures, which capture semantic similarities between texts [6, 12]. When a query is made, the system compares the vector embedding of the query with those of the documents to retrieve the most relevant information. This retrieved information is then combined with the original input to generate more contextually aware responses."}, {"title": "3.1 Retrieval Augmented Generation", "content": "In addition to vector embeddings, graph embeddings are also utilized to implement RAG. Graph embeddings involve representing entities and their relationships in a knowledge graph, where nodes represent concepts or entities, and edges denote the relationships between them [23]. By embedding these graph structures into continuous vector spaces, LLMs can retrieve and integrate relational information that is structured and interconnected. This method is beneficial for applications that require a deep understanding of relationships and context, such as question answering or dialogue systems [4]."}, {"title": "3.1.1 Vector Embeddings", "content": "Chain-of-Thought (CoT) is a technique used to emulate human-like reasoning patterns. It involves breaking down complex problems or tasks into a series of manageable, sequential steps, allowing models to follow a logical progression in their response generation [25].\nIn the context of LLMs, implementing CoT involves training models to generate intermediate reasoning steps before arriving at a final conclusion or response. This approach helps the model to maintain coherence and logical consistency, as each step builds on the previous one. Chain-of-Thought is particularly valuable in tasks that require deductive reasoning, problem-solving, and multi-turn dialogue management, where the system must track and integrate various pieces of information over several interactions [8, 25].\nTo leverage CoT effectively, models can be designed to output intermediate steps as part of their response generation, encouraging transparency in the decision-making process. Additionally, during training, incorporating datasets that model logical reasoning paths and employing techniques such as reinforcement learning can enhance the model's ability to execute a coherent and structured chain of reasoning [25]. As a result, CoT aids in improving the interpretability and reliability of LLM outputs, making them more suitable for complex and nuanced applications [8]."}, {"title": "3.1.2 Graph Embeddings", "content": "Structured Outputs refer to the capability of Large Language Models (LLMs) to generate responses that adhere to a predefined format or structure, rather than producing free-form text [26]. This approach is particularly beneficial in applications that require precise and consistent information presentation, such as generating tables, filling out forms, producing JSON objects, or creating code snippets with specific syntax [20].\nTo implement structured outputs, LLMs are typically trained with data that includes examples of the desired output format. This training helps the model understand the rules and structure necessary to produce outputs that conform to specific templates. Moreover, prompt engineering can be used effectively to guide the model towards generating structured data. By providing well-formatted examples and specifying clear instructions in the input prompt, users can influence the LLM to follow a particular structure in its responses [16, 20].\nAnother key technique in achieving structured outputs involves aligning the model's language generation process with task-specific constraints. For instance, using schemas or templates as a guide during generation can help LLMs maintain consistency in data fields and attributes. Additionally, incorporating post-processing validation steps ensures that the generated outputs meet structural requirements, correcting any deviations or errors introduced during the generation phase [20].\nBut the most important method for ensuring the consistency of structured outputs is the utilization of Context-Free Grammars (CFGs) [26]. CFGs provide a formal way to define the permissible structure of output by specifying the syntax rules that the generated text must adhere to. This method is particularly valuable in enforcing uniformity across outputs, as CFGs can systematically constrain the model's outputs to match the desired structural patterns. By integrating CFGs into the generation process, LLMs can be nudged to produce outputs that are not only semantically rich but also structurally consistent with the specified requirements.\nStructured Outputs are essential in domains where uniformity and clarity are crucial, such as data entry automation, report generation, and interactions with APIs. By ensuring that outputs are structured, LLMs can improve accuracy, facilitate downstream processing, and provide more user-friendly and actionable information [16]. As models evolve, expanding their capabilities to produce various and complex structured outputs will remain a pivotal area of research and application development."}, {"title": "3.2 Chain-of-Thought", "content": "In this section, we propose two novel techniques designed to enhance the performance and adaptability of context-aware systems: Semantic Decomposition and Selective Context Filtering. These methodologies aim to improve how information is structured and utilized within these systems, enabling them to interact with Large Language Models (LLMs) more effectively."}, {"title": "3.3 Structured Outputs", "content": "This technique involves sequentially breaking down input prompts based on a predefined, hierarchal manner. The aim is to let an LLM agent pipeline break down input prompts in a way that conforms to a system's required schema. By decomposing prompts into discrete, manageable segments, automated systems can interpret and execute complex pipelines based on natural language commands with precision, thereby facilitating more accurate and relevant response generation.\nUnlike in CoT, this process not only optimizes the system's input parsing capabilities, but this also improves the fine-grain control of the system's \"reasoning process\". This is because the reasoning of the system is bounded within the schema's framework that the system designer supplies the LLM agents with."}, {"title": "4 APPROACH", "content": "Current LLM-based text generation pipelines often involve inputting whole sections of context into an LLM (usually the last nth items of an input context), along with external retrieved context snippets (via RAG), without refinement. It is a well understood property of LLMs that irrelevant snippets of information in your inputs may lead to short-term decoherence. This is why most modern RAG algorithms perform re-ranking in order to alleviate decoherence due to uncorrelated pieces of retrieved context.\nTo approach this issue, we propose Selective Context Filtering in which we embed our sections of input context, and consecutively remove irrelevant sections with embedding-based filters as the context data moves along our pipeline."}, {"title": "4.1 Semantic Decomposition", "content": "The process involves instructing our LLMs to discern and categorize the inputs from a set of pre-defined classes from a schema. We sequentially go down the schema's depth if the discerned category if the category is not a leaf class of the hierarchy. We essentially turn our LLM agent into a pseudo-classification model via Structured Outputs, so our output is guaranteed to follow a schema that is parsable by the integrated system."}, {"title": "4.2 Selective Context Filtering", "content": "In our method, each segment of the input context is processed through embedding models that capture semantic similarity and relevance. These embeddings are then matched against the task requirements or user query to evaluate their pertinence. By continuously applying this filtering, we ensure that only the most relevant pieces of information are retained. This helps maintain clarity and coherence in the generated outputs by reducing noise from unrelated context segments.\nThe use of embedding-based filters adds an adaptive layer capable of dynamically prioritizing context based on its relevance score. This process benefits from advancements in contextual embeddings that can factor in subtle semantic relationships, reducing the likelihood of context misalignment. Furthermore, by systematically pruning irrelevant data, the computational cost and memory usage during inference are also optimized, enhancing the system's efficiency."}, {"title": "5 EXPERIMENTS", "content": "To benchmark our proposed techniques, we created a synthetic datasets mimicking natural prompt utterances and complete assistant conversations for a context-aware system. The synthetic data is generated using 135 of the most commonly searched topics from Google in 2024 over a broad range of domains.\nFirst, we created a sample hierarchal schema for a hypothetical context-aware system shown in Fig. 4. This is to have a baseline guide on how to generate our data in a structured manner. Based on the hierarchal schema we created, we synthesized two datasets, namely Synthetic Single Prompt Dataset (SynPrompt), and Synthetic Assistant Conversations Dataset (SynAsst). SynPrompt contains synthesized utterances of multiple sentence lengths ranging from simple one sentence prompts up to complex five sentence paragraphs. Meanwhile SynAsst contains complete context trees of length ten and twenty (plus 1 synthetic user reference prompt)."}, {"title": "5.1 Synthetic Context-Aware System Dataset", "content": "LLM-based benchmarks often do not have well-defined baselines due to how unstructured and varied the domain of natural language is [13], so we propose a novel form of scoring to compute the performance of our techniques. We base our scoring on how consistent the LLM-agents generate a structured decision called Exponential Consistency Index (ECI) with the given formula:\n$S = \\frac{\\sum_{i=1}^{d} \\binom{k}{k_i} \\alpha^{k-k_i}}{\\sum_{i=1}^d \\binom{k}{k_i}}$"}, {"title": "5.2 Setup and Scoring", "content": "where S is the consistency index of a given output to evaluate, d is the total number of items to evaluate for a given output entry, ki is the correct number of evaluations for a given item, k is the total amount of evaluations, and \u03b1 is the exponential penalty term that decides how much to penalize mistakes across k evaluations for an item.\nFor Semantic Decomposition, the task is to evaluate the list of types of a given input sentence in a hierarchal fashion using the schema in Fig. 4.\nFor Selective Context Filtering, the task is to select which pieces of context in the conversation tree is relevant to the latest \"user\" prompt. We use both LLM-based evaluations and vector-based embeddings (threshold) to select the most relevant pieces of context. Furthermore, we calculate the accuracy of the extracted pieces of context by comparing it to whether the synthetic \"user\" prompt was instructed to diverge from the main topic.\nFor all setups, the input prompt's context information will either be pre-processed or not. If the context information is decomposed beforehand, it will be injected at every step of the evaluation. This is to test whether a more contextualized generation will result to more consistent outputs.\nAlongside our datasets SynPrompt and SynAsst, we evaluate our methods over the OpenAssistant Conversations dataset (OASST1 and OASST2) [11] in order to evaluated our methods over natural prompts and conversations."}, {"title": "5.3 Results and Discussion", "content": "For all the experiments, a subset of 150 entries will be utilized as the test datasets, in line with current standards for LLM-based evaluation setups [5]. Furthermore, a value of \u03b1 = 2.5 for ECI-based scoring will be used. We denote experiments without decomposing the initial context prompt as Normal, and C.D. if it was decomposed preemptively. For the LLM-agents, gpt-4o and gpt-40-mini are chosen.\nIn evaluating Semantic Decomposition, we perform ECI-based scoring over SynPrompt (1-5 sentences), OASST1 and OASST2. We calculate the consistency of the decomposed schema list over k = 5 evaluations per entry.\nIn the Selective Context Filtering experiments, we perform ECI-based scoring over SynAsst (5-pair and 10-pair), along with OASST1. We calculate the consistency of the indices list of extracted context pieces over k = 5 evaluations per entry.\nFor calculate the accuracy of the extracted context list, we perform the evaluations only over SynAsst (5-pair and 10-pair). We utilize both LLM-based and embedding vector-based methods. The accuracy for LLM-based methods is calculated k = 5 evaluations per entry, while for vector-based methods, k = 1 only (because vector-embedding models produce consistent outputs).\nIn analyzing the results of the experiments, we can see that having no pre-decomposed context being injected in all stages of evaluation produces more consistent results. We hypothesize this is because having too context tokens during evaluation can derail some types of evaluation due to ambiguity.\nHowever, too little context information seems to lessen the consistency, with the 3-sentence subset of SynPrompt with no Context Decomposition having the highest Semantic Decomposition ECI scores for both Normal and C.D. We posit that a balance needs to be achieved in how much context tokens needs to be passed on during all stages of evaluation.\nFor Selective Context Filtering accuracy, the vector-based methods performed marginally better than LLM-based ones. We initially hypothesized that LLM-based methods would perform better due to having more complex internal mechanisms in transformer models, but it seems that embedding vector-based methods still works better for similarity and relevancy calculations between different pieces of text.\nSurprisingly, gpt-40-mini produces more consistent evaluations across the board despite being a \"weaker\" model than gpt-4o due to having a lower model parameter count. This is the case for Selective Context Filtering accuracy as well, with text-embedding-3-small having the most accurate relevancy extraction. We currently do not know the reasoning behind why the smaller models perform better on our techniques, so more future work still needs to be done with regards to analysis in this aspect."}, {"title": "6 CONCLUSION", "content": "In conclusion, the integration of advanced techniques such as Semantic Decomposition and Selective Context Filtering within context-aware systems significantly enhances the capability and adaptability of LLM-based pipelines. These methodologies provide a structured framework that improves information parsing, ensures contextual relevance, and facilitates dynamic interactions between systems and users. By implementing these strategies, systems can deliver more contextually coherent and user-tailored responses, proving beneficial across diverse fields such as customer service, healthcare, education, and finance. Leveraging these enhanced capabilities will be vital in addressing complex challenges, optimizing automated workflows, and enabling more sophisticated applications. Ultimately, this approach underscores the potential of LLMs to transform how systems interpret and respond to contextual information, paving the way for more intelligent and responsive technology solutions. Future work will aim at refining and evaluating these techniques further, exploring additional decomposition techniques, utilizing other RAG algorithms, and testing on a wider set of models."}]}