{"title": "COMMENTATOR : A Code-mixed Multilingual Text Annotation Framework", "authors": ["Rajvee Sheth", "Shubh Nisar", "Heenaben Prajapati", "Himanshu Beniwal", "Mayank Singh"], "abstract": "As the NLP community increasingly addresses challenges associated with multilingualism, robust annotation tools are essential to handle multilingual datasets efficiently. In this paper, we introduce a code-mixed multilingual text annotation framework, COMMENTATOR, specifically designed for annotating code-mixed text. The tool demonstrates its effectiveness in token-level and sentence-level language annotation tasks for Hinglish text. We perform robust qualitative human-based evaluations to showcase COMMENTATOR led to 5x faster annotations than the best baseline. Our code is publicly available at https://github.com/ lingo-iitgn/commentator. The demonstration video is available at https://bit.ly/ commentator_video.", "sections": [{"title": "Introduction", "content": "Code mixing is prevalent in informal conversations and in social media, where elements from different languages are interwoven within a single sentence. A representative example in Hinglish such as \u201cI am feeling very thand today, so I'll wear a sweater.\u201d (In this sentence, \u201cthand\u201d is a Hindi word meaning \"cold\", while the rest of the sentence is in English), demonstrating seamless integration of Hindi and English. A major challenge in NLP research is the scarcity of high-quality datasets, which require extensive manual efforts, significant time, domain expertise, and linguistic understanding, as highlighted by Hovy and Lavid (2010). The rise of social media has further complicated annotation tasks due to non-standard grammar, platform-specific tokens, and neologisms (Shahi and Majchrzak, 2022). Annotating these datasets presents unique challenges, including ensuring data consistency, efficiently managing large datasets, mitigating annotator biases, and reporting poor-quality instances. Existing annotation tools often fail to address these diverse issues effectively.\nThis paper introduces COMMENTATOR, a robust annotation framework designed for multiple code-mixed annotation tasks. The current version\u00b9 of COMMENTATOR supports two token-level annotation tasks, Language Identification, POS tagging, and sentence-level Matrix Language Identification. While COMMENTATOR has already been used to generate a large number of annotations (more than 100K) in our ongoing project\u00b2, these are not part of the current demo paper. The focus of this paper is to present the capabilities and initial functionalities of the framework. Figure 1 presents the framework COMMENTATOR.\nWe evaluate COMMENTATOR by comparing its features and performance against five state-of-the-art text annotation tools, (i) YEDDA (Yang et al., 2018), (ii) MarkUp (Dobbie et al., 2021), (iii) INCEPTION (Klie et al., 2018), (iv) UBIAI\u00b3 and (v) GATE (Cunningham et al., 1996). The major perceived capabilities (see Section 4.1) of COMMENTATOR are (i) simplicity in navigation and performing basic actions, (ii) task-specific recommendations to improve user productivity and ease the"}, {"title": "Existing Text Annotation Frameworks", "content": "Text annotation tools are vital in NLP for creating annotated datasets for training and evaluating machine learning models. This summary reviews several key tools, each with unique features and limitations."}, {"title": "Web-based Annotation Tools", "content": "These tools have been created to provide annotation environments independent of operating systems. Some of the web-based annotation tools are: (1) MarkUp improves annotation speed and accuracy using NLP and active learning but requires re-annotation for updates and has unreliable collaboration features (Dobbie et al., 2021), (2) INCEPTION offers a versatile platform for semantic and interactive annotation but struggles with session timeouts and updating annotations (Klie et al., 2018), and lastly, (3) UBIAI provides advanced cloud-based NLP functions but faces problems with incorrect entity assignments and model integration (ubi, 2022)."}, {"title": "Locally-hosted Tools", "content": "These tools can be installed on a local machine and offer more robust features or better performance for large datasets. Some of the locally hosted tools are: (1) YEDDA is an open source tool that enhances annotation efficiency and supports collaborative and administrative functions, though it has limitations in customization and can break tokens during annotation (Yang et al., 2018), (2) GATE is an open-source tool known for its real-time collaboration, but it is complicated to configure and slow with API requests (Bontcheva et al., 2013), (3) BRAT is user-friendly for entity recognition and relationship annotation but lacks active learning and automatic suggestions (Stenetorp et al., 2012), (4) Prodigy integrates with machine learning workflows and supports active learning but requires a commercial license (Montani and Honnibal, 2018), and (5) Doccano is an open-source tool with a customizable interface for various annotation tasks but lacks advanced features like real-time collaboration (Nakayama et al., 2018). Additional tools include (6) Knowtator, designed for biomedical annotations within Prot\u00e9g\u00e9, but requires significant manual setup (Ogren, 2006), (7) WordFreak, which is flexible but challenging for non-technical users (Morton and LaCivita, 2003), (8) Anafora, known for its efficiency in biomedical annotation but lacking integration with machine learning models (Chen and Styler, 2013), (9) Atomic, which is modular and powerful but requires extensive customization (Druskat et al., 2014), lastly, (10) WebAnno supports a wide range of annotation tasks and collaborative work, but encounters performance issues with large datasets (Yimam et al., 2013).\nWhile these tools offer diverse functionalities, each exhibits limitations that affect efficiency and usability. Most state-of-the-art frameworks are either paid or closed-source and do not support annotator feedback. Additionally, the majority do not enable parallel annotations over the internet and perform poorly when multiple scripts or words from different languages appear in the same sentence. The introduction of COMMENTATOR seeks to address these challenges by providing a robust framework specifically designed for multiple code-mixed annotation tasks."}, {"title": "COMMENTATOR", "content": "The proposed system caters to two types of users: (i) the annotators and (ii) the admins. Annotators perform annotation tasks. The admins design the annotation task, employ annotators, administer the annotation task, and process the annotations. Given these roles, we describe the COMMENTATOR functionalities by introducing:"}, {"title": "The Functionalities", "content": "The annotator panel contains three pages:\n1. Landing page: Figure 2 presents an annotator landing page. Here, the annotators are presented with a selection of several NLP tasks, displayed as clickable options. Selecting a task directs them to the dedicated annotation page for that specific task.\n2. Annotation pages: We, next, describe annotation pages for the first three tasks:\n\u2022 Token-Level Language Identification (LID): This task involves identifying the language of individual words (tokens) within a sentence (Figure 3a, point 1). Each token is pre-assigned a language tag using a state-of-the-art language identifi-"}, {"title": "The Annotator Panel", "content": "cation API 5(more details are presented in Section 3.2.2). Annotators can update these tags by clicking the tag button until the desired tag appears. Textual feedback can be entered in the \"Enter Your Feedback Here\" section (Figure 3a, point 3). Textual feedback is essential to highlight issues with the current sentence. Some issues include grammatically incorrect sentences, incomplete sentences, sensitive/private information, toxic content, etc.\n\u2022 Token-Level Parts-Of-Speech Tagging (POS): Similar to LID, this task involves identifying the POS tags of individual tokens within a text. Each token is pre-assigned a language tag using a state-of-the-art POS tagging CodeSwitch NLP library (more details are presented in Section 3.2.2). In case of incorrect assignment of the tag, the annotators can select the correct tag from a drop-down menu (Figure 4a, point 1). We do not keep the toggling button feature due to many POS tags. Similarly to LID, annotators can provide"}, {"title": "The Admin Panel", "content": "Figure 6 shows the admin panel. The admin panel performs three major tasks:\n1. Data upload: The administrator can upload the source sentences using a CSV file (Figure 6, point 1).\n2. Annotation analysis: The administrator can: (i) analyze the quality of annotations using Cohen's Kappa score for inter-annotator agreement (IAA) (Figure 6, point 3) and (ii) analyze the degree of code-mixing in the annotated text using the code-mixing index (CMI) (Das and Gamb\u00e4ck, 2014a)7(Figure 6, point 2).\n3. Data download: The admin can download annotations of single/multiple annotators in a CSV file. Admins can select specific tasks from a dropdown menu to customize the data extraction (Figure 6, point 2) The data download functionality also supports the conditional filtering of data based on IAA and CMI."}, {"title": "The Architecture", "content": "Figure 1 showcases the highly modular architecture for COMMENTATOR. We describe it using two main modules:"}, {"title": "Client Module", "content": "The client is developed using ReactJS\u00ae. The client module comprises pages for the following functionalities: (i) User Login, (ii) User Signup, (iii) Annotation Panel, and (iv) History, and (v) Admin Panel. The user login page is used to log into the portal. The user signup page creates a new annotator account on the portal. The annotation panel is the main landing page that initiates the annotation process for all tasks. The history page lists the annotated sentences by the logged-in annotator for individual tasks."}, {"title": "Server Module", "content": "The client is served using a Flask Server. The server performs two major functions: (i) connection with the database and (ii) calling task-specific API/libraries. It connects to the MongoDB database through a Pymongo library. The MongoDB database can be locally hosted or on the cloud. We use the MongoDB Atlas database10 hosted locally. In the current setup, we use Microsoft API for LID\u00b91. For POS, we use the CodeSwitch NLP library. This also demonstrates the flexibility of COMMENTATOR to make web-based API calls or local-hosted library calls based on the task requirements."}, {"title": "Experiments", "content": "In this section, we perform two human studies to evaluate COMMENTATOR against recent state-of-the-art tools to ensure a comprehensive comparison with modern advancements and cutting-edge functionalities: (i) YEDDA (Yang et al., 2018), (ii) MarkUp (Dobbie et al., 2021), (iii) INCEPTION (Klie et al., 2018), (iv) UBIAI12, and (v) GATE (Bontcheva et al., 2013). The first study assesses the total time and perceived capabilities during the initial low-level setup and at higher-level annotation tasks (see Section 4.1 for more details). The second study examines the annotation time (see Section 4.2 for more details)."}, {"title": "Initial Setup and Perceived Capabilities", "content": "We employ three human annotators proficient in English and Hindi with experience using social media platforms such as X (formally \u2018Twitter'). Additionally, the annotators are graduate students with good programming skills and knowledge of version control systems. Each annotator has a detailed instruction document13 containing links to execute codebases or access the web user interface, descriptions of tool configurations, annotation processes, and guidelines for recording time.\nEach annotator measures the time taken for the initial setup, including installation and configuration. The initial setup includes installation (downloading source code, decompressing, and installing dependencies) and configuration (adding config-"}, {"title": "Annotation Time", "content": "In the second human study, we recruit three annotators with a good understanding of Hindi and English languages14. Each annotator annotates ten Hinglish sentences (available on the project's GitHub page) for token-level language tasks: (i) LID and (ii) POS. Both tasks involve assigning a tag to each token in a sentence. For LID, the tags are Hindi, English, Unidentified. For POS, we follow the list of tags proposed by Singh et al. (2018). This list includes NOUN, PROPN, VERB, ADJ, ADV, ADP, PRON, DET, CONJ, PART, PRON_WH, PART_NEG, NUM, and X. Here, X denotes foreign words, typos, and abbreviations.\nTable 3 shows that the libraries that preassign tags enable COMMENTATOR to perform at least five times faster in annotation than the existing tools. Overall, annotators find that COMMENTATOR takes slightly longer time in initial setup but significantly reduces annotation time and efforts. It showcases good recommendation capability, parallel annotations and post-annotation analysis capabilities."}, {"title": "Conclusion and Future Work", "content": "We introduce COMMENTATOR, an annotation framework for code-mixed text, and compared it against five state-of-the-art annotation tools. COMMENTATOR shows better user collaboration, operational ease, and efficiency, significantly reducing annotation time for tasks like Language Identification and Part-of-Speech tagging. Future plans include expanding COMMENTATOR to support tasks such as sentiment analysis, Q&A, and language generation, making it an even more comprehensive tool for multilingual and code-mixed text annotation."}, {"title": "Ethics", "content": "We adhere to the ethical guidelines by ensuring the responsible development and use of our annotation tool. Our project prioritizes annotator well-being, data privacy, and bias mitigation while promoting transparency and inclusivity in NLP research."}, {"title": "Appendix", "content": null}, {"title": "Inter-annotator agreement (IAA)", "content": "IAA measures how well multiple annotators can make the same annotation decision for a particular category. IAA shows you how clear your annotation guidelines are, how uniformly your annotators understand them, and how reproducible the annotation task is. Cohen's kappa coefficient (Hallgren, 2012; Cohen, 1960) is a statistic to measure the reliability between annotators for qualitative (categorical) items. It is a more robust measure than simple percent agreement calculations, as k considers the possibility of the agreement occurring by chance. It is a pairwise reliability measure between two annotators.\nThe formula for Cohen's kap\u0440\u0430 (\u043a) is:\n$\\\u041a=\\frac{Po - Pe}{1- Pe}$                                                                                                                              (1)\nwhere, $P_o$ is relative observed agreement among raters and $P_e$ is hypothetical probability of chance agreement."}, {"title": "Code-mixing Index (CMI)", "content": "CMI metric (Das and Gamb\u00e4ck, 2014b) is defined as follows:\n$CMI = {\\begin{cases}100 * 1 - \\frac{max(w_i)}{n-u} & \\text{if } n > u\\\\0 & \\text{if } n = u\\end{cases}}$                        (2)\nHere, $w_i$ is the number of words of the language i, max{wi} represents the number of words of the most prominent language, n is the total number of tokens, u represents the number of language-independent tokens (such as named entities, abbreviations, mentions, and hashtags). A low CMI score indicates monolingualism in the text whereas the high CMI score indicates the high degree of code-mixing in the text."}, {"title": "Limitations", "content": "We present some of the limitations in the COMMENTATOR tool, along with potential areas for future improvement:"}, {"title": "Web-hosting:", "content": "COMMENTATOR is not currently web-based, but we are developing a web version to improve accessibility and user experience."}, {"title": "Model Integration:", "content": "The tool does not yet support direct integration of pre-trained models through the user interface for predictions."}, {"title": "Post-annotation Analysis:", "content": "While offering basic post-annotation analysis, future versions will include task-specific metrics such as Fleiss' Kappa, Krippendorff's Alpha, and Intraclass Correlation for more detailed evaluations of inter-annotator reliability and annotation accuracy."}]}