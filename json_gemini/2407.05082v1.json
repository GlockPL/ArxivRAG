{"title": "DMTG: One-Shot Differentiable Multi-Task Grouping", "authors": ["Yuan Gao", "Shuguo Jiang", "Moran Li", "Jin-Gang Yu", "Gui-Song Xia"], "abstract": "We aim to address Multi-Task Learning (MTL) with a large number of tasks by Multi-Task Grouping (MTG). Given N tasks, we propose to simultaneously identify the best task groups from $2^N$ candidates and train the model weights simultaneously in one-shot, with the high-order task-affinity fully exploited. This is distinct from the pioneering methods which sequentially identify the groups and train the model weights, where the group identification often relies on heuristics. As a result, our method not only improves the training efficiency, but also mitigates the objective bias introduced by the sequential procedures that potentially lead to a suboptimal solution. Specifically, we formulate MTG as a fully differentiable pruning problem on an adaptive network architecture determined by an underlying Categorical distribution. To categorize N tasks into K groups (represented by K encoder branches), we initially set up KN task heads, where each branch connects to all N task heads to exploit the high-order task-affinity. Then, we gradually prune the KN heads down to N by learning a relaxed differentiable Categorical distribution, ensuring that each task is exclusively and uniquely categorized into only one branch. Extensive experiments on CelebA and Taskonomy datasets with detailed ablations show the promising performance and efficiency of our method. The codes are available at https://github.com/ethanygao/DMTG.", "sections": [{"title": "1. Introduction", "content": "Many real-world applications are essentially complex systems that involve the collaboration of a large number of tasks. For example, in autonomous driving (Caesar et al., 2020; Hu et al., 2023), the system needs to simultaneously perform lane detection (Tang et al., 2021), depth estimation (Godard et al., 2019), vehicle detection and instance segmentation (He et al., 2017; Cheng et al., 2021), pedestrians localization (Bertoni et al., 2019), etc. In order to tackle these real-world challenges, it is crucial to simultaneously learn a large number of diverse tasks within a Multi-Task Learning (MTL) framework (Kokkinos, 2017; Nekrasov et al., 2019; Dvornik et al., 2017; Bilen & Vedaldi, 2016; Zamir et al., 2016; Li & Gong, 2021; Wang & Tsvetkov, 2021; Liu et al., 2020; Yu et al., 2020), which reduces the inference time and facilitates an improved performance by leveraging the affinity among different tasks."}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Multi-Task Grouping", "content": "Multi-Task Grouping (MTG) aims to put collaborative tasks from a task pool into the same group, where a group of tasks can be learned efficiently by a shared network (Kang et al., 2011; Kumar & III, 2012; Li et al., 2021). Grouping tasks enables efficient learning of a vast array of tasks while also maintaining high interpretability. However, the primary challenge in MTG is that finding an optimal grouping solution in $2^{N-1}$ grouping candidates can be difficult. Existing grouping methods (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022) have attempted to model an evaluation function to determine high-order task relationships based on low-order observations. Nonetheless, these methods perform group identification and grouped task learning separately, and potentially considering only low-order task affinity. In contrast, our grouping approach integrates group identification and grouped task learning within a one-shot training process, significantly improving running efficiency in large-scale task scenarios while thoroughly considering higher-order task relationships."}, {"title": "2.2. Multi-Task Architecture", "content": "Multi-Task Architecture (MTA) (Caruana, 1997; Argyriou et al., 2008; Kang et al., 2011; Ruder, 2017; Vandenhende et al., 2021) is a prevailing technology line in the Multi-Task Learning domain. It can be categorized as hard-parameter sharing (Kokkinos, 2017; Nekrasov et al., 2019; Dvornik et al., 2017; Bilen & Vedaldi, 2016; Zamir et al., 2016) and soft-parameter sharing (Gao et al., 2019; 2020; 2024; Long et al., 2015; 2017; Misra et al., 2016). The former shares a common feature extraction module among tasks, while the latter assigns a special feature extraction branch for each task, exchanging features through extra fusion modules. Although great success has been witnessed in designing novel MTL network architectures, they are less appropriate in addressing an extreme large number of tasks. Specifically, it is difficult to avoid the negative transfer due to a full-shared encoder module in hard-parameter sharing methods (Vandenhende et al., 2019; Guo et al., 2020; Br\u00fcggemann et al., 2020; Sun et al., 2020), while soft-parameter sharing methods (Ruder et al., 2019; Zhang et al., 2018; Xu et al., 2018; Zhang et al., 2019b) better address the negative transfer but introduce efficiency issues."}, {"title": "2.3. Multi-Task Optimization", "content": "Multi-Task Optimization (MTO) develops in parallel with Multi-Task Architecture, which aims to adjust task loss to balance the learning process of different tasks (Kendall et al., 2018; Chen et al., 2018c; Liu et al., 2019b; Lin et al., 2022; Chen et al., 2020; Guo et al., 2018). Advanced MTO methods directly manipulate gradients from different tasks to mitigate the training conflicts (Li & Gong, 2021; Wang & Tsvetkov, 2021; Liu et al., 2020; Yu et al., 2020), e.g., projecting task gradients when their angle is greater than 90\u00b0. In practice, revising gradients necessitates additional memory to store the gradient graph for each task, which can be potentially infeasible when dealing with an extremely large number of tasks. Most recently, Kurin et al. (2022) and Xin et al. (2022) reveal that the existing MTO methods may be sensitive to hyperparameters when dealing with different combinations of tasks. Our method aims to learn the categorization of tasks and is orthogonal to MTO methods."}, {"title": "2.4. Network Pruning", "content": "Network pruning (Cai et al., 2018; Chen et al., 2018b; Elsken et al., 2019; Ghiasi et al., 2019; He et al., 2020; Li et al., 2019) aims to detect and remove the redundancy of the networks without significant performance degradation. This pruning process can be implemented by Bayesian optimization (Bergstra et al., 2013), evolutionary algorithms (Real et al., 2019; Xie & Yuille, 2017), network transformation (Gordon et al., 2018), reinforcement learning (Guo et al., 2019; Tan et al., 2019; Zoph et al., 2018), and gradient descent (Akimoto et al., 2019; Liu et al., 2019a; Wu et al., 2019; Zhang et al., 2019a). We use differentiated pruning operations, which effectively enable integrating group identification with grouped task learning jointly in one-shot training. We are the first to implement network pruning into MTG to unify group identification and grouped task learning in an end-to-end architecture."}, {"title": "3. Method", "content": "Given N tasks, we aim to efficiently chase the best categorization from the $2^N$ possibilities, with the high-order task affinities directly exploited. To this end, we formulate MTG into a network pruning framework, where we model the group identification as the architecture learning/pruning, and the grouped task learning as the model weights optimization under a certain architecture. As a result, the group identification and the grouped task learning are unified and can be jointly optimized in one-shot during the network pruning. Regarding the optimization, we design the group categorization as the learning of a Categorical distribution, which is then continuously relaxed into a differentiable Concrete distribution and subsequently optimized using the Gumbel softmax (Maddison et al., 2016).\nIn summary, our method is able to 1) exploit the high-order task affinities directly. 2) It avoids the potential objective bias when group identification and grouped task learning act as separated sequential procedures. 3) Given K groups, our pruning algorithm preserves the efficiency of O(K) training complexity for the encoder. 4) Our Categorical distri-"}, {"title": "3.1. Problem Formulation", "content": "Formally, we consider categorizing a set of N tasks T = {$T_1$, ..., $T_N$} into equal or less than K groups G = {$G_1$, ..., $G_K$}, such that each group contains 0 to N tasks $G_k$ = {..., $T_i$, ...}, and each task is exclusively and uniquely belongs to one group. Therefore, we have:\nT = $\\cup_{k=1}^K \\mathcal{G}_k$,\ns.t. $\\forall k, |\\mathcal{G}_k| \\in \\{0, ..., N\\}$,\n$\\forall (i, j), \\mathcal{G}_i \\cap \\mathcal{G}_j = \\emptyset$,\n                                        (1)\nwhere |\u00b7| is the cardinality. We optimize our problem exclusively to attain the highest average performance across these N tasks, without relying on heuristic criteria. We also note that K is the maximal-allowed number of groups, and we do not impose a strict requirement to yield precisely K groups, e.g., some groups may contain 0 task.\nObjective Bias in Two-Stage MTG Methods. The objective bias in pioneering (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022) appears in two aspects: 1) the group categorization is determined by heuristics but the retraining is based on the optimization of task losses, and 2) the difference in the inputs to the group identification and the grouped-model weights retraining stages lead to different objectives. In other words, the groups are identified heuristically when all the N tasks can synergy/regularize each"}, {"title": "3.2. Grouped Task Learning Module", "content": "We start with K branches in the grouped task learning module, where each branch represents the encoder of each task group. We connect each branch to N task heads to predict all the N tasks, facilitating the exploration of high-order task affinity. Our method possesses an efficient training complexity of O(K) for the network encoder.\nOur method also enables to further reduce the training complexity, by implementing optional group-wise shared layers before splitting into the group-specific branches. This is illustrated in the dashed gray box in Fig. 2."}, {"title": "3.3. Group Identification Module", "content": "We model the categorization of N tasks exclusively and uniquely into K groups as the learning of an unknown Categorical distribution, where the Categorical distribution is used to determine an adaptive network architecture. As such, the underlying Categorical distribution can be optimized jointly with the model weights in one-shot, which we formulate as a pruning problem."}, {"title": "3.4. The Joint Optimization", "content": "Equation (3) involves a discrete sampling from $s_{ik}$ to $z_{ik}$, which results in a gradient blockage in Eq. (4) when back-propagating gradients from Z to S. In this section, we continuously relax the discrete Categorical distribution, so that both the parameters for group identification S and the weights for grouped task learning \u03b8 can be jointly optimized in one-shot by back-propagating the gradients from the task loss $L_{task}(\u03b8, S)$, through Z(S) and L(\u03b8), respectively.\nContinuous Relaxation. By using the reparameterization trick from the Concrete distribution (Maddison et al.,"}, {"title": "4. Experiments", "content": "In this section, we extensively validate our method on both Taskonomy (Zamir et al., 2018) and CelebA (Liu et al., 2015) datasets for various candidate groups. We detail the experimental setup in the following."}, {"title": "4.1. Experimental Setup", "content": "Datasets. We perform experiments on the Taskonomy dataset (Zamir et al., 2018) following (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022), and the CelebA dataset (Liu et al., 2015) following (Fifty et al., 2021). We use the official tiny train, validation, and test split of Taskonomy. The images from Taskonomy and CelebA are bilinearly down-sampled to 256 \u00d7 256 and 64 \u00d7 64, respectively. Those datasets are introduced in detail in Appendix A.\nBenchmark Experiments. We follow the experiment setups in (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022) to conduct 5 tasks on Taskonomy, i.e., semantic segmentation, depth estimation, surface normal, keypoint detection, and edge detection, denoted as Taskonomy-5. We also conduct 9 tasks on CelebA dataset following (Fifty et al., 2021), i.e., 5_o_Clock_Shadow, Black_Hair, Blond_Hair, Brown Hair, Goatee, Mustache, No Beard, Rosy_Cheeks, and Wearing Hat, referred to as CelebA-9. We perform"}, {"title": "4.2. Experiments on Taskonomy with 5 Tasks", "content": "Following (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022), we compare our methods with the state-of-the-art MTG methods including HOA (Standley et al., 2020), TAG (Fifty et al., 2021), and MTG-Net (Song et al., 2022).\nWe also report the performance of Random Group (RG) which randomly divides the input tasks into a specific number of groups. We illustrate the baseline performance with Naive MTL, where all the tasks are trained simultaneously with a fully-shared encoder (i.e., within 1 group). The performance where each task is trained separately without grouping is denoted as Single Task Learning (STL). We perform candidate numbers of groups as 3, 4, and 5.\nThe results in terms of losses are shown in Table 1. Our method outperforms SOTA methods by a large margin with a more efficient O(K) training complexity for the encoder, we give detailed training time in Appendix B. Our method reduces the total loss by 22% compared to naive MTL and by 13% compared to STL when K = 3. As K increases, our grouping performance further improves. Regarding the normalized metric NormGain w.r.t. loss, i.e., Eq. (7), it also achieves a remarkable improvement of over 60% w.r.t. naive MTL. Consistent observation can be obtained regarding the error statistics (i.e., Eq. (8)), as shown in Appendix C.\nIt can be observed that the performances in terms of total loss and NormGain are not consistent for some MTG methods. For example, in Table 1 at K = 3, the NormGain of MTG-Net is over 10% higher than that of HOA, given that their total losses are comparative. This is because, as discussed in Evaluation Metrics of Sect. 4.1, the total loss is affected by loss magnitudes associated with different tasks, a slight improvement in a task with a large loss magnitude might overshadow a significant degradation in a task with a small loss magnitude. In contrast, the NormGain metrics address this issue by eliminating such undesirable influence through normalization, providing a more reasonable measurement w.r.t. the improvement of all the tasks. We further validate this by illustrating the loss and the relative gain w.r.t. Naive MTL for each task in Table 2, where our method achieves the best performance across almost all tasks.\nWe also show the training complexity of the heavy encoder relative to the Naive MTL method in Table 2. Given K << N, Our method achieves the best training efficiency except"}, {"title": "5. Ablation Analysis", "content": "We carefully investigate the following issues by ablation. 1) Whether our proposed one-shot MTG outperforms the common practice of two-shot methods (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022) given the same group categorization in Sect. 5.1. 2) Can our method generalize to the transformer backbones in Sect. 5.2. 3) The flexibility if we share more or less encoder layers in our method in Sect. 5.3. 4) Can our method scale to more input tasks in Appendix D. 5) The influence of different Gumbel Softmax temperatures in Appendix E. 6) The group categorization identified by different methods in Appendix F."}, {"title": "5.1. Merits of One-shot Nature for MTG", "content": "The one-shot simultaneous group identification and grouped task learning is one of the key features of our method. Benefit from that, our method is able to 1) avoid the potential objective bias in the existing two-shot methods (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022), where group identification and grouped task learning act as separated sequential procedures, 2) further accelerate the training procedure as retraining from scratch is no longer needed.\nIn order to validate those benefits, we perform ablation on Taskonomy-5 using the group categorizations discovered by our method, and compare our method w.r.t. two double-shot methods, which are retrained from scratch and from Naive MTL initialization, respectively. As shown in Table 4, our one-shot method significantly outperforms both two-shot counterparts with the same group categorization. This suggests that the one-shot training strategy of our methods allows the group branches to see more tasks at the early training stage than they have to predict in the end, therefore the results of our one-shot method are better than those of training those grouped subsets of tasks from scratch."}, {"title": "5.2. Generalizing-ability to Transformer Backbone", "content": "We perform the Taskonomy-5 experiment by replacing the variant of Xception with ViT-Base backbone (Dosovitskiy et al., 2021). For HOA, TAG, and MTG-Net, we use the same group categorization as those in Table 1. The results are shown in Table 5, illustrating that our method can generalize to transformer backbones, and consistently outperforms our counterparts given the same network backbone."}, {"title": "5.3. Flexibility with Amounts of Shared Layers", "content": "As shown in Fig. 2, our method enables to share the backbone encoder layers across different groups, which naturally introduces a flexible design regarding further training efficiency (i.e., sharing more layers) and a better representation capability (e.g., sharing specific layers).\nWe perform ablations with 0, 3, 6, 9 shared blocks on the 12-block Xception backbone of Taskonomy-5 with K = 3. Table 6 shows that sharing different amounts of backbone layers happens to perform comparable with each other, where sharing 3 blocks out of 12 slightly outperforms other counterparts. We also note that sharing 9 blocks delivers good results with a significantly improved efficiency, i.e., 58% FLOPs w.r.t. the model without any backbone sharing."}, {"title": "6. Discussion and Conclusion", "content": "Limitations and Future Works. Our method has the following two limitations. 1) The Gumbel Softmax is biased (Grathwohl et al., 2018; Tucker et al., 2017) and may be sensitive to different temperatures. We thus fix the temperature as a small value like (Chen et al., 2018a) to alleviate the bias issue, and we empirically show in Appendix E that different temperatures do not alter the performance significantly for our problem. We note that Gumbel Softmax is not our contribution and we will seek alternative optimizers in the future. 2) Our training efficiency can be impeded in uncommon cases where the network heads dominate the computations of the whole network (due to the expended KN heads), as discussed in Appendix B. A more efficient training strategy for heavy heads is desirable in the future.\nConclusion. We tackle the challenges of Multi-Task Learning with numerous tasks using Multi-Task Grouping (MTG) techniques. Our approach efficiently identifies the best task groups from $2^N$ candidates given N input tasks, with the high-order task affinity fully exploited. Moreover, our unified training approach of group identification and grouped task learning can be directly optimized using the task losses in one shot, which further improves the training efficiency and mitigates potential bias in separate training. We formulate MTG as a pruning problem, where the pruning process is also efficient as only the task heads are expended for pruning, instead of expanding the heavy encoders. We validate our methods on CelebA and Taskonomy datasets with extensive ablations. The results demonstrate the promising performance and the desirable efficiency of our method."}, {"title": "A. Datasets", "content": "CelebA (Liu et al., 2015) is a large-scale face dataset that contains more than 200,000 images from roughly 10,000 identities, each of which has annotated 40 face attributes representing the tasks to predict. Samples of CelebA datasets are illustrated in Fig. 1(a). For the CelebA experiments, the 9 tasks implemented in CelebA-9 (Fifty et al., 2021) are 5_o_Clock_Shadow, Black_Hair, Blond_Hair, Brown Hair, Goatee, Mustache, No_Beard, Rosy_Cheeks, and Wearing Hat. We also test our method with CelebA-40 using all the 40 labeled attributes of the CelebA dataset. All images in the CelebA dataset are resized to 64 \u00d7 64 resolution.\nTaskonomy (Zamir et al., 2018) is one of the largest datasets with multi-task labels, covering 26 vision tasks from 2D to 3D. For the Taskonomy experiments, we use the official tiny train, validation, and test split with roughly 300, 000 images. The tasks used to learn the categorization are the same as those in (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022), which are semantic segmentation, depth estimation, surface normal, keypoint detection, and canny edge detection, as shown in Figure 1(b). We also follow (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022) to bilinearly downsample the Taskonomy images to 256 \u00d7 256 before training and testing."}, {"title": "B. The Training Time, FLOPs, and the Influence of Heavy Heads", "content": "We illustrate the training time of all the methods for different experiments and network backbones in Table A1. Table Al shows that our method outperforms the previous arts in terms of both FLOPs and training time. We also note that for uncommon cases where the network heads dominate the computations of the whole network, i.e., Taskonomy-9 experiments with the Xception Variant backbone, the efficiency of our method will be impeded by those heavy heads, as we expand N heads to KN (given N tasks and K groups). We leave the efficiency training strategy for heavy heads as our future work."}, {"title": "C. Results in Terms of Errors on the Taskonomy-5 Experiments", "content": "We used the loss metric in the main text (i.e., Table 2) following the Taskonomy-5 experiments in the TAG paper (Fifty et al., 2021). In this section, we show the error statistics of the Taskonomy-5 experiments.\nSpecifically, we use mean Intersection over Union (mIoU) for semantic segmentation, Root Mean Square Error (RMSE) after aligning the transformation and scale for depth estimation, the percent of vectors with an angle less than 30 degrees for surface normal, F1-score for keypoint detection, and F1-score for edge detection. The results are shown in Table A2, demonstrating that our method remains the best for most tasks over the prior arts."}, {"title": "D. Scalability to More Input Tasks", "content": "The training complexity for the encoder of our method is O(K), indicating that our training complexity is only relevant to the group number K rather than the task number N. This is distinct from the previous state-of-the-art methods (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022), and therefore naturally scale to an arbitrary number of input tasks\u2074.\nWe validate our method with a more challenging case of categorizing all the 40 tasks into 5 groups on the CelebA dataset, denoted as CelebA-40. For the same candidate groups K = 5, our method for CelebA-9 and CelebA-40 takes 1.7 and 2.9 GPU hours, respectively, on a single Nvidia 4090 GPU, demonstrating the scalability of our method w.r.t. more input tasks.\nWe illustrate the relative gain w.r.t. Naive MTL method for each task in Fig. A2. Compared with Table 3, the improvement of our method for CelebA-40 in Fig. A2 is not as significant as that for CelebA-9 in Table 3. This is attributed to the exponential 2N difficulty in modeling the high-order task affinity, given a drastic increase of N from 9 to 40. Despite that prior state-of-the-art methods suffer significant difficulties in training complexity when scaling to such an extreme case of CelebA-40, our method successfully improves most tasks for CelebA-40 by categorizing and optimizing collaborative tasks within the same group."}, {"title": "E. Influence of Gumbel Softmax Temperatures", "content": "Gumbel Softmax (Maddison et al., 2016) is known to be biased (Grathwohl et al., 2018; Tucker et al., 2017) and can potentially be sensitive to different temperatures. In this section, we investigate different temperatures in Table A3, which demonstrates that, empirically, different temperatures do not alter the performance significantly for our problem."}, {"title": "F. Discussion on Group Categorization", "content": "We illustrate the group categorization identified by different MTG methods in Table A4 and Table A5, for the Taskonomy-5 experiments of Table 1 and CelebA-9 experiments of Table 3, respectively."}]}