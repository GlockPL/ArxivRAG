{"title": "DMTG: One-Shot Differentiable Multi-Task Grouping", "authors": ["Yuan Gao", "Shuguo Jiang", "Moran Li", "Jin-Gang Yu", "Gui-Song Xia"], "abstract": "We aim to address Multi-Task Learning (MTL) with a large number of tasks by Multi-Task Grouping (MTG). Given $N$ tasks, we propose to simultaneously identify the best task groups from $2^N$ candidates and train the model weights simultaneously in one-shot, with the high-order task-affinity fully exploited. This is distinct from the pioneering methods which sequentially identify the groups and train the model weights, where the group identification often relies on heuristics. As a result, our method not only improves the training efficiency, but also mitigates the objective bias introduced by the sequential procedures that potentially lead to a suboptimal solution. Specifically, we formulate MTG as a fully differentiable pruning problem on an adaptive network architecture determined by an underlying Categorical distribution. To categorize $N$ tasks into $K$ groups (represented by $K$ encoder branches), we initially set up $KN$ task heads, where each branch connects to all $N$ task heads to exploit the high-order task-affinity. Then, we gradually prune the $KN$ heads down to $N$ by learning a relaxed differentiable Categorical distribution, ensuring that each task is exclusively and uniquely categorized into only one branch. Extensive experiments on CelebA and Taskonomy datasets with detailed ablations show the promising performance and efficiency of our method. The codes are available at https://github.com/ethanygao/DMTG.", "sections": [{"title": "1. Introduction", "content": "Many real-world applications are essentially complex systems that involve the collaboration of a large number of tasks. For example, in autonomous driving (Caesar et al., 2020; Hu et al., 2023), the system needs to simultaneously perform lane detection (Tang et al., 2021), depth estimation (Godard et al., 2019), vehicle detection and instance segmentation (He et al., 2017; Cheng et al., 2021), pedestrians localization (Bertoni et al., 2019), etc. In order to tackle these real-world challenges, it is crucial to simultaneously learn a large number of diverse tasks within a Multi-Task Learning (MTL) framework (Kokkinos, 2017; Nekrasov et al., 2019; Dvornik et al., 2017; Bilen & Vedaldi, 2016; Zamir et al., 2016; Li & Gong, 2021; Wang & Tsvetkov, 2021; Liu et al., 2020; Yu et al., 2020), which reduces the inference time and facilitates an improved performance by leveraging the affinity among different tasks."}, {"title": "2. Related work", "content": "It is thus critical to harness the affinity among those diverse tasks. Compared to learning them independently, simply combining them and feeding them into a fully shared network oftentimes deteriorates the performance of several or even most tasks. Such phenomenon is attributed to the presence of the inherent negative transfer, where the intuition is that the gradients from different tasks may interfere with each other when flowing into a shared encoder.\nPioneering works alleviate the negative transfer by designing novel Multi-Task Architectures (MTA) (Caruana, 1997; Argyriou et al., 2008; Kang et al., 2011; Ruder, 2017; Vandenhende et al., 2021) or applying the Multi-Task Optimization (MTO) methods (Sener & Koltun, 2018; Lin et al., 2019; Liu et al., 2021; Suteu & Guo, 2019; Yang & Hospedales, 2016), where state-of-the-art MTA assigns independent network parameters to different tasks, while MTO directly manipulates the gradients from different tasks before applying them to update the shared parameters. However, both MTA and MTO methods pose challenges when scaling to a large number of tasks, i.e., the scalability is impeded in MTA for both training and evaluation due to the extra parameters, while the training of MTO cannot maintain scalability because it has to retain the backward graphs for each task. Recent researches also suggest that it is difficult to address the negative transfer solely by gradient manipulation in MTO (Xin et al., 2022; Kurin et al., 2022).\nWe instead propose to learn a large number of tasks by Multi-Task Grouping (MTG) (Standley et al., 2020). In MTG, input tasks are categorized into groups by their affinity, where a group of tasks, instead of a single task, is modeled by a unique encoder. When the group categorization is given, MTG for $K$ groups of $N$ tasks drastically reduces the training complexity from $O(N)$ (for MTA/MTO) to $O(K)$.\nThe primary challenge of MTG is to identify the group categorization, which involves investigating the exponential $2^N$ group candidates at maximum, given merely $N$ tasks. In order to migrate this issue, Standley et al. (2020) and Fifty et al. (2021) propose to average the pairwise affinities to approximate the high-order affinities. Despite a reduced complexity from $2^N$ to $N^2$, the less precise assumption of linear tasks affinity in (Standley et al., 2020; Fifty et al., 2021) degrades the final performance. On the other hand, Song et al. (Song et al., 2022) advocate to train a meta-learner that directly predicts the final performance given a group categorization. However, the training of the meta-learner per se is extremely difficult and involves collecting numerous well-trained group samples. Moreover, existing methods perform group identification and grouped task learning in separated sequential procedures. As a result, the former potentially introduces objective bias w.r.t. the latter, especially when the groups are categorized based on heuristics. This also leads to potential performance degradation.\nIn view of those limitations, we propose to formulate MTG as a pruning problem of an adaptive network architecture, as shown in Figure 1, which enables to 1) identify the best groups and train the grouped model weights simultaneously in one-shot, as well as 2) fully exploiting the high-order task affinities. In our unified one-shot learning, we formulate the group identification as the model architecture learning/pruning, and the grouped task learning is established as the model weights training under a certain architecture. In this way, both procedures mutually facilitate each other to a better convergence. We jointly train both procedures simply by the task losses, where the high-order task affinities are directly exploited. Our approach excels in both efficiency and accuracy, which is distinct from pioneering two-shot methods that first approximately identify the grouping results, then train the grouped model from scratch subsequently.\nSpecifically, we formulate the categorization of $N$ tasks into $K$ groups as learning of a Categorical distribution, where the Categorical distribution is used to determine an adaptive network architecture. We then optimize the unified group identification and grouped task learning leveraging a pruning algorithm that is fully differentiable. To this end, our method starts with $K$ branches, each equipped with $N$ heads. It indicates that at the beginning, all the tasks are predictable by every group, ensuring full exploitation of the high-order task-affinity. After that, we optimize the model weights as well as the Categorical distribution such that the $KN$ heads are gradually pruned down to $N$, facilitating that each task is exclusively and uniquely predicted by only one branch. Our Categorical distribution is continuously relaxed and then optimized by Gumbel softmax (Maddison et al., 2016). Our pruning procedure per se is efficient, as we only expand the light-weighted task heads (e.g., only the last network layers), instead of the heavy encoders, and the $K$ encoder branches (each for a group) in our method represent the minimal requirement of MTG.\nOur method has been extensively validated on CelebA (Liu et al., 2015) and Taskonomy (Zamir et al., 2018) with detailed ablations. Our method exhibits two unique features:\n\u2022 Accuracy with high-order task affinities exploited, which is ensured by 1) the grouping formulation of learning a continuous relaxed and differentiable Categorical distribution, and 2) the elimination of the objective bias by the one-shot training of unified group identification and grouped task learning.\n\u2022 Efficiency with $O(K)$ training complexity given $K$ groups, which comes from 1) our pruning formulation instead of sampling group candidates to train from scratch, and 2) the one-shot training that unifies group identification and grouped task learning."}, {"title": "2.1. Multi-Task Grouping", "content": "Multi-Task Grouping (MTG) aims to put collaborative tasks from a task pool into the same group, where a group of tasks can be learned efficiently by a shared network (Kang et al., 2011; Kumar & III, 2012; Li et al., 2021). Grouping tasks enables efficient learning of a vast array of tasks while also maintaining high interpretability. However, the primary challenge in MTG is that finding an optimal grouping solution in $2^{N-1}$ grouping candidates can be difficult. Existing grouping methods (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022) have attempted to model an evaluation function to determine high-order task relationships based on low-order observations. Nonetheless, these methods perform group identification and grouped task learning separately, and potentially considering only low-order task affinity. In contrast, our grouping approach integrates group identification and grouped task learning within a one-shot training process, significantly improving running efficiency in large-scale task scenarios while thoroughly considering higher-order task relationships."}, {"title": "2.2. Multi-Task Architecture", "content": "Multi-Task Architecture (MTA) (Caruana, 1997; Argyriou et al., 2008; Kang et al., 2011; Ruder, 2017; Vandenhende et al., 2021) is a prevailing technology line in the Multi-Task Learning domain. It can be categorized as hard-parameter sharing (Kokkinos, 2017; Nekrasov et al., 2019; Dvornik et al., 2017; Bilen & Vedaldi, 2016; Zamir et al., 2016) and soft-parameter sharing (Gao et al., 2019; 2020; 2024; Long et al., 2015; 2017; Misra et al., 2016). The former shares a common feature extraction module among tasks, while the latter assigns a special feature extraction branch for each task, exchanging features through extra fusion modules. Although great success has been witnessed in designing novel MTL network architectures, they are less appropriate in addressing an extreme large number of tasks. Specifically, it is difficult to avoid the negative transfer due to a full-shared encoder module in hard-parameter sharing methods (Vandenhende et al., 2019; Guo et al., 2020; Br\u00fcggemann et al., 2020; Sun et al., 2020), while soft-parameter sharing methods (Ruder et al., 2019; Zhang et al., 2018; Xu et al., 2018; Zhang et al., 2019b) better address the negative transfer but introduce efficiency issues."}, {"title": "2.3. Multi-Task Optimization", "content": "Multi-Task Optimization (MTO) develops in parallel with Multi-Task Architecture, which aims to adjust task loss to balance the learning process of different tasks (Kendall et al., 2018; Chen et al., 2018c; Liu et al., 2019b; Lin et al., 2022; Chen et al., 2020; Guo et al., 2018). Advanced MTO methods directly manipulate gradients from different tasks to mitigate the training conflicts (Li & Gong, 2021; Wang & Tsvetkov, 2021; Liu et al., 2020; Yu et al., 2020), e.g., projecting task gradients when their angle is greater than 90\u00b0. In practice, revising gradients necessitates additional memory to store the gradient graph for each task, which can be potentially infeasible when dealing with an extremely large number of tasks. Most recently, Kurin et al. (2022) and Xin et al. (2022) reveal that the existing MTO methods may be sensitive to hyperparameters when dealing with different combinations of tasks. Our method aims to learn the categorization of tasks and is orthogonal to MTO methods."}, {"title": "2.4. Network Pruning", "content": "Network pruning (Cai et al., 2018; Chen et al., 2018b; Elsken et al., 2019; Ghiasi et al., 2019; He et al., 2020; Li et al., 2019) aims to detect and remove the redundancy of the networks without significant performance degradation. This pruning process can be implemented by Bayesian optimization (Bergstra et al., 2013), evolutionary algorithms (Real et al., 2019; Xie & Yuille, 2017), network transformation (Gordon et al., 2018), reinforcement learning (Guo et al., 2019; Tan et al., 2019; Zoph et al., 2018), and gradient descent (Akimoto et al., 2019; Liu et al., 2019a; Wu et al., 2019; Zhang et al., 2019a). We use differentiated pruning operations, which effectively enable integrating group identification with grouped task learning jointly in one-shot training. We are the first to implement network pruning into MTG to unify group identification and grouped task learning in an end-to-end architecture."}, {"title": "3. Method", "content": "Given $N$ tasks, we aim to efficiently chase the best categorization from the $2^N$ possibilities, with the high-order task affinities directly exploited. To this end, we formulate MTG into a network pruning framework, where we model the group identification as the architecture learning/pruning, and the grouped task learning as the model weights optimization under a certain architecture. As a result, the group identification and the grouped task learning are unified and can be jointly optimized in one-shot during the network pruning. Regarding the optimization, we design the group categorization as the learning of a Categorical distribution, which is then continuously relaxed into a differentiable Concrete distribution and subsequently optimized using the Gumbel softmax (Maddison et al., 2016).\nIn summary, our method is able to 1) exploit the high-order task affinities directly. 2) It avoids the potential objective bias when group identification and grouped task learning act as separated sequential procedures. 3) Given $K$ groups, our pruning algorithm preserves the efficiency of $O(K)$ training complexity for the encoder. 4) Our Categorical distribution formulation guarantees each task to be categorized into one group exclusively and uniquely. Thus, our learned groups and model weights are readily to use without retraining or validation (validation is needed when a certain task is categorized into multiple groups (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022), as discussed in Appendix F)."}, {"title": "3.1. Problem Formulation", "content": "Formally, we consider categorizing a set of $N$ tasks $\\mathcal{T} = {\\mathcal{T}_1, ..., \\mathcal{T}_N}$ into equal or less than $K$ groups $\\mathcal{G} = {\\mathcal{G}_1, ..., \\mathcal{G}_K}$, such that each group contains 0 to $N$ tasks $\\mathcal{G}_k = {..., \\mathcal{T}_i, ...}$, and each task is exclusively and uniquely belongs to one group. Therefore, we have:\n$$\\begin{aligned}\n\\mathcal{T} = \\bigcup_{k=1}^{K} \\mathcal{G}_k, \\\ns.t. \\quad & \\forall k, |\\mathcal{G}_k| \\in {0, ..., N}, \\\\\n& \\forall (i, j), \\mathcal{G}_i \\cap \\mathcal{G}_j = \\emptyset,\n\\end{aligned}$$(1)\nwhere $|\\cdot|$ is the cardinality. We optimize our problem exclusively to attain the highest average performance across these $N$ tasks, without relying on heuristic criteria. We also note that $K$ is the maximal-allowed number of groups, and we do not impose a strict requirement to yield precisely $K$ groups, e.g., some groups may contain 0 task.\nObjective Bias in Two-Stage MTG Methods. The objective bias in pioneering (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022) appears in two aspects: 1) the group categorization is determined by heuristics but the retraining is based on the optimization of task losses, and 2) the difference in the inputs to the group identification and the grouped-model weights retraining stages lead to different objectives. In other words, the groups are identified heuristically when all the $N$ tasks can synergy/regularize each other, but the retraining phase only sees a subset (group) of tasks thus exhibiting different gradients from the former.\nAs shown in Figure 2, the group identification and the grouped task (weights) learning in our method complement each other and are trained jointly in one-shot. On one hand, during the training of the task groups, the group identification module selects collaborative tasks to back-propagate gradients to the corresponding branch of the grouped task learning module. On the other hand, each branch of the grouped task learning module is responsible for one group, which in turn facilitates group identification."}, {"title": "3.2. Grouped Task Learning Module", "content": "We start with $K$ branches in the grouped task learning module, where each branch represents the encoder of each task group. We connect each branch to $N$ task heads to predict all the $N$ tasks, facilitating the exploration of high-order task affinity. Our method possesses an efficient training complexity of $O(K)$ for the network encoder.\nOur method also enables to further reduce the training complexity, by implementing optional group-wise shared layers before splitting into the group-specific branches. This is illustrated in the dashed gray box in Fig. 2."}, {"title": "3.3. Group Identification Module", "content": "We model the categorization of $N$ tasks exclusively and uniquely into $K$ groups as the learning of an unknown Categorical distribution, where the Categorical distribution is used to determine an adaptive network architecture. As such, the underlying Categorical distribution can be optimized jointly with the model weights in one-shot, which we formulate as a pruning problem."}, {"title": "Categorical Distribution Modeling", "content": "Formally, let a random variable $z_{ik}$ indicate the assignment of task $i$ to group $k$, which is sampled from some discrete distribution. In order to assign $N$ tasks to $K$ groups, we have a set of random variables $\\mathcal{Z} = {z_{ik}} \\in \\mathbb{R}^{N \\times K}$\nRecall Eq. (1) that each task is exclusively and uniquely categorized into one group, therefore, we have:\n$$\\sum_{k} z_{ik} = 1, \\quad \\text{and} \\quad z_{ik} \\in {0, 1}, \\forall k,$$(2)\nwhich indicate that each row of $\\mathcal{Z}$ follows a Categorical distribution. Let the Categorical random variable $z_{ik}$ be parameterized by $s_{ik}$, we have:\n$$z_{ik} \\sim \\text{Categorical}(s_{ik}),$$(3)\nwhere $s_{ik}$ is the probability of assigning task $i$ to group $k$.\nNetwork Architecture Formulation. We establish an adaptive network architecture by the Categorical distribution, and formulate a pruning problem so that the Group Identification Module and the Grouped Task Learning Module can be optimized jointly in one-shot.\nTo this end, we formulate the sampled random variable $z_{ik}$ as a loss indicator or a task selector, which determines whether to back-propagate the loss of task $i$ to the $k$-th group. Specifically, let $\\mathcal{L} = {L_{ik}} \\in \\mathbb{R}^{N \\times K}$ be the loss matrix of $KN$ task heads, the final loss can be obtained by:\n$$\\mathcal{L}_{task} (\\theta, S) = \\mathcal{L}(\\theta) \\odot \\mathcal{Z}(S),$$(4)\nwhere $\\odot$ is the element-wise product, $\\theta$ is the model weights, and $S = {s_{ik}} \\in \\mathbb{R}^{N \\times K}$ is the set of parameters of the Categorical distributions.\nAs shown in Eq. 4, we formulate MTG as a pruning problem where $\\mathcal{Z}(S)$ is learned to prune the $KN$ losses $\\mathcal{L}(\\theta)$. We note that the cost of training a $N \\times K$ matrix $S$ and sampling $\\mathcal{Z}$ from $S$ is negligible w.r.t. the learning of $K$ group encoders, retaining the training complexity of our pruning formulation as $O(K)$ for the heavy encoder2."}, {"title": "3.4. The Joint Optimization", "content": "Equation (3) involves a discrete sampling from $s_{ik}$ to $z_{ik}$, which results in a gradient blockage in Eq. (4) when back-propagating gradients from $\\mathcal{Z}$ to $S$. In this section, we continuously relax the discrete Categorical distribution, so that both the parameters for group identification $S$ and the weights for grouped task learning $\\theta$ can be jointly optimized in one-shot by back-propagating the gradients from the task loss $\\mathcal{L}_{task} (\\theta, S)$, through $\\mathcal{Z}(S)$ and $\\mathcal{L}(\\theta)$, respectively.\nContinuous Relaxation. By using the reparameterization trick from the Concrete distribution (Maddison et al., 2016), we are able to continuously sample $s_{ik}$ to produce $\\tilde{z}_{ik}$ that approximate $z_{ik}$ of the Categorical distribution. This facilitates the gradient flow from $\\mathcal{L}_{task} (\\theta, S)$ to $s_{ik}$ through $\\tilde{z}_{ik}$. The reparameterized Categorical distribution is modeled by the differentiable Gumbel softmax:\n$$\\tilde{z}_{ik} = \\frac{\\exp((s_{ik} + g_{ik}) / T)}{\\sum_{m=1}^{K} \\exp((s_{im} + g_{im}) / T)},$$(5)\nwhere $g_{ik}$ is sampled from a Gumbel distribution, i.e., $g_{ik} = - \\log(-\\log(\\text{Uniform}(0, 1)))$ (Maddison et al., 2016). $T$ is a small or annealing temperature, producing a discrete $\\tilde{z}_{ik}$ after convergence as a good approximation of $z_{ik}$. Given $\\mathcal{Z} = {\\tilde{z}_{ik}} \\in \\mathbb{R}^{N \\times K}$, the loss in Eq. (4) becomes:\n$$\\mathcal{L}_{\\text{relaxed task}} (\\theta, S) = \\mathcal{L}(\\theta) \\odot \\tilde{\\mathcal{Z}}(S),$$(6)\nInitialization. We note that the parameter of the Categorical distribution, $s_{ik}$, can be initialized according to the prior knowledge of the task affinity. In our problem, we simply initialize each $s_{ik}$ to $1/K$, which implies that each task has an equal probability of being categorized into any group. In other words, we do not assume any task affinities and learn them in a fully data-driven manner. Based on that, we optimize our model by pruning the initial $KN$ task heads to $N$, where each task is exclusively and uniquely categorized into one group after convergence."}, {"title": "4. Experiments", "content": "In this section, we extensively validate our method on both Taskonomy (Zamir et al., 2018) and CelebA (Liu et al., 2015) datasets for various candidate groups. We detail the experimental setup in the following."}, {"title": "4.1. Experimental Setup", "content": "Datasets. We perform experiments on the Taskonomy dataset (Zamir et al., 2018) following (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022), and the CelebA dataset (Liu et al., 2015) following (Fifty et al., 2021). We use the official tiny train, validation, and test split of Taskonomy. The images from Taskonomy and CelebA are bilinearly down-sampled to 256 \u00d7 256 and 64 \u00d7 64, respectively. Those datasets are introduced in detail in Appendix A.\nBenchmark Experiments. We follow the experiment setups in (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022) to conduct 5 tasks on Taskonomy, i.e., semantic segmentation, depth estimation, surface normal, keypoint detection, and edge detection, denoted as Taskonomy-5. We also conduct 9 tasks on CelebA dataset following (Fifty et al., 2021), i.e., 5_o_Clock_Shadow, Black_Hair, Blond_Hair, Brown Hair, Goatee, Mustache, No Beard, Rosy_Cheeks, and Wearing Hat, referred to as CelebA-9. We perform the full 40 tasks of the CelebA dataset, i.e., CelebA-40, in Appendix D, showcasing our scalability to numerous tasks.\nNetwork Backbone. We use the same network backbone as (Standley et al., 2020; Fifty et al., 2021), i.e., a variant of Xception network (Chollet, 2017) with 12 blocks, for the Taskonomy experiments. For CelebA, we use a variant of ResNet (He et al., 2016) following (Fifty et al., 2021).\nOptimization. We use Adam optimizer for all of our experiments, where the initial learning rates are 0.0008 and 0.0001 for the CelebA and Taskonomy experiments, respectively. We use plateau learning rate decay which reduces by 0.5 when the validation loss no longer improves. We train all the experiments for 100 epochs, where our networks are initialized by the pre-trained naive MTL weights on the corresponding experiments. We copy the networks and the group-specific parameters for $K$ times to ensure that the same task is initialized identically across different groups. We initialize the Gumbel Softmax temperature $\\tau$ of Eq. (5) as 2.5 and 4 for the CelebA and Taskonomy experiments, respectively. We follow (Fifty et al., 2021) to use the cross-entropy loss for the CelebA experiments, and follow (Standley et al., 2020; Fifty et al., 2021) to use the cross-entropy loss for semantic segmentation and $\\ell_1$ loss for other tasks of the Taskonomy experiments.\nEvaluation Metrics. Pioneering research in MTG commonly relied on the total loss as the evaluation metric (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022), which straightforwardly sums up the losses of all tasks. However, the magnitudes of losses from different tasks significantly vary due to 1) different loss types, such as cross-entropy losses for classification and $\\ell_1$ losses for regression, and 2) diverse labels, such as image-level classification labels and pixel-level semantic segmentation labels. Consequently, simply calculating the total loss may lead to an overestimation of tasks with higher loss magnitudes while overshadowing those with lower loss magnitudes. This phenomenon contradicts the goal of MTG, i.e., boosting all the input tasks rather than a subset of them (Standley et al., 2020).\nTo comprehensively assess improvements of an MTG method across all tasks, we follow (Maninis et al., 2019; Vandenhende et al., 2020; 2021) to eliminate the influence of loss magnitudes, which is termed normalized gain. Specifically, we initially calculate the normalized loss improvement (expressed as a percentage) w.r.t. the naive MTL architecture (i.e., the shared-encoder architecture, oftentimes the worst baseline) for each task, then average them for all tasks:\n$$\\text{NormGain}_\\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\mathcal{L}_{task \\ n}^{\\text{Naive MTL}} - \\mathcal{L}_{task \\ n}^{\\text{method}}}{\\mathcal{L}_{task \\ n}^{\\text{Naive MTL}}},$$(7)\nwhere $\\mathcal{L}$ denotes loss and $N$ is the total number of tasks. Similarly, in cases where a unified evaluation is applicable for all input tasks (e.g., classification error when all input tasks are classifications), we can also present normalized gain w.r.t. such unified evaluation error:\n$$\\text{NormGain}_E = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{E_{task \\ n}^{\\text{Naive MTL}} - E_{task \\ n}^{\\text{method}}}{E_{task \\ n}^{\\text{Naive MTL}}},$$(8)\nwhere $E$ denotes the unified evaluation error."}, {"title": "4.2. Experiments on Taskonomy with 5 Tasks", "content": "Following (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022), we compare our methods with the state-of-the-art MTG methods including HOA (Standley et al., 2020), TAG (Fifty et al., 2021), and MTG-Net (Song et al., 2022).\nWe also report the performance of Random Group (RG) which randomly divides the input tasks into a specific number of groups. We illustrate the baseline performance with Naive MTL, where all the tasks are trained simultaneously with a fully-shared encoder (i.e., within 1 group). The performance where each task is trained separately without grouping is denoted as Single Task Learning (STL). We perform candidate numbers of groups as 3, 4, and 5.\nThe results in terms of losses are shown in Table 1. Our method outperforms SOTA methods by a large margin with a more efficient $O(K)$ training complexity for the encoder, we give detailed training time in Appendix B. Our method reduces the total loss by 22% compared to naive MTL and by 13% compared to STL when $K$ = 3. As $K$ increases, our grouping performance further improves. Regarding the normalized metric NormGain w.r.t. loss, i.e., Eq. (7), it also achieves a remarkable improvement of over 60% w.r.t. naive MTL. Consistent observation can be obtained regarding the error statistics (i.e., Eq. (8)), as shown in Appendix C.\nIt can be observed that the performances in terms of total loss and NormGain are not consistent for some MTG methods. For example, in Table 1 at $K$ = 3, the NormGain of MTG-Net is over 10% higher than that of HOA, given that their total losses are comparative. This is because, as discussed in Evaluation Metrics of Sect. 4.1, the total loss is affected by loss magnitudes associated with different tasks, a slight improvement in a task with a large loss magnitude might overshadow a significant degradation in a task with a small loss magnitude. In contrast, the NormGain metrics address this issue by eliminating such undesirable influence through normalization, providing a more reasonable measurement w.r.t. the improvement of all the tasks. We further validate this by illustrating the loss and the relative gain w.r.t. Naive MTL for each task in Table 2, where our method achieves the best performance across almost all tasks.\nWe also show the training complexity of the heavy encoder relative to the Naive MTL method in Table 2. Given $K << N$, Our method achieves the best training efficiency except for Naive MTL, but Naive MTL fails to deliver desirable accuracy through a fully shared encoder across all the tasks. Note that there are two terms for the training complexity of HOA and TAG, as they involve first identifying task groups according to $N$ tasks, then training $K$ networks, each for a task group, from scratch. The training complexity of MTG-Net is not included as it requires up to $O(2^N) + O(K)$ to sample up to $2^N$ task combinations and subsequent training them from scratch."}, {"title": "4.3. Experiments on CelebA with 9 Tasks", "content": "We compare our method with the state-of-the-art methods HOA (Standley et al., 2020) and TAG (Fifty et al., 2021). MTG-Net (Song et al., 2022) is not included in the CelebA-9 experiments as MTG-Net does not scale well w.r.t. number of input tasks $N$, i.e., MTG-Net requires to inefficiently sample up to $2^N$ task combinations and subsequent training them from scratch, which may take thousands of GPU hours as reported in (Song et al., 2022).\nFollowing (Fifty et al., 2021), we perform candidate numbers of groups as 2, 3, and 4 on CelebA-9. As all the input tasks in this experiment are classification tasks, therefore we report the total classification error and NormGain w.r.t. classification error, i.e., Eq. (8), in Table 3. Table 3 illustrates the experiment results on CelebA-9, showing that our method consistently outperforms the state-of-the-art methods on the CelebA-9 experiments by a large margin with a more efficient training complexity of $O(K)$ for the encoder."}, {"title": "5. Ablation Analysis", "content": "We carefully investigate the following issues by ablation. 1) Whether our proposed one-shot MTG outperforms the common practice of two-shot methods (Standley et al., 2020; Fifty et al., 2021; Song et al., 2022) given the same group categorization in Sect. 5.1. 2) Can our method generalize to the transformer backbones in Sect. 5.2. 3) The flexibility if we share more or less encoder layers in our method in Sect. 5.3. 4) Can our method scale to more input tasks in Appendix D. 5) The influence of different Gumbel Softmax temperatures in Appendix E. 6) The group categorization identified by different methods in Appendix F."}, {"title": "5.1. Merits of One-shot Nature for MTG", "content": "The one-shot simultaneous group identification and grouped task learning is one of the key features of our method. Benefit from that, our method is"}]}