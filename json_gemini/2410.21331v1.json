[{"title": "BEYOND INTERPRETABILITY: THE GAINS OF FEATURE\nMONOSEMANTICITY ON MODEL ROBUSTNESS", "authors": ["Qi Zhang", "Yifei Wang", "Jingyi Cui", "Xiang Pan", "Qi Lei", "Stefanie Jegelka", "Yisen Wang"], "abstract": "Deep learning models often suffer from a lack of interpretability due to poly-\nsemanticity, where individual neurons are activated by multiple unrelated se-\nmantics, resulting in unclear attributions of model behavior. Recent advances\nin monosemanticity, where neurons correspond to consistent and distinct seman-\ntics, have significantly improved interpretability but are commonly believed to\ncompromise accuracy. In this work, we challenge the prevailing belief of the\naccuracy-interpretability tradeoff, showing that monosemantic features not only\nenhance interpretability but also bring concrete gains in model performance.\nAcross multiple robust learning scenarios\u2014including input and label noise, few-\nshot learning, and out-of-domain generalization our results show that models\nleveraging monosemantic features significantly outperform those relying on pol-\nysemantic features. Furthermore, we provide empirical and theoretical under-\nstandings on the robustness gains of feature monosemanticity. Our preliminary\nanalysis suggests that monosemanticity, by promoting better separation of fea-\nture representations, leads to more robust decision boundaries. This diverse\nevidence highlights the generality of monosemanticity in improving model ro-\nbustness. As a first step in this new direction, we embark on exploring the\nlearning benefits of monosemanticity beyond interpretability, supporting the long-\nstanding hypothesis of linking interpretability and robustness. Code is available at", "sections": [{"title": "1 INTRODUCTION", "content": "A long-standing problem of deep learning is the so-called \"black-box\" nature. People find that an\nimportant factor for its lack of interpretability is feature polysemanticity, where a single neuron (a\ndimension of feature maps) is activated by multiple irrelevant semantics, preventing clear attributions of neural behaviors. Following this understanding, recent\nresearch has made breakthroughs towards attaining monosemanticity, i.e., neurons corresponding\nto consistent semantics (monosemantic), which dramatically improves model interpretability; see a\ncomparison in Figure 1(a). They achieve this through architectural designs or post-training explanation modules, and have successfully\nscaled to visual backbones (e.g., ResNet) and large language models (LLMs, e.g., Claude, GPT, and\nGemma), discovering many intriguing phenomena and applications.\nHowever, these works on monosemanticity suggest an inevitable \"accuracy-interpretability\u201d tradeoff:\nmonosemantic features, although more interpretable, come at the sacrifice of expressive power and\nunderperform polysemantic features at prediction accuracy. This widely accepted belief limits the applications of monosemanticity techniques to only\ninterepretability-related domains. In this paper, we aim to push this boundary one step forward by"}, {"title": null, "content": "demonstrating that monosemanticity can also bring significant gains on practical model performance\nbeyond interpretability.\nIn particular, we discover a widely appearing phenomenon, that monosemantic features are much\nmore robust compared to polysemantic features, across multiple scenarios related to \u201crobustness\u201d.\nOne such scenario is learning with noise. Real-world data are often imperfect with low-quality input\nand mislabeling, manifested in the form of various data noises and distribution shifts. We find that\nunder either input or label noises, learning a classifier upon (pretrained) monosemantic features can\nattain much higher accuracy (e.g., +13.7% top-1 accuracy under 90% label noise) than polysemantic\nfeatures, as shown in Figure 1(b). This feature-centric result also offers a new perspective to noisy\nlearning where existing studies primarily focus on robust learning objectives.\nThe second secenario is few-shot finetuning for downstream classification. Today's large visual\nbackbones often need to be finetuned on a small amount of downstream labeled data, where models\neasily overfit and deteriorate. We find that monosemantic finetuning, i.e., preserving the monose-\nmanticity of representations during finetuning (with a technique from Wang et al. (2024)), can attain\nmuch higher accuracy under few-shot data compared to vanilla polysemantic finetuning (e.g., +3.9%\ntop-1 accuracy with 10% samples). The same method also works for finetuning with noisy data or\ntraining from scratch.\nWith these benefits in mind, we further explore a third scenario, LLM finetuning, which receives\nwide applications these days. Pretrained LLMs need to be carefully finetuned\non small-scale language data for different purposes, e.g., instruction following and certain abilities\n(e.g., reasoning), while avoiding conflicting and forgetting. Since LLMs do not have a natural repre-\nsentation space like visual models, we devise a simple sparse variant of LoRA, named MonoLoRA,\nto encourage the monosemanticity of the updates of all features. We show preliminary evidence\nthat when finetuning an aligned LLM (Llama-2-7b-chat) on SST-2 (a classification task) and Dolly\n(instruction following task), MonoLoRA better preserves model alignment while improving task\nperformance.\nAt last, we attempt to offer a deeper understanding of the robustness gains of monosemanticity.\nEmpirically, we compare the salient features of different classifiers, observing that the more robust\nclassifiers tend to depend on more monosemantic features. Theoretically, as a preliminary step,\nwe compare polysemantic and monosemantic features under a toy model proposed in Elhage et al.\nThe theory suggests that because monosemantic features have better separation of features,\nthey are less prone to overfitting to noise, leading to more robust decision boundaries compared to\npolysemantic features.\nIn summary, this work challenges the common \u201caccuracy-interpretability\u201d tradeoff by demonstrating\nthe potential of feature monosemanticity to bring clear gains in model accuracy. These gains manifest\nthemselves in various aspects of \u201clearning robustness\" that we can think of: input noise, label noise,\nout-of-domain data, few-shot image data, and few-shot language data. The diverse set of evidence\nstrongly indicates that feature monosemanticity provides a general sense of robustness compared to\npolysemantic features, echoing with the long-lasting hypothesis on the relationship between better\nfeature interpretability and better robustness (e.g., human decisions are both interpretable and robust)\n. As a first step in this direction, we believe that it will embark on\nmore intriguing discoveries and understandings on the learning benefits of monosemanticity beyond\ninterpretability."}, {"title": "2 PRELIMINARY & RELATED WORK", "content": "Polysemanticity and Superposition Hypothesis. Across various domains, many previous studies\nhave consistently observed that a feature\ndimension in the neural networks is usually activated with multiple unrelated semantics. Researchers\ndefine this phenomenon as the feature polysemanticity. In contrast, when each dimension is activated\nwith a single latent natural concept, the features are denoted as monosemantic features. A popular\nexplanation of the feature polysemanticity is the superposition hypothesis, which states that each polysemantic dimension is an approximately linear combination\nof multiple natural concepts. To verify that, Elhage et al. (2022b) propose a toy model that obtains\npolysemantic features with the superposition hypothesis. Comparing polysemantic and monosemantic\nfeatures, there exists a common belief that monosemantic features exhibit better interpretability at the\ncost of downstream performance."}, {"title": "3 THE ROBUSTNESS GAINS OF MONOSEMANTICITY", "content": "In this section, we compare polysemantic with monosemantic features across three different robust\nlearning scenarios commonly encoutered in the foundation model regime: first, noisy linear probing\non pretrained features (either polysemantic or monosemantic); second, noisy and few-shot finetuning\nfrom pretrained weights; third, finetuning LLMs on small-scale supervised data."}, {"title": "3.1 \u039c\u039f\u039dOSEMANTIC FEATURES ARE ROBUST UNDER LINEAR PROBING", "content": "Foundation models typically have two training phases: 1) self-supervised learning (SSL) on massive\nunlabeled data, and 2) supervised finetuning on small human-labeled data (classification, instruction\nfollowing, or specific tasks). In fact, since SSL-pretrained features contain rich semantics, learning\na simpler linear classifier on top, known as linear probing (LP), can often attain competitive\nperformance to fully supervised ones. Therefore, we start with this simplest setting\nfor comparing the robustness of polysemantic and monosemantic pretrained features. Specifically,\nwe consider a standard linear probing setting, where we first pretrain features on unlabeled data and\nthen learn a linear classifier on top with noisy labeled data."}, {"title": "3.1.1 METHODS FOR FEATURE MONOSEMANTICITY", "content": "Among existing interpretability research, there are two categories of methods to attain monoseman-\nticity: 1) intrinsic methods, where pretrained features are intrinsically monosemantic; 2) post-hoc\nmethods, where we apply additional techniques to decode (polysemantic) pretrained features to\nmonomsemantic ones. Here, we consider two representative methods for each paradigm.\nIntrinsic Monosemanticity with NCL. Many previous works have tried to train interpretable features\nby adding sparsity regularization or identifiability constraints ; but they hardly scale to large-scale data with competitve performance. A recent work, NCL (non-\nnegative contrastive learning) , as a modern counterpart to NMF (non-negative\nmatrix factorization) , attains high sparsity and monosemanticity while having\nminimal influence on final performance. Specifically, NCL adopts the following InfoNCE loss with non-negative feature outputs:\n\\begin{equation}\nL_{NCL}(f) = -E_{x,x^{+}}log \\frac{exp(f_{+}(x)^{T}f_{+}(x^{+}))}{exp(f_{+}(x)^{T}f_{+}(x^{+})) + \\sum_{i=1}^{M}exp(f_{+}(x)^{T}f_{+}(x\\bar{i})},\\label{eq:1}\n\\end{equation}\nwhere $(x, x^{+})$, $(x, x^{-})$ are the positive and negative pairs in contrastive learning, $f_{+}(x) = \\sigma(f(x))$,\n$\\sigma$ is an activation function and $f$ is the original neural network. With the non-negative constraints,\nthe activations of learned representations become sparse and each dimension is almost only activated\nwith samples from the same class.\nPost-hoc Monosemanticity with SAE. Another approach is to apply downstream modification on\npretrained neural networks. Sparse autoencoders (SAEs) find wide success in\nattaining monosemanticity in language models. SAEs reconstruct the original outputs of pretrained networks from a sparse bottleneck layer.\nTo be specific, the encoder and decoder are defined as:\n\\begin{equation}\nz(x) = topK(W_{enc}(f(x) - b_{pre}) + b_{enc}),\\\\\nf(x) = W_{dec}z(x) + b_{pre}.\n\\end{equation}\nwhere $f(x)$ is the representation of input $x$; $W_{enc}, W_{dec}, b_{pre}$ and $b_{enc}$ are the parameters of SAE;\n$topK$ is a sparse activation function proposed by Gao et al. (2024) that only preserves the top $K$\nelements; and the SAE training loss is the reconstruction MSE $L_{SAE} = E_{x}||f(x) - f(x)||^{2}$. As a\nresult, the sparse latent feature $z(x)$ has much better monosemanticity than the original feature $f(x)$."}, {"title": "3.1.2 EXPERIMENTS", "content": "Setup. For the baseline, we pretrain a ResNet-18 backbone with the widely-used\ncontrastive framework SimCLR on CIFAR-100 and ImageNet-100. In comparison,\nwe use Non-negative Contrastive Learning and Sparse Autoencoder to represent two primary strategies for obtaining monosemantic features, i.e., improve the\npretraining algorithm and apply downstream modification. For Non-negative Contrastive Learning\n(NCL), we follow the default SimCLR settings, with the addition of a non-negative constraint using\nthe ReLU function. For the Sparse Autoencoder (SAE), we apply it following the pretrained backbone\nas Equation (2), and then we train the linear classifier on the frozen latent representation of SAE.\nMore details can be found in Appendix A.1.\nRobustness Against Label Noise. When evaluating the robustness against label noise, we train a\nlinear classifier following the frozen pretrained encoders, where the labels are uniformly flipped to\nthe other classes with a probability $\\eta$ (noise rate). As shown in Table 1, when the linear classifiers are\ntrained on the samples with clean labels, monosemantic and polysemantic features exhibit comparable\nperformance. However, in the presence of label noise, both NCL and SAE significantly outperform\nacross various datasets. Especially, when the noise rates are aggressive, the improvements are\nsubstantial, with NCL showing a 13.7% improvement on ImageNet-100 and a 9.2% improvement\non CIFAR-10 under 90% noisy labels. The results are consistent with the results in toy models and\nfurther verify that monosemnatic features obtain stronger robustness against label noise.\nRobustness Against Distribution Shifts. For evaluating the resilience of features to distribution\nshifts, we evaluate three types of shifts, including random input noise, random Gaussian noise, and\nreal-world distribution shifts on ImageNet-100 datasets.\nThe models and classifiers are trained on the clean ImageNet-100 dataset while their classification\nperformance is evaluated with noisy samples. As shown in Figure 2(a), 2(b), and 2(c), both the\npretraining constraints and downstream modifications that enhance feature monosemanticity improve\nclassification accuracy under noisy samples, and the benefits rise with the increase of noise strength.\nThe results suggest that the monosemantic features can also enhance the robustness against various\nnoises applied in inputs."}, {"title": "3.2 \u039c\u039f\u039dOSEMANTIC FEATURES ARE ROBUST UNDER FEW-SHOT AND NOISY FINETUNING", "content": "In practice, fully finetuning a large pretrained model on downstream labeled data can often achieve\nbetter performance than linear probing, but it also easily overfits if there are only a few amount of\nlabeled data. Here, we compare standard finetuning (polysemantic) to monosemantic finetuning."}, {"title": "3.2.1 \u039c\u0395THODS FOR MONOSEMANTIC FINETUNING", "content": "Standard Finetuning. For the baseline, we consider a common finetuning setting, i.e., we pretrain\nthe encoders with contrastive learning on unlabeled ImageNet-100 and then learn a linear classifier on\nlabeled Imagenet-100 with the cross-entropy loss: $L_{CE}(f) = E_{x,y} log\\frac{exp(f(x)^{T}w_{y})}{\\sum_{c=1}^{C} exp(f(x)^{T}w_{c})}$, where $f$ is\nthe encoder network and $w_{c}$ is the linear classifier weight of the related label. Unlike linear probing,\nwe train classifiers on the pretrained representations without clipping the gradient of encoders."}, {"title": "3.2.2 EXPERIMENTS", "content": "Few-shot Finetuning. As the finetuning process usually involves fewer training samples, a crucial\nchallenge for feature robustness is preventing overfitting on small training datasets. To evaluate the\nperformance of polysemantic and monosemantic features during few-shot finetuning, we respectively\nuse 10%, 20%, 50% and the entire training set of ImageNet-100 to finetune the pretrained repre-\nsentations with CE and NCE objectives. As shown in Figure 3(a), 3(b), the monosemantic features\nexhibit lower training accuracy but higher validation accuracy in few-shot finetuning, and the\nadvantages grow when the training set becomes smaller, which implies that the monosemanticity\nhelps representations to be less likely to overfit the training set in the downstream task.\nNoisy-label Finetuning. We also evaluate robustness against label noise in finetuning tasks on\nImageNet-100. During the finetuning process, the labels of training samples are uniformly flipped to\nthe other classes with a probability $\\eta$ (noise rate). As shown in Figure 3(c), non-negative finetuning\nleads to significant gains under label noise that keep growing with the increase of the noise rate.\nNotably, monosemantic features exhibit at most 11.9% improvement under large noise rate.\nThese empirical results indicate that maintaining feature monosemanticity during the finetuning\nprocess can bring better learning robustness against overfitting and label noise."}, {"title": "3.3 \u039c\u039f\u039dOSEMANTIC LORA FOR LARGE LANGUAGE MODELS", "content": "In Section 3.2.2, we show that maintaining feature monosemanticity during supervised finetuning\ncan be much more resistant to overfitting. This favorable property suggests that monosemanticity\ncan also benefit LLM finetuning of widely applications today. Existing LLM training has two\nstages: 1) pretraining on large-scale unlabeled data, and 2) supervised finetuning (or post-training)\non small-scale data. Since LLMs are very large and labeled data are small, overfitting becomes a\nsevere issue in LLM finetuning . Given that LLMs, unlike\nsupervised classifiers, do not have a natural representation space and they are more prone to overfit\ndue to the large model size, we extend LoRA, a standard efficient finetuning method, to have a more\nmonosemantic update per layer by prompting sparsity in its update."}, {"title": "3.3.1 METHODS FOR MONOSEMANTIC LLM FINETUNING", "content": "Low-rank Adaptation (LoRA). LoRA (low-rank adaptation) is a de facto method for finetuning\nLLM weights a lower cost by factorizing it into low-rank weights. Specifically, for each LLM weight\n$W_{o} \\in R^{d\\times k}$, we can reparameterize the fine-tuned weight as $\\Delta W = AB$, where $A \\in R^{d\\times r}, B\\in\nR^{r\\times k}$ are two low-rank matrices (with $r < min(d, k)$) actually being learned during finetuning.\nAfter finetuning, the updated output of the linear layer with weight $W$ becomes\n\\begin{equation}\ny = W_{LORA}x = (W_{o} + \\Delta W)x = W_{o}x + \\Delta Wx = W_{o}x + ABx.\n\\end{equation}\nThe LoRA weights can be used separately or merged back to model weights.\nMonosemantic LoRA. Inspired by non-negative finetuning (Section 3.2.1), we add non-negative\nconstraints inside LoRA modules to better promote feature monosemanticity:\n\\begin{equation}\ny = W_{MonoLORA}x = W_{o}x + \\Delta W(x) = W_{o}x + \\sigma(A\\sigma(B\\sigma(x))),\\label{eq:5}\n\\end{equation}\nwhere $\\sigma$ is the non-negative transformation (ReLU by default). Compared to Equation 4, the\nMonoLoRA update encourages the low-rank weights to yield sparse updates that help prevent\noverfitting."}, {"title": "3.3.2 EXPERIMENTS", "content": "When evaluating, we consider a common scenario related to robustness in large language model\nfine-tuning. Specifically, during the fine-tuning process, the large language models often compromise\nthe already learned alignment, which leads to a security risk. In practice, we use\nthe Llama-2-7B-Chat as the aligned model and further finetune it with SST2\nand Dolly datasets as downstream tasks. To evaluate the\nsecurity and alignment performance, we use the ShieldGemma-9B and Beavertails-\n7B models to evaluate the alignment of model responses based on the response on\nBeavertails datasets . More details can be found in Appendix A.3."}, {"title": "4 UNDERSTANDING THE ROBUSTNESS GAINS OF MONOSEMANTICITY", "content": "In Section 3, we provide a comprehensive evaluation of the robustness gains of feature monoseman-\nticity across multiple scenarios. Yet, we do not have a fully clear understanding of why monosemantic\nfeatures are more robust. As a preliminary step to demystify this phenomenon, in this section, we\ninvestigate the influence of monosemanticity on learned classifiers from both empirical (Section 4.1)\nand theoretical (Sections 4.2 & 4.3) perspectives. For simplicity, we focus on the label noise scenario."}, {"title": "4.1 NOISY CLASSIFIERS PREFER MONOSEMANTIC FEATURES IN PRACTICE", "content": "To further understand the robustness improvements brought by monosemanticity, we investigate\nthe difference in the salient features of the robust and non-robust classifiers under noisy conditions."}, {"title": "4.2 REPLICATING MONOSEMANTICITY GAINS WITH THE SUPERPOSITION MODEL", "content": "To further establish a theoretical understanding of the benefits brought by monosemanticity, we\nintroduce a toy model proposed by for the simplicity of analysis. The toy model\nconstructs polysemantic representations with the superposition hypothesis , a\nwidely-used explanation of feature polysemanticity. The hypothesis states that a polysemantic feature\nis an approximately linear combination of multiple latent semantics while a monosemantic feature is\nthe reconstruction of a single natural concept. With the hypothesis, the toy model enables researchers\nto replicate the polysemanticity phenomenon and theoretically analyze the properties of polysemantic\nand monosemantic features, e.g., occurrence conditions, learning dynamics, and geometric structures\n. In this section, we start by\nintroducing the setups and observing the robustness of different features on the toy model.\nToy Model Setups. In practice, we follow the settings proposed by Elhage et al. (2022b) and evaluate\nthe robustness of polysemantic features on the toy model. Specifically, we assume each sample $x$\nhas $n$ dimensions and each dimension represents a natural concept. As the features in real-world\ndatasets are usually sparsely activated , we assume each dimension of a"}, {"title": "4.3 THEORETICAL ANALYSES WITH THE SUPERPOSITION MODEL", "content": "After replicating the robustness gains of monosemanticity on the toy model, we then establish a\ntheoretical comparison between polysemantic and monosemantic features. For ease of theoretical\nanalysis, we consider a binary classification case in the toy model $(n = 2, m = 1, S = 0.2)$. To be\nspecific, a sample $x$ has two latent features $x_{1}, x_{2}$, and the model parameter $W \\in IR^{1\\times 2}$. When we\nobtain the monosemantic features, the model output is $V_{mono} := X_{1}$. In contrast, when obtaining\npolysemantic features, the model keeps more natural concepts than the representation dimension.\nAccording to Elhage et al. , one common geometric structure of polysemantic features is\nantipodal pairs formed by two concepts. Therefore, we assume the learned polysemantic feature to\nbe $V_{poly} := X_{1}X_{2}$.\nFor conciseness of expressions, we introduce the following notations on mean and variance for a given\nfeature representation $v$. For a clean distribution without label noise, we denote the conditional means"}, {"title": "5 CONCLUDING REMARKS", "content": "Recent work has made significant strides in enhancing model interpretability by promoting feature\nmonosemanticity through various techniques. However, a prevailing belief in the literature posits an\naccuracy-interpretability tradeoff, suggesting that achieving monosemantic features for better inter-\npretability necessarily compromises prediction accuracy. In this study, we have challenged this notion\nby demonstrating the advantages of monosemanticity beyond interpretability alone. Specifically,\nwe found that monosemantic features are significantly more robust to various types of distribution\nshifts, including input noise, label noise, and real-world out-of-domain inputs. Additionally, we have\nshown that maintaining feature monosemanticity during fine-tuning serves as an effective regularizer,\nreducing model overfitting in few-shot settings, noisy environments, and during large language\nmodel (LLM) fine-tuning. We also provide an in-depth analysis of the benefits of monosemantic\nfeatures from both theoretical and empirical aspects. These diverse sources of learning robustness\ncollectively indicate that monosemantic features have a general sense of robustness, resonating with\nits benefits in interpretability. Therefore, rather than viewing monosemanticity as a necessary cost for\ninterpretability, we advocate for embracing and exploring the multiple learning advantages it offers.\nWe believe our work, as a pioneering effort in this direction, will inspire future research to investigate\nthese possibilities further."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To ensure the reproducibility of our results, we elaborate on the details of our experiments and\ntheoretical analysis in the main paper and the appendix. In Section 3.1, 3.2, 3.3 of the main paper, we\nrespectively introduce the methods for capturing polysemantic and monosemantic features in linear\nprobing, finetuning vision models and finetuning LLMs. Furthermore, in Appendix A, we introduce\nthe hyperparameters and implementation details of adopted methods, and the detailed settings of\nthe robustness evaluation, including input and label noise, few-shot learning, and out-of-domain"}, {"title": "A EXPERIMENT DETAILS", "content": "A.1 EXPERIMENT DETAILS FOR NOISY LINEAR PROBING\nDuring the pretraining process, we utilize ResNet-18 (He et al., 2016) as the backbone and train the\nmodels on CIFAR-100 and ImageNet-100. We pretrain the model for 200 epochs. The projector is a\ntwo-layer MLP with a hidden dimension 16384 and an output dimension 2048. We train the models\nwith batch size 256 and weight decay 0.0001. When implementing NCL and SAE, we follow the\ndefault settings of SimCLR. For NCL, we adopt ReLU as the activation function $\\sigma$. For SAE, the\nencoder and decoder are linear layers with 2048 input and output dimensions, and the number of\nactivated features in the hidden layer is 256.\nDuring the linear evaluation, we train a classifier following the frozen backbone pretrained by different\nmethods for 50 epochs. For noisy label probing, we apply symmetric label noise when training the\nlinear classifiers, i.e., the labels are uniformly flipped to the other classes with the noisy rate. And for\nrandom input noise, we train the linear classifiers on clean datasets, while applying different scales of\nuniform noise and Gaussian noise to the validation sample. For real-world out-of-domain distribution\nshifts, we use ImageNet-sketch and ImageNet-stylized datasets . As we pretrain the network on ImageNet-100, we select the samples of the corresponding\n100 classes from these out-of-distribution datasets and evaluate the accuracy.\nA.2 EXPERIMENT DETAILS FOR FEW-SHOT AND NOISY FINETUNING FROM PRERTRAINED\nFEATURES\nDuring the pretraining process, we utilize ResNet-18 (He et al., 2016) as the backbone and train the\nmodels on ImageNet-100. We pretrain the model for 200 epochs. We use a projector which is a\ntwo-layer MLP with hidden dimension 16384 and output dimension 2048. We pretrain the models\nwith batch size 256 and weight decay 0.0001. During the finetuning process, we train a classifier\nfollowing the backbone for 100 epochs respectively with standard and non-negative tuning, following\nthe default settings of finetuning. When implementing the non-negative tuning, we select the ReLU\nfunction as the non-negative operator. For few-shot finetuning, we respectively random draw 10%,\n20%, 50%, and 100$ training samples from the original ImageNet-100 training set. For noisy label\nfine-tuning, we still apply symmetric label noise with different noise rates to the training samples.\nA.3 EXPERIMENT DETAILS FOR MONOSEMANTIC LLM FINETUNING\nHyper-parameters We finetune the Llama-2-7b-Chat model in SST2 with 20 epochs, batch size 16\nand learning rate le-4. we Lora with rank r = 8, scaling factor a = 4, and dropout rate 0.1 as default.\nFor Dolly, we finetune it 1 epoch (by common practice), with batch size 4. The LoRA module is\nadded to every query and value mapping module in the base model. For finetuning we use 5000\nsamples from the dataset. For inference, we use 1000 samples."}, {"title": "A.4 EXPERIMENTS DETAILS FOR FIGURE 1", "content": "For Figure 1(a), we respectively draw a random dimension from the models trained by CL and NCL,\nand then draw the top-activated samples along two dimensions. We utilize ResNet-18 (He et al.,\n2016) as the backbone and train the models on ImageNet-100 for 200 epochs.\nFor Figure 1(b), we evaluate the performance in linear probing with noise. During the linear\nevaluation, we train a classifier following the frozen backbone pretrained by different methods for\n50 epochs. For noisy label probing, we apply 90% symmetric label noise when training the linear\nclassifiers. For random input noise, we train the linear classifiers on clean datasets, while applying\nGaussian noise with 0.6 standard variation to the validation sample."}, {"title": "B PROOFS", "content": "B.1 PROOFS RELATED TO THEOREM 4.1\n\u0392.1.1 MONOSEMANTIC REPRESENTATIONS\nIn the monosemantic case, we assume the learned representation only keeps the most important\ndimension $v = X_{1}$.\nTheorem B.1 (Conditional mean and variance of monosemantic representations). The conditional\nmeans and variances of $v_{mono} = X_{1}$ are\n\\begin{equation}\n\\mu_{0}(v_{mono}) = \\frac{1}{3} \\frac{(1 - S)^{2}}{1 + S^{2}} and \\mu_{1}(v_{mono}) = \\frac{1}{3} \\frac{2 + S}{1 + S},\\label{eq:7}\n\\end{equation}\n\\begin{equation}\n\\sigma^{2}_{0}(v_{mono}) = \\frac{1}{6} \\frac{(1 - S)^{2}}{1 + S^{2}} - \\mu_{0}(v_{mono})^{2} and \\sigma^{2}_{1}(v_{mono}) = \\frac{1}{6} \\frac{3 + S}{1 + S} - \\mu_{1}(v_{mono})^{2}.\\label{eq:8}\n\\end{equation}\nProof of Theorem B.1. We first calculate the conditional probability density functions."}, {"title": null, "content": "\\begin{equation}\nP(x_{1} < x|y = 0) = \\frac{P(x_{1} \\leq x, x_{1} \\leq x_{2})}{P(x_{1} \\leq x_{2})}.\n\\end{equation}\n\\begin{equation}\n= \\frac{P(x_{1} \\leq x) - P(x_{1} \\leq x, x_{1} > x_{2})}{P(x_{1} \\leq x_{2})}.\n\\end{equation}\n\\begin{equation}\n= \\frac{P(x_{1} \\leq x) - P(x_{1} < x|y = 1)}{P(x_{1} \\leq x_{2})}.\n\\end{equation}\n\\begin{equation}\n= \\frac{S[S + (1 - S)x", "S)x": 1}, {"S)x": 1}, {"S)x": 1, "1": ""}], "1": ""}, "frac{2(1 - S)^{2}}{1 + S^{2}} (1 - x) dx\n\\end{equation}\n\\begin{equation}\n= \\frac{2(1 - S)^{2}}{1 + S^{2}} \\frac{1}{2} | x = \\frac{1}{3} \\frac{(1 - S)^{2}}{1 + S^{2}}.\n\\end{equation}\nand\n\\begin{equation}\n\\mu_{1} (mono) = \\int_{x\\in (0,"]