{"title": "Leveraging Labelled Data Knowledge: A Cooperative Rectification Learning Network for Semi-supervised 3D Medical Image Segmentation", "authors": ["Yanyan Wang", "Kechen Song", "Yuyuan Liu", "Shuai Ma", "Yunhui Yan", "Gustavo Carneiro"], "abstract": "Semi-supervised 3D medical image segmentation aims to achieve accurate segmentation using few labelled data and numerous unlabelled data. The main challenge in the design of semi-supervised learning methods consists in the effective use of the unlabelled data for training. A promising solution consists of ensuring consistent predictions across different views of the data, where the efficacy of this strategy depends on the accuracy of the pseudo-labels generated by the model for this consistency learning strategy. In this paper, we introduce a new methodology to produce high-quality pseudo-labels for a consistency learning strategy to address semi-supervised 3D medical image segmentation. The methodology has three important contributions. The first contribution is the Cooperative Rectification Learning Network (CRLN) that learns multiple prototypes per class to be used as external knowledge priors to adaptively rectify pseudo-labels at the voxel level. The second contribution consists of the Dynamic Interaction Module (DIM) to facilitate pairwise and cross-class interactions between prototypes and multi-resolution image features, enabling the production of accurate voxel-level clues for pseudo-label rectification. The third contribution is the Cooperative Positive Supervision (CPS), which optimises uncertain representations to align with unassertive representations of their class distributions, improving the model's accuracy in classifying uncertain regions. Extensive experiments on three public 3D medical segmentation datasets demonstrate the effectiveness and superiority of our semi-supervised learning method. The code is available at https://github.com/Yaan-Wang/CRLN.git.", "sections": [{"title": "1. Introduction", "content": "3D medical image segmentation plays a crucial role in healthcare tasks by automatically identifying internal structures in medical volumes. While fully supervised segmentation has achieved outstanding performance (Minaee et al., 2021; Wang et al., 2022), it requires large amounts of voxel-wise annotations that are time-consuming and labour-intensive to acquire. In contrast, collecting large amounts of unlabelled samples is easier to obtain (Li et al., 2020a; Lei et al., 2022). Hence, semi-supervised medical image segmentation can relieve the annotation burden by utilising numerous unlabelled data, accompanied by a few labelled ones (Chen et al., 2022; Wu et al., 2022). A key concern in this field is how to take advantage of unlabelled medical samples effectively (Luo et al., 2021a; Li et al., 2024).\nA classical approach is to employ a consistency learning strategy across different views of unlabelled data (Wu et al., 2021). An example of such strategy is the Mean Teacher (MT) model, which employs a \"teacher-student\" framework, where the student network generates predictions from the strongly augmented unlabelled data to converge towards the teacher's prediction of the weakly augmented version of the same unlabelled data (Adiga V. et al., 2024). However, the pseudo-labels generated by the teacher may include incorrect predictions, which could adversely impact training, leading to the phenomenon known as confirmation bias (Zhang et al., 2022). To address this problem, some methods employ multiple auxiliary teacher models that are initialized and updated differently, thereby generating diverse pseudo-labels for the distinct views (Zhao et al., 2023). The combination of such diverse pseudo-labels tends to yield more robust final labels, implicitly mitigating the confirmation bias. A potential drawback of employing multiple auxiliary teacher models is the challenge they may face in consistently enhancing the robustness of pseudo-labels across diverse views. This inconsistency stems from the varying predictions of different views, which may not consistently align with the true ground truth.\nAnother widely adopted strategy to alleviate confirmation bias involves filtering out the noisy regions based on prediction uncertainty (Luo et al., 2021b; Yu et al., 2019). As illustrated in Fig. 1(a), the percentage of reliable predictions (i.e., predictions with low uncertainty measures) from pseudo-labels is not high during training, especially in the early stages. This means that a large number of unlabelled training data containing unreliable predictions are discarded, so they are not used for training. An additional problem is that voxels classified as reliable may in fact contain wrong predictions, as shown in Fig. 1(b) (see results for 'Reliable Pseudo-labels'), which may result in confirmation bias.\nIn this paper, we introduce a new methodology to improve the prediction accuracy of pseudo-labels in consistency-based semi-supervised learning approaches, thereby addressing the limitations mentioned above. The methodology, shown in Fig. 2, has three important contributions. The first contribution is the Cooperative Rectification Learning Network (CRLN) that leverages the prior information present in the learned set of prototypes for each segmentation class. These prototypes are used to adaptively rectify pseudo-labels at the voxel level. The second contribution is the Dynamic Interaction Module (DIM) that provides accurate voxel-level clues for pseudo-label rectification by exploring pairwise and cross-class interactions between prototypes and multi-resolution image features. CRNL and DIM are designed to improve the quality of pseudo-labels, allowing more unlabelled data to be used during training, as shown by the methods after rectification in Fig. 1. Furthermore, semi-supervised segmentation models usually exhibit low classification confidence in discriminating samples located at uncertain regions of the feature space, such as at class bound-ary regions. To enhance the discrimination of uncertain re-gions, a Collaborative Positive Supervision (CPS) mechanism is proposed as the third contribution of this paper. CPS is a contrastive learning method, where the main innovation is in the definition of the positive representations based on an unassertive class representation that combines the learned prototypes and the mean class representation, as opposed to the more common positive representation based on the mean value of the class representations (Liu et al., 2021).\nTo summarise, our main contributions are:\n1. The new CRLN method that learns multiple prototypes per class to be explored as external knowledge priors for the rectification of pseudo-labels.\n2. The novel DIM module devised to capture holistic relationships across multiple class prototypes and unlabelled data in a pairwise and cross-class manner, providing critical clues to rectify pseudo labels to improve their accuracy.\n3. The innovative CPS mechanism to encourage the uncertain representations to move closer to their positive representations, defined by an unassertive class representation that combines the learned prototypes and the mean class representation, which gives the model the ability to distinguish the uncertain regions.\nWe show that our semi-supervised learning method produces the best result in the field on three public 3D medical segmentation datasets, namely: the Left Atrium (LA) (Xiong et al., 2021), Pancreas-CT (Clark et al., 2013), and Brain Tumour Segmentation 2019 (BraTS19) (Menze et al., 2014). The remainder of the paper is organised as follows: Section 2 provides an overview of medical image segmentation and semi-supervised medical image segmentation. Then, the details of the proposed method are presented in Section 3. Section 4 illustrates experimental results and relevant analysis. Finally, Section 5 concludes this paper."}, {"title": "2. Related Work", "content": "2.1. Medical Image Segmentation\nMedical image segmentation aims to assign a closed-set class label to each pixel. UNet (Ronneberger et al., 2015) has gradually become the preferred model in the medical segmentation field since it was first proposed in 2015. Such success is mainly attributed to its unique structural design, especially the encoder-decoder structure and the skip connection mechanism, which endows the model with strong detail recovery. Subsequently, it is enhanced by exploring dense skip connections (Guan et al., 2019; Zhou et al., 2018), multi-scale receptive fields (Xiao et al., 2018) and global information (Chen et al., 2021). In addition to 2D medical scenes, UNet has been extended to 3D medical scenes, such as MRI and CT, by replacing 2D convolutions with 3D convolutions (\u00c7i\u00e7ek et al., 2016). Another similar approach is VNet (Milletari et al., 2016), which also employs the encoder-decoder structure. The difference with 3D-UNet is that VNet utilises a convolutional layer instead of the pooling layer for downsampling, thereby mitigating information loss.\nMore recent studies (He et al., 2023; H\u00f6rst et al., 2024) have explored methods based on vision transformer (ViT) (Dosovitskiy et al., 2020) for the medical segmentation task, leveraging their capability for long-range modelling. Other recent approaches (Chowdary and Yin, 2023; Wu et al., 2024) can enhance the segmentation quality by introducing the Diffusion Probabilistic Model (DPM) (Ho et al., 2020). Although the fully supervised segmentation techniques described above exhibit solid performance, they require a large number of voxel-based annotations that are difficult and expensive to obtain in real medical scenarios. One way to mitigate the need for such annotations is based on the development of semi-supervised learning methods that require much smaller sets of annotated data and large sets of un-annotated data. We review semi-supervised learning methods below.\n2.2. Semi-supervised Medical Image Segmentation\nSignificant advancements have been made in semi-supervised medical image segmentation (Miao et al., 2023; Wang et al., 2023). Current models primarily adopt consistency regularisation strategies to leverage unlabelled information (Hang et al., 2020; Wang et al., 2020). The underlying idea behind these strategies is that the model's predictions for unlabelled samples should remain consistent under various perturbations (Bai et al., 2023; Gao et al., 2023). The Mean Teacher framework, which explores weak and strong augmentation strategies, is quite effective in the implementation of this idea. Based on this framework, various weak and strong augmentation techniques have been proposed to generate prediction disagreements. Li et al. (2020b) apply different data augmentation techniques, such as Gaussian noise and contrast variation, on the input data. Liu et al. (2022c) adjust the spatial context of the input samples to enrich their diversity. Also, Xu et al. (2022) and Zheng et al. (2022) focus on inducing prediction inconsistencies by adding perturbations at the feature level.\nDespite the promising performance of well-designed data augmentation techniques, the pseudo-labels generated by teacher networks still contain a fair amount of noise, which hinders the model's segmentation capability. Recent works argue that incorporating additional supervised information would help mitigate this problem. One of the representative efforts is the multi-teacher embedding approach (Liu et al., 2022b; Zhao et al., 2023). The core idea of this kind of approach lies in the generation of pseudo-labels from different perspectives. To ensure diversity, the teacher models typically employ different initialization parameters and update mechanisms. However, not all perspectives necessarily improve the accuracy of pseudo-labels, and sometimes conflicting labels may emerge. It is thus difficult to utilise different perspectives from unlabelled information to improve segmentation performance in complex situations.\nAnother technique to suppress the negative effects of noise in pseudo-labels is to filter out or reduce the weight of samples classified as uncertain during training (Wang et al., 2021; Xia et al., 2020). UA-MT estimates the uncertainty of the teacher's prediction with the classification entropy and uses only reliable (i.e., low-entropy) predictions to supervise the student network (Yu et al., 2019). Luo et al. (2021b) propose to"}, {"title": "3. Method", "content": "The semi-supervised 3D medical image segmentation task aims to achieve precise organ segmentation utilising a limited number of labelled samples and a large number of unlabelled samples. Let $\\mathcal{L} = \\{x_l, y_l\\}_{l=1}^{L}$ represents the labelled dataset, where $x_l \\in \\mathcal{X} \\subset \\mathbb{R}^{H \\times W \\times D}$ is the input volume, and $y_l \\in \\mathcal{Y} = \\{0,1\\}^{C \\times H \\times W \\times D}$ is the voxel-wise one-hot label within the $C$ classes. Additionally, the unlabelled dataset is denoted by $\\mathcal{U} = \\{x_u\\}_{u=1}^{U}$, where $U >> L$. As shown in Fig. 3, our method is built upon the teacher-student paradigm, so we begin with a brief review of its workflow in Sec. 3.1. Next, the Cooperative Rectification Learning Network (CRLN) and Dynamic Interaction Module (DIM) are introduced in Sec. 3.2, and the Collaborative Positive Supervision (CPS) is explained in Sec. 3.3. Finally, Sec. 3.4 summarises the overall training objective.\n3.1. Preliminaries\nThe teacher-student paradigm employs a weak-to-strong consistency regularisation to train the segmentation model, allowing for simultaneous utilisation of labelled and unlabelled samples. More concretely, each unlabelled volume $x_u \\in \\mathcal{U}$ undergoes a weak augmentation process, denoted by $A_w : \\mathcal{X} \\rightarrow \\mathcal{X}$ (e.g., random cropping and random flipping), as well as strong augmentation $A_s : \\mathcal{X} \\rightarrow \\mathcal{X}$ (e.g., random cropping, random flipping, random noise and CutMix (Yun et al., 2019)). Subsequently, the strongly augmented volume is fed into the student model, whereas the weakly augmented volume is fed to the teacher model to generate the pseudo-label. So the model can be optimised by the following objective:\n$\\mathcal{L}(\\mathcal{L}, \\mathcal{U}, \\theta) = \\frac{1}{L} \\sum_{(x_l, y_l) \\in \\mathcal{L}} l_s(f_{\\theta_{sd}}(f_{\\theta_{se}}(x_l)), y_l) + \\frac{1}{U} \\sum_{x_u \\in \\mathcal{U}} l_u(f_{\\theta_{sd}}(f_{\\theta_{se}}(A_s(x_u))), A_g(f_{\\theta_{td}}(f_{\\theta_{te}}(A_w(x_u))))$, (1)\nwhere $\\theta = \\{\\theta_{sd}, \\theta_{se}, \\theta_{td}, \\theta_{te}\\}$ represents the student and teacher model parameters, $f_{\\theta_{sd}}(f_{\\theta_{se}}(x))$ denotes the student predictor (with $f_{\\theta_{sd}} : \\mathcal{F} \\rightarrow [0,1]^{C \\times H \\times W \\times D}$ denoting the student decoder, $f_{\\theta_{se}} : \\mathcal{X} \\rightarrow \\mathcal{F}$ representing the student encoder, and $\\mathcal{F} \\in \\mathbb{R}^{F}$), $f_{\\theta_{td}}(f_{\\theta_{te}}(x))$ represents the teacher predictor (similarly, $f_{\\theta_{td}} : \\mathcal{F} \\rightarrow [0,1]^{C \\times H \\times W \\times D}$ is the teacher decoder and $f_{\\theta_{te}} : \\mathcal{X} \\rightarrow \\mathcal{F}$ is the teacher encoder), $A_g : [0, 1]^{C \\times H \\times W \\times D} \\rightarrow [0, 1]^{C \\times H \\times W \\times D}$ represents the geometric transformation used to align the teacher's and student's predictions, $l_s(\\cdot)$ is the supervised learning term (e.g., Dice loss and cross-entropy loss), and the unsupervised learning loss is defined by\n$l_u(\\hat{y}, \\tilde{y}) = \\frac{1}{\\Omega} \\sum_{\\omega \\in \\Omega} \\mathbb{1}(\\max_{\\hat{c} \\in \\{1,...,C\\}}(\\tilde{y}(\\omega, \\hat{c})) \\geq \\tau) \\times l_{nll}(\\hat{y}(\\omega, :), \\tilde{y}(\\omega, :))$, (2)\nwith $\\Omega$ denoting the image lattice, $\\hat{y} = f_{\\theta_{sd}}(f_{\\theta_{se}}(A_s(x)))$, $\\tilde{y} = f_{\\theta_{td}}(f_{\\theta_{te}}(A_w(x)))$, $l_{nll}(\\cdot)$ being the negative log-likelihood loss, and $\\mathbb{1}(\\max_{\\hat{c} \\in \\{1,...,C\\}}(\\tilde{y}(\\omega, \\hat{c})) \\geq \\tau)$ representing the indicator function that filters out uncertain predictions where the maximum probability is smaller than $\\tau \\in [0, 1]$.\nBoth the student and teacher models share the same architecture, with the weights of the teacher model updated by the Exponential Moving Average (EMA) of the student model weights.\n3.2. Pseudo-label Rectification\nThe filtering mechanism described in (2) effectively addresses the adverse effects of uncertainty in the pseudo-labels. However, an excessive filtration process runs the risk of underutilising the valuable information present in the unlabelled training set. To address this issue, we propose a method to rectify the uncertain predictions of the pseudo-labels by leveraging the knowledge present in the labelled data. Such rectification will allow the use of a larger number of unlabelled training samples, thereby improving the generalisation of the model.\nInspired by recent progress shown in the development of transformer-based architectures(Li et al., 2023) and few-shot learning (Liu et al., 2022a), we propose the Cooperative Rectification Learning Network (CRLN) to learn discriminative class prototypes from labelled data that are then used for the rectification of predicted pseudo-labels. As depicted in Fig. 3, CRLN comprises a prototype learning stage and a rectification stage. Moreover, to enhance the learning of prototypes and provide accurate voxel-level cues for correction, the Dynamic Interaction Module (DIM) is devised to take pairwise interactions, as well as spatial-aware and cross-class aggregation between prototypes and labelled data (in the learning stage) or the unlabelled data (in the rectification stage).\n3.2.1. Learning Stage\nLearning Multiple Class Prototypes. Class prototypes are denoted by vectors in the feature space $\\mathcal{F}$ of the decoder, offering a compressed representation of the class distribution. Since medical scenarios are complex and variable, it is difficult for a single prototype to cover the richness of the class representation. Therefore, we propose a method that represents a class with multiple prototypes. The prototypes are denoted by $P \\in \\mathbb{R}^{C \\times R \\times F}$, where $C$ is the number of classes, $R$ represents the number of prototypes for each class and $F$ is the dimensionality of the feature space $\\mathcal{F}$ of the decoder.\nThese multiple prototypes per class are estimated during the learning stage of the CRLN, which also depends on the DIM module. To capture the most relevant features of each class and maintain diversity among multiple prototypes from a global perspective, we propose first grouping multiple prototypes into $R$ matrices of size $C \\times F$, with each matrix containing one prototype per class, and then gradually interacting these prototype matrices with the features through cross-attention. As illustrated in Fig. 4, the prototypes $P$ are divided into $R$ sets $\\{p_i\\}_{i=1}^{R}$, with each $p_i \\in \\mathbb{R}^{C \\times F}$ containing $C$ prototypes. The feature $f_2 \\in \\mathbb{R}^{H/4 \\times W/4 \\times D/4 \\times F}$ extracted from the second intermediate layer of the student decoder is processed by a 1 \u00d7 1 \u00d71 convolution layer to produce $r_2 = Conv_{1\\times1\\times1}(f_2)$. These transferred contexts $r_2$ then interact with each prototype matrix in $\\{p_i\\}$ by the pair-wise cross attention block I, formulated as\n$m_i^I = \\frac{\\phi(p_i)\\psi(r_2)}{\\sqrt{F}}, i \\in \\{1, ..., R\\},$ (3)\n$p_i^I = softmax(m_i^I) \\phi(r_2), i \\in \\{1, ..., R\\},$ (4)\nwhere $\\phi(\\cdot)$, $\\psi(\\cdot)$ and $\\varphi(\\cdot)$ are the linear projectors of block I, $\\dagger$ denotes the matrix transpose operation, and $m_i^I \\in \\mathbb{R}^{C \\times H/4 \\times W/4 \\times D/4}$.\nSubsequently, the updated prototype matrices $\\{p_i^I\\}_{i=1}^{R}$ interact with the features $f_3 \\in \\mathbb{R}^{H/2 \\times W/2 \\times D/2 \\times F_3}$ that are extracted from the third intermediate layer of the student decoder, where $F_3 < F$. Such interaction is processed by the pair-wise cross attention block II, in the same way as in (3), to generate the proximity matrices defined as\n$m_i^{II} = \\frac{\\varphi(p_i^I)\\theta(r_3)}{\\sqrt{F_3}},$ (5)\nwhere $i \\in \\{1, ..., R\\}$, $r_3 = Conv_{1\\times1\\times1}(f_3)$, and $m_i^{II} \\in \\mathbb{R}^{C \\times H/2 \\times W/2 \\times D/2}$. By interacting with features from various decoder layers through the repeating pair-wise cross attention operations, the class prototypes can progressively absorb both texture and semantic information, thus continuously refining their representations.\nThe computed proximity matrices are used to produce the holistic relationship map that is taken as clues to rectify the pseudo-label predictions. A simple solution would be to produce these clues by summing the proximity matrices, but that would not allow us to account for dependencies between classes. Specifically, we first incorporate spatial consistency into our approach by re-evaluating the relationships between feature points and each prototype group based on the local context by applying a 3 \u00d7 3 \u00d7 3 convolution layer to $M^{II} \\in [\\mathbb{R}^{C \\times R \\times N}$ (with $N = H/2 \\times W/2 \\times D/2), which is a tensor containing the proximity matrices $\\{(m_i^{II})\\}_{i=1}^{R}$, with $m_i^{II} \\in \\mathbb{R}^{C \\times N}$ computed from (5).\nThen, we synthesise the relationships across the $R$ prototypes within each class using a 1 \u00d7 1 \u00d7 1 convolution layer that is more flexible than just summing tensor over the $R$ dimensions to produce the holistic relationship map. It is important to note that the convolutional parameters are shared between different classes to enable information interaction across classes. The whole process is formulated as\n$M(x) = Upsample (Conv_{1\\times1\\times1} (Conv_{3\\times3\\times3}(M^{II}))),$ (6)\nwhere $M(x) \\in \\mathbb{R}^{C \\times H \\times W \\times D}$ represents the holistic relationship map. We denote all learnable parameters in (3), (4), (5), (6) as $\\Theta_{dim}$, where the whole DIM is denoted by\n$f_{\\theta} : \\mathcal{X} \\rightarrow \\mathbb{R}^{C \\times H \\times W \\times D}$ (7)\nLearning to Rectify. The holistic relationship map from the DIM, defined in (6), serves to dynamically rectify each voxel from the original prediction $\\hat{y}$ with\n$\\hat{y}_r = \\hat{y} + (1 - \\mu) \\times M(x),$ (8)\nwhere $\\mu \\in [0, 1]$, as shown in Fig. 3, is a learnable parameter utilised to regulate the extent of the correction and $\\hat{y} = f_{\\theta_{sd}}(f_{\\theta_{se}}(A_s(x)))$, with $x$ belonging to the labelled set $\\mathcal{L}$.\n3.2.2. Rectification Stage\nAfter $S$ learning iterations, we rectify the pseudo-labels of the unlabelled samples in $\\mathcal{U}$ with the learned prototypes $P \\in \\mathbb{R}^{C \\times R \\times F}$. The features at different scales of unlabelled samples follow the same progressive method in the learning stage to interact with the multiple prototypes from labelled samples to generate the holistic relationship map $M(x)$. Since this progressive interaction integrates the advantages of features from multiple layers, it provides accurate voxel-level cues for correcting the pseudo-labels. Then, the original pseudo-labels $\\bar{y} = f_{\\theta_{td}}(f_{\\theta_{te}}(A_w(x)))$, with $x \\in \\mathcal{U}$, can be rectified by (8), thus generating higher-quality pseudo-labels $\\bar{y}_r = \\bar{y} + (1-\\mu) \\times M(x)$.\n3.3. Collaborative Positive Supervision\nAs discussed in Sec. 3.2, the CRLN leverages labelled data as priors to rectify pseudo-labels in voxel space, thus providing more reliable supervision for model training. Nonetheless, the model may still exhibit low confidence in segmenting challenging regions, such as edge regions and low-contrast regions. Therefore, we aim to mitigate this problem by enhancing the discriminative characteristics of the representations extracted from such challenging regions with contrastive learning. However, considering the practicality of the model's prediction of uncertain regions, we propose a more moderate contrastive learning strategy based on the InfoNCE loss (He et al., 2020), namely the Collaborative Positive Supervision (CPS) mechanism.\nAs shown in Fig. 5, the proposed CPS mechanism is applied to the representations, extracted from the fourth layer of the student decoder, denoted by $f_4 \\in \\mathbb{R}^{H \\times W \\times D \\times F_4}$, where $F_4 < F_3$. We first construct the anchor, negative and positive sets from $f_4$ for the InfoNCE loss, as explained below.\nAnchor Set. For each class $c$, we randomly sample anchor points from uncertain regions of the feature space to form the class-specific set:\n$\\mathcal{R}_c = \\{r(\\omega, :) | \\omega \\in \\Omega_c, r = Conv_{1\\times1\\times1} (Conv_{3\\times3\\times3}(f_4))\\},$ (9)\nwhere $\\Omega_c$ denotes a subset of the lattice of size $H \\times W \\times D$ in the feature space. For labelled data in $\\mathcal{L}$, this lattice subset is defined by $\\Omega_c = \\mathbb{1} (y(\\omega, c) = 1) \\times \\mathbb{1} (\\hat{y}(\\omega, c) < \\tau)$. For unlabelled data in $\\mathcal{U}$, $\\Omega_c$ is defined as\n$\\Omega_c = \\mathbb{1} (\\hat{y}(\\omega, c) < \\tau) \\times \\mathbb{1} (\\max_{\\hat{c} \\in \\{1,...,C\\}} \\bar{y}_r(\\omega, \\hat{c}) > \\tau_w) \\times \\mathbb{1} (\\underset{\\hat{c} \\in \\{1,...,C\\}}{\\text{argmax }} \\bar{y}_r(\\omega, \\hat{c}) = c),$ (10)\nwhere $\\tau_w \\in [0, 1]$ is used to filter out extremely unreliable pseudo-labels, with $\\tau_w < \\tau$.\nNegative Set. The negative set for class $c$ is defined as\n$\\mathcal{R}_c^- = \\{r(\\omega, :) | \\omega \\in \\Omega_c^-, r = Conv_{1\\times1\\times1} (Conv_{3\\times3\\times3}(f_4))\\},$ (11)\nwhere for labelled data in $\\mathcal{L}$, $\\Omega_c^- = \\mathbb{1} (y(\\omega, c) \\neq 1)$, For unlabelled data in $\\mathcal{U}$, $\\Omega_c^-$ is defined by\n$\\Omega_c^- = \\mathbb{1} (\\max_{\\hat{c} \\in \\{1,...,C\\}} \\bar{y}_r(\\omega, \\hat{c}) > \\tau_w) \\times \\mathbb{1} (\\underset{\\hat{c} \\in \\{1,...,C\\}}{\\text{argmax }} \\bar{y}_r(\\omega, \\hat{c}) \\neq c).$ (12)\nPositive Set. For the class $c$ positive set, we need to consider that the uncertain regions can be challenging for the student model because their representations may deviate significantly from the class prototypes. Therefore, the samples in the positive set should contain the key characteristics of the class, but these samples should not be too far from the uncertain representations to help the learning process-we refer to these positive samples as unassertive. Based on this argument, we propose a positive set containing samples formed with the combination of the learned class prototypes $P$ and the mean representation of the current class. More specifically, the class-specific positive sample is defined by\n$\\mathcal{R}_c^+ = \\{r | r = \\frac{r_m + \\xi \\cdot mean(P_c)}{1 + \\xi}, \\xi \\sim \\mathcal{U}(0, 1)\\},$ (13)\nwhere $\\mathcal{U}(0, 1)$ generates a uniformly distributed random number in (0,1], $P_c \\in \\mathbb{R}^{R \\times F}$ denotes the learned prototypes for class $c$ with $mean(\\cdot)$ being the mean operator to produce the class-specific representation of the prototypes of class $c$, and $r_m$ is the mean of the features computed with $r = Conv_{1\\times1\\times1}(Conv_{3\\times3\\times3}(f_4))$ from labelled data $(x, y) \\in \\mathcal{L}$, where $y(\\omega, c) = 1$, and from unlabelled data $x \\in \\mathcal{U}$, where $\\max_{\\hat{c} \\in \\{1,...,C\\}}(\\bar{y}_r(\\omega,\\hat{c})) > \\tau_w$, and $\\underset{\\hat{c} \\in \\{1,...,C\\}}{\\text{argmax }} \\bar{y}_r(\\omega, \\hat{c}) = c$.\nAfter obtaining the anchor, negative and positive sets, the contrastive loss of the proposed CPS is formulated as\n$l_{cp}(\\mathcal{L}, \\mathcal{U}, \\theta) = - \\sum_{c \\in \\{1,...,C\\}} \\sum_{r \\in \\mathcal{R}_c} \\log \\frac{e^{\\cos(r, r_c^+)/t}}{\\sum_{r' \\in \\mathcal{R}_c^-} e^{\\cos(r, r')/t} + e^{\\cos(r, r_c^+)/t}},$ (14)\nwhere $t$ represents the temperature parameter of the InfoNCE loss.\n3.4. Our Holistic Training Objective\nTo take into account CRLN's learning of prototypes, DIM's learning of cross-attention block parameters, and CPS's representation learning, we extend the overall loss in (1) as:\n$\\mathcal{L}(\\mathcal{L},\\mathcal{U},\\theta, P) = \\frac{1}{L} \\sum_{(x_l, y_l) \\in \\mathcal{L}} l_s(f_{\\theta_{sd}}(f_{\\theta_{se}}(x_l)), y_l) + l_s(M(x), y) + \\frac{1}{U} \\sum_{x_u \\in \\mathcal{U}} l_u(f_{\\theta_{sd}}(f_{\\theta_{se}}(A_s(x_u))), \\bar{y}_r) + l_{cp}(\\mathcal{L}, \\mathcal{U}, \\theta),$ (15)\nwhere $\\theta = \\{\\theta_{sd}, \\theta_{se}, \\theta_{td}, \\theta_{te}, \\theta_{sdim}, \\theta_{tdim}\\}$ (with $\\theta_{sdim}, \\theta_{tdim}$ representing the student's and teacher's DIM parameters defined in (7)) and $P = \\{P_t, P_s\\}$ (with $P_t, P_s$ denoting the student's and teacher's prototypes). We also learn DIM's correction parameter $\\mu$ in (8) with the following loss function:\n$l_{\\gamma}(\\mathcal{L}, \\mu) = \\frac{1}{L} \\sum_{(x_l, y_l) \\in \\mathcal{L}} l_s(\\bar{y}_r, y),$ (16)\nwith $\\bar{y}_r = \\hat{y} + (1 - \\mu) \\times M(x)$. The training alternates the minimisation of the losses in (15) and (16) and the Exponential Moving Average (EMA) to update the teacher's parameters, as described in Algorithm 1."}, {"title": "4. Experiments", "content": "4.1. Datasets\nThe effectiveness of the proposed method is assessed on three public 3D medical image datasets with diverse tissue types, including the Left Atrium (LA)(Xiong et al., 2021), Pancreas-CT(Clark et al., 2013), and Brain Tumour Segmentation 2019 (BraTS19)(Menze et al., 2014) datasets.\nLA dataset. The LA dataset (Xiong et al., 2021) is a standard 3D medical image semi-supervised segmentation benchmark, consisting of 100 3D MRI volumes with a fixed resolution of 0.625 \u00d7 0.625 \u00d7 0.625 mm. Following former works (Liu et al., 2022c; Yu et al., 2019), all volumes are cropped to 112 \u00d7 112 \u00d7 80, centred on the heart region. 80 volumes are used as the training dataset, while the remaining 20 volumes are allocated for validation.\nPancreas-CT dataset. The Pancreas-CT dataset (Clark et al., 2013) is collected from 53 male and 27 female subjects at the National Institutes of Health Clinical Centre. It contains 82 3D contrast-enhanced CT scans. The size of each scan is 512 \u00d7 512, but the thickness varies from 1.5 to 2.5 mm. We adopt the same pre-processing methods as prior studies (Liu et al., 2022c; Wu et al., 2022), including clipping the voxel values to the range of [-125, 275"}]}