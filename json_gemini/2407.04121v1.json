{"title": "Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models", "authors": ["Yuyan Chen", "Qiang Fu", "Yichen Yuan", "Zhihao Wen", "Ge Fan", "Dayiheng Liu", "Dongmei Zhang", "Zhixu Li", "Yanghua Xiao"], "abstract": "Large Language Models (LLMs) have gained widespread adoption in various natural language processing tasks, including question answering and dialogue systems. However, a major drawback of LLMs is the issue of hallucination, where they generate unfaithful or inconsistent content that deviates from the input source, leading to severe consequences. In this paper, we propose a robust discriminator named RelD to effectively detect hallucination in LLMs' generated answers. RelD is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics. Our experimental results demonstrate that the proposed RelD successfully detects hallucination in the answers generated by diverse LLMs. Moreover, it performs well in distinguishing hallucination in LLMs' generated answers from both in-distribution and out-of-distribution datasets. Additionally, we also conduct a thorough analysis of the types of hallucinations that occur and present valuable insights. This research significantly contributes to the detection of reliable answers generated by LLMs and holds noteworthy implications for mitigating hallucination in the future work.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have revolutionized various fields [9, 40, 88], including logical reasoning [3, 47], question answering [12, 14, 84], text generation [11, 13, 90], and vertical domains [16, 49, 82]. However, LLMs encounter numerous challenges that hinder their optimal performance. These challenges include the inability to update knowledge in real-time [15], the lack of genuine emotion and thought [7, 10], and the generation of long-winded and verbose answers [35], among others. Notably, one of the most critical failures is the presence of factual errors in the generated text [5], which gives rise to \"Hallucinations\u201d as depicted in Fig 1. The existence of such \"Hallucinations\" poses a severe hindrance to the widespread adoption of LLMs in non-chatbot scenarios, particularly in domains like medicine and finance where factual accuracy is crucial. The potential risks associated with erroneous information can lead to significant economic losses or even jeopardize human safety [1]. Consequently, the elimination of factual errors in LLMs has become an essential requirement in both industry and academia.\nThe issue of hallucinations in natural text generation has long been acknowledged by researchers [34, 42, 44], and the causes of these hallucinations are complex and multifaceted. On one hand, the large-scale data corpus employed for training LLMs unavoid-ably contains some erroneous information, which gets learned and stored in the model parameters [54, 62, 67]. Consequently, when generating text, LLMs tend to prioritize their parameterized knowledge, thereby resulting in the production of hallucinatory content [51]. On the other hand, the decoder component of LLMs is typically trained using maximum likelihood estimation [4, 64]. During training, ground-truth serves as the input prefix for predict-ing subsequent tokens. However, during inference, the next token is predicted based on the generated history sequence [30]. This dis-crepancy in the prediction process makes it easier for hallucinations to occur.\nExisting research on detecting hallucinations of LLMs' gen-erated answers primarily encompasses statistical, model-based, and human-based evaluations [34, 41]. Statistical evaluation in-volves direct calculation of vocabulary matching between the gen-erated text and reference target text, employing metrics such as ROUGE [45] and BLEU [59]. Some studies also utilize the Knowl-edge F1 (KF1) [73] metric to reduce knowledge hallucination in state-of-the-art chatbots. This KF1 metric is particularly suitable for detecting hallucinations in knowledge dialogue scenarios. Addi-tionally, Shen et al. [72] conduct a large-scale assessment, including correctness and unanswerable question identification, to evaluate ChatGPT's reliability in generic question-answering scenarios. Ye et al. [86] undertake a preliminary study to assess the robustness, consistency, and credibility of LLM systems. However, these metrics rely on vocabulary matching and surface-level metrics, which may not capture semantic coherence or accurately detect hallucinations. Model-based evaluation defines the hallucination score based on the entailment probability between the source text and the generated text. This involves judging whether a hypothesis (i.e., generated text) is entailed by the premise (i.e., reference text). Model-based evaluation incorporates various metrics, including Information Ex-traction (IE)-based metrics, QA-based metrics [23, 71, 78], Natural Language Inference (NLI) metrics [24, 25, 32], Faithfulness Clas-sification metrics [32, 48, 89], and LM-based metrics [26, 75]. For example, Honovich et al. [32] employ the Q\u00b2 method of QA sys-tems to assess the consistency between the response and external knowledge. Azaria et al. [2] utilize the internal state and hidden layer activations of LLMs to detect the truthfulness of generated statements. However, these methods lack a comprehensive set of metrics to effectively balance the advantages and disadvantages of different evaluation criteria. As a result, models often rely heavily on single labels without considering a broader range of factors. Human-based evaluation involves scoring hallucinatory text or di-rectly comparing it with the ground truth [69, 73], which inevitably increases research costs.\nTo address these limitations and achieve a more balanced ap-proach, we combine automatic metrics with model-based evalua-tion, which aims to align with trends observed in human evaluation scores [39]. Therefore, in this work, we focus on building a robust discriminator, RelD, which is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics, in order to effectively detect hallucinations in the generated answers of LLMs. Specifically, the RelQA dataset comprises 274,426 samples, encompassing diverse sources such as Wikipedia, Baidu Zhidao, Bing user queries, and Chinese high school reading comprehension, etc. These datasets cover a range of domains including Wikipedia, news, education, and stories, utilizing various formats such as extractive reading comprehension and multiple-choice questions. To compre-hensively evaluate LLMs' generated answers in the RelQA dataset, we adopt a set of comprehensive metrics, including LLM-assessment metrics, human metrics, machine metrics, and composite metrics. Additionally, we introduce a novel and robust discriminator, RelD, which is trained on RelQA, to detect hallucinations and analyze the types of them present in the generated answers of LLMs. Our experimental results demonstrate that RelD performs admirably in detecting hallucinations across diverse LLMs and for both in-distribution and out-of-distribution datasets. Our contributions in this paper can be outlined as follows:"}, {"title": "2 DATA CONSTRUCTION", "content": "In this section, we present the process of constructing RelQA. We begin by using questions from various existing nine datasets as inputs to different LLMs to generate corresponding answers. Next, we design a comprehensive set of metrics to evaluate the reliabil-ity of these generated answers. The combined collection of the original nine datasets, the generated answers by LLMs, and the"}, {"title": "2.1 DATA COLLECTION", "content": "RelQA consists of nine sub-datasets: SQUAD [63], DuReader [31], HotpotQA [85], MSMARCO [55], NewsQA [77], QuAC [18], CoQA [66], TriviaQA-Web [36], and TriviaQA-Wikipedia [36]. The detailed col-lecting steps are as follows:\nStep 1 (Dataset Selection): These datasets are selected due to their unique characteristics, diverse sources, and the enrichment they bring to the overall collection. They cover extractive read-ing comprehension (ERC), multiple-choice (MC), and multi-turn dialogues (MTD) categories. They originate from sources such as Wikipedia, Baidu Zhidao, Bing search, and other platforms, while encompassing domains such as student education, news, web arti-cles, and general knowledge.\nStep 2 (Formatting and Integration): To ensure compatibil-ity and remove dataset boundaries, we perform formatting and integration for all selected datasets based on the aforementioned categories. Each dataset follows a specific standardized format, as illustrated in Table 1 (the second column). We represent the datasets of all categories as {$L_i, D_i$}, where $L_i$ denotes a specific dataset and $D_i$ denotes its standardized format.\nStep 3 (Preprocessing): To facilitate effective processing and generation of answers, we employ preprocessing techniques on the dataset. This involves two primary aspects: personalized prompt instruction design and addressing the limitations associated with long texts. For personalized prompt instruction design, we create question-adaptive prompt instructions for each question based on the question type, as shown in Table 1 (the third column). These prompt instructions guide LLMs in generating better answers that align with different types of questions. To address the challenge of long texts, we implement a sliding window approach [37], seg-menting the texts into smaller windows, each containing 4,000 tokens. This ensures that LLMs receive clear prompt instructions and can effectively handle texts of varying lengths, resulting in more accurate and contextually appropriate answers.\nStep 4 (Answer Generation): We employ several powerful LLMs, including LLaMA [76], BLOOM [70], GPT-J [79], GPT-3 [6], and GPT-3.5 1, to generate answers for evaluation. In the case of longer texts, we slide the window over the text and generate out-puts for each window. The generated outputs for each window are stored to facilitate subsequent filtering and selection of the optimal answers. To maintain answer stability, we ask an LLM to generate the answer three times for each question and select the majority answer as the final answer. Furthermore, to ensure the overall quality and reliability of the generated answers, we con-duct quality assurance procedures, including automated checks to identify and re-generate incomplete sentences by detecting missing sentence-ending punctuation, among others."}, {"title": "2.2 METRIC SELECTION", "content": "To evaluate the reliability of LLMs' generated answers, it is crucial to select appropriate metrics that capture different aspects of answer quality. We employ four types of metrics, including LLM-assessment metric, human metric, machine metric, and composite metric, to comprehensively evaluate the generated answers.\nLLM-assessment metric is inspired by the concept of LLMs' self-evaluation, where LLMs occasionally demonstrate the ability to assess their own output correctly without human intervention [17, 83]. This metric comprises two specific indicators: the goodness of a generated answer and the similarity between the generated answer and the ground-truth answer. By obtaining the goodness score and similarity score of a generated answer, we can evaluate its quality and how closely it aligns with the ground-truth answer. Higher scores indicate better quality and semantic alignment. The LLM-assessment metric provides valuable insights into the LLMs' ability to evaluate the quality of generated answers.\nHuman metric plays a significant role in evaluating the LLM's performance from a human perspective. It includes a human score, which is a binary label assigned to each answer based on the degree of match between the LLM's generated answer and the ground-truth answer, along with the assigned goodness score. The human metric labeling is as follows: i) When the LLM's generated answer is the same as the ground-truth answer and receives a goodness score of 4 or 5, the human metric is labeled as 1. This indicates that the LLM has successfully generated a correct and high-quality answer that aligns with the expected answer. ii) When the LLM's generated answer is different from the ground-truth answer and receives a goodness score of 1, 2, or 3, the human metric is labeled as 2. This suggests that the LLM's generated answer is incorrect or of lower quality compared to the ground-truth answer. iii) For cases where the LLM's generated answer neither matches the ground-truth answer nor falls within the aforementioned goodness score ranges, the human metric is labeled as 0. This label represents a neutral or ambiguous classification, indicating that the answer may require further examination or subjective judgment. The human metric captures the human perception of the LLM's performance.\nMachine metric draws inspiration from question-answering and dialogue systems, which rely on objective metrics to assess the quality of generated answers. It encompasses various categories, including accuracy metrics, overlap metrics, similarity metrics, and diversity metrics. Examples of machine metrics include F1 score, Recall, BLEU [59], BERT score [87], ROUGE (ROUGE-1, ROUGE-2, ROUGE-L) [45], Distinct-N (Distinct-1, Distinct-2) [43], Greedy matching, and Embedding scores (average, extreme) [46]. Specifi-cally, accuracy metrics assess the correctness of generated answers compared to the ground truth, including F1 score. Overlap metrics measure the overlap between generated answers and the ground truth, including BLEU, Recall, ROUGE. Similarity metrics capture the semantic similarity between generated answers and the ground truth, including BERT score, Greedy matching and Embedding scores (average, extreme). Diversity metrics measure the diversity of the generated answers, including Distinct-N. These metrics ob-jectively evaluate the semantic alignment, relevance, diversity, and quality of generated answers, enabling a comprehensive assessment of LLMs' answers.\nComposite metric is designed to provide a comprehensive eval-uation of a model's performance by combining multiple aspects. It includes a final score and a final tag to summarize the evaluation. Each of the metrics mentioned above contributes to the final score, with specific emphasis given to certain metrics. For instance, Recall and ROUGE (ROUGE-1, ROUGE-2, ROUGE-L) may be assigned higher weights (e.g., twice the weight) to highlight the importance of maintaining information [52, 53]. The weights of different met-rics can be dynamically optimized to better assess their importance in real-world scenarios as demonstrated in Experiment 4.3. The final tag is a binary label assigned based on the average score. If the average score is greater than 0.5, it is labeled as 1; otherwise, it is labeled as 0. The final tag simplifies the evaluation outcome, indicat-ing whether the LLMs' generated answer is considered reliable or not. In summary, these metrics collectively evaluate the quality of answers generated by LLMs compared to the ground-truth answers."}, {"title": "2.3 DATA EXPLORATORY ANALYSIS", "content": "In this section, we conduct a data exploratory analysis of the con-structed RelQA dataset, which comprises a total of 1,372,130 sam-ples, including generated answers by five selected LLMs. Among these, 743,910 samples are assigned as reliable and 628,220 samples as unreliable based on the final tag metric. We divide the possi-ble ranges of all metrics into three equal parts, representing low, medium, and high levels. First, we analyze the differences in the LLM-assessment metric across different datasets. Regarding the \"goodness\" metric, the QUAC dataset performs poorly in terms of answer quality, with a high score percentage of 82.72%, while the SQuAD dataset excels in generating high-quality answers, with a high score percentage of 99.47%. Other datasets generally achieve high score percent-ages above 90%. Regarding the \"similarity\" metric, the MSMARCO dataset demonstrates the highest similarity to the reference an-swers, with a high similarity percentage of 74.89%. Conversely, the QUAC dataset also performs poorly in terms of similarity, with a low similarity percentage of 60.28%.\nNext, we analyze the differences in the human metric across different datasets. The proportions of reliable evaluations vary sig-nificantly in the \"human score\" metric. The lowest proportion is 0.42% for DuReader-master, while the highest is 32.79% for SQUAD. Similarly, the proportions of unreliable evaluations differ, with the lowest being 0.49% for SQUAD and the highest being 17.16% for QUAC. Additionally, the proportion of ambiguous evaluations is highest for newsQA at 96.38% and lowest for QUAC at 66.71%.\nAfterwards, we analyze the differences in the machine metric across different datasets. In terms of \"accuracy metrics\", the QUAC dataset performs the worst, with a high score percentage of only 4.54%. The high score percentages for other datasets range between 4.54% and 30.8%, with a median around 20%. In terms of \"overlap metrics\", the QUAC dataset also performs poorly in terms of low overlap, with a low score percentage of 87.52%. The low score per-centages for other datasets range from 32.47% to 75.28%, with no significant high scores observed overall. Regarding \"similarity met-rics\", DuReader, SQuAD, and MSMARCO perform well in terms of high similarity scores, with the highest scores being 95.89%, 94.71%, and 93.41% respectively. In contrast, newsQA and QUAC exhibit lower similarity scores, with the highest scores being 66.6% and 64.13% respectively. Notably, there are consistencies between the similarity scores in machine metrics and the similarity scores in LLM-assessment metrics. In \u201cdiversity metrics", "final score": "etric,"}, {"title": "3 DISCRIMINATOR", "content": "In this section, we introduce a novel and robust discriminator called RelD, which is designed to assess the reliability of answers gen-erated by LLMs. To ensure that RelD closely aligns with human evaluation, we employ an appropriate method to train RelD and make it fit the final score based on human evaluation. The process of constructing RelD is illustrated in Fig. 3."}, {"title": "3.1 REGRESSION TO MULTI-CLASS CLASSIFICATION", "content": "Initially, we employ a regression approach to train the discrimi-nator RelD in order to fit the final score and align with human evaluation. However, our experiments reveal that the regression approach performs poorly, possibly due to the use of the mean square error loss function. Consequently, we convert the regression task into a classification task to improve the fitting. Specifically, In this process, we normalize the final score into different numbers of classes, such as four, six, eight, and ten, for multi-class classification. For instance, we assign the first category in a four-category classi-fication to final scores ranging from 0 to 0.25. After experiments as shown in Sec. 4.3, we ultimately choose a ten-class classification approach. The theoretical foundation of this method mainly lies in information theory and the cross-entropy loss function. Cross-entropy is a common information theory measure used to quantify the distance between two probability distributions. In the case of multi-classification problems, the cross-entropy loss function is defined as follows:\n$$L = - \\sum(y_i.\\log(p_i)),$$\nwhere $y_i$ represents the true label of the i-th category, and $p_i$ represents the predicted probability of the i-th category by the discrimi-nator RelD. Our objective is to minimize this loss function during the training of RelD. In practice, we employ the softmax function to convert the original output of RelD into a probability distribution.\nOne potential advantage of this method is that the classifica-tion task, which focuses on distinguishing different categories, may facilitate capturing subtle differences among the final scores. Fur-thermore, the cross-entropy loss function exhibits greater stability compared to the mean square error loss function when dealing with imbalanced datasets. However, it is important to note that in certain situations, multi-class tasks may introduce overly complex informa-tion, leading to a notable disparity between the concepts learned by the discriminator and human intuitive perception. For example, di-viding a problem into five categories, such as \"not reliable\", \"weakly reliable\", \"moderately reliable\", \"strongly reliable\" and \"highly re-liable\", may surpass most people's intuitive understanding of the fundamental categories of \"reliable\" and \"unreliable\"."}, {"title": "3.2 MULTI-CLASS TO BINARY-CLASS CLASSIFICATION", "content": "Based on the aforementioned analysis, we further convert the multi-class task into a binary classification task, which may better align with human intuitive perception. Here, we present three possible approaches for this conversion, each with its theoretical support and definition:\nNormalization. This method is based on threshold decision theory. It involves converting all class information into binary labels by directly normalizing the final score to 0 and 1, which serves as the final probability value for classification. However, this approach may result in some information loss as continuous scores are transformed into discrete classes.\nDiscrete Values. This method is grounded in maximum likeli-hood estimation, a commonly used parameter estimation technique in statistics. Here, we consider the highest predicted probability from the discriminator as the final probability value for classifi-cation. For example, in a four-class classification scenario, if the probabilities corresponding to the classes are 0.1, 0.1, 0.1, and 0.7, respectively, we would use 0.7 as the final probability value. The advantage of this method lies in its simplicity, although the draw-back is that we do not know which class the maximum probability value corresponds to.\nWeighted Average Probability. The theoretical basis for this method stems from decision theory, particularly the concept of expected utility, which involves taking a weighted average of all possible outcomes and their corresponding utilities (in this case, predicted probabilities). The goal of this approach is to determine a weighted average value that best represents the predicted prob-abilities for each class from the discriminator. In this method, we multiply the probability of each class predicted by the discrimi-nator with its corresponding weight, summing them up to obtain a final probability value. This value can then be used for binary classification tasks. The formula for this method is as follows:\n$$P_i = \\frac{(\\sum (w_i p_i)) - w_{min}}{w_{max} - w_{min}},$$\nwhere $p_i$ represents the probability output of the discriminator for class i, $w_i$ denotes the weight for class i, and $w_{min}$ and $w_{max}$ are the minimum and maximum weights, respectively. We set the threshold to 0.5 and use the cross-entropy loss function for approximation. It allows for a more refined fitting of regression tasks and has demonstrated better performance compared to the previous two methods, as indicated by Sec. 4.3."}, {"title": "3.3 Backbone of the Discriminator", "content": "We utilize a Pre-trained Language Model (PLM), such as ELEC-TRA [19], as the backbone of the discriminator RelD. Through our experiments, we have demonstrated that ELECTRA outperforms other PLMs, including BERT [21], ROBERTa [50], and DeBERTa [29], as indicated in Section 4.3. RelD takes questions along with contexts and LLMs' generated answers as input, generating a classification label to determine the reliability of a generated answer. It uses the weighted average probability approach to fit the ground-truth answers."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct experiments to evaluate the effectiveness of RelD in detecting the reliability of LLMs' generated answers using both automatic metrics and human-in-the-loop metrics."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "The experiments are conducted using TESLA A100 GPUs for answer generation and GTX 3090 GPUs for training RelD with PyTorch in Python. During the training of RelD, we set the batch size to 32 and the sequence length to 128. Hyperparameters such as weight decay (0.01), \u03b21 (0.9), and \u1e9e2 (0.999) are maintained. The learning rate is set to 2e-05. We train RelD for 20 epochs.\nBaselines and metrics. We validate the effectiveness of the proposed RelD on well-known LLMs, including LLaMA (LLaMA-7B)[76], BLOOM (BLOOM-7B)[70], GPT-J (GPT-J-6B)[79], GPT-3[6], and GPT-3.5 1. To evaluate the performance of RelD, we use accuracy (ACC) as the automatic metrics and ROC curve analysis with the area under the ROC curve (AUC) as the human-in-the-loop metrics. The automatic evaluation process utilizes the final tag as the ground-truth label, while the human-in-the-loop evaluation involves human ratings as the ground-truth labels. Specifically, we randomly select 9,000 QA pairs, with 1,000 from each dataset in RelQA, for human ratings. We enroll nine volunteers and di-vide them into three groups to ensure evaluation stability. Each group provides scores of 0 or 1 for the randomly selected 3,000 QA pairs. Inter-rater agreement is calculated using Krippendorff's Alpha (IRA) to ensure the confidence of the human ratings. For controversial ratings with low agreement (<0.7), we discard the corresponding QA pair and replace it with another."}, {"title": "4.2 MAIN RESULTS", "content": "We conduct experiments to evaluate the effectiveness of the pro-posed RelD as follows:\nExperiment 1: RelD's Performance across Different LLMs. We conduct ten-fold cross-validation and report the average per-formance on the validation dataset. Based on the results presented in Table 6, it's observed that both the automatic and human-in-the-loop evaluations consistently exceed 0.8 for all LLMs, with minimal variation between different models (p<0.01). The strong correlation between the automatic and human-in-the-loop evaluations (p<0.01) suggests that the automatic scoring of the RelQA dataset could largely replace human scoring. It also indicates the robustness of RelD in detecting the reliability of different LLMs.\nExperiment 2: RelD's Performance on IID and OOD Datasets We evaluate the performance of RelD on both In-distribution (IID) and Out-of-distribution (OOD) datasets. We randomly assign nine datasets from RelQA to the IID and OOD sets in various ratios, such as 1:8, 2:7, 3:6, and 4:5, and vice versa. For example, we train on 8"}, {"title": "4.3 ABLATION STUDY", "content": "After that, we conduct several experiments to evaluate the effec-tiveness of different modules in the proposed RelD. All results are performed on the validation dataset using ten-fold cross-validation.\nExperiment 3: Effectiveness of Weighted Average Proba-bility. We compare the performance of using normalization, dis-crete values, and weighted average probability in the conversion from multi-class to binary-class classification in both automatic and human-in-the-loop metrics. The results are presented in Fig. 6. We observe that while using weighted average probability slightly underperforms normalization in terms of automatic metrics, it sig-nificantly outperforms normalization and discrete values in human-in-the-loop metrics across all LLMs. Therefore, we adopt weighted average probability as it offers a more intuitive and aligned ap-proach from a human perspective.\nExperiment 4: Optimal Number of Categories. We inves-tigate the impact of the number of categories when converting regression into multi-class classification. We test four categories, six categories, eight categories, and ten categories. The results are shown in Fig. 7. It is evident that a higher number of categories leads to improved performance in human-in-the-loop metrics. This suggests that a larger number of categories brings the classifica-tion task closer to regression and enhances alignment with human cognition. Consequently, we ultimately convert the regression task into a ten-category classification task and then discern it as a binary classification using weighted average probability.\nExperiment 5: Optimizing Weights of Each Metric. Relying solely on prior knowledge to determine the weights of each metric may not achieve the best performance. Therefore, we explore the optimal weights for each metric. To achieve this, we calculate the optimal weight for each metric as the weighted average of two values: the AUC when each metric is treated as the ground-truth compared to human evaluation, and the Pearson coefficient between each metric and human evaluation. In our experiment, we set the ratio for the former as 0.9 and for the latter as 0.1, as it yields the best performance. Subsequently, we evaluate whether the optimal weights can enhance the performance of RelD in detecting hallucination of LLMs' generated answers . Remarkably, we observe improvements in both automatic (b) and human-in-the-loop metrics (c) after optimizing the weights of each metric.\nExperiment 6: Backbone Selection for RelD. We experi-ment with different PLMs, including BERT [21], ROBERTa [50], DeBERTa [29], and ELECTRA [19], for RelD in order to choose the most effective backbone. Through this com-parison, we observe that ELECTRA achieves the best performance in both automatic and human-in-the-loop metrics. Consequently, we select ELECTRA as the preferred backbone for RelD."}, {"title": "4.4 EXPLORATORY ANALYSIS", "content": "We classify the predictions generated by RelD into four categories, as presented in Table 9. To gain insights into the characteristics of these categories and understand the functioning of RelD, we conduct an exploratory analysis.\nAnalysis 1: Distribution Analysis To analyze the distributions within each category, we utilize boxplots to illustrate key statistics such as median, quartiles, and outliers of samples. Additionally, we employ density plots to visualize the probability distribution of samples within each category. In the first category, the boxplot exhibits a wide range and the density plot shows a concentrated distribution with multiple peaks. This suggests that RelD may have some uncertainties in its predictions for this category. For the second and third categories, the boxplot widths fall between those of the first and fourth categories and the density plots display more dispersed probability distributions. This indicates that RelD is more hesitant in its predictions or has lower proficiency in learning for these types of questions. In contrast, the fourth category exhibits a narrower boxplot and the density plot shows a concentrated probability distribution. It indicates that RelD is more confident in its predictions for this category.\nAnalysis 2: Clustering Analysis. By applying clustering al-gorithms to the text data, we investigate whether each category exhibits distinct cluster centers, as illustrated in Fig. 10. For the first category, the data distribution appears clustered and relatively uniform, indicating consistent and accurate performance by RelD within this category. The second category contains an extremely small number of samples, suggesting that RelD rarely misclassifies the correct answers generated by the LLMs. In the third category, the clustering results reveal significant variability, indicating that errors can occur in various aspects when RelD misclassifies the incorrect answer as correct, such as grammar or comprehension errors. Similarly, the fourth category displays a wide and dispersed clustering distribution, indicating diverse performance by RelD within this category. This suggests the presence of different types of errors that make it challenging for RelD to detect. From the clustering graph, we observe that RelD performs best in the first category. However, for the second, third, and fourth categories, the performance of RelD may be influenced by the complexity and ambiguity of the input contexts or questions.\nAnalysis 3: Vocabulary Distribution. We can compare the vocabulary distribution between correctly predicted samples and incorrectly predicted samples by RelD, as depicted in Fig. 11. There is a noticeable distinction between the left side (RelD predicts cor-rectly) and the right side (RelD predicts incorrectly). It appears that content related to \u201cstory\u201d is relatively easy for RelD to classify cor-rectly, while content related to \"country\" poses more difficulty for RelD in accurate classification. However, it is important to note that vocabulary alone may not be the sole determining factor for RelD's recognition accuracy. The critical factors might involve underlying semantic relationships, which would necessitate further research and investigation."}, {"title": "5 RELATED WORK", "content": "Hallucination detection. Existing research primarily contains statistical metrics [28, 74, 80], model-based metrics (including Infor-mation Extraction (IE)-based metric, QA-based metric [32, 65, 68], Natural Language Inference (NLI) Metrics [33, 38, 81], Faithfulness Classification Metrics [32, 48, 89], LM-based Metrics [26, 75]), and human-based evaluations [69, 73]. We list some typical work as follows: Dhingra et al. [22] propose PARENT to measure hallucina-tions using both the source and target text as references. Goyal and Durrett [27] attempt to identify factual inconsistencies in a more fine-grained manner with a new dependency-level entailment. Liu et al. [48] and Zhou et al. [89] construct syntactic data by automati-cally inserting hallucinations into training instances. Chen et al. [8] and Nie et al. [56] use finer-grained metrics for intrinsic hallucina-tion and extrinsic hallucination separately. Azaria et al. [2] utilize the internal state and hidden layer activations of LLMs to detect the truthfulness of generated statements. Ye et al. [86] consider that errors in user-generated query input may cause unexpected responses from LLMs.\nHallucination mitigation. There are also some work that focus on mitigating hallucination. For example, Dale et al. [20] and Ji et al. [34] focus on hallucination in machine translation. Pagnoni et al. [58] address hallucination in text summarization. Peng et al. [61] adopt various methods to prompt LLMs, including posting multiple queries. Ouyang et al. [57] propose a method to enhance the content generated by LLMs. Yan et al. [83] introduce an iterative self-evaluating optimization mechanism based on prompt engineering. Park et al. [60] leverage search results corresponding to a user's input query to generate an augmented query."}, {"title": "6 CONCLUSIONS AND FUTURE WORK", "content": "Hallucination of LLMs poses a significant challenge. In this paper, we address this issue by proposing a robust discriminator, RelD, trained on the constructed RelQA dataset, which is a bilingual question-answering dialogue dataset along with generated answers by LLMs and a comprehensive set of metrics to effectively detect hal-lucinations in LLMs' generated answers. Our experimental results demonstrate the effectiveness of RelD in detecting hallucinations in LLMs' generated answers. Moreover, RelD exhibits strong robust-ness and generalization capabilities, performing well on both in-distribution and out-of-distribution datasets. These findings make a significant contribution to the detection of reliable answers gen-erated by LLMs and hold promising implications for future work in mitigating hallucination."}]}