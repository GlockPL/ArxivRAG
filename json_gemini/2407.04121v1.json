{"title": "Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models", "authors": ["Yuyan Chen", "Qiang Fu", "Yichen Yuan", "Zhihao Wen", "Ge Fan", "Dayiheng Liu", "Dongmei Zhang", "Zhixu Li", "Yanghua Xiao"], "abstract": "Large Language Models (LLMs) have gained widespread adoption in various natural language processing tasks, including question answering and dialogue systems. However, a major drawback of LLMs is the issue of hallucination, where they generate unfaithful or inconsistent content that deviates from the input source, leading to severe consequences. In this paper, we propose a robust discriminator named RelD to effectively detect hallucination in LLMs' generated answers. RelD is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics. Our experimental results demonstrate that the proposed RelD successfully detects hallucination in the answers generated by diverse LLMs. Moreover, it performs well in distinguishing hallucination in LLMs' generated answers from both in-distribution and out-of-distribution datasets. Additionally, we also conduct a thorough analysis of the types of hallucinations that occur and present valuable insights. This research significantly contributes to the detection of reliable answers generated by LLMs and holds noteworthy implications for mitigating hallucination in the future work.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have revolutionized various fields [9, 40, 88], including logical reasoning [3, 47], question answering [12, 14, 84], text generation [11, 13, 90], and vertical domains [16, 49, 82]. However, LLMs encounter numerous challenges that hinder their optimal performance. These challenges include the inability to up-date knowledge in real-time [15], the lack of genuine emotion and thought [7, 10], and the generation of long-winded and verbose an-swers [35], among others. Notably, one of the most critical failures is the presence of factual errors in the generated text [5], which gives rise to \"Hallucinations\u201d as depicted in Fig 1. The existence of such \"Hallucinations\" poses a severe hindrance to the widespread adoption of LLMs in non-chatbot scenarios, particularly in domains"}, {"title": "2 DATA CONSTRUCTION", "content": "In this section, we present the process of constructing RelQA. We begin by using questions from various existing nine datasets as inputs to different LLMs to generate corresponding answers. Next, we design a comprehensive set of metrics to evaluate the reliabil-ity of these generated answers. The combined collection of the original nine datasets, the generated answers by LLMs, and the"}, {"title": "2.1 DATA COLLECTION", "content": "RelQA consists of nine sub-datasets: SQUAD [63], DuReader [31], HotpotQA [85], MSMARCO [55], NewsQA [77], QuAC [18], CoQA [66], TriviaQA-Web [36], and TriviaQA-Wikipedia [36]. The detailed col-lecting steps are as follows:\nStep 1 (Dataset Selection): These datasets are selected due to their unique characteristics, diverse sources, and the enrichment they bring to the overall collection. They cover extractive read-ing comprehension (ERC), multiple-choice (MC), and multi-turn dialogues (MTD) categories. They originate from sources such as Wikipedia, Baidu Zhidao, Bing search, and other platforms, while encompassing domains such as student education, news, web arti-cles, and general knowledge.\nStep 2 (Formatting and Integration): To ensure compatibil-ity and remove dataset boundaries, we perform formatting and integration for all selected datasets based on the aforementioned categories. Each dataset follows a specific standardized format, as illustrated in Table 1 (the second column). We represent the datasets of all categories as {Li, Di}, where Li denotes a specific dataset and Di denotes its standardized format.\nStep 3 (Preprocessing): To facilitate effective processing and generation of answers, we employ preprocessing techniques on the dataset. This involves two primary aspects: personalized prompt instruction design and addressing the limitations associated with long texts. For personalized prompt instruction design, we create question-adaptive prompt instructions for each question based on the question type, as shown in Table 1 (the third column). These prompt instructions guide LLMs in generating better answers that align with different types of questions. To address the challenge of long texts, we implement a sliding window approach [37], seg-menting the texts into smaller windows, each containing 4,000 tokens. This ensures that LLMs receive clear prompt instructions and can effectively handle texts of varying lengths, resulting in more accurate and contextually appropriate answers.\nStep 4 (Answer Generation): We employ several powerful LLMs, including LLaMA [76], BLOOM [70], GPT-J [79], GPT-3 [6], and GPT-3.5 1, to generate answers for evaluation. In the case of longer texts, we slide the window over the text and generate out-puts for each window. The generated outputs for each window are stored to facilitate subsequent filtering and selection of the optimal answers. To maintain answer stability, we ask an LLM to generate the answer three times for each question and select the majority answer as the final answer. Furthermore, to ensure the overall quality and reliability of the generated answers, we con-duct quality assurance procedures, including automated checks to identify and re-generate incomplete sentences by detecting missing sentence-ending punctuation, among others."}, {"title": "2.2 METRIC SELECTION", "content": "To evaluate the reliability of LLMs' generated answers, it is crucial to select appropriate metrics that capture different aspects of answer quality. We employ four types of metrics, including LLM-assessment"}, {"title": "2.3 DATA EXPLORATORY ANALYSIS", "content": "In this section, we conduct a data exploratory analysis of the con-structed RelQA dataset, which comprises a total of 1,372,130 sam-ples, including generated answers by five selected LLMs. Among these, 743,910 samples are assigned as reliable and 628,220 samples as unreliable based on the final tag metric. We divide the possi-ble ranges of all metrics into three equal parts, representing low, medium, and high levels. Fig 2 illustrates the distribution of each dataset at the high level for each metric. We also present the dis-tributions of different datasets among various metrics as shown in"}, {"title": "3 DISCRIMINATOR", "content": "In this section, we introduce a novel and robust discriminator called RelD, which is designed to assess the reliability of answers gen-erated by LLMs. To ensure that RelD closely aligns with human evaluation, we employ an appropriate method to train RelD and make it fit the final score based on human evaluation. The process of constructing RelD is illustrated in Fig. 3."}, {"title": "3.1 REGRESSION TO MULTI-CLASS CLASSIFICATION", "content": "Initially, we employ a regression approach to train the discrimi-nator RelD in order to fit the final score and align with human evaluation. However, our experiments reveal that the regression approach performs poorly, possibly due to the use of the mean square error loss function. Consequently, we convert the regression task into a classification task to improve the fitting. Specifically, In this process, we normalize the final score into different numbers of classes, such as four, six, eight, and ten, for multi-class classification. For instance, we assign the first category in a four-category classi-fication to final scores ranging from 0 to 0.25. After experiments as"}, {"title": "3.2 MULTI-CLASS TO BINARY-CLASS CLASSIFICATION", "content": "Based on the aforementioned analysis, we further convert the multi-class task into a binary classification task, which may better align with human intuitive perception. Here, we present three possible approaches for this conversion, each with its theoretical support and definition:\nNormalization. This method is based on threshold decision theory. It involves converting all class information into binary labels by directly normalizing the final score to 0 and 1, which serves as the final probability value for classification. However, this approach may result in some information loss as continuous scores are transformed into discrete classes.\nDiscrete Values. This method is grounded in maximum likeli-hood estimation, a commonly used parameter estimation technique in statistics. Here, we consider the highest predicted probability from the discriminator as the final probability value for classifi-cation. For example, in a four-class classification scenario, if the probabilities corresponding to the classes are 0.1, 0.1, 0.1, and 0.7, respectively, we would use 0.7 as the final probability value. The advantage of this method lies in its simplicity, although the draw-back is that we do not know which class the maximum probability value corresponds to.\nWeighted Average Probability. The theoretical basis for this method stems from decision theory, particularly the concept of expected utility, which involves taking a weighted average of all possible outcomes and their corresponding utilities (in this case, predicted probabilities). The goal of this approach is to determine a weighted average value that best represents the predicted prob-abilities for each class from the discriminator. In this method, we multiply the probability of each class predicted by the discrimi-nator with its corresponding weight, summing them up to obtain a final probability value. This value can then be used for binary classification tasks. The formula for this method is as follows:\n$P_i = \\frac{(\\sum{w_ip_i}) - w_{min}}{w_{max} - w_{min}}$\nwhere $p_i$ represents the probability output of the discriminator for class i, $w_i$ denotes the weight for class i, and $w_{min}$ and $w_{max}$ are the minimum and maximum weights, respectively. We set the threshold to 0.5 and use the cross-entropy loss function for approximation. It allows for a more refined fitting of regression tasks and has demonstrated better performance compared to the previous two methods, as indicated by Sec. 4.3."}, {"title": "3.3 Backbone of the Discriminator", "content": "We utilize a Pre-trained Language Model (PLM), such as ELEC-TRA [19], as the backbone of the discriminator RelD. Through our experiments, we have demonstrated that ELECTRA outperforms other PLMs, including BERT [21], ROBERTa [50], and DeBERTa [29], as indicated in Section 4.3. RelD takes questions along with contexts and LLMs' generated answers as input, generating a classification label to determine the reliability of a generated answer. It uses the weighted average probability approach to fit the ground-truth answers."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct experiments to evaluate the effectiveness of RelD in detecting the reliability of LLMs' generated answers using both automatic metrics and human-in-the-loop metrics."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "The experiments are conducted using TESLA A100 GPUs for answer generation and GTX 3090 GPUs for training RelD with PyTorch in Python. During the training of RelD, we set the batch size to 32 and the sequence length to 128. Hyperparameters such as weight decay (0.01), \u03b21 (0.9), and \u1e9e2 (0.999) are maintained. The learning rate is set to 2e-05. We train RelD for 20 epochs.\nBaselines and metrics. We validate the effectiveness of the proposed RelD on well-known LLMs, including LLaMA (LLaMA-7B)[76], BLOOM (BLOOM-7B)[70], GPT-J (GPT-J-6B)[79], GPT-3[6], and GPT-3.5 1. To evaluate the performance of RelD, we use accu-racy (ACC) as the automatic metrics and ROC curve analysis with the area under the ROC curve (AUC) as the human-in-the-loop metrics. The automatic evaluation process utilizes the final tag as the ground-truth label, while the human-in-the-loop evaluation involves human ratings as the ground-truth labels. Specifically, we randomly select 9,000 QA pairs, with 1,000 from each dataset in RelQA, for human ratings. We enroll nine volunteers and di-vide them into three groups to ensure evaluation stability. Each group provides scores of 0 or 1 for the randomly selected 3,000 QA pairs. Inter-rater agreement is calculated using Krippendorff's Alpha (IRA) to ensure the confidence of the human ratings. For controversial ratings with low agreement (<0.7), we discard the corresponding QA pair and replace it with another."}, {"title": "4.2 MAIN RESULTS", "content": "We conduct experiments to evaluate the effectiveness of the pro-posed RelD as follows:\nExperiment 1: RelD's Performance across Different LLMs.\nWe conduct ten-fold cross-validation and report the average per-formance on the validation dataset. Based on the results presented in Table 6, it's observed that both the automatic and human-in-the-loop evaluations consistently exceed 0.8 for all LLMs, with minimal variation between different models (p<0.01). The strong correlation between the automatic and human-in-the-loop evaluations (p<0.01) suggests that the automatic scoring of the RelQA dataset could largely replace human scoring. It also indicates the robustness of RelD in detecting the reliability of different LLMs.\nExperiment 2: RelD's Performance on IID and OOD Datasets\nWe evaluate the performance of RelD on both In-distribution (IID) and Out-of-distribution (OOD) datasets. We randomly assign nine datasets from RelQA to the IID and OOD sets in various ratios, such as 1:8, 2:7, 3:6, and 4:5, and vice versa. For example, we train on 8"}, {"title": "4.3 ABLATION STUDY", "content": "After that, we conduct several experiments to evaluate the effec-tiveness of different modules in the proposed RelD. All results are performed on the validation dataset using ten-fold cross-validation.\nExperiment 3: Effectiveness of Weighted Average Proba-bility. We compare the performance of using normalization, dis-crete values, and weighted average probability in the conversion from multi-class to binary-class classification in both automatic and human-in-the-loop metrics. The results are presented in Fig. 6. We observe that while using weighted average probability slightly underperforms normalization in terms of automatic metrics, it sig-nificantly outperforms normalization and discrete values in human-in-the-loop metrics across all LLMs. Therefore, we adopt weighted average probability as it offers a more intuitive and aligned ap-proach from a human perspective.\nExperiment 4: Optimal Number of Categories. We inves-tigate the impact of the number of categories when converting regression into multi-class classification. We test four categories, six categories, eight categories, and ten categories. The results are"}, {"title": "4.4 EXPLORATORY ANALYSIS", "content": "We classify the predictions generated by RelD into four categories, as presented in Table 9. To gain insights into the characteristics"}, {"title": "5 RELATED WORK", "content": "Hallucination detection. Existing research primarily contains statistical metrics [28, 74, 80], model-based metrics (including Infor-mation Extraction (IE)-based metric, QA-based metric [32, 65, 68], Natural Language Inference (NLI) Metrics [33, 38, 81], Faithfulness Classification Metrics [32, 48, 89], LM-based Metrics [26, 75]), and human-based evaluations [69, 73]. We list some typical work as follows: Dhingra et al. [22] propose PARENT to measure hallucina-tions using both the source and target text as references. Goyal and Durrett [27] attempt to identify factual inconsistencies in a more fine-grained manner with a new dependency-level entailment. Liu et al. [48] and Zhou et al. [89] construct syntactic data by automati-cally inserting hallucinations into training instances. Chen et al. [8] and Nie et al. [56] use finer-grained metrics for intrinsic hallucina-tion and extrinsic hallucination separately. Azaria et al. [2] utilize the internal state and hidden layer activations of LLMs to detect the truthfulness of generated statements. Ye et al. [86] consider that errors in user-generated query input may cause unexpected responses from LLMs.\nHallucination mitigation. There are also some work that focus on mitigating hallucination. For example, Dale et al. [20] and Ji et al. [34] focus on hallucination in machine translation. Pagnoni et al. [58] address hallucination in text summarization. Peng et al. [61] adopt various methods to prompt LLMs, including posting multiple queries. Ouyang et al. [57] propose a method to enhance the content generated by LLMs. Yan et al. [83] introduce an iterative self-evaluating optimization mechanism based on prompt engineering. Park et al. [60] leverage search results corresponding to a user's input query to generate an augmented query."}, {"title": "6 CONCLUSIONS AND FUTURE WORK", "content": "Hallucination of LLMs poses a significant challenge. In this paper, we address this issue by proposing a robust discriminator, RelD, trained on the constructed RelQA dataset, which is a bilingual question-answering dialogue dataset along with generated answers by LLMs and a comprehensive set of metrics to effectively detect hal-lucinations in LLMs' generated answers. Our experimental results demonstrate the effectiveness of RelD in detecting hallucinations in LLMs' generated answers. Moreover, RelD exhibits strong robust-ness and generalization capabilities, performing well on both in-distribution and out-of-distribution datasets. These findings make a significant contribution to the detection of reliable answers gen-erated by LLMs and hold promising implications for future work in mitigating hallucination."}]}