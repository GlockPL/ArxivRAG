{"title": "R-AIF: SOLVING SPARSE-REWARD ROBOTIC TASKS FROM\nPIXELS WITH ACTIVE INFERENCE AND WORLD MODELS", "authors": ["Viet Dung Nguyen", "Zhizhuo Yang", "Christopher L. Buckley", "Alexander Ororbia"], "abstract": "Although research has produced promising results demonstrating the utility of active inference\n(AIF) in Markov decision processes (MDPs), there is relatively less work that builds AIF models\nin the context of environments and problems that take the form of partially observable Markov\ndecision processes (POMDPs). In POMDP scenarios, the agent must infer the unobserved\nenvironmental state from raw sensory observations, e.g., pixels in an image. Additionally,\nless work exists in examining the most difficult form of POMDP-centered control: continuous\naction space POMDPs under sparse reward signals. In this work, we address issues facing the\nAIF modeling paradigm by introducing novel prior preference learning techniques and self-\nrevision schedules to help the agent excel in sparse-reward, continuous action, goal-based\nrobotic control POMDP environments. Empirically, we show that our agents offer improved\nperformance over state-of-the-art models in terms of cumulative rewards, relative stability, and\nsuccess rate. The code in support of this work can be found at https://github.com/NACLab/\nrobust-active-inference.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has notably been widely utilized in robotic systems to solve a variety of manipulation\nand control tasks [67, 35, 51] using model-free RL algorithms such as the soft actor critic [28, 29] or the deep\nQ-network [49]. Model-based RL, on the other hand, predicts the dynamics of a Markov decision process (MDP)\nand utilizes this learned generative model to plan a useful policy. From the 'Dyna' framework [76], model-based\nRL has evolved into powerful modern-day models, including latent dynamics models [6], imagination-augmented\nRL [65], or recurrent-based state dynamics models capable of playing Atari games [89]. Within this domain of\nresearch, there exists a sub-field \u2013 formally known as active inference \u2013 that develops models that align with\nunderlying principles of model-based RL. Generally, an active inference agent maintains and adapts a best estimate\nof its world (generative world model). It further aims to take actions that lead to outcomes that are aligned with\nits 'preferences' while working to predict and minimize the degree of surprise it would potentially encounter in\nfuture engagements within its niche [61, 15]. However, current active inference research only tackles MDP robotics\nproblems, with far less consideration for pixel-based POMDP tasks that elicit sparse reward signals; see Table 1.\nIn this work, we make the following key contributions. 1) We propose a novel contrastive recurrent state prior\npreference (CRSPP) model, which allows the agent to learn its own preference over the world's state(s) online.\nThis online preference dynamically shapes the agent's policy distribution, improving general performance. 2)\nWe propose a new formulation of expected free energy and optimize it using the actor-critic method, improving\nthe stability of the action planner compared to other active inference baselines. 3) We propose our robust active"}, {"title": "2 Related Work", "content": "Model-based reinforcement learning (MBRL) is a pivotal approach in reinforcement learning that centers around\nthe 'world model', a concept that involves creating an internal model of the environment to guide the agent's\nfuture actions. This approach is exemplified in works related to state space models [7], \u2018embed to control' [85],\n'Plan2Explore' [73], 'dreaming' [54], 'PlaNet' [31], 'Dreamer' [30], divergence minimization [33], and perceptual\nuncertainty [62, 74].\nActive inference (AIF) is a framework in cognitive science and neuroscience which centers around the notion of\ngenerative models which \"understand\u201d a \u201clived world\" [61, 64] or (eco)niche. Importantly, AIF itself is in effect\na corollary of the free energy principle, which posits that biological systems minimize a quantity known as free\nenergy as they continuously work to preserve their existence (self-evidence) and interact with their environments\neffectively. Formally, AIF involves minimizing the quantity known as variational free energy which is formally\ndefined as follows:\n$F = E_{Q(s)} [lnQ(s) \u2013 ln P(o, s)]$\nwhere $Q(s)$ is the approximate posterior over states s and $P(0, s)$ is the joint probability of states s and observations\n0. This framework extends to machine intelligence since it casts the perception and planning problem as a Bayesian\ninference problem where an agent updates its beliefs about the sensory inputs and selects actions that lead to the\nminimization of expected free energy (EFE):\n$G(\u03c0) = E_{Q(s)} [ln Q(s, 0|\u03c0) \u2013 ln Q(0, s, 0|\u03c0)]$\nwhere $\\theta$ contains the model parameters, $\\pi$ is the planned action distribution, $Q(s, 0|\u03c0)$ is the approximate posterior\nover state and model parameters given the agent's actions, and $Q(0, s, \u03b8|\u03c0)$ is the approximate posterior over\nobservation, state, and model parameters given the agent's actions. The EFE is often broken down in two key\nterms: instrumental and epistemic. The instrumental term defines how future estimated states/observations align\nwith the agent's prior preference (interest) given its current action plan, whereas the epistemic term describes the\nsurprise/uncertainty level associated with the estimated future states under a given planned policy. Generally, this\nAIF framing offers a principled basis for RL models that learn and act by reducing the mismatch between predicted\nand observed data. Deep active inference seeks to utilize the tools of deep learning, e.g,. deep neural networks and"}, {"title": "3 Solving Sparse-Reward Pixel Robotic Tasks", "content": "In order to tackle a robotic task with a continuous action space, varied goals, and sparse reward signals, we first\nre-formulate the construction of the AIF/MBRL agent's generative world model [27, 18] in Section 3.1 (while\nSection 3.2 introduces our novel contrastive recurrent state prior preference model). We propose the robust active\ninference\u00b9 (R-AIF) agent in Section 3.3, which utilizes actor-critic methodology [77] in order to optimize the action\n'planner', ultimately seeking to minimize both instrumental and epistemic signals from its learned world model.\nOur agent operates on standard POMDPs, in discrete time, where the interaction between the agent and the\nenvironment is formally expressed as $M = (S, A, O, p, E, r, g)$. $S$ is the set of all environment states (hidden\nfrom the agent), $O$ is the set of observations, $E(o_t|s_t)$ is an emission function which produces the observable\nsignal $o_t$ conditioned on the unobserved state distribution $s_t$. Additionally, we have the action space $A$, the reward\nfunction $r : S \u00d7 A \u2192 R$, and the transition probability $p(s_{t+1}|s_t, a_t)$ [77]. Finally, we also consider the function\n$g: S\u2192 \\{0, 1\\}$ indicating whether the agent has achieved its goal (or not) in a particular time step."}, {"title": "3.1 The Generative World Model", "content": "Active inference posits that an agent finds an action sequence based on an estimated future state distribution [22,\n75, 21, 86, 16, 48]. To achieve this, we predict the next state $s_{t+1}$ given the current state $s_t$ and action $a_t$, along\nwith a recurrent state $h_t$ that serves as temporal memory ($z_t$ serving as the memory's output). For modeling $h_t$, we\nemploy a gated recurrent unit [11] following the approach from [52, 30, 32, 34]. In general, this framing of the\ntemporal integration of information is referred to as the recurrent state space model (RSSM) [31], which builds on\nconcepts from the state space model literature [7, 27, 38, 13]. The RSSM formulation can formally be expressed as"}, {"title": "3.2 Contrastive Recurrent State Prior Preference (CRSPP)", "content": "In active inference, the agent takes the actions that it believes would lead to its preferred outcomes (i.e. using\nthe instrumental signal) [68, 48, 23, 22, 21, 18]. To construct this prior preference, past work has provided a\ngoal state/observation(s) directly to the agent [45] or manually crafted a prior preference distribution [18, 86].\nHowever, the first approach suffers from sparsity over the preference space while the second is impractical in\nmore realistic POMDPs. To tackle this, we leverage a small quantity of seed imitation data to learn an ANN that\ndynamically produces the preference over states at each time step; this effectively provides an easily-generated\ndense instrumental/goal signal. Concretely, we design the agent such that it moves according to a trajectory\nthat is shaped towards its own estimation of future preferred states, \u201cnudging\" its own trajectory toward the\nimitation/positive data distribution (see Figure 1).\nDynamic Prior Preference Model Formulation. In this work, we consider a prior preference model that takes\nin the image observation $o_t \u2208 O$ and produces a posterior estimate over state $s_t \u2208 S$. Based on this latent\nrepresentation, the model then estimates or \"imagines\" \u2013 the future latent representation that has a high preference\nvalue $s_{T:H}$ over a time horizon $H$. To achieve this, we construct an RSSM without action encoded into the transition\nprior. Our model can then be further parameterized with an encoder posterior $q_{\\phi}(s_t|o_t)$ and a transition prior\n$P(s_t|s_{t-1})$ (see Figure 2).\nGoal-Oriented Credit Assignment and Prior Self-Revision. Assuming that the trajectories collected throughout\nthe R-AIF agent's learning process form a set $E = \\{e_0, e_1,...\\}$, if we follow the conventions of contrastive\nlearning methodology [41, 36, 80], we partition this set of experiences into two portions based on the success\nstatus of each experience, i.e. $P(E) = (\\{e+\\}, \\{e-\\})$ where $\\{e_-\\}= E \\\\ \\{e+\\}$. Intuitively, we aim to learn a prior\npreference model which estimates the preferred state $\\check{s}$ that is closer to (positive) states $s \u2208 e+$ while pushing $\\check{s}$\naway from $s \u2208 e_ (negative states). Specifically, to learn a model that performs roll-outs over a finite horizon with\nonly preferred states, one can maximize the similarity of states between the prior preference model and the actual\ngenerative world model \u2013 where \u201creached goal states\u201d are of \u201cstrong interest\u201d (yielding a positive signal) \u2013 while\nminimizing this similarity measurement for the situations that the agent fails the task within an episode (yielding a"}, {"title": "3.3 R-AIF Agent Behavioral Learning", "content": "In active inference, the agent estimates both future states and observations and then plans action sequences based\non the expected free energy computed from these future 'imagined realities' [24, 19, 18, 14]. Although one\ncan estimate both states and observations with respect to 'imagination space', for practical model inference, it\nis also possible to roll-out only the latent dynamics into the future [31, 54, 66]; in this work, we roll out only\nlatent states. Formally, with the planning time step 7 and imagination horizon H, we estimate the future state\n$s_{\\tau}\u223c P_\\theta(s_T|s_{T\u22121}, a_{\u03c4\u22121})$ using the (estimated) action $a_{\\tau}\u223c \u03c0_\u03c8(a_\u03c4|s_\u03c4)$. We train the agent to minimize the expected\nfree energy based on these \u201cimagined\u201d future states. Similarly to [34], we construct a policy network $\u03c0_\u03c8(a_\u03c4|s_\u03c4)$"}, {"title": "4 Experimental Results", "content": "We compare the performance of our agent with relevant model-based RL and AIF baselines, namely: 1) Dream-\nerV3 [34], 2) our generalization of model in [47] (DAIVPG-G), and 3) the model in [82] (Himst-G). Additionally,\nwe change the architecture of the active inference agent of [82] by replacing the 3D-convolution (applied over\nfour stacked frames) with the state space model to make the agent operate properly in a POMDP environment\n(e.g., allowing it to process one image, instead of stacked frames, at each time-step, which we found improve the\nmodel's performance and overall stability). For each baseline agent and benchmark environment, we run each\nexperiment/simulation for 4 uniquely seeded trials, simulating each agent for 1, 3, and 5 million steps on the\nmountain car, Meta-World [87], and robosuite [88] environments, respectively. Agent performance is reported"}, {"title": "5 Conclusions", "content": "In this work, we crafted what we called the robust active inference (R-AIF) framework, where agents are engaged in\nthe dynamic, active perception, manipulating their environments while driven by our proposed contrastive recurrent\nstate prior preference. An R-AIF agent learns to take actions by utilizing a policy network that optimizes through a\ngeneralized advantage value estimated from the instrumental and epistemic signals derived from our expected free\nenergy objective. The instrumental (goal-orienting) signal is constructed from the (sparse) reward and the dynamics\nof the CRSPP model while the epistemic (exploration-driving) signal is computed from the information gain and\nstatistics of a generative world model and the policy network. Overall, we provide empirical results showing that\nour R-AIF achieves greater performance compared to other baselines: DreamerV3 [34], DAIVPG [47], and Himst's\nmodel [82]. Finally, our results also demonstrate that R-AIF agents can operate well in varied goal, sparse-reward\nPOMDP environments. Future work can consider improving the latent state space model with methods such as\nvariational dynamics, discrete-variable autoencoders, and attention-weighting models [4, 81, 10, 2, 84]. It would\nalso be useful to study the integration of R-AIF into physical (neuro)robotic systems, which would entail an\nembodied, enactive, and survival-oriented formulation of active perception and world model learning [56]. syleacm"}, {"title": "Appendix: Implementation Details Documentation", "content": "Temporal Information. In active inference, the hidden state inferred by the agent is often computed by a\nlikelihood matrix $R^{m\u00d7n}$ where m is the number of possible state values and n is the number of possible observation\nvalues [18]. A single observation from the environment can then be directly mapped to a state using this scheme.\nSimilarly, in the amortized inference context, the recognition density parameterized by an artificial neural network\n(ANN) is often used to estimate the posterior probability density over the hidden states of the environment [14].\nHowever, in the POMDP setting, an observation from a single time step would not provide sufficient information\nabout the state as is done in classic active inference literature with a likelihood matrix. For example, higher-order\ninformation, such as velocity and acceleration of particular variables, cannot be captured in one single image but\ninstead must be inferred from a sequence of images (or manually integrated [59]). Therefore, it is crucial for\nevery active inference model in POMDP environments to maintain temporal information or deterministic beliefs\nthroughout time as additional information is input into the generative model framework.\nSince active inference posits that an agent finds a policy, i.e, a sequence of actions, based on the estimated future\nstate distribution [22, 75, 21, 16, 48], one approach is to predict the next state $s_{t+1}$ given the current state $s_t$\nand action $a_t$. When operating in POMDP environments, this methodology involves predicting the next partial\nobservation $o_{t+1}$ given the previous partial observation $o_t$ and the action $a_t$. In order to integrate temporal\ninformation into this generative model, we can use the 'carried-over' recurrent state in some forms of RNN such as\na gated recurrent unit [11] as was done in [30, 32, 34, 52]. Therefore, the prior $p(s)$ and posterior $q(s)$ distributions\nover states are able to encapsulate the temporal information h embodied in previous observations ; see Figure 5 for\na visual representation of this process."}, {"title": "B Improving Numerical Stability", "content": "Minimizing the complexity (term) aids the agent in closing the gap between its prior and its approximate posterior\nwhereas minimizing the accuracy (term) improves the model's future observation estimation. We utilize the world\nmodel which has a discretized state space [32] where each hidden state is represented by a vector of discrete\ndistributions instead of a vector of Gaussian distribution parameters as is done in other deep active inference\nformulations. We also employ the KL balancing [32] and applying \"symlog\" function to inputs [34] for numerical\nstability:\n$DKL [q_{\\theta}(s_t|o_t) || p_{\\theta}(s_t|s_{t\u22121}, a_{t-1})] \u2190 N_{rep}DKL [q_{\\theta}(s_t|o_t) || Sg(p_{\\theta}(s_t|s_{t\u22121}, a_{t-1}))]$\n$+ N_{dyn}DKL [Sg(q_{\\theta}(s_t|o_t)) || p_{\\theta}(s_t|s_{t\u22121}, a_{t-1})]$\nwhere sg is the stop gradient operation, $N_{rep}$ and $n_{dyn}$ are the coefficients for representation and dynamics KL\nlosses, respectively. We also clip the KL term to a minimum value of free bits [30] and finally apply the symlog\nfunction [34] on its inputs for numerical stability."}, {"title": "C The prior preference self-revision mechanism", "content": "For each step in a trajectory e, we record the following statistics: 1) a boolean showing whether the step was carried\nout by an expert pt or not, 2) a boolean representing whether the agent immediately achieved the goal at a step dt,\nand 3) a boolean indicating whether the agent succeeded in reaching its goal at least one time within the episode k.\nFor each time step, we want to craft a decaying signal that emphasizes the degree of preference $p_t \u2208 [-1,1]$. In\norder to do so, we equip the agent with a self-revision mechanism which allows the agent to \u201clook back\u201d on how\nit performed and determine whether a certain state is preferred or not (see Algorithm 1). Note that we complete\nthe for-loop for positive samples and set the preference at its highest value at the state and decay this backward\nwhenever there is a successful state. In contrast, when the episode fails, the agent only needs to decay negatively\nbackward from the end of the episode."}, {"title": "D Computing the Information Gain using a Network Ensemble", "content": "Taking actions that reduce the uncertainty of model parameters requires estimating the uncertainty in the first place.\nOne can compute such a term from an explicit ANN ensemble or by using Monte Carlo dropout [25] to compute\ninformation gain [14]; however, as the world model grows in size/complexity, it becomes impractical to maintain a\ncollection of multiple world models or to sample from a large state distribution. Therefore, we instead construct a"}, {"title": "E Using Percentage Exponential Moving Average (PEMA)", "content": "We introduce and set the coefficient of the generative world model entropy and actor distribution entropy\n= 3 \u00d7 10-4 and perform percentile exponential moving average normalization as in [34]; these coefficients\ndepend on both the reward value [34] and model parameters, and therefore, given that they would be impractical\nto dynamically adjust, we choose to keep and fixed and small enough for numerical stability. As a result,\nnormalizing the return to the range between 0 and 1 can align with and range [34]. We then divide the difference\nbetween the return and the computed value by the range S as in [34]\n$PEMA(G_\u03c4, f_x(v_\u03c4|s_\u03c4)) = (G_\u03c4 - f_x(v_\u03c4|s_\u03c4))/max(1, S)$,\nwhere S is computed using percentile (Per) exponential moving average (EMA) [34]:\n$S = EMA(Per(G_\u03c4, 95) \u2013 Per(G_\u03c4, 5), 99)$."}, {"title": "F R-AIF Agent Algorithm", "content": "See implementation details in Algorithm 2."}, {"title": "G Implementation of environments", "content": "Pixel-level Mountain Car. The mountain car environment [77] is a standard testing problem in reinforcement\nlearning in which an agent (the car) has to drive uphill. A difficult aspect of this problem is that the gravitational\nforce is greater than full force that can be exerted by the car such that it cannot go uphill by simply moving forward.\nThe agent has to learn to build up the car's potential energy by driving to the opposite hill (behind it) in order to\ncreate enough acceleration to reach the goal state in front of it. The original mountain car problem was proposed\nas an MDP where the environment state included the exact position and velocity of the car at any step in time.\nIn the POMDP extension of the task, agents are not permitted to use this state directly. Instead, an agent must\nuse rendered pictures (height 64, width 64) as its observations (and must infer useful internal states that aid it\nin its completion of the taks). Critically, the reward signal provided by this task is very sparse, i.e., it is -1 for\nevery step and 0 when the agent reaches the goal, and the action space is continuous. In the actual environment,\nwe modify it slightly to provide a dark background and light objects, e.g., white car, to improve visualization for\nhuman experimenters.\nMeta-World. In this environment, the agent (a controllable robotic arm) has to control the proprioceptive joints\n(represented as a vector of continuous actions) in a velocity-based control system [87]. Since the workspace is\n3D, an observation from a single viewpoint might cause the agent to struggle when inferring environment hidden\nstates, e.g., the height of the goal can never be inferred from the top-down view. Therefore, we construct a raw\npixel observation image using three different (camera) viewpoints: these include a top-down, an agent workspace,\nand a side camera view. Furthermore, we ensure that the reward space for the tasks in this environment are sparse,\nsimilar in form to the sparse signals produced by the mountain car environment with a reward of 0 provided when\nthe agent achieves the control objective (it reaches a successful state) and -1 everywhere else. Note that, unlike\nthe mountain car, the environment simulation continues even after the agent reaches the goal state.\nrobosuite. Similar to the Meta-World environment, robosuite [88] simulates a robotics environment where the\nagent controls different joints as continuous actions. Since the robot's workspace in robosuite is larger, we utilized\nfour different camera viewpoints as streams of pixel observations for the agent instead of three as we did in\nMeta-World, namely: bird's-eye view (similar to the top-down view in Meta-World), agent view (the agent's\nperspective of the workspace), side view, and front view. Additionally, we modified the environment to have a\nsparse reward system, yielding a reward of 1 when the agent successfully achieves the task goal, and 0 otherwise.\nFinally, the environment continues even after the agent successfully reaches the goal state."}, {"title": "H Training R-AIF Agent", "content": "In contrast to on-policy learning algorithms [69, 71], we train our agent in an off-policy fashion (using memory\nbuffers). Specifically, we utilize two replay buffers: a standard replay buffer M stores all agent's encountered\ntransitions, and a positive replay buffer M+ only contains episodes with at least one step successfully-reached\ngoal state. While training, we sample from these buffers equally as training with more successful samples is found\nto boost the convergence of CRSPP. We then simulate the agent in the environment and train the world model,\nCRSPP, information gain ensemble [79], policy network, and value function periodically (see Algorithm 2 for\nspecific details)."}, {"title": "I Derivation of Expected Free Energy", "content": "According to [72, 14, 23], the expected free energy given the planned action distribution \u03c0 can be formulated as:\n$G_\u03c4(\u03c0) = E_\u03b3[lnQ(\u015b_\u03c4,0|\u03c0) \u2013 ln Q(0, s_\u03c4, \u03b8|\u03c0)]$\nwith $Q = Q(0_\u03c4, s_\u03c4, \u03b8|\u03c0)$ the joint probability of future observation, state, and model parameter given planned\naction \u03c0. This expected free energy formula can be further decomposed into:\n$G(\u03c0, \u03c4) = E_\u03b3[lnQ(\u03b8|\u03c0) \u2013 ln Q(\u03b8|0_\u03c4, s_\u03c4, \u03c0)] + E_\u03b3[lnQ(s_\u03c4|\u03b8, \u03c0) \u2013 ln Q(s_\u03c4|0_\u03c4, \u03c0)] \u2013 E_\u03b3[ln P(0_\u03c4)]$\nwith the first term is the model parameter exploration, denoting the mutual information between the model parameter\nbefore and after making an observation and state. The second term is the mutual information of agent's hidden\nstate before and after making a new observation. The third term is realizing preference term where the agent tries\nto compute the amount of information about the observation that matches its prior preference."}, {"title": "J Discussion", "content": "Being able to estimate a particular goal or preferred state at any particular time step is very useful for cognitive\ncontrol agent. As our results demonstrate, the agent can then learn to adapt to take actions that lead from a specific\nstate to its estimated goal(s). In contrast to the approaches taken in the imitation learning and behavior cloning\nliterature, which train the agent's policy based on a fixed collected expert dataset, in our R-AIF framework, the\nexpert signal (the preferred observation) is estimated dynamically through an adaptive prior preference model,\nclosing the domain gap between the actual trajectories and the collected training imitation (preferred) trajectories.\nNote that, with respect to our CRSPP sub-module, our model utilizes contrastive objective to adapt its parameters\n(i.e., it solely learns how to estimate the world model's encoded states while pushing itself away from undesired\nworld model states/trajectories) and thus does not require or learn any decoder. We only make use of the learned\ndecoder in the generative world model to visualize the preferred observation from the estimated CRSPP's states.\nExperimentally, we remark that using an auxiliary decoder proved useful for clearly visualizing the preferred\nobservation \u00f4t given the produced preferred state \u015dt at each time step.\nTheoretically, behavior cloning will be unable to achieve a great of success in multi-goal environments due to its\nreliance on a fixed, finite-size imitation sample pool. The expert trajectories in this fixed pool might further differ\nfrom the actual trajectories that the agent needs to take to solve the problem at hand, i.e., there is a distributional\ngap in observations, and therefore require different sets of actions to be taken. On the other hand, CRSPP learns to\nproduce a dynamic goal state while jointly training the agent's core policy to reach task goal states. Therefore, as\nwe empirically confirmed in simulation, R-AIF does not suffer from goal-mismatch problem in the training phase\nthat other AIF schemes would.\nBroader Impacts. In line with active inference process theory's focus on optimizing a policy that minimizes future\nexpected free energy, R-AIF agents do so by taking actions that they are able to predict will lead to their preferences\n(in line AIF's instrumental signal), while also jointly taking actions that they are mostly sure about and working to\nreduce uncertainty by taking intelligent explorative actions of their niches (in line with AIF's epistemic signals).\nThis is practical for different robotic tasks, particularly those with potential dangers in their operation/functioning\n(i.e., when human safety must be considered). For example, a self-driving car agent can take the actions that it\nis sure to be safe rather than focusing exploring wildly (as random exploration policies encourage, potentially\ncausing traffic accidents. Furthermore, the R-AIF framework could prove useful to the imitation learning research\ncommunity, as its ability to optimize a policy that achieves dynamic goals as produced from an adaptive prior\npreference model was found to be quite useful for the more complex POMDP tasks we sought to solve in this work.\nThis carries with it possible positive implications for practical application in downstream tasks such as autonomous\ndriving [53] and complex robotic control and navigation [40, 55]."}, {"title": "A Recurrent State Space Model and World Model", "content": "A Recurrent State Space Model and World Model"}, {"title": "B Improving Numerical Stability", "content": "B Improving Numerical Stability"}, {"title": "C The prior preference self-revision mechanism", "content": "C The prior preference self-revision mechanism"}, {"title": "D Computing the Information Gain using a Network Ensemble", "content": "D Computing the Information Gain using a Network Ensemble"}, {"title": "E Using Percentage Exponential Moving Average (PEMA)", "content": "E Using Percentage Exponential Moving Average (PEMA)"}, {"title": "F R-AIF Agent Algorithm", "content": "F R-AIF Agent Algorithm"}, {"title": "G Implementation of environments", "content": "G Implementation of environments"}, {"title": "H Training R-AIF Agent", "content": "H Training R-AIF Agent"}, {"title": "I Derivation of Expected Free Energy", "content": "I Derivation of Expected Free Energy"}, {"title": "J Discussion", "content": "J Discussion"}]}