{"title": "Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery", "authors": ["Yuni Susanti", "Michael F\u00e4rber"], "abstract": "Causal discovery aims to estimate causal structures among variables based on observational data. Large Language Models (LLMs) offer a fresh perspective to tackle the causal discovery problem by reasoning on the metadata associated with variables rather than their actual data values, an approach referred to as knowledge-based causal discovery. In this paper, we investigate the capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1 billion parameters) with prompt-based learning for knowledge-based causal discovery. Specifically, we present \"KG Structure as Prompt\", a novel approach for integrating structural information from a knowledge graph, such as common neighbor nodes and metapaths, into prompt-based learning to enhance the capabilities of SLMs. Experimental results on three types of biomedical and open-domain datasets under few-shot settings demonstrate the effectiveness of our approach, surpassing most baselines and even conventional fine-tuning approaches trained on full datasets. Our findings further highlight the strong capabilities of SLMs: in combination with knowledge graphs and prompt-based learning, SLMs demonstrate the potential to surpass LLMs with larger number of parameters. Our code and datasets are available on GitHub.", "sections": [{"title": "1 Introduction", "content": "One of the fundamental tasks in various scientific disciplines is to find underlying causal relationships and eventually utilize them [10]. Causal discovery is a branch of causality study which estimates causal structures from observational data and generates a causal graph as a result. A causal graph, as illustrated in Fig. 1, is a directed graph modeling the causal relationships between observed variables; a node represents a variable and an edge represents a causal relationship."}, {"title": "2 Background and Related Work", "content": "Small Language Models. Small Language Models (SLMs) refer to language models with fewer parameters, resulting in a reduced capacity to process text compared to larger-parameter LLMs. However, SLMs typically require less computation resources, making them faster to train and deploy, and maintaining them is generally more cost-effective. On the contrary, LLMs are trained on vast amounts of diverse data, thus have significantly more parameters and are capable of handling more complex language tasks than SLMs. Nevertheless, LLMs are expensive and difficult to train and deploy as they typically require more computational resource. For instance, GPT-3 [4], which consists of 175 billion parameters, is impractical to run on hardware with limited resources.\nIn this work, we define SLMs as LMs with less than 1 billion parameters. We explore the causal capability of SLMs with different architectures: (1) Masked Language Model (MLM) especially the encoder-only model, (2) Causal Language Model (CLM) or decoder-only language model, and (3) Sequence-to-Sequence Language Model (Seq2SeqLM) or encoder-decoder model. We provide an overview of each type of architecture below.\nMLMs, especially encoder-only models such as BERT [8], are a type of LM that utilizes encoder blocks within the transformer architecture and are trained to predict masked tokens based on the context provided by surrounding words. They excel in natural language understanding (NLU) tasks, e.g., text classification, as they are able to capture relationships between words in a text sequence.\nCLMs, such as GPT-3 [4], use the decoder blocks within the transformer architecture and are trained to generate text one token at a time, by conditioning each token on the preceding tokens in the sequence. Meanwhile, Seq2SeqLMs, such as T5 [32], consist of both encoder and decoder blocks. The encoder transforms the input sequence into vector representation, while the decoder produces"}, {"title": "Prompt-based Learning & Knowledge Injection", "content": "Research on classifying causal relations from text has predominantly occurred within supervised settings, utilizing classical machine learning (ML) approaches [3,5,6,19,20,28] or fine-tuning pre-trained language models [11,18,36,33,38]. Classical ML techniques often require extensive feature engineering and have shown inferior performance compared to fine-tuning language models such as BERT [8]. Therefore, we evaluate our method against fine-tuning methods as baselines.\nMeanwhile, prompt-based learning, also known as prompt-tuning, has recently emerged as a promising alternative to the conventional fine-tuning approach for a variety of Natural Language Processing (NLP) tasks [1,26,34,35]. Typically, a prompt is composed of discrete text (hard prompt); however, recent work has introduced soft prompt, a continuous vector that can be optimized through backpropagation [23,25]. In the relation classification task, prompt-based learning often involves inserting a prompt template containing masked tokens into the input, essentially converting the task into masked language modeling or text generation problems [7,13,14]. This approach is particularly well-suited for few-shot or zero-shot scenarios, where only limited labeled data is available [9,35]. This motivates us to investigate such prompt-based learning under few-shot settings, given the scarcity of datasets for our causal relation classification task.\nOther works explore knowledge injection for the prompt construction, for instance, KnowPrompt [7] injects latent knowledge contained in relation labels into prompt construction with learnable virtual words. KAPING [2] retrieves top-K similar triples of the target entities from Wikidata and further augments them as a prompt. KiPT [24] uses WordNet to calculate semantic correlation between the input and manually constructed core concepts to construct the prompts. Our work differs from them since we focus on leveraging structural information of knowledge graphs to construct the prompt (see \u00a74)."}, {"title": "3 Task Formulation", "content": "In this work, we focus on pairwise knowledge-based causal discovery: given a pair of entities e\u2081 and e\u2082, i.e., variable or node pairs such as FGF6 and prostate cancer, the task is to predict if a causal relation can be inferred between the pair. We formulate the task as a binary classification task, classifying the relation as causal or non-causal. We evaluate our approach on a dataset D = {X, Y}, where X is a set of training instances and y = {causal, non-causal} is a set of relation labels. Each instance $x \\in X$ consists of a token sequence $x = {W_1, W_2, ...W_{|n|}}$ and the spans of a marked variable pair, and is annotated with a label $y_x \\in V$."}, {"title": "4 Approach", "content": "We illustrate our proposed approach in Fig. 2. First, we generate a graph context, which is derived from the structural information of a knowledge graph with our KG Structure as Prompt method. Next, we feed the generated graph context and the inputs, i.e., the variable pair and its textual context, into the SLMs to train a prompt-based learning model.\nWe elaborate our proposed approach in the following subsections. We start with preliminaries (\u00a74.1), followed by the design of the KG structure as Prompt for generating the graph context (\u00a74.2), and the incorporation of the generated graph context into the SLMs architecture with prompt-based learning (\u00a74.3)."}, {"title": "4.1 Preliminaries", "content": "Formally, we define a directed graph G = (\u03bd,\u03b5) where V is a set of vertices or nodes, and $E \\subseteq V \\times V$ is a set of directed edges. A knowledge graph KG is a specific type of directed graph representing a network of entities and the relationships between them. Formally, we define a KG as a directed labeled graph KG = (N, E, R, F) where N is a set of nodes (entities), $E \\subseteq N \\times N$ is a set of edges (relations), R is a set of relation labels, and F : E \u2192 R, is a function assigning edges to relation labels. For instance, assignment label r to an edge e = (x, y) can be viewed as a triple (x, r, y), e.g., (Tokyo, IsCapitalof, Japan)."}, {"title": "4.2 Knowledge Graph Structure as Prompt", "content": "In the field of Graph Neural Networks (GNNs), [44] explores whether LLMs can replace GNNs as the foundation model for graphs by using natural language to describe the geometric structure of the graph. Their results on several graph datasets surpass traditional GNN-based methods, showing the potential"}, {"title": "4.3 Prompt-based learning with graph context", "content": "As illustrated in the model architecture in Fig. 2, we feed the textual context into SLMs together with the graph context generated with KG Structure as Prompt. We further design a prompt-based learning approach utilizing both contexts elaborated in this section. To get a clear distinction between conventional fine-tuning and our proposed prompt-based learning approach, we first provide a short overview of the conventional fine-tuning approach, as follows.\nGiven a pre-trained LM L to fine-tune on a dataset D, the conventional fine-tuning method encodes the training sequence x = {W\u2081,W\u2082,...W_{|n|}} into the corresponding output hidden vectors of the LMs h = {h\u2081,h\u2082,...h_{|n|}}. For MLMs such as BERT [8], the special token \u201c [CLS]\u201d is inserted at the beginning"}, {"title": "5 Evaluation", "content": "Experiment Settings. We evaluate the proposed approach under few-shot settings, using k = 16 training samples across all experiments. Precision (P), Recall (R), and F1-score (F1) metrics are employed to evaluate the performance. Since fine-tuning on low resources often suffers from instability and results may change given a different split of data [35], we apply 5-fold cross-validation and the metric scores are averaged. We restrict the number of contents from the KG structures to be included in the prompt since the length of the prompt for SLMs is limited, and we restrict the number of hops when querying the KG, as well. We experimented with different settings and reported the best performing models. Additional technical details are provided online as supplementary materials."}, {"title": "6 Results and Discussion", "content": "Table 2 & 3 summarize the results. We report the averaged Precision (P), Recall (R), and F1 scores, including the standard deviation values of the F1 scores over the 5-folds cross-validation. We provide a summary of the primary findings (\u00a76.1), followed by analysis and discussion of the results (\u00a76.2)."}, {"title": "6.1 Primary Findings", "content": "We listed the summary of the main results from Table 2 below."}, {"title": "6.2 Analysis and Discussion", "content": "NN vs. CNN vs. MP In our experiments, the KG structure metapath MP contributed the most to the top-performing models, while the neighbors nodes NN and common neighbors nodes CNN roughly exhibited comparable performance across models and datasets. The effectiveness of MP likely depends on the hop count between entity pairs in the dataset, i.e., the hop count is relatively high (2.8) for the COMACG dataset, where MP gave the best performance. Conversely, for the GENEC dataset, where the average MP hop is low (1.8), CNN and NN outperformed MP. To train a robust model that is able to generalize well given any KG structure, we opted to not optimize the content selection of the KG structures in the current experiments. For instance, when there are more than m metapaths for a pair, we randomly select m of them, m being a hyperparameter of the number of metapaths to be included as prompt. In spite of that, our proposed approach achieved a relatively satisfactory performance, suggesting that rather than the content of the structure, the type of the structural information, i.e., NN vs. CNN vs. MP, is arguably more important based on the experiments."}, {"title": "MLM vs. CLM vs. Seq2SeqLM", "content": "For classification tasks, language models trained with MLM architecture are often preferred. This preference comes from the fact that MLMs are trained to consider both preceding and succeeding words, a crucial aspect for accurately predicting the correct class in a classification model. In line with this, the top-performing models trained on both full and few-shot datasets are based on the MLM architecture. The second best-performing models using full dataset are based on the Seq2SeqLM architecture, followed by those based on the CLM architecture. This is most likely because, similar to MLMs, Seq2SeqLMs also include encoder blocks and are trained to recognize the surrounding words [32]. However, this trend slightly differs in experiments under few-shot settings, as the models based on CLM architecture outperformed those based on Seq2SeqLM architecture. Thus, selecting an appropriate architecture, specifically how the LMs are trained, is crucial when adapting the LMs for downstream tasks. As demonstrated by the outcomes of our experiments, LMs trained with the MLM architecture are generally more suitable for classification tasks than those with Seq2SeqLM and CLM architectures."}, {"title": "Wikidata vs. Hetionet", "content": "In the biomedical domain, the proposed approach injected with structural information from Hetionet demonstrates better performance in most experiments. This is expected considering the domain-specific nature of the dataset. Nevertheless, both Wikidata and Hetionet performed relatively well; the top-performing models for COMAGC and GENEC datasets are attained with Hetionet, while for DDI dataset are achieved with Wikidata. We also achieved 6.8 points of F1 score improvement on SEMEVAL dataset with Wikidata. This suggests that the proposed approach is rather flexible regarding the choice of KGs."}, {"title": "SLMs vs. LLMs", "content": "We selected OpenAI's GPT-3.5-turbo-instruct [31] as a representative of larger parameter-LLMs. However, OpenAI does not provide technical details such as the numbers of parameters; except the context windows which is 4,096 tokens in size for this model [31]. This is much larger than our choice of SLMs with a maximum token length ranging from 128 to 512. To summary, the results demonstrate that the SLMs outperformed this model across all datasets in most experiment. This further shows the potential of SLMs: combined with prompt-based learning and access to KGs, the proposed approach outperforms LLMs with considerably larger size and parameters, with minimal training effort. Note that we also provided k = 16 training samples as task demonstration to query the GPT model for a more fair comparison with the experiments under few-shot settings.\nTypically, SLMs are trained on significantly less data compared to LLMS, which leads to reduced capacity and inferior performance in downstream tasks. Therefore, the graph context derived from the structural information of KG by our proposed KG Structure as Prompt approach effectively serves as an additional evidence of causality; in other words, it assists the SLMs to rely"}, {"title": "7 Conclusion", "content": "In this paper, we presented \"KG Structure as Prompt\", a novel approach for integrating structural information from a KG into prompt-based learning, to further enhance the capability of Small Language Models (SLMs). We evaluated our approach on knowledge-based causal discovery tasks. Extensive experiments under few-shot settings on biomedical and open-domain datasets highlight the effectiveness of the proposed approach, as it outperformed most of the no-graph context baselines, including the conventional fine-tuning method with a full dataset. We also demonstrated the robust capabilities of SLMs: in combination with prompt-based learning and KGs, SLMs are able to surpass a language model with larger parameters. Our proposed approach has proven to be effective with different types of LMs architectures and KGs, as well, showing its flexibility and adaptability across various LMs and KGs.\nOur work has been centered on discovering causal relationships between pairs of variables. In future work, we aim to tackle more complex scenarios by developing methods to analyze causal graphs with multiple interconnected variables, which will offer a deeper understanding of causalities."}]}