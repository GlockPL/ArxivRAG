{"title": "Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery", "authors": ["Yuni Susanti", "Michael F\u00e4rber"], "abstract": "Causal discovery aims to estimate causal structures among variables based on observational data. Large Language Models (LLMs) offer a fresh perspective to tackle the causal discovery problem by reasoning on the metadata associated with variables rather than their actual data values, an approach referred to as knowledge-based causal discovery. In this paper, we investigate the capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1 billion parameters) with prompt-based learning for knowledge-based causal discovery. Specifically, we present \"KG Structure as Prompt\", a novel approach for integrating structural information from a knowledge graph, such as common neighbor nodes and metapaths, into prompt-based learning to enhance the capabilities of SLMs. Experimental results on three types of biomedical and open-domain datasets under few-shot settings demonstrate the effectiveness of our approach, surpassing most baselines and even conventional fine-tuning approaches trained on full datasets. Our findings further highlight the strong capabilities of SLMs: in combination with knowledge graphs and prompt-based learning, SLMs demonstrate the potential to surpass LLMs with larger number of parameters. Our code and datasets are available on GitHub.", "sections": [{"title": "1 Introduction", "content": "One of the fundamental tasks in various scientific disciplines is to find underlying causal relationships and eventually utilize them [10]. Causal discovery is a branch of causality study which estimates causal structures from observational data and generates a causal graph as a result. A causal graph, as illustrated in Fig. 1, is a directed graph modeling the causal relationships between observed variables; a node represents a variable and an edge represents a causal relationship."}, {"title": "2 Background and Related Work", "content": "Small Language Models. Small Language Models (SLMs) refer to language models with fewer parameters, resulting in a reduced capacity to process text compared to larger-parameter LLMs. However, SLMs typically require less computation resources, making them faster to train and deploy, and maintaining them is generally more cost-effective. On the contrary, LLMs are trained on vast amounts of diverse data, thus have significantly more parameters and are capable of handling more complex language tasks than SLMs. Nevertheless, LLMs are expensive and difficult to train and deploy as they typically require more computational resource. For instance, GPT-3 [4], which consists of 175 billion parameters, is impractical to run on hardware with limited resources.\nIn this work, we define SLMs as LMs with less than 1 billion parameters. We explore the causal capability of SLMs with different architectures: (1) Masked Language Model (MLM) especially the encoder-only model, (2) Causal Language Model (CLM) or decoder-only language model, and (3) Sequence-to-Sequence Language Model (Seq2SeqLM) or encoder-decoder model. We provide an overview of each type of architecture below.\nMLMs, especially encoder-only models such as BERT [8], are a type of LM that utilizes encoder blocks within the transformer architecture and are trained to predict masked tokens based on the context provided by surrounding words. They excel in natural language understanding (NLU) tasks, e.g., text classification, as they are able to capture relationships between words in a text sequence. CLMs, such as GPT-3 [4], use the decoder blocks within the transformer architecture and are trained to generate text one token at a time, by conditioning each token on the preceding tokens in the sequence. Meanwhile, Seq2SeqLMs, such as T5 [32], consist of both encoder and decoder blocks. The encoder transforms the input sequence into vector representation, while the decoder produces"}, {"title": "Prompt-based Learning & Knowledge Injection", "content": "Research on classifying causal relations from text has predominantly occurred within supervised settings, utilizing classical machine learning (ML) approaches [3,5,6,19,20,28] or fine-tuning pre-trained language models [11,18,36,33,38]. Classical ML techniques often require extensive feature engineering and have shown inferior performance compared to fine-tuning language models such as BERT [8]. Therefore, we evaluate our method against fine-tuning methods as baselines.\nMeanwhile, prompt-based learning, also known as prompt-tuning, has recently emerged as a promising alternative to the conventional fine-tuning approach for a variety of Natural Language Processing (NLP) tasks [1,26,34,35]. Typically, a prompt is composed of discrete text (hard prompt); however, recent work has introduced soft prompt, a continuous vector that can be optimized through backpropagation [23,25]. In the relation classification task, prompt-based learning often involves inserting a prompt template containing masked tokens into the input, essentially converting the task into masked language modeling or text generation problems [7,13,14]. This approach is particularly well-suited for few-shot or zero-shot scenarios, where only limited labeled data is available [9,35]. This motivates us to investigate such prompt-based learning under few-shot settings, given the scarcity of datasets for our causal relation classification task.\nOther works explore knowledge injection for the prompt construction, for instance, KnowPrompt [7] injects latent knowledge contained in relation labels into prompt construction with learnable virtual words. KAPING [2] retrieves top-K similar triples of the target entities from Wikidata and further augments them as a prompt. KiPT [24] uses WordNet to calculate semantic correlation between the input and manually constructed core concepts to construct the prompts. Our work differs from them since we focus on leveraging structural information of knowledge graphs to construct the prompt (see \u00a74)."}, {"title": "3 Task Formulation", "content": "In this work, we focus on pairwise knowledge-based causal discovery: given a pair of entities e\u2081 and e\u2082, i.e., variable or node pairs such as FGF6 and prostate cancer, the task is to predict if a causal relation can be inferred between the pair. We formulate the task as a binary classification task, classifying the relation as causal or non-causal. We evaluate our approach on a dataset D = {X, Y}, where X is a set of training instances and y = {causal, non-causal} is a set of relation labels. Each instance x \u2208 X consists of a token sequence x = {W1, W2, ...W|n|} and the spans of a marked variable pair, and is annotated with a label yx \u2208 V."}, {"title": "4 Approach", "content": "We illustrate our proposed approach in Fig. 2. First, we generate a graph context, which is derived from the structural information of a knowledge graph with our KG Structure as Prompt method. Next, we feed the generated graph context and the inputs, i.e., the variable pair and its textual context, into the SLMs to train a prompt-based learning model.\nWe elaborate our proposed approach in the following subsections. We start with preliminaries (\u00a74.1), followed by the design of the KG structure as Prompt for generating the graph context (\u00a74.2), and the incorporation of the generated graph context into the SLMs architecture with prompt-based learning (\u00a74.3)."}, {"title": "4.1 Preliminaries", "content": "Formally, we define a directed graph G = (\u03bd,\u03b5) where V is a set of vertices or nodes, and E \u2286 V \u00d7 V is a set of directed edges. A knowledge graph KG is a specific type of directed graph representing a network of entities and the relationships between them. Formally, we define a KG as a directed labeled graph KG = (N, E, R, F) where N is a set of nodes (entities), E \u2286 N \u00d7 N is a set of edges (relations), R is a set of relation labels, and F : E \u2192 R, is a function assigning edges to relation labels. For instance, assignment label r to an edge e = (x, y) can be viewed as a triple (x, r, y), e.g., (Tokyo, IsCapitalof, Japan)."}, {"title": "4.2 Knowledge Graph Structure as Prompt", "content": "In the field of Graph Neural Networks (GNNs), [44] explores whether LLMS can replace GNNs as the foundation model for graphs by using natural language to describe the geometric structure of the graph. Their results on several graph datasets surpass traditional GNN-based methods, showing the potential"}, {"title": "(1) Neighbor Nodes (NN)", "content": "The essence of GNNs lies in applying different aggregate functions to the graph structure, i.e., passing node features to neighboring nodes, where each node aggregates the feature vectors of its neighbors to further update its feature vector. Thus, it is evident that the neighbor nodes are the most crucial feature within a graph. Inspired by that, we examine the neighboring nodes of the target node pairs to infer their causal relationship.\nFormally, a node x is a neighbor of a node y in a knowledge graph KG = (V, E) if there is an edge {x,y} \u2208 E. We provide an example of neighbor nodes from Wikidata [40] in Fig. 4. According to the provided example, the node prostate cancer has urology as one of its neighbor nodes, while FGF6 has urinary bladder as one of its neigbors. Thus, it is likely that a connection exists between the node pair (FGF6, prostate cancer) due to their respective neighboring nodes: urinary bladder \u2192 urology."}, {"title": null, "content": "For utilizing the neighbor nodes structure in the prompt, we describe it in natural language to form a graph context C, which we formally denote as:\n$C(x, V, E) = {x} \u201cis connected to", "follows": "n$C(x, V, E) = {x} \u201chas\u201d {Ex,x2 } \u201crelation with"}, {"title": "(2) Common Neighbor Nodes (CNN)", "content": "Unlike neighbor nodes, common neighbor nodes capture the idea that the more neighbors a pair of nodes (x, y) shares, the more likely it is for the pair to be connected, i.e., there exists an edge e = x, y between them. We argue that common neighbors between two nodes help infer their relationship, so we examine the common neighbors information between the node pair as graph context, as well. Fig. 5 shows an example of common neighbor nodes for the pair (breast cancer, ERBB2), taken from Hetionet knowledge graph [17]. According to the provided example, the pair has in total 95 common neighbors, confirming a close relationship between them. Formally, common neighbors between the nodes x and y can be defined as:\n$CN{x,y} = N(x) \u2229 N(y)$ (3)"}, {"title": null, "content": "where N(x) is the set of nodes adjacent to node x (the neighbors of x), and N(y) is the set of nodes adjacent to node y (the neighbors of y). Subsequently, the graph context C for describing the common neighbors between the pairs x and y can be formed as follows:\n$C(x, CN, y) = \"Common neighbor nodes of {x} and {y} are\" : {[n]n\u2208CN}$ (4)\nwhere CN represents the list of common neighbor nodes of the pair x and y as defined in Eq. 3. Again, the additional template words \"Common neighbor nodes of...\" are optional and can be replaced by other words. Then, we can generate the graph context C including the common neighbor nodes information for the pair in Fig. 5, as follows:"}, {"title": "(3) Metapath (MP)", "content": "Metapaths, or meta-paths are sequences of node types which define a walk from an origin node to a destination node [37]. The term \"metapath\" in this work is borrowed from the biomedical domain, referring to specific node type combinations thought to be informative [30]. Due to its importance in biomedical network analysis [41,43], we investigate the metapaths of two nodes for inferring their causal relationship. Moreover, causal relationships are frequently observed in the biomedical domain. Fig. 6 shows examples of metapaths of prostate cancer and FGF6, from Hetionet [17].\nFormally, a metapath MP can be defined as a path Z1 R1 Z2 R2,... Rn Zn+1 describing a relation R between node types Z and Zn+1. The following examples illustrate metapaths of different path length n from Fig. 6."}, {"title": null, "content": "We describe the metapath structure from KG in natural language to form a graph context C, as follows:\n$C(x, y, MP{V, E}) = {x} \"is connected to\" {y} \"via the following path: ", "is connected to...\\\" are optional and can be replaced by other tokens. Then, the graph context C with metapath information for the example in Fig. 6 would be:\"\n    },\n    {\n      \"title\": \"4.3 Prompt-based learning with graph context\",\n      \"content\": \"As illustrated in the model architecture in Fig. 2, we feed the textual context into SLMs together with the graph context generated with KG Structure as Prompt. We further design a prompt-based learning approach utilizing both contexts elaborated in this section. To get a clear distinction between conventional fine-tuning and our proposed prompt-based learning approach, we first provide a short overview of the conventional fine-tuning approach, as follows.\nGiven a pre-trained LM L to fine-tune on a dataset D, the conventional fine-tuning method encodes the training sequence x = {W1,W2, ...W|n|} into the corresponding output hidden vectors of the LMs h = {h1,h2,...h|n|}. For MLMs such as BERT [8], the special token \u201c [CLS]": "s inserted at the beginning"}, {"title": null, "content": "of the sequence, and this special token is used as the final sequence representation h', since it is supposed to contain information from the whole sequence. A fully-connected layer and a softmax layer are further applied on top of this representation to calculate the probability distribution over the class set Y, as follows.\n$p = softmax(Wfh' + bf)$ (6)\nPrompt-based learning, on the other hand, adapts the pre-trained LMs for the downstream task via priming on natural language prompts-pieces of text that are combined with the input and fed to the LMs to produce an output for downstream tasks [1]. Concretely, we first convert each input sequence x with a template T to form a prompt x': T : x \u2192 x'. In addition, a mapping function M is used to map the downstream task class set Y to a set of label words V constituting all vocabularies of the LM \u0421, \u0456.\u0435., \u041c: \u0423 \u2192 V. As in the pre-training of LMs, we further insert the special token \" [MASK]\" into x' for L to fill with the label words V. We provide an example of the prompt formulation below.\nGiven x =\"Smoking causes cancer in adult male.\", we set a template T, e.g.,\nT = [x] \"It shows [MASK] relation.\"\nThen, the prompt x' would be:\nx' = \"Smoking causes cancer in adult male. It shows [MASK] relation.\"\nWe further feed the prompt x' into L to obtain the hidden vector h [MASK] of [MASK]. Next, with the mapping function M connecting the class set Y and the label words, we formalize the probability distribution over y at the masked position, i.e., p(y|x) = p([MASK] = M(y)|x'). Here, the mapping function can also be set manually e.g., M(true) = \u201cpositive\u201d and M(false) = \u201cnegative\u201d. Note that depending on the task, dataset, and the prompt design, the class labels themselves can be used directly without any mapping function M.\nIn this study, our prompt-based learning combines the input sequence x with the graph context C into the prompt x', as illustrated in Fig. 2. Specifically, we formulate the prompt x' to include the following elements:\n(1) textual context: input sequence x containing the pair,\n(2) graph context C: context generated from KG structures as described in \u00a74.2,\n(3) target pair: pair e1 and e2 as the target, e.g., (FGF6, prostate cancer),\n(4) [MASK] token,\n(5) (optional) template tokens.\nSubsequently, our final prompt x' as the input to the LM for the pair e\u2081 and e2 can be formally defined as:\nx' = [x] [C] The pair [e1] and [e1] shows a [MASK] relation. (7)\nIn this study, we select three SLMs, one for each of the three architectures: MLM, CLM, Seq2SeqLM. Since each type of SLMs is trained differently, we design the prompt x' differently across each type of SLMs. For instance, the prompt"}, {"title": null, "content": "x' in Eq. 7, which is a cloze-style task prompt, suits the MLM architecture, since this model is trained to be able to see the preceding and succeeding words in texts. As for CLM and Seq2SeqLM, we cast the task as a generation-type, with prompt x' such as:\nx' = [x] [C] The pair [e1] and [e2] shows a causal relation: [MASK]. (8)\nAs mentioned earlier, the design of the mapping function M to map the output into the downstream task labels varies depending on the task, dataset, and the prompt design. For instance, with the prompt x' as in Eq. 7, we can directly use the class labels set Y = {causal, non-causal} without any mapping function. Meanwhile, for prompt x' in Eq. 8, we manually define a mapping function, e.g., M(causal) = \u201ctrue\u201d and M(non-causal) = \u201cfalse\u201d. Note that the template \" The pair shows...\" is optional and can be replaced with other text."}, {"title": "5 Evaluation", "content": "Experiment Settings. We evaluate the proposed approach under few-shot settings, using k = 16 training samples across all experiments. Precision (P), Recall (R), and F1-score (F1) metrics are employed to evaluate the performance. Since fine-tuning on low resources often suffers from instability and results may change given a different split of data [35], we apply 5-fold cross-validation and the metric scores are averaged. We restrict the number of contents from the KG structures to be included in the prompt since the length of the prompt for SLMs is limited, and we restrict the number of hops when querying the KG, as well. We experimented with different settings and reported the best performing models. Additional technical details are provided online as supplementary materials.\nDatasets. The evaluation datasets are summarized in Table 1. Causality is often observed in the biomedical domain, thus we primarily evaluate our approach within this field, supplemented by an open-domain dataset. Each instance in the dataset comprises textual context where a variable pair co-occurs in a text (see Example 5 & 6), and is annotated by human experts to determine if there is a causal relation between the variables.\nChoice of SLMs. In this work, we define SLMs as LMs with less than 1 billion of total parameters. We experimented with SLMs with three different architectures, as follows.\n(a) MLM: roberta [27] model adapted to the biomedical domain, with 125 million parameters (biomed-roberta-base-125m [12]),\n(b) CLM: bloomz-560m [29] with 560 million parameters,\n(c) Seq2SeqLM: T5-base-220m [32] model with 220 million parameters"}, {"title": "Model Comparison", "content": "We compare the following models: Models (1) to (4) represent the models trained without the graph context, i.e., the baselines, while models marked with \u201cPBL\u201d (model 4 to 6) are our proposed prompt-based learning method injected with graph context information from KGs."}, {"title": "6 Results and Discussion", "content": "Table 2 & 3 summarize the results. We report the averaged Precision (P), Recall (R), and F1 scores, including the standard deviation values of the F1 scores over the 5-folds cross-validation. We provide a summary of the primary findings (\u00a76.1), followed by analysis and discussion of the results (\u00a76.2)."}, {"title": "6.1 Primary Findings", "content": "We listed the summary of the main results from Table 2 below."}, {"title": "6.2 Analysis and Discussion", "content": "vs. CNN vs. MP In our experiments, the KG structure metapath MP contributed the most to the top-performing models, while the neighbors nodes NN and common neighbors nodes CNN roughly exhibited comparable performance across models and datasets. The effectiveness of MP likely depends on the hop count between entity pairs in the dataset, i.e., the hop count is relatively high (2.8) for the COMACG dataset, where MP gave the best performance. Conversely, for the GENEC dataset, where the average MP hop is low (1.8), CNN and NN outperformed MP. To train a robust model that is able to generalize well given any KG structure, we opted to not optimize the content selection of the KG structures in the current experiments. For instance, when there are more than m metapaths for a pair, we randomly select m of them, m being a hyperparameter of the number of metapaths to be included as prompt. In spite of that, our proposed approach achieved a relatively satisfactory performance, suggesting that rather than the content of the structure, the type of the structural information, i.e., NN vs. CNN vs. MP, is arguably more important based on the experiments."}, {"title": "MLM vs. CLM vs. Seq2SeqLM", "content": "For classification tasks, language models trained with MLM architecture are often preferred. This preference comes from the fact that MLMs are trained to consider both preceding and succeeding words, a crucial aspect for accurately predicting the correct class in a classification model. In line with this, the top-performing models trained on both full and few-shot datasets are based on the MLM architecture. The second best-performing models using full dataset are based on the Seq2SeqLM architecture, followed by those based on the CLM architecture. This is most likely because, similar to MLMs, Seq2SeqLMs also include encoder blocks and are trained to recognize the surrounding words [32]. However, this trend slightly differs in experiments under few-shot settings, as the models based on CLM architecture outperformed those based on Seq2SeqLM architecture. Thus, selecting an appropriate architecture, specifically how the LMs are trained, is crucial when adapting the LMs for downstream tasks. As demonstrated by the outcomes of our experiments, LMs trained with the MLM architecture are generally more suitable for classification tasks than those with Seq2SeqLM and CLM architectures."}, {"title": "Wikidata vs. Hetionet", "content": "In the biomedical domain, the proposed approach injected with structural information from Hetionet demonstrates better performance in most experiments. This is expected considering the domain-specific nature of the dataset. Nevertheless, both Wikidata and Hetionet performed relatively well; the top-performing models for COMAGC and GENEC datasets are attained with Hetionet, while for DDI dataset are achieved with Wikidata. We also achieved 6.8 points of F1 score improvement on SEMEVAL dataset with Wikidata. This suggests that the proposed approach is rather flexible regarding the choice of KGs."}, {"title": "SLMs vs. LLMs", "content": "We selected OpenAI's GPT-3.5-turbo-instruct [31] as a representative of larger parameter-LLMs. However, OpenAI does not provide technical details such as the numbers of parameters; except the context windows which is 4,096 tokens in size for this model [31]. This is much larger than our choice of SLMs with a maximum token length ranging from 128 to 512. To summarize, the results demonstrate that the SLMs outperformed this model across all datasets in most experiment. This further shows the potential of SLMs: combined with prompt-based learning and access to KGs, the proposed approach outperforms LLMs with considerably larger size and parameters, with minimal training effort. Note that we also provided k = 16 training samples as task demonstration to query the GPT model for a more fair comparison with the experiments under few-shot settings.\nTypically, SLMs are trained on significantly less data compared to LLMS, which leads to reduced capacity and inferior performance in downstream tasks. Therefore, the graph context derived from the structural information of KG by our proposed KG Structure as Prompt approach effectively serves as an additional evidence of causality; in other words, it assists the SLMs to rely"}, {"title": "7 Conclusion", "content": "In this paper, we presented \"KG Structure as Prompt\", a novel approach for integrating structural information from a KG into prompt-based learning, to further enhance the capability of Small Language Models (SLMs). We evaluated our approach on knowledge-based causal discovery tasks. Extensive experiments under few-shot settings on biomedical and open-domain datasets highlight the effectiveness of the proposed approach, as it outperformed most of the no-graph context baselines, including the conventional fine-tuning method with a full dataset. We also demonstrated the robust capabilities of SLMs: in combination with prompt-based learning and KGs, SLMs are able to surpass a language model with larger parameters. Our proposed approach has proven to be effective with different types of LMs architectures and KGs, as well, showing its flexibility and adaptability across various LMs and KGs.\nOur work has been centered on discovering causal relationships between pairs of variables. In future work, we aim to tackle more complex scenarios by developing methods to analyze causal graphs with multiple interconnected variables, which will offer a deeper understanding of causalities."}]}