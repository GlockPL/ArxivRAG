{"title": "Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens", "authors": ["Ziwei Shan", "Yaoyu He", "Chengfeng Zhao", "Jiashen Du", "Jingyan Zhang", "Qixuan Zhang", "Jingyi Yu", "Lan Xu"], "abstract": "Human bodily movements convey critical insights into action intentions and cognitive processes, yet existing multimodal systems primarily focused on understanding human motion via language, vision, and audio, which struggle to capture the dynamic forces and torques inherent in 3D motion. Inertial measurement units (IMUs) present a promising alternative, offering lightweight, wearable, and privacy-conscious motion sensing. However, processing of streaming IMU data faces challenges such as instable wireless transmission, sensor noise, and drift, limiting their utility for long-term real-time motion capture (MoCap), and more importantly, online motion analysis. To address these challenges, we introduce Mojito, an intelligent motion agent that integrates inertial sensing with large language models (LLMs) for interactive motion capture and behavioral analysis. The core innovation of Mojito lies in a jitter-reduced inertial token representation with a novel IMU signal encoding framework, and an extended language model involving inertial tokens. By employing VQVAE, Mojito learns a discrete latent space of continuous IMU signals, mitigating sensor noise and drift through quantization. The inertial tokens are then aligned with inductive bias of natural language and mapped to textual semantics to enhance compatibility with LLMs, enabling efficient sequence modeling. To support domain-specific applications, Mojito further incorporates tunable LoRA adapters, facilitating personalized feedback tailored to roles such as fitness trainers or rehabilitation therapists. Extensive experiments demonstrate that Mojito outperforms existing IMU-based methods in motion capture under noisy conditions, and achieves comparable behavior analysis capability compared to large vision-language models. The user study further highlights its practical effectiveness in various scenarios as a versatile tool for intelligent human-agent interaction. Our code and data will be released at our project page.", "sections": [{"title": "1. Introduction", "content": "Human motion encapsulates rich information about action intentions and thought processes of us humans, serving as a crucial foundation for understanding human behavior patterns. Inertia, a contactless and continuously measurable physical quantity from IMUs, can directly reflect the dynamic forces and kinematic states underlying human movement. This measurement enables the reconstruction of physically consistent motion in virtual environments, transforming physical actions into analyzable digital representations. However, merely replicating motion in virtual spaces remains insufficient. Intelligent systems should also provide real-time feedback during human-computer interactions to enhance behavioral understanding and facilitate self-improvement. Therefore, a motion agent capable of real-time motion reconstruction and online behavior analysis becomes vital for various application scenarios such as exercising, rehabilitation, and skill development. Ideally, the capturing and analysis process should be user-friendly, highly accessible, and intuitive for interactions, just like modern conversational AI agents.\nRecent advances in large language models (LLMs), have driven significant progress in multimodal intelligent systems, enabling natural language interaction across text, vision, audio, and even human motion expressed in SMPL parameters. However, they struggle to capture the dynamic forces and torques governing three-dimensional movement. Existing parametric motion representations, though useful for approximating body poses, omit critical temporal derivatives like velocity and acceleration, limiting their capacity to model the physics underlying motion. In contrast, IMUs overcome these limitations by providing wearable, high-frequency measurements of acceleration, angular velocity, and rotation, thereby offering spatio-temporally precise motion characterization. Therefore, for building an intelligent multimodal system that is capable of understanding and analyzing human motions, it is crucial to integrate and align the IMU modality with foundational perception modalities such as natural language. Nonetheless, naively taking SMPL as an intermediary to link raw IMU signals with natural language is sub-optimal, since it inevitably discards certain raw signal patterns during parameterization. Moreover, achieving true multimodal alignment requires deeper integration of IMU data with foundational perception modalities, such as natural language investigated in this work.\nIn the field of computer graphics, IMU has become an essential tool for real-time human motion capture due to its practicality. Unlike camera-based systems for human mesh recovery, IMUs offer sparse, lightweight, occlusion-resistant sensing while preserving user privacy. Early IMU-based methods primarily relied on traditional optimization strategies to estimate human kinematic poses. More recent approaches, termed \u201cinertial posers\", utilized data-driven neural networks to directly translate raw IMU signals into parametric body models, enabling wearable and efficient motion capture. However, existing methods remain limited to motion reconstruction, lacking higher-level analysis and contextual understanding of human movements. Advancing beyond basic capture capabilities, an intelligent motion agent capable of real-time feedback and multi-turn interaction with users could unlock transformative applications in healthcare, education, and digital fitness. For instance, text-based conversational interfaces could democratize access to skill development\u2014novices in specialized exercises or rehabilitation routines might receive instant, tailored guidance through intuitive language interactions. Such a system would reduce learning barriers, lower costs, and enhance accessibility for diverse user groups.\nIn this work, we present Mojito, a novel IMU-based motion intelligence agent for real-time, continuous human motion capture and online analysis. Due to the inherent limitations of IMU sensors such as drift, cumulative errors, and external noise from connectivity or transmission issues, the feasibility of IMU-based systems in reliable and long-term motion analysis is hindered. To address these limitations, we introduce a noise-robust approach that diverges from prior methods. Specifically, instead of continuous representation, we encode IMU signals into a discrete latent space, within which the quantization strategy reduces jitter by mapping continuous IMU streams to fixed token sequences. Additionally, we learn a shared latent space between human motion and IMU data, incorporating with Zipf's law regularization to align the frequency distribution of tokens with the inductive bias of natural language.\nIn order to integrate learned inertial tokens with language vocabulary, it is required to map tokens of multiple modalities into a shared semantic space. However, modern LLMs typically rely on high-dimensional token embeddings (e.g., 3, 584-dimensional text embeddings for Qwen2-7B-Instruct model). Therefore, directly learning the discrete latent space of sparse IMU data on such a high dimension is inefficient. To address this challenge, we pretrain a projection layer composed of 8 Transformer blocks to project low-dimensional inertial tokens onto the LLM's text embedding space. The projected inertial tokens are then concatenated with text tokens and fed into causal language model with masks, enabling whole-\""}, {"title": "2. Related Work", "content": "Inertial Posers. Motion capture solutions using inertial measurement units (IMUs) have gained significant traction in recent years. Commercial products like Noitom and Movella leverage dense IMUs to offer high-quality, portable, and real-time applications. However, the usage of these IMU systems can be cumbersome because they require numerous sensors to be attached to the body, which can be inconvenient and intrusive for users. Since the exploration of SIP , learning-based methods under the sparse sensor configuration (called \u201cinertial poser"}, {"title": "3. Jitter-reduced IMU Tokenizer", "content": "In practical application of long-sequence motion capture and online motion analysis using IMU sensors, device connection, signal transmission and wearing fashion can significantly influence the quality of motion capture and the convenience of user experience. However, it is challenging due to the inherent defects of IMUs such as data drifting and jittery signals. Therefore, we start by proposing a jitter-reduced and motion-aware IMU tokenizer to represent sparse inertial signals by discrete tokens, which can compress continuous IMU signals into a fixed collection of latent codes shared with motion latent space. Consequently, the discrete inertial tokens can be integrated into the vocabulary of LLMs, while also support high-quality motion reconstruction. The IMU tokenizer is built upon the standard VQ-VAE framework , with a novel distribution matching strategy to learn an approximate latent space of corresponding human motion. Additionally, linguistic properties are assigned to the learned inertial tokens through regularization under Zipf's law , which facilitates following semantic alignment with natural language.\nWe first follow MotionGPT to learn a VQ-VAE for human motion. Differently, we represent human motion with a complete state of root joint and foot-ground contacts to suit IMU sensor characteristics. In addition, we involve regularization terms on foot-ground contacts to eliminate jittery results and sliding artifacts in decoded motion.\nWhile HumanML3D establishes an effective motion representation for text-to-motion generation tasks, it is limited to incomplete global dynamics and missing foot-ground contacts. Inspired by HuMoR , we incorporate root translation and angular velocity along all three axes into our representation to improve expressiveness. Specifically, we represent a motion sequence as\n$\\begin{equation}M^{1:T} = [r\\; \\dot{r}\\; \\Phi \\; \\dot{\\Phi} \\; j_r \\; j_p \\; j_{\\dot{r}} \\; p] \\in \\mathbb{R}^{T \\times d_m},\n\\end{equation}$\nwhere T is the sequence length. Within the representation, we first include the root translation $r \\in \\mathbb{R}^{T \\times 3}$, linear velocity $\\dot{r}\\in \\mathbb{R}^{T \\times 3}$, orientation $\\Phi \\in \\mathbb{R}^{T \\times 6}$, and angular velocity $\\dot{\\Phi}\\in \\mathbb{R}^{T \\times 3}$. Then, we use $j_r \\in \\mathbb{R}^{T \\times 6J}$, $j_p \\in \\mathbb{R}^{T \\times 3J}$, $j_{\\dot{r}} \\in \\mathbb{R}^{T \\times 3J}$ to represent local joint rotations, positions, and velocities, respectively. Finally, $p \\in \\mathbb{R}^{T \\times 4}$ records the binary contact labels of toes and heels. Here, $d_m = 271$ is the dimension of our motion representation, and $J = 21$ is the number of local joints. All the rotational parts are in 6D rotation convention .\nGiven a motion sequence $M^{1:T}$, we first encode it into discrete latent codes $Z_{\\text{motion}} \\in \\mathbb{R}^{S \\times d_z}$ using 1D convolution layers, where $d_z = 512$ is the dimension of latent code, and S is the number of the resulting latent codes. We define the hyperparameter $l = \\lceil T/S \\rceil$ as the compression rate for discretization. Following the encoding process, each latent code $z_{\\text{motion}}$ is quantized to a learned codebook $C_{\\text{motion}} \\in \\mathbb{R}^{K \\times d_z}$, where K is the codebook size. The quantization process runs as follows\n$\\begin{equation}b_{\\text{motion}} = \\underset{C_k}{\\text{argmin}} \\; || \\; z_{\\text{motion}} - c_{\\text{motion}} \\; ||^2_2,\n\\end{equation}$\nwhich selects the nearest codebook entry according to Euclidean metric, and results in the motion token sequence $B_{\\text{motion}} \\in \\mathbb{R}^{S \\times d_z}$. Subsequently, $B_{\\text{motion}}$ is fed into the decoder to reconstruct original motion sequence $M^{1:T'}$, with possible truncation $T' = lS$. To train the motion VQ-VAE, we utilize the discrete representation learning objective"}, {"title": "3.2. IMU Tokenizer", "content": "In this subsection, we introduce the jitter-reduced and motion-aware IMU tokenizer. To facilitate the integration of continuous inertial signals with natural language in a manner compatible with large language models (LLMs), we propose a novel approach that encodes inertial signals into discrete tokens, as shown in Fig. 3. These tokens are designed to align seamlessly with the LLM vocabulary, enabling direct incorporation into the language modeling framework. Meanwhile, to empower inertial tokens with the capability of reproducing high-quality 3D motion, we devise a novel distribution matching strategy to approximate the corresponding motion latent space. Therefore, the learned IMU codebook can be utilized for motion reconstruction and analysis.\nIn prior works , inertial signals are considered as the composition of orientation and linear acceleration. However, to fully utilize the sensor measurements of the accelerometer, gyroscope, and magnetometer, we represent an inertia sequence as follows\n$\\begin{equation}I^{1:T} = [q \\; a \\; w] \\in \\mathbb{R}^{T \\times d_u},\n\\end{equation}$\nwhich includes orientation $q \\in \\mathbb{R}^{T \\times 6N}$, free acceleration $a \\in \\mathbb{R}^{T \\times 3N}$ and angular velocity $w \\in \\mathbb{R}^{T \\times 3N}$. In this work, we utilize a configuration of N = 6 IMU sensors. The collected inertial data is represented in the feature space with a dimensionality of $d_u = 72$, capturing comprehensive motion characteristics.\nDue to the scarcity of MoCap data paired with real IMU readings , we simulate synthetic IMU signals on extensive motion data . To model the characteristics of IMU sensors, such as data drift, we follow PNP to use random walk variables to mimic cumulative error. Since acceleration data can fluctuate violently within a wide range, we normalize it to a standard normal distribution using the mean and variance determined on the training dataset. This preprocessing procedure mitigates the impact of high-frequency noise spikes and irregular waves while preserving the drifting feature, improving the learning stability of the tokenizer.\nGiven an inertia sequence $I^{1:T}$, we learn to construct a codebook $C_{\\text{imu}} \\in \\mathbb{R}^{K \\times d_z}$. To be specific, each codebook entry $c_{\\text{imu}}^k$ is updated through"}, {"title": "4. Language Model with Inertial Tokens", "content": "Using the aforementioned IMU tokenizer, our method discretizes continuous and jittery IMU signals into sequential jitter-reduced tokens. However, unlike the high-dimensional embedding space of LLMs, the learned inertial tokens reside in a compact, low-dimensional latent space. Consequently, it is essential to pre-align these inertial tokens with language embeddings to facilitate subsequent multimodal understanding. In this section, we first introduce our method for generating curated textual annotations paired with inertial and motion sequences. Following this, we detail our method for projecting the inertial tokens into the vocabulary space of Qwen2-7B-Instruct language model in Sec. 4.2, and introduce our LoRA model adapters, which enhance the system's professionalism, rationality, and stylization in Sec. 4.3\nTo prepare extensive training data, we instruct GPT-40-mini with carefully designed prompts to automatically rephrase raw textual annotations from original datasets or generate interactive dialogues based on concise action labels. Given the rich diversity of human motion, as illustrated in Fig. 4, we categorize the collected datasets into"}, {"title": "4.2. Projecting Inertial Token to Text Embedding", "content": "Seamless translation and understanding between IMU and textual modalities necessitate a shared and well-aligned embedding space. However, achieving this is challenging due to the significant disparity in embedding dimensions between well-known LLMs and inertial tokens. For instance, the Qwen2-7B-Instruct employs a text embedding dimension $d_h = 3584$, which is substantially larger than $d_z = 512$ dimension of our inertial tokens. To address this issue, we adopt a strategy inspired by OneLLM , introducing a projection module $P_\\theta$ that maps inertial tokens $b_{\\text{imu}}$ into the text embedding space.\nAs illustrated in Fig. 2, our projection module comprises eight transformer blocks followed by a linear layer. Each transformer block incorporates a self-attention layer, a feed forward network, and skip connections, following the architecture of Llama3 , to ensure effective gradient flow. For each inertial token $b_{\\text{imu}}$, the projection module maps it to the text embedding space of Qwen2-7B-Instruct\n$\\begin{equation}e_s = P_{\\theta}(b_{\\text{imu}}) \\in \\mathbb{R}^{d_h}.\n\\end{equation}$\nConcurrently, optional user-provided textual prompts are tokenized and embedded into the same space, after which they are concatenated with projected inertial tokens to form the input for the language model. At this stage, we keep the Qwen2-7B-Instruct model frozen and train only the projection layer $P_\\theta$ using cross-entropy loss. To establish semantic associations between inertial tokens and text, rather than focusing on the intrinsic causality within the inertial token sequence, we employ a training strategy inspired by chatting language models. This involves augmenting mask to all input tokens and excluding them from the loss computation during training."}, {"title": "4.3. Fine-tuning Language Model Adapters", "content": "To further empower our language model with greater flexibility and customization capabilities, we finetune 4 Low-Rank Adaptation (LoRA) adapters. These adapters enable the generation of stylized feedback with character-specific tones, allowing for tailored responses in customized roles. During fine-tuning, both the projection module $P_\\theta$ and the pretrained weights of the Qwen2-7B-Instruct language model remain frozen, with updates applied exclusively to the LoRA adapters. This fine-tuning process enriches the model's linguistic understanding of the same inertial sequences, facilitating personalized and adaptable usage scenarios."}, {"title": "5. Experiments", "content": "In this section, we first introduce motion datasets containing various modalities that we utilize for training and"}, {"title": "5.2. Evaluating Robustness of Motion Capture", "content": "Inertial poser methods based on recurrent neural networks (RNN) and physics solvers have consistently faced significant challenges related to noise sensitivity. IMU signals are highly susceptible to noises introduced by various uncontrolled factors, such as magnetic fields interference, prolonged usage, and sub-optimal sensor placement. As a result, robustness remains a crucial yet unresolved issue for real-world applications. The primary limitation of previous works lies in their reliance on data-driven neural networks trained to map IMU signals directly to human motion through continuous functions. Consequently, when IMU signals contain outliers, these methods fail to effectively eliminate noise, leading to the propagation of signal artifacts into the reconstructed motion results. In contrast, our method addresses this issue by tokenizing continuous IMU signals into discrete tokens through a motion-aware and jitter-reduced IMU tokenizer. The quantization operation within this process effectively mitigates various noisy conditions, enabling the system to filter out irregularities and even tolerate severely corrupted IMU signals. To comprehensively evaluate the robustness of our motion capture, we simulate multiple levels of noisy input conditions by adding random noises to the orientation, acceleration, and gyroscope data of different combinations of IMU configurations. Based on these simulated noisy inputs, we qualitatively and quantitatively demonstrate the advantages of our method over existing approaches.\nAs illustrated in Fig. 7, our method consistently outperforms prior works across various noise levels and configurations. In cases where noised signals affect specific body part (e.g., the left arm highlighted in red), RNN-based and physics solver-based methods frequently generate inaccurate motions. This is attributed to the continuous function mapping learned by their networks, which are inherently sensitive to input outliers. In contrast, the quantization step in our jitter-reduced IMU tokenizer effectively filters out corrupted parts in inputs, enabling accurate motion reconstruction even under imperfect IMU signals. Notably, our method demonstrates robust performance"}, {"title": "6. Discussions and Conclusions", "content": "We have introduced Mojito, an innovative system for real-time human motion capture and online motion analysis, via jitter-reduced inertial tokens. By integrating a novel jitter-reduced and motion-aware IMU tokenizer with a large language model, Mojito establishes an interaction-friendly framework for motion description and instructor applica-"}]}