{"title": "TOPOTUNE: A FRAMEWORK FOR GENERALIZED\nCOMBINATORIAL COMPLEX NEURAL NETWORKS", "authors": ["Mathilde Papillon", "Guillermo Bern\u00e1rdez", "Claudio Battiloro", "Nina Miolane"], "abstract": "Graph Neural Networks (GNNs) excel in learning from relational datasets, pro-\ncessing node and edge features in a way that preserves the symmetries of the\ngraph domain. However, many complex systems such as biological or social\nnetworks-involve multiway complex interactions that are more naturally repre-\nsented by higher-order topological spaces. The emerging field of Topological Deep\nLearning (TDL) aims to accommodate and leverage these higher-order structures.\nCombinatorial Complex Neural Networks (CCNNs), fairly general TDL models,\nhave been shown to be more expressive and better performing than GNNs. How-\never, differently from the graph deep learning ecosystem, TDL lacks a principled\nand standardized framework for easily defining new architectures, restricting its\naccessibility and applicability. To address this issue, we introduce Generalized\nCCNNS (GCCNs), a novel simple yet powerful family of TDL models that can be\nused to systematically transform any (graph) neural network into its TDL coun-\nterpart. We prove that GCCNs generalize and subsume CCNNs, while extensive\nexperiments on a diverse class of GCCNs show that these architectures consistently\nmatch or outperform CCNNs, often with less model complexity. In an effort to\naccelerate and democratize TDL, we introduce TopoTune, a lightweight software\nthat allows practitioners to define, build, and train GCCNs with unprecedented\nflexibility and ease.", "sections": [{"title": "INTRODUCTION", "content": "Graph Neural Networks (GNNs) (Scarselli et al., 2008; Corso et al., 2024) have demonstrated\nremarkable performance in several relational learning tasks by incorporating prior knowledge through\ngraph structures (Kipf & Welling, 2017; Zhang & Chen, 2018). However, constrained by the pairwise\nnature of graphs, GNNs are limited in their ability to capture and model higher-order interactions-\ncrucial in complex systems like particle physics, social interactions, or biological networks (Lambiotte\net al., 2019). Topological Deep Learning (TDL) (Bodnar, 2023) precisely emerged as a framework that\nnaturally encompasses multi-way relationships, leveraging beyond-graph combinatorial topological\ndomains such as simplicial and cell complexes, or hypergraphs (Papillon et al., 2023).\nIn this context, Hajij et al. (2023) has recently introduced combinatorial complexes, fairly general\nobjects that are able to model arbitrary higher-order interactions along with a hierarchical organiza-\ntion among them \u2013hence generalizing (for learning purposes) most of the combinatorial topological\ndomains within TDL, including graphs. The elements of a combinatorial complex are cells, being\nnodes or groups of nodes, which are categorized by ranks. The simplest cell, a single node, has rank\nzero. Cells of higher ranks define relationships between nodes: rank one cells are edges, rank two\ncells are faces, and so on. Hajij et al. (2023) also proposes Combinatorial Complex Neural Networks\n(CCNNs), machine learning architectures that leverage the versatility of combinatorial complexes to\nnaturally model higher-order interactions. For instance, consider the task of predicting the solubility\nof a molecule from its structure. GNNs model molecules as graphs, thus considering atoms (nodes)\nand bonds (edges) (Gilmer et al., 2017). By contrast, CCNNs model molecules as combinatorial\ncomplexes, hence considering atoms (nodes, i.e. cells of rank zero), bonds (edges, i.e. cells of rank"}, {"title": "BACKGROUND", "content": "To properly contextualize our work, we revisit in this section the fundamentals of combinatorial\ncomplexes and CCNNs\u2014closely following the works of Hajij et al. (2023) and Battiloro et al.\n(2024) as well as the notion of augmented Hasse graphs.\nCombinatorial Complex. A combinatorial complex is a triple (V, C, rk) consisting of a set V, a\nsubset C of the powerset P(S)\\{0\\}, and a rank function rk : C \u2192 Z>0 with the following properties:\n1. for all v \u2208 V, \\{v\\} \u2208 C and rk(\\{v\\}) = 0;\n2. the function rk is order-preserving, i.e. if \u03c3, \u03c4 \u2208 C satisfy \u03c3 \u2286 \u03c4, then rk(\u03c3) \u2264 rk(\u0442).\nThe elements of V are the nodes, while the elements of C are called cells (i.e. group of nodes). The\nrank of a cell \u03c3\u2208 C is k := rk(\u03c3), and we call it a k-cell. C simplifies notation for (V, C, rk), and its\ndimension is defined as the maximal rank among its cell: dim(C) := maxo\u2208c rk(\u03c3).\nNeighbhorhoods. Combinatorial complexes can be equipped with a notion of neighborhood among\ncells. In particular, a neighborhood N : C \u2192 P(C) on a CC C is a function that assigns to each cell\noin C a collection of \u201cneighbor cells\" N(0) C CU\u00d8. Examples of neighborhood functions are\nadjacencies, connecting cells with the same rank, and incidences, connecting cells with different\nconsecutive ranks. Usually, up/down incidences N1,\u2191 and N1,\u2193 are defined as\n$\u039d\u2081\u2081\u2081(\u03c3) = \\{\u03c4\u2208 C | rk(t) = rk(\u03c3) + 1,\u03c3\u2282\u03c4\\}, N1,\u2193(\u03c3) = \\{t \u2208 C | rk(t) = rk(\u03c3) \u2212 1, \u03c4 \u2282 \u03c3\\}.$\nTherefore, a k + 1-cell 7 is a neighbor of a k-cell o w.r.t. to N1,\u2191 if o is contained in 7; analogously,\nak - 1-cell is a neighbor of a k-cell o w.r.t. to N1, if 7 is contained in 0. These incidences induce\nup/down adjacencies Na,\u2191 and Na,\u2191 as\n$\u039d\u0391,\u2191(\u03c3) = \\{\u03c4\u2208 C | rk(t) = rk(\u03c3), \u2203d \u2208 C : rk(t) = rk(\u03c3) + 1, \u0442 C \u03b4, and \u03c3 \u2282 \u03b4\\},\n\u039d\u0391,\u2193(\u03c3) = \\{\u03c4\u2208 C | rk(t) = rk(\u2642), \u2203\u03b4 \u2208 C : rk(d) = rk(\u2642) \u2212 1, \u0431 \u0421 \u0442, and \u03b4 \u2282 \u03c3\\}.$\nTherefore, a k-cell 7 is a neighbor of a k-cell o w.r.t. to NA,\u2191 if they are both contained in a k + 1-cell\n\u03b4; analogously, a k-cell t is a neighbor of a k-cell o w.r.t. to Na,\u2193 if they both contain a k \u2013 1-cell d.\nOther neighborhood functions can be defined for specific applications (Battiloro et al., 2024)."}, {"title": "MOTIVATION", "content": "As outlined in the introduction, TDL lacks a comprehensive framework for easily creating and\nexperimenting with novel topological architectures-unlike the more established GNN field. Addi-\ntionally, a framework of this kind should ensure that its induced architectures retain the expressivity,\nequivariance, and generality of their TDL counterpart-regardless of the topological domain they\nare defined onto. This section outlines some previous works that have laid important groundwork in\naddressing this challenge.\nFormalizing CCNNs on graphs. The position paper (Veli\u010dkovi\u0107, 2022) proposed that any function\nover a combinatorial complex can be computed via message passing over a transformed graph, but\nwithout specifying how to design GNNs that reproduce CCNNs. Later, (Hajij et al., 2023) proposed\nthat, given a combinatorial complex C and a collection of neighborhoods Nc, a message-passing\nGNN that runs over the augmented Hasse graph GNe is equivalent to a specific CCNN as in (3)\nrunning over C using\n\u2022 Ne as collection of neighborhoods:\n\u2022 same intra- and inter-aggregations, i.e.,=.\n\u2022 and no rank- and neighborhood-dependent message functions, i.e., \u03a8N,rk(:) = \u03c8 \u2200N \u2208 Nc."}, {"title": "GENERALIZED COMBINATORIAL COMPLEX NEURAL NETWORKS", "content": "We propose Generalized Combinatorial Complex Neural Networks (GCCNs), a novel broad class of\nTDL architecture. GCCNs overcome the limitations of previous graph-based TDL architectures by\nleveraging the notions of strictly augmented Hasse graphs and per-rank neighborhoods.\nEnsemble of Strictly Augmented Hasse Graphs. This graph expansion method (see Fig. 3)\nextends from the the established definition of an augmented Hasse graph (see Fig. 2). Specifically,\ngiven a combinatorial complex C and a collection of neighborhood functions Ne, we expand it into\nNc graphs, each of them representing a neighborhood N \u2208 Nc. In particular, the strictly augmented\nHasse graph GN = (CN, En) of a neighborhood N \u2208 Ne is a directed graph whose nodes CN and\nedges EN are given by:\n$CN = \\{\u03c3\u2208 C | N(\u03c3) \u2260 0\\}, Ex = \\{(\u03c4,\u03c3)|\u03c4\u03b5 \u039d\u0384(\u03c3)\\} .$\nFollowing the same arguments from Hajij et al. (2023), a GNN over the strictly augmented Hasse\ngraph GN induced by N is equivalent to a CCNN running over C and using Nc = \\{N\\} up to the\n(self-)update of the cells in C/CN.\nPer-rank Neighborhoods. The standard definition of adjacencies and incidences given in Section 2\nimplies that they are applied to each cell regardless of its rank. For instance, consider a combinatorial\ncomplex of dimension two with nodes (0-cells), edges (1-cells), and faces (2-cells).\n\u2022 Employing the down incidence N\u012b,\u2193 as in (1) means the edges must exchange messages with their\nendpoint nodes, and faces must exchange messages with the edges on their sides. It is impossible\nfor edges to exchange messages while faces do not.\n\u2022 Employing the up adjacency Na,\u2191 as in (2) means the nodes must exchange messages with other\nedge-connected nodes, and edges must exchange messages the other edges bounding the same\nfaces. It is impossible for nodes to exchange messages while edges do not.\nThis limitation increases the computational burden of standard CCNNs while not always increasing\nthe learning performance, as we will show in the numerical results. For this reason, we introduce"}, {"title": "GENERALIZED COMBINATORIAL COMPLEX NETWORKS", "content": "We formally introduce a broad class of novel\nTDL architectures called Generalized Combinatorial Complex Networks (GCCNs), depicted in Fig.\n1. Let C be a combinatorial complex containing |C| cells and Nc a collection of neighborhoods on it.\nAssume an arbitrary labeling of the cells in the complex, and denote the i-th cell with \u03c3\u03c4. Denote by\nH\u2208 R|C|\u00d7F the feature matrix collecting some embeddings of the cells on its rows, i.e., [H]\u00bf = h\u01a1i,\nand by HN E RICN|\u00d7F the submatrix containing just the embeddings of the cells belonging to the\nstrictly augmented Hasse graph Gy of N. The l-th layer of a GCCN updates the embeddings of the\ncells H\u2208 RICI\u00d7Flas\n$Hl+1 = \u03a6(H,  \\underset{NENC}{\\text{\\Huge \u2611}} WN(HN,GN) ) ER|C|\u00d7Fl+1,$\nwhere H\u00ba collects the initial features, and the update function @ is a learnable row-wise update func-\ntion, i.e. [$(A, B)]i = $([A], [B]i). The neighborhood-dependent sub-module WN : R|CN|\u00d7Fl \u2192\n]R|CN|\u00d7Fl+1, which we refer to as the neighborhood message function, is a learnable (matrix) function\nthat takes as input the whole strictly augmented Hasse graph of the neighborhood, GN and the\nembeddings of the cells that are part of it, and gives as output a processed version of them. Finally,\nthe inter-neighborhood aggregation module \u2611 synchronizes the possibly multiple neighborhood\nmessages arriving on a single cell across multiple strictly augmented Hasse graphs into a single mes-\nsage. In this way, the embedding of a cell collects information about the whole relational structures\ninduced by each (nonempty) neighborhood.\nTheoretical properties of GCCNs.\n1. Generality. GCCNs formally generalize CCNNS.\nProposition 1. Let C be a combinatorial complex. Let Nc be a collection of neighbor-\nhoods on C. Then, there exists a GCCN that exactly reproduces the computation of a\nCCNN over C using Nc.\nProof. See Appendix A.1.\n2. Permutation Equivariance. Generalizing CCNNs, GCCNs layers are equivariant with\nrespect to the relabeling of cells in the combinatorial complex.\nProposition 2. A GCCN layer is cell permutation equivariant if the neighborhood mes-\nsage function is node permutation equivariant and the inter-neighborhood aggregator is\ncell permutation invariant.\nProof. See Appendix A.2\n3. Expressivity. The expressiveness of TDL models is tied to their ability to distinguish\nnon-isomorphic graphs. Variants of the Weisfeiler-Leman (WL) test, like the cellular\nWL for cell complexes (Bodnar et al., 2021a), set upper bounds on their corresponding\nTDL models' expressiveness, as the WL test does for GNNs (Xu et al., 2019a).\nProposition 3. Let C be a combinatorial complex and Nc be a collection of neighbor-\nhoods. There exist GCCNs being as expressive as CCNNs.\nProof. See Appendix A.3.\nGiven Proposition 1, GCCNs allow us to define general TDL models using any neighborhood message\nfunction w, such as any GNN. Not only does this framework avoid having to approximate CCNN\ncomputations, as is the case in previous works (Jogl et al., 2022b;a; 2023), but it also enjoys the\nsame permutation equivariance as regular CCNNs (Proposition 2). Differently from the work in\n(Hajij et al., 2023), the fact that GCCNs can have arbitrary neighborhood message functions implies\nthat non message-passing TDL models can be readily defined (e.g., by using non message-passing\nmodels as neighborhood message functions). Moreover, the fact that the whole strictly augmented\nHasse graphs are given as input enables also the usage of multi-layer GNNs as neighborhood message\nfunctions. To the best of our knowledge, GCCNs are the only objects in the literature that encompass\nall the above properties."}, {"title": "TOPOTUNE", "content": "Our proposed methodology, together with its resulting GCCNs architectures, addresses the challenge\nof systematically generating principled, general TDL models. Here, we introduce TopoTune, a\nsoftware module for defining and benchmarking GCCN architectures on the fly\u2014a vehicle for accel-\nerating and democratizing TDL research. TopoTune is made available as part of TopoBenchmarkX\nTelyatnikov et al. (2024) at github.com/pyt-team/TopoBenchmarkX. This section details TopoTune's\nmain features.\nChange of Paradigm. TopoTune introduces a new perspective on TDL through the concept of\n\"neighborhoods of interest,\" enabling unprecedented flexibility in architectural design. Previously\n\"fixed\" components of CCNNs become hyperparameters of our framework. Even the choice of\ntopological domain becomes a mere variable, representing a new paradigm in the design and imple-\nmentation of TDL architectures.\nAccessible TDL. Using TopoTune, a practitioner can instantiate customized GCCNs simply by\nmodifying a few lines of a configuration file. In fact, it is sufficient to specify (i) a collection of\nper-rank neighborhoods Nc, (ii) a neighborhood message function wy, and optionally (iii) some\narchitectural parameters-e.g. the number l of GCCN layers. For the neighborhood message\nfunction ww, the same configuration file enables direct import of models from standard PyTorch\nlibraries, including PyTorch Geometric (Fey & Lenssen, 2019) and Deep Graph Library (Chen et al.,\n2020b). TopoTune's simplicity provides both newcomers and TDL experts with an accessible tool for\ndefining higher-order topological architectures.\nAccelerating TDL Research. TopoTune is fully integrated into TopoBenchmarkX (Telyatnikov\net al., 2024). This package provides standardized methods for handling combinatorial complex\nobjects, including lifting procedures to lift them from graph-based data. TopoBenchmarkX also\nsimplifies the training and benchmarking of GCCNs, offering a variety of ready-to-use tasks, datasets,\nand evaluation metrics. Together, TopoTune and TopoBenchmarkX provide all the necessary tools to\ndefine, train, test, and compare a wide range of novel GCCN architectures, bringing unprecedented\nversatility and standardization to accelerate TDL research."}, {"title": "EXPERIMENTS", "content": "We present experiments showcasing a broad class of GCCN's constructed with TopoTune. These\nmodels consistently match, outperform, or improve upon existing CCNNs, often with smaller model\nsizes. TopoTune's integration into the TopoBenchmarkX experiment infrastructure ensures a fair\ncomparison with CCNNs from the literature, as implementations of data processing, domain lifting,\nand training are homogeonized. Moreover, TopoBenchmarkX aggregates TDL's leading open-source\nmodels from TopoX, making it an ideal framework for objectively evaluating GCCNs against the\ncurrent state of the field."}, {"title": "EXPERIMENTAL SETUP", "content": "We generate our library by considering ten possible choices of neighborhood structure Nc (including\nboth regular and per-rank, see Appendix C.1) and five possible choices of ww: GCN (Kipf & Welling,\n2017), GAT (Velickovic et al., 2017), GIN (Xu et al., 2019b), GraphSAGE (Hamilton et al., 2017),\nand Transformer (Vaswani et al., 2017). We import these models directly from PyTorch Geometric\n(Fey & Lenssen, 2019) and PyTorch Paszke et al. (2019). TopoTune enables running GCCNs on both\ngraph expansions previously discussed: an ensemble of strictly augmented Hasse graphs (eq. 5) and\na single augmented Hasse graph (eq. 4). In the latter case, each GCCN layer simply contains a single\n\u03c9\u03bd (e.g. GNN or Transformer) which processes the single augmented Hasse graph."}, {"title": "RESULTS AND DISCUSSION", "content": "GCCNs outperform CCNNs. Table 1 portrays a cross-comparison between top-performing CCNN\nmodels and our class of GCCNs. GCCNs outperform CCNNs in the simplicial and cellular domains\nacross all datasets. Notably, GCCNs in these domains achieve comparable results to hypergraph\nCCNNs, a feat unattainable by existing CCNNs in node-level tasks. Moreover, we find both of\nGCCN's underlying notions (see Section 4) to be advantageous: (i) Table 1 shows that representing\na complex as an ensemble of augmented Hasse graphs consistently yields better performance than\nusing a single augmented Hasse graph. (ii) Some GCCNs with a per-rank neighborhood structure\noutperform not only CCNNs but also all other GCCNs using regular neighborhoods. For example,\nthis is the case for a cellular GCCN with w on MUTAG. Its lightweight, per-rank neighborhood\nstructure makes it 19% the size of the best cellular CCNN on this task.\nGCCNs are smaller than CCNNs. GCCNs are generally more parameter efficient than existing\nCCNNs in simplicial and cellular domains, and in some instances (MUTAG, NCI1, NCI09), even\nsurpass hypergraph CCNNs in size efficiency. Even as GCCNs become more resource-intensive\nfor large graphs with high-dimensional embeddings\u2014as seen in node-level tasks\u2014they maintain a\ncompetitive edge. For instance, on the Citeseer dataset, a GCCN (\u03c9\u03bd = GraphSAGE) outperforms\nthe best existing CCNN while being 28% smaller. We refer to Table 4 in Appendix D.\nGCCNs improve existing CCNNs. TopoTune makes it easy to iterate upon and improve preexisting\nCCNNs by replicating their architecture in a GCCN setting. For example, TopoTune can generate\na counterpart GCCN by replicating a CCNN's neighborhood structure, aggregation, and training\nscheme. We show in Table 2 that counterpart GCCNs often achieve comparable or better results than\nSCCN (Yang et al., 2022) and CWN (Bodnar et al., 2021a) just by sweeping over additional choices\nof ww (same as in Table 1) available in PyTorch Geometric (Fey & Lenssen, 2019). In the single\naugmented Hasse graph regime, GCCN models are consistently more lightweight, up to half their\nsize (see Table 5 in Appendix D for details)."}, {"title": "CONCLUSION", "content": "This work introduces a simple yet powerful graph-based methodology for constructing Generalized\nCombinatorial Complex Neural Networks (GCCNs), TDL architectures that generalize and subsume\nstandard CCNNs. Additionally, we introduce TopoTune, the first lightweight software module for\nsystematically and easily implementing new TDL architectures, all the while leveraging existing\nGNN (or any other models') software. Building upon this foundation, we envision several promising\ndirections for future research. Application-specific TDL could seek to customize GCCNs for poten-\ntially sparse or multimodal datasets. Additionally, TopoTune's graph-based approach paves the way\nfor extending state-of-the-art GNN architectures beyond those currently implemented in open-source\nlibraries such as PyTorch Geometric. We hope TopoTune will not only accelerate TDL research but\nalso help bridge the gap with other machine learning fields."}, {"title": "ADDITIONAL DETAILS ON EXPERIMENTS", "content": "In this section, we delve into the details of the datasets, hyperparameter search methodology, and\ncomputational resources utilized for conducting the experiments."}, {"title": "NEIGHBORHOOD STRUCTURES", "content": "In order to build a broad class of GCCNs, we consider X different neighborhood structures on\nwhich we perform graph expansion. Importantly, three of these structures are lightweight, per-rank\nneighborhood structures, as proposed in Section 4. The neighborhood structures are:"}, {"title": "DATASETS", "content": "Table 3 provides the statistics for each dataset lifted to three topological domains: simplicial complex,\ncellular complex, and hypergraph. The table shows the number of 0-cells (nodes), 1-cells (edges),\nand 2-cells (faces) of each dataset after the topology lifting procedure. We recall that:\n\u2022 the simplicial clique complex lifting is applied to lift the graph to a simplicial domain, with\na maximum complex dimension equal to 2;\n\u2022 the cellular cycle-based lifting is employed to lift the graph into the cellular domain, with\nmaximum complex dimension set to 2 as well."}, {"title": "HYPERPARAMETER SEARCH", "content": "Five splits are generated for each dataset to ensure a fair evaluation of the models across domains.\nEach split comprises 50% training data, 25% validation data, and 25% test data. An exception is\nmade for the ZINC dataset, where predefined splits are used (Irwin et al., 2012).\nTo avoid the combinatorial explosion of possible hyperparameter sets, we fix the values of all\nhyperparameters beyond GCCNs: hence, to name a few relevant parameters, we set the learning"}, {"title": "HARDWARE", "content": "The hyperparameter search is executed on a Linux machine with 256 cores, 1TB of system memory,\nand 8 NVIDIA A100 GPUs, each with 80GB of GPU memory."}, {"title": "MODEL SIZE", "content": "We provide details on model size for reported results in Section 6."}, {"title": "PROOFS", "content": null}, {"title": "PROOF OF GENERALITY", "content": "The proof is trivial. It is sufficient to set wn(H'w, GN) to {\u2295y\u2208N(6) \u03a8N,rk(o) (h, h\u00b2)}\u03c3\u2208c in (8)\nas all y \u2208 N(\u03c3) are part of the node set C\u221a of the strictly augmented Hasse graph of N by definition."}, {"title": "PROOF OF EQUIVARIANCE", "content": "As for GNNs, an amenable property for GCCNNs is the awareness w.r.t. relabeling of the cells. In\nother words, given that the order in which the cells are presented to the networks is arbitrary -because\nCCs, like (undirected) graphs, are purely combinatorial objects-, one would expect that if the order\nchanges, the output changes accordingly. To formalize this concept, we need the following notions.\nMatrix Representation of a Neighborhood. Assume again to have a combinatorial complex C\ncontaining C := |C| cells and a neighborhood function N on it. Assume again to give an arbitrary\nlabeling to the cells in the complex, and denote the i-th cell with \u01a1i. The matrix representation of\nthe neighborhood function is a matrix N\u00eb \u2208 RC\u00d7C such that Ni,j = 1 if the \u03c3; \u2208 \u039d(oi) or zero\notherwise. We notice that the submatrix \u00d1\u201e \u2208 R|C|\u00d7|CN| obtained by removing all the zero rows\nand columns is the adjacency matrix of the strictly augmented Hasse graph GCN induced by N.\nPermutation Equivariance. Let C be combinatorial complex, Nc a collection of neighborhoods on it,\nand N = {NN}NEN, the set collecting the corresponding neighborhood matrices. Let P\u2208 RCXC\nbe a permutation matrix. Finally, denote by PH the permuted embeddings and by {PN&PT}N\u2208Nc,\nthe permuted neighborhood matrices. We say that a function f : (H\u00b9, B) \u2192 H+1 is cell permutation\nequivariant if f (PH', {PN&PT}N\u2208Nc) = Pf (H', {N}Nee) for any permutation matrix\nP. Intuitively, the permutation matrix changes the arbitrary labeling of the cells, and a permutation\nequivariant function is a function that reflects the change in its output.\nProof of Proposition 2. We follow the approach from (Bodnar et al., 2021a). Given any permuta-\ntion matrix P, for a cell oi, let us denote its permutation as op(i) with an abuse of notation. Let\nh+1 be the output embedding of cell \u03c3\u03b5 for the l-th layer of a GCCN taking (H\u00b9, {N}N\u2208N\nas input, and hop) h+1 be the output embedding of cell op(i) for the same GCCN layer taking\n(PH\u00b9, {PNNPT}N\u2208Ne) as input. To prove the permutation equivariance, it is sufficient to show\nthat h+1 = h+1 hop a the update function @ is row-wise, i.e. it independetly act on each cell. To do\nso, we show that the (multi-)set of embeddings being passed to the neighborhood message function,\naggregation, and update functions are the same for the two cells \u03c3\u03b5 and op(i). The neighborhood\nmessage functions act on the strictly augmented Hasse graph of Gcn of N, thus we work with the\nsubmatrix Ny. The neighborhood message function is assumed to be node permutation equivariant,\ni.e., denoting again the embeddings of the cells in Ger with Her \u2208 R|CM|\u00d7F\u00b2 and identifying\nGCN with NN, it holds that WN (PCN HCN, PCNNNPTN) = PCNWN (HCN) CN, NN), where PCN is\nthe submatrix of P given by the rows and the columns corresponding to the cells in GCN. This\nassumption, together with the assumption that the inter-neighborhood aggregation is assumed to be\ncell permutation invariant, i.e. NENG PCNWN (HCN, NN) = NENO WN (HCN, N\u221a), trivially\nmakes the overall composition of the neighborhood message function with the inter-neighborhood ag-\ngregation cell permutation invariant. This fact, together with the fact that the (labels of) the neighbors\nof the cell oi in N are given by the nonzero elements of the i-th row of Ny, or the corresponding\nrow of Ny, and that the columns and rows of N\u221a are permuted in the same way the rows of the\nfeature matrix He are permuted, implies\n$[\u00d1N]i,j = [Pcx\u00d1NPEN]PCN (i),Pcy (j),$\nthus that \u03c3\u03b5 and \u03c3p(i) receive the same neighborhood message from the neighboring cells in N, for\nall N\u2208 Nc."}, {"title": "PROOF OF EXPRESSIVITY", "content": "Proof of Proposition 3. The proof is pretty straightforward. Proposition 1 showed that GCCNS\nsubsume CCNNs, thus there exist GCCNs as expressive as CCNNs. Furthermore, we do not fix a\nsingle expressivity criterion (i.e., a specific variant of the WL test), as the work in (Jogl et al., 2024)\nshowed that several GNNs and TDL models can be expressed as CCNNs, each of them using a\ndifferent variant of the WL test as a metric for expressivity. We refer to Corollary 3.6 in (Jogl et al.,\n2024).\nRemark. In proving Proposition 3, we kept the description of the expressivity metric high-level\non purpose. In particular, discussing in details the equivalence between a relational structure from\n(Jogl et al., 2024) and a combinatorial complex is out of the scope of this paper, but a reader can\nimmediately spot it with a quick read. Our goal, here, is to prove that GCCNs preserve expressivity,\nwithout deriving to what extent in terms of architectural design. However, an in-depth study on how\nthe definition of the ww's relates to the notions of strong and weak simulation from (Jogl et al., 2024)\nis an interesting venue, that we plan to explore in future works."}, {"title": "SOFTWARE", "content": "Algorithm 1 shows how the TopoTune module instantiates a GCCN by taking a choice of model\n\u03c9\u03bd and neighborhoods Ne as input. Given an input complex x, TopoTune first expands it into an\nensemble of strictly augmented Hasse graphs that are then passed to their respective \u03c9\u03c2 models\nwithin each GCCN layer.\nRemark. We decided to design the software module of TopoTune, i.e., how to implement GCCNs, as\nwe did for mainly two reasons: (i) the full compatibility with TopoBenchmarkX (implying consistency\nof the combinatorial complex instantiations and the benchmarking pipeline), and (ii) the possibility of\nusing GNNs as neighborhood message functions that are not necessarily implemented with a specific\nlibrary. However, if the practitioner is interested in entirely wrapping the GCCN implementation into\nPytorch Geometric or DGL, they can do it by noticing that a GCCN is equivalent to a heterogeneous\nGNN where the heterogeneous graph the whole augmented Hasse graph, with node types given by\nthe rank of the cell (e.g. 0-cells, 1-cells, and 2-cells) while the edge type is given by the per-rank\nneighborhood function (e.g. \"0-cells to 1-cells\" or \"2-cells to 1-cells\" for N\u2081\u2081 and N\u00b2, respectively)."}, {"title": "TopoTune", "content": null}, {"title": "EXPERIMENTAL SETUP", "content": null}, {"title": "RESULTS AND DISCUSSION", "content": null}, {"title": "PERFORMANCE VERSUS MODEL SIZE", "content": "We show the plots similar to Fig. 5 for all datasets. Again here, the best model determines the amount\nof GCCN layers and GNN sublayers we keep constant."}]}