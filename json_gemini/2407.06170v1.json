{"title": "Real-Time Spacecraft Pose Estimation Using Mixed-Precision Quantized Neural Network on COTS Reconfigurable MPSoC", "authors": ["Julien Posso", "Guy Bois", "Yvon Savaria"], "abstract": "This article presents a pioneering approach to real-time spacecraft pose estimation, utilizing a mixed-precision quantized neural network implemented on the FPGA components of a commercially available Xilinx MPSoC, renowned for its suitability in space applications. Our co-design methodology includes a novel evaluation technique for assessing the layer-wise neural network sensitivity to quantization, facilitating an optimal balance between accuracy, latency, and FPGA resource utilization. Utilizing the FINN library, we developed a bespoke FPGA dataflow accelerator that integrates on-chip weights and activation functions to minimize latency and energy consumption. Our implementation is 7.7 times faster and 19.5 times more energy-efficient than the best-reported values in the existing spacecraft pose estimation literature. Furthermore, our contribution includes the first real-time, open-source implementation of such algorithms, marking a significant advancement in making efficient spacecraft pose estimation algorithms widely accessible. The source code is available at https://github.com/possoj/FPGA-SpacePose.", "sections": [{"title": "I. INTRODUCTION", "content": "Estimating the relative pose (position and orientation) of a known, uncooperative spacecraft from a monocular image is a crucial task in computer vision. It aims to enhance the autonomy of in-orbit spacecraft operations such as formation flying, autonomous docking, satellite maintenance, and debris removal [1]. The upcoming ClearSpace-1 mission, scheduled for 2026, underscores the significance of debris removal for space sustainability and will represent the first instance of a space mission that integrates a vision-based pose estimation algorithm [2].\n\nThe challenge posed by the European Space Agency (ESA) [3] has significantly boosted interest in spacecraft pose es-timation (SPE). It focuses on estimating the pose of the Tango spacecraft using the Spacecraft Pose Estimation Dataset (SPEED) [4], which has become a benchmark for training and evaluating SPE algorithms. This dataset is utilized in our study.\n\nTwo prominent techniques based on convolutional neural networks (CNNs) have emerged as highly effective in the SPE domain: keypoint detection [3], [5]\u2013[9] and a combination of soft classification with direct regression [1], [10]. However, challenges persist in the area of real-time SPE inference, with only a few publications addressing this issue [6], [11]\u2013[13].\n\nThis paper proposes a pioneering approach to real-time SPE on a commercial off-the-shelf (COTS) Xilinx MPSoC, already utilized in the space domain [14]. We begin with a trained Float32 Mobile-URSONet model, featuring an embedded-friendly MobileNetV2 backbone [10]. Mixed-precision quantization is used to find an optimal balance between accuracy, latency, and FPGA resource utilization for each neural network layer. We develop a custom energy-efficient dataflow accelerator and implement it on the programmable logic available on the Xilinx MPSoC, commonly called FPGA. This co-design methodology aims to provide high-quality SPE algorithms executed on low-cost, low-power, space-ready embedded computers. The contributions of our paper include:\n\n\u2022 The first real-time inference of a popular SPE neural network on a space-ready Xilinx MPSoC. Our FPGA dataflow accelerator is 7.7 times faster and 19.5 times more energy-efficient than the best-reported values in the existing SPE literature.\n\n\u2022 An in-depth study of layer-wise mixed-precision quanti-zation for the SPE neural network. This study leads us to propose a method for selecting the bit-width of the mixed-precision neural network.\n\n\u2022 The first open-source, real-time implementation of an SPE neural network [15]."}, {"title": "II. RELATED WORKS", "content": "Recent advancements in spacecraft pose estimation (SPE) have been driven by keypoint detection [3], [5], [7]\u2013[9] and soft classification techniques [1], [10]. Keypoint detection involves identifying and locating a priori defined landmarks of the target within an image, and combining them with a 3D model of these landmarks to recover the pose [3]. In contrast, soft classification assigns a probability distribution across predefined orientation classes in a discrete output space. When combined with direct regression for position estimation, this method has demonstrated the best generalization capa-bilities [10] and the potential to extend pose estimation to unknown targets [1]. However, the neural network complexity and embeddability balance have received less attention. The works of [6] and [10] have significantly contributed to the em-beddability of SPE algorithms by leveraging the MobileNetV2 architecture [16]. We argue that quantizing these previously reported models would further enhance their embeddability. Our paper primarily focuses on Mobile-URSONet [10], an open-source framework, but the same methodology could be applied to the work presented in [6].\n\nMixed-precision quantization [17]\u2013[22] has emerged as a superior method for enhancing neural networks' latency and energy efficiency beyond uniform Int8 quantization, primarily applied to benchmark datasets like CIFAR, ImageNet, and VOC. Uniquely, [23] introduced a module-wise quantization tailored for SPE but did not achieve real-time inference on embedded computers.\n\nPrevious research on real-time embedded SPE has made significant strides, as seen in works by [6], [13], [11], and [12], employing various hardware and quantization strate-gies. However, these studies often lack detailed methodology, particularly regarding quantization's effects on accuracy and efficiency, and frequently omit source code, hindering repro-ducibility. The absence of exhaustive performance metrics such as throughput and power consumption also restricts the understanding of their practical efficiency and applicability. Our research addresses these gaps by prioritizing methodolog-ical transparency, reproducibility, and comprehensive perfor-mance evaluation."}, {"title": "III. CO-DESIGN METHODOLOGY", "content": "Figure 1 outlines our proposed co-design methodology, which achieves an optimal balance between accuracy, latency, and FPGA resource utilization. The process begins with the Float32 weights of Mobile-URSONet [10]. Using Brevitas [24], we quantize the network for Quantization-Aware Train-ing (QAT) employing per-channel symmetric uniform quan-tization with layer-wise arbitrary precision. Over 20 epochs, we trained 146 model variants to optimize bit-widths across layers, a process detailed in Section III-C. Although such an exhaustive approach would be impractical for large datasets like ImageNet [17], [18], it was feasible within a week for a smaller dataset using an Nvidia P100 GPU. This efficiency was partly due to acceleration from Float32 pre-training, which reduced QAT time by a factor of three. Subsequently, we con-verted the neural network into a FINN-ONNX graph, which was then transformed into FINN C++ High-Level Synthesis (HLS)-compatible nodes [25], ensuring a one-to-one corre-spondence between the nodes and the neural network layers, including im2col and matrix-vector multiplication nodes for convolutions. Our custom folding algorithm was designed to optimize node parallelism, minimizing FPGA resource usage while meeting our target latency constraints, in line with FINN's inter-layer constraints. Through Python simulations, we fine-tuned FIFO depths to ensure optimal data flow, a process elaborated in Section III-D. Post-synthesis, meeting FPGA resource constraints may require design optimizations, such as reducing parallelism or adjusting the QAT bit-widths.\n\nThe final outcome is an accelerator pipeline, with one acceler-ator per CNN layer, featuring on-chip weights and activations to minimize both energy use [26] and latency, compared to conventional accelerators [27]."}, {"title": "B. Neural Network Architecture", "content": "Mobile-URSONet leverages a MobileNetV2 backbone [10], beginning with a 3x3 convolutional layer, followed by 17 inverted residual blocks, and ending with a 1x1 convolutional layer. We have tailored the architecture of the inverted residual blocks, as illustrated in Figure 2, by integrating linear acti-vations with shared scaling factors. This adaptation enables quantized tensor summation and allows for entirely integer-based computations throughout the network after applying FINN transformations. Moreover, we maintain true residual connections in the neural network, contrary to the FINN au-thors who insert activation functions or convolutions in every shortcut to facilitate the transformation to HLS-compatible nodes at the cost of the network's simplicity [28]. While the parameter count of MobileNetV2 increases across its layers, the number of Multiply-Accumulate (MAC) operations remains comparatively consistent. This stability is attributed to the feature maps' diminishing dimensions (width and height) as the network deepens, counterbalancing the increase in parameter count."}, {"title": "C. Mixed-Precision Quantization", "content": "This section proposes a method to assess each layer's sensitivity to low-bit quantization by setting all layers to 8-bit weights and activations, except for one layer's binarized weights. In Figure 3a, we observe that layers closer to the input of the neural network generally exhibit greater sensitivity to quantization. This trend is consistent with Figure 3b, which shows that sensitivity to low-bit quantization decreases with the parameter count of a layer, and the network's parame-ter count grows across its layers. The considerable volume of MAC operations in the early layers, driven by high-dimensional feature maps, does not reduce the sensitivity to quantization, as shown in Figure 3c. These findings challenge the prevailing belief that quantization impacts the first and last layers most significantly [18], [29]. Notably, the second convolutional layer diverges from these expectations [18], a variance linked to a characteristic of MobileNetV2. Specifi-cally, the architecture's first inverted residual block omits the expansion layer to keep a reasonable MAC operations count in the subsequent layer. This results in the initial depthwise convolution having merely 288 parameters, as highlighted in Figure 3. This finding suggests that architectural modifications in recent designs [30], influenced by vision transformers that position depthwise convolutions before expansion layers, might be sensitive to low-bit quantization. Further experiments have revealed that activations are notably more sensitive to low-bit quantization, with quantization errors below 4 bits fre-quently causing orientation decoding errors and disrupting the neural network's training process. This significant constraint influences the layer-wise bit-width activation choices in the next paragraph. Our findings also indicate that the binarization of early layers diminishes the generalization capabilities of the neural network. However, it improves in later layers, suggesting that low-bit quantization compromises information in the initial layers while acting as regularization in the last layers. Additionally, the sensitivity of early layers to quantiza-tion predominantly affects orientation estimation, as opposed to position estimation, which is comparatively simpler [10]. Therefore, quantization below 8 bits has a more pronounced impact on orientation accuracy than on position accuracy.\n\nThis analysis leads us to identify an optimal balance among layer-wise bit-width, FPGA resource utilization, and through-put. Given the limitations of FINN, we quantize only the neural network's backbone for FPGA deployment, leaving the heads in Float32. We observed that activations are highly sensitive to low-bit quantization, yet the hardware resources re-quired increase exponentially with the activation bit-width due to the current FINN C++ HLS backend [25]. Consequently, we quantize all activations to four bits to conserve FPGA resources, as higher bit-widths would disproportionately con-sume resources [27]. Weight bit-width selection varies by layer index due to differing quantization sensitivities; the weights of the first layer are quantized at four bits, reflecting their higher sensitivity. The most sensitive layer, the first depthwise convolution, uses six-bit weights. Subsequently, the first projection convolution weights are set to four bits, and the weights of all remaining 49 convolutional layers are set to three bits. This configuration will be used in the next section to evaluate and implement the neural network on the FPGA."}, {"title": "D. Experimental Results", "content": "Table I presents a comparison of pose estimation metrics across the FPGA workflow, including ESA score (where lower is better), position error ($e_t$), and orientation error ($e_q$) [3], from initial Float32 inferences on a computer to FPGA deployment via FINN, with images sized at 240x240 to meet FINN's requirements. The shift to Int8 quantization leads to a slight increase in the ESA score, primarily attributed to a rise in position error. This is likely due to the position estimation's reliance on direct regression, which is more susceptible to op-timization effects, in contrast to orientation estimation, which employs a probabilistic method through soft classification and thus exhibits greater robustness. Additionally, the quantization process introduces a regularization effect that benefits orien-tation estimation, given the large number of parameters in its neural network branch. Mixed-precision quantization affects both position and orientation errors despite using larger bit-widths for the most sensitive CNN layers, as explained in Section III-C. Nevertheless, the resulting degradation is kept to a manageable level. Our CNN FPGA accelerator generates outputs that exactly match the tensors computed with Brevitas on the computer. The mean square error analysis reveals no differences, ensuring identical Pose metrics on both the FPGA and computer, underscoring our FPGA accelerator's meticulous design and validation.\n\nOur FPGA accelerator operates at 187.5 MHz and utilizes 91% of the Lookup Tables (LUTs) and 90% of the Block RAMS (BRAMs), but only 32% of the Digital Signal Pro-cessors and 26% of the flip-flops on the FPGA. Contrary to the expectations of the FINN library authors [27], computing activation functions significantly impact FPGA resources, as it requires as many LUTs as computing convolutions. This dis-crepancy could stem from using the MobileNetV2 architecture tailored for mobile inference, unlike the less efficient ResNet architecture employed by FINN's authors. The multi-threshold units and convolution computations primarily consume flip-flops and LUTs. At the same time, nearly all BRAMS are dedicated to FIFOs between units, indicating potential for optimization in FINN's resource allocation strategies.\n\nTable II presents FPGA implementation results compared to estimates, covering power consumption (static and dynamic), frames per second (FPS), and energy efficiency. Our acceler-ator is structured as a pipeline of units connected by FIFOs, with its throughput limited by the slowest unit, which operates at 250 FPS. Combined with the power consumption derived from the Xilinx Vivado post-implementation report, it indicates high energy efficiency, as detailed in Section IV-A. On-board testing reveals a significant throughput discrepancy, more than four times lower than estimated, attributed to insufficient BRAMS, which lead to undersized FIFOs. This constraint slows down the pipeline and reduces power consumption by a factor of 4.4. Despite this limitation, the results underscore the real-time energy-efficient capabilities of our FPGA-accelerated SPE algorithm, with further comparisons provided in Section IV-A."}, {"title": "IV. COMPARISON", "content": "Table III summarizes the performance of our implementa-tions compared to existing works. The implementation by [11] does not provide throughput or latency data, which prevents the computation of the energy efficiency of their implemen-tation. Given that they use a Xilinx Ultra96 board, which contains a single-core Xilinx DPU on a small FPGA, achieving real-time inference may not be possible. The other FPGA Ultra96 implementation by [12] implemented only a single layer of their neural network on the FPGA, also hindering the calculation of energy efficiency. Our implementation is almost 9 times faster and over 38 times more energy efficient than the Intel Atom-based implementation by [6]. Additionally, our FPGA solution is 7.7 times faster and 19.5 times more energy efficient than the Google Edge TPU-based ASIC by [13], highlighting the efficiency of our mixed-precision dataflow FPGA accelerator."}, {"title": "B. Limitations and Future Works", "content": "This work advances embedded in-orbit SPE, highlighting achievements and suggesting future directions. Integrating the neural network's backbone into the FPGA marks a signif-icant step forward, with potential enhancements including transitioning the network's head from the CPU to the FPGA for full capability utilization. Future research could explore additional SPE neural networks, such as [6]'s keypoint-based model, despite challenges in experiment reproducibility due to unavailable code and data. Investigating newer convolu-tional architectures such as ConvNext V2 [30] could offer accuracy improvements and insights into the impact of low-bit quantization on advanced model structures, particularly regarding quantization effects on modified inverted residual blocks. Further experiments are essential to validate these preliminary findings."}, {"title": "V. CONCLUSION", "content": "This paper aims to provide efficient spacecraft pose esti-mation (SPE) algorithms that can be effectively implemented on low-cost, low-power, space-ready embedded computers to achieve autonomous space navigation. We propose a co-design methodology to achieve real-time and energy-efficient inference of a popular SPE neural network on the FPGA of a commercial, space-proven Xilinx MPSoC. Thanks to the use of mixed-precision quantization and custom dataflow FPGA acceleration, our implementation is 7.7 times faster and 19.5 times more energy-efficient than the best previously reported SPE algorithms. Furthermore, we present the first real-time, open-source implementation, marking a significant step toward democratizing efficient spacecraft pose estimation algorithms."}]}