{"title": "A Decomposition Modeling Framework for Seasonal Time-Series Forecasting", "authors": ["Yining Pang", "Chenghan Li"], "abstract": "Seasonal time series exhibit intricate long-term dependencies, posing a significant challenge for accurate future prediction. This paper introduces the Multi-scale Seasonal Decomposition Model (MSSD) for seasonal time-series forecasting. Initially, leveraging the inherent periodicity of seasonal time series, we decompose the univariate time series into three primary components: Ascending, Peak, and Descending. This decomposition approach enhances the capture of periodic features. By addressing the limitations of existing time-series modeling methods, particularly in modeling the Peak component, this research proposes a multi-scale network structure designed to effectively capture various potential peak fluctuation patterns in the Peak component. This study integrates Conv2d and Temporal Convolutional Networks to concurrently capture global and local features. Furthermore, we incorporate multi-scale reshaping to augment the modeling capacity for peak fluctuation patterns. The proposed methodology undergoes validation using three publicly accessible seasonal datasets. Notably, in both short-term and long-term fore-casting tasks, our approach exhibits a 10% reduction in error compared to the baseline models.", "sections": [{"title": "I. INTRODUCTION", "content": "Seasonal time series, marked by recurring and cyclical patterns, play a crucial role in our daily lives. Traditional statistical methods, such as ARIMA and Holt-Winters [1, 2] have utilized linear features for prediction. However, their accuracy is rather limited [3, 4] prompting the need for exploration of more sophisticated models. In contrast, Re- current Neural Networks (RNNs) and their variations [5, 6] have demonstrated enhanced performance by incorporating nonlinear features [7, 8]. Despite their progress, these models encounter challenges related to computationally inefficient feature extraction, as early features gradually diminish while hidden states epvolve.\nMopdels based on transformers., spuch as Informer [9] and Autoformer[10], have emerged as leading contenders. In the realm of time series, these models employ self-attention mech- anisms to capture intricate sequence features, facilitating the comprehension of complex data relationships. This represents a significant paradigm shift, overcoming certain limitations inherent in traditional approaches and demonstrating remark- able predictive capabilities. Despite the progress achieved by transformer-based models, the challenge of computational overhead remains a noteworthy concern. Several optimization strategies have been implemented to mitigate this challenge; however, additional attention and enhancements are warranted. The pursuit of more efficient and scalable transformer architec- tures continues, with the goal of achieving a balance between computational efficiency and predictive accuracy.\nCurrent efforts involve continual research on variants of transformer models, investigating innovative attention mech- anisms, and developing strategies to optimize computational processes. The goal is not only to maintain or enhance predictive accuracy but also to ensure their scalability and practicality for real-world applications. To conclude, although traditional statistical methods and RNNs have established the foundation for time-series prediction, transformer-based models signify a substantial advancement. The self-attention mechanisms enable these models to capture intricate data relationships, yet the challenge of computational overhead persists. Continuous research and development in this field are imperative to refine transformer models, rendering them more efficient and practical across diverse applications, including the precise prediction of seasonal time-series datasets.\nThe convolution-based structures for time-series prediction, exemplified by Timesnet [11] and MICN [12], have success- fully reduced the time-memory overhead in series prediction. Nevertheless, these network structures have not fully consid- ered the intricate periodic patterns inherent in seasonal time series. Currently, convolution-based models have achieved notable advancements in enhancing computational efficiency, effectively alleviating the time-memory burden associated with traditional methods and recurrent neural networks. However, when applied to seasonal time series, these models fall short of fully capturing their distinctive cyclical characteristics.\nA significant portion of current research is focused on examining correlations between time steps through attention mechanisms, with the aim of extracting seasonal features but often neglecting the inherent richness of the periodic nature of seasonal sequences. To tackle these challenges, a multi- scale seasonal decomposition time-series modeling framework named MSSD is proposed. In this framework, we decompose univariate seasonal time series into three components, each representing distinct fluctuation trends: Ascending, peaking, and descending components. The Ascending and Descending components display fixed fluctuation patterns modeled using linear regression. To address the complex fluctuation patterns"}, {"title": "II. METHODOLOGY", "content": "Fig. 1 illustrates the overall structure of MSSD. Drawing inspiration from the periodicity inherent in time-series data, this paper introduces a module for decomposing time-series trend patterns. Subsequently, we employ distinct modules to predict the Ascending, Peak, and Descending components individually. Then, we aggregate the individual prediction results to obtain the final prediction, denoted as yt. Further details will be provided in the subsequent sections (refer to Fig. 1).\nA. Decomposition module\nPrevious studies frequently employed one-dimensional convolution-based methods for time-series decomposition[13]. Previous research has consistently identified the standard pe- riod of seasonal cycle time series as typically 24 hours[14]. In the methodology employed in this paper, we categorize the daily cycle sequence into ascending, peak, and descending phases based on the average score method. For a compre- hensive understanding, refer to formulas (1) through\n$\\begin{aligned} T &= 24.i \\\\ X_u = x[0:\\frac{T}{3}],  X_p &= x[\\frac{T}{3}: \\frac{2T}{3}],   X_d = x[\\frac{2T}{3}: T] \\\\ X &= X_u + X_p + X_d  \\end{aligned}$\nIn the given formula, i denotes the sampling interval of the dataset, where, for example, 1 hour is represented as 1 and 15 minutes as 4. The variable x represents a time series within a 24-hour period. The details of visualizations for the CAISO datasets are illustrated in Fig.2.\nB. Linear Regression module\nIn this study, we utilize simple linear regression to model both ascending and descending components. As illustrated in Fig.2, seasonal time series display relatively stable fluctuation trends in ascending and descending components. Using simple regression for modeling purposes can reduce computational costs and improve the interpretability of the model.\n$\\Yu = linear regression(xu)$\n$\\Ya = linear regression(xd)$\nAs depicted in Fig. 3, the SDNet prediction network pri- marily models the intricate peak fluctuation segment. Multiple multi-scale operations are applied to the input sequence Xp to capture local features and global correlations. Following this, the results from the distinct branches are aggregated to integrate the information within the sequence. This process can be summarized as follows:\n$\\begin{aligned} {X_{p,1},...,X_{p,l}} &= Multi-head(xp) \\\\ Y_{p,i} &= SDNet(X_{p,i}) \\\\ Y_p &= Concat(y_{p,i}) \\end{aligned}$\nMulti-scale: Seasonal periodic series usually contain com- plex features. In order to fully tap into these features, we are inspired by the work of multi-head attention modules. From Eq. (6), I means the number of heads.\nLocal-Global convolution block architecture: SDNet con- sists of multiple branches, each handling distinct multi-scale results to simulate potentially diverse temporal patterns. As depicted in Fig. 4, the local-global module extracts both local features and global dependencies in each branch. Specifically, the local modules use one-dimensional convolution to facilitate the learning of similar wave features. This process includes the following steps:\n$\\X^{local,i}_{p,i} = Norm(Conv1d(xp,i)), for ikernel=i$\nIn the case of Convld, we configure stride = kernel = i, effectively compressing local features. Xp,i xlocal, signifies the outcome of local feature compression, resulting in a brief sequence.\nDilated Causal Convolutions: Dilated convolution in- creases the receptive field by injecting holes into standard convolution. In this case, we use parameter d to represent the sampling rate. A low-level d = 1 implies sampling each point in the input, while an intermediate-level d = 2 means sampling every 2 points in the input. Intuitively, dilated convolution results in an exponentially growing effective window size with the number of layers."}, {"title": "III. EXPERIMENT", "content": "A. Experimental Setup\nDataset: In this research, we present a thorough overview of the experimental datasets.\nBaselines: This investigation employs four advanced trans- former models, specifically FEDformer [15], Autoformer [10], Informer [9], and LogTrans [16], as integral components of our baseline models. Furthermore, we incorporate two en- hanced RNN models\u2014LSTM[5] and LSTNet [8] alongside the CNN-based model MICN.\nUnivariate results: Table II presents the outcomes of univariate time-series forecasting, highlighting the remarkable success of MSSD. Significantly, in short-term forecasting tasks using the CAISO dataset, MSSD exhibits a noteworthy average reduction of 3.8% in MAE (6.3%/2.7%/2.6%) compared to MICN. Likewise, MSSD demonstrates a substantial average reduction of 36.6% (26.7%/36.7%/48.9%) and -1.67% (- 4.7%/8.3%/-9.1%) in MAE for the electricity dataset. In long- term forecasting tasks, it is evident that MSSD outperforms most error metrics. For multivariate time series analysis, MSSD was independently tested for each variable.\nMultivariate results: In the domain of multivariate long- term series forecasting, MSSD stands out as a pinnacle of state-of-the-art performance across diverse benchmarks and prediction length configurations, supported by the comprehen- sive results in Table IV. Significantly, MSSD consistently outperforms its counterparts, including the formidable MICN, demonstrating its prowess with an impressive 18% average reduction in MSE across two distinct datasets. This significant improvement in MSE substantiates MSSD's capacity to excel in addressing a spectrum of challenges in time-series forecast- ing within real-world seasonal time-series applications. These findings highlight MSSD as a robust and versatile solution with the potential to significantly enhance predictive accuracy across various forecasting scenarios.\nB. Model Analysis\nLocal-Global Module vs. Auto-correlation, Self- Attention: In this investigation, we introduced a Seasonal Decomposition Network (SDNet) featuring a convolutional module. This design strategically aims to discern and capture nuanced patterns associated with peak fluctuations in seasonal time-series. This sophisticated architecture excels at capturing correlations between local and global features. To substantiate the efficacy of our proposed network structure, we conducted experiments by replacing SDNet with a self-attention module in the training of MSSD. The subsequent results, detailed in Table V, unequivocally confirm the superior effectiveness of our novel SDNet framework in enhancing the modeling and prediction capabilities compared to the alternative self-attention module.\nIn addition, we replaced the original convolutional module in MICN with the convolutional module proposed in this paper, and the result is shown in Table V. The effectiveness of the module proposed in this paper can be seen in Table VI.\nRobustness analysis: We used a direct noise injection approach to evaluate the robustness of our model. Using this method, we introduce varying degrees of perturbation ( to the training data and subsequently record the Mean Absolute Error (MAE) index. The findings, detailed in Table 6, reveal a slight increase in the predicted MAE index with escalating perturbation ratios. Significantly, these results highlight the robust nature of MSSD, demonstrating its resilience, especially up to a 20% perturbation threshold. This resilience signifies MSSD's proficiency in effectively handling mildly noisy data and excelling in accurately forecasting anomalous patterns within seasonal time series.\nEfficiency analysis: With increasing input length, our mod- ule exhibits a significant advantage, demanding less time and memory compared to both self-attention and Auto-correlation counterparts. This efficiency becomes especially pronounced with the increasing input length, confirming the scalability and resource efficiency of our module. The results emphasize its computational superiority, indicating that our module provides a more streamlined and resource-efficient solution, well-suited for managing extended input sequences with improved speed and reduced memory requirements compared to self-attention and Auto-correlation approaches."}, {"title": "IV. CONCLUSION", "content": "In this paper, we present MSSD, a highly specialized convolution-based prediction framework meticulously crafted to address the nuances of seasonal time-series data. MSSD stands out by predicting the rising, peak, and falling segments of seasonal time series individually, offering a comprehensive understanding of the underlying patterns. Across three diverse real-world datasets, MSSD consistently demonstrates state-of- the-art performance, showcasing its efficacy and reliability in forecasting seasonal trends. Notably, our approach to peak prediction involves the strategic use of diverse convolution operations, marking a departure from conventional methods. Through rigorous comparative experiments, we establish that our convolution-based prediction module surpasses the perfor- mance of the attention mechanism, the most popular technique in time-series analysis.\nA key feature of MSSD is its simultaneous partitioning of seasonal time series into rising, peak, and falling segments. This segmentation not only contributes to the model's accuracy but also significantly enhances interpretability. By addressing each phase of the seasonal pattern independently, MSSD provides valuable insights into the distinct characteristics of the data.\nAt present, MSSD is mainly aimed at periodic time-series, and there is no obvious periodic time-series data such as Exchange and Weather, so it cannot achieve a more accurate prediction. In future work, we plan to make targeted im- provements to MSSD to accommodate a wider range of time- series data characteristics. Especially for volatile data such as trading and weather, we will explore new model structures and parameter adjustments to improve forecast accuracy and robustness. This will involve a deeper level of model tuning to ensure that the MSSD exhibits superior performance across different types of time-series data."}]}