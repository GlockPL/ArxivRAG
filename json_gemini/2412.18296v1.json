{"title": "Navigating Data Corruption in Machine Learning: Balancing Quality, Quantity, and Imputation Strategies", "authors": ["Qi Liu", "Wanjing Ma"], "abstract": "Data corruption, including missing and noisy data, is an inevitable challenge in real-world machine learning applications. This paper investigates the impact of data corruption on model performance and explores strategies to mitigate its effects through two distinct experimental setups: supervised learning with NLP tasks (NLP-SL) and deep reinforcement learning for traffic signal optimization (Signal-RL). We evaluate the relationship between data corruption levels and model performance, assess the effectiveness of various data imputation methods, and analyze the utility of enlarging datasets to offset data corruption.\nOur findings reveal that model performance under data corruption follows a diminishing return curve, well-modeled by the exponential function $S = a(1 \u2013 e^{-b(1-p)})$, where p is the corruption ratio. Missing data, while harmful, is less detrimental than noisy data, which causes rapid performance degradation and increased training instability, particularly in sequential decision-making tasks like Signal-RL. Imputation strategies introduce a trade-off between recovering missing information and introducing noise, with their effectiveness heavily dependent on imputation accuracy and corruption ratio. We identify a distinct \"imputation advantageous corner\" and \"imputation disadvantageous edge\" on an imputation advantage heatmap, and categorize tasks as either \"noise-sensitive\" or \"noise-insensitive\" based on their imputation decision boundaries.\nAdditionally, we show that increasing the dataset size mitigates, but does not fully eliminate, the effects of data corruption, especially under high noise levels. The marginal utility of additional data diminishes as corruption levels increase, confirming the exponential nature of the trade-off between data quality and quantity. An empirical rule emerges from our study: approximately 30% of the data is critical for determining model performance, while the remaining 70% has a minimal impact.\nThese findings provide practical insights into data preprocessing, imputation strategy selection, and resource allocation for data collection, offering actionable guidance for building robust machine learning systems in noisy, real-world environments.", "sections": [{"title": "1 Introduction", "content": "The advent of large language models (LLMs), such as GPT-3 and BERT, has revolutionized natural language processing (NLP), enabling machines to understand and generate human-like text with remarkable proficiency. Pre-training these models requires vast amounts of data to capture the nuances and complexities of human language. However, real-world datasets are often riddled with corrupted data, which can significantly impede the performance of LLMs if not properly addressed. Handling corrupted data is, therefore, a critical step in the pre-training pipeline, necessitating robust methods to ensure data integrity and model efficacy.\nCorrupted data can arise from various sources, such as transmission errors or incomplete data collection processes. Data corruption manifests in different forms, including missing data and noisy data. The presence of corrupted data poses serious challenges to a model's learning ability. Consequently, employing effective strategies to handle corrupted data is essential to maximize the potential of LLMs."}, {"title": "2 Related Work", "content": "Types of Data Corruption\nData corruption in machine learning encompasses various forms, including missing data, noisy data, and adversarial perturbations. This study focuses on two major types of corruption: missing data and noisy data, which commonly occur in real-world scenarios.\nMissing Data: Missing data can result from sensor dropout, incomplete data collection, or non-response in surveys. Rubin's classification categorizes missing data into three types: Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR) [Rub76]. In NLP, missing data often manifests as masked or unknown tokens, while in reinforcement learning, it appears as incomplete state observations.\nNoisy Data: Data noise can affect both labels and features, with sources ranging from environmental factors to measurement errors. Noise is often categorized by its statistical distribution (e.g., Gaussian, uniform) or type (e.g., additive, multiplicative). Noisy data significantly disrupts model learning, particularly when noise affects key features or labels critical to decision-making.\nHandling Corrupted Data\nData imputation aims to recover missing information, and various strategies have been proposed: \u2022 Statistical Methods: These include simple approaches like mean, mode, or median imputation [SG02], as well as more sophisticated techniques like regression imputation and multiple imputation [Rub87]. While straightforward, these methods may introduce bias or underestimate variability. \u2022 Machine Learning Methods: Techniques such as K-Nearest Neighbors (KNN) imputation [TCS+01], decision tree-based imputation [BFOS84], and Random Forest imputation [Bre01] leverage patterns in data to predict missing values. Deep Learning-Based Imputation: Autoencoders and Generative Adversarial Networks (GANs) have emerged as powerful tools for data imputation. Denoising autoencoders reconstruct inputs with missing values [VLBM08], while GANs generate plausible synthetic data to fill gaps [GPAM+14]. Yuan et al. (2021) used masked language models (e.g., BERT) to impute missing tokens in text data [YWZ21].\nFor many applications, particularly LLM pre-training, masking missing data is often sufficient. Che et al. (2018) showed that masking missing time-series data combined with recurrent neural networks (e.g., GRU) can effectively handle missingness [CPC+18].\nImpact of Data Corruption on Neural Network Learning\nSupervised Learning (LLM Pre-training) Data corruption, including missing or noisy tokens, can significantly impair pre-trained language models like BERT and GPT. Missing data reduces the available context, weakening learned representations and hindering downstream tasks such as summarization or question answering [Dev18, Liu19]. Joshi et al. (2020) showed that missing rare tokens during pre-training leads to incomplete token embeddings, limiting the model's ability to capture fine-grained semantics [JCL+20].\nNoisy data, particularly in large-scale corpora, introduces biases and degrades model robustness. Brown et al. (2020) highlighted that noisy training data increases the likelihood of generating biased or low-quality outputs, while filtering and robust training objectives mitigate such effects [Bro20]. Semantic noise, such as contradictory or irrelevant text, reduces the model's ability to retain factual knowledge and generalize across tasks [PPF+20]. Studies by Gehman et al. (2020) further demonstrated that noisy datasets amplify harmful biases, underscoring the importance of pre-training data quality [GGS+20].\nReinforcement Learning In reinforcement learning (RL), data corruption affects state observations, which are critical for decision-making. Missing features reduce state informativeness, leading to suboptimal policies, particularly in partially observable environments (POMDPs) [HS15]. Studies by Bai et al. (2021) demonstrated that missing features disrupt state transition dynamics, causing instability in model-free RL algorithms and inaccurate environment models in model-based RL [BGW19]. Noise in feature data further degrades RL performance. Pathak et al. (2017) showed that noisy features distort latent state representations, leading to poor decision-making in high-dimensional environments [PAED17]. Mnih et al. (2015) found that Q-values fluctuate with noisy observations, resulting in unstable policies [MKS+15]. Moreover, noise hampers transfer learning, as policies trained in corrupted environments fail to generalize to clean environments [TS09].\nKey Research Gaps\nWhile many studies address specific aspects of corrupted data handling, key questions remain unanswered:\n\u2022 Impact of Data Corruption: What is the quantitative relationship between data corruption ratio and model performance? Can this relationship be consistently modeled across tasks?\n\u2022 Effectiveness of Imputation: How do different imputation methods compare in mitigating the effects of missing data? Is it possible to fully restore the utility of corrupted data through imputation?"}, {"title": "3 Learning with corrupted data", "content": "Experiment Design\nWe designed two experiments, NLP supervised learning (NLP-SL) and traffic signal deep reinforcement learning (Signal-RL), to investigate the impact of data corruption. These two vastly different experimental setups were chosen because the questions we aim to explore are general and relevant across a wide range of machine learning tasks. By selecting tasks that are diverse in nature, we hope to derive more general insights and uncover deeper connections between these seemingly unrelated domains.\nThe first experiment focuses on the GLUE benchmark tasks. We use the pretrained BERT model as the base model and add feedforward layers on top to fine-tune it on eight tasks: COLA, SST-2, MRPC, STSB, QQP, MNLI, QNLI, and RTE. The input data is corrupted by replacing certain words with the \"[UNK]\" token, while the labels y remain uncorrupted. This type of data corruption is commonly encountered in natural language processing tasks. For instance, when digitizing text corpora, some words may become indistinguishable and are thus masked as unknown, whereas the associated labels are typically unaffected. Evaluating the amount of knowledge learned by a model remains a subjective challenge. In this experiment, we measure performance using classification-related accuracy metrics. Specifically:\n\u2022 The Matthews Correlation Coefficient (MCC) is used for COLA.\n\u2022 The average of Pearson and Spearman correlation coefficients is used for STSB.\n\u2022 Test accuracy is used for the remaining tasks.\nBaseline scores (1/2 for binary classification and 1/3 for three-way classification) are subtracted from these metrics. The final model score is calculated as the average of the scores across all tasks. To ensure consistency, hyperparameters such as training epochs and learning rates are tuned using uncorrupted data and kept fixed for all subsequent experiments. Model convergence for these experiments is illustrated in Figure 2. For the sake of clarity, we will refer to this experiment as the \"Natural Language Processing - Supervised Learning (NLP-SL)\" experiment in the following sections.\nThe second experiment is a deep reinforcement learning (RL) task. We built an isolated intersection environment using SUMO. The environment is illustrated in Figure 1. The objective is to optimize the traffic signal at this intersection. The intersection consists of four approaches, each with three lanes. The traffic demand is generated using a binomial distribution, and the ratio of left-turn, through, and right-turn traffic demands is 1:3:2. The arrival rates are time-varying: East-West traffic demands follow a sine curve, while North-South demands follow a cosine curve, both within the interval [0, \u03c0/2]. The time horizon H for the simulation is one hour.\nA Deep Q-Network (DQN) model is used to learn the traffic signal control strategy. The state of the environment consists of road occupancy, the current signal phase, and the duration of the current signal phase, resulting in a state vector of dimension 965. A neural network with two hidden layers of sizes 256 and 48 is used to extract features. Layer normalization is applied, but no dropout layers are included. Standard techniques such as double networks and replay buffers are implemented. The action space is discrete, with four possible actions: East-West left turn, East-West through, North-South left turn, and North-South through. Each time step in the simulation corresponds to 6s. The reward $r_t$ for each time step is defined as the queuing vehicles' value transformed according to Equation 1. The performance metric for the model is the episode cumulative reward, R. Although the reward is accumulated over one hour, the process itself is infinite."}, {"title": "Observations", "content": "$\n\\begin{equation} R = \\sum_{t=1}^H r_t = \\sum_{t=1}^H \\frac{q_t - 80}{80} \\tag{1} \\end{equation}\n$\nwhere $q_t$ is the number of queuing vehicles at time $t$; and the threshold for deciding whether a vehicle is stopped is 0.3m/s.\nWe trained the model using linearly decaying exploration ($\\epsilon$) and learning rate (lr) for 80% of the training time, after which the values were fixed. The initial and final $\\epsilon$ values were 1.0 and 0.01, respectively, while the initial and final learning rates were 1e-3 and 1e-4, respectively. The DQN model convergence results are shown in Figure 2(b).\nWe experimented with three types of data corruption: vehicle-missing, inserting-noise, and masking-region:\n1. Vehicle-missing: A proportion of the vehicles are not detected. This scenario is relevant in Vehicle-to-Everything (V2X) environments, where roadside units detect vehicles' presence through communication channels like DSRC [THBM19]. However, only a proportion of vehicles are equipped with onboard devices. This type of corruption is analogous to the data missing scenario in the NLP-SL experiment.\n2. Inserting-noise: Noise is added to the road occupancy state and rewards. This scenario is relevant in environments where vehicles are detected using computer vision systems, which can introduce errors.\n3. Masking-region: This special type of corruption is specific to traffic signal settings. The simulation environment assumes a lane length of 400 meters. A masking-region ratio p means that the farthest 400*p meters of each lane will be invisible to the model, simulating the range limitations of video cameras used for vehicle detection.\nFigure 4 illustrates the relationship between the data missing ratio and model performance. Both experiments (NLP-SL and Signal-RL) show a gradual initial decline in performance, followed by a steeper descent, culminating in a sharp drop-off as the data missing ratio approaches 1.0. For reference, we include the performance of an optimized fixed-timing signal as a benchmark in the figure. The RL-trained signal outperforms the fixed-timing signal when the data missing ratio is less than 0.8.\nTables 1 and 2 provide the detailed data for these experiments. In Figure 5, we change the x-axis to represent 1 - corruption ratio and fit the curve to the function in Equation 1. The fitted parameters for the NLP-SL experiment are: a = 0.475, b = 3.517, where b represents the decay rate that controls the curve's steepness. The goodness-of-fit analysis results in R2 = 0.995, indicating that the exponential cumulative distribution function (CDF) is an excellent model for the observed performance. Similarly, for the Signal-RL experiment, the fitted curve parameters are: a = 395.827, b = 7.493 with R2 = 0.956.\nThis striking coincidence reveals an important and universal rule in machine learning: the diminishing return of data. The decay rate b reflects the task's nature, with the RL task demonstrating a much larger decay rate. This implies that RL tasks are more sensitive to noise compared to NLP tasks.\n$\n\\begin{equation} S = a(1 \u2013 e^{-b(1-p)}) \\tag{2} \\end{equation}\n$\nwhere parameter $a = \\frac{S_0}{150\\%}$; and $S_0$ is the model score when $p = 0$ (i.e., no corruption).\nHere, we provide an explanation for the observed pattern. For both NLP and DRL tasks, the model relies on recognizing patterns (e.g., semantic patterns, queuing patterns) in the data. When the data is completely corrupted, the critical patterns necessary for performance are entirely lost. As corruption decreases, the model rapidly recovers key patterns, resulting in a steep improvement in performance. However, once most of the critical patterns have been recovered, the marginal utility of additional clean data diminishes, leading to a saturation of performance.\nAs we know, the probabilities of observing rare events over a large number of trials converge to the Poisson distribution. When n (the number of trials) is large, p/ (the probability of success per trial) is small, and the expected number of successes $\\lambda = n \\cdot p/n$ is finite, the binomial distribution approximates the Poisson distribution."}, {"title": "4 Effectiveness of Data Imputation", "content": "$\n\\begin{equation} P(K = k) \\approx \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\tag{3} \\end{equation}\n$\nwhere $\\lambda = n \\cdot p$ is the rate parameter, representing the expected number of successes.\nThe exponential term $e^{-\\lambda}$ in the Poisson distribution describes the probability of observing 0 successes, or equivalently, the probability of failure. In both experiments, the dataset size n is large, making the recovery of individual patterns a rare event that can be modeled using the Poisson distribution. In our experiments, each pattern has an equal probability p of being corrupted, or a probability $\\lambda = 1-p$ of being recovered. A pattern can be recovered multiple times in the dataset. The probability of failing to recover such a pattern in a dataset with corruption level p is proportional to $e^{-\\lambda}$. This leads to a system where the rate of change in performance depends on the difference between the system's current state and its limit. Such behavior can be modeled using Equation 4, whose solution corresponds to Equation 1:\n$\n\\begin{equation} \\frac{dS}{dx} = ab e^{-bx} = b(a - S) \\tag{4} \\end{equation}\n$\nwhere $x = 1 - p$.\nThis model effectively explains the exponential improvement observed in performance as corruption decreases and clean data proportion increases.\nThe Signal-RL model scores under inserting-noise and masking-region types of data corruption are shown in Figure 6. From Figure 6(a), we observe that inserting noise is significantly more detrimental than data-missing. The model's performance deteriorates much more rapidly as the noise level increases, falling below the fixed-timing signal performance as soon as the noise level exceeds 10%. Additionally, the model scores become unstable, as indicated by the oscillations in Figure 6(a). The training process also becomes unstable, as shown in Figure 7.\nMasking-region is a unique type of data-missing corruption. Information about vehicles farther away from the intersection is less critical. By gradually discarding less important data, we observe that the model score only experiences a sharp decline when the masking ratio p exceeds 0.7. This observation leads to an empirical rule: 30% of the data is critical and determines the model's performance, while the remaining 70% can be discarded without significantly affecting the model's performance.\nIn this section, we assess whether and how different imputation strategies mitigate the impact of missing data. We note that the decision on whether to impute missing data involves a trade-off between data-missing and noise-inserting. We distinguish two common types of data-missing scenarios often encountered in practice:\n1. The first type occurs when the exact location of missing data is known. This is typical for NLP tasks, where missing words are marked with tokens like \"[UNK].\""}, {"title": "5 Effectiveness of Enlarging Dataset", "content": "Experiment Design\nThe aim of this section is to evaluate the effectiveness of enlarging the dataset and to quantify how much additional data is needed to offset the effects of data corruption. For the NLP-SL experiment, we recorded the performance of the model trained on datasets of different sizes and varying data corruption ratios. The variable \"subset ratio\" represents the proportion of the GLUE dataset used for fine-tuning. The results are shown in Figure 12 (a). For the Signal-RL task, we conducted experiments with different numbers of training episodes and data-missing ratios, as shown in Figure 12 (b).\nObservations\nAs shown in the figure, as the dataset size increases, model performance converges. However, we observe that data corruption leads to a decline in model performance that cannot be fully recovered by increasing the sample size. When p is in the range [0, 0.4], the performance decline is nearly linear with respect to p (Figure 12 (a)). For larger p, the model's performance drops sharply to near zero (Figure 4).\nThis behavior is characteristic of an exponential function: for $e^x = 1 + x + ...$, the linear term dominates when x is small. Hence, we conclude that the performance drop increases approximately exponentially as the data corruption ratio increases. In addition, data corruption also hampers learning efficiency. To achieve the same level of performance (if achievable at all for a corrupted model), the number of samples and therefore the training time required increases exponentially with the data corruption level. This is illustrated by the dashed benchmark line in Figure 12 (b). Quantitative curves showing the relationship between data quality and the required amount of clean data provide practical insights into data collection strategies. Code and detailed results of this study are shared at"}, {"title": "6 Conclusions", "content": "In this paper, we explored the impact of data corruption, including missing and noisy data, on deep learning performance across two distinct domains: supervised learning with NLP tasks (NLP-SL) and deep reinforcement learning for traffic signal optimization (Signal-RL). Our experiments aimed to provide insights into the relationship between data quality and model performance, the trade-offs of imputation strategies, and the effectiveness of increasing data quantity as a remedy for data corruption.\nKey Findings\n1. Diminishing Returns of Data Quality Improvement: Both NLP-SL and Signal-RL experiments revealed that model performance follows a diminishing return curve as data corruption decreases. The relationship is well-modeled by the exponential function $S = a(1 \u2013 e^{-b(1-p)})$, highlighting that additional clean data yields less utility once most patterns are recovered. This universal trend emphasizes the importance of balancing data quality and preprocessing efforts.\n2. Noise vs. Missing Data: Our results demonstrate that noisy data is significantly more detrimental than missing data, leading to faster performance degradation and increased training instability. This was particularly evident in the Signal-RL task, where inserting noise caused substantial fluctuations in both training and final policy stability.\n3. Imputation Trade-Offs: Imputation methods can restore critical information for missing data but introduce a trade-off by potentially adding noise. The decision to impute depends on the imputation accuracy and the corruption ratio. The \"imputation advantage heatmap\" revealed two regions: a \u201cred corner\u201d where accurate imputation significantly improves performance and a \"blue edge\" where noisy imputation harms the model. Tasks were categorized as \"noise-sensitive\" or \"noise-insensitive\" based on their decision boundaries, with RL tasks exhibiting greater sensitivity due to the compounding effects of sequential decision-making.\n4. Enlarging Dataset as a Remedy: Increasing the dataset size partially mitigates the effects of data corruption but cannot fully recover the lost performance, especially under high noise levels. The analysis showed that the number of samples required to achieve a certain performance level increases exponentially with the corruption ratio, confirming the exponential nature of the trade-off between data quality and quantity.\n5. Empirical Rule for Critical Data: An empirical rule emerged from the experiments: approximately 30% of the data is critical for determining the model's performance, while the remaining 70% can be lost without substantial impact. This observation provides practical guidance for prioritizing efforts in data collection and preprocessing.\nImplications\nOur findings offer valuable insights for machine learning practitioners and researchers. In resource-constrained environments, focusing on improving data quality for a small, critical portion of the dataset may yield higher returns than attempting to clean all data. Additionally, understanding the trade-offs of imputation strategies can inform the choice of preprocessing methods based on task sensitivity and corruption characteristics. For RL tasks, where noise compounds over sequential decisions, more robust strategies to handle corrupted observations are critical.\nFuture Work\nThis study opens up several avenues for future research. First, the observed patterns and empirical rules should be validated across broader datasets and additional machine learning tasks, such as computer vision and time-series forecasting. Second, developing adaptive imputation strategies that dynamically balance missing data recovery and noise introduction could further enhance model robustness. Finally, theoretical work on the relationship between information entropy, marginal utility, and model learning dynamics under corrupted data could deepen our understanding of these phenomena. By addressing these challenges, we hope to advance the field's ability to build robust machine learning models that perform reliably even in the presence of real-world data corruption."}]}