{"title": "HUMAN-AI TEAMING USING LARGE LANGUAGE MODELS: BOOSTING BRAIN-COMPUTER INTERFACING (BCI) AND BRAIN RESEARCH", "authors": ["Maryna Kapitonova", "Tonio Ball"], "abstract": "Recently, there is an increasing interest in using artificial intelligence (AI) to automate aspects of\nthe research process, or even autonomously conduct the full research cycle from idea generation,\nover data analysis, to composing and evaluation of scientific manuscripts. Examples of working\n\"AI scientist\u201d systems have been demonstrated for computer science tasks and running molecular\nbiology labs. While some approaches aim for full autonomy of the scientific AI, others rather aim\nfor leveraging human-AI teaming. Here, we address how to adapt such approaches for boosting\nBrain-Computer Interface (BCI) development, as well as brain research resp. neuroscience at large.\nWe argue that at this time, a strong emphasis on human-AI teaming, in contrast to fully autonomous\n\"AI BCI researcher,\" will be the most promising way forward. We introduce the collaborative\nworkspaces concept for human-AI teaming based on a set of Janusian design principles, looking\nboth ways, to the human as well as to the AI side. Based on these principles, we present ChatBCI, a\nPython-based toolbox for enabling human-AI collaboration based on interaction with Large Language\nModels (LLMs), designed for BCI research and development projects. We show how ChatBCI was\nsuccessfully used in a concrete BCI project on advancing motor imagery decoding from EEG signals.\nOur approach can be straightforwardly extended to broad neurotechnological and neuroscientific\ntopics, and may by design facilitate human expert knowledge transfer to scientific AI systems in\ngeneral.", "sections": [{"title": "1 Introduction", "content": "Human-Artificial Intelligence (AI) collaborations may play an important role in promoting scientific discovery Wang\net al. [2023]. Efforts to automate research and development tasks have recently led to the creation of \u201cAI scientists\u201d\u2014AI\nagents specialized to autonomously carry out various roles in the scientific process. AI scientists may run virtual or even\nphysical labs (self-driving labs, SDLs Rapp et al. [2024]) and thus help to \"super-charge\u201d fields including biomedical\nresearch, material science, or computer science.\nFor example, in an effort to leverage AI for biomedical research, Swanson and colleagues have recently created a\nvirtual laboratory that brings together multiple AI scientists-implemented as large language models (LLMs) with\nspecialized roles to collaborate autonomously on objectives defined by human investigators. This system rapidly\nproposed nearly 100 designs for antibody fragments, known as nanobodies, capable of binding to the virus responsible\nfor COVID-19\u2014accomplishing in a short span what might otherwise take a purely human team much longer Swanson\net al. [2024]. In the field of computer science, Lu and colleagues introduced an AI scientist system that aims to automate\nthe complete research cycle. They also use a LLM to survey scientific literature, formulate hypotheses, test them by\nrunning experiments, and then compile the results into a written paper. The system goes a step further by performing its\nown 'peer review,' evaluating and refining the outputs it generates Lu et al. [2024]. However, despite these impressive\nfirst demonstrations, substantial challenges remain for AI-driven scientific discovery in general, as well as for the\nadaptation of AI-scientists to the unique requirements that the scientific domains of research and development may\nimpose on such a system.\nFor AI-scientist systems (AISS) in general, for example, one major challenge is how to integrate the vast amount\nof implicit, unstructured, and often context-dependent knowledge and expertise that human scientists rely on for\nexperimental design, interpretation, and innovation\u2014which may not be sufficiently represented in the available training\ndata. For example, Rihm and colleagues argue that current AI-driven labs are only able to incorporate explicit knowledge\nsuch as recorded data, scientific documentation, and publications and hence only see the \"tip of the iceberg\" of the\nrelevant knowledge Rihm et al. [2024].\nHowever, even explicit knowledge such as that expressed\nin scientific publications may hold problems of its own\nwhen used as training data for scientific AI systems. Ge-\noffrey Boulton and Moumita Koley observed in a contri-\nbution for the International Science Council: \"Many sci-\nentific journals and papers maintain high standards, too\nmany lack proper editorial oversight, many lack rigour\nand integrity, some engage in fraudulent practices, [...],\nand agreed standards for overall governance of the pro-\ncess are lacking.\" International Science Council [2023].\nAll of these point might compromise training data quality.\nThis situation, together with the recent \u201cpublishing explo-\nsion"}, {"title": "2 ChatBCI toolbox overview and Janusian Design Principles", "content": "challenges of this task, we introduce a concept for human-AI workspaces designed to foster scientific human-AI\nco-learning and collaboration, summarized in our Janusian Design Principles. As an instance of this design concept,\nwe present the LLM-based ChatBCI toolbox.\nWe show how ChatBCI enabled a human-AI team to perform a prototypical scientific BCI project based on a public\nEEG dataset in a very time-efficient and productive manner, addressing the full cycle of project phases from data import,\nexploration, and validation, over machine learning model selection and implementation (a deep convolutional neural\nnetwork as the LLM-generated brain signal decoder), setting up a training loop with integrated data augmentation, to\nthe joint interpretation of the obtained results. We illustrate how our structured interaction framework flexibly supports\ndifferent levels of agent autonomy and demonstrate how it allows transfer of expert knowledge about non-obvious,\nnon-trivial aspects of the EEG dataset used, from the human expert to the ChatBCI AI agent. This, in turn, proved\na valuable teaching and training partner for the human side. Thus, we achieved meaningful human-AI co-learning\ntogether with substantially faster collaborative completion of our BCI research task than would have been possible\nwith a purely human approach. Our approach holds great potential for teaching and training in the field of human brain\nresearch and BCI development. LLM-generated brain signal decoders also present a novel class of AutoML for brain\nsignal analysis. The \u201cAI brain researcher\u201d or \u201cAI BCI researcher\u201d presented here is an instance of an \"AI worker\u201d\nin general. It will be interesting to study how abilities learned to handle the complexities of the human brain, and of\nthe signals that can be measured from it, may also be helpful for AI workers in other domains. We anticipate that\ncollaborative human-AI research and development in the field of brain research and BCI, as enabled by our design\nconcept and ChatBCI toolbox, will pave the way toward AI systems with a deep and holistic understanding of human\nbrain function - a \"brain-grokking AI\" - with a far-reaching application potential.\nThe ChatBCI toolbox\u00b2 is fully implemented in Python,\ndeep learning functionality is relying on PyTorch; in the\npresent work we use GPT-4o as underlying LLM OpenAI\n[2024]. It comprises the following main components:\n\u2022 A collection of public EEG data sets in ChatBCI\ncompatible format, including documented data\nvalidation\n\u2022 EEG and BCI knowledge base\n\u2022 Core functionality: EEG preprocessing, analy-\nsis, decoding models, training, interpretability,\nand visualization\n\u2022 Auxiliary components and supporting infrastruc-\nture, such as for generating summaries of data\nand code directory structures and contents at\nadjustable levels of granularity.\n\u2022 LLM communication tools.\nChatBCI is by-design build to fostering human-AI col-\nlaboration (Figure 2). To this aim, we have defined a\nset of design principles which are the foundation of our\ngeneral framework for human-AI teams. In the following\nwe give a summary of our current working version of\nthese principles. 3."}, {"title": "Working version of the seven Principles of Janusian Design", "content": "1. Speaking the Same Language: Janusian workspaces foster the development of a joint collaboration language,\nminimizing misunderstanding and confusion. This enables AI systems to act as intuitive partners, aligning\nwith human intentions and promoting seamless interactions. To achieve this, the workspace leverages intuitive\ninterfaces, contextual feedback, and iterative refinement, ensuring that communication between humans and\nAI becomes progressively more natural and effective.\n2. Transparency and Trust: Trust requires mutual clarity and predictability. Explainability is a cornerstone,\nallowing users to progressively build confidence in AI systems by understanding their decision-making\nprocesses. The framework incorporates bidirectional transparency, encouraging not only explainable\nAI (XAI) methods, but also mechanisms for AI systems to ask clarifying questions when encountering\nambiguous inputs or tasks. This fosters a collaborative dynamic where both human and AI agents seek to\nrepresent/understand each other's intentions, enhancing mutual trust and reducing the likelihood of errors.\n3. Building a Shared Knowledge Base: The workspace amplifies human and AI collaboration by creating and\nmaintaining a shared knowledge base. This repository encodes specialized expert knowledge and evolves\niteratively. The Janusian Wiki (Jiki) serves as the foundation for this knowledge base, providing a structured,\ncentralized system for documenting workflows, best practices, and domain-specific knowledge. By linking\ntasks, tools, and insights, the Jiki ensures that both human collaborators and AI systems have access to the\ncollective intelligence of the workspace, facilitating informed decision-making and continuous improvement.\n4. Joint Integration of Priorities: The Janusian framework acknowledges that human-AI teams must balance\nand integrate priorities jointly, addressing trade-offs such as the need for immediate actions versus long-term\nobjectives, or reconciling priorities driven by strategic considerations or overarching research questions\n(\"top-down\") with reactive, task-specific responses (\"bottom-up\").\n5. Adaptive Autonomy: AI systems adjust their level of autonomy based on task demands, enabling granular\nhuman supervision where needed while offering higher degrees of independence for routine or low-stakes tasks.\nFor example, in the ChatBCI toolbox, the adaptive autonomy principle is exemplified through Collaborative\nTest-Driven Development (TDD), where the AI autonomously generates test cases and mock data for routine\nvalidations, while human oversight is applied to critical or ambiguous scenarios. This flexibility ensures that\nthe level of autonomy dynamically aligns with the complexity and importance of the task.\n6. Accessibility from Novice to Expert: The workspace aims to balance accessibility for novice users, such as\nthose in teaching and training scenarios, with maintaining advanced functionality tailored for expert users. It\nalso differentiates how information from these scenarios is processed and utilized on the AI side, ensuring that\ninsights from both novice interactions and expert workflows are effectively integrated. This inclusive design\npromotes efficiency and reduces the supervision workload, enabling seamless collaboration across varying\nlevels of expertise.\n7. Continuous Evolution: A shared Janusian workspace is not static, but designed to foster mutual co-learning\nbetween humans and AI. This principle ensures that the ChatBCI toolbox or other workspaces based by-design\non this concept not only supports effective and scalable data analysis but also fosters a dynamic workspace\nwhere both human and AI partners can co-evolve to achieve aligned goals.\nIn summary, the Janusian framework establishes a synergistic foundation for human-AI collaboration by aligning\nintuitive interfaces, transparent processes, and adaptive autonomy with evolving user needs. In the following, we\nillustrate how the ChatBCI toolbox based on these design ideas can be used in a concrete project example."}, {"title": "3 Analyzing the BCI competition IV2a dataset using ChatBCI", "content": "As illustrated in Figure 1, we propose a conceptual framework for AI-driven scientific research processes tailored to\nBrain-Computer Interfacing (BCI) and brain research. This concept builds upon the approach introduced by Lu et al.\n[2024], which emphasizes fully autonomous AI systems executing all phases of the research cycle. In contrast, our\napproach highlights the importance of integrating numerical results and data visualization as essential elements within\nthe experimental iteration loop. Furthermore, instead of aiming for complete autonomy, we advocate for flexible levels\nof shared autonomy, enabling human-AI collaboration to be tailored dynamically for each phase of the cycle. The\ncolor-coded segments in Figure 1 correspond to the phases of an example BCI project reported here, with the exception\nof automatic paper write-up, which was not addressed in this work.\nIn the following sections, we will structure the the main part of the manuscript along the schematic depicted in Figure 1,\ndetailing each phase of the research process and how human-AI collaboration is implemented to maximize efficiency,\nadaptability, and the transfer of expertise."}, {"title": "3.1 Idea generation", "content": "The research ideas presented in Table 1 were generated using GPT-40, as our underlying LLM. The model was provided\nwith a detailed prompt to explore advancements in EEG-based motor imagery classification within the context of\nBrain-Computer Interfaces (BCIs) using the BCI Competition IV 2a dataset Brunner et al. [2008]. We decided to\nuse this specific dataset because it has been widely used, maybe the closest to a \"MNIST of EEG\", providing a rich\nbackground of published results.\nThe prompt included instructions to identify key challenges, research gaps, motivations, and potential approaches\nacross a range of topics, including domain adaptation, lightweight architectures, temporal dynamics, and explainability.\nGPT-40 synthesized the list of ideas by leveraging its training on a wide range of scientific and technical texts; notably\nhere we did not fine-tune on more specific literature. Overall, this demonstrates the utility of LLMs in assisting\nresearchers by rapidly generating diverse, structured, and actionable ideas that may have merit in guiding future\ninvestigations. Notably, the generated research ideas included indeed notoriously under-researched yet important topics\nsuch as the role of data preprocessing (c.f. example 10 in 1). On the other hand, an expert in the field in a single glance\ncan find that all generated research questions have already been addressed by the research community (Peterson et al.\n[2019], Meng et al. [2023], Li et al. [2022], Jin et al. [2022], Kostas et al. [2021], Kasahara et al. [2015], Echtioui et al.\n[2021], Saha and Baumert [2020], Ju et al. [2020], Feng et al. [2020], Kessler et al. [2024], Duan et al. [2020], Ferrez\nand Mill\u00e1n [2008]). Therefore, the introduction of iterative refinement in the process of idea generation (e.g. like chain\nof thoughts Lu et al. [2024]) and tools for novelty verification (through Semantic Scholar API) and quantification are\nnecessary. Clearly, the scientific ideas generation stage itself could also benefit from more sophisticated approaches.\nFor instance, leveraging creative templates grounded in evolutionary computation and open-endedness principles offers\nsignificant potential. These approaches, as exemplified in works by Lu et al. [2024] and Faldor et al. [2024], use\niterative, mutation-driven frameworks to refine and evolve novel ideas, mimicking the exploratory processes of natural\nevolution. Furthermore, combining knowledge graphs with LLM, as demonstrated by Gu and Krenn [2024], allows for\nthe synthesis of diverse interdisciplinary ideas. Another promising approach is curiosity-driven learning, as proposed by\nPathak et al. [2017], which emphasizes exploring the unknown and prioritizing novelty over immediate utility.\nHowever, it was also observable that many of the proposed topics were under-specified and lacked exact knowledge\nof what has already been done and reported in the relevant literature. For example, the suggestion was to explore\nlightweight neural networks that achieve state-of-the-art accuracy while being optimized for low-power devices, which\nis a promising topic, but one that is already well-trodden in the field. Studies like the widely known EEGNet paper\nLawhern et al. [2018] have specifically addressed this challenge by designing compact, efficient neural networks tailored\nto EEG data. Therefore, while this idea aligns with key goals in the field, it lacks specificity in identifying novel\ndirections. For instance, future advancements might focus on extending lightweight models to handle more diverse\ndatasets, integrating adaptive techniques that dynamically scale complexity based on resource availability, or innovating\nin model interpretability for constrained devices. Without domain-specific fine-tuning, the model had also difficulties in\nretrieving the published SoA in decoding accuracies on the BCI Competition IV 2a dataset. While a re-implementation\nof the FBCSP winning model of the original BCI Competition IV yielded 67.8% accuracy across the 4 motor imagery\nclasses, and convnets allowed for a gain of approx. 4% Schirrmeister et al. [2017]. In the meanwhile, accuracies close\nto 100% have been reported (e.g., 97.61% in Xie and Oniga [2023]).\nOn this background, we decided for the following, human-specified topic for the first ChatBCI project: What do the\nclose to 100% decoding accuracies reported on the BCI Competition IV 2a dataset mean? To which extent do they\nreflect sophistication of the brain signal decoding methodology, or possibly also over-fitting to this (relatively small)\ndataset, and/or contributions from non-brain signals (artifacts which may contain class-specific information and might\nbe easier to decode than brain signals)? The BCI Competition IV 2a data set is particularly interesting in this context,\nas it was part of the original competition instructions that since three EOG channels were provided, it was \"required\nto remove EOG artifacts before the subsequent data processing using artifact removal techniques such as highpass\nfiltering or linear regression\" Brunner et al. [2008]. The winning entry of the competition and based on their approach,\nalso subsequent works on the same data set used a 4-Hz high-pass filter to address this requirement Ang et al. [2012]\nTangermann et al. [2012] Schirrmeister et al. [2017].\nThus, for our first ChatBCI project, we set two objectives:\n\u2022 Perform an exploratory data analysis of the BCI Competition IV 2a data set\n\u2022 Design, implement and test a convolutional neural network (CNN) decoder with a good tradeoff of simplicity\nand hence interpretability on the one side, and meaningful decoding results in terms of decoding accuracies on\nthe other, including a corresponding within-subject training loop.\nWe addressed these objectives using the ChatBCI toolbox in a human-AI collaborative framework (Stage 2 \"Experiment\nIteration\" according to our generic research cycle plan 1)."}, {"title": "3.2 Experimental interation", "content": "Basic steps such as such data epoching, re-referencing, or filtering could be conveniently implemented, typically\nwith a few prompts per step. The key functionality of the ChatBCI toolbox enabling this smooth workflow is the\nknowledge base with the relevant background knowledge and specific instructions, provided at different levels of detail\nand aggregation, such that the relevant information can be efficiently kept in the LLM context.\nMore fine-grained instructions were (as expected) needed for data visualization. Figures 3, 4 (A and B) and 5 are\ncollaboratively ChatBCI-generated. Code generation and running the experiments were the two steps with the highest\nlevel of AI autonomy (see Fig. 1)"}, {"title": "3.2.1 Exploratory data analysis and data validation", "content": "Analysis for data exploration and validation let to several interesting observations. ChatBCI suggested initially three\nstarting steps for data validation and exploration. First, examine the basic signal statistics to understand the distribution\nand variability of the EEG data across classes. This involves calculating the mean, standard deviation, and variance for\neach channel and class, identifying systematic differences in signal variance, and detecting outliers, which may indicate\nartefacts. Second, analyze event-related potentials (ERPs) to investigate time-locked neural activity by segmenting the\ndata around key events, calculating average ERP waveforms for each class, and identifying distinctive features such as\nP300 peaks while ensuring the observed differences are neural rather than artefact-related. Third, explore class-specific\ndifferences in the frequency domain by computing power spectral density (PSD) for each class and channel, identifying\nfrequency bands where power varies, and detecting potential artefacts such as line noise or muscle activity.\nThe questions whether we can indeed expect a P300 as the dominating effect in the ERPs based on this dataset, and\nwhat we can expect, revealed a mixture of accurate and wrong assumptions (e.g., correctly expecting visually and\nauditory-evoked potentials (VEPs, AEPs) elecited by the respective cues, but also expecting a readiness potential which\nin fact is associated with self-paced tasks Ball et al. [1999]), illustrating the necessity of expert knowledge transfer.\nERP results (EEG common average rereferenced, 40 Hz low-pass filtered) are shown in Fig. 3, for 5 EEG and 2 EOG\nchannels.\nNext, we asked for an interpretation of this plot with the prompt \"this figure shows plots of the ERPs (mean across\nall trials of all subjects for each of the 4 classes). What do we see here, how can we interpret this result?\" with the\nresponse which we fully reproduce here (in blue font):\n\"The provided figure shows event-related potentials (ERPs) averaged across all trials and subjects for each of the 4\nmotor imagery classes (tongue, both feet, right hand, and left hand). Each subplot corresponds to a different electrode\nor channel (e.g., EOG1, Fz, C3, Cz, C4, Pz), with voltage plotted as a function of time (0\u20134000 ms).\nKey Observations\n1. Cue-Evoked Potentials:\n\u2022 For channels like Fz, Cz, and Pz, there is a clear deflection around 200\u2013300 ms after the cue onset, which\ncould represent the early processing of the cue. This is consistent with components such as N1/P2 (cue-related\nprocessing).\n\u2022 These components are visible across all classes, indicating that these potentials are not class-specific but\ninstead reflect the shared cognitive processing of the presented cue.\n2. Motor-Related Activity:\n\u2022 Channels C3, Cz, and C4 are primarily associated with motor areas:\nFor C3 (left motor cortex), the right hand class shows a small deviation compared to other classes,\nconsistent with contralateral activity.\nFor C4 (right motor cortex), the left hand class exhibits a subtle difference, though less pronounced.\n\u2022 Both feet and tongue motor imagery do not show distinct deflections in these motor channels, likely due to\ntheir bilateral or non-cortical representation.\n3. Eye Movement Artefacts:\n\u2022 EOG1 and EOG3 channels clearly show larger deviations, especially for the tongue and both feet classes.\nThis suggests that these classes may involve stronger or unintended eye movement artefacts, particularly in\nthe early time window (0\u20131000 ms).\n\u2022 These artefacts could partially overlap with class-specific ERP components and may need to be addressed\n(e.g., through filtering or artifact rejection).\n4. Overall Class Differences:\n\u2022 While class-specific differences are subtle, the right hand and left hand classes show slight distinctions in the\nC3 and C4 channels, corresponding to their expected motor areas.\n\u2022 The both feet and tongue classes show fewer clear class-specific ERP features, and their differences may\ninstead emerge more clearly in the frequency domain (e.g., mu/beta desynchronization).\nInterpretation\n\u2022 Cue-Related Potentials: These are robust across all classes, primarily seen in midline channels (e.g., Fz, Cz,\nPz), reflecting the shared processing of visual stimuli.\n\u2022 Motor-Specific Activity: Subtle differences in the motor-related channels (C3, C4) align with the expected\ncontralateral activation for hand motor imagery tasks.\n\u2022 Artefact Contribution: Large deviations in EOG channels suggest that some ERPs might be influenced by\neye movement artefacts, particularly for classes like tongue and both feet.\"\nThis interpretation addresses many of the relevant aspects and in our opinion demonstrates a very promising starting\npoint for a useful visual understanding of results in the EEG and BCI research context of. The three major signal\nsources that need to be considered (cue-evoked, movement-related, and ocular artifacts) are correctly identified and\ndiscussed, with several valid points such as that class differences can be expected to be clearer in the frequency domain.\nHowever, the conclusion that ocular effects mainly show in the tongue and feet movement classes is not likely given the"}, {"title": "3.2.2 LLM-generated deep neural network decoder and training loop", "content": "Not surprisingly, given the well-documented abilities of LLMs in code generation (Rozi\u00e8re et al. [2024]), ChatBCI was\nable to design and implement both deep networks for EEG decoding and working training loops, including plausibly\nmotivated data-augmentation strategies. Figure 5 shows results obtained with a simple network consisting of a sequence\nof spatial and temporal convolutions, using batch normalization, dropout, and a SWISH activation function, with\nwithin-subject training using the original competition data split.\nResults showed clear learning on the validation set across all subjects, reaching accuracies, on the one hand, clearly\nabove chance, and on the other hand, also substantially below previous reports Zhang et al. [2021]. We consider our\nsimple models as a good starting point for, in a next step, implementing interpretability tools, and gradually moving\nto more complex decoding models. Setting up the network as well as the training loop used in this example, in total\nrequired less than 10 prompts."}, {"title": "4 Conclusions", "content": "This paper introduced the ChatBCI toolbox as a practical and innovative solution to foster human-AI collaboration in\nthe fields of Brain-Computer Interface (BCI) research and brain signal analysis. By adopting our Janusian Design\nPrinciples, we demonstrated how human-AI teaming can address challenges in BCI development, such as small\ndatasets, complex signal dynamics, and knowledge transfer. Specifically:\n1. Collaborative Human-AI Workspaces: The ChatBCI framework was built to integrate the strengths of both\nhuman expertise and AI capabilities, providing an adaptable, interactive workspace that supports all phases of\nscientific research, from idea generation to experimental analysis and result interpretation.\n2. Application to Motor Imagery Decoding: Using the ChatBCI toolbox, we explored the BCI Competition IV\n2a dataset and implemented a deep learning decoder in a collaborative, time-efficient manner. This process\nhighlighted the utility of ChatBCI for accelerating workflows, identifying domain-specific challenges, and\nfostering human-AI co-learning.\n3. Transfer of Expertise and Knowledge: The structured interaction framework enabled effective transfer of\nnon-obvious, domain-specific knowledge from human experts to the AI, ensuring meaningful co-learning. The\nAI, in turn, proved to be a valuable partner, suggesting analyses, generating code, and adapting to user inputs.\n4. Scalable Design for Broader Neurotechnological Applications: While demonstrated for a specific EEG\ndataset, the principles and methods behind ChatBCI are extensible to a wide range of neuroscience and\nneurotechnology challenges, laying a foundation for future advancements in brain research and AI-driven\ndiscovery.\nBy merging the adaptability of AI systems with the nuanced understanding of human researchers, ChatBCI represents a\nstep forward in achieving efficient and collaborative research processes. Our findings indicate that human-AI teaming, as\nenabled by ChatBCI, holds significant promise for accelerating scientific discovery while maintaining the interpretability\nand trust essential for impactful advancements in neuroscience and BCI.\nWe envision a future where in some scenarios brain-computer interfaces (BCIs) transcend traditional keyboard inputs,\nreplacing them with direct interaction through physiological signals a transformative goal of BCI research and\ndevelopment. For instance, advances like Meta's silent speech decoding demonstrate the potential of such technologies\nto enable seamless communication between humans and machines D\u00e9fossez et al. [2023]. This innovation aligns\nperfectly with the evolving paradigm of human-AI collaboration, where neural interfacing could ensure seamless"}]}