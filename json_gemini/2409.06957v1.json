{"title": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation", "authors": ["Chuheng Zhang", "Wei Shen"], "abstract": "Reinforcement learning from human feedback (RLHF) is one of the key techniques that helps large language models (LLMs) to follow instructions and provide helpful and harmless responses. While direct policy optimization methods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in RLHF to train the policy to generate good responses guided by a reward model learned from preference data. The main challenge of these methods is the inaccuracy of the intermediate reward model, especially in code generation tasks that require long and complex reasoning to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtration strategy for a given reward model, the coefficient of determination (R2) between rewards and actual scores on filtered samples serves as a good metrics and helps us find several promising strategies. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation tasks, and find that some variants of PF-PPO are highly effective and achieve new state-of-the-art performance across 7-billion-parameter models on HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning from Human Feedback (RLHF) becomes a key technique to align large language models (LLMs) with human values and preferences. RLHF has been proven to be an essential process for LLMs to produce more helpful, harmless, and honest responses. Despite various non-RL algorithms such as DPO are proposed, state-of-the-art applications such as ChatGPT/GPT-4, Claude, and Gemini adopt the RL algorithm (e.g., PPO) for policy optimization. The key challenge of RLHF is the inaccuracy of the intermediate reward model. While there are researchers investigate how to learn reliable reward models, we focus on how to learn better policy under the guidance of such inaccurate reward models.\nWe observe that, though the reward models generally give inaccurate rewards to responses, it can be more reliable in specific regions (e.g., when it give high rewards) than the others. We use a policy model fine-tuned for code generation to generate a set of responses for prompts in the HumanEval dataset. Later, we score these responses using a reward model trained with the common recipe and compare them with the actual scores. We find that, across different sets of samples, the reward model is more reliable when it gives high rewards than when it gives moderate rewards. Considering that PPO updates the policy based on the advantages"}, {"title": "2 Preliminary", "content": "Notations. We use [a, b] to denote the set {a, a + 1,\u2026\u2026\u2026, b} and use [b] as the shorthand for [1, b]. We use to denote the concatenation on tokens, and use xa:b as the shorthand for the concatenation . We use ci and yj to indicate the individual tokens with concatenation being context c and response y respectively.\nMDP formulation. We adopt a Markov decision process (MDP) formulation for RLHF. Language generation is formulated as an MDP M = (S, A, P, R) with states s \u2208 S, actions a \u2208 A, transition probabilities P\u2208 \u2206(S)S\u00d7A, and the next-state-based reward function R : S \u2192 [0,1]. Given a"}, {"title": "3 Related Work", "content": "Limitation of the reward model. The outcome of RLHF highly relies on the quality of the reward model. Unfortunately, the reward model can hardly provide accurate scores due to 1) the mis-specified reward modeling to represent human preferences; 2) the presence"}, {"title": "4 Methods", "content": "Our method is motivated by the observation that the reward model is more reliable for the responses assigned with high rewards (cf. Figure 1). Consequently, we conjecture that, if we wrap the policy with proper filtration during policy optimization of RLHF, the reward model can avoid yielding unreliable rewards and thus give better signal to guide policy learning.\nPolicy filtration. Given an unfiltered policy model \u03c0\u03bf(y|c) that generates responses y to the context c, we denote the filtered policy as \u00b5o (y|c). We consider a family of policy filtration, from which we can sample responses to the context c as follows: We first sample N responses from \u03c0\u03bf(\u00b7|c) and rank them by the reward model R\u00f8, obtaining y1,\u2026\u2026, yn with R$(y1|c) \u2265 \u2265 R$(yN|C). Then, given a weight vector w = (w\u2081,\u2026\u2026,\u03c9\u03bd) satisfying \u2211i\u2208[N] Wi = 1, we sample a one-hot vector z = (\u22481,\u2026, ZN) from the categorical distribution parameterized by w such that P[zi = 1] = Wi. At last, the filtered policy \u03bc\u0473(\u00b7|c) yield the response selected by z, i.e., y = \u2211i\u2208[N] ZiYi\u00b7\nWe can define several filtered policies under this family. Specifically, we obtain the best-of-N (BoN), best-random (BR), and best-worst (BW) filtered policy by setting wBoN = (1,0,...,0),\nWBR = (2/(N(N-1)) ,2/(N(N-1))), and wBW = (1,0,0,0,1) respectively."}, {"title": "5 Experiments", "content": "To demonstrate the effectiveness of our method, we conduct experiments on the code generation task, which is a typical reasoning task where the quality of the responses from code LLMs can be precisely measured. Specifically, we compare different algorithms on three widely used benchmarks:"}, {"title": "5.1 Benchmarks", "content": "HumanEval benchmark and MBPP benchmark. HumanEval and MBPP are two popular benchmarks for evaluating code LLMs. HumanEval consists of 164 hand-written Python problems, each of which is validated using test cases to assess the accuracy of the code generated by a code LLM in a zero-shot setting. Similarly, MBPP includes 378 test problems, which also feature problem descriptions, code solutions, and test cases to evaluate the model's ability to generate correct code. Both benchmarks play crucial roles in understanding the performance of large language models on code generation tasks.\nLeetCode contest benchmark. To further evaluate the capability of the model on real-world programming problems, we construct the LeetCode Contest benchmark. This benchmark includes competition-level problems designed for human, and therefore is more challenging since it require human-level problem understanding and code generation skills. In this benchmark, we collect 160 problems from LeetCode weekly contests from July 2022 to January 2024. For each problem, we include 100 test cases to ensure the generated code is assessed thoroughly."}, {"title": "5.2 Datasets and Pre-processing", "content": "For our experiments on the HumanEval and MBPP benchmarks, we select data from the 75k Magicoder-OSS-instruct dataset and the 55k evol-codealpaca-v1 dataset to construct the SFT dataset, the reward model dataset, and the PPO query dataset. Specifically, we use all the 130k training samples from Magicoder-OSS-instruct and evol-codealpaca-v1 as the SFT dataset. To train a reward model, we curate 7k prompts from these 130k samples and generate five responses using the SFT model for each prompt. Following the methodology in Pal et al. (2024), we select two responses with the maximum edit distance to create response pairs for each prompt. These 7k prompts with generated response pairs constitute the reward model dataset. For policy optimization, we curate 3k prompts from the 130k samples as the PPO query dataset.\nFor the LeetCode benchmark, we construct LeetCode training datasets comprising 1,000 problems collected from the LeetCode website. For SFT, we use self-generated correct answers to create the SFT dataset following the methodology in Setlur et al. (2024). For reward modeling, we generate five responses using the SFT model for each of the 400 curated prompts and selected two responses with the maximum edit distance to form the response pairs for each prompt. We use these prompts and response pairs to train the reward model. Finally, we used the full 1,000 prompts as our PPO query dataset to train the code LLM."}, {"title": "5.3 Implementation Details", "content": "We use deepseek-6.7B as our base model. In the SFT phase, we train on the SFT dataset for 5 epochs with the learning rate 1 \u00d7 10-5, resulting in the SFT policy. In the reward model training phase, we follow Ouyang et al. (2022) and train on our reward model dataset for 1 epoch with the learning rate 1 \u00d7 10\u22125. In the PPO phase, we adopt the training tricks from the blog. Specifically, we adopt reward normalization and advantage normalization for stable training. In addition, we set the learning rate for the policy network as 5 \u00d7 10-7 and learning rate for the value network as 5 \u00d7 10\u20139. In the PPO algorithm, we collect responses for the context in the PPO query dataset and iterate through this dataset for 5 iterations (enough for convergence) and select the best checkpoints on evaluation set as the outcome policy. For each collected context-response pair, we use it to accumulate loss and gradient for 3 times on average. We use full parameter fine-tuning in all the phases. We provide the source code for all experiments on https://github.com/swtheing/PF-PPO-RLHF."}, {"title": "5.4 Baselines", "content": "We compare different variants of PF-PPO with not only reinforcement learning algorithms but also supervised fine-tuning methods and direct policy optimization methods. We use greedy decoding during inference and pass@1 as the performance metrics. For fair comparison between different baselines, we re-implement these baselines with the same code base and the same datasets. We also use the same reward model and the same SFT policy if applicable.\nSupervised fine-tuning. Starting from deepseek-6.7B, we first fine-tune this policy on the SFT dataset. Other algorithms learn based on this SFT policy. RAFT and BOND train the policy to fit the best-of-N (BON) responses or the BoN policy via different supervised learning losses. RAFT maximizes the log-probability of the BoN response, whereas BOND minimizes a combination of the forward and backward KL divergence w.r.t. the BoN policy. We set the coefficient to combine these two loss terms as . BOND is an iterative algorithm to fit the BoN policy based on the policy of the last iteration, and we train the policy for 4 iterations.\nDirect policy optimization. To implement direct policy optimization methods, we use our reward model dataset as the preference dataset required in these methods. We implement DPO, IPO, KTO, and iterative DPO. For iterative DPO, we train the DPO model for three iterations. For each iteration, we construct the preference dataset as follows: The prompts are sampled from the reward model dataset and responses are generated by the trained DPO model from the previous iteration (if exists) or the previous SFT phase.\nReinforcement Learning. For standard RLHF, we use the implementation from OpenRLHF, which incorporates several advanced PPO training techniques and has demonstrates strong performance on various benchmarks. We denote this baseline as PPO-S. For our method PF-PPO, we implement three variants (BoN, BR, and BW) as introduced in the previous section. Since PF-PPO collects multiple responses given a prompt/context, we introduce a baseline called PPO-M (PPO with multiple responses) that uses all the N responses for training without filtering. Comparing with PPO-M can help us distinguish the effect of collecting multiple responses and that of filtering collected responses. The difference between PPO-S and PPO-M is that the buffer B in PPO-M contains more samples with the same context c but with different responses y which may provide detailed token-level instruction by comparing the responses corresponding to the same context. PPO-M can also be regarded as integrating GRPO into PPO, which has been adopted by Deepseek-V2 and Qwen2. We refer the readers to Section 5.7 for the analysis on the computational efficiency of PPO-S, PPO-M, and PF-PPO."}, {"title": "5.5 Experiment Results on Three Benchmarks", "content": "We present the pass@1 results of different methods on the three benchmarks in Table 2. The experiment results show that PF-PPO (BR) and PF-PPO (BW) obtain the highest scores on these benchmarks, indicating the effectiveness of our method. Furthermore, we have the following observations:\n\u2022 IPO and KTO (improved versions of DPO) do not outperform DPO when trained on properly selected datasets. This indicates that appropriate dataset construction can address some weaknesses of DPO, enabling DPO to achieve a performance comparable to its improved versions.\n\u2022 PPO-based algorithms outperform SFT-based and DPO-based algorithms in general, demon-strating that PPO is superior to these algorithms on reasoning tasks. We speculate that the good performance of PPO may stem from the value network used in PPO, which can be used to transform trajectory-level reward modeling to token-wise advantages and thus provides more fine-grained guidance. Moreover, the gap between PPO-based algorithms and the others becomes larger on the more challenging LeetCode benchmark, which further highlights the advantage of RL on complex reasoning tasks\n\u2022 BOND achieves the highest score among the baseline methods. It demonstrates that iterative best-of-N (BON) distillation is an effective alignment approach. We speculate that BOND also benefits from its ability to reduce learning on samples with unreliable rewards by selecting the best candidate from a set of N samples.\n\u2022 Motivated by the good performance of BOND, we implement PF-PPO (BON) as a natural attempt to apply BoN to an RL-based algorithm. However, PF-PPO (BoN) results in poor performance. This indicates that compared with SFT methods that only need good samples, bad samples for contrastive learning is also important for RL-based methods. This explains the reason why PF-PPO (BR) and PF-PPO (BW) outperform PF-PPO (BoN).\n\u2022 PF-PPO (BR) and PF-PPO (BW) outperform the others with a larger gap challenging LeetCode tasks. We find that the accuracy of the reward model decreases on this benchmark since it is more difficult for the reward model to distinguish whether one response is better than another, especially when both responses contain errors. This decreases the reliability of the reward model in the moderate reward region (cf. Figure 1). Consequently, PF-PPO (BR) and PF-PPO (BW) try to avoid learning on unreliable rewards and thus can improve the performance in these complex reasoning tasks."}, {"title": "5.6 Choosing from Different Policy Filtering Strategies", "content": "PF-PPO modifies the sampling procedure of standard PPO by sampling N responses and randomly filtering responses based on their ranks. In this part, we consider other alternatives to filter by threshold or down-weight the responses with unreliable rewards in the sampling procedure.\n\u2022 Filtering based on reward thresholds. Given a reward model, we can filter the responses based on their rewards using specified threshold. This results in three strategies, PPO-top that only keeps the top samples whose rewards exceeding a certain threshold, PPO-top-random that keeps also keeps random samples with 50% probability, and PPO-top-bottom that keeps top samples and bottom samples whose rewards are below another specified threshold. These strategies can be regarded as the threshold version of PF-PPO (BoN), PF-PPO (BR) and PF-PPO (BW) respectively. The thresholds are tuned coarsely to achieve good results on a separate validation set.\n\u2022 Filtering based on reward reweighting. Compared with the above strategies that use thresholds, we consider a softer version that adjusts the sample weights based on their rewards, aiming at down-weight the samples with moderate and possibly unreliable rewards. Specifically, we increase the sample weight of the responses with rewards in the reliable region and decrease the sample weight otherwise. To achieve this goal, given a reward model R that returns rewards in the range [-1,1], we assign the weight for the sample (c, y) proportional to |R\u2084(y|c)|k and collect samples with these weights from the buffer B to train the policy network and the value network. We denote these strategies as PPO-pow-k."}, {"title": "5.7 Further Analysis", "content": "Computational efficiency of PPO-S, PPO-M, and PF-PPO. PPO-S, PPO-M, and PF-PPO collect different number of responses per query and train using different number of samples. For clarity, we list the computational complexity of these algorithms in Table 4. Note that, for all algorithms, we select the best checkpoint on the evaluation set and report the performance of this checkpoint. We can draw the following conclusions: First, the total computational complexity of PPO-S and PPO-M is almost the same, and the only difference is that PPO-M is more likely to learn from different responses with the same query in the same batch or adjacent batches, which improves the performance. Second, the computational complexity of PF-PPO is less than that of PPO-S and PPO-M, while PF-PPO outperforms them. This indicates the effectiveness of our method.\nThe training process of PPO-S, PPO-M, and PF-PPO. To provide a comprehensive view of the three algorithms, we show the training process."}, {"title": "6 Conclusion", "content": "In this paper, we propose a new reinforcement learning with human feedback (RLHF) method, Policy Filtration for Proximal Policy Optimization (PF-PPO), aimed at mitigating the adverse effects of reward noise. When training the reward model using the Bradley-Terry approach, the reward signal is generally more reliable in the high or low reward regions but less reliable in the moderate reward regions. Motivated by this observation, we adopt a rank-based method to selectively use sample from these reliable regions more in PPO to improve the quality of the signal provided by the"}]}