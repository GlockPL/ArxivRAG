{"title": "Deep-Motion-Net: GNN-based volumetric organ\nshape reconstruction from single-view 2D\nprojections", "authors": ["Isuru Wijesinghe", "Michael Nix", "Arezoo Zakeri", "Alireza Hokmabadi", "Bashar Al-Qaisieh", "Ali Gooya", "Zeike A. Taylor"], "abstract": "We propose Deep-Motion-Net: an end-to-end graph neural\nnetwork (GNN) architecture that enables 3D (volumetric) organ shape\nreconstruction from a single in-treatment kV planar X-ray image ac-\nquired at any arbitrary projection angle. Estimating and compensating\nfor true anatomical motion during radiotherapy is essential for improving\nthe delivery of planned radiation dose to target volumes while sparing\norgans-at-risk, and thereby improving the therapeutic ratio. Achieving\nthis using only limited imaging available during irradiation and with-\nout the use of surrogate signals or invasive fiducial markers is attractive.\nThe proposed model learns the mesh regression from a patient-specific\ntemplate and deep features extracted from kV images at arbitrary projec-\ntion angles. A 2D-CNN encoder extracts image features, and four feature\npooling networks fuse these features to the 3D template organ mesh. A\nResNet-based graph attention network then deforms the feature-encoded\nmesh. The model is trained using synthetically generated organ motion\ninstances and corresponding kV images. The latter is generated by de-\nforming a reference CT volume aligned with the template mesh, creating\ndigitally reconstructed radiographs (DRRs) at required projection an-\ngles, and DRR-to-kV style transferring with a conditional CycleGAN\nmodel. The overall framework was tested quantitatively on synthetic\nrespiratory motion scenarios and qualitatively on in-treatment images\nacquired over full scan series for liver cancer patients. Overall mean pre-\ndiction errors for synthetic motion test datasets were 0.16\u00b10.13 mm,\n0.18\u00b10.19 mm, 0.22\u00b10.34 mm, and 0.12\u00b10.11 mm. Mean peak predic-\ntion errors were 1.39 mm, 1.99 mm, 3.29 mm, and 1.16 mm.", "sections": [{"title": "1 Introduction", "content": "In this paper, we present a method, labelled Deep-Motion-Net, for estimating 3D\nvolumetric organ deformation from a single in-treatment kV planar X-ray image\nacquired at any arbitrary gantry (projection) angle. Internal anatomical motion\nconfounds the precise delivery of radiation to target volumes during external\nbeam radiotherapy. Precision is critical for achieving tumour coverage while pre-\nserving surrounding sensitive healthy tissues( [1]) since normal tissue damage\nprevents dose escalation to the gross target volume (GTV) to the desired ther-\napeutic level. In hypo-fractionated treatments (i.e. Stereotactic Ablative Body\nRadiotherapy), wherein higher, but more precisely targeted doses are used, un-\naccounted motion is yet more critical and sometimes prohibitive. Hence, to fully\nexploit the potential of external beam radiation, tumour and organ movements\nmust be addressed during irradiation such that more radiation is delivered to\nthe target tumour while sparing organs-at-risk (OARs).\nTypically, a cone beam CT image is acquired at the start of treatment and\nused to ensure the patient is correctly positioned with respect to the linear\naccelerator (linac). However, this static single-timepoint representation ignores\nanatomical motion that subsequently occurs during radiation delivery, which\nmay cause overdosing of OARs, or underdosing of tumour, leading to poorer out-\ncomes for survival and post-treatment morbidity ([1]). To deal with respiratory-\ninduced tumour/organ motions, numerous mitigation strategies have been devel-\noped, broadly categorised as either passive or active ([1,2]). Defining an internal-\ntarget-volume (ITV) that includes a motion-encompassing margin is an example\nof a passive mitigation technique( [2]). The motion-encompassing margin is nor-\nmally estimated based on 4D-CT data acquired at treatment planning and hence\nreflects an average estimate of the motion that corresponds with in-treatment\nmotion with uncertain accuracy. While these margins allow for inaccuracies dur-\ning treatment, they also result in greater irradiation of normal tissues( [3]),\nmeaning overall radiation intensity must be reduced, and treating the target\ntumour becomes more difficult.\nTo minimise, or even eliminate the ITV, effective active motion management\nis critical. Respiratory-gating is a popular method since it is relatively easy\nto implement, but it implies a longer time to deliver the specified dose because\nradiation is only delivered for a segment of the respiratory cycle( [1]). In contrast,\nreal-time tracking, which repositions and/or reshapes the radiation beam as\nthe target moves, implies no prolongation of treatment sessions but is more\ntechnically challenging to realise. Moreover, its effectiveness can be limited by\nthe time delay between detecting a change in target position and the system\nadjustment, resulting in a persistent lag in the system's response to the target\nposition.\nAll such active methods critically rely on real-time information on the tu-\nmour position during treatment. Gating and tracking techniques often rely on\nimplanted markers to track the target mobility in real-time. These markers are\ninvasive, and in any case, only provide information on specific locations (i.e.,\nmarker positions) inside tissues, rather than the target/OARs as a whole( [4])."}, {"title": "2 Previous work: 3D shape reconstruction from\nsingle-view projections", "content": "We here review existing techniques for recovering 3D geometry from 2D images,\ncomparable to our approach. Another major class of techniques are so-called\nsurrogate-driven models, which estimate internal anatomical motion using some\nsurrogate signal under the assumption the two are well correlated. For further\ninformation on such approaches, interested readers are referred to [7]."}, {"title": "2.1 RGB image-based reconstruction", "content": "Several reports have described techniques for reconstruction from RGB images.\nFor example, [8] proposed the GNN-based Pixel2Mesh algorithm, which deforms\nan ellipsoidal surface mesh using CNN-derived semantic characteristics from\nan input image, and applies it to the analysis of natural shapes (aeroplanes,\nchairs, cars, etc.). The ellipsoidal starting mesh limits the approach to genus-0\nshapes, though in principle it could be adapted to other topologies. [9] extended\nthe method to better capture local surface geometry, though the topological\nconstraints remained. Similar ideas were used in [10,11] to reconstruct 3D human\nbody shapes from single RGB images. In the medical domain, [12] proposed a\nCNN architecture for reconstructing 3D lung shapes, in the form of point clouds,\nfrom a single-view 2D laparoscopic image."}, {"title": "2.2 Projection image-based reconstruction", "content": "While clearly sharing elements of our target problem, reconstruction from RGB\nimages rather than 2D projections is nonetheless a substantially different one.\nVarious approaches addressing the latter scenario have appeared recently. [13]\nproposed X2CT-GAN to reconstruct 3D-CT volumes from bi-planar 2D X-ray\nimages using generative adversarial networks. Reconstruction of full CT volumes\nis impressive, however, in our scenario only single-view projections are available,\nmaking the problem significantly more challenging.\n[14] proposed a CNN-based approach for reconstructing lung surface shapes\nfrom single-view 2D projections. Their method employs free-form deformation\nto learn the optimal smooth deformation from multiple templates to match the\nquery 2D image. [15] proposed X-ray2Shape to reconstruct 3D liver surface\nmeshes by combining GNN and CNN networks (the latter to encode image fea-\ntures). A mean shape derived from 124 patients was used as prior (i.e. initial\ntemplate) and deformed by the GNN to match the individual organ shape. Later,\nthe same authors [16,17] extended the approach to reconstruct multiple abdomi-\nnal organ shapes from a single projection image. These approaches [14,15,16,17]\nare designed to operate on images acquired at fixed projection angles-front\nview projections, equivalent to gantry angle 0 in our case and therefore cannot\ndirectly accommodate images from arbitrary angles, as required here.\nThis limitation was addressed by [18], who proposed a CNN architecture\nto predict principal component analysis (PCA) coefficients in a 4D-CT-based\nbreathing motion model. The authors used an angle-dependent region of inter-\nest (ROI) 2D projection mask to remove pixels unrelated to respiration and an\nangle-dependent fully-connected (FC) layer. This layer was designed to handle\ndiscrete (integer-valued) angles ranging from 0 to 360, and only a single group\nof weights and biases were used to generate its output for each degree of the\nprojection angle. For real, continuously varying angles, binary projection masks\nat the nearest integer were chosen. However, challenges appeared in cases with\nsignificant intensity variations between DRRS and CBCT projections, affecting\nlocalization accuracy due to intensity shift issues. Additionally, reconstruction"}, {"title": "2.3 Recapitulation", "content": "Several approaches to reconstructing organ geometry from single-view projection\nimages have been proposed. Their main shortcomings, addressed by our proposed\nmethod, are as follows:\nFixed projection angle: Most existing approaches [14,15,16,17,19] are de-\nsigned to operate on images acquired at fixed projection angles rather than\nthe varying angles used during RT.\nSurface reconstruction only: Most existing approaches [14,15,16,17,18] re-\nconstruct only surface representations of the involved organs and do not,\ntherefore, describe deformations within those organs.\nWide field of view: Most existing approaches [14,15,16,17,19] depend on DRR\nimages with a wide FOV, not reflective of the usual clinical situation."}, {"title": "3 Methodology", "content": "Building on several of the works cited above, the core of our approach is a\nGNN that learns mappings from kV image features to nodal displacements of a\npatient-specific organ mesh. The model is trained individually for each patient.\nThis section provides a comprehensive description of the components of the\napproach, including the 3D organ representation, model architecture, and loss\nfunctions."}, {"title": "3.1 3D organ shape representation", "content": "We use a 3D unstructured tetrahedral mesh to describe the volumetric organ\nshape, rather than its surface only. The mesh can be described as an undirected\ngraph $G = {V, E, F}$, where V is the set of N vertices in the mesh, E represents"}, {"title": "3.2 Model architecture", "content": "The proposed architecture (shown in Figure 1) consists of two components: a\n2D-CNN encoder with four learnable feature pooling networks (FPNs), which\nextract perceptual features from the kV image and attach them to mesh nodes\nand a GAT-based mesh deformation network.\nInputs to the 2D-CNN encoder are a single-\nIncorporating projection angle\nview kV image and its corresponding projection angle. We first normalise the\nprojection angle by dividing it by 360 since the gantry rotation varies from 0 to\n360 degrees. Before sending the input image to the image encoder, we concate-\nnate the projection angle information as an extra channel. This concatenation\npart is accomplished by expanding and reshaping the projection angle to align\nwith the spatial resolution of the input image (i.e. 256x256).\nThe projection angle-dependant perceptual features\nof the image are then extracted using four convolutional layers in the image en-\ncoder, containing 16, 32, 64, and 128 filters, respectively. For all convolutional\nlayers, the kernel size was set to 3\u00d73, and stride and padding were both set to\n1. Exponential Linear Units (ELUs) were applied for non-linearity after every\nconvolutional operation. Hidden layer outputs were normalised using group nor-\nmalisation ([20]) since this helps reduce the internal covariate shift, which regu-\nlarly alters the distribution of the hidden-layer activations during model training.\nOutput feature maps of each convolutional layer were then down-sampled using\n2\u00d72 max-pooling layers with stride two before passing into the next layer.\n2D-CNN configuration\nWe integrated learnable feature pooling networks\n(FPNs) to efficiently learn optimal associations between image features and mesh\nnodes in our architecture. This utilization of FPNs allows the network to max-\nimise the predictive performance through end-to-end training, eliminating non-\ntrainable components and enabling full optimization of feature learning capabil-\nities for the downstream graph network. We use four FPNs, each coupled with\nits respective CNN convolutional layer (Figure 1). Each FPN has two layers: an\nadaptive average pooling layer (AAP) and a FC layer. The AAP, with output\nsize 7\u00d77, is applied over the output of the corresponding convolutional layer to\nreduce the dimension so that an input feature map with dimensions H\u00d7W\u00d7C\nis reduced to 7 \u00d7 7 \u00d7 C. The output is flattened before sending it into the FC\nlayer, which contains 5N neurons. We reshaped the output feature vector of each\nFPN into (N, 5). These outputs were then concatenated with 3D coordinates\nFeature pooling networks"}, {"title": "3.3 Loss functions", "content": "The overall objective function Lis defined as:\n$L = LShape + \\lambda LLaplacian$,\n(1)\nwith weighting term \u03bb. $LShape$ quantifies the difference between the predicted\nand ground-truth 3D meshes. The template mesh starting shape is defined by its\nvertices V (Sect. 3.1). We define Y and \u0176 to be the ground-truth and predicted\ndeformed positions of these vertices. An intuitive objective is then to minimize\nthe per vertex $L_1$ loss between Y and \u00dd:\n$LShape = \\sum_{i=1}^{N} ||\\hat{Y}_i - Y_i||$\n(2)\nwhere Y and \u0176; are the ith vertices in the respective sets.\nUsing $LShape$ alone the vertices were found to move too freely. We introduced\na discrete Laplacian ([8]) loss $LLaplacian$ as a regularization term to limit this\nfreedom by ensuring vertices do not move too far in relation to their neighbours.\nThe additional constraint ensures that the mesh has a smooth surface. The\ndiscrete Laplacian of a vertex \u0176 is denoted as:"}, {"title": null, "content": "$\n\\delta_{\\hat{Y}_i} = \\frac{1}{|S(\\hat{Y}_i)|} \\sum_{j \\in S(\\hat{Y}_i)} (\\hat{Y}_i - \\hat{Y}_j),\n$\n(3)\nwhere \u00dd; is a neighbouring vertex of \u00dd; and $S(\\hat{Y}_i)$ denotes the set of all such\nneighbours. The discrete Laplacian loss is then given by:\n$LLaplacian = \\frac{1}{N} \\sum_{i=1}^{N} ||\\delta_{Vi} - \\delta_{\\hat{Vi}} || $\n(4)\nwhere dv; and dy are the discrete Laplacian before and after the deformation,\nrespectively.\nIn our experiments, we used \u03bb = 0.1 to balance the weights of the two losses."}, {"title": "3.4 Implementation and training details", "content": "The complete network was implemented using PyTorch and PyTorch-geometric\n([25]). The model was trained using the Adam optimizer, with a learning rate of\n0.0002 and weight decay of 0.001. The batch size for both training and validation\ndatasets was set to 16. Group normalization was used to normalize the hidden\nlayer outputs of both 2D-CNN and GNN. We used single-headed attention in the\nGAT layers due to memory restrictions. Early stopping was employed to monitor\nthe validation loss with a patience of 30 consecutive epochs, and if the loss did\nnot improve, the optimizer itself stopped the training. However, if there was no\nprogress after eight consecutive epochs, the learning rate was decreased by a\nfactor of 0.8. All weights were initialized using the scheme in [26]. The gradient\ndescent converged after 400 epochs to the optimal solution. As described in A, we\nconducted a series of experiments to determine the optimal network components.\nThe training lasted approximately 36 hours with 256\u00d7256 image resolution on\na Nvidia Quadro RTX 4000 GPU using a Precision 7820 Tower XCTO Base\nworkstation."}, {"title": "4 Synthetic dataset generation", "content": "Model training requires paired sets of organ motion instances and corresponding\nkV images. However, to the best of our knowledge, there are no means of di-\nrectly measuring such motions while also acquiring the requisite images. Hence,\nwe use synthetically generated data to train and, in this work, to evaluate the\nmodel. Plausible patient-specific motion patterns are extracted from 4D-CT im-\nages, and new synthetic instances are produced by interpolating and, within\nreasonable bounds, extrapolating from these. The process is as follows: 1) 4D-\nCT images are analysed using the SuPReMo toolkit (Surrogate Parameterized\nRespiratory Motion Model - [27,28]), which produces, among other things, a\nmodel of the motion present in the images, linked with appropriate surrogate\nsignals; 2) new, yet plausible motion instances are generated from this model"}, {"title": "4.1 Generation of synthetic motion states from 4D-CT data", "content": "SuPReMo is a toolkit for simultaneously estimating a motion model and con-\nstructing motion-compensated images from a 4D-CT dataset. The resulting\nmodel describes the anatomical motion present in the raw data over a single\n(averaged) breath cycle and is linked with corresponding scalar surrogate signals\nderived, for example, from breathing traces or image features. For formulation\nreasons (see [27,28]), two surrogate signals are required; however, these needn't\nbe independent, and as a practical measure in each case we constructed the\nsecond signals as temporal derivatives of the first, computed for example using\nfinite differences. Example surrogate signals are plotted in Fig. 2. Our 4D-CT\ndatasets each decompose the breath cycle into 10 bins (phases).\nFirst, we fit SuPreMo's motion model for each individual case with input\nsurrogate signal and 4D-CT data. This returns the motion-compensated recon-\nstructed 3D-CT (MCR) image volume and the fitted motion model, which are\nthen utilised to simulate new motion states by varying the input surrogate sig-\nnal. Each point $s_i, i \\in [0,9]$ on the curve is randomly perturbed by a value in"}, {"title": null, "content": "$\nthe range \u00b10.4s\u2081. Extrapolating beyond this range is more likely to cause unre-\nalistic motion and even folding in the resulting images. A new surrogate signal\nis then created by fitting a 3rd order polynomial to the new points. The latter\nensures the new signal remains smooth over the full breath cycle. Examples of\nsuch generated signals are shown in Fig. 2. Using this new signal, and MCR as a\nreference volume, SuPReMo's motion model generates corresponding deformed\n3D-CT volumes and their related deformation vector fields.\nFor each test case (Sect. 5), we created 11 separate surrogate signals, each\ncomprising ten deformed configurations, resulting in 110 synthetic deforma-\ntion states in total. To introduce more diversity into the synthetic motion in-\nstances and substantially deviate from the original 4D-CT data, we incorporated\nrigid motions by applying random translations/shifts along the left-right (LR),\nanterior-posterior (AP), and superior-inferior (SI) directions and produced a\ntotal of 550 deformed states. This is also advantageous during testing on real in-\ntreatment kV images, as it reflects the variations in onboard patient setup across\ndifferent scan series or fractions, potentially leading to shifts in the in-treatment\nkV images.\n4.2 Generation of synthetic kV X-ray images\nFor each deformation state, DRRs were obtained from the deformed 3D-CT\nvolumes using the Siddon-Jacobs ray tracing technique ([29,30]). The latter,\ngiven a source position and projection direction, computes the line integral of\nHounsfield unit densities along ray lines to a prescribed 2D plane. In radio-\ntherapy, in-treatment kV images are acquired perpendicularly to the anatomical\naxial direction.\nThe resulting DRRs lack scatter properties and noise characteristics of gen-\nuine kV X-ray images, making them appear sharper and higher in contrast. We,\ntherefore, used a CycleGAN ([31]) by conditioning on projection angle to effect\nDRR-to-kV style transformation. The conditional CycleGAN was trained on an\nunpaired set of DRRs and real in-treatment kV images for each case separately\nsince the FOV acquisition varies from patient to patient. Histogram equalization\nwas first applied on the kV images to stretch the intensity histograms concen-\ntrated on a narrow interval to a broader range and increase contrast. All DRRS\nwere then passed to the conditional CycleGAN to create the final synthetic kV\nX-ray images."}, {"title": "4.3 Creation of template meshes", "content": "Binary masks of the relevant organs were extracted from reference 3D-CT vol-\numes using 3D Slicer's Segment Editor. These masks were then used to generate\ntetrahedral meshes using the Iso2Mesh( [32]) tool in MATLAB. The resulting\nmeshes were scaled and translated to reflect image voxel sizes and correct im-\nage origins. By these means, the meshes are physically aligned with the relevant\nanatomical regions in the 3D-CT volumes. Ground-truth positions of the mesh"}, {"title": "5 Model evaluation and results", "content": "nodes for each of the generated deformation states are produced by interpolating\nthe displacement vector field at the node positions."}, {"title": "5.1 Clinical datasets", "content": "We evaluated the Deep-Motion-Net framework using data from four liver cancer\npatients, with focus on liver motion. Each patient dataset comprised: 1) a 4D-CT\nwith 10 phases (i.e. 10 \u00d7 3D-CT volumes), spatial resolutions of 0.98 \u00d7 0.98 \u00d7 2.0\nmm\u00b3, and image dimensions of 512 \u00d7 512 \u00d7 105; and 2) two kV scan series, each\ncovering approx. 4-5 mins of free breathing and a full rotation of the treatment\ngantry, acquired at the start of treatment sessions on different days. As per\nclinical practice, the kV scans were centered on the liver region and used a\nconstrained FOV. As a result, for some projection angles, the images include\nonly a segment of the liver."}, {"title": "5.2 Experiments on synthetic data", "content": "We first evaluated the framework's ability to recover motion states synthetically\ngenerated from the clinical data, and for which ground-truth deformations were\ncorrespondingly available. For each patient, synthetic motion and image data\nwere generated as per Sect. 4. Each patient's 550 deformation states were then\nsplit into training, validation, and test sets in the proportions 350, 100, and\n100, respectively. For training and validation deformation states, 100 uniformly\nsampled (i.e. at projection angle intervals of 3.6\u00b0) synthetic kV images were\ngenerated (giving 35,000 training and 10,000 validation images). For test states,\n50 kV images were generated at randomly sampled projection angles (giving\n5,000 test images). To ensure good FOV alignment of the synthetic and real\nkV images for each patient, each synthetic image was generated using FOV\norigin header information from a corresponding (i.e. acquired at a comparable\nprojection angle) real image within one of the scan series. To avoid doubt, the\nreal kV images subsequently played no further role in the experiment. Model-\npredicted and ground-truth 3D liver shapes were compared for all synthetic kV\nimages in the test sets.\nSummary results are presented in Table 1. The study employed Euclidean\ndistance as the metric to evaluate the distance errors between ground-truth and\nestimated shapes. Distributions of mean and peak errors for each test set, and for\neach projection angle (divided into bins), are shown in Figure 3. Finally, surface\nrenderings of the ground-truth, reference, and predicted mesh shapes, overlaid\non deformed 3D-CT volumes, are presented in Figure 4.\nFor each of the four test sets the overall mean error was low: < 0.22 mm.\nWithin each test set, the maximum peak errors (i.e. overall max error found in\nany of the deformation states and at any node) were rather higher, ranging from\n4.36 mm for patient 4 to 14.66 mm for patient 3. These peak values occurred"}, {"title": "5.3 Evaluation on real kV images", "content": "Our second set of experiments used real in-treatment kV images from each pa-\ntients' second scan series (i.e. the series not used during training data creation).\nThese series contained 1378, 1310, 1320, and 1292 images for patients 1, 2, 3, and\n4, respectively. In the absence of ground-truth deformations, direct assessment\nof prediction errors is impossible. Therefore, two approaches were adopted: 1)\nsemi-quantitative assessment based on an image similarity metric between in-\nput real kV images and model-generated DRRs; and 2) qualitative assessment\nbased on overlaying model-predicted liver boundaries on input kV images. For"}, {"title": "5.4 Comparison model", "content": "We compared the performance of our approach with that of the recently pre-\nsented IGCN model ([16,17]). As described, like our approach, IGCN predicts 3D\norgan shapes from single-view X-ray images. It is, however, limited to images\nwith a constant projection angle (e.g. 0\u00b0, corresponding to anterior-posterior\nprojection), and predicts only 3D surface geometries rather than 3D volumetric\nconfiguration. To ensure fairness, we therefore conducted the comparison on this\nbasis.\nWe trained our model as previously described, and using the data described\nin sect. 5.2. To train the IGCN model, we first extracted surface meshes from\nthe ground-truth volumetric meshes in the training, validation, and test sets. We\nthen trained the model using the deformed surface meshes and associated syn-\nthetic kV images for projection angle of zero. Image dimensions were 256x256."}, {"title": "6 Discussion and conclusions", "content": "We presented a GNN-based model for recovering 3D volumetric organ mesh\ndeformation from a single in-treatment kV planar X-ray image acquired at arbi-\ntrary projection angles. This approach has several attractive features: it uses only"}, {"title": null, "content": "readily accessible in-treatment imaging capabilities, rather than expensive and\nrare systems like MRI; it requires no extra sensing to provide surrogate signals;\nand no invasive fiducial marker implantation. Moreover, the model is end-to-end\ntrainable, ensuring all components, and especially the image feature encoder,\nare optimised with respect to the overall prediction accuracy. To the best of our\nknowledge, this is the first example of a deep learning framework able to recon-\nstruct volumetric 3D organ models accurately from arbitrary-angled single-view\nimages, and thereby to enable such reconstructions across complete in-treatment\nscan series. We demonstrated the feasibility and accuracy of the technique using\ndata from four liver cancer patients.\nThe model was trained with synthetic data constructed using the SuPReMo\ntoolkit. Training in this way is essential in the absence of ground-truth deforma-\ntions corresponding to real kV images; that is, there appears to be no other way\nof acquiring paired deformation/image sets for this scenario. For similar reasons,\nin this first study, direct quantitative evaluation of the model performance was\nalso carried out using synthetic data. Naturally, the performance of the model\nwill depend on the fidelity with which real patient motions are reproduced in\nthe synthetic data. While SuPReMo appears to do a reasonable job of charac-\nterising the motion present in the input 4D-CT data, these data represent only\naveraged breathing cycles and do not by themselves provide information about\nthe variability of this motion. Pragmatically, therefore, in the present work, we\nassumed some bounds on the variations from this average, and randomly gen-\nerated motion states within these (Sect. 4.1). We are exploring more rigorous\napproaches to characterising the patient motion variability (and incorporating\nthis in model training data) based on kV image sequences acquired over several\nminutes during treatment.\nWhile we focused here on respiratory motion estimation, our approach can\nin principle be used to predict any motion patterns, e.g. peristaltic motion of\nthe gut or longer-term structural changes, given appropriate training data. As\nmentioned, the model involves no assumptions of periodicity or other specific\nmotion characteristics. Clearly, the generation of training data is more challeng-\ning in some scenarios than in others, but this presents a practical difficulty rather\nthan a theoretical one.\nAs mentioned, our approach has been developed specifically to accommodate\ninput kV images acquired at arbitrary projection angles. We demonstrated this\nby training and evaluating the models with images generated at different projec-\ntion angles. As shown in Figure 3, the prediction accuracy was indeed virtually\nindependent of the projection angle.\nWe observed that the model prediction accuracy can be influenced by the\nquality and characteristics of the CT volumes from which training data are con-\nstructed. The higher peak errors found for patients 2 and 3 appear to be at\nleast partly attributable to this. The CT volumes of these two patient cases\nexhibited certain reconstruction artefacts related to motion. For patient 3, in\nlarge amplitude deformed cases, these resulted in some mesh-image misalign-\nment problems even in the ground-truth data (specifically, with the deformed"}, {"title": null, "content": "states at initial and final time points) produced from SuPReMo. The deformed\nimages for Patients 1 and 4, by contrast, contained no obvious artefacts or am-\nbiguous anatomy, and peak errors were correspondingly smaller (Table 1). As\nemphasised, however, the regions of higher peak errors, even in patients 2 and\n3, were nonetheless very localised, and mean errors remained low.\nIn the absence of ground-truth motion states for real in-treatment kV im-\nages, the evaluation was conducted through qualitative and semi-quantitative\nmethods as described in Section 5.3. Qualitative assessment involved visualiz-\ning overlaid organ contours on the input kV images, while semi-quantitative\nevaluation employed deforming reference CT volumes using predicted motions\nand calculating resulting image similarities. In the former experiment, the model\nappeared (again, qualitatively) to make sensible predictions. The animations in-\ncluded as supplementary material showed consistent and smoothly transitioning\norgan contours from one image to the next without noticeable jumps between\nframes, despite the prediction for each frame being independent of its neigh-\nbours (the model contains no recurrence mechanism, for example, and makes\npredictions based on single input images). This suggests the model is not greatly\naffected by noise or other irregularities present in the real images. In the latter\nexperiment, compensating for the patient motion using the predicted deforma-\ntion fields improved the similarity between the input images and corresponding\nmodel-derived DRRs, again indicating the model makes sensible predictions for\nthe real images.\nA limitation of our method, as currently implemented, is that we only es-\ntimate the deformation of the target organ; nearby OARs and other anatomy\nare ignored. However, the model itself imposes no restrictions in this respect. If\nsuitable training data covering the relevant anatomy, and meshes of the relevant\nanatomical structures can be generated, the model can in principle estimate mo-\ntions for these in the same way. SuPReMo, for example, provides deformation\nvector fields for the whole image volume, which could be used for this purpose.\nOAR and other meshes can normally be created similarly to the liver meshes\nused here. Potentially, the prediction of deformations for multiple disconnected\nmeshes could result in anomalous overlapping regions. However, such instances\nwould hopefully be rare when the training data include no such behaviour. If\nneeded, separate non-overlap constraints in the formulation could also be con-\nceived, which penalise mesh interpenetration in a way analogous to some contact\nformulations in computational mechanics. Albeit, this would increase the com-\nplexity of the approach. A simpler alternative could be deploying a single mesh\ncovering all relevant anatomy. In all scenarios, it is possible that both model\nand training data size requirements would increase, though the overall approach\nwould remain the same.\nA key part of the synthetic data creation was the development of a method\nfor generating realistic synthetic kV X-ray images. While DRRs produced from\nray tracing through CT volumes ultimately are also X-ray-based images, they\ndo not suffer from the same scatter and noise phenomena of in-treatment kV\nX-ray images. Their appearance, consequently, is noticeably different. Therefore"}, {"title": null, "content": "we first trained a CycleGAN [31", "33": "."}]}