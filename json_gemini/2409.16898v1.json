{"title": "Al-driven View Guidance System in Intra-cardiac Echocardiography Imaging", "authors": ["Jaeyoung Huh", "Paul Klein", "Gareth Funka-Lea", "Puneet Sharma", "Ankur Kapoor", "Young-Ho Kim"], "abstract": "Intra-cardiac Echocardiography (ICE) is a crucial imaging modality used in electrophysiology (EP) and structural heart disease (SHD) interventions, providing real-time, high-resolution views from within the heart. Despite its advantages, effective manipulation of the ICE catheter requires significant expertise, which can lead to inconsistent outcomes, particularly among less experienced operators. To address this challenge, we propose an Al-driven closed-loop view guidance system with human-in-the-loop feedback, designed to assist users in navigating ICE imaging without requiring specialized knowledge. Our method models the relative position and orientation vectors between arbitrary views and clinically defined ICE views in a spatial coordinate system, guiding users on how to manipulate the ICE catheter to transition from the current view to the desired view over time. Operating in a closed-loop configuration, the system continuously predicts and updates the necessary catheter manipulations, ensuring seamless integration into existing clinical workflows. The effectiveness of the proposed system is demonstrated through a simulation-based evaluation, achieving an 89% success rate with the 6532 test dataset, highlighting its potential to improve the accuracy and efficiency of ICE imaging procedures.", "sections": [{"title": "I. INTRODUCTION", "content": "THE Intra-cardiac Echocardiography (ICE) is a sophisticated imaging modality that offers real-time, high-resolution views from within the heart, making it an invaluable tool in both electrophysiology (EP) and structural heart disease (SHD) interventions. The ICE is commonly used in procedures such as atrial fibrillation ablation, transcatheter valve repairs, and closures of septal defects. The key advantages of ICE include its ability to provide clear and detailed near-field images of cardiac structures, perform under conscious sedation, and be easily manipulated and interfaced with other interventional equipment. Additionally, the ICE has been shown to reduce procedural and fluoroscopy times, decrease overall radiation exposure to both the patient and physician, and shorten hospital stays. As a result, the ICE has become widely used in interventional procedures for the management of complex cardiac conditions [1-4]."}, {"title": "II. RELATED WORKS AND BACKGROUND", "content": ""}, {"title": "A. US view navigation", "content": "To improve the clinical workflow, various research efforts have focused on developing guidance systems for tasks such as view finding and clinical decision-making. In particular, Artificial Intelligence (AI)-based methods have been widely utilized to guide and support these workflow. For instance, Narang et al. [10] proposed novel software that provides real-time prescriptive guidance to novice operators for obtaining the Trans-Thoracic Echocardiographic (TTE) images. Similarly, Sabo et al. [11] proposed a real-time guidance system to improve the echo-cardiographic acquisition process, and Pasdeloup et al. [12] developed a method to help users navigate toward preferred standard views during scanning. Additionally, Amadou et al. [13] demonstrated a synthetic image generation pipeline for navigation in an echocardiography view classification experiment. Building on this, Amadou et al. [14] proposed a framework using reinforcement learning to guide novice sonographers in navigating to standard interventional views during Transesophageal echocardiography (TEE).\nWhile many pioneering works have significantly improved clinical workflows, the aforementioned studies primarily deal with acquiring echocardiograms externally or rely on simulation data. In contrast, our research is the first to specifically address a view guidance system for ICE catheter manipulation based on real clinical data. This approach requires a real-time understanding of internal cardiac structures, providing continuous guidance through the interactive mapping of images to spatial and catheter joint space. Furthermore, unlike simulation-based methods, which may not fully capture the complexities of real clinical environments, our method is designed for seamless integration into actual clinical workflows, effectively guiding the catheter within the heart's intricate anatomy.\nAlthough some robotic systems [15\u201317] exist to assist with catheter control, they are often expensive and complex to integrate into the clinical workflow, typically focusing on reproducing previously saved views by manual view survey [15, 16] or relying on positional sensors within a spatial coordinate system [17]. In contrast, our approach relies solely on ICE-imaging-based state estimation to guide the user in manipulating the ICE catheter within the control space from the current view to the target view, eliminating the need for positional sensors and avoiding disruption to the existing clinical workflow."}, {"title": "B. From Transformer to the Mamba and its medical applications", "content": "To address the relationship between non-linear behaviors in images and associated multi modal states, many recent studies have turned to AI, particularly leveraging a Transformer architecture [18]. The Transformer has been widely adopted in vision tasks due to its powerful ability to understand context across entire images through its self-attention mechanism, as well as its scalability to large models and robustness. As a result, many tasks that traditionally relied on CNN structures are now being addressed with Transformer-based architectures [19-23].\nRecently, a new alternative to the transformer architecture, called Mamba [24], has emerged. Unlike the transformer architecture, which relies on self-attention mechanisms with quadratic computational complexity for long sequences, the State Space Model (SSM) maintains linear computational complexity. Mamba is based on the SSM, which can be represented as follows:\n$$h[k] = \\overline{A}h_{1k - 1} + Bx[k]$$\n$$y[k] = Ch[k],$$\nwhere the h[k] represents the state at the k-th step, and the matrix C is the projection matrix that projects the current state to the output. The matrices A and B are the discretized versions of the system matrix and projection matrix, respectively, and they can be represented as follows:\n$$\\overline{A} = exp(A\\Delta)$$\n$$B = (\\Delta A)^{-1}(exp(\\Delta A) \u2013 I) \\cdot \\Delta B,$$\nwhere the \\Delta denotes the timescale parameter. Mamba implements the Selective Scan Mechanism with SSM (S6), filtering out irrelevant information to effectively handle long-range sequences and induction heads. To achieve this, it allows the matrices B, C, and A to depend on the input sequence. Additionally, its hardware-aware implementation makes Mamba a computationally efficient model, resulting in significant performance gains and resource efficiency. Gu and Dao [24] first proposed the Mamba structure to address the computational burden of long sequences in transformer architecture. They deployed selective State Space Models (SSMs) and a hardware-aware parallel algorithm without using attention or MLP blocks.\nDue to its outstanding performance, many researchers have started replacing the Transformer architecture with the Mamba structure. Recently, there have been numerous attempts to adapt the Mamba structure to the medical domain for tasks such as segmentation, classification, and detection, where it has shown superior performance compared to conventional methods. For instance, the authors in [25\u201329] proposed"}, {"title": "III. MATERIALS AND METHODS", "content": ""}, {"title": "A. Problem Setup and Notation", "content": "The ICE catheter is manually introduced into the femoral vein at the groin through an introducer sheath. A basic ICE study typically begins with the catheter positioned in the mid-right atrium (RA), achieving the home view with the ICE catheter in a neutral position, providing imaging of the RA, tricuspid valve, and right ventricle [33]. Clinicians often rely on this home view as a view that can be easily recovered when encountering difficulties in ICE imaging. We assume this home view as the reference coordinate system for our proposed method."}, {"title": "Since most ICE catheter work is conducted in the RA,", "content": "our guidance system assumes that there are no obstacles in the RA, and therefore, obstacle avoidance is not addressed in this article. Additionally, we assume that ICE imaging is captured during the diastolic phase, when the heart is relaxed and the ventricles are filled with blood, particularly during atrial systole when the QRS complex is prominent [34].\nFinally, while we use the tip's position and orientation information for modeling, it is important to note that our actual guidance system does not require attaching position/orientation sensors to the ICE catheter. This allows our system to function as a purely ICE-image-based guidance system, eliminating the need for additional sensor attachments on the catheter.\nWe define the ICE imaging state S = (x, y, z, dx, dy, dz) as position and orientation in SE(3). The ICE catheter has four degrees of freedom: two DOFs for steering the catheter tip in two planes (anterior-posterior knob angle \u03b81 and the right-left knob angle \u03b82) using two knobs on the catheter handle, bulk rotation \u03b83, and translation d4 along the major axis of the catheter. Thus, the ICE joint state, J = (\u03b81, \u03b82, \u03b83, d4). The ICE imaging, I, is defined as the acquired images, which can be either 2D or 3D.\nSince the home view is a neutral position and one of the pre-defined clinical views, we have a transition function from S to J using inverse kinematics (TK) [35] based on the home view (i.e., Shome) as a reference coordinate system:\n$$J_{home} = IK(S_{home}).$$\nwhere i represents the index of the possible ICE views."}, {"title": "Then, we can define \u0394J(i,j) as follows:", "content": "$$\\Delta J_{(i,j)} = IK(S_{home}) \u2013 IK(S_{home}),$$\nwhere i and j represent the index of the possible ICE views.\nProblem Definition: Given the current imaging Icurr and a goal index g from a library of views, the objective is to estimate the relative ICE joint state \u0394J(curr,g) over time:\n$$S^g_{home} = M(I_{home}, g),$$\n$$S^{home}_{curr} = M(I_{curr}, home),$$\n$$\\Delta J_{(curr,g)} = IK(S^{home}_{curr}) - IK((S^g_{home})^{-1}),$$\nwhere M is our proposed model, which will be detailed in the following sections (Sections III-C to IV)."}, {"title": "B. Procedure Overview", "content": "Figure 2 (a) presents an overview of the proposed ICE view guidance system. The procedure is broken down into several key steps, each of which is detailed below:\n1) Home view positioning: The procedure begins by positioning the ICE catheter in the mid-right atrium (RA) to achieve the home view. Then, the ICE catheter is placed in a neutral position, which serves as our reference coordinate system.\n2) The desired view selection: The clinician selects the goal view (as index g) from a predefined library of clinical views. The selected view index is then fed into our model M to estimate the current imaging state in spatial space Shome (Equation (5)).\n3) Compute relative vectors \u0394J(curr,g): This process requires the continuous prediction of the relative vector \u0394J(curr,g) to provide feedback to the user. To achieve this, it is essential to know the current catheter imaging state Shome (Equation (6)) to calculate the relative vectors needed for movement. A novel aspect of our approach is that our same model also serves as a state estimator for this purpose. Then, using IK, the relative joint vectors \u0394J(curr,g) from the current state to the goal state are computed at Equation (7). Figure 2 (b) illustrates the iterative process.\n4) Iteration: Repeat the above steps 3-4 until the goal view is successfully reached. Once the goal view is reached, the clinician can repeat the process starting from step 2 to transition from the current view to any other desired view."}, {"title": "C. Dataset", "content": "To achieve our goal, we need to model a transition function M. For this, we utilize a dataset of ICE images that are labeled according to the anatomical structures they visualize. These images are paired with corresponding tip position and orientation, which are obtained from the electromagnetic (EM) sensors, providing precise tracking of the catheter's spatial configuration. Ultimately, the dataset consists of pairs of images with their corresponding anatomical labels and position/orientation information. Additionally, the corresponding volume mesh is required to obtain the target information. For this purpose, we reconstruct volumetric information of cardiac structures using volume contouring algorithms [36]. It generates intra-cardiac images targeting the LA, LAA, Left Inferior Pulmonary Vein (LIPV), Left Superior Pulmonary Vein (LSPV), Right Inferior Pulmonary Vein (RIPV), Right Superior Pulmonary Vein (RSPV), and ESO, as shown in Figure 3 (a). Each dataset contains a target volume mesh with world coordinate system-based positioning, several images visualizing each cardiac part, and the corresponding position and orientation information. We used a dataset collected from the CARTO system (Biosense Webster Inc., USA) [37], comprising data from 858 subjects, with 793 used for training, 25 for validation, and 40 for testing."}, {"title": "D. Missing mesh", "content": "Our dataset mainly consisted of the LA, LAA, LIPV, LSPV, RIPV, and RSPV volumes. Thus, it includes only a limited dataset for the RA, RV, and LV volumes, comprising five subjects. However, the RV and LV are crucial for intra-cardiac examinations, with the RV view often serving as the home view the starting point for most procedures.\nTo overcome this limitation, we estimated the centers of the RA, RV, and LV volumes based on other structures, such as the center points of the LA and LAA, using a subject dataset that includes RA, RV, and LV volumes. This allowed us to determine the relationships between these centers. By applying this relationship to our whole dataset, we estimated the missing centers of the RV, LV, and RA, as illustrated in Figure 3 (b)."}, {"title": "E. Data pre-processing", "content": "In the dataset, each subject has its own coordinate system, known as the world coordinate system, which can vary due to the EM sensors used during data acquisition, leading to inconsistencies across subjects. To address this, we normalized all datasets based on the transducers used in each image, eliminating dependence on the world coordinate system. This normalization is also more practical since transducer manipulation should be performed relative to the transducer itself."}, {"title": "The transducer state in the world coordinate system can", "content": "be represented as Str and its corresponding transformation matrix is Tur. We can obtain the target position and orientation relative to the transducer following below equation:\n$$T^{tar}_{tr} = (T^{tr})^{-1} \\cdot T^{tar},$$\nwhere the Ttar (Star) is target position and orientation, which visualizing each center of the mesh. According to clinical guidance in ICE imaging procedures, the structures are visualized by rotating the ICE, with minimal variation in position. Therefore, we selected a single point inside the RA volume for each target view as target position. To determine the orientation, we aligned the fan direction (target position to center point of fan long axis) to point toward the center of the mesh. The final positions and orientations, Star, was utilized our target dataset."}, {"title": "IV. IMPLEMENTATION DETAILS", "content": ""}, {"title": "A. Network details", "content": "We utilized MedMamba as the base model for our method [30]. MedMamba combines convolutional neural networks (CNN) with Mamba, which uses State Space Models (SSM). This combination leverages both local and global information, showing superior performance in medical classification tasks compared to other conventional methods. Consequently, we applied MedMamba to our view navigation research, addressing the regression problem.\nThe entire architecture is shown in Figure 4. The ICE image, representing the current view, is fed into the network and processed through ResNet50 to extract features [38]. Simultaneously, the target view class, represented as a one-hot vector of size 6 (corresponding to the number of target views), is passed through a single linear layer that adjusts its length to match the squared feature size of the ResNet50 output. This is then resized and concatenated with the image features before being passed through another feature mixing layer composed of a single convolution layer.\nMedMamba consists of four sequences of SS-Conv-SSM blocks and patch merging steps, except for the last block, which contains only an SS-Conv-SSM block. The SS-Conv-SSM block splits its feature map into two branches: one for local feature extraction using convolution and the other for global feature extraction using the core element, 2D-Selective-Scan (SS2D). It employs the Cross-Scan Module (CSM), which uses a four-way scanning strategy to achieve a global receptive field without increasing the computational burden. The selective scan state-space model (S6) captures long sequence dependencies from all directional features and merges them to construct the 2D feature map. The patch merging step reduces the spatial dimension to achieve hierarchical representations. At the output of MedMamba, we added two single linear layer heads: one for position regression and the other for orientation regression."}, {"title": "The entire network was trained using quantile loss to", "content": "estimate the boundaries of estimation. This approach enables the network to estimate not only the median value but also the qth percentile, providing the boundary of estimation with qth reliability. The simplified loss function is shown below:\n$$l_{quant} (Y, \\hat{Y}) = \\begin{cases} a \\cdot (y - \\hat{y}), & \\hat{y} \\leq y \\\\ (1 - a) \\cdot (\\hat{y} - y), & \\hat{y} > y, \\end{cases}$$\nwhere the a denote the percentage of qth quantile, y and \u0177 represent elements of label vector y and prediction vector \u0177, respectively. The total loss function used to train the network is shown below:\n$$l_{total} = l_{quant}(p, \\hat{p}) + \\lambda * l_{quant}(o, \\hat{o}),$$\nwhere the p, o represent the labels for position and orientation, respectively and \u00f4, \u00f4 denote the predicted position and orientation, respectively. The \u03bb is weighting parameter."}, {"title": "B. Evaluation metric", "content": "To evaluate the proposed method, we aimed to determine how accurately the relative position and orientation can guide"}, {"title": "C. Training details", "content": "The network was trained using the Adam optimizer with a learning rate of 1e-4 during 580 epochs. We employed a batch size of 40, and the entire image dataset was resized to 224 x 224. In Figure 4, the N is set to [2,2,4,2] for each block, which is default setting of MedMamba-Tiny. The \u03bb in Equation 10 is 10 and the a is 0.98 which denotes the 98th quantile. The entire framework was implemented in PyTorch and trained on a single A100 GPU."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "Since the proposed method is designed for real clinical settings and trained on real clinical data, our ability to perform actual validation is limited. Therefore, we present simulation-based results derived from real clinical data to demonstrate the method's effectiveness. To validate the method, we conducted"}, {"title": "A. Qualitative result", "content": "The qualitative results are shown Figure 6. From a current view, we determined the relative position and orientation focusing on a specific view among six different target views. The light gray color indicates the current view position and orientation, while black and dark gray represent the 50 and 98 or 2 percentiles of the predicted fan, respectively.\nAs shown in the Figure 6 (a), the results include 12 randomly selected cases where the input image corresponds to specific views such as RV, LV, LPV, RPV, LAA, or ESO. In all cases, the predicted fan is clearly contained within the target volume. Particularly, for cases requiring significant movement, such as from LV to RPV, the predictions accurately reflect the desired state. Conversely, for cases requiring minimal movement, such as from LPV to LPV, the predictions also effectively facilitate view navigation. In the Figure 6 (b), it shows 4 randomly selected cases where the input image"}, {"title": "In these cases, the proposed method provides accurate target", "content": "state information for both small and large movements."}, {"title": "B. Quantitative result", "content": "The proposed method provides prediction boundaries, and we assessed its accuracy by determining how well the predicted quantile planes are contained within the target volume. The method achieved an accuracy of 89%. This demonstrates that the proposed approach significantly improves view guidance performance.\nThe Figure 7 (a) shows the normalized distance between the center of the target volume and the predicted fan. The histogram illustrates the distribution of these distances, including out-of-boundary (OOB) samples. The normalized distance is calculated by dividing the distance from the predicted fan to the center of the target volume by the distance from the center plane to the edge plane. Thus, a distance of 1 represents the edge of the volume, distances below 1 indicate correct predictions, and distances above 1 indicate incorrect predictions. In Figure 7 (b), it shows the histogram for correct sample distances only, with smaller distances corresponding to better predictions. In Figure 7 (c), it gives the correct sample distance distribution for each target view, visualizing histograms for various input images corresponding to target views such as RV, LV, LPV, RPV, LAA, or ESO. For the RV and LV target views, the histograms are slightly noisier due to the smaller number of training samples compared to other target views."}, {"title": "C. Nearby view result", "content": "Since our proposed model is trained based on real clinical data, we evaluated it through a simulation-based process. To assess the robustness of the method, we tested it with views that are similar or nearby to the target viewpoint."}, {"title": "In most", "content": "of cases, the results show a good result in position and orientation difference for each target view. We demonstrate that the proposed method can account for difference in image to the difference in coordinate."}, {"title": "D. State estimation result", "content": "As shown in Figure 2 (b), the proposed method can function as a current state estimator. For it to be an effective state estimator, it should provide a stable target state throughout the entire movement from the starting point to the target point. To"}, {"title": "VI. CONCLUSION AND DISCUSSION", "content": "The intra-cardiac echocardiography (ICE) is an imaging tool used to capture images of the cardiac structures within the cardiac chamber. It is valuable for anatomical reviews, diagnosing congenital heart defects, or assessing valvular heart disease. Additionally, ICE is essential for interventional cardiology, guiding catheter ablation or the closure of septal defects. Despite its importance, effective manipulation of the ICE requires a skilled operator and substantial training due to the difficulty in achieving precise and repeatable controls. To address this issue, We proposed an AI-driven closed-loop view guidance system designed to simplify ICE manipulation based on the current ICE image. Specifically, there exist clinically predefined common views for most procedures, such as RV, LV, LPV, RPV, LAA, and ESO views. Our method provides the relative position and orientation vectors needed to guide the user to their preferred viewpoint from the current view, facilitating easier control of the ICE.\nTo achieve this, we developed a Mamba-based regression model. The Mamba model, based on the State Space Model (SSM), outperforms traditional transformer architectures. Its selective scan and hardware-aware features enable efficient handling of long-range sequences. Our approach utilizes the MedMamba structure, which combines CNN and Mamba to efficiently capture both global and local features in medical images. For multi-target class regression, we incorporated a feature mixing layer to combine image features with target class codes, and a multi-head structure at the network's end to separately estimate position and orientation. The network was trained using quantile loss to accurately estimate the boundaries of the predictions. We trained our regression model on a CARTO dataset containing view, position, orientation, and cardiac volume information.\nThe qualitative and quantitative results demonstrate that our method provides accurate target state information for view visualization. Whether the view is focused on specific structures or not, our method effectively supports view navigation. Additionally, when movement simulation test from the start point to the target state, the intermediate images stably show the target state well, proving that our method functions well as a state estimator.\nFor future work, we plan to validate our system performance in a beating heart phantom. We believe that our method will simplify ICE manipulation for clinicians and enhance the overall workflow of the procedure."}]}