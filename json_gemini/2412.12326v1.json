{"title": "Achieving Collective Welfare in Multi-Agent Reinforcement Learning via Suggestion Sharing", "authors": ["Yue Jin", "Shuangqing Wei", "Giovanni Montana"], "abstract": "In human society, the conflict between self-interest and collective well-being often obstructs efforts to achieve shared welfare. Related concepts like the Tragedy of the Commons and Social Dilemmas frequently manifest in our daily lives. As artificial agents increasingly serve as autonomous proxies for humans, we propose using multi-agent reinforcement learning (MARL) to address this issue-learning policies to maximise collective returns even when individual agents' interests conflict with the collective one. Traditional MARL solutions involve sharing rewards, values, and policies or designing intrinsic rewards to encourage agents to learn collectively optimal policies. We introduce a novel MARL approach based on Suggestion Sharing (SS), where agents exchange only action suggestions. This method enables effective cooperation without the need to design intrinsic rewards, achieving strong performance while revealing less private information compared to sharing rewards, values, or policies. Our theoretical analysis establishes a bound on the discrepancy between collective and individual objectives, demonstrating how sharing suggestions can align agents' behaviours with the collective objective. Experimental results demonstrate that SS performs competitively with baselines that rely on value or policy sharing or intrinsic rewards.", "sections": [{"title": "Introduction", "content": "Multi-agent reinforcement learning enables collaborative decision-making in diverse real-world applications, such as autonomous vehicle control [Xia et al., 2022, Qiu et al., 2023, Jin et al., 2021], robotics [Wang et al., 2022, Peng et al., 2021, Sun et al., 2020], and communications systems [Siedler and Alpha, Huang and Zhou, 2022]. In these scenarios, artificial agents often act as autonomous decision makers. MARL provides a powerful framework for these settings, enabling agents to learn coordination strategies based on rewards reflecting a common goal.\nHowever, in many cases, a fundamental challenge arises when agents, reflecting the preferences of individuals, are incentivised by interests that conflict with the collective good. This tension is exemplified by the Tragedy of the Commons [Ostrom, 1990] and Social Dilemmas [Kollock, 1998, Van Lange et al., 2013], where pursuit of individual interests can lead to collectively harmful outcomes. For instance, when individuals can benefit from a shared resource without contributing to its maintenance, they often face incentives to 'free-ride' on others' efforts rather than contribute fairly. Without mechanisms to align individual actions with collective welfare, such systems can collapse into inefficient equilibria where shared resources are depleted or congested, harming all participants. Decades of research in economics and sociology have shown that resolving these dilemmas requires careful mechanism design to foster coordination while respecting individual interests [Hauser et al., 2019, Macy and Flache, Gersani et al., 2001, Milinski et al., 2002].\nTo illustrate these challenges, consider a smart grid system where consumers balance electricity costs against personal comfort. Each consumer optimises their own trade-off, but electricity costs depend on the collective demand patterns across all users. High simultaneous usage drives up prices for everyone, suggesting that consumers should coordinate to avoid peak times. However, individuals may be reluctant to compromise their comfort, instead hoping others will reduce their consumption. This misalignment between individual comfort optimisation and collective cost minimisation can result in inefficient peak loads and higher costs for all participants. A similar dynamic occurs in traffic networks, where drivers independently choose routes to minimise their personal travel times. Without coordination, too many drivers selecting the same optimal routes create congestion, leading to increased delays for everyone.\nA straightforward way to formalise the problem as a MARL problem for collective welfare is to train agents' policies that maximise long-term collective return. Existing solutions often involve introducing designed intrinsic rewards and exchanging individual rewards, values or model parameters. Previous works have proposed various intrinsic rewards based on factors such as social influence, morality, and inequity aversion [Tennant et al., 2023, Hughes et al., 2018, Jaques et al., 2019]. While intrinsic rewards can encourage agents to cooperate, designing appropriate rewards can be intractable in some scenarios.\nAlternatively, sharing rewards has been explored as a means to guide agents towards a collective optimum [Chu et al., 2020b, Yi et al., 2022, Chu et al., 2020a]. Other approaches involve sharing model parameters or the output values of value functions [Zhang et al., 2018a,b, 2020, Suttle et al., 2020, Du et al., 2022]. By aggregating individual values or model parameters from neighbouring agents, these methods enable agents to estimate a global value and adjust their policies to maximise it. Similarly, strategies that share policy model parameters rather than value estimates have been proposed [Zhang and Zavlanos, 2019, Stankovic et al., 2022a,b], where agents learn a shared joint policy through parameter-sharing and consensus techniques. While these methods have shown promise in maximising collective returns in some cases, they rely on the assumption that agents can freely exchange potentially sensitive information. Moreover, they may suffer from exploration issues: when cooperation experiences are rare, agents often lack sufficient motivation to cooperate.\nIn practice, agents typically do not have access to others' rewards, value functions, or policy functions. For instance, in a smart grid system, consumers' electricity usage policies reflect sensitive information about their daily routines and financial constraints, and their interests (rewards/ values) related to comfort are also private. This information may not be something they are willing to share with other participants or a central coordinator. Similarly, an a traffic network, drivers' routing preferences and time valuations, which reveal sensitive details about their destinations, schedule constraints, and willingness to pay for faster travel, are rarely shared with others. Traditional MARL approaches that rely on agents sharing rewards, policies, or value estimates thus become problematic in such settings.\nBased on these observations, we propose Suggestion Sharing (SS), a novel approach for coop- erative policy learning that facilitates effective coordination for collective welfare. SS is grounded in the premise that each agent benefits more when others cooperate, regardless of its own decision to cooperate. For example, in the smart grid scenario, whether or not an agent reduces its elec- tricity usage (cooperates), it always receives a higher reward if other agents cooperate by using less electricity. Thus, agents can share suggestions to encourage cooperation, even in the absence of prior cooperation examples. In SS, agents learn suggestions, share them with one another, and incorporate them into each agent's policy optimisation objective, which is derived from a lower bound of the original collective objective.\nConsequently, in SS, instead of sharing policies or rewards, agents exchange only action sugges- tions proposals for how others could act to help achieve collective benefits. This iterative process aligns individual behaviours with collective objectives while revealing significantly less private in- formation compared to existing approaches. Empirical results across multiple domains, including sequential social dilemmas and the tragedy of the commons, demonstrate that SS achieves coop- eration performance competitive with traditional MARL methods that rely on sharing policies or value functions.\nThe main contributions of this paper are as follows. We propose using MARL to address sociological problems such as Social Dilemmas and the Tragedy of the Commons, in scenarios where agents can make decisions or provide advice to humans while communicating within systems like smart grids and cooperative driving systems. We introduce the SS algorithm, which promotes cooperation for collective welfare while reducing the sharing of private information. Theoretically, we show that the optimisation objective of SS serves as a lower bound for the original collective objective. Empirical results demonstrate that SS performs competitively with existing MARL algorithms that rely on sharing policies or values.\nThe remainder of this paper is structured as follows. Section 2 reviews related work on coop- erative MARL under individual reward settings. Section 3 provides the technical background and problem formulation. Section 4 details our methodology, including theoretical foundations and the proposed algorithm. Section 5 outlines the experimental setup and results. Finally, Section 6 discusses the implications of our findings and suggest directions for future research."}, {"title": "Related Work", "content": "In this work, we focus on cooperative MARL under individual reward, which is distinguished from numerous contemporary studies that focus on optimising multi-agent policies under the assumption of an evenly split shared team reward [Kuba et al., 2022, Wu et al., 2021, Sun et al., 2022, Jiang and Lu, 2022]. Cooperation under individual rewards reflects a more realistic scenario in many real-world applications, where agents need to learn to cooperate based on limited and individual information due to privacy or scalability concerns.\nWith an individual reward setup, many works [Lowe et al., 2017, Iqbal and Sha, 2019, Foerster et al., 2017, Omidshafiei et al., 2017, Kim et al., 2021, Jaques et al., 2019] focus on solving Nash equilibrium of a Markov game, i.e., agent seeks the policy that maximises its own expected return. However, that may not result in collective optimum when agents have conflicting individual interests, such as in social dilemmas, which can hinder collective cooperation. Our research focuses on maximising the total return across all agents where each agent needs to cooperate to achieve collective optimum. In the rest of this section, we introduce related works aiming to solve this problem.\nMARL for Social dilemmas Social dilemmas highlight the tension between individual pur- suits and collective outcomes. In these scenarios, agents aiming for personal gains can lead to compromised group results. For instance, one study has explored self-driven learners in sequen- tial social dilemmas using independent deep Q-learning [Leibo et al., 2017]. A prevalent research direction introduces intrinsic rewards to encourage collective-focused policies. For example, moral learners have been introduced with varying intrinsic rewards [Tennant et al., 2023] while other approaches have adopted an inequity-aversion-based intrinsic reward [Hughes et al., 2018] or re- wards accounting for social influences and predicting other agents' actions [Jaques et al., 2019]. Borrowing from economics, our method integrated formal contracting to motivate global collabo- ration [Christoffersen et al., 2023]. While these methods modify foundational rewards, we maintain original rewards, emphasizing a collaborative, information-sharing strategy to nurture cooperative agents.\nValue sharing Value sharing methods use shared Q-values or state-values among agents to better align individual and collective goals. Many of these methods utilize consensus techniques to estimate the value of a joint policy and guide individual policy updates accordingly. For instance, a number of networked actor-critic algorithms exist based on value function consensus, wherein agents merge individual value functions towards a global consensus by sharing parameters [Zhang et al., 2018a,b, 2020, Suttle et al., 2020]. Instead of sharing value function parameters, [Du et al., 2022] shares function values for global value estimation. However, these methods have an inherent limitation: agents modify policies individually using fixed Q-values or state-values, making them less adaptive to immediate policy shifts from peers and potentially introducing policy discoordination. In contrast, our approach enables more adaptive coordination by having agents directly share and respond to peer suggestions.\nReward sharing Reward sharing is about receiving feedback from a broader system-wise outcome perspective, ensuring that agents act in the collective best interest of the group. Some works have introduced a spatially discounted reward function [Chu et al., 2020b,a]. In these ap- proaches, each agent collaboratively shares rewards within its vicinity. Subsequently, an adjusted reward is derived by amalgamating the rewards of proximate agents, with distance-based dis- counted weights. Other methods advocate for the dynamic learning of weights integral to reward sharing, which concurrently evolve as agents refine their policies [Yi et al., 2022]. In our research, we focus on scenarios where agents know only their individual rewards and are unaware of their peers' rewards. This mirrors real-world situations where rewards are kept confidential or sharing rewards suffers challenges such as communication delays and errors. Consequently, traditional value or reward sharing methods fall short in these contexts. In contrast, our method induces coordination without requiring reward sharing.\nPolicy sharing Policy sharing strives to unify agents' behaviors through an approximate joint policy. However, crafting a global policy for each agent based on its individual reward can lead to suboptimal outcomes. Consensus update methods offer a solution by merging individually learned joint policies towards an optimal joint policy. Several studies have employed such a strategy, fo- cusing on a weighted sum of neighboring agents' policy model parameters [Zhang and Zavlanos, 2019, Stankovic et al., 2022a,b]. These methods are particularly useful when sharing individual rewards or value estimates is impractical. Yet, sharing policy model parameters risks added com- munication overheads and data privacy breaches. PS is based on the idea of federated learning and shares the parameters of joint policies among agents. In contrast, our method focuses on learning individual policies and sharing only the relevant action distributions of the suggesting policies with the corresponding agents, which typically involves less communication overhead compared to sharing entire policy parameters with all the neighbouring agents.\nTeammate modeling Teammate/opponent modeling in MARL often relies on agents having access to, or inferring, information about teammates' goals, actions, or rewards. This information is then used to improve collective outcomes [Albrecht and Stone, 2018, He et al., 2016, Wen et al., 2019, Zheng et al., 2018]. Our approach differs from traditional team modeling. Rather than fo- cusing on predicting teammates' exact actions or strategies, our method has each agent calculate and share action suggestions that would benefit its own strategy. These suggestions are used by other agents (not the agent itself) to balance their objectives with those of the agent sending the suggestion. This approach emphasizes suggestions that serve the agent's own objective optimisa- tion. Coordination occurs through policy adaptation based on others' suggestions that implicitly include information about their returns, rather than modeling their behaviors. It contrasts with conventional team modeling in MARL that focuses on modeling teammates' behaviors directly."}, {"title": "Preliminaries and Problem Statement", "content": "To optimise the collective welfare, we formulate the problem as a Multi-agent Markov Decision Process (MMDP). Specifically, we consider an MMDP with $N$ agents represented as a tuple $< S, \\{A^i\\}_{i=1}^N, P, \\{R^i\\}_{i=1}^N, \\gamma>$, where $S$ denotes a global state space, $A^i$ is the individual action space, $A = \\Pi_{i=1}^N A^i$ is the joint action space, $P : S \\times A \\times S \\rightarrow [0, 1]$ is the state transition function, $R^i : S \\times A \\rightarrow \\mathbb{R}$ is the individual reward function, and $\\gamma$ is a discount factor. Each agent $i$ selects an action $a^i \\in A^i$ based on its individual policy $\\pi^i : S \\times A^i \\rightarrow [0, 1]$. The joint action of all agents is represented by $a \\in A$, and the joint policy across these agents is denoted as $\\pi(\\cdot|s) = \\Pi_{i=1}^N \\pi^i(\\cdot|s)$. The objective is to maximise the expectation of collective cumulative return of all agents,\n$\\eta(\\pi) = \\sum_{i=1}^N \\mathbb{E}_{\\tau \\sim \\pi} \\Big[\\sum_{t=0}^{\\infty} \\gamma^t r_t^i\\Big],$ (1)\nwhere the expectation, $\\mathbb{E}_{\\tau \\sim \\pi}[\\cdot]$, is computed over trajectories with an initial state distribution $s_0 \\sim d(s_0)$, action selection $a_t \\sim \\pi(\\cdot|s_t)$, state transitions $s_{t+1} \\sim P(\\cdot|s_t, a_t)$, and $r_t^i = R^i(s, a)$ is the reward for individual agent $i$. Here, we use $r^i = R^i(s, a)$ for simplicity of notation, but this can be easily extended to a stochastic reward function without affecting the core of our method."}, {"title": "Methodology", "content": "In this section, we start from solving Eq. 1, the collective optimisation objective formulated in Section 3. We derive a lower bound of this objective based on trust region policy optimisation (TRPO) work [Schulman et al., 2015]. The lower bound applies to the setting where agents have individual rewards, distinguishing from previous works where agents share team rewards [Wu et al., 2021, Su and Lu, 2022]. Then we introduce Suggesting Policies to replace the other agents' policies in the individual term corresponding to each agent in the lower bound and derive a bound for the gap caused by such a replacement. By leveraging the gap, agents can learn policies to maximise the collective return in an individual way without explicit reward or policy sharing. We will see that the gap for each agent is related with the discrepancy between the action distribution suggested by others and the agent's own action distribution. Practically, we propose SS algorithm, where agents share their action suggestions with each other. These suggestions are then considered by other agents when maximising their individual objectives, enabling each agent to align with the collective goal.\nUnlike traditional methods that share explicit rewards or objectives, SS involves agents ex- changing suggestions that implicitly contain information about others' objectives. By observing how its actions align with aggregated suggestions, each agent can perceive the divergence between its individual interests and the collective goals. This drives policy updates to reduce the identified discrepancy, bringing local and global objectives into closer alignment."}, {"title": "Theoretical developments", "content": "We commence our technical developments by Analysing joint policy shifts based on global in- formation. This extends foundational TRPO to multi-agent settings with individual advantage values. We prove the following bound on the expected return difference between new and old joint policies:\nLemma 1. We establish a lower bound for expected collective returns:"}, {"title": "A Surrogate Optimisation Objective", "content": "Our preceding results established analytical foundations for assessing joint policy improvement in multi-agent settings with individual advantage values and suggesting joint policies. We now build upon these results to address the practical challenge of optimising collective returns when agents lack knowledge of others' policies, rewards, and values.\nDirectly maximising the expected collective returns, $\\eta(\\pi)$, is intractable without global knowl- edge of the joint policy and collective return. However, Theorem 1 provides insight into a more tractable approach: agents can optimise a localized surrogate objective, $\\zeta_{\\pi^{old}}(\\Pi)$, which is the sum of individual objectives concerning suggesting joint policies and individual advantage values. This simplifies the global objective into an individual form dependent on the suggesting joint policy that is composed of an agent's individual policy, $\\pi^i$, and its suggestions for others, $\\pi^{ij}$.\nTo leverage this insight, we use the lower bound given by Theorem 1. By maximising this lower bound, we can maximise the collective return. We can ignore the terms $\\eta(\\pi^{old})$ and $f^{old}$ from Theorem 1 in our optimisation problem, as they are not relevant to optimising $\\Pi$ and their values are usually bounded. To be specific, the value of $\\eta(\\pi^{old})$ is bounded as the reward value is bounded. For $f^{old}$, as defined in Eq. 8, its value is also bounded since (1) We focus on scenarios with finite and relatively small action spaces, which are common in many real-world applications, so |A| (the size of the action space) is not excessively large. (2) The term $||d^{\\pi^{old}}||^2$ is the square L2- norm of the state visitation distribution, which is bounded. (3) The advantage function $A^{old}(s, a)$ is also bounded as the reward value is bounded.\nConsequently, we propose the following constrained optimisation problem as a surrogate for the original collective objective:\n$\\max \\sum_i \\mathbb{E}_{s \\sim d^{old}(s), a \\sim \\tilde{\\pi}^i(a|s)} [A^{old} (s, a)]$\\ns.t. $\\sum_j \\mathbb{D}_{KL}(\\pi^{old}| |\\pi^{ij}) \\leq \\delta, \\forall i, \\\\ \\max_{s, a} | A^{old} (s, a)| \\sum_i (\\pi^i (a|s) - \\tilde{\\pi}^{ij} (a|s))^2 < \\delta'.$ (11)\nNote that, taking into account of the results given by [Schulman et al., 2015], we do not directly include the lower bound of the discrepancy given by Eq. 10 in Eq. 11, but instead use constraints to facilitate learning.\nEq. 11 captures the essence of coordinating joint policies to maximise individual advantages with suggesting joint policies. However, it still assumes full knowledge of $\\Pi$. To make this feasible in individual policy learning, we reformulate it from each agent's perspective. Remarkably, we can distill the relevant components into an individual objective and constraints for each individual agent $i$, as follows:\n$\\max \\mathbb{E}_{s \\sim d^{old}(s), a \\sim \\tilde{\\pi}^i(a|s)} [A^{old} (s, a)]$\\ns.t.: (a) $\\mathbb{D}_{KL}(\\pi^{ii}| |\\pi^i) \\leq \\delta_1$,  (b) $\\kappa_i \\cdot \\sum_j \\sum_a (\\pi^i (a_j|s) - \\tilde{\\pi}^{ij} (a_j|s))^2 < \\delta_2, \\forall j \\neq i$,\\\\ (c) $\\kappa_i \\cdot \\sum_a (\\tilde{\\pi}^{ij} (a_i|s) - \\pi^{ii} (a_i|s))^2 < \\delta_2, \\forall j \\neq i$, (12)\nwhere $\\kappa_i = \\max_{s, a} | A^{old} (s, a)|$.\nThe constraints in Eq. 12 are imposed on $\\pi^{ii}$ and $\\pi^{ij}$ ($j \\neq i$), which together compose $\\tilde{\\pi}^i$. Therefore, these constraints effectively limit the space of possible $\\tilde{\\pi}^i$ by constraining its components. Constraint (a) limits how much the agent's own policy can change, while constraints (b) and (c) ensure that the suggestions are close to the actual policies of other agents. The constraints also depend on other agents' policies $\\pi^{jj}$ and their suggestions for agent $i$'s policy, $\\tilde{\\pi}^{ji}$. To enable the evaluation of these terms, each agent $j$ shares its action distribution $\\pi^{jj} (\\cdot|s)$ and the action distribution suggestion $\\tilde{\\pi}^{ji}(\\cdot|s)$ with agent $i$. This sharing enables each agent $i$ to assess the constraint terms, which couples individual advantage optimisations under local constraints. These constraints reflect both the differences between the policies of others and an agent's suggestions on them, as well as the discrepancy between an agent's own policy and others' suggestions on it. By distributing the optimisation while exchanging policy suggestions, this approach balances individual policy updates while maintaining global coordination among agents.\nIt's important to distinguish our method from teammate modeling. In teammate modeling, agent $i$ typically approximates peer policies $\\hat{\\pi}^j$ and uses these approximations when solving for its own policy $\\pi^i$. In contrast, our approach in Eq. 12 aims to optimise the suggestions $\\pi^{ii}$ alongside $\\pi^i$. These optimised suggestions $\\pi^{ii}$ are then used by agent $j$ to solve for its policy $\\pi^{jj}$. This method allows the suggestions to implicitly incorporate information about individual objectives. Through the exchange of these suggestions, individual agents can balance others' objectives and, consequently, the collective performance while optimising their own objectives."}, {"title": "A Practical Algorithm for Learning with SS", "content": "We propose a structured approach to optimise the objective in Eq. 12. The derivation of the algorithm involves specific steps, each targeting different aspects of the optimisation challenge. Note that in this practical algorithm, we present a setup where agent $i$ exchanges information with neighbours $\\{j|j \\in \\mathcal{N}\\}$ that may not include all other $(N - 1)$ agents, and is not subject to a particular protocol used for determining $\\mathcal{N}_i$. In experiments, we use different neighbourhood definitions/protocols to investigate corresponding effects.\nAddressing the KL divergence constraint (a) in Eq. 12 is crucial in ensuring each agent's policy learning process remains effective. This constraint ensures that updates to an agent's individual policy do not deviate excessively from its previous policy. To manage this, we incorporate a clipping mechanism, inspired by PPO-style clipping [Schulman et al., 2017], adapted for individual agents in our method.\nWe start by defining probability ratios for the individual policy and suggesting policies for peers:\n$\\xi^i = \\frac{\\pi^{ii} (a_i s; \\theta^{ii})}{\\pi^{i}(a_i s; \\theta^{i})}, \\\\ \\xi^{ij} = \\Pi_{j \\in \\mathcal{N}_i}  \\frac{\\pi^{ij} (a_j s; \\theta^{ij})}{\\pi^{jj}(a_j s; \\theta^{j})}.$ (13)\nThese ratios measure the extent of change in an agent's policy relative to its previous one and its suggestions to others' true policies. We then apply a clipping operation to $\\xi^i$, the individual policy ratio:\n$\\mathbb{E}_{s \\sim d^{old}(s), a \\sim \\pi^{old}(a|s)}  [min (f_i \\xi^i \\tilde{A}_i, clip(f_i, 1 - \\epsilon, 1 + \\epsilon)\\xi^i \\tilde{A}_i)]$.\nThis method selectively restricts major changes to the individual policy $\\pi^{ii}$, while allowing more flexibility in updating suggestions on peer policies. It balances the adherence to the KL constraint with the flexibility needed for effective learning and adaptation in a multi-agent environment.\n$\\mathcal{X}_i^i = \\{(s, a) | \\frac{\\pi^{ii} (a_i s; \\theta^{ii})}{\\pi^{ii}(a_i s)} \\geq \\Lambda_i\\}$,\\\\ $\\mathcal{X}_{ij} = \\{(s, a) | \\frac{\\pi^{ij} (a_j s; \\theta^{ij})}{\\pi^{jj}(a_j s)} \\geq \\Lambda_i\\},$ (14)\nwhere the pairs $(s, a)$ represent scenarios in which the gradient influenced by $\\tilde{A}_i$ increases the divergence between the two policies. The following indicator function captures this effect:\n$I_{\\mathcal{X}} (s, a) = \\begin{cases} 1 & \\text{if } (s, a) \\in \\mathcal{X}, \\\\ 0 & \\text{otherwise}.\\end{cases}$ (15)\nIn the final step, we combine the clipped surrogate objective with coordination penalties to form our dual clipped objective:\n$\\max \\mathbb{E}_{s \\sim d^{old}(s), a \\sim \\pi^{old}(a|s)} [min (f_i \\xi^i \\tilde{A}_i, clip(f_i, 1 - \\epsilon, 1 + \\epsilon)\\xi^i \\tilde{A}_i)\\\\ - \\kappa_i \\cdot \\sum_{j \\in \\mathcal{N}_i}  \\rho I_{\\mathcal{X}_{ij}} (s, a) ||\\tilde{\\pi}^{ij} (\\cdot|s; \\theta^{ij}) - \\pi^{jj} (\\cdot|s)||^2 + \\rho'I_{\\mathcal{X}_{ii}} (s, a) ||\\tilde{\\pi}^{ii} (\\cdot|s; \\theta^{ii}) - \\pi^{ii} (\\cdot|s)||^2],$ (16)\nwhere $\\theta^{ij}$ denotes the parameters of $\\pi^{ij}$ and $\\theta^{-i}$ denotes the parameters of all the $\\pi^{ij} (j \\in \\mathcal{N}_i)$. With this objective, each agent optimises its own policy $\\pi^{ii}$ under the constraint of staying close to the suggested policies. In the meanwhile, the suggestions $\\pi^{ij}$ which are involved in $\\mathbb{E}_{\\mathcal{N}_i}$, are optimised to maximise the agent's individual advantage function $\\tilde{A}_i$ under the constraint of avoiding deviating too far from the actual policies of other agents. This objective function balances individual policy updates with the need for coordination among agents, thereby aligning individual objectives with collective goals. \n$\\tilde{A} = \\sum_{l=0}^{\\infty} (\\lambda \\gamma)^l r_{t+l} + \\gamma^{t+l} V_i (s_{t+1+l}) - V_i(s_{t+l}),$ (17)\nwhere $V_i$ is approximated by minimising the following loss,\n$\\mathcal{L}_{V_i} = \\mathbb{E}[(V_i(s_t) - \\sum_{l=0}^{\\infty} \\gamma^{l}r_{t+l} )^2].$ (18)\nThe algorithm is presented in the experiment section."}, {"title": "Experimental Settings and Results", "content": "We evaluate our method with four diverse environments where agents have conflicting individual rewards. Three environments are adapted from related works, while we propose one of our own environment to facilitate the analysis of the problem and the performance of our method."}, {"title": "Environments", "content": "We evaluate our approach in diverse environments designed to capture distinct cooperation and dilemma scenarios. The environments are described below:\nCleanup. This environment represents a public goods dilemma, adopted from the setting in [Christoffersen et al., 2023]. Agents must clean a river and eating apples. Apples spawn only if the river's waste density is below a threshold, with the spawn rate inversely proportional to the waste density. Eating an apple rewards an agent with +1, while cleaning the river provides neither a reward nor a cost. This setup creates a free-rider problem, where agents may prioritise eating apples over cleaning the river, potentially undermining collective performance. For efficiency, we reduce the environment size to 11 \u00d7 18 and the episode time horizon to 100 time steps, smaller than in [Christoffersen et al., 2023], to decrease training time.\nHarvest. This environment represents a tragedy of the commons dilemma, where agents harvest apples in a shared space. Based on [Christoffersen et al., 2023], apples spawn at a rate proportional to the number of apples around the spawn positions. Only eating an apple provides a reward of +1. The challenge is for agents to harvest apples sustainably while collaborating to avoid over-harvesting in the same region. To reduce training time, we set the episode time horizon to 100 time steps and environment size to 7 \u00d7 38, both smaller than in [Christoffersen et al., 2023].\nCooperative navigation (C. Navigation). In this environment, each agent must navigate to a designated landmark. We use the same observation and action configurations as in [Zhang et al., 2018b]. Agents earn rewards based on their proximity to targets but incur a -1 penalty for collisions. Communication is limited to adjacent agents. We set the time horizon of an episode as 100 time steps and use three agents. The environment size is 5 \u00d7 5, with three agents and an episode time horizon of 100 time steps. \nCooperative predation (C. Predation). This environment involves a sequential social dilemma in a continuous domain, where multiple predator agents aim to capture a single prey. All predators cooperating (approaching the prey) results in each receiving a reward of -1. Universal defection (not approaching) yields a reward of -2N + 1 for each predator, where N is the total number of agents. In mixed scenarios, predators pursuing the prey receive a reward of -2N, while non-participating predators gain 0. The challenge is to incentivise agents to cooperate and capture the prey rather than acting selfishly. At the start of each episode, the prey's position, Xtar \u2208 X, and the agents' initial positions, Xag; \u2208 X, are randomly assigned within X = [0,30]. The state is represented as st = [xag\u2081 - Xtar,...,xagv - Xtar], a continuous variable. The action set A = -1,+1 corresponds to left and right movements. Neighbouring agents are defined as those within a normalised distance of 0.1. The episode time horizon is set to 30, and our main experiments use 8 predator agents."}, {"title": "Baselines", "content": "We evaluate our SS framework against five baseline algorithms designed to optimise the collective return of all agents under individual rewards, ensuring a fair comparison that highlights SS's competitiveness without relying on value or policy sharing. While many other MARL algorithms are commonly used as baselines in the literature, we exclude them due to fundamental differences in problem settings.\nTo ensure comparability, all baseline algorithms and our SS algorithm are built on the same PPO-based MARL framework. This ensures that observed performance differences arise from the information-sharing mechanisms rather than underlying algorithmic variations. The hyperparam- eters used in the experiments are detailed in Appendix A.5 and were selected based on standard practices in the field. For example, we set the discount factor to 0.99 and used the same clipping threshold as in the original PPO paper [Schulman et al., 2015]. Network sizes were tailored to the state and action dimensions of each environment.\nValue Function Parameter Sharing (VPS) [Zhang et al., 2018b]: This approach employs a consensus method to update individual value functions. Each update utilises the agent's unique reward while incorporating a weighted aggregation of value function parameters from neighbouring agents.\nValue Sharing (VS) [Du et al., 2022]: In this method, each agent independently learns a value function and shares the output values with its neighbours. The individual policy network is then updated based on the average of the shared values.\nPolicy Parameter Sharing (PS) [Zhang and Zavlanos, 2019]: This algorithm uses con- sensus updates to learn policies for all agents. Each agent learns N policies based on individual rewards and aggregates policy parameters with neighbours. Value functions, however, are learned independently without consensus updates.\nCentralized Learning (CL): In this method, a centralised value function is learned based on the sum of individual rewards, while each agent learns an individual policy. To avoid the high dimensionality of joint action spaces, a single policy for joint actions is not employed.\nIntrinsic Moral Rewards (IMR): This approach provides intrinsic rewards to cooperative agents in addition to environmental rewards, based on the virtue-kindness moral type proposed in [Tennant et al., 2023]. Each agent learns independently using both individual external rewards and IMR. However, performance is evaluated based solely on external rewards to ensure comparability with other algorithms. Specifically, in Cleanup, IMR rewards an agent for cleaning the river. In Harvest, an agent receives IMR for abstaining from eating apples. In C. Predation, IMR is given to each agent that approaches the prey. For C. Navigation, applying IMR is challenging because cooperative behaviour is not tied to specific actions.\nIt is important to note that CL requires a centralised learning unit, and IMR involves additional rewards, which may limit their practical feasibility. Nonetheless, we include these methods in the baselines to provide a comprehensive comparison for evaluating the performance of our algorithm."}, {"title": "Experimental Results", "content": "Main results. We conducted 5 runs with different seeds for each algorithm and environment. Our SS algorithm demonstrates consistently strong performance across all tasks, with averaged returns matching or exceeding those of baseline algorithms that rely on sharing values or policy parameters. This highlights the effectiveness of SS.\nIn Fig. 1, SS converges faster than PS, which implies that sharing action distributions is more efficient than sharing parameters of policy networks. In Fig. 2, SS outperforms both VS and VPS.\nAdditionally, PS shows better performance than VS and VPS, which may indicate that"}]}