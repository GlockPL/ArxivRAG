[{"title": "Achieving Collective Welfare in Multi-Agent Reinforcement Learning via Suggestion Sharing", "authors": ["Yue Jin", "Shuangqing Wei", "Giovanni Montana"], "abstract": "In human society, the conflict between self-interest and collective well-being often obstructs efforts to achieve shared welfare. Related concepts like the Tragedy of the Commons and Social Dilemmas frequently manifest in our daily lives. As artificial agents increasingly serve as autonomous proxies for humans, we propose using multi-agent reinforcement learning (MARL) to address this issue-learning policies to maximise collective returns even when individual agents' interests conflict with the collective one. Traditional MARL solutions involve sharing rewards, values, and policies or designing intrinsic rewards to encourage agents to learn collectively optimal policies. We introduce a novel MARL approach based on Suggestion Sharing (SS), where agents exchange only action suggestions. This method enables effective cooperation without the need to design intrinsic rewards, achieving strong performance while revealing less private information compared to sharing rewards, values, or policies. Our theoretical analysis establishes a bound on the discrepancy between collective and individual objectives, demonstrating how sharing suggestions can align agents' behaviours with the collective objective. Experimental results demonstrate that SS performs competitively with baselines that rely on value or policy sharing or intrinsic rewards.", "sections": [{"title": "1 Introduction", "content": "Multi-agent reinforcement learning enables collaborative decision-making in diverse real-world applications, such as autonomous vehicle control [Xia et al., 2022, Qiu et al., 2023, Jin et al., 2021], robotics [Wang et al., 2022, Peng et al., 2021, Sun et al., 2020], and communications systems [Siedler and Alpha, Huang and Zhou, 2022]. In these scenarios, artificial agents often act as autonomous decision makers. MARL provides a powerful framework for these settings, enabling agents to learn coordination strategies based on rewards reflecting a common goal.\nHowever, in many cases, a fundamental challenge arises when agents, reflecting the preferences of individuals, are incentivised by interests that conflict with the collective good. This tension is exemplified by the Tragedy of the Commons [Ostrom, 1990] and Social Dilemmas [Kollock, 1998, Van Lange et al., 2013], where pursuit of individual interests can lead to collectively harmful outcomes. For instance, when individuals can benefit from a shared resource without contributing to its maintenance, they often face incentives to 'free-ride' on others' efforts rather than contribute fairly. Without mechanisms to align individual actions with collective welfare, such systems can collapse into inefficient equilibria where shared resources are depleted or congested, harming all participants. Decades of research in economics and sociology have shown that resolving these dilemmas requires careful mechanism design to foster coordination while respecting individual interests [Hauser et al., 2019, Macy and Flache, Gersani et al., 2001, Milinski et al., 2002]."}, {"title": "2 Related Work", "content": "To illustrate these challenges, consider a smart grid system where consumers balance electricity costs against personal comfort. Each consumer optimises their own trade-off, but electricity costs depend on the collective demand patterns across all users. High simultaneous usage drives up prices for everyone, suggesting that consumers should coordinate to avoid peak times. However, individuals may be reluctant to compromise their comfort, instead hoping others will reduce their consumption. This misalignment between individual comfort optimisation and collective cost minimisation can result in inefficient peak loads and higher costs for all participants. A similar dynamic occurs in traffic networks, where drivers independently choose routes to minimise their personal travel times. Without coordination, too many drivers selecting the same optimal routes create congestion, leading to increased delays for everyone.\nA straightforward way to formalise the problem as a MARL problem for collective welfare is to train agents' policies that maximise long-term collective return. Existing solutions often involve introducing designed intrinsic rewards and exchanging individual rewards, values or model parameters. Previous works have proposed various intrinsic rewards based on factors such as social influence, morality, and inequity aversion [Tennant et al., 2023, Hughes et al., 2018, Jaques et al., 2019]. While intrinsic rewards can encourage agents to cooperate, designing appropriate rewards can be intractable in some scenarios.\nAlternatively, sharing rewards has been explored as a means to guide agents towards a collective optimum [Chu et al., 2020b, Yi et al., 2022, Chu et al., 2020a]. Other approaches involve sharing model parameters or the output values of value functions [Zhang et al., 2018a,b, 2020, Suttle et al., 2020, Du et al., 2022]. By aggregating individual values or model parameters from neighbouring agents, these methods enable agents to estimate a global value and adjust their policies to maximise it. Similarly, strategies that share policy model parameters rather than value estimates have been proposed [Zhang and Zavlanos, 2019, Stankovic et al., 2022a,b], where agents learn a shared joint policy through parameter-sharing and consensus techniques. While these methods have shown promise in maximising collective returns in some cases, they rely on the assumption that agents can freely exchange potentially sensitive information. Moreover, they may suffer from exploration issues: when cooperation experiences are rare, agents often lack sufficient motivation to cooperate.\nIn practice, agents typically do not have access to others' rewards, value functions, or policy functions. For instance, in a smart grid system, consumers' electricity usage policies reflect sensitive information about their daily routines and financial constraints, and their interests (rewards/ values) related to comfort are also private. This information may not be something they are willing to share with other participants or a central coordinator. Similarly, an a traffic network, drivers' routing preferences and time valuations, which reveal sensitive details about their destinations, schedule constraints, and willingness to pay for faster travel, are rarely shared with others. Traditional MARL approaches that rely on agents sharing rewards, policies, or value estimates thus become problematic in such settings.\nBased on these observations, we propose Suggestion Sharing (SS), a novel approach for coop- erative policy learning that facilitates effective coordination for collective welfare. SS is grounded in the premise that each agent benefits more when others cooperate, regardless of its own decision to cooperate. For example, in the smart grid scenario, whether or not an agent reduces its elec- tricity usage (cooperates), it always receives a higher reward if other agents cooperate by using less electricity. Thus, agents can share suggestions to encourage cooperation, even in the absence of prior cooperation examples. In SS, agents learn suggestions, share them with one another, and incorporate them into each agent's policy optimisation objective, which is derived from a lower bound of the original collective objective.\nConsequently, in SS, instead of sharing policies or rewards, agents exchange only action sugges- tions proposals for how others could act to help achieve collective benefits. This iterative process aligns individual behaviours with collective objectives while revealing significantly less private in- formation compared to existing approaches. Empirical results across multiple domains, including sequential social dilemmas and the tragedy of the commons, demonstrate that SS achieves coop- eration performance competitive with traditional MARL methods that rely on sharing policies or value functions.\nThe main contributions of this paper are as follows. We propose using MARL to address"}, {"title": "3 Preliminaries and Problem Statement", "content": "sociological problems such as Social Dilemmas and the Tragedy of the Commons, in scenarios where agents can make decisions or provide advice to humans while communicating within systems like smart grids and cooperative driving systems. We introduce the SS algorithm, which promotes cooperation for collective welfare while reducing the sharing of private information. Theoretically, we show that the optimisation objective of SS serves as a lower bound for the original collective objective. Empirical results demonstrate that SS performs competitively with existing MARL algorithms that rely on sharing policies or values.\nThe remainder of this paper is structured as follows. Section 2 reviews related work on coop- erative MARL under individual reward settings. Section 3 provides the technical background and problem formulation. Section 4 details our methodology, including theoretical foundations and the proposed algorithm. Section 5 outlines the experimental setup and results. Finally, Section 6 discusses the implications of our findings and suggest directions for future research.\nTo optimise the collective welfare, we formulate the problem as a Multi-agent Markov Decision Process (MMDP). Specifically, we consider an MMDP with $N$ agents represented as a tuple $<S, \\{A^i\\}_{i=1}^N, P, \\{R^i\\}_{i=1}^N, \\gamma>$, where $S$ denotes a global state space, $A^i$ is the individual action space, $A = \\prod_{i=1}^N A^i$ is the joint action space, $P : S \\times A \\times S \\rightarrow [0, 1]$ is the state transition function, $R^i : S \\times A \\rightarrow \\mathbb{R}$ is the individual reward function, and $\\gamma$ is a discount factor. Each agent $i$ selects an action $a^i \\in A^i$ based on its individual policy $\\pi^i : S \\times A^i \\rightarrow [0, 1]$. The joint action of all agents is represented by $a \\in A$, and the joint policy across these agents is denoted as $\\pi(\\cdot|s) = \\prod_{i=1}^N \\pi^i(\\cdot|s)$. The objective is to maximise the expectation of collective cumulative return of all agents,\n$\\eta(\\pi) = \\sum_{i=1}^N \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t^i \\right],$ (1)\nwhere the expectation, $\\mathbb{E}_{\\tau \\sim \\pi}[\\cdot]$, is computed over trajectories with an initial state distribution $s_0 \\sim d(s_0)$, action selection $a_t \\sim \\pi(\\cdot|s_t)$, state transitions $s_{t+1} \\sim P(\\cdot|s_t, a_t)$, and $r_t^i = R^i(s, a)$ is the reward for individual agent $i$. Here, we use $r^i = R^i(s, a)$ for simplicity of notation, but this can be easily extended to a stochastic reward function without affecting the core of our method."}, {"title": "4 Methodology", "content": "An individual advantage function is defined as:\n$A^i(s, a) = Q^i(s, a) - V^i(s)$ (2)\nwhich depends on the individual state-value and action-value functions, respectively,\n$V^i(s) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r^i_{t} | s_0 = s \\right], \\quad Q^i(s, a) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r^i_{t} | s_0 = s, a_0 = a \\right]$ (3)\nMMDP has also been employed in previous works. \nHowever, in our setup, agents do not have direct access to others' policies, rewards, or values. This setting is particularly relevant for applications where users prefer not to reveal their exact policies and rewards or values. Our work aims to bridge this gap between individual and collective return maximisation. It enables agents to approximate the optimisation of the collective objective while operating solely with their individual reward signals. In the next section, we present a method where agents iteratively share suggestions to maximise a lower bound of Eq.1. This method is general and not dependent on any specific protocol for communicating suggestions between agents. In Sec.4.3, we propose a practical algorithm that involves sharing information within agents' neighbourhoods. Our experiments demonstrate the effects of different sharing protocols on the performance of MARL cooperation.\nIn this section, we start from solving Eq. 1, the collective optimisation objective formulated in Section 3. We derive a lower bound of this objective based on trust region policy optimisation (TRPO) work [Schulman et al., 2015]. The lower bound applies to the setting where agents have individual rewards, distinguishing from previous works where agents share team rewards [Wu et al., 2021, Su and Lu, 2022]. Then we introduce Suggesting Policies to replace the other agents' policies in the individual term corresponding to each agent in the lower bound and derive a bound for the gap caused by such a replacement. By leveraging the gap, agents can learn policies to maximise the collective return in an individual way without explicit reward or policy sharing. We will see that the gap for each agent is related with the discrepancy between the action distribution suggested by others and the agent's own action distribution. Practically, we propose SS algorithm, where agents share their action suggestions with each other. These suggestions are then considered by other agents when maximising their individual objectives, enabling each agent to align with the collective goal.\nUnlike traditional methods that share explicit rewards or objectives, SS involves agents ex- changing suggestions that implicitly contain information about others' objectives. By observing how its actions align with aggregated suggestions, each agent can perceive the divergence between its individual interests and the collective goals. This drives policy updates to reduce the identified discrepancy, bringing local and global objectives into closer alignment."}, {"title": "4.1 Theoretical developments", "content": "We commence our technical developments by Analysing joint policy shifts based on global in- formation. This extends foundational TRPO to multi-agent settings with individual advantage values. We prove the following bound on the expected return difference between new and old joint policies:\nLemma 1. We establish a lower bound for expected collective returns:"}, {"title": "4.2 A Surrogate Optimisation Objective", "content": "$\\eta(\\pi^{new}) \\geq \\eta(\\pi^{old}) + \\zeta_{\\pi^{old}}(\\pi^{new}) - C \\cdot D_{KL}^{max}(\\pi^{old}||\\pi^{new}),$ (4)\nwhere\n$\\zeta_{\\pi^{old}}(\\pi^{new}) = \\mathbb{E}_{s \\sim d^{old}(s), a \\sim \\pi^{new}(a|s)} [A^{old}(s, a)], \\quad C = \\frac{4 \\max_{s,a} |A^{old}(s, a)| \\gamma}{(1 - \\gamma)^2}, $ (5)\n$D_{KL}^{max}(\\pi^{old}||\\pi^{new}) = \\max_{s} D_{KL}(\\pi^{old}(\\cdot|s)||\\pi^{new}(\\cdot|s)).$\nThe proof is given in Appendix A.1.1.\nThe key insight is that the improvement in returns under the new policy depends on both the total advantages of all the agents, as well as the divergence between joint policy distributions. This quantifies the impact of joint policy changes on overall system performance given global knowledge, extending trust region concepts to multi-agent domains.\nHowever, as the improvement in returns is measured by joint policy distributions and total advantages of all agents, it is hard to be used by single agent in MARL settings where each agent has no access to others' policies and rewards. To address this limitation, we first introduce the concept of suggesting joint policy from each agent's local perspective to replace the true joint policy. As we will show in Sec. 4.2, the suggesting joint policy of each agent is solved by optimising an individual objective. Analysing suggesting policies is crucial for assessing the discrepancy between individual objectives and the collective one in cooperative MARL.\nDenotation 1. For each agent in a multi-agent system, we denote the suggesting joint policy as $\\Tilde{\\Pi}^i$, formulated as $\\Tilde{\\Pi}^i(a|s) = \\prod_{j=1}^N \\pi^{ij}(a^j|s)$. Here, for each agent $i$, $\\pi^{ij}$ represents the suggestion of agent $i$ about agent $j$'s policy when $j \\neq i$. When $j = i$, we have $\\pi^{ii} = \\pi^{i}$, which is agent $i$'s own policy. To represent the collection of all such suggesting joint policies across agents, we use the notation $\\Tilde{\\Pi} := (\\Tilde{\\Pi}^1, \\dots, \\Tilde{\\Pi}^N)$.\nThe suggesting joint policy represents an agent's perspective of the collective strategy con- structed from its own policy and suggestions to peers. We will present how to solve such suggesting joint policy in Sec. 4.2.\nDefinition 1. The total expectation of individual advantages over the suggesting joint policies and a common state distribution, is defined as follows:\n$\\zeta_{\\pi'}(\\Tilde{\\Pi}) = \\sum_i \\mathbb{E}_{s \\sim d'(s), a \\sim \\Tilde{\\Pi}^i(a|s)}[A^i(s, a)],$ (6)\nwhich represents the sum of expected advantages for each agent $i$, calculated over their suggesting joint policy $\\Tilde{\\Pi}^i$ and a shared state distribution, $d'(s)$. The advantage $A^i(s, a)$ for each agent is evaluated under a potential joint policy $\\pi'$, which may differ from the true joint policy $\\pi$ in play. This definition captures the expected benefit each agent anticipates based on the suggesting joint actions, relative to the potential joint policy $\\pi'$.\nThis concept quantifies the expected cumulative advantage an agent could hypothetically gain by switching from a reference joint policy to the suggesting joint policies of all agents. It encap- sulates the perceived benefit of the suggesting policies versus a collective benchmark. Intuitively, if an agent's suggestions are close to the actual policies of other agents, this expected advantage will closely match the actual gains. However, discrepancies in suggestions will lead to divergences, providing insights into the impacts of imperfect local knowledge.\nEquipped with these notions of suggesting joint policies and total advantage expectations, we can analyse the discrepancy of the expectation of the total advantage caused by policy shift from the true joint policy, $\\pi$, to the individually suggesting ones, $\\Tilde{\\Pi}$. Specifically, we prove the following bound relating this discrepancy:\nLemma 2. The discrepancy between $\\zeta_{\\pi'}(\\Tilde{\\Pi})$ and $\\zeta_{\\pi'}(\\pi)$ is upper bounded as follows:\n$\\zeta_{\\pi'}(\\Tilde{\\Pi}) - \\zeta_{\\pi'}(\\pi) \\leq f^{\\pi'} + \\frac{1}{2}\\sum_{s,a} \\max_{i} |A^i(s, a)|(\\Tilde{\\pi}^i(a|s) - \\pi(a|s))^2,$ (7)\nwhere\n$f^{\\pi'} = \\frac{1}{2}\\sum_{i} \\max_{s,a} |A^i(s, a)| \\cdot |A| \\cdot ||d^{\\pi'}||^2, $ (8)\nand $||d^{\\pi'}||^2 = \\sum_s (d^{\\pi'}(s))^2$.\nThe proof is given in Appendix A.1.2.\nThis result quantifies the potential drawbacks of relying on imperfect knowledge in cooperative MARL settings, where agents' suggestions may diverge from actual peer policies. It motivates reducing the difference between the suggesting and true joint policies.\nPrevious results bounded the deviation between total advantage expectations under the true joint policy versus under suggesting joint policies. We now build on this to examine how relying too much on past experiences and suggesting joint policies can lead to misjudging the impact of new joint policy shifts over time. To this end, we consider the relationship between $\\zeta_{\\pi^{old}}(\\Tilde{\\Pi}^{new})$, the perceived benefit of the new suggesting joint policies $\\Tilde{\\Pi}^{new}$, assessed from the perspective of the previous joint policy $\\pi^{old}$, and $\\eta(\\pi^{new})$, which measures the performance of the new joint policy. Specifically, $\\zeta_{\\pi^{old}}(\\Tilde{\\Pi}^{new})$ is defined like Definition 1 as:\n$\\zeta_{\\pi^{old}}(\\Tilde{\\Pi}^{new}) = \\sum_i \\mathbb{E}_{s \\sim d^{old}(s), a \\sim \\Tilde{\\Pi}^i(a|s)}[A^{old}(s, a)],$ (9)\nwhich represents a potentially myopic and individual perspective informed by the advantage values, $A_{\\pi^{old}}$, of past policies, as well as individually suggesting joint policies, $\\Tilde{\\Pi}^{new}$, and thus, it may inaccurately judge the actual impact of switching to $\\pi^{new}$ as quantified by $\\eta(\\pi^{new})$. The following theorem provides a lower bound of the collective return, $\\eta(\\pi^{new})$, of the newer joint policy, based on $\\zeta_{\\pi^{old}}(\\Tilde{\\Pi}^{new})$.\nTheorem 1. Based on suggesting joint policies, a lower bound of the collective return of the true joint policy is given as:\n$\\eta(\\pi^{new}) \\geq \\eta(\\pi^{old}) + \\zeta_{\\pi^{old}}(\\Tilde{\\Pi}^{new}) - C \\cdot \\sum_i D_{KL}^{max}(\\pi^{old}||\\pi^{new}) - f^{\\pi^{old}} \\frac{1}{2}\\sum_i \\max_{s,a} | A^{old}(s, a)| \\cdot \\sum_{s,a} (\\Tilde{\\pi}^{new}(a|s) - \\pi^{new}(a|s))^2.$ (10)\nThe full proof is given in Appendix A.1.3. This theorem explains the nuanced dynamics of policy changes in MARL where agents learn separately. It sheds light on how uncoordinated local updates between individual agents affect the collective performance. At the same time, this result suggests a potential way to improve overall performance by leveraging the suggesting joint policies held by each agent."}, {"title": "4.3 A Practical Algorithm for Learning with SS", "content": "Our preceding results established analytical foundations for assessing joint policy improvement in multi-agent settings with individual advantage values and suggesting joint policies. We now build upon these results to address the practical challenge of optimising collective returns when agents lack knowledge of others' policies, rewards, and values.\nDirectly maximising the expected collective returns, $\\eta(\\pi)$, is intractable without global knowl- edge of the joint policy and collective return. However, Theorem 1 provides insight into a more tractable approach: agents can optimise a localized surrogate objective, $\\zeta_{\\pi^{old}}(\\Tilde{\\Pi})$, which is the sum of individual objectives concerning suggesting joint policies and individual advantage values. This simplifies the global objective into an individual form dependent on the suggesting joint policy that is composed of an agent's individual policy, $\\pi^{i}$, and its suggestions for others, $\\pi^{ij}$.\nTo leverage this insight, we use the lower bound given by Theorem 1. By maximising this lower bound, we can maximise the collective return. We can ignore the terms $\\eta(\\pi^{old})$ and $f^{\\pi^{old}}$ from Theorem 1 in our optimisation problem, as they are not relevant to optimising $\\Tilde{\\Pi}$ and their values are usually bounded. To be specific, the value of $\\eta(\\pi^{old})$ is bounded as the reward value is bounded. For $f^{\\pi^{old}}$, as defined in Eq. 8, its value is also bounded since (1) We focus on scenarios\nConsequently, we propose the following constrained optimisation problem as a surrogate for the original collective objective:\n$\\max_{\\Tilde{\\Pi}} \\sum_{i} \\mathbb{E}_{s \\sim d^{old}(s), a \\sim \\Tilde{\\pi}^i(a|s)} [A_{\\pi^{old}}(s,a)] $\ns.t.$ \\sum_{i} D_{KL}(\\pi^i||\\pi^{ij}) \\leq \\delta, \\quad \\max_{s,a} | A^{old}(s, a)|\\sum_{i} (\\pi^{ij}(a|s) - \\pi(a|s))^2 < \\delta'.$ (11)\nNote that, taking into account of the results given by [Schulman et al., 2015], we do not directly include the lower bound of the discrepancy given by Eq. 10 in Eq. 11, but instead use constraints to facilitate learning.\nEq. 11 captures the essence of coordinating joint policies to maximise individual advantages with suggesting joint policies. However, it still assumes full knowledge of $\\Tilde{\\Pi}$. To make this feasible in individual policy learning, we reformulate it from each agent's perspective. Remarkably, we can distill the relevant components into an individual objective and constraints for each individual agent $i$, as follows:\n$\\max \\mathbb{E}_{s \\sim d^{old}(s), a \\sim \\pi^i(a|s)} [A_{\\pi^{old}}(s,a)]$\n$\\pi^i \\Tilde{\\Pi}^i \\s.t.: \\quad (a) D_{KL}(\\pi^i||\\pi^i) \\leq \\delta_1, \\quad (b) k_i \\cdot \\sum_{s, a_j} (\\pi^i(a_j|s) - \\pi^{ij}(a_j|s))^2 < \\delta_2, \\forall j \\neq i, $\n$(c) k_i \\cdot \\sum_{s, a_i} (\\pi^{ij}(a_i|s) - \\pi^{ii}(a_i|s))^2 < \\delta_2, \\forall j \\neq i,$ (12)\nwhere $k_i = \\max_{s,a} | A^{old}(s, a)|$.\nThe constraints in Eq. 12 are imposed on $\\pi^{ij}$ and $\\pi^{ji} (j \\neq i)$, which together compose $\\Tilde{\\Pi}^i$. Therefore, these constraints effectively limit the space of possible $\\Tilde{\\Pi}^i$ by constraining its components. Constraint (a) limits how much the agent's own policy can change, while constraints (b) and (c) ensure that the suggestions are close to the actual policies of other agents. The constraints also depend on other agents' policies $\\pi^{jj}$ and their suggestions for agent $i$'s policy, $\\pi^{ii}$. To enable the evaluation of these terms, each agent $j$ shares its action distribution $\\pi^{jj}(\\cdot|s)$ and the action distribution suggestion $\\pi^{ji}(\\cdot|s)$ with agent $i$. This sharing enables each agent $i$ to assess the constraint terms, which couples individual advantage optimisations under local constraints. These constraints reflect both the differences between the policies of others and an agent's suggestions on them, as well as the discrepancy between an agent's own policy and others' suggestions on it. By distributing the optimisation while exchanging policy suggestions, this approach balances individual policy updates while maintaining global coordination among agents.\nIt's important to distinguish our method from teammate modeling. In teammate modeling, agent $i$ typically approximates peer policies $\\hat{\\pi}^j$ and uses these approximations when solving for its own policy $\\pi^i$. In contrast, our approach in Eq. 12 aims to optimise the suggestions $\\pi^{ij}$ alongside $\\pi^i$. These optimised suggestions $\\pi^{ij}$ are then used by agent $j$ to solve for its policy $\\pi^{jj}$. This method allows the suggestions to implicitly incorporate information about individual objectives. Through the exchange of these suggestions, individual agents can balance others' objectives and, consequently, the collective performance while optimising their own objectives.\nWe propose a structured approach to optimise the objective in Eq. 12. The derivation of the algorithm involves specific steps, each targeting different aspects of the optimisation challenge. Note that in this practical algorithm, we present a setup where agent $i$ exchanges information with neighbours $\\{j|j \\in \\mathcal{N}\\}$ that may not include all other $(N - 1)$ agents, and is not subject to a particular protocol used for determining $\\mathcal{N}^i$. In experiments, we use different neighbourhood definitions/protocols to investigate corresponding effects."}, {"title": "5 Experimental Settings and Results", "content": "4.3 A Practical Algorithm for Learning with SS\nAddressing the KL divergence constraint (a) in Eq. 12 is crucial in ensuring each agent's policy learning process remains effective. This constraint ensures that updates to an agent's individual policy do not deviate excessively from its previous policy. To manage this", "2017": "adapted for individual agents in our method.\nWe start by defining probability ratios for the individual policy and suggesting policies for peers:\n$\\xi^i = \\frac{\\pi^{ii"}], "ratio": "n$\\mathbb{E"}, {"2": "Penalizing Suggestion Discrepancies\nThe objective of this step is to enforce constraints (b) and (c) in Eq. 12", "as": "n$X^{ij"}, {"effect": "n$I_{X"}, "s", "a) = \\begin{cases", 1, "text{if", "s", "a) \\in X", 0, "text{otherwise", ".", "end{cases", 15, "nStep 3: Dual Clipped Objective\nIn the final step", "we combine the clipped surrogate objective with coordination penalties to form our dual clipped objective:\n$\\max_{\\theta^{ii", "theta^{ij", "mathbb{E", {"approach": "agents can optimise a localized surrogate objective", "objective": "n$\\max_{\\Tilde{\\Pi}} \\sum_{i} \\mathbb{E}_{s \\sim d^{old}(s), a \\sim \\Tilde{\\pi}^i(a|s)} [A_{\\pi^{old}}(s,a)]$\n$\\text{s.t.} \\quad"}]