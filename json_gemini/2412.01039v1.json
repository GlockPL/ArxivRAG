{"title": "Reducing Inference Energy Consumption Using Dual Complementary CNNs", "authors": ["Michail Kinnas", "John Violos", "Ioannis Kompatsiaris", "Symeon Papadopoulos"], "abstract": "Energy efficiency of Convolutional Neural Networks (CNNs) has become an important area of research, with various strategies being developed to minimize the power consumption of these models. Previous efforts, including techniques like model pruning, quantization, and hardware optimization, have made significant strides in this direction. However, there remains a need for more effective on device AI solutions that balance energy efficiency with model performance. In this paper, we propose a novel approach to reduce the energy requirements of inference of CNNs. Our methodology employs two small Complementary CNNs that collaborate with each other by covering each other's \"weaknesses\" in predictions. If the confidence for a prediction of the first CNN is considered low, the second CNN is invoked with the aim of producing a higher confidence prediction. This dual-CNN setup significantly reduces energy consumption compared to using a single large deep CNN. Additionally, we propose a memory component that retains previous classifications for identical inputs, bypassing the need to re-invoke the CNNs for the same input, further saving energy. Our experiments on a Jetson Nano computer demonstrate an energy reduction of up to 85.8% achieved on modified datasets where each sample was duplicated once. These findings indicate that leveraging a complementary CNN pair along with a memory component effectively reduces inference energy while maintaining high accuracy.", "sections": [{"title": "1. Introduction", "content": "Energy efficiency is essential for edge devices in resource-constrained smart environments, where reducing energy consumption is critical for extending device life and lowering maintenance costs [1]. Although techniques like network condition estimation and transmission power adjustment have been explored to improve energy efficiency at the network edge, optimizing deep neural network (DNN) models for on-device AI applications remains underexplored [2]. DNNs have proven to be highly effective for solving complex problems in smart environments, leveraging advancements from various fields, such as computer vision [3]. As Internet of Things (IoT), smart homes, and edge computing devices become more widespread, there is an increasing need to run DNN models directly on edge devices to enable real-time processing and minimize latency [4]. However, deploying large DNNs on these devices presents significant challenges due to their limitations, such as low-power processors and limited memory, making it difficult to manage intensive computations and large storage needs [5]. Given that these devices typically operate on batteries, the high energy demands of DNNs can rapidly drain their power, reducing their operational longevity.\nTo address these limitations, one strategy is to compress DNN models into smaller ones. Smaller models, with fewer parameters and layers, require less computational power and energy, making them more suitable for resource-constrained devices. However, their limited capacity can restrict their ability to capture complex patterns and nuances in data, potentially leading to lower accuracy on challenging tasks [6]. In contrast, large DNN models, with more parameters and layers, have a greater capacity to learn intricate features and relationships, often resulting in superior performance on complex problems. This increased complexity, however, demands substantial computational resources and memory [7], posing a significant challenge for deployment on edge devices.\nEfforts to mitigate the computational and memory burdens of DNNs have led to numerous works in the area of DNN compression [8]. These aim to reduce the model's computational and memory demands without compromising its performance. However, in practice, these efforts typically involve a trade-off between reducing model size and computational resources against the potential loss in model performance and predictive accuracy [9].\nA wide range of smart environment applications, from homes to cities, are driven by advancements in the field of computer vision [10]. In computer vision, several methods have been developed to build efficient CNNs [11]. Quantization reduces the precision of weight values to conserve memory. Pruning involves the removal of redundant or insignificant neurons to decrease the computational load [12]. Convolutional filter compression and matrix factorization reduce model size by optimizing the structure of the network. Network Architecture Search (NAS) is another approach that finds DNNs optimized for individual devices, ensuring guaranteed performance. Additionally, knowledge distillation involves training a smaller student neural network to mimic the behavior of a larger teacher model, with"}, {"title": "2. Related Work", "content": "The objective of our research is to reduce the energy consumption and response time of DNN inference, making DNN solutions suitable for resource-constrained edge devices without compromising prediction accuracy. As we will show in this section, the literature reveals a gap in solutions that can run on a single edge device, are not hardware-specific, and do not rely exclusively on compression techniques. Although compression techniques are the go-to approach, they involve a computationally intensive process like knowledge distillation or fine-tuning of pruned models, and they often result in significant performance degradation.\nApart from pruning and knowledge distillation, the other two prominent compression approaches are quantization, and low-rank factorization [8]. These compression methods do not require a computational heavy process but they still result in a performance degradation. Recently, researchers have been investigating approaches such as automating the deactivation of DNN layers, offloading computational workloads to other computing nodes, and co-designing hardware and software. We will briefly present these approaches along with their limitations to better contextualize our work within the relevant literature.\nIn addition to pruning and knowledge distillation, two other widely used compression techniques are quantization and low-rank factorization [8]. While these methods are computationally efficient, they still lead to performance degradation. Recently, researchers have been investigating approaches such as automating the deactivation of DNN layers, offloading computational workloads to other computing nodes, and co-designing hardware and software. We will briefly present these approaches along with their limitations to better contextualize our work within the relevant literature.\n[17] present a method aimed at improving the energy efficiency of DNN inference on edge devices by dynamically adapting the network structure in real time. This technique, known as adaptive DNN surgery, selectively activates or deactivates certain layers and neurons based on the current computational load and resource availability.\nBy doing so, the network can optimize its inference speed and reduce energy consumption, making it well-suited for energy-constrained edge environments. The dynamic adjustments ensure that only the necessary parts of the network are active at any given time, thus minimizing unnecessary computations and associated energy costs. The limitation of this method is that it is trained for on a specific prediction model and for every new use case the process should be retrained to activate and deactivate the layers and neurons.\n[18] present a method to enhance energy efficiency in DNN inference by offloading tasks across multiple complementary edge devices. CoEdge dynamically distributes the inference workload based on each device's computational capabilities and current energy levels, ensuring optimal resource utilization and minimizing overall energy consumption. Real-time adaptability and dynamic load balancing enable efficient resource allocation. In this context, Adaptive Stochastic Learning Automata can dynamically distribute resources to achieve optimal per-"}, {"title": "3. Proposed Methodology", "content": "Our proposed methodology leverages two complementary CNNs to optimize prediction performance while minimizing computational and energy demands. These CNNs operate sequentially: for a given input, the first CNN is initially invoked. The output of the first CNN is evaluated by a prediction confidence score based on the CNN's logits vector. This score is then compared against a threshold, and if it falls short, the second CNN is invoked. The threshold is a predefined value that determines the extent of usage of the second CNN as described in subsection 3.4. For the second CNN, a prediction confidence score is similarly calculated using the score function. If both CNNs are utilized for a prediction, their respective confidence scores are compared, and the final decision is based on the more confident prediction.\nWe also employ a memory component designed to store prior classifications for identical or highly similar inputs. When an input first passes through the memory component, a unique image fingerprint is generated. This fingerprint is used as a key to access a hash table. If the hash table contains a value for the key, this is returned without invoking the CNNs. If no value exists, indicating a new input, the prediction process as described above is initiated. After classification using the CNNs, the classification label is saved to the hash table with the image fingerprint as the key. This component is illustrated in the upper part of Figure 1, enclosed in a dashed box. Note that the proposed dual-CNN architecture can also operate without this component if it is considered preferable in specific application contexts.\nFigure 1 illustrates an overview of our solution's architecture, which comprises three main components: the small CNN architecture A, the small CNN architecture B, and the memory component. These components are activated based on score comparisons. When an input image arrives, it is first processed by the memory component, specifically the perceptual hashing module, to generate a fingerprint hash of the image. If this fingerprint hash already exists in the access hash table, the corresponding label is retrieved, and the result is returned from the hash table. If the hash is not found, the image is passed to the small CNN architecture A, which makes a prediction and computes a confidence score. If this score exceeds a predefined threshold, the prediction is returned and the hash table is updated. Otherwise, the image is forwarded to the small CNN architecture B, which also generates a prediction and a confidence score. The confidence scores are compared in a post-check, and the prediction with the highest score is returned, while the hash table is updated accordingly.\nAn important aspect, shown by the energy arrow on the right of Figure 1, is the varying energy consumption. If the image already exists in the hash table, the energy usage is minimal. When the image is processed by the small CNN architecture A, the energy increases, and it rises further if the small CNN architecture B is also engaged. The following subsections describe each component and the checks of the proposed solution in more detail."}, {"title": "3.1. Two Complementary CNNs", "content": "Our methodology incorporates two distinct and compact CNN architectures. This strategy capitalizes on the diverse design principles of each model, allowing them to complement each other's deficiencies. In instances of classification ambiguity, the utilization of an alternative CNN mitigates such limitations, given that each neural network captures unique facets of the dataset's information. Opting for two small models, as opposed to a mixture of large and small ones, further minimizes power consumption as we will see in the experimental evaluation.\nThe concept of complementarity is illustrated in Figure 2. The gray box represents the whole dataset, of which we consider that the volume outside the circles represent the percentage of labels that are incorrectly predicted by both models and the correct predictions made by a pair of CNNs represented by blue and green circles as subsets of the dataset. The complementarity is function of the symmetric difference between the correct predictions of the two CNNs. The four parts of the Figure 2 shows four examples that help us to understand intuitively the idea of complementarity.\nIn the case of Figure 2a, both models correctly predict a large portion of the dataset, but their predictions overlap significantly. This substantial intersection indicates low complementarity, as the models frequently predict the same samples. Consequently, this results in redundant resource utilization and increased energy consumption.\nFigure 2b depicts a scenario with high complementarity and extensive correct prediction coverage. However, the size disparity between the models is not optimal, as the larger green model requires higher energy consumption.\nFigure 2c illustrates a fully complementary pair with no overlap in their correct predictions. However, their overall prediction coverage is low, resulting in limited accuracy. Despite their perfect complementarity and equal size, the limited complexity and small size of these models lead to suboptimal accuracy.\nFinally, Figure 2d presents the ideal scenario, where both models have similar size, and together they cover a large portion of the dataset with minimal overlap, thus achieving high complementarity and optimal resource utilization.\nWe define the complementarity of a pair of CNN models with the formula given in Eq. 1\ncomplementarity(a, b) =  \\frac{n(a \\cup b) \u2013 n(a \\cap b) \u2013 |n(a) \u2013 n(b)|}{N}\nGiven all the correct predictions from a dataset as represented by the set N, then the subsets a and b represent the correct predictions of models a and b respectively.\nThe first term is defined as:\nn(a \\cup b) = \\sum_{i=1}^{N} I^{1}(y_{i}^{a}, y_{i}^{b}, y_{i}^{true})\nwhere the indicator function $I^{1}$ is defined as:\nI^{1}(y_{i}^{a}, y_{i}^{b}, y_{i}^{true}) = \\begin{cases}\n1, & \\text{if } y_{i}^{a} = y_{i}^{true} \\\\\n1, & \\text{if } y_{i}^{b} = y_{i}^{true} \\\\\n0, & \\text{otherwise}\n\\end{cases}\nThe second term is defined as:\nn(a \\cap b) = \\sum_{i=1}^{N} I^{2}(y_{i}^{a}, y_{i}^{b}, y_{i}^{true})"}, {"title": "3.2. Confidence Score Functions", "content": "CNNs in a classification problem produce a logits vector z, which, upon passing through a softmax function (10), is converted into a probability distribution vector p.\n\\sigma(z)_{i} = \\frac{e^{z_{i}}}{\\sum_{j=1}^{N} e^{z_{j}}}\nwhere i and j are the i-th and j-th element of the logits vector. Each dimension of this vector corresponds to a class, with the value \\sigma(z)_{i} indicating the probability that the input belongs to that class. We then operate on this probability vector applying one of three score functions, as presented in [25]:\na) The Max Probability score function (11) simply selects the highest value from the probability distribution vector. When the CNN is confident in its prediction, this top probability will approach 1.\nscore(p) = max({p_{1}, ..., p_{n}})\nb) The Difference score function (12) calculates the disparity between the first and second highest values in the output probability vector.\nscore(p) = p_{i} - p_{j}\nwhere p_{i} is the largest value in the probability vector p and p_{j} is the second largest value. A larger difference signifies greater"}, {"title": "3.3. Score Comparison & Post-check", "content": "The confidence score is utilized in two steps. First, it is employed to compare the score value of the initial CNN against a predetermined threshold. This comparison determines whether to trigger the subsequent CNN. Second, after the invocation of the second CNN, a second confidence score is computed. The two confidence scores are compared, and the prediction of the CNN with the highest (or lowest, if the entropy score function is applied) score is selected. We name this comparison \"post-check\", indicating an assessment that follows the inference and confidence score of the second CNN. This approach is advantageous as we will see in the experimental evaluation because in some cases the initial CNN may show greater confidence and potentially higher accuracy in its predictions than the second CNN, even if it fails to surpass the threshold test."}, {"title": "3.4. Threshold Hyper-parameter", "content": "The threshold hyperparameter is a fixed value that determines the extent of usage of the second CNN. By employing lower values (or higher in the case of the entropy score function), the utilization of the second CNN is reduced, thereby decreasing energy consumption but also affecting accuracy. However, by employing a higher threshold value, although the invocation of"}, {"title": "3.5. Memory Component", "content": "As an enhancement to our CNN complementary methodology, we incorporate a memory component designed to reduce energy consumption during predictions under specific conditions. This component aims to recall whether a previous classification has been made for a given input, thereby bypassing the need to invoke the CNN when possible. As noted by [7], data movement through memory is a highly energy-intensive process. To address this, we utilized a combination of perceptual hashing [26], which generates a unique fingerprint based on the contents of an image, and a hash table data structure to store and retrieve classification labels for each input. For each input, the required memory access operations are either one read, if the image classification label exists in the hash table, or one read and one write operation, if the input image is new. This approach ensures that energy consumption is kept to a minimum."}, {"title": "3.6. Enhancing Complementarity", "content": "To enhance the complementarity of a chosen pair, we fine-tune the CNN models. Our objective was to \"push\" each model toward different areas of the dataset, thereby improving accuracy gains and overall performance. The core idea was to use the failed prediction instances from one model to train the other model for a few epochs, and vice versa. We explored two primary approaches to fine-tuning: (a) using only the instances where predictions failed and (b) using both the failed and successful prediction instances.\nIn the first approach, shown on the left side of Figure 3, we select a complementary pair of models and pass the validation dataset through the first model. We then collect all instances where the model makes incorrect predictions into a separate subset. This subset is used to train the second model for a few epochs. After fine-tuning the second model, we reverse the process. We pass the validation dataset through the second, now fine-tuned model, collect the failed predictions into a separate subset, and use this subset to fine-tune the first model.\nIn the second approach, shown on the right side of Figure 3, we again select a complementary pair of models and pass the validation dataset through the first model as before. However, this time, we also pass the validation dataset through the second model, capturing the instances where predictions are correct. We then combine the incorrect predictions from the first model with the correct predictions from the second model into a single subset, which is used to train the second model. Next, we reverse the process by passing the validation dataset through both models again. This time, we collect the failed instances from the fine-tuned second model and the correct instances from the first model, combine them into a single subset, and use it to train the first model. This method aims to prevent the overfitting observed in the first approach, which occurs when training on a small subset of data."}, {"title": "4. Experimental Evaluation", "content": "In this section, we present the experimental evaluation of our proposed methodology. Our objective is to assess the performance and efficiency of our method, utilizing Accuracy, Precision, Recall, and F1 Score for performance evaluation, as well as measuring energy consumption, current, and tail latency.\nSubsections 4.1, 4.2, and 4.3 detail the experimental edge device, the datasets used, and the evaluation metrics, respectively. The CNN models used in our proposed methodology are described in subsection 4.4. In subsection 4.6, we summarize the experimental outcomes and discuss the evaluation results."}, {"title": "4.1. Edge Device", "content": "We conducted our experiments on a Jetson Nano computer with a Quad-core ARM Cortex-A57 MPCore processor, 4GB RAM, and 128 NVIDIA CUDA cores, operating in 5W mode. The system OS was Ubuntu 20.04.6 LTS, and for the machine learning framework we used the PyTorch 1.13.0 library with Python 3.6. The experiments' source code is available on GitHub \u00b9. The Jetson was powered through the USB at 5.15V, and we used a USB power meter capable of measuring milliamps (mAh) and watt-hours (Wh) to the second decimal digit."}, {"title": "4.2. Datasets", "content": "We conducted our experiments on CIFAR-10 [28], comprising 10,000 validation images of size 3x32x32 spread across 10 classes, Intel Image Classification \u00b2, comprising of 3,000 validation images of size 3x150x150 spread across 6 classes, FashionMNIST [29], comprising of 10,000 images of size 1x28x28 spread across 10 classes and ImageNet [30], comprising 50,000 images of size 3x224x224 spread across 1,000 classes. More specifically, the ImageNet model architectures are designed to receive 3x224x224 size inputs, but the actual images in the ImageNet dataset have higher varying widths and heights. The vast majority of these images contain 3 channels (RGB), while a few are grayscale with only 1 channel. The Jetson Nano was unable to load all 50,000 test images into main memory, at their default dimensions, as this would require significantly more than the 4GB of available RAM. To address this limitation, we selected a subsample of 10,000 images from the test dataset, comprising of 10 images from each of the 1,000 classes. We pre-processed these images by resizing them to 224x224 pixels and converting grayscale images to 3-channel images.\nTo determine the optimal hyperparameter \u03bb\u2217 for achieving the highest accuracy, as mentioned in equation (16), we can use a validation dataset or a part of the training dataset. For ImageNet, we utilized the additional 40,000 images from the test set as our validation dataset. However, for the CIFAR-10 dataset, which does not have a designated validation dataset, we employed the training set for this purpose. To test the memory component we duplicated our samples for each dataset at different ratios, while also applying simple transformations such as"}, {"title": "4.6. Outcomes", "content": "To identify suitable pairs of CNNs for our proposed methodology, we applied the complementarity formula (1) to the models available in PyTorch Hub, resulting in the complementarity matrix as shown in Figure 4. This matrix displays all possible combinations of model pairs and their corresponding complementarity scores for the CIFAR-10 dataset. The highest score was observed between ResNet-20 and MobileNetV2-0.5, making this pair our selection for our methodology. For better illustration, all values are multiplied by 10. The diagonal elements of the matrix have a complementarity score of 0, as identical models produce identical predictions, representing full homogeneity.\nWe employed a similar process for selecting the model pair for the ImageNet dataset which we do not illustrate for the sake of brevity. After selecting the most suitable models, we evaluated each one individually to establish a baseline. The results are summarized in Table 1."}, {"title": "4.6.4. Energy Evaluation", "content": "Figure 6 presents a comparison of energy consumption across different experimental configurations (Table 2) for the CIFAR-10, ImageNet, Intel and FashionMNIST datasets. For the CIFAR-10 dataset, our configuration (CI3) demonstrates a substantial reduction in energy consumption, achieving a 62.6% decrease compared to the big/little configuration (CI2) and a 76.9% decrease relative to the single large model (CI1). Additionally, it reduces energy consumption by 25.6% compared to the large pruned model (CI6) and by 30.7% when compared to the small distilled model (CI7).\nA similar trend is observed with the ImageNet dataset. Our configuration (IM3) reduces energy consumption by 62% relative to the big/little configuration (IM2) and by 65.3% when compared to the single large model (IM1). In comparison to the large pruned model (IM6), energy consumption is decreased by 32.6%.\nFor the Intel dataset, our configuration (IN3) achieves a 43.5% reduction in energy consumption compared to the big/little configuration (IN2) and a 52.9% reduction relative to the single large model (IN1). However, it shows a slight 4% increase in energy consumption compared to the large pruned model (IN6) and a 13% increase compared to the small distilled model (IN7).\nLastly, for the FashionMNIST dataset, our configuration (FM3) results in a 49% decrease in energy consumption compared to the big/little configuration (FM4) and a 57.5% reduction relative to the single large model (FM1). It also achieves a 6.2% reduction in energy consumption compared to the large pruned model (FM6), while consuming an equivalent amount of energy as the small distilled model (FM7).\nTo evaluate the system's performance in a dynamic IoT setting with constantly changing inputs, we conducted an experiment. We provided 10 randomly selected batches (each containing 100 images) from CIFAR-10 and 10 batches (each containing 100 images) from ImageNet, which were fed to the Jetson Nano device in a random order. The Jetson Nano forwarded"}, {"title": "4.6.6. Complementarity Evaluation", "content": "To evaluate the improvement in accuracy achieved by leveraging the complementarity of the CNN pairs we made experimental comparisons of the CNNs pairs against the single CNNS they include and six CNNs pairs with different complementarity values. In Figure 7 we see the performance of the pair configurations against the baseline performance of each individual model within each pair, as shown in Table 1.\nFor the CIFAR-10 dataset, our implementation (CI3) exhibits a 0.76% increase in prediction accuracy over the best-performing individual model it includes (i.e. MobileNetV2-0.5), while the big/little configuration (CI2) shows a 0.28% improvement against the single big CNN (i.e. RepVGG-A2).\nSimilarly, for the ImageNet dataset, our implementation (IM3) achieves a 1.34% increase in prediction accuracy compared to the highest-performing single model (i.e. DenseNet-121), whereas the big/little configuration (IM2) demonstrates only a 0.07% enhancement compared to the single large CNN (RegNet-X-8GF).\nFor the Intel dataset our configuration (IN2) achieves a 0.7% increase in prediction accuracy over the best-performing individual model in includes (i.e. MobileNetV2-X1.0), while the big/little configuration (IN3) shows a 0.9% decrease in predictive performance against the single big CNN (i.e. RepVGG-A2).\nLastly for the FashionMNIST dataset our implementation (FM3) shows a 1.35% increase in prediction accuracy over the best-performing individual model it includes (i.e Resnet44) whereas the big/little configuration (FM3) shows only an 0.06% increase in prediction accuracy compared to the single large model (i.e. RepVGG-A2).\nThe experiments using six pairs of CNNs with different complementarity values are illustrated in Figure 12. The green and orange segments of the stacked bars represent the accuracy of the individual CNNs. The purple segment indicates the accuracy of CNN pairs based on our proposed methodology. The numbers inside the stacked bar charts show the accuracy gain"}, {"title": "4.6.7. Response Time Evaluation", "content": "Additionally, our methodology improves response times, as shown in Figure 8. For the CIFAR-10 dataset, our configuration (CI3) has 52.7% lower mean response time, 44.6% lower 95-th percentile tail latency and 43.7% lower 99-th percentile tail latency compared to the big/little configuration (CI2). Similarly it has 56.9% lower mean response time, 2.5% lower 95-th percentile tail latency and 2.5% lower 99-th percentile tail latency compared to the single large model (CI1). Compared to the large pruned model (CI6) it has 41.1% lower mean response time but a 26.1% higher 95-th percentile tail latency and 24.6% higher 99-th percentile tail latency. blueCompared to the small distilled model (CI7) it has 47.3% lower mean response time but 17.1% higher 95-th percentile tail latency and 16.4% 99-th percentile tail latency.\nWe have the same observations with the ImageNet dataset: our configuration (IM3) has 65.2% lower mean response time, 49.5% lower 95-th percentile tail latency and 49% lower 99-th percentile tail latency compared to the big/little setup (IM2) and 60.3% lower mean response time, 22.9% lower 95-th percentile tail latency and 21.1% lower 99-th percentile tail latency compared to the single large model (IM1). Compared to the large pruned model (IM6) it has 31.6% lower mean response time but 33.8% higher 95-th percentile tail latency and 34.5% increase in 99-th percentile tail latency.\nFor the Intel dataset, our configuration (IN3) achieves a 27.4% reduction in mean response time, a 17.6% decrease in 95th percentile tail latency, and a 17.4% decrease in 99th percentile tail latency compared to the big/little setup (IN2). When compared to the single large model (IN1), our implementation shows an 18.2% reduction in mean response time, though it exhibits a 48.9% increase in 95th percentile tail latency and a 45.0% increase in 99th percentile tail latency. Relative to the large pruned model (IN6), it features a 1.7% decrease in mean"}, {"title": "4.6.8. Perceptual Hashing Ablation", "content": "Figure 13 presents the energy consumption comparisons when using the memory component. The comparison includes a single large model (CI1, IM1, IN1 and FM1) and a big/little architecture (CI2, IM2, IN2 and FM2) against our implementation without the memory component (CI3, IM3, IN3 and FM3), our implementation using the memory component with the Difference Hash (DHash) method (CI4, IM4, IN4 and FM4), and"}, {"title": "4.6.9. Enhancing Complementarity Evaluation", "content": "Table 3 summarizes the results of our methodology, using fine-tuned model pairs from both approaches outlined in sub-"}, {"title": "4.7. Discussion", "content": "Our methodology contributes to advancing knowledge in the field of energy-efficient on-device AI applications by making research on a relatively unexplored approach: dynamically switching between small DNNs based on confidence scores. This strategy significantly enhances energy efficiency while maintaining predictive accuracy, offering a novel solution for resource-constrained smart environments. Two innovative aspects of our approach have the potential to shape future research directions. First, we illustrate how an effective collaboration between complementary DNNs can improve prediction accuracy by leveraging the strengths of each model to compensate for the weaknesses of the other. This work demonstrates the potential for DNNs to collaborate on the same task, setting the stage for further exploration of cooperative model architectures. Second, we highlight the importance of integrating a memory component that retains previous decisions, allowing the system to bypass redundant inference processes for repeated inputs. This synergy between inference and memory opens new opportunities for optimizing both energy efficiency and predictive performance in AI-driven edge computing environments.\nIn terms of effectiveness, unlike traditional compression techniques or the big-little architecture, the proposed dual complementary CNN methodology leverages two smaller models that compensate for each other's weaknesses. This design achieves accuracy comparable to larger, more complex models while significantly reducing energy consumption. By dynamically selecting predictions based on confidence scores, the method ensures that only the most accurate predictions are used, enhancing overall inference reliability. This approach effectively maintains high accuracy without the computational overhead typically associated with large models.\nIn terms of efficiency, the methodology is specifically designed to reduce energy consumption and response time, a critical concern for edge devices with limited power and computational resources. The dual-CNN setup selectively activates"}, {"title": "5. Conclusion and Future Work", "content": "In this study, we propose a methodology for reducing the energy requirements of on-device CNNs inference through the utilization of two complementary CNNs integrated with a memory component. Each CNN addresses the weaknesses of the other, while the memory component retains previous classifications, thereby bypassing the need for repeated CNN invocations. Our implementation demonstrates up to 85.8% reduction in energy consumption compared to a single large CNN for inferences with multiple identical inputs on the CIFAR-10 dataset up to 80.9% energy reduction on the ImageNet dataset, up to 76.0% energy reduction on the Intel dataset and up to 77.5% energy reduction on the FashionMNIST dataset with negligible loss of accuracy.\nOur future work involves adapting the concept of complementarity to various data modalities and applications in smart environments. Further research directions include examining complementarity based not only on the number of predictions made by the models in a validation dataset but also by focusing solely on confidence scores. Additionally, we aim to develop a new complementarity formula that considers the intrinsic structural characteristics of the CNNs. Lastly, since the concept of complementarity is a significant contribution of our work, We aim to explore its application for enhancing accuracy, rather than solely focusing on reducing energy consumption."}]}