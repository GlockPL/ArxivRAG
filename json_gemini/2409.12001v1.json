{"title": "Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning", "authors": ["Claude Formanek", "Louise Beyers", "Callum Rhys Tilbury", "Jonathan P. Shock", "Arnu Pretorius"], "abstract": "Offline multi-agent reinforcement learning (MARL) is an exciting direction of research that uses static datasets to find optimal control policies for multi-agent systems. Though the field is by definition data-driven, efforts have thus far neglected data in their drive to achieve state-of-the-art results. We first substantiate this claim by surveying the literature, showing how the majority of works generate their own datasets without consistent methodology and provide sparse information about the characteristics of these datasets. We then show why neglecting the nature of the data is problematic, through salient examples of how tightly algorithmic performance is coupled to the dataset used, necessitating a common foundation for experiments in the field. In response, we take a big step towards improving data usage and data awareness in offline MARL, with three key contributions: (1) a clear guideline for generating novel datasets; (2) a standardisation of over 80 existing datasets, hosted in a publicly available repository, using a consistent storage format and easy-to-use API; and (3) a suite of analysis tools that allow us to understand these datasets better, aiding further development. These contributions are all publicly available on our website.", "sections": [{"title": "1 Introduction", "content": "Many complex real-world problems can naturally be formulated as multi-agent systems-e.g. managing traffic (Zhang et al., 2019), controlling fleets of ride-sharing vehicles (Sykora et al., 2020) or a network of trains (Mohanty et al., 2020), optimising electricity grid usage (Khattar and Jin, 2022), and improving dynamic packet routing in satellite communication (Lozano-Cuadra et al., 2024). Improving on solutions to such problems is an important endeavour, because of the potentially immense societal benefits that they offer. Multi-Agent Reinforce-"}, {"title": "2 The Current State of Datasets for Offline MARL", "content": "Offline MARL remains a relatively nascent field, with only a handful of papers released on the topic to date (see Table 1), but progress is accelerating. We want to understand how authors have been handling the data component of their research in the work done thus far by trying to answer the question: what is the state of the field, with respect to data itself? To do so, we present a comprehensive survey of work in empirical offline MARL, from leading peer-reviewed academic venues, to assess (1) how their data were generated and (2) what information about their datasets was provided. Though a simple assessment, we find these two axes already particularly telling in what they reveal. Table 1 summarises our findings.\nFrom Table 1, we firstly notice that the majority of papers assessed generated their own datasets. Each paper also creates these datasets in different ways using a wide variety of underlying online algorithms to learn policies for the generated trajectories. The dataset labels themselves also vary across papers, with no consistent naming convention. Information about the dataset properties is also sparse. To measure a given dataset's 'quality', the mean episode return of trajectories is often reported, but even this metric is not always given. The return distribution is mostly ignored, except for the occasional proxy of reporting the standard deviation. Essentially, there is little information presented on the contents and diversity of the experience in a given dataset. Yet these aspects of a dataset are crucial to the resulting performance of offline learning. In the single-agent literature, it has been shown that dataset properties have a marked impact on results (Schweighofer et al., 2022). In the multi-agent context, other complex aspects of coordination (Tilbury et al., 2024; Barde et al., 2024) make the contents and characteristics of datasets even more important to understand.\nWe observe here that the field of offline MARL has struggled to find common ground to benchmark proposed algorithms. Even accepting that many authors generate their own datasets for their papers, there has been carelessness in reporting information about such"}, {"title": "3 Why Dataset Characteristics Matter", "content": "Claims of algorithmic improvement become moot if acommon basis of data is missing, since a dataset is one of the control variables in empirical offline MARL experiments which can impact performance significantly. We illustrate this point by giving four examples that progressively show how algorithmic results are tightly coupled with data. These examples are not intended to make sweeping claims about the field, but should rather serve as a series of \"proof by existence\" demonstrations cautioning researchers of how peculiarities of datasets may be influencing their experimental findings. When reflecting on this evidence, it becomes clear that overlooking data is problematic for the field, and that ultimately, there is a serious need for a shift in current research practices.\nDataset Mean. We begin our illustration with likely the most intuitive example: what happens to final performance when the average return of the dataset changes? We construct four distinct datasets with increasing means on three scenarios (5m_vs_6m, 3s5z_vs_3s6z and 2s3z) from the SMACv1 environment (Samvelyan et al., 2019), by subsampling episodes from OG-MARL datasets (Formanek et al., 2023a). For each dataset, we fix the standard deviation at approximately 2.0 as calculated over 2000 episodes. We then train two offline MARL algorithms, IQL+CQL (Formanek et al., 2024) and MAICQ (Yang et al., 2021), on the individual datasets and report the final evaluation episode return, averaged across 10 random"}, {"title": "4 Putting Data at the Centre of Offline MARL", "content": "Having illustrated how a lack of consideration for the impact of data in offline MARL is problematic, we take a step towards alleviating some of the issues we have identified in the previous section. By making three data-driven contributions to the community, we hope to bring data closer to the centre of research in the field. Our first contribution is a set of clear guidelines for how researchers should approach generating datasets in offline MARL. If there is an important reason for authors to generate their own datasets, there should at least be sound principles to follow. Secondly, we significantly enhance the standardisation of data in the field by converting over 80 datasets from prior works into a consistent format (Toledo et al., 2023), which has an emphasis on speed, ease-of-use, clear documentation, and integration into existing frameworks. We upload these datasets to Hugging Face for reliable access in perpetuity. We recommend using existing OG-MARL (Formanek et al., 2023a) datasets for future research and encourage any new datasets generated by the community to be added to the repository following the standards and formats outlined there. That said, we still converted all the datasets we could get access to from prior work for the sake of continuity and the possibility of comparing with those works. Finally, we present an ever-growing repository of open-source tools that can be used to access, analyse, and edit these standardised datasets, for future research."}, {"title": "4.1 Dataset guidelines", "content": "The gold standard solution is to standardise all of the existing datasets (which we attempt through OG-MARL) and to use a shared methodology when generating novel datasets. When generating a new dataset, there are certainly basic guiding principles that ought to be followed to ensure good scientific practice. We outline such guidelines in the blue box below."}, {"title": "4.2 Standardising existing datasets", "content": "The single-agent offline RL community have benefited significantly from the widespread adoption of common datasets such as RL Unplugged (Gulcehre et al., 2020) and D4RL (Fu et al., 2020). The offline MARL field could similarly benefit from the adoption of a common set of benchmark datasets."}, {"title": "4.3 Dataset analysis tools", "content": "Our final contribution towards improving data awareness in offline MARL is a set of tools which can be used to download, subsample, combine, and analyse datasets. These tools, which live in OG-MARL, can be used on any dataset which conforms to the Vault API. The tools are accompanied by a demonstrative notebook, which explains how to use them and provides enough understanding of the Vault and OG-MARL systems to be able to work on custom tools and workflows. Our set of utilities are outlined below.\nWe provide a demonstration of our tools using the 2s3z scenario from SMACv1 (Samvelyan et al., 2019), with the dataset from OG-MARL (Formanek et al., 2023a). We focus on dataset analysis (where we give insights into dataset composition), as well as subsampling and combining tools (which can be used for a variety of reasons: to make datasets smaller so that they use less memory, to create datasets for ablations, and to combine datasets when more training data is required).\nAnalysis Our analysis tools cover all requirements stipulated in the analysis section of our dataset generation guidelines. We provide four high-level functions to generate various insights for a user-specified selection of datasets in the Vault format. Calling descriptive_summary with a provided Vault will generate a summary such as the one illustrated by Figure 5, with both tabular and histogram information returned. Users can further access episode return violin plots by calling describe_episode_returns, detailed structural information about the Vault by calling describe_structure, and state-action count information by calling describe_coverage.\nFrom these outputs, we can now analyse the dataset and notice interesting insights. For example, we can see in Table 5 that the Poor dataset contains far fewer trajectories than the Good and Medium datasets, despite containing a similar number of transitions. On average, the episodes in the Poor dataset contain more than 100 transitions, which means that the episodes usually roll out to their full length. We also notice that there is not much of a"}, {"title": "5 Conclusion", "content": "In this paper, we have surveyed the literature and found that data is largely neglected in the data-driven field of offline MARL. We have shown why paying attention to data in offline MARL research is crucial, through simple yet illuminating examples. We contribute to the community: a guideline for generating multi-agent datasets; a standardised repository of over 80 environment-scenario-quality combinations, with a well-documented and accessible API; and useful tooling to aid the understanding of these datasets. In conjunction, these efforts aim to catalyse progress by aligning the field towards scientific rigour. In doing so, along with other standardisation efforts\u2014e.g. standardised baselines (Formanek et al., 2024)-we feel that the discipline is ripe with opportunity to solve hard problems. We encourage researchers to adopt and extend our offerings, working collectively to push the field forward. By working from a strong foundation together, significant breakthroughs can be made."}, {"title": "Broader Impact Statement", "content": "Our contribution opens the door to an offline MARL field in which careful attention is paid to data. However, the path from our contribution to that goal needs to be taken collectively by the community. If our work becomes widely adopted, then we may be shaping what datasets in offline MARL research look like - standardisation by nature places some restriction on the object being standardised, but our guidelines and contributions take into account that some level of flexibility is also required for progress. It is possible that, using our tools, the field could trend towards either using pre-existing standardised datasets (which is preferable) or again generating their own datasets for new experiments (which will be easy to do using our tooling). It is also true that the subsampling and combining tools which we provide can both improve accessibility and standardisation of datasets but also allow researchers to manipulate data without disclosing their alterations.\nWe must consider not only research impact, but real-world impact. Our long-term goal for impact is to build solid foundations on which offline MARL can develop, which may accelerate the progress in the field. The potential effects of progress in offline MARL are vast, but our work specifically assists in standardising and analysing datasets. We hope that transparency around the nature of datasets will filter through not only in research but also into real-world scenarios. Dataset transparency in the real world is, however, a far more complex topic since datasets from real-world scenarios may contain sensitive information. Care must therefore be taken in performing and presenting our suggested analyses on real-world datasets.\nAs with any Deep Learning research, our contribution is likely to have computational expense implications. If datasets are standardised, fewer new datasets need to be created, and fewer baselines need to be rerun. Both of these effects reduce computational expense. Overall, while we have hopes for the impact of our research, we acknowledge areas in which it may be misused. We urge the community to adopt the guidelines of our work in the hopes that progress can be made with care."}]}