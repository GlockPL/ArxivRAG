{"title": "Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation", "authors": ["Yuyang Ye", "Zhi Zheng", "Yishan Shen", "Tianshu Wang", "Hengruo Zhang", "Peijun Zhu", "Runlong Yu", "Kai Zhang", "Hui Xiong"], "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated significant potential in the field of Recommendation Systems (RSs). Most existing studies have focused on converting user behavior logs into textual prompts and leveraging techniques such as prompt tuning to enable LLMs for recommendation tasks. Meanwhile, research interest has recently grown in multimodal recommendation systems that integrate data from images, text, and other sources using modality fusion techniques. This introduces new challenges to the existing LLM-based recommendation paradigm which relies solely on text modality information. Moreover, although Multimodal Large Language Models (MLLMs) capable of processing multi-modal inputs have emerged, how to equip MLLMs with multi-modal recommendation capabilities remains largely unexplored. To this end, in this paper, we propose the Multimodal Large Language Model-enhanced Sequential Multimodal Recommendation (MLLM-MSR) model. To capture the dynamic user preference, we design a two-stage user preference summarization method. Specifically, we first utilize an MLLM-based item-summarizer to extract image feature given an item and convert the image into text. Then, we employ a recurrent user preference summarization generation paradigm to capture the dynamic changes in user preferences based on an LLM-based user-summarizer. Finally, to enable the MLLM for multi-modal recommendation task, we propose to fine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT) techniques. Extensive evaluations across various datasets validate the effectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt to the evolving dynamics of user preferences.", "sections": [{"title": "Introduction", "content": "The development of Large Language Models (LLMs) has significantly enhanced the capacity for natural language understanding (Floridi and Chiriatti 2020; Achiam et al. 2023; Touvron et al. 2023), which has been instrumental in advancing recommendation systems (RSs). LLMs have demonstrated remarkable improvements in processing complex user preferences due to its strong semantic understanding and summarization ability. These attributes significantly enhance personalization and accuracy in recommendations(Wu et al. 2023; Zhang et al. 2024a; Ren et al. 2024), particularly in Sequential Recommendations (SRs) where extracting long historical preferences is crucial (Hou et al. 2023; Zheng et al. 2024; Zhai et al. 2023; Li et al. 2023). Simultaneously, beyond solely modeling textual information, there has been a growing interest in leveraging multimodal information (Liu et al. 2023b). Techniques such as multimodal fusion and gated multimodal units have been utilized to integrate data from various sources\u2014images, videos, and audio-enriching the context for recommendations. This offers a deeper understanding of user-item interactions and naturally leads to the exploration of Multimodal Large Language Models (MLLMs) for enhancing multimodal recommendation systems (Zhang and Tong 2020; Liu et al. 2021b; Zhou and Miao 2024; Kim et al. 2024). MLLMS merge multimodal information into a unified textual semantic space, enhancing the system's ability to understand and interpret complex data inputs, thereby can significantly improving recommendation accuracy (Liu et al. 2024c; Zhang et al. 2024b). The application of MLLMs in sequential recommender systems presents a promising avenue for dynamically adapting to user preferences and handling the intricate interplay of multimodal data, which holds considerable untapped potential. However, integrating Multimodal Large Language Models (MLLMs) into multimodal sequential recommendation systems introduces a set of notable challenges. First, the inherent complexity and computational demands of processing sequential multimodal data, particularly with multiple ordered image inputs, significantly constrain the scalability and efficiency of these systems (Yue et al. 2024; Koh, Fried, and Salakhutdinov 2024). Moreover, conventional MLLMS often exhibit limitations in comprehending the temporal dynamics of user interactions and preferences, particularly in the context of sequential multimodal interactions (Gao et al. 2021; Liu et al. 2024c). This critical limitation undermines the systems' capacity to accurately capture and reflect the evolving nature of user interests over time. Furthermore, fine-tuning multimodal large language models (MLLMs) for specific recommendation scenarios while avoiding overfitting and preserving the generalizability gained during pre-training presents a significant challenge (Borisov et al. 2022; Yin et al. 2023; Li, Zhang, and Chen 2023). These hurdles underscore the need for innovative approaches that can navigate the complexities of multimodal sequential data, ensuring that MLLMs can be effectively leveraged to enhance recommendation systems. To address these challenges, this paper introduces the Multimodal Large Language Model-enhanced Multimodal Sequential Recommendation (MLLM-MSR), a pioneering approach that leverages the capabilities of MLLMs to enhance and integrate multimodal item data effectively. Specifically, we introduce a Multimodal User Preferences Inference approach, which merges traditional multimodal fusion with sequence modeling techniques with MLLMs. Initially, we employ MLLMs to transform visual and textual data of each item into a cohesive textual description, preserving the integrity of the information as demonstrated by a preliminary study. Subsequently, utilizing the enriched item information processed through the MLLM, we develop an innovative LLM-based recurrent method to infer user preferences, capturing the temporal dynamics of these preferences. This method addresses the above mentioned challenges in processing sequential image inputs by harnessing superior text process capabilities of LLMs and improves the interpretability of recommendation compared with traditional representation based approaches, by providing detailed user preference. Further, we fine-tune an MLLM to function as a recommender, utilizing a carefully designed set of prompts that integrate this enriched item data, inferred user preferences, and the ground-truth of user-item interactions. This process of Supervised Fine-Tuning (SFT) on an open-source MLLM, equips the model with the ability to accurately match user preferences with potential items, thereby enhancing the personalization and accuracy of recommendations. To validate the effectiveness of MLLM-MSR, we conduct extensive experiments across three publicly available datasets from various domains, which confirm the superior performance of our approach. The major contributions of this paper are summarized as follows:\n\u2022 To best of our knowledge, our work is the first attempt to fine-tune multimodal large models to address the challenges of sequential multimodal recommendation, where our fine-tune strategies achieving significant improvements in recommendation performance.\n\u2022 We introduce a novel image summarizing method based on MLLMs to recurrently summarize user preferences on multi modality, facilitating a deeper understanding of user interactions and interests over time.\n\u2022 Our approach is extensively validated across various datasets, demonstrating its effectiveness in enhancing the accuracy and interpretability of recommendations."}, {"title": "Related Work", "content": "Sequential Recommenders (SRs) have progressed from matrix-based models to sophisticated neural architectures. Initially, Factorizing Personalized Markov Chains (FPMC) integrated matrix factorization with Markov chains to model sequential behavior (Rendle, Freudenthaler, and Schmidt-Thieme 2010). The transition to neural models started with GRU4Rec, which employed gated recurrent units for session-based recommendations (Hidasi et al. 2015). Subsequently, SASRec utilized self-attention mechanisms to handle long-term dependencies in user-item interactions (Kang and McAuley 2018), and BERT4Rec introduced transformers to SRs, significantly enhancing performance with deep bidirectional training (Sun et al. 2019). Furthermore, the evolution of multimodal information-enhanced SRs has leveraged additional contextual information to improve recommendation quality. Fusion methods in SRs are categorized into early, late, and hybrid approaches (Hu et al. 2023). Early fusion techniques involve invasive methods that integrate various modalities at the input level, enhancing initial feature representation through techniques like concatenation and gating (Tang and Wang 2018; Sun et al. 2019; Lei, Ji, and Li 2019). Besides, non-invasive early fusion employs attention mechanisms to merge multiple attributes before processing (Rendle et al. 2019; Liu et al. 2021a). In contrast, late fusion merges feature sequences from separate modules before the final stage, as evidenced in (Zhang et al. 2019; Ji et al. 2020; Du et al. 2023). Hybrid fusion methods flexibly combine modality fusion and sequential modeling by evaluating inter-modality relationships, offering a versatile fusion strategy (Zhao, Lee, and Wu 2020; Hu et al. 2023).\nLLM for Recommendation\nThe integration of Large Language Models (LLMs) into recommendation systems has been profoundly influenced by foundational models such as BERT (Devlin et al. 2018) and GPT-3 (Brown et al. 2020), which demonstrated the potential of LLMs in processing vast amounts of textual data to understand user behaviors deeply. This foundation has been expanded upon by subsequent models like BERT4Rec (Sun et al. 2019) and innovations such as RLMRec (Ren et al. 2024), which tailor LLM capabilities to generate personalized, context-aware recommendations by analyzing detailed user-item interactions. In the current landscape, LLM applications in recommendation systems are categorized into three main approaches: embeddings-based, token-based, and direct model applications (Wu et al. 2023; Cao et al. 2024). Embeddings-based applications, such as (Cui et al. 2022; Liu et al. 2024b), use LLMs to extract rich feature representations from item and user data, enhancing the system's understanding of user preferences. Token-based approaches, highlighted in works like (Zhai et al. 2023), focus on generating tokens that capture semantic meanings and potential user preferences, integrating this data into the recommendation logic. Lastly, direct model applications (Hou et al. 2024; Geng et al. 2022) involve using LLMs as end-to-end solutions where the models directly generate recommendations based on user queries and profiles, offering a streamlined and potentially more powerful system architecture. Additionally, there has been an emergence of multimodal LLM-based recommendation frameworks designed to handle scenarios involving multimodal information. These frameworks integrate and process diverse data types such as images, text, and video to enhance the recommendation system's accuracy and user ex-"}, {"title": "Preliminary", "content": "In this section, we will give the definition of our research problem and conduct a preliminary study to discuss the effectiveness of image summarizing approach.\nProblem Definition\nWe first introduce the problem formulation of the Sequential Multimodal Recommendation problem. The dataset used in this work contains the interaction records between users and items. Given a user u, let us first define the historical user behavior sequence of u as $S_u = [I_1, ..., I_n]$, where $I_i$ represents the i-th item with which the user has interacted, through actions such as clicking, purchasing, or watching, and n denotes the length of the user behavior sequence. In addition, each item corresponds to a textual description W and a image I (e.g., product diagram, video cover). Consequently, our problem can be formulated as follows.\nDefinition 1 (Sequential Multimodal Recommendation) Given a user u with the corresponding historical behavior sequence $S_u$, including both textual and visual data, and a candidate item $I_c$, the objective of the Sequential Multimodal Recommendation is to predict the probability of next interacted item $I_{n+1}$ (for example, the probability of clicking) with the candidate item $I_c$ for the user u, denoted as $g_u: I_c \\rightarrow R$.\nEffectiveness of Multiple Images Summary\nAs highlighted in the Introduction, current multimodal large language models (MLLMs) face challenges in processing multiple image inputs, limiting their effectiveness for sequential multimodal analysis. To overcome this problem, we introduce an image summary approach that leverages MLLMs to convert and summarize image content. The effi-cacy of this technique is evaluated using the basic sequential recommender, GRU4Rec, on real-world datasets (detailed in the Experiment section). In our approach, we employed simple prompts like \"Please summarize the image\" with LLaVA (Liu et al. 2023a, 2024a) to generate image summaries. These summaries were transformed into latent vectors using BERT (Devlin et al. 2018), which then fed into the GRU4Rec model. This method is benchmarked against direct image representations from VGG19 (Simonyan 2014), assessing performance via the AUC metric."}, {"title": "Technical Details", "content": "This section will introduce the technical details of our proposed MLLM-MSR framework, which contains two main components as Multimodal User Preferences Inference and Tuning MLLM based Recommender, illustrated as Figure 1.\nMultimodal User Preferences Inference\nIn the context of sequential recommendation, a common approach is to learn user representations and predict future interactions with candidate items via calculating affinity scores. Unlike traditional methods that utilize embeddings, LLMs typically analyze user preferences and interaction probabilities directly at token level. This section will detail how our method employs Multimodal Large Language Models (MLLMs) to specifically address challenges associated with multimodal recommendation scenarios.\nMultimodal Item Summarization To effectively predict user preferences, it is crucial to analyze historical item sequences. In multimodal recommendation scenarios, handling multiple image inputs presents a significant challenge for MLLMs, especially in maintaining the sequence of these inputs and aligning textual information with corresponding images. To overcome these issues, we propose a Multimodal Item Summarization approach, which simplifies the processing by summarizing multimodal information of images into unified textual descriptions by designing effective prompts to integrate the multimodal data of items. Our prompt design adheres to foundational methods of multimodal information fusion. Item information can be separated into textual descriptions and image. Hence, In the initial phase, distinct prompts (i.e., text summarization and image description prompt) are used to guide Multimodal Large Language Models (MLLMs) to process these modalities independently, to ensure a more thorough comprehension and detailed feature extraction from each modality, ensuring nuanced characteristics often missed in unified analyses are captured. To ensure both modalities contribute equally to item modeling, the outputs of text summarization and image description are calibrated to similar lengths. After independently analyzing each modality, our design integrates insights from both textual and visual information using a fusion prompt. This approach aligns with traditional multimodal recommendation strategies that emphasize synthesizing diverse data types to create a comprehensive item profile, enhances the multifaceted understanding of the item (Baltru\u0161aitis, Ahuja, and Morency 2018; Huang, Xiao, and Yu 2019; Zhang and Tong 2020).\nRecurrent User Preference Inference In the Sequential Multimodal Recommendation framework, achieving detailed personalization relies on an accurate understanding of user preferences. The advent of Multimodal Large Language Models (MLLMs) marks a significant advancement in understanding multimodal information. However, as we introduced above, they are struggle in dealing with sequential"}, {"title": "Tuning MLLM based Recommender", "content": "After gathering user preferences using the methods described above, we can propose a supervised fine-tuning of an open-sourced Multimodal Large Language Model, such as LLaVA\u00b9, which excels at understanding images. This model would be used to build a multimodal recommender system through SFT. In line with the definition of sequential recommendation, given a user-item interaction, the MLLM-based recommender system utilizes the prompt contained the obtained user preferences, the textual description and image of the given item and the designed system instruction prompt to predict the probability that the user will interact with the candidate item. Specifically, the prompt designed for the tuned MLLM recommender module is illustrated in Figure 3, where we restrict the output to only include 'yes' or 'no' to avoid irrelevant information about the predicted label. Thus, the probability of item interaction can be calculated from the probability score of the predicted first new token as follows:\n$p = \\frac{p(\\text{'yes'})}{p(\\text{'yes'}) + p(\\text{'no'})}$ (1)\nTo construct a multimodal sequential recommender system based on Multimodal Large Language Models (MLLMs), we implement supervised fine-tuning to optimize the model parameters. This fine-tuning process involves adjusting the model to minimize the discrepancy between predicted and actual user interactions. Our dataset construction strategy employs negative sampling, a commonly used training technique in recommendation systems, wherein each positive user-item interaction is coupled with multiple negative samples representing items with which the user did not interact. This methodology aids the model in distinguishing between relevant and irrelevant items through contrastive learning, thereby improving its predictive accuracy. The model is trained on a dataset comprising sequences of user-item interactions, with each interaction encapsulated as a sequence of user preferences, item descriptions, and images. The fine-tuning leverages the next token prediction paradigm, training the model to predict the subsequent token in a sequence based on preceding tokens. This ensures the generation of coherent and contextually pertinent outputs from the input sequences. The supervised fine-tuning loss function is defined as:\n$L = \\sum_{i=1}^{L} \\log P(v_i | v_{0}, I),$ (2)\nwhere $v_i$ represents the i-th token of the prompt text, L denotes the prompt length and I is the given image. The probability $P(V_i | V_{<i}, I)$ is calculated using MLLMs within the next token prediction framework, which maximizes the likelihood of the ground truth tokens given the prompt. This ensures the model learns to accurately predict the subsequent token based on the provided context, which is critical for generating precise and contextually aware recommendations. Specifically, we employ LoRA (Hu et al. 2021) to adhere to a parameter-efficient fine-tuning framework (PEFT), which accelerates the training process."}, {"title": "Experiments", "content": "In this section, we detail the comprehensive experiment to validate the effectiveness of our proposed Multimodal Large Language Model for Sequential Multimodal Recommendation (MLLM-MSR).\nDataset\n#User\n#Item\n#Interaction\n#Avg Seqlen\nSparsity\nOur experimental evaluation utilized three open-source, real-world datasets from diverse recommendation system domains. These datasets include the Microlens Dataset(Ni et al. 2023), featuring user-item interactions, video introductions, and video cover images; the Amazon-Baby Dataset; and the Amazon-Game Dataset(He and McAuley 2016; McAuley et al. 2015), all of them contain user-item interactions, product descriptions, and images. These selections enabled a thorough analysis across different recommendation systems. We preprocessed each dataset by removing infrequent users and items to ensure user history sequences met our minimum length criteria. Additionally, we implemented a 1:1 ratio for negative sampling during training and a 1:20 ratio for evaluation. Further details on these datasets are provided in Table 2.\nTo evaluate the effectiveness of our proposed LMM-MSR method, we selected some compared methods which can be categorized into following groups:\n\u2022 Basic SR Models: These models use item attributes including IDs and textual information. We selectively integrate the most effective information from these attributes to achieve optimal performance. GRU4Rec (Hi-dasi et al. 2015): Utilizes Gated Recurrent Units (GRU)"}, {"title": "Performance Analysis", "content": "The performance of the compared methods and our MLLMMSR is presented in Table 3, where all results were obtained using 5-fold cross-validation and various random seeds, and achieved a 95% confidence level. It is evident that, in our evaluation, MLLM-MSR consistently outperforms all other metrics in terms of both classification and ranking, underscoring the personalization accuracy of our recommendation system. We also observed additional insights: Firstly, compared to basic sequential recommendation (SR) models, our adaptations that incorporate multimodal inputs, particularly SASRec, show significantly better results. This emphasizes the critical role of multimodal integration within the sequential recommendation framework and confirms the efficacy of self-attention-based models in handling both multi-modality and sequential inputs. Moreover, MMSR distinguishes itself from other non-LLM baselines, highlighting the importance of integrating multimodal fusion modules with sequential modeling components in SR tasks, thereby indirectly supporting our prompt design idea for user preference inference. Conversely, purely multimodal recommendation models such as MMGCN and MGAT exhibit lower performance due to their lack of dedicated sequential modeling components. This indicates that for optimal effectiveness in SR, the integration of both multimodal and sequential processing capabilities is essential. Lastly, within the realm of large language model (LLM)-based SR models, our approach significantly outperforms LLaVA without specific fine-tuning. This success validates the effectiveness of our strategically designed prompts for SR tasks. Additionally, our method outperforms TALLREC, demonstrating our success in integrating multimodal information and unlocking the potential of large multimodal models compared to other LLM-based approaches using only textual information. This comparative advantage underscores the integration of advanced MLLM training techniques and the strategic application of multimodal data processing in enhancing sequential recommendation systems."}, {"title": "Ablation Study", "content": "To evaluate the individual contributions of certain components within our MLLM-MSR framework, we developed several variants of MLLM-MSR, each described as follows:\n\u2022 MLLM-MSRR: This variant employs direct user preference inference instead of the recurrent method. It uses the entire chronological order of historical interaction data to infer user preferences.\n\u2022 MLLM-MSR1: In this version, we omit the image summarization component in item summarization. It relies solely on textual data for user preference inference while still incorporating image information in the recommender module, leveraging capabilities that are readily achievable with current MLLMs.\nAs Figure 4 shows, in the evaluations across three diverse datasets, our primary model, MLLM-MSR, consistently outperformed its variants, MLLM-MSRR and MLLM-MSR1, demonstrating the essential roles of its key components. The MLLM-MSRR variant, which employed direct user preference inference, achieved suboptimal performance. This result validates the importance of our model's recurrent method in capturing the dynamic evolution of user preferences, indicates our methods can reflect current interests more accurately and reduce the negative impact of lengthy prompts. Besides, the worse performance of MLLM-MSR1 variant, which excluded image summaries and depended solely on textual data for user preference inference, illustrated the significance of integrating multimodal data. This integration is crucial to understand user preferences across different modalities, thereby significantly compensating for the incompleteness of textual information."}, {"title": "Parameter Analysis", "content": "In this section, we first analyze the optimal block size for the recurrent user preference inference component of our MLLM-MSR model. As Figure 5 shows, striking the right balance on the block size is crucial; Too small a block size simplifies the approach to direct inference, potentially missing the dynamic evolution of user preferences due to the limited contextual span. Conversely, too large a block size leads to long prompts, increasing computational load and reducing the number of blocks available to effectively capture temporal dynamics, thereby diminishing the system's adaptive capabilities. Optimal block sizing ensures the model processes sequential data efficiently and adapts dynamically to changes in user behavior.\nAdditionally, we evaluate the impact of context length on the predictive performance of our model. By fixing the output length during user preference generation, we assess how different context lengths affect recommendation outcomes. The results are shown in Figure 6. We found short context lengths cause a loss of information, resulting in suboptimal predictions. However, once the context length reaches a certain threshold, the results stabilize, indicating that the large model has strong summarizing capabilities and can capture all necessary information within a specific optimal range. This demonstrates the importance of selecting a proper context length to maximize information utility without incurring unnecessary computational complexity."}, {"title": "Conclusion", "content": "In this study, our proposed model, the Multimodal Large Language Model-enhanced Multimodal Sequential Recommendation (MLLM-MSR), effectively leverages MLLMS for multimodal sequential recommendation. Through a novel two-stage user preference summarization process and the implementation of SFT techniques, MLLM-MSR showcases a robust ability to adapt to and predict dynamic user preferences across various datasets. Our experimental results validate the outstanding performance of MLLM-MSR compared to existing methods, particularly in its adaptability to evolving preferences. This paper introduces a innovative use of MLLMs that enriches the recommendation process by integrating diverse modalities and enhances the personalization and accuracy of the recommendations, and meanwhile providing added interpretability through detailed user preference analysis."}]}