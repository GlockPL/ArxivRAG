{"title": "Benchmark Evaluations, Applications, and Challenges\nof Large Vision Language Models: A Survey", "authors": ["Zongxia Li", "Xiyang Wu", "Hongyang Du", "Huy Nghiem", "Guangyao Shi"], "abstract": "Multimodal Vision Language Models (VLMS) have emerged as a transformative\ntechnology at the intersection of computer vision and natural language processing,\nenabling machines to perceive and reason about the world through both visual and\ntextual modalities. For example, models such as CLIP [209], Claude [10], and\nGPT-4V [273] demonstrate strong reasoning and understanding abilities on visual\nand textual data and beat classical single modality vision models on zero-shot\nclassification [107]. Despite their rapid advancements in research and growing\npopularity in applications, a comprehensive survey of existing studies on VLMS\nis notably lacking, particularly for researchers aiming to leverage VLMS in their\nspecific domains. To this end, we provide a systematic overview of VLMs in\nthe following aspects: [1] model information of the major VLMS developed over\nthe past five years (2019-2024); [2] the main architectures and training methods\nof these VLMS; [3] summary and categorization of the popular benchmarks and\nevaluation metrics of VLMS; [4] the applications of VLMs including embodied\nagents, robotics, and video generation; [5] the challenges and issues faced by\ncurrent VLMS such as hallucination, fairness, and safety. Detailed collections\nincluding papers and model repository links are listed in https://github.com/\nzli12321/Awesome-VLM-Papers-And-Models.git.", "sections": [{"title": "1 Introduction", "content": "Pretrained large language models (LLMS), such as LLaMA [233], GPT-4 [195] have achieved remark-\nable success across a wide range of NLP tasks [169, 180]. However, as these models continue to scale\n[187], they face two challenges: (1) The finite supply of high-quality text data [237, 139]; (2) The\ninherent limitations of single-modality architectures in capturing and processing real-world infor-\nmation that requires understanding the complex relationships between different modalities [72, 94].\nThese limitations motivate the efforts to explore and develop VLMS, which combine both visual (e.g.,\nimages, videos) and textual inputs, providing a more comprehensive understanding of visual spatial\nrelationships, objects, scenes, and abstract concepts [22, 84]. VLMS expand the representational\nboundaries that have previous confined single-modality approaches, supporting a richer and more\ncontextually informed view of the world [58, 240, 164], such as visual question answering (VQA) [4],\nautonomous driving [231]. Meanwhile, VLMs encounter new challenges distinct from single-modality\nmodels, such as visual hallucination, which occurs when VLMS generate responses without mean-\ningful visual comprehension, instead relying primarily on parametric knowledge stored in the LLM\ncomponent [75, 148]. There are already several reviews on single-modality models [186, 30] while\nthe multi-modality one is still missing. In this paper, we provide a critical examination of research"}, {"title": "2 State-of-the-Art VLMS", "content": "In recent years, leading Artificial Intelligence (AI) organizations are consistently releasing new\nVLMS [149]. From OpenAI's CLIP [208], Salesforce's BLIP [130], DeepMind's Flamingo [7] to GPT-\n4V [273] and Gemini [9], these models are becoming larger and more interactive and illustrate the\nintegration of chatbot functionality within VLM frameworks to support multimodality user interaction\nto improve user experience. The SOTA VLMS from 2019 to the end of 2024 are listed in Table 1\naccording to the following three principal research directions.\nVision-Language correlation considers how training objectives or architectural design facilitate\nmultimodal integration [295]. Training objectives such as contrastive learning are exemplified by\napproaches like SimCLR [37], which is originally developed for self-supervised vision tasks, adapts\nneatly to multimodal settings by bringing paired images and text closer together in the embedding\nspace while pushing apart unpaired examples. Vision-language architecture considers how structural\nchoices in model design facilitate or constrain multimodal integration [295]. Older architectural\napproaches primarily train models from scratch (CLIP [275]), whereas more recent methods (LLaMA\n3.2-vision [61]) leverage the power of pre-trained LLMS as a backbone to improve the ability to\ncorrelate vision and language to better understand visual content (Section 3).\nBenchmarks and evaluation focuses on designing, collecting, and generating multimodal data,\nprimarily in the format of question-answering (QA), to test VLMS on a variety of tasks such as visual\ntext understanding, chart understanding, video understanding (Section 4).\nApplications of VLMS focuses on deploying VLM models in real-world scenarios. Virtual ap-\nplications typically involve controlling personal device screens or simulated agent game playing\n(Section 5.1). Meanwhile, physical applications of VLMS primarily pertain to interactions with\nreal-world physical objects, such as robotic human interaction or autonomous driving (Section 5.3).\nThese three directions provide a structured framework for analyzing, comparing, and guiding future\nprogress in the rapidly evolving domain of vision-language modeling. [149, 164]"}, {"title": "3 Building Blocks and Training Methods", "content": "The architectures of VLMs are changing from pre-training from scratch to using pre-trained LLMS as a\nbackbone to align the vision and textual information (Table 1). However, the fundamental components\nremain largely unchanged. We summarize the most foundational and widely adopted architectural\ncomponents of VLMS, followed by an explanation of the popular pre-training and alignment methods.\nDetails of SoTA VLM are given in Table 1 to show the shift in basic VLM architectures and newer\narchitecture innovations that fuse visual features with textual features by treating visual features as\ntokens (Section 3.4).\n3.1 Common Architecture Components\nVision Encoder plays a crucial role in projecting visual components into embedding features\nthat align with embeddings from large language models (LLMs) for tasks such as text or image\ngeneration [62]. It is trained to extract rich visual features from image or video data, enabling\nintegration with language representations [167, 299].\nSpecifically, vision encoders used in many VLMs [152, 245, 48, 41], are pretrained on large-scale\nmultimodal or image data: These encoders are jointly trained on image-text pairs, allowing them to\ncapture visual and language relationships effectively. Notable examples include CLIP [209], which\naligns images and text embeddings via contrastive learning, and BLIP [131], which leverages boot-\nstrapped pretraining for robust language-image alignment. Pretrained on large scale ImageNet [51]\nor Similar Datasets: These encoders are trained on vast amounts of labeled visual data or through\nself-supervised training [200], enabling them to capture domain-specific visual features. While"}, {"title": "3.2 Building Blocks of Training From Scratch", "content": "Training a VLM from scratch typically uses distinct training objectives and methodologies compared\nto using an LLM as the backbone. Self-Supervised Learning (SSL) pre-trains without needing\nhuman labeled data to scale up pretraining [87]. Variants of SSL techniques include masked image\nmodeling [86], contrastive learning [236], and image transformation prediction [177]. In this section,"}, {"title": "3.3 Building Blocks of Using LLMS as Backbone", "content": "Large Language Models\nserve as the text generation component that processes encoded visual\nand textual inputs to produce text outputs autoregressively [25, 233, 195] for VLMS. In the context of\nVLMS, LLMS include their original text decoders. In this section, we list two common ways to align\nvisual and pre-trained LLM text features.\nProjector maps visual features extracted by the vision encoder into a shared embedding space\naligned with the text embeddings from the LLM. It typically consists of multi-layer perceptron (MLP)\nlayers [185], which transform high-dimensional visual representations into compact embedding\ntokens compatible with the textual modality. The projector can be trained jointly with the rest of\nthe model to optimize cross-modal objectives or freezing certain parts of the model, such as the\nLLM, to preserve pre-trained knowledge. Most cotemporary examples include LLaVA [152], QWen-\n2-VL [245], Nvidia VLM [48], Baichuan Ocean-mini [135], Emu3 [248], and Pixtral (multimodal\ndecoder) [5].\nJoint Training is an end-to-end approach that updates weights of all components of the model in\nparallel without freezing any weights, including the LLM and projector layers. This approach has\nbeen used in models such as Flamingo [7].\nFreeze Training Stages involves selectively freezing model components during training, preserving\npre-trained knowledge while adapting to new tasks [96]. Common strategies include freezing pre-\ntrained vision encoders while fine-tuning projector layers, and implementing gradual unfreezing of\ncomponents [204] or freezing LLM layers while only updating vision encoder weights [234]."}, {"title": "3.4 Newer Architectures", "content": "Recent works have focused on enhancing the fusion of visual and textual features which we discuss\nin this section.\nTreating all modalities as tokens is a more recent approach that reads and encodes visual inputs\n(images and videos) as tokens similar to text tokens. Emu3 [247] uses SBER-MoVQGAN to encode\nvisual inputs into tokens and employs special separators, such as [SOT] and [EOV], to mark the start\nand end of visual tokens.\u00b9 It still retains the LLMs architectures such as Llama [233], but comes with\nan expansion of the embedding layer to accommodate discrete vision tokens (Root Mean Square\nLayer Normalizatio layer [291] and Multi-query attention [6]). Additionally, it treats the generation\nof both visual and textual outputs as a token prediction task for a unified multimodal representation.\nTransfusion processes different modalities simultaneously within a single transformer architec-\nture [305]. This method treats discrete text tokens and continuous image vectors in parallel by\nintroducing strategic break points. While not yet perfected, this approach shows promising potential\nfor developing more unified multimodal models that can handle diverse input types."}, {"title": "4 Benchmarks and Evaluation", "content": "The number of VLM benchmarks has grown rapidly with the quick development of new VLMS since\n2022 [43, 296]. Comprehensive benchmarking is important for evaluating model performance and\nensuring robust training across different capabilities various aspects such as math reasoning, scene\nhttps://github.com/ai-forever/MoVQGAN"}, {"title": "4.1 How Are Benchmark Data Collected", "content": "Benchmark datasets are typically created using one of three common data collection pipelines:\nfully human-annotated datasets; partially human-annotated datasets scaled up with synthetic data\ngeneration and partially validated by humans; and partially human-annotated datasets scaled up with\nsynthetic data and fully validated by humans.\nFully human-annotated datasets are created by having humans collect or generate adversarial\nor challenging test questions from diverse subjects and fields. For example, MMMU[284] has 50\ncollege students from various disciplines to collect existing test questions from textbooks and lecture\nmaterials, often in multiple choice format. Another approach involves humans creating questions and\nhaving annotators provide answers to these questions. In VCR[287], Mechanical Turks are tasked\nwith using contexts, detected objects, and images to write one to three questions about each image,\nalong with reasonable answers and explanations. Fully human annotated datasets are time-consuming\nand hard to scale up, which brings inspiration to automatic question generation with human validation.\nSynthetic question generation has become a more popular part of benchmark generation pipeline\non various disciplines such as chart understanding [173], video understanding [171] to quickly\nscale up dataset sizes. Common practices include using human written examples as seed examples,\ngiving a powerful LLM to generate more adversarial example questions and answers [126]. Often,\nthe generation process is only involved with texts. Chart and video data are often paired with\nvisual content and captions, which are often used by authors as context to prompt LLMS to extract\nanswers and generate questions [171, 133]. However, LLMS are not always accurate and may produce\nunfaithful content or hallucinations [268]. To address this, pipelines typically include automatic filters\nto remove low-quality outputs, followed by crowdworker validation of either randomly sampled or\nall generated examples [173, 126, 171]. Automatic benchmark generation helps scale dataset size\nwith reduced human effort. However, current automatic question-generation methods primarily rely\non captions and textual contexts, which can lead to the creation of questions that are easy to answer\nwithout requiring significant visual reasoning [75], which undermines the benchmark's primary\ngoal-evaluating a VLM's ability to comprehend and reason about visual content."}, {"title": "4.2 Evaluation Metrics", "content": "Benchmarks are designed for evaluation, with metrics established during their creation. VLM\nevaluation metrics are automatic to support repeated use at scale, and they often influence the question\nformats used in the benchmarks. We show the common evaluation metrics used in our surveyed\nbenchmarks (Figure 2b, 3).\nAnswer matching is widely used for open-ended and closed-ended question types, which are the\nanswers are short-form entities, long-form answers, numbers, or yes/no. Generative VLMS are more\nverbose than extractive LLMS and VLMS, where they often generate verbose but correct answers [141],\ncontainment exact match [108] is a more practical version used more often in the evaluation, which\nincludes removing articles and space of predicted answers and check whether the normalized predicted\nanswer is contained in the normalized gold answer [125, 33]. However, exact match tends to have high\nrecall, which often fails to account for semantic equivalence between the gold and predicted answers,\nfrequently misjudging human-acceptable correct answers as incorrect [26, 31, 141] and becomes\nimpossible for benchmarks that seek long-form answers [265]. Prior to the instruction following\nsuccess of LLM period, standard token overlapping socres such as F\u2081, ROUGE [142], BLEU [198] to\nmeasure the similarity score between the gold and predicted answers, but start failing when generative\nmodels are generating more complex and diverse but correct answers [265, 31, 141, 26].\nThus, some of the benchmarks like MM-Vet [281] adopts LLMS to evaluate generated responses\nwhen the responses are long-form answers that requires semantic understanding to judge correctness.\nLLM evaluations are shown to have the highest correlations to human evaluation, but they also\nface the struggles of producing consistent outputs with internal model updates or changing prompt\ninstructions [170, 300, 116]. While no current answer-matching evaluation method is perfect, yes/no\nquestions are the easiest to evaluate compared to open-ended ones. As a result, most benchmarks rely\non a multiple-choice format to assess VLMS (Figure 2b)."}, {"title": "5 Applications", "content": "VLMS are adopted to a wide variety of tasks, from virtual world applications such as virtual embodied\nagents to real world applications such as robotics and autonomous driving.\n5.1 Embodied VLM agents\nVisual question answering (VQA) is a foundational task that involves answering questions based on\nvisual and textual content [4]. It requires extracting meaningful information from images or video\nsequences, such as identifying objects, scenes, and activities. In practice, embodied VLM agents [156]\nis a popular application of VQA, ranging from embodied personal device chatbot assistance to visual\nchart interpretation and diagram generation for low-vision users [82, 181].\nEmbodied agents\nare AI models with virtual or physical bodies that can interact with their envi-\nronment [230]. Pure textual agents such as Apple Intelligence [79] can process, reason, and execute\nuser requests by converting them to executable code to control phone applications, but lacks visual\nreasoning abilities. In this context, we focus specifically on embodied agents with virtual bodies,\nparticularly in relation to the application of VQA models for personal assistance and accessibility.\nEmbodied VLM agents as assistive applications and accessibility aims at helping users perform actions\non devices or providing on-screen answers to assist individuals with low vision. Recent developments\ninclude: ScreenAI [13] specializes in understanding user interface (UI) components and answering\nquestions about screen elements. Smartphone assistant [55] extends this capability by using an\nend-to-end VLM that directly reads visual screen inputs and user requests and converts into executable\ncode sequences to fulfill user request actions. Similar to Smartphone assistant, ScreenAgent [189]\nuses a three-step approach (planning, acting, reflecting) to process user requests. It first understands\nUI components through natural language descriptions, then decomposes user requests into subtasks,\nand finally generates mouse and keyboard operations in a function-call format to execute actions\non user screens. In addition, some of these VLM agents might also require chart understanding or\ngeneration capabilities to tell a user what the graphics, diagrams or charts are about. VLMS are\noften prone to hallucination, especially for chart understanding that often extracts wrong numbers.\nChartLLaMA [82] is finetuned specifically for understanding various chart or plot visual inputs with\nmore accuracy number extraction and interpretation. Nonetheless, these VLM applications serve as an\nassistant to help users automatically execute actions without user involving and help disabled people\naccess and understand UI pages better to improve accessibility [271].\nDespite the advancements of embodied virtual VLM agents, there is a limitation of their reliance on\nlanguage models, often using vision as a supplementary role rather than fully integrating the two\nmodalities [75]. These models often use language reasoning as the primary driver, with visual input\nplaying a secondary role, leading to insufficient visual understanding to inform decision-making\neffectively. [293, 100]. Besides virtual applications, embodied VLM agents are also used to perform"}, {"title": "5.2 Generative Visual Media Applications", "content": "Generative VLM models, including generative adversarial networks (GAN)[71], diffusion models[91],\nand newer frameworks like Transfusion are widely used in media applications to aid art and content\ncreations. One notable application of generative VLM models is in the creation of memes, a universal\nlanguage of the Internet. Platforms like Supermeme.ai [2] uses VLM models to generate customized\nmemes in over 110 languages, enabling users to express emotions or ideas effectively through\nhumorous or relatable visual content. In addition, generative VLM models are used in cinematic\nand visual effects. For instance, MovieGen[205] allows users to create dynamic movie scenes by\ntransforming static images into visually stunning video effects based on user input."}, {"title": "5.3 Robotics and Embodied AI", "content": "The integration of vision-language models with robotics is a very heated topic that bridges the\nfoundation models residing in cyberspace and the physical world [157]. An enormous amount\nof research work has emerged in the last few years, focusing on using VLMS' abilities on visual\nreasoning [60, 32], complicated scene understanding [222, 155], planning [274, 39] over various tasks\nacross manipulation [113, 102], navigation [168, 76, 280], human-robot interaction [225, 238], multi-\nrobot coordination [40, 251], motion planning [215, 111], reward function design [288, 282, 165],\netc. The revolutionary development in this area triggers many unexplored research problems that\ngather much attention from the robotics community, while also revealing many hidden limitations\nduring implementation (Section 5.3.5)."}, {"title": "5.3.1 Manipulation", "content": "The application of VLM in robot manipulation tasks focuses on improving robots' abilities to ma-\nnipulate out-of-domain objects or perform more demanding, expensive action planning using their\nlanguage priors. VIMA [113] designs a transformer-based robot agent that processes these prompts\nand outputs motor actions autoregressive. Instruct2Act [102] uses an LLM model to generate Python\nprograms that constitute a comprehensive perception, planning, and action loop for robotic tasks.\nRoboVQA [216] proposes an approach for the efficient collection of robotics data, with a large and\ndiverse dataset for robotics visual question answering and a single model with embodied reasoning.\nRobotool [266] proposes a system developed to enable robots to employ creative tools use through the\nintegration of foundation models. The RT series [194, 24, 23] purpose a vision-language action model\nthat encodes visual observations and text prompts and computes the target positions and orientation\nfor robot manipulation tasks. Though the current VLM applications in robotics show impressive\nabilities in visual reasoning and scene understanding in manipulation tasks, their abilities are still\nconstrained by their generalization levels, given the diversity of the robot manipulators."}, {"title": "5.3.2 Navigation", "content": "The incorporation of VLMs in robot navigation tasks focuses on open-world zero-shot or few-shot\nobject-goal navigation or semantic cue-driven navigation. ZSON [168] trains agents on image-goal\nnavigation using a multimodal semantic embedding space, enabling zero-shot ObjectNav from natural\nlanguage instructions and robust generalization to complex, inferred instructions. LOC-ZSON [76]\nintroduces a Language-driven Object-Centric image representation and LLM-based augmentation\ntechniques for zero-shot object navigation. LM-Nav [219] is a system for robotic navigation that\ncombines pre-trained models to enable natural language-based long-horizon navigation in real-world\noutdoor environments without requiring fine-tuning or language-annotated data. NaVILA [42]\nproposes a vision-language-action (VLA) model for legged robot navigation under challenging and\ncluttered scenes. VLFM [280] builds occupancy maps from depth observations to identify frontiers,\nand leverages RGB observations and a pre-trained vision-language model to generate a language-\ngrounded value map to identify the most promising frontier and explore for finding the given target\nobject. LFG-Nav [218] uses the language model to bias exploration of novel real-world environments\nby incorporating the semantic knowledge stored in language models as a search heuristic for planning.\nMany existing works follow the Task and Motion Planning (TAMP) [69] pipeline, a framework"}, {"title": "5.3.3 Human-robot Interaction", "content": "Human-robot interaction (HRI) is a sub-field demanding cognition and adaptation, as well as the\nability to interpret human intentions in reality and take actions accordingly. VLM-powered HRI has\nshown much better ability in understanding human intentions and adaptability during interaction.\nMUTEX [220] is a transformer-based approach for policy learning and human-robot collaboration\nfrom multimodal task specifications, enabling robots to interpret and follow tasks across six modalities\n(video, images, text, and speech). LaMI [239] revolutionizes multi-modal human-robot interaction by\nenabling intuitive, guidance-driven regulation of robot behavior, dynamically coordinating actions\nand expressions to assist humans while simplifying traditional state-and-flow design processes.\nWang et al. [238] designs a pipeline that uses VLMS to interpret human demonstration videos and\ngenerate robot task plans by integrating keyframe selection, visual perception, and VLM reasoning,\ndemonstrating superior performance on long-horizon pick-and-place tasks across diverse categories.\nVLM-Social-Nav [225] leverages Vision-Language Models to enable socially compliant navigation\nby detecting social entities and guiding robot actions in human-centered environments."}, {"title": "5.3.4 Autonomous Driving", "content": "Autonomous driving is a very intensive research area in robotics, while the long-tail corner cases\ncovering out-of-domain objects and traffic events have been a long-lasting problem in this field. The\non-board VLM agents for autonomous driving have revealed abilities to overcome both problems with\nthe better abilities in object recognition [154, 258], navigation and planning [232, 172], and decision-\nmaking [217, 36]. VLPD [154] leverages self-supervised segmentation and contrastive learning to\nmodel explicit semantic contexts like small or occluded pedestrians without additional annotations.\nMotionLM [215] reframes multi-agent motion prediction as a language modeling task by using\ndiscrete motion tokens and autoregressive decoding, enabling efficient and temporally causal joint\ntrajectory forecasting. DiLU [256] combines reasoning and reflection modules to enable the system\nto perform decision-making based on common-sense knowledge and evolve continuously in traffic.\nRecently, more efforts have been made towards end-to-end autonomous driving models that produce\nactions from VLMS without generating intermediate tasks. VLP [197] introduces a Vision-Language-\nPlanning framework that integrates language models to enhance reasoning, contextual understanding,\nand generalization in autonomous driving. DriveGPT4 [267] proposes the first interpretable end-to-\nend autonomous driving system leveraging multimodal large language models, capable of processing\nvideo inputs, textual queries, and predicting vehicle control signals."}, {"title": "5.3.5 Limitations", "content": "Despite the success of VLMs' applications in virtual agents, robotics, and autonomous driving, they\nstill face several limitations.\n1. Generalization vs. Flexibility. Many existing works depend on the TAMP [69] pipeline that uses\nthe vision-language methods to procedure programming code-like workflows [102, 282] constructed\nby pre-defined executable modules, or produce waypoints for external low-level planners to execute\nactions. Such a pipeline allows efficient modulized robot action planning, but its upper-bound is\nconstrained by the scope of available executable modules or low-level planners that are vulnerable\nto out-of-domain (OOD) scenarios. On the other hand, many efforts [215, 111] have been made to\ntokenize the robot's motions as language-like tokens, and outputs the low-level actionable trajectories\ndirectly. Such methods, though reconciling with the nature of robot planning, their abilities are highly\nconstrained by the robots models or datasets encountered in their training procedure, which could be\nhighly diverse in the real world.\n2. Intelligence vs. Safety. Though the applications of VLMs improves the abilities of robots, but they\nalso introduce potential risks that may not be encountered before in robotics research. Risks may\nbe inherited from the jail-breaking [211] and biases of VLMS [106, 12], and robot malfunctioning\nwhen executing VLM-determined actions [260] when applying robot-specific attacks or performing\nreward-hacking. These risks must gather more attraction in revealing and resolving as robots have the"}, {"title": "5.4 Human-Centered AI", "content": "One important and promising application of VLMS is to use their understanding and reasoning abilities\nfor human intentions and behaviors during human interaction with AI agents. LVLMs help to perform\nsentiment analysis [244], predict human intentions [104], and assist human interaction with the real\nworld [199] across many applications for social goodness like AI4Science [28, 303], agriculture [308],\neducation [166, 257], accessibility [192, 294], healthcare [277, 140], climate change [34], etc. VLMs\nshow impressive potential in all these fields and help the widespread AI revolutions have a broad\nimpact on every corner of society."}, {"title": "5.4.1 Web Agents", "content": "Web agent [80] is designed to assist human's daily interaction and activities on the webpages.\nEmpowered by VLMs, web agents show enhanced abilities in understanding human behaviors\nand better adaptation and generalization abilities for human assistance. CogAgent [95] excels in\nGUI understanding and navigation by utilizing high-resolution image encoding. WebVoyager [85]\ndemonstrates complete user instructions end-to-end by interacting with real-world websites by\nleveraging multimodal understanding abilities. ShowUI [207] introduces UI-Guided visual token\nselection to reduce computational costs, interleaved Vision-Language-Action streaming for flexible\ntask handling, and curated GUI instruction-following datasets. ScreenAgent [190] is a VLM that\nutilizes a planning-acting-reflecting control pipeline. This VLM agent is trained to interact with real\ncomputer screens by observing screenshots and executing GUI actions."}, {"title": "5.4.2 Accessibility", "content": "Accessibility intends to help those disabilities living more conveniently, while VLMs help to interpret\nthe visual contexts to those with vision impairment during their interaction with the webpages and\nthe physical world. X-World [294] is an accessibility-focused environment generating annotated sim-\nulation data with dynamic agents using mobility aids, enabling analysis of challenges like occlusion\nand interaction. Oliveira et al. [192] explores using Multimodal Large Language Models (MLLMs)\nto generate high-quality text descriptions for 360 VR scenes based on Speech-to-Text prompts,\nenhancing accessibility and dynamic experiences, as demonstrated in educational VR museum set-\ntings. Mohanbabu et al. [77]introduces a Chrome Extension that incorporates webpage context into\nGPT-4V-generated image descriptions, showing that context-aware descriptions significantly enhance\nquality, imaginability, relevance, and plausibility."}, {"title": "5.4.3 Healthcare", "content": "AI for Healthcare is a sub-field that requires much expertise knowledge in information interpretation\nand very demanding in the accuracy level, due to the severe outcomes. During the recent few years,\ngiven the rapid development of LVLMs, AI for healthcare has been increasingly investigated with\nmany exciting breakthroughs, helping it become much more practical in the real-world applica-\ntions. VisionUnite [140] introduces a vision-language foundation model pretrained on extensive\nophthalmology datasets, exceling in multi-disease diagnosis, clinical explanations, and patient in-\nteractions. Yildirim et al. [277] explores the clinical utility of VLMS in radiology through various\nclinical applications, revealing high potential from assessment from multiple radiologists and clin-\nicians. M-FLAG [146] presents a novel method for pre-training medical vision-language models,\nutilizing a frozen language model for efficiency and an orthogonality loss to optimize latent space"}, {"title": "5.4.4 Social Goodness", "content": "The strong abilities of VLMs help a wide range of applications for social goodness. In K-12 education,\nthe recent works help to reason mathematically over educational content using VLMs [257], or\nsimulate students with various personalities to improve teachers' teaching skills [166]. VLMs help\nto diagnosis disease for plants [308] and optimize the utilization of farmlands [19] in agriculture\napplications. VLMs are also used for promoting fundamental science research like chemistry [124],\nmathematics [18, 202], etc., and for other impactful field like climate change [34], mitigating social\nbiases [68], and urban planning [83]."}, {"title": "6 Challenges", "content": "This section focuses on efforts on 3 challenging areas in VLM evaluation: hallucination, safety, and\nfairness. While recent improvements have enabled VLMs to continuously attain SOTA performance\n(subsubsection 5.4.4), understanding the risks from their misapplication is paramount to assess and\nprevent harms to end users, especially those who belong in marginalized groups. The following\ndiscussion serves to highlight current limitations and ongoing research to ensure the reliable and\nethical use of VLMs."}, {"title": "6.1 Hallucination", "content": "Hallucination refers to the VLM's tendency to refer to objects and/or artifacts that do not appear in the\nrelevant image [212", "212": "proposed CHAIR", "variants": "per-instance", "137": "developed POPE", "78": "released M-HalDetect", "289": ".", "54": "ncontains 1260 images of 11 object types with detailed annotation to detect various hallucination types\nthat occur in perturbed input. [276", "hallucination": true, "75": "proposed HallusionBench to investigate VLM's visual reasoning via dependent questions\nthat have no affirmative answers without visual content to on diverse topics (e.g.: food", "e.g.": "logo, poster, chart) to detect hallucination. [261", "147": "uses GPT-4 to generate 400,000 samples in the form of open-\\"}]}