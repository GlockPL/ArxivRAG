{"title": "Multi-modal Situated Reasoning in 3D Scenes", "authors": ["Xiongkun Linghu", "Jiangyong Huang", "Xuesong Niu", "Xiaojian Ma", "Baoxiong Jia", "Siyuan Huang"], "abstract": "Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models' situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.", "sections": [{"title": "1 Introduction", "content": "Understanding and interacting with the 3D physical world is fundamental to the development of embodied AI. A central challenge in equipping agents with these capabilities is the integration of situational awareness into models. This is particularly critical given the pivotal role of situation awareness in bridging agents' multi-modal local context (e.g., text descriptions, images, point clouds, etc.) with the global environment status, thereby facilitating reasoning and planning in 3D scenes.\nHowever, compared to recent advancement in 3D vision-language learning [9, 1, 72, 24, 30], the study of situation modeling in 3D scenes remained largely underexplored. This is primarily due to the absence of a scalable method to collect diverse multi-modal situational data. Previous studies have mainly relied on simulated environments [63, 55, 47] to generate egocentric observations of virtual agents. These approaches severely limit the quality of situational data due to the constrained diversity and complexity of available synthetic scenes. Recent efforts such as SQA3D [41] have aimed to extend situated understanding to real-world scenes like ScanNet [17] by collecting situated question-answer pairs under imaginative situations represented by locations and orientations in 3D scenes. Nonetheless, this data collection process is prohibitively expensive, thereby restricting the scale of situational data available for model learning and evaluation."}, {"title": "2 Related Work", "content": "Situated understanding in 3D scenes. Existing efforts in 3D VL research primarily focus on understanding and reasoning within 3D scenes, including object grounding [9, 1, 70, 11, 46, 32, 64, 67], captioning [14, 12], and question answering [6, 41, 24]. Recently some initiatives propose unified frameworks for various 3D VL tasks [8, 15, 72, 26, 28, 13, 73, 62], yielding promising outcomes. Nonetheless, a prevailing limitation pertains to the absence of situated understanding in these tasks [40, 9, 1, 72, 6, 28], which accounts for a notable gap between 3D VL and embodied \u0391\u0399 [4, 54, 49, 52, 55, 2, 20, 65]. While earlier works on situated reasoning [18, 21, 56] typically encompass answering simple questions via exploring the simulative environments, SQA3D [41] introduces real-world scenes with a particular focus on spatial reasoning and scene understanding. SIG3D [43] underscores situational awareness and proposes an effective method to address the challenge. In this paper, we extend the 3D situated reasoning task to more diverse and complex scenarios. Furthermore, we devise innovative multi-modal situated next-step navigation to consolidate the evaluation of situated reasoning.\nLLM-assisted data generation. Large Language Models (LLMs) exhibit remarkable proficiency in text generation and serve as a valuable resource for collecting diverse textual instruction-following data [61, 57, 16] and multi-modal instruction-following data [38, 34, 37]. This method also exhibits notable promise to aid the scarcity of 3D VL data [40, 28, 35, 30]. However, the quality of LLM-generated data has been a common concern in the community, especially considering the inherent complexity of 3D scenes. To address this problem, existing efforts [24, 50, 28, 35] have improved the LLM prompting techniques and post-processing procedures to enhance the reliability and diversity of LLM-generated data. And some prior works [10, 19] attempt to evaluate the quality of LLM-generated data yet have not resolved the concerns on the quality of LLM-generated data and how it compares to human-annotated data. In this paper, in addition to advanced prompting techniques and post-processing procedures, we also conduct a human study on the quality of LLM-generated data to demonstrate the efficacy of our LLM-assisted data generation approach.\nInterleaved multi-modal understanding. It has been a critical challenge to precisely delineate the situation within intricate 3D scenes. Natural as it is, adopting textual descriptions [56, 41] may encounter issues of object referral ambiguity, especially when situated within cluttered environments. On the other hand, ego-view visual observations [18, 4, 54, 23, 22] are widely adopted in embodied tasks but bridging the modality gap demands extra training. Recently, interleaved multi-modal data has become prevalent in both VL [58, 3, 27, 34, 71, 29, 69, 35] and embodied AI [53, 31, 36]. In the context of 3D situated reasoning, the interleaved multi-modal format can remedy the ambiguity and thus stands as a general scheme to delineate the situation. Such an interleaved multi-modal scheme strengthens the challenge of our situation reasoning task, requiring comprehensive capabilities of VL grounding and multi-modal situated reasoning."}, {"title": "3 Multi-modal Situated Reasoning Dataset", "content": "We propose a novel and scalable approach to collecting high-quality 3D situated reasoning data, guided by three core principles: (1) ensuring comprehensive and diverse situations, (2) crafting highly situation-dependent questions with accurate answers, and (3) accommodating the multi-modal interleaved input format for avoiding ambiguities. We construct the MSQA dataset by employing our data collection pipeline on complex real-world scenes sourced from ScanNet [17], 3RScan [60] and ARKitScenes [7]. MSQA comprises 251K multi-modal situated question-answering data. Each data instance can be formulated as a tuple (P, S, q, a), where P denotes the scene point cloud; S = (stxt,img, sloc, srot) includes a multi-modal situation description stxt,img, the corresponding location sloc and orientation srot, the interleaved multi-modal question q = qtxt,img collected under situation S, and the ground truth answer a. In the following sections, we delineate our data collection pipeline in Appendix A.2 and present data statistics in Appendix A.6."}, {"title": "3.1 Data Collection", "content": "As illustrated in Fig. 3, we meticulously devise an LLM-based automatic data collection pipeline comprising three stages: situation sampling, QA pairs generation, and data refinement. Our goal for data collection is to ensure the high quality of generated data. We detail the pipeline below.\nSituation sampling The situation consists of four components: (i) the location sloc = (x, y, z), (ii) the orientation represented by a rotation angle srot within the XY plane, (iii) location description, and (iv) surrounding object descriptions. In our setup, we first sample the location and orientation considering four scenarios: (i) standable area on the floor with arbitrary viewpoint, (ii) sittable area with front viewpoint when sitting, (iii) reachable area of large objects (e.g., cabinets and fridge) with viewpoints facing or against the object, and (iv) reachable area of small objects (e.g., trashcan) with viewpoints directing standing point to object centers. We then generate location descriptions according to the interaction types (e.g., \"I'm standing on/sitting on/in front of the fridge ...\"). For surrounding object descriptions, we first calculate the spatial relations between the location and surrounding objects, including distance, coarse direction (e.g., left), and fine-grained relative direction (e.g., 2 o'clock). We then utilize these spatial relations to prompt GPT-3.5 for surrounding object descriptions. We provide more details and illustrative examples of sampled situations in Fig. 8.\nQA pairs generation Similar to prior works [28, 30], we adopt scene graphs to prompt LLM for data generation. We first instantiate each object in the scene graph with their attributes obtained by prompting GPT-4V [45] using the cropped object images. We then perform pair-wise calculations among the initialized objects to derive relations, which can be categorized into five types: in-contact vertical relations (e.g., support), non-contact vertical relations (e.g., above), horizontal distance (e.g., near), horizontal proximity relations (e.g., right) and multi-object relations (e.g., between)."}, {"title": "3.2 Data Quality Control", "content": "Despite the scalability of the LLM-based data collection pipeline, the quality of generated data has raised major concerns, especially in 3D vision-language tasks where grounding language is challenging. To address these concerns, we conduct a human study comparing our generated data to human-annotated data in SQA3D. Specifically, we sample 100 data instances from MSQA and SQA3D and mix them for human assessment. The human evaluators are instructed to score the data on three aspects: (1) the naturalness and clarity of situation descriptions, (2) the situational dependence and clarity of questions, and (3) the accuracy and completeness of answers. Each aspect was rated on a scale from 1 to 5. Detailed information about the evaluation workflow is provided in Appendix B. The evaluation results, shown in Fig. 4(b), indicate that MSQA's quality is comparable to SQA3D across all aspects. Additionally, Fig. 4(c) shows that the proportion of high-scoring data (i.e., quality with score \u2265 4) in MSQA matches or exceeds that of SQA3D. This highlights the quality of MSQA and also the effectiveness of our data refinement procedures."}, {"title": "4 Evaluation Benchmarks", "content": "In this section, we give a detailed description of the evaluation tasks considered for multi-modal situated reasoning. Specifically, we consider the following two benchmarking tasks:\nMulti-modal Situated Question Answering (MSQA) As mentioned Sec. 3, we evaluate models' capability in situation awareness and handling interleaved multi-modal input in MSQA. Specifically, given a multi-modal situation description, the model answers a text-image interleaved question grounded in the 3D scene. Since the responses are open-ended, former metrics, such as classification accuracy and exact-match accuracy can not give a correct evaluation. To solve this problem, we use a GPT-based evaluation metric for open-ended responses following OpenEQA [42] and extend its prompt sets for 3D situated reasoning (see detailed prompt in Appendix B.1.1). Above all, we report the correctness score C for the test set with N samples following OpenEQA, C could be calculated by:\n$$C = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{s_i - 1}{4} \\times 100 \\%,$$ where si (ranging from 1 to 5, the higher the better) is generated by the LLM when prompted with the question, ground truth answer, and the model response.\nMulti-modal Situated Next-step Navigation (MSNN) In addition to MSQA, we also aim to evaluate the models' capability of situation awareness through embodied AI tasks such as navigation. To separate long-horizon planning from situated understanding, we propose the MSNN task, which focuses on predicting the best immediate next step action grounded by the current situation and navigation target in a 3D scene. Specifically, given the agent's current interleaved multi-modal description of the situation (i.e., location, orientation, and text description), textual goal description, and the overall scene, we instruct models to answer the immediate next action for navigating to the goal in a textual form. For evaluation, we generate MSNN data following a pipeline similar to situated QA pair generation with four critical components: (1) starting situation sampling, (2) goal sampling, (3) optimal trajectory prediction, and (4) calculation of ground truth immediate next-step action. The optimal trajectory is sampled by running an A* algorithm planning the shortest path from the starting location to the goal on the floor plan and the immediate next-step action is determined by following the direction of optimal trajectory relative to the starting situation. In total, we generate a dataset comprising 34K MSNN data samples across 378 3D scenes in ScanNet. This dataset is further utilized for supervised fine-tuning and MSNN evaluation. We provide more details on MSNN data generation and data statistics in the Appendix."}, {"title": "5 Experiment", "content": "5.1 Model Settings\nInspired by recent advancement in 3D generalist models, LLMs and VLMs, we propose several potential approaches for MSQA and MSNN including models that can be directly applied to these tasks in a zero-shot setting, and models that require instruction tuning."}, {"title": "5.2 Evaluation Results", "content": "In this section, we provide evaluation results of models on MSQA and MSNN. We report the average correctness score (as illustrated in Sec. 4) across test sets for both tasks. Additionally, we consider different settings on the modality of the situation and question input (Input), the representation of 3D scenes (Scene), and the model setting (Setting). For MSNN, we ablate the choice of pre-training data (PT data) as an additional axis to verify the usefulness of MSQA for embodied tasks.\n5.2.1 Multi-modal Situated Question Answering (MSQA)\nWe present the experimental results of MSQA in Tab. 2 and report the following findings:\nZero-shot models struggle in situated spatial reasoning. Zero-shot models excel in answering commonsense questions, such as those related to affordance and room type (categorized as Other), likely due to LLMs' proficiency in natural language tasks. Given that object attributes are provided in the list, these models show superior performance in attributes and descriptions compared to fine-tuned models. However, they fall short in addressing spatial relationships and navigation questions, highlighting their limitations in multi-modal situated reasoning.\nSituation modeling matters in situated spatial reasoning. 3D vision-language models like LEO struggle without fine-tuning on MSQA, reflecting its limitations as a generalist foundation model. Our model trained without interleaved input outperforms LEO on spatial relationships and navigation, highlighting the importance of our situation modeling method. Meanwhile, the performance of MSR3D declines sharply in fine-tuning without 3D scene input (blind). This underscores the importance of situation awareness and 3D scene understanding in addressing MSQA.\n3D point cloud is a better scene representation compared to textual descriptions. We conduct an additional experiment with solely textual descriptions, which are derived by prompting GPT-3.5 based on situated scene graphs. The situations used for generating textual descriptions are the same as those for QA pairs in MSQA. The results in Tab. 2 (row \u201cDES\u201d) indicate a notable drop when provided with textual descriptions, especially in object attribute, spatial relation, and navigation. To proceed, we probe the reason why \u201cDES\u201d shows better performance in counting. As shown in Tab. 3, \"DES\" is better for GT< 3 but worse for GT\u2265 3. This is intuitive since \u201cDES\u201d explicitly depicts the target objects in the input. However, when the count of target objects exceeds a certain threshold, some target objects are likely to be truncated due to limited context length. In summary, the results demonstrate that the 3D point cloud serves as a more efficient representation for situated reasoning compared to textual descriptions.\nSituation component matters for situated reasoning. To reveal the effectiveness of the situation for FT models, we add an FT baseline with the situation component entirely removed, retaining the 3D scene and question as input. The results in Tab. 2 (w/o situation) show a notable drop in performances"}, {"title": "5.2.2 Multi-modal Situated Next-step Navigation (MSNN)", "content": "We present the experimental results of MSNN in Tab. 5 and report the following findings:\nMSNN is challenging. The results in Tab. 5 indicate that both state-of-the-art LLMs (i.e., GPT-3.5 and GPT-40) and 3D VL models encounter considerable challenges in solving MSNN. This implies the value of the proposed MSNN task for 3D situated reasoning and embodied AI.\nMSQA is beneficial as a pretraining source for embodied AI. We find that adopting MSQA for pretraining (both LEO and MSR3D) significantly improves the performances on MSNN, which indicates the effectiveness of MSQA as a pretraining source for addressing embodied navigation.\nSituation modeling of MSR3D is effective. We find that MSR3D (T), endowed with situation modeling, shows a significantly higher accuracy in navigation action prediction (+8.56%) compared with LEO (T). This demonstrates the effectiveness of our situation modeling method. Additionally, we test MSR3D without situation by masking the location and orientation of the agent, which leads to a great performance drop as shown in Tab. 5 (w/o situation). Such a drop demonstrates the critical role of situation information and that MSR3D can utilize the situation information well."}, {"title": "5.3 Additional Analysis", "content": "Scaling effect We explore the scaling effect on MSQA by training MSR3D with different data scales. We investigate three factors for scaling: QA (randomly downsampling QA pairs), situation (downsampling both QA pairs and situations), and scene (downsampling both QA pairs and scenes). As shown in Fig. 7, we observe a consistent trend of improvement when scaling up along the three factors, which exhibits a significant scaling effect and manifests the potential of further scaling up. We also provide additional analysis of the scaling effect on the MSNN task in Appendix D.1.\nCross-domain transfer We divide the MSQA data into three subsets according to the scene domain: ScanNet [17], 3RScan [60] and ARKitScenes [7]. Then we investigate cross-domain transfer by training MSR3D on each subset and evaluating on all the subsets, respectively. The results in Tab. 6 show that the best performance on each subset is achieved by in-domain training (bold) rather than cross-domain transfer, showcasing the domain gap. And training on ARKitScenes elicits inferior cross-domain transfer results. Considering the relatively simple scenes in ARKitScenes, it implies that training on complex scenes would be beneficial for cross-domain generalization."}, {"title": "6 Conclusion", "content": "In this paper, we introduce Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset collected with a scalable data generation pipeline. MSQA comprises 251K situated QA pairs across a variety of real-world scenes, presented in a unified format with interleaved text, images, and point clouds. We present a challenging benchmark based on MSQA for evaluating multi-modal situated reasoning in 3D scenes. Additionally, we propose Multi-modal Situated Next-step Navigation (MSNN), a task to assess the capability of situated reasoning and embodied navigation. Our comprehensive experiments highlight the value of our dataset and benchmarks. We hope this work will advance the development of situated scene understanding and embodied AI."}, {"title": "Limitations and future work", "content": "Our work proposes an automatic pipeline to scale up multi-modal situated reasoning data based on existing real-world 3D assets. We also introduce an innovative evaluation task MSNN for situated reasoning and embodied navigation. Despite our contributions, some limitations remain to be addressed.\nFirstly, LLM-generated data needs further alignment with human preference to achieve higher data quality. Despite our meticulous design in refinement procedures and data balance, some unnatural data remains due to the rule-based scene graph and biases of LLMs. For instance, LLMs may select distant objects for situational descriptions, which might be an improbable behavior for humans. We encourage further exploration of human feedback integration in the data generation process to better align with human preference.\nSecondly, we have not yet fully leveraged the available 3D assets. Expanding our data generation pipeline to cover more real-world and synthetic 3D scenes will further enhance the scale and diversity of the situated reasoning data, probably inducing stronger models. Given the expense of creating large-scale QA pairs by prompting LLMs with situated scene graphs, we anticipate that training a specific LLM tailored for generating QA pairs from situated scene graphs could substantially reduce the cost of data generation. We leave this path for future research.\nFinally, the evaluation tasks for assessing situational awareness and situated reasoning should not be confined to question answering and action prediction. For example, some other tasks focusing on scene understanding like object grounding could also be considered. We would explore more evaluation suites in future work."}, {"title": "A Dataset Details", "content": "A.1 Situation Sampling\nThe sampling rules of location and orientation are as follows.\nStanding. We evenly sample a point from the floor area as the location and an angle within [0, 2\u03c0) as the orientation.\nSitting. We randomly sample a point from the sitable area, e.g., chairs and couches. The orientation is calculated based on the normal direction of the backrest.\nInteracting with large objects. For large objects, e.g., cabinets and refrigerators, we first parse the interactive part such as the door. Then we sample a standing point from the nearby floor as the location and use the normal direction of the interactive part as the orientation.\nInteracting with small objects. For small objects, e.g., bags and trash cans, we first sample a standing point from the nearby floor as the location and then use the direction from the standing point to the object center as the orientation."}, {"title": "A.2 Data Collection", "content": "A.2.1 HOI Examples\nTo boost diversity in situation descriptions, we create varied Human-Object Interaction (HOI) descriptions using LLMs."}, {"title": "A.2.2 Object Attribute Detection", "content": "We utilize GPT-4V for object attribute detection. Seven attribute types, i.e., color, 3D shape, material, usage, texture, structure, and state, are considered. We first crop the object from the corresponding multi-view color images provided in each dataset, and then use the largest-size cropped image for prompting. The multi-view images are deblurred to enhance the image quality before cropping. Since GPT-4V is powerful enough to generate detailed attribute descriptions, besides concise attributes, we also prompt GPT-4V to generate the descriptive attributes of the object at the same time."}, {"title": "A.3 LLM-assisted Data Generation", "content": "Fig. 10 illustrates the prompts used to generate data. In particular, we instruct ChatGPT [44] to produce both the question-answer pairs and their corresponding types."}, {"title": "A.3.1 Refinement", "content": ""}, {"title": "B Evaluation Details", "content": "B.1 GPT-score\nB.1.1 Prompts for GPT-score\nWe craft a precise prompt to evaluate open-ended replies in MSQA, depicted in Fig. 17. Specifically, we embed scoring examples to bolster robustness and synchronize the scores with human evaluation."}, {"title": "B.1.2 Correlation of GPT-scores and Human Scores", "content": "We selected GPT-score as the metric for MSQA benchmark and crafted a detailed prompt to match human preferences. To confirm the soundness of this decision, we assessed responses and computed the Pearson Correlation Coefficients between human evaluations and various metrics. We manually scored 200 responses (randomly selected from the model's predictions) and calculated the corresponding metrics."}, {"title": "B.2 MSNN Data Generation Details", "content": "The MSNN data generation pipeline can be separated into four parts, i.e., start situation sampling, goal sampling, optimal trajectory prediction and ground truth action calculation. Details about each part are stated as follows:\nStart situation sampling. We sample the start situations with the same sampling strategy stated in Section 3 of the paper. The location, orientation, and text-image interleaved descriptions are provided as the start situation.\nGoal sampling. We define the goal as navigating and interacting with one object in the scene. We first random sample an object in the scene and then generate a text description of the interaction description by prompting GPT-3.5\nOptimal trajectory prediction. With the sampled start and goal location, we can then predict the optimal navigation trajectory. Floor areas are regarded as the passable area for navigation, and the A* algorithm is employed to get the best navigation trajectory.\nGround truth action calculation. After determining the optimal navigation trajectory from the starting point to the target destination, we subsequently determine the agent's immediate action by calculating the required orientation adjustment relative to the initial situation. We consider four potential actions, i.e., moving forward, turning left, moving backward, and turning right. The agent's ground truth one-step action is determined by the calculated orientation adjustment."}, {"title": "C Model Details", "content": "C.1 MSR3D\nC.1.1 Overall Structure\nThe overview structure of the MSR3D is illustrated in Fig. 19. This model is adapted from LEO [28], and further extended to accommodate text-image interleaved inputs. The tokenization for different modalities is stated as follows:\nText. For texts in the instructions (i.e., system messages, situation description, text in multi-modal instruction, and response), we use SentencePiece tokenizer [33] to encode them with 32k subwords."}, {"title": "C.1.2 Model Training", "content": "We train MSR3D in a GPT-style auto-aggressive prefix language modeling fashion, where the prefix spans from system message to multi-modal instruction and the target sequence to learn is response. We choose Vicuna-7B [16] as the base LLM to process the token sequence. In order to preserve the rich knowledge and strong reasoning capability of LLM, we use LoRA [25] to tune the LLM by introducing additional tunable parameters to the original LLM. We optimize the parameters 0 of MSR3D in a prefix language modeling fashion, and the loss function for a batch B of the sequence s is formulated as:\n$$L(\\theta,B) = - \\sum_{b=1}^{B} \\sum_{t=1}^{T} log p(s_{(b,t)} | s_{(b,\\lt t)}, S_{prefix})$$"}, {"title": "C.1.3 Choice of Situation Modeling", "content": "We have detailed tested several situation modeling methods for MSR3D. The methods we have tested are stated as follows:\nas object The agent is treated as a special object in the scene with blank object features, and the situation of the agent is directly encoded using the scene encoder.\nas embedding The location and orientation of the agent are encoded as a special position embedding for all the other objects in the scene. The position embedding is computed by a projection layer form the original location and orientation to the object feature dimension."}, {"title": "C.1.4 Prompt Details", "content": "The first part of the prompt is the system message, which is the same for pre-training and all downstream tasks. It is stated as follows:\nYou are an AI visual assistant situated in a 3D scene. You can perceive (1) an ego-view image (accessible when necessary) and (2) the objects (including yourself) in the scene (always accessible). You should properly respond to the USER's instructions according to the given visual information. The USER's instructions may contain object-level information from images.\nWe use an object-centric 3D scene encoder to encode the 3D scene. We add a common sentence to the beginning of 3D scene tokens, shown as follows:\nObjects (including you) in the scene:\nFor the situation prompt, we add a common sentence before the situation description, shown as follows:\nYou are at a selected location in the 3D scene.\nWhen fine-tuning the model on the MSNN task, the instruction prompt we use is given as follows:\n{GOAL_ACTION}, what action should I take next step?"}, {"title": "C.1.5 Scene Encoder Details", "content": "We employ a frozen PointNet++ [48], pre-trained on the ScanNet [17] dataset with object classification task, to encode the objects present in the scene. For each object, we sample 1024 points, following the approach outlined in [11].\nTo capture the spatial relationships between different objects, we utilize a Spatial Transformer [11] module, a modified transformer architecture that explicitly encodes the spatial relations between object pairs. Specifically, consider the vanilla self-attention mechanism [59], which operates on a feature matrix X \u2208 RN\u00d7d, where N represents the number of tokens and d is the feature dimension. The self-attention mechanism first computes Q = XWQ, K = XWK, and V = XWv from X using learnable projection matrices WQ, WK, Wy \u2208 Rd\u00d7dh, where dh stands for the output feature dimension. Subsequently, the attention weight matrix is calculated as (w\u00bf\u00a1)N\u00d7N = \u03a9\u00b0 = softmax(QK\u1d40) and used to reweight \u03a9\u00b0V.\nThe intuition behind the Spatial Transformer is to rescale the elements wij in the weight matrix \u03a9\u00ba based on spatial information. In the object-centric reasoning setting, the input feature matrix is O \u2208 RN\u00d7d. For an object pair (0\u017c, Oj) with geometric centers ci and cj, the Spatial Transformer [11] computes the Euclidean distance dij = ||Ci - Cj||2 and the horizontal and vertical angles \u03b8\u03b7, \u03b8\u03c5 of the line connecting ci and cj. The spatial feature between the two objects (O\u017c, Oj) is a 5-dimensional vector fij = [dij, sin (0h), cos (0h), sin (\u03b8\u2082), cos (\u03b8)]. To combine this feature with the objects, the spatial attention computes wij = gifij, where gi = W3oz is a 5-dimensional vector. The spatial attention then reweights the original self-attention weight matrix as\n$$w_{ij} = \\frac{\\sigma(w_{ij}) exp(w_{ij})}{\\sum_{l=1}^{N} \\sigma(w_{il}) exp(w_{il})}}$$\nFor more details, readers can refer to [11]. We employ a three-layer Spatial Transformer with 8 heads to process the object-centric features produced by PointNet++ and output object tokens. For other settings, we follow all the default hyperparameters in [11]."}, {"title": "C.2 Zero-shot Models", "content": "C.2.1 Prompts for MSQA\nThe prompts for MSQA using GPT-40 are stated in Fig. 20.\nWe replace all the images with the corresponding class labels for prompting GPT-3.5, and the prompt messages are stated in Fig. 21.\nC.2.2 Prompts for MSNN\nThe prompts for MSNN using GPT-40 are stated in Fig. 22.\nWe also replace all the images with the corresponding class labels for prompting GPT-3.5, and the prompt messages are stated in Fig. 23."}, {"title": "D Additional Experiments and Analysis", "content": "D.1 Scaling Effect on MSNN\nWe also reveal the impact of scaling on the MSNN task by training MSR3D with different MSQA scales. Results are shown in Fig. 24. Observably, the performance improves as the MSQA scale increases, suggesting the effectiveness and scalability of MSQA.\nD.2 More Qualitative Results and Failure Cases in MSQA\nWe provide more qualitative examples and failure cases in Fig. 25 and Fig. 26. The results manifest 1) GPT-40 struggles in spatial reasoning even provided with accurate object coordinates and sizes; 2) current models show insufficient abilities in perception and reasoning when handling the situated reasoning task in MSQA.\nD.3 Additional analyses for situation component\nIn the analyses of situation component in Tab. 2, the difference in Exist. and Spatial is minor. We conjecture these two domains contain many questions that are agnostic to situation. Therefore, we conduct additional experiments on the subsets where question answering is highly dependent on the cues from situation. Specifically, we consider two addtional settings regarding such a hypothesis and present the analyses as follows.\nExist. For Exist., we filter the questions querying in-the-wild objects (e.g., car, elephant) since these questions (e.g., \"Is there a car on my right?\") can be answered without understanding the situation and scene."}]}