{"title": "TEXTDESTROYER: A TRAINING- AND ANNOTATION-FREE DIFFUSION METHOD FOR DESTROYING ANOMAL TEXT FROM IMAGES", "authors": ["Mengcheng Li", "Mingbao Lin", "Fei Chao", "Chia-Wen Lin", "Rongrong Ji"], "abstract": "In this paper, we propose TextDestroyer, the first training- and annotation-free method for scene text destruction using a pre-trained diffusion model. Existing scene text removal models require complex annotation and retraining, and may leave faint yet recognizable text information, compromising privacy protection and content concealment. TextDestroyer addresses these issues by employing a three-stage hierarchical process to obtain accurate text masks. Our method scrambles text areas in the latent start code using a Gaussian distribution before reconstruction. During the diffusion denoising process, self-attention key and value are referenced from the original latent to restore the compromised background. Latent codes saved at each inversion step are used for replacement during reconstruction, ensuring perfect background restoration. The advantages of TextDestroyer include: (1) it eliminates labor-intensive data annotation and resource-intensive training; (2) it achieves more thorough text destruction, preventing recognizable traces; and (3) it demonstrates better generalization capabilities, performing well on both real-world scenes and generated images.", "sections": [{"title": "1 Introduction", "content": "Recently, diffusion models [2\u20139] have made remarkable achievements in text-conditioned image generation, enabling users to effortlessly transform their vivid imaginations into reality. They have shown impressive success in accurately and coherently rendering textual content. In particular, with the use of T5 text encoder component [10], preliminary text generation capabilities have been demonstrated by Imagen [6], eDiff-I [7], and DeepFloyd-IF [8]. Liu et al. [11] further improved text generation by employing character-aware text encoders [12]. To provide more precise guidance in text generation through diffusion, a series of efforts [13-16] have been dedicated to designing specialized network architectures for generating refined text. Stable Diffusion 3 [9] abandons the traditional U-Net architecture in favor of DiT [17] for denoising. In addition to producing awe-inspiring images, it also shows the ability to accurately represent textual information within the images. Easy access to tools that create images from text can lead to problems with copyright, privacy, and the law. Sharing personal information like phone numbers or addresses in images online has already been a big issue, as shown in Fig. 1(a). People try to hide this information by making it blurry or covering it up, but this leads to unintended complications. With more powerful image-making tools, these issues could get worse. Bad people might use these tools to spread false information by putting sensitive details in fake scenes. It's important to think about both the good and bad sides, especially how to stop the spread of unwanted text in images.\nIn line with our motivation, the field of scene text removal has been extensively studied. Before the era of deep learning, numerous efforts had already employed traditional machine learning and computer vision techniques to tackle this task [18-21]. Neural networks, with their powerful learning capabilities, have further enhanced the effectiveness of text erasure. Scene Text Eraser [22] trained an end-to-end CNN, selectively erasing text from images divided into multiple patches. EnsNet [23] features a more sophisticated network structure for erasing both text and other objects. MTRNet [24], using a conditional generative adversarial network (cGAN) with an auxiliary mask, further improved the effectiveness of erasure. PERT [25] embeds a detection branch within the network, providing explicit guidance for erasure. DeepEraser [1] adopts a recursive architecture to gradually remove text over multiple iterations. Nonetheless, as depicted in Fig. 1(b), removal models may excessively focus on background restoration accuracy, potentially leaving subtle residues that enable the text to remain readable in Fig. 1(b). Furthermore, these methods necessitate training, consuming considerable computational resources. Additionally, the training datasets demand labor-intensive manual annotation of masks and ground-truth images post-inpainting. Although STRDD [26] recently used diffusion for text removal, it still requires retraining and data annotation. We recognize that by using a pre-trained diffusion model, the necessity for retraining or data annotation in text destruction can be circumvented. This is due to two factors: first, during the pre-training phase, diffusion models have already been exposed to numerous images, including those with scene text. Second, the diffusion architecture's cross-attention mechanism exhibits rough localization capabilities for textual regions in latent space, providing an alternative to manual mask annotation. Thus, it becomes feasible to explore a training-free and annotation-free text deconstruction method.\nIn this paper, we introduce TextDestroyer, the first training- and annotation-free diffusion method for scene text destruction. We emphasize training-free techniques, automatic text localization, and comprehensive destruction of textual regions. We then employ a hierarchical process for progressive and precise text localization. In the introductory text capturing stage, we aggregate multiple token-level attention maps from the inversion process and segment them to capture an introductory text region mask. In the continuous text adjustment stage, we crop and resize all text regions in the original image and apply the same inversion process to adjust text regions with reduced background interference. In the meticulous text delineation stage, we perform 2-means clustering on the original image, using the non-text areas from the second stage as a reference to distinguish between text and background clusters. With a precise mask of text areas, we destroy their latent codes using random Gaussian noise before reconstructing the image through the diffusion denoising process. Also, we introduce a denoising process to guide image reconstruction, replacing the erroneous latent codes with original ones at each step for low distortion of background. This diffusion process offers key K and value V of non-text areas at specific time steps and self-attention layers for denoising reconstruction, enabling background restoration. To further ensure low distortion in non-text areas, we replace the latent code during the reconstruction when denoising is nearly complete. Finally, we accomplish a complete obliteration of scene text as illustrated in Fig. 1(c).\nThe major contributions of this paper are summarized as:"}, {"title": "2 Related Work", "content": "Scene text removal involves erasing text information from real images and filling the erased areas with content similar to the remaining portions. Early research employed non-learning-based methods [18\u201321] for text erasing, using color-histogram- or threshold-based techniques to locate text regions and similarity-based smoothing methods for inpainting. Recent studies applied deep learning-based methods [1, 22, 23, 25-29] to scene text removal. Several studies [1,24, 26\u201328, 30] maintain separate stages for localization and erasure, while others integrate the erasure into an end-to-end model [22, 23, 25, 29], reducing the difficulty of data collection and training. However, these pipelines and end-to-end models still require training, and their highly specialized structures make it challenging to transfer or extend to other tasks. Diffusion models have been applied [26], serving as a black box, limiting its potential to locate image content."}, {"title": "2.1 Scene Text Removal", "content": "Scene text removal involves erasing text information from real images and filling the erased areas with content similar to the remaining portions. Early research employed non-learning-based methods [18\u201321] for text erasing, using color-histogram- or threshold-based techniques to locate text regions and similarity-based smoothing methods for inpainting. Recent studies applied deep learning-based methods [1, 22, 23, 25-29] to scene text removal. Several studies [1,24, 26\u201328, 30] maintain separate stages for localization and erasure, while others integrate the erasure into an end-to-end model [22, 23, 25, 29], reducing the difficulty of data collection and training. However, these pipelines and end-to-end models still require training, and their highly specialized structures make it challenging to transfer or extend to other tasks. Diffusion models have been applied [26], serving as a black box, limiting its potential to locate image content."}, {"title": "2.2 Diffusion Editing", "content": "Diffusion models [2\u20135, 9] initiate from Gaussian noise and generate images through random denoising steps. DDPM [3] uncovers images from noise during Markovian reverse processes. DDIM [4] denoising on non-Markovian processes reduces sampling steps. LDMs [5] transition to the latent space, operating at a reduced resolution. Several studies [31-36] have leveraged the generative capacity of diffusion to develop real image editing, targeting the removal and replacement of undesirable image components. Some works refine the diffusion model [35, 36] or its trainable counterpart [37] to enhance control over the edited object's structure and texture. However, fine-tuning diffusion models is resource-intensive. Many studies [34] utilize feature map to control shape and texture without tuning. Models based on LDMs employ pre-trained language models like CLIP [38] as text encoders, infusing conditions into the U-Net, thus aligning prompts with image semantics. Liu et al. [11] pointed out that character-blind and capacity-limited diffusion models face challenges in perceiving text regions. Many priors on text perception and editing, such as fine-tuning [15], external components [13], or user-provided masks [13], increase computational demands and impact user experience."}, {"title": "3 Methodology", "content": "In this section, we formally introduced our TextDestroyer method, framework of which is provided in Fig. 2."}, {"title": "3.1 Preliminaries", "content": null}, {"title": "3.1.1 Latent Diffusion Models", "content": "Latent diffusion models (LDMs) employ an autoencoder $E$ that encodes an image $x_0 \\in R^{H\\times W\\times 3}$ into a low-dimensional latent space $z_0 = E(x_0) \\in R^{h\\times w\\times c}$. Here, $f = H/h = W/w$ represents the downsampling factor, and $c$ denotes the channel dimension. The forward diffusion process is defined as:\n$z_t = \\sqrt{\\bar{\\alpha}_t} z_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon, \\epsilon \\sim N(0, I),$ \nwhere ${\\{\\alpha_t\\}}_{t=1}^T$ represents a set of predetermined variance schedules, and $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$. A U-Net $\\epsilon$ serves as a conditional denoiser, which estimates noise incrementally to recover the image's latent representation $z_0$ from the random Gaussian noise $z_T$:\n$z_{t-1} = \\sqrt{\\frac{\\alpha_{t-1}}{\\alpha_t}} z_t + \\Big( \\sqrt{\\frac{1}{\\alpha_{t-1}}} - \\sqrt{\\frac{1}{\\alpha_t}} \\Big) \\epsilon_\\theta (z_t, t, T_\\theta(P)),$"}, {"title": "3.1.2 DDIM Inversion", "content": "Utilizing DDIM sampling [4], a deterministic sampling process can be attained by fixing the variance per Eq. (2). Under the assumption that the ordinary differential equation (ODE) process is reversible with small steps, the DDIM sampling enables an inversion process to facilitate the transition from $z_0$ to $z_t$, which can be formulated by the following equation:\n$z_{t-1}^* = \\sqrt{\\alpha_t} z_t^* + \\sqrt{\\alpha_t} \\Big(\\sqrt{\\frac{1}{\\alpha_{t-1}}} - \\sqrt{\\frac{1}{\\alpha_t}} \\Big) \\epsilon_\\theta (z_{t-1}^*, t-1, T_\\theta(P)).$\nBy initiating the process with $z_t^*$ and continuing the denoising by Eq. (2), we can obtain an approximate $z_0^*$ of the original latent $z_0$. Our major objective is thus to create $z_0^*$ without text information compared to the original $z_0$."}, {"title": "3.2 Hierarchical Text Localization", "content": "We have developed a hierarchical text localization process to accurately identify text regions for destruction, refining outlines incrementally until precise text edges are determined. This process, shown in Fig. 2, consists of three stages. In the first stage, the average cross-attention map generated during the inversion process is sliced to capture an introductory text area $M_1$. In the second stage, each captured text area is cropped and magnified from the original image, to continuously adjust better text areas $M_2$. In the final stage, the original image undergoes a two-means clustering analysis, finally delineating the meticulous text boundaries $M_3$."}, {"title": "3.2.1 Introductory Text Capturing", "content": "Due to the limited latent resolution of stable diffusion, e.g., 64 x 64 for 1.5, encoding the entire image may cause the text region to occupy only a few pixels. Therefore, our introductory objective is to roughly capture the text area.\nWe make $P$ = \"text letter character\" as the conditional prompt during inversion of Eq. (4) to capture the text areas. The \u201ctext\u201d, \u201cletter\u201d and \u201ccharacter\u201d ensure comprehensive coverage of text areas. We compute $M^* = Q^*K^{*T}$ from cross-attention layers during the inversion process, following existing studies [16, 39] to yield a set of token-level attention maps ${\\{M_{token}\\}}$ where $token$ = {\u201ctext\u201d, \u201cletter\u201d, \u201ccharacter\u201d}. Fig. 3 visualizes the attention maps. We also notice the \"end\" attention map $M_{end}$ mostly corresponds to noise in other maps, inspiring us to perform a weighted sum of these attention maps for an aggregated attention map at the first stage as:\n$M_1 = mean( \\sum_{i \\in token} \\gamma \\cdot M_i - M_{end}),$\nwhere $\\gamma$ represents the strengths of the noise-reduced attention maps. The aggregated cross-attention map $M_1$ highlights the text and surrounding areas. To capture the rough text areas, we perform 3-means clustering on $M_1$, selecting the top two categories with higher pixel brightness as the mask areas. This can be expressed as:\n$C_1, C_2, C_3 = 3-means(M_1)$,\n$M_1(i, j) = \\begin{cases} 1 & \\text{if } M_1(i, j) \\in C_1 \\cup C_2, \\\\ 0 & \\text{otherwise}, \\end{cases}$\nwhere $C_1$, $C_2$, and $C_3$ denote the three clustering results, ordered by pixel values from high to low. $M_1$ is a rough mask capturing the text regions as illustrated in Fig. 2. If user provides a mask for fine or selective text destruction, it can be directly treated as $M_1$."}, {"title": "3.2.2 Continuous Text Adjustment", "content": "After obtaining an introductory text region mask, we use a similar process to continuously adjust the mask. We treat each connected component in the mask $M_1$ as an isolated text region and crop $n$ corresponding sub-images, ${\\{x_i^f\\}}_{i=1}^n$, from the original image $x_0$. Following on, we carry out $n$ inversion processes, each further adjusting the text region mask for every cropped image. For the k-th sub-image's inversion process, we compute its aggregated attention map according to Eq. (5) and then resize it to the sub-image shape, denoted as $M_1^k$. To further exclude non-text regions, we handle $M_1^{',k}$ using 2-means:\n$C_1^{2,k}, C_2^{2,k} = 2-means(M_1^{',k})$,\n$M_2^k(i, j) = \\begin{cases} 1 & \\text{if } M_1^{',k}(i, j) \\in C_1^{2,k}, \\\\ 0 & \\text{otherwise}. \\end{cases}$\nAfter $n$ inversion processes, we obtain a set of adjusted masks ${\\{M_{2,1}, M_{2,2}, ..., M_{2,n}\\}}$, removing more non-text regions. Taking the union of all mask regions yields a refined mask $M_2$. To ensure all text is within the mask, we perform a dilation operation on $M_2$:\n$M_2 = dilation(M_2, (k_1, k_1))$,\nwhere $(k_1, k_1)$ denotes the kernel size. Fig. 2 visualizes $M_2$ which further filters out most non-text regions of $M_1$."}, {"title": "3.2.3 Meticulous Text Delineation", "content": "In our continued efforts to meticulously delineate the details of text edges from the image, we adopt a specific approach. For each sub-image $x_i^f$, we engage in 2-means clustering, allowing us to segregate the image into two distinct classes. It's worth noting that during this process, we do not have prior knowledge of which segment represents text and which constitutes the background:\n$C_1^{3,k}, C_2^{3,k} = 2-means(x_i^f)$,\n$M_{tmp}^k(i, j) = \\begin{cases} 1 & \\text{if } x_i^f(i, j) \\in C_1^{3,k}, \\\\ 0 & \\text{otherwise}. \\end{cases}$\nLet $R = M_1 - M_2$ define the representation of non-text regions that have been filtered through the second stage. By closely examining the distribution ratios of the two clusters within areas that were previously identified as non-text, we can effectively discern and pinpoint the text regions:\n$M_{3,k} = \\begin{cases} M_{tmp}^k & \\text{if } sum(R \\cdot M_{tmp}^k) \\le sum(R \\cdot M_{tmp}^k), \\\\ M_{tmp}^k & \\text{otherwise}. \\end{cases}$\nBy intersecting $M_1$ with the union of all clusters that represent text regions, we derive the final text mask $M_3$:\n$M_3 = M_1 \\cdot \\sum_{k=1}^n M_{3,k}$"}, {"title": "3.3 Text Region Destruction", "content": "Although the latent start code $z_t^*$ generated by inversion of Eq. (4) adheres to a Gaussian distribution, it is not entirely stochastic, as deterministic inference processes govern this aspect. The text regions are no exception in this respect. Consequently, we must destroy the original latent code of the text regions to prevent their recovery during the denoising process. In order to achieve this, we put forward a strategy that involves filling the latent code within the $M_3$ region with fresh random Gaussian noise.\nThe characteristics of this noise, specifically its mean $\\mu$ and variance $\\sigma$, are not arbitrarily chosen. Instead, they are calculated based on the entire latent. This approach ensures that the noise introduced aligns with the overall distribution of the latent code, maintaining the integrity of the data while still disrupting the original latent code of the text regions. This method is detailed as follows:\n$\\mu = \\frac{1}{h w} \\sum_{i=1}^h \\sum_{j=1}^w z_t^*(i, j)$, $\\sigma = \\frac{1}{h w} \\sum_{i=1}^h \\sum_{j=1}^w (z_t^*(i, j) - \\mu)^2,$\n$z_t^{'} =(1-M_3) \\cdot z_t^* + M_3 \\cdot \\epsilon, \\epsilon \\sim N(\\mu \\cdot I, \\sigma \\cdot I),$\nwhere the newly $z_t^{'}$ denotes the destroyed latent code."}, {"title": "3.4 Non-Text Region Restoration", "content": "In denoising $z_t^{'}$, we also expect to well restore the non-text region. Inspired by Cao et al. [34], we recognize the role of the key K and value V in providing robust guidance for the structure and texture of specific objects. We employ a process where we extract $K^*$ and $V^*$ from the self-attention layer of the denoising procedure for the source image $z_0$. We then inject these into $K'$ and $V'$ during denoising $z_t^{'}$ in accordance with the mask $M_3$, ensuring well object restoration:\n$K' = K^* \\cdot (1 - M_3) + K' \\cdot M_3,$\n$V' = V^* \\cdot (1 - M_3) + V' \\cdot M_3.$\nTo avoid reintroducing text into the background reconstruction, we limit this operation to specific denoising steps and self-attention layers. As manifested in Fig. 2, we use the KV combination from $t = 45 \\rightarrow 0$ steps at one self-attention layer of the U-Net's front end and two layers at the back end.\nTo mitigate the errors between the reconstructed image and the original one, we save the latent at each inversion step and replace it during source image reconstruction at each denoising step. We also use latent code replacement for explicit background restoration. At the $t = 2$ step, we reintroduce the source latent of non-text regions into the background for the matching with the original image:\n$M_3^{'} = dilation(M_3, (k_2, k_2)),$\n$z_2 = z_2 \\cdot (1 - M_3^{'}) + z_2^* \\cdot M_3^{'}.$"}, {"title": "4 Experimentation", "content": null}, {"title": "4.1 Experimental Setups", "content": "We use the pre-trained stable diffusion 1.5 model [40] to validate our TextDestroyer approach. During both the denoising and inversion processes, we employ the DDIM sampling strategy [4], encompassing 50 steps. In the attention aggregation phase, we set $\\gamma$ to 1.5. For mask dilation, we designate the kernel sizes as $k_1 = 5$ and $k_2 = 9$. When a user-provided mask is available, we not only replace it with $M_1$, but also forgo the use of dilated $M_3$, opting instead to employ it in identifying the areas that require restoration during the latent code replacement process."}, {"title": "4.4 Ablation Studies", "content": "In this section, we conduct extensive ablation experiments to examine the characteristics of our model components, including hierarchical text localization in Sec. 3.2, KV combination in Eq. (13) and latent code replacement in Eq. (14)."}, {"title": "4.4.1 Steps and Layers for KV Combination", "content": "Subsequently, we conduct ablation studies on the layers where the KV combination is employed. To achieve a more pronounced effect, during the visual ablation process, we do not replace the latent code for the diffusion providing KV, but in the quantitative ablation, we maintain the standard settings. As depicted in Fig. 7, an excessive number of KV combination layers leads to the re-emergence of textual artifacts within the generated images. Insufficient KV comparison layers result in the retention of areas disrupted by random noise, thereby compromising the visual fidelity. When the KV combination layers are fixed and positioned closer to the middle, owing to limitations in resolution, their capacity to restore background textures diminishes, consequently favoring the generation of smoother, blurred textures.\nWe have also implemented ablation on the step where the KV combination is employed. Similar to the ablation on KV combination self-attention layers, an excessive number of KV combination steps leads to text reappearance, while too few result in background disorder. In Fig. 8, during this process of change, there might not exist a perfect point where the background is completely restored while also not introducing information from the text area.\nAs shown in the second data block of Table 2, the phenomena mentioned above are also validated in quantitative results: a lack of KV combination may lead to a chaotic background, such that although text-like distributions cannot be detected"}, {"title": "4.4.2 Stages of Hierarchical Text Localization", "content": "To optimize the precision of token localization within the diffusion process, we have meticulously crafted a hierarchical text localization strategy that unfolds in as three-tiered sequence. Since masks from three stages have different optimal parameter choices when applied individually in subsequent steps, especially the size of dilation kernels, isolating a mask from any single stage results in biased metrics. Therefore, we only provide an intuitive display on the visual effects.\nAs delineated in Fig. 9, there is a progressive refinement in the accuracy of the mask as the process nears completion. Concurrently, the image that emerges above exhibits enhanced visual fidelity and plausibility. The mask encapsulating the textual region is progressively refined, ensuring that the background is effectively excluded while meticulously preserving the integrity of the text area, ultimately converging to the text's edge. Employing the comprehensive three-stage text localization technique yields images that are visually superior and more convincingly devoid of textual artifacts."}, {"title": "4.4.3 Step for Latent Code Replacement", "content": "The objective of latent code replacement is to rectify the damage incurred by the background during the process of approximate inversion and the ensuing disrupted reconstruction. As depicted in Fig. 10 and the third block of Table 2, the premature introduction of the source latent code into the denoising reconstruction process leads to a compromised restoration efficacy. On the other hand, the delayed introduction of the latent code may give rise to a perceptible discontinuity at the boundary between the eradicated text area and the surrounding background. In light of the delicate balance that must be struck between the thorough restoration of the background and the seamless integration of the latent code, we have strategically selected to execute the latent code replacement at the time step denoted as $t = 2$. This decision is informed by a nuanced consideration of both the quality of background restoration and the continuity of the image's composition."}, {"title": "4.5 Limitations and Discussions", "content": "Our TextDestroyer faces certain challenges: (1) the refinement in restoring background regions is lacking, occasionally leading to inaccuracies in color, texture, and structure; (2) it struggles with curved and small text due to limitations of the pre-trained model; (3) the inference process is time-consuming, taking approximately 25 to 60 seconds per image on a single 3090 GPU. These limitations imply that our future research could focus on using pre-trained models with enhanced text detection capabilities to streamline text localization and thus expedite inference. Moreover, improving the robustness of text localization and the quality of background restoration are areas that merit further exploration."}, {"title": "5 Conclusion", "content": "In this study, we have presented TextDestroyer, a novel model for destroying scene text without training or annotation. Using a pre-trained diffusion model, TextDestroyer achieves hierarchical text localization and introduces random noise to disrupt text distribution. The denoising phase employs KV combination and latent code replacement for background restoration. Our approach differs from others by fully destroying text distribution before reconstructing the background, avoiding residual traces. Experiments show our proposed TextDestroyer excels at complete text removal and provides enhanced generalization for varied inputs, creating realistic backgrounds rather than just smoothing."}]}