{"title": "Single-Channel EEG Tokenization Through Time-Frequency Modeling", "authors": ["Jathurshan Pradeepkumar", "Xihao Piao", "Zheng Chen", "Jimeng Sun"], "abstract": "We introduce TFM-Tokenizer, a novel tokenization framework tailored for EEG analysis that transforms continuous, noisy brain signals into a sequence of discrete, well-represented tokens for various EEG tasks. Conventional approaches typically rely on continuous embeddings and inter-channel dependencies, which are limited in capturing inherent EEG features such as temporally unpredictable patterns and diverse oscillatory waveforms. In contrast, we hypothesize that critical time-frequency features can be effectively captured from a single channel. By learning tokens that encapsulate these intrinsic patterns within a single channel, our approach yields a scalable tokenizer adaptable across diverse EEG settings. We integrate the TFM-Tokenizer with a transformer-based TFM-Encoder, leveraging established pretraining techniques from natural language processing-such as masked token prediction-followed by downstream fine-tuning for various EEG tasks. Experiments across four EEG datasets show that TFM-Token outperforms state-of-the-art methods. On TUEV, our approach improves balanced accuracy and Cohen's Kappa by 5% over baselines. Comprehensive analysis of the learned tokens demonstrates their ability to capture class-distinctive features, enhance frequency representation, and ability to encode time-frequency motifs into distinct tokens, improving interpretability.", "sections": [{"title": "1 INTRODUCTION", "content": "Electroencephalograms (EEGs) reveal real-time neuronal activity with millisecond precision, reflecting the responses to various event stimuli. This makes EEGs essential not only for fundamental research [7, 23] but also for diverse applications such as sleep staging [32, 35, 54], neurological disease detection [2, 21], and brain-computer interfaces [25, 36, 48]. Practical EEG analysis is a complex, multi-step process involving filtering, feature engineering, pattern recognition, annotation, etc. Deep learning (DL) models have shown remarkable success in automating EEG analysis across various tasks [5, 38, 41]. These achievements stem from their representation learning capability, projecting noisy EEGs onto a discriminative feature space that captures associations with discrete neurophysiological events. Recent neuroscientific literature reports that DL methods successfully match the reliability and accuracy of 20 clinical experts in identifying seizure-related events [18].\nDespite their success, effectively representing EEGs remains a primary focus in the research community. Real-world EEG recordings are acquired under diverse scenarios, using different devices, and even for the same task, data formats often vary due to mismatched channels and variable lengths [53]. Unfortunately, most existing methods typically learn representations on a case-by-case basis with specific architectures or fixed channel settings. These methods exhibit limited generalization across tasks and poor scalability to different data formats. For example, a sleep apnea model is not readily applicable to a sleep staging task, since the former aims to capture anomaly patterns while the latter is to learn discriminative representations [35]. Likewise, a sleep staging model trained on central region EEGs cannot directly generalize to frontal EEG data. As a result, retraining, elaborate fine-tuning, or model adjustments are often required [53], escalating practical costs and further restricting applicability to advanced devices or non-standard data, such as ECOG [34]. There is thus an urgent need to develop an EEG analysis method that serves broader research objectives.\nRecently, the transformative impact of large foundation models[1, 42] has inspired and elevated EEG representation learning to new heights. Several foundation EEG models have been proposed [9, 28, 53, 55], demonstrating both enhanced performance and generalization. Researchers often tokenize EEGs into discrete, short-duration snapshots across different data formats and model their dependencies leveraging foundational yet powerful Transformers. By leveraging the paradigm of pretraining on large-scale, cross-task datasets, these models can learn universal representation, enabling effective adaptation to downstream tasks without requiring task-specific architectures [21, 28]. Thanks to unified tokenization and the automated input adaptation of the Transformer, such methods also address challenges such as complex data format reconfigurations [53] and labeled data scarcity [17]. However, this direction remains nascent, and several limitations remain:\n\u2022 Inappropriate Tokenization Representation. One reason large language models (LLMs) succeed is their effective tokenization and similar benefits have been demonstrated in image [43] and video [3, 45] tokenization. However, existing foundation EEG models generally do not adopt a discrete tokenization paradigm. Although some methods claim to provide an EEG \"tokenizer,\" they typically lack a discrete approach similar to NLP. For instance, LaBraM [17] employs a neural tokenizer only during pretraining but relies on raw EEG signals during inference. BIOT [53] suggests a \"biosignal tokenizer\" that remains continuous rather than discretized. Although time-series tokenization methods have shown promise [4, 37], they do not scale well to EEG's higher sampling rates and other artifacts.\n\u2022 Insufficient Frequency Representation. Capturing eventful EEG features, which are characterized by distinct frequencies, is a primary focus of EEG analysis. However, tokenizing and modeling raw EEGs often lead to a loss of frequency diversity. This frequency representation collapse is a common issue in time-series modeling, as low-frequency components typically dominate the EEG data, biasing models toward lower frequencies while overlooking critical high-frequency features, such as spikes. Although LaBraM has recently incorporated frequency reconstruction into its objective function, it still relies on raw EEG signals, inevitably resulting in sub-optimal representation, as shown in Figure 1.\n\u2022 Scalability and Generalization. EEG-related tasks vary significantly in channel configurations. For example, seizure detection typically utilizes 16 channels, whereas sleep studies often require only 1-2 channels. However, existing models are primarily designed for multi-channel settings, heavily relying on cross-channel prediction. This design limits their scalability and adaptability to configurations with fewer or even single channels, as well as to varying acquisition setups.\nTherefore, in this paper, we propose TFM-Token, an effective, fully discretized EEG tokenization framework that captures time-frequency motifs from single-channel EEG signals into distinct tokens. Technically, our contributions are as follows:\n\u2022 TFM-Tokenizer and TFM-Encoder: We introduce a scalable discrete tokenization framework for EEG, transforming single-channel signals into discrete token sequences akin to NLP models. TFM-Tokenizer converts EEG into discrete tokens, and TFM-Encoder utilizes them for downstream tasks.\n\u2022 Joint Modeling of Frequency and Temporal Dynamics: Our tokenizer integrates raw EEG patches with time-frequency representations, using frequency band and temporal masking to capture essential frequency patterns while disentangling temporal variations (Figure 1).\n\u2022 Scable tokenization: Our single-channel approach enables flexible adaptation across EEG tasks and channel configurations. TFM-Tokenizer further enhances state-of-the-art EEG models, such as LaBraM [17].\n\u2022 Empirical Validation and Token Quality Analysis: We evaluate our framework on four EEG downstream tasks, demonstrating state-of-the-art performance. Beyond performance, we comprehensively analyze token quality, including token visualization, class-specific uniqueness, and frequency learning analysis, validating that our learned tokens are informative and interpretable."}, {"title": "2 RELATED WORK", "content": "EEG Representation Learning. To learn general representations and address issues of label scarcity in EEG data, self-supervised learning (SSL) has emerged as a prominent paradigm, and existing works can be categorized into two main approaches: contrastive learning and self-prediction. Contrastive learning methods, including TS-TCC [10], TF-C [57] and SplitSEE [21], leverage augmentation or transformation of EEG inputs to learn consistent representations. In contrast, self-prediction methods, namely BENDR [20], MAEEG [6], BIOT [53] and EEG2REP [28], aim to accurately reconstruct masked or corrupted input. However, their learning objectives heavily rely on cross-channel prediction to focus on spatial characteristics. In contrast, our method emphasizes inherent time-frequency features within a single-channel setting and can adapt to any channel configuration.\nFoundation EEG Models. Inspired by the success of foundation models in NLP, recent efforts have sought to develop foundation models for EEG analysis. These models can be categorized into decoding and encoder-based methods. Decoding-only methods focus on generative tasks like EEG-to-text translation, with representative works including DeWave [9], EEG2Text [26], and E2T-PTR [46]. In contrast, encoder-only methods concentrate on fundamental EEG classification tasks and representation learning. Notable models include LaBraM [17], BIOT [53], BRANT [56], and MMM [55]. Our work aligns with this latter category, focusing on enhancing the representation quality to improve classification performance.\nEEG Tokenization. Tokenization has been instrumental in NLP, where discrete subword units have proven to reduce data complexity and improve model performance and interoperability. Existing attempts for EEGs include patch-based continuous tokenization, such as BIOT [53] and BRANT [56], and vector quantization (VQ)-based methods like DeWave [9]. Patch-based methods do not involve encoding or quantization, leading to unbounded and continuous representations that lack distinctiveness and interpretability. In contrast, VQ-based tokenizers, traditionally successful in tokenizing continuous images [12], have recently been adapted for EEG by LaBraM [17], However, its tokenizer is utilized only during training and not during inference. Conceptually, its primary role is to pre-train classification layers, rather than encoding inputs and reducing data complexity. Here, our method is explicitly VQ-based, treating the codebook as a real tokenizer for EEG data. Moreover, we enforce each token to capture time-frequency motifs [52] in EEG inputs, ensuring a more structured and interpretable representation.\nFrequency Representation Collapse. Frequency domain analysis is crucial in EEG and general time series analysis [11, 49-51]. In real-world signals, time-domain observations inherently mix multiple frequency components, and high-energy, low-frequency signals often dominate the spectrum [16, 22]. As a result, these entangled frequency features makes it difficult for models to distinguish between them [33, 59]. Recent studies have shown that these entangled signals can lead to a collapse in the learned frequency representations [33, 58]. Models tend to overemphasize the dominant low-frequency features while neglecting the high-frequency details. This issue can lead to a lack of capturing various EEG waveforms and degenerating data representation [30]. Motivated by these works, our paper focuses on developing methods to learn diverse, informative frequency features. In Section 5.3, we provide an analysis of our proposed frequency-domain tokenizer and its impact on model performance."}, {"title": "3 PRELIMINARY", "content": "3.1 Notations and Problem Statements\nEEG Data. Let \\(X \\in \\mathbb{R}^{C \\times T}\\) denote a multi-channel EEG recording with C channels and T time samples. For each channel \\(c \\in \\{1, ..., C\\}\\), we denote a single channel EEG by \\(x \\in \\mathbb{R}^{T}\\). To capture complementary structures in both the time and frequency domains, we decompose x into: (1) time-frequency representation S and (2) raw EEG patches \\(\\left\\{x_{i}\\right\\}_{i=1}^{N}\\). For simplicity, we omit the channel index; henceforth, x will refer to a single-channel EEG signal unless otherwise specified.\nShort-Time Fourier Transform (STFT). To obtain the time-frequency representation, i.e.g, spectrogram, S, we apply a STFT to x using a windowing function w(.) of length L and a hop size H:\n\\(S(\\omega, \\tau) = \\sum_{l=0}^{L-1} x(\\tau H + l)w(l)e^{-j2\\pi\\omega l}\\)\nwhere \\(\\omega\\) indexes the discrete frequencies and \\(\\tau\\) indexes the time segments (i.e., time windows shifted by H). We retain only the magnitude \\(|.|\\) to form \\(S \\in \\mathbb{R}^{F \\times N}\\), where F is the number of frequency bins and N is the number of time windows.\nProblem Statement 1 (EEG Tokenization): Given a single channel EEG x, we aim to learn a tokenization function\n\\(f_{\\text{tokenizer}} : \\mathbb{R}^{T} \\rightarrow V^{N \\times D}\\)\nwhere V is a finite EEG token vocabulary of size k, and D is the dimension of each token embedding. The tokenizer function \\(f_{\\text{tokenizer}}\\) should project x (or transformations) into a sequence of discrete tokens \\(\\left\\{v_{i}\\right\\}_{i=1}^{N}\\), where each \\(v_{i} \\in V\\). These tokens represent various temporal and frequency \"motifs\": meaningful EEG patterns characterized by distinct temporal and frequency characteristics. Therefore, V is learnbale from S and the temporal patches \\(\\left\\{x_{i}\\right\\}_{i=1}^{N}\\).\nRemark. We here hold several expectations for the learned motif tokens. First, these tokens are expected to reduce redundancy, noise, and complexity, providing a compact, sparse, and informative representation of EEGs. Second, these motifs should effectively capture essential neurophysiological patterns from both temporal and frequency domains. Third, the tokens should generalize well across different EEG tasks, enhancing the efficiency and interpretability of the data. We set up related research questions to evaluate these expectations in Section. 5.\nProblem Statement 2 (Multi-Channel EEG Classification): Given EEGs X and a fixed, learned single-channel tokenizer \\(f_{\\text{tokenizer}}\\), we apply \\(f_{\\text{tokenizer}}\\) independently to each channel c to obtain a tokenization representation \\(\\left\\{\\left\\{v_{i}^{c}\\right\\}_{i=1}^{N}\\right\\}_{c=1}^{C}\\). Then, these tokens can serve various downstream tasks. For classification tasks, they are mapped to labels by:\n\\(f_{\\text{classifier}}: (V^{D})^{N \\times C} \\rightarrow Y\\)\nwhere Y is the target labels (e.g., EEG events, seizure types). By aggregating and processing the tokens across all channels, \\(f_{\\text{classifier}}\\) predicts a label \\(y \\in Y\\). Notably, \\(f_{\\text{classifier}}\\) can be any downstream-oriented model, and its training is performed separately from the EEG tokenizer \\(f_{\\text{tokenizer}}\\)."}, {"title": "4 METHODOLOGY", "content": "TFM-Token comprises two components: (1) TFM-Tokenizer: addresses problem statement 1 by converting continuous EEG signals into discrete tokens, capturing key time-frequency features, and (2) TFM-Encoder: tackles problem statement 2 by leveraging these tokens for downstream EEG tasks, such as classification. To mitigate the quadratic complexity of standard Transformers [44], we employ a linear attention mechanism [19, 47] across all transformer modules. Given input EEGs, TFM-Tokenizer independently tokenizes each channel into discrete token sequences. The TFM-Encoder then processes these tokens to perform classification. For training TFM-Token, we first conduct an unsupervised pre-training of TFM-Tokenizer in a single-channel setting to encode time-frequency motifs into discrete tokens (see Figure 2a, Sec 4.1). The tokenizer is then frozen, and TFM-Encoder undergoes masked token prediction pretraining (see Figure 2b, Sec 4.2), followed by fine-tuning for downstream EEG tasks.\nTime-Frequency discretization. The input to TFM-Token consists of discrete time and frequency patches, applied to both raw EEG data and its corresponding spectrogram. This approach addresses the transient nature of EEG waveforms, which may decompose the entangled patterns and is foundational for motif learning. Specifically, we segment x into time-domain length patches L with the same hop size of H. This yields a sequence of patches \\(\\left\\{x_{i}\\right\\}_{i=1}^{N}\\), where \\(N = \\lfloor (T-L)/H \\rfloor + 1\\) is the total number of patches. Using the same L and H for both STFT and temporal segmentation, we ensure that each patch \\(x_{i}\\) corresponds one-to-one with a spectral window \\(S_{i}\\). In practice, we obtain the raw EEG patches by configuring the convolutional layers with a kernel size and stride that match the STFT window length L and hop size H. Additionally, we denote embeddings and masks as E and M, respectively."}, {"title": "4.1 Single Channel TFM-Tokenizer", "content": "We introduce the TFM-Tokenizer, a scalable module for tokenizing single-channel EEG signals x by effectively capturing both their temporal and frequency dynamics. Our design is inspired by the Vector-Quantized Variational Autoencoder (VQ-VAE) [43], which has been widely adopted for tokenization efforts in other domains such as video processing [3]. As illustrated in Figure 2a, the TFM-Tokenizer comprises three primary components: (1) Localized Spectral Window Encoder, (2) Temporal Encoder, and (3) Temporal Transformer. At a high level, TFM-Tokenizer adopts a frequency-then-time paradigm. First, we isolate each spectral window \\(S_{i}\\) (for \\(i = 1, ..., N\\)) to capture intra-frequency band dependencies, yielding an aggregated representation for each window (Localized Spectral Window Encoder, see Figure 2d). Next, these frequency-based representations are combined with features extracted from the corresponding raw EEG patches \\(\\left\\{x_{i}\\right\\}_{i=1}^{N}\\) (Temporal Encoder). Finally, the combined features are passed through the Temporal Transformer to capture long-range temporal dependencies, and the resulting embeddings are quantized into discrete tokens.\nLocalized Spectral Window Encoder. Capturing frequency-band characteristics or compositions is crucial for EEG analysis, as the data often contains oscillatory components (e.g., alpha, beta) with varying amplitudes and time. Unlike previous studies that project an entire spectral window with a single linear layer [53], we propose to patch along the frequency axis, enabling better modeling of cross-frequency dependencies. In detail, this encoder consists of three main steps (see Figure 2d):\n\u2022 Frequency Patch Encoder. Given a set of spectral windows \\(\\left\\{S_{i}\\right\\}_{i=1}^{N}\\), we isolate and divide each spectral window \\(S_{i}\\) into P non-overlapping patches \\(\\left\\{S_{(i,p)}\\right\\}_{p=1}^{P}\\), each spanning \\(\\Delta f\\) frequency bins such that \\(P \\cdot \\Delta f = F\\). We then project each frequency patch into a latent space:\n\\(e_{(i,p)} = \\text{GroupNorm} \\left(\\text{GeLU} \\left(W_{p}S_{(i,p)}\\right)\\right)\\)\nwhere \\(W_{p} \\in \\mathbb{R}^{D \\times \\Delta f}\\) is a learnable matrix.\n\u2022 Frequency Transformer. In order to capture the interactions among the frequency patch embeddings \\(e_{(i,p)}\\), we apply a frequency transformer that operates along the frequency axis within each spectral window \\(S_{i}\\). This enables the capturing of intra-frequency band dependencies (e.g., interactions between low and high-frequency bands).\n\u2022 Gated Patchwise Aggregation: In many EEG scenarios, large portions of the frequency spectrum can be irrelevant. For instance, tasks related to sleep primarily focus on frequency bands up to approximately 32 Hz [5]. Also, the frequencies of interest vary across conditions and tasks. To emphasize important frequency patches and suppress the rest, we adopt a gated aggregation mechanism. Formally, we first group frequency patch embeddings, then apply gated weighting before concatenating them into a single embedding \\(E_{i}\\):\n\\(E_{i} = \\text{Concat} \\left[\\sigma\\left(W_{g1}e_{(i,p)}\\right) \\oplus W_{g2}e_{(i,p)}\\right]\\)\nwhere \\(W_{g1}\\), \\(W_{g2}\\) are trainable parameters and \\(\\sigma(\\cdot)\\) is the element-wise sigmoid function.\nTemporal Encoder. To capture temporal dynamics from the raw EEG patches \\(\\left\\{x_{i}\\right\\}_{i=1}^{N}\\), we perform a linear projection of the patch followed by GELU[15] activation and group normalization, producing temporal embeddings \\(\\left\\{E_{i}^{T}\\right\\}_{i=1}^{N}\\).\nTemporal Transformer. We form a unified time-frequency embedding by combining each aggregated frequency embedding \\(E_{i}\\) with its corresponding temporal embedding \\(E_{i}^{T}\\) and feed the resulting vectors into a temporal transformer. This module attends jointly to time and frequency features across all windows, capturing long-range dependencies. The output is then quantized into discrete tokens \\(\\left\\{v_{i}\\right\\}_{i=1}^{N}\\) using a learnable codebook \\(V^{k}\\). Notably, we do not add any positional encoding as EEG signals are inherently non-stationary and often exhibit chaotic behavior; thus, our goal is to capture their distinctive features without enforcing positional constraints. We further provide an ablation study (Appendix A.3) showing the effect of omitting positional encodings.\nTokenizer Codebook. Our tokenizer is designed to capture temporal frequency motifs by applying vector quantization along the time axis, that is, individual patches. This is different from conventional quantization methods in computer vision, which typically operate on the embedding dimension of a patch [12, 43]. As a result, each token in our vocabulary directly represents a waveform within a specific short duration and can facilitate the retrieval of the timestamp of input EEGs to understand the composition of an EEG. This enhances interpretability; for instance, specific codes in the vocabulary are activated in response to the occurrence of particular temporal or frequency patterns. To validate this, we conducted a visual inspection study, with results presented in Section 5.5.\nFrequency Making Prediction for Tokenizer Learning. The input to the tokenizer is the full frequency band of a temporal patch. To facilitate effective frequency learning, we employ a frequency-band and temporal masking strategy during TFM-Tokenizer training. We partition S into \\(N_{F} = \\lfloor\\frac{F}{\\Delta f}\\rfloor\\) frequency groups of size \\(\\Delta f\\) and randomly mask a subset based on a predefined mask ratio (e.g., 0.5), forming a frequency-band mask \\(M_{F}\\). Simultaneously, a temporal mask \\(M_{T}\\) is applied along the time axis. The final mask, \\(M = M_{F} \\cdot M_{T}\\), is used to mask the STFT representation, \\(S_{M} = M \\odot S\\), where \\(\\odot\\) denotes element-wise multiplication. For data augmentation and efficient training, we adopt a symmetric masking approach from [17] by defining \\(M_{sym} = 1 - M\\). Given \\(S_{M}\\) and x, our TFM-Tokenizer first quantized them into a sequence of distinct tokens, which is then passed through a transformer decoder followed by a linear decoder that predicts the masked regions. The tokenizer is trained using a masked reconstruction loss:\n\\(L_{\\text{rec}} = \\sum_{(f,t) \\in I_{\\text{mask}}} ||S(f, t) - \\hat{S}(f, t)||^{2}\\)\nwhere \\((f, t) \\in I_{\\text{mask}}\\) represents masked frequency-time indices and \\(\\hat{S}\\) is the reconstruction of S. Alongside the masked reconstruction loss, we incorporate the codebook and commitment losses [43], yielding the total loss:\n\\(L_{\\text{token}} = L_{\\text{recon}} + \\alpha \\sum_{i} ||\\text{sg}[E_{i}] - v_{i}||^{2} + \\beta \\sum_{i} ||E_{i} - \\text{sg}[v_{i}]||^{2}\\)\nwhere, sg[] is the stop-gradient operator, and \\(\\alpha, \\beta\\) are hyperparameters. We also employ exponential moving average strategy [43] for stable codebook updates."}, {"title": "4.2 Token-Wise TFM-Encoder", "content": "The TFM-Encoder is designed to aggregate tokenized representations across channels and perform downstream classification. It consists of two main components: (1) a token-embedding lookup table and (2) linear attention transformer layers. Given a multi-channel recording \\(X \\in \\mathbb{R}^{C \\times T}\\), we apply the fixed (pretrained) TFM-Tokenizer to each channel independently, obtaining discrete token sequences \\(\\left\\{\\left\\{v_{i}\\right\\}_{i=1}^{N}\\right\\}_{c=1}^{C}\\). Each token is mapped to its embedding via a lookup table initialized with the TFM-Tokenizer's learned codebook weights. We flatten the embeddings across channels and incorporate channel and position embeddings to encode respective information. We then prepend a [CLS] token [8], the output of which is designed to capture the overall representation of the sequence for downstream tasks. The resulting embeddings are then processed by linear attention transformers in TFM-Encoder. Token Prediction for TFM-Encoder Learning. We adopt a strategy akin to masked language modeling. We randomly mask tokens across multiple channels and time steps and train the model to predict these masked tokens via a cross-entropy loss. Along with representation learning, this approach enhances robustness to missing or corrupted data, common in real-world EEG systems where channels or time segments may be dropped or noisy. Finally, the TFM-Encoder is finetuned to downstream tasks."}, {"title": "5 EXPERIMENTS AND RESULTS", "content": "In this section, we perform comprehensive experiments to evaluate our proposed method to answer the following research questions:\n\u2022 Q1: Can a fully discretized EEG tokenization framework outperform baseline methods that rely on continuous embeddings? How crucial is joint frequency-temporal modeling in EEG analysis and tokenization?\n\u2022 Q2: How effectively do our learned tokens represent EEG features and capture class-specific characteristics? Do the generated tokens capture relevant frequency information?\n\u2022 Q3: Are the tokens generated by our TFM-Tokenizer scalable, and can they enhance the performance of existing EEG foundation models such as LaBraM?\n\u2022 Q4: Do tokens produced by TFM-Tokenizer offer interpretability, and do they correspond to distinct, recognizable EEG patterns?"}, {"title": "5.1 Experiment Setup", "content": "5.1.1 Datasets: We conducted our evaluation on four EEG datasets:\n\u2022 TUH EEG Events (TUEV) [14]: TUEV is a subset of the TUH EEG Corpus [29], which comprises clinical EEG recordings collected at Temple University Hospital between 2002 and 2017. The dataset is annotated for six EEG event types: spike and sharp wave (SPSW), generalized periodic epileptiform discharges (GPED), periodic lateralized epileptiform discharges (PLED), eye movement (EYEM), artifact (ARTF), and background (BCKG).\n\u2022 TUH Abnormal EEG Corpus (TUAB) [27]: TUAB comprises EEG recordings collected at Temple University Hospital, which are labeled for normal and abnormal EEG activity.\n\u2022 IIIC Seizure [13, 18]: The IIIC Seizure dataset is curated for the detection of six distinct ictal-interictal-injury continuum (IIIC) patterns and is sourced from [13, 18]. The annotations include: (1) others (OTH), (2) seizure types (ESZ), (3) lateralized periodic discharge (LPD), (4) generalized periodic discharge (GPD), (5) lateralized rhythmic delta activity (LRDA), and (6) generalized rhythmic delta activity (GRDA).\n\u2022 CHB-MIT [39]: The CHB-MIT dataset is a widely used benchmark for epilepsy seizure detection. It comprises EEG recordings from 23 pediatric subjects with intractable seizures.\n5.1.2 Preprocessing: We follow the preprocessing setup of BIOT [53]. Unlike LaBraM [17], which utilized 23 channels in the TUEV and TUAB datasets, we adhere to the 16-channel bipolar montage from the international 10-20 system, as used in [53]. All EEG recordings are resampled to 200 Hz. For TUEV and TUAB, we apply a bandpass filter (0.1-75 Hz) and a notch filter (50 Hz), following the preprocessing pipeline of LaBraM [17]. STFT computation of the signals is performed using PyTorch, with detailed parameters provided in Appendix C.2. For training, validation, and test splits, we follow the recommendations from [53]. Additional details on dataset statistics and splits are provided in Appendix C.1.\n5.1.3 Baselines and Metrics: We evaluated our approach against the baselines from [53] as well as the current state-of-the-art methods, including BIOT [53] and LaBraM [17]. All baselines were reproduced using their respective open-source GitHub repositories. To ensure a fair comparison, our experiments follow a single-dataset setting for all the baselines. Specifically for BIOT, we conducted their proposed unsupervised pretraining followed by fine-tuning on the same dataset. Similarly, for LaBraM, we used their base model and conducted neural tokenizer training, masked EEG modeling, and fine-tuning within the same dataset. For performance evaluation, we used balanced accuracy, Cohen's Kappa coefficient, and weighted-F1 score for multi-class classification tasks, while balanced accuracy, AUC-PR, and AUROC were used for binary classification tasks. For TUAB, we used binary cross-entropy loss for fine-tuning, while the cross-entropy loss was applied to the TUEV and IIIC datasets. Given the class imbalance in the CHB-MIT dataset, we employed focal loss for all experiments. All experiments were conducted using five different random seeds, and we report the mean and standard deviation for each metric. Appendix C provides additional details on the experiment settings."}, {"title": "5.2 Q1: Performance and Importance of Joint Frequency-Temporal Modeling", "content": "Q1.1 - Performance Evaluation: presents EEG event classification results on TUEV and abnormal detection performance on TUAB. Our TFM-Token consistently outperforms all baselines across all metrics. For event-type classification on TUEV, TFM-Token achieves a 5% increase in balanced accuracy over BIOT (0.4679 \u2192 0.4943) and LaBraM (0.4682 \u2192 0.4943). In Cohen's Kappa, TFM-Token improves by 9% over BIOT (0.4890 \u2192 0.5337) and 5% over LaBraM (0.5067 \u2192 0.5337). In abnormal detection on TUAB, TFM-Token achieves a 5% improvement across all metrics compared to LaBraM. This highlights that our fully discrete tokenization approach surpasses the performance of existing continuous embedding-based approaches. Another advantage of TFM-Token is its reduced model footprint. As shown in , TFM-Token achieves better results with significantly fewer parameters-a 3-fold reduction compared to LaBraM (5.8M \u2192 1.9M) and a 1.5-fold reduction compared to BIOT (3.2M \u2192 1.9M). This reduction can be attributed to discrete tokenization approach, which compresses EEG into a token sequence, thereby reducing data complexity.\nQ1.2 - Importance of Joint Frequency and Temporal Modeling: To evaluate the importance of joint frequency-temporal modeling, we conducted an ablation study comparing three tokenization variants: (1) TFM-Token-Raw Signal Only (TFM-Token-R), which uses only raw EEG patches \\(\\left\\{x_{i}\\right\\}_{i=1}^{N}\\) to predict the spectrum S, (2) TFM-Token-STFT Only (TFM-Token-S), and (3) TFM-Token, which jointly models both temporal and frequency features. Masked modeling was applied for token learning in the latter two, with consistent TFM-Encoder training across all variants. As shown in , all three variants outperform existing baselines. In event classification, TFM-Token-S improves Cohen's Kappa over TFM-Token-R (0.5194 \u2192 0.5275). However, in abnormal detection, TFM-Token-R achieves a higher AUC-PR (0.8814 \u2192 0.8908). These results indicate that different EEG tasks rely on distinct feature domains, underscoring the necessity of joint modeling. The primary TFM-Token consistently outperforms both single-domain approaches across all settings, further underscoring the importance of joint modeling."}, {"title": "5.3 Q2: EEG Token Quality Analysis and Frequency Learning", "content": "We study the quality of the EEG tokens learned by our TFM-Tokenizer by analyzing four key aspects: (1) token utilization, (2) class-specific distinctiveness, (3) similar class retrieval, and (4) frequency learning capability. We conducted our analysis using all three TFM-Tokenizer variants and the neural tokenizer from LaBraM [17], testing them on the test splits of both the TUEV and IIIC datasets, which have multiple classes. All tokenizers employed a fixed vocabulary size of 8,192 tokens for consistency and fair comparison.\nQ2.1 - Token utilization and Class uniqueness: Token utilization (%) score was calculated as the percentage of unique tokens activated from the total available vocabulary size. To quantify whether the tokenizers capture class-distinctive representations, we introduce the Class-Token Uniqueness Score, defined as:\nClass-Token Uniqueness % = \\(\\frac{\\text{# Unique Tokens in Class}}{\\text{# Tokens Utilized by Class}} \\times 100\\)\nvisualizes the class-token uniqueness scores for each class in both datasets. A robust tokenizer should capture class-distinctive tokens across all dataset classes through unsupervised pretraining. To assess this, we computed the geometric mean (GM) of class-token uniqueness scores, as shown in . Our TFM-Tokenizer reduces token utilization by more than two-fold compared to the neural tokenizer on TUEV (21.13% \u2192 9.78%) and nearly two-fold on IIIC (15.25% \u2192 8.26%). It also significantly improves learning of class-unique tokens compared to neural tokenizer (0.034% \u2192 2.14%on TUEV, 0.0% \u2192 1.429% on IIIC). These results demonstrate that the TFM-Tokenizer captures more compact and useful tokens than the neural tokenizer. Additionally, TFM-Tokenizer achieves a higher class-token uniqueness score across all classes compared to TFM-Tokenizer-R (0.0% \u2192 1.429% on IIIC) and TFM-Tokenizer-S (0.619% \u2192 1.429% on IIIC), as depicted in . This further validates joint frequency-temporal modeling in EEG analysis.\nQ2.2 - Tokens for Similar-Class Sample Mining: We conducted an EEG signal mining experiment based on similar-class sample retrieval. Given a multi-channel EEG sample, we first obtain its discrete token representation. Using the Jaccard similarity score, we then retrieve the top K most similar samples from the dataset and compute the precision score for correctly retrieving samples of the same class. For this study, we constructed a balanced subset from the IIIC and TUEV datasets and tested all four tokenization methods. The retrieval performance, illustrated in , shows that all TFM-Tokenizer variants significantly outperform neural tokenizer. Notably, TFM-Tokenizer-S and TFM-Tokenizer achieve nearly 60% precision on the TUEV for K = 1. While the Jaccard similarity measure demonstrates initial feasibility, further research is needed to identify optimal metrics for token-based EEG retrieval.\nQ2.3 - Evaluating the Frequency Learning of TFM-Tokenizer Tokens: In this experiment, we compare the frequency and temporal-domain encoders of the TFM-Tokenizer to evaluate their ability to capture diverse frequency features in EEG signals. Specifically, we arrange all tokens in temporal order and perform a discrete Fourier transform on the token sequence. This process decomposes the tokens into frequencies, where each frequency reflects the degree of change between tokens at various scales. Larger changes indicate more diverse token representations. Then, we compute spectral entropy, defined as the normalized Shannon entropy of the amplitude values, to quantify how energy is distributed across the spectrum. Higher spectral entropy means that the model has learned a broader range of frequency features, capturing differences from both large-scale trends and fine details.  shows that on the TUEV, TUAB, and CHBMIT datasets, the frequency encoder produces tokens with significantly higher spectral entropy than the temporal encoder. For example, on the TUEV dataset, the frequency encoder achieved an average spectral entropy of 0.26, while the temporal encoder reached only 0.14. This multi-scale sensitivity benefits downstream tasks such as classification, where learning detailed differences in EEG tokens can improve performance."}, {"title": "5.4"}]}