{"title": "BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds", "authors": ["Huayi Wang", "Zirui Wang", "Junli Ren", "Qingwei Ben", "Tao Huang", "Weinan Zhang", "Jiangmiao Pang"], "abstract": "Traversing risky terrains with sparse footholds poses a significant challenge for humanoid robots, requiring precise foot placements and stable locomotion. Existing approaches designed for quadrupedal robots often fail to generalize to humanoid robots due to differences in foot geometry and unstable morphology, while learning-based approaches for humanoid locomotion still face great challenges on complex terrains due to sparse foothold reward signals and inefficient learning processes. To address these challenges, we introduce BEAMDOJO, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds. BEAMDOJO begins by introducing a sampling-based foothold reward tailored for polygonal feet, along with a double critic to balancing the learning process between dense locomotion rewards and sparse foothold rewards. To encourage sufficient trail-and-error exploration, BEAMDOJO incorporates a two-stage RL approach: the first stage relaxes the terrain dynamics by training the humanoid on flat terrain while providing it with task terrain perceptive observations, and the second stage fine-tunes the policy on the actual task terrain. Moreover, we implement a onboard LiDAR-based elevation map to enable real-world deployment. Extensive simulation and real-world experiments demonstrate that BEAMDOJO achieves efficient learning in simulation and enables agile locomotion with precise foot placement on sparse footholds in the real world, maintaining a high success rate even under significant external disturbances.", "sections": [{"title": "I. INTRODUCTION", "content": "Traversing risky terrains with sparse footholds, such as stepping stones and balancing beams, presents a significant challenge for legged locomotion. Achieving agile and safe locomotion on such environment requires robots to accurately process perceptive information, make precise footstep placement within safe areas, and maintain base stability throughout the process [46, 49].\nExisting works have effectively addressed this complex task for quadrupedal robots [11, 12, 21, 41, 46, 47, 49]. However, these methods encounter great challenges when applied to humanoid robots, primarily due to a key difference in foot geometry. Although the foot of most quadrupedal robots and some simplified bipedal robots [6, 22] can be modeled as a point, the foot of humanoid robots is often represented as a polygon [4, 13, 18, 35]. For traditional model-based methods, this requires additional half-space constraints defined by linear inequalities, which impose a significant computational burden for online planning [5, 13, 28, 35]. In the case of reinforcement learning (RL) methods, foothold rewards designed for point-shaped feet are not suitable for evaluating foot placement of polygon-shaped feet [46]. Hybrid methods, which combine RL with model-based controllers, face similar challenges in online planning for humanoid feet [11, 21, 41]. Furthermore, the higher degrees of freedom and the inherently unstable morphology of humanoid robots make it even more difficult to achieve agile and stable locomotion over risky terrains.\nOn the other hand, recent works in learning-based humanoid robot locomotion have demonstrated impressive robustness across walking [2, 15, 16, 32, 38], walking up and down stairs [14, 25], and parkour [51], etc. However, these methods still struggle with complex terrains and agile locomotion on fine-grained footholds. Enabling agile movement on risky terrains for humanoid robots presents several challenges. First, the reward signal for evaluating foot placement is sparse, typically provided only after completing a full sub-process (e.g., lifting and landing a foot), which makes it difficult to assign credit to specific states and actions [36]. Second, the learning process is highly inefficient, as a single misstep often leads to early termination during training, hindering sufficient exploration. Additionally, obtaining reliable perceptual information is challenging due to sensory limitations and environmental noise [51].\nIn this work, we introduce BEAMDOJO, a novel reinforcement learning-based framework for controlling humanoid robots traversing risky terrains with sparse footholds. The name BEAMDOJO combines the words \"beam\" (referring to sparse footholds such as beams) and \u201cdojo\u201d (a place of training or learning), reflecting the goal of training agile locomotion on such challenging terrains. We begin by defining a sampling-based foothold reward, designed to evaluate the foot placement of a polygonal foot model. To address the challenge of sparse foothold reward learning, we propose using double critic architecture to separately learn the dense locomotion rewards and the sparse foothold reward. Unlike typical end-to-end RL methods [46, 49], BEAMDOJO further incorporates a two-stage approach to encourage fully trial-and-error exploration. In the first stage, terrain dynamics constraints are relaxed, allowing the humanoid robot to practice walking on flat terrain while receiving perceptive information of the target task terrain (e.g., sparse beams), where missteps will incur a penalty but do not terminate the episode. In the second stage, the policy is fine-tuned on the true task terrain. To enable deployment in real-world scenarios, we further implement a LiDAR-based, robot-centric elevation map with carefully designed domain randomization in simulation training.\nAs shown in Fig. 1, BEAMDOJO skillfully enables humanoid robots to traverse risky terrains with sparse footholds, such as stepping stones and balancing beams. Through extensive simulations and real-world experiments, we demonstrate the efficient learning process of BEAMDOJO and its ability to achieve agile locomotion with precise foot placements in real-world scenarios.\nThe contributions of our work are summarized as follows:\n\u2022\nWe propose BEAMDOJO, a two-stage RL framework that combines a newly designed foothold reward for the polygonal foot model and a double critic, enabling humanoid locomotion on sparse footholds.\n\u2022\nWe implement a LiDAR-based elevation map for real-world deployment, incorporating carefully designed domain randomization in simulation training.\n\u2022\nWe conduct extensive experiments both in simulation and on Unitree G1 Humanoid, demonstrating agile and robust locomotion on sparse footholds, with a high zero-shot sim-to-real transfer success rate of 80%. To the best of our knowledge, BEAMDOJO is the first learning-based method to achieve fine-grained foothold control on risky terrains with sparse footholds."}, {"title": "II. RELATED WORKS", "content": "Walking on sparse footholds has been a long-standing application of perceptive legged locomotion. Existing works often employs model-based hierarchical controllers, which decompose this complex task into separate stages of perception, planning, and control [12, 13, 20, 29, 30, 39]. However, model-based controllers react sensitively to violation of model assumptions, which hinders applications in real-world scenarios. Recent studies have explored combining RL with model-based controllers, such as using RL to generate trajectories that are then tracked by model-based controllers [11, 47, 41], or employing RL policies to track trajectories generated by model-based planners [21]. While demonstrating remarkable performance, these decoupled architectures can constrain the adaptability and coordination of each module.\nSubsequent works have explored end-to-end learning frameworks that train robots to walk on sparse footholds using perceptive locomotion controllers [1, 3, 45, 46, 49]. Despite their focus being limited to quadrupeds, a majority of these works rely on depth cameras for exteroceptive observations, which are limited by the camera's narrow field of view and restrict the robot to moving backward [1, 3, 45, 46]. Additionally, an image processing module is often necessary to bridge the sim-to-real gap between the captured depth images and the terrain heightmap used during training [1, 3, 45, 46].\nIn contrast to the aforementioned literature, this work achieves agile humanoid locomotion over risky terrains that addressing unique challenges specific to humanoid systems, such as foot geometry. Additionally, we implement a lidar-based elevation map to enhance the task, demonstrating that the robot can move smoothly both forward and backward using the robotics-centric elevation map as the perception module."}, {"title": "III. PROBLEM FORMULATION", "content": "This work aims to develop an terrain-aware humanoid locomotion policy, where controllers are trained via reinforcement learning (RL). The RL problem is formulated as a Markov Decision Process (MDP) M = (S,A,T,O,r, \u03b3), where S and A denote the state and action spaces, respectively. The transition dynamics are represented by T(s' |s, a), the reward function by r(s,a), and the discount factor by \u03b3\u2208 [0,1]. The primary objective is to optimize the policy \u03c0(at |st) to maximize the discounted cumulative rewards:\n$\\max J(\u039c, \u03c0) = \u0395_{\u03c0} [\\sum_{t=0}^{\\infty} \\gamma^{t}r(s_{t}, a_{t})]$ (1)\nIn this work, the agent only has access to partial observations \u03bf \u2208 O due to sensory limitations and environmental noise, which provide incomplete information about the true state. Consequently, the agent functions within the framework of a Partially Observable Markov Decision Process (POMDP)."}, {"title": "IV. METHODS", "content": "To accommodate the polygonal foot model of the humanoid robot, we introduce a sampling-based foothold reward that evaluates foot placement on sparse footholds.This evaluation is determined by the overlap between the foot's placement and designated safe areas, such as stones and beams. Specifically, we sample n points on the soles of the robot's feet, as illustrated in Fig. 2. For each j-th sample on foot i, let $d_{ij}$ denotes the global terrain height at the corresponding position. The penalty foothold reward foothold is defined as:\n$roothold = - \\sum_{i=1}^{2} C_i \\sum_{j=1}^{n} l{d_{ij} < \\epsilon},$\n(2)\nwhere $C_i$ is an indicator function that specifies whether foot i is in contact with the terrain surface, and l is the indicator function for a condition. The threshold $\\epsilon$ is a predefined depth tolerance, and when $d_{ij} < \\epsilon$, it indicates that the terrain height at this sample point is significantly low, implying improper foot placement outside of a safe area. This reward function encourages the humanoid robot to maximize the overlap between its foot placement and the safe footholds, thereby improving its terrain-awareness capabilities.\nThe task-specific foothold reward foothold is a sparse reward. To effectively optimize the policy, it is crucial to carefully balance this sparse reward with dense locomotion rewards which are crucial for gait regularization [48]. Inspired by [19, 42, 48], we adopt a double critic framework based on PPO, which effectively decouples the mixture of dense and sparse rewards.\nIn this framework, we train two separate critic networks, {$V_{1}, V_{2}$}, to independently estimate value functions for two distinct reward groups: (i) the regular locomotion reward group (dense rewards), $R_{1} = {r_{i}}^{T}_{i=0}$, which have been studied in quadruped locomotion tasks [27] and humanoid locomotion tasks [25], and (ii) the task-specific foothold reward group (sparse reward), $R_{2} = {r_{foothold}}$.\nThe double critic process is illustrated in Fig. 3. Specifically, each value network $Vo_{i}$ is updated independently for its corresponding reward group $R_{i}$ with temporal difference loss (TD-loss):\n$L(\\phi_{i}) = E [||R_{i,t} + \\gamma Vox (S_{t+1}) \u2013 Vo_{i} (S_{t})||^{2}]$, (3)\nwhere y is the discount factor. Then the respective advantages {$A_{i,t}$} are calculated using Generalized Advantage Estimation (GAE) [33]:\n$\\delta_{i,t} = R_{i,t} + \\gamma V_{\\phi_{i}}(S_{t+1}) - V_{\\phi_{i}}(S_{t}),$\n(4)\n$A_{i,t} = \\sum_{l=0}^{\\infty}(\\gamma\\lambda)^{l} \\delta_{i,t+l},$\n(5)\nwhere X is the balancing parameter. These advantages are then individually normalized and synthesized into an overall advantage:\n$A_{t} = w_{1}\\frac{A_{1.t} - \\mu_{A1}}{\\sigma_{A1}} + w_{2} \\frac{A_{2,t} - \\mu_{A2}}{\\sigma_{A2}},$\n(6)\nwhere $w_i$ is the weight for each advantage component, and $\\mu_{A_{i,t}}$ and $\\sigma_{A_{i,t}}$ are the batch mean and standard deviation of each component. This overall advantage is then used to update the policy:\n$L(\\theta) = E [min (a_{\\theta}(0) A_{t}, clip (a_{\\theta}(0), 1 \u2013 \\epsilon,1 + \\epsilon)\u00c2_{t})]$,\n(7)\nwhere $a_{\\theta} (0)$ is the probability ratio, and e is the clipping hyperparameter.\nThis double critic design provides a modular, plug-and-play solution for handling specialized tasks with sparse rewards, while effectively addressing the disparity in reward feedback frequencies within a mixed dense-sparse environment [48].\nTo address the early termination problem in complex terrain dynamics and encourage full trial-and-error exploration, we adopt a two-stage reinforcement learning (RL) approach for terrain-aware locomotion in simulation, inspired by [50, 51]. As illustrated in Fig. 3, in the first stage, termed the \u201csoft terrain dynamics constraints\u201d phase, the humanoid robot is trained on flat terrain while being provided with a corresponding height map of the true task terrains (e.g., stepping stones). This setup encourages broad exploration without the risk of early termination from missteps. Missteps are penalized but do not lead to termination, allowing the humanoid robot to develop foundational skills for terrain-aware locomotion. In the second stage, termed the \"hard terrain dynamics constraints\" phase, we continue training the humanoid on the real terrains in simulation, where missteps result in termination. This stage fine-tunes the robot's ability to step on challenging terrains accurately.\n1) Stage 1: Soft Terrain Dynamics Constraints Learning:\nIn this stage, we first map each task terrain (denoted as T) to a flat terrain (denoted as F) of the same size. Both terrains share the same terrain noise, with points are one-to-one corresponding. The only difference is that the flat terrain F fills the gaps in the real terrain T.\nWe let the humanoid robot traverse the terrain F, receiving proprioceptive observations, while providing perceptual feedback in the form of the elevation map of terrain Tat the corresponding humanoid's base position. This setup allows the robot to \"imagine\" walking on the true task terrain while actually traversing the safer flat terrain, where missteps do not lead to termination. To expose the robot to real terrain dynamics, we use the foothold reward (introduced in Section IV-A). In this phase, this reward is provided by the terrain T, where $d_{ij}$ is the height of the true terrain at the sampling point, while locomotion rewards are provided by the terrain F.\nThis design successfully decouples the standard locomotion task and the task of traversing sparse footholds: flat terrain, F, provides proprioceptive information and locomotion rewards to learn regular gaits, while risky task terrain, T, offers perceptive information and the foothold reward to develop terrain-awareness skills. We train these two reward components separately using a double critic framework, as described in Section IV-B.\nFurthermore, by allowing the humanoid robot to traverse the flat terrain while applying penalties for missteps instead of terminating the episode, the robot can continuously attempt foothold placements, making it much easier to obtain successful positive samples. In contrast, conventional early termination disrupts entire trajectories, making it extremely difficult to acquire safe foothold samples when learning from scratch. This approach significantly improves sampling efficiency and alleviates the challenges of exploring terrains with sparse footholds.\n2) Stage 2: Hard Terrain Dynamics Constraints Learning:\nIn the second stage, we fine-tune the policy directly on the task terrain T. Unlike in Stage 1, missteps on T now result in immediate termination. This enforces strict adherence to the true terrain constraints, requiring the robot to develop precise and safe locomotion strategies.\nTo maintain a smooth gait and accurate foot placements, we continue leveraging the double-critic framework to optimize both locomotion rewards and the foothold reward foothold Here, $d_{ij}$ again represents the height of terrain Tat the given sampling point.\n$\\Ot = [Ct, oproprio, opercept, a_{t-1}].$ (8)"}]}