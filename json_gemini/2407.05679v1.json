{"title": "BEVWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space", "authors": ["Yumeng Zhang", "Shi Gong", "Kaixin Xiong", "Xiaoqing Ye", "Xiao Tan", "Fan Wang", "Jizhou Huang", "Hua Wu", "Haifeng Wang"], "abstract": "World models are receiving increasing attention in autonomous driving for their ability to predict potential future scenarios. In this paper, we present BEVWorld, a novel approach that tokenizes multimodal sensor inputs into a unified and compact Bird's Eye View (BEV) latent space for environment modeling. The world model consists of two parts: the multi-modal tokenizer and the latent BEV sequence diffusion model. The multi-modal tokenizer first encodes multi-modality information and the decoder is able to reconstruct the latent BEV tokens into LiDAR and image observations by ray-casting rendering in a self-supervised manner. Then the latent BEV sequence diffusion model predicts future scenarios given action tokens as conditions. Experiments demonstrate the effectiveness of BEVWorld in autonomous driving tasks, showcasing its capability in generating future scenes and benefiting downstream tasks such as perception and motion prediction. Code will be available at https://github.com/zympsyche/BevWorld.", "sections": [{"title": "1 Introduction", "content": "Autonomous driving has made significant progress in recent years, but it still faces several challenges. First, training a reliable autonomous driving system requires a large amount of precisely annotated data, which is resource-intensive and time-consuming. Thus, exploring how to utilize unlabeled multimodal sensor data within a self-supervised learning paradigm is crucial. Moreover, a reliable autonomous driving system requires not only the ability to perceive the environment but also a comprehensive understanding of environmental information for decision-making.\nWe claim that the key to addressing these challenges is to construct a multimodal world model for autonomous driving. By modeling the environment, the world model predicts future states and behaviors, empowering the autonomous agent to make more sophisticated decisions. Recently, some world models have demonstrated their practical significance in autonomous driving [12, 42, 40]. However, most methods are based on a single modality, which cannot adapt to current multisensor, multimodal autonomous driving systems. Due to the heterogeneous nature of multimodal data, integrating them into a unified generative model and seamlessly adapting to downstream tasks remains an unresolved issue.\nIn this paper, we introduce BEVWorld, a multimodal world model that transforms diverse multimodal data into a unified bird's-eye-view (BEV) representation and performs action-conditioned future prediction within this unified space. Our BEVWorld consists of two parts: a multimodal tokenizer network and a latent BEV sequence diffusion network.\nThe core capability of the multimodal tokenizer lies in compressing original multimodal sensor data into a unified BEV latent space. This is achieved by transforming visual information into 3D"}, {"title": "2 Related Works", "content": "2.1 World Model\nThis part mainly reviews the application of world models in the autonomous driving area, focusing on scenario generation as well as the planning and control mechanism. If categorized by the key applications, we divide the sprung-up world model works into two categories. (1) Driving Scene Generation. The data collection and annotation for autonomous driving are high-cost and sometimes risky. In contrast, world models find another way to enrich unlimited, varied driving data due to their intrinsic self-supervised learning paradigms. GAIA-1 [12] adopts multi-modality inputs collected in the real world to generate diverse driving scenarios based on different prompts (e.g., changing weather, scenes, traffic participants, vehicle actions) in an autoregressive prediction manner, which shows its ability of world understanding. ADriver-I [13] combines the multimodal large language model and a video latent diffusion model to predict future scenes and control signals, which significantly improves the interpretability of decision-making, indicating the feasibility of the world model as a fundamental model. MUVO [3] integrates LiDAR point clouds beyond videos to predict future driving scenes in the representation of images, point clouds, and 3D occupancy. Further, Copilot4D [42] leverages a discrete diffusion model that operates on BEV tokens to perform 3D point cloud forecasting and OccWorld [45] adopts a GPT-like generative architecture for 3D semantic occupancy forecast and motion planning. DriveWorld [27] and UniWorld [26] approach the world model as 4D scene understanding task for pre-training for downstream tasks. (2) Planning and Control. MILE [11] is the pioneering work that adopts a model-based imitation learning approach for joint dynamics future environment and driving policy learning in autonomous driving. DriveDreamer [33] offers a comprehensive framework to utilize 3D structural information such as HDMap and 3D box to predict future driving videos and driving actions. Beyond the single front view generation, DriveDreamer-2 [44] further produces multi-view driving videos based on user descriptions. TrafficBots [43] develops a world model for multimodal motion prediction and end-to-end driving, by facilitating action prediction from a BEV perspective. Drive-WM [34] generates controllable multiview videos and applies the world model to safe driving planning to determine the optimal trajectory according to the image-based rewards.\n2.2 Video Diffusion Model\nWorld model can be regarded as a sequence-data generation task, which belongs to the realm of video prediction. Many early methods [11, 12] adopt VAE [17] and auto-regression [6] to generate future predictions. However, the VAE suffers from unsatisfactory generation quality, and the auto-regressive method has the problem of cumulative error. Thus, many researchers switch to study on diffusion-based future prediction methods [44, 20], which achieves success in the realm of video generation"}, {"title": "3 Method", "content": "In this section, we delineate the model structure of BEVWorld. The overall architecture\nis illustrated in Figure 1. Given a sequence of multi-view image and Lidar observations\n$\\{O_{t-P}, \\ldots, O_{t-1}, O_t, O_{t+1},\\dots, O_{t+N}\\}$ where $o_t$ is the current observation, +/- represent the fu-\nture/past observations and P/N is the number of past/future observations, we aim to predict\n$\\{O_{t+1},\\dots, O_{t+N}\\}$ with the condition $\\{0_{t-p}, \\ldots, O_{t-1},0_t\\}$. In view of the high computing costs\nof learning a world model in original observation space, a multi-modal tokenizer is proposed to\ncompress the multi-view image and Lidar information into a unified BEV space by frame. The\nencoder-decoder structure and the self-supervised reconstruction loss promise proper geometric\nand semantic information is well stored in the BEV representation. This design exactly provides\na sufficiently concise representation for the world model and other downstream tasks. Our world\nmodel is designed as a diffusion-based network to avoid the problem of error accumulating as those\nin an auto-regressive fashion. It takes the ego motion and $\\{x_{t-P},\\dots,x_{t-1},x_{t}\\}$, i.e. the BEV\nrepresentation of $\\{0_{t-P},\\ldots, O_{t-1}, O_t\\}$, as condition to learn the noise $\\{\\epsilon_{t+1},\\dots,\\epsilon_{t+N}\\}$ added to"}, {"title": "3.1 Multi-Modal Tokenizer", "content": "Our designed multi-modal tokenizer contains three parts: a BEV encoder network, a BEV Decoder\nnetwork and a multi-modal rendering network. The structure of BEV encoder network is illustrated\nin the Figure 2. To make the multi-modal network as homogeneous as possible, we adopt the\nSwin-Transformer [22] network as the image backbone to extract multi-image features. For Lidar\nfeature extraction, we first split point cloud into pillars [19] on the BEV space. Then we use the Swin-\nTransformer network as the Lidar backbone to extract Lidar BEV features. We fuse the Lidar BEV\nfeatures and the multi-view images features with a deformable-based transformer [46]. Specifically,\nwe sample K(K = 4) points in the height dimension of pillars and project these points onto the\nimage to sample corresponding image features. The sampled image features are treated as values and\nthe Lidar BEV features is served as queries in the deformable attention calculation. Considering the\nfuture prediction task requires low-dimension inputs, we further compress the fused BEV feature into\na low-dimensional(C' = 4) BEV feature.\nFor BEV decoder, there is an ambiguity problem when directly using a decoder to restore the images\nand Lidar since the fused BEV feature lacks height information. To address this problem, we first\nconvert BEV tokens into 3D voxel features through stacked layers of upsampling and swin-blocks.\nAnd then we use voxelized NeRF-based ray rendering to restore the multi-view images and Lidar\npoint cloud.\nNr\nThe multi-modal rendering network can be elegantly segmented into two distinct components, image\nreconstruction network and Lidar reconstruction network. For image reconstruction network, we first\nget the ray r(t) = o + td, which shooting from the camera center o to the pixel center in direction d.\nThen we uniformly sample a set of points $\\{(x_i, y_i, z_i)\\}_{i=1}^{N_r}$ along the ray, where $N_r(N_r = 150)$ is\nthe total number of points sampled along a ray. Given a sampled point $(x_i, y_i, z_i)$, the corresponding\nfeatures $v_i$ are obtained from the voxel feature according to its position. Then, all the sampled\nfeatures in a ray are aggregated as pixel-wise feature descriptor (Eq. 1).\n$v(r) = \\sum_{i=1}^{N_r} w_i v_i, w_i = \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j), \\alpha_i = \\sigma(MLP(v_i)) $\n(1)\nWe traverse all pixels and obtain the 2D feature map $V \\in R^{H_f \\times W_f \\times C_f}$ of the image. The 2D feature\nis converted into the RGB image $I_g \\in R^{H \\times W \\times 3}$ through a CNN decoder. Three common losses are\nadded for improving the quality of generated images, perceptual loss [14], GAN loss [8] and L1\nloss. Our full objective of image reconstruction is:\n$L_{rgb} = ||I_g - I_t||_1 + \\lambda_{perc} || \\phi_j(I_g) - \\phi_j(I_t) || + \\lambda_{gan} L_{gan}(I_g, I_t)$  (2)"}, {"title": "3.2 Latent BEV Sequence Diffusion", "content": "Most existing world models [42, 12] adopt autoregression strategy to get longer future predictions,\nbut this method is easily affected by cumulative errors. Instead, we propose latent sequence diffusion\nframework, which inputs multiple frames of noise BEV tokens and obtains all future BEV tokens\nsimultaneously.\nThe structure of latent sequence diffusion is illustrated in Figure 1. In the training process, the\nlow-dimensional BEV tokens $(x_{t-P},\\dots,x_{t-1},x_{t},x_{t+1},\\dots,x_{t+N})$ are firstly obtained from the\nsensor data. Only BEV encoder in the multi-modal tokenizer is involved in this process and the\nparameters of multi-modal tokenizer is frozen. To facilitate the learning of BEV token features\nby the world model module, we standardize the input BEV features along the channel dimension\n(xt-P,..., xt-1, xt, xt+1,...,xt+N). Latest history BEV token and current frame BEV token"}, {"title": "4 Experiments", "content": "4.1 Dataset\nNuScenes [5] NuScenes is a widely used autonomous driving dataset, which comprises multi-modal\ndata such as multi-view images from 6 cameras and Lidar scans. It includes a total of 700 training\nvideos and 150 validation videos. Each video includes 20 seconds at a frame rate of 12Hz.\nCarla [7] The training data is collected in the open-source CARLA simulator at 2Hz, including 8\ntowns and 14 kinds of weather. We collect 3M frames with four cameras (1600 \u00d7 900) and one Lidar\n(32p) for training, and evaluate on the Carla Town05 benchmark, which is the same setting of [30].\n4.2 Multi-modal Tokenizer\nIn this section, we explore the impact of different design decisions in the proposed multi-modal\ntokenizer and demonstrate its effectiveness in the downstream tasks. For multi-modal reconstruction\nvisualization results, please refer to Figure7 and Figure8."}, {"title": "4.2.1 Ablation Studies", "content": "Various input modalities and output modalities. The proposed multi-modal tokenizer supports\nvarious choice of input and output modalities. We test the influence of different modalities, and\nthe results are shown in Table 1, where L indicates Lidar modality, C indicates multi-view cameras\nmodality, and L&C indicates multi-modal modalities. The combination of Lidar and cameras achieves\nthe best reconstruction performance, which demonstrates that using multi modalities can generate\nbetter BEV features. We find that the PSNR metric is somewhat distorted when comparing ground\ntruth images and predicted images. This is caused by the mean characteristics of PSNR metric, which\ndoes not evaluate sharpening and blurring well. As shown in Figure 12, though the PSNR of multi\nmodalities is slightly lower than single camera modality method, the visualization of multi modalities\nis better than single camera modality as the FID metric indicates.\nRendering approaches. To convert from BEV features into multiple sensor data, the main challenge\nlies in the varying positions and orientations of different sensors, as well as the differences in imaging\n(points and pixels). We compared two types of rendering methods: a) attention-based method,\nwhich implicitly encodes the geometric projection in the model parameters via global attention\nmechanism; b) ray-based sampling method, which explicitly utilizes the sensor's pose information\nand imaging geometry. The results of the methods (a) and (b) are presented in Table 2. Method (a)\nfaces with a significant performance drop in multi-view reconstruction, indicating that our ray-based\nsampling approach reduces the difficulty of view transformation, making it easier to achieve training\nconvergence. Thus we adopt ray-based sampling method for generating multiple sensor data.\n4.2.2 Benefit for Downstream Tasks\n3D Detection. To verify our proposed method is effective for downstream tasks when used in the\npre-train stage, we conduct experiments on the nuScenes 3D detection benchmark. For the model\nstructure, in order to maximize the reuse of the structure of our multi-modal tokenizer, the encoder in\nthe downstream 3D detection task is kept the same with the encoder of the tokenizer described in 3.\nWe use a BEV encoder attached to the tokenizer encoder for further extracting BEV features. We\ndesign a UNet-style network with the Swin-transformer [22] layers as the BEV encoder. As for the\ndetection head, we adopt query-based head [21], which contains 500 object queries that searching the\nwhole BEV feature space and uses hungarian algorithm to match the prediction boxes and the ground\ntruth boxes. We report both single frame and two frames results. We warp history 0.5s BEV future\nto current frame in two frames setting for better velocity estimation. Note that we do not perform\nfine-tuning specifically for the detection task all in the interest of preserving the simplicity and clarity\nof our setup. For example, the regular detection range is [-60.0m, -60.0m, -5.0m, 60.0m, 60.0m,\n3.0m] in the nuScenes dataset while we follow the BEV range of [-80.0m, -80.0m, -4.5m, 80.0m,\n80.0m, 4.5m] in the multi-modal reconstruction task, which would result in coarser BEV grids and\nlower accuracy. Meanwhile, our experimental design eschew the use of data augmentation techniques\nand the layering of point cloud frames. We train 30 epoches on 8 A100 GPUs with a starting\nlearning rate of 5e-4 that decayed with cosine annealing policy. We mainly focus on the relative\nperformance gap between training from scratch and use our proposed self-supervised tokenizer\nas pre-training model. As demonstrated in Table 3, it is evident that employing our multi-modal\ntokenizer as a pre-training model yields significantly enhanced performance across both single and\nmulti-frame scenarios. Specifically, with a two-frame configuration, we have achieved an impressive\n8.4% improvement in the NDS metric and a substantial 13.4% improvement in the mAP metric,\nattributable to our multi-modal tokenizer pre-training approach.\nMotion Prediction. We further validate the performance of using our method as pre-training model\non the motion prediction task. We attach the motion prediction head to the 3D detection head. The\nmotion prediction head is stacked of 6 layers of cross attention(CA) and feed-forward network(FFN)."}, {"title": "4.3 Latent BEV Sequence Diffusion", "content": "In this section, we introduce the training details of latent BEV Sequence diffusion and compare this\nmethod with other related methods.\n4.3.1 Training Details.\nNuScenes. We adopt a three stage training for future BEV prediction. 1) Next BEV pretraining. The\nmodel predicts the next frame with the $\\{x_{t-1},x_{t}\\}$ condition. In practice, we adopt sweep data of\nnuScenes to reduce the difficulty of temporal feature learning. The model is trained 20000 iters with\na batch size 128. 2) Short Sequence training. The model predicts the $N(N = 5)$ future frames of\nsweep data. At this stage, the network can learn how to perform short-term (0.5s) feature reasoning.\nThe model is trained 20000 iters with a batch size 128. 3) Long Sequence Fine-tuning. The model\npredicts the $N(N = 6)$ future frames (3s) of key-frame data with the $\\{Xt\u22122, Xt\u22121, xt\\}$ condition."}, {"title": "4.3.2 Lidar Prediction Quality", "content": "NuScenes. We compare the Lidar prediction quality with existing SOTA methods. We follow the\nevaluation process of [42] and report the Chamfer 1s/3s results in Table 5, where the metric is\ncomputed within the region of interest: -70m to +70m in both x-axis and y-axis, -4.5m to +4.5m in\nz-axis. Our proposed method outperforms SPFNet, S2Net and 4D-Occ in Chamfer metric by a large\nmargin. When compared to Copilot4D [42], our approach uses less history condition frames and no\nCFG schedule setting considering the large memory cost for multi-modal inputs. Our BEVWorld\nrequires only 3 past frames for 3-second predictions, whereas Copilot4D utilizes 6 frames for the\nsame duration. Our method demonstrates superior performance, achieving chamfer distance of 0.73\ncompared to 1.40, in the no CFG schedule setting, ensuring a fair and comparable evaluation.\nCarla. We also conducted experiments on the Carla dataset to verify the scalability of our method.\nThe quantitative results are shown in Table 5. We reproduce the results of 4D-Occ on Carla and\ncompare it with our method, obtaining similar conclusions to this on the nuScenes dataset. Our\nmethod significantly outperform 4D-Occ in prediction results for both 1-second and 3-second."}, {"title": "4.3.3 Video Generation Quality", "content": "NuScenes. We compare the video generation quality with past single-view and multi-view generation\nmethods. Most of existing methods adopt manual labeling condition, such as layout or object label,\nto improve the generation quality. However, using annotations reduces the scalability of the world\nmodel, making it difficult to train with large amounts of unlabeled data. Thus we do not use the\nmanual annotations as model conditions. The results are shown in Table 4. The proposed method\nachieves best FID and FVD performance in methods without using manual labeling condition and\nexhibits comparable results with methods using extra conditions. The visual results of Lidar and\nvideo prediction are shown in Figure 5. Furthermore, the generation can be controlled by the action\nconditions. We transform the action token into left turn, right turn, speed up and slow down, and the\ngenerated image and Lidar can be generated according to these instructions. The visualization of\ncontrollability are shown in Figure 6.\nCarla. The generation quality on Carla is similar to that on nuScenes dataset, which demonstrates\nthe scalability of our method across different datasets. The quantitative results of video predictions\nare shown in Table 4 with 36.80(FID 1s) and 43.12(FID 3s). Qualitative results of video predictions\nare shown in the appendix."}, {"title": "4.3.4 Benefit for Planning Tasks", "content": "We further validate the effectiveness of the predicted future BEV features from latent diffusion\nnetwork for toy downstream open-loop planning task [41] on nuScenes dataset. Note that we do not\nuse actions of ego car in future frames here and we adopt xo-parameterization [1] for fast inference.\nWe adopt four vectors, history trajectory, command, perception and optional future BEV vectors,\nas input for planning head. History trajectory vector encodes the ego movement from last frame\nto current frame. Command vector refers to the routing command such as turning left or right.\nPerception vector is extracted from the object query in the detection head that interacted with all\ndetection queries. Future BEV vector is obtained from the pooled BEV features from the fixed\ndiffusion model. When using future BEV vectors, PNC L2 3s metric is decreased from 1.030m to\n0.977m, which validates that the predicted BEV from world model is beneficial for planning tasks."}, {"title": "5 Conclusion", "content": "We present BEVWorld, an innovative autonomous driving framework that leverages a unified Bird's\nEye View latent space to construct a multi-modal world model. BEVWorld's self-supervised learning\nparadigm allows it to efficiently process extensive unlabeled multimodal sensor data, leading to a\nholistic comprehension of the driving environment. We validate the effectiveness of BEVWorld in\nthe downstream autonomous driving tasks. Furthermore, BEVWorld achieves satisfactory results\nin multi-modal future predictions with latent diffusion network, showcasing its capabilities through\nexperiments on both real-world(nuScenes) and simulated(carla) datasets. We hope that the work\npresented in this paper will stimulate and foster future developments in the domain of world models\nfor autonomous driving."}]}