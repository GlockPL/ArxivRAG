{"title": "Estimating Probabilities of Causation with Machine Learning Models", "authors": ["Shuai Wang", "Ang Li"], "abstract": "Probabilities of causation play a crucial role in modern decision-making. This paper addresses the challenge of predicting probabilities of causation for subpopulations with insufficient data using machine learning models. Tian and Pearl first defined and derived tight bounds for three fundamental probabilities of causation: the probability of necessity and sufficiency (PNS), the probability of sufficiency (PS), and the probability of necessity (PN). However, estimating these probabilities requires both experimental and observational distributions specific to each subpopulation, which are often unavailable or impractical to obtain with limited population-level data. We assume that the probabilities of causation for each subpopulation are determined by its characteristics. To estimate these probabilities for subpopulations with insufficient data, we propose using machine learning models that draw insights from subpopulations with sufficient data. Our evaluation of multiple machine learning models indicates that, given sufficient population-level data and an appropriate choice of machine learning model and activation function, PNS can be effectively predicted. Through simulation studies, we show that our multilayer perceptron (MLP) model with the Mish activation function achieves a mean absolute error (MAE) of approximately 0.02 in predicting PNS for 32,768 subpopulations using data from around 2,000 subpopulations.", "sections": [{"title": "1 INTRODUCTION", "content": "Understanding causal relationships and estimating probabilities of causation are crucial in fields such as healthcare, policy evaluation, and economics [Pearl, 2009, Imbens and Rubin, 2015, Heckman and Pinto, 2015]. Unlike correlation-based methods, causal inference enables decision-makers to determine whether an action or intervention directly leads to a desired outcome. This is particularly essential in personalized medicine, where accurately assessing treatment effects ensures both efficacy and safety [Mueller and Pearl, 2023]. Moreover, causal reasoning enhances machine learning applications by improving accuracy [Li et al., 2020], interpretability, and fairness [Plecko and Bareinboim, 2022] in automated decision-making. Despite its broad significance, estimating probabilities of causation remains challenging due to data limitations. In this paper, we address this challenge by leveraging machine learning techniques to predict probabilities of causation for subpopulations with insufficient data.\nThe study of probabilities of causation began around 2000 when Pearl [1999] first defined three fundamental probabilities-PNS, PS, and PN-within Structural Causal Models [Galles and Pearl, 1998, Halpern, 2000, Pearl, 2009]. Subsequently, Tian and Pearl [2000] derived tight bounds for these probabilities using Balke's linear programming [Balke, 1995], incorporating both observational and experimental data. Nearly two decades later, Li and Pearl [2019] formally proved these bounds and introduced the unit selection model, a decision-making framework based on their linear combination. More recently, Li and Pearl [2024b] extended the definitions and bounds to a more general form. Additionally, Mueller et al. [2022], as well as Dawid et al. [2017], demonstrated that these bounds could be further refined given specific causal structures.\nHowever, any above estimation of the probabilities of causation requires both observational and experimental data. Additionally, estimating (sub)populations, based on Li et al. [2022c]'s suggestions, requires approximately 1,300 entries of both data types for each (sub)population, making the process impractical. Li et al. [2022a,b] demonstrated the potential of machine learning models to achieve accurate estimations for (sub)populations. In this research, we select five diverse machine learning models based on the characteristics of the probabilities of causation. We then evaluate"}, {"title": "1.1 CONTRIBUTIONS", "content": "Despite the extensive theoretical research on probabilities of causation, practical estimation methods have remained unexplored. Our work provides the first systematic approach to predicting probabilities of causation using machine learning. Specifically, we make the following contributions:\n\u2022 First Machine Learning Pipeline for Predicting Probabilities of Causation: We propose a novel machine learning framework to estimate the bounds of PNS, PS, and PN, filling a critical gap between theoretical causal inference and practical applications.\n\u2022 First Accurate Machine Learning Model for PNS Prediction: We demonstrate that a MLP can accurately predict PNS, proving that machine learning is a feasible and effective tool for estimating probabilities of causation.\n\u2022 First Dataset for PNS Bound Prediction: We construct and release the first synthetic dataset specifically designed to evaluate machine learning models for estimating PNS, providing a foundation for future research in this area.\nTo the best of our knowledge, no prior work has applied machine learning to the problem of predicting probabilities of causation. Our study establishes a new research direction by bridging causal inference and machine learning for practical estimation tasks.\nThe remainder of the paper is structured as follows: first, we review key causal inference concepts to provide necessary context. Next, we introduce the model and dataset used in our study. Finally, we present our five machine learning models developed for the task. All code for data generation and machine learning models is included in the appendix."}, {"title": "2 PRELIMINARIES", "content": "In this section, we review the fundamental concepts of causal inference necessary for understanding the rest of the paper. We begin by discussing the definitions of PNS, PS, and PN as introduced by Pearl [1999], followed by the definitions of identifiability and the conditions required to identify PNS, PS, and PN [Tian and Pearl, 2000]. Additionally, we examine the tight bounds of PNS, PS, and PN in cases where they are unidentifiable [Tian and Pearl, 2000]. Readers already familiar with these concepts may skip this section.\nSimilar to the works mentioned above, we adopt the causal language of Structural Causal Models (SCMs) [Galles and Pearl, 1998, Halpern, 2000]. In this framework, the counterfactual statement \"Variable Y would have the value y had X been x\" is denoted as $Y_x = y$, abbreviated as $y_x$. We consider two types of data: experimental data, expressed as causal effects $P(y_x)$, and observational data, represented by the joint probability function $P(x, y)$. Unless otherwise specified, we assume X and Y are binary variables in a causal model M, with x and y denoting the propositions X = true and Y = true, respectively, and x' and y' representing their complements. For simplicity, we focus on binary variables; extensions to multi-valued cases are discussed by Pearl [2009] (p. 286, footnote 5) and Li and Pearl [2024a].\nFirst, the definitions of three basic probabilities of causation defined using SCM are as follow [Pearl, 1999]:\nDefinition 1 (Probability of necessity (PN)). Let X and Y be two binary variables in a causal model M, let x and y stand for the propositions X = true and Y = true, respectively, and x' and y' for their complements. The probability of necessity is defined as the expression\n$PN \\triangleq P(Y_{X=false} = false|X = true, Y = true) \\triangleq P(y_{x'}|x,y)$"}, {"title": "Definition 2 (Probability of sufficiency (PS)).", "content": "Let X and Y be two binary variables in a causal model M, let x and y stand for the propositions X = true and Y = true, respectively, and x' and y' for their complements. The probability of sufficiency is defined as the expression\n$PS \\triangleq P(Y_{X=true} = true|X = false, Y = false) \\triangleq P(y_x|x',y')$"}, {"title": "Definition 3 (Probability of necessity and sufficiency (PNS)).", "content": "Let X and Y be two binary variables in a causal model M, let x and y stand for the propositions X = true and Y = true, respectively, and x' and y' for their complements. The probability of necessity and sufficiency is defined as the expression\n$PNS \\triangleq P(Y_{X=true} = true, Y_{X=false} = false) \\triangleq P(y_x, Y'_{x'})$\nThen, we review the identification conditions for PNS, PS, and PN [Tian and Pearl, 2000].\nDefinition 4. (Monotonicity) A Variable Y is said to be monotonic relative to variable X in a causal model M iff $Y_x \\neq Y_{x'} = false$.\nTheorem 5. If Y is monotonic relative to X, then PNS, PN, and PS are all identifiable, and\n$PNS = P(y_x) - P(y_{x'}),$\n$PN = \\frac{P(x,y)}{P(y) - P(y_{x'})},$\n$PS = \\frac{P(y_x) - P(x,y)}{P(x',y')}$,"}, {"title": "3 STRUCTURAL CAUSAL MODEL", "content": "In general, the equations in SCMs are in implicitly form (e.g., $Z = f_Z(X, Y, U_Z)$). However, in order to verify the accuracy of the learned bounds of PNS, we need to explicitly define the SCM and the data-generating process to determine the true PNS value and its bounds. Followed the setup in Li et al. [2022a], we will use the following SCM.\n$Z_i = U_{Z_i}$ for $i \\in \\{1, ..., 20\\},$\n$X = f_X(M_X, U_X)$\n$=\\begin{cases}\n1 & \\text{if } M_X + U_X > 0.5 \\\\\n0 & \\text{otherwise,}\n\\end{cases}$\n$Y = f_Y(X, M_Y, U_Y)$\n$=\\begin{cases}\n1 & \\text{if } 0 < C_Y \\cdot X + M_Y + U_Y < 1 \\\\\n1 & \\text{if } 1 < C_Y \\cdot X + M_Y + U_Y < 2 \\\\\n0 & \\text{otherwise.}\n\\end{cases}$\nwhere X, Y, $Z_i$ are all binary, $U_{Z_i}$, $U_X$, $U_Y$ are binary exogenous variables with Bernoulli distributions, $C_Y$ is a constant, and $M_X$, $M_Y$ are linear combinations of $Z_i$. The randomly generated value of $C_Y$, $M_X$, $M_Y$ and the distributions of $U_X$, $U_Y$, $U_{Z_i}$ for the model are provided in the appendix."}, {"title": "4 DATA GENERATING PROCESS", "content": "Based on the defined model, 20 binary features are considered (i.e., $Z_1, ..., Z_{20}$). We made 15 observable ($Z_1, ..., Z_{15}$) and 5 unobservable, and the exogenous variables are also unobservable, leading to $2^{15}$ observed subpopulations (i.e., the combination of $Z_1, ..., Z_{15}$ defined a subpopulation).\n4.1 INFORMER DATA\nTo evaluate the learned bounds, the informer data must have access to the actual PNS bounds for each subpopulation. Given the explicit form of the SCM and the distributions of all exogenous variables, the PNS bounds, as well as the experimental and observational distributions, can be computed for each combination of the features $Z_1, ..., Z_{15}$ (i.e., a subpopulation) using the SCM. For detailed mathematical formulations, refer to the appendix.\n4.2 SAMPLE COLLECTION\nA total of 50, 000, 000 experimental and 50, 000, 000 observational samples were generated as follows for each sample: In both settings, the exogenous variables $U_X$, $U_Y$, and $U_{Z_i}$ were randomly generated according to their distributions specified in Section 3. In the experimental setting, X was then assigned according to a Bernoulli(0.5) distribution, while Y and $Z_i$ were computed using the structural functions described in Section 3. In the observational setting, X, Y, and $Z_i$ were all determined by the structural functions. The final datasets include only the observable features $Z_1,..., Z_{15}$, along with X and Y, while $Z_{16},..., Z_{20}$ were masked.\n4.3 DATA FOR MACHINE LEARNING MODELS\nWe selected subpopulations from the $2^{15}$ possible groups that contained at least 1, 300 experimental and observational samples (1,300 based on Li et al. [2022c]'s suggestions). For these selected subpopulations, we computed the experimental and observational distributions and determined the bounds of PNS using Equations 1 and 2. These results served as the data for our machine learning models (i.e., each data entry consists of 15 features and the PNS bounds as the label.) The obtained data includes 2, 054 entries for the lower bound (LB) and 2,065 entries for the upper bound (UB) of the PNS."}, {"title": "5 MACHINE LEARNING PREDICTION", "content": "To evaluate the feasibility of machine learning in predicting the bounds of the PNS, we employed five distinct machine learning models to assess their effectiveness in this task: Support Vector Machine (SVM) [Cortes, 1995], Random Forest (RF) [Ho, 1995], Gradient Boosting Decision Trees (GBDT) [Friedman, 2001], Transformer [Vaswani, 2017], and Multilayer Perceptron (MLP) [Rumelhart et al., 1986]. These models were chosen to represent a diverse range of machine learning paradigms, including kernel-based methods (SVM), ensemble learning techniques (RF and GBDT), and deep learning approaches (MLP and Transformer). This selection ensures a comprehensive evaluation of their ability to approximate causal quantities across different settings. A detailed pipeline is illustrated in Figure 1.\n5.1 SUPPORT VECTOR MACHINE\nSupport Vector Machines (SVM) [Cortes, 1995] are widely used and well-established supervised learning models. Given their strengths, we selected Support Vector Regression (SVR), a variant of SVM, as the first model for our experiments. To effectively capture complex patterns, we employed the Radial Basis Function (RBF) kernel to map the data into a high-dimensional feature space.\nKey hyperparameters include the penalty parameter (C), the insensitive loss threshold (${\\epsilon}$), and the kernel coefficient ($\\gamma$). The parameter C controls the trade-off between model complexity and error tolerance, where larger values may lead to overfitting. The threshold $\\epsilon$ defines the margin of tolerance for errors, while $\\gamma$ determines the influence range of individual data points.\nA two-stage hyperparameter tuning strategy was adopted. First, Randomized Search [Bergstra and Bengio, 2012] was employed to efficiently explore the parameter space and identify promising ranges. Then, Grid Search [Bergstra and Bengio, 2012] was used to fine-tune parameters within these ranges. Cross-validation ensured robust generalization throughout the tuning process.\nFinally, the mean squared error (MSE) and mean absolute error (MAE) values of the SVR model can be found in Table 1. Confusion matrices are presented in Figures 3a and 3d, while Figures 2a and 2e provide a clearer comparison with the true PNS bounds. For the prediction of the lower bound, SVR demonstrates reasonable effectiveness; however, for the more complex upper bound, it exhibits a significant decline in accuracy.\n5.2 RANDOM FOREST\nRandom Forests (RF) [Ho, 1995] are a widely used ensemble learning method for classification, regression, and other predictive tasks. The core idea behind RF is to construct multiple decision trees during training and aggregate their outputs to enhance overall performance. As an ensemble model, RF exhibits strong robustness, motivating us to assess its effectiveness in predicting PNS bounds.\nKey hyperparameters of RF include the number of trees (nestimators), maximum tree depth (max_depth), minimum samples required to split a node (min_samples_split), and the number of features considered for splitting (max_features). Increasing nestimators generally improves performance but at the expense of higher computational costs. The parameters max_depth, min_samples_split, and max_features regulate tree complexity, balancing bias-variance trade-offs.\nFor hyperparameter optimization, we employed a two-stage tuning strategy similar to that used for SVM. Table 1 also presents RF's MAE and MSE results, while Figures 3b and 3e show its confusion matrices. A more direct comparison with true PNS bounds is provided in Figures 2b and 2f. RF performs comparably to SVM on the lower bound but exhibits significantly higher accuracy on the upper bound.\n5.3 GRADIENT BOOSTING DECISION TREES\nGradient Boosting Decision Trees (GBDT) [Friedman, 2001] is an ensemble learning method that builds models sequentially, with each new tree correcting the errors of its predecessors. Unlike traditional boosting, GBDT optimizes pseudo-residuals, enabling flexible loss function optimization. Simple decision trees serve as weak learners, allowing GBDT to effectively capture complex data patterns.\nKey hyperparameters include the number of trees (nestimators), learning rate (learning_rate), maximum tree depth (max_depth), and subsample ratio (subsample). The learning rate determines each tree's contribution, while Nestimators and max_depth regulate model complexity and performance.\nFollowing the approach used for SVM and RF, we applied a two-stage tuning strategy. Again, table 1 presents the MSE and MAE results, while Figures 3c and 3f show the confusion matrices. A more direct comparison with true PNS bounds is provided in Figures 2c and 2g. GBDT demonstrates moderate performance on both the lower and upper bounds.\n5.4 TRANSFORMER\nThe Transformer [Vaswani, 2017], originally developed for Natural Language Processing, has expanded into Computer Vision and become a cornerstone of deep learning, particularly with the rise of large language models. Given its significant impact, this study also evaluates the Transformer for testing."}, {"title": "5.5 MULTILAYER PERCEPTRON", "content": "MLP [Rumelhart et al., 1986] consists of an input layer, one or more hidden layers, and an output layer. With appropriate activation functions, it can effectively model both linear and nonlinear relationships. As a fundamental structure in deep learning, MLP holds significant representativeness, motivating its inclusion in our experiments.\nA key consideration for MLP is the choice of activation function, particularly for predicting the lower bound. Since the lower bound of PNS cannot be negative, we initially selected the ReLU [Nair and Hinton, 2010] activation function (7). However, ReLU can lead to the loss of negatively correlated features, prompting us to adopt LeakyReLU [Maas et al., 2013] (8) as a complementary solution. Furthermore, given the considerable number of zero values in the data, the non-differentiability of ReLU and LeakyReLU at $s = 0$ imposes limitations on backpropagation. To address this, we proposed using Mish [Misra, 2019] (9) as an alternative activation function. The corresponding equations are:\n$ReLU(s) = max(0, s)$         (7)\n$LeakyReLU(s) = \\begin{cases}\ns, & \\text{if } s \\geq 0 \\\\\n\\alpha s, & \\text{if } s < 0\n\\end{cases}$          (8)\n$Mish(s) = s \\cdot tanh(ln(1 + e^s))$ (9)\nAdditionally, we implemented an MLP with the architecture 15\u219264\u219232 \u2192 16 \u2192 1, utilizing ReLU-like functions and Sigmoid as activation functions. The model was optimized using the Adam optimizer with a learning rate of 0.01 and trained for 1000 epochs.\nAgain, the final results are presented in Table 1. With the Mish activation function, the MLP achieved an MSE of 0.0011 on the lower bound and 0.0010 on the upper bound. For MAE, it attained 0.0225 on the lower bound and 0.0247 on the upper bound. Overall, MLP significantly outperformed other machine"}, {"title": "5.6 EXPERIMENTAL COMPARISON", "content": "As shown in Table 1, MLP delivers the best overall performance. Among the other four machine learning models, SVM performs well on the lower bound but fails almost entirely on the upper bound. RF shows significantly better results, achieving acceptable performance on both bounds. Despite also being a tree-based model, GBDT underperforms compared to RF, with only a slight improvement over SVM on the upper bound. The Transformer, as an MLP-based model, outperforms the other machine learning models but still falls short of MLP's performance.\nFor MLP models, the dataset's characteristics around zero (we will discuss these characteristics in the discussion section) lead to notable differences in activation function performance. Basic ReLU shows suboptimal performance on the lower bound, while LeakyReLU, which accounts for negative values, performs slightly better. Mish, which not only handles negative values but also ensures differentiability around zero, achieves the best results."}, {"title": "6 DISCUSSION", "content": "Our study demonstrates that among common machine learning methods, the MLP (Mish) is the most effective and accurate in estimating the bounds of PNS. Below, we discuss key considerations and future directions.\nFirst, in complex settings-especially those involving non-binary probabilities of causation, as indicated in [Li and Pearl, 2024a, Zhang et al., 2024]-the lower bounds of PNS often approach zero, further emphasizing the potential of MLP (Mish) in such scenarios.\nSecond, while our model successfully addresses the challenge of predicting from approximately 2,000 subpopulations to $2^{15}$ subpopulations, a significant issue remains: the 2,000 reliable training samples may require up to 50 million data points at the population level. This is due to data sparsity and the minimum data requirements needed to estimate PNS bounds for specific subpopulations (this paper uses 1,300 observational and experimental data points). However, collecting such a large volume of data is often impractical. Fortunately, we propose three practical approaches to data collection: instead of gathering population-level data, we can directly collect data for the 2,000 predefined subpopulations; the required 1,300 data points can be reduced to approximately 400, as suggested by Li et al. [2022c], while maintaining basic accuracy; and we can rely solely on experimental data, as proposed by Li and Pearl [2023], or use only observational data and estimate experimental data through adjustment formulas [Pearl, 1993, 1995].\nA fundamental goal in causal inference is identifying subpopulations exhibiting desirable counterfactual behavior patterns [Li and Pearl, 2019] to effectively guide policy-making and decision implementation. Specifically, this involves finding subpopulations with sufficiently small PNS upper bounds or sufficiently large PNS lower bounds. Therefore, accurately predicting all subpopulations is unnecessary for this task. A promising research direction is determining a minimal training set that can reliably predict these key subpopulations.\nNext, the current data-generating process consists of a relatively simple causal structure with only 20 confounders, making MLP the optimal choice. However, if the underlying causal structure were more complex or included additional confounding variables, would more sophisticated models be necessary? Since this paper focuses solely on illustrating the feasibility and potential of machine learning models for predicting causal quantities, a deeper exploration of the relationship between model complexity and causal structure remains an important direction for future research.\nFinally, traditional methods for estimating probabilities of causation, such as PNS, PS, and PN, rely on direct computation using observational and experimental data within SCMs. However, they assume sufficient data for each subgroup, which is often unrealistic. Our work addresses this challenge by developing a machine learning-based framework to predict PNS for subpopulations with insufficient data, a gap not covered by traditional causal inference techniques.\nAlthough there are no classical methods explicitly designed for this task, we establish a robust benchmark for evaluation by leveraging the known data-generating process in"}, {"title": "7 CONCLUSION", "content": "In this paper, we demonstrated that the bounds of probabilities of causation can be effectively learned and predicted using machine learning models. Specifically, we proposed five different models to predict the bounds of PNS. Experiments showed that an MLP with the Mish activation function achieved a mean absolute error of approximately 0.02 for an SCM with 15 observed and 5 unobserved confounders. Our results suggest that machine learning is a powerful tool for causal inference, particularly in real-world scenarios where direct estimation using SCM formulas is infeasible due to data limitations. Future research will explore larger datasets with more complex SCMs.\nAlthough our study demonstrates the feasibility of machine learning for estimating probabilities of causation, we acknowledge that our experiments are based on synthetic data generated from a structured SCM. Most existing research on probabilities of causation remains theoretical, often without practical validation, despite claims of real-world applicability. Due to page limitations, we could not extend our study to real-world applications, but this remains a critical direction for future research. We believe that bridging this gap will require developing datasets from real-world causal systems where experimental and observational data can be systematically collected. Our work serves as a first step in this direction, providing a foundation for future studies to explore the practical deployment of machine learning models for causality estimation."}, {"title": "A THE CAUSAL MODEL", "content": "The coefficients for Mx, My, and Cy were uniformly generated from the range [-1,1], while the parameters of the Bernoulli distribution were uniformly generated from [0, 1]. The detailed model is as follows:\n$Z_i=U_{Z_i}$, for $i \\in \\{1, ..., 20\\}$,\n$X = f_X(M_X, U_X)$\n$=\\begin{cases}\n1 & \\text{if } M_X + U_X > 0.5 \\\\\n0 & \\text{otherwise,}\n\\end{cases}$\n$Y = f_Y(X, M_Y, U_Y)$\n$=\\begin{cases}\n1 & \\text{if } 0 < C_Y \\cdot X + M_Y + U_Y < 1 \\\\\n1 & \\text{if } 1 < C_Y \\cdot X + M_Y + U_Y < 2 \\\\\n0 & \\text{otherwise.}\n\\end{cases}$\nwhere, $U_{Z_i}$, $U_X$, $U_Y$ are binary exogenous variables with Bernoulli distributions."}, {"title": "B DETAILED DATA GENERATING PROCESS", "content": "If all 20 binary features are observable, then for a given feature set z = (z1, ..., \u224820), the values of Mx and My are fixed (denoted as Mx(z) and My(z)). Under these conditions, the PNS, experimental distribution, and observational distribution corresponding to this set of features are:\n$PNS(z) = P(Y = 0_{X=0}, Y = 1_{X=1}|z)$\n$= P(U_Y = 0) \\cdot T_0 + P(U_Y = 1) \\cdot T_1$,\nwhere, $T_0 = \\begin{cases} 1 & \\text{if } f_Y(0, M_Y(z), 0) = 0 \\text{ and } f_Y(1, M_Y(z), 0) = 1, \\\\\n0 & \\text{otherwize,} \\end{cases}$\n$T_1 = \\begin{cases} 1 & \\text{if } f_Y(0, M_Y(z), 1) = 0 \\text{ and } f_Y(1, M_Y(z), 1) = 1, \\\\\n0 & \\text{otherwize.} \\end{cases}$\n$P(Y = 1|do(X), z) = P(U_Y = 0) \\cdot f_Y(X, M_Y(z), 0) + P(U_Y = 1) \\cdot f_Y(X, M_Y(z), 1)$.\n$P(Y = 1|X, z) = P(U_X = 0) \\cdot P(U_Y = 0) \\cdot f_Y(f_X(M_X(z), 0), M_Y(z), 0) + P(U_X = 0) \\cdot P(U_Y = 1) \\cdot f_Y(f_X(M_X(z), 0), M_Y(z), 1)) + P(U_X = 1) \\cdot P(U_Y = 0) \\cdot f_Y(f_X(M_X(z), 1), M_Y(z), 0)) + P(U_X = 1) \\cdot P(U_Y = 1) \\cdot f_Y(f_X(M_X(z), 1), M_Y(z), 1))$.\nWe assume that 15 of the features are observable (i.e., $Z_1, ..., Z_{15}$). This implies that each subpopulation, denoted as c = ($z_1, ..., z_{15}$), consists of 32 possible sets of 20 binary features. Specifically, these sets are: $s_0 = (z_1, ..., z_{15}, 0, 0, 0, 0, 0), s_1 = (z_1, ..., z_{15}, 0, 0, 0, 0, 1), s_2 = (z_1, ..., z_{15}, 0, 0, 0, 1, 0), ..., s_{31} = (z_1, ..., z_{15}, 1, 1, 1, 1, 1)$.\nUnder this setup, we obtain the $PNS_{subpopulation}$, experimental distribution, and observational distribution for any observed subpopulation c as follows:\n$PNS_{subpopulation}(c) = P(Y = 0_{X=0}, Y = 1_{X=1}|c)$\n$=\\frac{P(s_0)}{\\sum_{i=0}^{31}P(s_i)}PNS(s_0) + \\frac{P(s_1)}{\\sum_{i=0}^{31}P(s_i)}PNS(s_1) + \\frac{P(s_2)}{\\sum_{i=0}^{31}P(s_i)}PNS(s_2) + ... + \\frac{P(s_{31})}{\\sum_{i=0}^{31}P(s_i)}PNS(s_{31})$\n$=\\frac{P(Z_{16} = 0)P(Z_{17} = 0)P(Z_{18} = 0)P(Z_{19} = 0)P(Z_{20} = 0)}{\\sum_{i=0}^{31}P(s_i)}PNS(s_0) + \\frac{P(Z_{16} = 0)P(Z_{17} = 0)P(Z_{18} = 0)P(Z_{19} = 0)P(Z_{20} = 1)}{\\sum_{i=0}^{31}P(s_i)}PNS(s_1) + ... + \\frac{P(Z_{16} = 1)P(Z_{17} = 1)P(Z_{18} = 1)P(Z_{19} = 1)P(Z_{20} = 1)}{\\sum_{i=0}^{31}P(s_i)}PNS(s_{31})$.\n$P(Y = 1|do(X), c) = \\frac{P(Z_{16} = 0)P(Z_{17} = 0)P(Z_{18} = 0)P(Z_{19} = 0)P(Z_{20} = 0)}{\\sum_{i=0}^{31}P(s_i)}P(Y = 1|do(X), s_0) + \\frac{P(Z_{16} = 0)P(Z_{17} = 0)P(Z_{18} = 0)P(Z_{19} = 0)P(Z_{20} = 1)}{\\sum_{i=0}^{31}P(s_i)}P(Y = 1|do(X), s_1) + \\frac{P(Z_{16} = 0)P(Z_{17} = 0)P(Z_{18} = 0)P(Z_{19} = 1)P(Z_{20} = 0)}{\\sum_{i=0}^{31}P(s_i)}P(Y = 1|do(X), s_2) + ... + \\frac{P(Z_{16} = 1)P(Z_{17} = 1)P(Z_{18} = 1)P(Z_{19} = 1)P(Z_{20} = 1)}{\\sum_{i=0}^{31}P(s_i)}P(Y = 1|do(X), s_{31})$.\n$P(Y = 1|X, c) = \\frac{P(Z_{16} = 0)P(Z_{17} = 0)P(Z_{18} = 0)P(Z_{19} = 0)P(Z_{20} = 0)}{\\sum_{i=0}^{31}P(s_i)}P(Y = 1|X, s_0) + \\frac{P(Z_{16} = 0)P(Z_{17} = 0)P(Z_{18} = 0)P(Z_{19} = 0)P(Z_{20} = 1)}{\\sum_{i=0}^{31}P(s_i)}P(Y = 1|X, s_1) + \\frac{P(Z_{16} = 0)P(Z_{17} = 0)P(Z_{18} = 0)P(Z_{19} = 1)P(Z_{20} = 0)}{\\sum_{i=0}^{31}P(s_i)}P(Y = 1|X, s_2) + ... + \\frac{P(Z_{16} = 1)P(Z_{17} = 1)P(Z_{18} = 1)P(Z_{19} = 1)P(Z_{20} = 1)}{\\sum_{i=0}^{31}P(s_i)}P(Y = 1|X, s_{31})$.\nThe true bounds of the $PNS_{subpopulation}(c)$ can be obtained using Equations 1 and 2, along with the above observational and experimental distributions."}, {"title": "C CODE", "content": "All code for data generation and machine learning models is available at the following anonymous link: https:// anonymous.4open.science/r/2025uai-ED50/."}]}