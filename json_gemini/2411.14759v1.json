{"title": "HAMMER: TOWARDS EFFICIENT HOT-COLD DATA\nIDENTIFICATION VIA ONLINE LEARNING", "authors": ["Kai Lu", "Siqi Zhao", "Jiguang Wan"], "abstract": "Efficient management of storage resources in big data and cloud computing environments requires\naccurate identification of data's \"cold\" and \"hot\" states. Traditional methods, such as rule-based\nalgorithms and early AI techniques, often struggle with dynamic workloads, leading to low accuracy,\npoor adaptability, and high operational overhead. To address these issues, we propose a novel\nsolution based on online learning strategies. Our approach dynamically adapts to changing data\naccess patterns, achieving higher accuracy and lower operational costs. Rigorous testing with both\nsynthetic and real-world datasets demonstrates a significant improvement, achieving a 90% accuracy\nrate in hot-cold classification. Additionally, the computational and storage overheads are considerably\nreduced.", "sections": [{"title": "Introduction", "content": "In the contemporary landscape of big data and cloud computing, the efficient management of storage resources has\nemerged as a paramount concern. One of the most critical aspects of this challenge is the accurate identification of\ndata's \"cold\" and \"hot\" states. Data is classified as \"hot\" if it is frequently accessed, necessitating fast and readily\navailable storage solutions. Conversely, \"cold\" data, which is rarely accessed, can be stored more cost-effectively in\nslower, less expensive storage mediums. Effective hot-cold identification not only optimizes storage costs but also\nenhances system performance by ensuring that the most relevant data is quickly accessible[1, 2, 3].\n\nThe importance of hot-cold data identification cannot be overstated. In large-scale data centers, the ability to distinguish\nbetween hot and cold data can lead to substantial savings in both storage and energy costs[4]. Moreover, it plays a\ncrucial role in improving the overall efficiency of data retrieval processes, thereby enhancing user experience and\nsystem responsiveness[5]. As data volumes continue to grow exponentially, the need for robust and adaptive hot-cold\nidentification mechanisms becomes increasingly evident[6].\n\nTraditional approaches to cold-hot identification have primarily relied on rule-based algorithms and early artificial\nintelligence (AI) techniques. Rule-based methods often involve predefined criteria for classifying data as hot or cold,\nsuch as the number of accesses within a specific time frame[7]. These methods are relatively simple to implement and\ncan be effective in stable environments. However, they suffer from several limitations. Under dynamic workloads,\nwhere access patterns change frequently, the accuracy of rule-based methods can significantly degrade[8]). Additionally,\ntheir lack of adaptability means they cannot easily adjust to new types of data or usage scenarios, leading to suboptimal\nperformance[9].\n\nMore advanced traditional strategies incorporate AI to improve upon the limitations of rule-based systems. These\nAI-based methods typically use machine learning models trained on historical data to predict future access patterns[10].\nFor example, decision trees and neural networks have been employed to classify data based on various features such as\naccess frequency and recency[11, 12, 13, 14, 15]. Despite these advancements, these approaches still face challenges in\nreal-time adaptation and handling large-scale datasets efficiently[16].\n\nProblems with traditional strategies. One of the primary issues with traditional hot-cold identification strategies\nis their low accuracy under dynamic loads. As data access patterns evolve, the models used in these strategies may"}, {"title": "Motivation", "content": "Rule-Based Algorithms. Traditional hot-cold identification methods often rely on rule-based algorithms, which use\npredefined criteria to classify data. One of the most well-known rule-based algorithms is the Least Recently Used\n(LRU) policy. LRU evicts the least recently accessed data items from the cache, assuming that infrequently accessed\ndata is likely to remain cold. While LRU is simple and effective in stable environments, it can struggle with dynamic\nworkloads where access patterns change frequently[22]. Another common rule-based method is the Least Frequently\nUsed (LFU) policy, which tracks the frequency of data access and evicts the least frequently accessed items. LFU is\nmore resilient to temporal variations in access patterns but can be computationally expensive due to the need to maintain\nand update access counters[23].\n\nHybrid approaches combine multiple rule-based policies to improve accuracy and adaptability. For example, the\nAdaptive Replacement Cache (ARC) combines elements of LRU and LFU to provide better performance under varying\nworkloads[24]. ARC maintains two lists of recently and frequently accessed items and dynamically adjusts the size of\nthese lists based on the observed access patterns. However, hybrid approaches can still be limited by their reliance on\nfixed rules and may not adapt well to highly dynamic environments.\n\nLearned-Based Algorithms. With the advent of machine learning, more sophisticated methods have been developed\nto identify cold and hot data[16, 9]. Decision trees, random forests, and support vector machines (SVMs) have been\nused to classify data based on features such as access frequency, recency, and file size[10]. These models can capture\ncomplex relationships in the data and provide more accurate predictions than rule-based methods. However, they often\nrequire extensive training on historical data and may not adapt well to new or changing patterns."}, {"title": "Limitations of Current Solutions", "content": "Accuracy Issues: Concept Drift. One of the primary challenges with existing hot-cold identification methods is the\nconcept drift problem[27, 28, 29]. Concept drift occurs when the statistical properties of the target variable, which the\nmodel is trying to predict, change over time in unforeseen ways. This is particularly prevalent in dynamic environments\nwhere data access patterns can change rapidly and unpredictably.\n\n1) Rule-Based Algorithms: Rule-based methods like LRU and LFU are static and do not adapt to changes in data\naccess patterns. As a result, they can become less accurate over time, especially in environments with frequent and\nunpredictable changes in workload[30, 31, 12].\n\n2) Machine Learning Models: While machine learning models can capture complex patterns in data, they are often\ntrained on historical data and may not generalize well to new or changing patterns. This can lead to a decrease in\naccuracy as the data distribution evolves [10, 13].\n\n3) Reinforcement Learning: Reinforcement learning algorithms can adapt to changes in the environment, but they may\nrequire significant time to converge to optimal policies, especially in highly dynamic settings[32, 33].\n\nOverhead Issues: Metadata Explosion. Another significant challenge is the overhead associated with managing\nmetadata, which can become a bottleneck in large-scale systems.\n\n1) Rule-Based Algorithms: Rule-based methods often require maintaining and updating metadata, such as access\ncounters and timestamps. As the volume of data increases, the overhead of managing this metadata can become\nsubstantial, leading to performance degradation[34].\n\n2) Machine Learning Models: Machine learning models, particularly deep learning models, can generate a large\namount of metadata during the training and inference processes. This includes model parameters, feature vectors, and\nintermediate results, which can consume significant storage and computational resources[11].\n\n3) Reinforcement Learning: Reinforcement learning algorithms often require storing and processing large amounts of\nstate-action pairs and reward signals, which can lead to metadata explosion. This can be particularly problematic in\nreal-time systems where quick decisions are necessary [35]."}, {"title": "Design", "content": "Hammer Overview\nA data hot/cold identification system has been designed based on the data access behaviour and system state information,\nas illustrated in Fig. 1. The system employs online learning algorithms to assess and categorise data as hot or cold in\nreal-time during extensive data processing operations. Additionally, it facilitates multi-path selection for intelligent I/O\nand optimises data layout for heterogeneous memory or storage systems. The system is structured into three principal\nmodules: feature extraction, data heat prediction and online heat judgement."}, {"title": "Feature extraction", "content": "In order to accurately predict the hot and cold states of data, it is essential to extract key features from the storage\naccess behaviour. Such features should encompass not only data flow information, but also control flow information.\nFurthermore, system information is an essential component. This multi-dimensional feature extraction approach offers\na more comprehensive view, enabling the model to comprehend the intricacies of data access behaviour and identify the\npivotal factors influencing the hot and cold states of data. The heat feature extraction module constructs feature vectors\nto characterise data heat based on information such as application behaviour and system state. The information sources\nare divided into three main categories: data flow, control flow, and system information.\n\nData flow information: Data flow information serves as the foundation for understanding access patterns. It includes\nparameters such as the access address, address difference, and operation size. By analyzing these parameters, we can\ngain insights into access behavior, such as identifying which data are accessed frequently and how they are accessed\nover time. This information is crucial for predicting data hotness and can help optimize data storage and retrieval\nstrategies.\n\nControl flow information: Control flow information aids in understanding the execution context of data access. It\nencompasses elements like the program counter (PC), PC difference, and operation type. By examining control flow, we\ncan discern access tendencies, such as whether certain files are accessed sequentially or randomly. This understanding\ncan inform decisions about data caching and prefetching, ultimately improving system performance.\n\nSystem information: System information provides additional context that influences data access patterns. It includes\nmetrics such as the CPU occupancy of the migration process, bandwidth usage, and the frequency of memory thrashing\nbehavior. These factors can affect how data is accessed and processed, and understanding them allows for better\nresource allocation and management. For instance, a high CPU occupancy by the migration process, coupled with\nsevere memory thrashing, may indicate data misplacement. In such cases, it suggests that the current distribution of\ndata across the storage hierarchy is suboptimal, necessitating adjustments in data migration strategies to better align\ndata with the access patterns and reduce thrashing. This proactive approach to data management can lead to enhanced\nsystem performance and efficiency.\n\nThe idea can be applied across various system architectures and environments to optimize data handling and processing.\nWhen deployed in communication-intensive scenarios, the algorithm leverages the proximity to the communication\nendpoint to offload the data collection process. This strategy is particularly effective because it capitalizes on the ease\nof data acquisition at the communication endpoint, reducing the overhead on the main system and allowing for more\nefficient use of resources.\n\nData Collection at Communication Endpoints: By offloading data collection to the near-communication endpoint,\nthe system can take advantage of the low-latency access to data streams. This approach minimizes the need for data\ntransmission back to a central processing unit, thus reducing bandwidth consumption and latency. The algorithm can be\ntailored to collect specific data access address and size information directly from the endpoint. This information is\ncrucial for understanding the data flow patterns and can be used to optimize data caching and prefetching strategies.\nThe historical memory mechanism introduced by the algorithm ensures that the PC values, which are critical for control\nflow analysis, are captured at the moment of communication initiation. This mechanism provides a historical context to\nthe data access patterns, enabling the system to make more informed decisions about data management.\n\nApplication in Heterogeneous Memory or Storage Systems: In more complex systems, such as heterogeneous\nmemory or storage environments, the algorithm employs the Performance Monitoring Counter (PMC) sampling method\nfor data collection. These systems often involve multiple types of memory or storage media with different performance\ncharacteristics, which can make tracking each access behavior computationally expensive and complex. The PMC\nmethod allows for efficient monitoring by sampling the access behavior at regular intervals, thus reducing the monitoring\noverhead. This method is particularly useful in systems where the cost of tracking each access behavior is high, as it\nprovides a balance between the detail of the collected data and the resources required for monitoring."}, {"title": "Online Classification and Evaluation", "content": "The design of the online learning classifier is intricately tailored to meet the demands of modern data storage systems.\nThis system is designed to handle a substantial and unceasing flow of requests, necessitating a solution that operates\nonline, processes large volumes of requests continuously, and adapts to the evolving attributes of data over time. The\nonline learning classifier is architected with a online classification and online evaluation process, as depicted in Fig. 2.\n\nInitial Prediction Stage: The first stage involves the online classifier generating predicted classification results based\non past training as soon as a new data access occurs. This design ensures that the system can provide immediate\nfeedback, which is crucial for real-time data processing systems.\n\nFeature Vector Extraction: Following the initial prediction, the feature vector associated with the new data access is\nextracted. This vector encapsulates the essential characteristics of the data access, including but not limited to, data\nflow, control flow, and system information.\n\nEvaluation Queue Management: The extracted feature vector is then added to an evaluation queue, which operates on\na first-in-first-out (FIFO) principle. This queue has a finite capacity, ensuring that only the most recent data accesses are\nretained for evaluation. This design is pivotal for maintaining the system's responsiveness and efficiency.\n\nHeat Index Update: While the feature vector is in the evaluation queue, the system accesses the corresponding part of\nthe memory and updates the heat index. This index is a critical metric that reflects the recency and frequency of data\naccess, thus influencing data storage and retrieval strategies.\n\nIn this study, a Sketch-Min counting-based online method is employed for the evaluation of data heat. The online\nevaluation algorithm employs a hashing algorithm and a compact data structure to efficiently estimate the number of\nelements in a high-speed, massive data stream while maintaining a low space overhead. The principal advantage of this\ntechnique is its capacity to theoretically guarantee an equilibrium between estimation accuracy and memory usage.\n\nThe particular configuration of the online evaluation algorithm is illustrated in Fig. 3. The algorithm achieves the\nestimation of the number of data accesses by constructing a hash group consisting of D hash functions and allocating W\nfinite hash slots to each hash function. Upon the arrival of a new access record, the count value of each hash function\nmapped position is increased. Once the access record is removed from the evaluation queue, the actual count value\nof the element is determined by the minimum of the count values of multiple hash function mapped positions. In the\ncontext of large address spaces and massive or even infinite online memory access records, the Sketch-Min counting\nmethod effectively controls the total amount of storage, providing an estimate of the number of data accesses with\nlimited storage overhead.\n\nOnline Training with Real Labels: Once the queue is full and the feature vector is no longer in the queue, the real\nlabel of the data access is obtained through the online evaluation algorithm. This real label is then fed back into the"}, {"title": "Dynamic Threshold Tuning", "content": "In order to respond flexibly to changes in application memory behaviour that are not predictable in advance, we adopt a\ndynamic hot/cold threshold as a key criterion for determining the hot/cold status of data. The setting of this threshold\nis based on the percentile value of the hotness of the cells that have recently exited from the evaluation queue and is\naffected by the usage of the system's slow tier and the memory thrashing behaviour. The setting of the percentile value\ndetermines the sensitivity of identifying hot data: the smaller the value, the greater the tendency to identify hot data.\nThe designation of data as hot data may enhance the utilisation of the hot tier, but may also precipitate thrashing issues\nor an oversupply of the hot tier's resources. Conversely, the designation of data as cold data may help to circumvent\nthrashing and resource overload, but may also result in the underutilisation of the thermal layer resources.\n\nThe precise methodology for adjusting the dynamic threshold is illustrated in Algorithm 1. The initial threshold is\nestablished as the ratio of the physical capacity of the hot and cold layers, and the dynamic threshold adjustment\nmechanism will be triggered at regular intervals as the system operates. In the process of adjusting the threshold, a\nnumber of factors are taken into account, including the CPU occupancy of the hot and cold sensing and migration\nmodules, the usage of the slow layer, and the frequency of recent memory thrashing behaviour. The dynamic threshold\n(i.e. the new percentile) is inversely correlated with the slow tier usage and directly correlated with the memory\nthrashing behaviour. To quantify this relationship, two empirical coefficients, $\u03b1$ and $\u03b2$, are introduced to characterise\nthe effect of slow layer usage and memory thrashing behaviour on threshold adjustment, respectively.\n\nFurthermore, the dynamic threshold adjustment is constrained by the CPU occupancy of the hot and cold modules. In\ninstances where the hot and cold sensing and threshold adjustment are monitored to be excessive for the CPU occupancy,\nthe system will consider raising the thresholds to reduce the impact on the foreground applications and ensure the\nstability and responsiveness of the system. This dynamic adjustment strategy enables the hot and cold thresholds\nto adaptively respond to changes in the system state, thus guiding data storage and migration decisions with greater\nprecision."}, {"title": "Evaluation", "content": "The evaluation of our online learning-based classification system was conducted using a rigorous and comprehensive\napproach to ensure the accuracy and effectiveness of our algorithm. The dataset utilized in our evaluation was collected\nin real-world workloads using the Drmemtrace tool, a powerful utility designed for tracing and analyzing memory\naccess patterns in various applications.\n\nData Collection and Preparation. We instrumented the Drmemtrace tool to attach a statistical module to the target\nprocesses, sampling memory access instructions at a rate of 10%. This approach allowed us to capture a representative\nsubset of the memory access behavior without incurring excessive overhead. The applications selected for data\ncollection spanned four diverse domains: artificial intelligence computations, big data processing, graph computations,\nand high-performance computing. These domains were chosen for their distinct memory access patterns and the\ncomplexity they introduce to data classification algorithms. The details of workloads are shown in Table 1.\n\nSimulation and Algorithm Assessment. To simulate a system running multiple workloads continuously, we con-\ncatenated the memory access behaviors collected from all categories into a continuous record. This method was\nemployed to validate the algorithm's ability to adapt to concept drift, a phenomenon where the statistical properties of\nthe data change over time. Using a custom simulator, we replayed the collected memory access records and conducted\nonline predictions to evaluate the accuracy of our hot and cold data classification algorithm. We employed a rigorous\nevaluation protocol that included precision and F1 Score as primary metrics. For establishing a baseline, we used the\nLeast Recently Used (LRU) 2Q algorithm, a well-established method for differentiating between hot and cold data\nbased on the presence of access addresses in the LRU hot queue. This baseline served as a reference point to assess the\nperformance of our online learning-based classifiers."}, {"title": "Evaluation Results", "content": "We implemented and tested several classifiers based on classic online learning algorithms. Notably, we included a\nprobabilistic Bayes classifier, a decision tree-based Hoeffding adaptive tree, and an Adaptive random forest (ARF)\nclassifier. Among these, the ARF classifier demonstrated exceptional performance in terms of accuracy and F1 Score.\n\nThe ARF classifier's superior performance can be attributed to its ability to adapt to the evolving data characteristics in\nreal-time. Unlike traditional batch learning methods, which require periodic retraining to adjust to concept drift, the\nARF classifier continuously updates its model with each new instance, ensuring that it remains highly responsive to\nchanges in the data.\n\nTo further validate our findings, we performed a series of cross-validation tests, dividing the dataset into multiple folds\nand training the classifiers on different subsets. This approach allowed us to assess the generalizability of our classifiers\nand their robustness to variations in the data.\n\nWe employed statistical methods to determine the significance of the performance differences observed between the\nclassifiers. Using paired t-tests, we compared the mean accuracy and F1 Score of the ARF classifier against the baseline\nLRU 2Q algorithm. The results indicated that the performance improvements were statistically significant, with p-values\nwell below the conventional threshold of 0.05.\n\nTo underscore the efficacy of online learning in the context of data storage systems, we conducted a thorough comparative\nanalysis against traditional batch learning methodologies. The traditional batch learning approach was tested by first\ndividing the collected dataset using an 80-20 split method, where 80% of the data was used for training and the\nremaining 20% for testing.\n\nThe comparative results between the online and batch learning models are compelling. The online learning model\ndemonstrated superior performance in terms of Accuracy and F1 Score, which are standard benchmarks for classification\nmodels. The batch learning model, while achieving high initial Accuracy and F1 Score, showed a decline in performance\nas the data distribution evolved over time. This decline was only arrested when the model was retrained with updated\ndata, which is a time-consuming process.The result is shown in xxx.\n\nThe results of our testing revealed a significant difference in performance between the online and batch learning models.\nThe online ARF classifier demonstrated a higher Accuracy and F1 Score, which was consistently maintained as new\ndata was introduced. This was particularly evident in scenarios where the data exhibited concept drift, where the online\nmodel was able to adapt and maintain its performance, whereas the batch model showed a decline in performance until\nit was retrained."}, {"title": "Conclusion", "content": "Our approach, leveraging online learning techniques, is designed to transcend the constraints of conventional method-\nologies by swiftly adjusting to evolving data access patterns. This dynamic adaptation allows our solution to maintain\nsuperior accuracy while also minimizing operational expenditures. Extensive evaluations utilizing a mix of synthetic\nand real-world datasets have confirmed a noteworthy enhancement, with an achieved accuracy rate of 90%. Our method\nhas shown to be more cost-effective, with lower operational expenditures, positioning it as an attractive option for\ncontemporary data storage systems."}]}