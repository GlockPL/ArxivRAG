{"title": "Does Your AI Agent Get You? A Personalizable Framework for Approximating Human Models from Argumentation-based Dialogue Traces", "authors": ["Yinxu Tang", "Stylianos Loukas Vasileiou", "William Yeoh"], "abstract": "Explainable AI is increasingly employing argumentation methods to facilitate interactive explanations between AI agents and human users. While existing approaches typically rely on predetermined human user models, there remains a critical gap in dynamically learning and updating these models during interactions. In this paper, we present a framework that enables AI agents to adapt their understanding of human users through argumentation-based dialogues. Our approach, called Persona, draws on prospect theory and integrates a probability weighting function with a Bayesian belief update mechanism that refines a probability distribution over possible human models based on exchanged arguments. Through empirical evaluations with human users in an applied argumentation setting, we demonstrate that Persona effectively captures evolving human beliefs, facilitates personalized interactions, and outperforms state-of-the-art methods.", "sections": [{"title": "Introduction", "content": "As AI systems become more integrated into real-world applications, the need for transparency and trust in human-AI interactions grows. Explainable AI (XAI) addresses this need by focusing on generating understandable explanations for human users that foster trust and accountability (Gunning and Aha 2019). A key paradigm within XAI is argumentation (\u010cyras et al. 2021), which enables interactive, dialogue-based explanation processes between AI agents and human users. These processes offer improved clarity and foster stronger human-AI interactions.\n\nA core assumption in most existing argumentation-based XAI work is that the AI agent has a static, deterministic model of the human user that it uses in its deliberative processes. While assuming the AI agent has access to an a-priori human model has its advantages (Sreedharan, Chakraborti, and Kambhampati 2021; Vasileiou, Previti, and Yeoh 2021), this approach often falls short of capturing the intricate complexities of real-world interactions. It is not only likely that humans hold beliefs at different levels of granularity and with varying degrees of certainty, but also that their beliefs evolve dynamically over time. Such simplifications can lead to significant misalignments between AI agents and human users (Russell 2019), as the AI agent might base its decisions or explanations on an inaccurate or incomplete understanding of the human.\n\nAs a step towards addressing this issue, in this paper, we propose a novel approach that enables AI agents to adapt their decisions and explanations based on a dynamic understanding of human mental states. Our method represents human models as probability distributions that are continuously refined through ongoing argumentative interactions, building upon established frameworks in computational argumentation (Gordon 1994; Parsons, Wooldridge, and Amgoud 2003; Prakken 2006; Hunter 2015, 2016; Rago, Li, and Toni 2023; Vasileiou et al. 2024). Our proposed framework, Personalized Human Model Approximations (Persona), integrates two key components to achieve this goal. First, it employs a Bayesian belief update mechanism that systematically refines the human model based on observed interaction patterns. Second, it incorporates a probability weighting function derived from prospect theory (Tversky and Kahneman 1992), which accounts for human tendencies to overweight low probabilities and underweight high probabilities in decision-making contexts. This dual approach enables Persona to offer personalized interactions by capturing individual differences in how users evaluate probabilistic information during argumentative exchanges.\n\nFurthermore, we conduct an extensive evaluation of our approach using real argumentation-based dialogue traces collected via a human-subject study. Our results show that Persona not only effectively captures and updates human models, but it also outperforms existing state-of-the-art argumentation-based methods.\n\nThe main contributions of this paper are as follows:\n\u2022 We introduce Persona, a novel framework for approximating and updating a probabilistic human model through argumentation-based dialogue traces. Our framework incorporates a prospect-theory-inspired probability weighting function with a Bayesian belief update mechanism.\n\u2022 We conduct a human-subject study on an argumentation-based dialogue scenario and collect dialogue traces involving human users. We empirically evaluate the effectiveness of our approach on these traces and demonstrate its ability to capture evolving human models and facilitate"}, {"title": "Related Work", "content": "Argumentation-based Dialogues\nAccording to the influential work by Walton and Krabbe (1995), dialogues can be categorized based on the knowledge of the participants, the objectives they wish to achieve through the dialogue, and the rules that are intended to govern the dialogue. Contextual to each type, each dialogue revolves around a topic, typically a proposition, that is the subject matter of discussion. Related dialogue types include: Persuasion (Gordon 1994; Prakken 2006), where an agent attempts to convince another agent to accept a proposition they initially do not hold; information-seeking (Parsons, Wooldridge, and Amgoud 2003; Fan and Toni 2012), where an agent seeks to obtain information from another agent believed to possess it; and inquiry (Hitchcock and Hitchcock 2017; Black and Hunter 2009), where two agents collaborate to find a joint proof for a query that neither could prove individually. The advent of argumentative dialogue-based systems (Black, Maudet, and Parsons 2021) illustrates the great potential of argumentation for collaborative decision-making and consensus-building in human-AI interaction settings. However, these approaches often neglect the dynamic nature of belief updating during dialogues.\n\nOn a similar thread, our work fits well within the literature on argumentation-based explainable AI (Fan and Toni 2015; Shams et al. 2016; Fan 2018; Collins, Magazzeni, and Parsons 2019; Bud\u00e1n et al. 2020; Dennis and Oren 2022; Rago, Li, and Toni 2023; Vasileiou et al. 2024). While these approaches provide a solid foundation for argumentation-based explanations, they do not explicitly focus on approximating the human users model, which is central to this paper.\nHuman Model Approximation\nAccurate human models are crucial for effective human-AI interactions. In argumentation, several approaches have emerged. Rienstra, Thimm, and Oren (2013) proposed a probabilistic opponent model for move selection based on perceived awareness. Hadjinikolis et al. (2013) explored dialogue history analysis to predict opponent arguments. Hadoux et al. (2015) introduced probabilistic finite state machines and partially observable Markov decision processes for modeling dialogue progression under uncertainty.\n\nIn other domains, various approaches to human model approximation exist. Deep learning has been used to simulate and predict human behavior from large datasets (Hamrick 2019; Lake et al. 2017). Game-theoretic models reveal how agents' mental states affect choices and strategies in competitive scenarios (Yoshida, Dolan, and Friston 2008; Camerer 2011). Planning formalisms have been utilized to learn human models in human-AI interaction settings (Sreedharan, Chakraborti, and Kambhampati 2018; Sreedharan, Srivastava, and Kambhampati 2018; Black, Coles, and Bernardini 2014). The problem of learning human preferences has also been extensively studied, particularly in recommendation systems (F\u00fcrnkranz and H\u00fcllermeier 2010). Preferences are often elicited via ranking or comparisons (Ailon 2012; Wirth et al. 2017), or reinforcement learning paradigms (Wilson, Fern, and Tadepalli 2012; B\u0131y\u0131k, Talati, and Sadigh 2022).\n\nMost relevant to our work are those by Hunter (2013, 2015, 2016), which present methods for representing and updating human beliefs through probability distributions during persuasion dialogues. While they provided essential theoretical groundwork, our approach extend them by incorporating insights from prospect theory and introducing personalized modeling capabilities that account for individual differences in probability assessment.\n\nAs our approach is specifically designed for approximating human models in argumentation-based dialogues, we compare it to the most relevant work in this space, namely the work by Hunter (2015, 2016). We do not compare it against non-argumentation approaches, as they lack the specific structures and mechanisms necessary for handling structured arguments and belief updates in dialogue settings. Our focus on argumentative reasoning and uncertainty in dialogues requires specialized techniques that these general approaches do not provide."}, {"title": "Background", "content": "We will use classical propositional logic to describe aspects of the world. Consider a finite (propositional) language \\(L\\) that utilizes the classical entailment relation, represented by \\(\\models\\). The set of models (i.e., possible words) of \\(L\\) is denoted by \\(M\\), where each model \\(m_i \\in M\\) is an assignment of true or false to the formulae of \\(L\\) defined in the usual way for classical logic. For \\(\\phi \\in L\\), let \\(Mod(\\phi) = \\{ m_i \\in M \\mid m_i \\models \\phi \\}\\) denote the set of all models of \\(\\phi\\).\n\nBuilding on a propositional language \\(L\\), we model the uncertainty of arbitrary formulae using a probability distribution over the models \\(M\\) of \\(L\\):\nDefinition 1 (Probability Distribution). Let \\(M\\) be the set of models of the language \\(L\\). A probability distribution \\(P\\) on \\(M\\) is a function \\(P : M \\rightarrow [0, 1]\\) such that \\(\\sum_{m \\in M} P(m) = 1\\).\n\nIn essence, the probability distribution allows an agent to create a ranking between possible words with respect to how likely they are to be true. This then allows the agent to compute the probability of a formula as follows:\nDefinition 2 (Probability of Formula). Let \\(M\\) be the set of models of language \\(L\\) and \\(P\\) a probability distribution over \\(M\\). The probability of formula \\(\\phi \\in L\\) is \\(P(\\phi) = \\sum_{m \\models \\phi} P(m)\\).\n\nArgumentation-based Dialogues: In an argumentation-based dialogue, agents take turns exchanging arguments that prove (or disprove) specific claims, where the structure and relationships between these arguments are governed by the underlying argumentation semantics (Black, Maudet, and Parsons 2021). In this paper, we consider the semantics of structured (deductive) argumentation (Besnard and Hunter 2014), where each argument is constructed using formulae from language \\(L\\). Formally,\nDefinition 3 (Argument). Let \\(L\\) be the language and \\(\\phi \\in L\\) a formula. Then,"}, {"title": "Approximating Human Models", "content": "We now introduce our framework that enables an agent to progressively update its approximation of the human model through argumentation-based dialogue traces.\n\nProblem Setting and Assumptions: Our setting consists of an AI agent (denoted \\(\\alpha\\)) interacting with a human user (denoted \\(\\eta\\)) via an argumentation-based dialogue. We make the following key assumptions:\n\u2022 Shared Domain Language: Both \\(\\alpha\\) and \\(\\eta\\) have access to and communicate in the same language \\(L\\) using a shared vocabulary of atomic variables. This allows them to construct domain-specific formulae.\n\u2022 Probabilistic Human Model: The human model is represented as a probability distribution \\(P^t_\\eta\\) over the possible models \\(M\\) of \\(L\\) at each timestep \\(t_i\\). Initially, we assume a uniform distribution: \\(P^0_\\eta (m) = \\frac{1}{|M|}\\) for all \\(m \\in M\\).\n\u2022 Dialogue Traces: We have access to (finite) dialogue traces \\(T\\) produced by argumentation-based dialogues between \\(\\alpha\\) and \\(\\eta\\) (Vasileiou et al. 2024).\n\nIn real-world argumentation, arguments often come with some degree of uncertainty. To capture this, we associate a probability \\(p(A_i)\\) with each argument \\(A_i\\) in the dialogue trace. It is crucial to note that these probabilities represent uncertainty from the perspective of the human user, that is, how likely the human thinks that the argument is true.\u00b9\nUpdating the Human Model\nGiven a dialogue trace \\(T\\), we employ a Bayesian approach to update the agent's probability distribution \\(P^h\\) over possible human models. At each timestep \\(t_i\\), when an argument \\(A_i\\) is presented, we perform the following update:\n\\[\nP^{t_i}(m) = \\begin{cases}\n\\frac{P^{t_{i-1}}(m)}{\\sum_{m \\models A_i} P^{t_{i-1}}(m)} \\cdot p(A_i) & \\text{if } m \\models A_i \\\\\n\\frac{P^{t_{i-1}}(m)}{ \\sum_{m \\not\\models A_i} P^{t_{i-1}}(m)} \\cdot (1 - p(A_i)) & \\text{if } m \\not\\models A_i\n\\end{cases}\n\\]\n\nThis update mechanism increases the probability of human models that are consistent with the presented argument, weighted by the argument's associated probability \\(p(A_i)\\). Models that are inconsistent with the argument have their probabilities decreased accordingly.\nA More Personalized Approach to Uncertainty Estimation: While the Bayesian update approach provides a solid foundation for estimating the human model, it does not account for the subjective nature of how humans perceive and think about uncertainty. To address this, we introduce a more personalized approach based on prospect theory (Kahneman and Tversi 1979), which allows us to capture individual differences in how humans evaluate probabilities in argumentative contexts.\u00b2\n\nWe propose the following probability weighting function to model the relationship between \u201cactual\u201d probabilities"}, {"title": "Human-Subject Study Description", "content": "We simulated a scenario where participants interacted with an AI assistant named Blitzcrank to evaluate the suitability of a fictional venue, Luminara Gardens, for a company team-building event. This scenario was chosen to provide a concrete context for argumentation while being accessible to a general participant pool.\n\nThe study consisted of a series of interaction rounds (maximum 5) between each participant and Blitzcrank. Each round followed this structure:\n\u2022 Blitzcrank presented an argument about Luminara Gardens' suitability.\n\u2022 Participants rated their confidence in Blitzcrank's argument on a five-point scale: Very low (0.1), low (0.3), average (0.5), high (0.7), or very high (0.9).\n\u2022 Participants selected and presented a counterargument to Blitzcrank from a set of three options, each associated with a confidence level.\n\u2022 Participants ranked four different perspectives (i.e., models) on Luminara Gardens' suitability.\n\nThe dialogue continued for up to five rounds, with the option to end earlier if agreement was reached.\nData Collection: We recruited 200 participants via the Prolific platform (Palan and Schitter 2018), ensuring a diverse sample.\u00b3 Participants were required to be fluent in English and were compensated USD 4.00 for their time. After applying attention checks and coherence filters, we retained data from 184 participants for analysis. For each participant i, we collected:\n\u2022 Dialogue traces \\(T_i = ((A_1, x_1, \\sigma_1)^{t_1}, ..., (A_{n_i}, x_{n_i}, \\sigma_{n_i})^{t_{n_i}})\\), where \\(n_i \\in \\{8, 10\\}\\) is the number of completed interactions, \\(x_j \\in \\{Blitzcrank, Participant\\}\\), and \\(\\sigma_i\\) is the participant's confidence value on argument \\(A_j\\).\n\u2022 Model rankings \\(M_i = (m_1^{t}, m_2^{t}, m_3^{t}, m_4^{t})\\) after each round t, where each round consists of two interactions (e.g., two exchanged arguments).\n\u2022 Final argument rankings \\(R_i = (a_1, a_2, ..., a_m)\\), where m is the total number of arguments presented.\n\u2022 Post-study questionnaire responses assessing satisfaction and interaction quality."}, {"title": "Experiment 1: Learning Optimal Personalization Parameters", "content": "Our first experiment aimed to learn the optimal values for the personalization parameters s and r in our probability weighting function (Equations 2 and 3). This data-driven approach uses dialogue traces and user-provided model rankings to maximize the correlation between our computed model rankings and the ground truth rankings provided by the participants.\n\nMethodology: For each participant i with \\(n_i\\) interactions, we performed the following steps: First, we iterated over"}, {"title": "Experiment 2: Comparative Evaluation", "content": "The goal of our second experiment is two-fold: (1) To evaluate the effectiveness of our approach on approximating human models; and (2) To evaluate the effectiveness of our approach on estimating the human beliefs of arguments. We used the same evaluation metrics as in Experiment 1.\nExperiment 2.1: Human Model Approximation\nIn this experiment, we evaluated the efficacy of our personalized approach, referred to as Persona in subsequent figures, in approximating human models. We compared our method against the following baselines:\u2074\n\u2022 Generic: Instead of personalizing parameters for each participant, we learned the same (s, r) for each participant in the first k rounds, i.e., Equation 5 can be modified as:\n\\[\n(s^{k*}, r^{k*}) = \\underset{(s', r')}{\\text{argmax}} \\sum_{i} \\sum_{t=1}^{k} \\rho (M_i^t, M_i^t(s', r'))\n\\]\n\nThis serves as an ablation study for Persona.\n\u2022 SBU: The simple Bayesian update we proposed in Equation 1. This serves as an ablation study for Persona as well.\n\u2022 HM\u2081: An argumentation-based method for updating probability distributions of human models based on argument graphs (Hunter 2015).\n\u2022 HM2: An enhanced version of Hunter's HM\u2081 that utilizes the argument structure for updating the distribution (Hunter 2015)."}, {"title": "Computational Results", "content": "We implemented Persona and evaluated its performance on a MacBook Pro with a 2.2 GHz Quad-Core Intel Core i7 processor and 16GB of RAM. Persona took approximately 0.6 seconds to compute probabilities for each s and r pair of hyperparameter values per participant. In comparison, methods HM1 and HM2 took around 9 seconds and 41 seconds per participant, respectively, whereas HA required just 0.003 seconds to compute argument beliefs. These runtimes indicate that all approaches, particularly Persona, are suitable for real-time evaluations in practical applications. However, it is important to note that additional time would be required for translating between natural language and logic, which is an area we plan to address in future work."}, {"title": "Conclusions and Future Work", "content": "In this paper, we introduced Persona, a novel framework for personalizing human model approximations in argumentation-based dialogues. Persona combines a Bayesian belief update mechanism that refines probability distributions over potential human models during dialogues with a prospect theory-inspired probability weighting function. This combination allows for the incorporation of uncertainty estimates for both agent and human arguments while capturing individual differences in how humans evaluate probabilities in argumentative contexts.\n\nThrough a comprehensive human-subject study involving 184 participants, we demonstrated the effectiveness of Persona in both model approximation and argument belief estimation. Our empirical evaluations showed that Persona significantly outperforms state-of-the-art methods in terms of Spearman's rank correlation and statistical significance tests. Furthermore, our computational results indicate that Persona is suitable for practical applications, with competitive runtime performance compared to existing methods.\n\nFor future work, we plan to investigate how these learned human models can be used to generate more persuasive arguments as well as apply them to other applications, including automated planning (Chakraborti et al. 2017; Sreedharan, Chakraborti, and Kambhampati 2020; Vasileiou et al. 2022; Vasileiou and Yeoh 2023) and scheduling (\u010cyras et al. 2019; Agrawal, Yelamanchili, and Chien 2020; Pozanco et al. 2022; Vasileiou, Xu, and Yeoh 2023)."}, {"title": "Details of Empirical Evaluations and Additional Results", "content": "We provide the details of baselines and additional evaluation results below.\nBaselines in Comparative Evaluation\n\u2022 HM\u2081: An argumentation-based method for updating probability distributions of human models based on argument graphs (Hunter 2015). Inspired by the redistribution function, we apply this concept to our model distribution update.\nAt each time step \\(t_i\\), when an argument \\(A_i\\) is presented by either the agent or the human, we perform the following naive update on the probability distribution:\n\\[\nP^{t_i}(m) = \\begin{cases}\n\\frac{P^{t_{i-1}}(m) + P^{t_{i-1}}(h_{A_i}(m))}{2} & \\text{if } m \\models A_i \\\\\n0 & \\text{if } m \\not\\models A_i\n\\end{cases}\n\\]\n\nwhere \\(h_{A_i}(m) = m \\setminus \\{a\\}\\) and a is of the form \\(A \\models a\\).\n\u2022 HM2: An enhanced version of Hunter's HM\u2081 that utilizes the argument structure for updating the distribution (Hunter 2015). Specifically, consider an argument graph G where \\(Attacks(G)\\) represents the set of attack relations in G. For instance, if \\(A_1 = (\\{a\\}, \\{a\\})\\) and \\(A_2 = (\\{b, b \\rightarrow \\neg a\\}, \\{\\neg a\\})\\), \\(A_2\\) is a counterargument of \\(A_1\\), indicating that \\((A_2, A_1) \\in Attacks(G)\\). In this way, this method first applies Equation (8) and then proceeds with the following update:\n\\[\nP^{t_i}(m) = \\begin{cases}\nP^{t_i}(m) + P^{t_i}(h_{\\Phi}(m)) & \\text{if } m \\models \\Phi \\\\\n0 & \\text{if } m \\not\\models \\Phi\n\\end{cases}\n\\]\n\nwhere \\(\\Phi = \\{\\neg B \\mid (B, A_i) \\in Attacks(G) \\text{ or } (A_i, B) \\in Attacks(G)\\}\\).\n\u2022 HA: A state-of-the-art method for learning probability distributions of arguments by Hunter (2016). This baseline method updates the belief in each argument throughout the dialogue by considering the initial probability of each argument and the human's confidence in their arguments. Specifically, for each argument \\(A_i\\), the final distribution is:\n\\[\nP(A_i) = \\begin{cases}\n0.2 & \\text{if } x_i = \\alpha, \\exists B \\in Opp(A_i), P(B) > 0.5 \\\\\n0.2 & \\text{if } x_i = \\eta, \\exists B \\in Pro(A_i), P(B) > 0.5 \\\\\n0.8 \\sigma_i & \\text{if } x_i = \\alpha, \\forall B \\in Opp(A_i), P(B) \\leq 0.5 \\\\\n\\sigma_i & \\text{if } x_i = \\eta, \\forall B \\in Pro(A_i), P(B) \\leq 0.5\n\\end{cases}\n\\]\n\nwhere \\(Opp(A_i) = \\{A_{i+1} \\mid \\exists i, x_{i+1} = \\eta\\}\\) and \\(Pro(A_i) = \\{A_j \\mid \\exists j, i < j, x_j = \\alpha, (A_j, A_i) \\in Attacks(G)\\}\\)."}]}