{"title": "TEXT CONDITIONED SYMBOLIC DRUMBEAT GENERATION USING LATENT DIFFUSION MODELS", "authors": ["Pushkar Jajoria", "James McDermott"], "abstract": "This study introduces a text-conditioned approach to generating drumbeats with Latent Diffusion Models (LDMs). It uses informative conditioning text extracted from training data filenames. By pretraining a text and drumbeat encoder through contrastive learning within a multimodal network, aligned following CLIP, we align the modalities of text and music closely. Additionally, we examine an alternative text encoder based on multihot text encodings. Inspired by music's multi-resolution nature, we propose a novel LSTM variant, MultiResolutionLSTM, designed to operate at various resolutions independently. In common with recent LDMs in the image space, it speeds up the generation process by running diffusion in a latent space provided by a pretrained unconditional autoencoder. We demonstrate the originality and variety of the generated drumbeats by measuring distance (both over binary pianorolls and in the latent space) versus the training dataset and among the generated drumbeats. We also assess the generated drumbeats through a listening test focused on questions of quality, aptness for the prompt text, and novelty. We show that the generated drumbeats are novel and apt to the prompt text, and comparable in quality to those created by human musicians.", "sections": [{"title": "1. INTRODUCTION", "content": "Research in AI-generated music has seen fast progress in recent years [1-8]. Some recent deep learning models have successfully generated realistic-sounding music [9] by training on large datasets like The Lakh MIDI Dataset. Although researchers have created models that can generate drum accompaniments (see Section 2), creating new drumbeats conditioned on text prompts remains to be solved. In this research we show that such a model can be trained by taking inspiration from state of the art text-conditioned image generation models [10-13].\nRecent advances in Text-Conditioned Image Generation using diffusion models have laid the groundwork for generative systems, particularly when applied within latent spaces as opposed to the more natural pixel space [13] in the case of images. This shift to latent space has the benefit of dealing with a more compressed representation of data, which can lead to more efficient processing and the pretrained encoder-decoder can lead to potentially superior generation outcomes using the compressed representations. The adoption of diffusion techniques in latent space along with conditioning it on extraneous variables like text not only improves the model's stability during training but also enhances the quality and relevance. This paper builds upon these advantages, underscoring the potential latent space diffusion holds for generating musically coherent outputs that are responsive to textual prompts.\nConditioning the deep learning generative model on text in the case of music or drumbeat generation is not trivial. As opposed to the case of images, which have rich datasets linking text and images, music lacks such datasets. Commonly used MIDI datasets such as The Lakh Midi Dataset and the Magenta Groove MIDI Dataset do not offer the rich text required to train such a model. To circumvent this issue, we work with the Groove Monkee dataset which provides descriptive filenames for its MIDI drumbeats (see Section 3). To deal with this text we investigate both a large language model (LLM) with CLIP-like pretraining, and a multihot keyword-based approach (see Section 4). Either approach gives a text encoding that can be used to guide drumbeat generation. Figure 1 shows the overall architecture of our system. The provided text prompt is converted into text embeddings using the text encoder described in Section 4.1. The random noise, denoted as Ztmax, along with these text encodings are autoregressively passed into the LDM that iteratively generates Ztmax... Zo is then passed into a pianoroll Decoder that generates the final pianoroll representing the drumbeat. This pianoroll is converted back into a MIDI file to get the final MIDI drumbeat.\nIn Section 2, we discuss some related research and how our research differs from it. We discuss the dataset in Section 3, including its text annotations and our preprocessing. We explain the three main parts of our model \u2013 the text embedding layer, autoencoder, and diffusion \u2013 in Section 4. Details of training are here also. This is followed by evaluation and results in Section 5. Conclusions and future work are in Section 6."}, {"title": "2. RELATED WORK", "content": "We begin by highlighting the commonalities between drumbeat generation and image generation models using Latent Diffusion Models (LDM). LDMs have been used in the image domain with applications in the real world. Rombach et. al. [13] have showcased generating high resolution images using a similar architecture. The translation of these methods into music is not trivial. Firstly, music lacks such a rich dataset of text and symbolic music. Secondly, the data distribution of images and music is very different. Symbolic music in the form of a pianoroll is highly sparse. In addition to this, the relevant features in music are more temporal than local. The opposite is the case for images [14], motivating architectures like CNN. We solve both these issues by firstly, using a semi-labeled dataset (see Section 3). Secondly, we design a model based on RNNs as opposed to CNNs in order to better capture the temporal features in music. We take this a step further by creating a novel feature-extracting LSTM layer which works at multiple temporal resolutions. Thus we demonstrate that LDMs can work for a different data distribution other than images, i.e. drumbeats.\nResearch has shown that a good lower dimensional representation of drumbeats can be learned by a Variational Autoencoder (VAE), e.g. a convolutional VAE [15,16]. We go beyond this research by introducing more recent deep learning architectures in the encoder and decoder and also conditioning the generation process on text.\nResearchers have also used methods other than AEs to generate novel drumbeats and drum accompaniments [8, 17-20]. Both Kaliakatsos-Papakostas and Hoover and Stanley use evolutionary algorithms to generate drums: in one case to achieve conceptual blending, in the other for interactive control taking a \"scaffold\" from existing instrumental tracks. Dahale et al. generate drumbeats as accompaniments by conditioning it on other instruments like string, bass, etc using the Lakh Midi Dataset. Their model has no text conditioning element. Makris et al. address the challenge of conditional drums generation by using a novel data encoding scheme inspired by Compound Word representation, focusing on a sequence-to-sequence architecture that utilizes a Bidirectional Long Short-Term Memory (BiLSTM) Encoder and a Transformer-based Decoder.\nThere have been other experiments using MusicVAE [21] to generate drumbeats by interpolating between known drumbeats. Beat Blender [22] uses such a methodology to create an interactive website where users can do these interpolations on the fly. These methodology diverges from our research in the use of AEs and not conditioning the generation model on text or natural language. They also exclude note duration and velocity from the output, limiting expressivity. Other systems such as JukeDrummer [18] work with audio signals instead of symbolic music, often using VQ-VAE. We diverge from this research both from the perspective of output format and the underlying generation method. We also considered another annotated dataset of The Expanded Groove MIDI Dataset [23]. Although this dataset also offers annotations for each drumbeat in the form of genre tags and bpm, it does lack more information about the parts of the drumset that were used for creating that drumbeat. In addition to that we found the drumbeats in The Groove Monkee dataset to be of high quality. Considering these two points, we decided on using The Groove Monkee dataset for our research.\nA recent publication by Wu et al. have published a dataset of linked symbolic music and text along with using the linked dataset to train a contrastive learning model [24]. Since the dataset is not for drumbeats we opted to work with a different dataset instead. The authors have proved in this research that the text encoder learned by jointly training a multimodal music and text encoder is meaningful for downstream tasks and shows promising results. These results are consistent with image-text case as well [10]. We take advantage of these learnings and extend the model architecture by using the generated text embeddings for generating text-conditioned drumbeats using LDMs."}, {"title": "3. DATASET", "content": "For the development and evaluation of our model, we use the Groove Monkee \u00b9 dataset, a collection of MIDI drum loops. This dataset has a wide range of styles and genres like Rock, Blues, Latin, African, Electronic, etc., and song parts such as Verse, Chorus, and Fill. The dataset includes a total of 37,523 MIDI drum loops, including a variety of time signatures."}, {"title": "3.1 MIDI Preprocessing", "content": "To use MIDI in a typical deep learning setting, we typically convert to a pianoroll format as follows. We follow a previously described lossy procedure [15]. We extract metadata from the Groove Monkee MIDI files, such as the file's resolution, BPM, time signature, and track length in both beats and ticks. We assume that 128 time-slices are sufficient to represent 4 bars. Loops are tiled to give a standard length of 4 bars. We assume 9 drum channels (kick, snare, closed hi-hat, open hi-hat, ride, crash, low-tom, mid-tom, high-tom) are sufficient to represent the large majority of tracks. All drum types (multiple kicks, snares, hi-hats, bongos, etc.) are mapped into one of the 9 channels. We create an array of 128 \u00d7 9 float values. A value of zero indicates no event, and a non-zero value indicates a note-on event. Note-off events are not represented but are rarely needed in drumbeats."}, {"title": "3.2 Text Processing", "content": "The textual metadata for our model was extracted from the hierarchical folder structure of the Groove Monkee dataset, which organizes MIDI files into folders and subfolders based on genre and other descriptive characteristics. Each MIDI file's filepath includes information indicative of its genre as well as specific attributes of the drumbeat it contains (see Table 1 for examples). From this path, common identifiers such as \"Groove Monkee\", \"GM\", \"Bonus\", etc., were removed, resulting in a unique string for each MIDI file. This approach allowed us to utilize the full path names as a proxy for the musical genre and characteristics of the drumbeats, under the assumption that the structured naming convention and folder organization provide a representative context for each MIDI file."}, {"title": "4. METHOD", "content": "In this section, we discuss our methodology. The overall algorithm is as described in Algorithm 1.\nWe first train an AE model on drumbeat data, as described in Section 4.2. The encoder maps from the data space to a latent space, and the decoder later maps back. A large language model (LLM) text encoder which embeds the text information into text embeddings is described in Section 4.1. We also hypothesized that due to the keyword-type text in the dataset, an alternative, training-free text encoder based on keyword multihot encoding could work as an alternative to the LLM, and this is described also.\nThe DDPM model which runs the denoising process, guided by text, is described in Section 4.3."}, {"title": "4.1 Text Encoding", "content": "The text information corresponding to each drumbeat is extracted by converting the path (including the filename) for each MIDI file into a string that represents that particular MIDI file. Given that the text is more oriented towards keywords rather than natural language, we chose to explore and compare two alternative methods for generating text embeddings. In Section 4.1.1, we discuss the text embeddings produced through contrastive Midi-Text pretraining, which involves adding a projection head over the pretrained BERT embeddings and then training the Midi and Text encoders using contrastive loss. The alternative method, focusing on keyword-based text embeddings, is examined in Section 4.1.2 where a multi-hot vector denoting the presence/absence of a curated list of musically relevant keywords that are present in the full dataset."}, {"title": "4.1.1 Contrastive Language-MIDI Pretraining", "content": "As is the case with CLIP [10], we learn a multi-modal embedding space by jointly training a MIDI encoder and a text encoder. The model is trained to minimize the cosine similarity between N\u00b2 \u2013 N contrasting pairs mi and tj (i \u2260 j), while maximizing the cosine similarity between N same pairs mi and ti. The loss function is a symmetric cross entropy loss over the similarity scores, i.e. both over N MIDI embeddings given the text and N text embeddings given the MIDI file. The trained text encoder is then used to generate the text embeddings that are provided as context to the denoising diffusion model discussed in Section 4.3."}, {"title": "4.1.2 Multihot text embedding", "content": "Since the nature of our dataset is more keyword-based and not natural language, we created a curated list of 57 keywords that were musically relevant in the top 95% percentile of the keywords in the dataset. Similar to text encoder trained in the previous section, the denoising model ingests the textual information in the form of a Multihot Text Context Vector where a position in this vector is hot/active if that particular keyword is present in the text prompt. The limitation of this approach is that firstly, there is a limited number of tags that can be provided as text to the generation model; secondly, the model does not understand natural language and text prompts like \"No Ride Cymbals\" would still yield in a drumbeat that has ride cymbals in them. We also append the bpm as an integer if present in the text prompt."}, {"title": "4.2 Autoencoder", "content": "For training an effective latent diffusion model, we need a MIDI encoder-decoder which can create a compact representation of a drumbeat. Just like a typical AE, the encoder transforms pianoroll drumbeats into a latent variable Z, which is then subsequently decoded back into the pianoroll space. The model is trained on reconstruction loss. Motivated by the multi-resolution nature of music, we also introduce a novel Multi-Resolution LSTM (MRLSTM) component as part of the AE encoder. It is designed to analyze pianorolls at multiple temporal resolutions. In our implementation, the MRLSTM works at resolutions of 1:1, 1:2, and 1:4. The lowest resolution (1:4) focuses on every fourth time-slice, which in a typical simple-time rhythm will be of higher metric weight."}, {"title": "4.3 Diffusion in Latent Space", "content": "To improve the stability and speed [13] of our diffusion process, we implement stable diffusion within a latent space. The Latent Diffusion Model (LDM) is trained to learn the conditional probability distribution\n\\(p_{\\theta}(\\epsilon \\vert Z_{t}, t, w)\\) (1)\nwhere \\(\\epsilon\\) is the noise present in the noised latent variable \\(Z_{t}\\) after t timestep of adding noise, with this process being conditioned upon textual information in the form of text embeddings w. This enables the iterative and autoregressive denoising of a randomly sampled \\(Z_{t}\\), progressively estimating \\(Z_{t-1}\\) through to \\(Z_{0}\\), thus refining the generated output with each step.\nWhile at sampling time, \\(Z_{t}\\) is sampled from a Normal Distribution, at training, we use the MIDI encoder discussed in Section 4.2 to generate Z. This latent embedding is then noised as per the noising schedule and by sampling a t between 1-1000 to create \\(Z_{t}\\). The loss for each step is computed using the mean squared error between \\(\\epsilon\\) and the predicted noise, \\(\\hat{\\epsilon}\\). Algorithm 1 contains the training algorithm for reference."}, {"title": "4.4 Model & Training Details", "content": "The denoising model consists of 3 linear layers with batch normalization and ReLU activation. The input to the first linear layer is the concatenation of Z along with sinusoidal positional encoding of the timestep 't' and the text embeddings (either using the Multihot text embeddings or the text embeddings as per Section 4.1.1). The latent encoding is of 128 dimensions. We also pass an empty text embedding for 5% of training steps to encourage the model to predict the noise without the text embeddings. This is especially useful when there are no active musical tags in the text prompt as per our curated list."}, {"title": "5. EXPERIMENTS AND RESULTS", "content": "In this section, we describe how we tested our drumbeat generation deep learning model for creating MIDI drumbeats.\nFirstly, following [26] we have compared distributions of inter-set and intra-set distances in different setups, visualised as probability density functions with kernel smoothing. We measure how dissimilar the generated drumbeats are, both from other generated drumbeats for the same text prompt (intra-set) and also from the most similar elements of the dataset (inter-set).\nAdditionally, we experimented by adding random noise to the latent variable Z during training, to see how it affects these distances.\nLastly, we evaluated the quality of the generated music through a listening test. For this case, we avoid the problems associated with \u201cmusical Turing tests\" [26] by focusing on questions of quality, suitability for the text, and novelty, rather than questions of artificiality."}, {"title": "5.1 Empirical Experiments", "content": "We create 8 text prompts namely, 'latin triplet', '4-4 electronic', 'funky 16th', 'rock fill 8th', 'blues shuffle', 'pop ride', 'funky blues', 'latin rock' combining different genres and elements of a drumbeat. For each of these text prompts, 10 drumbeats are generated using our model. We compute the 45 pairwise distances using each of two metrics: Hamming Distance on the binarized pianorolls, and Euclidean Distance in the AE embedding space. In total we get 360 pairwise distance values for each metric. We also compute the distance of the generated drumbeats from the closest 1% of the drumbeats from the dataset for each text-prompt variant. We first show these results in Table 2. Comparing the third column with the first two, we see that the generated drumbeats are not mere repetitions of those in the dataset, i.e. the model is generalising.\nWe also study the effects of text-conditioning by analyzing the Hamming distances between drumbeats generated from the same text prompts compared to those generated from different text prompts. Notably, the final column of Table 2 reveals a crucial insight: even when utilizing identical text prompts to generate multiple drumbeats, the resulting drumbeats exhibit differences. This variation highlights the variability inherent in the text-conditioned drumbeat generation model.\nDespite this variability, our further analysis, illustrated in Figure 4, demonstrates a discernible impact of textconditioning on drumbeat generation while using the Hamming distance. In the Euclidean space, while the same-text drumbeats do not cluster as closely as one might anticipate, given the autoencoder's design and training objective focused on just reconstruction loss, there remains a substantial difference between the distances of same-text and different-text drumbeats. This outcome suggests that, although the autoencoder does not explicitly encode genre-specific characteristics or ensure close clustering of similar genres in the latent space, text-conditioning nonetheless exerts a subtle but significant influence on the generated drumbeats.\nIn our exploration of the model's behavior under perturbation, we explored the addition of noise into the latent variable Z during training to enhance the robustness and stability of the latent space against perturbations. Our investigation focused on three distinct levels of noise application: High Noise, Low Noise, and No Noise. For High Noise, we introduced noise within the range [0.01, 0.1], and for Low Noise, the range was set between [0.01, 0.001]. The No Noise condition did not involve any additional noise. The findings indicated that the High Noise model yielded the most unique drumbeats, albeit at the expense of quality. In contrast, the drumbeats generated under Low and No Noise conditions were similar in terms of both quality and their proximity in the latent space."}, {"title": "5.2 Listening Test", "content": "To evaluate the quality of generated drumbeats and their correlation with text prompts, we constructed a survey comprising 40 drumbeats. These were created using 10 randomly selected text prompts from our dataset, resulting in four drumbeat variants for each prompt: (1) the original dataset drumbeat, (2) a drumbeat generated via the multihot text encoding, (3) a drumbeat generated by the BERT text encoding, and (4) a drumbeat generated with empty text, serving as a control or negative example.\nDrumbeats ranged from 8 to 15 seconds in length, leading to an estimated survey completion time of 30 to 60 minutes. Participants were blind to the specifics of the drumbeat generation methods along with the nature of each variant. There were three evaluation criteria: quality, aptness with the given text prompt, and novelty. These were each assessed using a Likert scale.\nWe received a total of 14 responses to our survey, out of which 12 were fully completed and used for all subsequent statistical evaluations.\nThe results, depicted in Figure 6, indicate that participants generally perceived the quality of each category of drumbeats as comparable and satisfactory. Thus, generated drumbeats were as good as professionally-recorded ones.\nConcerning 'aptness', as hypothesized the generated drumbeats were much more apt to the text, versus the control drumbeats which were generated by ignoring the text. The BERT model, in particular, was noted for its ability to generate drumbeats that were perceived as notably novel. It is also worth mentioning that the drumbeats originating from the dataset, which were created by human musicians, received the lowest scores for novelty."}, {"title": "6. CONCLUSION AND FUTURE WORK", "content": "In this study, we developed a system for generating drumbeats conditioned on textual prompts using Latent Diffusion Models. The results demonstrate the system's capability to produce high-quality, coherent drumbeats that aligns with human perception. Analysis from both the Listening Test and the empirical data in Table 2 confirms the system's capability to generate new drumbeats. Figures 4, 5, and 6 illustrate that text conditioning plays an important role in generating the drumbeats. Participants in the listening test rated the quality of these drumbeats as comparable to those in The Groove Monkee dataset, underscoring our model's ability to create drumbeats that match human-generated ones. The use of diffusion in the latent space enhances the quality and generation speed and opens possibilities for real-time integration. We have made the code along with some generated samples available for the readers on GitHub \u00b2 and SoundCloud \u00b3.\nLooking ahead there are some improvements that could be made to the system. The text prompt generation could be improved by employing text augmentation techniques that converts the keyword type textual information into a more free flow natural language. LLMs models like BERT are better suited for free flow natural language and hence such augmentation may also result in better text embeddings. Conducting a larger-scale study in a more controlled environment will provide a more comprehensive data, providing insight into the human perceived capabilities of the system. The observed differences in Same-Text vs Different-Text in the Latent space warrants further investigation into how text prompts shape the musical output in the encoded or latent space. Investigating more by isolating the dimensions with high variance might yield some meaningful insights about the latent space and hence on the impact on text conditioning in the latent space."}]}