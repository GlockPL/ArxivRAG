{"title": "Optimizing Code Runtime Performance through Context-Aware Retrieval-Augmented Generation", "authors": ["Manish Acharya", "Yifan Zhang", "Kevin Leach", "Yu Huang"], "abstract": "Optimizing software performance through automated code refinement offers a promising avenue for enhancing execution speed and efficiency. Despite recent advancements in LLMs, a significant gap remains in their ability to perform in-depth program analysis. This study introduces AUTOPATCH, an in-context learning approach designed to bridge this gap by enabling LLMs to automatically generate optimized code. Inspired by how programmers learn and apply knowledge to optimize software, AUTOPATCH incorporates three key components: (1) an analogy-driven framework to align LLM optimization with human cognitive processes, (2) a unified approach that integrates historical code examples and CFG analysis for context-aware learning, and (3) an automated pipeline for generating optimized code through in-context prompting. Experimental results demonstrate that AUTOPATCH achieves a 7.3% improvement in execution efficiency over GPT-40 across common generated executable code, highlighting its potential to advance automated program runtime optimization.", "sections": [{"title": "I. INTRODUCTION", "content": "Optimizing program performance is increasingly vital as hardware advancements plateau and computational demands rise [1]. While traditional compilers excel at tasks like instruction scheduling and register allocation, higher-level optimizations such as restructuring logic, refining control flows, and addressing inefficiencies still depend heavily on human expertise [2]. Recent progress in large language models (LLMs) has introduced new opportunities for tackling these challenges [3], [4], leveraging their ability to understand and generate complex code. At the same time, fully harnessing contextual and structural insights for LLM-driven optimization remains challenging, as current methods often fall short of emulating the nuanced analysis and dynamic application of improvements performed by human programmers [5].\nTo bridge this gap, LLMs must address key challenges, including identifying structural inefficiencies, adapting to diverse code contexts, and dynamically refining execution paths [6]. Traditional methods rely heavily on manual analysis and predefined heuristics, limiting scalability and adaptability [7]. LLMs, however, offer the potential to overcome these limitations by learning from historical examples and applying optimization patterns in real-time [8], [9]. While promising, the potential for LLMs to emulate human-like reasoning by leveraging program structure and contextual retrieval remains largely untapped. Exploring this avenue could enable more adaptive and scalable optimization techniques, seamlessly combining structural and semantic insights to address complex programming challenges.\nBuilding on these opportunities, we highlight the importance of studying how human programmers optimize code, focusing on contextual understanding and leveraging historical knowledge. To address this, we propose AUTOPATCH, a context-aware framework enabling LLMs to perform in-depth program optimization. AUTOPATCH includes three components: (1) Formalizing methods to incorporate domain knowledge by studying programmer behavior and identifying inefficiencies. (2) Integrating context analysis with augmented retrieval to enable LLMs to learn from past optimizations. (3) Using enriched context, the LLM generates optimized code by combining learned patterns with historical insights. Evaluated on the IBM Project CodeNet dataset [10], AUTOPATCH achieves a 7.3% improvement in runtime performance over GPT-40, bridging manual expertise and automated optimization.\nAUTOPATCH highlights the potential for a unified LLM-powered pipeline for program analysis and optimization. Future advancements can expand its scope by incorporating complex retrieval mechanisms and neural program analysis, addressing more sophisticated programming challenges."}, {"title": "II. METHODOLOGY", "content": "This section presents the methodology for optimizing code using a context-aware framework inspired by expertise-driven analysis, as shown in Fig. 1. The framework consists of three steps: \u2460 formalizing optimization strategies based on human expertise to identify inefficiencies; \u2461 leveraging LLMs to implement a retrieval pipeline that dynamically integrates historical examples; and \u2462 designing context-enriched prompts combining human strategies with LLM-driven insights to produce optimized code."}, {"title": "A. Studying Human Behavior in Code Optimization", "content": "Human programmers optimize code by analyzing control flow structures and identifying inefficiencies. This process involves comparing the unoptimized and optimized versions of code to extract actionable insights, providing structured guidance for refinement.\n1) Understanding Control Flow through Programmer Analysis: Programmers start by examining the logical flow and decision points within code, focusing on sequences of operations and transitions to identify inefficiencies. This intuitive mapping of execution paths uncovers redundant loops, unnecessary branches, or suboptimal operations. Translating this process into CFG formalizes their analysis, enabling a structured approach to pinpoint bottlenecks and refine execution paths systematically.\n2) Extracting Optimization Patterns from Insights: After identifying inefficiencies, programmers compare unoptimized and optimized code to uncover transformations like simplified logic, refined control flows, and optimized loops. Recurring patterns, such as loop unrolling and redundancy elimination, are distilled into reusable strategies, forming a knowledge base that aligns with Retrieval-Augmented Generation (RAG) to guide future optimizations.\n3) Generalizing Knowledge for Actionable Strategies: Beyond immediate fixes, programmers abstract insights into systematic strategies, ensuring their applicability to diverse scenarios. This iterative process of analysis, refinement, and abstraction creates a feedback loop that formalizes human expertise into structured methodologies."}, {"title": "B. Leveraging Historical Insights for LLM Optimization", "content": "Building on human expertise in code optimization, we develop a framework combining CFG Diff Analysis and RAG to guide LLMs in optimizing target code (Ct). Preprocessed examples (Er) are stored in a vector database for efficient retrieval and context-aware refinement.\n1) CFG Diff as Analytical Basis: Given the CFGs of the original (Go) and optimized (Gp) code, we compute the difference, \\(AG = Gp \u2013 Go\\), capturing:\n\\[AG = (AS, AF, AC),\\]\nwhere AS represents structural changes (e.g., added or removed blocks), AF denotes flow adjustments (e.g., modified connectivity), and AC captures content refinements (e.g., altered statements).\n2) Prompting the LLM with CFG Diff: The LLM is prompted with \\(AG\\) to analyze the target code Ct and suggest optimizations. The prompt includes:"}, {"title": "3) Retrieval-Augmented Guidance", "content": "To enhance the LLM's contextual understanding, a RAG pipeline retrieves examples Er from a dataset D, stored in a vector database. Each entry in D contains:\nRelevance is determined using cosine similarity: \\(Sim = cos(e_{cfg}, e_{rfg})\\), where \\(e_{cfg}\\) and \\(e_{rfg}\\) represent the embeddings of the target code and retrieved example, respectively. The vector database ensures efficient retrieval of examples that align with the structural and semantic context of the target code, guiding the LLM in producing optimized outputs."}, {"title": "C. Combining Retrieved Insights for Code Optimization", "content": "After computing the CFG differences \\(AG = (AS, AF, AC)\\) and compiling any associated optimization rationales, we incorporate these details into a structured prompt together with exactly one retrieved example. Let us denote this prompt as:\n\\[P = (AG, R_{opt}, E_r),\\]\nwhere \\(AG\\) captures structural differences, \\(R_{opt}\\) represents optimization rationales, and \\(E_r\\) is the single retrieved example. In practice, we rank potential examples by structural similarity and then pick the top candidate. Preliminary experiments"}, {"title": "III. EXPERIMENTAL DESIGN", "content": "This section details the experimental setup, dataset, evaluation metrics, and results to validate the effectiveness of our proposed context-aware optimization pipeline. By integrating context-aware analysis with historical patch retrieval for LLMs, we evaluate its capability to optimize C++ code and achieve measurable improvements over baseline methods."}, {"title": "A. Experiment Preparation", "content": "Table I summarizes the dataset, drawn from the IBM Project CodeNet [10], containing 1,200 C++ code pairs of source and optimized programs. The data is split into 80% (1,000 pairs) for the vector database and 20% (200 pairs) for testing. Experiments were conducted on a system equipped with an Intel Xeon Gold 6330N CPU featuring 28 cores, 56 threads, and 43 MB of cache, ensuring efficient processing and robust evaluation across various optimization scenarios."}, {"title": "B. Baseline Methods", "content": "We evaluate AUTOPATCH against two baselines: Zero-Shot Generation, which uses GPT-40 without context or retrieval, and Naive Generation, which retrieves repair examples using source code embeddings. All methods share the same prompt structure to ensure fair performance comparison."}, {"title": "C. Evaluation Metrics", "content": "To comprehensively evaluate the pipeline, we adopt two categories of metrics: lexical similarity and execution time. Lexical metrics assess how closely the generated patches align with ground truth, while execution time metrics measure the practical impact of optimizations across different types.\n1) Lexical Similarity Metrics: Lexical similarity metrics evaluate how closely generated patches match the ground truth, providing an initial assessment of patch fidelity:"}, {"title": "2) Execution Time Metrics", "content": "Execution time provides a direct measure of the practical impact of generated patches, validating improvements in program performance beyond lexical similarity. By evaluating execution time across the fine-grained optimization types in Table II, we gain deeper insights into how effectively the pipeline addresses specific challenges like execution speed, memory usage, and algorithmic complexity, demonstrating its versatility in diverse scenarios."}, {"title": "D. Code Preprocessing and Model Selection", "content": "To enable effective analysis, C++ code snippets are converted to CFGs using Clang's static analyzer\u00b9. Preprocessing standardizes headers, resolves dependencies, and removes unsupported attributes, creating structured inputs for analysis."}, {"title": "IV. RESULTS AND ANALYSIS", "content": "This section evaluates the performance of AUTOPATCH using lexical similarity and execution time metrics. These metrics offer complementary perspectives on how effectively the generated patches align with the ground truth and how much they improve runtime efficiency. By integrating CFG-based analysis, our approach addresses higher-level structural edits that pure text-based or naive retrieval methods can overlook."}, {"title": "A. Lexical Similarity Analysis", "content": "As shown in Table III, AUTOPATCH consistently outperforms baselines in LO, EDS, and TO. These improvements indicate that including CFG-focused prompts and relevant examples encourages the generation of patches more closely aligned with the structure and semantics of the optimized code. While higher lexical similarity does not guarantee logical correctness, it suggests that the approach captures patterns of effective edits that typically reflect deeper program analysis."}, {"title": "B. Execution Time Analysis", "content": "Table IV shows that AUTOPATCH reduces execution time by 7.3 percent over zero-shot generation. Although modern compilers already eliminate many low-level inefficiencies, higher-level structural constraints often require human judgment. By integrating CFG insights, our method identifies small but critical edits, such as refining loop conditions or removing redundant code, that translate into tangible runtime benefits. This result highlights the importance of retrieving relevant examples that expose systematic changes, rather than relying solely on token-level patterns."}, {"title": "C. Optimization Type Analysis", "content": "Figure 2 illustrates the average execution times for different optimization categories. AUTOPATCH demonstrates the lowest runtimes in tasks involving code refactoring, memory optimization, algorithmic simplification, and loop optimization. By leveraging CFG-based prompts, it better recognizes repetitive control flows and identifies specific opportunities for streamlining. A slight underperformance in the performance enhancement category suggests that certain specialized changes may require more domain-specific knowledge or an expanded set of retrieval examples. Even so, the overall adaptability across diverse challenges underscores the effectiveness of guiding large language models with structural context, enabling AUTOPATCH to produce optimizations that align more closely with real-world developer best practices."}, {"title": "V. DISCUSSION AND FUTURE WORK", "content": "Our CFG-guided framework demonstrates promising runtime gains, but adopting it in real-world systems introduces new considerations for maintainability and domain-specific constraints.\nPotential Maintainability Issues: Although CFG-based edits can streamline control paths and speed execution, transformations such as loop unrolling or branch merging may affect readability. For example, expanded loops or fewer conditionals could complicate debugging by obscuring critical logic. Similarly, removing intermediate variables to optimize speed may reduce code clarity. Integrating maintainability metrics (such as cyclomatic complexity) and developer feedback could help balance short-term performance gains with long-term code upkeep.\nFuture Directions: Beyond optimization, the context-aware framework is well suited for advanced debugging, leveraging historical examples of logical error resolution. CFG-focused prompts also align with deeper program repair, including targeted security patches and domain-specific enhancements."}, {"title": "VI. THREATS TO VALIDITY", "content": "Our findings draw on C++ programs from IBM Project CodeNet, which may not represent the full diversity of real-world codebases, languages, or hardware. The retrieval process in our RAG pipeline (such as how many examples are fetched or how CFG knowledge is embedded) could also influence results. We only compared a few baselines (for instance, GPT-40 in zero-shot mode and a naive retrieval approach), so improvements might vary with different configurations or advanced prompting strategies.\nWe primarily measured performance gains through execution time, supplemented by lexical similarity metrics. However, these do not cover maintainability, scalability, or project-specific constraints. Some optimizations may also depend on hardware features or compiler behaviors outside our scope. Moving forward, evaluating generated code in broader contexts and providing replication packages-will be vital for improving generalizability and relevance."}, {"title": "VII. RELATED WORK", "content": "We build on advances in human-centered AI, retrieval-augmented generation (RAG), in-context learning, and program analysis. By leveraging insights from developer attention studies and CFG-based analysis, our method addresses program-specific dependencies and structural nuances for more precise, context-aware code optimizations.\nHuman-Centered AI for Neural Code Comprehension: Human-centric research highlights how developers and AI systems align when interpreting code. Eye-tracking and scanpath prediction reveal points where neural and human attention diverge, affecting explainability [13]-[16]. Efforts to learn representations for source code and binaries further underscore how modeling human-like focus can enhance automated code understanding [17], [18]. However, most of this work targets interpretability rather than deeper structural analysis (e.g., CFGs). Our approach extends these insights to optimize runtime, combining human-inspired reasoning with automated transformations.\nRAG and In-Context Learning for Code LLMs: Retrieval-Augmented Generation (RAG) enriches large language models with external knowledge, improving tasks like code generation, refactoring, and bug fixing [19]-[23]. Frameworks such as ARKS [24] and CodeRAG-Bench [25] integrate structured and unstructured data but often overlook intricate syntactic dependencies [26]-[28]. Meanwhile, in-context learning allows LLMs to adapt without fine-tuning by embedding labeled examples in prompts [29]\u2013[33], yet struggles with complex control-flow reasoning [30], [34]. By uniting RAG with in-context learning under a CFG-based framework, we provide explicit structural cues that yield more reliable, performance-centered code generation.\nProgram Analysis for Optimization: Traditional program analysis uses static or dynamic techniques to address correctness, resource usage, and speed [7], [35]\u2013[37]. While ML-based methods can detect inefficiencies, many overlook context-dependent details in large-scale projects [6], [8], [9], [38]-[40]. Minor loop or conditional edits can radically change execution behavior, underscoring the need for deeper structural insight. By embedding CFG analysis into an LLM workflow, we align semantic and syntactic contexts for more precise, performance-oriented optimizations that surpass traditional ML or heuristic-driven solutions."}, {"title": "VIII. CONCLUSION", "content": "In conclusion, this work introduces AUTOPATCH, an approach that combines in-context learning with CFG analysis to empower LLMs in generating optimized code effectively. Inspired by human cognitive processes, AUTOPATCH integrates historical examples and program-specific context, bridging the gap between retrieval-based methods and static analysis. Experimental results showcase a 7.3% improvement in execution efficiency over GPT-40, underscoring its ability to deliver consistent and meaningful optimizations. Future research will extend AUTOPATCH's capabilities to applications like automated debugging, program repair, and domain-specific performance tuning, paving the way for a unified framework for intelligent and adaptable program optimization in diverse software engineering scenarios."}, {"title": "DATA AVAILABILITY STATEMENT", "content": "All data and code used in this study are available at rag-optimization\u00b2. The repository includes:"}]}