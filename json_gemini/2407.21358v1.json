{"title": "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs", "authors": ["Elan Markowitz", "Anil Ramakrishna", "Jwala Dhamala", "Ninareh Mehrabi", "Charith Peris", "Rahul Gupta", "Kai-Wei Chang", "Aram Galstyan"], "abstract": "Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing reliable, structured, domain-specific, and up-to-date external knowledge. However, KGs and LLMs are often developed separately and must be integrated after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning algorithm that enables augmentation of black-box LLMs with one or more KGs. The algorithm equips a LLM with actions for interfacing a KG and enables the LLM to perform tree search over possible thoughts and actions to find high confidence reasoning paths. We evaluate on two popular benchmark datasets. Our results show that Tree-of-Traversals significantly improves performance on question answering and KG question answering tasks.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are used for a range of knowledge-intensive tasks such as information retrieval (Zhu et al., 2023b), summarization (Zhang et al., 2023), and question answering (Tan et al., 2023). Trained on large amounts of textual data, these models learn a wide breadth of information. However, LLMs suffer from several limitations they produce hallucinated information (Ji et al., 2022; Bang et al., 2023), lack deep domain-specific knowledge (Pan et al., 2023b), and have a static knowledge cutoff when training ends.\nKnowledge graphs (KGs) naturally complement LLM weaknesses. KGs contain up-to-date information covering general (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014; Lehmann et al., 2015) and/or domain-specific topics (Abu-Salih, 2020; Liu et al., 2019; Zhu et al., 2017; Choi and Lee, 2019; Farazi et al., 2020) in highly structured and interpretable format. Augmenting an LLM's ability to reason and respond in natural language with an external KG's up-to-date knowledge presents a path toward a reliable and factual LLM.\nThe rise of powerful LLMs with new capabilities has renewed interest in combining LLMs with KGs. Numerous survey and position papers have recently emphasized their combined potential (Pan et al., 2023a; Zhu et al., 2023a; Yang et al., 2023; Pan et al., 2023b). Existing works augmented LLMs with KGs in multiple ways, such as integration into pretraining (Yasunaga et al., 2022), fine-tuning (Zhang et al., 2022), or later adaptation with subsequently trained components (Lin et al., 2019; Hu et al., 2022). All of these carry some limitations. In particular, training or fine-tuning a large scale LLMs is computationally expensive. In some cases, model weights are unavailable publicly. Finally, the largest KGs require their own servers and cannot be integrated in-memory with LLMs. Additionally, previous works do not consider the case of augmenting with multiple KGs.\nAn algorithm that allows the augmentation of a powerful black-box LLM with any number of internal or external KGs without training from scratch or fine-tuning the model is valuable. Such a zero-shot algorithm would enable several innovative use cases such as (1) customers using a black-box LLM API in conjunction with an internal domain-specific KG, (2) integration of personalized KGs into an LLM without the risks associated with training a model with such personal data, (3) integration of deep domain knowledge via an array of API accessible KGs (e.g., IMDb, MusicBrainz).\nWe introduce Tree-of-Traversals, a novel algorithm that addresses the above issues by allowing the augmentation of any powerful LLM with arbitrary number of KGs in a zero-shot fashion. It requires no training, functions with black-box access to the LLM, and works with any API accessible KG. Our contributions are:\n1. Tree of traversals: A novel zero-shot algorithm for augmenting any powerful LLM with arbitrary number of KGs and enabling advanced KG reasoning using tree search.\n2. Evaluation of Tree-of-Traversals on two question answering tasks: 2WikiMultiHop and QALD-10 and comparison with baselines.\n3. Development of a new dataset to test combined reasoning over a general and a domain-specific KG, and evaluation of Tree-of-Traversals on this dataset.\nWe conduct detailed experiments on three models of varied sizes hosted on Amazon Bedrock and present detailed ablation studies."}, {"title": "2 Tree-of-Traversals", "content": "The Tree-of-Traversals algorithm maintains a local KG subgraph that is repeatedly expanded until it contains all the information required by an LLM to answer the given query. At the start, a local KG subgraph is initialized to contain the entities present in the original query. It is then expanded using a tree search algorithm to choose actions and thoughts generated by an LLM in order to obtain relevant knowledge from the KG using a KG interface. The algorithm halts when the LLM is able to answer the original query using the local KG subgraph as context. Tree-of-Traversals consists of three major components. (1) A knowledge graph interface implemented to interact with one or more required KGs. (2) An action state machine (ASM) which is a finite state machine that defines the feasible space of actions, states, and prompt templates when the LLM interacts with a KG to expand the local KG subgraph. (3) A tree search algorithm which defines the overall LLM search trajectory such as best first search, backtrack upon making a mistake, and termination condition when an answer is found.\n2.1 Knowledge Graph Interface\nThe knowledge graph interface allows Tree-of-Traversals to interact with one or multiple KGs. Let $K = (E,R, T)$ be a single KG. E is the set of entities in which each entity consists of an identifier, a label, and an optional description (e.g., Q35332, \u2018Christopher Nolan', \u2018British-American filmmaker'). R is the set of relation types, each consisting of an identifier, a label, and an optional inverse label (P57, \u2018director', 'is director of'). T is the set of edges or facts in the KG in which each edge is of the form $(s,r, o)$ where $s,o \\in E$ and $r \\in R$, e.g., (\u2018Inception', \u2018director', \u2018Christopher Nolan'). Tree-of-Traversals can be used for K as long as the following interfaces are implemented.\n1. $initialize(q) \\rightarrow E_0$: It takes as input a query q, extracts entities from q and returns the linked entities $E_0 \\subset E$ where $E_0$ are the entities from K that are referenced in q.\n2. $get\\_relations(E_{selected}) \\rightarrow R_{options}$: It takes a set of entities, $E_{selected}$, and returns the relation types, $R_{options} \\subset R$, that $E_{selected}$ have in K: ${r|(s, r, o) \\in T, s \\in E_{selected}}$\n3. $get\\_edges(E_{selected},r) \\rightarrow T_{added}, E_{added}$: It takes a set of entities and a selected relation type, and returns all edges with relation type r for source entities in $E_{selected}$: ${(s, r, o) \\in T|s \\in E_{selected}, r = r, o \\in E}$. It also returns the new entities, $E_{added}$, that are entities reached with $T_{added}$.\nThis interface is implemented with SPARQL queries when available; otherwise, we use the graph API that is available for the KG. For multiple KGs, each interface is implemented separately.\n2.2 Action State Machine (ASM)\nOne of the challenges with developing a zero-shot LLM algorithm that works with arbitrary KGs is that the LLM does not know what relations are available in the graph or what relations are valid for a given entity. Few-shot or in-context learning approaches can only cover a handful of the possible relation types (e.g., Wikidata has over 11,000 relation types) (Brown et al., 2020).\nTo overcome these issues we break the task of expanding a local KG subgraph into multiple subtasks. We use a finite state machine with the following actions: Think, Answer, ExpandKG, Select_Entities, and Select_Relation, and states: default, selecting-entities, selecting-relation, and done as shown in Figure 2. This is named as Action State Machine (ASM) throughout the paper.\nFrom the default state, Tree-of-Traversals can either Think, Answer, or choose to ExpandKG. After Tree-of-Traversals chooses to ExpandKG, it first is prompted to Select_Entities about which it needs more information (e.g., 'Inception' in Figure 1). It is then prompted to Select_Relation from a list of candidate relations provided by the KG interface's get_relations method. After selecting a relation (e.g., 'cast member' in Figure 1), all edges containing one of the selected entities as the source and the selected relation as the relation are added to the local KG subgraph. Tree-of-Traversals is then able to Answer, Think, or ExpandKG again.\nPrompt Templates. Each state in the ASM other than the 'done' state is associated with a unique prompt template. Prompt templates are filled with information from the local KG subgraph and the KG interface before presenting them to the LLM. A customized prompt for each state allows us to present precise and relevant information along with specific instructions for each state to the LLM simplifying the LLM's task. For instance, the prompt for 'selecting-entities' presents the available options of entities to choose from that are in the local KG subgraph (see Figure 3). Prompt templates for all the states are in the Appendix F.1-F.3.\nLocal KG subgraph is represented using a a token efficient YAML format that minimizes repetition for multiple edges on the same entity.\nInvoking KG Interfaces. Outside of initialization, there are two times in which the ASM needs to invoke the KG interface: (1) When constructing the selecting-relation prompt, the algorithm calls get_relations($E_{selected}$) to gather available choices of relations. (2) When executing the transition from selecting-relation to default after having selected a relation type r, the algorithm calls get_edges($E_{selected}, r$) so that it can add the new edges and entities to the local subgraph.\n2.3 Tree Search Algorithm\nOur approach draws inspiration from the Tree-of-Thoughts approach (Yao et al., 2023) in which an LLM is given increased reasoning power by allowing generation of multiple thoughts at a time and building a search tree over the resulting reasoning chains. We extend this approach to augment LLMs with KGs through allowing the generation of actions in addition to thoughts, and building a search tree over them. Challenges arise because Tree-of-Thoughts was not designed to incorporate actions and was not designed for knowledge intensive question answering tasks. As a result, we introduce some modifications: incorporation of actions via the ASM, a slightly different search procedure and stopping condition to better handle QA, and a different sampling procedure for improved diversity when doing constrained sampling.\nAlgorithm 1 presents the Tree-of-Traversals tree search algorithm. Given a query q it begins with initialization using $initialize(q)$ as described in Section 2.1. After initialization, it searches by (1) choosing the unexplored node to expand based on the node's value assigned by the value function (Best First\u2192 Depth First as tie-breaker), (2) sampling k actions from the LLM (k is branching factor) using the prompt associated with the selected node's state in the ASM, (3) for each of the sampled action, applying the transition function, and (4) evaluating the value of the resulting nodes with the LLM value function. The search stops when an answer is found with a node value exceeding the threshold \u03c4. \u03a4\u03bf bound the search space we add two hyper-parameters: max depth of the tree search, after which the algorithm is forced to transition to the done state (i.e., answer the query), and max expansions after which the model halts exploration and returns \"Cannot find answer\".\nValue Function Guidance. Tree-of-Traversals computes the value of a node to determine its utility. This value is created by the LLM using evaluation prompts (step evaluate in Algorithm 1). The value can be between 0 to 1 where 1 indicates highest utility. We use two types of evaluation prompts: one for intermediate states and one for answer states. The prompts include the original query, the local KG subgraph, the trajectory of previous actions, followed by instructions for evaluating the node (see Appendix F.4 and F.5). These values are then used to guide the exploration of the action space. Specifically, choose_node returns the unexplored node with the highest value (Best First). If there are nodes with the same value, Depth First Search is used.\nChain-of-Traversals. In some cases we can find the answer to a query with a single sequence of thoughts and actions using the ASM and KG interface. This is equivalent to Tree-of-Traversals with a branching factor of k = 1. We refer to this as Chain-of-Traversals. While useful for comparison, experiments show the benefit of considering multiple branches.\n2.4 Tree-of-Traversals with Multiple KGS\nAugmenting an LLM with more than one KG mainly involves building KG interfaces for each of the added KGs. There are a few other changes to the algorithm. (1) The entities extracted in $initialize(q)$ are matched with each KG interface. (2) When presenting the options of relations during selecting-relation, $get\\_relations(E_{selected})$ is called on each KG interface. (3) When adding a new entity to the local KG subgraph we call an entity linking function for the other KG interfaces. We allow for the entity linking function to be a separate function in the KG interface or to fallback on $initialize(o)$ where o is the text label of the entity that has just been added. This makes allows the use of explicit links between common KGs if available while still functioning without."}, {"title": "3 Experiments", "content": "We evaluate Tree-of-Traversals using three different models available on Amazon Bedrock: Claude-Instant (claude-instant-v1), Llama2 70b (1lama2-70b-chat-v1), and Llama2 13b (llama2-13b-chat-v1). AWS Bedrock provides on-demand access through a single API to various Foundation Models, including both open source and black box ones. This is precisely a use case Tree-of-Traversals is designed for.\n3.1 Tasks and Datasets.\nWe first evaluate the Tree-of-Traversals algorithm on two common tasks used for evaluating an LLM's knowledge: 2WikiMultiHop and QALD-10. To allow testing on complex questions requiring knowledge from multiple KGs we create a new dataset that requires knowledge from multiple KGs.\n2WikiMultiHop Dataset (Ho et al., 2020) was constructed by extracting multi-hop templates from HotPotQA (Yang et al., 2018), combining templates to get complex reasoning questions, generating candidate questions with Wikidata, and then confirming that the entity mentions for each edge appear in the Wikipedia paragraphs. Answers to these questions can be derived from both Wikipedia and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014). We subsample 500 questions from the test set following the approach used in ReAct (Yao et al., 2022), including the sampling seed value of 233.\nQALD-10 Dataset (Usbeck et al.) is a multi-lingual Knowledge Graph Question Answering (KGQA) dataset with 395 questions created by humans, translated into other languages, and then constructed as a SPARQL query over Wikidata. These questions are more varied in terms of reasoning structure than the questions in 2WikiMultiHop (e.g. requiring multiple answers or aggregations). We used the questions in English language.\nMusicBrainz-x-Wikidata Dataset One novel use-case of Tree-of-Traversals is synthesizing and reasoning over multiple knowledge graph sources. There is no existing dataset that requires synthesizing information from multiple KGs to answer individual questions. Therefore, to test the reasoning ability using multiple KGs, we create a new dataset, MusicBrainz-x-Wikidata, containing 109 questions that require reasoning with information from both MusicBrainz and Wikidata. Unlike Wikidata which contains general knowledge, MusicBrainz is a deep domain-specific database on the music industry. The vast majority of this information is unlikely to be known by large language models. We construct the MusicBrainz-x-Wikidata dataset with human annotators who were provided with instructions to find reasoning paths between these two KGs, the question types to focus on, and some example questions. The curated questions were checked for ambiguity and sensibility. By design, each question requires extracting information from both knowledge graphs in order to answer it successfully. In addition to the reasoning types in 2WikiMultiHop, this dataset contains questions that involve aggregations, comparisons of aggregations, qualifications, and complex combinations of these. An example can be seen in Figure 3.1. Detail on instructions, question types, and examples are in Appendix A.\n3.2 Metric.\nWe use Exact Match Included (EM-in) as the evaluation metric. EM-in is 1 if the ground truth answer appears with an exact match anywhere in the answer and 0 otherwise. This accounts for the propensity of an LLM to output answers in a sentence with varying syntax. This is a common metric but is often referred to interchangeably with Exact Match (EM) (Sun et al., 2023). When there are multiple answers, we compute average EM-in over all labels.\n3.3 Comparison Baselines.\nWe experiment against three related approaches that can work with any black-box LLMs: (1) Chain-of-Thought (CoT) prompting (Wei et al., 2022) (2) ReAct (Yao et al., 2022) which iterates between generating thoughts and generating actions for searching and retrieving from a text knowledge base like Wikipedia, and (3) Forward Looking Active Retrieval (FLARe) (Jiang et al., 2023b) which iterates between generating thoughts and then retrieving from a knowledge base to correct inaccuracies.\n3.4 Implementation details.\nFor all models, we use a sampling temperature of 0.0 for the LLM when multiple samples are not required and a temperature of 1.0 when diverse samples are required. We test our approach in two settings: (1) with a branching factor of k = 1 termed as Chain-of-Traversals, and (2) with a branching factor of k = 3 termed as Tree-of-Traversals. In both setups, we set maximum depth to 7 which means that upon reaching the default action state beyond depth 7, the only available action is to answer the question. For Tree-of-Traversals, we set the maximum total expansions to 20. The answer threshold \u03c4 is set to 0.8 which corresponds to high confidence answers supported by the KG according the evaluate prompt (Appendix F.5).\nFor 2WikiMultiHop and QALD-10, we use Wikidata as the knowledge graph (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014). For MusicBrainz-x-Wikidata, we use MusicBrainz in addition to Wikidata. We implement the KG interface for Wikidata using Wikidata SPARQL queries, and implement the KG interface for MusicBrainz KG using the MusicBrainz"}, {"title": "4 Results and Discussion", "content": "Table 1 presents the results of our experiments on 2WikiMultiHop and QALD-10. For all the models, Tree-of-Traversals outperforms the baselines on 2WikiMultiHop, setting state-of-the-art results for these tasks in the zero-shot setting. We hypothesize that much of this gain is due to Tree-of-Traversals' access to the knowledge base via proposed KG interface and its thought-action procedure guided by the ASM. This is evident, as even Chain-of-Traversals, which does not perform a tree traversal (including multiple thoughts/actions, backtracking, and node value computation), significantly outperforms ReAct's knowledge grounding: 8.7% higher accuracy than ReAct\u2192 CoT on 2WikiMultiHop when averaged over all models. Compared to Chain-of-Traversals, Tree-of-Traversals further improves performance. It gives on average a 12.8% absolute accuracy increase for 2WikiMultiHop and 4.3% absolute improvement for QALD-10. We note that the better performing the model is, the more it stands to gain from Tree-of-Traversals as noted by the difference between Llama-70b and Llama-13b.\nOn MusicBrainz-x-Wikidata (Table 2), which contains challenging reasoning questions requiring access to two KGs, there is an average of 37.4% relative improvement from Tree-of-Traversals over Chain-of-Traversals as shown in Table 2. Both Chain-of-traversals and Tree-of-Traversals outperform Chain-of-Thoughts on this dataset. We only compare with Chain-of-Thoughts as other retrieval methods have low coverage over the MusicBrainz knowledge base.\n4.1 Effect of the Value Function.\nTree-of-Traversals relies on signal from the value function to pick the final tree trajectory. If the values were arbitrary, then Tree-of-Traversals would not do any better than Chain-of-Traversals. Figure 5 shows that there is a meaningful signal from the value function for all models. There is an average performance difference of 31.0% between the accuracy for answers valued at 1.0 vs 0.0. This represents an average relative improvement of 83.2% when selecting answers valued at 1.0 over those valued at 0.0. See Appendix C for individual profiles of each model's value function.\n4.2 Effect of Backtracking.\nTo determine the effect of backtracking in Tree-of-Traversals, we ask the counter-factual of how would the model perform if it could not backtrack (Figure 6). We limit the analysis to 2WikiMultiHop questions in which Tree-of-Traversals generates answers on a subtree, and the model eventually backtracks on that subtree (i.e., the cases where we have a counter-factual). As a result, these questions are generally more challenging than the overall distribution of questions. In these cases, we compare the result if the highest-valued answer was taken from the first searched subtree compared to the ultimate answer after backtracking. We find that the ability to backtrack gives Tree-of-Traversals a significant accuracy increase ranging from 4.1% to 12.3%.\n4.3 Performance on MusicBrainz-x-Wikidata.\nFor MusicBrainz-x-Wikidata dataset, we only compare against Chain-of-Thought since the implementations for other baseline algorithms do not have access to similar music-specific knowledge bases. Despite this, we observed some interesting results. Chain-of-Thought does far worse on MusicBrainz-x-Wikidata than on the more general datasets based exclusively on Wikipedia/Wikidata (10.1%-13.8% vs. 25.2%-47.4%). This could be because LLMs are trained on vast amount of general knowledge such as that found in Wikipedia, but they are not trained with as much data from specific domains such as music. Besides the presence of KG interface and ASM with Tree-of-Traversals, this could be an additional reason why Tree-of-Traversals does 2.2 times as well as Chain-of-Thought on MusicBrainz-x-Wikidata. This demonstrates the importance of augmenting LLMs with domain-specific KGs and/or multiple KGs which the proposed Tree-of-Traversals is capable of.\n4.4 Analysis of Baselines.\nReAct underperforms Chain-of-Thought in most cases. This phenomenon was seen in the original ReAct paper which noted that their approach significantly reduced hallucinations despite lower accuracy (Yao et al., 2022). Therefore, falling back on Chain-of-Thought results in an accuracy improvements for for Llama2-70b and Claude-Instant. We note that claude-instant vastly underperforms Llama2-70b on ReAct. The primary reason for this is claude-instant refuses to \"search\" people and apologizes after performing invalid actions. Despite this, ReAct\u2192CoT still improves over CoT on both 2WikiMultiHop and QALD-10. FLARE improves model performance on 2WikiMultiHop but not on QALD-10. This may be due to better overlap between the text knowledge base for 2WikiMultiHop."}, {"title": "5 Related Works", "content": "Knowledge Base Question Answering There is a large history of work that has looked into answering questions using a knowledge graph (Wu et al., 2019; Lan et al., 2021). The approaches can broadly be broken into those that attempt to parse the question into a logical form (Berant and Liang, 2014; Luo et al., 2018; Zhu et al., 2022), and information retrieval approaches (Bordes et al., 2015; Chen et al., 2019)\nKnowledge Enhancement. Recently, researchers have looked into enhancing pretrained language models and LLMs using knowledge graphs. Many works have focused on incorporating knowledge graph data into LLMs through training, often with architectural changes to the model (Zhang et al., 2019; Wang et al., 2019; Peters et al., 2019; Yamada et al., 2020; He et al., 2021). Some of these have found success using specialized graph encoder layers (Yasunaga et al., 2021; Sun et al., 2021). Some have sought to mix this process with pretraining of the language model (Yasunaga et al., 2022). The limitations of including KGs during LLM training are: the models are incapable of incorporating KG updates without retraining, are not able to change KG source (e.g., a domain-specific one), and may have low explainability resulting from learning knowledge in model weights rather than explicit retrieval. In addition, these methods add complexity to the training process, increase cost, and do not work with LLM without access to model weights. As a result, scaling these methods has often been seen as too risky.\nOther methods treat a KG as a more complete source of truth and use the LLM to generate structured queries for the KG. Tianle et al. (2023) and Choudhary and Reddy (2023) teach the LLM to generate a logical KG query which then returns matching entities from the KG. These methods share similarities with the semantic parsing based approaches mentioned earlier. However, by returning a logical query for the KG, these methods lose the reasoning and commonsense abilities of the LLM.\nSome approaches have looked at injecting KG or knowledge base data into LLM prompts. Many methods do a single round of retrieval based on the query (Lewis et al., 2020; Li et al., 2023) using a retrieval mechanism, such as dense passage retrieval (Karpukhin et al., 2020). These methods cannot answer more complex multi-hop questions as the initial retrieval is unlikely to contain the secondary or tertiary information that will ultimately be required. Later methods have made the retrieval process iterative. E.g. FLARe (Jiang et al., 2023b) uses prediction of the upcoming sentence to retrieve relevant documents and regenerates until the sentence contains high-confidence tokens. In Wang et al. (2023) multi-round QA format is used for multi-round retrieval. ReAct (Yao et al., 2022) does multiple rounds of thoughts and actions to query a text-based knowledge base. Jiang et al. (2023a) repeatedly interfaces with a KG. In other parallel works, (Sun et al., 2023; Wen et al., 2023) explore methods for building KG context for LLMs using path and neighborhood based search methods. The above methods do not incorporate tree search or forms of advanced reasoning beyond the LLM's innate ability. They cannot explore multiple reasoning paths or solutions and cannot backtrack if making a mistake. This results in lesser capabilities. Additionally, none of the above methods explore reasoning over multiple KGs.\nMultiple Knowledge Base QA. Some works studied QA on questions derived from differing domains. These works learn to pick between different domain-specific QA models, each trained on a single knowledge base (Puerto et al., 2021; Geigle et al., 2021; Puerto et al., 2023). Besides requiring training, such systems cannot answer questions requiring the synthesis of information from different domains (MusicBrainz-x-Wikidata)."}, {"title": "6 Conclusion", "content": "Tree-of-Traversals is a powerful algorithm that enables LLMs to utilize KG reasoning with zero examples, no schema dependency, and no training. We demonstrate its efficacy experimentally on multiple LLMs and datasets. We hope Tree-of-Traversals continues to be developed and see value in studying integration with personalized user KGs as well as with other domain-specific datasets."}, {"title": "7 Limitations", "content": "Tree-of-Traversals is slower than simpler retrieval alternatives. Improving the value function would reduce the number of LLM and KG API calls by avoiding incorrect paths along the tree to explore. Other engineering solutions, such as hosting graph servers, could be implemented to accelerate LLM and KG access. In terms of the token cost, the KG text representation is token efficient compared to many retrieval methods using unstructured knowledge bases as the latter tend to maximize usage of the LLM context window. However, compared to non-retrieval baselines, there is significant increase in token usage cost.\nThe types of KG questions that can be answered are limited by the context window, search depth, and LLM's reasoning ability. For instance, a very large aggregation (\u2018How many mountains have elevation above 3500 meters?') would require more entities than what fits in the LLM context. Future work could look at adding other actions to the ASM, such as aggregations, that could distill the local KG into more relevant information.\nEM-in is also an imperfect metric. False negatives can arise from discrepancies in date and number formatting, text formatting, and aliasing. False positives can arise in cases in which the model does not definitively answer but includes the correct answer in its response."}, {"title": "8 Ethical Impact", "content": "We do not anticipate Tree-of-Traversals to introduce new areas of risk but it may have unstudied effects on existing risks of LLMs or KGs. We highlight the following areas. (i) Tree-of-Traversals gives new capabilities to LLMs after training. While positive in terms of accuracy, we have not evaluated its effect on wider safety metrics. (ii) Our evaluation is limited to only English versions of KGs and datasets. Tree-of-Traversals should be evaluated in other languages to ensure consistent and fair experience. (iii) We have not performed analysis under misleading or deceptive knowledge graphs. Using a publicly modifiable knowledge graph does come with the risk that information could be deceptively changed.\nIn terms of positive ethical impact, making this research public democratizes access to knowledge-augmented LLMs as this method does not require"}]}