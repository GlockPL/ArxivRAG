{"title": "Foundations and Frontiers of Graph Learning Theory", "authors": ["Yu Huang", "Min Zhou", "Menglin Yang", "Zhen Wang", "Muhan Zhang", "Jie Wang", "Hong Xie", "Hao Wang", "Defu Lian", "Enhong Chen"], "abstract": "Recent advancements in graph learning have revolutionized the way to understand and analyze data with complex structures. Notably, Graph Neural Networks (GNNs), i.e. neural network architectures designed for learning graph representations, have become a popular paradigm. With these models being usually characterized by intuition-driven design or highly intricate components, placing them within the theoretical analysis framework to distill the core concepts, helps understand the key principles that drive the functionality better and guide further development. Given this surge in interest, this article provides a comprehensive summary of the theoretical foundations and breakthroughs concerning the approximation and learning behaviors intrinsic to prevalent graph learning models. Encompassing discussions on fundamental aspects such as expressiveness power, generalization, optimization, and unique phenomena such as over-smoothing and over-squashing, this piece delves into the theoretical foundations and frontier driving the evolution of graph learning. In addition, this article also presents several challenges and further initiates discussions on possible solutions.", "sections": [{"title": "1 INTRODUCTION", "content": "Real-world datasets can be naturally represented as graphs, where nodes represent entities interconnected by edges denoting relationships. Graph related tasks encompass a broad spectrum, spanning node classification [1, 2, 3], link prediction [4, 5], graph classification/regression [6, 7], as well as generation tasks [8, 9]. The applications of these tasks extend to diverse areas such as property prediction in molecules [10, 11], traffic analysis [12, 13], social network analysis [14, 15], physics simulations [16, 17], and combinatorial optimization [18, 19].\nSolving these diverse tasks demands a sufficiently rich embedding of the graph or node that captures structural properties as well as attribute information. While graph embeddings have been a widely-studied topic, including spectral embeddings and graph kernels, recently, Graph Neural Networks (GNNs) have emerged as an empirically and broadly successful model class that, as opposed to, e.g., spectral embeddings, allows to adapt the embedding to the task at hand, generalizes to other graphs of the same input type, and incorporates attributes.\nThe objective of graph learning entails the discovery of a function that can approximate the target function based on the available information of a given graph. This process involves several sequential steps. Firstly, it identifies a comprehensive set of functions, such as graph neural networks, capable of representing the target function with sufficient expressiveness. Subsequently, the function that provides the best approximation of the target function is estimated by minimizing a designated loss function (such as noise contrastive estimation loss or cross-entropy). Finally, the estimated function from the previous step is utilized to predict outcomes on test data, resulting test errors that composed of error accumulated in the above steps. In crafting an optimal graph model, three pivotal considerations typically shape the approach:\nExpressiveness also known as representation power explores if target functions can be approximated well by a graph model. For functions on graphs, representational power has mainly been studied in terms of graph isomorphism, i.e., studying the graphs that a designed model can distinguish or not.\nTopics to this question include graph isomorphism testing, subgraph counting, representing invariance/equivariance under permutations, etc.\nGeneralization asks how well the estimated function is performing according to the population risk, as a function of the number of data points and model properties. To quantify the generalization ability, the generalization error bound is a good measurement. Typically, generalization analyses involve the complexity of the model class, the target function, the data, and the optimization procedure.\nOptimization investigates conditions under which training algorithms like gradient descent or training tricks can be shown to be provably effective in terms of faster convergence or good generalization of a given GNN. Possible solutions are training tricks or strategies to ensure the algorithm converges to the global or acceptable local minima as soon as possible.\nBy carefully addressing one or more of the aforementioned aspects mentioned-above, considerable graph learning models have emerged daily, as consolidated in references [20, 21, 22]. Since the models evolve to be characterized by highly intricate network architectures to meet the diverse scenario and high performance needs, the importance of comprehending the fundamental principles underlying these models becomes evident. Regarding the rapid growth of theoretical analysis on graph models, there are fragments in the area [23, 24, 25], with overlap in certain subjects covered but different focuses. A holistic landscape of the progress and advancements in graph learning is still vacant.\nFor instance, Jegelka [23] summarizes a selection of emerging theoretical results on approximation and generalization properties of messaging passing GNNs, with emphasis on the mathematical connections, serving for the mathematicians. Regarding expressiveness, Morris et al. [24] provide an extensive review on algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem. Zhang et al. [25] further broaden the regime to popular GNN variants through the lens of graph bi-connectivity. In the domain of deep learning, training strategies and hyperparameter selections play crucial roles. However, current surveys on optimization in deep learning primarily focus on general machine learning or neural networks operating with Euclidean data [26, 27]. There is a clear need for a comprehensive summary of optimization strategies tailored specifically for graph structures to enhance the performance and efficiency of graph neural networks and related models. Very recently, Morris et al. [28] spotlight the future directions in the theory of graph machine learning that is most related to this article while the authors only provide a brief summary of existing works.\nTo fill the gap, this article delves into the aforementioned three aspects by giving precise mathematical settings and summarizing theoretical tools as well as current progress, offering readers resource on the design principles in the context of graph-structured data. In practical applications, the ideal graph models exhibit strong expressive power, minimal generalization error, as well as rapid and stable convergence speed. In theoretical analysis, however, the three subjects are often explored independently to derive possible results or insights. For each topic, we first explain their respective goals and basic concepts, then provide a detailed classification of the methods used with relevant theoretical results. Finally, we establish their interconnections, suggesting potential research directions for future graph learning theories. In addition to common fundamental aspects shared by typical graph models, issues such as performance degradation in deeper Graph Neural Networks (GNNs) and the information bottleneck stemming from long-range dependencies, known as over-squashing, are critical phenomena. Addressing and mitigating these challenges are pivotal for improving the efficacy and robustness of GNNs, particularly in tasks necessitating intricate hierarchical representations of graph-structured data. Given that resolve these two issues involves a multifaceted approach, they are detailed in a separate section for a comprehensive review."}, {"title": "2 PRELIMINARY", "content": "Graph embedding, graph kernels, and GNNs are fundamental approaches for representing and analyzing graph-structured data. While graph embedding and graph kernels have been effective in representing and analyzing graph-structured data, they face challenges in capturing complex graph interactions and may require pre-processing steps. GNNs address these limitations by combining the power of neural networks with the expressive capacity of graphs, enabling end-to-end learning of node and graph representations. Recently, Graph Transformers have emerged as an advanced technique in graph learning, applying self-attention mechanisms to capture long-range dependencies between nodes while incorporating graph structural information. These advancements have opened up new possibilities for understanding graph-structured data in various domains. The following subsections provide a detailed overview of these approaches."}, {"title": "2.1 Graph Embedding and Graph Kernels", "content": "Graph embedding and graph kernels are two fundamental approaches for representing and analyzing graph-structured data. Graph embedding converts an input graph $G=(V, E)$ into a low-dimensional vector space, capturing essential graph properties such as connectivity, community structures, or clustering patterns. Graph embedding methods can be categorized into matrix factorization-based methods (e.g., GF [29], GraRep [30]), random walk-based methods (e.g., DeepWalk [31], node2vec [32]), and deep learning-based methods (e.g., SDNE [33], DNGR [34]). While effective in capturing various graph properties, graph embedding techniques may not fully capture complex interactions and dependencies in graphs.\nGraph kernels, a subset of kernel functions, quantify the similarity between pairs of graphs by computing the inner product between their feature representations in a high-dimensional space. The Weisfeiler-Lehman (WL) subtree kernel [35] has been highly influential, inspiring extensions such as the WL Optimal Assignment kernel [36] and the WL shortest-path kernel [35]. Shortest-path kernels [37] measure similarity based on the properties of shortest paths, considering both path lengths and vertex attributes. Random walk kernels [38, 39] assess similarity by comparing label sequences of random walks on graphs. Graph kernels provide a comprehensive measure of graph similarity by considering both structure and label information, making them suitable for various graph comparison tasks.\nDespite their strengths, graph embedding and graph kernels face challenges in capturing complex graph interactions and may require pre-processing steps. GNNs address these limitations by combining the power of neural networks with the expressive capacity of graphs, enabling end-to-end learning of node and graph representations."}, {"title": "2.2 Graph Neural Networks", "content": "GNNs have emerged as a powerful framework for analyzing graph-structured data, leveraging the message-passing mechanism to enable nodes to iteratively update their representations by aggregating information from their neighbors. GNNs can be broadly categorized into three types: spectral GNNs, spatial GNNs, and geometric GNNs based on the way they operate on graph-structured data.\nSpectral GNNs operate on the spectral representation of the graph, which is obtained by the eigendecomposition of the graph Laplacian matrix [40, 41]. Spectral convolutions are defined in the Fourier domain and the graph Fourier transform of a signal $x$ is given by $F(x) = U^Tx$, and the inverse transform is $F^{-1}(x) = Ur$. Then, the graph convolution of $x$ with a filter $g$ is defined as:\n$x *_g g = U (U^Tx \\odot Ug)$, (1)\nwhere $\\odot$ denotes element-wise multiplication.\nSeveral spectral GNN variants have been proposed, including Spectral CNN [42], ChebNet [43], and GCN [44]. However, spectral GNNs face challenges in generalizing across different graph structures and suffer from high computational complexity.\nSpatial GNNs operate directly on the graph structure, leveraging spatial relationships between nodes to perform graph convolutions using a message-passing mechanism.\nThe Message Passing Neural Network (MPNN) [45] provides a general framework for spatial GNNs, which is defined as:\n$x_v^{(k)} = U_k (x_v^{(k-1)}, \\sum_{u \\in N(v)} M_k (x_v^{(k-1)}, x_u^{(k-1)}, x_{vu}))$. (2)\nwhere $x_v^{(k)}$ represents the embedding of node $v$ at layer $k$, $N(v)$ denotes the neighborhood of node $v$, $M_k(\\cdot)$ is the message function that computes the message from node $u$ to node $v$, $U_k(\\cdot)$ is the update function that updates the node embedding based on the aggregated messages, and $x_{vu}$ represents the edge features between nodes $v$ and $u$ (if available).\nGraphSAGE [46] addresses the inductive and scalability by employing neighborhood sampling:\n$x_v^{(k)} = \\Phi(W^{(k)}. AGG\\({x_u^{(k-1)}, \\forall u \\in SN(v)}\\))$, (3)\nwhere $SN(v)$ represents the sampled neighborhood of node $v$, $AGG(\\cdot)$ is the aggregation function, and $\\Phi(\\cdot)$ is an activation function.\nGraph Attention Network (GAT) [47] introduces attention mechanisms to learn the relative importance of neighboring nodes:\n$x_v^{(k)} = (\\Sigma_{u\\in N(v)} \\alpha_{vu}^{(k)}) W^{(k)}x_v^{(k-1)})$, (4)\nwhere $\\alpha_{vu}^{(k)}$ represents the attention weight assigned to the edge between nodes $v$ and $u$ at layer $k$.\nGraph Isomorphism Network (GIN) [48] introduces an adjustable weight parameter to better capture structural information:\n$x_v^{(k)} = MLP((1+ \\epsilon^{(k)})x_v^{(k-1)} + \\sum_{u \\in N(v)} x_u^{(k-1)})$, (5)\nwhere $\\epsilon^{(k)}$ is a learnable parameter that adjusts the weight of the central node's own features.\nGeometric GNNs operate on graphs with additional geometric features, such as node positions and edge lengths or angles, to capture the spatial relationships between nodes and edges in graph-structured data. These additional geometric features are particularly relevant in domains like molecular modeling, where the spatial arrangement of atoms and bonds plays a crucial role in determining the properties and behavior of molecules. By leveraging the geometric information, Geometric GNNs can learn more expressive and informative representations compared to standard GNNs. However, when dealing with geometric graphs, Geometric GNNs must also consider the symmetries and equivariances present in the data.\nTo address these challenges, several Geometric GNN architectures have been proposed. Directional Message Passing Neural Networks (DMPNNs) [49] extend the message passing framework by incorporating directional information based on the relative positions of nodes in space. This allows the model to capture the spatial relationships between nodes and learn direction-aware representations. Equivariant Graph Neural Networks (EGNNs) [50] are designed to be equivariant to rotations and translations of the input graph. They achieve this by using equivariant message passing operations and representing node features as high-dimensional vectors that transform according to the group of symmetries.\nGNNs have emerged as a powerful and effective framework for analyzing graph-structured data. However, GNNs also face challenges such as the over-smoothing problem, over-squashing issue, and difficulty in capturing long-range dependencies. Despite these limitations, ongoing research aims to address these challenges and further advance the field of GNNs, enabling their application to a wide range of real-world problems involving graph-structured data."}, {"title": "2.3 Graph Transformer", "content": "Graph Transformers aim to leverage the power of self-attention mechanisms to capture long-term dependencies among nodes while incorporating graph structural information. Existing graph transformer models can be categorized into the following three groups.\nDesigning the architecture of graph Transformer. Depending on the relative positions of GNN and Transformer layers, three typical designs have been proposed: (a) building Transformer blocks on top of GNN blocks, e.g., GraphTrans [51], GraphiT [52] and Grover [53], (b) alternately stacking GNN and Transformer blocks, e.g., Mesh Graphormer [54], and (c) running GNN and Transformer blocks in parallel and combining their outputs, e.g., Graph-BERT [55].\nImproving positional embeddings with graph structural information. For example, Graph Transformer [56] proposes to use Laplacian eigenvectors as positional embeddings, which are defined by the eigendecomposition of the graph Laplacian matrix. Other methods, such as Graphormer [57] and Graph-BERT [55], propose to use degree centrality and other heuristic measures as additional positional embeddings.\nModifying attention mechanisms based on graph priors. The third group of Graph Transformers aims to modify the attention mechanisms by injecting graph priors. One common approach is to mask the attention matrix based on the graph structure, allowing each node to attend only to its neighbors [56]. Another approach is to add graph-based bias terms to the attention scores, such as the spatial bias in Graphormer [57] and the proximity-enhanced multi-head attention in Gophormer [58].\nGraph Transformer models have achieved remarkable success in various domains. However, the optimal way to incorporate graph information into Transformers remains an open question, and the choice of architecture may depend on the specific task and dataset at hand."}, {"title": "3 EXPRESSIVE POWER", "content": "In deep learning theory, the term \"expressive power\" is often used interchangeably with function approximation capability. However, defining the expressive power of Graph Neural Networks (GNNs) in graph learning proves challenging due to the intricate nature of graph-related tasks. Some studies, inspired by deep learning methodologies, explore functions that GNNs can effectively approximate. Alternatively, a conventional approach involves assessing the capacity of GNNs to differentiate non-isomorphic graphs, a fundamental challenge in graph learning. Additionally, certain research endeavors connect the expressive power of GNNs to combinatorial problems or the computation of specific graph properties. These investigations are intimately linked and offer valuable insights into understanding the expressive capabilities of GNNs within the context of graph-based tasks.\nIn this section, we will elaborate on the theory of the expressive power of GNNs. The hierarchy of the WL algorithm for graph isomorphism problem is the most intuitive measurement and it is the mainstream approach to describe and compare the expressive power of different GNN models. From this point, there are various methods to devise expressive GNNs that are more powerful than 1-WL and we provide their corresponding theoretical results. Finally, we return to the approximation ability that is fundamental for the expressive power of neural networks in deep learning to analyze the universality of GNN."}, {"title": "3.1 Notations", "content": "Before reviewing the the theory of expressive power, we introduce some basic notations. $G=(V, E)$ denotes a graph where $V=\\{V_1, V_2, . . ., V_n\\}$ is the node set and $E \\subseteq V \\times V$ is the edge set. $A \\in \\{0, 1\\}^{N\\times N}$ denotes the adjacency matrix and $\\tilde{A} = A + I$ denotes the adjacency matrix considering self-loops. The Laplacian matrix of an undirected graph is defined as $L = D \u2013 A$ where $D \\in \\mathbb{R}^{N \\times N}$ is the degree matrix of $A$ with $D_{ii} = \\sum_{j=1}^N A_{ij}$. The degree matrix and Laplacian matrix of $A$ is denoted as $\\tilde{D}$ and $\\tilde{L} = \\tilde{D} - \\tilde{A}$ respectively. $\\mathbb{A} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$ denotes the normalized $\\mathbb{A}$. If available, $X$ denotes the initial feature matrix of the nodes and $x^{(l)}_v$ denotes the embedding of node $v$ in l-th layer. $N(v)$ denotes the neighbors of $v$. $\\{...\\}$ denotes the sets while $\\{\\{...\\}\\}$ denotes the multi-sets."}, {"title": "3.2 Graph isomorphism problem and WL algorithm", "content": "The graph isomorphism problem involves determining whether two graphs, $G_1$ and $G_2$, are structurally identical. Formally, two graphs are considered isomorphic if a bijection exists between their nodes such that edges in $G_1$ are preserved in $G_2$.\nTo deal with the graph isomorphism problem, the Weisfeiler-Lehman (WL) algorithm [59] is a well-known heuristic algorithm that can be implemented effectively. Its classical form, 1-dimensional Weisfeiler-Lehman (1-WL), also known as color refinement algorithm assigns a label to each node initially and iteratively updates the label via aggregating information from its neighbors. The procedure of the 1-WL algorithm is given in Algorithm 1, where the HASH function plays the most crucial part as it is required to be injective to guarantee that the different neighborhood information can map to different labels. The Figure 1 provides an illustration of 1-WL in distinguishing non-isomorphic graphs within 2 iterations. Initially, two non-isomorphic graphs $G_1$ and $G_2$ are given without node features, thus the embedding of each node is set to be identical."}, {"title": "3.3 Connect GNN with 1-WL", "content": "It is noticed that the role of neighbor aggregation and update in GNN is analogous to that of the hash function operated on one node and its neighbor in 1-WL. Inspired by this close relationship, Xu et al. [61] first study the expressive power of GNNs with a theoretical framework by viewing the message passing on a node as a rooted subtree and representing the set of its neighbor feature by a multiset. In this way, the aggregation function of GNN can be seen as functions defined on multisets. The authors compare the expressive power between MPNN and 1-WL in distinguishing non-isomorphic graphs and conclude MPNN is at most as powerful as the 1-WL, which is formally given as follows:\nTheorem 1 (Expressive power of MPNN). Let $G_1$ and $G_2$ be any two non-isomorphic graphs. If a message passing GNN maps $G_1$ and $G_2$ to different embeddings, the 1-WL also decides $G_1$ and $G_2$ to be non-isomorphic.\nSince the aggregation and readout function in GNN are not necessarily injective, the Theorem 1 holds. Further, they prove that if the neighbor aggregation function and graph readout function are injective, the GNN is exactly as powerful as 1-WL. Based on the condition and theory of multisets, they devise a novel GNN architecture named Graph Isomorphism Network (GIN) that is exactly as powerful as the 1-WL.\nGraph Isomorphism networks(GINs) [61]:\n$x_v^{(k)} = MLP^{(k)} ((1+\\epsilon^{(k)})x_v^{(k-1)} + \\sum_{u \\in N(v)} x_u^{(k-1)})$, (6)"}, {"title": "3.4 GNNs beyond 1-WL", "content": "Although previous works have built GNNs that are as powerful as 1-WL, they have weaknesses due to the limited expressive power of 1-WL. For example, they cannot distinguish some pairs of graphs with any parameter matrix unless they have identical labels. More severely, they fail to count simple graph substructures such as triangles [63] which is of great importance in computational chemistry [64] and social network analysis [65]. Therefore, many works try to devise GNNs beyond the expressive power of 1-WL."}, {"title": "3.4.1 High-order GNNs", "content": "k-WL based. One straightforward way to build GNNs beyond 1-WL is resorting to k-WL. Morris et al. [62] propose k-GNNs based on set k-WL algorithm which is a variant of k-WL. Literally, they consider the subgraphs of k nodes instead of k-tuple of nodes to make the algorithm more efficient and take the graph structure information into account. To be specific, the set containing all k-sets of V is denoted as $\\[V\\]_k = \\{S \\subseteq V||S| = k\\}$ which are the subgraphs of k nodes. In addition, the neighbor of a k-set S is defined as the k-set with exactly one different node i.e. $N_{v, k}(S) = \\{J \\in [V]_k||J\\cap S| = k \u2013 1\\}$. Although the set k-WL is weaker than k-WL, it has its own advantage that is more powerful than 1-WL and more scalable than k-WL.\nIn k-GNNs, the embedding of the subgraph S of k nodes in layer t is denoted by $x^{(t)}(S)$ and the initial feature assigned to each subgraph induced by S represents the isomorphic type of the corresponding subgraph. Then the embeddings of k-GNNs can be updated by a message passing scheme according to the Equation (7).\n$x^{(t)}(S) = \\sigma(W^{(t)} \\odot x^{(t-1)}(S) + \\sum_{U\\in N_{V, k}(S)} W^{(t)}x^{(t-1)}(U))$. (7)\nSince the set k-WL is more powerful than 1-WL, the k-GNN is more powerful than MPNNs and has proven to be as powerful as set k-WL with suitable initialization of parameter matrices. The expressive power of the k-GNN is characterized by Theorem 2 given as follows:\nTheorem 2 (Expressive power of k-GNN). Let $G_1$ and $G_2$ be any two non-isomorphic graphs. If a k-GNN maps $G_1$ and $G_2$ to different embeddings, the k-set WL also decides $G_1$ and $G_2$ to be non-isomorphic.\nInvariant and equivariant layer based. Graph Neural Networks constructed with high-order tensors offer a novel strategy to address the constraints associated with the 1-WL algorithm. Demanding the representation of a graph remain unchanged under permutations of nodes (invariance) and ensuring node representations reflect consistent transformations corresponding to node reordering (equivariance) respectively, invariance and equivariance stand as pivotal tenets in invariant graph learning.\nWe use $S_n$ to denote the symmetry group acting on $\\[n\\] = \\{1, 2, . . ., n\\}$ and $\\mathbb{R}^{n^k}$ to denote the set of k-order tensors. For a tensor $X \\in \\mathbb{R}^{n^k}$ and a permutation $\\sigma \\in S_n$, we define the permutation on the tensor as $(\\sigma \\cdot X)_{\\sigma(i_1), \\sigma(i_2),..., \\sigma(i_k)} = X_{i_1, i_2,..., i_k}$. Then the invariant and equivariant functions can be defined formally as follows:\nDefinition 1 (Invariant function). A function $f: \\mathbb{R}^{n^k} \\rightarrow \\mathbb{R}$ is said to be invariant if $f (\\sigma \\cdot X) = f(X)$ for every permutation $\\sigma \\in S_n$ and every $X \\in \\mathbb{R}^{n^k}$.\nDefinition 2 (Equivariant function). A function $f: \\mathbb{R}^{n^k} \\rightarrow \\mathbb{R}^{n^{k'}}$ is said to be equivariant if $f(\\sigma \\cdot X) = \\sigma \\cdot f(X)$ for every permutation $\\sigma \\in S_n$ and every $X \\in \\mathbb{R}^{n^k}.^1$\nNote that, in graph learning, the $X \\in \\mathbb{R}^{n^k}$ is the tensor representation of the graph, and each k-tuple $(i_1, i_2,..., i_k)$ can be seen as a hyperedge in the graph. For example, for k = 2, the adjacency matrix is a 2-order tensor representation of the graph and $X_{ij}$ indicates the existence of edge $(i, j)$. When attaching a feature vector of dimension d to each hyperedge, the tensor is represented by $X \\in \\mathbb{R}^{n^k \\times d}$. Since the permutation is only defined on node indices, i.e. $(\\sigma \\cdot X)_{\\sigma(i_1), \\sigma(i_2),..., \\sigma(i_k), k+1)} = X_{i_1, i_2,..., i_k, k+1}$, the invariant function $f : \\mathbb{R}^{n^k \\times d} \\rightarrow \\mathbb{R}$ and equivariant function $f: \\mathbb{R}^{n^k \\times d} \\rightarrow \\mathbb{R}^{n^{k'} \\times d}$ follow similar modification in definition.\nMaron et al. [66] provide a full characterization of all linear invariant and equivariant layers acting on a k-order tensor for the first time by solving the fixed-point equation of the permutation matrix group. Specifically, for layers devoid of bias and features, the dimension of the linear invariant and equivariant layer space is precisely given as follows:\nTheorem 3 (Dimension of linear invariant and equivariant layers). The space of invariant linear layer $L : \\mathbb{R}^{n^k} \\rightarrow \\mathbb{R}$ and equivariant linear layer $L : \\mathbb{R}^{n^k} \\rightarrow \\mathbb{R}^{n^{k'}}$ are of dimension $b(k)$ and $b(k + 1))$ respectively, where $b(k)$ is the k-th Bell number that represents the number of ways a set of n elements can be partitioned into non-empty subsets.\nFrom Theorem 3, it is surprising to find that the dimension of the space is independent of the size of the graph which enables us to apply the same GNN with a given order of linear invariant and equivariant layers to graphs of any size. The Theorem 3 can be further generalized to the layers with bias and features or multi-node sets and derive similar results. For more detailed information, readers can refer to the original paper.\nWith the formula of all linear invariant and equivariant layers, Maron et al. [66] prove that the GNN built by the layers can approximate any message passing network to an arbitrary precision on a compact set, which implies that\n1. if l \u2260 k, \u03c3 also needs to be mapped to a group representation in the target space"}, {"title": "3.4.2 Graph property based GNNs", "content": "Substructure based GNN. In addition to the challenges in distinguishing non-isomorphic graphs, GNNs also encounter obstacles in quantifying simple substructures like triangles and cliques [63], which is of great importance in various real applications such as drug discovery [64] and social network studies [65]. Therefore, the capability to detect and count substructures serves as an intuitive metric to evaluate the expressive power of GNNs.\nChen et al. [63] initiate the exploration by providing a theoretical framework for studying the expressive power of GNNs via substructure counting. Specifically, they define two types of counting on attribute graphs: containment-count and matching-count, representing the number of subgraphs and induced subgraphs isomorphic to a specified substructure respectively.\nDefinition 3 (Containment-count and matching-count). Let $G_P$ be a graph that we refer to as a pattern or substructure. We define $C(G, G_P)$, called the containment-count of $G_P$ in $G$, to be the number of subgraphs of $G$ that are isomorphic to $G_P$. We define $M(G, G_P)$, called the matching-count of $G_P$ in $G$, to be a number of induced subgraphs of $G$ that are isomorphic to $G_P$.\nSince the induced graphs belong to graphs, $M(G, G_P) < C(G, G_P)$ always holds. With this framework, they analyze the previous GNN architectures and WL algorithm concerning the two substructure counting criteria. The derived results are given below.\n1-WL, 2-WL and 2-IGN cannot perform matching-count of any connected substructures with 3 or more nodes. However, they can perform containment-count of star-shaped substructures.\nk-WL and k-IGN is able to perform both matching-count and containment-count of patterns of k nodes."}, {"title": "3.4.3 Subgraph GNNs", "content": "Motivated by the observation that non-isomorphic graphs always have non-isomorphic subgraphs, the subgraph GNNs have been popular recently for its solid theoretical guarantee as well as flexible design. Typically, the subgraph GNNs can be categorized into node-based and edge-based. The node-based subgraph GNNs are more common in practice while edge-based subgraph GNNs compute the representation of edge-node pairs additionally. Roughly, they implement three steps: subgraph extraction by a specific policy, message passing to obtain individual representation and graph pooling.\nFollowing the procedure, existing works propose subgraph GNNs with a variety of architectures. Cotta et al. [98] adopt node removal to generate subgraphs based on graph reconstruction conjecture. Papp and Wattenhofer, [99] prove that node-marking is a more expressive approach than intuitive node removal. Bevilacqua et al. [100] represent each graph as a set of subgraphs and study four simple but effective subgraph selection policies: node-deleted subgraphs, edge-deleted subgraphs, and two corresponding variants of ego-networks. Wijesinghe and Wang [101] incorporate the local structure captured by overlap subgraphs into a message passing scheme to obtain GNN architecture that is more powerful than 1-WL. You et al. [102] add identity information to the center node of each subgraph to break the symmetry while Huang et al. [83] implement pairs of node identifiers assigned to the center node and one of its neighborhoods. Zhang et al. [103] perform message passing on rooted subgraphs around each node instead of the rooted subtree. Similar to Zhang's work, Zhao et al. [104] utilize a base GNN as a kernel to encode the star subgraph of each node to generate multiple subgraph-node embeddings. Thiede et al. [105] consider an automorphism group of subgraphs to construct expressive GNNs.\nBesides the various methods to design powerful subgraph GNNs, there are some works that focus on analyzing the expressive power of subgraph GNNs. Frasca et al. [106] provides an extensive analysis of the expressive power of node-based subgraph GNNs from the perspective of symmetry. They observe that the node-based policies define a bijection between nodes and subgraphs thus the expressive power of node-based subgraphs GNNs can be characterized by one single permutation group acting jointly on nodes and subgraphs while previous works often consider two permutation groups defined on nodes and subgraphs separately. Besides, The symmetry structure described by the new permutation group is highly consistent with that of IGN. With this observation, they bound the expressive power of node-based subgraphs GNNs as follows:\nTheorem 8 (Expressive power of 3-IGN). The 3-IGN can implement the node-based policies of subgraph GNNs thus the expressive power of node-based subgraph GNNs is bounded by 3-IGN which is proved to be as powerful as 2-FWL(3-WL)."}, {"title": "3.4.4 Non-equivariant GNNs", "content": "Resorting to some non-equivariant operations can directly break the symmetry of MPNN, thus enhancing the expressive power of GNNs to go beyond 1-WL. For instance, the relational pooling [112] inspired by joint exchange-ability [113] is inherently permutation-invariant for taking an average of all permutations on a graph. Formally, the relational pooling obtains the embedding of the graph G with an arbitrary function f as follows.\n$\\phi(A,X) = \\frac{1}{n!} \\sum_{\\sigma \\in S_n} f(\\sigma \\cdot A, \\sigma \\cdot X)$, (11)\nwhere \u03c3 is the permutation defined on the symmetric group $S_n$. To improve the expressive power of GNN with the relational pooling, the authors attach each node a permutation-sensitive identifier thus making the method non-equivariant, which can formulated as concatenating a one-hot encoding to the feature. The derived novel GNN architecture called RP-GNN is defined as follows.\n$\\phi(A,X) = \\frac{1}{n!} \\sum_{\\sigma \\in S_n} f(A, [X, \\sigma \\cdot I_n])$, (12)\nwhere $I_n \\in \\mathbb{R}^{n\\times n}$ is the identity matrix and we omit the permutation acting on the graph since the GNN is permutation-invariant.\nBased on the the Equation 12, the authors further prove that the RP-GNN is strictly more powerful than the original GNN in terms of distinguishing non-isomorphic graphs, which provides a practical tool to boost the expressive power GNN. Therefore, equipping the GIN with the relational pooling can easily derive a GNN that is more powerful than 1-WL. In addition, the local relational pooling is able to help GNNs count triangles and 3-stars empirically [63].\nBesides the relational pooling, there are some other intuitive non-equivariant techniques to increase the expressive power of GNN. Papp et al. [114] utilize dropout techniques to remove a certain proportion of nodes during the train and test phase. Sato et al. [115] and Abbound et al. [116] add random features drawn from a standard uniform distribution to the initialization of node features. Sato et al. [117] introduces port numbering that is widely used in distributed local algorithms to GNN which we will discuss in the next subsection. Those non-equivariant techniques are easy to implement but their performance cannot always be guaranteed since they do not preserve the permutation equivariance property of GNNs."}, {"title": "3.5 Connect GNN with combinatorial problems", "content": "Besides the graph isomorphism problem, GNNs have been used to solve some NP-hard combinatorial problems in recent years, including minimum dominating set problem and minimum vertex cover problem [118, 119]. Since those problems cannot be solved in polynomial time concerning the input size if we assume that P \u2260 NP, the GNNs are merely able to provide sub-optimal solutions with certain approximation ratios. Therefore, it is also feasible to analyze the expressive power of GNNs by the approximation ratio that they can achieve for those combinatorial problems [117].\nTo better reveal the role of GNNs in combinatorial problems, Sato et al. [117] connect GNNs to distributed local algorithm [120] that is efficient in solving combinatorial problems and specify two classes of GNNs: multiset-broadcasting GNN (MB-GNN) and set-broadcasting GNN (SB-GNN) as follows.\nDefinition 4 (MB-GNN and SB-GNN).\n$x_v^{(k)} = f(\\{\\{x_u^{(k-1)}|u \\in N(v)}\\})$, (MB-GNN), (13)\n$x_v^{(k)} = f(\\{x_u^{(k-1)}|u \\in N(v)\\})$, (SB-GNN). (14)\nAccording to Definition 4, the MB-GNN corresponds to the MPNN while SB-GNN is a special class of MB-GNN that restricts the aggregated embeddings to be a set. To break the symmetry of message passing, the authors introduce port numbering that is widely used in distributed algorithms to GNNs that enables the GNN to send different messages to different neighbors, which obtains a new class of GNN named vector-vector consistent GNN(VVc \u2013 GNN) that is strictly more powerful than previous MB-GNNs. The VVC - GNNs updates the node feature as follows.\n$x_v^{(k)} = f(p(u, v), p(v, u), x_u^{(k-1)}|u \\in N(v)) (VVc-GNN), (15)\nwhere $p(v, u)$ is the port number of $v$ that edge $(v,u)$ connects to. Further, the authors propose the most powerful VVC - GNN called consistent port numbering GNNs(CPNGNNs) that aggregate the features by concatenation. The theorem given as follows demonstrates the performance of CPNGNNs in solving combinatorial problems.\nTheorem 10 (Approximation ratio of CPNGNN in solving combinatorial problems). CPNGNNs can achieve at most $\\frac{\\Delta}{8}+1$-approximation for the minimum dominating set problem and at most 2-approximation for the minimum vertex cover problem where \u0394 is the maximum degree in the input graph.\nAlthough the derived approximation ratio is far from optimal, it can be further improved by additional information about the graph. Later, Sato et al. [115] proposed a simple but efficient technique to boost the expressive power of GNN by concatenating a random feature sampled i.i.d. form a uniform distribution to the initial feature. Equipped With this slight modification, the authors prove that the GIN can achieve a near-optimal approximation ratio for the minimum dominating set problem and minimum vertex cover problem."}, {"title": "3.6 Approximation ability of GNN", "content": "Having explored the expressive capabilities of GNNs across diverse graph-related tasks thus far, this subsection discusses the approximation theory\u2014an essential framework for describing expressive power within deep learning [121, 122]. Specifically, our attention shifts towards the graph functions that GNNs can effectively approximate to by analyzing current approximation results. Additionally, we illustrate the close relationship between the approximation ability and the ability to distinguish non-isomorphic graphs.\nSince the graph embedding is always assumed to be invariant to the permutation of nodes, it is natural to ask whether a GNN can approximate any invariant functions to evaluate its expressive power. From this point of view, Maron et al. [123] first analyze the expressive power of G-invariant networks formulated in Equation 8 that are networks with invariant or equivariant linear with respect to arbitrary subgroup G of a symmetric group $S_n$, in terms of approximating any continuous invariant functions. Formally, the universality of G-invariant networks is given as follows.\nTheorem 11 (Universality of G-invariant GNN). Let $f: \\mathbb{R}^{n^k} \\rightarrow \\mathbb{R}$ be a continuous G-invariant function that satisfies $f(x) = f(\\sigma x)$ for all $x \\in \\mathbb{R}^n$ and $\\sigma \\in G \\leq S_n$, and $K \\subset \\mathbb{R}^{n^k}$ a compact set. Then there exists a G-invariant network that can approximate $f$ to an arbitrary precision."}, {"title": "3.7 Discussion", "content": "In this section, we have reviewed the theory of the expressive power of GNNs from multiple perspectives and categorized the methods to devise GNN architectures that are more powerful than 1-WL. Since the hierarchy of the WL algorithm for graph isomorphism problem is the mainstream measurement to characterize the expressive power of different GNN models, we summarize the expressive power of existing GNN architectures in terms of the WL hierarchy and the corresponding techniques in Table 1. After that, we spotlight and discuss four possible future directions on the theory of the expressive power of GNNs.\nBreak the limitation of WL algorithm. Although the hierarchy of WL algorithm has been prevalent for characterizing the expressive power of GNN in the past few years, the limitations of it has drawn more and more attention recently. On the one hand, the WL algorithm fails to measure the degree of similarity between non-isomorphic graphs due to the binary output for the graph isomorphism problem. Therefore, a more fine-grained metric for expressive power is expected. On the other hand, the WL algorithm is suspected of having the ability to represent the true expressive power of GNN model as it is demonstrated empirically that the more expressive GNN with respect to WL hierarchy do not necessarily have better performance on real world tasks [132]. As mentioned before, the WL algorithm neglects some graph properties such as distance between nodes, and thus can leave out potentially important structural information, which make WL algorithm not suitable for other interested graph-related tasks in real world. Hence, a metric with practical value is desirable.\nHowever, both of these above-mentioned two points are very challenging from the perspective of theory since it requires a transition from qualitative to quantitative and the knowledge of graph theory should be considered to grasp the essential expressive power across multifarious tasks and graphs. There are still some notable attempts. To derive a more fine-grained metric, approximate isomorphism [133] that quantifies the similarity by some graph distance metric is put forward and Boker et al. [134] propose continuous extension of both 1-WL and MPNNs to graphons via evaluating a specific graph metrics on graphons, which is capable of subtly representing the ability to capture the similar graph structure. Notably, Zhang el al. [135] derive a novel expressive measure termed homomorphism expressivity based on substructure counting under homomorphism, which provides a quantitative and practical framework to solve both two issues.\nExpressive power for node classification and link prediction. Due to the dominant WL algorithm for graph isomorphism problem, the majority of introduced works focus on the graph classification task. While node classification and link prediction are two fundamental tasks in graph learning, it is meaningful to analyze the expressive power of GNNs with respect to them in theory. A direct thought inspired by WL algorithm is to distinguish nodes and potential links that can not be determined by the 1-WL algorithm, which can be resolved through techniques discussed previously such as node-based subgraph extraction and node identifier. To characterize the expressive power of GNNs for node classification and link prediction tasks further, one can derive a novel version of WL.\nFor example, Barcelo et al. [136] propose the relational WL algorithm to study the expressive power of architectures for link prediction over knowledge graphs. Later, Huang et al. [137] generalized the results to a wider range of models and designed a framework called C-MPNN whose expressive power can be characterized by both the relational WL algorithm and first-order logic. Hu et al. [138] discusses the link expressive power of a series of GNNs based 2-WL, 2-FWL and their local variants. Zhang et al. [139] reveal the fundamental expressivity limitation of combining two node representations obtained by a GNN as a link representation and propose a labeling trick to enhance GNN's link expressive power.\nFor the node classification task, the similarity between different nodes should be taken into consideration and a phenomenon called over-smoothing that the feature of nodes become indistinguishable in deep GNNs has gained much attention. We will introduce it in detail in Section 6.\nExpressive power of graph transformer. The graph transformer is a popular topic in graph representation learning in past few years for producing SOTA results on several graph benchmarks, especially the tasks requiring long-range dependency. To delve into the great success of graph transformers, researchers have studied the expressive power of graph transformers extensively. One attempt is to establish the connection between graph transformers and MPNN. Kim et al. [97] prove that the graph transformer with appropriate positional encoding can approximate any linear permutation-equivariant operators and thus is not less powerful than k-IGN and k-WL and strictly more powerful than MPNN. Conversely, Cai et al. [140] show that by augmenting the input graph with a virtual node connecting to all graph nodes, MPNN can simulate certain type of graph transformer under mild assumptions.\nAlthough the connection demonstrates the powerful expressivity of graph transformer, the general transformer architecture does not have an advantage over GNN architecture in terms of expressive power since it is permutation-equivariant and thus fails to distinguish nodes with different positions. More specifically, Zhou et al. [141] prove that the k-order graph transformers operating on k-order tensors without the positional encoding to structural information are strictly less expressive than k-WL. Therefore, more efforts are made to study and utilize the additional expressive power brought by positional encoding and structural encoding.\nFor instance, Black et al. [142] compare the expressive power of Absolute Positional Encoding(APE) and Relative Positional Encoding(RPE) in terms of distinguishing non-isomorphic graphs by introducing a variant of WL algorithm. With this framework, the authors prove that the two types of positional encoding are equivalent in distinguishing non-isomorphic graphs and further provide an approach to convert the positional encoding to each other while maintaining the expressive power. Similarly, Zhu et al. [143] also propose a novel WL algorithm named SEG-WL based on structural encoding to characterize the expressive power of graph transformers. One notable position encoding is the eigenfunction of Laplacian. Equipped with the positional encoding, the graph transformers can go beyond 1-WL [95, 96] and further proved to be universal [96]. However, such positional encodings are not permutation-equivariant thus recent works attempt to design equivariant Laplacian positional encoding [144, 145].\nExpressive power of GNNs on specific graphs. Besides the common graph represented by (V, E, X), many graphs arising in real applications have additional properties and constraints thus corresponding GNN architectures are proposed to handle the tasks on them. To provide a theoretical guarantee for applying the architectures on real-world tasks and further improve the performance, the characterization of expressive power of GNNs on the specific graphs is instrumental since the original WL hierarchy does not take the distinction of specific graphs into account. For geometric graphs that are widely used to represent a 3D atomic system, Joshi et al. [146] propose a geometric version of WL(GWL) by considering geometric graph isomorphism that requires underlying graphs to be not only topologically isomorphic but also equivalent concerning some symmetry groups of permutation, translation, rotation, and reflection. Using the framework, the authors analyze the impact of some key factors of geometric GNNs including depth, tensor order, and body order on the expressive power and further derive the equivalence between the geometric graph isomorphism test and universal approximation ability of geometric GNNs [147, 148]."}, {"title": "4 GENERALIZATION", "content": "Generalization refers to the ability of a hypothesis or a learning algorithm to work well on unseen data, which is one of the most critical perspectives of machine learning algorithms. To quantitatively analyze the generalization property, the generalization (error) bound provides a theoretical guarantee and has drawn much attention in deep learning.\nIn this paper, we focus on the generalization bound on graph learning and provide a systematic analysis. Specifically, we consider the generalization bound of GNNs for graph classification task and node classification task. Although the dependent and unstructured property of graph data and the complex design of graph neural networks pose difficulty for deriving the generalization bound of GNNs precisely, the framework and methods developed in deep learning theory can facilitate the computation and provide insightful generalization bound with some assumptions and simplification. Consistent with the conventions in deep learning, we classify the literature into four groups: the complexity of hypothesis space-based, PAC-Bayes based, stability-based, and graph neural tangent kernel (GNTK) based, with GNTK serving as an extension of the neural tangent kernel (NTK). Notably, the primary disparity lies in how the generalization bound on graphs integrates the statistical characteristics of graphs and the learning matrix of Graph Neural Networks (GNNs). Subsequent subsections will detail these categories individually."}, {"title": "4.1 Notations and problem formulation", "content": "Before delving into the concrete methods to derive generalization bounds of GNN, we first introduce some necessary notations and provide the formulation of the problem. Consider the training dataset $Z = \\{(X_1,Y_1), (X_2,Y_x) ...,(X_m,Y_m)\\}$ with m samples where $X_i \\in X$ is the feature and $y_i \\in Y$ is the label. All $(x_i, Y_i)$ are i.i.d. observed from an underlying distribution $D$ over $X \\times Y$. Then the learning algorithm A attempts to learn hypothesis $h : X \\rightarrow Y$ from the training dataset and the hypothesis space $H$ consists of all possible hypothesis $h$. For any hypothesis h learned by an algorithm A from the training dataset Z, the empirical risk $\\hat{R}_z(h)$ and expected risk $R(h)$ with respect a loss function $l$ are defined respectively as follows,"}, {"title": "4.2 Complexity of hypothesis space based", "content": "Given that the learning algorithm is situated within a hypothesis space, the complexity of this space plays a crucial role in defining the range of problems the algorithm can address. Therefore, it is common practice to analyze the complexity of the hypothesis space in order to establish a generalization bound, which can be assessed using theoretical measures such as Vapnik-Chervonenkis dimension (VC-dim) [150] and Rademacher complexity [151, 152], along with covering numbers [153]. Here we only introduce the first two methods that have been used to obtain the generalization bound of GNN as the covering number often acts as an alternative of Rademacher complexity and is applied to more complex settings in the theory of deep learning.\nNote that the VC-dim [150] for binary classification task is defined upon growth function. Therefore, we will first introduce the definition of the growth function before discussing the generalization guarantee associated with VC dimension.\nDefinition 7 (Growth function). For any non-negative integer m, the growth function of hypothesis space H is defined as follows:\n$\\Pi_H(m) := \\max_{X_1,...,X_m \\in X} .|\\{h(x_1),..., h(x_m) : h \\in H\\}|$. (18)\nThe growth function represents the maximum number of possible labeling of m data points by H. If $\\Pi_H(m) = 2^m$ which means any labeling of m data points can be perfectly classified by one hypothesis in the hypothesis space, we say H shatters the dataset $\\{x_1,...,x_m \\in X\\}$, and the VC-dim of H is defined as the largest m. From the above definition, the VC dim is independent of the data distribution thus a more universal approach. The generalization bound based on VC-dim can be obtained by the following theorem.\nTheorem 12 (Generalization guarantee of VC-dim). Assume hypothesis space H has VC-dim D, m is the training set size. Then, for any $\\delta > 0$, with probability $1 - \\delta$, the following inequality holds for any $h \\in H$,\n$R(h) \\leq \\hat{R}(h) + \\sqrt{\\frac{2D \\log(\\frac{em}{D}) + log(\\frac{1}{\\delta})}{2m}}$. (19)\nBounding the VC-dim by $O(p^4N^2)$ where p is the number of parameters in GNN and N is the size of input nodes, Scarselli [154] derives generalization bounds of node classification task on graphs for the first time. The result suggests that generalization ability improves with the increasing number of nodes and parameters. Besides, it is worth noting that the bound is identical to that of recurrent neural networks.\nIn contrast to VC-dimension, Rademacher complexity [151, 152] takes into account the distribution of data and provides a more nuanced assessment of the richness of the hypothesis space by quantifying how well the hypothesis set can accommodate random noise. The empirical definition of Rademacher complexity is outlined below.\nDefinition 8 (Empirical Rademacher complexity). Given a function class H and a dataset Z with m samples $Z = \\{x_1,...,x_m\\}$, the empirical Rademacher complexity of H can be defined by:\n$\\Re_m(H) = E_{\\sigma} [\\sup_{h \\in H} \\frac{1}{m} \\sum_{i=1}^m \\sigma_i h(x_i)]$, (20)\nwhere $\\sigma = \\{\\sigma_1,..., \\sigma_m\\}$ is a random vector whose items are uniformly chosen form $\\{\u22121, +1\\}$ and samples are i.i.d. generated from a distribution $D$.\nFurther, the Rademacher complexity of H is defined as $\\Re(H) = E_{S \\sim D^m} \\Re_m(H)$. The (empirical) Rademacher complexity can deduce the generalization bound on binary classification and regression tasks according to the following theorem.\nTheorem 13 (Generalization guarantee of Rademacher complexity). Given a function class H containing functions $h : X \\rightarrow [a, b]$ and a dataset S with m samples $S = \\{x_1,...,x_m\\}$. Then for any $\\delta > 0$ and $h \\in H$, with probability $1 - \\delta$, we have,\n$E[h(x)] \\leq \\frac{1}{m} \\sum_{i=1}^m h(x_i) + 2\\Re_m(H) + (b-a) \\sqrt{\\frac{In \\frac{2}{\\delta}}{2m}}$, (21)\n$E[h(x)] \\geq \\frac{1}{m} \\sum_{i=1}^m h(x_i) - 2\\Re_m(H) + 3(b \u2212 a)\\sqrt{\\frac{In \\frac{2}{\\delta}}{2m}}$. (22)\nIn order to apply the theorem to the graph classification task that calculates the average of binary prediction on each node to obtain the graph label, Garg et al. [155] bound the Rademacher complexity of GNN by considering the Rademacher complexity of the computation tree of each node whose update formula of GNN follows a mean field form:\n$x_v^{(l)} = \\Phi(W_1 x_v^{(l-1)} + W_2\\rho(\\Sigma_{u\\in N(v)} g(x_u^{(l-1)})))$. (23)\nTo illustrate the varying generalization bound with respect to the different parameters, they define a combination parameter $C = C_\\Phi C_g C_\\rho B_w$ that is the product of the Lipschitz constant of $\\rho, g, \\Phi$ and $B_w$ denotes the bound norm of weight $W_2$. Let d denote the maximum degree of nodes in the graph, r denote the dimension of embedding, L denote the number of layers, m denote the size of training nodes and \u03b3 denote margin parameter in the loss, the dependency can be expressed as follows:\n$O(\\frac{rd}{\\sqrt{m}\\gamma})$  for $C<\\frac{1}{4}$\n$O(\\frac{rdL}{m\\gamma})$ for $C=\\frac{1}{4}$\n$O(\\frac{rd\\sqrt{r}L}{\\sqrt{m}\\gamma})$ for $C>\\frac{1}{4}$ (24)\nThe generalization bound given by Equation (24) is much tighter than the counterpart of VC-dim since the latter has a higher order dependency on the size of parameters of the neural network and the size of input N is at least the maximum degree d. In addition, they find that the bound for GNN in the equation is comparable to that of RNN, which indicates that GNN can be seen as the sequentialized RNN. It is noticed that the observation is consistent with that of the generalization bound obtained by VC-dim. Besides, Lv [156] also proves a generalization bound by Rademacher complexity of GCN with one hidden layer on the node classification task. The derived bound is sharp for having a lower bound matching the upper bound.\nIn contrast to above-mentioned works which focus on generalization bounds for GNNs in the inductive setting, there is a growing body of research examining generalization bounds in the more realistic transductive setting. These studies typically leverage transductive Rademacher complexity [157] to derive generalization bound, which considers unobserved samples when deriving generalization bounds. Formally, denote the size of training set and test set as m and u respectively, the target is to learn a function that generates the best predictions for the label of the test set based on the features for both training and test set and the labels for the training set. Within the transductive framework, Oono and Suzuki [158] establish a generalization bound for multi-scale GNNs in node classification tasks, while Esser et al. [159] refine the bound for multi-layer GNNs within a planted model to investigate graph and feature alignment. In addition, Deng et al. [160] provide a generalization upper bound to guarantee to performance of GNN based recommendation systems. Recently, Tang and Liu [161] introduce a high probability generalization bound for GNNs trained with SGD in the transductive setting, accounting for the comparable performance of shallow and deep models, as well as the effectiveness of techniques like early stopping and dropedge to some extent."}, {"title": "4.3 PAC-Bayes based", "content": "Over recent years, the probably approximately correct (PAC)-Bayesian approach [162, 163] has garnered significant attention for providing a more realistic and tighter generalization bound compared to traditional VC-dimension-based and Rademacher complexity-based bounds. Originally introduced to measure the learnability of a problem, the PAC concept [164] assesses whether a learning algorithm can output the correct result with high probability when given a specific number of training examples from an unknown distribution. Since the bounds provided by classical PAC framework is unsatisfactory due to the large hypothesis space, the PAC-Bayesian approach incorporates the Bayesian view by putting a prior distribution over the hypothesis space to derive tighter generalization bounds for the target machine learning model. In the following discussion, we will present the framework developed by PAC-Bayesian theory for analyzing the generalization bound of GNNs.\nIn this context, we focus on the margin bound, utilizing a multi-class margin loss function with a threshold \u03b3 utilized. The original empirical loss is defined as:\n$\\hat{l}_{Z, \\gamma}(h) = \\frac{1}{m} \\sum_{i=1}^m [l(x_i, y_i) + \\gamma] \\cdot \\mathbb{I} [h(x_i)[y_i] \\leq \\max_{j \\neq y} h(x_i)[j] + \\gamma]$, (25)\nwhere Z is the training set with m examples. Further, the original generalization loss is given as:\n$l_{D, \\gamma}(h) = Pr_{x \\sim D} [h(x)[y] \\leq \\max_{j \\neq y} h(x)[j] + \\gamma]$, (26)\nwhere D is an unknown distribution from which samples are generated.\nAssume there is a prior distribution P and a posterior distribution Q over the model parameter \u03b8 in Bayesian theory, then the empirical loss and generalization loss are defined by expectation which are denoted as $E_{\\theta \\sim Q}[\\hat{R}_\\gamma(h(\\theta))]$ and $E_{\\theta \\sim Q}[R_\\gamma(h(\\theta))]$ respectively. The generalization bound of the model based on PAC-Bayesian can be obtained via the following theorem.\nTheorem 14 (Generalization guarantee of PAC-Bayesian). [165]: Let P and Q be the prior distribution and posterior distribution over the model with parameter \u03b8, and Z be a dataset with m samples generated i.i.d. from distribution D. Then, for any $\u03b3,\u03b4 > 0$, with probability 1 \u2013 \u03b4, we have,\n$E_{\\theta \\sim Q}[\\hat{R}_\\gamma(h(\\theta))] \\leq E_{\\theta \\sim Q}[R_\\gamma(h(\\theta))] + \\sqrt{\\frac{KL(Q||P) + log \\frac{1}{\\delta}}{2(m - 1)}}$ (27)\nIn Theorem 27, the selection of distribution pairs P and Q can be arbitrary. However, choosing disparate distributions may render the computation of KL-divergence challenging, while oversimplified choices could result in significant empirical and generalization losses. To address this issue, Neyshabur et al. [166] offer an effective approach to determine the bound by considering the posterior distribution over parameters as a perturbation distribution derived from two known distributions. The perturbation-based Bayesian generalization bound can be formally defined as follows:\nTheorem 15 (Perturbation based Bayesian generalization bound). Let $h \\in H : X \\rightarrow \\mathbb{R}^K$ be any model with parameter \u03b8, S be the dataset with m samples that generated i.i.d. from distribution D, and P be the prior distribution on the parameters that is independent of the training data. Then, for any \u03b3,\u03b4 > 0, any parameter \u0398 and any random perturbation $\\Delta \\theta$,s.t. Pro $\\[\\max_{x \\in X} |h(\\theta + \\Delta \\theta) \u2013 h(\\theta)|_{\\infty} < 1\\] > 1 \u2013 \u03b4, with probability at least 1 \u2013 \u03b4, we have:\n$l_{D, \\gamma}(h(\\theta)) \\leq l_{Z, \\gamma}(h(\\theta)) + 1 \\sqrt{\\frac{2KL(Q(\\theta + \\Delta \\theta)||P) + log \\frac{m}{\\delta}}{2(m - 1)}}$ (28)"}, {"title": "4.4 Stability based", "content": "Besides the perturbation analysis that exerts perturbation to the weight matrix in PAC-Bayesian approach to guarantee the generalization, it is intuitive that the performance of an algorithm with good generalization ability does not degrade much after a slight change in the training data. The stability is the measurement to quantify the change of the output of an algorithm when the training data is modified. Here we only introduce the uniform stability [171] that is most widely-used in deriving the generalization bound. Before giving the formal definition of the uniform stability, we introduce modification operation to the training data in advance.\nData Modification [171] Let X be the input space, Y be the output space and $Z = X \u00d7 Y$. For $X_i \\in X$ and $y_i \\in Y \\subseteq R$, let Z be a training set with m examples $Z = \\{z_1 = (x_1, y_1),..., z_m = (x_m, y_m)\\}$ and all samples are i.i.d. from D. Two fundamental modifications to the training set Z are as follows:\nRemoving $i^{th}$ data point in the set Z is represented as,$\nZ^{\\i} = z_1,..., z_{i-1}, z_{i+1},..., z_m$, (30)\nReplacing $i^{th}$ data point in the set Z is represented as,\n$Z^{i} = z_1,..., z_{i-1}, z_{i'}, z_{i+1},..., z_m$. (31)\nThen the uniform stability for a randomized algorithm can be defined as follows,\nDefinition 9 (Uniform stability). Let A be a randomized algorithm trained on dataset S and $A_z$ is the output hypothesis, then A is $\\beta_m$-uniformly stable with respect to a loss function l, if it satisfies,\n$sup_{Z,z} |E_A[l(A_z, z)] - E_A[l(A_z^{\\i}, z)]| \\leq \\beta_m$ (32)\nThe request for uniform stability is strict since it holds for every possible training set with m samples. In addition, a randomized algorithm is taken into consideration to analyze the model optimized by some randomized algorithm e.g. stochastic gradient descent (SGD). Then, the generalization gap based on the uniform stability can be derived by the following Theorem.\nTheorem 17 (Generalization guarantee of uniform stability). Assume a uniformly stable randomized algorithm $(A_z, \\beta_m)$ with a bound loss function $0 \\leq l(A_z, z) \\leq M$ for any Z, z. Then, for any \u03b4 > 0, with probability 1 \u2013 \u03b4 over choice of an i.i.d size-m training set Z, we have:\n$E[R(A_z) - \\hat{R}(A_z)] < 2\\beta_m + (4m\\beta_m + M) \\sqrt{\\frac{log \\frac{1}{\\delta}}{2m}}$ (33)\nWith Theorem 17, the generalization bound can be obtained via proving the uniform stability of an algorithm. Besides, to ensure that the generalization gap can converge to 0, it requires $\\beta_m$ to decay faster than $O(\\frac{1}{\\sqrt{m}})$ as $m \\rightarrow \\infty$.\nAssuming the randomized algorithm to be a single-layer GCN optimized by SGD, Verma and Zhang [172] first analyze the uniform stability of GCN and derives the stability-based bound on the node classification task. They derive the uniform stability constant and corresponding generalization bound of the model, which are given in Theorem 18 and Theorem 19, respectively.\nTheorem 18 (Uniform stability of GCN using SGD). Let the loss and activation function be Lipschitz-continuous and smooth functions. Then a single layer GCN model training by SGD algorithm for T iterations is $\\beta_m$-uniformly stable, where\n$\\beta_m < \\prod_{t=1}^T (1 + \\eta \\nu_{\\epsilon v} \\sigma a_l x_{max}) \\frac{\\eta \\alpha_\\sigma^2 (x_{max})^2}{m}$, (34)\nwhere $\\eta > 0$ is the learning rate, $a_l, a_\\sigma > 0$ are Lipschitz constants for the loss function and activation function respectively, $\\nu_\\epsilon, \\nu_\\sigma > 0$ are the Lipschitz constants for the gradient of loss function and activation function respectively, and $x_{max}$ is the largest absolute eigenvalue of the graph diffusion matrix.\nTheorem 19 (Generalization bound for GCN training by SGD). Let the loss and activation function be Lipschitz-continuous and smooth functions, then a single layer GCN model training by SGD algorithm for T iterations is $\\beta_m$-uniformly stable. The generalization bound based on uniform stability is given as follows,\n$\\mathbb{E}_{SGD} [\\hat{R}(A_z) \u2013 R(A_z)] \\leq 2O((\\prod_{t=1}^T (1 + \\eta \\nu_{\\epsilon v} \\sigma a_l x_{max})) \\frac{\\eta \\alpha_\\sigma^2 (x_{max})^2}{m} )+ O(((\\prod_{t=1}^T (1 + \\eta \\nu_{\\epsilon v} \\sigma a_l x_{max})) \\frac{\\eta \\alpha_\\sigma^2 (x_{max})^2}{m} ) + M) \\sqrt{\\frac{log 1/ \\delta}{2m}}$, (35)\nThe obtained uniform stability constant and generalization bound make sense since the largest absolute eigenvalue of graph diffusion matrix can be controlled by various normalization methods, which is in accord with the stable training behavior and better performance when adopting a normalized graph diffusion matrix. They also stress the importance of batch-normalization that has similar effect on the training of multi-layer GNNs. However, the bound is incomparable to those of previous for considering a randomized learning algorithm. Later, Zhou and Wang [173] extend the work to multi-layer GNNs and further demonstrate that increasing number of layers can enlarge the generalization gap.\nDifferent from Zhang [172]'s setting, Cong et al. [174] consider a transductive setting and study the generalization bound of multi-layer GCN optimized by full batch gradient descent on node classification task by transductive uniform stability. They delve into the Lipschitz continuity, smoothness, and gradient scale to compare the generalization bound of different models."}, {"title": "4.5 GNTK based", "content": "Neural tangent kernel (NTK) [175", "176": "is a kernel-based method to analyze over-parameterized neural networks trained by gradient descent in the infinite-width limit in deep learning. Du et al. [177", "complexity": "nTheorem 20 (GNTK Generalization bound for GNN). Given n training data $\\{(G_i", "l": "mathbb{R} \u00d7 \\mathbb{R} \\rightarrow [0,1"}]}