{"title": "Foundations and Frontiers of Graph Learning Theory", "authors": ["Yu Huang", "Min Zhou", "Menglin Yang", "Zhen Wang", "Muhan Zhang", "Jie Wang", "Hong Xie", "Hao Wang", "Defu Lian", "Enhong Chen"], "abstract": "Recent advancements in graph learning have revolutionized the way to understand and analyze data with complex structures. Notably, Graph Neural Networks (GNNs), i.e. neural network architectures designed for learning graph representations, have become a popular paradigm. With these models being usually characterized by intuition-driven design or highly intricate components, placing them within the theoretical analysis framework to distill the core concepts, helps understand the key principles that drive the functionality better and guide further development. Given this surge in interest, this article provides a comprehensive summary of the theoretical foundations and breakthroughs concerning the approximation and learning behaviors intrinsic to prevalent graph learning models. Encompassing discussions on fundamental aspects such as expressiveness power, generalization, optimization, and unique phenomena such as over-smoothing and over-squashing, this piece delves into the theoretical foundations and frontier driving the evolution of graph learning. In addition, this article also presents several challenges and further initiates discussions on possible solutions.", "sections": [{"title": "1 INTRODUCTION", "content": "Real-world datasets can be naturally represented as graphs, where nodes represent entities interconnected by edges denoting relationships. Graph related tasks encompass a broad spectrum, spanning node classification [1, 2, 3], link prediction [4, 5], graph classification/regression [6, 7], as well as generation tasks [8, 9]. The applications of these tasks extend to diverse areas such as property prediction in molecules [10, 11], traffic analysis [12, 13], social network analysis [14, 15], physics simulations [16, 17], and combinatorial optimization [18, 19].\nSolving these diverse tasks demands a sufficiently rich embedding of the graph or node that captures structural properties as well as attribute information. While graph embeddings have been a widely-studied topic, including spectral embeddings and graph kernels, recently, Graph Neural Networks (GNNs) have emerged as an empirically and broadly successful model class that, as opposed to, e.g., spectral embeddings, allows to adapt the embedding to the task at hand, generalizes to other graphs of the same input type, and incorporates attributes.\nThe objective of graph learning entails the discovery of a function that can approximate the target function based on the available information of a given graph. This process involves several sequential steps. Firstly, it identifies a comprehensive set of functions, such as graph neural networks, capable of representing the target function with sufficient expressiveness. Subsequently, the function that provides the best approximation of the target function is estimated by minimizing a designated loss function (such as noise contrastive estimation loss or cross-entropy). Finally, the estimated function from the previous step is utilized to predict outcomes on test data, resulting test errors that com-posed of error accumulated in the above steps. In crafting an optimal graph model, three pivotal considerations typically shape the approach:\nExpressiveness also known as representation power explores if target functions can be approximated well by a graph model. For functions on graphs, representational power has mainly been studied in terms of graph isomorphism, i.e., studying the graphs that a designed model can distinguish or not.\nTopics to this question include graph isomorphism testing, subgraph counting, representing invariance/equivariance under permutations, etc.\nGeneralization asks how well the estimated function is performing according to the population risk, as a function of the number of data points and model properties. To quantify the generalization ability, the generalization error bound is a good measurement. Typically, generalization analyses involve the complexity of the model class, the target function, the data, and the optimization procedure.\nOptimization investigates conditions under which training algorithms like gradient descent or training tricks can be shown to be provably effective in terms of faster convergence or good generalization of a given GNN. Possible solutions are training tricks or strategies to ensure the algorithm converges to the global or acceptable local minima as soon as possible.\nBy carefully addressing one or more of the aforementioned aspects mentioned-above, considerable graph learning models have emerged daily, as consolidated in references [20, 21, 22]. Since the models evolve to be characterized by highly intricate network architectures to meet the diverse scenario and high performance needs, the importance of comprehending the fundamental principles underlying these models becomes evident. Regarding the rapid growth of theoretical analysis on graph models, there are fragments in the area [23, 24, 25], with overlap in certain subjects covered but different focuses. A holistic landscape of the progress and advancements in graph learning is still vacant.\nFor instance, Jegelka [23] summarizes a selection of emerging theoretical results on approximation and generalization properties of messaging passing GNNs, with emphasis on the mathematical connections, serving for the mathematicians. Regarding expressiveness, Morris et al. [24] provide an extensive review on algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem. Zhang et al. [25] further broaden the regime to popular GNN variants through the lens of graph bi-connectivity. In the domain of deep learning, training strategies and hyper-parameter selections play crucial roles. However, current surveys on optimization in deep learning primarily focus on general machine learning or neural networks operating with Euclidean data [26, 27]. There is a clear need for a comprehensive summary of optimization strategies tailored specifically for graph structures to enhance the performance and efficiency of graph neural networks and related models. Very recently, Morris et al. [28] spotlight the future directions in the theory of graph machine learning that is most related to this article while the authors only provide a brief summary of existing works.\nTo fill the gap, this article delves into the aforementioned three aspects by giving precise mathematical settings and summarizing theoretical tools as well as current progress, offering readers resource on the design principles in the context of graph-structured data. In practical applications, the ideal graph models exhibit strong expressive power, minimal generalization error, as well as rapid and stable convergence speed. In theoretical analysis, however, the three subjects are often explored independently to derive possible results or insights. For each topic, we first explain their respective goals and basic concepts, then provide a detailed classification of the methods used with relevant theoretical results. Finally, we establish their interconnections, suggesting potential research directions for future graph learning theories. In addition to common fundamental aspects shared by typical graph models, issues such as performance degradation in deeper Graph Neural Networks (GNNs) and the information bottleneck stemming from long-range dependencies, known as over-squashing, are critical phenomena. Addressing and mitigating these challenges are pivotal for improving the efficacy and robustness of GNNs, particularly in tasks necessitating intricate hierarchical representations of graph-structured data. Given that resolve these two issues involves a multifaceted approach, they are detailed in a separate section for a comprehensive review."}, {"title": "2 PRELIMINARY", "content": "Graph embedding, graph kernels, and GNNs are fundamental approaches for representing and analyzing graph-structured data. While graph embedding and graph kernels have been effective in representing and analyzing graph-structured data, they face challenges in capturing complex graph interactions and may require pre-processing steps. GNNs address these limitations by combining the power of neural networks with the expressive capacity of graphs, enabling end-to-end learning of node and graph representations. Recently, Graph Transformers have emerged as an advanced technique in graph learning, applying self-attention mechanisms to capture long-range dependencies between nodes while incorporating graph structural information. These advancements have opened up new possibilities for understanding graph-structured data in various domains. The following subsections provide a detailed overview of these approaches."}, {"title": "2.1 Graph Embedding and Graph Kernels", "content": "Graph embedding and graph kernels are two fundamental approaches for representing and analyzing graph-structured data. Graph embedding converts an input graph $G = (V, E)$ into a low-dimensional vector space, capturing essential graph properties such as connectivity, community structures, or clustering patterns. Graph embedding methods can be categorized into matrix factorization-based methods (e.g., GF [29], GraRep [30]), random walk-based methods (e.g., DeepWalk [31], node2vec [32]), and deep learning-based methods (e.g., SDNE [33], DNGR [34]). While effective in capturing various graph properties, graph embedding techniques may not fully capture complex interactions and dependencies in graphs.\nGraph kernels, a subset of kernel functions, quantify the similarity between pairs of graphs by computing the inner product between their feature representations in a high-dimensional space. The Weisfeiler-Lehman (WL) subtree kernel [35] has been highly influential, inspiring extensions such as the WL Optimal Assignment kernel [36] and the WL shortest-path kernel [35]. Shortest-path kernels [37] measure similarity based on the properties of shortest paths, considering both path lengths and vertex attributes. Random walk kernels [38, 39] assess similarity by comparing label sequences of random walks on graphs. Graph kernels provide a comprehensive measure of graph similarity by considering both structure and label information, making them suitable for various graph comparison tasks.\nDespite their strengths, graph embedding and graph kernels face challenges in capturing complex graph interactions and may require pre-processing steps. GNNs address these limitations by combining the power of neural networks with the expressive capacity of graphs, enabling end-to-end learning of node and graph representations."}, {"title": "2.2 Graph Neural Networks", "content": "GNNs have emerged as a powerful framework for analyzing graph-structured data, leveraging the message-passing mechanism to enable nodes to iteratively update their representations by aggregating information from their neighbors. GNNs can be broadly categorized into three types: spectral GNNs, spatial GNNs, and geometric GNNs based on the way they operate on graph-structured data.\nSpectral GNNs operate on the spectral representation of the graph, which is obtained by the eigendecomposition of the graph Laplacian matrix [40, 41]. Spectral convolutions are defined in the Fourier domain and the graph Fourier transform of a signal x is given by $F(x) = U^Tx$, and the inverse transform is $F^{-1}(x) = Ur$. Then, the graph convolution of x with a filter g is defined as:\n$x *_g g = U (U^T x \\odot U g),$ \nwhere $\\odot$ denotes element-wise multiplication.\nSeveral spectral GNN variants have been proposed, including Spectral CNN [42], ChebNet [43], and GCN [44]. However, spectral GNNs face challenges in generalizing across different graph structures and suffer from high computational complexity.\nSpatial GNNs operate directly on the graph structure, leveraging spatial relationships between nodes to perform graph convolutions using a message-passing mechanism.\nThe Message Passing Neural Network (MPNN) [45] provides a general framework for spatial GNNs, which is defined as:\n$x_v^{(k)} = U_k (x_v^{(k-1)}, \\sum_{u \\in N(v)} M_k (x_v^{(k-1)}, x_u^{(k-1)}, x_{vu})).$ \nwhere $x_v^{(k)}$ represents the embedding of node v at layer k, $N(v)$ denotes the neighborhood of node v, $M_k(\\cdot)$ is the message function that computes the message from node u to node v, $U_k(\\cdot)$ is the update function that updates the node embedding based on the aggregated messages, and $x_{vu}$ represents the edge features between nodes v and u (if available).\nGraphSAGE [46] addresses the inductive and scalability by employing neighborhood sampling:\n$x_v^{(k)} = \\phi(W^{(k)}. AGG({x_u^{(k-1)}, \\forall u \\in SN(v)})),$ \nwhere $SN(v)$ represents the sampled neighborhood of node v, $AGG(\\cdot)$ is the aggregation function, and $\\phi(\\cdot)$ is an activation function.\nGraph Attention Network (GAT) [47] introduces attention mechanisms to learn the relative importance of neighboring nodes:\n$x_v^{(k)} = (\\sum_{u \\in N(v)} \\alpha_{vu}^{(k)}) W^{(k)}x_u^{(k-1)}),$ \nwhere $\\alpha_{vu}^{(k)}$ represents the attention weight assigned to the edge between nodes v and u at layer k.\nGraph Isomorphism Network (GIN) [48] introduces an adjustable weight parameter to better capture structural information:\n$x_v^{(k)} = MLP((1+ \\epsilon^{(k)})x_v^{(k-1)} + \\sum_{u \\in N(v)} x_u^{(k-1)}),$ \nwhere $\\epsilon^{(k)}$ is a learnable parameter that adjusts the weight of the central node's own features.\nGeometric GNNs operate on graphs with additional geometric features, such as node positions and edge lengths or angles, to capture the spatial relationships between nodes and edges in graph-structured data. These additional geometric features are particularly relevant in domains like molecular modeling, where the spatial arrangement of atoms and bonds plays a crucial role in determining the properties and behavior of molecules. By leveraging the geometric information, Geometric GNNs can learn more expressive and informative representations compared to standard GNNs. However, when dealing with geometric graphs, Geometric GNNs must also consider the symmetries and equivariances present in the data.\nTo address these challenges, several Geometric GNN architectures have been proposed. Directional Message Passing Neural Networks (DMPNNs) [49] extend the message passing framework by incorporating directional information based on the relative positions of nodes in space. This allows the model to capture the spatial relationships between nodes and learn direction-aware representations. Equivariant Graph Neural Networks (EGNNs) [50] are designed to be equivariant to rotations and translations of the input graph. They achieve this by using equivariant message passing operations and representing node features as high-dimensional vectors that transform according to the group of symmetries.\nGNNs have emerged as a powerful and effective framework for analyzing graph-structured data. However, GNNs also face challenges such as the over-smoothing problem, over-squashing issue, and difficulty in capturing long-range dependencies. Despite these limitations, ongoing research aims to address these challenges and further advance the field of GNNs, enabling their application to a wide range of real-world problems involving graph-structured data."}, {"title": "2.3 Graph Transformer", "content": "Graph Transformers aim to leverage the power of self-attention mechanisms to capture long-term dependencies among nodes while incorporating graph structural information. Existing graph transformer models can be categorized into the following three groups.\nDesigning the architecture of graph Transformer. Depending on the relative positions of GNN and Transformer layers, three typical designs have been proposed: (a) building Transformer blocks on top of GNN blocks, e.g., GraphTrans [51], GraphiT [52] and Grover [53], (b) alternately stacking GNN and Transformer blocks, e.g., Mesh Graphormer [54], and (c) running GNN and Transformer blocks in parallel and combining their outputs, e.g., Graph-BERT [55].\nImproving positional embeddings with graph structural information. For example, Graph Transformer [56] proposes to use Laplacian eigenvectors as positional embeddings, which are defined by the eigendecomposition of the graph Laplacian matrix. Other methods, such as Graphormer [57] and Graph-BERT [55], propose to use degree centrality and other heuristic measures as additional positional embeddings.\nModifying attention mechanisms based on graph priors. The third group of Graph Transformers aims to modify the attention mechanisms by injecting graph priors. One common approach is to mask the attention matrix based on the graph structure, allowing each node to attend only to its neighbors [56]. Another approach is to add graph-based bias terms to the attention scores, such as the spatial bias in Graphormer [57] and the proximity-enhanced multi-head attention in Gophormer [58].\nGraph Transformer models have achieved remarkable success in various domains. However, the optimal way to incorporate graph information into Transformers remains an open question, and the choice of architecture may depend on the specific task and dataset at hand."}, {"title": "3 EXPRESSIVE POWER", "content": "In deep learning theory, the term \"expressive power\" is often used interchangeably with function approximation capability. However, defining the expressive power of Graph Neural Networks (GNNs) in graph learning proves challenging due to the intricate nature of graph-related tasks. Some studies, inspired by deep learning methodologies, explore functions that GNNs can effectively approximate. Alternatively, a conventional approach involves assessing the capacity of GNNs to differentiate non-isomorphic graphs, a fundamental challenge in graph learning. Additionally, certain research endeavors connect the expressive power of GNNs to combinatorial problems or the computation of specific graph properties. These investigations are intimately linked and offer valuable insights into understanding the expressive capabilities of GNNs within the context of graph-based tasks.\nIn this section, we will elaborate on the theory of the expressive power of GNNs. The hierarchy of the WL algorithm for graph isomorphism problem is the most intuitive measurement and it is the mainstream approach to describe and compare the expressive power of different GNN models. From this point, there are various methods to devise expressive GNNs that are more powerful than 1-WL and we provide their corresponding theoretical results. Finally, we return to the approximation ability that is fundamental for the expressive power of neural networks in deep learning to analyze the universality of GNN."}, {"title": "3.1 Notations", "content": "Before reviewing the the theory of expressive power, we introduce some basic notations. $G = (V, E)$ denotes a graph where $V = {v_1, v_2, . . ., v_n }$ is the node set and $E \\subseteq V \\times V$ is the edge set. $A \\in {0,1}^{N \\times N}$ denotes the adjacency matrix and $\\tilde{A} = A + I$ denotes the adjacency matrix considering self-loops. The Laplacian matrix of an undirected graph is defined as $L = D - A$ where $D \\in \\mathbb{R}^{N \\times N}$ is the degree matrix of A with $D_{ii} = \\sum_{j=1}^N A_{ij}$. The degree matrix and Laplacian matrix of A is denoted as $\\tilde{D}$ and $\\tilde{L} = \\tilde{D} - \\tilde{A}$ respectively. $A = \\tilde{D}^{-\\frac{1}{2}} A \\tilde{D}^{-\\frac{1}{2}}$ denotes the normalized $\\tilde{A}$. If available, X denotes the initial feature matrix of the nodes and $x_v^l$ denotes the embedding of node v in l-th layer. $N (v)$ denotes the neighbors of v. $\\{...\\}$ denotes the sets while $\\{\\{...\\}\\}$ denotes the multi-sets."}, {"title": "3.2 Graph isomorphism problem and WL algorithm", "content": "The graph isomorphism problem involves determining whether two graphs, $G_1$ and $G_2$, are structurally identical. Formally, two graphs are considered isomorphic if a bijection exists between their nodes such that edges in $G_1$ are preserved in $G_2$.\nTo deal with the graph isomorphism problem, the Weisfeiler-Lehman (WL) algorithm [59] is a well-known heuristic algorithm that can be implemented effectively. Its classical form, 1-dimensional Weisfeiler-Lehman (1-WL), also known as color refinement algorithm assigns a label to each node initially and iteratively updates the label via aggregating information from its neighbors. The procedure of the 1-WL algorithm is given in Algorithm 1, where the HASH function plays the most crucial part as it is required to be injective to guarantee that the different neighborhood information can map to different labels. The Figure 1 provides an illustration of 1-WL in distinguishing non-isomorphic graphs within 2 iterations. Initially, two non-isomorphic graphs $G_1$ and $G_2$ are given without node features, thus the embedding of each node is set to be identical.\nIn the first iteration, each node pass the embedding of itself together with the multi-set of embedding of the neighbor nodes through an injective HASH function, which obtain the novel embedding representing the degree of the node. Since the two graph $G_1$ and $G_2$ have the same degree distribution, single iteration of 1-WL cannot distinguish them. In the second iteration, the same operation is implemented again but on novel embeddings. This time the two graphs $G_1$ and $G_2$ get different node embedding distributions so the algorithm outputs 'non-isomorphic', which means that two non-isomorphic graphs $G_1$ and $G_2$ are distinguished by the 1-WL in the second iteration.\nThe 1-WL algorithm terminates in $O(|U|+|V|)$ iterations and has been shown to effectively distinguish any pair of non-isomorphic random graphs with high probability as the graph size approaches infinity. However, it may struggle to differentiate certain classes of non-isomorphic graphs, like regular graphs of the same order. To address this limitation, more powerful algorithms capable of distinguishing a broader range of non-isomorphic graphs are desired. One such advancement is the k-dimensional Weisfeiler-Lehman (k-WL) algorithm, which extends the capabilities of the 1-WL algorithm by assigning labels to each k-tuple of nodes and the set of k-tuple of nodes is denoted as $V^k$. In the k-WL algorithm, the i-th neighbor of a k-tuple is defined by replacing the i-th element in the k-tuple with every node in the graph. This approach enhances the expressive power of the algorithm compared to 1-WL, allowing for more robust differentiation between complex graph structures. Additionally, the k-dimensional folklore Weisfeiler-Lehman (k-FWL) algorithm is another extension that shares similarities with k-WL but differs slightly in the aggregation of k-tuples. These advancements in the Weisfeiler-Lehman framework offer improved capabilities for distinguishing non-isomorphic graphs and contribute to enhancing the overall performance and versatility of graph isomorphism testing algorithms. The procedure of the k-WL is given in Algorithm 2.\nEven though the k-WL is more powerful than 1-WL and actually increasing k can obtain more powerful algorithm in distinguishing non-isomorphic graphs, the algorithm has its limitation since there always exists a pair of non-isomorphic graphs for each k such that the k-WL algorithm outputs 'cannot determine'.\nAt the end of the subsection, we provide some useful results about the expressive power of WL algorithm and its variants [60] which will be utilized later:\n1-WL and 2-WL have equal expressive power.\nk-FWL is equivalent to (k+1)-WL and thus has equal expressive power.\nFor $k \\geq 2$, (k+1)-WL is strictly more powerful than k-WL which means that there exists a pair of non-isomorphic graphs that (k+1)-WL can distinguish but k-WL can not. This implies that the WL algorithm naturally forms a hierarchy."}, {"title": "3.3 Connect GNN with 1-WL", "content": "It is noticed that the role of neighbor aggregation and update in GNN is analogous to that of the hash function operated on one node and its neighbor in 1-WL. Inspired by this close relationship, Xu et al. [61] first study the expressive power of GNNs with a theoretical framework by viewing the message passing on a node as a rooted subtree and representing the set of its neighbor feature by a multiset. In this way, the aggregation function of GNN can be seen as functions defined on multisets. The authors compare the expressive power between MPNN and 1-WL in distinguishing non-isomorphic graphs and conclude MPNN is at most as powerful as the 1-WL, which is formally given as follows:\nTheorem 1 (Expressive power of MPNN). Let $G_1$ and $G_2$ be any two non-isomorphic graphs. If a message passing GNN maps $G_1$ and $G_2$ to different embeddings, the 1-WL also decides $G_1$ and $G_2$ to be non-isomorphic.\nSince the aggregation and readout function in GNN are not necessarily injective, the Theorem 1 holds. Further, they prove that if the neighbor aggregation function and graph readout function are injective, the GNN is exactly as powerful as 1-WL. Based on the condition and theory of multisets, they devise a novel GNN architecture named Graph Isomorphism Network (GIN) that is exactly as powerful as the 1-WL.\nGraph Isomorphism networks(GINs) [61]:\n$x_v^{(k)} = MLP^{(k)} ((1+\\epsilon^{(k)})x_v^{(k-1)} + \\sum_{u \\in N(v)} x_u^{(k-1)}),$"}, {"title": "3.4 GNNs beyond 1-WL", "content": "Although previous works have built GNNs that are as powerful as 1-WL, they have weaknesses due to the limited expressive power of 1-WL. For example, they cannot distinguish some pairs of graphs with any parameter matrix unless they have identical labels. More severely, they fail to count simple graph substructures such as triangles [63] which is of great importance in computational chemistry [64] and social network analysis [65]. Therefore, many works try to devise GNNs beyond the expressive power of 1-WL."}, {"title": "3.4.1 High-order GNNs", "content": "k-WL based. One straightforward way to build GNNs beyond 1-WL is resorting to k-WL. Morris et al. [62] propose k-GNNs based on set k-WL algorithm which is a variant of k-WL. Literally, they consider the subgraphs of k nodes instead of k-tuple of nodes to make the algorithm more efficient and take the graph structure information into account. To be specific, the set containing all k-sets of V is denoted as $[V]_k = \\{S \\subseteq V||S| = k\\}$ which are the subgraphs of k nodes. In addition, the neighbor of a k-set S is defined as the k-set with exactly one different node i.e. $N_{v,k}(S) = \\{J \\in [V]_k||J\\cap S| = k - 1\\}$. Although the set k-WL is weaker than k-WL, it has its own advantage that is more powerful than 1-WL and more scalable than k-WL.\nIn k-GNNs, the embedding of the subgraph S of k nodes in layer t is denoted by $x^{(t)} (S)$ and the initial feature assigned to each subgraph induced by S represents the isomorphic type of the corresponding subgraph. Then the embeddings of k-GNNs can be updated by a message passing scheme according to the Equation (7).\n$x^{(t)} (S) = \\sigma(W^{(t)})\\sum_{U\\in N_{V,k}(S)} x^{(t-1)}(U)).$\nSince the set k-WL is more powerful than 1-WL, the k-GNN is more powerful than MPNNs and has proven to be as powerful as set k-WL with suitable initialization of parameter matrices. The expressive power of the k-GNN is characterized by Theorem 2 given as follows:\nTheorem 2 (Expressive power of k-GNN). Let $G_1$ and $G_2$ be any two non-isomorphic graphs. If a k-GNN maps $G_1$ and $G_2$ to different embeddings, the k-set WL also decides $G_1$ and $G_2$ to be non-isomorphic."}, {"title": "Invariant and equivariant layer based.", "content": "Graph Neural Networks constructed with high-order tensors offer a novel strategy to address the constraints associated with the 1-WL algorithm. Demanding the representation of a graph remain unchanged under permutations of nodes (invariance) and ensuring node representations reflect consistent transformations corresponding to node reordering (equivariance) respectively, invariance and equivariance stand as pivotal tenets in invariant graph learning.\nWe use $S_n$ to denote the symmetry group acting on $[n] = \\{1,2,..., n\\}$ and $\\mathbb{R}^{n^k}$ to denote the set of k-order tensors. For a tensor $X \\in \\mathbb{R}^{n^k}$ and a permutation $\\sigma\\in S_n$, we define the permutation on the tensor as $(\\sigma\\cdot X)_{\\sigma(i_1),\\sigma(i_2),...,\\sigma(i_k)} = X_{i_1,i_2,...,i_k}$. Then the invariant and equivariant functions can be defined formally as follows:\nDefinition 1 (Invariant function). A function $f : \\mathbb{R}^{n^k} \\rightarrow \\mathbb{R}$ is said to be invariant if $f (\\sigma \\cdot X) = f(X)$ for every permutation $\\sigma\\in S_n$ and every $X \\in \\mathbb{R}^{n^k}$.\nDefinition 2 (Equivariant function). A function $f : \\mathbb{R}^{n^k} \\rightarrow \\mathbb{R}^{n^{k'}}$ is said to be equivariant if $f(\\sigma\\cdot X) = \\sigma\\cdot f(X)$ for every permutation $\\sigma\\in S_n$ and every $X \\in \\mathbb{R}^{n^k}$.\nNote that, in graph learning, the $X \\in \\mathbb{R}^{n^k}$ is the tensor representation of the graph, and each k-tuple $(i_1,i_2,..., i_k)$ can be seen as a hyperedge in the graph. For example, for k = 2, the adjacency matrix is a 2-order tensor representation of the graph and $X_{ij}$ indicates the existence of edge (i, j). When attaching a feature vector of dimension d to each hyperedge, the tensor is represented by $X \\in \\mathbb{R}^{n^k \\times d}$. Since the permutation is only defined on node indices, i.e. $(\\sigma\\cdot X)_{\\sigma(i_1),\\sigma(i_2),...,i_k),d+1)} = X_{i_1,i_2,...,i_k,d+1}$, the invariant function $f : \\mathbb{R}^{n^k \\times d} \\rightarrow \\mathbb{R}$ and equivariant function $f : \\mathbb{R}^{n^k\\times d} \\rightarrow \\mathbb{R}^{n^{k'}\\times d}$ follow similar modification in definition.\nMaron et al. [66] provide a full characterization of all linear invariant and equivariant layers acting on a k-order tensor for the first time by solving the fixed-point equation of the permutation matrix group. Specifically, for layers devoid of bias and features, the dimension of the linear invariant and equivariant layer space is precisely given as follows:\nTheorem 3 (Dimension of linear invariant and equivariant layers). The space of invariant linear layer $L : \\mathbb{R}^{n^k} \\rightarrow \\mathbb{R}$ and equivariant linear layer $L : \\mathbb{R}^{n^k} \\rightarrow \\mathbb{R}^{n^{k'}}$ are of dimension b(k) and b(k + 1)) respectively, where b(k) is the k-th Bell number that represents the number of ways a set of n elements can be partitioned into non-empty subsets.\nFrom Theorem 3, it is surprising to find that the dimension of the space is independent of the size of the graph which enables us to apply the same GNN with a given order of linear invariant and equivariant layers to graphs of any size. The Theorem 3 can be further generalized to the layers with bias and features or multi-node sets and derive similar results. For more detailed information, readers can refer to the original paper.\nWith the formula of all linear invariant and equivariant layers, Maron et al. [66] prove that the GNN built by the layers can approximate any message passing network to an arbitrary precision on a compact set, which implies that"}, {"title": "3.4.2 Graph property based GNNs", "content": "Substructure based GNN. In addition to the challenges in distinguishing non-isomorphic graphs, GNNs also encounter obstacles in quantifying simple substructures like triangles and cliques [63], which is of great importance in various real applications such as drug discovery [64] and social network studies [65]. Therefore, the capability to detect and count substructures serves as an intuitive metric to evaluate the expressive power of GNNs.\nChen et al. [63] initiate the exploration by providing a theoretical framework for studying the expressive power of GNNs via substructure counting. Specifically, they define two types of counting on attribute graphs: containment-count and matching-count, representing the number of subgraphs and induced subgraphs isomorphic to a specified substructure respectively.\nDefinition 3 (Containment-count and matching-count). Let $G_P$ be a graph that we refer to as a pattern or substructure. We define $C(G, G_P)$, called the containment-count of $G_P$ in G, to be the number of subgraphs of G that are isomorphic to $G_P$. We define $M(G, G_P)$, called the matching-count of $G_P$ in G, to be a number of induced subgraphs of G that are isomorphic to $G_P$.\nSince the induced graphs belong to graphs, $M(G, G_P) < C(G, G_P)$ always holds. With this framework, they analyze the previous GNN architectures and WL algorithm concerning the two substructure counting criteria. The derived results are given below.\n1-WL, 2-WL and 2-IGN cannot perform matching-count of any connected substructures with 3 or more nodes. However, they can perform containment-count of star-shaped substructures.\nk-WL and k-IGN is able to perform both matching-count and containment-count of patterns of k nodes."}, {"title": "3.4.3 Subgraph GNNs", "content": "Motivated by the observation that non-isomorphic graphs always have non-isomorphic subgraphs", "steps": "subgraph extraction by a specific policy", "98": "adopt node removal to generate subgraphs based on graph reconstruction conjecture. Papp and Wattenhofer", "99": "prove that node-marking is a more expressive approach than intuitive node removal. Bevilacqua et al. [100", "policies": "node-deleted subgraphs", "101": "incorporate the local structure captured by overlap subgraphs into a message passing scheme to obtain GNN architecture that is more powerful than 1-WL. You et al. [102", "83": "implement pairs of node identifiers assigned to the center node and one of its neighborhoods. Zhang et al. [103", "104": "utilize a base GNN as a kernel to encode the star subgraph of each node to generate multiple subgraph-node embeddings. Thiede et al. [105", "106": "provides an extensive analysis of the expressive power of node-based subgraph GNNs from the perspective of symmetry. They observe that the node-based policies define a bijection between nodes and subgraphs thus the expressive power of node-based subgraphs GNNs can be characterized by one single permutation group acting jointly on nodes and subgraphs while previous works often consider two permutation groups defined on nodes and subgraphs separately. Besides", "follows": ""}]}