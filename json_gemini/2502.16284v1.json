{"title": "MOLSPECTRA: PRE-TRAINING 3D MOLECULAR REPRESENTATION WITH MULTI-MODAL ENERGY SPECTRA", "authors": ["Liang Wang", "Shaozhen Liu", "Yu Rong", "Deli Zhao", "Qiang Liu", "Shu Wu", "Liang Wang"], "abstract": "Establishing the relationship between 3D structures and the energy states of molecular systems has proven to be a promising approach for learning 3D molecular representations. However, existing methods are limited to modeling the molecular energy states from classical mechanics. This limitation results in a significant oversight of quantum mechanical effects, such as quantized (discrete) energy level structures, which offer a more accurate estimation of molecular energy and can be experimentally measured through energy spectra. In this paper, we propose to utilize the energy spectra to enhance the pre-training of 3D molecular representations (MolSpectra), thereby infusing the knowledge of quantum mechanics into the molecular representations. Specifically, we propose SpecFormer, a multi-spectrum encoder for encoding molecular spectra via masked patch reconstruction. By further aligning outputs from the 3D encoder and spectrum encoder using a contrastive objective, we enhance the 3D encoder's understanding of molecules. Evaluations on public benchmarks reveal that our pre-trained representations surpass existing methods in predicting molecular properties and modeling dynamics.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning 3D molecular representations from geometric conformations offers a promising approach for understanding molecular geometry and predicting quantum properties and interactions, which is significant in drug discovery and materials science (Musaelian et al., 2023; Batatia et al., 2022; Liao & Smidt, 2023; Wang et al., 2023b; Du et al., 2023b). Given the scarcity of molecular property labels, self-supervised representation pre-training has been proposed and utilized to provide generalizable representations (Hu et al., 2020; Rong et al., 2020; Ma et al., 2024).\nIn contrast to contrastive learning (Wang et al., 2022; Kim et al., 2022) and masked modeling (Hou et al., 2022; Liu et al., 2023c; Wang et al., 2024b) on 2D molecular graphs and molecular languages (e.g., SMILES), the design of pre-training strategies on 3D molecular geometries is more closely aligned with physical principles. Previous studies (Zaidi et al., 2023; Jiao et al., 2023) have guided representation learning through denoising processes on 3D molecular geometries, theoretically demonstrating that denoising 3D geometries is equivalent to learning molecular force fields, specifically the negative gradient of molecular potential energy with respect to position. Essentially, these studies reveal that establishing the relationship between 3D geometries and the energy states of molecular systems is an effective pathway to learn 3D molecular representations.\nHowever, existing methods are limited to the continuous description (i.e., the potential energy function) of the molecular energy states within the classical mechanics, overlooking the quantized (discrete) energy level structures from the quantum mechanical perspective. From the quantum perspective, molecular systems exhibit quantized energy level structures, meaning that energy states can only assume specific discrete values. Specifically, different types of molecular motion, such as electronic, vibrational, and rotational motion, correspond to different energy level structures. Knowledge of these energy levels is crucial in molecular physics and quantum chemistry, as they determine the spectroscopic characteristics, chemical reactivity, and many other important molecular properties. Fortunately, experimental measurements of molecular energy spectra can reflect these structures. Meanwhile, there are many molecular spectra data obtained through experimental measurements or simulations (Zou et al., 2023; Alberts et al., 2024). Therefore, incorporating the knowledge of energy levels into molecular representation learning is expected to facilitate the development of more informative molecular representations.\nIn this paper, we propose MolSpectra, a framework that incorporates molecular spectra into the pre-training of 3D molecular representations, thereby infusing the knowledge of quantized energy level structures into the representations, as shown in Figure 1. In MolSpectra, we introduce a multi-spectrum encoder, SpecFormer, to capture both intra-spectrum and inter-spectrum peak correlations by training with a masked patches reconstruction (MPR) objective. Additionally, we employ a contrastive objective to distills the spectral features and its inherent knowledge into the learning of 3D representations. After pre-training, the resulting 3D encoder can be fine-tuned for downstream tasks, providing expressive 3D molecular representations without the need for associated spectral data. Extensive experiments over different downstream molecular property prediction benchmarks shows the superiority of MolSpectra.\nIn summary, our contributions are as follows:\n\u2022 We introduce quantized energy level structures and molecular spectra into 3D molecular representation pre-training for the first time, surpassing previous work that relied solely on physical knowledge within the scope of classical mechanics.\n\u2022 We propose SpecFormer as an expressive multi-spectrum encoder, along with the masked patches reconstruction objective for spectral representation learning.\n\u2022 We propose a contrastive objective to align molecular representations in the 3D modality and spectral modalities, enabling the pre-trained 3D encoder to infer molecular spectral features in downstream tasks without relying on spectral data.\n\u2022 Experiments across different downstream benchmarks demonstrate that our method effectively enhances the expressiveness of the pre-trained 3D molecular representations."}, {"title": "2 PRELIMINARIES", "content": null}, {"title": "2.1 NOTATIONS", "content": "Consider a molecule characterized by its 3D structure and spectra, represented as $M = (a,x, S)$. Here, $a \\in \\{1, 2, ..., 118\\}$ specifies the atomic numbers, indicating the types of atoms within the molecule. The vector $x \\in \\mathbb{R}^{3N}$ describes the conformation of the molecule, while $S$ represents its spectra. The parameter $N$ denotes the number of atoms in the molecule. Note that the atoms are arranged in the same order in both $a$ and $x$, ensuring consistency between the atomic numbers and their corresponding spatial coordinates.\n$S = (s_1,..., s_{|S|})$ represents the set of spectra for a molecule, where $|S|$ denotes the number of spectrum types considered. In our study, we focus on three types, so $|S| = 3$. The first spectrum, $s_1 \\in \\mathbb{R}^{601}$, is the UV-Vis spectrum, which spans from 1.5 to 13.5 eV with 601 data points at intervals of 0.02 eV. The second spectrum, $s_2 \\in \\mathbb{R}^{3501}$, is the IR spectrum, covering a range from 500 to 4000 cm-\u00b9 with 3501 data points at intervals of 1 cm-\u00b9. The third spectrum, $s_3 \\in \\mathbb{R}^{3501}$, is the Raman spectrum, with the same range and intervals as the IR spectrum. Together, these spectra provide a comprehensive description of the molecular characteristics across different spectral modalities."}, {"title": "2.2 PRE-TRAINING 3D MOLECULAR REPRESENTATION VIA DENOISING", "content": "Denoising has emerged as a prominent pre-training objective in 3D molecular representation learning, excelling in various downstream tasks. This method involves training models to predict and remove noise introduced deliberately into molecular structures. This approach is physically interpretable due to its proven equivalence to learning the molecular force field.\nEquivalence between denoising and learning molecular force fields. The equivalence between coordinate denoising and force field learning is established by Zaidi et al. (2023). For a given molecule $M$, perturb its equilibrium structure $x_0$ according to the distribution $p(x|x_0)$, where $x$ is the noisy conformation. Assuming the molecular distribution adheres to the energy-based Boltzmann distribution with respect to the energy function $E(\\cdot)$, then\n$\\mathcal{L}_{Denoising}(M) = \\mathbb{E}_{p(x|x_0)p(x_0)}||GNN_\\theta(x) - (x - x_0)||^2$\n$\\sim \\mathbb{E}_{p(x)} ||GNN_\\theta(x) - (-\\nabla_x E(x))||^2,$\nwhere $GNN_\\theta(x)$ denotes a graph neural network parameterized by $\\theta$, which processes the conformation $x$ to produce node-level predictions. The notation $\\sim$ signifies the equivalence of different objectives. The proof of this equivalence is provided in the Appendix A. In prior research, the energy function $E(\\cdot)$ has been defined in several forms. Below are three representative studies.\nEnergy function I: mixture of isotropic Gaussians. In Coord (Zaidi et al., 2023), the energy function is approximated using a mixture of isotropic Gaussians centered at the known equilibrium structures to replace the Boltzmann distribution, since these structures are local maxima of the Boltzmann distribution. Leveraging the equivalence between the score-matching objective and denoising autoencoders (Vincent, 2011), the following denoising-based energy function $E_{coord}(\\cdot)$ is derived:\n$E_{coord}(x) = \\frac{1}{2\\tau^2} (x - x_0)^T (x - x_0).$\nNote that this objective is derived under the assumption of isotropic Gaussian noise, i.e., $p(x|x_0) \\sim \\mathcal{N}(x_0, \\tau^2 I_{3N})$, where $I_{3N}$ represents the identity matrix of size $3N$, and the subscript $e$ indicates the coordinate denoising approach.\nEnergy function II: mixture of anisotropic Gaussians. Considering rigid and flexible components in molecular structures, isotropic Gaussian can lead to significant approximation errors. To address the anisotropic distribution, Frad (Feng et al., 2023) introduces hybrid noise on dihedral angles of rotatable bonds and atomic coordinates, incorporating fractional denoising of the coordinate noise. The equilibrium structure $x_0$ is initially perturbed by dihedral angle noise $p(\\psi_\\alpha|\\psi_0) \\sim \\mathcal{N}(\\psi_0, \\sigma^2_\\psi I_m)$, followed by coordinate noise $p(x|x_a) \\sim \\mathcal{N}(x_a, \\tau I_{3N})$. Here, $\\psi_\\alpha, \\psi_0 \\in [0,2\\pi)^m$ represent to the dihedral angles of rotatable bonds in structures $x_a$ and $x_0$, respectively, with $m$ denoting the number of rotatable bonds. The subscript $f$ indicates the fractional denoising approach. Subsequently, the energy function is induced:\n$E_{Frad}(x) = \\frac{1}{2} (x - x_0)^T \\Sigma^{-1}_{\\tau I, \\sigma\\xi} (x - x_0),$\nwhere $\\Sigma_{\\tau I,\\sigma\\xi} = \\tau I_{3N} + C C^T$, and $C \\in \\mathbb{R}^{3N\\times m}$ is a matrix used to linearly transform the dihedral angle noise into coordinate change, expressed as $\\Delta x \\approx C \\Delta \\psi$.\nEnergy function III: classical potential energy theory. SliDe (Ni et al., 2024) derives energy function from classical molecular potential energy theory (Alavi, 2020; Zhou & Liu, 2022). In this form, the total intramolecular potential energy is mainly attributed to three types of interactions: bond stretching, bond angle bending, and bond torsion. The following energy function is derived:\n$E_{slide}(r, \\theta, \\phi) = \\frac{1}{2} [k_B^T (r - r_0)] (r - r_0) + \\frac{1}{2} [k_A^T (\\theta - \\theta_0)] (\\theta - \\theta_0)\n+ [k_{\\Phi}^T (\\Phi - \\Phi_0)] (\\Phi - \\Phi_0),$\nwhere $r \\in (\\mathbb{R}_{>0})^{m_1}$, $\\theta \\in [0,2\\pi)^{m_2}$, $\\varphi \\in [0,2\\pi)^{m_3}$ represent vectors of the bond lengths, bond angles, and bond torsion angles of the molecule, respectively. $r_0$, $\\theta_0$, $\\Phi_0$ correspond to the respective equilibrium values. The parameter vectors $k_B$, $k_A$, $k_{\\Phi}$ determine the interaction strength."}, {"title": "3 THE PROPOSED MOLSPECTRA METHOD", "content": "Considering the complementarity of different spectra, we introduce multiple spectra into molecular representation learning. To effectively comprehend molecular spectra, we designed a Transformer-based multi-spectrum encoder, SpecFormer, along with a masked reconstruction objective to guide its training. Finally, a contrastive objective is employed to align the 3D encoding guided by the denoising objective with the spectra encoding guided by the reconstruction objective, endowing the 3D encoding with the capability to understand spectra and the knowledge they encompass."}, {"title": "3.1 SPECFORMER: A SINGLE-STREAM ENCODER FOR MULTI-MODAL ENERGY SPECTRA", "content": "For different types of spectra, each spectrum is independently patched and initially encoded. Then, all the resulting patch embeddings are concatenated and encoded using a Transformer-based encoder.\nPatching. Compared to directly encoding individual frequency points, we divided each spectrum into multiple patches. This approach offers two distinct advantages: (i) By forming patches from adjacent frequency points, local semantic features, such as absorption peaks, can be captured more effectively. (ii) It reduces the computational overhead of subsequent Transformer layers. Technically, each spectrum $s_i \\in \\mathbb{R}^{L_i}$ where $i = 1,\\cdots,|S|$ is first divided into patches according to the patch length $P_i$ and the stride $D_i$. When $0 < D_i < P_i$, the consecutive patches will be overlapped with overlapping region length $P_i - D_i$. When $D_i = P_i$, the consecutive patches will be non-overlapped. $L_i$ denotes the length of $s_i$. The patching process on each spectrum will generate a sequence of patches $p_i \\in \\mathbb{R}^{N_i \\times P_i}$, where $N_i = \\lfloor \\frac{L_i-P_i}{D_i} + 1\\rfloor$ is the number of patches.\nPatch encoding and position encoding. Prior to be fed into the encoder, the patches of the i-th spectrum are mapped to the latent space of dimension $d$ via a trainable linear projection $W_i\\in \\mathbb{R}^{P_i\\times d}$. A learnable additive position encoding $W_{Pos} \\in \\mathbb{R}^{N_i \\times d}$ is applied to maintain the order of the patches: $p'_i = p_iW_i + W_{Pos}$, where $p'_i \\in \\mathbb{R}^{N_i \\times d}$ denotes the latent representation of the spectrum $s_i$ that will be fed into the subsequent SpecFormer encoder.\nSpecFormer: multi-spectrum Transformer encoder. Although several encoders have been proposed to map molecular spectrum into implicit representations, such as the CNN-AM (Tao et al., 2024) based on one-dimensional convolution, these encoders are designed to encode only a single type of spectrum. In our approach, multiple molecular spectra (UV-Vis, IR, Raman) are jointly considered. When encoding multiple spectra of a molecule simultaneously, an observation caught our attention and led us to adopt a Transformer-based encoder with multiple spectra as input, similar to the single-stream Transformer in multi-modal learning (Shin et al., 2021). The observation refers to the fact that the same functional group not only causes multiple peaks within a single spectrum, but also generates peaks across different spectra. As shown on the left of Figure 3, the different vibrational modes of the methyl group (-CH3) in methanol (CH3OH) result in three peaks in the IR spectrum, indicating intra-spectrum dependencies among these peaks. A similar phenomenon occurs with the hydroxyl group (-OH) in methanol. Additionally, the aromatic ring in phenol (C6H5OH), shown on the right of Figure 3, not only produces multiple peaks in the IR spectrum due to different vibrational modes but also causes an absorption peak near 270 nm in the UV-Vis spectrum due to the $\\pi \\rightarrow \\pi^*$ transition in the aromatic ring, demonstrating the existence of inter-spectrum dependencies. Such dependencies have been theoretically studied, for example, in the context of vibronic coupling (Kong et al., 2021).\nTo capture intra-spectrum and inter-spectrum dependencies, we concatenate the embeddings obtained from patch encoding and position encoding of different spectra: $p' = p'_1 || \\cdots || p'_{|S|} \\in \\mathbb{R}^{(\\sum_i N_i)\\times d}$, and then input them into the Transformer encoder as depicted in Figure 2. Then each head $h = 1,..., H$ in multi-head attention will transform them into query matrices $Q_h = p' W_Q^h$, key matrices $K_h = p' W_K^h$ and value matrices $V_h = p' W_V^h$, where $W_Q^h, W_K^h \\in \\mathbb{R}^{d\\times d_k}$ and $W_V \\in \\mathbb{R}^{d\\times d_v}$. Afterward, a scaled product is utilized to obtain the attention output $O_h \\in \\mathbb{R}^{(\\sum_i N_i)\\times \\frac{d}{H}}$:\n$O_h = Attention(Q_h, K_h, V_h) = Softmax(\\frac{Q_h K_h^T}{\\sqrt{d_k}}) V_h.$\nThe multi-head attention block also includes BatchNorm layers and a feed forward network with residual connections as shown in Figure 2. After combining the outputs of all heads, it generates the representation denoted as $z \\in \\mathbb{R}^{(\\sum_i N_i)\\times d}$. Finally, a flatten layer with representation projection head is used to obtain the molecular spectra representation $z_S \\in \\mathbb{R}^d$."}, {"title": "3.2 MASKED PATCHES RECONSTRUCTION PRE-TRAINING FOR SPECTRA", "content": "Before distilling the spectra information into 3D molecular representation learning, we need first ensure that the spectrum encoder can effectively comprehend molecular spectra and generate spectral representations. Considering the success of masking modeling across various domains (Devlin et al., 2019; He et al., 2022; Hou et al., 2022; Xia et al., 2023; Wang et al., 2024b; Nie et al., 2023), we propose a masked patches reconstruction (MPR) objective to guide the training of SpecFormer. After the patching step, we randomly select a portion of patches according to the mask ratio $\\alpha$ and replace them with zero vectors to implement the masking. Subsequently, the masked patches undergo patch encoding and position encoding. In this way, the semantics of the masked patches (the absorption intensity at specific wavelengths) are obscured during patch encoding, while the positional information is retained to facilitate the reconstruction of the original semantics.\nAfter encoding by SpecFormer, the encoded results corresponding to the masked patches are input into a spectrum-specific reconstruction head to reconstruct the original spectral values that were masked. The mean squared error (MSE) between the reconstruction results and the original masked spectra serves as the loss function for the MPR task, guiding the training of SpecFormer:\n$\\mathcal{L}_{MPR} = \\sum_{i=1}^{|S|} \\sum_{p_{i,j} \\in P_i} || p_{i,j} - \\hat{p}_{i,j} ||^2,$\nwhere $P_i$ denotes the set of masked patches in the i-th type of molecular spectra, and $\\hat{p}_{i,j}$ denotes the reconstructed patch corresponding to the masked patch $p_{i,j}$."}, {"title": "3.3 CONTRASTIVE LEARNING BETWEEN 3D STRUCTURES AND SPECTRA", "content": "Under the guidance of the denoising objective for 3D representation learning and the MPR objective for spectral representation learning, we further introduce a contrastive objective to align the representations across these two modalities. We treat the 3D representation $z_x \\in \\mathbb{R}^d$ and spectral representation $z_s \\in \\mathbb{R}^d$ of the same molecule as positive samples, and negative samples otherwise. Subsequently, the consistency between positive samples and the discrepancy between negative samples are maximized through the contrastive objective. Given the theoretical and empirical effectiveness, we employ InfoNCE (van den Oord et al., 2018) as the contrastive objective:\n$\\mathcal{L}_{Contrast} = \\mathbb{E}_{p(z_i, z_i')} [\\log \\frac{\\exp(f_x(z_x, z_s))}{\\exp(f_1(z_1,z_s)) + \\sum_j \\exp(f_x(z_x, z_{s_j}))}\n+ \\log \\frac{\\exp(f_s(z_s, z_x))}{\\exp(f_s(z_s, z_z)) + \\sum_j \\exp(f_s(z_s, z_{x_j}))}],$\nwhere $z_i, z_i'$ are randomly sampled 3D and spectra views regarding to the positive pair $(z_x, z_s)$. $f_x(z_x, z_s)$ and $f_s(z_s, z_x)$ are scoring functions for the two corresponding views, with flexible formulations. Here we adopt $f_x(z_x, z_s) = f_s(z_s, z_x) = \\langle z_x, z_s \\rangle$.\nNote that the denoising objective can utilize any form from existing 3D molecular representation pre-training studies, enabling seamless integration of our method into these frameworks."}, {"title": "3.4 TWO-STAGE PRE-TRAINING PIPELINE", "content": "Previous pre-training efforts for 3D molecular representation have been conducted on unlabeled datasets using denoising objective. These datasets typically provide only equilibrium 3D structures without offering spectra for all molecules. To enhance the pre-training effect by incorporating spectra while leveraging denoising pre-training, we employ a two-stage pre-training approach. The first stage involves training on a larger dataset (Nakata & Shimazaki, 2017) without spectra using only the denoising objective. Subsequently, the second stage involves training on a dataset that includes spectra using the complete objective as follows:\n$\\mathcal{L} = \\beta_{Denoising} \\mathcal{L}_{Denoising} + \\beta_{MPR} \\mathcal{L}_{MPR} + \\beta_{Contrast} \\mathcal{L}_{Contrast},$\nwhere $\\beta_{Denoising}, \\beta_{MPR}, and \\beta_{Contrast}$ denote the weights of each sub-objective."}, {"title": "4 EXPERIMENTS", "content": "To comprehensively evaluate the impact of molecular spectra on molecular tasks, we first verify the effectiveness of molecular spectra in the training-from-scratch method for the downstream task. Furthermore, we evaluate the effectiveness of our pre-training framework MolSpectra."}, {"title": "4.1 EFFECTIVENESS OF MOLECULAR SPECTRA IN TRAINING FROM SCRATCH", "content": "This pilot experiment aims to demonstrate the rationality for incorporating molecular spectra into pre-training. We introduce additional spectral features into a train-from-scratch molecular property prediction model to observe the impact of spectral information on prediction outcomes. We employ EGNN (Satorras et al., 2021), a representative 3D molecular encoder, equipped with an MLP-based prediction head as the baseline model. While EGNN encodes the 3D representations, the UV-Vis spectrum of each molecule provided by the QM9S (Zou et al., 2023) dataset is encoded into spectral representations by a spectrum encoder. Before making predictions with the final MLP, we concatenate the spectral and 3D representations for prediction. The results are presented in Table 1.\nWe observe that by directly concatenating spectral representations, the performance of molecular property prediction can be effectively enhanced. This indicates that the information from molecular spectra is beneficial for downstream molecular property prediction. Further incorporating molecular spectra into the pre-training phase of molecular representation has the potential to enhance the informativeness and generalization capability of the representations, thereby broadly improving the performance of downstream tasks."}, {"title": "4.2 EFFECTIVENESS OF MOLECULAR SPECTRA IN REPRESENTATION PRE-TRAINING", "content": "We conduct experiments to evaluate MolSpectra by first introducing spectral data into the pre-training of 3D representations, followed by evaluating the performance on downstream tasks. For a comprehensive comparison, two types of baselines are adopted: (1) training-from-scratch methods, including SchNet (Sch\u00fctt et al., 2017), EGNN, DimeNet (Klicpera et al., 2020b), DimeNet++ (Klicpera et al., 2020a), PaiNN (Sch\u00fctt et al., 2021), SphereNet (Liu et al., 2021), and TorchMD-Net (Th\u00f6lke & Fabritiis, 2022); and (2) pre-training methods, including Transformer-M (Luo et al., 2023), SE(3)-DDM (Liu et al., 2023b), 3D-EMGP (Jiao et al., 2023), and Coord.\nMolSpectra can be seamlessly plugged into any existing denoising method. To evaluate the enhancement provided by our method compared to denoising alone, we select the representative coordinate denoising (Coord) as our denoising sub-objective. This method also serves as our primary baseline."}, {"title": "4.2.1 PRE-TRAINING DATASET.", "content": "As described in Section 3.4, we first perform denoising pre-training on the PCQM4Mv2 (Nakata & Shimazaki, 2017) dataset, followed by a second stage of pre-training on the QM9Spectra (QM9S) (Zou et al., 2023) dataset, which includes multi-modal molecular energy spectra. In both stages, we adopt the denoising objective provided by Coord (Zaidi et al., 2023), as defined in Eq. 2.\nThe QM9S dataset comprises organic molecules from the QM9 (Ramakrishnan et al., 2014) dataset. The UV-Vis, IR, and Raman spectra of the molecules are calculated at the B3LYP/def-TZVP level of theory, through frequency analysis and time-dependent density functional theory (TD-DFT)."}, {"title": "4.2.2 QM9", "content": "The QM9 dataset is a quantum chemistry dataset comprising over 134,000 small molecules, each consisting of up to 9 hydrogen (H), carbon (C), nitrogen (N), oxygen (O), and fluorine (F) atoms. This dataset provides an equilibrium geometric conformation for each molecule along with 12 property labels. The dataset is divided into a training set of 110k molecules, a validation set of 10k molecules, and a test set containing the remaining over 10k molecules. Prediction errors are measured using the mean absolute error (MAE). The experimental results are presented in Table 2.\nThe 3D molecular representations pre-trained using our method are fine-tuned and used for prediction across various properties, achieving state-of-the-art performance in 8 out of 12 properties and outperforms Coord in 10 out of 12 properties. In conjunction with the observations in Section 4.1, the performance improvement can be attributed to our incorporation of an understanding of molecular spectra and the knowledge they entail into the 3D molecular representations."}, {"title": "4.2.3 MD17", "content": "The MD17 dataset contains molecular dynamics trajectories for eight organic molecules, including aspirin, benzene, and ethanol. It offers 150k to nearly 1M conformations per molecule, with energy and force labels. Unlike QM9, MD17 emphasizes dynamic behavior in addition to static properties. We use a standard limited data split: models train on 1k samples, validate on 50, and test on the rest. Performance is evaluated using MAE, with results in Table 3.\nOur approach also results in the expected performance improvement on MD17. MD17 is a dataset comprising a large number of non-equilibrium molecular structures and their corresponding force fields, which serves to evaluate a model's understanding of molecular dynamics. However, previous pre-training methods based solely on denoising have only learned force field patterns at static equilibrium states, failing to adequately capture the dynamic evolution of molecular systems. In contrast, our MolSpectra learns the dynamic evolution of molecules by understanding energy level transition patterns, thereby outperforming denoising-based pre-training methods."}, {"title": "4.3 SENSITIVITY ANALYSIS OF PATCH LENGTH $P_I$, STRIDE $D_I$, AND MASK RATIO $\\ALPHA$", "content": "We conduct experiments to evaluate the impact of patch length $P_i$, stride $D_i$, and mask ratio $\\alpha$. Results are summarized in Table 4 and Table 5.\nFrom Table 4, we observe that when consecutive patches have overlap ($D_i < P_i$), the performance of pre-training is superior compared to scenarios without overlap ($D_i = P_i$). Specifically, the performance is optimal when the stride is half of the patch length. This is because appropriate overlap can better preserve and capture local features, particularly the information at the patch boundaries. Additionally, we find that choosing an appropriate patch length further enhances performance. In our experiments, the configuration of $P_i = 20, D_i = 10$ yields the best results.\nRegarding the mask ratio, $\\alpha = 0.10$ is a preferable choice. A small mask ratio results in insufficient MPR optimization, hindering SpecFormer training. Conversely, a large mask ratio causes excessive spectral perturbation, degrading performance when aligning with the 3D representations with the contrastive objective. An appropriate mask ratio strikes a balance between these two aspects."}, {"title": "4.4 ABLATION STUDY", "content": "To rigorously demonstrate the contributions of masked patches reconstruction, the incorporation of molecular spectra, and each spectral modality, we conducted an ablation study on them.\nAblation study of masked patches reconstruction. We remove the MPR loss to analyze the impact of masked patches reconstruction, referred to as \"w/o MPR\" in Table 6. Removing the MPR objective leads to performance deterioration. This is consistent with the sensitivity analysis of the mask ratio $\\alpha$ in Section 4.3, as removing MPR is an extreme case where $\\alpha = 0$. This decline is due to the lack of effective guidance in training SpecFormer. Using an undertrained SpecFormer for contrastive learning with 3D encoder outputs limits performance improvement.\nAblation study of molecular spectra. We retain only the denoising loss, removing both the MPR loss and contrastive loss, referred to as \"w/o MPR, Contrast\" in Table 6. The only difference between this variant and MolSpectra is the incorporation of molecular spectra into the pre-training. The \"w/o MPR, Contrast\u201d results are inferior to those of MolSpectra, highlighting that incorporating molecular spectra effectively enhances the quality and generalizability of molecular 3D representations.\nAblation study of each spectral modality. To evaluate the contributions of each spectral modality to the performance, we conduct an ablation study for each modality. The results are presented in Table 7. It can be observed that each spectral modality contributes differently, with the UV-Vis spectrum having the smallest contribution and the IR spectrum the largest, likely due to the varying information content in each modality."}, {"title": "5 RELATED WORK", "content": "3D molecular pre-training. Molecular 2D structures are typically represented as graphs and modeled using graph learning methods (Gilmer et al., 2017; Li et al., 2023; Jiang et al., 2024). However, 3D molecular structures provide critical geometric information that is essential for understanding physicochemical properties (Chen et al., 2023; 2024; Wang et al., 2024a; Sun et al., 2024), which cannot be directly inferred from 2D graphs or SMILES representations (Gong et al., 2024). Designing effective strategies for pre-training 3D molecular representations remains challenging due to the geometric symmetries inherent in 3D structures and their strong connection to physical knowledge, such as potential energy functions.\nDenoising the geometric structure has been demonstrated as an effective strategy for 3D representation pre-training (Liu et al., 2023b; Jiao et al., 2023; Kim et al., 2023; Zhou et al., 2023; Wang et al., 2025). Coordinate denoising (Coord) (Zaidi et al., 2023) first theoretically proves that the denoising objective is equivalent to learning the gradient of the potential energy with respect to positions, essentially the force field. Building on this work, fractional denoising (Frad) (Feng et al., 2023) introduces dihedral angle noise to optimize the sampling of low-energy structures. Further, SliDe (Ni et al., 2024) incorporates a more rigorous potential energy from classical mechanics. Another line of research simultaneously leverages both 2D and 3D structures for pre-training molecular representations, addressing the complementarity of the two modalities (Li et al., 2022; Zhu et al., 2022; Liu et al., 2023a; Du et al., 2023a; Yu et al., 2024) or the computational complexity of 3D structure determination (Liu et al., 2022; St\u00e4rk et al., 2022; Wang et al., 2023a).\nAlthough these studies elucidate the relationship between molecular 3D structures and their energy states, they remain limited to the description of molecular energy states within classical mechanics, without considering the quantized energy level structures as described by quantum mechanics.\nMolecular spectroscopy. Molecular spectroscopy studies interactions between molecules and electromagnetic radiation. Analyzing spectra provides valuable insights into molecular structure, composition, and dynamics (Lancaster et al., 2024). When encountering unknown substances, researchers conduct spectroscopic measurements on samples and compare the observed spectra with libraries for identification. To expand library coverage, machine learning methods are widely used to predict molecules' spectra (Zou et al., 2023; Wei et al., 2018; Zong et al., 2024).\nSome studies incorporate physical principles into spectra prediction models as inductive biases, including molecular dynamics simulations via equivariant message passing (Sch\u00fctt et al., 2021), fragmentation (D\u00fchrkop et al., 2020; Cao et al., 2020; Goldman et al., 2023a), motifs (Park et al., 2023), and long-distance atomic interactions (Young et al., 2024). Another line of research approach bypasses spectral library comparison and directly performs de novo structure elucidation from spectra (Stravs et al., 2021; Goldman et al., 2023b; Tao et al., 2024).\nSince different spectroscopic techniques offer complementary advantages, the joint analysis of multiple spectra can provide comprehensive information (Alberts et al., 2024). In this study, we encodes multiple spectra, and introduce them into molecular representation pre-training for the first time."}, {"title": "6 CONCLUSION", "content": "In this study, we explore pre-training molecular 3D representations beyond classical mechanics. By leveraging the correlation between molecular energy level structures and molecular"}]}