{"title": "Trust No AI: Prompt Injection Along The CIA Security Triad", "authors": ["JOHANN REHBERGER"], "abstract": "The CIA security triad-Confidentiality, Integrity, and\nAvailability is a cornerstone of data and cybersecurity. With the\nemergence of large language model (LLM) applications, a new\nclass of threats, known as prompt injection, was first identified in\n2022. Since then, numerous real-world vulnerabilities and exploits\nhave been documented in production LLM systems, including\nthose from leading vendors like OpenAI, Microsoft, Anthropic\nand Google. This paper compiles real-world exploits and proof-of-\nconcept examples, based on the research conducted and publicly\ndocumented by the author, demonstrating how prompt injection\nundermines the CIA triad and poses ongoing risks to\ncybersecurity and Al systems at large.", "sections": [{"title": "I. INTRODUCTION", "content": "This paper demonstrates prompt injection vulnerabilities in\nreal-world production systems impact the CIA security triad. It\nhighlights how prompt injection from untrusted data impacts\ncore security attributes of Al and information systems, and fixes\nvendors applied to mitigate exposure. Furthermore, the author\nhopes to continue helping bridge a gap that seems to exist\nbetween academic and industry security research."}, {"title": "A. What is the CIA security triad?", "content": "CIA stands for Confidentiality, Integrity and Availability,\nwhich are the three core attributes that guide organizations'\ninformation security policies. Its origins are difficult to track\ndown, and it appears the first official mention only happened in\nthe early 2000s, although the core concepts had been around for\ndecades, even centuries, earlier.\nSome argue that the model is incomplete and could benefit\nfrom further expansion. However, there seems to be consensus\nthat the triad forms the minimum required security attributes in\nrelation to data [1].\nFurthermore, NIST highlighted Privacy, Integrity and\nAvailability in a recent AI attack taxonomy [2]."}, {"title": "B. What is Prompt Injection?", "content": "At its core, prompt injection refers to the commingling of\ntrusted and untrusted data, like system instructions mixed with\nuser instructions or data.\nThe result is a final prompt (sometimes also called a query)\nthat is sent to an LLM for inference. If parts of the prompt are\ncontrolled by an attacker, the attacker can read or override the\noriginal instructions and data, potentially performing harmful\nand dangerous actions.\nThe team that originally discovered the vulnerability type\n(behavior) was Preamble [3] who responsibly disclosed it to\nOpenAI as Command Injection Vulnerability in May of 2022,\nand publicly disclosed it later that year. Goodside helped shine\nmore light on it in parallel [4], and Willison gave \u201cPrompt\nInjection\" its name [5].\nGreshake, et al. highlight the failure modes that can arise\nwhen including untrusted data in prompts, referred to as indirect\nprompt injection [6].\nEven though there is often a correlation made between\nprompt injection and SQL injection (both are application\nsecurity flaws on the caller's side), it is important to highlight\nthat prompt injection represents a unique challenge that\ncurrently does not have a deterministic fix. For SQL Injection\nprecise guidance can be given to developers to prevent it,\nhowever no such guidance exists for prompt injection."}, {"title": "II. PROMPT INJECTION ALONG THE CIA TRIAD", "content": "Prompt injection attacks can compromise all aspects of the\nCIA triad. By analyzing specific real-world examples, we can\nbetter understand the vulnerabilities and implications of these\nattacks on the core security principles of Confidentiality,\nIntegrity, and Availability."}, {"title": "III. LOSS OF CONFIDENTIALITY", "content": "\"the fact of private information being kept secret\" [7]\nConfidentiality is about data protection. There have been\nnumerous findings on how LLM applications and chatbots\nmight send private information to third party servers because of\nindirect prompt injection. But let's start with a basic scenario\nthat developers at times get wrong, which is assuming the\nsystem prompt is private."}, {"title": "A. Scenario 1: Leaking the system prompt", "content": "Sometimes developers place sensitive information into the\nsystem prompt, but the system prompt is within the same trust\nboundary as the user's input prompt. Because of that, it is often\nstraightforward for a user to retrieve the system prompt.\nThe popular Gandalf game by Lakera [8] is a great example for\neducating users about this, challenging users to find prompt\ninjection to reveal secrets, and Microsoft's Bing Chat prompt\nleak is another example as explained by Learn Prompting. [9]"}, {"title": "B. Scenario 2: Automatic Image Rendering and Image\nMarkdown Data Exfiltration in LLM applications", "content": "It was shown that LLM applications are commonly vulnerable\nto data exfiltration via rendering images from untrusted\ndomains, where an attacker has the LLM append data to the\nimage URL, thereby leaking the information to the third-party\nserver [10] [11].\nIn fact, this is one of the most common application security\nvulnerabilities in LLM applications and the list of vulnerable\nsystems the author discovered and reported to vendors include\n(but not limited):\n\u2022\tMicrosoft Bing Chat [11]\n\u2022\tOpenAI ChatGPT [12]\n\u2022\tAnthropic Claude [13]\n\u2022\tMicrosoft Azure AI [14]\n\u2022\tGoogle Bard [15]\n\u2022\tGoogle Vertex AI [16]\n\u2022\tGoogle NotebookLM [17]\n\u2022\tGoogle AI Studio [18]\n\u2022\tGoogle Colab [19]\n\u2022\tDiscord [20]\n\u2022\tMicrosoft GitHub Copilot Chat [21]\nBesides web applications, this vulnerability is often present in\nmobile and desktop applications."}, {"title": "C. Scenario 3: Automatic unfurling of hyperlinks", "content": "This data exfiltration issue involves, again, hyperlinks.\nSoftware such as Slack, Discord, Teams... will automatically\nconnect and attempt to retrieve basic information from a link to\ndisplay a preview of the page to the user.\nAn attacker can leverage this during prompt injection to render\na link, append information from the chat context to exfiltrate\nthe data automatically [22]."}, {"title": "D. Scenario 4: Clickable hyperlinks", "content": "Like with image retrieval, many applications render clickable\nlinks. This might be used by an attacker to trick users via\nphishing and scams, but also allow the exfiltration of data by\nappending chat context information to the hyperlink [6].\nIn combination with attacks such as ASCII Smuggling [23]\nmore nuanced exfiltration containing invisible characters can\noccur [24].\nIn February 2024 a vulnerability in Microsoft 365 Copilot was\ndiscovered, and responsibly disclosed to Microsoft, that\ncombined prompt injection delivered via a phishing email with\nautomatic tool invocation to retrieve sensitive information from\nthe user's inbox, and then render the data into a clickable\nhyperlink, containing to the user invisible data.\nThis attack staged the data for exfiltration and asks the user to\nclick the link, which causes the exfiltration of the sensitive\ncontent to the attacker-controlled server. The vulnerability was\nfixed in July 2024 by Microsoft. [25]\nAmazon Q for Business was vulnerable to data exfiltration\nvector, and Amazon addressed it by not rendering links [26]."}, {"title": "E. Scenario 5: Invocation of Tools - Browsing the Web", "content": "Another example is that a prompt injection can trigger an\nautomatic invocation of tools which sends information to a\nthird-party server.\nThis was demonstrated in an end-to-end exploit that showed\nhow an attacker can steal a user's email using the Zapier Plugin\n[27]. The mitigation that was implemented was to ask for user\nconfirmation before sending an email."}, {"title": "F. Scenario 6: Invocation of Tools - Chat with Code Plugin", "content": "Tools might have the capability to modify permissions on\nobjects, which, much like Cross Site Request Forgery or\nServer-Side Request Forgery can be used by an adversary to\nmodify configuration settings (authentication\nauthorization) and expose information to attackers.\nand/or\nA real-world exploit was documented and described in a\nChatGPT Plugin named \"Chat with Code\", where visiting a\nwebsite with ChatGPT a prompt injection payload on the page\nmodified permission settings of GitHub repositories and\nchanged private repositories to public [28] The plugin was\nremoved by OpenAI."}, {"title": "G. Scenario 7: SpAlware \u2013 Memory Persistent Attacks [31]", "content": "In December 2023 it was discovered that OpenAI's fix for\nmarkdown image rendering via the url_safe feature was\nincomplete and mobile clients remained vulnerable. [31]\nIn May 2024 OpenAI introduced persistent memory, allowing\nChatGPT to remember information about the user in its long-\nterm storage. This means in future chat conversation; the\nmemories are placed into the prompt context and ChatGPT can\nreference them during the conversation.\nWhen OpenAI released the macOS desktop application in 2024,\nit also lacked the implementation of the url_safe feature to\nprevent data leakage.\nDuring my research it was discovered that the memory tool can\nbe invoked via prompt injection by websites, documents and\nimages and it allowed the creation of spyware. It was possible\nto persist malicious instructions in memory to continuously,\nand quietly, exfiltrate all chat conversations while the user was\ninteracting with ChatGPT.\nFurthermore, as demonstrated in my Black Hat Europe talk, it\nis possible to build an entire Command & Control system,\nwhich continuously reaches out to a central server for new\ninstructions by compromising the memory of a ChatGPT user-\n-all purely based on prompt injection. [32]"}, {"title": "H. Scenario 8: ZombAIs \u2013 Claude Computer Use:\nCommand & Control", "content": "Last, but not least it was shown how a simple prompt injection\npayload on a web page compromised Claude Computer Use:\nThis page was shown to entirely hijack Claude Computer Use,\ntrick it to download and run the malware, which turned the\nuser's computer into a bot net zombie. [31]"}, {"title": "I. Mitigations for loss of confidentiality", "content": "The following recommendation help to ensure confidentiality:\n\u2022\tDo not put sensitive data into the system prompt\n\u2022\tAvoid rendering links or images to untrusted domains\n\u2022\tAvoid automatic tool invocation of sensitive operations\n(incl. data retrieval, as they can be used for exfiltration)\n\u2022\tIf tool invocation is performed, clearly display to the\nuser what action will be taken and what data will be\ninvolved in the action (before taking the action)\n\u2022\tLeveraging Content Security Policies to prevent\nrendering of resources from untrusted domains\n\u2022\tDo not automatically unfurl hyperlinks [33]\n\u2022\tContext aware output encoding to prevent application\nsecurity vulnerabilities (like XSS)\n\u2022\tSecurity reviews and penetration testing\nThese steps help improve the security posture of AI systems."}, {"title": "IV. LOSS OF INTEGRITY", "content": "\"guarding against improper information modification\"\n[34]\nIntegrity is quite an interesting attribute when it comes to LLM\napplications and chatbots. Because inference results over data\nare typically non-deterministic. Creative output is commonly\nreferred to as \"hallucinations\" and it's also considered as a core\nfeature of LLMs as Andrej Karpathy pointed out [35].\nFrom user experience and security point of view the\ncommon guidance is to not implicitly trust the output of an\nLLM applications.\nThe untrustworthiness of LLM output represents a challenge for\nusers, developers, and agents. Malicious output can lead to\nexploitation of the user (social engineering, phishing) or the\nLLM application itself (cross site scripting, data exfiltration or\neven remote code execution). Obviously, bias and fairness fall\ninto this category as well. Let's explore exploits that\nembracethered.com documented."}, {"title": "A. Scenario 1: Google Docs AI - Scams and Phishing", "content": "Google Docs, a popular word processing tool, introduced new\nGenerative Al features in 2023. This allows to summarize or\nrephrase content of Docs. While these features can be\nbeneficial, they also pose security risks, such as adversaries\nhiding instructions in documents to trick users.\nA basic attack demonstrated and reported to Google involved\nmalicious content that hijacks the AI to generate a scam\nmessage that tries to trick the user in calling a phone number.\nTo mitigate these risks, it is advised to only use AI on trusted\ndata and to be cautious of the Al's output. The original issue\nwas reported to Google on June 22, 2023, remains unresolved,\ndemonstrating the loss of integrity when processing data with\nan LLM. A video showing this demo was presented HITCON\nCMT 2023 [36]."}, {"title": "B. Scenario 2: Google Gemini - Google Drive. LLM\nconnects user directly to attacker via Google Meet", "content": "Google Gemini features in Google Documents and Google\nDrive are another good example. This time the proof-of-\nconcept attack prompt injection payload renders a hyperlink\nthat connects the victim directly with the scammer via a Google\nMeet link [37]."}, {"title": "C. Scenario 3: Conditional Prompt Injections", "content": "Microsoft 365 Copilot is integrated into Office products,\nincluding Outlook. What is unique is that Copilot has access to\nthe user's name and organizational structure (e.g. job title,\nmanager information). This allows an attacker to create prompt\ninjection attacks that are tailored to and only activate when a\nspecific user processes the email as demonstrated by the\nEmbrace the Red blog [38].\nImagine a phishing payload that only activates when the CEO\nlooks at the email, or content that is changed and different\ninformation is shown based on who the user is."}, {"title": "D. Scenario 4: ASCII Smuggling", "content": "That LLMs interpret, to users invisible, Unicode Tag characters\nwas discovered by Riley Goodside [39]. A python script by\nThacker [40] and \u201cASCII Smuggler\" by Rehberger are tools\nthat can aid with testing LLM applications for this potentially\nunwanted behavior [41].\nSoon after it was discovered that LLMs can also emit such\nhidden Unicode Tag characters [42]. This includes inserting\nsuch hidden characters in hyperlinks that a user might be tricked\nin clicking [41]. The later can lead to loss of confidentiality as\nwe showed earlier with the end-to-end data exfiltration exploit\nin Microsoft Copilot already [25].\nThe \"ASCII Smuggler\" tool allows encoding/decoding of\nhidden messages created by LLM applications, including\ndecoding such LLM created hyperlinks with hidden characters\nto aid in testing such attack scenarios."}, {"title": "E. Scenario 5: Terminal DiLLMa - ANSI Escape Codes", "content": "Recently it was discovered that LLMs can emit non-printable\nANSI terminal escape codes. [43]\nBased on this research it was then demonstrated that this can be\nexploited during prompt injection on LLM-powered CLI\n(command line interface) tools, to leak data, interfere with the\nuser's terminal, copy information into the user's clipboard, etc.\nAt the same time mitigations, such as encoding non-printable\ncharacters with caret notation was provided. [44]"}, {"title": "F. Mitigations for loss of integrity", "content": "The current mitigation all vendors seem to follow and have\naligned on is to display text that states the output is\nuntrustworthy and that AI makes mistakes!\nFor software developers it is important that the output of an\nLLM inference cannot be trusted, and proper context aware\nusage of the data must be ensured.\nThis includes proper output encoding to prevent attacks such as\nCross Site Scripting in web applications, which was shown to\nlead to full account takeover with DeepSeek AI [45], ANSI\nescape code rendering in terminals, filtering, human\ninvolvement and similar mitigations.\nCiting sources can help, as well as researching and developing\nsolutions to allow highlight which text was not modified at all\nfrom a source when shown to the user via an LLM application."}, {"title": "V. Loss OF AVAILABILITY", "content": "\"present or ready for immediate use\" [46]\nDiscussed less frequently, especially in the context of prompt\ninjection, are availability concerns."}, {"title": "A. Scenario 1: Infinite Loops or Long-Running Inference", "content": "An attacker might attempt to create an infinite recursion by\ninvoking a tool, that returns a prompt injection that again\ninvokes the same tool.\nThis attack idea was tested with ChatGPT and OpenAI had\nconsidered this threat, and ChatGPT only allowed 10\ninvocations in a loop [47]."}, {"title": "B. Scenario 2: Refusal Via (Hidden) Instructions", "content": "A prompt injection can cause a simple refusal to prevent\nprocessing of information, for instance it was shown that a\nprompt injection in an email can cause Copilot to refuse\nsummarizing the email. [38]"}, {"title": "C. Scenario 3: Persistent Output Refusal", "content": "ChatGPT added the ability to store long term memories.\nUnfortunately, OpenAI decided that a human in the loop\nprompt is not required to persist a long-term memory [48].\nThis means that during prompt injection, an attacker can add\nfake or even harmful memories to the user's ChatGPT memory,\none such attack is to have ChatGPT refuse any further answers\n[49]."}, {"title": "D. Mitigations to prevent loss of availability", "content": "The following mitigation steps should be explored to prevent\nloss of availability.\n\u2022\tLimit the token length and time a query can execute.\n\u2022\tRate limiting of requests per user or IP.\n\u2022\tAvoid automatic tool invocation w/o human in loop.\n\u2022\tConsider recursions when tool invocation is involved.\n\u2022\tContext-aware output encoding (e.g usage of caret\nnotation for ANSI control codes in CLI applications)\n\u2022\tRestrict access to inference endpoints.\nIt is worth highlighting that resource intense attacks also incur\nadditional costs for operators, which is an attack in itself."}, {"title": "VI. CONCLUSION", "content": "This paper highlighted many real-world examples of prompt\ninjection exploits and how they impact the three core attributes\nof the CIA security triad.\nSince there is no deterministic solution for prompt injection, it\nis important to highlight and document security guarantees\napplications can make, especially when building automated\nsystems that process untrusted data. The message, often used in\nthe author's exploit demonstration remains: Trust No AI.\nThorough threat modeling and data flow analysis, as well as\npenetration testing and red teaming can further help to identify\nvulnerabilities and should be performed before releasing AI\nsystems and chatbots to the public."}]}