{"title": "Density-Calibrated Conformal Quantile Regression", "authors": ["Yuan Lu"], "abstract": "This paper introduces the Density-Calibrated Conformal Quantile Regression (CQR-d) method, a novel approach for constructing prediction intervals that adapts to varying uncertainty across the feature space. Building upon conformal quantile regression, CQR-d incorporates local information through a weighted combination of local and global conformity scores, where the weights are determined by local data density. We prove that CQR-d provides valid marginal coverage at level 1-a-e, where e represents a small tolerance from numerical optimization. Through extensive simulation studies and an application to the a heteroscedastic dataset available in R, we demonstrate that CQR-d maintains the desired coverage while producing substantially narrower prediction intervals compared to standard conformal quantile regression (CQR). Notably, in our application on heteroscedastic data, CQR-d achieves an 8.6% reduction in average interval width while maintaining comparable coverage. The method's effectiveness is particularly pronounced in settings with clear local uncertainty patterns, making it a valuable tool for prediction tasks in heterogeneous data environments.", "sections": [{"title": "Introduction", "content": "In many fields of study, from economics to environmental science, the ability to provide accurate prediction intervals is crucial for informed decision-making under uncertainty (Lei and Wasserman, 2014). Traditional methods often struggle to maintain reliable coverage across diverse data distributions, particularly in the presence of heteroscedasticity or complex non-linear relationships (Steinwart and Christmann, 2011; Angelopoulos and Bates, 2021). To address these challenges, we introduce the Density-Calibrated Conformal Quantile Regression (CQR-d) method, a novel approach that combines the strengths of conformal prediction and quantile regression."}, {"title": "Methodology", "content": "Conformal prediction, introduced by Vovk et al. (2005), offers a model-agnostic framework for constructing prediction intervals with guaranteed marginal coverage under the assumption of exchangeability. However, standard conformal methods may produce overly wide intervals in regions of low uncertainty (Nouretdinov et al., 2001). Quantile regression, on the other hand, directly estimates conditional quantiles of the response variable, allowing for a more nuanced capture of the underlying data distribution (Koenker and Bassett, 1978).\nThe CQR-d method builds upon recent work in conformal quantile regression (Romano et al., 2019) by introducing a local adjustment factor. This innovation allows the method to adapt more flexibly to local data structures, potentially resulting in tighter prediction intervals while maintaining the desired coverage level. By leveraging both the distribution-free guarantees of conformal prediction (Vovk et al., 2005; Lei et al., 2018) and the local adaptivity of quantile regression (Meinshausen, 2006), CQR-d aims to provide more informative and efficient prediction intervals across a wide range of data scenarios (Sesia and Cand\u00e8s, 2019).\nIn this paper, we present the theoretical foundations of the CQR-d method, proving its validity in terms of marginal coverage and discussing its asymptotic properties (Gammerman and Vovk, 2007). We then demonstrate its performance through extensive simulations and real-world data applications, comparing it to existing methods such as standard conformal prediction and the original conformal quantile regression approach. Our results suggest that CQR-d offers a promising new tool for researchers and practitioners seeking reliable and informative prediction intervals in complex, heterogeneous data environments.\nThe remainder of this paper is organized as follows: Section 2 provides the methodological details of CQR-d, Section 3 presents theoretical results, Section 4 describes our empirical studies, and Section 5 concludes with a discussion of the implications and potential future directions for this research."}, {"title": "Problem Setup", "content": "Given a set of n points {(Xi, Yi)}=1, where Yi \u2208 R and Xi \u2208 Rd, we aim to construct a prediction interval for a new point Yn+1 based on the observed value of Xn+1. We assume that the points {(X, Y)}+1 are drawn exchangeably from a common distribution Pxy (Vovk et al., 2005; Melluish et al., 2001; Kuchibhotla, 2020). Our goal is to construct a prediction interval Ca(Xn+1) such that:\nP[Yn+1 \u2208 \u0108a(Xn+1)] \u2265 1 \u2212 a,\nrelying only on the exchangeability of the n + 1 points (Lei et al., 2018)."}, {"title": "Conformal Quantile Regression (CQR)", "content": "We follow the split-conformal approach to conformal prediction (Vovk et al., 2005). The data samples are split into two disjoint subsets, I\u2081 and I2. Lower and upper quantile regression functions, \u011da/2 and 91-0/2 : Rd \u2192 R, are fitted on the observations in I\u2081 using quantile regression (Koenker and Bassett, 1978; Takeuchi et al., 2006). For each i \u2208 I2, we compute the conformity scores:\nECQRi = max{qa/2(Xi) \u2013 Yi, Yi - 91-a/2(Xi)}.\nThe standard CQR prediction interval for a new point X is then given by:\n\u0108CQR(X) = [qa/2(X) \u2013 Q1-a(ECQR; I2), 91-4/2(X) + Q1-a(ECQR; I2)],\nwhere Q1-a (ECQR; I\u2082) is the (1-x)(1+1/|I2|)-th empirical quantile of {ECQR : i \u2208 I2} (Romano et al., 2019)."}, {"title": "Density-Calibrated Conformal Quantile Regression (CQR-d)", "content": "The CQR-d method extends standard CQR by incorporating local information to adapt the prediction intervals to varying data density (Meinshausen, 2006; Taylor, 2000; Kivaranovic et al., 2019). Suppose we have two datasets, training set I1, calibration set I2. Given any quantile regression function, we fit an upper and a lower quantile function, said q1-a(Xi) and qa(Xi), and compute the global conformity scores as in standard CQR (Romano et al., 2019):\nEi = max{qa/2(Xi) \u2013 Yi, Yi - 91-a/2(Xi)}\nThen we compute the global quantile using the global conformity scores above:\nQ\u00ba = Q1-a(E, I\u2082) = (1 \u2212 a)(1+1/|I2|)-th empirical quantile of {Ei}i\u220812 (Lei et al., 2018).\nFor each point in the calibration set, we find the k nearest neighbors (Chaudhuri, 1991). Denote Nk (x) = {1, ...ik} such that ||x - Xi|| < ||x - X\u0131|| for all i \u2208 Nk(x) and l \u2209 Nk(x), where i, l \u2208 I2. Then we compute the local conformity score for each i \u2208 Nk(x):\nEi = {E: i \u2208 Nk(x)}\nNote that for implementation, we want to include the point itself in the search but exclude it from the actual neighborhood used for calculations. Hence, in practice, we actually find k + 1 nearest neighbors to ensure that we have k neighbors even after excluding the point itself. And we compute the local quantile with local conformity scores: Q = Q1-a(E, I\u2082) = (1 \u2212 a)(1 + 1/|I2|)-th empirical quantile of {Ei}i\u220812 (Meinshausen, 2006)."}, {"title": null, "content": "Now, we define local density as:\n\u03c1i = 1 / \u03a3j\u2208Nk(x) ||x - Xj||\nand local weight as:\nwi = 1 / 1 + \u03c1i\nIn data dense regions, points are closer together, so the mean distance is smaller, resulting in a larger local density value, and taking the reciprocal of the mean distance gives us a proper density measure (Steinwart and Christmann, 2011). We also want to rely more on local information, so local weight should be closer to 1 in data dense region. Thus we applies this function to the local density which maps any real number to the range between 0 and 1. When local density is high, local weight approaches 1; when local density is low, local weight approaches 0. Then we define global weight as:\nwg = 1 \u2212 wl\nwhich ensures that local and global weights sum to 1. As we rely more on one information, we rely less on the other (Meinshausen, 2006).\nThe combined quantile is then:\nQ(X, \u03bb) = (wl \u00b7 Ql + wg \u00b7 Q\u00ba) \u00b7 \u03bb\nwhere \u03bb here is an adjustment parameter. It is introduced to fine-tune the coverage of the prediction intervals (Lei et al., 2018). While the combination of local and global quantiles aims to adapt to varying data density, the actual coverage may still deviate from the target 1 a due to finite sample effects and the discrete nature of the data. \u03bb allows us to calibrate the intervals to achieve the desired coverage more accurately.\nTo determine the optimal value of \u03bb, we define an objective function that measures the discrepancy between the achieved coverage and the target coverage (Lei et al., 2018; Sesia and Cand\u00e8s, 2019):\nobjective(\u03bb) = |coverage(\u03bb) \u2013 (1 \u2212 a)|\nwhere coverage(\u03bb) is the empirical coverage on the calibration set I2 using the combined quantile Q with the given \u03bb. Specifically:\ncoverage(\u03bb) = 1/|I2| \u03a3i\u2208I2 1{Yi \u2208 [9/2(Xi) - Ql, q1-a/2(Xi) + Q]}\nWe then find the optimal \u03bb* that minimizes this objective function:\n\u03bb* = argmin objective(\u03bb)"}, {"title": null, "content": "This optimization is typically performed using numerical methods such as Brent's method (Brent, 1973), which is effective for one-dimensional optimization problems (Kingma and Ba, 2014). The optimization of the adjustment factor \u03bb is constrained to a symmetric interval around 1. The value \u03bb = 1 represents no adjustment to the combined quantile, while values \u03bb < 1 produce narrower intervals and values \u03bb > 1 yield wider intervals. This symmetry allows the optimization procedure equal freedom to increase or decrease interval widths as needed.\nOnce we have determined \u03bb*, we can compute the final prediction interval for any new points in Xnew (Gal and Ghahramani, 2016). We predict lower quantile \u011da(Xj) and upper quantile 91-a(Xj) for each point in Xnew using the previous quantile regression model. For each new point, we find k nearest neighbors in the calibration set. Let Nk (Xj) = {i1, ...ik} such that ||Xj \u2013 Xi|| \u2264 ||Xj \u2013 Xl|| for all i \u2208 Nk (Xj) and l \u2209 Nk(Xj), where i, l are indices in the calibration set. Then, the local conformity scores for new points can be written as:\n\u00caj = {E : i \u2208 Nk(X)}\nwhere E are the pre-computed global conformity scores from the calibration set, and we compute the local quantile for each new point as: Q = Q1-a(E). Similarly as the above, the local density for each new point is:\n\u03c1j = 1 / \u03a3i\u2208Nk(X) ||Xj \u2013 Xi||\nand the local weights and global weights for each new point are: wl = 1/1+e^\u03c1j and wg = 1 \u2212 wl. The combined quantile is then:\nQ(X, \u03bb*) = (wl \u00b7 Ql + wg \u00b7 Q\u00ba) \u00b7 \u03bb*\nwhere \u03bb* is the optimal adjustment factor derived from the calibration set. And then we construct the prediction interval as:\n\u0108a(X) = [9a/2(X\u2081) \u2013 Ql(Xj, \u03bb*), 91-a/2(X) + Q(Xj, \u03bb*)]\nThis interval adapts to the local data density around Xn+1 through the local quantile and weights, while also incorporating global information (Nouretdinov et al., 2001). The adjustment parameter \u03bb* ensures that the overall method achieves the desired coverage level across diverse datasets and sample sizes."}, {"title": "Density-Calibrated Conformal Quantile Regression(CQR-d)", "content": "The CQR-d method allows for adaptive interval widths based on local data density and the distribution of conformity scores in the neighborhood of each test point. This approach aims to produce narrower intervals in regions of low uncertainty while maintaining wider intervals where uncertainty is high, all while preserving the desired coverage level. By incorporating local information, CQR-d aims to produce more efficient prediction intervals compared to standard CQR, particularly in heterogeneous data settings where the level of uncertainty varies across the domain of X."}, {"title": "Theoretical Analysis", "content": "Despite the introduction of local adaptivity in CQR-d, the fundamental coverage guarantee of conformal prediction is largely preserved (Vovk et al., 2005), with only a small deviation due to numerical optimization. The proof of CQR-d's validity follows a structure similar to that of CQR (Romano et al., 2019), leveraging the key principles of exchangeability (Melluish et al., 2001) and the properties of empirical quantiles (Chaudhuri, 1991). However, CQR-d's proof requires additional considerations to account for both the locally adaptive components (Meinshausen, 2006) and the numerical optimization of the adjustment factor. These extensions, while maintaining the core logic of the CQR proof (Lei and Wasserman, 2014), demonstrate that the local adaptations in CQR-d provide valid prediction intervals with coverage 1 \u03b1 6, where e represents a small tolerance from numerical optimization. This slight relaxation of the coverage guarantee is a practical necessity due to the optimization procedure, but has minimal impact on the method's performance as demonstrated in our empirical studies."}, {"title": "Theorem", "content": "Assume that the data {(X, Y\u2081)}+1 are exchangeable (Vovk et al., 2005; Nouretdinov et al., 2001). Then the conformal prediction interval C(Xn+1) constructed by the CQR-d algorithm satisfies:\nP{Yn+1 \u2208 C(Xn+1)} \u2265 1 \u2212 a \u2212 \u03b5."}, {"title": "Proof", "content": "By the construction of the prediction interval (Gammerman and Vovk, 2007), we know that the prediction interval\nYn+1 \u2208 \u0108a(Xn+1) if and only if \u00can+1 \u2264 Q\u00ba(Xn+1, \u03bb*),\nthus\nP{Yn+1 \u2208 \u0108a(Xn+1) | (Xi, Yi) : i \u2208 I\u2081} = P{\u00can+1 \u2264 Q\u00ba(Xn+1, X*) | (Xi, Yi) : i \u2208 I\u2081}.\nSince the pairs (Xi, Yi) are exchangeable, the calibration variables E\u017c for i \u2208 I2 and i = n + 1 are also exchangeable (Vovk et al., 2005), and let Q1-a(E; I2) be the (1-x)(1+1/|I2|)-th empirical quantile of {E\u00bf : i \u2208 I2} (Sesia and Cand\u00e8s, 2019).\nRecall that \u03bb* is chosen to optimize the coverage on the calibration set (Lei et al., 2018):\n\u03bb* arg minx |coverage(x) \u2013 (1 \u2212 a)|\nwhere\ncoverage(x) = \u03a3i\u2208I2 1{E \u2264 Q(Xi, 1)}\nBy the properties of Brent's method (Brent, 1973; Kingma and Ba, 2014) used for optimization, for any e > 0, we can ensure:\n|coverage(*) \u2013 (1 \u2212 a)| < \u03b5\nHence,\ncoverage(1*) = \u03a3i\u2208I2 1{E} < Q\u00ba(Xi, X*)} \u2265 1 \u2212 a \u2212 \u0454\nThis implies that,\nP{En+1 \u2264 Q(Xn+1, X*) | (Xi, Yi) : i \u2208 I\u2081 } > 1 \u2212 \u03b1-\u03b5\nTherefore,\nP{Yn+1 \u2208 C(Xn+1)} \u2265 1 \u2212 \u03b1-\u03b5\nIn practice, the magnitude of e is typically on the order of 10-6 when using Brent's method for optimization (Brent, 1973). Our empirical studies show that this small deviation from the target coverage has negligible impact on the method's performance, as demonstrated by the achieved coverage rates in Section 4 (Taylor, 2000; Steinwart and Christmann, 2011)."}, {"title": "Empirical Study", "content": "To evaluate the performance of the proposed Density-Calibrated Conformal Quantile Regression (CQR-d) method compared to the standard Conformal Quantile Regression (CQR), we conducted a comprehensive study comprising both simulation experiments and an application to a heteroscedastic dataset available in R."}, {"title": "Simulation Study", "content": null}, {"title": "Experimental Setup", "content": "To evaluate the performance of CQR-d, we designed a simulation study that captures key scenarios where local adaptation would be beneficial. The synthetic data were generated from a heteroscedastic model with non-linear relationships and varying data density regions (Gal and Ghahramani, 2016). The predictor variable X follows a bimodal distribution, with half of the samples drawn from N(-1,0.5) and the other half from N(1,0.5). This design creates natural regions of different data density, allowing us to assess the method's ability to adapt to local data structures (Meinshausen, 2006; Chaudhuri, 1991).\nThe response variable Y was generated according to Y = sin(2\u03c0X) +0.5X2 + \u20ac, where the error term e follows N(0,0.1 + 0.2|X|). This specification introduces both non-linearity through the sinusoidal and quadratic terms, and heteroscedasticity through the error term that scales with X (Steinwart and Christmann, 2011). Such a setup presents a challenging scenario for traditional prediction interval methods, making it suitable for evaluating the advantages of local adaptation (Romano et al., 2019; Taylor, 2000).\nWe conducted simulations across a range of sample sizes: n = 500, 1000, 2500, 5000, 7500, and 10000. For each sample size, the data was split into training (60%), calibration (20%), and test (20%) sets (Sesia and Cand\u00e8s, 2019). For"}, {"title": "Performance Metrics and Results", "content": "We evaluate both methods using coverage rate and average interval width across test samples. Together, these metrics allow us to assess the fundamental trade-off in prediction interval estimation: the balance between coverage guarantee and interval efficiency (Lei et al., 2018; Lei and Wasserman, 2014).\nTogether, these metrics allow us to assess the fundamental trade-off in prediction interval estimation: the balance between providing reliable coverage and producing informative (narrow) intervals. A method that achieves the nominal coverage level while consistently producing narrower intervals can be considered more efficient, as it provides more precise uncertainty quantification (Romano et al., 2019; Gammerman and Vovk, 2007).\nThe simulation results are summarized in Table 4.1.2 and visualized in Figure 1."}, {"title": "Discussion of Simulation Results", "content": "The simulation results demonstrate CQR-d's ability to maintain coverage while achieving improved efficiency across different sample sizes (Sesia and Cand\u00e8s, 2019). Both methods consistently achieve mean coverage close to the nominal 0.9 level, with CQR ranging from 0.8869 to 0.8990 and CQR-d ranging from 0.8854 to 0.9050 across sample sizes. The coverage rates become more stable as sample size increases, as evidenced by decreasing standard deviations from 0.0393 (CQR) and 0.0373 (CQR-d) at n = 500 to 0.0096 (CQR) and 0.0102 (CQR-d) at n = 10000 (Lei and Wasserman, 2014; Nouretdinov et al., 2001).\nIn terms of interval width, CQR-d produces consistently narrower prediction intervals compared to CQR (Meinshausen, 2006). While CQR maintains relatively stable interval widths ranging from 3.2396 to 3.2696 across all sample sizes, CQR-d's intervals become progressively narrower as sample size increases, from 3.1006 at n = 500 to 2.5798 at n = 10000. This represents an improvement in efficiency of 21.05% at the largest sample size, calculated as (3.2675 - 2.5798)/3.2675 \u00d7 100% (Lei et al., 2018; Taylor, 2000).\nBoth methods show improved stability in their estimates as sample size increases, with standard deviations of both coverage and width decreasing (Steinwart and Christmann, 2011; Srivastava et al., 2014). Notably, CQR-d demonstrates particularly stable performance at larger sample sizes, with width standard deviation reducing from 0.2068 at n = 500 to 0.0558 at n = 10000, while CQR's width standard deviation decreases from 0.3242 to 0.0707 over the same range (Gal and Ghahramani, 2016; Gammerman and Vovk, 2007)."}, {"title": "Application to Diamonds Dataset", "content": "To validate the performance of CQR-d's further, we applied our method to the diamonds dataset, which contains prices and characteristics of nearly 54,000 diamonds and is available in R (Lei and Wasserman, 2014). This dataset presents a natural test case due to its heterogeneous price variations across different diamond characteristics (Takeuchi et al., 2006)."}, {"title": "Experimental Setup", "content": "We employed a standard train-calibration-test split with 38,000, 8,000, and 8,000 samples respectively (Sesia and Cand\u00e8s, 2019). The target coverage level was set to 0.9, consistent with our simulation study. The feature set includes carat weight, cut, color, and clarity, creating a higher-dimensional prediction task that contrasts with our simulation study (Romano et al., 2019; Chaudhuri, 1991)."}, {"title": "Results", "content": "The application of CQR-d to the diamonds dataset revealed that CQR-d achieved better coverage and narrower prediction intervals compared to CQR, as shown in Table 4.2.2 and Figure 2."}, {"title": "Discussion of Diamonds Results", "content": "The results on the diamonds dataset corroborate our simulation findings, though with more modest improvements. CQR-d maintained comparable coverage to CQR (0.89603 versus 0.89932) while achieving a 0.73% reduction in average interval width (3819.240 versus 3847.532). The smaller efficiency gain compared to our simulation study (0.73% versus 21.05%) likely reflects the more complex data structure (Lei et al., 2018; Chaudhuri, 1991): while the simulation featured explicit heteroscedasticity, the diamonds dataset exhibits more subtle"}, {"title": "Conclusion", "content": "We presented Density-Calibrated Conformal Quantile Regression (CQR-d), a novel method for constructing prediction intervals that combines the strengths of conformal prediction (Vovk et al., 2005; Gammerman and Vovk, 2007) and quantile regression (Koenker and Bassett, 1978) while incorporating local data characteristics (Meinshausen, 2006). Our research demonstrates that CQR-d offers a promising approach to addressing the challenges of maintaining reliable coverage (Lei et al., 2018) and producing efficient intervals, particularly in the presence of heteroscedasticity and complex non-linear relationships (Steinwart and Christmann, 2011).\nAn important direction for future research lies in extending CQR-d to causal inference problems, particularly in estimating heterogeneous treatment effects (Lei et al., 2021). The method's ability to adapt to local uncertainty patterns could be particularly valuable in constructing prediction intervals for individual treatment effects, where the uncertainty often varies substantially across subpopulations (Nouretdinov et al., 2001; Sesia and Cand\u00e8s, 2019). Another promising avenue is the exploration of alternative local adaptation mechanisms beyond k-nearest neighbors (Lei and Wasserman, 2014). For instance, kernel-based approaches with adaptive bandwidth selection could provide smoother transitions between regions of different uncertainty (Takeuchi et al., 2006), potentially leading to more refined interval width adjustments in complex data settings (Gal and Ghahramani, 2016)."}]}