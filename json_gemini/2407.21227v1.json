{"title": "Assessing Programming Task Difficulty for Efficient Evaluation of Large Language Models", "authors": ["Florian Tambon", "Amin Nikanjam", "Foutse Khomh", "Giuliano Antoniol"], "abstract": "Large Language Models (LLMs) show promising potential in Software Engineering, especially for code-related tasks like code completion and code generation. LLMs' evaluation is generally centred around general metrics computed over benchmarks. While painting a macroscopic view of the benchmarks and of the LLMs' capacity, it is unclear how each programming task in these benchmarks assesses the capabilities of the LLMs. In particular, the difficulty level of the tasks in the benchmarks is not reflected in the score used to report the performance of the model. Yet, a model achieving a 90% score on a benchmark of predominantly easy tasks is likely less capable than a model achieving a 90% score on a benchmark containing predominantly difficult tasks. This paper devises a framework, HardEval, for assessing task difficulty for LLMs and crafting new tasks based on identified hard tasks. The main idea of our framework is to use a diverse array of prompts for a single task across multiple LLMs in order to obtain a difficulty score for each task of a benchmark. Using two code generation benchmarks, HumanEval+ and ClassEval, we show that HardEval can reliably identify the hard tasks within those benchmarks, highlighting that only 21% for HumanEval+ and 27% for ClassEval of the tasks are hard for LLMs. Through our analysis of task difficulty, we also characterize 6 practical hard task topics which we used to generate new hard tasks. Orthogonal to current benchmarking evaluation efforts, HardEval can assist researchers and practitioners in fostering better assessments of LLMs. The difficulty score can be used to identify hard tasks within existing benchmarks. This, in turn, can be leveraged to generate more hard tasks centred around specific topics either for evaluation or improvement of LLMs. Moreover, while we focus on code generation tasks in our experiments, HardEval generalistic approach can be applied to other domains such as code completion or Q/A.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have been widely used for various code-related tasks in Software Engineering (SE) Nguyen and Nadi (2022); Lemieux et al. (2023); Chen et al. (2023); Moradi Dakhel et al. (2023). Although the performance of LLMs in generating code for different programming languages, such as Python, Java, and C Yu et al. (2023); Jin et al. (2023); Liu et al. (2023) is promising, LLM-generated code like human-written one is not flawless. Researchers already acknowledged that LLM-generated code are prone to bugs Moradi Dakhel et al. (2023); Tambon et al. (2024), and may suffer from vulnerability T\u00f3th et al. (2024); Majdinasab et al. (2023); Klemmer et al. (2024), and quality issues Yeti\u015ftiren et al. (2023).\nGenerally, to assess LLMs on code generation tasks and compare them to each other, well-known benchmarks such as HumanEval Chen et al. (2021); Liu et al. (2023), MBPP Austin et al. (2021),\nCoderEval Yu et al. (2023), and ClassEval Du et al. (2023) are used. These benchmarks are generally obtained by handcrafting tasks or mining existing public repositories over GitHub and filtering them. This way, the code generation capability of LLMs on programming tasks is assessed indirectly at the benchmark level based on general characteristics such as dependency level (e.g., function level being less difficult than class level) and through general metrics such as Pass@k over the benchmark Chen et al. (2021). Moreover, the assessment of those tasks is conducted via a single prompt, which is a single formulation of the task. Yet, recent studies Mizrahi et al. (2024); Wang et al. (2022b) showed that the way prompts are formulated has a high impact on the output quality of LLMs. Thus, current benchmarks' usage and assessment paint a general yet incomplete picture as they fail to provide a more fine-grained analysis of individual tasks within a benchmark. In particular, such benchmarks' usage does not provide a way of quantifying the difficulty of a programming task for LLMs.\nTo tackle this issue, we propose HardEval, a framework for the evaluation, identification, and generation of difficult programming tasks for LLMs. We show with HardEval that this task is not difficult. Indeed, using our framework, all models except one indicate a low difficulty score for the task with a low difficulty averaged over all LLMs. As it turns out, rephrasing this basic prompt slightly or providing small additional contextual information helped the LLMs to easily address the task. As such, while the above prompt could be considered hard, the task per se is not.\nTo guide our study, we formulate the following Research Questions (RQs):\nRQ1 Can HardEval identify hard tasks in a benchmark?\nRQ2 How difficult are tasks contained in the studied benchmarks for LLMs?\nRQ3 Can HardEval support the generation of targeted new difficult tasks?\nHardEval is orthogonal to the existing benchmark's usage and works on top of available benchmarks. HardEval starts by generating a variety of prompts for each task in two well-known code generation benchmarks, HumanEval+ Chen et al. (2021) and ClassEval Du et al. (2023), representing multiple possible formulations per task. For each programming task, we craft prompts with different levels of context information and distinctive phrasing to represent diverse possible wording of the task using GPT-4, resulting in 18 different prompts per task. The framework then queries an array of 5 LLMs (i.e., CodeLLama 7B Roziere et al. (2023), MagiCoder 6.7B Wei et al. (2023), DeepSeekCode 7B Guo et al. (2024), CodeGemma 7B Team et al. (2024) and GPT-3.5 GPT (2023a)) using the formulated prompts to obtain different outputs for every single task. Those outputs are then used to compute a task difficulty score by averaging overall LLMs a combination of the functional correctness (passing all available tests for the task) and the syntactic correctness (using code similarity metrics) obtained on each code sample obtained. This difficulty score separates hard from easy tasks. Such hard tasks uncover particular shortcomings of LLMs that can then be used for improving LLMs. Then, we cluster hard tasks to recognize similar ones using a topic modeling technique called BERTopic Grootendorst (2022). To further generate new tasks, we leverage hard tasks from those topics in two ways: either by transferring constraints from hard tasks onto an easy task, or by modifying an already hard task by adding an additional constraint. Our results show that while benchmark-wide metrics such as accuracy using pass/fail code can give a general trend of a benchmark difficulty, they fail at assessing the difficulty of individual tasks contrary to the difficulty score proposed in HardEval. Moreover, using the identified topics we managed to generate 15 new tasks for both benchmarks for"}, {"title": "2 HardEval", "content": "In this section, we discuss the methodology followed to propose HardEval. After an overview of the framework, we describe how we generate different prompts for programming tasks. We then delve into the details of leveraging LLMs to generate code for our prompts, measuring the difficulty of tasks for LLMs, identifying hard coding tasks, and generating new hard tasks.\nHardEval uses existing benchmarks to operate. Such benchmarks are generally composed of an ensemble of tasks which are represented by a single prompt per task. In our study, we make the distinction: the task is the essence of the functionality to be implemented. For instance, \u201cQuicksort\" or \"Fibonacci sequence\" are examples of such tasks. A prompt is a way to formulate the task so it can be understood by and fed to an LLM. For instance, \u201cWrite a function to calculate the n-th element of the Fibonacci sequence.\" or \"Create a function to return the n-th element of the Fibonacci sequence. The Fibonacci sequence is defined as ...\" are two instances of prompts for the task \"Fibonacci sequence\". As such, each task can be expressed by many different (even infinite) prompts, including different wordings, and amount of contextual information about the task. HardEval aims to evaluate the difficulty of those tasks for code LLMs in order to generate targeted benchmarks to probe potential hard tasks of LLMs."}, {"title": "2.2 Generating Different Prompts based on Transformations", "content": "The first step involves choosing transformations to generate different prompts for the task. We want those transformations to introduce variabilities in our prompts to represent the many ways an LLM could be prompted for the same task. However, those transformations should not introduce changes that alter the semantics of the tasks. In our case, we use two types of such transformations, but any number could be used and combined.\nFirst, we use the Context Information transformation. Indeed, a straightforward way to act on the variability of a prompt is to simply disclose or withdraw certain information in the prompt, while not changing the task. EvoEval Xia et al. (2024), for instance, proposes also to act on the difficulty. However, they do so by adding/removing constraints in the prompt thus modifying the semantics of the task which is not desirable in our case. In our study, we instead propose to divide the information provided in the prompt into three levels. Those three levels of prompts are generated incrementally using the previous level as a starting point. We assume that prompts with more information should be more likely to lead to correct code for LLMs. The levels are defined such as:\n\u2022 First level: contains a minimal amount of information that is the inputs/outputs of the task as well as a high-level description of what is intended. We could use the original prompt in benchmarks for each task as the first level, yet Siddiq et al. Siddiq et al. (2024) showed that there can be inconsistencies across tasks' prompts in benchmarks. Thus, we prefer to generate a new prompt to have similar formatting across all tasks and levels to reduce biases. Moreover, this level is the closest one to the original prompt in benchmarks and contains the same level of information regarding the tasks.\n\u2022 Second level: further adds in the prompt a description of the targeted function in natural language using available oracle code. However, it does not include any direct reference to the function (variable names, helper functions used, ...) unless it was explicitly mentioned in the original prompt of the benchmark.\n\u2022 Third level: complete the prompt with direct references to the function such as variable names, and helper functions used similarly to a pseudo-algorithm but in natural language.\nThe second transformation used is Rephrasing which is paraphrasing a given prompt. Indeed, how the prompts or instructions are formulated has been shown to have an impact on LLM responses Mizrahi et al. (2024); Weber et al. (2023); Wang et al. (2022b). Thus, similarly to these studies, we try to see if different ways of rephrasing a given prompt for an LLM can affect how the LLM addresses the task. To rephrase our prompts, we will use a similar approach as done by Gonen et al. Gonen et al. (2022), that is prompting an LLM to rephrase a given text. Other techniques could be used to assist in"}, {"title": "2.3 Assessing difficulty of tasks", "content": "The second step is to evaluate the difficulty of each task. To do so, we leverage several Code LLM that will serve as our evaluators for code generation. For each prompt per task, each LLM is asked to generate code with varying seeds using the sampling mode of LLMs. The rationale is that most LLMs, especially non-open-source ones, are used with this non-greedy approach in order to generate code. Moreover, using the sampling method for the generation allows us to have a wider variety of generated code in order to evaluate the difficulty of each task. Indeed, we assume an easier task should not be as impacted by the stochasticity of the generation compared to a harder task. The obtained code is then processed to be executed. Sample codes were then executed against available test inputs in the benchmark.\nOnce we have obtained all code for all prompts of all tasks and executed them against tests, we collect different metrics. In our case, we are interested in two metrics: the Correctness and the Similarity to Correct Code of a code sample. Correctness simply quantifies if a given code sample is correct or not using available test cases. As such, it assesses the functional correctness of a code sample. Formally, the Correctness $Cor_s$ for a code sample s is defined as:\n$Cors = \\begin{cases}\n1 & \\text{if } \\forall (i, o) \\in T, s(i) = o \\\\\n0 & \\text{else}\n\\end{cases}$\nwhere T is the set of test cases symbolized by an input i and an output o. That is, the code sample passes all test cases, i.e., gets a value of 1. Intuitively, we expect a harder task to lead to a lower number of prompts leading to correct codes across rephrasing and different levels of context information.\nNonetheless, code samples on a task can be wrong to different degrees depending on the bugs affecting the code sample Tambon et al. (2024). Indeed, a code sample could simply be wrong because of a missing corner case (e.g., forgetting an initial condition) or it could follow a completely false logic and not implement the desired tasks at all. One way to assess this aspect would be to quantify the proportion of test cases for which a code sample s is correct Ouyang et al. (2023). However, this can lead to a biased measure depending on the test set. For instance, if a test set has more test cases assessing one corner case rather than another one, some code samples might end up with a higher proportion of passed tests artificially. Instead, we propose to use classical similarity metrics to measure the distance between a code sample and the the most similar correct code sample. To do so, we will rely on the CodeBLEU Ren et al. (2020) metric to measure the similarity, which measures the syntactical correctness of the code sample. We chose CodeBLEU as it is a widely used metric in code generation tasks Lu et al. (2021); Mastropaolo et al. (2023); Lozhkov et al. (2024); Wang et al. (2021). Thus, the Similarity to Correct Code $Sim_s$ for a code sample s is defined as:\n$Sim_s = max_{sc} CodeBLEU(s, sc), C_{sc} = 1$\nwhere sc are correct code samples according to Correctness. Note that, in practice, we have at least one correct code as a reference which is the oracle code of the task in its benchmark. If s is correct, the similarity is 1 (closest is itself). This allows us to assess, when a code sample is incorrect, how similar it is to a correct code. The Correctness and Similarity to Correct Code are then used to evaluate a code sample separately, or in a Composite manner by aggregating the scores.\nFinally, we use those metrics to compute a score per code sample which in turn helps to assess the difficulty score of the task itself for a given LLM. We used different transformations in this study: Rephrasing and Context Information. So, for instance, we expect an LLM to be more likely to generate a correct code if given a prompt with more context information (of a higher level) than given a prompt of a lower level. To represent this, the scores of the code samples obtained for each prompt can be weighted depending on the transformation. In our case, as we apply the Context Information before the Rephrasing, the difficulty score $R_i$ of the ith Rephrasing is defined as:\n$R_i = \\sum_j s_j \\times scores_j$\nwhere $scores_j$ is the score obtained for the jth code sample using the ith rephrasing, as we generate multiple code samples for a given prompt through sampling generation. $s_j$ are weights to give more or less importance to certain code samples. In our case, all samples are of equal importance and so all $s_j$ equals 1 over the number of samples. Then, we calculate the score $L_i$ for a given level of prompts i:\n$L_i = \\sum_j r_j \\times R_j$\nwhere $r_j$ are weights used to give more or less importance to a certain rephrasing. In our case, all rephrasings are of equal importance and so all $r_j$ equals 1 over the number of rephrasings. Finally, the difficulty score of the ith task in the benchmark for a given LLM is calculated as:\n$Difficulty_i = 1 - (\\alpha \\times L_1 + \\beta \\times L_2 + \\gamma \\times L_3)$\nwhere $\\alpha$, $\\beta$ and $\\gamma$ are weights over the scores obtained for each level. An explanation of the choice of parameters is given in Section 4.2.\nFinally, once all the Difficulty scores have been computed for all tasks and LLMs, the score of a given task is simply the average over all LLMs. One can then use the score to determine which tasks are hard and which are not. To do so, we will use a threshold of 0.5. The choice of the threshold is motivated by our definition of the score for each code sample: a score of 0.5 would mean, in the most extreme case and for the Composite score, that all generated code samples for a task are incorrect but are very similar to a correct one. In practice, a score higher than 0.5 for a task would be more reflective of a high number of incorrect codes with a lower similarity, pointing towards a hard task."}, {"title": "2.4 Analyzing and Generating new Hard Tasks", "content": "Once the hard tasks have been identified, the third step is to generate new tasks based on the identified hard tasks. To do so, we propose to leverage a topic modeling approach with BERTopic Grootendorst (2022), as was used in similar studies when dealing with natural language data H\u00e4m\u00e4l\u00e4inen et al. (2023); F\u00fctterer et al. (2023); Shen et al. (2023). For a given benchmark, the topic model clusters all tasks (easy and hard) using the prompts of Level 1 before rephrasing. We use this specific prompt type to have similar formatting across tasks while being as close as possible to the original prompt as mentioned in Section 2.2. As the process will inherently label some tasks as \"noise\", noisy tasks are not considered in the analysis. Then, for each topic, the proportion of hard tasks among all tasks in the topic is calculated and the topics with the highest proportion of hard tasks are analyzed. Those topics constitute tasks that have a similar functionality/goal, for instance, tasks dealing with sequence generation. Finally, we aim to generate more hard tasks leveraging the tasks from those selected topics with a Task LLM to generate additional new tasks. To do so, we leverage the Self-Instruct approach Wang et al. (2022a) which allows generating new tasks based on a subset of sampled tasks. Nonetheless, just providing easy and hard tasks to the Task LLM and asking it to generate more hard tasks did not lead to meaningful results according to our observation. Indeed, the small number of tasks per topic makes it quite impossible to determine what constitutes a hard task. Instead, we use a combination approach, where we ask the Task LLM to augment a hard task by adding a constraint, or by adding a constraint from a hard task to an easy one. This ultimately reduces the diversity of tasks generated but increases the likelihood of the generated tasks being hard. The newly generated tasks constitute a smaller sub-benchmark of targeted tasks related to the topic that can be used to further assess LLMs corner cases for code generation tasks."}, {"title": "3 Experiments", "content": "To have different prompts, we choose to generate 3 levels and 6 rephrasings per level. We chose those parameters as the trade-off between generating diverse enough prompts while limiting the amount to generate. For a benchmark of n tasks, this effectively means we have to generate 18 \u00d7 n prompts. We further ask each Code LLM to generate 5 code samples per individual prompt resulting in 90 \u00d7 n code samples for each Code LLM to generate. As an example, for a benchmark containing 200 tasks, this process yields 18,000 generated code for our evaluation per Code LLM. We make available all the prompts and code generated in our replication package rep (2024).\nIn our experiment, we chose two benchmarks: HumanEval+ Liu et al. (2023) and ClassEval Du et al. (2023). Those benchmarks were chosen as they cover different types of tasks and dependency levels. HumanEval+ is widely used as a standard benchmark in code generation Big (2024). It is an extended variant of HumanEval Chen et al. (2021), that contains more tests to assess the correctness of code samples. The benchmark consists of 164 handwritten Python programming problems with a function signature, docstring, body and several unit tests. HumanEval+ tasks are at the function level dependency as they do not require external libraries, besides Python standard library, or class/file contexts to be operated. For the original prompt, we use the docstring as given in HumanEval+.\nClassEval is a benchmark of 400 handcrafted classes coded in Python. Each class contains a description and on average 4 methods each with a docstring. We consider a task to be one of the 400 methods of ClassEval. Contrary to HumanEval+, ClassEval tasks are more similar to practical code used in projects and rely on a class context to operate. ClassEval proposes three prompting strategies: holistic, incremental and compositional. In our case, we use the compositional strategy, that is, providing the class context to the LLM and asking it to complete one method of the class, only modifying the docstring of the method to complete through our crafted prompts. We do not make use of the other generation approaches as it reduces the control we have over the prompts and could introduce biases: the holistic approach, as it implies asking the LLM to generate the whole class from scratch, can impact the performance artificially. Similarly, the incremental approach requires the reuse of codes generated by the LLM for other methods when generating new ones, which could also alter the difficulty. As we need to generate a high number of code samples and prompts, we reduce"}, {"title": "4 Results", "content": "We aim to assess whether the level of information in the prompts impacts the generation of the code by the LLM in such a way that is quantifiable by our metrics. While for rephrasing, the impact on the output has been analyzed in the literature Mizrahi et al. (2024); Wang et al. (2022b), the context information transformation as we formulate it in our work has not been so. Some studies Ouyang et al. (2023); Fagadau et al. (2024) showed that prompt-added contents can range from no benefit to negative effect on the code generated. For instance, Ouyang et al. Ouyang et al. (2023) showed that a longer description in the prompt could decrease performance. As such, we verify that our added context transformation has the intended functionality, i.e., the more context we provide, the better the generation.\nAs reported in Table 1, the Pass@1 rate increases with the Context Information. We now set out to show there is a correlation between the scores computed by our metric and the Context Information. To do so, we use the scores of the generated code for each rephrasing (see Equation 3) and compare it to the prompt level it comes from. To see the impact of both the Correctness and Similarity metric, we compute each metric individually as well as the composite of the two. As we introduce gradation in the injected information in the prompts through the Context information, one expects to observe a correlation between the level of information and the score obtained (i.e., the more information in the prompt, the higher the score). To calculate the correlation, we use the Spearman-p Spearman (1904), as we are interested in a monotonic increase between the two variables. We report the results for each Code LLM independently and for the averaged over all LLMs."}, {"title": "4.2 Validation of the parameters for the Difficulty Score", "content": "Without ground truth, we describe the rationale behind the choice of parameters $\\alpha$, $\\beta$ and $\\gamma$ of the Difficulty score. In the previous section (Section 4.1), we show that the lower the amount of information in the prompt, the less likely the model could produce a correct code. As such, we should consider that $\\alpha \\geq \\beta > \\gamma$, that is, emphasizing more on lower information context. Otherwise, this could artificially bias the difficulty score towards assessing tasks as easy. Consider that this would be equivalent to having an exam where most questions would have tips to help the participants: the exam would easily be passed, but it would not necessarily be representative of the tasks in the exam. The issue then becomes which values to assign to parameters. To decide, we analyze the shape of the Difficulty score distributions obtained by varying the parameters.\nAs illustrated in Figure 2, emphasizing too much on $\\alpha$ will give too much importance to low information context. In this case, tasks will tend to be declared harder (see Figure 2, orange), because LLMs will struggle more in a low information context to produce correct code fragments. However, on the contrary, the lower the $\\alpha$ the more emphasis is put on higher information context (see Figure 2, blue), for which we provide more information to the LLMs than typical prompts in benchmark normally would. This instead makes more tasks be declared as easy. A good balance would be found between those two extremes: too much and too little reliance on the low information context. To do so, we assume that benchmarks are balanced. That is, we consider that LLMs should not consider"}, {"title": "4.3 RQ1: Can HardEval identify hard tasks in a benchmark?", "content": "The goal of this RQ is to show HardEval can reliably identify hard tasks in a benchmark. To do so, using greedy decoding (i.e. Temperature = 0.0), as done in the-state-of-the-art leaderboard Eva (2024); Cla (2024) for each of 5 LLMs of our study, we evaluate each task using a single prompt to see if each LLM outputs a correct code using available tests. For the single prompt used, we chose the level 1 prompt obtained before rephrasing for each task, which is the closest to the original prompt of each benchmark. We did so for a similar reason as mentioned in Section 2.2, i.e. removing potential formatting issues and providing a fair comparison across tasks. For greedy decoding, we consider a task to be hard for an LLM if it does not manage to generate a correct output. We consider a task to be hard for all LLMs if a majority of LLMs fails to generate a correct code for that task. We use the opposite settings for easy tasks. At the same time, we collect tasks that are labelled as hard/easy by HardEval using our difficulty score (see Equation 5), which is when the score is above/below 0.5. Using those sets of tasks, we can compute the overlap between the sets, e.g., the tasks that are hard according to HardEval but not for the greedy decoding definition, for a given LLM or all LLMs. This allows us to see if the sets of tasks are similar across approaches or not. Then, we compute the accuracy of the greedy decoding over the tasks by splitting them into two sets of tasks, using the set of hard/easy tasks of HardEval. Similarly, we compute the median of the difficulty score of HardEval obtained on the tasks by splitting them into two sets, based on the set of hard/easy tasks from the greedy assessment, for each LLM. Intuitively, we should see a drop in performance of the accuracy of the greedy decoding on the tasks that are noted hard by HardEval and an increase in performance for the tasks that are labelled as easy by HardEval compared to the greedy accuracy of each LLM.\nComputing the overlap between the sets of hard tasks for both approaches results in an overlap ratio ranging from 0.24 to 0.63 for HumanEval+ and from 0.56 to 0.62 for ClassEval depending on the LLM. Similarly, the overlap for the sets of easy tasks ranges from 0.54 to 0.78 for HumanEval+ and from 0.69 to 0.80 for ClassEval depending on the LLM. If we consider all LLMs, the overlap on the hard tasks is 0.46 for HumanEval+ and 0.62 for ClassEval, while for the easy tasks, it is 0.71 and 0.78 respectively. Thus the assessment of hard tasks between the two approaches is dissimilar while"}, {"title": "4.4 RQ2: How difficult are tasks contained in the studied benchmarks for LLMS?", "content": "Our goal in this RQ is to study the distribution of difficulty scores across tasks as well as the type of tasks that are difficult in the benchmarks under study. To do so, first, we use HardEval difficulty scores for each task calculated in RQ1, both for the Code LLM individually as well as all LLMs. By doing so, we can output the cumulative distribution of the difficulty score across tasks for analysis. Then, we use the BERTopic model as described in Section 2.4. The topic analysis returns 17 topics for HumanEval+ and 21 topics for ClassEval. The two first authors manually check the tasks in those topics to provide a general name to the topic that is as representative as possible of the tasks within the topic. We then calculate the proportion of hard tasks in each topic."}, {"title": "4.5 RQ3: Can HardEval support the generation of targeted new difficult tasks?", "content": "Finally, this RQ aims to show that, using identified hard tasks, we can further generate new hard tasks that are targeted at specific problems. We do not aim to be systematic, but merely to show that, using identified hard tasks and topics, it is possible to generate new tasks. To do so, we use the result from the RQ2 (i.e., the topics) and select the first three topics in terms of the number of tasks with at least a third of the tasks being hard. We do so to have a sufficient number of LLM-easy and hard tasks for the Task LLM to be able to provide meaningful new tasks. For each topic, using the Task-LLM, we generated tasks, checking each of them manually for validity, until we got 5 different new tasks using the process described at the end of Section 2.4. Then, we manually write an Oracle code as well as unit tests for this task. When possible, we try to remain as close as possible to the oracle code of the base task used in the generation process. Finally, we assess the newly generated tasks to the same Step 1 and Step 2 of HardEval to measure the difficulty score. We thus obtain, for each of the picked topics, new tasks that are assessed."}, {"title": "5 Discussion", "content": "With HardEval, we show that assessing the difficulty of a task from the point of view of LLMs is feasible as well as identifying hard tasks. In particular, hard tasks identified with HardEval are truly hard as they are more resilient to the potential variability of the prompt contrary to assessing using a single prompt. While relying on human judgment to define a task difficulty could be used, it is unlikely to work for LLMs as they process code and task instructions differently than humans Kou et al. (2023). Ouyang et al. Ouyang et al. (2023) showed that the difficulty assessed by humans on code competition only weakly correlates with the test pass rate variance across code samples generated by ChatGPT. Thus, it is likely that human assessment or related metrics might not be accurate for estimating LLMs' task difficulty contrary to the difficulty score of HardEval. Our proposed approach, while applied in experiments to code generation, can be extended to other code-related tasks. Indeed, the transformations we used are easily transferable to other paradigms and new transformations could be defined.\nAnalyzing the tasks that were labeled as hard, we observe that LLMs tend to disregard certain parts of the instructions, which can lead to certain test cases failing thus making the code incorrect. That is why similarity helps (i.e., code is incorrect but very similar to a correct one) (Finding 1). This phenomenon happens generally as the number of constraints in the prompt (and in the function) increases or if the prompt becomes longer. This explains why coding-competition or algorithmic-like problems in both benchmarks, which require a lot of description to express the many operations and constraints, generally lead to a higher difficulty, as the LLM is overwhelmed with information. This echoes Ouyang et al. Ouyang et al. (2023), as they showed that description length impacts the test pass rate in ChatGPT. This however can not on its own explain why difficulty could be higher as Level 3 prompts tend to have a better score and are longer than lower-level prompts. We also note that"}, {"title": "6 Threats to Validity", "content": "Internal validity: One threat to the validity of our results comes from the way we measure the difficulty of tasks. To mitigate this threat, we combined two metrics to see if a programming task is difficult for LLMs to code using functional and syntactic correctness and showed it correlates with the Context Information level. Those metrics were used in studies investigating LLM capabilities for code generation. We used a linear combination of all levels having the minimum weight for Level 3 (with maximum contextual information) and maximum for Level 1 (with the least information) to put more emphasis on the Level 1 prompt score. While this might influence the decision on the hard tasks, it is more likely to have some easy tasks be considered as hard, as we showed that the metrics correlate positively with the level. Another threat might come from the generation of the prompts. The transformations used are motivated based on existing works. Moreover, while we have made use of state-of-the-art GPT-4 to generate them, we manually checked the prompts to make sure that the generative process did not alter semantics. Finally, another threat to validity is the fact we did not consider examples in the prompts. Not adding examples in prompts, as we mentioned, can decrease the performance of the models and so add artificially hard tasks. This was nonetheless necessary to make the comparison fair across tasks as not all examples contain the same information. However, beyond having some easy tasks be considered as hard, it does not affect the applicability of HardEval.\nExternal validity: Our sets of tasks from the selected benchmarks could be a threat. The collection of tasks might not be representative of real-world programming tasks to assess LLMs' capabilities. Nonetheless, both HumanEval+ and ClassEval are used to evaluate Code LLM performance Eva (2024); Cla (2024). Moreover, the objective of the study was to set the scene for the assessment of task difficulty rather than dwell on the practicalities of the task. So, we believe using tasks from these two benchmarks provides a diverse set of tasks to assess the difficulty of code generation using LLMs.\nReliability validity: We detailed our methodology, and to enable other researchers to reproduce or expand upon our work, we make our code and data publicly available rep (2024)."}, {"title": "7 Conclusion", "content": "In this paper", "generation": "HumanEval+ and ClassEval. A set of diverse prompts is crafted for each task including different levels of contextual information about the task and various rephrasings, yielding 18 different prompts per task. 5 state-of-the-art LLMs were then employed to solve coding tasks using the set of prompts. We proposed a difficulty score based on functional and syntactic correctness. Tasks with high average difficulty scores across all LLMs were identified as hard coding tasks. Based on thematic similarities, we clustered tasks from both benchmarks into respectively 17 and 21 topics to inspect their characteristics. For selected topics, we generated new tasks. New tasks were then assessed to see if they were really hard based on the proposed score"}]}