{"title": "SAM2POINT: SEGMENT ANY 3D AS VIDEOS\nIN ZERO-SHOT AND PROMPTABLE MANNERS", "authors": ["Ziyu Guo", "Renrui Zhang", "Xiangyang Zhu", "Chengzhuo Tong", "Peng Gao", "Chunyuan Li", "Pheng-Ann Heng"], "abstract": "We introduce SAM2POINT, a preliminary exploration adapting Segment Anything\nModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2POINT\ninterprets any 3D data as a series of multi-directional videos, and leverages SAM\n2 for 3D-space segmentation, without further training or 2D-3D projection. Our\nframework supports various prompt types, including 3D points, boxes, and masks,\nand can generalize across diverse scenarios, such as 3D objects, indoor scenes,\noutdoor scenes, and raw LiDAR. Demonstrations on multiple 3D datasets, e.g.,\nObjaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight the robust general-\nization capabilities of SAM2POINT. To our best knowledge, we present the most\nfaithful implementation of SAM in 3D, which may serve as a starting point for\nfuture research in promptable 3D segmentation.", "sections": [{"title": "1 INTRODUCTION", "content": "Segment Anything Model (SAM) (Kirillov et al., 2023) has established a superior and fundamental\nframework for interactive image segmentation. Building on its strong transferability, follow-up\nresearch further extends SAM to diverse visual domains, e.g., personalized objects (Zhang et al.,\n2023b; Liu et al., 2023d), medical imaging (Ma et al., 2024; Mazurowski et al., 2023), and temporal\nsequences (Yang et al., 2023a; Cheng et al., 2023). More recently, Segment Anything Model 2\n(SAM 2) (Ravi et al., 2024) is proposed for impressive segmentation capabilities in video scenarios,\ncapturing complex real-world dynamics.\nDespite this, effectively adapting SAM for 3D segmentation still remains an unresolved challenge.\nWe identify three primary issues within previous efforts, as compared in Table 1, which prevent them\nfrom fully leveraging SAM's advantages:\n\u2022 Inefficient 2D-3D Projection. Considering the domain gap between 2D and 3D, most\nexisting works represent 3D data as its 2D counterpart as input for SAM, and back-project\nthe segmentation results into 3D space, e.g., using additional RGB images (Yang et al.,\n2023b; Yin et al., 2024; Xu et al., 2023a), multi-view renderings (Zhou et al., 2023b), or\nNeural Radiance Field (Cen et al., 2023). Such modality transition introduces significant\nprocessing complexity, hindering efficient implementation.\n\u2022 Degradation of 3D Spatial Information. The reliance on 2D projections results in the loss\nof fine-grained 3D geometries and semantics, as multi-view data often fails to preserve\nspatial relations. Furthermore, the internal structures of 3D objects cannot be adequately\ncaptured by 2D images, significantly limiting segmentation accuracy.\n\u2022 Loss of Prompting Flexibility. A compelling strength of SAM lies in its interactive capa-\nbilities through various prompt alternatives. Unfortunately, these functionalities are mostly\ndisregarded in current methods, as users struggle to specify precise 3D positions using 2D\nrepresentations. Consequently, SAM is typically used for dense segmentation across entire\nmulti-view images, thereby sacrificing interactivity.\n\u2022 Limited Domain Transferability. Existing 2D-3D projection techniques are often tailored\nto specific 3D scenarios, heavily dependent on in-domain patterns. This makes them chal-\nlenging to apply to new contexts, e.g., from objects to scenes or from indoor to outdoor\nenvironments. Another research direction (Zhou et al., 2024) aims to train a promptable net-\nwork from scratch in 3D. While bypassing the need for 2D projections, it demands substantial\ntraining and data resources and may still be constrained by training data distributions.\nIn this project, we introduce SAM2POINT, adapting SAM 2 for efficient, projection-free, promptable,\nand zero-shot 3D segmentation. As an initial step in this direction, our target is not to push the\nperformance limit, but rather to demonstrate the potential of SAM in achieving robust and effective\n3D segmentation in diverse contexts. Specifically, SAM2POINT exhibits three features as outlined:\n\u2022 Segmenting Any 3D as Videos. To preserve 3D geometries during segmentation, while\nensuring compatibility with SAM 2, we adopt voxelization to mimic a video. Voxelized 3D"}, {"title": "2 SAM2POINT", "content": "The detailed methodology of SAM2POINT is presented in Figure 2. In Section 2.1, we introduce how\nSAM2POINT efficiently formats 3D data for compatibility with SAM 2 (Ravi et al., 2024), avoiding\ncomplex projection process. Then, in Section 2.2, we detail the three types of 3D prompt supported\nand their associated segmentation techniques. Finally, in Section 2.3, we illustrate four challenging\n3D scenarios effectively addressed by SAM2POINT."}, {"title": "2.1 3D DATA AS VIDEOS", "content": "Given any object-level or scene-level point cloud, we denote it by P\u2208 Rn\u00d76, with each point as\np = (x,y,z,r, g, b). Our aim is to convert P into a data format that, for one hand, SAM 2 can\ndirectly process in a zero-shot manner, and, for the other, the fine-grained spatial geometries can\nbe well preserved. To this end, we adopt the 3D voxelization technique. Compared to RGB image\nmapping (Yang et al., 2023b; Yin et al., 2024; Xu et al., 2023a), multi-view rendering (Zhou et al.,\n2023b), and NeRF (Cen et al., 2023) in previous efforts, voxelization is efficiently performed in 3D\nspace, thereby free from information degradation and cumbersome post-processing.\nIn this way, we obtain a voxelized representation of the 3D input, denoted by V \u2208 Rw\u00d7h\u00d71\u00d73 with\neach voxel as v = (r, g, b). For simplicity, the (r, g, b) value is set according to the point nearest to the\nvoxel center. This format closely resembles videos with a shape of w \u00d7 h \u00d7 t \u00d7 3. The main difference\nis that, video data contains unidirectional temporal dependency across t frames, while 3D voxels are\nisotropic along three spatial dimensions. Considering this, we convert the voxel representation as a\nseries of multi-directional videos, inspiring SAM 2 to segment 3D the same way as videos."}, {"title": "2.2 PROMPTABLE SEGMENTATION", "content": "For flexible interactivity, our SAM2POINT supports three types of prompt in 3D space, which can be\nutilized either separately or jointly. We specify the prompting and segmentation details below:\n\u2022 3D Point Prompt, denoted as pp = (Xp, Yp, Zp). We first regard pp as an anchor point in\n3D space to define three orthogonal 2D sections. Starting from these sections, we divide the\n3D voxels into six subparts along six spatial directions, i.e., front, back, left, right, up, and\ndown. Then, we regard them as six different videos, where the section serves as the first\nframe and pp is projected as the 2D point prompt. After applying SAM 2 for concurrent\nsegmentation, we integrate the results of six videos as the final 3D mask prediction.\n\u2022 3D Box Prompt, denoted as bp = (Xp, Yp, Zp, Wp, hp, lp), including 3D center coordinates\nand dimensions. We adopt the geometric center of bp as the anchor point, and represent\nthe 3D voxels by six different videos as aforementioned. For video of a certain direction,\nwe project bp into the corresponding 2D section to serve as the box point for segmentation.\nWe also support 3D box with rotation angles, e.g., (ap, \u03b2p, Yp), for which the bounding\nrectangle of projected bp is adopted as the 2D prompt."}, {"title": "2.3 ANY 3D SCENARIOS", "content": "With our concise framework design, SAM2POINT exhibits superior zero-shot generalization perfor-\nmance across diverse domains, ranging from objects to scenes and indoor to outdoor environments.\nWe elaborate on four distinct 3D scenarios below:\n\u2022 3D Object, e.g., Objaverse (Deitke et al., 2023), with a wide array of categories, possesses\nunique characteristics across different instances, including colors, shapes, and geometries.\nAdjacent components of an object might overlap, occlude, or integrate with each other,\nwhich requires models to accurately discern subtle differences for part segmentation.\n\u2022 Indoor Scene, e.g., S3DIS (Armeni et al., 2016) and ScanNet (Dai et al., 2017), are typically\ncharacterized by multiple objects arranged within confined spaces, like rooms. The complex\nspatial layouts, similarity in appearance, and varied orientations between objects pose\nchallenges for models to segment them from backgrounds.\n\u2022 Outdoor Scene, e.g., Semantic3D (Hackel et al., 2017), differs from indoor scenes, primarily\ndue to the stark size contrasts of objects (buildings, vehicles, and humans) and the larger\nscale of point clouds (from a room to an entire street). These variations complicates the\nsegmentation of objects whether at a global scale or a fine-grained level.\n\u2022 Raw LiDAR, e.g., KITTI (Geiger et al., 2012) in autonomous driving, is distinct from\ntypical point clouds for its sparse distribution and absence of RGB information. The sparsity\ndemands models to infer missing semantics for understanding the scene, and the lack of\ncolors enforces models to only rely on geometric cues to differentiate between objects. In\nSAM2POINT, we directly set the RGB values of 3D voxels by the LiDAR intensity."}, {"title": "3 DISCUSSION AND INSIGHT", "content": "Building on the effectiveness of SAM2POINT, we delve into two compelling yet challenging issues\nwithin the realm of 3D, and share our insights on future multi-modality learning."}, {"title": "3.1 HOW TO ADAPT 2D FOUNDATION MODELS TO 3D?", "content": "The availability of large-scale, high-quality data has significantly empowered the development of large\nmodels in language (Brown et al., 2020; Touvron et al., 2023; Zhang et al., 2023a), 2D vision (Liu\net al., 2023b; Team et al., 2023; Chen et al., 2024), and vision-language (Gao et al., 2024; Liu et al.,\n2023a; Li et al., 2024; Zhang et al., 2024) domains. In contrast, the 3D field has long struggled with a\nscarcity of data, hindering the training of large 3D models. As a result, researchers have turned to the\nalternative of transferring pre-trained 2D models into 3D.\nThe primary challenge lies in bridging the modal gap between 2D and 3D. Pioneering approaches,\nsuch as PointCLIP (Zhang et al., 2022), its V2 (Zhu et al., 2022), and subsequent methods (Ji et al.,\n2023; Huang et al., 2023), project 3D data into multi-view images, which encounter implementation\ninefficiency and information loss. Another line of work, including ULIP series (Xue et al., 2022;\n2023), I2P-MAE (Zhang et al., 2023c), and others (Liu et al., 2023c; Qi et al., 2023; Guo et al., 2023a),\nemploys knowledge distillation using 2D-3D paired data. While this method generally performs\nbetter due to extensive training, it suffers from limited 3D transferability in out-of-domain scenarios.\nRecent efforts have also explored more complex and costly solutions, such as joint multi-modal\nspaces (e.g., Point-Bind & Point-LLM (Guo et al., 2023b)), larger-scale pre-training (Uni3D (Zhou\net al., 2023a)), and virtual projection techniques (Any2Point (Tang et al., 2024)).\nFrom SAM2POINT, we observe that representing 3D data as videos through voxelization may offer an\noptimal solution, providing a balanced trade-off between performance and efficiency. This approach\nnot only preserves the spatial geometries inherent in 3D space with a simple transformation, but\nalso presents a grid-based data format that 2D models can directly process. Despite this, further\nexperiments are necessary to validate and reinforce this observation."}, {"title": "3.2 WHAT IS THE POTENTIAL OF SAM2POINT IN 3D DOMAINS?", "content": "To the best of our knowledge, SAM2POINT presents the most accurate and comprehensive imple-\nmentation of SAM in 3D, successfully inheriting its implementation efficiency, promptable flexibility,\nand generalization capabilities. While previous SAM-based approaches (Yang et al., 2023b; Xu\net al., 2023a; Yin et al., 2024) have achieved 3D segmentation, they often fall short in scalability and\ntransferability to benefit other 3D tasks. In contrast, inspired by SAM in 2D domains, SAM2POINT\ndemonstrates significant potential to advance various 3D applications.\nFor fundamental 3D understanding, SAM2POINT can serve as a unified initialized backbone for\nfurther fine-tuning, offering strong 3D representations simultaneously across 3D objects, indoor\nscenes, outdoor scenes, and raw LiDAR. In the context of training large 3D models, SAM2POINT\ncan be employed as an automatic data annotation tool, which mitigates the data scarcity issue by\ngenerating large-scale segmentation labels across diverse scenarios. For 3D and language-vision\nlearning, SAM2POINT inherently provides a joint embedding space across 2D, 3D, and video\ndomains, due to its zero-shot capabilities, which could further enhance the effectiveness of models\nlike Point-Bind (Guo et al., 2023b). Additionally, in the development of 3D large language models\n(LLMs) (Hong et al., 2023; Xu et al., 2023b; Wang et al., 2023; Guo et al., 2023b), SAM2POINT can\nfunction as a powerful 3D encoder, supplying LLMs with 3D tokens, and leveraging its promptable\nfeatures to equip LLMs with promptable instruction-following capabilities."}, {"title": "4 DEMOS", "content": "In Figures 3-7, we showcase demonstrations of SAM2POINT in segmenting 3D data with various 3D\nprompt on different datasets (Deitke et al., 2023; Armeni et al., 2016; Dai et al., 2017; Hackel et al.,\n2017; Geiger et al., 2012). For further implementation details, please refer to our open-sourced code."}, {"title": "5 CONCLUSION", "content": "In this project, we propose SAM2POINT, which leverages Segment Anything 2 (SAM 2) to 3D\nsegmentation with a zero-shot and promptable framework. By representing 3D data as multi-\ndirectional videos, SAM2POINT supports various types of user-provided prompt (3D point, box,\nand mask), and exhibits robust generalization across diverse 3D scenarios (3D object, indoor scene,\noutdoor environment, and raw sparse LiDAR). As a preliminary investigation, SAM2POINT provides\nunique insights into adapting SAM 2 for effective and efficient 3D understanding. We hope our\nmethod may serve as a foundational baseline for promptable 3D segmentation, encouraging further\nresearch to fully harness SAM 2's potential in 3D domains."}]}