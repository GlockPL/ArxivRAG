{"title": "SAM2POINT: SEGMENT ANY 3D AS VIDEOS IN ZERO-SHOT AND PROMPTABLE MANNERS", "authors": ["Ziyu Guo", "Renrui Zhang", "Xiangyang Zhu", "Chengzhuo Tong", "Peng Gao", "Chunyuan Li", "Pheng-Ann Heng"], "abstract": "We introduce SAM2POINT, a preliminary exploration adapting Segment Anything Model 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2POINT interprets any 3D data as a series of multi-directional videos, and leverages SAM 2 for 3D-space segmentation, without further training or 2D-3D projection. Our framework supports various prompt types, including 3D points, boxes, and masks, and can generalize across diverse scenarios, such as 3D objects, indoor scenes, outdoor scenes, and raw LiDAR. Demonstrations on multiple 3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight the robust generalization capabilities of SAM2POINT. To our best knowledge, we present the most faithful implementation of SAM in 3D, which may serve as a starting point for future research in promptable 3D segmentation.", "sections": [{"title": "1 INTRODUCTION", "content": "Segment Anything Model (SAM) (Kirillov et al., 2023) has established a superior and fundamental framework for interactive image segmentation. Building on its strong transferability, follow-up research further extends SAM to diverse visual domains, e.g., personalized objects (Zhang et al., 2023b; Liu et al., 2023d), medical imaging (Ma et al., 2024; Mazurowski et al., 2023), and temporal sequences (Yang et al., 2023a; Cheng et al., 2023). More recently, Segment Anything Model 2 (SAM 2) (Ravi et al., 2024) is proposed for impressive segmentation capabilities in video scenarios, capturing complex real-world dynamics.\nDespite this, effectively adapting SAM for 3D segmentation still remains an unresolved challenge.\nWe identify three primary issues within previous efforts, as compared in Table 1, which prevent them from fully leveraging SAM's advantages:\n\u2022 Inefficient 2D-3D Projection. Considering the domain gap between 2D and 3D, most existing works represent 3D data as its 2D counterpart as input for SAM, and back-project the segmentation results into 3D space, e.g., using additional RGB images (Yang et al., 2023b; Yin et al., 2024; Xu et al., 2023a), multi-view renderings (Zhou et al., 2023b), or Neural Radiance Field (Cen et al., 2023). Such modality transition introduces significant processing complexity, hindering efficient implementation.\n\u2022 Degradation of 3D Spatial Information. The reliance on 2D projections results in the loss of fine-grained 3D geometries and semantics, as multi-view data often fails to preserve spatial relations. Furthermore, the internal structures of 3D objects cannot be adequately captured by 2D images, significantly limiting segmentation accuracy.\n\u2022 Loss of Prompting Flexibility. A compelling strength of SAM lies in its interactive capabilities through various prompt alternatives. Unfortunately, these functionalities are mostly disregarded in current methods, as users struggle to specify precise 3D positions using 2D representations. Consequently, SAM is typically used for dense segmentation across entire multi-view images, thereby sacrificing interactivity.\n\u2022 Limited Domain Transferability. Existing 2D-3D projection techniques are often tailored to specific 3D scenarios, heavily dependent on in-domain patterns. This makes them chal-lenging to apply to new contexts, e.g., from objects to scenes or from indoor to outdoor environments. Another research direction (Zhou et al., 2024) aims to train a promptable net-work from scratch in 3D. While bypassing the need for 2D projections, it demands substantial training and data resources and may still be constrained by training data distributions.\nIn this project, we introduce SAM2POINT, adapting SAM 2 for efficient, projection-free, promptable, and zero-shot 3D segmentation. As an initial step in this direction, our target is not to push the performance limit, but rather to demonstrate the potential of SAM in achieving robust and effective 3D segmentation in diverse contexts. Specifically, SAM2POINT exhibits three features as outlined:\n\u2022 Segmenting Any 3D as Videos. To preserve 3D geometries during segmentation, while ensuring compatibility with SAM 2, we adopt voxelization to mimic a video. Voxelized 3D"}, {"title": "2 SAM2POINT", "content": "The detailed methodology of SAM2POINT is presented in Figure 2. In Section 2.1, we introduce how SAM2POINT efficiently formats 3D data for compatibility with SAM 2 (Ravi et al., 2024), avoiding complex projection process. Then, in Section 2.2, we detail the three types of 3D prompt supported and their associated segmentation techniques. Finally, in Section 2.3, we illustrate four challenging 3D scenarios effectively addressed by SAM2POINT."}, {"title": "2.1 3D DATA AS VIDEOS", "content": "Given any object-level or scene-level point cloud, we denote it by \\(P \\in \\mathbb{R}^{n \\times 6}\\), with each point as \\(p = (x,y,z,r, g, b)\\). Our aim is to convert \\(P\\) into a data format that, for one hand, SAM 2 can directly process in a zero-shot manner, and, for the other, the fine-grained spatial geometries can be well preserved. To this end, we adopt the 3D voxelization technique. Compared to RGB image mapping (Yang et al., 2023b; Yin et al., 2024; Xu et al., 2023a), multi-view rendering (Zhou et al., 2023b), and NeRF (Cen et al., 2023) in previous efforts, voxelization is efficiently performed in 3D space, thereby free from information degradation and cumbersome post-processing.\nIn this way, we obtain a voxelized representation of the 3D input, denoted by \\(V \\in \\mathbb{R}^{w \\times h \\times 1 \\times 3}\\) with each voxel as \\(v = (r, g, b)\\). For simplicity, the \\((r, g, b)\\) value is set according to the point nearest to the voxel center. This format closely resembles videos with a shape of \\(w \\times h \\times t \\times 3\\). The main difference is that, video data contains unidirectional temporal dependency across t frames, while 3D voxels are isotropic along three spatial dimensions. Considering this, we convert the voxel representation as a series of multi-directional videos, inspiring SAM 2 to segment 3D the same way as videos."}, {"title": "2.2 PROMPTABLE SEGMENTATION", "content": "For flexible interactivity, our SAM2POINT supports three types of prompt in 3D space, which can be utilized either separately or jointly. We specify the prompting and segmentation details below:\n\u2022 3D Point Prompt, denoted as \\(p_p = (X_p, Y_p, Z_p)\\). We first regard \\(p_p\\) as an anchor point in 3D space to define three orthogonal 2D sections. Starting from these sections, we divide the 3D voxels into six subparts along six spatial directions, i.e., front, back, left, right, up, and down. Then, we regard them as six different videos, where the section serves as the first frame and \\(p_p\\) is projected as the 2D point prompt. After applying SAM 2 for concurrent segmentation, we integrate the results of six videos as the final 3D mask prediction.\n\u2022 3D Box Prompt, denoted as \\(b_p = (X_p, Y_p, Z_p, W_p, h_p, l_p)\\), including 3D center coordinates and dimensions. We adopt the geometric center of \\(b_p\\) as the anchor point, and represent the 3D voxels by six different videos as aforementioned. For video of a certain direction, we project \\(b_p\\) into the corresponding 2D section to serve as the box point for segmentation. We also support 3D box with rotation angles, e.g., \\((\\alpha_p, \\beta_p, \\gamma_p)\\), for which the bounding rectangle of projected \\(b_p\\) is adopted as the 2D prompt."}, {"title": "2.3 ANY 3D SCENARIOS", "content": "With our concise framework design, SAM2POINT exhibits superior zero-shot generalization perfor-mance across diverse domains, ranging from objects to scenes and indoor to outdoor environments. We elaborate on four distinct 3D scenarios below:\n\u2022 3D Object, e.g., Objaverse (Deitke et al., 2023), with a wide array of categories, possesses unique characteristics across different instances, including colors, shapes, and geometries. Adjacent components of an object might overlap, occlude, or integrate with each other, which requires models to accurately discern subtle differences for part segmentation.\n\u2022 Indoor Scene, e.g., S3DIS (Armeni et al., 2016) and ScanNet (Dai et al., 2017), are typically characterized by multiple objects arranged within confined spaces, like rooms. The complex spatial layouts, similarity in appearance, and varied orientations between objects pose challenges for models to segment them from backgrounds.\n\u2022 Outdoor Scene, e.g., Semantic3D (Hackel et al., 2017), differs from indoor scenes, primarily due to the stark size contrasts of objects (buildings, vehicles, and humans) and the larger scale of point clouds (from a room to an entire street). These variations complicates the segmentation of objects whether at a global scale or a fine-grained level.\n\u2022 Raw LiDAR, e.g., KITTI (Geiger et al., 2012) in autonomous driving, is distinct from typical point clouds for its sparse distribution and absence of RGB information. The sparsity demands models to infer missing semantics for understanding the scene, and the lack of colors enforces models to only rely on geometric cues to differentiate between objects. In SAM2POINT, we directly set the RGB values of 3D voxels by the LiDAR intensity."}, {"title": "3 DISCUSSION AND INSIGHT", "content": "Building on the effectiveness of SAM2POINT, we delve into two compelling yet challenging issues within the realm of 3D, and share our insights on future multi-modality learning."}, {"title": "3.1 HOW TO ADAPT 2D FOUNDATION MODELS TO 3D?", "content": "The availability of large-scale, high-quality data has significantly empowered the development of large models in language (Brown et al., 2020; Touvron et al., 2023; Zhang et al., 2023a), 2D vision (Liu et al., 2023b; Team et al., 2023; Chen et al., 2024), and vision-language (Gao et al., 2024; Liu et al., 2023a; Li et al., 2024; Zhang et al., 2024) domains. In contrast, the 3D field has long struggled with a scarcity of data, hindering the training of large 3D models. As a result, researchers have turned to the alternative of transferring pre-trained 2D models into 3D.\nThe primary challenge lies in bridging the modal gap between 2D and 3D. Pioneering approaches, such as PointCLIP (Zhang et al., 2022), its V2 (Zhu et al., 2022), and subsequent methods (Ji et al., 2023; Huang et al., 2023), project 3D data into multi-view images, which encounter implementation inefficiency and information loss. Another line of work, including ULIP series (Xue et al., 2022; 2023), I2P-MAE (Zhang et al., 2023c), and others (Liu et al., 2023c; Qi et al., 2023; Guo et al., 2023a), employs knowledge distillation using 2D-3D paired data. While this method generally performs better due to extensive training, it suffers from limited 3D transferability in out-of-domain scenarios. Recent efforts have also explored more complex and costly solutions, such as joint multi-modal spaces (e.g., Point-Bind & Point-LLM (Guo et al., 2023b)), larger-scale pre-training (Uni3D (Zhou et al., 2023a)), and virtual projection techniques (Any2Point (Tang et al., 2024)).\nFrom SAM2POINT, we observe that representing 3D data as videos through voxelization may offer an optimal solution, providing a balanced trade-off between performance and efficiency. This approach not only preserves the spatial geometries inherent in 3D space with a simple transformation, but also presents a grid-based data format that 2D models can directly process. Despite this, further experiments are necessary to validate and reinforce this observation."}, {"title": "3.2 WHAT IS THE POTENTIAL OF SAM2POINT IN 3D DOMAINS?", "content": "To the best of our knowledge, SAM2POINT presents the most accurate and comprehensive imple-mentation of SAM in 3D, successfully inheriting its implementation efficiency, promptable flexibility, and generalization capabilities. While previous SAM-based approaches (Yang et al., 2023b; Xu et al., 2023a; Yin et al., 2024) have achieved 3D segmentation, they often fall short in scalability and transferability to benefit other 3D tasks. In contrast, inspired by SAM in 2D domains, SAM2POINT demonstrates significant potential to advance various 3D applications.\nFor fundamental 3D understanding, SAM2POINT can serve as a unified initialized backbone for further fine-tuning, offering strong 3D representations simultaneously across 3D objects, indoor scenes, outdoor scenes, and raw LiDAR. In the context of training large 3D models, SAM2POINT can be employed as an automatic data annotation tool, which mitigates the data scarcity issue by generating large-scale segmentation labels across diverse scenarios. For 3D and language-vision learning, SAM2POINT inherently provides a joint embedding space across 2D, 3D, and video domains, due to its zero-shot capabilities, which could further enhance the effectiveness of models like Point-Bind (Guo et al., 2023b). Additionally, in the development of 3D large language models (LLMs) (Hong et al., 2023; Xu et al., 2023b; Wang et al., 2023; Guo et al., 2023b), SAM2POINT can function as a powerful 3D encoder, supplying LLMs with 3D tokens, and leveraging its promptable features to equip LLMs with promptable instruction-following capabilities."}, {"title": "4 DEMOS", "content": "In Figures 3-7, we showcase demonstrations of SAM2POINT in segmenting 3D data with various 3D prompt on different datasets (Deitke et al., 2023; Armeni et al., 2016; Dai et al., 2017; Hackel et al., 2017; Geiger et al., 2012). For further implementation details, please refer to our open-sourced code."}, {"title": "5 CONCLUSION", "content": "In this project, we propose SAM2POINT, which leverages Segment Anything 2 (SAM 2) to 3D segmentation with a zero-shot and promptable framework. By representing 3D data as multi-directional videos, SAM2POINT supports various types of user-provided prompt (3D point, box, and mask), and exhibits robust generalization across diverse 3D scenarios (3D object, indoor scene, outdoor environment, and raw sparse LiDAR). As a preliminary investigation, SAM2POINT provides unique insights into adapting SAM 2 for effective and efficient 3D understanding. We hope our method may serve as a foundational baseline for promptable 3D segmentation, encouraging further research to fully harness SAM 2's potential in 3D domains."}]}