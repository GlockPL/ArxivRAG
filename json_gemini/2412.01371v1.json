{"title": "An overview of diffusion models for generative artificial intelligence", "authors": ["Davide Gallon", "Arnulf Jentzen", "Philippe von Wurstemberger"], "abstract": "This article provides a mathematically rigorous introduction to denoising diffusion probabilistic models (DDPMs), sometimes also referred to as diffusion probabilistic models or diffusion models, for generative artificial intelligence. We provide a detailed basic mathematical framework for DDPMs and explain the main ideas behind training and generation procedures. In this overview article we also review selected extensions and improvements of the basic framework from the literature such as improved DDPMs, denoising diffusion implicit models, classifier-free diffusion guidance models, and latent diffusion models.", "sections": [{"title": "1 Introduction", "content": "The goal of generative modelling is to generate new data samples from an unknown underlying distribution based on a dataset of samples from that distribution. Many different machine learning approaches for this goal have been proposed, such as generative adversarial networks (GANs) [12], variational autoencoders (VAEs) [22], autoregressive models [47], normalizing flows [37], and energy-based models [25]. In this article, we provide an introduction to denoising diffusion probabilistic models (DDPMs), a class of generative methods (sometimes also called diffusion models or diffusion probabilistic models) which is based on the idea to reconstruct a diffusion process, which starts at the underlying distribution and gradually adds noise to its state until it arrives at a terminal state that is purely noise, backwards. Through this backward reconstruction, pure noise is transformed into meaningful data, and as such DDPMs provide a natural generative framework. We aim to provide a basic but rigorous understanding of the motivating ideas behind DDPMs and precise descriptions of some of the most influential DDPM-based methods in the literature.\nDDPMs were originally introduced in [44] and further popularized in [15] and have been able to achieve state of the art results in many domains like image synthesis and editing [31, 35, 36, 38, 40], video generation [17, 53], natural language processing [3, 26], and anomaly detection [50, 52]. In the canonical formulation, a DDPM is a framework consisting of two stochastic processes, a forward process and a backward process. The forward process - the diffusion process - starts at the initial time step at the (approximate) underlying distribution (for instance, its initial state could be a random sample from the dataset) and then gradually adds noise to its state so that its state at the terminal time step is (approximately) purely noise. The backward process the denoising process \u2013 is a parametric process which starts (at the terminal time step) at a purely noisy state. The idea in the context of DDPMs is to learn parameters for this backward process such that the distribution at each time step of the backward process is approximately the same as the distribution at the corresponding time step of the forward process. If this is achieved, the backward process can be interpreted to gradually remove noise from its initial state until it is at the initial distribution of the forward process. In that sense, the backward process gradually denoises its purely noisy initial state. Once appropriate parameters for the backward process have been found, the generative procedure consists in sampling realizations of the backwards process.\nWe rigorously set up a general mathematical framework for DDPMs and explain the ideas behind the training of the backward process and the creation of generative samples in Section 2. We then consider the most common special case of this framework when the noise is Gaussian and the backward process is governed by a denoising artificial neural network (ANN) in Section 3. In Section 4 we thereafter discuss some metrics from the literature on how to evaluate the quality of generated samples. We conclude in Section 5 with a discussion of some of the most popular DDPM-based methods that have been proposed in the literature such as Improved DDPMs (see [15]), denoising diffusion implicit models (DDIMs) (see [45]), classifier-free diffusion guidance models (see [16]), and latent diffusion models (see [38]). In particular, classifier-free diffusion guidance models and latent diffusion models show how to guide the backward process to generate data from different classes and based on a given text, respectively. Code supporting"}, {"title": "2 Denoising diffusion probabilistic models (DDPMs)", "content": "In this section we introduce the main ideas behind DDPMs. Specifically, we introduce and discuss a general mathematical framework for DDPMs and elaborate some of its elementary properties in Subsection 2.1, we discuss the training objective with which DDPMs aim to achieve the goal of generative modelling in Subsection 2.2, and we present a simplified DDPM methodology based on this training objective in Subsection 2.3."}, {"title": "2.1 General framework for DDPMs", "content": "Setting 2.1 (General framework for DDPMs). Let $d, \\tilde{d}, T \\in \\mathbb{N}$, let $(\\Omega, \\mathcal{F},\\mathbb{P})$ be a probability space, for every $\\theta \\in (\\mathbb{R}^{\\tilde{d}} \\cup \\{\\emptyset\\})$ let $X^{\\theta} = (X^{\\theta}_t)_{t \\in \\{0,1,\\dots,T\\}}: \\{0,1,\\dots,T\\} \\times \\Omega \\to \\mathbb{R}^d$ be a stochastic process, assume that $(X^{\\theta})_0 \\in \\mathbb{R}_0$ and $X^{\\emptyset}$ are independent, for every $\\theta \\in (\\mathbb{R}^{\\tilde{d}} \\cup \\{\\emptyset\\})$ let $p^{\\theta} : (\\mathbb{R}^d)^{T+1} \\to (0,\\infty)$ be a measurable function which satisfies\u00b9 for all $B_0, B_1, \\dots, B_T \\in \\mathcal{B}(\\mathbb{R}^d)$ that\n$\\mathbb{P}(X^{\\theta}_0 \\in B_0, X^{\\theta}_1 \\in B_1, \\dots, X^{\\theta}_T \\in B_T) = \\int_{B_0} \\int_{B_1} \\dots \\int_{B_T} p^{\\theta} (x_0,x_1,\\dots,x_T) \\, dx_0 \\, dx_1 \\dots \\, dx_T, $\nfor every $\\theta \\in (\\mathbb{R}^{\\tilde{d}} \\cup \\{\\emptyset\\})$, $S \\in \\{1, \\dots, T\\}$, $a_1, \\dots, a_{T+1} \\in \\mathbb{N}_0$ with $\\{a_1,\\dots,a_{T+1}\\} = \\{0,1,\\dots,T\\}$ let $p^{\\theta}_{a_1,\\dots,a_S}: (\\mathbb{R}^d)^S \\to (0, \\infty)$ satisfy for all $x_{a_1},\\dots,x_{a_S} \\in \\mathbb{R}^d$ that\n$p^{\\theta}_{a_1,\\dots,a_S} (x_{a_1},\\dots,x_{a_S})\\begin{cases}\n\\int_{\\mathbb{R}^d} \\dots \\int_{\\mathbb{R}^d} p^{\\theta} (x_0,x_1,\\dots,x_T) \\, dx_{a_{S+1}} \\, dx_{a_{S+2}} \\dots \\, dx_{a_{T+1}} &: S < T \\\\\np^{\\theta} (x_0,x_1,\\dots,x_T) &: S = T + 1,\n\\end{cases}$\nfor every $\\theta \\in (\\mathbb{R}^{\\tilde{d}} \\cup \\{\\emptyset\\})$, $S, K \\in \\{1, \\dots, T\\}$, $a_1, \\dots, a_{S+K} \\in \\{0,1,\\dots,T\\}$ with $\\|\\{a_1,\\dots,a_{S+K}\\}| = S + K$ let $p^{\\theta}_{a_1,\\dots,a_S|a_{S+1},\\dots,a_{S+K}} = (p^{\\theta}_{a_1,\\dots,a_S|a_{S+1},\\dots,a_{S+K}} : (\\mathbb{R}^d)^S \\times (\\mathbb{R}^d)^K \\to (0,\\infty))$ satisfy for all $x_{a_1},\\dots,x_{a_{S+K}} \\in \\mathbb{R}^d$ that\n$p^{\\theta}_{a_1,\\dots,a_S|a_{S+1},\\dots,a_{S+K}} (x_{a_1},\\dots,x_{a_S}| x_{a_{S+1}},\\dots,x_{a_{S+K}}) = \\frac{p^{\\theta}_{a_1,\\dots,a_{S+K}} (x_{a_1},\\dots,x_{a_{S+K}})}{p^{\\theta}_{a_{S+1},\\dots,a_{S+K}} (x_{a_{S+1}},\\dots,x_{a_{S+K}})}$,\nlet $\\Pi: \\mathbb{R}^d \\to (0,\\infty)$ be a function, and assume for all $\\theta \\in \\mathbb{R}^{\\tilde{d}}$ that $p^{\\theta}_{T} = \\Pi$.\nRemark 2.2 (Explanations for Setting 2.1). In this remark we provide some intuitive interpretations for the mathematical objects appearing in Setting 2.1 and roughly explain their role in the context of DDPMs for generative modelling. Roughly speaking, we note that"}, {"title": "2.2 Training objective in DDPMs", "content": "In this section we discuss the objective used to train the parameters of the backward process in Setting 2.1. As discussed in Remark 2.2, the goal in the context of DDPMs is to find parameters for the backward process such that the terminal value of the backward process is approximately distributed like the initial value of the forward process (cf. (4) in Remark 2.2). To achieve this, [44] propose to minimize the expected negative log-likelihood (ENLL) (sometimes called cross-entropy in the context of information theory) of the PDF of the initial value of the forward process with respect to the PDF of the terminal value of the backward process (see [11, Section 5.5] for an introduction to minimizing the ENLL in the context of machine learning). Roughly speaking, this ENLL measures how similar the distribution of the terminal value of the backward process is to the distribution of the initial value of the forward process.\nWe start this section by introducing the concept of the ENLL in Definition 2.6 and the related concept of the Kullback-Leibler (KL) divergence (see [24]) in Definition 2.7. We then justify the choice of the ENLL as a training objective in Lemma 2.8. Thereafter, in Lemma 2.9 and Remark 2.10 we discuss an upper bound for the ENLL in the context of Setting 2.3 which can be used as an alternative training objective for the parameters of the backward process.\nDefinition 2.6 (ENLL). Let $d \\in \\mathbb{N}$ and for every $i \\in \\{1,2\\}$ let $p_i : \\mathbb{R}^d \\to (0, \\infty)$ be a measurable function which satisfies $\\int_{\\mathbb{R}^d} p_i(x) \\, dx = 1$. Then we denote by $H(p_1||p_2) \\in \\mathbb{R}\\cup \\{\\infty\\}$ the number given by\n$H(p_1||p_2) = \\int_{\\mathbb{R}^d} -\\ln(p_2(x)) \\, p_1(x) \\, dx$\nand we call $H(p_1||p_2)$ the ENLL of $p_2$ with respect to $p_1$ (we call $H(p_1||p_2)$ the cross-entropy from $p_1$ to $p_2$).\nDefinition 2.7 (KL divergence). Let $d \\in \\mathbb{N}$ and for every $i \\in \\{1,2\\}$ let $p_i: \\mathbb{R}^d \\to (0,\\infty)$ be a measurable function which satisfies $\\int_{\\mathbb{R}^d} p_i(x) \\, dx = 1$. Then we denote by $D_{\\text{KL}}(p_1||p_2) \\in \\mathbb{R}\\cup \\{-\\infty,\\infty\\}$ the extended real number given by\n$D_{\\text{KL}}(p_1||p_2) = \\int_{\\mathbb{R}^d} \\ln(\\frac{p_1(x)}{p_2(x)}) \\, p_1(x) \\, dx$\nand we call $D_{\\text{KL}}(p_1||p_2)$ the KL divergence of $p_1$ from $p_2$.\nLemma 2.8 (Properties of the ENLL and the KL divergence). Let $d \\in \\mathbb{N}$, for every $i \\in \\{1,2\\}$ let $p_i: \\mathbb{R}^d \\to (0,\\infty)$ be a measurable function which satisfies $\\int_{\\mathbb{R}^d} p_i(x) \\, dx = 1$, let $(\\Omega, \\mathcal{F},\\mathbb{P})$ be a probability space, and let $X : \\Omega \\to \\mathbb{R}^d$ satisfy for all $B \\in \\mathcal{B}(\\mathbb{R}^d)$ that $\\mathbb{P}(X \\in B) = \\int_{B} p_1(x) \\, dx$. Then\n(i) it holds that $H(p_1||p_2) = \\mathbb{E}[-\\ln(p_2(X))]$,\n(ii) it holds that $D_{\\text{KL}}(p_1||p_2) = \\mathbb{E} [\\ln(\\frac{p_1(X)}{p_2(X)})]$,\n(iii) it holds that $H(p_1||p_2) - H(p_1||p_1) = D_{\\text{KL}}(p_1||p_2) \\geq 0$, and\n(iv) it holds that the following three statements are equivalent:"}, {"title": "2.3 A first simplified DDPM generative method", "content": "In this section we discuss in Method 2.11 and Remark 2.12 a DDPM methodology which makes use of the upper bound in Lemma 2.9 to minimize the ENLL of the PDF of the initial value of the forward process with respect to the PDF of the terminal value of the backward process in Setting 2.3. Method 2.11 can be regarded as a simplified version of the DDPM methodologies proposed in [15, 44].\nMethod 2.11 (A simplified DDPM generative method). Assume Setting 2.3, assume $T > 1$, let $M \\in \\mathbb{N}$, $\\gamma \\in (0,\\infty)$, let $L: \\mathbb{R}^{\\tilde{d}} \\times \\{1,\\dots,T\\} \\times \\mathbb{R}^d \\times \\mathbb{R}^d \\times \\dots \\times \\mathbb{R}^d \\to \\mathbb{R}$ satisfy for all $\\theta \\in \\mathbb{R}^{\\tilde{d}}$,"}, {"title": "3 DDPMs with Gaussian noise", "content": "In this section we consider DDPMs with Markov assumptions when the transition kernels are given by Gaussian distributions. The setup and methodology considered in this section essentially correspond to the one proposed in [15]. Intuitively speaking, in this setup we think that the forward process gradually adds Gaussian noise to a training sample which the backward process then aims to gradually remove to recover the original training sample.\nWe first discuss some elementary properties of Gaussian distributions in Subsection 3.1. We then motivate and describe a DDPM framework involving such Gaussian distributions as transition kernels in Subsection 3.2. Thereafter, we discuss some consequences of this choice of transition kernels on distributions of the forward process in Subsection 3.3 and on the upper bound for the training objective from Lemma 2.9 above in Subsection 3.4. Motivated by the previous sections we then describe a training and generation scheme for DDPMs with Gaussian noise in Subsection 3.5. Finally, in Subsection 3.6 we point to some possible choices of architectures for the ANNs appearing in the method description in Subsection 3.5."}, {"title": "3.1 Properties of Gaussian distributions", "content": "In this section we recall some elementary and well-known properties of Gaussian distributions which will be used in the definition of transition kernels throughout Section 3. We start by recalling the definition of PDFs of Gaussian distributions.\nDefinition 3.1 (Gaussian PDFs). Let $d \\in \\mathbb{N}$ and let $\\mathcal{S} = \\{Q \\in \\mathbb{R}^{d\\times d}: Q^* = Q \\text{ and } (\\forall v \\in \\mathbb{R}^d\\backslash\\{0\\}: v^* Q v > 0)\\}$. Then we denote by $N: \\mathbb{R}^d \\times \\mathbb{R}^d \\times \\mathcal{S} \\to \\mathbb{R}$ the function which satisfies for all $x, v \\in \\mathbb{R}^d$, $Q \\in \\mathcal{S}$ that\n$N(x, v, Q) = (2\\pi)^{-\\frac{d}{2}} \\text{det}(Q)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2} (x - v)^* Q^{-1} (x - v))$\nand for every $v \\in \\mathbb{R}^d$, $Q \\in \\mathcal{S}$ we call $N(\\cdot, v, Q): \\mathbb{R}^d \\to \\mathbb{R}$ the PDF of the Gaussian distribution with mean $v$ and covariance matrix $Q$."}, {"title": "3.1.1 On Gaussian transition kernels", "content": "The next two results illustrate how distributions propagate in Markov chains with transition kernels involving Gaussian distributions. We first present a result on the level of PDFs in Lemma 3.2 and then state the consequence on the level of random variables in Corollary 3.3.\nLemma 3.2. Let $d \\in \\mathbb{N}$, let $\\mathcal{S} = \\{Q \\in \\mathbb{R}^{d\\times d}: Q^* = Q \\text{ and } (\\forall v \\in \\mathbb{R}^d\\backslash\\{0\\}: v^* Q v > 0)\\}$, and let $\\mu_1, \\mu_2 \\in \\mathbb{R}^d$, $A\\in \\mathbb{R}^{d\\times d}$, $\\Sigma_1, \\Sigma_2 \\in \\mathcal{S}$. Then it holds for all $x \\in \\mathbb{R}^d$ that\n$\\int_{\\mathbb{R}^d} N(x, Ay + \\mu_1, \\Sigma_1) N(y, \\mu_2, \\Sigma_2) \\, dy = N(x, A\\mu_2 + \\mu_1, A\\Sigma_2 A^* + \\Sigma_1)$\nCorollary 3.3. Let $d \\in \\mathbb{N}$, let $\\mathcal{S} = \\{Q \\in \\mathbb{R}^{d\\times d}: Q^* = Q \\text{ and } (\\forall v \\in \\mathbb{R}^d\\backslash\\{0\\}: v^* Q v > 0)\\}$, let $\\mu_1,\\mu_2 \\in \\mathbb{R}^d$, $A \\in \\mathbb{R}^{d\\times d}$, $\\Sigma_1, \\Sigma_2 \\in \\mathcal{S}$, let $(\\Omega, \\mathcal{F},\\mathbb{P})$ be a probability space, let $X: \\Omega \\to \\mathbb{R}^d$ and $Y: \\Omega \\to \\mathbb{R}^d$ be random variables, and assume for all $B \\in \\mathcal{B}(\\mathbb{R}^d)$ that\n$\\mathbb{P}(Y \\in B) = \\int_{B} N(y, \\mu_2, \\Sigma_2) \\, dy \\quad \\text{ and }\\quad \\mathbb{P}(X \\in B|Y) \\stackrel{\\mathbb{P}-a.s.}{=} \\int_{B} N(x, AY + \\mu_1, \\Sigma_1) \\, dx$\nThen it holds for all $B \\in \\mathcal{B}(\\mathbb{R}^d)$ that\n$\\mathbb{P}(X \\in B) = \\int_{B} N(x, A\\mu_2 + \\mu_1, A\\Sigma_2 A^* + \\Sigma_1) \\, dx.$"}, {"title": "3.1.2 Explicit constructions for Gaussian transition kernels", "content": "The result below shows an explicit way to simulate a step in a Markov chain with Gaussian transition kernels based on realizations of standard normal random variables.\nLemma 3.4. Let $d \\in \\mathbb{N}$, let $\\mathcal{S} = \\{Q \\in \\mathbb{R}^{d\\times d}: Q^* = Q \\text{ and } (\\forall v \\in \\mathbb{R}^d\\backslash\\{0\\}: v^* Q v > 0)\\}$, let $\\mu: \\mathbb{R}^d \\to \\mathbb{R}^d$ and $\\Sigma: \\mathbb{R}^d \\to \\mathcal{S}$ be functions, let $(\\Omega, \\mathcal{F},\\mathbb{P})$ be a probability space, let $X : \\Omega \\to \\mathbb{R}^d$, $Y: \\Omega \\to \\mathbb{R}^d$, and $Z: \\Omega \\to \\mathbb{R}^d$ be random variables, and assume for all $B \\in \\mathcal{B}(\\mathbb{R}^d)$ that\n$\\mathbb{P}(X \\in B|Y) \\stackrel{\\mathbb{P}-a.s.}{=} \\int_{B} N(x, \\mu(Y), \\Sigma(Y)) \\, dx \\quad \\text{ and }\\quad X = \\mu(Y) + (\\Sigma(Y))^{1/2} Z$\nThen\n(i) it holds for all $B\\in \\mathcal{B}(\\mathbb{R}^d)$ that $\\mathbb{P}(Z \\in B) = \\int_{B} N(x, 0, \\text{I}) \\, dx$ and\n(ii) it holds that $Z$ and $Y$ are independent."}, {"title": "3.1.3 Bayes rule for Gaussian distributions", "content": "The next two results illustrate an explicit form of the Bayes rule for Gaussian distributions. We first present a result on the level of PDFs in Lemma 3.5 and then state the consequence on the level of random variables in Corollary 3.6.\nLemma 3.5. Let $d \\in \\mathbb{N}$, let $\\mathcal{S} = \\{Q \\in \\mathbb{R}^{d\\times d}: Q^* = Q \\text{ and } (\\forall v \\in \\mathbb{R}^d\\backslash\\{0\\}: v^* Q v > 0)\\}$, let $\\mu_1, \\mu_2 \\in \\mathbb{R}^d$, $A\\in \\mathbb{R}^{d\\times d}$, $\\Sigma_1, \\Sigma_2 \\in \\mathcal{S}$, and let $\\Sigma_3 \\in \\mathbb{R}^{d\\times d}$ satisfy $\\Sigma_3 = \\Sigma_2 A^* (A\\Sigma_2 A^* + \\Sigma_1)^{-1}$. Then it holds for all $x, y \\in \\mathbb{R}^d$ that\n$\\frac{N(x, Ay + \\mu_1, \\Sigma_1) N(y, \\mu_2, \\Sigma_2)}{N(x, A\\mu_2 + \\mu_1, A\\Sigma_2 A^* + \\Sigma_1)} = N(y, \\Sigma_3 (x - A\\mu_2 - \\mu_1) + \\mu_2, \\Sigma_2 - \\Sigma_3 A \\Sigma_2)$"}, {"title": "3.1.4 KL divergence between Gaussian distributions", "content": "In the next result we recall a formula for the KL divergence between two PDFs of Gaussian distributions.\nLemma 3.7 (KL divergence between Gaussian distributions). Let $d \\in \\mathbb{N}$, let $\\mathcal{S} = \\{Q \\in \\mathbb{R}^{d\\times d}: Q^* = Q \\text{ and } (\\forall v \\in \\mathbb{R}^d\\backslash\\{0\\}: v^* Q v > 0)\\}$, and let $\\mu_1, \\mu_2 \\in \\mathbb{R}^d$, $\\Sigma_1, \\Sigma_2 \\in \\mathcal{S}$. Then\n$D_{\\text{KL}}(N(\\cdot, \\mu_1, \\Sigma_1)||N(\\cdot, \\mu_2, \\Sigma_2)) = \\frac{1}{2} \\Big( \\ln(\\frac{\\text{det} \\Sigma_2}{\\text{det} \\Sigma_1}) - d + \\text{tr}(\\Sigma_2^{-1} \\Sigma_1) + (\\mu_2 - \\mu_1)^* \\Sigma_2^{-1} (\\mu_2 - \\mu_1) \\Big)$"}, {"title": "3.2 Framework for DDPMs with Gaussian noise", "content": "In this section we present in Setting 3.8 a framework for DDPMs with Markov assumptions when the transition kernels are given by Gaussian distributions. In Lemma 3.9 we then show a constructive way to sample the forward and backward processes in this setting using standard normal random variables.\nSetting 3.8 (DDPMs with Gaussian transition kernels). Assume Setting 2.3, let $\\mathcal{S} = \\{Q \\in \\mathbb{R}^{d\\times d}: Q^* = Q \\text{ and } (\\forall v \\in \\mathbb{R}^d\\backslash\\{0\\}: v^* Q v > 0)\\}$, let $\\alpha_1,\\dots,\\alpha_T \\in [0,1)$, for every $\\theta \\in \\mathbb{R}^{\\tilde{d}}$ let $\\mu^{\\theta} = (\\mu^{\\theta}_t)_{t \\in \\{1,\\dots,T\\}}: \\mathbb{R}^d \\times \\{1,\\dots,T\\} \\to \\mathbb{R}^d$ and $\\Sigma^{\\theta} = (\\Sigma^{\\theta}_t)_{t \\in \\{1,\\dots,T\\}}: \\mathbb{R}^d \\times \\{1,\\dots,T\\} \\to \\mathcal{S}$ be measurable functions, and assume for all $t \\in \\{1, \\dots,T\\}$, $x_{t-1}, x_t \\in \\mathbb{R}^d$ that\n$p^{\\theta}_{t|t-1}(x_t|x_{t-1}) = N(x_t, \\sqrt{\\alpha_t} x_{t-1}, (1 - \\alpha_t) \\text{I}),$\n$\\Pi = N(\\cdot, 0, \\text{I}),$\nand\n$p^{\\theta}_{t-1|t}(x_{t-1}|x_{t}) = N(x_{t-1}, \\mu^{\\theta}_t(x_t), \\Sigma^{\\theta}_t(x_t))$"}, {"title": "3.3 Distributions of the forward process in DDPMs with Gaussian noise", "content": "In this section we discuss some consequences of the choice of transition densities in Setting 3.8 on PDFs of the forward process."}, {"title": "3.3.1 Conditional distributions going forward", "content": "In Lemma 3.11 below we show that in Setting 3.8 the conditional distribution of any time step of the forward process given the initial value of the forward process is again given by a Gaussian distribution. As a consequence of Lemma 3.11, we obtain in Corollary 3.12 that to sample a realization of an arbitrary step of the forward process it suffices to sample a random variable from the initial distribution and a further independent standard normal random variable.\nLemma 3.11 (Multi-step transition density of the forward process). Assume Setting 3.8 and let $\\tilde{\\alpha}_1, \\dots, \\tilde{\\alpha}_T \\in [0,1)$ satisfy for all $t \\in \\{1,\\dots,T\\}$ that $\\tilde{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$. Then it holds for all $t \\in \\{1, \\dots, T\\}$, $x_0, x_t \\in \\mathbb{R}^d$ that\n$p^{\\theta}_{t|0}(x_t|x_0) = N(x_t, \\sqrt{\\tilde{\\alpha}_t} x_0, (1 - \\tilde{\\alpha}_t) \\text{I}).$"}, {"title": "3.3.2 Terminal distributions", "content": "In this section we illustrate a consequence of Lemma 3.11 on the distribution of the terminal value of the forward process. We first prove in Lemma 3.13 an auxiliary result which then allows us to explain in Remark 3.14 that the terminal distribution of the forward process tends towards a standard normal distribution when, roughly speaking, we add enough Gaussian noise throughout the forward process."}, {"title": "3.3.3 Conditional distributions going backwards", "content": "In this section we show that the conditional distribution of any time step of the forward process given the next value of the forward process and the initial value of the forward process is again given by a certain Gaussian distribution. The considered conditional distributions are precisely the ones appearing in the upper bound in Lemma 2.9."}, {"title": "3.4 Reformulated training objective in DDPMs with Gaussian noise", "content": "The goal in this section is to choose suitable functions $(\\mu^{\\theta})_{\\theta \\in \\mathbb{R}^{\\tilde{d}}}$ and $(\\Sigma^{\\theta})_{\\theta \\in \\mathbb{R}^{\\tilde{d}}}$ in Setting 3.8 such that the upper bound for the training objective in Lemma 2.9 admits a convenient expression"}, {"title": "3.5 DDPM generative method with Gaussian noise", "content": "We now formulate a generative method for DDPMs with Gaussian noise which is based on the upper bound for the training objective in Proposition 3.19. This scheme was proposed in [15].\nMethod 3.21 (DDPM generative method with Gaussian noise). Let $d, \\tilde{d}, M\\in \\mathbb{N}$, $T\\in\\mathbb{N}\\{1\\}$, $\\gamma\\in (0,\\infty)$, $\\alpha_1, ..., \\alpha_T \\in (0,1)$, $\\tilde{\\alpha}_0, \\tilde{\\alpha}_1, ..., \\tilde{\\alpha}_T, \\beta_1, ..., \\beta_T \\in [0, 1]$, assume for all $t \\in \\{0,1, ..., T\\}$ that $\\tilde{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$, for every $\\theta \\in \\mathbb{R}^{\\tilde{d}}$ let $V^\\theta: \\mathbb{R}^d \\times \\{1, ..., T\\} \\to \\mathbb{R}^d$ be a function, let $L: \\mathbb{R}^{\\tilde{d}} \\times \\mathbb{R}^d \\times \\mathbb{R}^d \\times \\{1, ..., T\\} \\to \\mathbb{R}$ satisfy for all $\\theta \\in \\mathbb{R}^{\\tilde{d}}, x,\\varepsilon\\in \\mathbb{R}^d, t \\in \\{1, ..., T\\}$ that\n$L(\\theta,x, \\varepsilon,t) = ||\\varepsilon - V^\\theta(\\sqrt{\\tilde{\\alpha}_t}x + \\sqrt{1 - \\tilde{\\alpha}_t}\\varepsilon, t)||^2,$\nlet $G: \\mathbb{R}^{\\tilde{d}} \\times \\mathbb{R}^d \\times \\mathbb{R}^d \\times \\{1, ..., T\\} \\to \\mathbb{R}^{\\tilde{d}}$ satisfy for all $x,\\varepsilon \\in \\mathbb{R}^d, t \\in \\{1, ..., T\\}, \\theta \\in \\mathbb{R}^{\\tilde{d}}$ with $L(\\cdot,x,\\varepsilon,t)$ differentiable at $\\theta$ that\n$G(\\theta,x, \\varepsilon,t) = (\\nabla_{\\theta}L)(\\theta,x,\\varepsilon,t),$\nlet $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space, let $X_{n,i}: \\Omega \\to \\mathbb{R}^d, n, i \\in \\mathbb{N}$, be random variables, let $\\varepsilon_{n,i}: \\Omega \\to \\mathbb{R}^d, n, i \\in \\mathbb{N}$, be i.i.d. standard normal random variables, let $T_n: \\Omega \\to \\{1,2,...,T\\}, n \\in \\mathbb{N}$, be"}, {"title": "3.6 Network architectures for the backward process", "content": "In this section we discuss the most popular choice for the architecture of the ANN $(V^{\\theta})_{\\theta \\in \\mathbb{R}^{\\tilde{d}}}$ from Method 3.21. Specifically, we explain UNets in Subsection 3.6.1 and present how the temporal component is commonly incorporated in Subsection 3.6.2. For general introductions to ANN architectures we refer, for instance, to [4, Section 9], [7, Section 5], [19, Section 1], and [43, Section 20]."}, {"title": "3.6.1 UNets", "content": "In the following we introduce the most common architecture used in diffusion models, the UNet architecture [29]. UNets have gained popularity in the field of computer vision, particularly for their effectiveness in semantic segmentation tasks but it has also been applied in various other domains, see, for example, [8, 31, 38, 40, 44]. Roughly speaking, UNets have an encoder-decoder structure made up of blocks. We now provide some comments on major components and aspects of UNets. See Figure 3.2 for a graphical illustration of its architecture."}, {"title": "3.6.2 Time embedding", "content": "We now aim to describe how the temporal component is commonly incorporated in UNets. The time step is a fundamental input since the model parameters are shared across time. Passing a structured temporal signal"}]}