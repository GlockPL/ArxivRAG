{"title": "ESPERANTO: EVALUATING SYNTHESIZED PHRASES TO ENHANCE ROBUSTNESS IN AI DETECTION FOR TEXT ORIGINATION", "authors": ["Navid Ayoobi", "Lily Knab", "Wen Cheng", "David Pantoja", "Hamidreza Alikhani", "Sylvain Flamant", "Jin Kim", "Arjun Mukherjee"], "abstract": "While large language models (LLMs) exhibit significant utility across various domains, they simultaneously are susceptible to exploitation for unethical purposes, including academic misconduct and dissemination of misinformation. Consequently, AI-generated text detection systems have emerged as a countermeasure. However, these detection mechanisms demonstrate vulnerability to evasion techniques and lack robustness against textual manipulations. This paper introduces back-translation as a novel technique for evading detection, underscoring the need to enhance the robustness of current detection systems. The proposed method involves translating AI-generated text through multiple languages before back-translating to English. We present a model that combines these back-translated texts to produce a manipulated version of the original AI-generated text. Our findings demonstrate that the manipulated text retains the original semantics while significantly reducing the true positive rate (TPR) of existing detection methods. We evaluate this technique on nine AI detectors, including six open-source and three proprietary systems, revealing their susceptibility to back-translation manipulation. In response to the identified shortcomings of existing AI text detectors, we present a countermeasure to improve the robustness against this form of manipulation. Our results indicate that the TPR of the proposed method declines by only 1.85% after back-translation manipulation. Furthermore, we build a large dataset of 720k texts using eight different LLMs. Our dataset contains both human-authored and LLM-generated texts in various domains and writing styles to assess the performance of our method and existing detectors. This dataset is publicly shared for the benefit of the research community.", "sections": [{"title": "1 Introduction", "content": "Through the training on a substantial volume of textual data, large language models (LLMs) encapsulate knowledge across various fields, incorporate a range of writing styles, and maintain contextual comprehension within their parameters. This has rendered them ubiquitous and state-of-the-art in numerous applications like translation [1], summarization [2], text classification [3], chat bots and virtual assistants [4]. With their ability to be prompted effortlessly at minimal cost and their proficiency in generating high-quality and human-like text, LLMs are becoming an attractive tool for malicious users. Malicious activities include, but are not limited to, academic dishonesty [5], the production of fake news [6], scam messages [7], fraudulent reviews [8], and automated cyberbullying [9]. Beyond deliberate misuse, there are instances where LLMs inadvertently generate outdated information [10], such as within a question-answering framework, due to training on obsolete data. LLMs are also prone to producing hallucinations [11, 12], which are convincingly realistic yet factually incorrect or nonsensical information. In addition, given the pervasive presence of AI-generated content, it is occasionally essential to distinguish and filter out human-generated"}, {"title": "2 Related work", "content": "The identification of machine-generated text has been an active field of study preceding the unveiling of LLMs [49, 50]. The emergence of LLMs has heightened the urgency and priority of devising effective techniques for the identification of synthetic content. Broadly speaking, AI-text detection techniques can be classified into four categories: statistical [13, 51, 52], information retrieval [32, 53, 54], zero-shot [22, 55], and watermarking [28, 29, 30, 31, 56] methods. Statistical methods involve analyzing the distribution of linguistic patterns in a text to extract statistical features, which are subsequently used to determine whether the text is human-written or AI-generated. Building on the fact that most language models tend to sample from the head of the distribution, Gehrmann et al. [13]. introduce a statistical approach that incorporates three tests: the probability of the word, the absolute rank of a word, and the entropy of the predicted distribution. These tests enable them to quantify the likelihood that a generated word is derived from the top of the distribution and to evaluate whether the previously generated context is recognized by the detection system. The research conducted by Crothers et al. [57] demonstrates that despite the fact that neural network features outperform statistical features, the integration of statistical features can enhance the robustness against particular adversarial attacks. By leveraging information retrieval principles, Krishna et al. [32] suggest a defense against paraphrase attacks through the retrieval of earlier-created AI-text. Their approach involves storing all LLM-generated texts in a database and then searching the entire database for a text that approximately matches the content of the input query. However, retrieval-based detection methods require maintaining a substantial database of LLM-generated texts, and querying this database to find matches can be excessively time-consuming.\nIn an alternative approach to detecting AI-generated text, researchers have made attempts to utilize LLMs to compel them to identify the content that they have generated themselves in a zero-shot manner. Based on the assumption that the ChatGPT [42] model make fewer modifications to LLM-generated texts compared to human-written texts, Zhu et al. [22] develop a zero-shot and black-box detection method. This approach generates revised versions of a text using ChatGPT and measures the similarity between the original text and its revised version. They use the criterion that a higher similarity score suggests a higher probability of the text being LLM-generated to assess whether a text is AI-generated. In another research effort, Bhattacharjee and Liu [55] assess the zero-shot performance of ChatGPT by providing it with a simple prompt along with the text to be classified in the task of distinguishing between human-written and AI-generated text. They test this approach on samples from 19 models, ranging from an 82M-parameter model to a 1.6B-parameter model, as found in the TuringBench dataset [16]. Their findings indicate that although ChatGPT has difficulty identifying AI-generated text, it performs effectively on human-written text.\nTo investigate the reliability of existing AI-text detectors, numerous studies have been dedicated to designing prompts that may allow LLMs to generate texts capable of evading detection. In one such work, Kumarage et al. [58] present a framework named \u201cESCaPe\", which directs pre-trained language models (PLMs) to circumvent AI-generated-text detectors using a universal evasive prompt. The EScaPe framework involves initially crafting a specific evasive prompt for a particular PLM through prompt tuning and then capitalizes on the transferability of soft prompts to transfer the evasive prompt from one PLM to another. In a related study, Lu et al. [59] propose \"SICO\", an in-context learning approach that iteratively replaces words and sentences within the in-context examples to assist LLMs in generating text that can evade detection. The substitution procedure is directed by a proxy detector. The authors demonstrate that, in addition to reducing the effectiveness of existing AI text detectors, SICO decreases the likelihood of being recognized by humans. Kirchenbauer et al. [28] present a watermarking strategy designed to make synthetic text detectable even in short token spans. This method operates by generating a pseudo-random \u201cred\u201d list of tokens for each position in the sentence, where the \"red\" list generator is seeded with the prior token of that position only. A third party with access to the random number generator can recreate the red list for each token and count how often the red list rule is violated. However, studies like [26, 32] indicate that watermarking is vulnerable to text manipulations such as paraphrasing. As an example, Cai and Cui [60] reveal that a minor alteration, such as inserting a single space character before a random comma in AI-generated text, can deceive a detector. To achieve robustness against paraphrasing, Hu et al. [26], propose RADAR, which employs adversarial training to concurrently train a paraphraser and a detector in a two-player game"}, {"title": "3 Dataset", "content": "For the purposes of this research, a large dataset was compiled, encompassing 72k instances of human-written texts and their corresponding AI-generated versions. Additionally, a further 720k instances were generated from both human and AI-produced content via the technique of back translation, which will be detailed later in this section. To ensure a diverse range of writing styles, our dataset includes four distinct text categories: news articles to represent journalistic style, paper abstracts to exemplify scientific style, Amazon product reviews to represent the informative review style, and responses to questions posted online to reflect the everyday writing style prevalent on the internet.\nNews articles: For the news articles, we utilized 3000 samples collected by Ayoobi et al. [19]. We selected those generated articles produced by their \"Summary Expanding\" strategy and the Mistral-7b model [40]. These articles were originally sourced from reputable news agencies and subsequently converted into AI-generated counterparts following a process of summarization and expansion. Detailed information about this pipeline is available in [19].\nPaper abstracts: A subset of 3000 scientific paper abstracts was sampled from the two million arXiv abstracts dataset introduced in [61]. The Llama3-8b model [41] was employed to identify the 10 most significant keywords and key phrases by providing the following prompt and a corresponding paper abstract: \u201cYou are a knowledgeable editor of a scientific journal trained to extract only 10 most important key words or phrases of a paper's abstract\u201d. Additionally, we tasked the Llama3-8b model with summarizing each abstract into a single sentence by providing it with the prompt: \u201cYou are a knowledgeable editor of a scientific journal trained to summarize a paper's abstract in only one sentence with less than 30 words\u201d. After extracting the keywords and one-sentence summaries of the abstracts, we employed the Llama3-70b model [41] to generate the AI counterpart for each abstract. The model was guided by the prompt: \"You are a knowledgeable scientific author trained to write a paper abstract containing [N] words given a list of key words and one-sentence summary of desired abstract\". we substituted [N] with the original human-authored abstract's total word count to ensure length consistency.\nReddit QA: For the question and long-answer data, we collected questions from the \"Explain Like I'm Five (ELI5)\" forum on Reddit, following a methodology similar to [62]. Initially, we filtered the questions to include only those with at least one answer exceeding 300 words. Subsequently, 25k questions were randomly sampled from the filtered questions. Five distinct LLMs, namely GPT 3.5 Turbo [42], Llama3-8b, Phi3-Medium [43], Mistral-7b, and Yi-34b [44], were employed to generate AI answers in three different proficiency levels: simplified, expert, and without any specific condition. The respective prompts used for generating answers were: \"Answer my question like I am five years old in about 300 words\u201d, \u201cAnswer my question like an expert in about 300 words\u201d, and \u201cAnswer my question in about 300 words\".\nProduct reviews: To include shorter AI texts, we randomly selected 5000 product reviews, each containing 40 to 50 words, from five Amazon product categories as described in [63]. These categories included office products, fashion, pet supplies, health and personal care, and toys and games. To generate AI counterparts, we first utilized the Llama3.1-8b model [45] to extract three keywords from each review, prompted with: \"You are an expert product reviewer trained to extract three most important keywords or phrases from a product review I give you\". Subsequently, from these 5000 reviews, we generated 2500 AI counterparts using the Llama3.1-8b model and another 2500 using the GPT4o-mini model. The prompt used was: \u201cYou are an Amazon customer. You write a review about a product I give you in about [N] words. You must also use the keywords I give you in writing your review. The product is [PRODUCT] and the keywords are [KEYWORDS]\". In this prompt, [N] was substituted with the total word count of the original human-authored review, [PRODUCT] with the product title, and [KEYWORDS] with the previously extracted keywords.\nFor both human-authored and AI-generated instances, we translated the text to an intermediate language and then back-translated it to English using Google Translate. We selected 10 different languages: Portuguese (PT), Spanish (ES), French (FR), Italian (IT), Chinese (ZH), Dutch (NL), Danish (DA), Japanese (JA), German (DE), and Korean (KO). To maintain consistency, all texts (except reviews) were truncated to approximately 300 words. We observed that truncating mid-sentence decreases the detectability of AI-generated text. Therefore, to maintain fairness in our"}, {"title": "4 Methodology", "content": "In this section, we initially outline our proposed method of text manipulation through back-translation to evade detection. We demonstrate that the manipulated text retains a high level of semantic similarity to the original AI-generated text using two distinct similarity metrics. Furthermore, we detail a countermeasure to mitigate the impact of this manipulation on the robustness of a detection system."}, {"title": "4.1 Manipulation of AI-generated text using back-translation", "content": "Figure 1 illustrates an overview of the proposed manipulation technique for evading detection. Initially, an LLM generates the desired content for a malicious user. The AI-generated text $D_{Eng}$ is then translated into $m$ various languages (excluding English), indicated as $L_1, L_2, ..., and L_m$, by a translation agent. Subsequently, a back-translation agent re-translates the text in language $L_j$ back into the original language (English), $D^j_{Eng}$. We hypothesize that different languages may use synonyms or phrases that deviate from the original wording to maintain the same meaning. Intermediate languages may also utilize distinct grammatical structures that leads to changes in sentence construction. In addition, the presence of idiomatic expressions or cultural references that lack direct translations may also necessitate the adoption of alternative phrasing when the text is back-translated.\nTo integrate back-translated texts, it is essential to identify semantically equivalent sentences in each back-translated text derived from different languages. Initially, we tokenize the original text and back-translated text from language $L_j$ into individual sentences, $\\{S_{Eng}^{0,1}, S_{Eng}^{0,2}, ..., S_{Eng}^{0,N_0}\\}$ and $\\{S_{L_j}^{j, 1}, S_{L_j}^{j, 2}, ..., S_{L_j}^{j, N_j}\\}$, respectively. Where $N_0$ and $N_j$ indicate the total number of sentences in original AI text and the back-translated text from language $L_j$. Then, for each sentence in the original text, we compute the similarity between that sentence and every sentence in a back-translated text, $\\phi$. The sentence with the highest similarity score in $\\phi$ is subsequently designated as the corresponding sentence in the back-translated text. This process is carried out in the \"sentence-wise similarity alignment\u201d block as illustrated in Figure 1.\nTo combine the selected sentences into a unified text, we employ the WER metric to compare the original sentence with the selected sentences from different back-translated texts, $\\delta$. WER measures the discrepancy between two texts by calculating the number of substitutions, deletions, and insertions needed to transform the target text to match the reference text, normalized by the total word count of the reference text. In this study, to increase the likelihood of"}, {"title": "4.2 Evaluating similarity between AI-generated text, back-translated texts and combined text", "content": "To confirm that the manipulated texts convey the same meaning as the original AI-generated texts, we employ two similarity measures, namely P-SP [38], and USEE [39]. P-SP is a lightweight semantic similarity measure trained on over 25 million paraphrase pairs from the ParaNMT dataset [64] using negative sampling. It produces sentence embeddings by averaging the embeddings of sub-words within a sentence, as tokenized by SentencePiece [65]. The similarity between two texts is reported by calculating the cosine similarity of their respective embeddings. In the PAR3 dataset [66], human paraphrases yield an average P-SP score of 0.76 [32]. In line with the methodology in [32], we regard semantics as approximately preserved if the P-SP score exceeds this average human paraphrase score.\nThe Universal Sentence Encoder for English (USEE) is a deep averaging network-based sentence encoding model that leverages multitask learning to generate effective sentence representations. Specifically, it calculates the mean of both word- and bi-gram-level embeddings, which are then passed through a feedforward deep neural network to produce sentence embeddings. We adopted a similar procedure as described in [32] to establish a threshold for semantic preservation between two texts using the USEE metric. Accordingly, we calculated the USEE similarity between two translations derived from the same reference paragraph by two different human translators within the PAR3 dataset. We considered the average USEE score of 0.69 to be a critical threshold, above which semantic preservation was deemed to be approximately maintained.\nWe apply P-SP and USEE to compute the semantic similarity between the original AI-generated text and the back-translated texts derived from different languages. We then use these metrics again to measure the semantic similarity between the original AI-generated texts and the final manipulated texts created by our proposed method. The results of the former analysis are presented in Table 1, while those of the latter are shown in Table 2. The results consistently"}, {"title": "4.3 Reducing impact of back-translation evasions", "content": "Our proposed countermeasure is grounded in the ESAS metric [19]. By facilitating the prioritization of entities within the vocabulary, the ESAS metric provides a framework for identifying the most critical entities that differentiate human-written texts from those generated by LLMs. The ESAS metric is computed as follows:\n$F(AI \\text{ vs. Human}) = \\frac{P(w_i) \\left(H(A) - H(A|W = w_i)\\right)}{N} = \\frac{N_i}{N} \\left(1 - \\frac{\\frac{N_{L,i}}{N} \\text{log}(\\frac{N_i}{N})}{\\frac{1}{\\text{log}(N_i)}} + \\frac{\\frac{N_{H,i}}{N} \\text{log}(\\frac{N_{H,i}}{N})}{\\frac{1}{\\text{log}(N_H)}}\\right)$\nwhere $P(w_i)$, $H(A)$, $H(A|W = w_i)$, $N$, $N_i$, $N_{L,i}$, and $N_{H,i}$ represent the likelihood of occurrence of $w_i$ in a text, entropy of authorship, entropy of authorship conditioned on the presence of entity $w_i$ in the text, the size of the vocabulary, the frequency of entity $w_i$, its frequency in LLM-generated text, and its frequency in human written text, respectively.\nTo adapt ESAS to account for back-translation, we begin by separating texts into three groups: human-written (H), AI-generated (A), and back-translated (B). ESAS scores are then calculated by comparing two sets at a time, resulting in six possible scenarios: 1. {H} vs {A}, 2. {H} vs {B}, 3. {H} vs {A,B}, 4. {A} vs {B}, 5. {A} vs {B,H}, and 6. {B}"}, {"title": "5 Result and Discussion", "content": "Although enhancing an AI detector's capability to identify AI-generated text is the primary objective, it is essential for the detector to minimize false positives by not labeling human-authored content as AI. To facilitate fair comparison across detectors employing varied probability thresholds, we maintain a fixed false positive rate (FPR) of 1% and report the true positive rate (TPR)."}, {"title": "5.1 Impact of back-translation on current AI text detection systems", "content": "To assess the robustness of existing AI text detection methods against our back-translation manipulation, we conduct experiments on nine detectors: six open-source models (RADAR [26], LLMDet [46], Likelihood [13], Rank [13, 67], Log-Rank, and ESAS [19]) and three commercial detectors (Pangram [27], GPTZero [47], and ZeroGPT [48]). RADAR employs an adversarial learning framework and utilizes two language models: one functioning as a paraphraser and the other as a detector. During the training phase, the detector is optimized to differentiate between human-authored and AI-generated text, while the paraphraser model evolves to modify AI-generated text to elude detection. The LLMDet operates in two distinct phases: dictionary compilation and text source detection. For the latter, the algorithm computes proxy perplexity scores for specific LLMs by leveraging next-token probabilities of salient n-grams as features. The text's origin is subsequently determined through an analysis of these LLM-dependent proxy perplexities. The Likelihood, Rank, and Log-Rank methods are statistical approaches grounded in token probability analysis. In the Likelihood method, the model's average token log probability is used to determine if a text is AI-generated. Rank and Log-Rank methods rely on the average rank or log-rank of tokens. Texts exhibiting lower average rank or log-rank values are indicative of AI generation. Table 3 presents the TPRs before and after applying back-translation manipulation. Open-source methods are displayed above the thick line, while closed-source methods are shown below. Owing to budgetary limitations, GPTZero and ZeroGPT analyses were confined to 200 randomly sampled texts per dataset, whereas full datasets were utilized for all other methods. In instances where sample size limitations precluded fixing the FPR at 1%, the actual FPR is indicated in parenthetical superscript format alongside the TPR. The dataset containing answers to Reddit questions generated by GPT-3.5 Turbo, Llama3, Mistral, Phi3, and Yi is represented by ELI-G, ELI-L, ELI-M, ELI-P, and ELI-Y, respectively. Additionally, the review datasets produced by GPT4o and Llama3.1 are denoted as R-G and R-L, respectively, in the table.\nThe results reveal that the review datasets raise significant concerns about the robustness of six out of nine detectors when it comes to identifying short AI-generated text, even before the application of back-translation manipulation. LLMDet exhibits a bias towards classifying texts as AI-generated, resulting in substantially diminished TPRs across all datasets when maintaining a low FPR. The implementation of back-translation leads to an average 54.3% reduction in RADAR's TPR. While Likelihood, Rank, and Log-Rank methods demonstrate poor performance in detecting AI-generated texts within News, Abstract, and review datasets, the application of back-translation significantly reduces their TPRs in ELI datasets. A reduction of 50%, 65.4%, and 52% in the average TPR is observed for the Likelihood, Rank, and Log-Rank methods, respectively. The outcomes for GPTZero and ZeroGPT indicate a lack of robustness against back-translation techniques. For example, GPTZero's efficacy on the R-L dataset, in terms of TPR, decreases considerably from 0.65 to 0.09. Similarly, ZeroGPT experiences a dramatic TPR reduction on the ELI-M dataset, decreasing from 0.98 to 0.03. In comparison to other methods, ESAS and Pangram exhibit a degree of robustness, particularly for datasets with longer texts (News, Abstract, and ELIs). However, back-translation manipulation can"}, {"title": "5.2 Evaluation of the proposed countermeasure", "content": "In our experimental design, we implement MESAS with q=4000 for entity selection (uni-grams or bi-grams). An LR model is trained on TF-IDF features, restricted to a vocabulary containing the 4000 entities with maximal MESAS scores. The FPR is fixed at 1%, and the TPR is reported. Table 4 shows the effectiveness of the proposed MESAS method in counteracting back-translation manipulation. MESAS (Uni) demonstrates robust resilience, with only a 1.54% average TPR reduction after manipulation. MESAS (Uni+Bi) shows comparable stability, experiencing a mere 1.85% decrease in average TPR. It is noteworthy that although the ensemble method (MESAS (Uni+Bi)) shows a marginally higher TPR reduction, its average TPR of 0.947 after back-translation surpasses MESAS (Uni) at 0.88, emphasizing the ensemble's enhanced detection capabilities.\nFurthermore, beyond enhancing robustness to back-translation, in some cases, the TPR even improved. This observation hints at the possibility of using back-translation to enhance detection accuracy, similar to its effect in data augmentation. We leave further exploration of this potential for future research."}, {"title": "5.3 Ablation study", "content": "We conduct an ablation study to evaluate the influence of intermediate languages and combiner on the performance of back-translation manipulation as a detection evasion technique."}, {"title": "5.3.1 Evaluation of each intermediate language to evading detection", "content": "We conduct an iterative exclusion process, removing one language at a time from the set of intermediate languages. The WER combiner subsequently integrates back-translated texts from the nine remaining languages. Table 5 presents the TPRs for each language exclusion scenario after subjecting the manipulated text to the ESAS detection method. The symbol \u00d8 represents the baseline condition where all ten languages are included. A TPR exceeding \u00d8 indicates a positive contribution of the excluded language to the efficacy of back-translation manipulation in evading detection. Conversely, a TPR below \u00d8 suggests that the excluded language could potentially be eliminated or substituted with a more effective alternative.\nAnalysis of the results reveals negligible deviation from the baseline, indicating that the proposed method is robust to the choice of languages. However, Japanese emerges as the most influential intermediate language. In 8 out of 9 datasets, the exclusion of Japanese yields increased TPR. Japanese demonstrates the most significant impact across multiple datasets, including News, ELI-L, ELI-M, ELI-P (alongside Dutch and Korean), ELI-Y, and R-G, when compared to other languages. For the Abstract dataset, the exclusion of Korean most substantially impairs the manipulation efficacy, while for the R-L dataset, Chinese exclusion yields the highest impact. German slightly outperforms other languages in its impact on the ELI-G dataset."}, {"title": "5.3.2 Effect of the number of selected intermediate languages", "content": "We assess the proposed manipulation by varying the number of intermediate languages involved. Beginning with all 10 languages, we sequentially eliminate one language at a time, adhering to the following order: Portuguese, Spanish, French, Italian, Chinese, Dutch, Danish, Japanese, German, and Korean. At each step, the WER combiner merges the back-translated texts from the remaining languages. The resultant manipulated texts are then evaluated using ESAS to measure the corresponding TPR."}, {"title": "5.3.3 Impact of combiner method on decreasing TPR", "content": "We perform an experiment to evaluate the impact of the proposed WER-based combiner on evading ESAS detection. The combiner method used throughout the paper, \u201cWER-max\u201d, selects back-translated texts based on the maximum WER metric. For comparison, we develop \"WER-min\", which selects texts based on the lowest WER metric. Additionally, a random combination approach, designated as \"Random\u201d is implemented, wherein original AI-generated sentences are"}, {"title": "6 Conclusion and future work", "content": "In this work, we highlight the concerning vulnerability in existing AI text detectors by introducing back-translation as an effective manipulation strategy to circumvent AI text detection. Our findings demonstrate that this method preserves the semantic content of the original AI-generated text while significantly reducing the TPR of existing detectors. As a proactive defense against such exploits, we devised a detection mechanism that exhibits strong performance, experiencing only a 1.85% drop in TPR following back-translation. Furthermore, we contribute to the field by introducing a comprehensive dataset called ESPERANTO comprising texts in different writing styles and from 8 distinct LLMs, which has been made publicly accessible to support future research endeavors.\nOur research was limited to an analysis of 10 preselected languages. Further studies are required to examine and rank additional languages, enabling the identification of superior candidates for the proposed back-translation manipulation technique. In addition, future research should focus on developing more sophisticated combiner methods that incorporate additional linguistic features such as part-of-speech tags and grammatical structures. We hypothesize that such advanced techniques could further degrade the TPR of AI text detectors, presenting an avenue for subsequent investigation."}, {"title": "Ethical Considerations", "content": "The intention of this study is to assess the robustness of current AI text detection algorithms. The widespread use of LLMs and their potential for misuse, makes the robustness of AI text detectors essential for their role in investigative"}]}