{"title": "Generalization and Informativeness of Weighted Conformal Risk Control Under Covariate Shift", "authors": ["Matteo Zecchin", "Fredrik Hellstr\u00f6m", "Sangwoo Park", "Shlomo Shamai (Shitz)", "Osvaldo Simeone"], "abstract": "Predictive models are often required to produce reliable predictions under statistical conditions that are not matched to the training data. A common type of training-testing mismatch is covariate shift, where the conditional distribution of the target variable given the input features remains fixed, while the marginal distribution of the inputs changes. Weighted conformal risk control (W-CRC) uses data collected during the training phase to convert point predictions into prediction sets with valid risk guarantees at test time despite the presence of a covariate shift. However, while W-CRC provides statistical reliability, its efficiency - measured by the size of the prediction sets - can only be assessed at test time. In this work, we relate the generalization properties of the base predictor to the efficiency of W-CRC under covariate shifts. Specifically, we derive a bound on the inefficiency of the W-CRC predictor that depends on algorithmic hyperparameters and task-specific quantities available at training time. This bound offers insights on relationships between the informativeness of the prediction sets, the extent of the covariate shift, and the size of the calibration and training sets. Experiments on fingerprinting-based localization validate the theoretical results.", "sections": [{"title": "I. INTRODUCTION", "content": "In many applications, the dependability of prediction models relies on their ability to quantify the uncertainty in their outputs [1]\u2013[4]. A significant challenge in producing reliable uncertainty quantification is the presence of distribution shifts between training and testing conditions, which can lead to mismatched and unreliable estimates [5], [6]. A particular class of distribution shifts is covariate shift, where the distribution of the prediction target given the input covariates remains fixed between training and testing, while the marginal distribution of the covariates is allowed to change [7].\nTo illustrate the problem of interest, consider the task of fingerprinting-based localization in cellular systems [8], [9], depicted in Figure 1. In this scenario, measurements and transmitter locations are collected for training under specific base station activation patterns, whereby base stations are dynamically switched off to save energy. However, cellular operators may need reliability estimates under different base station activation conditions depending on the conditions encountered at test time [10].\nWeighted conformal risk control (W-CRC) transforms a potentially unreliable predictor into a calibrated set predictor [11]\u2013[13]. For example, in fingerprinting-based localization, W-CRC converts point estimates of a mobile's location into geographical uncertainty regions. The prediction sets come with assumption-free risk guarantees \u2013 e.g., on the coverage probability for the localization task \u2013 that hold irrespective of the covariate shift.\nAs illustrated in Figure 1, W-CRC first splits the available data into training and calibration sets. The training set is used to train a base prediction model, while the calibration set is used to construct error bars more generally, a prediction set \u2013 around the model's outputs. To account for a covariate shift, W-CRC leverages knowledge of the likelihood ratio of the training and test marginal distributions.\nWhile the prediction sets produced by W-CRC guarantee a user-defined risk level, the informativeness of the prediction sets can only be evaluated at test time. In this context, the goal of this work is to relate the generalization properties of the base predictor to the average size of the set predictor produced by W-CRC, where larger prediction sets are less informative.\nIn the absence of distribution shift between training and testing conditions, the expected size of conformal prediction sets [3] has been analyzed [14] and connected to the generalization performance of the base predictor in the finite-sample regime [15]. Prior works studied optimal prediction sets with minimal size in the asymptotic regime [16]. Also related are works with an algorithmic focus that aim to increase prediction efficiency by optimizing the scoring function [17], [18].\nIn this paper, after defining the problem (Section II), we provide a bound on the average prediction set size that depends on algorithmic hyperparameters and task-dependent quantities available at training time (Section III). This result offers insights into the relationship between informativeness, the extent of the covariate shift, reliability requirements, and the amount of calibration and training data. Experimental evidence based on a real-world fingerprinting localization task validates the conclusions derived from the analysis (Section IV)."}, {"title": "II. PROBLEM DEFINITION", "content": "We consider a supervised learning setting, where a data point $z = (x, y) \\in \\mathcal{X} \\times \\mathcal{Y} = \\mathcal{Z}$ consists of an input feature $x \\in \\mathcal{X}$ and a label $y \\in \\mathcal{Y}$. The learner has access to a data set $\\mathcal{D} = \\{Z_i\\}_{i=1}^n \\in \\mathcal{Z}^n$ consisting of independent and identically distributed (i.i.d.) pairs $Z_i = (X_i, Y_i)$ following an unknown training distribution $P_{XY}$. The test data $Z = (X, Y)$ is sampled from an unknown test distribution $P'_{XY}$.\nSpecifically, reflecting a covariate shift, the test distribution $P'_{XY} = P'_X P_{Y|X}$ has the same conditional label distribution as the training distribution $P_{XY} = P_X P_{Y|X}$, but differs in the input distribution. That is, we have the conditions $P'_{Y|X} = P_{Y|X}$ and $P'_X \\neq P_X$ [7].\nFor a test data point $Z = (X,Y)$, a set predictor $\\Gamma$ outputs a subset of the label space $\\Gamma(X) \\subseteq \\mathcal{Y}$. The quality of the set predictor is evaluated through a bounded loss function $L(\\Gamma(X), Y) \\in [0, \\overline{L}]$. One popular loss function is the miscoverage loss\n$L(\\Gamma(X), Y) = \\mathbb{1}\\{Y \\notin \\Gamma(X)\\}$,\nwhich equals 1 if the true label $Y$ is not included in the set $\\Gamma(X)$, and equals 0 otherwise, yielding $\\overline{L} = 1$. Another common choice is the point-to-set distance\n$C(\\Gamma(X), Y) = \\min_{Y'\\in\\Gamma(X)} ||Y - Y'||_p$,\nwhich corresponds to the Minkowski distance of order $p$ between the predicted set $\\Gamma(X)$ and the label $Y$. If the target domain is bounded, i.e. there exists $B$ such that $||y||_p \\leq B$ for all $y \\in \\mathcal{Y}$, the point-to-set distance yields $\\overline{L} = 2B$ [19]. When the label $Y$ itself is a set, one can also consider loss functions such as the false negative rate or the F1-score [12].\nFor a given loss function, the set predictor $\\Gamma$ is said to be $\\alpha$-reliable if the average risk under the test distribution does not exceed a user-defined threshold $\\alpha \\in [0, \\overline{L}]$, i.e.,\n$E[L(\\Gamma(X), Y)] \\leq \\alpha$\nwhere the expectation is over the test point $(X, Y) \\sim P'_{XY}$."}, {"title": "A. Setting", "content": "We consider a supervised learning setting, where a data point $z = (x, y) \\in \\mathcal{X} \\times \\mathcal{Y} = \\mathcal{Z}$ consists of an input feature $x \\in \\mathcal{X}$ and a label $y \\in \\mathcal{Y}$. The learner has access to a data set $\\mathcal{D} = \\{Z_i\\}_{i=1}^n \\in \\mathcal{Z}^n$ consisting of independent and identically distributed (i.i.d.) pairs $Z_i = (X_i, Y_i)$ following an unknown training distribution $P_{XY}$. The test data $Z = (X, Y)$ is sampled from an unknown test distribution $P'_{XY}$.\nSpecifically, reflecting a covariate shift, the test distribution $P'_{XY} = P'_X P_{Y|X}$ has the same conditional label distribution as the training distribution $P_{XY} = P_X P_{Y|X}$, but differs in the input distribution. That is, we have the conditions $P'_{Y|X} = P_{Y|X}$ and $P'_X \\neq P_X$ [7].\nFor a test data point $Z = (X,Y)$, a set predictor $\\Gamma$ outputs a subset of the label space $\\Gamma(X) \\subseteq \\mathcal{Y}$. The quality of the set predictor is evaluated through a bounded loss function $L(\\Gamma(X), Y) \\in [0, \\overline{L}]$. One popular loss function is the miscoverage loss\n$L(\\Gamma(X), Y) = \\mathbb{1}\\{Y \\notin \\Gamma(X)\\}$,\nwhich equals 1 if the true label $Y$ is not included in the set $\\Gamma(X)$, and equals 0 otherwise, yielding $\\overline{L} = 1$. Another common choice is the point-to-set distance\n$C(\\Gamma(X), Y) = \\min_{Y'\\in\\Gamma(X)} ||Y - Y'||_p$,\nwhich corresponds to the Minkowski distance of order $p$ between the predicted set $\\Gamma(X)$ and the label $Y$. If the target domain is bounded, i.e. there exists $B$ such that $||y||_p \\leq B$ for all $y \\in \\mathcal{Y}$, the point-to-set distance yields $\\overline{L} = 2B$ [19]. When the label $Y$ itself is a set, one can also consider loss functions such as the false negative rate or the F1-score [12].\nFor a given loss function, the set predictor $\\Gamma$ is said to be $\\alpha$-reliable if the average risk under the test distribution does not exceed a user-defined threshold $\\alpha \\in [0, \\overline{L}]$, i.e.,\n$E[L(\\Gamma(X), Y)] \\leq \\alpha$\nwhere the expectation is over the test point $(X, Y) \\sim P'_{XY}$."}, {"title": "B. Training", "content": "We partition the data $\\mathcal{D}$ into a training set $\\mathcal{D}_{tr}$, used to train a base predictor, and a calibration set $\\mathcal{D}_{cal} = \\mathcal{D} \\setminus \\mathcal{D}_{tr}$ that is used to calibrate the predictor's outputs to ensure the $\\alpha$-reliability condition (3). We denote the size of $\\mathcal{D}_{tr}$ as $n_{tr}$ and the size of $\\mathcal{D}_{cal}$ as $n_{cal}$. Specifically, we set $(X_i, Y_i) \\in \\mathcal{D}_{cal}$ for $i \\in \\{1,..., n_{cal}\\}$, while $(X_i, Y_i) \\in \\mathcal{D}_{tr}$ for $i \\in \\{n_{cal} + 1,..., n\\}$.\nGiven a model class $\\mathcal{F} = \\{f_\\theta : \\theta\\in \\Theta\\}$, where $f_\\theta : \\mathcal{X} \\rightarrow \\mathcal{Y}$ is a point predictor parameterized by a vector $\\theta$, the training data $\\mathcal{D}_{tr}$ is used to run a learning algorithm that produces a probability distribution $Q(\\theta|\\mathcal{D}_{tr})$ over the space of model parameters $\\Theta$. Examples of learning algorithms include stochastic gradient descent for conventional frequentist learning [20], [21], as well as variational inference algorithms for Bayesian learning [22], [23].\nFor an input $x$, predictions are obtained using Thompson sampling: A parameter $\\theta \\sim Q(\\theta|\\mathcal{D}_{tr})$ is first sampled, and the resulting model is then used to obtain a prediction $\\hat{y} = f_\\theta(x)$.\nThe generalization error of a learning algorithm $Q(\\theta|\\mathcal{D}_{tr})$ is the difference between the performance of the predictor during training and testing. In the case of a covariate shift, at training time, the performance of the predictor is evaluated using samples from the training distribution $P_{XY}$, while at test time it is assessed using samples from the test distribution $P'_{XY}$. Thus, the generalization error of a learning algorithm depends on both the training set size $n_{tr}$ and the extent of the covariate shift [24], [25]."}, {"title": "C. Weighted Conformal Risk Control", "content": "Given a test input $x$, W-CRC augments the point prediction $f_\\theta(x)$ with a set predictor $\\Gamma_\\theta(x, \\lambda)$. To this end, W-CRC relies on the evaluation of a non-conformity (NC) scoring function $R: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow [0, \\overline{R}]$ that evaluates the mismatch of the predictor $f_\\theta(x)$ with respect to label $y$ as $R(f_\\theta(x), y)$. For example, a typical scoring function for regression is the squared loss $R(f_\\theta(x), y) = (y-f_\\theta(x))^2$. The set predictor is parameterized by a threshold $\\lambda\\in [0, \\overline{R}]$, and it includes all the labels $y \\in \\mathcal{Y}$ whose NC score does not exceed the threshold $\\lambda$:\n$\\Gamma_\\theta(x, \\lambda) = \\{y \\in \\mathcal{Y} : R(f_\\theta(x), y) \\leq \\lambda\\}$.\nThe threshold $\\Lambda$ controls the size of the set predictor (7). In W-CRC, the value of $\\lambda$ is determined based on the calibration set $\\mathcal{D}_{cal}$ and on information about the covariate shift.\nThe covariate shift determines the likelihood ratios\n$w_i = w(X_i) = \\frac{P'_X(X_i)}{P_X(X_i)}$\nfor all the calibration data points $i \\in \\{1, . . ., N_{cal}\\}$. Denote as\n$L_i(\\lambda) = L(\\Gamma_{\\theta_i}(X_i, \\lambda), Y_i)$\nthe loss of the set predictor for each calibration point $(X_i, Y_i)$ with $i \\in \\{1,...,n_{cal}\\}$, where $\\{\\theta_i\\}_{i=1}^{n_{cal}}$ are drawn i.i.d. from $Q(\\theta|\\mathcal{D}_{tr})$ following Thompson sampling. Note that this implementation deviates slightly from previous versions of W-CRC, which use a fixed (deterministic) point predictor for all samples [12]. Based on the calibration loss values (9) and likelihood ratios (8), given the test input X, W-CRC evaluates the following empirical estimate of the calibration loss\n$\\hat{L}_Q(\\lambda, X|\\mathcal{D}) = \\frac{1}{n_{cal}} \\sum_{i=1}^{n_{cal}} \\frac{W_i}{W_i+W} (W L_i(\\lambda)+W \\overline{L}),$\nwhere $W = w(X)$ denotes the likelihood ratio evaluated at the test data point $X$. The loss (10) can be interpreted as the covariate-shift corrected empirical loss of the set predictor when evaluated on the calibration set augmented with a test point X with maximal loss value $\\overline{L}$.\nFor a user-defined reliability level $\\alpha \\in [0, \\overline{L}]$ in the con- straint (3), W-CRC evaluates the threshold $\\lambda$ in (7) as\n$\\Lambda^{W-CRC}(X|\\mathcal{D}) = \\inf\\{\\lambda : \\hat{L}_Q(\\lambda, X|\\mathcal{D}) \\leq \\alpha\\}$.\nAccordingly, the selected threshold is the smallest value of $\\lambda$ that ensures an empirical estimate (10) not exceeding the target $\\alpha$. This yields the set predictor\n$\\Gamma^{W-CRC}(X|\\mathcal{D}) = \\{y\\in \\mathcal{Y} : R(f_\\theta(X), y) \\leq \\Lambda^{W-CRC}(X|\\mathcal{D})\\}$\nwith the sampled model parameter $\\theta \\sim Q(\\theta|\\mathcal{D}_{tr})$.\nFor loss functions satisfying Assumption 1, W-CRC pro- duces $\\alpha$-reliable prediction sets.\nProposition 1 (Proposition 4 [12]): Under Assumption 1, the W-CRC predictor (12) is $\\alpha$-reliable, i.e.\n$E[L(\\Gamma^{W-CRC}(X|\\mathcal{D}), Y) | \\mathcal{D}_{tr}] \\leq \\alpha$,\nwhere the expectation is over both the test and calibration data as well as over the model parameters, with fixed training set $\\mathcal{D}_{tr}$. This result follows in a manner similar to [11], [12], with the only caveat that one needs to account for the random choice of model parameters via Thompson sampling. For completeness, a proof is provided in Appendix A."}, {"title": "III. INFORMATION-THEORETIC INEFFICIENCY BOUND FOR W-CRC", "content": "As seen in the previous section, W-CRC ensures the re- liability guarantee (3) for the set predictor $\\Gamma^{W-CRC}(X|\\mathcal{D})$. However, the inefficiency\n$\\Lambda^{W-CRC}(\\mathcal{D}_{tr}) = E[|\\Gamma^{W-CRC}(X|\\mathcal{D})| | \\mathcal{D}_{tr}]$\ncan generally only be evaluated at test time. In this section, we derive theoretical bounds on the inefficiency (14)."}, {"title": "A. Generalization Properties of the Base Predictor", "content": "The bound provided in this work relates the generalization properties of the base predictor to the informativeness of the set predictor produced by W-CRC. Accordingly, to start, we make a standard assumption on the behavior of the generalization gap of the training algorithm.\nSpecifically, we consider the generalization properties of the base predictor $Q(\\theta|\\mathcal{D}_{tr})$ in terms of the risk $L(\\cdot, \\cdot)$ of the set predictor (7). The generalization gap accounts for the difference between training and test risk measures. The training risk of the set predictor with hyperparameter $\\lambda$ is defined as the following sum over the training data points\n$\\mathcal{L}_Q(\\lambda|\\mathcal{D}_{tr}) = \\frac{1}{n_{tr}} \\sum_{i=n_{cal}+1}^{n} E[L(\\Gamma_{\\theta_i}(X_i, \\lambda), Y_i) | \\mathcal{D}_{tr}]$,\nwhile the corresponding population risk under the test distri- bution is given by\n$\\mathcal{L}_Q(\\lambda) = E [L(\\Gamma_{\\theta}(X, \\lambda), Y)]$.\nNote that the average in (15) is with respect to the model parameters, while in (16) the average is also over the test distri- bution $P'_{XY}$. As generalization metric, we adopt the maximum absolute difference between the training risk estimate (15) and the population test risk (16) over the hyperparameter $\\lambda$:\n$\\Delta(Q|\\mathcal{D}_{tr}) = \\sup_{\\lambda\\in [0,\\overline{R}]} |\\mathcal{L}_Q(\\lambda|\\mathcal{D}_{tr}) - \\mathcal{L}_Q(\\lambda)|$.\nWith this definition, we make the following assumptions.\nAssumption 2 (Bounded likelihood ratio): There exists a constant $W < \\infty$ such that the likelihood ratio is upper bounded as $w(x) \\leq W$ for all $x \\in \\mathcal{X}$."}, {"title": "B. Analysis of the NC Score", "content": "In order to derive bounds on the inefficiency of the W-CRC set predictor, an important quantity to study is the size of the NC score $R(\\cdot, \\cdot)$ [27]. In fact, the prediction set (7) includes all labels with an NC score smaller than threshold $\\lambda$.\nTo this end, we let $P_{R|Y=y,\\theta}$ denote the probability density function of the NC score $R(f_\\theta(X), y)$ for a given model $\\theta$ under the test distribution $P'_{X|Y=y} = P'_{X,y=y}/P_{y=y}$. Hence, $P_{R|Y=y,\\theta}$ indicates how likely an NC score value is given the model $\\theta$ and the label $y$. Averaging over the sampled model $\\theta \\sim Q(\\theta|\\mathcal{D}_{tr})$ and a uniformly chosen label $Y \\sim U(\\mathcal{Y})$, the size of an NC score value $r$ [27] is defined as\n$\\gamma(r|Q, \\mathcal{D}_{tr}) = \\frac{1}{|\\mathcal{Y}|} \\int E [P_{R|Y=y,\\theta}(r) | \\mathcal{D}_{tr}] dy$.\nTo streamline notation, we denote $\\gamma(r|Q, \\mathcal{D}_{tr})$ as $\\gamma(r)$. A larger $\\gamma(r)$ indicates that the NC scorer is more likely."}, {"title": "C. Main Result", "content": "With the assumptions listed above, we obtain the following guarantee on the informativeness of the W-CRC set predictor, the proof of which is provided in Appendix C.\nTheorem 1: Suppose that Assumptions 1-3 hold. Then, with probability at least $1 - \\delta$ with respect to the training set $\\mathcal{D}_{tr} \\sim P_{X,Y}^{n_{tr}}$, the expected set size of the W-CRC predictor satisfies the inequality\n$\\Lambda^{W-CRC}(\\mathcal{D}_{tr}) < \\int_{0}^{\\hat{\\lambda}} \\gamma(r)dr + \\int_{\\hat{\\lambda}}^{\\overline{R}} \\exp{\\frac{2n_{cal}(\\frac{W(\\alpha - \\overline{L})}{n_{cal}} -L_Q(r|\\mathcal{D}_{tr})+\\alpha)^2}{\\overline{L}^2W^2}} \\gamma(r)dr$,\nwhere the value $\\hat{\\lambda}$ is defined as\n$\\hat{\\lambda} = \\inf \\{r : \\mathcal{L}_Q(r|\\mathcal{D}_{tr}) \\leq \\alpha + \\frac{W(\\alpha - \\overline{L})}{\\gamma_{cal}} - \\frac{\\beta(\\delta, n_{tr})}{\\sqrt{n_{tr}}} - \\frac{\\overline{L}}{\\sqrt{\\log(W)}}\\}.$"}, {"title": "D. Discussion", "content": "Analyzing the bound in Theorem 1, it is possible to study the interplay between the efficiency of W-CRC, the extent of the covariate shift, and the sizes of the calibration and training sets. In particular, we make the following observations:\n* A larger covariate shift, reflected in a larger maximum likelihood ratio $W$, causes the inefficiency of W-CRC to increase. This is a consequence of the negative impact of a covariate shift on the generalization properties (18) of the base predictor.\n* A larger size of the training set, $n_{tr}$, improves the perfor- mance of the base predictor, as reflected in a smaller value of the generalization error bound (18). However, for large values of the covariate shift measure $W$, increasing the value of $n_{tr}$ has a limited effect on reducing the inefficiency, and the effect of covariate shift dominates.\n* A larger covariate shift reduces the decay rate of the exponential function in the second term of the bound (20). Given the dependence of this term on the calibration set size $n_{cal}$, this result suggests that a larger calibration set is required in order to counteract the effect of a covariate shift.\n* More broadly, a larger calibration set size $n_{cal}$ reduces the inefficiency bound for a fixed training set size and covariate shift. A larger calibration set enables a sharper estimation of the CRC threshold, reducing the impact of covariate shift. This improvement is reflected both in the exponential term of (20), which decays faster as $n_{cal}$ grows, and in the quantity which decreases as $n_{cal}$ grows.\nOverall, the bound suggests that the performance of W- CRC is negatively affected by covariate shift, especially when limited calibration data is available. Accordingly, in settings with severe covariate shift, it may be beneficial to allocate a larger portion of the data for calibration. This is illustrated in Figure 2, where the bound is evaluated for a fixed data budget $n$ with varying training data fraction $\\kappa = n_{tr}/n$. It is observed that an optimized data split that minimizes the inefficiency of W-CRC shifts in favor of larger calibration sets as the covariate shift becomes more significant, and thus W increases."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we consider the RSSI-based localization problem introduced in Section I. The experiments use data from the rural Sigfox data set [28], which contains real- world RSSI measurement vectors $X^{RSSI} \\in \\mathbb{R}^d$ collected from $d = 137$ base stations (BSs) located between the cities of Antwerp and Ghent.\nWe assume that, during the operation of the system, a fixed subset B of the BSs can be switched off [10], and this event is denoted by the random variable $X^{act} = \\mathbb{1}\\{B \\text{ is inactive}\\}$."}]}