{"title": "Beyond Backpropagation: Optimization with Multi-Tangent Forward Gradients", "authors": ["Katharina Fl\u00fcgel", "Daniel Coquelin", "Marie Weiel", "Achim Streit", "Markus G\u00f6tz"], "abstract": "The gradients used to train neural networks are typically computed using backpropagation. While an efficient way to obtain exact gradients, backpropagation is computationally expensive, hinders parallelization, and is biologically implausible. Forward gradients are an approach to approximate the gradients from directional derivatives along random tangents computed by forward-mode automatic differentiation. So far, research has focused on using a single tangent per step. This paper provides an in-depth analysis of multi-tangent forward gradients and introduces an improved approach to combining the forward gradients from multiple tangents based on orthogonal projections. We demonstrate that increasing the number of tangents improves both approximation quality and optimization performance across various tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Neural networks are typically trained by minimizing a loss using gradient descent, that is, updating the network parameters by stepping in the opposite direction of their gradient. Backpropagation (Rumelhart et al., 1986; Linnainmaa, 1970) is widely used to compute these gradients efficiently and accurately but comes with several drawbacks. Taking about twice the time of the forward pass (Kaplan et al., 2020), it accounts for a large fraction of the training time, directly contributing to the consumed energy (Debus et al., 2023). Its forward-backward dependencies furthermore lead to suboptimal memory access patterns (Crafton et al., 2019) and hinder parallelization (Belilovsky et al., 2020). For example, when pipelining the training over the network layers, these dependencies can lead to pipeline bubbles and under-utilization (Narayanan et al., 2019; Huang et al., 2019). Besides these practical issues, backpropagation is incompatible with the biological motivation behind artificial neural networks, as biological neural networks lack such backward pathways to communicate update information (Bengio et al., 2016).\nTo overcome these issues, there has been much interest in alternative approaches to train neural networks. One promising approach, the recently proposed forward gradient (Baydin et al., 2022; Silver et al., 2022), uses forward-mode automatic differentiation to compute a directional derivative along a random tangent, approximating the gradient in a single forward pass. This method has shown great potential for training both fully-connected and convolutional neural networks. However, with increasing dimension, i.e., model parameter count, the approximation quality decreases while the variance of the forward gradient increases (Belouze, 2022; Ren et al., 2023). By sampling a random direction, the forward gradient acts as a Monte-Carlo approximation of the gradient. We thus expect that increasing the number of tangents will improve the approximation quality, enabling forward gradients to scale to larger dimensions and improving their robustness. Using multiple tangents has been previously mentioned (Baydin et al., 2022; Silver et al., 2022) but has not yet been analyzed in detail or evaluated experimentally.\nIn this work, we aim to fill this gap by taking a detailed look at how using multiple tangents can improve the forward gradient. Specifically, we aim to answer the following research questions:\nRQ1 Does using multiple tangents improve the forward gradient?\nRQ2 How should the forward gradient information from multiple tangents be combined?\nRQ3 Can multi-tangent forward gradients scale to state-of-the-art architectures?\nRQ4 What are the trade-offs of multiple tangents?"}, {"title": "2 RELATED WORK", "content": "Our study builds upon previous research on forward gradients. Baydin et al. (2022) and Silver et al. (2022) introduce the forward gradient, an approach to approximate the gradient with the directional derivative along a random direction, resulting in an unbiased estimator of the gradient. A follow-up study by Belouze (2022) further analyzes the forward gradient on the Beale and Rosenbrock functions, revealing significant shortcomings in high dimensions. The author suggests sampling tangents from the Rademacher distribution to minimize the variance. Ren et al. (2023) show that the variance of the forward gradient estimation increases with the dimension and find that activity perturbation reduces this variance compared to weight perturbation. They use a custom architecture based on MLPMixer (Tolstikhin et al., 2021) and combine activity perturbation with numerous local losses to scale up to ImageNet (Deng et al., 2009). Fournier et al. (2023) introduce a systematic framework to combine different gradient targets and gradient guesses, i.e., tangents. They observe significant improvements from using local update signals as tangents instead of random noise, scaling up to ResNet18 (He et al., 2016) and ImageNet32 (Chrabaszcz, 2017).\nBesides forward gradients, numerous other approaches seek to train neural networks without backpropagation, often to overcome its biological implausibility. Feedback Alignment (Lillicrap et al., 2016) aims to enhance biological plausibility and solve the so-called weight transport problem (Grossberg, 1987) by communicating feedback signals backward along random, untrained feedback paths. Follow-up works (N\u00f8kland, 2016; Frenkel et al., 2021; Fl\u00fcgel et al., 2023) replace this backward step with direct forward communication to each layer, effectively addressing the update locking problem (Jaderberg et al., 2017). Bacho and Chu (2024) combine direct feedback alignment (N\u00f8kland, 2016) with forward gradients and momentum to reduce the gradient variance. The forward-forward algorithm (Hinton, 2022) trains neural networks using two forward passes- -one on positive data and another on negative data- while applying local updates to differentiate between them. Similarly, PEPITA (Dellaferrera and Kreiman, 2022) applies a second forward pass on a randomly modulated input, which has been shown to be a special case of forward-forward (Srinivasan et al., 2024). Synthetic gradients (Jaderberg et al., 2017; Czarnecki et al., 2017) approximate the gradient by training layer-local models that predict the gradient using only local information, effectively decoupling the layers."}, {"title": "3 MULTI-TANGENT FORWARD GRADIENTS", "content": "A typical neural network consists of layers 11,...,IL, each processing an input hi\u22121 from the previous layer, starting from the network input ho, and passing the output hi on to the next layer. Each layer depends on parameters \u03b8i, which are adjusted during training. A layer can thus be seen as a function li: \u04e8\u00a1 \u00d7 Hi\u22121 \u2192 Hi, (0i, hi-1)\u2192hi, mapping the parameter space Oi and the input space Hi-1 to the output space Hi. In the forward pass, an input x = ho is mapped to an output y = h\u2081 by successively applying these layers. Using this layer definition, we can define the entire model as a function m: \u04e8\u00d7 X \u2192 Y with \u0398 = 01 \u00d7 = \u00d7 \u0398L, X = Ho, and Y = HL, where m(0,x) \u2192 1L(OL, 1L-1(..., 11(01,x))) with 0 = (01,...,01).\nTo train the model in a supervised fashion, we need an optimization metric, the loss function L : Y \u00d7 Y \u2192 R, which measures how much the model output y = m(0,x) for input x differs from the corresponding target y*. We then train the model by minimizing the loss L(y, y*), i.e., minimizing the objective function f : \u04e8\u00d7 X x Y \u2192 R, (0,x,y*) \u2192 L(m(0,x), y*) for a set of input samples (x, y*) by varying the parameters 0. A common method to minimize such a function is gradient descent, where we take small steps in the opposite direction of the gradient\n\u2207 f(0,x,y*) = $\\frac{\\partial f(\\theta,x,y^*)}{\\partial \\theta}$ (1)\nof f(0, x, y*) with respect to the parameters 0. During training, the parameters are iteratively updated as\n\u03b8 0 \u2013 \u03b7 f(0,x,y*) (2)\nusing a scalar learning rate \u03b7. More advanced optimizers like Adam (Kingma and Ba, 2015) introduce additional terms, such as momentum and parameter-wise adaptive learning rates, while retaining the core idea of computing the gradient and stepping in the opposite direction. Instead of using the exact gradient Vf, one can also train the model with an approximate gradient g, such as the forward gradient introduced in Section 3.2, by replacing \u2207f in Equation 2 with g.\nComputing the gradient f is a non-trivial task. Multiple methods exist, for example, by analytically deriving a closed-form solution or numerically approximating the gradient using the finite differences approach. However, most are impractical for complex neural networks due to inefficient computation and numerical instability (Baydin et al., 2018). This is why automatic"}, {"title": "3.2 Forward Gradient", "content": "The forward gradient (Baydin et al., 2022; Silver et al., 2022) is an approach to approximate the gradient using the directional derivative df/dv = \u2207 fv along a tangent v \u2208 R\", and multiplying it with the tangent v. This directional derivative can be computed efficiently in a single forward-mode AD pass.\nDefinition 1 (Forward Gradient). The forward gradient go for a tangent v \u2208 R\" is defined as\ngv = (\u2207f\u00b7\u03c5)\u03c5. (4)\nThe forward gradient has multiple useful properties.\nProperty 1 (Unbiased Estimator). The forward gradient gu is an unbiased estimator of \u2207f, i.e., E[gv] = \u2207f, when sampling the tangent v randomly with iid. entries [v]i with zero mean and unit variance (Baydin et al., 2022; Silver et al., 2022).\nProperty 2 (Within 90\u00b0). The forward gradient gu is within 90\u00b0 of Vf for any tangent v if gv \u2260 0.\nProof. The dot-product of gr \u2260 0 and Vf is positive\ngv \u00b7 \u2207 f = (\u2207 f \u00b7 v) (v \u00b7 \u2207 f) = (\u2207 f \u00b7 v)\u00b2 \u2265 0, (5)\nand thus their angle is between 0\u00b0 and 90\u00b0.\nProperty 3 (Descending Direction). The forward gradient is always a descending direction of f (Silver et al., 2022). This follows directly from Property 2.\nProperty 4. The forward gradient maps Vf from Rn onto the one-dimensional subspace spanned by v \u2260 0. If v is a unit vector, gv is a projection.\nProperty 4 follows directly from Definition 1 and is illustrated in Figure 1. Besides these beneficial properties, there are several drawbacks to consider. While the forward gradient gu is an unbiased estimator, it suffers from a high variance that increases with the number of parameters n (Belouze, 2022; Ren et al., 2023), thus reducing the accuracy of the gradient approximation. To measure the approximation quality, we can decompose the difference between gu and Vf\""}, {"title": "3.3 Multi-Tangent Forward Gradients", "content": "When sampling tangents randomly, gu acts as a Monte-Carlo approximation of Vf. Consequently, we expect the approximation quality to improve with more samples, leading us to the definition of a multi-tangent forward gradient (RQ1).\nDefinition 2 (Multi-Tangent Forward Gradient). The multi-tangent forward gradient gv over a set of k tangents V = {V1, ..., Vk}, Vi \u2208 Rn is defined as\ngv = \\oplus_{V_i EV}gv_i (10)\nfor an aggregation operator \u2295.\nLemma 1. Let V = {1, ..., vk} be k linearly independent tangents vi \u2208 Rn and U = span(V) R the k-dimensional subspace spanned by V. For any linear combination\u2295 applies gv \u2208 U.\nA key question is how to combine the single-tangent forward gradients 9v1,..., 9vk into a multi-tangent forward gradient, i.e., how to choose (RQ2). Two approaches have been suggested so far:\nDefinition 3. The multi-tangent forward gradient as a sum (Baydin et al., 2022) is defined as\ng+= \\sum_{i=1}^{k}g_{v_i} = VV^{\\top}\\nabla f (11)\nwith V = (v1| ... |Uk).\nDefinition 4. The multi-tangent forward gradient as average (Silver et al., 2022) is defined as\ngv = $\\frac{1}{k}\\sum{g_{v_i}}$ (12)\nThey correspond to choosing \u2295 = \u2211 and \u2295 = $\\frac{1}{k}$ respectively. The resulting multi-tangent forward gradient differs only in its magnitude but not its direction, which is easily offset by the learning rate.\nWhen using all n standard basis vectors e1,..., en as tangents (i.e., k = n), g+ corresponds to computing the exact gradient with full forward-mode AD, computing f for one parameter at a time as described in Section 3.1. However, when sampling the tangents randomly, e.g., vi ~ N(0, In), g does generally not equal f, even for k = n tangents, as shown in Section 4.1. This limitation arises from the aggregation method used to combine multiple forward gradients gu: both average and sum are conical combinations, which restrict the result to the cone spanned by the gu\u2081; and, consequently, the tangents vi, as gu\u2081 is always (anti-)parallel to vi. Moreover, they ignore the approximation quality of the different gui and their interrelations. That is why even when using k > n linearly independent tangents, g and gv may fail to perfectly approximate Vf, despite having sufficient information to reconstruct the entire gradient."}, {"title": "3.4 Improved Multi-Tangent Forward Gradients With Orthogonal Projection", "content": "To improve the resulting multi-tangent forward gradient, we aim to use all information available from the single-tangent forward gradients.\nDefinition 5 (Forward Orthogonal Gradient). Let V = {V1,...,Uk} be k linearly independent tangents with U = span(V) and V = (01|...|vk), then the orthogonal projection of \u2207f onto U is defined as\nPu(\u2207f) = V(VTV)-1v\u00aeVf. (13)\nLemma 2. Pu(\u2207f) is the most accurate approximation g of \u2207 f in U and minimizes ||\u2207f - 9||2.\nCompared to g+ (Definition 3), Pu (Vf) uses the same vector of directional derivatives VTVf but \"corrects\" V to an orthonormal basis using the inverse Gram matrix G(V)-1 = (VTV)-1 before aggregating it. The"}, {"title": "3.5 Time Complexity of Forward Gradients", "content": "Computing a k-tangent forward gradient g\u2193 or gv has a time complexity of O(kops(f)) and aligns with the complexity of k forward-mode AD passes (see Section 3.1). Constructing G(V)-\u00b9 for Pu(\u2207f) introduces an additional overhead of O(k2n), which is dominated by O(kops(f)) when k is restricted to small constants, but must be considered for arbitrarily large k. A detailed derivation of these complexities is given in Appendix A.2. In both cases, the time complexity scales with k, which limits the applicability of multi-tangent forward gradients since there is only about a factor of two to gain over backpropagation (Kaplan et al., 2020), making any value of k beyond small constants rather impractical (RQ4). Despite the computational overhead, it is crucial to study the potential gain from multi-tangent forward gradients as the the-oretical overhead may be partially offset by more efficient implementations, beneficial memory access patterns, and additional parallelization opportunities."}, {"title": "4 EVALUATION", "content": "We evaluate multi-tangent forward gradients in terms of their approximation quality and optimization performance. To highlight the differences between the gradients, we use stochastic gradient descent without momentum or learning rate schedule. We optimize multiple closed-form functions and train neural networks on the image classification datasets MNIST (Lecun et al., 1998) and CIFAR10 (Krizhevsky, 2009) using cross-entropy loss and activity perturbation. To adjust for the different gradient norms, we perform a learning rate search for each variant using Propulate (Taubert et al., 2023). More details are given in Appendix B. All experiments were conducted using an Intel Xeon Platinum 8368 processor and an NVIDIA A100-40 GPU and implemented in Python 3.9.16 using PyTorch (Paszke et al., 2019) 2.2.2 with CUDA 12.4. Our code is publicly available at github.com/Helmholtz-AI-Energy/frog."}, {"title": "4.1 Approximation Quality", "content": "First, we evaluate how well different forward gradients approximate the true gradient \u2207 f by considering their difference in terms of direction, via the cosine similarity (Eq. 6), and magnitude, via the ratio of their norms (Eq. 7). We use a fixed gradient \u2207f = (1,...,1) \u0454 R\" and approximate it using the forward-gradient approaches introduced in Section 3: the single-tangent forward gradient gu and multi-tangent forward gradients using either a sum (gt), an average (gv), or the orthogonal projection (PU) for a set of k tangents V = {1, ..., Uk}, Vi ~ N(0, In). Additionally, we test sum and average with normalized tangents W = {W1,...,wk}, Wi = Vi/||Vi||2, labeled as gw and gw, respectively. Due to the rotational invariance of N(0, In), the choice of Vf does not affect our results."}, {"title": "4.2 Optimizing Closed-Form Functions", "content": "We first evaluate the practical application of multi-tangent forward gradients by minimizing three classical, multidimensional, differentiable closed-form functions with gradient descent. While the strictly con-vex Sphere function is straightforward to optimize, the Rosenbrock (Rosenbrock, 1960) and Styblinski-Tang functions (Styblinski and Tang, 1990) pose harder problems, with a long, narrow, flat valley around the global minimum and many local minima, respectively.\nWe evaluate the best value reached using different gradient approximations for increasing dimensions n, giving the mean over five random seeds in Figure 4. First, we observe that the single-tangent baseline gu struggles to optimize even trivial functions like the Sphere for high-dimensional inputs. Adding more tangents consistently improves the optimization result, effectively extending the range at which the forward gradient achieves performance comparable to the true gradient Vf. The only exception is Rosenbrock, where we find that for small n and k, adding more tangents can actually decrease performance, especially for gy. Due to the complexity of optimizing Rosenbrock, where a few missteps can quickly lead to divergence, and the learning rate increasing with k, gy can be more sensitive to the random seed.\nInterestingly, the differences between gy and Pu are less pronounced than those observed in Section 4.1 when comparing the gradients themselves. For Sphere and Styblinski-Tang, they seem to have no impact on the final result. Only for Rosenbrock do we observe a notable improvement of Pu over gv, where Pu shows a more consistent improvement when adding more tangents. This might be due to the fact that tangents sampled from N(0, In) are already very likely to be near orthogonal, limiting the potential impact of orthonormalization. On top of that, the difference in approximation quality observed in Section 4.1 between Pu and conical combinations is largest for k close to n. Yet the largest k tested here is k = 16, and for both Sphere and Styblinski-Tang, even the baseline gu converges to the minimum for n \u2265 16."}, {"title": "4.3 Non-Orthogonal Tangents", "content": "To investigate the benefits of orthogonal projection for arbitrary tangents, we study the behavior of Pu and gv with non-orthogonal tangents. For this, we con-"}, {"title": "4.4 Fully-Connected Neural Networks", "content": "We transfer our results to neural networks by training a fully-connected network with two hidden layers and ReLU activations on MNIST using different forward gradient variants. We test three layer widths w = 256, 1024, 4096 which correlate with the gradient dimension n. Figure 6 gives the best test error after training for each layer width and the test loss curves for w = 4096. As in the previous experiments, we observe a clear improvement from using more tangents, with k = 16 achieving results close to the true gradient Vf despite k\u226an. We again find only little practical difference between Pu and gv, although Pu appears"}, {"title": "4.5 Scaling to Larger Networks", "content": "To analyze the scalability of multi-tangent forward gradients (RQ3), we train a ResNet18 (He et al., 2016) and a small vision transformer (ViT) (Dosovitskiy et al., 2021) with six layers, four heads, 256 hidden dimensions, and 512 MLP dimensions on MNIST and CIFAR10. Table 1 states the minimum test errors after training with different gradient approximations. In line with our prior results, using more tangents reduces the test error, with k = 16 achieving the best results among the tested forward gradients. Again, the distinction between the combination approaches is less clear and further complicated by the impact of the chosen learning rate since gv tends to be significantly larger than Pu, especially for small k. While this can, in theory, be corrected for, determining a suitable learning rate for such complex tasks is non-trivial. Suggesting the learning rate analytically to reduce the search space is thus an important avenue for further research. Overall, a significant gap between the forward gradients and the true gradient remains, in particular for more complex tasks. Combining multi-tangent forward gradients with better tangent sampling approaches appears to be a promising approach to further reduce this gap."}, {"title": "5 DISCUSSION", "content": "In this paper, we systematically investigate how using multiple tangents can improve forward gradients.\nAnswer to RQ1: More tangents improve both gradient approximation and optimization. Our results confirm prior assumptions that aggregating a forward gradient over multiple tangents improves its performance. Increasing the number of tangents k consistently improves both the approximation of the gradient's direction (Section 4.1) and optimization perfor-"}, {"title": "A THEORETICAL RESULTS AND PROOFS", "content": ""}, {"title": "A.1 Additional Proofs for Section 3", "content": "Property 2 (Within 90\u00b0). The forward gradient gr is within 90\u00b0 of Vf for any tangent v if gu \u2260 0.\nProof. The dot-product of go and Vf is positive\ngv \u00b7 \u2207 f = ((\u2207 f. v)v) \u00b7 \u2207 f = (\u2207 f \u00b7 v)(v \u00b7 \u2207 f) = (\u2207 f \u2022 v)\u00b2 \u2265 0. (14)\nIf gv \u2260 0 then Vf must also be non-zero as otherwise \u2207 f. v = 0 and thus gv = 0. Hence, for gv \u2260 0 the gradient norms are strictly positive: ||9v||2 > 0 and ||f||2 > 0. It follows that the cosine similarity of gu and Vf is positive\ncos(x) = Sc(\u2207f,gv) = $\\frac{g_v \\nabla f}{\\|g_v\\|_2\\|\\nabla f\\|_2}$ \u2265 0 (15)\nconsequently\n0 \u2264 y = arccos(cos(y)) \u2264 $\\frac{\\pi}{2}$ (16)\nThe angle y between gu and Vf is thus between 0\u00b0 and 90\u00b0.\nProperty 4. The forward gradient gu maps Vf from Rn onto the one-dimensional subspace spanned by v \u2260 0. If v is a unit vector, gu is a projection.\nProof. Per Definition 1,\ngv = (\u2207 f\u00b7\u03c5)\u03c5 (17)\nwhere the directional derivative \u2207fv is a scalar and v \u2208 span{v}. Thus, gv \u2208 span{v}. If v is a unit vector, e.g., ||v||2 = 1, gv is a projection:\n(gr \u00b7 v)v = ((\u2207f \u00b7 \u03c5)\u03c5\u00b7 \u03c5)v = (\u2207 f \u00b7 v) (v \u00b7 \u03c5) \u03c5 = (\u2207f \u00b7 v)v = gv. (18)\n=1\nProposition 1. The normalized dot-product Sc(x,v) for a vector x \u2208 Rn, n > 2 and a random vector v ~ N(0, In) has expectation zero, variance $\\frac{2}{n \\pi}$, and approaches the normal distribution N (0,$\\frac{1}{\\sqrt n}$) for large n.\nProof. Since N(0, In) is rotation invariant, i.e., every direction has the same probability, the distribution of angles to v ~ N(0, In) is the same for any vector x \u2260 0 and the norm ||0||2 is independent of the direction $\\frac{\\pi}{7}$. Let us therefore consider x = (1,...,1) with ||x||2 = \u221an. Let further [v]; be the i-th entry of v and since v ~ N(0, In), the marginal distributions [v]i are iid with [v]i ~ N(0,1). Then\nSc(x,v) = $\\frac{\\sum_{i=1}^n[v]_i}{\\|x\\|_2\\|v\\|_2} = \\frac{\\sum_{i=1}^n[v]_i}{\\sqrt n\\|v\\|_2}$ (19)\nwith\nE [Sc(x, v)] E [$\\frac{\\sum_{i=1}^n[v]_i}{\\sqrt n\\|v\\|_2}$] $\\frac{1}{\\sqrt n}E \\sum_{i=1}^n$\\frac{[v]_i}{\\|v\\|_2} = \\frac{1}{\\sqrt n} \\sum_{i=1}^nE \\frac{[v]_i}{\\|v\\|_2}$ =$\\frac{1}{\\sqrt n}$ \\sum_{i=1}^n 0 =0 (20)"}, {"title": "A.2 Detailed Derivation of the Forward Gradient Time Complexity", "content": "Evaluating f and computing k forward-mode AD passes requires\nw.ops(f) with w < 1 + 1.5k (42)\ntime, where ops(f) is the time required for evaluating f, i.e., inference (Griewank and Walther, 2008). The time complexity is thus O(kops(f)).\nComputing the single-tangent forward gradient gu performs one forward-mode AD pass (k = 1) and multiplies the resulting scalar directional derivative \u2207 fv with the n-dimensional tangent v. This results in a complexity of\nTg \u2208 O(ops(f) + n) = O(ops(f)), (43)\nwhich is dominated by ops(f) since f needs to at least consider each of its n parameters once, and thus O(n) \u2286 O(ops(f)). We use T, to indicate the time required to compute an approximate gradient g.\nThe multi-tangent forward gradients gt require k forward-mode AD passes with time complexity O(kops(f)) to compute a vector V\u00af\u2207f of k directional derivatives, followed by multiplication with the n\u00d7 k tangent matrix V. This results in a complexity of\nTg+ \u2208 O(kops(f) + nk) = O(k \u00b7 ops(f)). (44)\nUsing gv instead of g+ corresponds to multiplying go by the scalar , which is again dominated by the other terms, and thus\nTgv \u2208 O(kops(f)). (45)\nCorrecting the forward gradient with orthogonal projection Pu (\u2207f) requires the additional computation of, and multiplication by, the k \u00d7 k inverse Gram matrix (VTV)-1. This results in an overhead of O(k2n) for the Gram matrix VTV, O(k\u00b3) for its inversion, and O(k\u00b3) for the multiplication with the vector of directional derivatives VTVf. Since Pu (Vf) is exact for all k \u2265 n, there is no advantage to using k > n, and we can thus assume k\u2264n. This simplifies the overhead to\nTPU (Vf) - Tg+ \u2208 O(k\u00b2n) + O(k\u00b3) + O(k\u00b3) = O(k\u00b2n), (46)\nresulting in an overall complexity of\nTPv (\u2207f) \u2208 O(k\u00b7 ops(f) + k2n). (47)\nFor small, constant k as used for most of our experiments, this overhead is negligible compared to the forward-mode AD passes in O(kops(f)). However, it must be considered for arbitrarily large k, where it can limit scalability."}]}