{"title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation", "authors": ["Shota Onohara", "Atsuyuki Miyai", "Yuki Imajuku", "Kazuki Egashira", "Jeonghun Baek", "Xiang Yue", "Graham Neubig", "Kiyoharu Aizawa"], "abstract": "Accelerating research on Large Multimodal Models (LMMs) in non-English languages is crucial for enhancing user experiences across broader populations. In this paper, we introduce JMMMU (Japanese MMMU), the first large-scale Japanese benchmark designed to evaluate LMMs on expert-level tasks based on the Japanese cultural context. To facilitate comprehensive culture-aware evaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA) subset, where the culture-independent subjects (e.g., Math) are selected and translated into Japanese, enabling one-to-one comparison with its English counterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly crafted subjects that reflect Japanese cultural context. Using the CA subset, we observe performance drop in many LMMs when evaluated in Japanese, which is purely attributable to language variation. Using the CS subset, we reveal their inadequate Japanese cultural understanding. Further, by combining both subsets, we identify that some LMMs perform well on the CA subset but not on the CS subset, exposing a shallow understanding of the Japanese language that lacks depth in cultural understanding. We hope this work will not only help advance LMM performance in Japanese but also serve as a guideline to create high-standard, culturally diverse benchmarks for multilingual LMM development.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs) have revolutionized the field of language processing (Chen et al., 2023a; vic, 2023; Touvron et al., 2023; Wei et al., 2023). Building on the success of LLMs, large multimodal models (LMMs) have demonstrated remarkable performance across tasks ranging from common sense reasoning to domain-specific, expert-level challenges (Antol et al., 2015; Liu et al., 2023a, 2024c; Yue et al., 2024). As their capabilities grow, the need for robust criteria to evaluate LMMs has become increasingly important, highlighting the role of comprehensive benchmarks in assessing the full scope of their abilities.\nHowever, current benchmarks focus primarily on performance in English (Liu et al., 2024c; Yue et al., 2024; Li et al., 2024b; Liu et al., 2023b; Yu et al., 2024; Fu et al., 2024), with less emphasis on evaluation in other languages. Given that LMMs are widely used across diverse languages, it is imperative to evaluate their performance beyond English. Additionally, such multilingual evaluations should actively involve contributions from diverse communities, ensuring that the associated cultural contexts are appropriately considered.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first benchmark designed to evaluate LMMs on extensive, multi-disciplinary tasks in Japanese that require college-level subject knowledge, deliberate reasoning, and cultural understanding. The overview of JMMMU is shown in Figure 1. JMMMU draws inspiration from the well-established MMMU (Yue et al., 2024) and expands existing culture-aware Japanese benchmarks (Inoue et al., 2024b; SakanaAI, 2024c) by over 10 times, with 1,320 questions using 1,118 images, covering a diverse range of subjects.\nJMMMU offers two key subsets: (i) Culture-Agnostic (CA) Subset: We extracted and translated the culture-agnostic components from MMMU. This subset allows for a direct comparison of the performance gaps between English and Japanese that are purely attributable to language variations. (ii) Culture-Specific (CS) Subset: We carefully crafted brand-new questions that align with the Japanese cultural context. With CS subset, developers can assess capabilities specifically tailored to Japanese culture. Together, JMMMU serves as a diagnostic tool for model developers, providing valuable feedback for future improvements.\nEvaluating 15 open-source LMMs and three advanced proprietary LMMs on JMMMU, our key findings are summarized as follows:\n\u2022 Overall performance is up to 58.6%, leaving great room for improvement in the utility of the Japanese context.\n\u2022 The CA subset reveals that most models perform worse when asked in Japanese than in English (up to 8.6%), even when the question asks exactly the same content. This apple-to-apple comparison clearly indicates that the utility in non-English languages is falling behind in current LMMs.\n\u2022 The CS subset reveals that models trained on Japanese datasets perform the best among open-source models, suggesting that such fine-tuning effectively contributes to incorporating Japanese cultural knowledge into the models.\n\u2022 Combining both subsets, we reveal a significant discrepancy among the state-of-the-art proprietary models. While they perform similarly on English benchmarks and even on culture-agnostic questions in Japanese, their performances are significantly different on CS subset. This finding is particularly alarming, as it indicates that evaluation exclusively on a translation-based benchmark could risk overestimation of an LMM's multilingual capability without truly understanding the context of the individual cultures.\nOur findings indicate that English-centered performance evaluation may lead to biased development, neglecting non-English languages. We hope our findings not only spark interest in Japanese performance but also motivate the community to craft a variety of high-standard benchmarks that encompass diverse cultures and their associated languages, thereby promoting more inclusive LMM development."}, {"title": "2 Related Work", "content": "Large Multimodal Models (LMMs) Following the success of large language models (LLMs), many LMMs have been developed with improved knowledge and instruction-following capabilities (Liu et al., 2023b, 2024a,b; Li et al., 2024a; Ye et al., 2024; Zhao et al., 2023; Li et al., 2023; Monajatipoor et al., 2023; Zhao et al., 2024). However, the progress of these models is typically evaluated on English benchmarks (Yue et al., 2024; Liu et al., 2024c). Therefore, a significant challenge remains in accurately evaluating the capabilities of other languages, highlighting the need for non-English benchmarks.\nLMM Benchmarks Among various recent benchmarks (Li et al., 2024b; Liu et al., 2023b, 2024c; Lu et al., 2024; Yue et al., 2024; Miyai et al., 2024), MMMU (Yue et al., 2024) is the most widely used to measure the advancements of cutting-edge LMMs. MMMU requires advanced university-level knowledge and reasoning across a broader range of subjects, enabling a more comprehensive and expert-level evaluation. Subsequently, CMMMU (Zhang et al., 2024) has been proposed as its Chinese counterpart. While CMMMU comprises entirely new culture-specific questions, our JMMMU has not only culture-specific subjects but also translation-based culture-agnostic subjects, facilitating one-to-one comparisons between English and Japanese using the exact same questions. In line with multilingual ability evaluation, several VQA benchmarks have been proposed (Gao et al., 2015; Changpinyo et al., 2022; Gupta et al., 2020; Liu et al., 2021; Pfeiffer et al., 2021; Tang et al., 2024; Romero et al., 2024). However, unlike the MMMU series, their primary focus is on daily knowledge, (e.g., Pop Culture, Sports in CVQA (Romero et al., 2024)), still leaving the multilingual expert-level reasoning skills as an important direction for future work.\nJapanese LMM Benchmarks The development of Japanese LMM benchmarks remains behind that of English benchmarks. While efforts have been made to create Japanese benchmarks as shown in Table 1, they still exhibit the following critical limitations: (i) Existing benchmarks (Shimizu et al., 2018; Turing, 2024c,b; Inoue et al., 2024b; SakanaAI, 2024c,a) focus primarily on common sense knowledge but do not adequately address expert-level knowledge, despite the advancement in LMMs and the importance of evaluating such capabilities. (ii) Many do not account for cultural differences. They are often created by directly translating existing English benchmarks (Shimizu et al., 2018; Turing, 2024c,b), resulting in questions that may feel unfamiliar to Japanese people due to cultural context. (iii) Although recent benchmarks attempt to consider cultural differences (Inoue et al., 2024b; SakanaAI, 2024c,a), they are limited in size (up to 102 questions), raising concerns about the reliability of quantitative evaluation. Our proposed JMMMU addresses all three of the aforementioned challenges, significantly advancing the benchmark in the realm of Japanese evaluation."}, {"title": "3 JMMMU Benchmark", "content": "As illustrated in Figure 1, JMMMU contains a total of 1,320 questions and 1,118 images, covering 28 different subjects. This benchmark is strategically divided into two distinct categories: culture-agnostic and culture-specific subjects.\nCulture-agnostic subset consists of 24 subjects with 720 questions across five disciplines: (1) Art & Psychology, (2) Business, (3) Health & Medicine, (4) Science, and (5) Tech & Engineering. Culture-specific subset consists of 600 questions across four subjects: (1) Japanese Art, (2) Japanese Heritage, (3) Japanese History, and (4) World History. We provide sample questions in Appendix E"}, {"title": "3.2 Data Curation Process", "content": "JMMMU is derived from the widely-used validation set of MMMU, consisting of 900 questions across 30 subjects. To construct JMMMU, we first examined the cultural dependencies in the original MMMU subjects. For culture-agnostic subjects, we translated the questions into Japanese. We further replaced culture-dependent subjects with new subjects that are conceptually similar, but better aligned with the Japanese context. All the process has been conducted with the help of 19 university students, including the authors, who have expert knowledge in the respective fields and native fluency in Japanese. Here, we describe the dataset creation process in detail.\nExamining Cultural Dependencies in MMMU Among the 30 subjects in MMMU, we identified that questions in six subjects are particularly unfamiliar to Japanese people and thus we categorized them as culture-specific subjects; Art, Art Theory, Geography, History, Literature, and Sociology. The remaining subjects (e.g., Biology, Chemistry, Computer Science, Electronics) exist in Japan with similar contents, and thus we categorized them as culture-agnostic subjects. As a result, we excluded the six culture-specific subjects while keeping the remaining 24 culture-agnostic subjects in JMMMU."}, {"title": "3.3 Comparison with Other Japanese Multimodal Benchmarks", "content": "Here, we compare JMMMU with other Japanese multimodal benchmarks, provided in Table 1, to demonstrate its uniqueness. First and foremost, JMMMU is the only benchmark that includes expert-level questions, while the rest of the benchmarks (Shimizu et al., 2018; Turing, 2024a; SakanaAI, 2024a,c; Inoue et al., 2024b) are focused on common knowledge. Further, JMMMU is carefully designed to take the Japanese cultural context into account. While some existing benchmarks consider Japanese culture, they are all limited in size (only up to 102 questions in Inoue et al. (2024b)), raising concerns about whether reliable quantitative evaluations can be conducted. In contrast, JMMMU contains more than 10 times larger than any of the existing culture-aware benchmarks."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Setup", "content": "LMMs We evaluate a diverse set of LMMs.\n\u2022 Proprietary LMMs: GPT-40 (OpenAI, 2024) Gemini 1.5 Pro (DeepMind, 2024; Reid et al., 2024) and Claude 3.5 Sonnet (Anthropic, 2024).\n\u2022 Japanese LMMs: LLaVA CALM2 (Inagaki, 2024) and EvoVLM JP v2 (Inoue et al., 2024a), which are trained on both English and Japanese datasets.\n\u2022 Open-source LMMs: LLaVA-OneVision 0.5B & 7B (Li et al., 2024a), LLaVA1.6-13B & 34B (Liu et al., 2024b), Phi-3 & 3.5 Vision (Abdin et al., 2024), InternVL2-2B &\n8B (Chen et al., 2023b), xGen-MM (Xue et al., 2024), Idefics2-8B (Lauren\u00e7on et al., 2024b), Idefics3-8B (Lauren\u00e7on et al., 2024a),"}, {"title": "4.2 Main Result", "content": "Table 2 demonstrates the evaluation results on our JMMMU benchmark. We provide the average scores across all subjects, culture-agnostic (CA) subjects, and culture-specific (CS) subjects, as well as scores on individual subjects. For comparison, we also provide the performance on CA suset in English CA (EN). Note that CA (EN) is often smaller than the overall average of MMMU given by Yue et al. (2024) because subjects selected as CA are relatively difficult among all subjects in MMMU as it often requires stronger reasoning capabilities (e.g., Math).\nHere, we summarize our key observations.\nChallenging Nature In our experiment, the performance is up to 40.5% for open-source, and 58.6% for proprietary models, leaving great room for improvement. This also highlights a significant gap between open-source and proprietary models, presenting a more difficult challenge for open-source models."}, {"title": "5 Analysis", "content": null}, {"title": "5.1 Ablation on Image Translation", "content": "Here, we investigate how translating text and images affects the model performances. Using 360 questions from the culture-agnostic subset which involved translation of both texts and images, we compare the scores in English ($I_{en}T_{en}$), when only text is translated ($I_{en}T_{jp}$), and when both text and images are translated ($I_{jp}T_{jp}$). We provide scores for selected models in Table 3 and the full set in Appendix B.2. Many models experience a drop in scores by text translation, with further degradation observed when images are also translated (i.e., 0 > \u03941 > \u03942). However, some models exhibit different performance trends, showing a drop by text translation but an improvement by translating both (i.e., \u03941 < 0 < \u22062), or vice versa. Overall, while the trends are complex, our result indicates that text-only translation, as is done in many non-English benchmarks, could result in a biased performance evaluation. Rigorous investigation on this point is left for future work."}, {"title": "5.2 Errors in Culture-agnostic Subjects", "content": "JMMMU shares 720 culture-agnostic questions with MMMU, which allows us to compare the output one by one. Using these questions, we evaluate how translation affects model performance. Taking GPT-40 as an example, we classify the responses into four categories based on whether they are correct or incorrect in each language. Figure 4 presents the results before and after translation. The results on the other models are provided in Appendix B.1 While GPT-40 performs similarly in both languages on culture-agnostic split (only 0.3% difference in Table 2), we have found that there are a significant amount (28.6%) of questions to which it answered correctly only in either one of the languages. We now investigate this phenomenon. For questions answered correctly only in English (orange in Figure 4(a)), we observe simple performance degradation after translation. In contrast, we have found some distinctive examples in the opposite case (yellow). In an example of Figure 4(b), GPT-40 outputs only the direct answer in English, whereas in Japanese, the model includes the reasoning process in its response although the model is instructed to generate the choice directly by using the prompt in Section 4.1. For a fair comparison with MMMU (Yue et al., 2024), we count a response to be correct as far as the model's response is accurate and can be parsed by a rule-based algorithm, regardless of its instruction-following ability. As a result, the scores can sometimes be counterintuitively overestimated due to the lack of instruction-following skills in Japanese. While the primary focus of JMMMU is on evaluating expert knowledge and supporting the improvement of such capabilities, our findings highlight a crucial direction for future work: measuring and enhancing instruction-following ability in non-English languages."}, {"title": "5.3 Errors in Culture-specific Subjects", "content": "This section presents an analysis of the tendency of GPT-40's errors in the culture-specific subjects. To investigate the causes of these errors, we manually review GPT-40's responses and classify the errors into four categories: (i) Lack of Knowledge, where the model successfully extracts the necessary information from the image but lacks the culture-specific knowledge required to produce a correct answer, (ii) Image Recognition Errors, where it fails to correctly interpret the image during the visual understanding stage, (iii) Answer Rejection, where it declines to provide an answer, and (iv) Textual Misunderstanding, where the response is not aligned with the question. The overall distribution of these error types is shown in Figure 5. Lack of Knowledge is the overwhelming majority at over 50%, indicating that culture-specific knowledge is the most critical requirement to achieve high performance in JMMMU. In this section, we discuss notable examples for each error category.\nLack of Knowledge (53.8%) Figure 6(a) shows an example of an error in Japanese Heritage. Here, GPT-40 correctly recognizes Shuri Castle in the image but fails to provide the related contextual knowledge. Similar cases have been observed in Japanese Art, where GPT-40 correctly answers the name of the artwork but is unable to specify the era in which it was created.\nImage Recognition Errors (30.8%) Figure 6(b) shows an example of an image recognition error of a question. Here, GPT-40 mistakes the image of Sado Island for Ishigaki Island, and it answers the famous animal in Ishigaki (correctly if the image was indeed Ishigaki).\nAnswer Rejection (10.6%) This type of error is particularly evident in Japanese History and World History, where GPT-40 declines to answer questions requiring the identification of historical figures from images. In Figure 6(c), GPT-40 responds that it is unable to identify the person in the image (Hideyo Noguchi), resulting in a failure to select the option associated with him. We hypothesize this is due to their strong privacy awareness to avoid giving private information (Wang et al., 2024), even when the question asks for information that is widely known about a historical figure.\nTextual Misunderstanding (4.8%) There are rare instances where GPT-40 provides an incorrect response despite correctly identifying the content of the image. For example, in Figure 6(d), GPT-40 accurately names the title of the artwork, but its answer does not correspond to the question."}, {"title": "6 Conclusion", "content": "We propose JMMMU, a benchmark designed to comprehensively evaluate the expert-level knowledge, reasoning abilities, and understanding of Japanese culture. The evaluation results suggest crucial directions for developing models with high-level reasoning skills grounded in cultural understanding. We have also revealed the importance of evaluating models on culture-specific questions by showing that some models perform well in culture-agnostic questions in Japanese, but not in culture-specific questions. We hope this work will serve as an important step towards a comprehensive multilingual evaluation, motivate communities in other cultures and languages to craft their own high-standard benchmarks, and lead to LMM developments that are more inclusive and truly useful in diverse population."}, {"title": "Limitations", "content": "Throughout our experiment and extensive analysis, we have shown a number of critical directions of improvement in multilingual benchmarks and model developments. While they are outside of the scope of this paper, they are left as important directions for future work, and thus we summarize them here:\nSubject Set Expansion While JMMMU can assess the latest LMMs' expert-level skills, it cannot evaluate model performance on subjects outside of those currently covered. As models gain more knowledge and improve their reasoning abilities, it will be necessary to expand the range of subjects and include more challenging questions.\nBenchmarks in Other Cultures Since JMMMU only covers the Japanese, evaluating model performance in other languages and cultural contexts remains an important area for future work. We hope these efforts will help mitigate the underrepresentation of diverse cultures and languages.\nInstruction Following Ability in Japanese In Section 5.2, we have shown a gap in instruction-following ability between languages and that models go against the instruction and generate their reasoning more often in Japanese. While the primary focus of our benchmark is on evaluating expert knowledge and thereby helping improve such skills, it is left as an important future work to improve the instruction-following ability in Japanese. Further, it is also important to design an evaluation protocol to measure instruction-following ability to enhance the development of such skills. While there are some methods to evaluate the model's instruction-following ability (Zhou et al., 2023; Qian et al., 2024), these should be appropriately incorporated in the context of multilingual performance evaluation."}, {"title": "Appendix", "content": null}, {"title": "A LMMs' Japanese Support", "content": "To discuss the multilingual capabilities of LMMs, we summarize whether each model officially supports Japanese. Table A presents the Japanese language support status for each model. \"/\" indicates official support for Japanese, while \"X\" indicates the absence of such support. Also, we denote \u201c?\u201d for models of which we could not find the information.\nEven if a model is marked as \"X\", it may still demonstrate some Japanese language capability due to the presence of Japanese data in publicly available datasets like ShareGPT-4V (Chen et al., 2024) and ShareGPT-402, or data crawled from the web.\nProprietary commercial models, such as GPT-40, Gemini 1.5 Pro, and Claude 3.5 Sonnet, do not publicly disclose detailed information about their training data. However, based on their release blog posts, it can be inferred that these models support many languages, including Japanese.\nLLaVA CALM2 is based on the Japanese LLM CALM23, and it has been trained using Japanese multimodal datasets, officially supporting Japanese. EvoVLM JP v2, a merged model (Akiba et al., 2024), also incorporates Japanese data for optimization and is officially released as a Japanese LMM.\nPhi-3.5 Vision does not officially support Japanese, despite its base model, Phi-3.5, having official support for multiple languages, including Japanese. Phi-3 Vision, likewise, does not support non-English languages.\nIn the LLaVA series, LLaVA-OneVision explicitly mentions support for Chinese in its training but does not extend this to other non-English languages. However, Qwen2, the base LLM for the LLaVA-OneVision models, officially supports Japanese. LLaVA-1.6 models are trained from different base LLMs, such as Vicuna v1.5 and Nous Hermes 2 Yi, neither of which officially support Japanese. Thus, Japanese language capabilities are not guaranteed in their visual instruction training.\nInternVL and its base model, InternLM2, officially support only English and Chinese. Similarly, CogVLM2 claims proficiency in both English and Chinese, with no explicit mention of Japanese support.\nIdefics2, Idefics3, xGen-MM, and Mantis use large-scale datasets for multimodal training. However, there is no clear evidence of Japanese data inclusion, and in some datasets, such as OBELICS (Lauren\u00e7on et al., 2023), non-English data is explicitly filtered out. While Llama 3, the base model for some of these LMMs, mentions multilingual training, it does not explicitly confirm support for Japanese. Mistral v0.1 also does not disclose its training data.\nThe performance of these models depends on a complex interplay of factors, including the quantity and quality of the training data and the size and capabilities of the base language model. Official support for Japanese is not the only consideration; there are reports of models trained on English-only multimodal data generalizing to other languages (Hu et al., 2024), including Japanese. Moreover, since many models are designed with Chinese support, the cultural and linguistic proximity between Japanese and Chinese-speaking regions may result in a high performance in Japanese."}, {"title": "B More Result", "content": null}, {"title": "B.1 Error Analysis in Culture-Agnostic subjects", "content": "In Section 5.2, we present the error analysis for GPT-40 on the CA subjects. In this section, we provide the error analysis for all models. While we have shown in Table 2 that most models perform worse in Japanese, there are some amount of questions where the model answers correctly only in Japanese for every model. The number of such questions is particularly high for LLaVA-OV 0.5B and InternVL2 2B. This occurrence, however, appears to be a random phenomenon, likely attributable to the overall weaker performance of these models."}, {"title": "B.2 Ablation on Image Translation", "content": "The full set of Table 3 is presented in Table B. As discussed in Section 5.1, each model reacts differently as the translation proceeds, and the tendency is difficult to summarize. Notably, here, GPT-40 shows a 7.2% improvement in score after text translation. This partly stems from its weak instruction-following skills in Japanese, as discussed in Section 5.2, which allows it to infer answers more easily. Note that our experiment here has been conducted by using questions that involved translation of both texts and images. Many of them consist of table data, which requires stronger reasoning based on data processing, so the result may vary when investigating different data types that do not exist in the CA subset of JMMMU."}, {"title": "B.3 Score Correlation between languages", "content": "Using the culture-agnostic subset, we have demonstrated in Section 4.2 that (i) models perform worse in Japanese and (ii) Japanese LMMs show robustness to translation. To illustrate these points, we provide Figure B."}, {"title": "C Further Experimental Details", "content": null}, {"title": "C.1 Experimental Setup", "content": "Computing Infrastructures We conduct all our evaluations of open-source models on a single NVIDIA A100 (80GB) GPU.\nParameters for LMM Inference A maximum output length is set to 1,024 and a temperature is set to 0 for all models during inference."}, {"title": "C.2 Evaluation Protocol", "content": "Answer Extraction in Multiple Choice Question While the models are instructed to answer their choice directly, they often generate some contextual information or unnecessary symbols. To tackle this point, following MMMU (Yue et al., 2024), we extract an answer from the model response with a rule-based method. For multiple-choice questions, this parser can extract the model's choice even when the choice is surrounded by some symbol (e.g., '(A)', 'A.', 'A ') or by text.\nFor example, these answers, which are all some variants of \"The answer is A.\" in Japanese, can be parsed as \"A\":"}, {"title": "D Annotation Instruction", "content": "Recruitment and Payment Annotators were paid at least the minimum wage set in Japan, according to the time spent on the task.\nData Consent They were informed that translated data would be used for evaluation purposes.\nInstructions Given to Participants The document containing the instructions presented to the annotators is shown in Figure C."}, {"title": "E Examples", "content": "We provide sample questions from culture-agnostic subset in Figure D, and questions from culture-specific subset in Figure E"}]}