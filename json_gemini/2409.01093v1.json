{"title": "DS MYOLO: A Reliable Object Detector Based\non SSMs for Driving Scenarios", "authors": ["Yang Li", "Jianli Xiao"], "abstract": "Accurate real-time object detection enhances the safety of\nadvanced driver-assistance systems, making it an essential component in\ndriving scenarios. With the rapid development of deep learning technol-\nogy, CNN-based YOLO real-time object detectors have gained significant\nattention. However, the local focus of CNNs results in performance bot-\ntlenecks. To further enhance detector performance, researchers have in-\ntroduced Transformer-based self-attention mechanisms to leverage global\nreceptive fields, but their quadratic complexity incurs substantial compu-\ntational costs. Recently, Mamba, with its linear complexity, has made sig-\nnificant progress through global selective scanning. Inspired by Mamba's\noutstanding performance, we propose a novel object detector: DS MY-\nOLO. This detector captures global feature information through a simpli-\nfied selective scanning fusion block (SimVSS Block) and effectively inte-\ngrates the network's deep features. Additionally, we introduce an efficient\nchannel attention convolution (ECAConv) that enhances cross-channel\nfeature interaction while maintaining low computational complexity. Ex-\ntensive experiments on the CCTSDB 2021 and VLD-45 driving scenarios\ndatasets demonstrate that DS MYOLO exhibits significant potential and\ncompetitive advantage among similarly scaled YOLO series real-time ob-\nject detectors.", "sections": [{"title": "Introduction", "content": "In recent years, the rapid development of deep learning has continuously injected\nnew energy into the field of object detection. In autonomous driving scenarios,\nreal-time detection and accurate identification of traffic signs and vehicle iden-\ntities are crucial for enhancing the safety of driving systems[1]. However, in\ndriving scenarios, targets often vary significantly in scale and size, leading to\npoor visual features and susceptibility to noise interference. This makes object\ndetection one of the most challenging tasks in autonomous driving. CNNs, with\ntheir parameter sharing and optimized hardware acceleration, have made sig-\nnificant progress in real-time object detectors. However, their local focus makes\nit difficult to effectively capture targets of different scales in driving scenarios,\nlimiting their performance. Therefore, developing a high-performance real-time\nobject detector is an important and meaningful endeavor."}, {"title": "Related works", "content": "With the rapid development of autonomous driving, developing real-time and\nefficient object detectors is crucial for real-world applications. To balance speed\nand accuracy, researchers have dedicated significant time and effort to developing\nefficient object detectors. Among these, the YOLO series models have garnered\nwidespread attention due to their simple structure and end-to-end detection\ncharacteristics. Starting from the initial YOLOv3[9], the architectural design of\nbackbone-neck-head networks has been a key factor in enhancing model per-\nformance. YOLOv4[13]], based on CSPNet[32], optimized the previously used\nDarkNet backbone structure[9] and introduced a series of data augmentation\nmethods[13][33]. YOLOv5[14] incorporated strategies such as adaptive anchor\nbox computation and automated learning rate adjustment. YOLO-X[10] em-\nployed a label assignment strategy (SimOTA) and introduced a decoupled head\nto further improve training efficiency and detection performance. YOLOv6[15]\nintegrated re-parameterization methods into the YOLO architecture to balance\naccuracy and speed. YOLOv7[11] introduced the Extended Efficient Layer Ag-\ngregation Network (E-ELAN) as the backbone to further enhance performance.\nYOLOv8[12] focused on analyzing the shortcomings of previous YOLO models\nand achieved higher performance by integrating their strengths. Gold-YOLO[34]\nproposed the GD mechanism to improve multi-scale object fusion performance.\nYOLOv9[16] introduced the GELAN backbone and enhanced the model's expres-\nsive capabilities through PGI. YOLOv10[19] proposed a dual-label assignment\nstrategy without NMS, improving the overall efficiency of the model."}, {"title": "Transformer-base object detection", "content": "Transformers [35], with their self-attention mechanism, excel in addressing long-\nrange dependency issues. DETR[20] was the first to apply the Transformer ar-\nchitecture to object detection, simplifying the pipeline by eliminating manu-\nally designed anchor boxes and NMS components, garnering significant atten-\ntion. However, DETR's training convergence remains inefficient. Subsequently,"}, {"title": "SSMs-Based Vision State Space Model", "content": "Recently, Mamba[25][26] has garnered significant attention for its linear com-\nplexity in addressing long-range dependency problems. Subsequently, Vision\nMamba[27] was the first to apply the SSM to visual backbone networks, achiev-\ning performance comparable to, or even surpassing, Vision Transformers (ViT).\nVMamba[44] introduced the Cross-Scan Module (CSM) to capture the global\nreceptive field, enhancing visual representation with linear computational com-\nplexity. LocalMamba[45] proposed a local scanning strategy to strengthen fea-\nture dependencies within local windows while maintaining a global perspective.\nEfficient VMamba[29] combined efficient selective scanning with convolution in\nthe backbone, achieving a balance between accuracy and efficiency. MambaOut[46]\nexplored the necessity of SSM in visual tasks, experimentally validating SSM's\nhigher value for tasks with long sequences and autoregressive characteristics,\nand providing foundational support for downstream tasks like segmentation.\nMSVMamba[47] introduced a multi-scale scanning mechanism, enhancing the\nability to learn dependencies across different resolutions. Inspired by Mamba's\noutstanding contributions to various visual tasks, we integrated the SSM mod-\nule into our network's feature fusion, achieving significant performance enhance-\nment."}, {"title": "Method", "content": "The overall architecture of DS MYOLO is illustrated in Figurel. In the back-\nbone network, the Stem is composed of SC, batch normalization, and a SiLU\nactivation function, stacked sequentially and downsampled twice, resulting in a\n2D feature map with dimensions $(\\frac{H}{4},\\frac{W}{4})$, and $C_1$ channels. To effectively ex-\ntract rich features in the backbone network, ECAConv is used for downsampling"}, {"title": "Fusion Layer Based on SimVSS Block", "content": "The traditional YOLO model transmits features extracted by the backbone net-\nwork directly to the neck network for feature communication. While this method\neffectively enhances the salience of local features, it overlooks the feature depen-\ndencies within the global receptive field. Previous research has demonstrated\nthat increasing the receptive field can beneficially enhance model performance.\nGiven the larger feature map size of shallow networks, we employ a simplified\nSimVSS Block based on SSM to process the output features of the backbone net-\nwork. The fused global features are then subjected to nonlinear transformations\nthrough a forward network to improve the model's fitting capacity.\nThe structure of SimVSS Block is illustrated in Fig.2. The primary design\nis based on the SSM and a feedforward network, with residual connections and\nnormalization layers included to stabilize gradient training and accelerate model\nconvergence. A traditional SSM can be viewed as a linear time-invariant system\nfunction that maps a univariate sequence $x(t) \\in \\mathbb{R}$ to an output sequence $y(t) \\in\n\\mathbb{R}$ via an intermediate hidden state $h(t) \\in \\mathbb{R}^N$. Given the state transition matrix\n$A\\in\\mathbb{R}^{N\\times N}$ as the evolution factor, the weight matrix and the observation matrix\n$B, P\\in \\mathbb{C}^N$ as projection factors respectively, and the skip connection defined as\n$Q\\in \\mathbb{C}^1$, the mathematical formulation is as follows:\n$h'(t) = Ah(t) + Bx(t)$ \n(1)"}, {"title": "ECACony and ECACSP Module", "content": "Previous studies[21][22] have shown that standard convolutions lack attention to\nchannel salience. Inspired by ECA[22], we propose a novel Efficient Channel At-\ntention Convolution (ECAConv), as illustrated in Fig.3. Specifically, we perform"}, {"title": "Experiments", "content": "We conducted extensive experiments on the publicly available traffic\nsign detection dataset CCTSDB 2021[30] and the vehicle logo detection dataset\nVLD-45[31] to validate the effectiveness of the proposed object detector. Notably,\nthe CCTSDB 2021 dataset includes three categories, each consisting of multi-\nscale targets from real traffic scenes under different lighting conditions. The\nVLD-45 dataset comprises 45 categories of large vehicle logos collected from the\ninternet using web crawlers. To ensure a fair comparison, we followed the dataset\ndivision methods provided in CCTSDB 2021 and VLD-45.\nImplementation Details: We conducted experiments using a single NVIDIA\n4090 GPU within the PyTorch framework. All experiments were trained from\nscratch for 200 epochs without using pre-trained weights, with a 3-epoch warm-\nup period. We used the SGD optimizer, setting the initial learning rate to de-\ncrease from 0.01 to 0.0001 and the momentum to 0.937. The input size was fixed\nat 640 \u00d7 640, and the batch size was set to 16. Our data augmentation strategies\nincluded random scaling, translation, and Mosaic[13], with Mosaic data augmen-\ntation being disabled during the last 10 epochs."}, {"title": "Comparison with state-of-the-arts", "content": "In this section, we compare the proposed DS MYOLO with other latest state-of-\nthe-art real-time detectors in the YOLO series, including YOLOv5[14], YOLOv6[15],\nYOLOv7[11], YOLOv8[12], Gold YOLO[34], YOLOv9[16], and YOLOv10[19].\nWe primarily measure model parameters (M), FLOPs(G), mAP(%), detection\nbox precision, and recall rate.\nAs shown in Table.1, we compared different versions of DS MYOLO (-N/-S/-\nM) with the latest YOLO series real-time detectors on CCTSDB 2021. Overall,\nDS MYOLO models excelled in multiple metrics. In the lightweight models,\nDS MYOLO-N achieved a 52.22% mAP with 4M parameters and 9G FLOPs,"}, {"title": "Ablation Studies", "content": "In this section, we perform a series of ablation studies on the proposed DS\nMYOLO using the CCTSDB 2021 dataset. To further validate the effectiveness of\nDS MYOLO, we take DS MYOLO-N as an example and independently examine\neach of its major modules, focusing on Params (M), FLOPs (G), and mAP (%).\nTo facilitate observation of the impact of each module on the overall model\nperformance, all models are trained for 80 epochs to amplify the differences.\nAs shown in Table.3, ECAConv significantly improved the mAP by 1.14%\nwith similar parameter and computational costs, demonstrating the enhance-\nment of model performance through the incorporation of local inter-channel\ndependencies. The addition of the SSM-based fusion layer in the SimVSS Block"}, {"title": "CAM Visualization", "content": "Fig.5 shows the CAM visualization results for YOLOv5[14], YOLOv8[12], YOLOv10[19],\nand our DS MYOLO on CCTSDB 2021[30]. It can be observed that our model\naccurately detects target locations and assigns higher weights to the detection\nareas. Additionally, our DS MYOLO is capable of focusing on targets at different\nscales, thereby reducing the false detection rate."}, {"title": "Conclusions", "content": "In this paper, we propose a novel high-performance object detector for driving\nscenarios, named DS MYOLO. The designed SimVSS Block effectively enhances\nfeature fusion in deep networks. Additionally, the proposed Efficient Channel\nAttention Convolution (ECAConv) significantly boosts cross-channel feature in-\nteractions. Extensive experiments conducted on the CCTSDB 2021 traffic sign\ndataset and the VLD-45 vehicle logo dataset demonstrate that our DS MYOLO\nachieves the highest performance among YOLO series real-time object detectors\nof comparable scale and exhibits strong competitiveness."}]}