{"title": "Addressing Multilabel Imbalance with an Efficiency-Focused Approach Using Diffusion Model-Generated Synthetic Samples", "authors": ["F. Charte", "M. A. D\u00e1vila", "M. D. P\u00e9rez-Godoy", "M. J. del Jesus"], "abstract": "Predictive models trained on imbalanced data tend to produce biased results. This problem is exacerbated\nwhen there is not just one output label, but a set of them. This is the case for multilabel learning (MLL) algorithms\nused to classify patterns, rank labels, or learn the distribution of outputs. Many solutions have been proposed in\nthe literature. The one that can be applied universally, independent of the algorithm used to build the model,\nis data resampling. The generation of new instances associated with minority labels, so that empty areas of the\nfeature space are filled, helps to improve the obtained models. The quality of these new instances depends on the\nalgorithm used to generate them. In this paper, a diffusion model tailored to produce new instances for MLL data,\ncalled MLDM (MultiLabel Diffusion Model), is proposed. Diffusion models have been mainly used to generate\nartificial images and videos. Our proposed MLDM is based on this type of models. The experiments conducted\ncompare MLDM with several other MLL resampling algorithms. The results show that MLDM is competitive\nwhile it improves efficiency.", "sections": [{"title": "Introduction", "content": "Training a machine learning (ML) model with imbalanced data [1] typically introduces bias into the outputs it\nproduces. The fewer samples there are for a given class during training, the greater the likelihood that the model\nwill make incorrect predictions. The challenges machine learning algorithms face in dealing with this problem have\nbeen widely studied and documented [2].\nAlthough this is a problem that exists in different types of data and affects disparate types of ML models, the\npresent work focuses on a particularly difficult case, namely multilabel learning (MLL). This includes tasks such as\nmultilabel classification (MLC) [3], multilabel ranking [4] and label distribution learning [5]. All of them have to\nwork with data presenting high imbalance levels [6], as well as other difficulties related to the multiple label nature\nof the data [7].\nSeveral ways to handle imbalanced data while training machine learning models can be found in the literature.\nThese methods fall into three main groups:\n\u2022 Algorithm-level tweaks: involve changing existing methods to work better with minority classes. For\nexample, we might adjust decision thresholds [8] or calibrate levels of confidence.\n\u2022 Cost-sensitive learning: means making the model pay more attention to minority classes by assigning\nhigher costs to their misclassification. This can be done through cost-sensitive boosting or by weighting classes\ndifferently during training [9].\n\u2022 Classifier Ensemble: addresses the imbalance problem by using ensembles of models [10], which are more\nrobust to this problem, instead of a single classifier.\n\u2022 Data resampling: tries to balance out the classes in the training data. This can be achieved by oversampling\nminority classes (the SMOTE approach), undersampling majority classes, or a combination of the two."}, {"title": "Related work", "content": "This section provides the necessary background to contextualize our proposal. The following subsection provides\nan overview of the multilabel learning task. Subsequently, the challenge of learning from imbalanced data and the\ndistinctive characteristics of imbalanced MLDs are elucidated. Finally, the foundation of diffusion models is outlined."}, {"title": "Multilabel Learning", "content": "Supervised machine learning models are trained on labeled data, meaning that each combination of features is\nassigned an output value. This can be continuous or discrete, resulting in regression or classification tasks. In\nthe latter case, a single label is usually assigned to each data instance. However, real-world scenarios can be more\ncomplex than binary or multiclass classification problems. One such scenario is multilabel learning (MLL) [3], i.e.\neach data point has multiple labels."}, {"title": "Learning from Imbalanced Data", "content": "Often, the data used to train classifiers is imbalanced. This means that some class labels have more instances than\nothers. To measure this imbalance, we use the imbalance ratio (IR), which is defined as the number of samples in the\nmajority class divided by the number of samples in the minority class. Metrics other than IR have been proposed in\nthe literature, but they have not been adapted to the multilabel case.\nIn binary scenarios, the IR is computed directly, as there are only two labels, one representing the majority and\nthe other representing the minority. In multiclass scenarios, individual IRs are calculated pairwise and then averaged\nto obtain the mean IR [26]. A similar procedure is followed in MLL (4) to obtain the MeanIR metric introduced\nin [6].\n\nMeanIR = \\frac{1}{\\left|Y\\right|} \\sum_{y=Y_1}^{Y_L} IR_{lbl}(y), where\n\n\nIR_{lbl}(y) = \\frac{\\sum_{y' \\in Y \\setminus \\{y\\}} h(y', Y_i)}{\\sum h(y, Y_i)} with\n\n\nh(y, Y_i) = \\begin{cases}\n1 & y \\in Y_i\\\\\n0 & y \\notin Y_i\n\\end{cases}\n\nIn addition to the way in which the IR is calculated, imbalanced MLDs manifest a number of distinctive charac-\nteristics, with the following two being of particular note:\n\u2022 High imbalance levels: In MLL, the degree of imbalance is typically much higher than in binary or multiclass\nscenarios. While a MeanIR greater than 10 is typically regarded as a high value in the context of traditional\nclassification, it is not uncommon in the domain of MLL. Many multilabel datasets exhibit MeanIR values\nbetween 30 and 100, with some extreme cases exceeding 1000. This indicates that rare labels may occur only\nonce for every several hundred instances of common labels, rendering it exceedingly challenging to train models\nthat can effectively handle these low-frequency labels. The issue can be further aggravated when certain MLL\ntechniques, such as binary relevance [27], are employed, as this results in the development of an independent\nbinary model for each label in opposition to all the others.\n\u2022 Coupling of frequent and rare labels in MLL: In MLDs, it is not uncommon for less frequent labels\nto appear in conjunction with majority labels within the same instances. This coupling, as quantified by the\nSCUMBLE (7) metric [28], introduces further complexities when attempting to address imbalance in MLL. In"}, {"title": "Oversampling Multilabel Data", "content": "The resolution of imbalanced data through resampling methods represents a common preprocessing approach in the\nfield of machine learning. Among these techniques, oversampling is typically preferred over undersampling due to\nits capacity to preserve and enhance the available information. Chawla et al. [11] introduced SMOTE, a widely\nused oversampling method that generates synthetic minority instances, effectively increasing the representation of\nrare classes without discarding valuable data. In contrast, undersampling techniques, such as random undersampling\n(RUS) as discussed in [12], entail the removal of instances belonging to the majority class, which can result in\nthe loss of potentially crucial information. A comprehensive review of various resampling methods, emphasizing\nthe advantages of oversampling in maintaining data integrity, is provided by [1]. Moreover, hybrid approaches like\nSMOTEENN [31] integrate oversampling with selective undersampling, aiming to capitalize on the strengths of both\ntechniques while mitigating their individual weaknesses. In general, the preference for oversampling stems from its\ncapacity to augment the dataset without sacrificing existing samples, thereby preserving and potentially enhancing\nthe model's ability to learn from all available information.\nThere are a number of different approaches that can be taken with regard to the implementation of a MOA. The\nfollowing ones are particularly worthy of consideration:\n\u2022 Random cloning: The replication of instances that have been assigned the minority label represents a\nstraightforward method for reducing imbalance levels. This approach is typically effective when working with\nbinary and multiclass data, as each instance of the minority class can be cloned independently (it only has one\nlabel). In MOA, however, it is necessary to consider the possibility that a sample with a minority label may\nalso be associated with one or more majority labels. Therefore, the cloning process will not necessarily result\nin a reduction of the imbalance level, and in certain instances, it may even lead to an increase. Some MOAS\nbased on cloning are LPROS and MLROS [6].\n\u2022 Synthetic generation: This represents a more sophisticated methodology. In contrast to cloning existing\ninstances, these MOAs generate new instances through the combination of feature values and labels, the\ncomputation of which is dependent on the specific method employed. The majority of these methods are\nbased on the aforementioned SMOTE, although they include specific modifications to address the particular\ncharacteristics of MLDs. Two of them are MLSMOTE [18] and MLSOL [19].\n\u2022 Labelset decoupling: The coupling of majority and minority labels in the same instances represents a\nsignificant challenge for MOAs. This approach entails the partitioning of the labelset associated with these\ninstances, resulting in the creation of two distinct subsets: one comprising solely majority labels and the\nother exclusively with minority ones, thus creating two samples from one. This straightforward technique does\nnot inherently guarantee an improvement in imbalance; however, it can be effectively integrated with other\noversampling methodologies. REMEDIAL and its variants [32,33] exemplify this solution."}, {"title": "Diffusion Models", "content": "As previously stated, SMOTE-based MOAs are dependent on NN search of seed samples. The process of com-\nputing distances between all instances in order to determine which are the nearest is inherently time-consuming. The\nruntime is determined by the number of attributes and labels. The way in which each MOA produces the synthetic\nsample attributes and labels can also exert a certain impact.\n\nq(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t I).\n\n\nP_\\theta (x_{t-1}|x_t) = N(x_{t-1}; \\mu_\\theta (x_t, t), \\Sigma_\\theta(x,t)).\n\n\nL = E_q\\Big[ D_{KL}(q(x_{t-1}|x_t) || p_\\theta (x_{t-1}|x_t)) - \\log p_\\theta (x_0)\\Big]\n\\sum_{t=1}^{T}\n\nOver the past decade, a new field of study has emerged, termed generative AI. Many of these models, including\nvariational autoencoders (VAEs) [13], generative adversarial networks (GANs) [14], normalizing flows (NMs) [15],\ndenoising diffusion probabilistic models (DDPMs) [16], and more recently, transformers [17], have been primarily\nutilized for image and text generation. As is the case with most approaches, each of these has its own set of\nadvantages and disadvantages. We choose DDPMs for a number of reasons:\n\u2022 They are able to produce higher quality data in comparison to VAEs without the issue of collapse during\ntraining.\n\u2022 Their training process is more stable than that of GANs, where convergence is frequently challenging.\n\u2022 They are not constrained by the limitations of NMs due to bijective transformations.\n\u2022 They require fewer resources and time to be trained than transformers.\nAs illustrated in Fig. 1, a DDPM starts its training process by introducing Gaussian noise into the original\ndata samples. This is an iterative process whereby a specific quantity of noise is introduced at each stage. Upon\ncompletion of the T steps, the procedure yields pure noise that follows a known Gaussian distribution with mean\nvalue of \\mu_\\theta and standard deviation of \\sigma_\\theta. A neural network is employed to determine the requisite parameters for\nthe inverse process, which involves inverting each step until the original data distribution is achieved.\nEquation (8) is in charge of the forward process. It takes the sample xt-1 in the time t -1 and adds a quantity of\nnoise to obtain the sample in time t. The quantity is controlled by the term \\beta_t, which sets the variance to add at\neach step. \\beta_tI denotes the covariance matrix of the Gaussian distribution."}, {"title": "MLDM: A Diffusion-based MOA Model", "content": "Once the background has been established, this section is devoted to describing the proposed MLDM algorithm.\nInitially, a high-level overview of its functioning is provided. Subsequently, the constituent elements of its internal\nworkings are elucidated. Finally, the computational complexity of MLDM is compared with that of other MOAS."}, {"title": "High-level overview", "content": "The main steps in training and using MLDM are illustrated in Fig. 2. The left branch of the diagram is in charge\nof generating the model. To do so, samples with at least one minority label are obtained. These are preprocessed\nand then used in the training algorithm to fit the generative model. The right side shows how synthetic samples are\ncreated from pure noise."}, {"title": "Internal workings", "content": "Having considered the foundations of MLDM as presented in the overview, the following sections will elaborate on\nthe aspects that are of particular significance within its operational context."}, {"title": "Minority samples selection", "content": "The method begins with the MLD provided as input and proceeds to select a subset (trainSubset) of samples for\ntraining the model. The decision of whether a sample is included in the subset is based on a straightforward criterion:\nit must contain one or more labels l whose \\text{IRLbl}(l) > \\text{MeanIR}, i.e. the frequency of label l in the MLD is lower than\nthe average frequency."}, {"title": "Attributes encoding", "content": "Once the trainSubset has been obtained, a one-hot encoding [34] is applied to all nominal attributes, including the\nlabels. Similarly, numeric attributes undergo a normalization process. Specifically, a quantile normalization process,\nas described in [35], is applied. This approach allows using of both discrete and continuous variables as numeric\ninputs for the training of the diffusion model."}, {"title": "Model training", "content": "The training of MLDM follows the steps described in section 2.4, common to any standard diffusion model. However,\nthere are two major differences from other proposals. The first is that we use all attributes, including labels, to train\nour model. This approach differs from other DDPMs used with tabular data, where the class is excluded from\ntraining, so that the generative model produces only the set of attributes, while the label is determined afterwards.\nOnce trained, MLDM generates complete instances, including all attributes and labels. The second difference lies in\nthe distribution used in the process, where the usual Gaussian has been changed to a multinomial distribution [36]\naiming to model the disparate distributions followed by numeric and nominal attributes. This latter decision was\ninspired by the TabDDPM model [37], but unlike it, MLDM does not separate numeric attributes from nominal ones,\nbut transforms them through one-hot encoding as explained above."}, {"title": "Synthetic samples generation", "content": "Once the model is fitted, it is used to generate the synthetic samples needed to balance the MLD. Starting with\nGaussian noise, new samples are produced until the percentage specified by the D parameter is reached. The\nattributes of each synthetic instance are adjusted through an inverse normalization process so that they return to\ntheir original range of values. The one-hot encoded labels are also processed to create the corresponding labelset."}, {"title": "Computational Complexity", "content": "The two primary objectives of the MLDM design were to generate high-quality synthetic samples and to be efficient.\nWhile the actual running times will be evaluated at a later stage, this section presents a concise comparison of the\ncomputational complexity of MLDM and other MOAs.\nFirst of all, it has to be considered whether the resampling methods to be analyzed work in a single stage, only\ndoing sample generation, or in two stages, fitting a model and then doing sample generation. Simple approaches,\nsuch as LPROS and MLROS, are very fast because they simply clone existing instances after verifying that they\ncontain some of the minority labels. The computational complexity of these methods is linear to the number of\nsamples to generate. Algorithms based on SMOTE, such as MLSOL and MLSMOTE, do not build a model, but\nwork in two steps: first, the subset of samples with minority labels is extracted; then, new samples are generated\nby NN search. If the dataset has n samples and m of them are on the minority label subset, with t attributes, the\nfirst step has complexity O(n), while the second step has complexity O(m\u00b2t) if distances between all samples are\nneeded. The latter factor must be multiplied by the number of new samples to be generated. The number of labels\nl in the MLD could also have some impact on complexity if it is large. Some MLDs have thousands of labels, in fact\nthere are methods to reduce the label dimensionality such as [38], and l could outweigh the importance of the other\nfactors. The number of NNs to be obtained, k, is always a small value, so it should not have a significant impact on\nthe runtime."}, {"title": "Experimentation and Results", "content": "This section first describes the experimental setup used to evaluate the performance of MLDM, including the datasets,\nclassifiers and metrics used, as well as the oversampling methods compared with MLDM."}, {"title": "Experimental setup", "content": "MLL experiments are time consuming and resource intensive. Most classifiers rely on data transformations and\nensembles of models, which need large amounts of memory and run time. The resampling methods themselves are\nalso costly, as the imbalance level of each label has to be computed, and many of them use NN search for each instance.\nOn the other hand, a robust experiment should use as many datasets, methods and classifiers as possible. Looking\nfor a balance between these two perspectives, our experimental setup is a combination of 8 MLDs, 6 resampling\nmethods and 5 different classifiers. Each of these 240 configurations was run with 5-fcv, resulting in a total of 1200\nruns."}, {"title": "Datasets", "content": "The eight datasets whose properties are shown in Table 1 were downloaded from the Cometa MLD repository [39].\nSpecifically, we used the 5-fcv partitions provided in this repository, so that anyone interested can use exactly the\nsame data partitions to reproduce our experiments. Since the goal is to test how MLDM deals with the imbalance\nproblem, most of the MLDs chosen have high MeanIR and SCUMBLE. The exceptions are emotions and scene, two\nclassic MLDs (they can be found in almost every MLL paper), which were included as a baseline performance control."}, {"title": "Resampling methods", "content": "Fall into two main groups, oversampling and undersampling. The former creates new instances with minority labels,\nwhile the latter removes data points with majority labels. Both tend to produce more balanced datasets, but it is"}, {"title": "Classifiers", "content": "To assess the effectiveness of each resampling method, once the training partitions have been processed, they are\ngiven as input to a set of classifiers. Since not all methods are equally affected by imbalance, this set includes a\nvariety of approaches to MLC, as shown in Table 3.\n\nThe binarization approach (BR-J48) is highly impacted by imbalance levels, since a separate binary classifier is\ntrained for each label. Therefore, some of them are trained with only a handful of positive samples. An effective\noversampling method should improve the behavior of these minority classifiers by training them with more positive\ndata points.\nWith LP-J48 only one classifier is trained, using each different labelset as a class. The labelsets containing\nminority labels will be rare, especially if the MLD has a large SCUMBLE, as the instances with minority labels will\nmostly also be associated with majority labels.\nNeural networks such as BPMLL are sensitive to imbalance due to their training process. The most common\npatterns, i.e. majority labels, have more influence in gradients than those rarely seen, i.e. minority labels. Never-\ntheless, BPMLL should have a more global view of the data than the former approaches, so the impact of imbalance\nand resampling in their results might not be as important as with BR or LP.\nAs the BR approach, MLkNN is highly impacted by imbalance level. The MLkNN algorithm searches for neighbors\nof the given instance to be classified. Since most of the space is occupied by samples with majority labels, the\nprobability of misclassifying the minority ones is very high.\nEnsembles tend to be resilient to imbalance and are often proposed as an effective approach for dealing with this\ntype of data. HOMER is a model built as an ensemble of classifiers trained on subsets of the original labelsets, so it\nshould be less affected by the imbalance levels."}, {"title": "Performance metrics", "content": "A few dozen performance metrics are defined in the MLL literature. Each provides a different perspective on the\noutcomes being analyzed [3]. There are ranking-based and bipartition-based metrics. Some are calculated by instance\n(sample-based) and then averaged, while others are computed by label (micro- and macro-averaged). A common\nproblem in MLL studies is how to interpret the results when many performance metrics are used to evaluate the\nclassifiers and they give conflicting views.\nIn the present experiments, the interest is not to compare the classifiers, but to see how they improve depending\non the resampling applied to the MLDs. We decided to include the metrics summarized in Table 4: two sample-based\nmetrics (Hamming loss and F-measure), a label-based metric with different averaging (Macro-F1 and Micro-1), and\na ranking-based metric (One error). By choosing F-measure as the reference metric, we are assessing a balance (the"}, {"title": "Results", "content": "After running all the classifiers on the datasets before and after applying the different resampling algorithms, three\nsets of results were collected:\n\u2022 The new imbalance levels of the datasets after preprocessing them with each oversampling method.\n\u2022 The performance measures given by the classifiers.\n\u2022 The runtime for each oversampling method."}, {"title": "Changes in imbalance levels", "content": "The aim of a MOA is to reduce the amount of imbalance present in the data. It is therefore of interest to analyze how\nthe MeanIR changes after it has been applied. To facilitate this analysis, we calculated the percentage improvement\n(or deterioration) of the MeanIR metric with respect to its original value for each dataset/resampling method\ncombination. The resulting values are presented in a heatmap (see Fig. 4), with precise values within each cell."}, {"title": "Classification improvements", "content": "Once the datasets had been processed with each of the MOAs, the classifiers were run and the above metrics were\nobtained. The set of 1400 values, properly grouped, are provided in tables as additional material.\nTo make it easier to analyze these results, they are presented as radar plots in Fig. 5, with the metrics as rows\nand the resampling methods as columns. For each plot, the classifiers appear as vertices. The larger the colored\narea, the better the performance.\nTo assess the statistical significance of differences among these results, the Nemenyi statistical test [55] was\napplied. Significant differences were found between some of the methods. These are shown as CD (Critical Distance)\nplots in Fig. 6, with one CD plot per performance metric.\nThe CD plots rank the resampling algorithms from better (lower rank) to worse. The critical distance marks\nthe minimum difference between two ranks to be considered statistically significant, drawn as lines connecting those\nmethods whose ranks do not differ more than this.\nLastly, to offer a broader view of these results, the average ranking for each resampling algorithm has been\nobtained and compared with the other in Fig. 7."}, {"title": "Running times", "content": "In addition to the performance measurements, the time taken by each MOA to process each dataset was also recorded.\nThe collected data is presented in Table 5. The plot in Fig. 8 allows a quick comparison of the efficiency of each\nalgorithm. In this case, no statistical tests were performed because the differences are very obvious."}, {"title": "Discussion", "content": "In order to draw conclusions, it is necessary to conduct an exhaustive analysis of the reported results. This analysis\nfollows the same structure as previously used, beginning with an examination of imbalance levels, then proceeding\nto an evaluation of classifier performance, and finally, an assessment of efficiency.\nFrom the data shown in Figure 4, it is clear that MLDM is able to consistently and significantly reduce the\nimbalance level of all MLDs. MLSMOTE is close to this behavior, but the imbalance reduction is lower in almost\nall cases and it seems to have problems with the cal500 dataset. The behavior of MLSOL is also very consistent,"}, {"title": "Conclusions", "content": "Multilabel learning, whether applied to classification, ranking, or distribution learning, is a complex task. Large\nimbalances, meaning that some labels are very common while others are rare, make it even more difficult. Resampling\nalgorithms have been proposed as a model-independent approach to alleviate this problem. Oversampling tends\nto be preferred over undersampling because it does not suffer from potentially useful data loss. Most multilabel\noversampling proposals in the literature are based on NN search, with some simpler algorithms that merely clone\nexisting data patterns.\nIn this paper, we introduced MLDM, a novel oversampling method for multilabel data. It is based on a modern\ndiffusion model instead of NN search. The model is trained on the subset of samples with minority labels, which is\nalways much smaller than the full dataset. The synthetic samples produced by MLDM include not only the features,\nbut also a synthetic labelset, instead of computing it separately as most existing MOAs do. Although MLDM requires\na training process, the computational complexity of the new sample generation stage is more efficient than that of\nother competing proposals because it does not require time-consuming distance computations.\nThe empirical experiments, conducted on a set of MLDs with different characteristics (amount of samples, number\nof labels, imbalance levels, etc.), several multilabel classifiers based on diverse approaches (binarization, ensemble,\nneural networks, NN, etc.), and a variety of performance metrics (sample-based, label-based and ranking-based),\nhave shown that MLDM is very competitive. Although there are no statistically significant differences, MLDM ranks\nabove and is more efficient than excellent MOAs such as MLSOL and MLSMOTE.\nAn open source implementation of MLDM is already available for anyone to use. We hope that practitioners and\nresearchers will find it useful, and perhaps it will be the seed for other multilabel resampling proposals based on\nmodern generative models, with better performance and efficiency, goals we will try to pursue."}]}