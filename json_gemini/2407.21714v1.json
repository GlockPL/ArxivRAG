{"title": "UMMAN: Unsupervised Multi-graph Merge Adversarial Network for Disease Prediction Based on Intestinal Flora", "authors": ["Dingkun Liu", "Hongjie Zhou", "Yilu Qu", "Huimei Zhang", "Yongdong Xu"], "abstract": "The abundance of intestinal flora is closely related to human diseases, but diseases are not caused by a single gut microbe. Instead, they result from the complex interplay of numerous microbial entities. This intricate and implicit con-nection among gut microbes poses a significant challenge for disease prediction using abundance information from OTU data. Recently, several methods have shown potential in predicting corresponding diseases. However, these methods fail to learn the inner association among gut microbes from different hosts, leading to unsatisfactory performance. In this paper, we present a novel architecture, Unsupervised Multi-graph Merge Adversarial Network (UMMAN). UMMAN can obtain the embeddings of nodes in the Multi-Graph in an unsupervised scenario, so that it helps learn the multiplex association. Our method is the first to combine Graph Neural Network with the task of intestinal flora disease prediction. We employ complex relation-types to construct the Original-Graph and disrupt the relationships among nodes to generate corresponding Shuffled-Graph. We introduce the Node Feature Global Integration (NFGI) module to represent the global features of the graph. Furthermore, we design a joint loss comprising adversarial loss and hybrid attention loss to ensure that the real graph embedding aligns closely with the Original-Graph and diverges from the Shuffled-Graph. Comprehensive experiments on five classical OTU gut microbiome datasets demonstrate the effectiveness and stability of our method. (We will release our code soon.)", "sections": [{"title": "I. INTRODUCTION", "content": "ACCORDING to incomplete statistics, there are at least 500 trillion microorganisms in the human body, outnumbering human cells by more than tenfold [23]. Microorganisms are not only large in number, but also diverse in types, including viruses, bacteria, fungi, eukaryotes, etc. Collectively, these microbes constitute the human microbiome, an essential focus of contemporary biomedical research.\nSome specific diseases are closely related to the abundance of microbial communities in the human body, with the vast majority of these microorganisms residing in the gut [37]. As the largest microbial habitat in the human body, the gut microbiome has a direct impact on human health and diseases. For example, researchers have found significant differences in the proportions of Bacteroides and Actinobacteria in the intestines of obese and lean individuals, with the proportion of Firmicutes also influencing obesity [6, 15]. Various metabolites of gut microbes, such as short-chain fatty acid and aromatic amino acids, all directly or indirectly induce type 2 diabetes [24]. Patients with inflammatory bowel disease (IBD) exhibit a relatively disordered intestinal microbiota [8, 19], with microbial metabolites such as acetate and butyrate playing crucial roles in immune regulation. The concentration of butyrate is notably decreased in patients with ulcerative colitis, highlighting the influence of gut microbes on IBD. In recent years, with the rise of 16SrRNA sequencing technology [17], new studies have shown that the number of lactic acid bacteria in the intestines of patients with irritable bowel syndrome decreased significantly, while the number of Veillonella in patients with constipation-predominant irritable bowel syndrome increased [29]. These findings further confirm the close connection between intestinal microbes and human health [4, 28].\nWith the rapid advancement of medical technology, obtaining relevant data has become increasingly convenient. 16SrRNA sequencing technology [5] can be used to obtain the abundance of human intestinal flora. Compared to whole genome sequencing, this technology is widely used due to its relatively low cost. However, processing this data to derive meaningful disease analysis results remains a significant challenge. Expert analysis of intestinal flora is complex and time-consuming, making it imperative to leverage artificial intelligence to enhance efficiency and reduce costs.\nHowever, diagnosing diseases based on intestinal flora information has not achieved satisfactory results due to the complex and multiplex connections among the intestinal flora. Most existing approaches consider only a single relation-type even ignore the connection, fail to cover the intricate connection among the gut microbes of different hosts. Graph learning [33, 35, 36] are well-suited for handling graph-structured data with rich relationships [30, 3], and can represent information at various depths. Therefore, we utilize Graph Neural Networks (GNNs) to learn the connections among gut microbes from different hosts, effectively guiding disease prediction. It is worth noting that our method does not rely on the labels in the process of obtaining the embeddings of the nodes and the graphs, which can obtain satisfactory embeddings in an unsupervised scenario.\nConsidering all the above challenges, we summarize the contributions of this paper as follows:\n\u2022 We are the first to introduce graph machine learning to the field of disease prediction based on gut microbiota.\n\u2022 We propose a novel Unsupervised Multi-graph Merge"}, {"title": "II. RELATED WORK", "content": "Benefiting from the rapid advancement of medical technology, many studies have been able to use OTU data for analysis [7, 20]. Pasolli et al. [22] comprehensively evaluated the prediction tasks based on shotgun metagenomics and the method of microbial phenotype association evaluation, mainly using support vector machines and random forest models to predict diseases, and with the help of Lasso and elastic net (ENet) Regularized Multiple Logistic Regression. In their study, cirrhosis was the most predictive disease, and the model used in the study had good generalization ability for cross-stage data, but poor generalization ability for cross-datasets. Sharma et al. [27] proposed TaxoNN to predict the conn between gut microbes and diseases, but only used two datasets to test the effect.\nManandhar et al. [18] used fecal 16S metagenomics data to analyze 729 IBD patients and 700 healthy individuals through 5 machine learning methods. After identifying 50 microorganisms with significant differences, the prediction was obtained through the random forest algorithm. Based on the data of the gut project in the United States, Linares et al. [16] used the glmnet model and the random forest model to predict the country of origin. Wong et al. [32] investigated the possible gastrointestinal effects of neratinib in the treatment of breast cancer. By collecting stool samples from 11 drug-taking patients and classifying patients who may develop diarrhea by a tree-based classification method.\nIn general, at present, for data in the form of OTU datasets that are extended to table types, the traditional machine learning algorithm may be relatively more effective [34]. However, these algorithms are relatively fixed and there is not much space for improvement, reaching a bottleneck on the problem. More disappointing, basic CNNs perform well in many applications, but it can't beat the traditional machine learning algorithm in this case. This is because the basic CNN's method cannot solve the problem of long-distance dependencies, nor can it adapt to data whose order can be changed at will. The exchange of rows and columns of the tabular datasets do not affect the results of pattern recognition, and more challengingly, diseases are not caused by a single gut microbe, but by a combination of a large number of microbial information. However, Graph Learning can help learn the intricate connection among the intestinal flora of different hosts. We propose the UMMAN unsupersived method, combining Graph Neural Network with gut microbiome for the first time, and it has achieved excellent performance on benchmark datasets."}, {"title": "III. METHOD", "content": "In this section, we will first introduce the overview of the architecture of our method UMMAN. In the following section, we introduce how to construct the Original-Graph and the Shuffled-Graph. We then present the details of the two stages of the Node Feature Global Integration descriptor: node-level stage and graph-level stage. Then we introduce the attention block to derive the embedding of each node. We also propose the joint loss which consists of adversarial loss $L_{adv}$ and hybrid attention loss $L_{h-attn}$ in the end."}, {"title": "A. THE OVERVIEW OF UMMAN ARCHITECTURE", "content": "The architecture of UMMAN we propose will be introduced detailedly in this part. As stated above, it exists close but ex-tremely complex connection among gut microbes of different hosts, and such relation is implicit. Specifically, it is not the abundance of a single intestinal microbe that can establish a direct relationship with the final disease, but it is caused by the combined information of various microbes. For tabular datasets in the form of OTUs(A table with rows representing gut microbiota abundance and columns representing samples), their adjacent data are not correlated, so traditional convolutional neural networks are not applicable. Therefore, how to learn the connection among gut microbes of different hosts has become the biggest difficulty in this field, and we are the first to combine graph machine learning with gut flora disease prediction tasks which helps learn the connection. The following will introduce the architecture of UMMAN that we propose in detail.\nThe overview architecture of UMMAN is shown in Fig .1. Each node represents a host. We use multiplex indica-tors to measure the similarity among nodes to build the Original-Graph. In order to make UMMAN learn associations more accurately, we destroy the Original-Graph's association among nodes to get Shuffled-Graph. The Original-Graph and Shuffled-Graph are updated through the Graph Convolutional Network at the same time in order to obtain the embeddings of each node. We introduce the attention block to obtain the embeddings of the nodes in the Original-Graph and the embeddings of the nodes in the Shuffled-Graph respectively. In addition, we design a Node Feature Global Integration (NFGI) descriptor to denote the embedding of a graph. Then, in order to make the true embedding that includes the complex and implicit relationships among gut microbes of different hosts agree with the Original-Graph as much as possible and dis-agree with the Shuffled-Graph as much as possible, we design the total joint loss function consists of an adversarial loss and"}, {"title": "B. CONSTRUCT MULTI-GRAPH AND NODE EMBEDDING", "content": "We find that the abundance data of a part of the intestinal flora in the dataset was extremely low in most hosts whose impact on the performance of the algorithm is detailed above. If all the data of the original dataset is retained, it may cause some intestinal flora to be almost completely absent in all hosts. We can consider such data as \"dirty data\", which will lead to training effects decline. Therefore, we removed the intestinal flora with low abundance in most hosts to improve the effect of feature extraction.\nIn order to make the motivation more convincing, we show the connection of the 10 most abundant flora in the cirrhosis dataset in the sample intestine in Fig.2. The diagonal graph describes the distribution histogram of the flora, and the off-diagonal represents the relationships between the flora and the abundance in the intestinal tract of the sample, where the red dots represent healthy hosts, and the blue dots represent diseased samples. It can be found that the correlation is not obvious and appears to be disorganized, which proves that it is unreliable to measure the relation-type among hosts only based on the abundance of flora. To explore the implicit information of fused features in intestinal flora, we consider multiplex relation-types [1, 10] to measure the relationships among vectors during the initialization of building the edges among nodes in the graph:\n$S_1(m, n) = \\sum_{i=1}^{d} |m_i - n_i|$ (1)\n$S_2(m, n) = \\sqrt{\\sum_{i=1}^{d} (m_i - n_i)^2}$ (2)\n$S_3(m,n) = \\sum_{i=1}^{d} \\frac{|m_i - n_i|}{m_i + n_i},$ (3)\nwhere Bray Curtis Distance, Euclidean Distance and Canberra Distance are used to initialize a graph. We construct the original graph by measuring the multivariate discrepancy between nodes, two nodes are considered to be connected if the discrepancy among their embeddings is below a variable threshold $\\theta$. In order to make our model more robust, and to obtain the correlation among nodes more reliably, we design the adversarial control group, i.e., keep the position of the edges unchanged, randomly disrupt the nodes to get the Shuffled-graphs, and train the discriminator to get the adversarial loss. The specific process will be introduced in detail later.\nWe introduce an encoder module for each relation type inspired by the Graph Convolutional Network[12], aiming to obtain the embedding of each node in the graph. We define a conditional function $F$ as an update function between layers, the Shuffled-Graph is operated by the same process $I^{RN\\times D}$.\n$X_j^{(l+1)} = F(X_j^{(l)}, \\hat{A}[j], W^{(l)}, b^{(l)})$ \n$= \\sigma(\\sum_{j\\in N_i} \\frac{1}{\\sqrt{\\hat{d_i}}\\sqrt{\\hat{d_j}}} X_j^{(l)}W^{(l)}+b^{(l)})$ (4)\nwhere $X_j$ is the embedding of the l layer of the node whose index is j, $N_i$ represents the set of nodes adjacent to $X_j$, $\\hat{A}$ and $D$ are the adjacency matrix and degree matrix of a certain graph, $W^{(l)}$ and $b^{(l)}$ are the trainable parameters of the l-th layer's weight matrix and bias, $\\sigma$ is the nonlinearity layer which is designed as ReLU in our method. Through Graph Convolutional Network, $X_j^{(t)}$ become a D-dimensional tensor representing the embeddings of the node with index j of the t-th relation-type."}, {"title": "C. NODE FEATURE GLOBAL INTEGRATION DESCRIPTOR WITH TWO STAGES", "content": "In the NFGI module, we propose a novel contribution to characterize a graph $\\mathcal{J}^{(t)}/\\bar{\\mathcal{J}^{(t)}}$ with two stages($\\mathcal{J}^{(t)}$ is the Original Graph constructed according to the t-th relation-type, and $\\bar{\\mathcal{J}^{(t)}}$ is the corresponding graph after being shuffled, since the operations of the two graphs are equivalent, we use $\\mathcal{J}^{(t)}$ uniformly in the following text). In the conventional method, only the features of each node are fused, but using this method to characterize a graph only considers the information of each node. Therefore, our NFGI module adopt a two-stage process, i.e., node-level stage and graph-level stage to jointly characterize the features of the graph to better obtain the embedding.\nNode-Level stage. We first compute the node-level con-tribution to graph $\\mathcal{J}^{(t)}$ based on the characteristics of each node. That is, to fuse the embedding of each node after the Graph Convolutional Network. We employ a function $M : \\mathbb{R}^{N\\times D} \\rightarrow \\mathbb{R}^{D}$:\n$\\rho^{(t)} = M(\\mathcal{X}^{(t)}) = \\sigma(\\frac{1}{N}\\sum_{i=1}^{N} X_i^{(t)}).$ (5)\n$\\rho^{(t)}$ summarizes the node-level features of the graph $\\mathcal{J}^{(t)}$. Among them, $\\mathcal{X}^{(t)}$ is a matrix with n rows and d columns, $X_i^{(t)}$ is the i-th row of matrix $\\mathcal{X}^{(t)}$ and $\\sigma$ denotes the logistic sigmoid nonlinearity function.\nGraph-Level stage. In the graph-level stage, we concatenate the embedding of each node to build a tensor, and then flatten the tensor to get the total information of the whole graph which can form K bins. Let {$x_j$}$_{j=1...n}$ be the each center of the histogram bins. The value included in each bin is within the range of r centered on {$x_j$}. Given a certain feature value $x_i$, we define a function $\\Psi$, the index {$\\varphi(x_i^{(t)})$}, corresponding to the bin that the value belongs to. On the whole, the function $G$ denotes the embedding of the graph in the graph-level stage: $\\mathbb{R}^{N \\times D} \\rightarrow \\mathbb{R}^{K}$:\n$q^{(t)} = G(\\mathcal{X}^{(t)}) = \\sum_{i=1}^{N} C(\\rho(\\mathcal{X}^{(t)}))\\delta[\\varphi(x_i^{(t)}) - x_i]$ (6)\n$\\bar{q}^{(t)} = E[q^{(t)}_1, ..., q^{(t)}_K],$\n(7)\nwhere $\\delta$ is the Kronecker delta function. The normalization constant $C$ is derived by imposing $\\sum_{i=1}^{N}\\bar{x}_i^{(t)} = 1$, performing data smoothing on the graph-level results. The graph-level embedding result can describe the global feature of the graph. Therefore, after concatenating each graph-level stage feature using the symbol & , the final graph-level embedding result is obtained: $\\mathbb{R}^{N\\times D} \\rightarrow \\mathbb{R}^{D+K}$. Finally, we concatenate the node-level embedding and graph-level embedding by the axis 0.\nG^{(t)} = E[\\rho^{(t)}, \\bar{q}^{(t)}]$ (8)"}, {"title": "D. CONVERGE MULTI-GRAPH BY ATTENTION", "content": "Single-scale CNN fail to capture the intricate relationships among intestinal flora, while conventional multi-scale CNNs sometimes fall short in effectively integrating information from various scales using concatenation-based approaches.\nThe significance of self-attention CNNs in amalgamating multi-scale information is due to several factors. To begin with, the self-attention mechanism in CNNs improves classifying capability by selectively concentrating on significant regions or features in input data, consequently increasing classification accuracy and aiding in detecting nuanced patterns. Moreover, self-attention CNNs generate interpretable outcomes by indicating the regions that most influence the classification decision. Additionally, self-attention CNNs perform exceptionally well in managing high-dimensional data, allowing them to learn distinct features in multiple dimensions, thereby capturing intricate patterns and dependencies in gut microbiome abundance data. In general, self-attention is a notable subtype of attention mechanisms because of its ability to capture long-range dependencies, adaptively weigh input elements, manage variable-length sequences, ensure interpretability, and combine with other deep learning techniques. In this part, we utilize it to merge Original-graph and Shuffled-graphs that have been updated by Graph Convolutional Network.\nAfter using Graph Convolutional Network to update the Original-graph and Shuffled-graphs, we get the embedding of each node. Each node in the Multi-graph $\\mathcal{J}^{(t)}$ gets a feature vector to represent, and then we use attention block to update the embeddings of the corresponding nodes [31] of the Multi-graph to an embedding result representing the node feature. The attention block is defined as follows:\n$X_{attn} = Attn(X) \\mid t\\in T)$\n$= \\sum_{t\\in T} \\frac{exp(query^{(t)} \\cdot x_i^{(t)})}{\\sum_{t'\\in T} exp(query^{(t')} \\cdot x_i^{(t')})} x_i^{(t)},$ (9)\nwhere $query^{(t)} \\in \\mathbb{R}^{D+K}$ is the feature vector of relation-type t. The Original Graphs and the Shuffled Graphs can obtain the embedding results $x_i$ and $\\bar{x}_i$ of each node after passing through the attention block. Finally, the embedding of nodes can be sent to the MLP for prediction."}, {"title": "E. LOSS FUNCTIONS", "content": "The loss functions of our model consists of an adversarial loss $L_{adv}$ and hybrid attention loss $L_{h-attn}$. The total loss function is as follows:\n$L = L_{adv} + \\eta L_{h-attn},$ (10)\nwhere $\\eta$ is a learnable coefficient for the loss terms.\n1) ADVERSARIAL LOSS: We not only use a variety of relation-types to construct Multi-graph and obtain the embedding of each Node through Graph Convolutional Network and Attention Block, but also the Original-Grpah Randomly shuffle to break the correlation among nodes. Therefore, we calculate the positive correlation loss between the global embedding obtained by NFGI (Node Feature Global Integration Descriptor) and the embedding of the nodes of the Original-Graph obtained by each relation-type. At the same time, on the converse side, we calculate the negative correlation loss between the global embedding and the embedding of the nodes of the Shuffled-Graph, and define this joint loss as Adversarial Loss inspired by [21]. In other words, we calculate the adversarial loss from the embedding obtained from the constructed Original-Graph (Positive) and the Shuffled-Graph (Negative) that destroys the correlation among nodes, which is defined as:\n$L_{adv} = \\sum_{t\\in T} \\sum_{i=1}^{N} log\\sigma ((H^{(t)})^T W^{(t)} x_i^{(t)}) $\n$+ \\sum_{t\\in T} \\sum_{j=1}^{N} log (1 - \\sigma ((H^{(t)})^T W^{(t)} \\bar{x}_j^{(t)})).$ (11)\n2) HYBRID ATTENTION LOSS: The Hybrid Attention Loss we proposed comprehensively considers the embedding after the nodes of the Original-Graph and the Shuffled-Graph pass through the attention block. The Hybrid Attention Loss $L_{h-attn}$ can make the global embedding matrix of real graph agree with $\\mathcal{X}_{attn}^{(t)}$ and disagree with $\\bar{\\mathcal{X}}_{attn}^{(t)}$, thereby improving the confidence of the attention block. The Hybrid Attention Loss function is defined as:\n$L_{h-attn} = || (P - \\mathcal{X}_{attn}) ||^2 - || (P - \\bar{\\mathcal{X}}_{attn}) ||^2$. (12)"}, {"title": "Algorithm 1 Unsupervised Multi-graph Merge Adversarial Network", "content": "Input: feature F, origin data O, shuffled data S, graph num Ng, head num Nh\nOutput: loss L\nfor i = 1 to Ng do\npos \u2190 GCN(F[i], O[i], i)\np\u2190 NFGI(pos)\nneg \u2190 GCN(S[i], O[i], i)\nLadv \u2190 Disc(p, pos, neg)\nP.append(pos)\nN.append(neg)\nend for\nfor h = 1 to Nh do\nPattn, Nattn\u2190 Attn[h](P,N)\nend for\n$\\mathcal{X}_{attn}$\u2190 mean(Pattn)\n$\\bar{\\mathcal{X}}_{attn}$\u2190 mean(Nattn)\nLpos\u2190 ((P-$\\mathcal{X}_{attn})^2).sum()\nLneg\u2190 ((P-$\\bar{\\mathcal{X}}_{attn})^2).sum()\n$L_{h-attn}$ \u2190 Lpos - Lneg\nL\u2190 Ladv + $\\eta$$L_{h-attn}$\nreturn L"}, {"title": "IV. EXPERIMENT", "content": "We performe extensive experiments on five real-world datasets to evaluate the performance of our UMMAN model. We also compare it to several state-of-the-art machine learning and deep learning models. In pre-processing, we removed the intestinal flora which abundance lower than 0.01 in most hosts (set to 120). During the initialization of the graph, set the threshold $\\theta$ to 0.6. We randomly divide each dataset into five"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel architecture UMMAN which combines GNN with disease prediction tasks based on intestinal flora for the first time. It helps learn the mul-tiplex connection among gut microbes of different hosts. We construct the Multi-Graph and Shuffled-Graph with multiple relation-types, get the embedding of hosts through improved GCN. In addition, we introduce the Node Feature Global Integration (NFGI) to describe the global embedding of a graph with node-level stage and graph-level stage. Finally, we design a joint loss consisting of adversarial loss and hybrid attention loss as the final loss function. Extensive experiments show that our UMMAN performs well on the task of disease prediction based on intestinal flora and achieves the state-of-the-art. Our UMMAN model can be applied to assist in the diagnosis of gut microbiome-related diseases.\nIn the future, the main work may be to find a more suitable way to construct a gut microbiota, in order to avoid building microbial interaction networks incorrectly, which could negatively impact the accuracy of subsequent graph classification tasks. Lastly, the biggest challenge in this study is the data, as there are currently relatively few publicly available gut microbiome datasets for disease classification. However, with the development of sequencing technology, more datasets that can reflect sample features may emerge in the future. At that time, the model's performance may improve after extensive training."}]}