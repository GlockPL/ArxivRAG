{"title": "POSTERSUM: A Multimodal Benchmark for Scientific Poster Summarization", "authors": ["Rohit Saxena", "Pasquale Minervini", "Frank Keller"], "abstract": "Generating accurate and concise textual summaries from multimodal documents is challenging, especially when dealing with visually complex content like scientific posters. We introduce POSTERSUM12, a novel benchmark to advance the development of vision-language models that can understand and summarize scientific posters into research paper abstracts. Our dataset contains 16,305 conference posters paired with their corresponding abstracts as summaries. Each poster is provided in image format and presents diverse visual understanding challenges, such as complex layouts, dense text regions, tables, and figures. We benchmark state-of-the-art Multimodal Large Language Models (MLLMs) on POSTERSUM and demonstrate that they struggle to accurately interpret and summarize scientific posters. We propose SEGMENT & SUMMARIZE, a hierarchical method that outperforms current MLLMs on automated metrics, achieving a 3.14% gain in ROUGE-L. This will serve as a starting point for future research on poster summarization.", "sections": [{"title": "Introduction", "content": "Scientific posters play a critical role in academic communication, offering a visually rich medium that combines text, images, charts, and other graphical elements to present research findings. Summarizing these visually complex posters into concise and accurate textual abstracts presents a unique challenge, requiring models to integrate multimodal information effectively.\nMultimodal Large Language Models (MLLMs; OpenAI et al., 2024; Grattafiori et al., 2024) have demonstrated remarkable capabilities in vision-and-language tasks, including image captioning (Fu et al., 2024; Koh et al., 2023; Yu et al., 2024; Garg et al., 2024) and visual question answering (Liu et al., 2024e; Yue et al., 2024). While these models exhibit strong generalization across various domains, their performance often declines when applied to scientific text (Li et al., 2024; Lu et al., 2024; Pramanick et al., 2024). Additionally, the complexity of poster layouts, the use of technical terminology, and the intricate interplay between text, tables, and figures make summarizing scientific posters a particularly challenging task, which has remained under-explored due to the lack of specialized datasets.\nTo address this gap, we introduce POSTERSUM, a novel multimodal benchmark for summarizing scientific posters into research paper abstracts. Our dataset consists of 16,305 scientific posters and corresponding abstracts as summaries collected from the main Machine Learning conferences, namely ICLR, ICML, and NeurIPS. These posters cover a broad range of scientific disciplines and present unique challenges, including complex layouts and intricate combinations of text, tables, and figures as shown in Fig. 1. Information is often distributed across the poster, requiring careful navigation and integration of diverse elements to identify and summarize the key points effectively.\nWe benchmark state-of-the-art MLLMs on POSTERSUM and demonstrate that, despite their impressive performance on a range of other multimodal tasks, these models face significant limitations when summarizing scientific posters. For instance, the best-performing closed-source model in our experiments, GPT-40 (OpenAI et al., 2024), achieves a ROUGE-L score of 22.30, underscoring the difficulty of this task specifically with the posters with figures and tables.\nTo address this challenge, we propose SEGMENT & SUMMARIZE, a hierarchical approach inspired by the divide-and-conquer principle (Chen and Zhao, 2023). The method involves three key steps: (1) Segmentation: we segment each poster into coherent regions; (2) Localized Summarization: a multimodal large language model extracts and interprets the text within each segment, generating localized summaries for each region; and (3) Global Summarization: these localized summaries are combined using text-based large language model to produce a cohesive abstract that spans the entire poster. Notably, this approach does not require additional training or fine-tuning. Local summaries allow the model to focus on fine details within that specific area, which are useful for tables and figures. Also, it aligns with the inherent structure of the poster, which has sections with a specific focus. This approach achieves a ROUGE-L score of 24.18, outperforming both closed-source and open-source models, setting a new benchmark for scientific poster summarization.\nThe proposed dataset and baselines will enable future research in multimodal scientific poster understanding. Our contributions can be summarized as follows:\n\u2022 We introduce POSTERSUM, a large-scale multimodal dataset of 16,305 scientific posters paired with their abstracts, tailored for research poster summarization.\n\u2022 We benchmark state-of-the-art MLLMs on POSTERSUM, showing their limitations in processing and summarizing scientific posters.\n\u2022 We propose SEGMENT & SUMMARIZE, a hierarchical approach that segments each poster into coherent regions, extracts the textual content from those regions and then composes a final summary; we also demonstrate POSTERSUM'S utility for fine-tuning MLLMs, showing promising improvements over zero-shot results."}, {"title": "Related Work", "content": "Multimodal Large Language Models. After the emergence of LLMs, recent work (Liu et al., 2023; Wang et al., 2024b; Alayrac et al., 2022) investigated their use in processing multimodal inputs, giving rise to Multimodal Large Language Models (MLLMs). The core idea in this line of research is to align visual and textual features by using shared representations. This framework typically involves using a pre-trained visual encoder to extract visual features, a projection layer to map visual representations into corresponding text representations, and a pre-trained LLM to generate textual responses, allowing the model to condition the output on visual and textual inputs. MLLM architectures such as LLaVA (Liu et al., 2023) and MiniCPM (Yao et al., 2024) demonstrated impressive zero-shot generalization across diverse visual and language tasks. However, most existing MLLMs focus on general domain tasks and relatively simple visual inputs; the challenge of understanding complex and information-dense visual documents like scientific posters remains under-explored.\nSummarization in Scientific Domains. Scientific summarization consists of generating concise summaries for scientific content (Yasunaga et al., 2019; Cachola et al., 2020; Ju et al., 2021; Sotudeh and Goharian, 2022). Several scientific summarization benchmarks have been proposed, designed to process modalities such as videos (Lev et al., 2019; Chen et al., 2024), slide decks (Tanaka et al., 2023), surveys (Liu et al., 2024d), and research papers (Takeshita et al., 2024; Liu et al., 2024a). However, scientific poster summarization remains unexplored despite the widespread use of posters in academic communication.\nDocument Layout Analysis and Segmentation. Understanding document layouts plays a significant role in processing complex visual documents like scientific posters. Recent work in document layout analysis (Peng et al., 2022; Wang et al., 2024a; Luo et al., 2024; Appalaraju et al., 2024) aims at identifying and classifying different regions within a document considering spatial relationships and content type. Previous work has also focused on understanding individual elements in documents, such as charts (Masry et al., 2022) and tables (Zheng et al., 2024). However, most existing approaches are designed for either standard documents or individual elements like charts and tables and do not capture the complex layouts and the rich multimodal structure of scientific posters, which typically consist of text, charts, equations, and tables."}, {"title": "The PoOSTERSUM Dataset", "content": "We introduce POSTERSUM, a novel dataset and benchmark for multimodal abstractive summarization of scientific posters. The dataset consists of 16,305 pairs of academic posters as images (PNG format) and their corresponding research paper abstracts. These posters were collected from major machine learning and artificial intelligence conferences, which accept papers from various sub-fields of machine learning, including computer vision, natural language processing, optimization, and computational biology.\nPOSTERSUM captures the diverse and heterogeneous nature of academic posters, which are commonly used at conferences to present research findings. These posters vary in layout, content, and visual complexity-some are text-heavy, while others emphasize visual elements such as charts, graphs, and figures, as shown in Fig. 1. This variability presents a significant challenge for MLLMs, requiring them to interpret and summarize multimodal information effectively.\nEach poster in the dataset is paired with its corresponding abstract, which serves as the ground-truth summary. The abstract highlights the key contributions and findings of the research, making it an ideal summary for the poster. Unlike image captioning, poster summarization requires a deeper understanding of multiple elements in the poster to generate a comprehensive and meaningful abstract-based summary."}, {"title": "Dataset Creation", "content": "The POSTERSUM dataset was collected from the websites of top-tier machine learning and artificial intelligence conferences: ICLR, ICML, and NeurIPS. We selected these conferences based on the availability of research posters. We first collected research paper links and paper identifiers from the conference websites. We filtered out any entries where the poster of the paper was not available, ensuring that only papers with accessible posters were included in the dataset. We exclusively collected posters from the years 2022 to 2024, as shown in Fig. 2. Additionally, we manually reviewed the dataset to remove any posters with placeholder images. We assume that the research reported in the posters is of a high standard, and the posters are of high quality, as the corresponding papers appeared at top machine learning conferences.\nTo build a robust summarization dataset, it was essential to pair each poster with a human-written summary. We collected the research paper abstracts from the corresponding paper pages using the paper identifiers. These abstracts serve as the summaries for the posters, as they highlight the core findings and contributions of the research. For papers where the abstract was missing from the webpage, we manually extracted the abstract from the research paper's PDF to ensure completeness."}, {"title": "Dataset Statistics and Analysis", "content": "This process resulted in the 16,305 poster-summary pairs, providing a comprehensive multimodal resource for evaluating abstractive summarization of academic research posters.\nTable 1 provides an overview of key statistics for the dataset. The average length of the poster summaries is 224 word-piece tokens, with an average of seven sentences per summary. The poster images are of high-resolution, with a mean size of 3547 \u00d7 2454. We randomly split the dataset into training, validation, and test sets using a 10305/3000/3000 split, which can be utilized for training and fine-tuning models.\nTo better understand the diversity within the dataset, we categorized each poster into topics. Since topics were not available on the conference websites, we employed the GPT-40 vision model to generate topic labels by prompting the model in a zero-shot setting using the images of the posters. As a result, we identified 137 distinct topics within machine learning and artificial intelligence for the posters, spanning areas such as reinforcement learning, natural language processing (NLP), computational biology, and healthcare applications. Fig. 3 illustrates the distribution of the top 25 topics by frequency.\nTo assess the abstractiveness of the poster summaries, we report the percentage of novel n-grams in the summaries compared to the Optical Character Recognition (OCR) extracted text from the posters. We used MMOCR (Kuang et al., 2021) to extract the text. While most posters do not explicitly include abstracts, we found that approximately 8% of the total posters may contain an abstract in poster, based on the occurrence of the word \"abstract\" in the OCR text. As shown in Table 2, a significant portion of the summaries contains novel content, particularly in the 3-gram and 4-gram categories. This demonstrates that the summaries are not simple restatements of poster text but instead provide a more comprehensive abstraction.\nWe also find a mean CLIP score (Hessel et al., 2021) of 29.08 when we evaluate the alignment between the images of the posters and their summaries. This score was computed at the sentence level and averaged across the dataset. The relatively low CLIP score highlights the challenge that POSTERSUM poses for existing MLLMs. Unlike image-captioning tasks, where captions directly describe visual features, academic posters are composed of diverse and complex visual elements, such as charts, graphs, equations, and dense textual explanations. This complexity makes it more difficult for models to capture the semantic relationships between these elements and the corresponding abstract summaries."}, {"title": "Multimodal Poster Summarization", "content": "Task Formulation\nGiven a scientific poster $I$ in image format as input, the objective is to generate a textual summary $Y = \\{Y_1, Y_2,..., \\hat{Y}_m\\}$ that encapsulates the key points and essential content of the poster. Formally, a model $M_\\theta$, parameterized by $\\theta$, takes the poster $I$ as input, optionally accompanied by a prompt $P$, and generates a summary $Y$. The key challenge in this task is that model $M_\\theta$ must effectively abstract from the diverse visual and textual elements present in the poster, including text, charts, diagrams, and equations, to produce a coherent and informative summary."}, {"title": "Baselines", "content": "We evaluate various multimodal models, both open-source and closed-source, to assess their performance on the abstractive summarization task for scientific posters. As the posters include textual elements, we also evaluate OCR-based methods as baselines. For MLLMs, evaluation is conducted in a zero-shot and Chain-of-Thought (CoT) setting to assess the capability of models to generate accurate summaries. Additionally, we explore parameter-efficient fine-tuning techniques on selected open-source models. Below are the categories of models used in our experiments."}, {"title": "Optical Character Recognition (OCR)", "content": "For OCR-based baselines, we used two OCR methods (MMOCR (Kuang et al., 2021) and Pytesseract\u00b3) to extract text from the poster images and concatenated the results to generate a summary. Additionally, we combined the best OCR output with a text-based large language model (LLM). In this approach, we first extract text from the posters and then use the Llama-3.1-8B-Instruct (Grattafiori et al., 2024) model for summarization. This allows us to evaluate the performance of text-only LLMs when provided with OCR-extracted text."}, {"title": "Closed-source MLLMs.", "content": "We evaluated GPT-40 (OpenAI et al., 2024), Claude 3.5 Sonnet (Anthropic, 2024), and Gemini 2.0 (Anil et al., 2024) as closed-source MLLMs. All the models were prompted with the image of the poster in a zero-shot setting to generate abstractive summaries based on the input. The prompt template can be found in Appendix B."}, {"title": "Open-source MLLMs.", "content": "As open-source/open-weights models, we evaluated Llama-3.2-11B-Vision-Instruct (Meta, 2024), Qwen2-VL-7B-Instruct (Yang et al., 2024), LLaVA-NeXT (Liu et al., 2024c,b), mPLUG-DocOwl2 (Hu et al., 2024), and MiniCPM-Llama3-V-2.5 (Yao et al., 2024). Each model was evaluated in both zero-shot and CoT settings. The CoT prompt was used to steer the models to extract relevant information, such as the title, research problem, methods, results, and conclusion, from the poster. We report the full prompt template in Appendix B."}, {"title": "Fine-tuned Models (LoRA)", "content": "We also evaluated the fine-tuned Llama-3.2-11B-Vision-Instruct and LLaVA-NeXT models. We used parameter-efficient fine-tuning using the Low-rank Adaptation (LoRA; Hu et al., 2022) method to fine-tune both of these models using the training and validation set of the POSTERSUM dataset."}, {"title": "SEGMENT & SUMMARIZE", "content": "We now introduce SEGMENT & SUMMARIZE, a hierarchical approach inspired by the divide-and-conquer principle. Rather than processing the entire poster $I$ as a single input, SEGMENT & SUMMARIZE decomposes the task into three key steps: (1) Segmentation and Clustering (2) Localized Summarization, and (3) Global Summarization. The SEGMENT & SUMMARIZE pipeline is outlined in Fig. 4."}, {"title": "Segmentation and Clustering", "content": "Given the image of a poster $I$, the first step is to segment it into $n$ coherent regions $M = \\{M_1, M_2, ..., M_n\\}$. This is achieved using a segmentation model $S_\\phi$, parameterized by $\\phi$, Since the number of regions $n$ can be large and can contain redundant and small segments, the regions are further clustered into groups $R$ with the number of the clustered regions as $k$ using a clustering algorithm $C$ such that $k \\ll n$. The clustering step groups similar regions together, reducing redundancy and ensuring complete coverage of the poster. Formally, $M = S(I)$ and $R = C(M)$.\nBy segmenting the poster and summarizing each region independently, the method ensures a detailed and accurate understanding of the content."}, {"title": "Localized Summarization", "content": "For each clustered region $R_i$, a localized summary $Y_i = \\{\\hat{Y}_{i1}, \\hat{Y}_{i2},...,\\hat{Y}_{ik}\\}$ is generated using an MLLM $V_\\psi$. The model is used to extract and interpret the content within $R_i$, including text, figures, and tables, to generate a localized summary for that specific region. This also helps in processing the high-resolution image."}, {"title": "Global Summarization", "content": "The localized summaries $Y_1, Y_2, ..., \\hat{Y}_k$ are combined into a cohesive global summary $Y$ using a text-based large language model $L_\\omega$, parameterized by $\\omega$. The model $L_\\omega$ takes as input the individual summaries and generates a single, well-structured output that represents the overall content of the poster. This step ensures that the final abstract is not only comprehensive but also maintains logical flow and coherence. Formally, $\\hat{Y} = L_\\omega(Y_1, Y_2, ..., Y_k)$.\nThis processing pipeline helps summary generation through a structured, localized, and hierarchical approach. By segmenting the poster and summarizing each region independently, the method captures fine-grained details that might be overlooked in a global approach. This also aligns with the structure of these posters, which are mostly divided into sections. This approach does not require additional training or fine-tuning, and both the models ($V_\\psi$, $L_\\omega$) are frozen."}, {"title": "Experimental Details", "content": "All the models in each category were evaluated using the same hyperparameter settings for fair evaluation. We generate at most 768 new tokens for all the experiments. For closed-source models, we used the default platform settings. Open-source models were evaluated with a beam size of 4 with greedy decoding to ensure reproducibility. The fine-tuning experiments were conducted for 10 epochs with a batch size of 4. More details about the hyperparameters and prompt templates can be found in Appendices B and E.\nFor SEGMENT & SUMMARIZE, we used the Segment Anything Model (Kirillov et al., 2023) for segmentation with k-Means for clustering. The number of clusters (k) was set to 8 based on the analysis in Appendix D. We used MiniCPM-Llama3-V-2.5 as the local summarize ($V_\\psi$) and Llama 3.1-8B-Instruct as the global summarizer ($L_\\omega$). We used the training set for fine-tuning and the validation set for hyperparameter tuning. All the final results are evaluated on the test set."}, {"title": "Evaluation Metrics", "content": "We use ROUGE F1 (R-1/2/L/LSum) scores (Lin, 2004), Sacre-BLEU (SBLEU; Post, 2018), METEOR (MET; Banerjee and Lavie, 2005), and BERTScore (Zhang et al., 2020) to evaluate the accuracy of all models."}, {"title": "Results", "content": "Table 3 presents the poster summarization performance of all baselines alongside our proposed SEGMENT & SUMMARIZE method, evaluated on the POSTERSUM test set. Our method outperforms both open-source and closed-source models, achieving the best results across all metrics.\nClosed-source Models. GPT-4o achieves relatively high performance among the closed-source models across all metrics, with ROUGE-1/2/L scores of 44.98, 13.12, and 22.30, respectively. Claude-3.5 Sonnet also performs well, attaining a ROUGE-L score of 19.51.\nOCR Baselines. The two OCR-based methods, MMOCR and Pytesseract, achieve relatively low scores across all metrics. This is likely due to the limitation of concatenating raw OCR text without leveraging other visual elements. Combining OCR with the text-only Llama-3.1 model results in a substantial improvement, with ROUGE-L, increasing from 12.73 to 15.49. Interestingly, these OCR methods still outperform certain multimodal models, indicating that text extraction remains a challenge for some MLLMs.\nOpen-source Models. Among the open-source MLLMs evaluated in zero-shot settings, MiniCPM-Llama3-V-2.5 obtains the highest ROUGE-1/L score (39.88/20.14) and a strong BERTScore-F1 of 59.22. Meanwhile, mPLUG-DocOwl2 achieves a competitive ROUGE-L of 19.06 and a BERTScore-F1 of 56.99.\nChain of Thought (CoT). Adding an explicit CoT prompt improves the performance of most models. For instance, MiniCPM-Llama3-V-2.5 improves its ROUGE-1/L/METEOR scores to 41.50/21.04/26.34, while mPLUG-DocOwl2's performance also increases (ROUGE-1/L of 37.04/19.71). Additionally, LLaVA-NeXT and Qwen2-VL-7B exhibit similar gains. Although the performance boosts are not large, these results suggest that guiding models via CoT prompt can help in extracting relevant poster content.\nFine-tuned Models. Using LoRA substantially boosts performance for both MLLMs. In particular, Llama-3.2-11B-Instruct demonstrates notable improvements in ROUGE, ScareBLEU, and METEOR scores, though it does not surpass the best CoT variants of MPLUG-DOCOWL2 and MINICPM-LLAMA3-V-2.5, which likely benefit from pre-training on multimodal scientific data.\nSEGMENT & SUMMARIZE. Our proposed method outperforms all other models, including closed-source models, on all metrics, achieving ROUGE-1/2/L scores of 46.68, 15.73, and 24.18, respectively, with a 3.14% gain on ROUGE-L compared to open-source models. It also attains a substantially higher ScareBLEU score (12.63) and a BERTScore-F1 of 61.37. These results indicate that local-region summaries effectively preserve small details and handle posters of varying complexity by processing each region independently rather than attempting to analyze the entire poster as a single input."}, {"title": "Ablation Studies and Analysis", "content": "Effect of Clustering on Summarization. To quantify the impact of clustering in our SEGMENT & SUMMARIZE approach, we conduct an ablation study that removes the clustering step. Specifically, we select the top-k segments (with k = 8) based on their region size to generate local and global summaries. Table 4 shows that clustering improves the ROUGE-1 score by +4.43, ROUGE-2 by +1.43, and ROUGE-L by +1.42 over the non-clustered baseline. We hypothesize that clustering helps reduce redundant segments and improves context aggregation.\nEffect of Local Vision Summarization. To assess the role of the local summarization model in SEGMENT & SUMMARIZE, we replaced MiniCPM-Llama3-V-2.5 with mPLUG-DocOwl2, which previously ranked second among open-source models under the CoT setting. Table 5 shows that using mPLUG-DocOwl2 with our hierarchical approach boosts ROUGE-1 to 42.48 and METEOR to 26.72 compared to using the model in the CoT setting. However, it does not outperform our method using MiniCPM. These findings highlight that the segmentation and summarization approach substantially improves performance compared to using the poster as a single input."}, {"title": "Challenges in Human Evaluation and Reliance on Automatic Metrics", "content": "Evaluating scientific summaries against their posters is both costly and logistically complex for human annotators. Scientific posters consist of dense technical content (including specialized terminology, tables, figures, and equations), requiring domain expertise and making the recruitment of qualified annotators time-consuming and expensive. Moreover, the diversity of research topics could lead to inconsistent judgments even among experts. For this reason, we rely on automatic metrics. Additionally, we conducted a factuality evaluation, as discussed in Appendix A. However, existing factuality metrics, such as SummaC Conv (Laban et al., 2022) and FActScore (Min et al., 2023), perform poorly on scientific text, highlighting the need for improved evaluation methods for multimodal scientific data."}, {"title": "Conclusions", "content": "We presented POSTERSUM, a multimodal benchmark for scientific poster summarization comprising 16,305 poster-abstract pairs. Our experiments show that even state-of-the-art MLLMs struggle with key aspects of scientific poster summarization. Furthermore, we propose SEGMENT & SUMMARIZE, a hierarchical approach that outperforms existing models by breaking down the summarization task into localized segments before generating a cohesive abstract. We find that our method outperforms MLLMs in both zero-shot and fine-tuned settings and that there remains significant room for improvement in multimodal understanding of complex scientific documents such as posters. We believe POSTERSUM will be a valuable resource for developing and evaluating MLLMs capable of processing information-dense scientific content."}, {"title": "Limitations", "content": "While our work advances scientific poster summarization, we should highlight a few limitations. First, our dataset is restricted to machine learning conference posters from 2022 to 2024, which may limit the generalization to other scientific domains. Second, while practical, automated topic labeling using GPT-40 may introduce biases or inaccuracies in the topic distribution. The proposed SEGMENT & SUMMARIZE method relies heavily on the quality of the initial segmentation \u2014 suboptimal segmentation can lead to fragmented or redundant local summaries. Our method also assumes that the content can be meaningfully decomposed into spatial regions, which may not hold for posters with complex cross-referencing or interdependent visual elements. We considered the abstract as a ground-truth summary of the poster, but the poster may sometimes differ from the paper."}, {"title": "Ethics Statement", "content": "Dataset. All the scientific posters and abstracts in our dataset are sourced from publicly accessible conference resources. Additionally, we sought permission from the conference website contacts to use the publicly available data for research purposes.\nMultimodal Large Language Models. This paper utilizes pre-trained multimodal large language models, which have been shown to exhibit various biases, occasionally hallucinate, and generate non-faithful text. Therefore, summaries generated using our dataset should not be released without automatic filtering or manual verification to ensure accuracy and reliability.\nBias. Despite efforts to include a wide range of posters, the dataset may not fully represent the diversity of research poster styles, languages, or scientific disciplines. As a result, models trained on POSTERSUM may exhibit biases towards the types of posters included in the dataset. Future work should consider expanding the dataset to encompass a broader spectrum of academic fields and visual formats to mitigate potential biases."}, {"title": "A Factuality Evaluation", "content": "To evaluate the performance of our method in generating factually correct summaries, we compute two text-based metrics: SummaC Conv (Laban et al., 2022) and FActScore (Min et al., 2023) on the best models in each category. Following common practice in long-document summarization evaluation (Fabbri et al., 2022; Saxena and Keller, 2024), we treat the reference summary as the ground truth (instead of the original document, which is poster image) when computing these metrics. Table 6 presents the results for both metrics on the generated summaries.\nBoth metrics perform poorly, as they are not specialized for scientific text. SummaC scores were substantially low, while FActScore showed extremely high values, indicating failures in natural language inference and atomic fact extraction for scientific text. We found factuality evaluation to be a challenging task in this domain, highlighting the need for new methods to measure factual accuracy in multimodal scientific documents such as posters."}, {"title": "Prompt Templates", "content": "Write an abstract for an Al conference paper for the given research poster image."}, {"title": "Effect of Poster Text Content on Summarization Performance", "content": "To investigate whether posters with a high amount of text result in better summarization performance, we analyze the relationship between OCR-extracted text length and ROUGE-L scores using our SEGMENT & SUMMARIZE method. Specifically, we use MMOCR to extract text from each poster and compute its total length in characters (not in tokens).\nFig. 5 presents the mean ROUGE-L scores across different OCR text-length bins. The dotted line represents the number of posters in each text-length bin. We observe that summarization performance tends to improve as the amount of text in poster increases. However, the correlation remains weak (Pearson r = 0.213, Spearman r = 0.210), suggesting that text in poster alone is not a strong predictor of summarization quality. Low performance in posters with minimal text also highlights the need for more robust multimodal understanding of figures, charts, equations, and tables."}, {"title": "Selecting the Number of Clusters", "content": "To select the number of clusters (k) for our SEGMENT & SUMMARIZE, we conducted an empirical analysis on a subset of 100 posters from the validation set, varying the number of clusters from 2 to 10. Fig. 6 presents the mean ROUGE-L score for each cluster configuration. In these experiments, the local and global summarization components remained fixed.\nWe observe that the best performance is achieved at k = 8 which was used in our final experiments. Additionally, we limit the maximum number of clusters to 10 in the analysis to keep the inference time of our local summarization manageable."}]}