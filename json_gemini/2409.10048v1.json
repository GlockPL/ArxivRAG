{"title": "Audio-Driven Reinforcement Learning for Head-Orientation in Naturalistic Environments", "authors": ["Wessel Ledder", "Yuzhen Qin", "Kiki van der Heijden", "Mortimer B. Zuckerman"], "abstract": "Although deep reinforcement learning (DRL) approaches in audio signal processing have seen substantial progress in recent years, audio-driven DRL for tasks such as navigation, gaze control and head-orientation control in the context of human-robot interaction have received little attention. Here, we propose an audio-driven DRL framework in which we utilise deep Q-learning to develop an autonomous agent that orients towards a talker in the acoustic environment based on stereo speech recordings. Our results show that the agent learned to perform the task at a near perfect level when trained on speech segments in anechoic environments (that is, without reverberation). The presence of reverberation in naturalistic acoustic environments affected the agent's performance, although the agent still substantially outperformed a baseline, randomly acting agent. Finally, we quantified the degree of generalization of the proposed DRL approach across naturalistic acoustic environments. Our experiments revealed that policies learned by agents trained on medium or high reverb environments generalised to low reverb environments, but policies learned by agents trained on anechoic or low reverb environments did not generalise to medium or high reverb environments. Taken together, this study demonstrates the potential of audio-driven DRL for tasks such as head-orientation control and highlights the need for training strategies that enable robust generalization across environments for real-world audio-driven DRL applications.", "sections": [{"title": "I. INTRODUCTION", "content": "For natural human-robot interaction, eye-contact between a robot and a talker is an important nonverbal signal [1], [2]. Eye contact is established through gaze direction and head orientation, both of which can be controlled in robotics using deep reinforcement learning techniques (DRL, [3]). Yet, very few DRL approaches have been proposed for gaze control [4] and, to the best of our knowledge, none for head-orientation control.\nIn contrast, DRL for robot navigation a related, comparable task - has received widespread attention [5]. Most DRL approaches in the field of robot navigation operate on visual input (e.g. [6], [7]), or on multi-modal audio-visual input (e.g. [8], [9]). While these multi-modal DRL approaches demonstrated that providing audio signals improves an agent's"}, {"title": "II. NATURALISTIC ACOUSTIC SCENES", "content": "We applied our audio-driven DRL approach to anechoic speech scenes and to naturalistic, reverberant speech scenes. Speech clips (10 s duration) were selected from the Lib-rispeech database [17]. We selected 30 speech clips for each of 40 randomly sampled talkers to generate the training set and 20 clips of 10 different talkers to generate an independent test set. We presented the selected speech clips at 64 talker locations in the frontal hemifield: 13 azimuth locations ranging"}, {"title": "III. AUDIO-DRIVEN REINFORCEMENT LEARNING APPROACH", "content": "Our goal is to find the shortest sequence of actions that orients the head of an agent from its starting position towards the direction of arrival of a talker's voice, such that the agent directly faces the talker (Fig. 1 A). We define this problem in terms of the orientation deviation between the agent's head and the talker's location in the environment in which the agent's objective is to eliminate the orientation deviation (OD). Fig. 1 B shows a 2-dimensional grid visualization of the task.\nOD is defined as the Euclidean distance between the direction of arrival of a talker's voice (V), [azimuthv, elevationv], and the head orientation of the agent (HOA), [azimuthHOA, elevationHOA]:"}, {"title": "A. Problem statement", "content": "Our goal is to find the shortest sequence of actions that orients the head of an agent from its starting position towards the direction of arrival of a talker's voice", "elevationv": "and the head orientation of the agent (HOA)", "elevationHOA": "n\n$OD = \\sqrt{(\\Delta azimuth)^2 + (\\Delta elevation)^2}$"}]}