{"title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with Finite Attention Scope", "authors": ["Xiaoran Liu", "Qipeng Guo", "Yuerong Song", "Zhigeng Liu", "Kai Lv", "Hang Yan", "Linlin Li", "Qun Liu", "Xipeng Qiu"], "abstract": "The maximum supported context length is a critical bottleneck limiting the practical application of the Large Language Model (LLM). Although existing length extrapolation methods can extend the context of LLMs to millions of tokens, these methods all have an explicit upper bound. In this work, we propose LongCache, a training-free approach that enables LLM to support an infinite context with finite context scope, through full-context cache selection and training-free integration. This effectively frees LLMs from the length extrapolation issue. We validate LongCache on the LongBench and L-Eval and demonstrate its performance is on par with traditional full-attention mechanisms. Furthermore, we have applied LongCache on mainstream LLMs, including LLaMA3 and Mistral-v0.3, enabling them to support context lengths of at least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of LongCache by GPU-aware optimization soon.", "sections": [{"title": "1 Introduction", "content": "Large Language Model (LLM) based on Transformers[1, 2, 3] has succeeded in the applications of Natural Language Processing (NLP). However, LLMs are still limited by the maximum supported context length. Therefore, research on length extrapolation has emerged, enabling LLM to maintain strong performance on context lengths beyond its training length[4, 5]. Traditional methods are based on full attention[5, 6, 7, 8], and mainly focus on adjusting the position embedding[9] to make LLM adapt to the position information it has not been trained on[5, 6, 10]. However, some studies have shown that even if the position information is not out-of-distribution (OOD), as the length of attention scope increases, the explosion of self-attention entropy still leads to performance decline[11, 12]. Additionally, the enormous memory and computational overhead introduced by full attention in a long context has become a major challenge for length extrapolation researches [13, 14, 15].\nFor human beings, although we have very long-term memories, we only need a small amount of information for thinking. Similarly, while LLM requires the full context to complete the entire inference process, it only needs limited context information at each inference step. This fact has inspired research on length extrapolation with sparse attention. Fixed sparse methods like StreamingLLM[14] and LM-Infinite[11] can maintain stable outputs when the input length increases by only retaining the starting and ending parts of the input. Comparatively, dynamic sparse methods like MInference[16] have achieved efficient inference on long contexts, even surpassing the performance of full attention. However, the fixed methods drop the contextual information directly, still leading to a performance decline in long-context benchmark[15, 17], while the dynamic methods, though able to process long contexts, neglect that sparse attention itself is also a strategy for extending context length[18, 19].\n*Work in progress.\n\u2020 Equal contribution."}, {"title": "2 Method", "content": "LongCache builds upon streaming input and makes a very simple modification to the original self-attention operation in LLMs, as shown in Figure 1. This modification includes two steps: a full-context cache selection and the training-free integration."}, {"title": "2.1 Full-Context Cache Selection", "content": "As discussed above, while LLM requires a complete long context to perform the entire inference process, a limited context segment is needed in each inference step[14, 18, 19]. Considering that the beginning and ending parts of the input context correspond to the globally important prompts and local information for inference[14], LongCache, similar to StreamingLLM[14], retains the global and local parts of the KV cache to be included in the self-attention transform.\n$$K_{cache} = [K_{global}, K_{middle}, K_{local}], V_{cache} = [V_{global}, V_{middle}, V_{local}].$$ (1)\nHowever, unlike StreamingLLM which completely discards the middle part of the KV cache, LongCache uses the query vector of the current step to perform a top-k selection on the middle part of the"}, {"title": "2.2 Training-Free Integration", "content": "LongCache concatenates the selected KV cache segments between the global and local parts, and then performs positional embedding sequentially, ignoring the absolute distance between the selected segments, while only preserving the relative order, as shown in Figure1b. Self-attention can then be applied to the concatenated KV cache.\n$$K_{cache} = [K_{global}, K_{select}, K_{local}], V_{cache} = [V_{global}, V_{select}, V_{local}],$$\n$$q_t, K_{cache} = PE (q_t, K_{cache}), O_t = SelfAttn (q_t, K_{cache}, V_{cache} ).$$ (3)\nIt's important to note that, unlike the conventional implementation in Huggingface Transformers[24], that positional embedding is applied before KV caching, in LongCache, the positional embedding is separated from KV cache and performed after the KV cache selection. That is to say, the cached KV does not include the positional information. The benefit is multifold. On one hand, as mentioned in Section4, the attention score without positional embedding is more conducive to locating the key information in the context.\nOn the other hand, since the length of the concatenated cache is controllable, as long as it is controlled within the pre-training context length or the extrapolation upper bound (if other extrapolation methods are used concurrently), the positional encoding will not be OOD[11]. Furthermore, the selected KV cache segments are not important in the current inference step, as their self-attention scores are very small[13]. Therefore, this modification will not harm the self-attention output and can also exclude the interference of irrelevant information[13, 25]. Thus, LongCache can disregard the position information, achieve an unlimited attention context with a limited attention scope without any training, and keep compatible with the existing attention acceleration methods[26, 27]."}, {"title": "3 Experiment", "content": ""}, {"title": "3.1 Setup", "content": "We conduct experiments on LLaMA2-7B-4K[28], LLaMA3-8B-8K[20], InternLM2-7B-200K[29],\nInternLM2-1B-32K[29], InternLM2.5-7B-1M[30], Qwen2-7B-128K[31], Qwen2-1B-32K[31],\nMistral-v0.3-7B-32K[21]. For all models, the hyper-parameters of LongCache are summarized\nas shown in Table1. The main reason for this setting is that it allows the attention window size\nclose to the pre-training context length, thus adapting LLM to the length distribution learned in the\npre-training stage. We use the OpenCompass[32] to conduct the following validation. All experiments\nare performed with FP16 precision and accelerated with FlashAttention2[27]."}, {"title": "3.2 Long Context Benchmark Evaluation", "content": "We first evaluate all 8 LLMs on the commonly used long-context benchmark LongBench[33], with the default context length set to 32K and middle truncation. For LLaMA2-7B-4K and LLaMA3-8B-8K,"}, {"title": "3.3 Needle-In-A-Haystack Evaluation", "content": "Building upon the long-context benchmark evaluation, we use models with strong retrieval capabilities within their training context lengths and conduct the Needle-In-A-Haystack evaluation[32, 35]. We perform experiments on 4 A100 GPUs and allow LLMs with LongCache to extend their context lengths until the memory explosions or runtime errors. As shown in Figure2, LLMs with LongCache maintain remarkably high retrieval accuracy across the entire range of context lengths they could support, regardless of their original attention window sizes. Notably, we also extend the context of mainstream LLMs like LLaMA3-8B-8K[20] and Mistral-v0.3-7B-32K[21] to at least 400K, providing the community with an embarrassingly easy yet effective solution for deploying long-context LLMs.\nWe also compare the performance of LongCache with the mainstream extrapolation approach, Dynamic NTK[6], on the Multi-Needle-In-A-Haystack evaluation[3, 32]. Since this task requires stronger instruction-following capabilities, we performed the comparison on the LLaMA3-Instruct-8B-8K[20] as shown in Figure3. While LongCache shows some fluctuations in different context lengths, it still maintains a clear advantage over Dynamic NTK, which shows a rapid decline beyond a certain context length. This demonstrates that LongCache can extend the context length disregarding the position information and utilize a finite attention window to achieve an infinite attention context."}, {"title": "4 Discussion", "content": "In the computation process of LongCache, the full-context cache selection is based on the dot product between the query and key vectors without position embedding. This can create a gap between the"}, {"title": "6 Conclusion", "content": "In this paper, we introduce LongCache, an embarrassingly easy but highly effective method to extend the context length of LLMs. LongCache employs full-context cache selection and training-free integration, using a finite attention window to realize an infinite context length in each inference step. We evaluate the performance of 8 popular models on LongBench and LEval to validate the long-context capabilities of LongCache. For LLMs without long context, LongCache can achieve the same extrapolation effect as the dominant Dynamic NTK method. For long-context LLMs, LongCache is on par with full attention in performance. LongCache has been shown to successfully extend the context of mainstream LLMs, including LLaMA3-8B-8K and Mistral-v0.3-7B-32K, to up to 400K tokens. We plan to optimize the efficiency of LongCache in a GPU-aware manner."}]}