{"title": "MoMQ: Mixture-of-Experts Enhances Multi-Dialect Query Generation across Relational and Non-Relational Databases", "authors": ["Zhisheng Lin", "Yifu Liu", "Zhiling Luo", "Jinyang Gao", "Yu Li"], "abstract": "The improvement in translating natural language to structured query language (SQL) can be attributed to the advancements in large language models (LLMs). Open-source LLMs, tailored for specific database dialects such as MySQL, have shown great performance. However, cloud service providers are looking for a unified database manager service (e.g., Cosmos DB from Azure, Amazon Aurora from AWS, Lindorm from AlibabaCloud) that can support multiple dialects. This requirement has led to the concept of multi-dialect query generation, which presents challenges to LLMs. These challenges include syntactic differences among dialects and imbalanced data distribution across multiple dialects. To tackle these challenges, we propose MoMQ, a novel Mixture-of-Experts-based multi-dialect query generation framework across both relational and non-relational databases. MoMQ employs a dialect expert group for each dialect and a multi-level routing strategy to handle dialect-specific knowledge, reducing interference during query generation. Additionally, a shared expert group is introduced to address data imbalance, facilitating the transfer of common knowledge from high-resource dialects to low-resource ones. Furthermore, we have developed a high-quality multi-dialect query generation benchmark that covers relational and non-relational databases such as MySQL, PostgreSQL, Cypher for Neo4j, and nGQL for NebulaGraph. Extensive experiments have shown that MoMQ performs effectively and robustly even in resource-imbalanced scenarios.", "sections": [{"title": "1 Introduction", "content": "The ability to convert natural language into structured query language (SQL) has made it much easier to interact with relational database management systems. In recent years, the use of large language models (LLMs) has significantly improved SQL generation tasks [11, 19]. This shift has enhanced the quality of generated queries, moving away from encoder-decoder-based approaches [10, 18, 20] to those driven by LLMs. Open-source LLMs [3, 19, 27] that have been fine-tuned through supervision have become the primary method due to their lower data privacy risks and cost compared to closed-source LLMs [1-3]. These LLMs are typically designed to work best with a specific database dialect, like MySQL. However, for general database management services in cloud computing, LLMs that support most dialects are needed. Therefore, SQL generation LLMs should not only cover major dialects like MySQL and PostgreSQL but also non-relational graph databases such as Neo4j [25] and NebulaGraph [31]. Figure 1 illustrates the similarities and notable differences of the queries for the same question across different databases. The differences in relational database query languages are mainly seen in the usage of specific keywords, while the discrepancies between relational and non-relational database query languages are more distinct, reflecting differences in the underlying query logic. These differences are collectively referred to as the database dialect issue.\nThis task is regarded as multi-dialect query generation. Past research [4, 29, 35] has shown that training models on multiple tasks can help them integrate knowledge from different sources, leading to improved performance compared to training on a single task. However, directly fine-tuning dense LLMs on multi-dialect data encounters several challenges:"}, {"title": "2 Related Work", "content": "MoMQ is a multi-dialect query generation framework that is highly related with the tuning-based text-to-SQL task and Mixture-of-Experts."}, {"title": "2.1 Tuning-Based Text-to-SQL", "content": "Before the era of large-scale models, the text-to-SQL task typically employs an encoder-decoder-based architecture. Research in this domain primarily focuses on enhancing the encoder's ability to"}, {"title": "2.2 Mixture-of-Experts", "content": "In recent years, Mixture-of-Experts (MoE) has emerged as an effective structure for reducing inference computational costs and enhancing multi-task learning capabilities in scenarios where model parameters are continuously scaled up [5, 7, 8, 14, 16, 33, 36, 38]. A gate unit is then utilized for expert activation. However, training MoE models from scratch or converting dense models into MoE through upcycling requires substantial pre-training costs [15, 24, 33]. Recent studies have proposed the construction of MoE based on the Low-Rank Adaptation (LoRA) module [9, 17, 32]. This approach not only mitigates catastrophic forgetting of pre-trained knowledge in dense models but also enhances the multi-task learning capability."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Task Definition", "content": "Given N natural language questions $Q = \\{q_1, q_2, q_N\\}$, a set of target query language dialects $L = \\{l_1, l_2\u00b7\u00b7\u00b7, l_M\\}$ and a database schema $D = \\{C, T\\}$, where C and T represent columns and tables. Formally, the goal is to learn a mapping function as:\n$\\mathcal{F}: (Q, L, D) \\rightarrow S$,\n(1)\nwhere for each input question $q_i \\in Q$, schema D and dialect $l_j \\in L$, the function $\\mathcal{F}$ generates a valid database query $S_{i,j}$."}, {"title": "3.2 Architecture Overview", "content": "The overall architecture of MoMQ is illustrated in Figure 2. MoMQ constructs MoE structure by leveraging LoRA modules as fine-grained experts. Dialect expert groups for each dialect and a shared expert group visible to all dialects are further introduced. The multi-level routing strategy, composed of a dialect router and an expert router, ensures the correct routing of tokens between different expert groups and within each group. When the tokens of different dialects are input, they are initially directed through the dialect router to the appropriate dialect expert groups. Within these groups, the tokens are further routed by the expert router to the final activated experts. Notably, all tokens are fully processed by the shared expert group, which is beneficial for the fusion and transfer of multi-dialect knowledge. Besides, to better assist the dialect router in effectively routing tokens of various dialects to the appropriate expert group, we introduce a novel Dialect Router Loss.\nIn the following sections, we first present the construction of the MoE structure, along with the design of the expert groups and dialect router. Then, we outline the training objectives of MoMQ."}, {"title": "3.3 MoE Construction", "content": "MoE structures are constructed in a variety of ways, such as pre-training from scratch or replicating multiple FFNs followed by a step of continual pre-training, all of which require additional pre-training to inject knowledge into the MoE [15, 24, 33]. Inspired by recent works [9, 17, 32]. We use a simple but efficient way to"}, {"title": "3.4 Dialect Expert Group", "content": "In multi-dialect generation, there are non-trivial syntax differences between two database dialects. For example, in MySQL, the \"DATE_ADD\" function can be used to add a specified time interval to a date or datetime value. While PostgreSQL uses the \"INTERVAL\" keyword along with the \"+\" operator to achieve a similar result. These differences will interfere with learning dialect-specific knowledge.\nTo address this issue, we design multiple dialect expert groups to isolate dialect-specific knowledge, thereby mitigating interference and improving the quality of generated queries. The expert group consists of multiple LoRA experts and a top-k expert router. Each database dialect has a separate expert group to learn knowledge of the corresponding query syntax. Concretely, given multiple dialect expert groups $G = \\{G_1, G_2, G_M\\}$ in a specific Transformer layer, the output of the i-th expert group is calculated as:\n$\\begin{aligned} \\\\ H_i = \\sum_{k=1}^{9_k} E_k\\\\\\end{aligned}\n$,\n(3)\n$\\begin{aligned} \\\\\\\\\\ S_k = \\begin{cases} \\sum_{s \\in TopK(\\{s_{1j} \\mid 1 \\leq j \\leq N\\}, K)} & \\\\\\ 10, & otherwise\\\\ \\end{cases}\n$,\n(4)\n$\\begin{aligned} \\\\\\\\\\ S = softmax(WH)_k, \\\\\\\\\\end{aligned}\n(5)\nwhere $E_k$ is the k-th LoRA expert in the i-th expert group, and N is the total number of experts in the group, $g_k$ denotes the k-th gate value for the expert, s denotes the token-to-expert affinity, TopK(, K) denotes the set comprising K highest affinity scores among those calculated for the tokens in all N experts, $W_i \\in R^{d \\times K}$ is a trainable matrix of the expert router, and H is the hidden states of all input tokens input. Note that $g_i$ is sparse, indicating that only K out of N gate values are nonzero. This sparsity property ensures computational efficiency within an expert group, i.e., each token will be assigned to and computed in only K experts.\nThe routing strategy within the expert group faces the problem of load imbalance [28]. This can lead to a situation known as routing collapse, where the expert router continually selects a limited set of experts, thereby inhibiting adequate training for the others. In order to mitigate the risk of routing collapse, we employ an expert-level balance loss, which is computed as follows:"}, {"title": "3.5 Shared Expert Group", "content": "There is a lack of inherent information communication between different dialects by using only a separate expert group for each dialect. When encountering data imbalance, it can negatively impact the common knowledge transfer from high-resource dialects to low-resource dialects. Meanwhile, multiple experts may converge in acquiring shared knowledge in their respective parameters, thereby resulting in redundancy in expert parameters. If there are shared experts dedicated to capturing and consolidating common knowledge across varying dialects, knowledge transfer will be more efficient and parameter redundancy will be alleviated.\nToward this objective, we further add a shared expert group to integrate information across multiple dialects at the sentence level. Regardless of the router module, all tokens in a sentence will be deterministically assigned to experts in the shared expert group. Formally, the MoE output in the complete MoMQ architecture is formulated as follows:\n$H = \\sum_{i=1}^{M}H_i + \\sum_{k=1}^{Ns}E_k$,\n(9)\nwhere M is the number of dialect expert groups, Ns is the number of shared experts and $E_k$ is the output of k-th shared expert."}, {"title": "3.6 Dialect Router", "content": "After the construction of multiple dialect expert groups, how to route the tokens of different dialects to the appropriate group remains to be addressed. Intuitively, the dialect router is able to make correct routing under the guidance of sentence-level dialect hard labels, thus forming a complete isolation between different dialect expert groups. However, there may exist certain similarities between different dialects, e.g., \"LIMIT\" and \"ORDER BY\" in relational and non-relational database dialects are both valid tokens. Moreover, different dialects exhibit a high degree of similarity in the understanding of natural language questions and database schemas. If these similar tokens have the opportunity to enter multiple dialect expert groups, especially from high-resource dialects to low-resource ones, may further facilitate token-level common knowledge transfer.\nTo this end, we have designed a novel dialect router trained with a Dialect Router Loss (DRL) incorporating dialect smoothing. Dialect smoothing is employed to further reduce dialect isolation by replacing hard dialect labels with a smooth distribution. This distribution assigns a lower value to the true dialect while allocating a"}, {"title": "3.7 The Training Objectives", "content": "Finally, we formulate the multi-dialect query generation task as a text-to-text problem. The training objective is to minimize the negative log-likelihood of output y conditioned on the input question x and the task prompt P. The fine-tuning loss on the task is defined as:\n$L_{FT} = - \\sum_{j}P(y_j|y  j; x, P) + \\alpha L_{DR} + \\lambda L_{Bal}$,\n(12)\nwhere $\\alpha$ and $\\lambda$ are hyper-parameters that are used to adjust the impact of auxiliary losses."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Datasets", "content": "There is currently no unified benchmark for evaluating the performance of LLMs in multi-dialect query generation. For the text-to-SQL task, there are several widely used benchmarks, such as Spider [34] and BIRD [21] in English, as well as Chase [12] in Chinese. All of these benchmarks are constructed based on SQLite. Additionally, there are multilingual benchmarks like MultiSpider [6], designed to assess the multilingual comprehension capability of the model. Consequently, we developed a training and evaluation dataset for"}, {"title": "4.2 Experimental Setup", "content": null}, {"title": "4.2.1 Evaluation Metric.", "content": "We consider two prevalent evaluation metrics: execution accuracy (EX) and executable (EXEC). The EX metric evaluates whether the predicted and ground-truth queries yield the same execution results on the database. The EXEC metric evaluates whether the generated query can be executed correctly in the corresponding database without syntax errors."}, {"title": "4.2.2 Models and Baselines.", "content": "MoMQ is a generic multi-task framework, especially for multi-dialect query generation, that can be used on a variety of open-source LLMs. We select three model sizes of current SOTA models, namely Qwen2-1.5B, Qwen2-7B, and Qwen1.5-14B from HuggingFace\u00b3, as backbones respectively to validate MoMQ's multi-dialect query generation capabilities and robustness. Note that Qwen2 does not currently have open-source 14B weights, so we select Qwen1.5-14B as an alternative. All models use the Instruct or Chat version instead of the Base version to obtain better performance.\nWe compare MoMQ with the following methods: (1) Single-dialect full fine-tuning, which fine-tunes all parameters of the model in each dialect data; (2) Multi-dialect full fine-tuning, which fine-tunes in a mixed dataset of multiple dialects; (3) Vanilla LoRA, which freezes the pre-trained model and replaces all linear layers with LoRA modules in a mixed dataset of multiple dialects."}, {"title": "4.2.3 Implementation Details.", "content": "Our experiments are conducted using PyTorch 2.3.1 on a computer running the Ubuntu 20 operating system, equipped with 8 NVIDIA A100 80GB GPUs. We establish a shared expert group with 2 LoRA experts and four dialect-specific expert groups corresponding to MySQL, PostgreSQL, Cypher, and nGQL. Each dialect expert group comprises 8 LoRA experts, with each input token activating the top-2 experts. For models with different parameter sizes, we employ varying expert dimensions: 64 dimensions for the 1.5B model, 128 dimensions for the 7B model, and 256 dimensions for the 14B model. To optimize the objectives, we use the AdamW optimizer with parameters set to \u03b2\u2081 = 0.9 and B2 = 0.95. The learning rate is set to 1e-6 for full fine-tuning and 1e6 for the others, accompanied by a weight decay of 0.1 and \u20ac = 0.1 for the smoothing factor. We set a = 0.1 and \u03bb = 0.001 to adjust the impact of auxiliary losses. All experiments are run for 3 epochs. Each experiment is repeated three times with different random seeds, and the mean values are reported. The random seed is shared by all compared methods for a fair comparison."}, {"title": "4.3 Results", "content": null}, {"title": "4.3.1 Full Data.", "content": "Experimental results in Table 1 indicate that MoMQ significantly outperforms both single-dialect and multi-dialect full fine-tuning over nearly 3-5 percent on EX and EXEC in the full data setting. Additionally, MoMQ shows consistent improvement as the model size increases from 1.5B to 14B. Notably, for the 1.5B"}, {"title": "4.3.2 Imbalanced Data.", "content": "We validate MoMQ's transfer capabilities in two data imbalance settings. The first is the MySQL high-resource setting, where we sample the entire MySQL training data and 128 samples from each of the other three dialects. As shown in Table 3, even under this setting, our method still yields stable better performance over full fine-tuning and LoRA. In particular, on the 14B model size, the average EX of the four dialects is improved by nearly 5 percent on average.\nThe second is the Cypher high-resource setting, where we sample the entire Cypher training data and 128 samples from each of the other three dialects. As shown in Table 4, MoMQ generally outperforms other methods. Notably, it achieves the highest average EX for the 1.5B, 7B, and 14B parameter models, particularly excelling in Cypher and nGQL due to their high syntactical similarity. It facilitates the transfer of extensive dialect-common knowledge from Cypher to nGQL. However, other methods occasionally perform better in EX, e.g., MySQL and PG. There are two primary reasons why MoMQ does not fully surpass performance on relational database dialects. First, the training data for relational dialects is insufficient, and the sparsity of the dialect expert group structure leads to incomplete convergence for some experts. Second, the differences between relational and non-relational databases are more pronounced, resulting in less transferable dialect-common knowledge across dialects."}, {"title": "4.4 Analysis", "content": "To further validate the effectiveness, robustness, and interpretability of MoMQ, we design a variety of analytical experiments. All experiments are conducted using the Qwen2-7B backbone in the full data setting."}, {"title": "4.4.1 Ablation Study.", "content": "To evaluate the effectiveness and impact of different components of MoMQ, we conduct ablation studies on Qwen2-7B. As shown in Table 5, we remove different components from MoMQ and evaluate the average EX. Upon conducting the ablations, notable decreases in performance are observed. Specifically, the exclusion of the shared expert group results in a drop in average EX from 49.15% to 48.57%. Further removal of the dialect router loss leads to an additional decline, with the average EX decreasing to 47.08%. In this case, tokens are routed with hard dialect labels at the sentence level, resulting in complete dialect isolation. The most significant reduction occurs when the dialect router is removed, resulting in an average EX of 44.79%. In this case, tokens are randomly assigned to the dialect expert groups without any supervision. Finally, eliminating dialect expert groups causes the entire structure to revert to LoRA, further reducing the EX to 43.48%. These results underscore the critical contributions of the shared expert group, dialect router loss, dialect router, and dialect expert group to the overall performance of MoMQ."}, {"title": "4.4.2 Effect of Expert Dimension.", "content": "We further analyze the impact of the expert dimension on the performance of MoMQ, where the expert dimension refers to the LoRA module's rank. As illustrated in Table 6, MoMQ demonstrates a consistent increase in average EX as the expert dimension increases from 16 to 128. Specifically, with an expert dimension of 128, MoMQ achieves the highest average EX of 49.15%. It is interesting to note that when the expert dimension is further increased to 256, there is a slight decrease in the average EX. This decline suggests a potential issue where a large expert dimension may lead to inadequate training within 3 epochs and consequently impact MoMQ's performance negatively."}, {"title": "4.4.3 Effect of Expert Number.", "content": "To comprehensively analyze the impact of the number of experts, we evaluate MoMQ with configurations of 8, 16, 32, and 64 experts, respectively. As shown in Table 7, MoMQ demonstrates robust performance across all configurations, particularly excelling with 8 and 32 experts. When the number of experts reaches 64, there is also a certain decline in MoMQ's performance. This is attributed to a similar reason as when the expert dimension reaches 256, indicating that the experts are not sufficiently trained."}, {"title": "4.4.4 Case Study.", "content": "Figure 3 shows the comparison of different methods for generating nGQL and MySQL queries. From the above results, only MoMQ generates the correct queries. For nGQL dialect, the multi-dialect full fine-tuning method uses the \"MATCH\" statement instead of the \"GO FROM\" statement, which is interfered by"}, {"title": "4.4.5 Expert Weight Distribution.", "content": "To further analyze the effect of the multi-level routing strategy, we collect the output logits of the dialect router and the expert router from all Transformer layers when generating the nGQL query in the case study. As illustrated in Figure 4, under the constraint of the Expert Balance Loss, expert weights within each dialect expert group remain balanced after"}, {"title": "5 Conclusion", "content": "In this paper, we propose MoMQ, a novel Mixture-of-Experts-based multi-dialect query generation framework across relational and non-relational databases. MoMQ employs specific dialect expert groups for each dialect to isolate dialect-specific knowledge and mitigate generation interference. To deal with multi-dialect data imbalance, we introduce a shared expert group to enhance the transfer of common knowledge from high-resource dialects to low-resource dialects. We further design a multi-level routing strategy that consists of a dialect router and an expert router to ensure correct routing at the token level. The dialect router enhances knowledge transfer across expert groups with the help of the Dialect Router Loss. We have constructed a high-quality multi-dialect dataset covering MySQL, PostgreSQL, Cypher for Neo4j, and nGQL for NebulaGraph. Both the code and multi-dialect datasets will be openly released to support further research in this area."}, {"title": "Ethical Considerations", "content": "The deployment of database query generation systems raises several ethical considerations, particularly regarding data privacy, security, and misuse. First and foremost, such systems must ensure that sensitive data is protected from unauthorized access. As these models parse and generate queries from natural language, there is a risk of inadvertently exposing confidential information if proper"}]}