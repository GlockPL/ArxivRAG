{"title": "Baklava - Budgeted Allocation of KV cache for Long-context Inference", "authors": ["Ahmed Burak Gulhan", "Krishna Teja Chitty-Venkata", "Murali Emani", "Mahmut Kandemir", "Venkatram Vishwanath"], "abstract": "In Large Language Model (LLM) inference, Key-Value (KV) caches\n(KV-caches) are essential for reducing time complexity. However,\nthey result in a linear increase in GPU memory as the context\nlength grows. While recent work explores KV-cache eviction and\ncompression policies to reduce memory usage, they often consider\nuniform KV-caches across all attention heads, leading to subop-\ntimal performance. We introduce BaKlaVa, a method to allocate\noptimal memory for individual KV-caches across the model by\nestimating the importance of each KV-cache. Our empirical anal-\nsis demonstrates that not all KV-caches are equally critical for\nLLM performance. Using a one-time profiling approach, BaKlaVa\nassigns optimal memory budgets to each KV-cache. We evaluated\nour method on LLaMA-3-8B, and Qwen2.5-7B models, achieving\nup to a 70% compression ratio while keeping baseline performance\nand delivering up to an order-of-magnitude accuracy improvement\nat higher compression levels.", "sections": [{"title": "CCS CONCEPTS", "content": "\u2022 Computing methodologies \u2192 Machine learning approaches."}, {"title": "KEYWORDS", "content": "LLM, KV-cache, inference, GPU memory"}, {"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have achieved great success in\nrecent years and have been successfully used in several natural\nlanguage processing tasks such as chatbots, search engines, sum-\nmarization, and customer service. This success has led to the devel-\nopment of LLMs with exponentially increasing parameter counts\nand context lengths (how much previous text an LLM can remem-\nber), with the latest models having more than a trillion parameters\nwith more than a million context lengths [16, 14]. Although larger\nmodels with longer context lengths have improved performance,\nthey come at the cost of significantly higher GPU memory usage\nduring inference, posing challenges for efficient deployment.\nLLMs generate text in an autoregressive manner given an\ninput of any length, the model generates only a single token (word).\nTo generate multiple tokens, the previously generated token is\nappended to the input, and the process repeats. This method of\ninference leads to significant 'redundant computations', leading\nto quadratic time complexity. To mitigate this inefficiency, LLMs\nemploy Key-Value (KV) caches to store previous calculations\nkey and value tokens for each attention head\u00b9 \u2013 and remove these\nunnecessary computations. Yet, this comes at the cost of substantial\nGPU memory to hold these tokens, limiting how many tokens can\nbe stored and, thus, a limit to how much an LLM can remember.\nThis is currently one of the major bottlenecks in LLM scaling for\nlong-context inference.\nRecent works have tried to address this challenge mainly by\nreducing the amount of data that an LLM needs to cache. How-\never, many of these compression-based policies allocate memory\nuniformly across all KV caches, which is suboptimal. Recent re-\nsearch [19, 21, 6], has begun to explore the benefits of assigning\nheterogeneous memory budgets to different KV-caches. The key\nchallenge in this approach lies in determining the optimal alloca-\ntion of KV-cache memory, that is, which KV-caches in an LLM are\nmore or less critical than the others to model performance.\nIn our work, Baklava, we demonstrate that different attention\nheads in an LLM have varying levels of importance, and therefore\nKV-cache memory should be allocated accordingly- more impor-\ntant heads receiving larger (space for) KV-caches and less important\nones receiving smaller allocations. To achieve this, we introduce\na one-time 'profiling' method, which does not require fine-tuning\nof the LLM. Using a simple heuristic, our method estimates the\nimportance of each attention head and optimally distributes a given\nKV-cache budget to maximize inference performance. We run multi-\nple benchmarks with different KV-cache eviction and compression\npolicies and show that our method can increase the inference qual-\nity up to an order of magnitude, without using additional memory or\ncomputation, and allows for near-baseline (a cache with maximum\ncontext length) performance for lower compression ratios.\nBakLaVa is complementary to most existing KV-cache manage-\nment and optimization methods, such as FlashAttention [3] and\nVLLM [11], as well as various KV-cache compression, eviction, and\noffloading policies. We emphasize that our proposed method is not\na policy for managing KV-cache memory or optimizing KV-cache\ncalculations; rather, it is a method for allocating memory budgets\namong existing KV-caches in an LLM.\nContributions: The main contributions of this paper can be\nsummarized as follows:"}, {"title": "2 BACKGROUND", "content": "2.1 Self-Attention\nConsider an input matrix $Z \\in \\mathbb{R}^{T\\times D}$, where $T$ represents the se-\nquence length and $D$ is the feature dimension. The multi-head\nself-attention mechanism facilitates learning from different rep-\nresentational subspaces by executing multiple attention compu-\ntations in parallel. Query (Q), Key (K), and Value (V) are derived\nfrom linear projections: $Q = ZM_Q, K = ZM_K$, and $V = ZM_V$,\nwhere $M_Q, M_K, M_V \\in \\mathbb{R}^{D \\times D_h}$ are trainable weight matrices. The\nattention weights are computed via scaled dot-product attention,\nas given in Eq. 1:\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{D_h}})V$.\n(1)\nThis process is repeated over $H$ heads, each utilizing distinct weight\nmatrices $M_Q^{(h)}, M_K^{(h)}, M_V^{(h)}$. The concatenated outputs from all heads\nare projected back to the original dimension $D$ using a learned\nweight matrix $M_O \\in \\mathbb{R}^{HD_h \\times D}$:\n$MultiHead(Q, K, V) = Concat(head_1,..., head_H) M_O$, (2)\nwhere each attention head is defined as follows:\n$head_h = Attention(Q^{(h)}, K^{(h)}, V^{(h)})$.\n(3)\n2.2 Key-Value (KV) Cache\nDuring autoregressive LLM inference, the tokens are generated\nsequentially. Without caching, Key (K) and Value (V) matrices are\nrecomputed at each generation step for all preceding tokens. KV\ncaching mitigates this inefficiency by storing computed K and V\nprojections. Rather than recomputing these values, the model re-\ntrieves and appends the cached matrices to the current token's\nprojections. The updated attention computation follows Eq. 4:\n$Attention(Q_t, [K_{1:t-1}; K_t], [V_{1:t-1}; V_t])$, (4)\nwhere [;] denotes concatenation along the sequence axis, and\ncached values $K_{1:t-1}, V_{1:t-1}$ are loaded from memory. Although\nKV caching reduces redundant computation, storing cached projec-\ntions for each token demands substantial memory, growing linearly\nwith sequence length. For a transformer with $L$ layers, $H$ heads, and\nsequence length $T$, memory consumption scales as $2 \\times T \\times L \\times H \\times 16$-\nbit.\n2.3 KV-Cache Eviction\nKV-Cache eviction aims to eliminate less significant tokens from\n$K_{1:t-1}$ and $V_{1:t-1}$ using a function $f_{evict}$ that identifies and\nremoves redundant elements. The eviction mechanism is depicted\nin Eqs. 5 and 6, where the $m^{th}$ token is removed from the cache.\n$f_{evict}(K_{1:t-1}) = K'_{1:t-1}$\n$= [k_1,..., k_{m-1}, k_{m+1},..., k_{t-1}]$ (5)\n$f_{evict}(V_{1:t-1}) = V'_{1:t-1}$\n$= [v_1,..., v_{m-1}, v_{m+1},..., v_{t-1}]$. (6)\nAfter eviction, attention is computed using the reduced cache, as\nshown in Eq. 7:\n$softmax(\\frac{Q_t[K'_{1:t-1}; K_t]}{\\sqrt{D_h}})[V'_{1:t-1}; V_t]$. (7)"}, {"title": "3 THE BAKLAVA METHOD", "content": "Our method for optimizing KV-cache memory allocation consists\nof 3 main steps for a given LLM: (i) A one-time collection of profil-\ning data for a given prompt(s) (Algorithm 1 step 1); (ii) Using a\nheuristic to estimate the 'importance' of KV caches, which is also a\none-time calculation (Algorithm 1 step 2); and (iii) Performing\na parameter search to allocate memory accordingly (Algorithms 2\nand 3).\nAll three steps in BaKlaVa only need to be run once. The most\ntime-consuming part currently is Step (iii), where a parameter\nsearch is performed for the target compression level. For this pa-\nrameter search, we quickly evaluate each parameter combination\nusing 'perplexity', rather than running a long-context evaluation\nbenchmark, since perplexity does not require autoregressive token\ngeneration and consequently is much faster and gives a good ap-\nproximation of actual performance. This parameter search, for the\nmodels we evaluated (which contain 7 to 8 billion parameters), takes\n10 to 20 minutes on 8x A100 GPUs for around 200 combinations\nof parameters on 98k tokens for a chosen compression ratio. The\nnumber of tokens can be decreased for a proportional decrease in\nruntime, though they should be at least as much as the maximum\ncontext length being evaluated.\nOnce the ideal parameters for an LLM are obtained, no addi-\ntional computation is required. To begin inference, we initialize\nour custom huggingface transformers' KV-cache object to use in\ninference.\n3.1 Determining KV-cache Importances\n3.1.1 Head Importance Heuristic. To determine the significance\nof an individual attention head, we used several key observations\nto come up with a heuristic. The first is that the more change"}, {"title": "3.2 Assigning Memory Budgets to KV-Caches", "content": "3.2.1 Memory Allocation. Once the importance values for each\nKV-cache and layer are obtained, the next step is to determine how\nto allocate memory budgets.\nBased on our observation of token similarities as shown in Fig-\nure 2, we find that low-importance attention heads are more con-\nsistent with how they change each individual token in a prompt\n(that is, the dot-product between input and output of the attention\nhead has low variance), while other attention heads can display\nsignificant changes across tokens (that is, high variance in the token\ncosine similarity). Thus, to reduce the chances for decreasing the\nmemory of KV-caches belonging to potentially critical attention\nheads, we take a conservative approach and only target KV-caches\nwith an importance score below a threshold $t$ by a predetermined\namount $r$, as shown in Algorithm 2. The freed memory is then\nassigned to up to top $n$ KV caches of highest importance (where $n$\nis the number of low-importance KV-caches selected), in order to\nprioritize increasing memory for the most important KV-caches.\n3.2.2 Parameter Search. To determine the optimal values for $r$ and\n$t$, we performed a parameter search over different compression"}, {"title": "3.3 Empirical Evaluation of Heuristics", "content": "To evaluate the effectiveness of our layer and KV-cache importance\nheuristics in comparison to the 'true' importance, we conducted\ncomputationally intensive experiments. These tests empirically\nassessed the impact of individual layers on model performance\nby measuring the variation in benchmark scores resulting from\nmodifications to each component. The underlying principle is that\nthe greater the performance degradation caused by a change (e.g.,\nmemory reduction) in a layer or KV-cache, the more critical that\ncomponent is to the model's overall functionality.\nTo evaluate the layer importance heuristic (see Section 3.1.4) we\ntested our LLM on the triviaqa LongBench dataset after reducing\nthe memory allocated to different groups of layers. We selected a\nsingle dataset to minimize the computational cost of our empirical\nevaluation. triviaqa was specifically chosen because it exhibits the\nwidest range of scores, making it more sensitive to performance\nvariations and thus a better candidate for detecting changes in\noutput.\nAs described in algorithm 4, we systematically reduced the mem-\nory budgets of layers within a sliding window of size 5, running a\nseparate benchmark for each window position. Rather than eval-\nuating individual layers in isolation, we compressed groups of 5\nadjacent layers at a time. If crucial layers were arbitrarily scattered,\nrather than forming coherent clusters, it would suggest an unintu-\nitive and unlikely distribution of importance. Additionally, testing"}, {"title": "4 RESULTS", "content": "In this section, we report the results of BaKlaVa for KV-cache\ncompression on the models LLaMA-3-8B and Qwen2.5-7B. Qwen\nweights are quantized to 8 bits due to hardware limitations. Sec-\ntion 4.1 shows the results of LongBench on different KV-cache\nreduction methods for different compression ratios. Section 4.2 re-\nports the results of empirically evaluating how well the heuristics\nused in BaKlaVa reflect actual layer and KV-cache importances."}, {"title": "4.1 LongBench", "content": "We used the LongBench [2] evaluation suite to test how our pro-\nposed approach works with real-life scenarios with significant\nKV-cache memory usage. LongBench contains 14 English (qmsum,\nmultifieldqa_en, triviaqa, hotpotqa, samsum, musique, multi_news,\n2wikimqa, gov_report, trec, narrativeqa, passage_count, passage_re-\ntrieval_en, and qasper) and 2 coding tasks (lcc and repobench-p),\nwith average contexts for these tasks ranging between 5000 and\n15000 tokens - though the longest contexts exceed 32000 tokens.\nNote that LongBench's default prediction script allows truncat-\ning prompts longer than a user-defined threshold (the max_length\nparameter) before being input into the LLM model, so that the in-\nstructions at the beginning or end of the prompt are not removed.\nWe used the default LongBench configuration, where the prompt\nis truncated to a size below the maximum context length of the\nLLM. For our results, this equals a max_length of 7500 tokens if the\ncontext length is 8196 (LlaMA-3 8B) and 31500 tokens for a context\nlength of 32768 (qwen-2.5 7B)\nLongBench results for triviaqa, samsum (few-shot learning),\nrepobench-p (coding), 2wikimqa, hotpotqa (multi-document Q&A),\nmultifieldqa_en (single-document Q&A), gov_report (summariza-\ntion) and the total aggregate scores of all 16 English and coding\ntasks are plotted in Figure 3. Higher values indicate better perfor-"}, {"title": "4.2 Empirical Evaluation of Heuristics", "content": "Figure 4 presents the empirical test results (detailed in Section 3.3)\nfor the LlaMA3-8B, Qwen2.5-7B, and Mistral-7B models, compared\nagainst the layer importance heuristics used in BaKlaVa and SqueezeAt-\ntention for estimation of layer importance with low computational\ncost. The heuristic results are expressed as similarity scores, as\ndefined in Section 3.1, while the empirical results are reported as av-\nerage LongBench scores on the triviaqa dataset across compression\nratios ranging from 0.5 to 0.95. Both heuristic and empirical results\nare visualized using a red-yellow-green color gradient, where less"}, {"title": "5 RELATED WORKS", "content": "In this section, we discuss previous work relevant to BaKlaVa in\nfive main areas: KV-cache eviction policy, profiling for determining\nmemory budget, KV-cache quantization, cache merge, and system-\nlevel optimizations.\n5.1 KV Cache Eviction Policy\nStreamingLLM [20] discovered the 'attention sink' effect, where\nearly sequence tokens play a crucial role in maintaining model\nperformance through asymmetric attention weight accumulation.\nH2O [25] introduces an eviction strategy based on cumulative at-"}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this work, we introduce Baklava, a simple yet effective approach\nto optimize LLM inference through intelligent KV-cache memory\nallocation. By leveraging a heuristic-based method to estimate layer\nand KV-cache importance, BaKlaVa significantly improves memory\nefficiency while maintaining model performance across a range of\ncompression ratios. Our empirical evaluations demonstrate that\nBaKlaVa outperforms existing KV-cache allocation strategies, such\nas uniform allocation (StreamingLLM) and allocating KV-cache\nmemory with layer-wise granularity (SqueezeAttention), particu-\nlarly in tasks where preserving long-range dependencies is cru-\ncial. Notably, BaKlaVa maintains near-baseline performance up to\n70% compression, surpassing alternative methods on long-context\ndatasets by preserving essential information and achieving higher\naccuracy under high compression across multiple tasks\nA key advantage of our method is its ability to adapt to different\nmodel architectures by dynamically adjusting memory allocation\nbased on computationally inexpensive heuristics. Unlike prior ap-\nproaches that apply uniform compression or coarse layer-wise\ncompression, BaKlaVa efficiently distributes KV-cache memory to\nmaximize performance under constrained budgets. These improve-\nments highlight the potential of fine-grained memory allocation\nin enhancing the efficiency of LLM inference without requiring\nmodifications to model architecture or training procedures.\nIn future work, our aim is to develop a generalized framework for\nadaptive KV-cache memory allocation, reducing the need for man-\nual parameter tuning. Additionally, extending BaKlaVa to support\nadditional KV eviction policies and dynamically adjusting mem-\nory budgets at runtime could further enhance its applicability to\nreal-world deployments."}, {"title": "APPENDIX", "content": "A FULL LONGBENCH RESULTS\nB PARAMETER SEARCH RESULTS"}]}