{"title": "PoM: Efficient Image and Video Generation with the Polynomial Mixer", "authors": ["David Picard", "Nicolas Dufour"], "abstract": "Diffusion models based on Multi-Head Attention (MHA) have become ubiquitous to generate high quality images and videos. However, encoding an image or a video as a sequence of patches results in costly attention patterns, as the requirements both in terms of memory and compute grow quadratically. To alleviate this problem, we propose a drop-in replacement for MHA called the Polynomial Mixer (PoM) that has the benefit of encoding the entire sequence into an explicit state. PoM has a linear complexity with respect to the number of tokens. This explicit state also allows us to generate frames in a sequential fashion, minimizing memory and compute requirement, while still being able to train in parallel. We show the Polynomial Mixer is a universal sequence-to-sequence approximator, just like regular MHA. We adapt several Diffusion Transformers (DiT) for generating images and videos with PoM replacing MHA, and we obtain high quality samples while using less computational resources.", "sections": [{"title": "1. Introduction", "content": "In a sudden change of pace, high quality image and video generation have evolved from a task seemingly impossible to achieve to a task almost solved by available commercial or open-source tools like Stable Diffusion 3 [19], Sora [7] or MovieGen [61]. At the heart of this success lies the Multi-head Attention (MHA) in the transformer architecture [72] that has excellent scaling properties [58, 82]. These so-called scaling laws [44] enable brute-forcing complex problems such as image and video generation by using very large models trained on gigantic data, at the expense of an ever increasing computational cost. The main focus of current research lies thus in scaling transformer-based approaches to larger models handling larger datasets.\nThe issue with transformers is that the computational cost increases quadratically with the sequence length due to the pairwise computation in MHA. This means that generating an image at twice the spatial resolution (respectively a video at twice the resolution and double the duration) results in 4 times more patches and thus 16 times more computational cost (respectively 8 times more patches and thus 64 times more computational cost). Attempts at having transformers with sub-quadratic complexity [11, 47, 76] introduce the additional constraint of fixing the number of tokens, which prevents generating images or videos of different sizes. Alternatively, recurrent models such as State-Space Models (SSM) [26, 27] have been investigated for the task [38, 69, 79] since their complexity is linear with the sequence length [25]. However, they introduce an arbitrary causal raster scan of the sequence that does not fit the 2D geometry of images very well.\nIn this paper, we enable better scaling in large generative models by introducing a new building block called the Polynomial Mixer (POM). PoM has a linear complexity"}, {"title": "2. Related Work", "content": "Diffusion Diffusion models [35, 57, 67] learn a neural operator that produces natural images from noise using a forward-reverse set of processes. The forward process consists in pushing the distribution of natural images forward to a known distribution, typically Gaussian, which can be done by adding increasing level of noise to the image. The reverse process does not have an explicit solution, but can be approximated by a neural network by regressing the local inverse of the forward process, i.e., solving\n$\\min_{\\theta} \\mathbb{E}_{t \\sim u(0,1)} [|| \\varepsilon_t - f_\\theta(x_t,t)||^2]$, (1)\ns.t. $x_t = \\sqrt{\\alpha_t} x_0 + \\sqrt{\\gamma_t} \\varepsilon_t, \\varepsilon_t \\sim \\mathcal{N}(0,1)$. (2)\nHere, $\\alpha_t$ and $\\gamma_t$ are chosen such that $x_0$ corresponds to a natural image whereas $x_1$ corresponds to pure Gaussian noise. A great amount of research has been put into finding better noise schedules ($\\alpha_t$ and $\\gamma_t$) [4, 31, 45], or improving the quantity that is regressed [51, 52, 64], keeping the general idea of learning to invert step by step the stochastic differential equation that transforms an image into noise.\nFor image and generation, most efforts have been poured into designing efficient architectures at the task. While the original DDPM papers [35, 57] sample images in pixel space, making it unsuitable for large resolution, the most groundbreaking improvement was introduced by Stable Diffusion [63] with the addition of a variational auto-encoder (VAE) that allows the diffusion process to be performed in a lower dimensional latent space. Stable Diffusion uses a U-Net architecture complemented by attention layers [63, 65]. To benefit more from the scaling properties of transformers [44, 82], simpler approaches based solely on transformer layers has been proposed in DiT [58] and the subsequent flow-matching version SiT [56]. Most modern text-to-image generation models are now based on Transformer layers rather than the U-Net [9, 19, 20, 32]. [12, 28], train efficient pixel space transformers models by leveraging multiscale training and SwinAttention. Similarly, RIN [10, 40] also proposes an approach using attention only, albeit in a Perceiver-IO [42] inspired architecture that uses cross-attention to perform most of the computation in a smaller latent space, and has been successfully extended to text-to-image [18]. In addition to architectures and sampling [2, 84, 86], the importance of training is also highlighted in recent works, from resampling the training data [24, 54] to RL [49, 74, 78] and model averaging [46].\nIn video generation [29, 36, 66, 73, 83], early attempts have focused on extending existing text-to-image models to benefit from their large scale pretraining [5, 21, 22, 34, 37, 48]. However, the drawback of such approaches is that they re-use the VAE of existing text-to-image models which does not encode temporal information, which is thus not compressed. As such, novel architectures using a 2D+t VAE such as CogVideoX [80], PyramidFlow [43] can benefit from a smaller latent space leading to less computational costs.\nFast alternative to attention Since the introduction of Transformers [72], many effort have been made to reduce the quadratic complexity of MHA [11, 47, 76]. Notably, methods like Reformer [47] use fast approximate neighbors to reduce the size of the attention matrix based on the assumption that most tokens will have zero attention. To go further, Linformer [76] proposes to compute an explicit low rank projection of the keys and the values to reduce the complexity of MHA for each query from the size of the sequence n to an arbitrary chosen number k < \u03b7. The main drawback of such approach is that n and k are fixed, which means that the model can no longer process sequences of varying length. With the advent of Large Language Models and their ability to process extremely long sequences [1, 17, 68], recent efforts have been put on more efficient implementations such as Flash-Attention [13, 14] or KV-cache [6, 55] which seem sufficient for text. However for visual content, the sequence length grows quadratically with the resolution, which, because MHA is also quadratic"}, {"title": "3. Polynomial Mixer and Polymorpher", "content": "We define a Polymorpher block as a sequence-to-sequence function mapping $R^{d \\times n}$ to $R^{d \\times n}$, composed of two residual blocks, a Polynomial Mixer and a feed-forward block.\nFor a sequence $X \\in R^{d\\times n}$, the Polynomial Mixer (PoM) shown on Figure 2 is defined as follows:\nPoM(X) = W_o [\\sigma(W_sX) \\circ H(X)1], with (3)\nH(X) = \\big[h(W_1X);...;h(W_mX)\\big] \\frac{1}{m}1, (4)\nwhere k is the degree of the Polynomial Mixer, $\\sigma$ is the sigmoid function, h an activation function, $\\circ$ and $1$ the element-wise (Hadamard) product, and $1$ a vector of the appropriate dimension filled with ones. The notation $[\\u00b7;\\u00b7]$ is for vertical concatenation. The matrices $W_o \\in R^{d\\times kD}$, $W_s \\in R^{kD\\times d}$ and $W_1,..., W_k \\in R^{D\\times d}$ are the learnable parameters of the Polynomial Mixer.\nThe idea of the Polynomial Mixer is to that the sequence $X \\in R^{d\\times n}$ is uniquely summarized into the representation $H(X) \\in R^{kD\\times 1}$. Each element in X then gets to query $H(X)$ independently thanks to the map $S(W_sX) \\in R^{kD\\times n}$. The queried information is then projected back into the original space with $W_o$.\nContrarily to MHA that computes all pairwise exchanges of information between tokens in the sequence, the Polynomial Mixer follows a state-representation ($H(X)$) approach where all information is shared in a common memory location that all tokens can access. This state-representation is defined by mixing all tokens of the sequence after they are mapped to a high dimensional space by a learned polynomial, hence the name Polynomial Mixer, and a similar approach has been successfully used for learning image representation [41]. The main benefit is that the complexity of the approach is no longer quadratic but linear with the sequence length n.\nTaking inspiration from transformers with MHA, we define a Polymorpher block P as alternating residual Polynomial Mixers with feed-forward networks as follows:\nP(X) = X + PoM(X) + FF(X + PoM(X)), (5)\nwith FF(X) being a two-layer feed-forward network.\nA Polymorpher is a drop-in replacement for any Transformer-based architecture as it performs the same role of sequence-to-sequence mapping. The main difference is in its parametrization: A Transformer is configured by the number of heads and their dimension in MHA, whereas the Polymorpher is configured by its degree k and the dimension D of each polynomial."}, {"title": "3.1. Polymorpher for causal sequences", "content": "A causal sequence can easily be modeled in PoM by adding a mask M that prevents summing future tokens into the blackboard. This corresponds to the following definition\nPoM(X, M) = W_o [\\sigma(W_sX) \\circ H(X)], (6)\nH(X) = \\big[h(W_1X); h(W_1X); ...; \\prod_{m=1}^{k} h(W_mX)\\big] M^T. (7)\nNow $H(X) \\in R^{kD\\times n}$ and $M \\in \\{0,1\\}^{n\\times n}$ is a binary matrix that defines which pairs of tokens are related. Just like for MHA, a binary matrix defines an attention pattern that can be arbitrarily chosen.\nIn the special case of causal sequences, M is a lower triangular matrix. Moreover, one can express the mixing part of the Polynomial Mixer as an iterative process as follows:\nH(X)_{:,i} = \\sum_{j \\leq i} \\big[ h(W_1X)_{:,j}...; \\prod_{m=1}^{k} h(W_mX)_{:,j} \\big] , (8)\n= H(X)_{:,i-1}+ \\big[ h(W_1X)_{:,i}...; \\prod_{m=1}^{k} h(W_mX)_{:,i} \\big] (9)\nIn this formula, $H(X)_{:,i}$ is an explicit hidden state that is updated by adding the polynomial mapping of the next token. Such a configuration enables O(1) inference complexity in the auto-regressive setup, a property that is shared with recurrent networks, but not transformers. Like SSMS, Polymorphers have the best of both worlds, they can train on the whole sequence in parallel and do the inference in the recursive way.\nIn addition, Polymorphers can handle block causal sequences. Let M be a block causal matrix for some integer block size K:\nM_{i,j} = 1 \\text{ if } j < \\lfloor i/K \\rfloor K \\text{ else } 0. (10)\nWe can now rewrite H as\nH(X)_{:,i} =H(X)_{:,\\lfloor i/K \\rfloor K}\n+ \\sum_{j=\\lfloor i/K \\rfloor K}^{\\lfloor i/K \\rfloor K} \\big[ h(W_1X)_{:,j}...; \\prod_{m=1}^{k} h(W_mX)_{:,j} \\big] (11)\nIn this configuration, we can sequentially process groups of tokens at a time during inference, which reduces the memory requirement. This is in particular practical for video sequences where it makes sense to have a causal mask in the temporal dimension that makes each frame depend on the previous ones, while keeping the ability of all the tokens (patches) of a frame to look at each others, since causality does not have much sense in the spatial dimension."}, {"title": "3.2. Theoretical analysis", "content": "We first show that PoM is equivariant, which means that permutations in the input sequence result in permuted outputs. This is a key property that made transformers popular and does not hold for other architectures like convolutions:\nProposition 1 (Permutation equivariance). A Polynomial Mixer is permutation equivariant, i.e., let $X \\in R^{d\\times n}$ be a set of vectors and P a column permutation matrix, then PoM(XP) = PoM(X)P.\nProof. For a permutation P, we have\nPoM(XP) = W_o [\\sigma(W_sXP) \\circ H(XP)1]. (12)\nNotice that H(XP) = H(X) because the sum is permutation invariant, and $\\sigma(W_sXP) = \\sigma(W_sX)P$ because $\\sigma$ is an element-wise operation. Noticing that $H(X)1^T$ has all identical columns allows us to move P outside of the brackets to conclude the proof.\nMore importantly, we can also prove a universal approximation theorem for Polymorphers similar to what is well known for Transformers [81]. As the polynomial mixer is equivariant, it requires the use of positional encoding, which also underlines the similarity between PoM and MHA.\nWe use the following standard definition of distance between functions that map sequences to sequences. Given two functions $f$ and $g : R^{d\\times n} \\rightarrow R^{d\\times n}$ and an integer $\\Lambda \\leq p < \\infty$, we define the distance $d_p$ as:\nd_p(f,g) = (\\int_{k} ||f(x) - g(x)||^p dX)^{\\frac{1}{p}} (13)\nThe following theorem holds:\nTheorem 2 (Universal approximation). Let $1 < p < \\infty$ and $ \\epsilon > 0$, then for any given $f \\in F$ the set of continuous functions that map a compact domain in $R^{d\\times n}$ to $R^{d\\times n}$, there exists a Polymorpher $g$ with learned positional encoding such that $d_p(f, g) \\leq \\epsilon$.\nThe proof follows exactly the same scheme as in [81], where most of the heavy lifting is done by the feed-forward networks. Their main argument is to show that MHA can map every token in the sequence to a unique value that depends on the entire sequence, and then the feed-forward blocks can map those unique values to the desired output. In our case, we just have to ensure that the Polynomial Mixer has the same properties as MHA, which is obtained using the following lemma:\nLemma 3 (Contextual mapping (informal)). There exists $k > 0$ for which any Polynomial Mixer $q$ of degree $k$ is a contextual mappings on $R^{d\\times n}$, that is:\n* For any $X \\in R^{d\\times n}$ with different entries, $q(X)$ has different entries.\n* For any $X, X' \\in R^{d\\times n}$ that differ at least by one element, then all entries of $q(L)$ and $q(L')$ are different.\nThe proof is deferred to the appendix and primarily uses the fact that a sufficiently high degree polynomial is uniquely defined by a sequence of point-wise evaluation. As noted in [81], having the contextual mapping property is not so common as it requires to summarize uniquely the context while preserving the identity of the current token. With these results, we show that a Polymorpher is as potent as a Transformer for sequence modeling."}, {"title": "4. Diffusion with PoM", "content": "Armed with the definition of PoM and Polymorphers, we now design diffusion models taking inspiration from models based on MHA, and show that PoM can replace attention in practice. We follow the design choices of DiT [58] and propose a class-conditional image generation polymorpher as well as a text-to-video generation polymorpher."}, {"title": "4.1. Architecture design", "content": "Image generation For image generation, the class-conditional polymorpher is similar to the AdaLN variant of DiT. The image is encoded through the VAE of SD1.5 [63] and then features are aggregated into visual tokens X. We add a 2D cosine positional encoding to them before we feed them to the model. The class c and the time step t are embedded using an embedding matrix and a cosine embedding respectively before being summed together.\nThe model consists in several blocks that combine modulations, PoM and feed forward networks as shown on Figure 3a. In each block, the modulation consists in predicting from the condition c + t a scale \u03b3 and a shift \u03b2 that modify the input by\nx \u2190 \u03b3(x \u2013 \u03b2). (14)\nSimilarly to DiT, the MLP also predicts gates \u03c3 that can shut down an entire block f thanks to\nx \u2190 x + (1 + s)f(x), (15)\nwith the 1 in 1 + s being added so that there is a full residual connection when the MLP predicts f(x) = 0. For naming the architectures, we follow the same parametrization as in DiT. Namely, an S/2 model has a kernel size and stride of 2 for aggregating the VAE features into tokens, and 12 blocks of dimension 384. Similarly, an XL/2 model that has 28 blocks of dimension 1152. For the PoM operation inside each block, we use an polynomial of order 2 with an expansion factor of 2 unless specified otherwise. Pytorch code for the blocks is given in appendix.\nVideo generation For video generation from text, we extend the DiT architecture to handle text as a condition. We first encode video clips using the 3D VAE from CogXVideo [80] and then group the features into visual tokens using a kernel size of 2\u00d72\u00d72 (with 2\u00d72 for the spatial axes, and 2 for the temporal axis resulting in a downscaling factor of 16 \u00d7 16 \u00d7 8). We add a 3D cosine positional encoding to the visual tokens before feeding them to the model. The text is encoded using T5 [62] embeddings and the time step is encoded using a cosine embedding.\nThe model consists in blocks using PoM to aggregate information between the text condition and the visual tokens as shown on Figure 3b. More precisely, a first PoM operation is used in a cross fashion, similar to cross-attention, to aggregate information from the text tokens into the visual tokens. Then, a second PoM operation is used to aggregate information among the visual tokens themselves, similar to what self-attention would do. Finally, a feed forward module processes the visual tokens only. The time step embedding is used in an MLP to predict the coefficients of modulations and gates at each of the operations.\nWe train a single model of size XL/2 that consists in 20 layers of dimension 1152 resulting in 1.1B parameters."}, {"title": "4.2. Training setup", "content": "For class-conditional image generation, we train on ImageNet. We rescale each image to 256 pixels on their smallest size and then take a crop of size 256 \u00d7 256. We use both the original images and horizontally flipped version for a total of 2.4M images. We train a model $f_\\theta$ either using the diffusion loss:\n$\\mathcal{L}_D = \\mathbb{E}_{t \\sim U[0,1]}||\\varepsilon_t - f_\\theta(x_t, c, t)||^2, (16)\nor the flow matching loss:\n$\\mathcal{L}_{FM} = \\mathbb{E}_{t \\sim U[0,1]}||v_t - f_\\theta(x_t, c, t)||^2, (17)\nwith $v_t = \\varepsilon_t \\frac{x_t - x_0}{ \\gamma_t}$. For each experimental result, we mention which loss is used, but the models are trained similarly"}, {"title": "5. Experiments", "content": "We first show result on class-conditional image generation and then of text-to-video generation."}, {"title": "5.1. Class-conditional image generation", "content": "Quantitative results We compare the results of our XL/2 model trained with the diffusion loss to the state of the art on Table 1. We compute the Fr\u00e9chet Inception Distance (FID), the Inception Score (IS), precision (P) and recall (R) using the code from ADM [16] on 50k generated images. The table is split between methods on masked encoding (Mask-GIT [8]), diffusion models based on SSM and diffusion models based on attention. Results are extracted from the corresponding papers. Our images are generated with 250 steps of the DDIM sampler for the model trained with the diffusion loss $\\mathcal{L}_D$, and 125 steps of Heun sampler for the model trained with the flow-matching loss $\\mathcal{L}_{FM}$, with classifier free guidance (CFG, w = 0.7 in both cases).\nUsing the evaluation code and reference set from ADM [16], we obtain an FID of 2.46, which is slightly above that of the comparable DiT architecture, but notice that our model was trained for only half of the number steps of DiT. In addition, we found FID to be very unreliable as a metric, as it is highly varying with the reference set. For example, using the validation set of ImageNet, we obtained 3.45 FID\u00b9. We obtain a slightly lower IS compared to DiT, but this could be improved by using a higher CFG. Indeed, we show in appendix that the trade-off between FID ans IS can reach as high as 300 IS at the cost of a much higher FID. We obtain a precision/recall trade-off comparable to DiT, slightly lower on precision but also higher on recall.\nOverall, the results obtained using PoM are on par with the literature, showing that PoM can be used as a drop-in replacement for multi-head attention in a neural architecture, without requiring either architectural changes or training hyper-parameter tuning.\nQualitative results We further fine-tune the model on higher resolution data for a small number of steps to ob-"}, {"title": "5.2. Text to video generation", "content": "We evaluate our model generating videos of 5 seconds at 16 fps and 240p resolution on VBench [39] and show the results in Table 3. Note that contrarily to ImageNet, video generation is not as well standardized and models differ dramatically in terms of size, complexity and training dataset. Notably, most text-to-video generation models are trained on a mix of images and videos to get more diverse captions. In our case, we want to study the impact of enforcing temporal causality in the generation process and as such we limit our train set to WebVid-2M [3] only. Due to this smaller training set, we observed that our models are limited to a smaller vocabulary of objects, motion and styles.\nWe compare the standard architecture (denoted no-mask) with the use of a block-causal mask as detailed in section 3.1 (denoted as b-causal). As we can see, the impact of using a block causal mask is negative on some tasks like subject consistency, background consistency and color. This can be explain by the model struggling to follow the prompt for the first frames in the block causal case, which penalizes consistency, whereas the no-mask case can leverage information from later frames to improve consistency. Interestingly, using a block causal mask improves temporal tasks like dynamic degree, human action and temporal style, which shows the importance of modeling properly the temporal aspect for these tasks."}, {"title": "6. Discussion", "content": "In this paper, we presented PoM, the Polynomial Mixer, a building block for neural networks that aims at replacing attention. PoM has a complexity linear with the sequence length and we prove it is a universal senquence-to-sequence approximator. To demonstrate the effectiveness of PoM, we train image and video generation models with it in lieu of Multi-Head Attention. These diffusion models obtain competitive results while being able to generate higher resolution images faster than with attention.\nPoM is very interesting for high-definition video of long duration. However, the extreme cost of training such model makes this endeavor clearly out of the scope of a research paper. Another area where PoM could shine is LLMs and more particularly multimodal LLMs. Indeed, LLMs are causal, which means the generation of text could greatly benefit from the O(1) complexity of PoM for causal sequence. In addition, recent works [85] show that next to-ken prediction and diffusion objectives can be merged in a single model. In that case the ability of PoM to seamlessly adapt from causal to block-causal masking scheme greatly reduces the complexity of such mixed training objective. As for high definition video, the extreme cost of training such large models also renders this endeavor out of the scope of a research paper."}, {"title": "A. PoM pytorch code", "content": "In this section, we provide code in Pytorch for the main parts of the Polynomial Mixer as well as our diffusion blocks.\nWe found that writing dedicated functions for specific degrees led to faster runtime due to the ability of the Py-Torch's compiler to optimize them. We show below implementation for degrees 2, 3 and 4.\nNext, we show the function that computes both the polynomial and the mixing depending on the degree and the presence of a mask.\nIn the PoM module, we add the projections W1...m, Ws and Wo for each part of the PoM operation."}, {"title": "B. Condition adherence", "content": "In this section we study the trade-off between image quality as measured with FID and condition adherence as measured with Inception Score (IS) by varying the weight w of the classifier-free guidance (CFG). We show the results for a model of size L2 trained with the diffusion loss LD on Gigure 6. Inference is performed with 250 steps of DDIM sampling. As we can see, the model is perfectly able to balance FID and IS, leading to a typical 'U' curve where CFG improves both FID and IS at first, but then improvements of"}, {"title": "C. Proof of Lemma 3", "content": "We first need to show that set with different entries are mapped to different vectors. We first separate PoM into its two components:\ns(X) = \u03c3(WX) (18)\nHk(X) = \\big[h(W\u2081X); . . . ;h(WmX)\\big] \\frac{1}{m}11 (19)\nPoM(X) = Wo(s(X)\u25cbHk(X)) (20)\nAssuming ker(Wo) =, and noting that Hk(X) is the same for every column, we just have to show that s(X) has different columns. This is easily achieved by having ker(Wo) =, since \u03c3 is injective and the composition of injective functions is itself injective.\nSecond, we have to show that sets that differ by at least one element are mapped to all different entries. To simplify notations, we will consider the special case where all matrices are the identity or an identity block positioned such as to perform submatrix selection. All the matrices can thus be removed from the formula. A similar argument can be made for matrices that are full rank as they preserve injectivity. We will also consider linear activations everywhere, which can be made as close as one wish by partitioning the image of the activation function and performing piecewise linear approximation.\nWith this simplified version of PoM, we have to show that for 2 sets X, X' differing by at least one element (i.e.,\n\u2203x' \u2208 X', \u2200x \u2208 X, x \u2260 x'), then there exist k such that\n\u2200x \u2208 X,x' \u2208 \u03a7', x \\neq xxx. (21)\nConsider the functions P(t) and P'(t) defined as fol-lows:\nP(t) = \u03a3\u03c7 (22)\nP'(t) = \u03a3\u03c7 (23)\nSince X and X' differ by at least one element, there exists at least one xi \u2208 X such that xi \u2260 xi, \u2200x \u2208 X'. This implies that the functions P(t) and P'(t) are not identical since are sums of exponentials with different bases.\nSince P(t) and P'(t) are different functions, there must exist some k for which P(k) \u2260 P'(k). In other words, there exists a k such that:\n\u03a3\u03b1 (24)\nFor this k, let us denote Sk = \u03a3x\u2081ex x. We need to show that xSk \u2260 x'S for all x \u2208 X and x' \u2208 X'. Assume for the sake of contradiction that there exist x \u2208 X and x' \u2208 X' such that xSk = x'S'. This implies:\nx \u03a3x = xx (25)\nRearranging, we get:\n (26)\nSince Sk \u2260 S, the right-hand side is not equal to 1. However, for this equality to hold for all x \u2208 X and x' \u2208 X', the ratio x/x' would need to be constant for all pairs (x, x'), which is not possible given that X and X' differ by at least one element.\nTherefore, there exists a k such that xSk \u2260 x'S for all x \u2208 X and x' \u2208 X'."}]}