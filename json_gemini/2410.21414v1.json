{"title": "CT\u00b2C-QA: Multimodal Question Answering over Chinese Text, Table and Chart", "authors": ["Bowen Zhao", "Tianhao Cheng", "Yuejie Zhang", "Ying Cheng", "Rui Feng", "Xiaobo Zhang"], "abstract": "Multimodal Question Answering (MMQA) is crucial as it enables comprehensive understanding and accurate responses by integrating insights from diverse data representations such as tables, charts, and text. Most existing researches in MMQA only focus on two modalities such as image-text QA, table-text QA and chart-text QA, and there remains a notable scarcity in studies that investigate the joint analysis of text, tables, and charts. In this paper, we present CT2C-QA, a pioneering Chinese reasoning-based QA dataset that includes an extensive collection of text, tables, and charts, meticulously compiled from 200 selectively sourced webpages. Our dataset simulates real webpages and serves as a great test for the capability of the model to analyze and reason with multimodal data, because the answer to a question could appear in various modalities, or even potentially not exist at all. Additionally, we present AED (Allocating, Expert and Desicion), a multi-agent system implemented through collaborative deployment, information interaction, and collective decision-making among different agents. Specifically, the Assignment Agent is in charge of selecting and activating expert agents, including those proficient in text, tables, and charts. The Decision Agent bears the responsibility of delivering the final verdict, drawing upon the analytical insights provided by these expert agents. We execute a comprehensive analysis, comparing AED with various state-of-the-art models in MMQA, including GPT-4. The experimental outcomes demonstrate that current methodologies, including GPT-4, are yet to meet the benchmarks set by our dataset.", "sections": [{"title": "1 INTRODUCTION", "content": "Text, tables, and charts are widely used in the fields of finance, healthcare, market research, data analysis, etc., owing to their significant advantages in information presentation: text deepens understanding of topics, providing comprehensive explanations and contextual background; tables present data clearly in a structured format; while charts effectively demonstrate trends and patterns in data through their intuitiveness. These manifold modalities of data collectively reveal and convey complex information. In the scenario of people browsing this information, the answers to their diversity questions appear in different modalities.\nIn recent years, there has been a significant interest on Multimodal Question Answering (MMQA), which involves understanding and responding to questions that incorporate multiple modalities, such as text, images, and audio [26]. The initial work on MMQA, as presented in [12], introduced the innovative concept of \"Manymodal\", which places a spotlight on QA tasks that interact with data spanning more than two modalities. Central to this effort was the development of a diverse dataset, comprised of text, tables, and images, all sourced from Wikipedia. Subsequent research has proposed MMQA datasets with larger scale, more modalities, closer correlation between modalities, and more intricate inference requirements [5, 21, 22, 39, 41]. Although existing MMQA datasets offer significant insights into multimodal interactions, they have overlooked the synergistic potential of combining table, text, and chart data. This trio is fundamental in fields such as statistics and finance, where data interpretation often requires the concurrent analysis of narrative, tabular, and graphical information.\nTo bridge this research gap, we introduce CT\u00b2C-QA, the first Chinese reasoning-based QA dataset imitating real webpages, that encompasses Text, Tables, and Charts, including 9,981 question and answer pairs, and each set of QA pairs associates information about one or more modalities. Our innovative dataset is gathered from 200 websites associated with the National Bureau of Statistics of China\u00b9, encompassing a comprehensive collection of 200 text, 796 tables, and 1051 charts. To mimic the structure of authentic web content, we convert all HTML content into Markdown text format. This involves substituting the HTML content of all tables with specific labels like \"table1\", \"table2\", and so on, while ensuring the content of each table is stored separately. Similarly, we represent charts with placeholders such as \"img1\", \"img2\", etc., replacing the original hyperlinks found in the HTML source. Additionally, these placeholders are linked to both local storage links and image bed links, providing a versatile and comprehensive representation of the data. This approach maintains a consistent and clear representation of the web content within our framework. Figure1 presents an example and an illustration of the contents encompassed in our dataset: \"What was the total profit of the mining industry in January-February 2024?\". Answering this question entails (i) understand the content and meaning of the question, (ii) judge the relevance of the question to the data of different modalities: text and table are both relevant to the question, (iii) retrieve information in the relevant modalities, i.e., \"Mining industry\", \"January-February 2024\", \"total profits\" all appear together in question, text and table, (iv) integrate the information and generate the answer: \"188.10 billion yuan\"."}, {"title": "2 RELATED WORKS", "content": "In earlier researches, MMQA datasets primarily focused on two modalities, such as image-text QA [10, 18, 28, 38], table-text QA [6, 16, 34, 52], video-text QA [15, 19, 42, 47], chart-text QA [17, 30, 32]. Each of these datasets presents its unique challenges and has been instrumental in advancing the state of the art in MMQA research. They are commonly used as benchmarks to test the performance of various models in understanding and correlating queries of various modality content with text.\nIn the real world, scenarios often necessitate the integration and interpretation of information from more than two sources. This necessity led to the development of QA datasets that contain three or even more modalities simultaneously, such as text, images, and tables [12, 22, 41]. However, existing MMQA datasets have not adequately addressed the combination of table, text, and chart data. Recognizing this gap, we introduce the first dataset integrating text, tables, and charts, thereby presenting new challenges to the existing methodologies in the MMQA domain."}, {"title": "3 DATA ASSEMBLY", "content": "Data Collection. Our data is sourced from the National Bureau of Statistics of China, spanning across more than 1,000 publicly accessible webpages. These pages contain a rich variety of modality data, including text, tables, charts, and more. It is imperative to note that all of this data is publicly available and easily accessible.\nQA Pairs Construction. The statistical data contains a wealth of information but lacks explicit questions. Therefore, we follow previous works [43, 46, 49] and generate QA pairs automatically. However, due to the unique characteristics of the data, including HTML-formatted tables and charts in image format, as well as redundant HTML tags, we restructure the formats of the various modality data before inputting the original text. To preserve the authenticity of the webpage's format and sequence, the restructuring process, as depicted in Figure 2, entails converting HTML-formatted text into Markdown format, transforming HTML-formatted tables into tuples [57], and substituting instances of statistical charts in the webpage source code with \"imgi\" tags (\"i\" denotes the index of the chart, ranging from 1 to n, where \"n\" signifies the total number of charts present on the webpage being analyzed). Each tag is linked to a local storage path for the corresponding chart and an image bed. Subsequently, we utilize GPT-3.5-turbo-0125 to generate QA pairs for text and tables, while employing GPT-4-vision-preview to create QA pairs specifically tailored for charts. In particular, to maintain data diversity, when crafting QA pairs, we instruct GPT to generate high-quality pairs that lean towards numerical and entity-based QA pairs, rather than binary yes or no inquiries."}, {"title": "4 DATA ANALYSIS", "content": "CT2C-QA is composed of data extracted from 200 text, 369 tables, and 494 charts retrieved from 200 webpages. It encompasses a total of 9,981 questions, distributed as follows: 3,335 text-related questions, 3,681 table-related questions, and 1,051 chart-related questions. To highlight the properties of CT2C-QA, we analyze the questions and answers in the question types and answer types."}, {"title": "5 APPROACH", "content": "In this section, we propose AED, a multi-agent system comprised of three parts to performance QA on CT\u00b2C-QA. The overall framework is illustrated in Figure 5."}, {"title": "5.1 Allocating Agent", "content": "Our dataset, as depicted in Figure 1, is capable of identifying the webpage relevant to a given question but lacks the precision to pinpoint the specific segment or modality of data associated with it. So we develop an Allocating Agent aimed at discerning the interconnectedness of the question with various data modalities present in a document. The Allocating Agent is structured into three pivotal modules: the Profile Module, Memory Module, and Action Module.\nThe Profile Module characterizes the Agent as an adept assistant for multimodal web-based QA. It is tasked with determining the likelihood of answer distribution across different modalities and setting the output format. The Memory Module is bifurcated into two segments: long-term memory, which encompasses the webpage content, and short-term memory, holding the dialogues for each question and answer pair (retaining only the most recent interaction). The Action Module assigns specific probabilities (for instance, P(text) = a, P(table_i) = b, P(chart_j) = d, with \"i\" representing the count of tables on the webpage and \"j\" indicating the total number of charts) and to activate different Expert Agents based on these probabilities. In our system, an expert Agent is triggered when the set probability exceeds 0.1. It is important to note that the inputs for the Allocating Agent comprise all web content and the posed questions, wherein tables are represented as tuples and charts are denoted by corresponding tags."}, {"title": "5.2 Expert Agent", "content": "We develop three unique Expert Agents, each adept in managing QA tasks specific to different modalities. Mirroring the structure of the Allocating Agent, Text and Table Expert Agents comprise three fundamental modules: the Profile Module, Memory Module, and Action Module. Chart Expert Agent only contains two modules Profile and Memory.\nText Expert Agent. The Text Agent receives all text and the question from the webpage as its input. Within its Profile Module, the Agent is designated as a proficient economic analyst, tasked with reading web content and responding to queries, alongside defining the format for the output content. The Memory Module is split into two parts: Long Memory, encompassing the entirety of the webpage's textual content, and Short Memory, which holds the latest round of Q&A. In the Action Module, the Agent is responsible for providing answers as per the requirements, determining the confidence level of each response, and subsequently relaying this information to the Decision Agent.\nTable Expert Agent. The Allocating Agent assigns probabilities to each specific table, so the input of the Table Expert Agent includes not just the query but also the full text of the table pertinent to the problem. The Profile Module of the Table Expert Agent defines it as a skilled data analyst, acquainting it with the rules for reading tables in tuple format and guiding it to respond to queries in a predetermined format. The Memory Module of this agent consists solely of long-term memory, encompassing the content of each relevant table. In the Action Module, the agent's tasks include converting tables from HTML format to tuple format, answering the question as per the requirements, assessing the confidence level of the response, and forwarding this answer to the Decision Agent. A noteworthy aspect is that the original HTML format of tables often contains extraneous information like tags, while the tuple form simplifies the table's content. Furthermore, we optimize our approach from previous work [57] by eliminating hierarchical representation elements within the tuples, further streamlining the expression.\nChart Expert Agent. The Chart Expert Agent is an adept statistician designated to handle inquiries related to charts, adhering to a specific procedural format. It is important to emphasize that the primary objective of our proposed task extends beyond merely answering queries based on a single chart. Instead, it involves the retrieval of the most pertinent chart from a collection of multiple charts prior to providing an answer. Consequently, the principal workflow of our Chart Expert Agent can be outlined as follows: 1) Implementing OCR (Optical Character Recognition) on all charts within an article, this process yields detailed OCR outcomes including the bounding box, numerical values, and their corresponding confidence levels; 2) Extracting and aggregating the values containing Chinese characters from each chart; 3) Independently embedding the aggregated values and the posed question, creating distinct but related data representations; 4) Evaluating the degree of similarity between the embedded chart values and the question, subsequently arranging them in descending order based on similarity scores; 5) Identifying and referencing the chart that exhibits the highest similarity to the question for a precise response. Subsequently, the answer is provided along with an indicated confidence level; 6) Activating the Decision Agent and conveying the gathered information for further action."}, {"title": "5.3 Decision Agent", "content": "The Decision Agent is composed of three integral parts, each serving a distinct function: 1) Profile Module: This module establishes the Decision Agent as a proficient data synthesis analyst. Its primary role is to analyze the input from all Expert Agents comprehensively. By doing so, it integrates various pieces of information to formulate a final judgment, ensuring a well-rounded and informed decision-making process; 2) Memory Module: This is dedicated to short memory, specifically retaining information from the most recent question-and-answer cycle; 3) Action Module: As the operative heart of the Decision Agent, this module is responsible for delivering the final answer and making necessary selections. It synthetically analyzes the question and the previous inputs and picks the answer of the correct modality as the input. It is noteworthy that our system ultimately outputs both the selected modality and the corresponding answer, enabling a more detailed evaluation of the experimental results and the capability of the Agent."}, {"title": "6 EXPERIMENT", "content": "We utilize GPT-3.5-turbo-0125, GPT-4-0125-preview, and GPT-4-vision-preview as the foundational models for AED. Specifically, GPT-4-vision-preview is primarily employed for image parsing, while allocation and comprehensive analysis are executed based on GPT-4-0125-preview. All other tasks are completed using GPT-3.5-turbo-0125. In the Action Module of Chart Expert Agent, the OCR task is implemented based on PaddleOCR [8], and the embedding model used in similarity ranking is text-embedding-3-large."}, {"title": "6.2 Evaluation Metrics", "content": "Prior studies have adopted Exact Match (EM) as the evaluation metric, following the precedent set by [37]. However, EM may not be apt for assessing generative QA tasks. Hence, this paper introduces a novel evaluation method, Keyword Match.\nKeyword Match. The rise of generative large-scale models has revolutionized sub-tasks within the AI field, prompting research into effective methods for evaluating the generated content. This paper introduces a novel evaluation approach KM to assess accuracy by determining whether the keywords from the golden answer are present in the generated response. As depicted in Figure 6, different from EM, KM extracts the keywords \"38,336.9\" and \"8.6%\" from Golden Answer, and ignores the extra symbols when judging \"in\" with Generated Answer. The final judgment is that \"38336.9\" and \"8.6\" are in Generated Answer, so the match is successful. Additionally, KM disregards case differences when evaluating words, further showcasing the capabilities of the methods being assessed.\nCross-Linguistic Keyword Match Validation. Additionally, since the majority of the models we evaluate are trained using English corpora, they occasionally generate responses in English. To assess the model's comprehension capabilities beyond mere language selection errors, we introduce Cross-Linguistic Keyword Match Validation (CLKM). This method emphasizes the accurate capture of essential information across different linguistic contexts, ensuring a focus on content relevance rather than linguistic form.\nHuman Performance. We assess human performance on a held-out set from the test set containing 300 instances. To evaluate human performance, we present each question alongside its corresponding webpage to three distinct individuals for response. Subsequently, we select the second responses as the human-generated answer and designated the other two as ground truth answers. To assess the accuracy, we calculate the KM and CLKM, comparing the human-predicted answer with the two ground truth answers. The findings revealed that the scores of human performance, indicated by KM = 94.9 and CLKM = 94.9, were significantly superior to those achieved by AED. The primary causes of mismatches can often be attributed to the intricate content of tables and charts, where it is inevitable that the human eye may inaccurately perceive colors and positions. It should be noted that, as the respondents' native language is Chinese, the occasional appearance of English expressions poses no issue, resulting in equal KM and CLKM values."}, {"title": "6.3 Baseline Models", "content": "MMQA. Given the existing gaps in the fields of text, tables, and charts, we opt to benchmark against the MultimodalQA [41] research, which addresses text, tables, and images. Notably, we are unable to find any open-source code related to Manymodal [12] work for comparison.\nChart QA. In the realm of Chart QA, the approach involves training models subsequent to the transformation of charts into tables and the subsequent linearization of these tables. We primarily conduct comparisons with three renowned methodologies: ChartQA [29], PlotQA [31], and MatCha [24]. Furthermore, with the advancements in multimodal large language models, we also choose to include GPT-4v\u00b3, LLaVA-1.6 [25], MiniGPT4-v2 [58], mPLUG-owl1 [50], and mPLUG-owl2 [51] in our comparisons."}, {"title": "6.4 Results", "content": "We conduct tests across the three modalities-table, text, and chart-and present the evaluation results using the KM and CLKM metrics, as shown in Table 4. Overall, compared to other modals, our method AED soundly outperforms all previous works. The overall KM and CLKM metrics are achieved KM = 33.9 and CLKM = 34.3, respectively, which is a significant leap compared to the KM = 2.0 and CLKM = 2.4 of the the previous method MultiModalQA. However, it still falls short of human performance, which stands at 94.9 for both KM and CLKM. It is noteworthy that in the QA evaluations across the three modalities, results for the text are significantly better than those for the table, which in turn surpasses the chart category. This may be attributed to the Allocating Agent's deeper understanding of text data, followed by tables, and charts being the least comprehensible. This leads to a higher accuracy rate for text-modality related questions. Additionally, within a single webpage, all text data are typically stored in a Markdown file, whereas each table and chart are often stored separately in different files. This means that even after successfully identifying the relevant modality, further identification is required to determine the specific table or chart involved, thereby increasing the potential for errors.\nTo further illustrate the advantages of the Chart QA task, in our study, we randomly select two questions from each webpage containing a chart and documented their URLs for testing purposes. It is noteworthy that current multimodal large-scale models, as well as Chart QA models, are limited to processing only one chart at a time. To ensure fairness and better demonstrate the varying capacities of different methods in understanding charts, we specifically chose the Chart Expert Agent from AED for our QA task. Additionally, we omit the ranking module to simplify the task into a direct question-and-answer format focused on a single chart. As shown in Table 5, models that are trained and fine-tuned using previous Chart QA datasets have demonstrated suboptimal performance. In contrast, general-purpose multimodal large language models, such as GPT-4v and Llava, have exceeded expectations in the Chart QA task.Compared with these works, our Chart Expert Agent has an absolute improvement of KM = 49.1 and CLKM = 54.4 under the same conditions. Additionally, it has been observed that the performance of most methods improves under the CLKM metric. This improvement is attributed to the focus shifting away from language consistency towards the models' ability to parse and reason about the data presented in charts."}, {"title": "6.5 Analysis", "content": "The results show that our AED for CT\u00b2C-QA effectively outperforms the previous method and shows remarkable results in the task with only a single modal QA of chart. However, compared with the excellent single-modal QA of chart, the overall AED method is still unsatisfactory. We consider that is due to the serial operation of the AED method, which progresses from the Allocating Agent to the Expert Agent, and finally to the Decision Agent. When the Allocating Agent makes an error in modality classification, the likelihood of correctly selecting the appropriate object from multiple tables and charts is consequently reduced. This, in turn, leads to a cumulative increase in errors at each subsequent stage."}, {"title": "7 CONCLUSION", "content": "We introduce CT\u00b2C-QA, a new Chinese multimodal QA dataset comprising 9,981 QA pairs across text, tables, and charts, presenting fresh challenges to MMQA research. We also develop a multi-agent system AED for unified reasoning across these modalities. To better evaluate parsing and reasoning capabilities, we introduce new metrics, KM and CLKM. Despite our advances, human performance still significantly outstrips our methods, highlighting extensive opportunities for further exploration in this field."}]}