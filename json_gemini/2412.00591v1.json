{"title": "AUDIO ATLAS: VISUALIZING AND EXPLORING AUDIO DATASETS", "authors": ["Luca A. Lanzend\u00f6rfer", "Florian Gr\u00f6tschla", "Uzeyir Valizada", "Roger Wattenhofer"], "abstract": "We introduce Audio Atlas, an interactive web application\nfor visualizing audio data using text-audio embeddings.\nAudio Atlas is designed to facilitate the exploration and\nanalysis of audio datasets using a contrastive embedding\nmodel and a vector database for efficient data management\nand semantic search. The system maps audio embeddings\ninto a two-dimensional space and leverages DeepScatter\nfor dynamic visualization. Designed for extensibility, Au-\ndio Atlas allows easy integration of new datasets, enabling\nusers to better understand their audio data and identify both\npatterns and outliers. We open-source the codebase of Au-\ndio Atlas, and provide an initial implementation containing\nvarious audio and music datasets.", "sections": [{"title": "1. INTRODUCTION", "content": "The increasing size of machine learning datasets presents\nsignificant challenges in data visualization and analysis.\nTraditional tools are often insufficient for effectively man-\naging and interpreting unlabeled audio datasets at scale.\nHaving the ability to visualize large-scale datasets is cru-\ncial in helping to understand the structure and patterns\nwithin the data. It allows users to quickly grasp relation-\nships between variables, identify trends, and detect outliers\nor anomalies that may affect the performance of a machine\nlearning model.\nAlthough there have been various projects focusing on\nproviding high-level insight into audio and music datasets,\nthey do not allow users to visualize their own data or\nwere not concerned with large-scale datasets [1-5]. As\nsuch, these tools are mostly unsuitable for machine learn-\ning projects. To this end, we present Audio Atlas, an open-\nsource interactive web application that helps users navi-\ngate audio and music datasets. Inspired by existing work\nin the image domain [6], Audio Atlas can visualize any\naudio dataset, providing a responsive user interface even\nwhen displaying tens of millions of samples."}, {"title": "2. AUDIO ATLAS", "content": "Audio Atlas is a visualization tool designed to help users\ninteract with audio data through an intuitive and dynamic\nweb interface. The application leverages the Contrastive\nLanguage-Audio Pretraining (CLAP) [7] model to gener-\nate embeddings that fuse audio and text into a shared vector\nspace. These embeddings are then projected onto a two-\ndimensional plane using t-SNE [9], and are visualized as a\npoint cloud. We use Milvus [10] to store the CLAP embed-\ndings. Milvus is a high-performance open-source vector\ndatabase which efficiently manages embeddings and en-\nables nearest-neighbor lookup for semantic searches with\nboth text and audio snippets.\nAudio Atlas enables users to perform zero-shot classi-\nfication on their audio data. Zero-shot classification cate-\ngorizes datasets without prior explicit training on specific\nclasses, which is particularly useful when labeled data is\nscarce or missing entirely. The CLAP embeddings are used\nfor zero-shot classification with a user-definable list of\nclasses. The visualization highlights the classes with dif-\nferent colors, and a click on the label highlights only the se-\nlected class. The frontend is powered by DeepScatter [11],\nbuilt on top of WebGL [12] and React [13], and renders the\nvisualizations interactively, allowing users to explore audio\ndatasets by navigating through clusters, performing seman-\ntic searches, and listening to the audio snippets. A click on\na datapoint opens a detailed view with more information\nprovided in the dataset, such as classes, labels, and descrip-\ntions, as well as a list of closest neighbors in the embed-\nding space sorted by similarity (cf. Fig. 2). Our initial im-\nplementation provides access to MusicCaps [14], YT8M-\nMTC [15], VCTK [16], ESC-50 [17], MTG-Jamendo [18],\nand FMA [8]. Additionally, Audio Atlas remains respon-\nsive on large-scale datasets [19].\nFurthermore, Audio Atlas can be used for semantic\nsearch. Users can search for audio using text or audio,\nwith the provided search bar in Audio Atlas. The modality\nprovided in the search bar (text or audio) is converted into\nan embedding vector using a contrastive model. The near-\nest neighbors of this embedding vector are computed us-\ning Annoy, an approximate nearest neighbor search frame-\nwork. This semantic search enables users to find and\nfilter audio using descriptive text, which historically has\nonly been possible if the audio data contained rich anno-\ntations. Using contrastive models, we can perform a se-\nmantic search on disjoint modalities. While using audio\nto search for audio has existed previously, using a con-\ntrastive approach allows us to search on the basis of se-\nmantic meaning instead of the similarity of extracted audio\nfeatures or waveform similarity."}, {"title": "3. USE CASES", "content": "We demonstrate a series of practical applications of Audio\nAtlas, showing its utility in audio analysis through dimen-\nsionality reduction and search techniques.\nAudio Atlas makes it easy to browse an audio dataset\nusing text and audio queries. We can therefore also qual-\nitatively assess the classification capabilities of the con-\ntrastive embedding model. Furthermore, we can explore\nthe dataset with descriptive queries instead of searching for\nspecific tracks using their titles.\nAdditionally, users can further explore datasets using\nthe upload function with audio queries. By doing so, users\nare able to find the results that are most similar to their au-\ndio file in the dataset according to the similarity of the em-\nbedding space, as well as finding the most similar result's\nlabel and embedding location. This helps users understand\nthe neighborhood of their query audio file.\nIn Figure 3, we use the ESC-50 classes to classify the\ndataset using zero-shot classification. In this way, we can\nvisualize the semantic meanings of the clusters. Moreover,\nwe can see that the t-SNE projection of the CLAP em-\nbeddings for the ESC-50 dataset has clustered all classes\ninto local pockets. This clustering helps us explore spe-\ncific datasets, in addition to understanding what the CLAP\nmodel has learned. We can easily apply Audio Atlas to\nother datasets with various labels to classify and explore\nthe data. Furthermore, we believe Audio Atlas could be\nused as a novel way to browse music samples. For exam-\nple, users could explore their unannotated audio libraries\nsimply by describing the sound they are looking for.\nIn summary, by transforming audio into a visual repre-\nsentation, Audio Atlas gives users a new tool to explore\nlarge-scale audio datasets interactively. By making the\ncodebase for Audio Atlas open-source we hope to advance\nthe study of large-scale audio datasets as well as qualita-\ntively assessing the performance of embedding models."}]}