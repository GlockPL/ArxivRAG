{"title": "Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models", "authors": ["Camilo Thorne", "Christian Druckenbrodt", "Kinga Szarkowska", "Deepika Goyal", "Pranita Marajan", "Vijay Somanath", "Corey Harper", "Mao Yan", "Tony Scerri"], "abstract": "The quality and capabilities of large language models cannot be currently fully assessed with automated, benchmark evaluations. Instead, human evaluations that expand on traditional qualitative techniques from natural language generation literature are required. One recent best-practice consists in using A/B-testing frameworks, which capture preferences of human evaluators for specific models. In this paper we describe a human evaluation experiment focused on the biomedical domain (health, biology, chemistry/pharmacology) carried out at Elsevier. In it a large but not massive (8.8B parameter) decoder-only foundational transformer trained on a relatively small (135B tokens) but highly curated collection of Elsevier datasets is compared to OpenAI's GPT-3.5-turbo and Meta's foundational 7B parameter Llama 2 model against multiple criteria. Results indicate even if IRR scores were generally low- a preference towards GPT-3.5-turbo, and hence towards models that possess conversational abilities, are very large and were trained on very large datasets. But at the same time, indicate that for less massive models training on smaller but well-curated training sets can potentially give rise to viable alternatives in the biomedical domain.", "sections": [{"title": "1 Introduction", "content": "While the use of standardized, automated testing on natural language processing (NLP) benchmarks is still a must to measure the quality of large language models (LLMs), there is an emerging consensus on their limitations [LBL+22,ZZL+23]. These limitations are more evident in industry or domain-specific contexts where NLP benchmarks may not fully capture all the quality dimensions implied by practical use cases [AMM+23,SAT+22]. The same is true for custom-trained LLMs. Lastly, current benchmarks as defined by well-known \"harness\" frameworks are often encyclopedic in their coverage, and assume training on well-known pre-training corpora [CWW+23]. Hence the need to complement benchmark evaluation with human evaluation.\nThe state of the art in LLM evaluation in general, and human evaluation in particular, is an ever-evolving field thanks in large part to the rapid development of neural language models. Human evaluation is the process whereby human raters assess the quality of outputs generated by NLP systems [SVAV23], and rank or rate different model outputs according to pre-determined criteria. Currently, a widespread technique is A/B testing that in the context of LLMs involves comparing the performance of two different models (or two different versions of the same model) on the same task. Rating is often based on factors such as relevance, coherence, or fluency [ZCS+23,WID+23,CZS+24]. A/B testing for LLMs draws heavily from software engineering [QWGS24], where A/B testing has been used to measure user satisfaction w.r.t. new features of software systems, user interfaces and chatbots. This has lead additionally to the emergence of"}, {"title": "2 Methodology", "content": "Models and Prompts In some domains such as the life sciences, due to regulation, there traditionally has been a preference in grounding the outputs of predictive models text in well-documented, highly curated, or copyrighted data. Additionally, users typically domain-focused answers from such systems. By contrast, current LLMs whether proprietary or open source target wider domains of knowledge, and are often trained on crawled data that can suffer from a \"gray\" copyright status. But it in unclear which approach is the most fruitful.\nThus as few if any commercial or open source LLMs that cover solely Elsevier's life sciences domains of interest exist, a scholarly LLM focused on the biology, health and chemistry domains was trained from scratch using only Elsevier proprietary data, and fully open source but high quality text (e.g. patents). This model was a peer to the medium-sized Llama 2 family of foundational models, albeit with 8.8B parameters instead of 7B. It was pre-trained with a significantly smaller, but domain-focused corpus of around 135B tokens, for 1-3 epochs, in order to adhere to scaling laws. Training took around a month on a cluster on approximately 1024 A100 GPUs."}, {"title": "A/B tests and Judges", "content": "A total of 7 annotators (including some of the authors of this paper) participated in an on and off basis in the evaluation experiment for a total of two weeks. The team had a relatively diverse background ranging from linguistics to software engineering, life sciences and health. We created a dataset of 141 prompts covering the domains of biology, chemistry and health, in the form of open and closed book question answering prompts, each paired with a gold answer. These were derived from a number of Elsevier products, resp. Embase, Reaxys and ClinicalKey. In addition, we created a few closed-book prompts from the taxonomies that power Elsevier's search for biomedical products. The 47 inputs were fed into the 3 LLMs. This yielded a total of 188 prompts. Each such prompt gave rise to an A/B test. A/B tests were set by pairing responses from the Elsevier LLM to gold, GPT-3.5-turbo and Llama 2 7B responses, giving rise to 141 A/B tests. The names of the models were thereafter blinded. Judges (annotators) were thereafter asked to cast a vote along the following 4 criteria\n1. factuality (\u201cdoes the answer state a true fact?", "is the answer logically consistent or grammatically correct?\"),\n3. relevance (\"is the answer relevant to the question?\"), and\n4. overview (\u201cwhich answer is better across all prior three criteria": "nin favor of either generation, or else declare a tie. Prompts were moreover grouped by domain, namely: (i) chemistry; (ii) biology; (iii) health; and (iv) taxonomies (spanned across all the 3 domains). All A/B tests had the model trained on Elsevier data as root comparison. LLM completions were generated via vanilla temperature-based sampling with temperature t = 0.7.\nThis approach was inspired by Llama 2 human evaluations [TMS+23]. There was also a text field for the judges to write structured comments (in English) directed to the evaluation team if they so wish. It is important to note the inclusion of gold annotations to act as a control scenario. One the one hand, because we would expect vote distributions of the better models to align with the distributions of votes against gold data. And, on the other hand, to test if the high English fluency of LLMs can offset the accuracy of answers. Lastly, we note that we did not control for phenomena such as toxicity, hallucination or bias. This choice was deliberate, as these phenomena are sometimes hard to define and measure, especially with small numbers of human evaluators."}, {"title": "Elsevier Arena", "content": "As A/B testing platform, we decided to opt for an in-house pure-Python re-implementation of Chatbot Arena. This was because both of limitations (e.g. security, identify management, integration with Elsevier's platform) and undesired features (e.g. it is designed for live evaluation of LLMs, rather than pre-computed generations) of this open source platform. See Figure 1. Elsevier Arena was developed by Elsevier's technology department with the goal of serving as Elsevier's platform of choice for the human evaluation of generative artificial intelligence systems at Elsevier."}, {"title": "IRR scores", "content": "Krippendorf's a-score [Krill] is a measure of inter-rater reliability (IRR) and inter-annotator agreement (IAA) for scenarios where data points are annotated by varying numbers of judges, as was the case in the experiment reported where A/B tests were carried out by groups of unequal sizes. It is defined as follows:\n$\\alpha = 1 - \\frac{D_{obs}}{D_{exp}}$\n i.e., as the complement of the ratio between observed ($D_{obs}$) and expected ($D_{exp}$) disagreement. The closer to 0 the closer we are to a random judgement -e.g. rolling the dice. The closer to 1, the closer we are to absolute consensus. Annotation or rating is believed to be (moderately) reliable when \u03b1 \u2265 0.7. Scores greater than or equal to 0.8 indicate strong agreement and rating reliability. A value below zero is possible but indicates either systematic disagreement or defective experiment design [MBM24]."}, {"title": "3 Results", "content": "Voting Results and Agreement Elsevier judges managed to evaluate a total of 47/141 prompts within the duration (2 weeks) of the campaign, i.e., only 33% of the sum total. We think this was due to the large number of long prompts (and answers), combined with the inherent complexity of the domains. Votes were aggregated by criterion and model. This gave rise to 12 sets of counts, for every criterion-model combination. As expected, judges expressed a clear preference for gold set prompts, but also for powerful LLMs such as GPT-3.5-turbo, and this across all criteria. Regarding GPT-3.5-turbo, we can indeed see a somewhat similar distribution of votes as for the gold answers, although in all cases the \"both are bad\" vote was high. When compared to Llama 2, which is in a class similar to that of the Elsevier model, ties (both positive and negative) however become more common, meaning that annotators felt that both models were more or less equal in performance. See Figures 2-5.\nWe measured a scores for each A/B test, and aggregated them by model and criterion. Agreement is higher w.r.t. the gold set, as expected. Also, a higher degree of agreement can be observed when focusing on the overview criterion. Agreement is also somewhat higher regarding Llama 2 (which is felt as \"equally bad\" to an Elsevier-only trained model). Agreement for coherence is very low, but for other criteria, scores were similar. An analysis of variance w.r.t. criteria and models (each treated as ordered factors) indicated that the differences were statistically significant (p-values < 0.05). See Figures 6a and 6b. Agreement however fell short of the recommended 0.7-0.8 score, maxing out at less than 0.4 and was thus unreliable overall, albeit higher than random agreement. This might be due to the comparatively small number of judges vs. the complexity of our A/B testing set-up."}, {"title": "Generation Examples", "content": "Table 4 shows examples of answers of the various models to the various input prompts created by Elsevier, broken by domain and language model. The examples chosen correspond to open book question answering (OBQA) and closed book question answering (CBQA) prompts. In the former, the model is asked to extract the answer from its input, whereas in the latter, the model has to generate an answer purely from its internal knowledge. We can see that Llama 2 replied with a more generic and longer but \"safe\" answer vs. the model trained on Elsevier biomedical data, which instead hallucinates a very specific answer, albeit very similar to the intended answer: Skraup synthesis, that is close to the (incorrect) Hantzsch Quinoline synthesis cited. See the first two rows. Similarly, we can see that while Llama 2 correctly extracts a requested relation, it does it with low fluency: e.g. repeats back part of the input prompt. On the other hand, GPT-3.5-turbo tends to generate highly fluent, correct answers, very much like the human curated gold answers. See row four."}, {"title": "Influence of Training Corpus", "content": "The Elsevier corpus (see Table 1) was built with scholarly documents, and in particular journal papers and patents (37% biomedical journals and 64% biomedical patents). Meta has at this point in time (mind2-24) only disclosed the total number of tokens (2 trillion) used to train their foundational models, but not their source or distribution (whereas for OpenAI not even the token counts are known). We can presume that it overlaps with e.g., The Pile [GBB+20], which does contains patent (over 5%) and scholarly data (over 15%), but is otherwise very diverse, and covers a wider variety of less formal English texts. Our narrow focus did not however negatively penalize generation against Llama2 7B, and required less than 10% of the data used by the Meta model."}, {"title": "Limitations", "content": "The scope of the experiment described was narrowly focused on comparing the factuality and scholarly fluency of custom LLMs trained on relatively small collections of scholarly biomedical texts, against publicly available (commercial or open source) LLMs. We did not conduct any red teaming analysis, or control for toxicity, hallucination or bias. One reason for its limited scope was a concern with costs. Indeed, 4 weeks of pre-training implies 720 hours of GPU cluster time at an approximate hourly rate of 1 US$ per GPU, which gives an estimate of \u2265 700,000 US$ for pre-training compute only. This excludes the time and cost of collecting and curating the pre-training dataset, running the evaluation, or analyzing the outcome, which consumed several months."}, {"title": "4 Conclusions", "content": "Our experiments show that it is possible to train viable \"smaller\" biomedical LLMs that compare reasonably well against similar models (foundational Llama 2 7B) trained on massive datasets on domain-specific generation tasks. But the responses from these models fall short compared to the generic commercial solutions pre-trained on massive data and tuned for conversation. The use of human evaluation methods allows for measuring quality dimensions that are often poorly captured by current quantitative benchmarks and performance metrics, namely fluency (coherence), accuracy (factuality) and relevance (consistency). Statistically significant differences between the models and criteria were observed, despite the small number of evaluation prompts (141) and human evaluators (7). On the other hand, agreement/IRR scores were generally low (0.3-0.4 range), indicating low consensus among evaluators. The small number of prompts prevented also a more fine-grained analysis across domains and prompt types (biology, chemistry, health and taxonomies on he one hand, and NLP tasks on the other hand)."}]}