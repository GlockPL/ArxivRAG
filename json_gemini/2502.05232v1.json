{"title": "Aligner-Encoders:\nSelf-Attention Transformers Can Be Self-Transducers", "authors": ["Adam Stooke", "Rohit Prabhavalkar", "Khe Chai Sim", "Pedro Moreno Mengibar"], "abstract": "Modern systems for automatic speech recognition, including the RNN-Transducer\nand Attention-based Encoder-Decoder (AED), are designed so that the encoder is\nnot required to alter the time-position of information from the audio sequence into\nthe embedding; alignment to the final text output is processed during decoding.\nWe discover that the transformer-based encoder adopted in recent years is actually\ncapable of performing the alignment internally during the forward pass, prior to\ndecoding. This new phenomenon enables a simpler and more efficient model, the\n\"Aligner-Encoder\". To train it, we discard the dynamic programming of RNN-T\nin favor of the frame-wise cross-entropy loss of AED, while the decoder employs\nthe lighter text-only recurrence of RNN-T without learned cross-attention-it\nsimply scans embedding frames in order from the beginning, producing one token\neach until predicting the end-of-message. We conduct experiments demonstrating\nperformance remarkably close to the state of the art, including a special inference\nconfiguration enabling long-form recognition. In a representative comparison, we\nmeasure the total inference time for our model to be 2x faster than RNN-T and\n16x faster than AED. Lastly, we find that the audio-text alignment is clearly visible\nin the self-attention weights of a certain layer, which could be said to perform\n\"self-transduction\".", "sections": [{"title": "1 Introduction", "content": "The task of sequence transduction requires mapping from an input sequence to an output sequence.\nIt appears in several widespread applications including machine translation and automatic speech\nrecognition (ASR). In ASR, which is the focus of this paper, the two sequences lie in completely\ndifferent modalities, and the audio input representation is typically much longer than the output text.\nThus, in addition to identifying and converting speech sounds, the model must find an \u201calignment\u201d\nthat moves information from wherever it appears in the input sequence to wherever it belongs in\nthe output. Among other issues, the many possible variations in the pace of pronunciation make\ntransduction a challenging problem.\nMore than a decade ago, and almost a decade apart, two powerful algorithms relying on dynamic\nprogramming were developed to perform the sequence transduction task with recurrent neural net-\nworks (RNNs). The motivation behind these algorithms was that RNNs require training every output\nframe in the sequence\u2014with one output per input frame, they needed to be trained with alignments\nalready prepared. The new algorithms allowed training without prepared alignments by computing\nthe probability of the label marginalized over all possible alignments as the optimization objective.\nThe differences in sequence length are accommodated by learning to output \"blank\" symbols in\nbetween true labels and post-processing them out at inference time. The first algorithm, Connectionist\nTemporal Classification (CTC) [1], models the output frames as conditionally independent, and this"}, {"title": "2 Model", "content": "The Aligner-Encoder model combines the best elements of RNN-T and AED systems into a more\ncompact form. By requiring the encoder itself to text-align its embedding, we avoid using 1) dynamic\nprogramming to sum probabilities in the loss and 2) full-sequence cross-attention in the decoder (in\nall models we refer to everything inside the auto-regressive portion as the \"decoder\"). It may be\nsimplest to first lay out our model, which is formulated using the same components as RNN-T, and\nafterwards draw contrasting points with the heritage.\nWe begin with an input sequence x = (x1, x2,..., xT) of length T, and output sequence y =\n(y1, y2, ..., yU) of length U < T. An encoder network, fenc, processes the input sequence into an\nacoustic embedding sequence, h = (h1, h2, ..., hT'), where each frame of the embedding sequence\ncan depend on every frame of input, and typically T' < T with subsampling. A prediction network,\nfpred, processes text labels with forward recurrence only to produce a text embedding sequence,\ng = (g1, g2, ..., gU). The acoustic and text embeddings are fed into a joint network, fjoint, to produce\nthe final prediction vector of dimension V according to the vocabulary, for each frame, with softmax\nprobability normalization. The difference in our model is that we enforce the alignment at the encoder\noutput by restricting the model to use the acoustic and text embedding frames in a one-to-one fashion\n(Equation (3)). The entire model is written by its recurrence relations as:"}, {"title": "2.1 Aligner-Encoder Model", "content": "The Aligner-Encoder model combines the best elements of RNN-T and AED systems into a more\ncompact form. By requiring the encoder itself to text-align its embedding, we avoid using 1) dynamic\nprogramming to sum probabilities in the loss and 2) full-sequence cross-attention in the decoder (in\nall models we refer to everything inside the auto-regressive portion as the \"decoder\"). It may be\nsimplest to first lay out our model, which is formulated using the same components as RNN-T, and\nafterwards draw contrasting points with the heritage.\nWe begin with an input sequence $x = (x_1, x_2,..., x_T)$ of length $T$, and output sequence $y = (y_1, y_2, ..., y_U)$ of length $U < T$. An encoder network, $f_{enc}$, processes the input sequence into an\nacoustic embedding sequence, $h = (h_1, h_2, ..., h_{T'})$, where each frame of the embedding sequence\ncan depend on every frame of input, and typically $T' < T$ with subsampling. A prediction network,\n$f_{pred}$, processes text labels with forward recurrence only to produce a text embedding sequence,\n$g = (g_1, g_2, ..., g_U)$. The acoustic and text embeddings are fed into a joint network, $f_{joint}$, to produce\nthe final prediction vector of dimension $V$ according to the vocabulary, for each frame, with softmax\nprobability normalization. The difference in our model is that we enforce the alignment at the encoder\noutput by restricting the model to use the acoustic and text embedding frames in a one-to-one fashion\n(Equation (3)). The entire model is written by its recurrence relations as:\n$h = f_{enc}(x)$                                                                                               (1)\n$g_i = f_{pred}(g_{i-1}, y_{i-1}), i \\leq U$                                                          (2)\n$P(y_i|x, y_{<i}) = f_{joint}(h_i, g_i), i \\leq U$                                             (3)\nThe encoder and the decoder-which includes both the prediction and joint networks-are parameterized\nand learned together in an end-to-end manner, with total parameters $\\theta$. We maximize the log\nprobabilities of the correct labels, resulting in the familiar cross-entropy loss:\n$L_{Aligner}(\\theta) = - \\sum_{i=1}^{U}log P(y_i|x, y_{<i}; \\theta)$                                                        (4)\nThe loss only applies to encoder frames within the length of the label, $T' < U$; all remaining frames\n($T' > U$) are ignored. Hence the encoder must also learn to be an aligner.\nWe seed the prediction network with a start-of-sentence token, <SOS>, at the first step, y0, and empty\nstate h0 = 0. As is often the case with AED and other uses of cross-entropy loss, we find label\nsmoothing to be a beneficial regularizer and always use it [23\u201325] (weight 0.1). During inference\nthe label length is unknown, so the model must predict it, as in AED. We train the model to always\npredict the end-of-sentence token, <EOS>, as the final token of every training example\u00b2, so decoding\nproceeds token-by-token until <EOS> is predicted."}, {"title": "2.2 Advantages over RNN-T & AED", "content": "Given that previous models were developed to overcome contemporary neural encoders' inability to\nalign, the new model naturally brings simplifications. RNN-T trains by explicitly marginalizing over\nall possible alignments in the loss. This can be visualized in a label-frame decoding lattice (see Figure\n1 in [2]), where each node represents an encoder-frame, text-frame} pair, and every pairing is a valid\nstate to be considered, resulting in a $U \\times T$ rectangular grid (abbreviating T' as T). Beyond requiring\na sophisticated dynamic programming implementation to calculate tractably (involving forward and\nbackward probabilities scanned across the lattice), the marginalization still leads to computing all\n$U \\times T \\times V$ logits of the lattice, a potentially memory-intensive step (it is computing Equation (3)\nover all pairs of indices as $f_{joint}(h_i, g_j)$). The Aligner loss can be viewed as prescribing only the main\ndiagonal of the lattice to be valid, reducing the realized logits to the bare minimum $U \\times V$.\nSavings come during inference, as well. The decoder in AED operates with $O(U \\times T)$ complexity,\nsince it cross-attends to the entire encoder embedding sequence at every step, for U steps. Furthermore\nthe constant factor is often relatively large in practice since a more powerful decoder network is\nneeded for computing the attention (the recurrence relation is $g_i = f_{dec,AED}(h, g_{i-1}, y_{i-1})$, compare\nat our Equation (2) which lacks h or even $h_i$). The RNN-T decoder has complexity $O(T + U)$,\nsince it must emit a blank at every frame to advance to the end of the lattice in a addition to the\nsteps emitting a label. In contrast, by preparing the alignment within the encoder, our model reduces\ndecoder complexity to $O(U)$, the most efficient possible for auto-regressive token prediction. In\npractice our constant factor will also be small like RNN-T.\nOne final savings relative to RNN-T comes when using beam search during inference, which can\nsignificantly improve overall accuracy. The emissions of blank tokens in RNN-T means that the same\ntext hypothesis can be represented by different paths through the decoding lattice. Thus a proper\nsearch requires a routine to combine the probabilities from equivalent lattice paths at every step.\nKnown as \"path merging\", it can actually be an expensive operation, leading others to sometimes\napproximate the search without it, despite the potential loss of beam diversity (e.g., see discussion\nin [26]). Since our model, like AED, does not emit any blank tokens, path merging does not apply."}, {"title": "3 Related Works", "content": "Before end-to-end ASR, the previous generation of \u201chybrid\u201d ASR systems [27] relied on a separate\nbase system to provide frame-level alignments which could then be used to train frame-level neural\nnetwork acoustic models. Most modern ASR systems use an end-to-end framework; the most\nprominent modeling techniques are CTC [1], RNN-T [2], and AED [4, 5].\nA number of works have aimed at modifying the base RNN-T model in order to gain efficiency or\nefficacy. In Monotonic RNN-T [28], the loss and decoder are modified to step forward every frame,\npreventing multiple token emissions per label and speeding up the decoder complexity to O(T).\nMore recently, [12] proposed a method to reduce memory usage of the RNN-T loss, especially\nfor cases with large vocabulary size such as Chinese character-based, by using a linearized joint\nnetwork first to identify a relevant sub-region of the lattice likely to contain the true alignment. They\nonly compute the full joint network on that sub-region and still train successfully. Another issue\nwith RNN-T systems arises when trying to treat the prediction network as a language model, which\ncan be improved through text-only training, because its language modeling function is polluted by\nhaving to predict blanks. To ameliorate this, multiple works [29\u201331] have introduced a separate\nnetwork component for predicting blanks, and re-formulated the overall prediction model accordingly,\nresulting in much greater receptiveness to language training. One aim of our design was to avoid the\nuse of blank tokens entirely, although in the present work we do not pursue additional text training."}, {"title": "4 Experiments", "content": "We conducted a range of experiments to explore the performance of Aligners and compare them\nagainst previous models on an equal footing. After describing the common configurations and\ndatasets used, we present three different sets of experiments. The first explores the performance\nof basic Aligner models, the second studies techniques to employ at test-time to attain long-form\nrecognition, and the final set examines the alignment process occurring within the encoders."}, {"title": "4.1 Datasets", "content": "We experiment on three U.S. English datasets with very different characteristics. The first is\nLibriSpeech-960 hour (LS) [42]. The second is a Voice Search (VS) dataset comprised of real\ntraffic, totalling over 100M utterances and nearly 500k hours of speech, with an average duration\nof 3.4 seconds; utterances are anonymized before processing. The majority of the utterances are\npseudo-labeled by a teacher model [43], and a small portion are human transcribed. Only 5% of\nthe queries are greater than 7.6 seconds long. The test sets for VS include a main set which covers\nmany common types of utterances, and several rare-word sets generated using TTS from specific\ndomains-maps, queries, and news. Lastly, we include a long-form dataset drawn from random\nYouTube videos (YT). The training set is comprised of utterances with pseudo-labels generated by\na previous ASR system [43], cleaned to ensure no speaker overlap occurs, and ranging from 5-15\nseconds each. The total training set size is roughly 670K hours. The test set includes 30 hours of\nlong-form audio, at an average of 8 minutes per example, spanning a range of topic sources including\nlifestyle, entertainment, sports, gaming, and society. In all datasets, the audio input is represented\nusing log-mel features with a 32ms window size at a 10ms stride."}, {"title": "4.2 Neural Networks Specifications", "content": "We used the same neural encoder architecture for every algorithm, with settings adjusted for each\ndataset as listed in Table 1. Our networks begin with learnable 2-D convolutional subsampling\nlayers, with kernel size 3 and stride 2, 128 features in the first layer and 32 thereafter. We employ\nstate-of-the-art Conformer encoders [20], which include in each layer: a feed-forward unit, a multi-\nheaded self-attention layer, a 1-D convolution layer for localized processing, followed by an outgoing\nfeed-forward unit and finally residual connection. The number of layers and model dimension vary by\ndataset, as shown in the table. We did not find RNN-T nor Aligner experiments to be highly sensitive"}, {"title": "4.3 Base Model Results", "content": "Table 2 shows results in Voice Search, includ-\ning the rare-word test sets, and Aligners perform\ncomparably to RNN-T in all cases. We also com-\npare CTC and non-AR Aligners, which perform\nnearly as well in Voice Search. We found that\nincreasing the vocabulary size to 32K improved\nthe performance of the non-AR Aligner, yet it\nstill suffers from many deletions on the NEWS\nrare-word set, which contains longer utterances.\nOur non-AR model performed relatively poorly\non LibriSpeech and YouTube. For sequences\nbeyond a very short length, an auto-regressive decoder, however small, is essential for producing a\nhigh quality final output from the encoder embedding.\nIn LibriSpeech, we report results comparing CTC, RNN-T, AED, and Aligners in Table 3. All models\nused the same 17-layer conformer encoder architecture and dimensions, except for differences in\nrelative position attention, described in the following paragraph. Performance of our Aligner models\nwas remarkably close to RNN-T, matched or beat AED, and clearly beat CTC. For each model, we\nreport the best score from a small number of runs and checkpoints. Full training settings are provided\nin the appendix, along with a brief commentary on the relative performance between RNN-T and\nAED.\nThe distribution of training utterance lengths in\nLibriSpeech has a sharp drop-off after 17 sec-\nonds, and we observed both AED and Aligners\nlosing performance when generalizing to longer\ntest utterances, which run as long as 36 seconds.\n(See Tables 7,8 in the appendix for a breakdown\nby test utterance duration.) To improve perfor-\nmance of these models, we randomly concate-\nnated a small subset of utterances in each train-\ning batch (e.g., 15%), creating some examples up to 36 seconds. Thus, with adequate training,\nAligners were able to self-transduce sequences of up to 900 frames. Learned relative attention encod-\ning [46] trained slowly, so AED, CTC, and our model were trained with the faster Rotary Position\nEmbedding [47] (RNN-T achieved slightly better test scores with learned relative encoding, reported\nin the table). In the following section, we discuss ways to use our model to perform long-form\nrecognition to arbitrary lengths beyond what is seen in training."}, {"title": "4.4 Long-Form Recognition", "content": "It is not always practical to train with examples as long as the desired usage, and for any given\nAligner-Encoder architecture there is likely some maximum alignable length. So it may be necessary\nto perform recognition on utterances longer than a trained model's capability. Our baseline method\nfor comparing long-form recognition is \"blind segmenting\": 1) cut the audio into fixed-length, non-\noverlapping segments, 2) transcribe them separately, and 3) concatenate the resulting text segments.\nPerhaps the main shortcoming of blind segmenting is that recognition at the boundaries is prone to\nerror, where the audio might cut in the middle of a word. One applicable solution is to use overlapping\nsegments with an appropriate routine to stitch the hypothesis together in post-processing [48, 49].\nHere, we instead describe how to improve continuity using the model itself, using only inference\nconfigurations which require no further training.\nIn our approach, cutting and re-concatenating the sequence happens within the model-called \u201cchunk-\ning\" to distinguish from the baseline. We preserve some continuity by cutting the audio only after\nthe 2-D convolutional feature layers. The chunks are processed independently in parallel through\nthe conformer, after which they are re-concatenated before decoding. The decoder processes the\nfull-length embedding sequence into the full transcription hypothesis, using awareness of the chunk\nboundaries. Within each chunk, the decoder ignores frames after the first <EOS> emission, and it\nresumes decoding at the beginning of the next chunk. Ideally, the decoder's prediction network will\ncarry its state forward across the chunk boundaries to evenly incorporate all tokens.\nWhen we experimented with chunk sizes close to the training length, however, it led to increased\ndeletions near the chunk ends. It was actually better to reset the prediction network state. This\nis understandable as the decoder was only ever trained to begin each utterance with a blank state,\nand it may be helping to count frames until <EOS>. The best performance, however, came from a\nmiddle approach: at the chunk boundary we reset the prediction network state but then \u201cprime\" it\nby re-processing the last several tokens through it. We found state-priming to reduce the number of\nerrors at the boundary without raising later deletions.\nFor testing, we turn to the YouTube domain, where the training utterances ranged from 5-15\nseconds long but test utterances spanned several\nminutes. Table 4 compares results from RNN-T\nand the Aligner-Encoder. They achieved equal\nWER (7.6%) with a 15-second blind segmenter,\ndue to equivalent errors at the boundaries. In\nthe unsegmented arrangement, the RNN-T, with local attention of width 256, generalized well to\nthe full utterance length; its WER improved to 6.8%3. Inside the Aligner, we applied a chunking\nperiod of 14 seconds (176 embedding frames). We found it best to reset the prediction network\nat the chunk boundaries and prime it with 10 tokens of history, which achieved 7.3% WER (7.5%\nwithout state-priming). While falling short of the RNN-T in this case, the Aligner did improve\nperformance by smoothing the chunk boundaries, even without having been trained to do so. This\nresult demonstrates our model's capability for long-form recognition, and could possibly be improved,\nsuch as by introducing additional training for this mode."}, {"title": "4.5 Alignments", "content": "To study how the encoder predicts the alignment, we examine the self-attention probabilities it\ncomputes during the forward pass."}, {"title": "4.5.1 Self-Attention Probabilities", "content": "To study how the encoder predicts the alignment, we examine the self-attention probabilities it\ncomputes during the forward pass."}, {"title": "4.5.2 Embedding Alignment", "content": "To further study the alignment process,\nwe also examined the intermediate em-\nbeddings themselves by the following pro-\ncedure. Given the converged Aligner-\nEncoder model, we trained an RNN-T\nmodel of the same size, which is randomly\ninitialized except that the parameter values\nfrom the beginning layers of the Aligner\nare used in the corresponding layers of the\nnew encoder. The Aligner weights were\nkept frozen, so that the portion of the model\ntrained by RNN-T receives as input the in-"}, {"title": "4.5.3 Failure Mode Analysis", "content": "Using the same diagnostic technique,\nwe can examine the failure mode of\npoor length generalization. Figure 4\nshows such a case, for an utterance\nthat is 1.5x longer than any training ex-\nample. The model is only able to align\na portion of the utterance, explaining\nwhy sometimes many deletions occur\nfrom dropping the latter part of the\ntext. Interestingly, the RNN-T model\ntrained atop the 15 frozen Aligner-\nEncoder layers (as in the previous sub-\nsection) still assigns some probabil-\nity to the remainder of the label se-\nquence, in a somewhat aligned fash-\nion but pushed to the tail of the em-\nbedding sequence. This part is not de-\ncoded successfully. As we increased\nthe number of Aligner layers used, we\nobserved that the RNN-T hypotheses\nbegan to lose tail words several layers\nprior to the apparent alignment layer.\nAlong with the self-attention visual-\nization of Figure 2, this supports the\nconclusion that the Aligner works for\nseveral layers to prepare for the align-"}, {"title": "4.5.4 Reverse Alignment", "content": "We conducted one final experiment to\ndemonstrate the likely applicability of\nAligner-Encoders to tasks other than\nASR. Specifically, we desired to show\nthat our model has greater flexibility\nin aligning information than only the\nmonotonic and increasing alignment\nrelationship of standard ASR. To do\nso, we constructed the extreme case of\ndecreasing alignment by completely\nreversing the audio input, so frames\nat the end of the audio input corre-\nspond to the beginning of the text out-\nput, and vice versa. Experimenting\nin LibriSpeech, the resulting model\nachieved similar performance as the\nregular model on utterances within\nthe training length. Further, the self-\nattention weights exhibited the same behavior as in the forward model, except that in the aligning\nlayer the weights trace from the upper-left to lower-right, showing the sequence reversal in action"}, {"title": "4.6 Computational Efficiency", "content": "Considering training speed, memory usage, and inference speed, we observed favorable comparisons\nfor our model owing to its reduced complexity. While the exact numbers will depend on many\nhardware and implementation details, we present a representative case using our 100M-parameter\nencoder LibriSpeech models in Table 5. Notably, the RNN-T spent roughly 290ms per training step\nin the decoder and loss (forward + backward propagation), whereas our model required only 29ms, a\n10x gain. Our RNN-T loss implementation is highly optimized, so the majority of the gains came\nfrom eliminating the scan associated with the T dimension of the joint network output. The peak\nmemory usage for our model decreased by 18% (by 1.4G per accelerator)4. The 4-layer AED decoder\ntrained with a step time essentially as fast as ours, thanks to good parallelization of transformers, and\ndid tend to converge in significantly fewer steps (see A.2).\nThe inference speed of our model, however,\nwas by far the fastest, especially relative to\nAED. We profiled all models using 11.5 sec-\nonds of audio, a batch size of 8, and beam\nsearch of size 6 (effective decoder batch size\n48). The decode step for the Aligner and RNN-T5 (LSTM + Joint network) measured a mere\n190\u03bcs. As discussed earlier, we can estimate\nthe total decode time as the step time multi-\nplied by the text length (U) for our model,\nversus multiplying by the sum of the audio\nand text lengths (T + U) for the RNN-T, re-\nsulting in substantial savings. We approximate with T = 300, U = 100. Turning to AED, each step\nthrough the transformer decoder was almost two orders of magnitude slower, at 8.5ms (and U steps\nare executed). The forward pass itself required 5ms per step, and re-ordering the decoder state as\npart of the beam search occupied the other 3.5ms. The re-ordering time was negligible in our model\ndue to the minuscule LSTM state. Including the encoder, the total inference time of our model is\nestimated at 2x faster than RNN-T and 16x faster than AED in this scenario."}, {"title": "5 Conclusion", "content": "We have shown that transformer-based encoders are able to perform audio-to-text sequence alignment\ninternally during a forward pass, using only self-attention (i.e., prior to decoding, unlike previous\nmodels). This finding enables the use of a much simpler ASR model, the Aligner-Encoder, which has\na simple training loss and achieves lower decoding complexity than past models. A key design point\nof our model is to keep the benefit of auto-regressive modeling while removing as much computation\nas possible from the auto-regressive loop. Desirable extensions to our model for ASR that would\nrequire further study include: use in streaming recognition, ability for multilingual recognition, and\nincorporation of traditional pretrained encoders, to name a few. Fusion with an external language\nmodel [24] could be simplified relative to RNN-T owing to the absence of blank tokens, and for the\nsame reason our model may be more receptive to training on other losses such as from text-only data.\nLooking beyond ASR, application to machine (text) translation would require some modification to\npermit output sequences longer than the input, although speech translation would not. Altogether,\nAligner-Encoders and their newly identified ability to learn self-transduction present promising\nopportunities for future research and application."}]}