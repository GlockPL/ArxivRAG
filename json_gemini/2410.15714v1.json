{"title": "OFFLINE REINFORCEMENT LEARNING FOR JOB-SHOP SCHEDULING\nPROBLEMS", "authors": ["Imanol Echeverria", "Maialen Murua", "Roberto Santana"], "abstract": "Recent advances in deep learning have shown significant potential for solving combinatorial optimization problems in\nreal-time. Unlike traditional methods, deep learning can generate high-quality solutions efficiently, which is crucial\nfor applications like routing and scheduling. However, existing approaches like deep reinforcement learning (RL) and\nbehavioral cloning have notable limitations, with deep RL suffering from slow learning and behavioral cloning relying\nsolely on expert actions, which can lead to generalization issues and neglect of the optimization objective. This paper\nintroduces a novel offline RL method designed for combinatorial optimization problems with complex constraints,\nwhere the state is represented as a heterogeneous graph and the action space is variable. Our approach encodes actions in\nedge attributes and balances expected rewards with the imitation of expert solutions. We demonstrate the effectiveness\nof this method on job-shop scheduling and flexible job-shop scheduling benchmarks, achieving superior performance\ncompared to state-of-the-art techniques.", "sections": [{"title": "Introduction", "content": "Recently, the use of deep learning has emerged as a relevant field of\nstudy for solving combinatorial optimization problems [1]. One of\nthe main advantages of deep learning over previous methods is its\nability to generate high-quality solutions in real-time, unlike exact\nor metaheuristic-based approaches whose execution times increase\nwith the complexity of the instances to be solved [2]. Real-time\nresolution offers significant benefits in multiple domains, such as\nrouting or scheduling [3], by enabling rapid responses to disrup-\ntive events and facilitating resource optimization through efficient\nscenario simulation.\nThese methods have largely relied on generating policies through\ndeep reinforcement learning (DRL) [4]. This involves training an\nagent to learn a policy by interacting with an environment and\nreceiving feedback in the form of rewards or penalties based on\nits actions. Another strategy, which falls under the umbrella of\nbehavioral cloning (BC) methods, involves generating a set of\noptimal solutions and training the policy to mimic the generation\nof these solutions [5].\nNevertheless, both learning methods have significant shortcomings.\nDRL-based approaches, which begin by exploring the solution\nspace randomly, can be slow to learn and may not always succeed\nin finding optimal solutions, especially in complex scenarios. On\nthe other hand, BC-based methods do not account for rewards and\nhence ignore the optimization objective, relying solely on expert\nactions. Furthermore, this reliance on expert observations can lead\nto generalization issues when the policy encounters new situations\nor suboptimal states due to flaws in the policy [6].\nOffline RL has been introduced as a hybrid approach that leverages\nthe strengths of both methods while requiring only a set of example\ninstances for training [7]. This method reduces dependency on\nreal-time data and allows the policy to learn from a large, diverse\nset of historical data. Although its application to combinatorial\noptimization problems is still under-explored [5], offline DRL is\nincreasingly recognized as a preferable method over BC, even\nwhen optimal solutions are available [8]. Therefore, this paper\nseeks to explore this promising area by proposing a new offline\nRL algorithm specifically tailored for combinatorial optimization\nchallenges that involve multiple difficult constraints and requires\nreal-time solutions."}, {"title": "Related work", "content": "In this section, we review literature on real-time scheduling solu-\ntions, graph-based RL, and offline RL algorithms to identify key\nresearch gaps."}, {"title": "Real-time scheduling problems", "content": "To organize the approaches that address scheduling problems in\nreal-time, we will first explore RL-based techniques, beginning\nwith the methods that solve the JSSP and then moving on to the\nFJSSP. Subsequently, methods using other approaches, such as BC\nor self-supervised learning, will be examined.\nMethods for solving real-time scheduling problems often build\nupon previous research that addressed simpler combinatorial chal-\nlenges as they contain less constraints, such as the Travelling\nSalesman Problem and the Capacitated Vehicle Routing Problem\n[27, 28, 29]. Recent approaches [28] make use of transformer ar-\nchitectures [30], modeling the problem as a sequence of elements\nwhere each element is defined by a distinct set of features. How-\never, these methods often do not explicitly consider the different\ntypes of nodes or the attributes of the edges that connect them.\nIn scheduling problems, which involve different types of entities\n(operations, jobs, and machines), most approaches model the prob-\nlem as a graph since this facilitates effective problem modeling,\nalbeit coupled with the use of specific neural networks that can\nprocess this type of representation. The most common way to gen-\nerate solutions is constructively, where solutions are constructed\niteratively: at each step, an element is selected based on its char-\nacteristics. For instance, in job scheduling, the process could be\nvisualized as sequentially assigning operations to machines.\nEmploying this approach, in [12], the JSSP was modeled as a\ndisjunctive graph, employing a graph neural network (GNN) to\nextract information from instances and the Proximal Policy Opti-\nmization (PPO) algorithm [31] to optimize its policy. Building on\nthis framework, several methods have been proposed for the JSSP\nthat vary in how the network is trained, the reward function used,\nor how the problem itself is modeled [13, 14, 15]. These meth-\nods leverage policy gradient algorithms, such as REINFORCE\n[32], which focus on optimizing policies by directly estimating the\ngradient of the expected reward. While policy gradient methods,\nincluding PPO, are commonly used in reinforcement learning, their\napplication in offline RL remains limited, as offline approaches\ntypically favor techniques better suited for working with static\ndatasets.\nA different approach to the JSSP involves neural improvement\nmethods. Unlike constructive methods, where solutions are built\nthrough the sequential assignment of operations to machines, these\nmethods aim to enhance a solution by modifying the execution\nsequence of operations [16]. These methods also utilize variants\nof the REINFORCE algorithm to generate the policy.\nA notable approach for solving the FJSSP is detailed in [18], where\nthe problem is represented as a heterogeneous graph and optimized"}, {"title": "Offline reinforcement learning for graphs", "content": "Despite the potential of RL, applying it to real-world problems\npresents persistent practical challenges. Direct interaction with\nreal-world environments can be risky and costly, and standard RL\nalgorithms often suffer from extrapolation errors in such scenarios\n[33].\nOffline RL has demonstrated promise in domains such as robotic\nmanipulation [34] and natural language processing [35]. A sig-\nnificant challenge in this field is distribution shift [36], where the\nlearned policy may find out-of-distribution states, which are states\nnot represented in the training data and can lead to potentially\nsuboptimal performance. Existing offline RL methods can be cat-\negorized into two main types: policy-constraint methods, which\nregularize the learned policy to stay close to the behavior policy\n[36, 37]; and conservative methods, which create a conservative\nestimate of return. This estimate of return refers to a cautious\nprediction of the total future rewards that can be obtained from a\ngiven state and action, helping to avoid overestimation and improve\npolicy optimization [38].\nHowever, it is not always clear if offline RL outperforms BC, espe-\ncially when expert instances are available, as explored in various\npapers. Rashidinejad et al. [39] introduced a conservative offline\nRL algorithm using lower-confidence bounds, theoretically outper-\nforming BC in contextual bandits, though this was not extended to\nMarkov Decision Processes (MDPs). This suggests offline RL's\npotential to exceed BC, but RL's susceptibility to compounding\nerrors complicates this generalization. Other studies suggest BC or\nfiltered BC (which eliminates trajectories with low-quality demon-"}, {"title": "Preliminaries", "content": ""}, {"title": "Problem formulation", "content": "An instance of the FJSSP problem is defined by a set of jobs\n$I = \\{j_1, j_2,...,j_n\\}$, where each $j_i \\in I$ is composed of\na set of operations $O_{j_i} = \\{o_1, o_2,... o_{im}\\}$, and each opera-\ntion can be performed on one or more machines from the set\n$M = \\{m_1, m_2, ..., m_p\\}$. The JSSP is a specific case of this prob-\nlem where operations can only be executed on a single machine.\nThe processing time of operation $o_{ij}$ on machine $m_k$ is defined\nas $p_{ijk} \\in R^+$. We define $M_{o_{ij}} C M$ as the subset of machines\non which that operation $o_{ij}$ can be processed, $O_{j_i} \\subset O$ as the\nset of operations that belong to the job $j_i$ where $i \\leq n, \\bigcup_{i=1}^{n} O_{j_i} = O$,\nand $O_{m_k} \\subset O$ as the set of operations that can be performed\non machine $m_k$. The execution of operations on machines must\nsatisfy a series of constraints:\n\u2022 All machines and jobs are available at time zero.\n\u2022 A machine can only execute one operation at a time, and\nthe execution cannot be interrupted.\n\u2022 An operation can only be performed on one machine at a\ntime.\n\u2022 The execution order of the set of operations $O_{j_i}$, for every\n$j_i \\in I$ must be respected.\n\u2022 Job executions are independent of each other, meaning\nno operation from any job precedes or has priority over\nthe operation of another job.\nIn essence, the FJSSP combines two problems: a machine selec-\ntion problem, where the most suitable machine is chosen for each\noperation, a routing problem, and a sequencing or scheduling prob-\nlem, where the sequence of operations on a machine needs to be\ndetermined. Given an assignment of operations to machines, the\ncompletion time of a job, $j_i$, is defined as $C_{j_i}$, and the makespan\nof a schedule is defined as $C_{max} = max_{j_i \\in I} C_{j_i}$, which is the most\ncommon objective to minimize."}, {"title": "Offline RL and Behavioral Cloning", "content": "RL is a method for solving tasks that are organized in sequences,\nknown as a MDP, defined by the elements S (states), A (actions),\nR (rewards), p (transition probabilities), and y (discount factor)\n[42]. In RL, an agent follows a policy \u03c0, which can either di-\nrectly map states to actions or assign probabilities to actions.\nThe agent's objective is to maximize the expected total reward\nover time, represented as $E_\\pi [\\sum_{t=0}^{\\infty} \\gamma^t r_{t+1}]$, where y is the dis-\ncount factor. This objective is evaluated using the value function\n$Q^{\\pi}(s, a) = E_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} | s_0 = s, a_1 = a]$, which estimates\nthe expected rewards starting from state s and action a.\nIn offline RL, there is a dataset D that contains tuples of states, ac-\ntions, and rewards. This dataset is used to train the policy without\nfurther interaction with the environment, addressing the challenges\nof direct interaction. By leveraging offline data, offline RL avoids\nthe risk and expense associated with deploying exploratory poli-\ncies in real-world settings. The dataset D allows the agent to learn\nfrom a wide variety of experiences, including rare or unsafe states\nthat might be difficult to encounter through online exploration.\nBC is another approach that trains a policy by imitating an expert's\nactions. This type of imitation learning uses supervised learning to\nteach the policy to replicate actions from a dataset. The effective-\nness of this method largely depends on the quality of the dataset\nused for training. While BC can be straightforward, it does not\naccount for future rewards and may struggle with generalization to\nnew situations."}, {"title": "Method", "content": "In this section, we present our novel offline RL algorithm designed\nfor combinatorial optimization problems with heterogeneous graph"}, {"title": "The JSSP and FJSSP as an MDP", "content": "Before introducing our offline RL method, we describe how the\nJSSP and FJSSP have been modeled as an MDP. This modeling\nincorporates two key concepts:\n\u2022 Selective Operation Visibility: Not all operations are\ndisplayed; instead, the number of operations visible to the\npolicy for each job is restricted. This limitation provides\nthe policy with more targeted information, enhancing its\ndecision-making process.\n\u2022 Expanded Action Space: The action space allows for\nthe simultaneous assignment of multiple machines to op-\nerations, which reduces the number of steps needed to\ngenerate a solution and minimizes the number of model\nusages.\nThe MDP is structured through the definition of the state and action\nspaces, reward function, and transition function as follows:\nState space. The state space is modeled using heterogeneous\ngraphs, as described in [20]. At each timestep t, the state st is\nrepresented by a heterogeneous graph $G_t = (V_t, E_t)$, consisting\nof nodes for operations, jobs, and machines, along with six types\nof edges, both directed and undirected. To streamline decision-\nmaking, the number of visible operations per job is limited, al-\nlowing only a subset of information to be processed at each step.\nThis selective visibility is particularly beneficial for large instances,\nwhere operations with many pending tasks are less impactful for\ncurrent assignments. Detailed descriptions of node and edge fea-\ntures are available in the Appendix A.\nAction space. The action space At at each timestep t consists\nof feasible job-machine pairs. When a job is selected, its first\nunscheduled operation is chosen. To prevent an excessive number\nof choices, the action space is constrained by defining $t_e$ as the\nearliest time a machine can start a new operation and masking\nactions where the start time exceeds $t_e \\times p$, where p is a parameter\nslightly greater than one.\nTransition function. The solution is constructed incrementally\nby assigning operations to machines. At each step, the policy can\nmake multiple assignments, but with specific constraints: only one"}, {"title": "Offline Reinforcement Learning for Graphs", "content": "Our approach follows a minimalist strategy in offline RL, inspired\nby the work of [41]. The goal is to balance the maximization of\nexpected rewards with the minimization of the discrepancy be-\ntween expert actions and the policy's actions. This dual focus is\ncritical in offline RL, where the policy must be effective within the\nconstraints of a static dataset.\nWe build our proposal on the established Twin Delayed Deep De-\nterministic Policy Gradient (TD3) algorithm [43], which is known\nfor its robustness in continuous action spaces. In the standard TD3\nframework, the policy is optimized by maximizing the expected\nQ-value, as represented by the equation:\n$\\pi = argmax E_{s \\sim D}[Q(s, \\pi(s))]$\\nHowever, this approach solely focuses on maximizing expected\nrewards, which may not be sufficient in offline settings where\nthe policy must generalize well to unseen states while staying\nwithin the distribution of the training data. To address this, [41]\nintroduced an additional BC term that encourages the policy to\ngenerate actions similar to those observed in the dataset. This term"}, {"title": "Actor-critic architecture", "content": "We utilize a Heterogeneous GNN architecture to extract features\nfrom the states, inspired by prior work using attention mechanisms\n[45]. As an example, we will show how the embedding of a job\nnode $j_i$ is calculated. This embedding is computed by aggregating\ninformation from related operations, machines that can process\nthese operations, and edges connecting the job to these machines.\nThe initial representation $h_{j_i} \\in R^{d_I}$ is updated by calculating\nattention coefficients that weigh the contributions of these con-\nnected nodes and edges. For instance, the attention coefficient\n$A_{m_kj_i}$ between a job $j_i$ and a machine $m_k$ is computed as:\n$A_{m_kj_i} = softmax\\left(\\frac{(W^{QM}h_{j_i})^T(W^{KM}h_{m_k} + W^{VM}h_{m_kj_i})}{\\sqrt{d'_{IM}}}\\right)$  \nwhere $W^{QM} \\in R^{d'_{IM}Xd_J}$, $W^{KM} \\in R^{d'_{IM}Xd_M}$, and\n$W^{VM} \\in R^{d'_{IM}Xd_{IM}}$ are learned linear transformations, $d'_{JM}$\nis the dimension assigned to the hidden space of the embeddings,\nand $h_{m_kj_i}$ is the edge attribute between the job $j_i$ and the machine\n$m_k$. As mentioned in the previous section, $log(softmax(a))$ is\nadded to this edge for the critic. The attention coefficients $A_{o_{ij}j_i}$ between\nthe job $j_i$ and the operations $o_{ij}$, and $A_{j_{i'}j_i}$ between the\njob $j_i$ and other jobs $j_{i'}$ are computed in a similar manner."}, {"title": "Instance Generation for Offline RL in Combinatorial\nOptimization Problems", "content": "In the domain of combinatorial optimization, it is common practice\nto employ solvers or metaheuristic methods that can effectively\nidentify optimal or near-optimal solutions for smaller problem\ninstances [46]. Given the inherent difficulty in discovering optimal\nsolutions through RL alone, these methods play a critical role in\ngenerating expert experiences that are subsequently leveraged as\ntraining data for offline RL."}, {"title": "Experiments", "content": "In this section, we present the experimental results to validate\nour method. First, we compare our offline RL method for graphs,\nwhich we refer to as H-ORL, with BC to demonstrate how incor-\nporating reward information enhances the policy's performance.\nNext, we compare our algorithm for both the JSSP and FJSSP\nagainst various state-of-the-art DRL and BC methods using bench-\nmark problems."}, {"title": "Experimental setup", "content": ""}, {"title": "Configuration", "content": "For our proposed method, we utilized Python 3.10 and PyTorch\nGeometric for implementing the graph neural networks [47]. For\nthe constraint programming method, we chose OR-Tools\u00b2 due\nto its open-source availability and extensive use in scheduling\nproblems [48]. Table 3 details the configuration hyperparameters\nused for training our model, which are similar to other DRL-based\napproaches [18] [19]. The parameters used related to the TD3\nalgorithm are the default parameters as in the TD3 offline imple-\nmentation from Offline RL-Kit [49]. The code will be published\nuppon acceptance of the paper.\nFor the environment, two parameters need to be set: first, the num-\nber of operations visible for each job (set to 10); and second, to\nlimit the action space, we define $t_e$ as the earliest time a machine\ncan start a new operation. Actions are masked if their start time\nexceeds $t_e \\times p$, where p is set to 1.05."}, {"title": "Dataset Generation and Benchmarks", "content": "Two models were trained for the JSSP and FJSSP, with instances\ngenerated according to the specifications detailed in Table 4.\nJSSP Dataset. A set of 1000 instances was generated for training\nand 50 for validation, solved using OR-Tools with a ten-second\nlimit. The generation followed the method proposed by Taillard\nFJSSP Dataset. Similarly, 1000 instances were generated for\ntraining and 50 for validation, with each instance solved using\nOR-Tools within a one-minute time limit, following the method\nadapted from [51]. The generation process involved sampling the\nnumber of jobs, machines, and operations per job from defined\nranges, as well as varying the processing times to reflect different\nscenarios."}, {"title": "JSSP Test Benchmarks", "content": "For the test set, we utilized the well-\nknown JSSP benchmark dataset by Taillard [50]. This dataset\nincludes 80 instances, ranging from 15 jobs and 15 machines (with\n225 operations) to 100 jobs and 20 machines (with 2000 oper-\nations), providing a comprehensive assessment of the method's\nperformance across different instance sizes."}, {"title": "FJSSP Test Benchmarks", "content": "For evaluating our FJSSP model, we\nused five benchmark datasets: Brandimarte [51], Hurink [52] (sub-\ndivided into three categories - vdata, edata, and rdata), and Dauz\u00e8re-\nP\u00e9r\u00e8s and Paulli [53]. These benchmarks are widely recognized\nand cover a range of instance sizes, from small to medium-large"}, {"title": "Baselines and evaluation metric", "content": "The following outlines the baselines for each of the problems.\nJSSP Baselines. We compared our approach with several DRL-\nbased state-of-the-art constructive methods, such as the method\nproposed in [15], (referred to in this paper as RLCP); the algorithm\nintroduced in [17] (ResSch); and the method proposed in [54]\n(BiSch). Additionally, we compared with an improvement-based\nDRL methods proposed on [16] called L2S and a method based on\nself-supervised learning [26] referred to as SPN. In all cases, the\ngreedy strategy was used for comparison, which involves select-\ning the action with the highest probability assigned by the policy,\nexcept for [16], where the variant that improves a solution in 500\nsteps was used due to its lower computational cost and similar\nexecution time.\nFJSSP Baselines. Our method was evaluated against seven recent\nDRL-based state-of-the-art approaches: [18] (referred to as HGNN\nfor proposing the use of heterogeneous graphs), [19] (DANIEL),\n[17] (ResSch), [21] (referred to as LMLP for proposing the use\nof a lightweight multi-layer perceptron), [20] (EDSP), [25] (BC),\nand [22] (referred to as GGCT for its use of Graph Gated Chan-\nnel Transformation). For [18], [20], [25], and [19], we utilized\nthe models provided in their respective code repositories with the\ndefault parameters. All approaches used the proposed dataset\nbenchmarks except for ResSch, GGCT, and LMLP, which did not\nuse the Dauz\u00e8re-P\u00e9r\u00e8s and Paulli benchmark. A summary of the\ndeep learning-based baselines is provided in Table 5 and a more\ndetailed explanation of the baselines in Table 1.\nMeta-heuristic Baselines. Additionally, comparisons with meta-\nheuristic methods are included, though they are not directly com-\nparable due to computational costs. Our approach is also evaluated\nagainst an enhanced metaheuristic [55], specifically the NLSAN\npolicy, which has proven to outperform traditional JSSP meta-\nheuristics on the Taillard benchmark. For the FJSSP, we have\nincluded metaheuristic algorithms like the Improved Jaya Algo-"}, {"title": "Experimental Results", "content": "The experimental results are divided into two sections: 1) The\nfirst validates that including a reward-related term is beneficial\ncompared to an approach exclusively based on BC. 2) The second\npresents results comparing the method with various public datasets\nand state-of-the-art methods."}, {"title": "Offline Reinforcement Learning vs Behavioral Cloning", "content": "First, we aim to answer whether using offline reinforcement learn-\ning benefits policy performance. As explained in [8], generally,\noffline DRL is preferable to BC when dealing with random or\nhighly suboptimal demonstrations, because the trained policy does\nnot improve upon the demonstrations and only mimics them. How-\never, this is not the case when optimal or expert demonstrations\nare available.\nTo experimentally validate that offline RL is indeed beneficial,\npolicies have been generated based on different training set sizes\n(100, 250, 500, 750, and 1000), starting in a scenario where there\nare few instances. The goal of generating different policies with\nvarying numbers of instances is to study how they evolve in differ-\nent scenarios. For each training set, a policy has been generated\nwith three different values of lambda $\\lambda = 0,0.5, 1$, where the\nresults on the validation set are shown in Figure 4. Using a lambda\nvalue of zero is equivalent to using BC.\nSeveral conclusions can be drawn from this figure. First, for every\ntraining set, the policy trained with $\\lambda = 0.5$ yields the best median\nresults. This suggests that including a term related to the reward\nis beneficial. Secondly, this term must be controlled and does not\nimply that a higher value is better. Indeed, with only 100 instances,\nthe worst results are obtained with the policy trained with $\\lambda = 1$.\nThis may be due to the critic not having enough information to be\ntrained properly, complicating policy generation.\nLastly, another interesting fact is that the policy based on BC does\nnot improve as the number of instances increases, unlike the other\npolicies. This may be due to the limitation of the network size in\nthese types of problems. That is, in typical supervised learning\nproblems, when it is assumed that the complexity of a neural net-\nwork is not sufficient to capture the entire complexity of a dataset,\na common practice is to increase the size of the network. However,\nin this case, this is not viable as an increase in network size would\nalso increase execution time, making the solution no longer real-\ntime. Therefore, strategies for generating the policy, such as the\nuse of offline DRL, are necessary to achieve a better policy."}, {"title": "Benchmarks Results", "content": ""}, {"title": "JSSP Results", "content": "This section presents an analysis of the performance of our method\non the Taillard benchmark. Table VI summarizes the optimal gaps\nachieved by various methods, grouped into deep learning-based\napproaches and meta-heuristic methods. Each row in the table\ncorresponds to a different problem size, defined by the number of\njobs and machines. The columns represent the different methods,\nwith the first five columns dedicated to deep learning-based meth-\nods and the last column representing an advanced meta-heuristic\napproach. In the appendix B, a comparison of the execution times\nof the deep learning-based methods is made.\nAn important finding from the table is that our method consistently\nachieves a lower average gap from the optimal solution compared\nto both DRL-based approaches and the enhanced metaheuristic\n$NLSAN$. This is especially evident in larger instances, particu-\nlarly those with 30 jobs or more, where our method surpasses all\nothers. Our approach outperforms other DRL methods in almost\nevery instance group, except for the three smaller ones, where the\n$L2S$ neural improvement method performs better.\nWe speculate that the superior performance of $L2S$ and the meta-heuristic method $NLSAN$ in smaller instances, but their relatively\npoorer performance in larger ones, can be attributed to their re-\nliance on state-space exploration. As the problem size increases,\nparticularly in complex scheduling problems like JSSP, the state\nspace expands exponentially, making it increasingly challenging\nfor these methods to efficiently navigate and optimize within such\na vast space. This limitation likely diminishes their effectiveness\nas the problem size scales up.\nMoreover, it is particularly interesting that our method generalizes\nwell to larger instances, despite being trained on smaller ones.\nThis strong generalization ability indicates that our approach effec-\ntively captures the underlying structure of the problem, allowing\nit to maintain low optimal gap values even when applied to more\ncomplex and larger-scale scenarios."}, {"title": "FJSSP Results", "content": "Finally, we present the results for the FJSSP. Table 7 displays\nthe average gaps achieved by various approaches across differ-"}, {"title": "Conclusions and future work", "content": "This paper introduces a new offline RL action-value-based method\ntailored for combinatorial optimization problems, where the state is\nmodeled as a heterogeneous graph and the action space is variable.\nBy leveraging edge attributes to track actions at each step, our\napproach enhances the capability of current offline RL methods\nin handling complex problem structures. We also propose a novel\nloss function that effectively balances the goals of maximizing\nexpected rewards and imitating expert solutions. This ensures\nthat the policy is not only efficient but also grounded in proven\nstrategies, leading to better generalization.\nThe efficacy of our method has been validated through extensive\ntesting on five well-known benchmarks, including two for the JSSP\nand three for the FJSSP. Our approach consistently outperforms\nstate-of-the-art methods, demonstrating its superiority in generat-\ning high-quality solutions across different problem settings. This\nwork underscores the potential of offline RL in addressing chal-\nlenging optimization problems with complex constraints. Future\nwork could explore other offline RL methods to further enhance\nperformance or to apply this approach to other combinatorial opti-\nmization problems."}, {"title": "Acknowledgements", "content": "This work was partially financed by the Basque Government\nthrough their Elkartek program (SONETO project, ref. KK-\n2023/00038) and the Gipuzkoa Provincial Council through their"}, {"title": "Node and edge features", "content": "In this appendix, we detail the features of the nodes and edges in\nthe state representation.\nFor job-type and operation-type nodes, the features are:\n\u2022 For jobs a binary indicator $b_j \\in \\{0, 1\\}$ that indicates if\nthe job is completed and for operations a binary indicator\n$b_o \\in \\{0, 1\\}$ that indicates if the operation is ready.\n\u2022 Completion time of the last operation: Tracks job\nprogress and aids in scheduling.\n\u2022 Number of remaining operations: Indicates the remaining\nworkload.\n\u2022 Sum of average processing times of remaining operations:\nEstimates the total remaining workload.\n\u2022 Sum of average processing times of remaining operations:\nAssesses remaining workload.\n\u2022 Mean processing time: Estimates operation duration.\n\u2022 Minimum processing time: Highlights the quickest possi-\nble execution time.\n\u2022 Ratio of mean processing time to sum of remaining oper-\nations' average processing times: Aids in prioritization.\nFor machine-type nodes, the features are:\n\u2022 Last operation completion time $t_{last}$: Determines machine\navailability.\n\u2022 Utilization percentage: $\\frac{T_{used}}{T_{total}}$: Indicates machine effi-\nciency.\n\u2022 Time difference $\\Delta t = t_{last} - min_{m \\in M} t_{last}$: Compares\nmachine completion times.\nEdges in the graph are characterized as follows:\n\u2022 Undirected edges between machines and operations: Rep-\nresent machine-operation compatibility and carry features\nlike processing time.\n\u2022 Directed edges from operations to jobs: Indicate job-operation relationships.\n\u2022 Directed edges between operations: Represent precedence constraints.\n\u2022 Directed edges from machines to jobs: Connect machines\nto jobs' first pending operations and carry processing time\nfeatures.\n\u2022 Connections between jobs and machines: Represent in-terdependence and mutual influence.\nOnly two edge types carry specific features: operation-machine\nand job-machine edges. Features for operation-machine edges\ninclude:\n\u2022 Processing time $p_{o, m}$ for operation o on machine m.\n\u2022 Normalized processing time $\\frac{p_{o, m}}{|M_o|}$, where $|M_o|$ is the\nnumber of capable machines.\n\u2022 Normalized machine processing capability, $\\frac{p_{o, m}}{|O_m|}$, where\n$|O_m|$ is the number of operations machine m can process.\n\u2022 Processing time divided by the sum of average processing\ntimes of remaining operations.\nFor job-machine edges, features are analogous, focusing on the\ndelay or gap caused by machine waiting times between operations,\nleading to idle time."}, {"title": "Computation times comparison", "content": "Table 8 and 9 present the computation times for deep learning\nmethods applied to the Taillard and FJSSP benchmark problems.\nFor the JSSP, execution times were obtained from the respective\npapers for all methods, except for RLCP, where its open-source im-\nplementation was used since it did not provide detailed execution\ntimes for different instance sizes. Most methods exhibit similar\nexecution times, with the exception of SPN, which is significantly\nfaster due to its use of a simpler neural network. However, it is\nimportant to note that a precise comparison of execution times\nlargely depends on the implementation and hardware used.\nFor the FJSSP, open-source implementations of the methods were\nused (except from ResSch), but we were unable to obtain times\nfor LMLP or GGCT as they did not publish inference times or\nprovide their implementations. In this case, there are no major\ndifferences between the methods since all of them utilize similar\ntypes of neural networks and model the problem in a comparable\nway."}]}