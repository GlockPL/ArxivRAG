{"title": "Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot Task Planning", "authors": ["Xuan Zhou", "Xiang Shi", "Lele Zhang", "Chen Chen", "Hongbo Li", "Lin Ma", "Fang Deng", "Jie Chen"], "abstract": "To improve the efficiency of warehousing system and meet huge customer orders, we aim to solve the challenges of dimension disaster and dynamic properties in hyper scale multi-robot task planning (MRTP) for robotic mobile fulfillment system (RMFS). Existing research indicates that hierarchical reinforcement learning (HRL) is an effective method to reduce these challenges. Based on that, we construct an efficient multi-stage HRL-based multi-robot task planner for hyper scale MRTP in RMFS, and the planning process is represented with a special temporal graph topology. Following its temporal logic, only critical events deserve attention, so system can sample efficiently when training and hold dynamic response in execution. To ensure optimality, the planner is designed with a centralized architecture, but it also brings the challenges of scaling up and generalization that require policies to maintain performance for various unlearned scales and maps. To tackle these difficulties, we first construct a hierarchical temporal attention network (HTAN) to ensure basic ability of handling inputs with unfixed lengths, and then design multi-stage curricula for hierarchical policy learning to further improve the scaling up and generalization ability while avoiding catastrophic forgetting. Additionally, we notice that policies with hierarchical structure suffer from unfair credit assignment that is similar to that in multi-agent reinforcement learning, inspired of which, we propose a hierarchical reinforcement learning algorithm with counterfactual rollout baseline to improve learning performance. Experimental results demonstrate that our planner outperform other state-of-the-art methods on various MRTP instances in both simulated and real-world RMFS. Also, our planner can successfully scale up to hyper scale MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on unlearned maps while keeping superior performance over other methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Task planning has long been a significant area of research in multi-robot systems, exerting a crucial influence on the intelligent capabilities and operational efficiency of the cluster system [1]. The process of multiple robot task planning (MRTP) focuses on the determination of scheme both in task scheduling (TS), task allocation (TA) and task decomposition (TD) to accomplish a particular objective [2]. Specifically, TS aims to determine the sequence of tasks assigned to each robot [3]. TA involves assigning specific tasks to either an individual robot or a group of robots [4]. TD focuses on solving complex tasks by breaking down the overall task into simpler and more manageable sub-tasks [5].\nIn this article, we focus on a specific MRTP problem abstracting from the robot mobile fulfillment system (RMFS) [6], which is a typical multiple robot system built to enhance warehouse operational efficiency [7]. In RMFS, the retrieval racks required by the top-level order will be delivered by mobile robots to their required picking stations, in which case, pickers don't need to walk to the rack locations to pick up cargoes any more, and this will greatly save labor cost and improve the picking efficiency. A description of the operational process for RMFS is shown in Fig. 1, where multiple mobile robots, movable racks, unoccupied storage locations, and picking stations are scattered in layout and they constitute the elements of this system. Initially, the mobile robot in orange color moves from its home location to its designated retrieval rack location (step I in Fig. 1). Then it proceeds to lift and transport the corresponding racks to its assigned picking station (step II). Subsequently, after the staff members retrieve the required cargoes at the picking station, the mobile robot relocates the completed rack to the allocated unoccupied storage location (step III). If there still exist retrieval racks remaining, the mobile robot will move towards a newly assigned rack (step IV) and repeats steps II to IV cyclically. On the contrary, if all retrieved racks have been picked up already, then the mobile robot will return home for recharging (step V), and the entire operational process in RMFS comes to an end.\nThe challenges in developing an ideal task planner for hyper scale MRTP in RMFS are evident in the following aspects:\n(1) Dimension disaster: the policies for TA and TS in MRTP are coupled and the potential computing time grows expo-nentially with the scale of elements in the RMFS involving robots, retrieval racks, picking stations and unoccupied storage locations, which makes traditional task planners difficult to generate a satisfactory solution within effective time.\n(2) Dynamic properties: the set of tasks assigned by the top-level order are temporal varying and robots will alter the features of system elements while executing tasks. Besides, the valid elements that can be selected by a robot are also dynamic because robots must obey specific task execution sequences. These dynamic properties augment planning complexity and impose higher requirements on dynamic response.\nThese challenges impede our goal of achieving higher quality and faster planning speed for various MRTP in RMFS. Some significant related studies have made important contri-butions to reduce these challenges, the most popular studies mainly involve heuristic methods [8]-[12] and deep rein-forcement learning (DRL) methods [13]-[15]. Between them, heuristic methods rely on manual design and their performance are usually limited by the experience of designers. In contrast, DRL methods can continuously improve their performance through self-exploration, and solve efficiently considering the strong nonlinear fitting capability of deep networks [16]. Hierarchical reinforcement learning (HRL) [17], as a branch of DRL, is considered to have a natural advantage in reducing dimensional disasters for complex tasks while inheriting good dynamic adaptability from DRL. AlphaStar [18] outperformed grandmaster human players in the extremely complex game of StarCraft via HRL. LEE et al. provided an efficient navigation and locomotion method [19] for wheeled-legged robot in complex environments through HRL. These significant works indicate that HRL has the potential to address the challenging MRTP concerned in this article.\nHowever, existing related works still face challenges of scaling up and generalization when dealing with hyper scale MRTP in RMFS. On the one hand, the MRTP in RMFS here is more dependent on global information and optimality, so we choose centralized framework instead of decentralized framework [9], [15], and scalability becomes a key challenge beacuse centralized training directly on hyper scale MRTP instances in RMFS with hundreds of robots is still impractical, although HRL can reduce dimension disaster to some extent by decomposing task space. On the other hand, it is impossible for policies to learn all possible scales and map layouts during training stage, thus excellent generalization performance is indispensable. In order to overcome all the challenges men-tioned, we construct an efficient multi-stage HRL-based multi-robot task planner, and the main contributions are as follows:\n(1) We model the problem of MRTP in RMFS as a Markov decision process (MDP) with options on asynchronous multi-robot temporal graph with cycle constraints (C2AMRTG), where C2AMRTG is a general extended concept based on temporal graph to abstract the special law of the system.\n(2) We construct a centralized hierarchical temporal multi-robot task planning framework following the MDP on C2AMRTG to provide an efficient information updating and interaction mechanism for training and execution. The cen-tralized design can ensure optimality, and the hierarchical architecture can reduce the dimension in action space.\n(3) We design a hierarchical temporal graph neural net-work (HTAN) with special temporal embedding layers based on C2AMRTG to enhance spatio-temporal feature extracting performance. It allows basic scaling up ability and can obey varying cycle arc constraints in action space.\n(4) We propose a HRL algorithm with counterfactual rollout baseline (HCR-REINFORCE) to reduce unfair credit assign-ment between different policy layers under the hierarchical framework, and use a joint optimization learning technique by combining BC loss with DRL loss to accelerate the convergence speed of learning in the early training stage.\n(5) We design a multi-stage curriculum learning method HCR2C by gradually expanding the random boundary of training instances for HCR-REINFORCE to further improve the scaling up and generalization performance while avoiding catastrophic forgetting, and the trained policy can adapt well on various random hyper scale instances and unlearned maps. After curriculum learning, the policy in our planner can scale up and outperform other heuristic and DRL methods on hyper scale random simulated instances and unlearned maps with up to 200 robots, 1000 retrieval racks and 2000 unoccupied storage racks. Also, our planner successfully outperforms the native company planner on real-world RMFS instances in both planning quality and speed indicators.\nOrganizational structure: The organizational structure of the rest of this article is as follows. Section II reviews the related works of MRTP. Section III gives the general concept of C2AMRTG. Section IV establishes the model of MDP with options on C2AMRTG. Section V introduces the centralized hierarchical temporal multiple robot task planning framework. Section VI introduces the network HTAN. In Section VII, the training methods are illustrated in detail. In Section VIII, the effectiveness of our methods are evaluated through comparison experiments. Finally, Section IX sums up this paper."}, {"title": "II. RELATED WORKS", "content": "Many state-of-the-art works have been done on the sub-problems of MRTP. In [20], the TS problem for multiple cooperative robot manipulators is studied with a priority con-straint optimization model. In [21], TA is efficiently addressed in a decentralized fashion across multiple agents with the nearest neighbor search method. In [22], both TA and TD are considered to address the sequential sub-task dependencies in decomposable tasks. In [23], all of the three sub-problems are simultaneously optimized with flexible task decomposing ways. The aforementioned studies either ignore system dy-namics, or fail to scale up. To further improve adaptability, [24] focused on exploring a dynamic and scalable framework for TA with distributed structure to adjust planning strategies according to the latest real-time information. However, there is still a lack of comprehensive research integrally optimizes the complete MRTP problem involving TA, TS and TD in a dynamic and scalable framework, which is a common challenge in many real-world application scenarios."}, {"title": "B. Multi-Robot Task Planning in RMFS", "content": "Due to the complexity of MRTP in RMFS [25], most of the research has employed various simplifications to reduce solution difficulty. Some researches modeled the simplied MRTP in RMFS as classic scheduling and routing problems or their variants. Ham et al. [3] transformed the problem into a standard job-shop scheduling problem. Gharehgozli et al. [12] modeled the problem as an asymmetric traveling salesman problem with priority constraints [26] considering the intricate rack restorage scenario. [9] modeled the problem as a pickup-and-delivery vehicle routing problem. Some research trans-formed the complete MRTP to partial sub-problem in MRTP based on specific assumptions. Shi et al. [27] considered the dynamic property but focused solely on TA in RMFS and utilized rules to streamline the planning process. Others [13], [28] assumed that once rack is brought to a picking station, the robot must deliver it to their original locations without considering the restorage of racks to avoid dealing with TD and dynamic issues. Lian et al. [29] addressed the dynamic TS in RMFS with a hierarchical planning framework, but considered only two mobile robots. These existing methods rely on many assumptions and simplifications, which will inevitably lead to the decreased task completion efficiency in RMFS. Different from the above works, Zhuang et al. [10] considered the requirement of rack restorage and delved into both TA and TS in RMFS, but the scale considered only involves 10 robots at most."}, {"title": "C. Solving Multi-Robot Task Planning in RMFS", "content": "The solving time grows exponentially with the scale of problem, considering even the simplified problem of MRTP in RMFS has been proved to be NP-hard or NP-complete [4]. When scale is not very large, the simplified problem of MRTP in RMFS can be solved precisely with existing mathematical tools [3], such as Gurobi, CPLEX, and SCIP [30]. But for large and hyper scale MRTP in RMFS, it is not realistic to use precise methods to solve on time because they fail to generate a satisfactory solution in a reasonable amount of time. So many heuristic methods with approximate solution are proposed to accelerate the process of solving. In [9], heuristic rules together with linear programming method is designed to solve the generalized pickup-and-delivery vehicle routing problem. Gharehgozli et al. [12] developed an adaptive large neighborhood search heuristic method to solve efficiently. Ha et al. [13] used a genetic algorithm with collision evaluation for TS. In [10], a heuristic decomposition method within a rolling horizon framework is proposed for MRTP in RMFS. Although these heuristic-based methods can greatly shorten solution time compared to precise methods, they require labor cost to design operators for specific scenarios and still takes a lot of time to achieve a solution with satisfactory quality.\nIn recent years, there have been many successful applica-tions of deep reinforcement learning that have demonstrated superior performance to human capabilities, extending from virtual gaming environments to real-world physical domains [18], [31]. Unlike heuristic-based methods, they do not require continuous search or iterative processes during the reasoning stage, and are capable of providing direct solutions with rapid solving speeds [16]. Furthermore, DRL does not depend on labels and has the potential to yield superior solutions beyond expectations. Many researchers [32]-[34] have ob-served these advantages of DRL. [32] provided the method of REINFORCE [35] with rollout baseline (R-REINFORCE) to improve solving efficiency for single-agent classical planning problems. Radosavovic et al. [36] proposed a joint optimiza-tion reinforcement learning method JPPO based on PPO [37] and supervision learning to improve learning efficiency for complex and highly dynamic location tasks. In work [28] and [29], DRL with heuristic rules is used for TA to reduce system costs. Ho et al. [38] adopted a distributed DRL method to realize scaling up. However, neither of these DRL-based works can solve the generalization challenge on unlearned maps well and successfully verify their methods on hyper scale instances with more than 100 robots and 1000 retrieval racks in RMFS. In conclusion, there is still a lack of sufficient solving methods for MRTP in RMFS that simultaneously consider scalability, generalization, hyper scale and dynamic challenges. Further works are required to improve planning efficiency for intricate real-world hyper scale MRTP in RMFS."}, {"title": "III. ASYNCHRONOUS MULTI-ROBOT TEMPORAL GRAPH WITH CYCLE CONSTRAINTS", "content": "In this section, we introduce the general concept of asyn-chronous multiple robot temporal graph with cycle constraints (C2AMRTG) to facilitate the description of model, framework and methods in the subsequent sections.\nA C2AMRTG consists of a global graph with temporal node features and several robot temporal digraphs. This kind of graph mainly has three kinds of properties: (1) the node information of robot temporal digraphs are updated from the latest global graph asynchronously; (2) the features in global graph and the robot temporal digraphs are time-varying; (3) the set of arcs in robot temporal digraphs have special cycle constraints. The definition for C2AMRTG is given as follows.\nDefine the global graph with temporal node features as Gg = (Vg, Ag), where Vg = {v1, v2, . . ., vi, . . ., vNw} is the set of nodes, Ag is the set of global arcs, and the global graph is fully connected. Denote the temporal feature vector at the timestamp of t for a node vi \u2208 Vg, as fvt = (px (vi, t), py (vi, t), \u2161(vi, t), \u03c6(vi, t)), where px (vi, t) and py (vi, t) are respectively the position of node vi at the timestamp of t on the x-axis and y-axis. \u2161(vi, t) is an instructor function with \u2161(vi, t) = 1 if the node vi exists at the timestamp of t, and \u2161(vi, t) = 0 otherwise. Assuming that the set of nodes in global graph Gg can be further split into Ng + 1 number of subsets Vg = \u222aNj=0Vj where s is the type ID of node. Then \u03c6(vt) \u2208 {0, 1, . . ., Ng} is the type ID of the node vi at the timestamp of tf, which means vt \u2208 V\u03c6(vt). Let \u03b5f be a global updating event at the timestamp of t = tf. When a global updating event happens, Gg will be updated. Assuming that event \u03b5f can be completed instantaneously with no time consumption. Events that hap-pened at different time constitute a sequence in accordance with time: \u03b51, \u03b52, . . ., \u03b5s, . . ., \u03b5T, and all the event elements form a set of global updating event Eg.\nAdditionally, each mobile robot owns a robot temporal digraph. Let Grl = (Vrl, Arl) be the robot temporal di-graph for the lth robot, where Vrl = {(v, tu, te)} and Arl = {((u, v), ta, tb)} are specifically the set of temporal nodes and set of temporal arcs for Grl. The node v exists from the starting timestamp of tu to the ending timestamp of te with tu < te and tu, te \u2208 R\u22650. The directed arc (u, v) exists from the starting timestamp of ta to the ending timestamp of tb with u, v \u2208 Vrl, ta < tb and ta, tb \u2208 R\u22650. Let Er be the set of robot graph updating events that will cause Grl to change, and the event elements form a se-quence \u03b5r1, \u03b5r2, . . . , \u03b5rk, . . . , \u03b5rTr, where \u03b5rk is the kth event that triggers the graph Grl to change. Once a robot graph updating events \u03b5rk happens, the lth robot will update its robot temporal digraph by pulling the graph node information from the global graph, and we record the updating timestamp as th. If we take a shot of the robot temporal digraph Grl at each timestamp tk, tk1, . . . , tkr, . . . , tkTrl, we can obtain a set of robot temporal digraph in the form of discrete time Grl = {Grl1, Grl2, . . . , Grlk, . . . , GrlTrl}, where Grlk = (Vrlk, Arlk) is a robot digraph shot. Vrlk = {vr1k, vr2k, . . . , vrvk, . . . , vrvk} and Arlk = {((u, v)ri,k) | u, v \u2208 Vrlk} are respectively the set of nodes and the set of arcs in discrete time for the lth robot. At the timestamp of th, both the set of nodes Vrlk and the node feature vector fik for the lth robot are directly up-dated from the current global graph Gg, which means vrk = vi, fik = (px(vi, tk), py (vi, tk), \u2161(vi, tk), \u03c6(vi, tk)), and fik = fvi . To be more specific, px(vi, tk) = px(vt), py(vi, tk) = py(vt), \u2161(vi, tk) = \u2161(vt), and \u03c6(vi, tk) = \u03c6(vt), where t is the latest global graph updating time before tk.\nLet Hl = (hl1, hl2, . . ., hlk, . . ., hlk) be the historical node ID sequence for the lth robot before tk, where hlk \u2208 N is the ID of the last node that was assigned for the lth robot before the timestamp of tk. It is obviously that the set of nodes in Grl can also be further split into Ng + 1 number of subsets Vrlk = \u222aNj=0Vrlj,k. Let \u047brl := (1 2 ... Nr1 ) be the cycle [39] of the lth robot to represent the cyclic visiting sequence that the lth robot has to follow. It is essentially a mapping from set {1, 2, ..., Nr1} consisting of Nr1 number of subset ID\nelement to itself \u047brl : j \u2194 \u047brl(j), and two cycles obtained by shifting are same.\nWe use an adjacency matrix Mrl whose element is mrij,k \u2208 {0, 1} | Arlk to represent the connectivity relationship starting from node vrk \u2208 Vrlk to node vrk \u2208 Vrlk for Arlk. If mrij,k = 1, the lth robot can travel from the node vrk to the node vrk . Otherwise, the lth robot cannot travel from the node vrk to the node vrk . If a historical node ID sequence Hl is given and k > Nr1, then for any arc (u, v) \u2208 Arlk at the timestamp of tk, it must follow the robot arc constraints as follows:\n(1) \u2161(vti , tk ) = 1, and \u2161(vti , tk+1 ) = 1 (node existence constraint);\n(2) hlk = i, and hlk+1 = j (spatiotemporal consistency constraint);\n(3) \u03c6(vti, tk+1 ) = \u03c6(vtl ,tk) if 1 \u2264 k < Trl (cycle constraint);\nhlk+1 \nhlk\n(4) \u03c6(vtTi, tk ) = 0, \u03c6(vt1 , tk ) = 1 and \u03c6(vtTi ) = 0 (terminal constraint).\nIf the arc (u, v) satisfies all of the constraints above, then mrij,k = 1, and mrij,k = 0 otherwise. A practical physical explanation for these constraints is the robot must start from the initial node and circularly visit the nodes in relative type node sets in a fixed cyclic sequence. Once the robot completes the required task, it must return to its initial node."}, {"title": "IV. MODEL FORMULATION", "content": "In this section, we assign specific physical meanings to variables in C2AMRTG and introduce the MDP model with options on C2AMRTG to describe MRTP in RMFS.\nTo facilitate the analysis and solution of the problem, we specify the following assumptions within a reasonable range:\n(1) Each robot can only carry one rack at a time.\n(2) Each unoccupied storage location can only be used to store just one rack.\n(3) The traveling time consumed only corresponds to the Manhattan distance traveled and average robot moving speed.\n(4) The time of picking, queuing, lifting up and putting down a rack for robots can be ignored.\nThe C2AMRTG has specific parameters and physical meanings for representing problem of MRTP in RMFS with these assumptions. In the global graph Gg, we can split the set of nodes Vg into four subsets with Ng = 3, which can be written as Vg = \u222a3j=0Vj , where V0 = {v1, v2, . . ., vNa } is the set of robot home nodes, and vi is the robot home node for the ith robots with i \u2208 {1, 2, . . ., Na}. V1 = {vNa+1, vNa+2, . . ., vNa+Nr } is the set of retrieval rack nodes, V2 = {vNa+Nr+1, vNa+Nr+2, . . ., vNa+Nr+Np } is the set of picking station nodes, and V3 = {vNa+Nr+Np+1, vNa+Nr+Np+2, . . ., vNa+Nr+Np+Ns } is the set of unoccupied storage location nodes. The total number of nodes in the global graph is N\u03c9 = Na + Nr + Np + Ns. Each node in the set of Vg just corresponds to a possible delivery destination location. The type ID property of the node in the global graph at the initial moment is defined as:\n\u03c6(vt0 ) =\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\n0, if 1 \u2264 i \u2264 Na;\n1, if Na< i < Na + Nr;\n2, if Na + Nr < i < Na + Nr + Np;\n3, if Na + Nr + Np \u2264 i \u2264 N\u03c9.\n(1)\nAll nodes exist at the initial moment with \u2161(vi, t0) = 1. The whole task in RMFS can be broken down into a series of delivery sub-tasks and each delivery sub-task just corresponds to a node in Vg according to its terminal location. When a mobile robot in the system is assigned with a node or the assigned robot just completed a delivery sub-task, the global updating event \u03b5 \u2208 Eg happens, and the global graph will be updated. The global updating event includes three kinds of cases: (1) If a robot is assigned with a node involving delivering a picked rack to the allocated unoccupied storage location, the related unoccupied storage location node will update its existence instructor function with \u2161(vi, t) = 0. (2) If a retrieval rack is assigned to a robot, the related retrieval rack node will update its existence instructor function with \u2161(vi, t) = 0 to prevent repeated rack assignment. (3) If the robot arrives at the location of the assigned retrieval rack, the related retrieval rack node will update its type ID with \u03c6(vi, t) = 3, and the related retrieval rack node will update its existence instructor function with \u2161(vi, t) = 1.\nLet Er = {\u03b5r1, \u03b5r2, . . . , \u03b5rk, . . . , \u03b5rTr } be the set of robot selecting decision events. A robot selecting decision event \u03b5rk \u2208 Er for MRTP will be triggered when there exists a robot that has no uncompleted delivery sub-task any more. Assuming that the lth robot is selected to assign a node, then the robot graph updating event \u03b5rk \u2208 Er will be triggered and the node features in Grl are updated directly from the Gg at tj. Let Hl = (hg, hg, . . ., hgk) be the history sequence of selected robot IDs until tj. The related discrete robot digraph shot is Grl = (Vrlk, Arlk). The cycle constraint in Grl represents the visiting sequence as follows: (a) firstly, the robot should visit a retrieval rack node; (b) then it should visit a picking station node; (c) finally, it should visit a unoccupied storage location\nnode. So Ng = 3, \u047brl := (1 2 3 ) and any arc in the robot digraph shot should follow all the four kinds of robot arc constraints in C2AMRTG. Except for these default constraints, the problem should also obey a special constraint that the retrieval rack should be delivered to its related picking station, which can be written as hlk+1 = f(hlk) if \u03c6(vi, t) = 2, and f(\u00b7) is a mapping from the set of retrieval rack node ID to the set of picking station node ID.\nBased on the specific C2AMRTG defined above, the op-timization objective of MRTP problem is to minimize the makespan until all the delivery sub-tasks are completed. Here, makespan is the total time consuming until the task termination condition is established, and the task termination condition for the RMFS system is defined as: all the retrieval racks are delivered by robots to the required picking stations and put to an unoccupied storage location, and all the robots go back to their home locations for recharging."}, {"title": "A. MDP with Options on C2AMRTG", "content": "According to these definitions, the MRTP problem in RMFS can be modeled as MDP with options on C2AMRTG based on the work of [40] by augmenting MDP with a predetermined set of options to facilitate the utilization of reinforcement learning methodology. The model of MDP with options can be described by a tuple (S, O, P, R, F), where S is the set of all possible states, which includes Gg, Grl, H-1 and Hl. O is the set of options. P : S \u00d7 O \u00d7 S \u2192 [0, 1] is the set of state and option transition probability function and it follows the updating rules of C2AMRTG for MRTP in RMFS defined above. R is an instant reward function. F : S \u00d7 O \u00d7 S \u2192 (R\u22650 \u2192 [0, 1]) is a continuous probability distribution function giving probability of transition times for each state-option pair, it is determined by the policy and termination condition defined in options.\nOption can be viewed as the generalization of primitive action with time extensibility. In the set of options O, each option element o can be described by (\u0399, \u03c0, \u03b2), where I \u2282 S is an initiation state set and \u03c0 : S\u03bf \u00d7 A\u03bf \u2194 [0, 1] is the policy. S\u03bf \u2282 S is the state set of O and A\u03bf \u2282 A is the action set of O. \u03b2 : S+ \u2194 [0, 1] is the termination condition, indicating the option will terminate at each time step following the probability of \u03b2 after an execution from the initial state based on policy \u03c0. S+ is the set of states including termination state, and we define the termination state set for O as S+. Considering the whole MRTP in RMFS consists of three sub-problems including TA, TS and TD, we break the set of options O into two parts: O = Or \u222a Og, where Or is the set of robot selecting option and it is related to TA. Og is the set of graph node selecting option and it is related to TS. Furthermore, we can split the node location option set Og into disjoint subsets Og = \u222a3j=0Ogj according to the decomposition mode in Vrl, and this process is related to TD. Among them, O0g is the set of robot home option related to selecting the required robot home node, O1g is the set of retrieval rack options related to selecting a rack node, O2g is the set of picking station options related to selecting a picking station node, and O3g is the set of unoccupied storage options related to selecting an unoccupied storage location node. Considering the two kinds of options are generated in succession, we define a joint option \u2126m = (\u03c9rm, \u03c9gm) to represent the sequential execution of robot option and graph node option at the mth joint option decision step, where \u03c9rm \u2208 Or is the robot option at the mth robot option decision step and \u03c9gm \u2208 Og is the graph node option at the mth graph node option decision step. The application method of joint option \u2126m is similar to the action in traditional Markov decision process framework, but we should notice that the duration of action is fixed. The detailed definitions for each option set are explained as follows."}, {"title": "B. Robot Selecting Option", "content": "In the set of robot selecting options Or = (Ir, \u03c0r, \u03b2r), the initial state set Ir \u2282 Sr encompasses all states sm when an event \u03b5rk \u2208 ER happens. Specifically, sm consists of the node features fik of global graph snapshots Gg when an event \u03b5rk \u2208 Er has just happened before \u03b5m happened. Policy \u03c0r : Sr \u00d7 Ar \u2192 [0, 1] outputs the probability distribution of robot selecting action through state Sr, where Sr is composed of Gg and H-1. The robot selecting action set Ar includes the IDs of all the robots that have not completed their delivery sub-tasks and returned home. \u03b2r is the robot selecting option termination condition. Let Str be the termination state set of Or, then \u03b2r(Str ) = 1 means that option ends up immediately when the system completes an action of selecting a robot."}, {"title": "C. Graph Node Selecting Option", "content": "For the set of graph node selecting options Og =(Ig, \u03c0g, \u03b2g), the initial state set Ig \u2282 Sg encompasses all states sm when \u03b5rk \u2208 Er is triggered, and this means it is the turn for system to select a graph node which is the goal location for the latest robot l = hlk\u22121 assigned before tj = tk. Policy \u03c0g : Sg \u00d7 Ag \u2192 [0, 1] outputs the probability distribution of the graph node selecting action through state Sg, where Sg is the set of graph node states. Specifically, it consists of Grl, Hl\u22121, and H-1 until tj = tk. Because the set Ig, Ag and the node selecting option termination condition \u03b2g differs for different subsets O0g , O1g , O2g , O3g in Og, we explain each type of graph node selecting option in further detail:\n1) Robot Home Option O0g : If I0g is the initial set of states in O0g , encompassing all states when an event in Erhappens. If the retrieval racks have all been carried to the related picking stations and the robot rl has delivered its last rack to an unoccupied storage location, the selected lth robot should return to its home location (step V in Fig. 1), and we denote the set of these events as E0r . Home action set A0g consists of only one action: selecting its home node vrl. \u03b20g is the termination condition. Let St0g be the termination state set of O0g , then \u03b20g (St0g ) = 1 means option ends up immediately when robot rl arrives at location of vrl or an event \u03b5rk \u2208 Er happens.\n2) Retrieval Rack Option O1g : The initial set of states in O1g , denoted as I1g , encompasses all states when it is the turn for the lth robot to make a decision of selecting a retrieval rack (step I and IV in Fig. 1). Retrieval rack action set A1g consists of actions of selecting a valid retrieval rack node vt \u2208 Vrl with the feature \u03c6(vti, tk ) = 1 and mrij,k = 1. Let St1g be the termination state set of O1g , then \u03b21g (St1g ) = 1 means option ends up immediately when the system completes an action of selecting a retrieval rack node or an event \u03b5rk \u2208 Er happens.\n3) Picking Station Option O2g : The initial set of states in O2g , denoted as I2g , encompasses all states when it is the turn for the lth robot to make a decision of selecting a picking station (step II in Fig. 1). Picking station action set A2g consists of the actions of selecting a valid picking station node vt \u2208 V2 with features of \u03c6(vti, tk ) = 2 according to the latest retrieval rack that has been assigned for the lth robot, which means z = f(hlk), and z is the current valid picking station node ID for the lth robot. Let St2g be the termination state set of O2g , then \u03b22g (St2g ) = 1 means that option ends up immediately when the assigned robot arrives at the location of selected picking station or an event \u03b5rk \u2208 Er happens.\n4) Unoccupied Storage Option O3g : The initial set of states in O3g , denoted as I3g , encompasses all states when it is the turn for the lth robot to make a decision of selecting an unoccupied storage location (step III in Fig. 1). Unoccupied storage action set A3g consists of actions of selecting an unoccupied storage location node vt with features of \u03c6(vti, tk ) = 3 and mrij,k = 1 from Vrl. Let St3g be the termination state set of O3g , then \u03b23g (St3g ) = 1 means that option ends up immediately when the lth robot arrives at the location of assigned unoccupied storage location or an event \u03b5rk \u2208 ER happens."}, {"title": "D. Reward", "content": "To calculate the instant reward", "2)": "n\u03c4kl =\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n\u0393l\nmax(0,h1) , if k = 0;\nr0,l + vr\u2217 (px(vtk+1) \u2212px(vt ) + py(vtk+1) \u2212py(vto ))\nrk\u22121,l + vr\u2217 (px(vtk+1 ) \u2212px(vt k )) + py(vtk+1 ) \u2212py(vt )),\nmax(0,h k+1)max(0,h)\nmax(0,h k+1)\nmax(0,h)\nmax(0,h)) , if k > 0, and \u03b5rl \u0338\u2208 Erl ;\nrk\u22121,l + vr\u2217 (px(vtk+1 ) \u2212px(vt )) + py(vtk+1 ) \u2212py(vtk )); if k > 0 and \u03b5rl \u2208 Erl .\nmax(0,h k+1)max(0,h)\nmax(0,h k+1)max(0,h)\nmax(0,h)\n\u0393"}]}