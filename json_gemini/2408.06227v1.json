{"title": "FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks", "authors": ["Min Ma", "Yuma Koizumi", "Shigeki Karita", "Heiga Zen", "Jason Riesa", "Haruko Ishikawa", "Michiel Bacchiani"], "abstract": "This paper introduces FLEURS-R, a speech restoration applied version of the Few-shot Learning Evaluation of Universal Repre-sentations of Speech (FLEURS) corpus. FLEURS-R maintains an N-way parallel speech corpus in 102 languages as FLEURS, with improved audio quality and fidelity by applying the speech restoration model Miipher. The aim of FLEURS-R is to advance speech technology in more languages and catalyze research in-cluding text-to-speech (TTS) and other speech generation tasks in low-resource languages. Comprehensive evaluations with the restored speech and TTS baseline models trained from the new corpus show that the new corpus obtained significantly improved speech quality while maintaining the semantic contents of the speech. The corpus is publicly released via Hugging Face.\nIndex Terms: Multilingual speech corpus, speech generative models, speech restoration, text-to-speech.", "sections": [{"title": "1. Introduction", "content": "There have been rapid development in the speech generation area in the past few years. Models such as denoising diffusion probabilistic models (DDPMs) [1,2], neural audio codec [3, 4], and large language models (LLMs) [5-7] have been successfully applied to speech generation tasks. As these speech generation tasks can be viewed as a sequence-to-sequence generative task, the progress in generative models in different areas can be in-troduced to improve the performance. Now naturally sounding synthetic speech with an arbitrary speaker's voice can be syn-thesized with a small amount of speech data [8]. There are models which support controlling their speaking styles and/or voice characteristics via natural language-based prompts [9, 10].\nAlthough there have been great advancements in the mod-eling side, the progress in the data side is relatively slow. As these new generative models are language agnostic and can be pre-trained from a large quantity of speech-only or text-only data (e.g., 100k hours) [5, 11], the required amount of speech-text paired data is getting smaller [7]. This nature is highly relevant for developing multilingual speech generation models, especially for low-resource languages.\nThe FLEURS [12] corpus covers 102 languages, which spans over 17 language families and 27 unique writing sys-tems. It was designed to enable speech technology in more languages and catalyze research in low-resource speech under-standing. However, as all recordings are kept as they-are, either from quiet or noisy environment, making it less ideal for speech generation tasks, where models are requested to produce high-quality speech.\nRecently, Koizumi et al. introduced LibriTTS-R [13], which is a speech restoration applied version of the LibriTTS cor-pus [14]. As it offers speech signals in higher sampling rate, less noise, and less reverberation, neural end-to-end TTS models trained with LibriTTS-R achieved better subjective naturalness than those with the original LibriTTS corpus [13].\nThis paper introduces FLEURS-R, a speech-restoration ap-plied version of the FLEURS corpus. It keeps the same proper-ties as the original FLEURS corpus with improved audio qual-ity, i.e., less noise and reverberation with higher sampling rate (24 kHz). Table 1 compares FLEURS-R with existing com-mon public multilingual TTS corpora. The key properties of FLEURS-R that are:\n\u2022 Containing N-way parallel speech and text in 102 languages; the improved speech quality makes it a better choice for speech generation tasks, including TTS, speech-to-speech translation (S2ST) and voice conversion (VC).\n\u2022 Highly multilingual (102 languages) where 80% languages are low-resource. It helps catalyze speech generation research in multilingual, cross-lingual and low-resource settings."}, {"title": "2. Speech Restoration Pipeline", "content": "We restored the FLEURS speech samples using the same methodology employed to create the LibriTTS-R corpus [13]. LibriTTS-R was created by applying a speech restoration model Miipher [19] to the LibriTTS corpus [14]. Miipher extracts acoustic features from noisy speech using w2v-BERT [20]. The system then employs DF-Conformer [21] to convert these noisy acoustic features into clean ones while using speaker and text conditioning features extracted by speaker-encoder [22] and PnG-BERT [23]. Finally, the WaveFit [24] neural vocoder gen-erates a clean speech waveform from the predicted features.\nSince FLEURS is a multilingual corpus and Miipher sup-ports only English [19], we made several updates to the Miipher model structure to accommodate this difference. First, we re-placed the acoustic feature extractor from w2v-BERT [20] to the Universal Speech Model (USM) [25]. Unlike w2v-BERT [20], which was trained on English speech samples, the USM was pre-trained on a massive dataset of 12 million hours of speech spanning over 300 languages. We used a non-fine-tuned USM encoder to preserve the speaker's acoustic characteristics in the extracted features. Specifically, we used the 2-billion parame-ter \"pre-trained\" model [25]. In self-supervised learning (SSL) speech feature extraction, it is known that deeper layers tend to lose detailed and local acoustic information [26]; therefore, we used the intermediate feature from the 13th of 32th layers based on preliminary experiments."}, {"title": "2.1. Speech Restoration Model", "content": "We restored the FLEURS speech samples using the same methodology employed to create the LibriTTS-R corpus [13]. LibriTTS-R was created by applying a speech restoration model Miipher [19] to the LibriTTS corpus [14]. Miipher extracts acoustic features from noisy speech using w2v-BERT [20]. The system then employs DF-Conformer [21] to convert these noisy acoustic features into clean ones while using speaker and text conditioning features extracted by speaker-encoder [22] and PnG-BERT [23]. Finally, the WaveFit [24] neural vocoder gen-erates a clean speech waveform from the predicted features.\nSince FLEURS is a multilingual corpus and Miipher sup-ports only English [19], we made several updates to the Miipher model structure to accommodate this difference. First, we re-placed the acoustic feature extractor from w2v-BERT [20] to the Universal Speech Model (USM) [25]. Unlike w2v-BERT [20], which was trained on English speech samples, the USM was pre-trained on a massive dataset of 12 million hours of speech spanning over 300 languages. We used a non-fine-tuned USM encoder to preserve the speaker's acoustic characteristics in the extracted features. Specifically, we used the 2-billion parame-ter \"pre-trained\" model [25]. In self-supervised learning (SSL) speech feature extraction, it is known that deeper layers tend to lose detailed and local acoustic information [26]; therefore, we used the intermediate feature from the 13th of 32th layers based on preliminary experiments."}, {"title": "2.2. Data Processing Pipeline", "content": "First we applied the new Miipher model-based speech restoration to the complete set in the original FLEURS corpus. Thanks to Miipher's audio super-resolution capability, the sampling rate of speech samples in the FLEURS-R samples was increased from 16 kHz to 24 kHz. Note that FLEURS-R maintains the same constituent samples as the original FLEURS corpus except the audio quality.\nDue to possible errors caused in the Miipher speech restora-tion process, some restored samples may exhibit signal pro-cessing artifacts. To identify successfully restored samples, we performed ASR-based filtering. The list of the rejected samples will also be published. Note that all experiments in Sec. 3 including TTS model training were conducted with the rejected samples."}, {"title": "3. Evaluations", "content": "We conducted ASR-based intelligibility evaluations, automatic subjective naturalness evaluations, and TTS model training ex-periments with the new FLEURS-R corpus. Some demo sam-ples from each experiment are available as a supplementary material to honor double-blind review."}, {"title": "3.1. ASR Evaluation", "content": "To validate the consistency of semantic contents between origi-nal FLEURS and new FLEURS-R corpora, we conducted ASR evaluations over all 102 languages. We used the Maestro-U [27] grapheme ASR model which performed reasonably well in terms of character error rates (CERs) in most of these 102 languages.\nTable 2 shows the language-specific CERs.The abbrevia-tions in the first row denote regions; Western European (WE), Eastern European (EE), Central-Asia, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South-Asia (SA), South-East Asia (SEA), and Chinese, Japanese and Korean (CJK). Please refer to [12] for the individual locale codes. It can be seen from the table that the average CERs over all locales for FLEURS and FLEURS-R were approximately equal (9.67% and 9.74%). This suggests the speech restoration process maintained the semantic contents in the original speech in most languages.\n32% languages got improved CERs, especially in Xhosa (xh), Umbundu (umb), Macedonian (mk), Tamil (ta), Turkish (tr), Hebrew (he) and Armenian (hy). The gains mostly come from the reduced substitution error rates, likely because enhanced speech quality makes the acoustically similar words more distinguish-able. Other locales maintained or only saw small regressions in CERs. The exceptional locales were Nepali (ne), Punjabi (pa), Indonesian (id), Latvian (lv), and Czech (cs). Such degrada-tion were from higher substitution and deletion error rates. On most locales, insertion error rates were generally reduced, indi-cating that Miipher reduced the environment noises of speech recordings. Three locales, Sorani-Kurdish (ckb), Sindhi (sd) and Cantonese (yue) observed high CERs, though their ASR baseline CERs on FLEURS are already high. We provide sam-ples from the two groups (most improved and most regressed) in the supplementary materials."}, {"title": "3.2. Speech Naturalness Evaluation", "content": "While the subjective 5-point Mean Opinion Score (MOS) is a standard evaluation method to assess the naturalness, it poses challenges to evaluate the FLEURS-R corpus. As this corpus has large linguistic variations, contains 102 languages, and 80% of these languages are low-resource, it is difficult to conduct-ing large-scale subjective evaluations. Therefore, we utilized the SQuId (Speech Quality Identifier) model [28], which was trained to predict a 5-scale subjective MOS in naturalness given an audio. It is known that SQuId doesn't map perfectly to sub-jective MOS and is less sensitive to linguistic correctness since the model has largely seen ratings for high-quality TTS samples (ranging between 3.0 and 5.0). However, it is still useful for rel-ative comparisons between samples in the same language [29].\nTable 3 shows that SQuId MOS in FLEURS-R were gen-erally higher than FLEURS across all languages. On average, FLEURS-R had a 0.2 point improvement over FLEURS in the SQuId MOS. Although improvements in the SQuId MOS were observed in almost all languages except Irish (ga). Languages spoken in South Asia exhibited large gains in the SQuId MOS. Since SQuId MOS model trained on 16 kHz speech, the 24 kHz speech generated by TTS model built on FLEURS-R was re-sampled to 16 kHz before scoring, therefore, the actual gains in naturalness from training TTS models on FLEURS-R would be larger than the SQuId MOS score differences indicate.\nWe also investigated whether these score improvements were independent of utterance duration. As shown in Figure 1, the speech restored by Miipher is consistently better than original FLEURS speech in naturnalness, in term of SQuId MOS. The restoration gains are especially significant on shorter utterances."}, {"title": "3.3. TTS Evaluations", "content": "We adopted the model configuration from Virtuoso 2 [29] to build a TTS baseline. It is a non-autoregressive TTS model, which uses UTF-8 byte as input representation. It aims to build a robust model supporting high and low resource languages via self-supervised and semi-supervised learning from speech-only, text-only, and speech-text pair datasets. Its speech encoder and shared encoder were composed of 6 and 18 Conformer [30] layers, respectively, with a hidden dimension of 768. The feed-forward text encoder consisted of 12 Conformer layers with a hidden dimension of 768. The semantic feature decoder in this model comprised 6 lightweight convolutional layers. The model was conditioned on both speaker and language IDs during train-ing, allowing both speaker and language control at inference. To capture intra-speaker prosodic variations that occur in nat-ural speech, this model has a global variational autoencoder (VAE) over input speech, which can be used to add prosodic diversity at the inference stage. Please refer to [29] for details.\nWe trained multi-speaker Virtuoso 2 models on either FLEURS to produce speech of 16 kHz sample rate, or trained on FLEURS-R to generate speech of 24 kHz sample rate. The same hyper-parameters were used between two models for consistent comparisons."}, {"title": "3.3.1. Model Configurations", "content": "We adopted the model configuration from Virtuoso 2 [29] to build a TTS baseline. It is a non-autoregressive TTS model, which uses UTF-8 byte as input representation. It aims to build a robust model supporting high and low resource languages via self-supervised and semi-supervised learning from speech-only, text-only, and speech-text pair datasets. Its speech encoder and shared encoder were composed of 6 and 18 Conformer [30] layers, respectively, with a hidden dimension of 768. The feed-forward text encoder consisted of 12 Conformer layers with a hidden dimension of 768. The semantic feature decoder in this model comprised 6 lightweight convolutional layers. The model was conditioned on both speaker and language IDs during train-ing, allowing both speaker and language control at inference. To capture intra-speaker prosodic variations that occur in nat-ural speech, this model has a global variational autoencoder (VAE) over input speech, which can be used to add prosodic diversity at the inference stage. Please refer to [29] for details.\nWe trained multi-speaker Virtuoso 2 models on either FLEURS to produce speech of 16 kHz sample rate, or trained on FLEURS-R to generate speech of 24 kHz sample rate. The same hyper-parameters were used between two models for consistent comparisons."}, {"title": "3.3.2. Speech Naturalness Evaluation", "content": "We evaluated the naturalness of speech synthesized by the Vir-tuoso models by the same SQuId model. Table 4 indicates that the TTS model trained by FLEURS-R produced more natural-sounding speech, with an overall score of 3.89 compared to 3.79 for that model trained by FLEURS. In the same manner, since the speech predicted by TTS model trained on FLEURS-R has to be resampled from 24 kHz to 16 kHz before scoring, the actual naturalness of the synthesized speech should surpass than what the rating of 3.89 suggests. The most significant improvements were observed in Khmer (km), Burmese (my), Mandarin (cmn), and several South Asian languages, including Oriya (or), Hindi (hi), and Tamil (ta). We hypothesize the gains might due to shared acoustic-prosodic properties among Southeastern Asian languages, and Southern Asian languages."}, {"title": "3.3.3. ASR Evaluation on Synthesized Speech", "content": "To evaluate the intelligibility of the synthesized speeches, we reused the same pre-existing ASR model to compute CERs on them. As shown in Table 5, the overall CERs remained consis-tent between two models. This suggests that the TTS models respectively trained on the restored / original corpus, could pro-duce speech with similar semantic contents. Although, there is a noticeable degradation in term of CER for the TTS gener-ated speech (15.9%) in Table 5, w.r.t. original speech (9.2%) in Table 2. This difference likely results from several fac-tors. First, the quality of the synthetic speech was still not as good as that of the natural speech. This can lead to worse ASR performance as the vast majority of training data for these ASR models consists of real speech. It is observed that the locales with extremely high CERs suffered primarily from dele-tion errors (e.g. Sorani-Kurdish, Sindhi, Panjabi, Japanese) or dominating substitution errors (e.g. Serbian, Madanrin, Can-tonese). Second, the CER differences between them arise from both data and TTS modeling aspects. The minor CER changes (15.9% vs. 16.0%) between the same TTS model on different data (FLEURS, FLEURS-R) imply that the degradation is not due to data restoration. Instead, the gap indicates that the TTS models need to learn to generate speech more close to the natural speech. Figure 2 illustrates detailed error rate changes for lan-guages where CER degraded by at least 10%. In languages like Panjabi, Serbian, Mandarin, Yoruba, and Thai, most errors re-sulted from substitutions. Japanese, Afrikaans, Sorani-Kurdish, and Occitan, on the other hand, experienced errors primarily due to deletions. Potential solutions include developing ASR mod-els specifically optimized for languages with large vocabularies (like Mandarin). Additionally, adapting ASR models to better handle the acoustic conditions of the FLEURS-R dataset could help improve its performance as an estimator of intelligibility of synthetic speeches."}, {"title": "4. Conclusion", "content": "This paper introduces FLEURS-R, a speech restoration-applied version of the multilingual parallel corpus FLEURS. As this new corpus maintains N-way parallel property, it can be used for TTS as well as other speech generation tasks such as speech-to-speech translation, voice conversion, and speech retrieval. Through CERs computed by the Maestro-U ASR model and 5-scale nat-uralness MOS estimated by the SQUID model, we show that FLEURS-R data has better naturalness than the original speech while accurately maintaining its semantic content. Furthermore, the baseline TTS models built on this new dataset demonstrates that it was useful to build a multilingual TTS model. This im-proved corpus can enable significant progress towards building speech generation applications for everyone, including zero-shot and few-shot TTS in many languages."}]}