{"title": "LTNtorch: PyTorch Implementation of Logic Tensor Networks", "authors": ["Tommaso Carraro", "Luciano Serafini", "Fabio Aiolli"], "abstract": "Logic Tensor Networks (LTN) is a Neuro-Symbolic framework that effectively incorporates deep learning and logical reasoning. In particular, LTN allows defining a logical knowledge base and using it as the objective of a neural model. This makes learning by logical reasoning possible as the parameters of the model are optimized by minimizing a loss function composed of a set of logical formulas expressing facts about the learning task. The framework learns via gradient-descent optimization. Fuzzy logic, a relaxation of classical logic permitting continuous truth values in the interval [0, 1], makes this learning possible. Specifically, the training of an LTN consists of three steps. Firstly, (i) the training data is used to ground the formulas. Then, (ii) the formulas are evaluated, and the loss function is computed. Lastly, (iii) the gradients are back-propagated through the logical computational graph, and the weights of the neural model are changed so the knowledge base is maximally satisfied. LTNtorch\u00b9 is the fully documented and tested PyTorch implementation of Logic Tensor Networks. This paper presents the formalization of LTN and how LTNtorch implements it. Moreover, it provides a basic binary classification example.", "sections": [{"title": "1 Introduction", "content": "Neuro-Symbolic integration (NeSy) (d'Avila Garcez et al., 2009) is the branch of Artificial Intelligence that studies the incorporation of symbolic AI (e.g., logical reasoning) and sub-symbolic AI (e.g., deep learning). The idea of NeSy is to build systems that can merge the advantages of these two paradigms. For example, neural networks effectively learn in the presence of noise; however, they require a lot of samples and are usually black boxes. On the other hand, symbolic AI can perform few-shot learning and, in some cases, even zero-short learning. Plus, it is usually explainable by design as it is often implemented as a set of logical formulas. An effective integration could address the major limitations of neural networks. Logic Tensor Networks (LTN) (Badreddine et al., 2022) take a step in this direction by proposing a framework to learn neural networks using logical reasoning. In particular, LTN allows for the specification of logical loss functions and their minimization by gradient-descent optimization. A specific First-Order language, plus the use of fuzzy"}, {"title": "2 Framework formalization", "content": "To define the knowledge base, LTN uses a specific first-order language called Real Logic. It is fully differentiable and has concrete semantics that allow mapping every symbolic expression into the domain of real numbers. This mapping function is referred to as grounding, denoted by G. Thanks to G, LTN can convert logical formulas into computational graphs that enable gradient-based optimization based on fuzzy logic semantics. Real Logic is a first-order (Flach, 2007) language with a signature containing constant, variable, functional, and predicate symbols. A term is constructed recursively from constants, variables, and functional symbols. An expression formed by applying a predicate symbol to some term(s) is called an atomic formula. Complex formulas are constructed recursively using connectives (i.e., \u00ac, \u2227, \u2228, \u21d2 ,\u2194) and quantifiers (i.e., \u2200,\u2203)."}, {"title": "2.1 LTN grounding", "content": "G maps each individual into a tensor of real features (e.g., sample's features), functions as real functions (e.g., regression functions), and predicates as real functions that specifically project onto a value in the interval [0, 1] (e.g., classifiers). A variable x is grounded to a sequence of nx individuals from a domain (see Fig. 1(a)), with $n_x \\in \\mathbb{N}^+$,$n_x > 0$. As a consequence, a term t(x) or a formula P(x), constructed recursively with variable x, will be grounded to a sequence of nx values too (see Fig. 1(b,c)). Afterward, connectives are grounded using fuzzy semantics, while quantifiers using special aggregation functions. LTN provides different fuzzy semantics for these operators, namely Product, G\u00f6del, and Lukasiewicz semantics (Cintula et al., 2011). In what follows, we define the Product configuration, better suited for gradient-based optimization (van Krieken et al., 2022). In the formalization, $u, v, u_1, ..., u_n \\in [0, 1]$, and $p > 1$.\nG(\u2227) : u, v \u2194 u\u00b7 v  G(\u2228) : u, v \u2192 u + v - u \u2022 v  G(\u21d2 ) : u, v \u2192 1 - u + u * v  G(\u00ac) : u\u2194 1 - u\n$G(\\exists): u_1,..., u_n \\mapsto (\\frac{1}{n} \\sum_{i=1}^n u_i^p)^{\\frac{1}{p}} G(\\forall) : u_1,..., u_n \\mapsto 1 + (\\frac{1}{n} \\sum_{i=1}^n (1 - u_i)^p)^{\\frac{1}{p}}$\nConnective operators are applied element-wise to the tensors in input (see Fig. 1(d)), while aggregators aggregate the dimension of the tensor in input that corresponds to the quantified variable (see Fig. 1(e, f)). Figure 1 gives an intuition of how LTN performs these operations to compute the formula \u2200x\u2203y P(x, y) \u2227 Q(y), where P and Q are logical predicates, while x and y are variables. Note these are also the steps LTNtorch uses to construct the computational graph for this formula."}, {"title": "2.2 LTN learning", "content": "Given a Real Logic knowledge base $K = {\\phi_1,...,\\phi_n}$, where $\\phi_1,...,\\phi_n$ are closed formulas, LTN allows to learn the grounding of constants, functions, and predicates appearing in them. In particular, if constants are grounded as embeddings, and functions/predicates onto neural networks,"}, {"title": "3 Binary classification in LTNtorch", "content": "We introduce a simple binary classification task where the objective is to discriminate between cats and dogs. The building blocks for this example are (1) a variable dog to iterate through batches of dog images, (2) a variable cat to iterate through batches of cat images, and (3) a unary predicate Dog distinguishing between cats and dogs. Given these symbols, the knowledge base LTNtorch maximizes to solve this task is $K = {\\phi_1,\\phi_2}$, where $\\phi_1 = \\forall dog Dog(dog)$ and $\\phi_2 = \\forall cat \\neg Dog(cat)$. When K is maximally satisfied, Dog returns high truth values when dogs are given and low truth values (see the use of \u00ac in (2) when cats are given.\nLTNtorch makes use of G to build the computational graph for this task. In particular, $G(dog) = [b, l, w, c]$, where b is the batch dimension, and l, w, and c are the length, width, and number of channels of the image, respectively. In other words, dog is grounded as a batch of dog images. Similarly, cat is grounded as a batch of cat images (see Fig. 2). Then, $G(Dog |\\theta) : x \\mapsto \\sigma(CNN_{\\theta}(x))$, namely Dog is implemented through a Convolutional Neural Network (i.e., CNN) with parameters \u03b8. Specifically, this predicate takes images in input and returns truth values indicating how plausible it is that the images depict dogs (see Fig. 2). Note the logistic function allows Dog to be interpreted as a fuzzy logic predicate, as the output is in the interval [0, 1]. The loss function for this task is:\n$L(\\theta) = 1. \u2013 SatAgg_{\\phi \\in K} G_{dog \\leftarrow B_{dog}}(\\phi|\\theta)$\n$_{cat \\leftarrow B_{cat}}$"}, {"title": "3.1 LTNtorch implementation", "content": "In what follows, we provide a partial code snippet implementing the training loop of the presented example in LTNtorch. We invite the reader to review the examples in the repository and the framework documentation to complete this implementation. Note the implementation details of CNN_model and train_data_loader are not given as it is standard PyTorch code."}, {"title": "4 Conclusions", "content": "In this paper, we introduced LTNtorch, the fully documented and tested PyTorch implementation of Logic Tensor Networks. We summarized the formalization of the framework and provided a simple example to help understand how LTNtorch implements the LTN. This paper does not have to be considered a comprehensive framework documentation. We strongly invite practitioners interested in approaching LTNs to read the original paper and the documentation carefully."}]}