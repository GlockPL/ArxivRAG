{"title": "TextCAVs: Debugging vision models using text", "authors": ["Angus Nicolson", "Yarin Gal", "J. Alison Noble"], "abstract": "Concept-based interpretability methods are a popular form of explanation for deep learning models which provide explanations in the form of high-level human interpretable concepts. These methods typically find concept activation vectors (CAVs) using a probe dataset of concept examples. This requires labelled data for these concepts an expensive task in the medical domain. We introduce TextCAVs: a novel method which creates CAVs using vision-language models such as CLIP, allowing for explanations to be created solely using text descriptions of the concept, as opposed to image exemplars. This reduced cost in testing concepts allows for many concepts to be tested and for users to interact with the model, testing new ideas as they are thought of, rather than a delay caused by image collection and annotation. In early experimental results, we demonstrate that TextCAVs produces reasonable explanations for a chest x-ray dataset (MIMIC-CXR) and natural images (ImageNet), and that these explanations can be used to debug deep learning-based models.", "sections": [{"title": "1 Introduction", "content": "Deep learning-based models are increasingly utilised in healthcare scenarios where mistakes can have severe consequences. One approach for creating safer, more reliable models is to use interpretability: the ability to explain or present a model in terms understandable to a human [4].\nMany different interpretabilty methods have emerged, with explanations taking a variety of different forms such as individual pixels, prototypes or concepts. We focus on concept-based methods which provide explanations using high-level terms that humans are familiar with. Concept activation vectors (CAVs) are a common approach used to represent concepts within the activation space of a model and are found using a probe dataset of concept exemplars [13].\nThe labels required for this can be expensive to obtain in medical domains where expert clinical input is necessary. We introduce TextCAVs, a concept-based interpretability method that uses solely the text label of the concept, or descriptions of it, rather than image examples.\nWe demonstrate that TextCAVs give meaningful explanations for both natural image (ImageNet [3]) and chest X-ray (MIMIC-CXR [11,12]) tasks. Further,"}, {"title": "2 Related Work", "content": "Kim et al. [13] introduce Testing with Concept Activation Vectors (TCAVs) where they use probe datasets of concept examples to create CAVs and then compare the CAVs with model gradients to measure a model's sensitivity to a concept for a specific class. We also use the directional derivative (dot product between CAV and gradient) to measure model sensitivity, but our CAVs are created using a multi-modal model and so do not require a probe dataset for each concept.\nIn order to reduce the cost of creating concept-based explanations, a variety of different methods automate the process of finding concepts [6,22,18,7,5]. However, the meaning of each concept is not always readily apparent and the concept must be visually present in the dataset used to discover the concepts.\nOur method reduces cost using a different approach as we also do not need to collect labelled data for each concept, but our resulting CAVs have inherent meaning from their text descriptions."}, {"title": "3 Text CAVS", "content": "For some target model, \u03a6, and a CLIP-like vision-language model, \u03a8, let $I \\in \\mathbb{R}^m$ and $I_{\\Psi} \\in \\mathbb{R}^n$ be the extracted features for some image dataset $D_I$. As \u03a8 contains a joint embedding space between text and images we can also extract text features: $T_{\\Psi} \\in \\mathbb{R}^n$ from some text dataset $D_T$. We train two linear layers h: $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ and g: $\\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ which can be used to convert between the features of the two models. To create TextCAVs, we only need h but to improve h's ability to convert text features we use a cycle loss term which requires g. The loss is composed of two parts: reconstruction loss and cycle loss. The reconstruction loss is simply the mean squared error (MSE) between the image features and converted features.\n$L_{mse} = ||h(I_{\\Psi}) - I_{\\Phi}||^2 + ||g(I_{\\Psi}) \u2013 I_{\\Phi}||^2$ (1)\nThe reconstruction loss can only be calculated for image features as we need features from both models (\u03a6 and \u03a8). To include information from the text features in the loss function we use cycle loss which ensures that the features are consistent with their original form when converted back to their original space:\n$L_{\u0441\u0443\u0441} = ||h(g(I_{\\Psi})) \u2013 I_{\\Phi}||$ (2)\n$+ ||g(h(I_{\\Psi})) \u2013 I_{\\Psi} ||$ (3)\n$+||g(h(T_{\\Psi})) \u2013 T_{\\Psi} ||.$ (4)\nOnce trained, we use h, \u03a8 and a concept label, c, to obtain a concept vector in the activation space of the target model:\n$v_c = h(\\Psi(c)).$ (5)"}, {"title": "4 Experiments", "content": "In this section we provide a description of our training setup, our model choices, evaluation and then a discussion and analysis of our results experiments with both the ImageNet and MIMIC-CXR datasets."}, {"title": "4.1 ImageNet", "content": "TextCAVs achieved 3rd place at the Secure and Trustworthy Machine Learning Conference (SaTML) interpretabilty competition to detect trojans (implanted bugs) in vision models trained on ImageNet [2]. Additionally, as part of the competition, TextCAVs was used to identify all four secret trojans demonstrating its potential for interactive debugging.\nIn this section, however, we simply demonstrate that TextCAVs produces reasonable explanations for a standard ResNet-50 [8] trained on ImageNet."}, {"title": "4.2 MIMIC-CXR", "content": "In this section we demonstrate TextCAVs ability to produce meaningful explanations for a model trained on the chest X-ray dataset MIMIC-CXR and how we can use TextCAVs to discover bias in a model trained on a biased version of the dataset.\nTraining Details We train both the linear transformations, h and g, and the target model, \u03a6, using the MIMIC-CXR training set. The target model is a ResNet-50 [8] pretrained on ImageNet and then fine-tuned for the 5-way multi-label classification of chest X-rays with the classes: No Finding, Atelectasis (lung collapse), Cardiomegaly (enlarged heart), Edema (fluid in the lungs) and Pleural Effusion (fluid between the lungs and the chest wall). We use the Adam optimiser [14] with weight decay of 1e \u2013 4 and initial learning rate of 1e \u2013 4. The learning rate is halved or the training is stopped if the validation loss does not decrease within 3 or 5 epochs, respectively. Images are resized to 256 \u00d7 256. We use random rotation of up to 15 degrees, random horizontal flipping, random crop and resize with a minimum size of 40%, and distortion to augment the images. We use the published data splits and, after removing images with no positive class labels, there are 368, 945 training, 2, 991 validation and 1,012 test images. We use labels from CheXpert [9] for the training and validation labels, which have been generated by a model using the text reports. Whereas, for the test dataset, we use the provided labels annotated by a single radiologist.\nWe train both hand g on the training set of MIMIC-CXR for 20 epochs. We use the output of the average pool operation as the features from the target model as it simplifies the extraction of model gradients (Eqn. 7).\nFor \u03a8, we use BiomedCLIP [23] \u2013 the current state of the art vision-language model for chest X-ray tasks.\nConcepts The MIMIC-CXR dataset has a clinical report associated with each image. We use these reports as a source of concepts. We extract the sentences from the \"FINDINGS\" and \"IMPRESSION\" sections of the reports and use a random subset of 5000 sentences to obtain a wide variety of concepts to test.\nBiased Data To evaluate TextCAVs as an interpretability tool we explore its usefulness in model debugging. We induced a dataset bias in the MIMIC-CXR training set by removing all participants with a positive label for Atelactesis and a negative label for Support Devices. This means that all participants with Atelactesis in the training set also had a Support Device (e.g. tube or pacemaker) as can be seen in Figure 2.\nMetrics To provide a quantitative metric, we labelled the top-50 sentences for each class, ordered by directional derivative, on whether they relate to the class. We report this information as a concept relevance score (CRS), which is simply the proportion of concepts that were related to the class. Using Edema as an example, a sentence was labelled as related if it directly diagnosed the class, e.g., \"Worsening cardiogenic pulmonary edema\", or if the class was implied, e.g., \"bilateral parenchymal opacities\" or \"there is alveolar opacity throughout much of the right lung\".\nResults We are comparing two models: one trained on the standard MIMIC-CXR dataset and the other trained on the biased version. We will refer to the models as \"standard\" and \"biased\", respectively. The standard model achieved a mean area under the receiver operator characteristic curve (AUC) of 0.83 and the biased model a mean AUC of 0.81. The individual class AUCs can be found in Table 2. We expect, and see that the biased version has higher performance on a biased version of the test set since Support Devices tend to be easy to detect. As evidence for this, we trained a reference model separately and achieved an AUC of 0.92 for Support Devices."}, {"title": "5 Conclusion", "content": "In this work we introduce TextCAVs, an interpretability method that, once two linear layers have been trained, can measure the sensitivity of a model to a concept with only a text description of the concept. We show that TextCAVs produce reasonable explanations for models trained on both natural images (ImageNet [3]) a chest X-ray dataset (MIMIC-CXR [11]). As first demonstrated in the SaTML CNN interpretability competition [2], we show that TextCAVs can be used to debug models. We generated explanations for a model trained on a biased version of the MIMIC-CXR dataset and showed that explanations for the biased class substantially changed with most (44/50) concepts referring to the bias compared to just 13/50 for the unbiased model.\nOnce the linear transformations, h and g, have been trained, TextCAVs enables fast feedback when testing the sensitivity of different concepts. This makes it ideally suited for interactive debugging which we aim to study in future work. Some of the concepts with a high directional derivative did not appear to be related to the class. In section 4.2 we state three possible sources of this: (1) \u03a6, (2) h or (3) \u2207b,k. In future work we will explore which of these have the greatest effect."}]}