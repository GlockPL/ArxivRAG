{"title": "A Preliminary Study of Multilingual Code Language Models for Code Generation Task Using Translated Benchmarks", "authors": ["Rohit Dandamudi", "Gema Rodr\u00edguez-P\u00e9rez"], "abstract": "Evaluating the performance of Code Language Models (CLMs) for software engineering tasks, especially in multilingual and low-resource programming language settings, poses significant challenges. These challenges are primarily due to the lack of high-quality benchmarks across various programming languages and the imbalanced nature of the CLMs training corpus. Although recent advances in one of the common downstream tasks, code generation, have shown promise by introducing translated benchmarks using different methodologies, there is a current lack of empirical evidence assessing these benchmarks. To address this gap, we conducted a preliminary study to evaluate the performance of PolyCoder, a pioneering open-source, multilingual CLM built for code generation. We utilized two existing state-of-the-art translations of the popular code generation benchmark, HumanEval, facilitated by the OctoPack and MultiPL-E studies. Our results suggest that the outcomes observed in these translated benchmarks align well with evaluation metrics used during the training phase, such as perplexity, thereby validating their effectiveness in estimating the performance of CLMs. However, we identified several inconsistencies in the CLMs' performance across the translated benchmarks and encountered challenges in replicating the results. These initial insights highlight the need for more comprehensive empirical studies to fully understand translated benchmarks' methodological approaches, limitations, and reproducibility. Such studies are essential to ensure their reliability before they are widely adopted.", "sections": [{"title": "1 INTRODUCTION", "content": "Significant research is being conducted to enhance the performance of Large Language Models (LLMs) in coding tasks [16]. These tailored models are also known as Large Language Models for Code or Code Large Language Models, and previous research focuses on assessing their performance in different software engineering downstream tasks, such as code generation [4], code summarization [1], code translation [18], and code clone detection [17], among others. We will refer to these models as Code Language Models (CLMs) in this study; these CLMs have been integrated into tools used by software engineers to assist with code writing. Notable examples of such models integrated into development tools are GitHub Copilot\u00b9, the world's most widely adopted AI developer tool, and CodeLlama [6], an open-source LLM dedicated to code generation.\nThese models are intended to work for everyone using any programming language [6, 9, 12]. However, they do not resemble similar performance for different programming languages [8, 10, 23]. In multilingual scenarios, the concept of low-resource programming languages emerges. A low-resource programming language is defined by two main criteria: 1) the programming language is either absent from the dataset or minimally represented in the corpus, and 2) there is limited availability of code samples online or low adoption of the language. This lack of representation and resources leads to reduced performance in these languages compared to more commonly used ones.\nConsequently, the current state of research for estimating model performance across multiple languages cannot rely solely on popular evaluation metrics such as CodeBLEU score [20] or perplexity [23]. While these metrics are helpful for initial assessments of the ability to understand and replicate patterns, they are not sufficient indicators of functional correctness as they mainly evaluate based on similarities and patterns instead of real-world performance in producing code solutions. Therefore, they should not be proxied as comprehensive evaluations of the CLMs' capabilities in different languages.\nFocusing on one of the most common downstream tasks, code generation, benchmarks are crucial for evaluating model performance. A benchmark is a standardized set of tasks or tests used to measure and compare the performance of different models. In the context of code generation, benchmarks often include extensive unit tests that provide quantifiable results based on the models' pass rate (i.e., the percentage of tests the model successfully completes). Research in this area has produced and adopted several key benchmarks such as HumanEval [9] and MBPP [2]. The main issue with these benchmarks is that they perpetuate the existing challenges"}, {"title": "2 BACKGROUND & RELATED WORK", "content": "This section will introduce several key concepts relevant to the current study. These include the translated benchmarks for evaluating code generation downstream tasks, the CLMs chosen for evaluation in this study, and an overview of previous related work in this area."}, {"title": "2.1 Code Generation Benchmarks", "content": "The code generation task is evaluated based on the code quality generated by CLMs. Various benchmarks are employed to assess the performance of CLMs in code generation tasks. Some benchmarks are automated and sourced from online platforms, such as the CoNala [24] benchmark, which uses StackOverflow data, and the Automated Programming Progress Standard (APPS) [7] benchmark, powered by coding platforms like Codewars. In contrast, there are manually curated datasets that aim to test functional correctness, such as HumanEval [9] and Mostly Basic Programming Problems (MBPP) [2]. Recent research in these manually curated benchmarks extends to finer details, such as class-level code generation [15].\nOur study focuses on the HumanEval [9] benchmark released by the OpenAI team, which comprises 164 Python problems of varying difficulty. As a popular benchmark for evaluating CLMs in code generation, there have been several efforts to translate it into other languages. Notable examples include HumanEval-X [11], which provides datasets in four additional programming languages; Multi-HumanEval [5], which extends the dataset to eleven programming languages; and HumanEval-XL [19]., which translates"}, {"title": "2.2 Code Language Model", "content": "Numerous CLMs have been developed to address software engineering tasks, with most production-ready models being closed-source, such as Codex [9], and a few smaller open-source models like CodeT5 [21]. However, recent studies have introduced open-source models pre-trained on multiple programming languages, such as CodeLlama [6]. In this study, we use PolyCoder [23], an open-source model designed to address the limitations of closed-source models. PolyCoder [23] is available in three different versions-160 million parameters, 0.4 billion parameters, and 2.7 billion parameters-trained on 12 different programming languages, including low-resource languages. We chose PolyCoder over CodeLlama because of its accessible sizes that align with the hardware we have. Additionally, the study [23] identified the problem of low-resource programming languages when they built the model and it wasn't already tested on MultiPL-E benchmark; instead, it used perplexity"}, {"title": "3 STUDY DESIGN", "content": "This section provides an overview of the research questions addressed in this study and outlines the methodology employed to address them."}, {"title": "3.1 Research Question", "content": "This research is motivated by the need to understand how different translated benchmarks influence the performance of CLMs and to what extent these benchmarks can accurately reflect CLMs' capabilities in diverse linguistic environments. Thus, this study answers the following research question:\nRQ: How do translated benchmarks capture and reflect the multilingual performance of CLMs in both high-resource and low-resource programming languages?\nThis research question stems from an understanding that the efficacy of CLMs in multilingual contexts is crucial, given their expanding use across diverse programming environments. The question addresses the need to evaluate how varied methodological approaches within translated benchmarks that diverge from the original benchmark affect CLM's performance. As these models are applied to a broader array of languages, some of which may be underrepresented in training datasets, assessing whether the benchmarks used can accurately measure their capabilities and limitations is imperative. This inquiry aims to uncover potential disparities in performance across languages and identify reliable evaluation practices. By examining the consistency of CLM performance across different benchmarks, the study seeks to identify patterns and anomalies that could inform future development, refinement, and adoption of these tools. By exploring these aspects, the study aims to inform future enhancements in developing CLMs and the methodologies used to evaluate them, ensuring that the benchmarks are robust and comprehensive enough to reflect the data it's trained on."}, {"title": "3.2 Methodology", "content": "Programming Languages Selection: We considered four programming languages that are common in both of the translated benchmarks, including Python. JavaScript and Java were chosen because they are among the top four most widely used programming languages, and Rust was included as it is the top emerging language according to the GitHub October 2023 Survey\u00b2. Additionally, Java and JavaScript account for a high percentage in the final training data corpus (21.3% and 7.37%), while Rust represents one of the lowest (1.27%) among the twelve programming languages in the PolyCoder model [23]. This provides a good mix of multilingual and low-resource scenarios to answer our research question.\nImplementation of the CLMs: The overview of the evaluation pipeline is illustrated in Figure 1. The process commenced with acquiring the three different sizes of a CLM: 160M, 0.4B, and 2.7B, as provided by PolyCoder [23] from the HuggingFace Hub using their Python library. Subsequently, the MultiPL-E benchmark was implemented by following the well-maintained tutorial provided by the original authors\u00b3, utilizing an up-to-date GitHub repository for reference. Conversely, for the HumanEvalSynthesize benchmarks, we leveraged an actively maintained code generation framework introduced in the StarCoder model from the BigCode community [12]. As mentioned in Table 1, the MultiPL-E translated benchmarks are incomplete and had 161, 161, 158, and 156 problems in Python, JavaScript, Java, and Rust, respectively. On the other hand, the CLM was tested against the complete set of 164 questions in all translated benchmarks of HumanEvalSynthesize. All computations were executed on a high-performance computing offline node equipped with an NVIDIA Tesla V100 32GB NVLink GPU.\nEvaluation metrics: The performance of the CLMs was evaluated using the pass@1 metric, with a conservative temperature setting of 0.2, as recommended by benchmarking studies [8, 10]. The pass@1 metric quantitatively measures the proportion of translated problems for which the CLM successfully passes all designated unit tests on the first attempt, providing a stringent test of the model's predictive accuracy in a controlled environment. Hence, the higher the pass@1 metric, the greater the ability to produce correct code solutions on its initial attempt for each given problem. The evaluation encompassed four programming languages: JavaScript, Java,"}, {"title": "4 INITIAL RESULTS AND DISCUSSION", "content": "The pass rate of running the PolyCoder models against the MultiPL-E translated benchmarks in the first attempt (pass@1) can be seen in Table 2. Java HumanEval translation benchmark has the highest performance in larger models of 0.4B and 2.7B out of all languages. The next best and almost comparable result was observed in Python, followed by JavaScript, both considered high resource languages per the data corpus [23]. Finally, Rust had the lowest performance, which indicates its low resource nature.\nThe HumanEvalSyntheSize benchmark's pass@1 of PolyCoder models are illustrated in Table 3. Here, Python has the highest results across all models compared to the rest of the other languages. However, the second-best results for PolyCoder 2.7B, were seen in Rust, a low-resource programming language, followed by JavaScript and Java.\nThe evaluation involved assessing the models' performance across various sizes using the HumanEval Python version included in the replication packages of both benchmarks as discussed in Section 3.2. This setup was consistent with the pass@1 scores of 2.13%, 2.96%, and 5.59% for 160M, 0.4B, and 2.7B size of PolyCoder respectively obtained in Xu et al. [23], ensuring the validity of the results as seen in Table 2 and 3. Interestingly, the highest performance apart from Python was observed in Java in MultiPL-E benchmarks, especially in larger sizes of the PolyCoder model (0.4B, 2.7B); this might be because of the larger Java size in the data corpus it was trained on and does not align with the perplexity scores the study relied on for testing multilingual nature [23]. Another observation from the superior performance in Java suggests that the translations and evaluation processes in the MultiPL-E approach may be more effective, providing a more accurate assessment of the models' capabilities. The performance of PolyCoder models is almost reversed in both translated benchmarks. In the MultiPL-E benchmark, Java performs best, as seen in Table 2, and Rust is the least performant. The opposite is observed in HumanEvalSynthesize benchmarks as shown in Table 3. These differences could stem from the methodological differences and, most importantly, how the benchmarks are replicated. Implementing the HumanEvalSynthesize translation through the framework [3] might change the intended behavior of CLMs due to the trade-offs made during its integration to be compatible with Python libraries used while maintaining consistency across languages, which may drift apart from the paper [10]. Additionally, the performance in Rust was higher than in the MultiPL-E with a score of 3.05% in PolyCoder 2.7B compared to HumanEvalSynthesize, where it achieved a pass@1 rate of 2.30%. The differences in problem sizes, with 156 and 164 problems in the former and latter benchmarks respectively, could potentially account for the observed discrepancies in performance. It also suggests that HumanEvalSynthesize was able to create a better translation in Rust and capture the CLMs low-resource nature well.\nA general pattern observed across both benchmarks was the improved performance with increased CLM size. Larger models consistently demonstrated better pass@1 rates in most languages. However, an exception to this trend was noted in Java within the HumanEvalSynthesize benchmark, where performance did not increase as expected with larger model sizes. This could again come from the implementation limitation of using the code generation framework [3]. Ultimately, the performance of CLMs is on the lower side, with the largest CLM of PolyCoder 2.7B giving 6.10%; this is still much lower when compared to the performance of state-of-the-art CLMs that are available out there [16], this is caused by the hardware limitation and not being able to accommodate larger CLMs. Consequently, such constraints magnify the impact of even minor inconsistencies in performance.\nFinding The CLMs exhibit inconsistent performance across the two translated benchmarks. However, a general pattern of improved performance is observed as the size increases and effectively captures the CLMs' performance across high-resource and low-resource languages."}, {"title": "5 IMPLICATIONS", "content": "Considering the observed inconsistencies and performance variations across different benchmarks and languages detailed in Section 4, this section explores the implications of our study's findings and suggests avenues for improvement."}, {"title": "5.1 Implications for Researching Translated Benchmarks", "content": "The findings of this study suggest that translated versions of well-established benchmarks could be a valuable resource for evaluating the multilingual capabilities of CLMs. However, selecting these benchmarks should be preceded by a comprehensive evaluation of the methodological approaches employed in their development. This study highlighted that even aspects such as problem size can vary between benchmarks, leading to varying results. For instance, the inconsistencies observed between the two benchmarks in this study underscore the necessity of thorough validation.\nMoreover, researchers should exercise caution when integrating new translated benchmarks into their studies. It is imperative to first verify the benchmark against existing models or studies to ascertain whether the replication yields consistent results. Such preliminary validations can help identify and mitigate potential discrepancies that might affect the robustness and reliability of research outcomes."}, {"title": "5.2 Implications for Developing Translated Benchmarks", "content": "The development of translated benchmarks is crucial for the progression of multilingual capabilities in CLMs. These benchmarks must be accompanied by comprehensive documentation and robust support mechanisms to maximize their effectiveness. Platforms like HuggingFace Hub provide an excellent infrastructure for distributing and utilizing machine learning models and datasets, and leveraging such platforms can significantly facilitate the adoption and replication of benchmarks across different research settings. Our study was constrained by the selection of benchmarks, which only included a limited number of common languages, with Rust being the sole representative of low-resource languages. This limitation is far from ideal and underscores the need for a broader approach to developing translated benchmarks. Expanding the range of languages covered by translated benchmarks would be immensely beneficial for the research community, particularly in enhancing model performance in underrepresented languages. Furthermore, there is significant potential in translating benchmarks to encompass domains beyond traditional coding problems. This suggestion is inspired by initiatives like [13], which propose extending the applicability and relevance of benchmarks to broader contexts. Finally, automating the translation process in benchmarks as shown in MultiPL-E [8] could potentially be the ideal way to extend the approach to various benchmarks, languages, and CLMs."}, {"title": "6 LIMITATIONS AND FUTURE WORK", "content": "This preliminary study presents foundational insights into the multilingual performance of CLMs using translated benchmarks while identifying several areas for future research and acknowledging inherent limitations.\nThe evaluation metrics used in this study primarily focused on pass rates at a threshold of 1. Expanding future evaluations to include pass rates at 10 and 20 could offer a more nuanced understanding of models' capabilities across varying difficulty levels. Additionally, integrating cutting-edge large language models, such as the Instruct variant of CodeLlama 4, could provide updated insights into the effectiveness of newer technologies in code generation tasks. A significant limitation noted in this study pertains to using the HumanEvalSynthesize benchmarks. The implementation of these benchmarks could be potentially limited by the tool used and methods employed by the BigCode community [3], which may differ from the original setup described in the OctoPack study [10].\nIn future work, we would like to explore parameter-efficient training methods [22] and quantized versions of larger CLMs to accommodate for insufficient resources or hardware to perform such studies. Moreover, extending the investigation to include other software engineering downstream tasks [16] would broaden the study's implications. Finally, analyzing the translations of various benchmarks offered by platforms such as CodeXGlue [14] could shed light on additional challenges and opportunities in the multilingual evaluation of CLMs."}, {"title": "7 CONCLUSION", "content": "In conclusion, this study has investigated the multilingual performance of CLMs by utilizing translated benchmarks. We introduce the concepts of code generation benchmarks and the need for translated versions to evaluate CLMs better. Our findings suggest that while translated benchmarks can serve as valuable tools for evaluating the capabilities of CLMs across different languages, their effectiveness is contingent upon the accuracy and consistency of the translation and benchmarking methodologies employed. Moreover, the notable variability between benchmarks emphasizes the necessity for researchers to undertake thorough validations and comparisons against existing models and literature to ensure the reliability and applicability of results. Looking forward, it is crucial to expand the scope of translated benchmarks to encompass a broader array of languages and domains, ultimately enhancing the generalizability of research findings and ensuring the robustness and applicability of CLMs across diverse real-world applications. This study contributes to the ongoing discourse in software engineering by shedding light on the challenges and opportunities associated with the multilingual evaluation of CLMs. By addressing these challenges, the research community can fully utilize translated benchmarks to improve the evaluation and development of CLMs."}]}