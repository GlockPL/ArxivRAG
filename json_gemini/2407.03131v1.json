{"title": "MVGT: A Multi-view Graph Transformer Based on Spatial Relations for EEG Emotion Recognition", "authors": ["Yanjie Cui", "Xiaohong Liu", "Jing Liang", "Yamin Fu"], "abstract": "Electroencephalography (EEG), a medical imaging technique that captures scalp electrical activity of brain structures via electrodes, has been widely used in affective computing. The spatial domain of EEG is rich in affective information. However, few of the existing studies have simultaneously analyzed EEG signals from multiple perspectives of geometric and anatomical structures in spatial domain. In this paper, we propose a multi-view Graph Transformer (MVGT) based on spatial relations, which integrates information from the temporal, frequency and spatial domains, including geometric and anatomical structures, so as to enhance the expressive power of the model comprehensively. We incorporate the spatial information of EEG channels into the model as encoding, thereby improving its ability to perceive the spatial structure of the channels. Meanwhile, experimental results based on publicly available datasets demonstrate that our proposed model outperforms state-of-the-art methods in recent years. In addition, the results also show that the MVGT could extract information from multiple domains and capture inter-channel relationships in EEG emotion recognition tasks effectively.", "sections": [{"title": "I. INTRODUCTION", "content": "Affective computing is commonly employed for the analysis of emotional states through Human-Computer Interaction (HCI) systems, which collect multimodal data from subjects, including voice signal, self-report, body gesture and physiological signals. Compared to other modalities, physiological signals have certain advantages. These signals are directly captured from the subjects' mental states, thus prevent subjects from disguising or hiding. The physiological signals commonly used to measure emotions are electroencephalography (EEG), electrocardiography (ECG), electromyography (EMG), and galvanic skin response (GSR), etc., among which EEG is often utilized for analyzing cognitive functions of human brain. Electrical signals from brain neurons are collected using the EEG method, which involves placing dry and noninvasive electrodes on the scalp [1]. Nowadays, due to its high temporal resolution, portability, and affordability, this method is widely employed to study brain changes in response to emotional stimuli [2].\nTraditional EEG features are mainly divided into three kinds, i.e., time domain, frequency domain, and time-frequency domain features. Given the signal-to-noise ratio and substantial fluctuations inherent in EEG signals, frequency domain features are commonly used for EEG-based emotion recognition tasks. The typical approach involves decomposing the raw signals into five frequency bands: \u03b4, \u03b8, \u03b1, \u03b2, \u03b3. Frequency domain features, such as power spectral density (PSD) [3], differential entropy (DE) [4], [5], differential asymmetry (DASM) [6] and rational asymmetry (RASM) [7], are subsequently extracted from each frequency band respectively.\nThe spatial structure of the brain also contains rich emotional information. Emotional states may involve distributed circuits rather than considering a single brain region in isolation [8]. Asymmetry between the left and right hemispheres can reflect changes in valence and arousal [9]. Recent studies have highlighted the importance of utilizing spatial domain information. Li et al. [10] introduced recurrent neural networks to learn the asymmetric differences between the left and right hemispheres. Li et al. [11] also utilized hierarchical neural networks to learn both regional and global information of spatial-temporal EEG features. Graph Neural Networks (GNN) are emerging as a powerful tool for analyzing spatial information in EEG emotion recognition. Song et al. [12] dynamically learned relationships between EEG channels using graph convolutional networks (GCN). Zhong et al. [13] incorporated asymmetry of the left and right hemispheres into the adjacency matrix to model graph structure. Li et al. [14] also utilized adaptive graph convolutional networks that integrate multi-domain information to learn relationships between channels. Ding et al. [15] incorporated lobe information as prior knowledge into the GNN. Jiang et al. [16] proposed an elastic graph Transformer to extract emotional information. Although these methods have achieved excellent performance in emotion recognition tasks, they have a common issue: they all rely on GNNs based on neighborhood aggregation schemes which may pose potential risks such as over-smoothing [17]\u2013[19], under-reaching [20], and over-squashing [21]. Additionally, these methods do not comprehensively consider the geometric and anatomical structure information of the brain.\nThe main contributions of this paper are as follows:\n\u2022 We propose a multi-view graph transformer based on spatial relations (MVGT), fusing information from multiple perspectives including temporal, frequency, and spatial domains.\n\u2022 Our method, based on Graph Transformer, mitigates the"}, {"title": "II. RELATED WORK", "content": "In this section, we review the related work from the perspectives of EEG-based emotion recognition and graph transformer.\nA. EEG-based emotion recognition\nEEG signals are inherently noisy and susceptible to channel crosstalk [22]. Due to the complexity of EEG signals, it is challenging to isolate clean and independent signals. Therefore, it is crucial to select what form of data to analyze under conditions of high noise. Effective features of EEG signals can reduce noise and facilitate the recognition of cognitive patterns in specific tasks. Experimental evidence suggests that frequency domain features are often associated with behavioral patterns [23], hence they are commonly used in EEG analysis.\nAlong with the development of deep learning, increasingly complex models with rich expressive abilities have emerged and have been extensively utilized in EEG signal analysis. Zheng et al. [5] employed deep belief networks to analyze important frequency domain components and effective channels based on the learned parameters. Song et al. [12] used a graph convolutional method based on Chebyshev polynomials [24] to dynamically learn the representations of EEG signals. Zhong et al. [13] innovatively incorporated the asymmetric information of the hemispheres as prior knowledge into the adjacency matrix in 3D space and used GCN to dynamically learn the inter-channel correlations. The reasonable combination of the multi-domain information contributes to improving the accuracy in the emotion recognition task. Li et al. [14] proposed an adaptive graph convolutional network that integrates the temporal domain, frequency domain, and functional connectivity. Ding et al. [15], inspired by neuroscience research, combined intra-region convolution and inter-region convolution based on brain lobe regions to learn brain cognitive patterns. Jiang et al. [16] utilized the advantages of GCN in the spatial domain and Transformer in the temporal domain to improve the accuracy of emotion classification.\nB. Graph Transformer\nThe GNNs used in the above methods are based on neighborhood aggregation schemes. However, classical GNNs based on message passing (MPGNNs) may lead to over-smoothing [17]\u2013[19], under-reaching [20], and may also fail to fit long-range signals due to over-squashing [21], which limit the expressive power of the model. Graph transformers (GTs) alleviate such effects as they have a global receptive field [25]. However, without sufficiently expressive structural and positional encodings, GTs cannot capture effective graph structures [26]. Dwivedi et al. [27] utilized eigenvectors of graph Laplacian as position encodings in fully connected Graph Transformers and integrated edge features into the attention mechanism. Building on this, SAN [28] used a full Laplacian spectrum to learn the positional encodings for each node. Graphormer [29], [30] employed node centrality and node distance metric to implement structural and relative positional encodings, achieving state-of-the-art performance on molecular prediction datasets. In EEG emotion recognition, Li et al. [31] innovatively combined a masked autoencoder based on self-supervised learning with a CNN-Transformer hybrid structure, effectively improving classification accuracy. However, this method only used sine-cosine positional encodings, limiting the Transformer's ability to learn spatial information."}, {"title": "III. PRELIMINARY", "content": "Let G = (V, E) define a graph, where V = {V1, V2, \u2026\u2026\u2026, Vn} represents the nodes in the graph, and E = {e1, e2, \u2026\u2026\u2026, em} is the edges between the nodes. The representation of node vi is denoted as xi \u2208 Rd. Most existing GNNs [17], [32]\u2013[35] adopt neighborhood aggregation schemes, iteratively aggregating representations of its first or higher-order neighbors, followed by using backpropagation (BP) to learn task-driven feature encodings. We define the representation of node vi at the l iteration as h(l) and define h(0) = xi. The l-th iteration can be represented as:\na = AGGREGATE(l) ({h, e : j\u2208N(vi)}) (1)\nh = UPDATE(l) (h(l\u22121), al) (2)\nwhere e represents a differentiable function used for feature transformation of node and edge information. The N(vi) is the set of neighbors of vi. The AGGREGATE function is used to aggregate the transformed representation using a differentiable, permutation invariant function, (such as mean, sum, max, etc.). The goal of UPDATE function is to integrate the information from neighbors into the node representation. For graph classification, the READOUT operation is typically used to obtain a representation of the entire graph, which is then fed into a classifier to determine the graph label.\nB. Graphormer\nThe Transformer [36] is undeniably one of the most popular deep neural network architectures today, driving significant advancements in natural language processing and computer vision. With its global receptive field and multi-head attention mechanism, Transformer can extract global semantic correlations between tokens in multiple feature subspaces, effectively enhancing the model's expressive power. From the perspective of GNNs, Transformer can be interpreted as a GNN acting on a fully connected graph. Therefore, it is reasonable and feasible to use Transformer to address tasks on graph data. The ability to properly incorporate the structural information of graphs"}, {"title": "IV. METHODS", "content": "In this section, we introduce the methods employed in the EEG emotion recognition task. Firstly, we elaborate on the embedding for temporal information. Secondly, leveraging the spatial geometry and physiological anatomy of the brain, we propose two novel and simple designs of encoding that enable the model to adaptively learn the inter-channel correlations. Finally, we present the detailed implementations of MVGT.\nA. Problem Definition\nEEG signals can be represented as a two-dimensional matrix with respect to channels and time. Given that channels exhibit spatial structure, they can be structured into fully connected graph data G = (V, E), where V denotes the nodes in the graph, representing EEG channels, and E denotes the edges, representing the connections between channels. The features of the nodes are denoted by X = (X1,X2,\u2026\u2026,Xn) \u2208 Rn\u00d7d, where n = |V| represents the number of nodes and d represents the feature dimension.\nEEG signals have high temporal resolution and contain rich temporal information. Because of the multi-electrode acquisition method, EEG signals can be regarded as multivariate time series. When processing time series, the embedding of temporal information are crucial. EmoGT [16] treats the features of different channels at the same time points as tokens and employs an attention mechanism to extract temporal correlations between them. Due to the different anisotropic volume conduction characteristics [37] in human brain tissues, there may be temporal delays between different channels, which in turn leads to time-unaligned events at a single moment thus causing performance degradation. MD-AGCN [14] utilizes convolutional neural networks to extract temporal information along the time axis from continuous EEG segments, with the receptive field limited by the size of the convolution kernel. Inspired by iTransformer [38], we broaden the receptive field by considering the entire time series as an embedded token rather than a single time point. First, following the methods of MD-AGCN and EmoGT, we use overlapping sliding windows of size T to segment EEG signals along the time axis and use these segments as tokens, which are then fed into the attention module in the form of continuous segments. After processing with sliding windows, we obtain X \u2208 RS\u00d7n\u00d7Tf, where S denotes the number of continuous EEG segments, n is the number of channels, and f denotes the dimension of frequency domain features.\nAccording to the universal approximation theorem [39], the feed-forward neural network (FFN), as the basic module of the Transformer encoder, can learn the intrinsic properties to describe a time series and is a superior predictive representation learner compared to self-attention [38]. Therefore, using continuous time segments as the input to the FFN may be"}, {"title": "B. Temporal Embedding"}, {"title": "C. Spatial Encoding", "content": "The special structure of the brain encompasses rich spatial information. Fully exploiting structural information is beneficial for the recognition and analysis of cognitive patterns in the brain. Therefore, to better identify emotional patterns in emotion classification tasks, we employed two simple but effective methods of spatial encoding: brain region encoding and geometric structure encoding.\n1) Brain Region Encoding: Neuroscience research demonstrated that the activation of a specific brain region often leads to the concurrent activation of related brain regions responsible for the same high-level cognition [40]. In EEG emotion recognition, incorporating relevant neuroscience findings can typically enhance recognition accuracy. RGNN [13] integrates the asymmetry of neural activity between the left and right hemispheres as prior knowledge into the adjacency matrix, effectively enhancing recognition accuracy. BiHDM [10] improves emotion pattern recognition performance by learning the differences between the left and right hemispheres. LGGNet [15] divides EEG channels into different regions and combines local intra-region convolution with global inter-region convolution, achieving good results on the DEAP [41] dataset. With reference to the three divisions of LGGNet, we adopt four brain region divisions, which divide the EEG channels into different regions based on a prior knowledge, aiming to incorporate the brain region information into the model.\nWe divide the regions based on the anatomical structure of the brain and implement LOBE scheme.\nTo further investigate the expressive power of brain region encoding, we conduct a detailed division of brain lobes according to the 10-20 system based on electrode positions, employing the GENERAL scheme.\nAsymmetric EEG activity in the frontal lobe can be utilized for discriminating valence changes [9]. The left frontal lobe exhibits a stronger correlation with joy and happy, while the right frontal lobe is more strongly correlated with fear and sadness. Thus we further divide the frontal lobe region into two symmetrical regions to obtain the FRONTAL scheme.\nAccording to the symmetry of brain structure [42], we make a finer division of the brain lobe regions, defining the HEMISPHERE scheme.\nThe four modes mentioned above are showed in Fig. 2. In terms of specific implementation, we assign each electrode a brain region tag, then project the tags into an embedding space using a learnable projection function, and simply add the embeddings to the node features. The encoding of node i is represented as follows:\nh\nr\u2081 = Embedding(Tag(xi)), ri \u2208 Rd, (4)\n(0) = xiWx + ri, (5)\nwhere Wx \u2208 RTf\u00d7d is a learnable projection function, and d represents the dimension of the embedding. Through the above encoding method, we integrate the information of the brain's physiological anatomy into the model.\n2) Geometric Structure Encoding: In the real world, the human reasoning process considers not only the semantic relationships between objects but also their spatial relations. EEG channels have a 3D structure, and the functional connectivity between these channels lack precise definitions. Therefore, we represent the relationships between EEG channels as a fully connected directed graph structure. The Euclidean distance between channels is calculated using their coordinates to learn the spatial correlations between nodes. Firstly, let (i, j) represents the Euclidean distance between node i and node j, and encode \u03c6(i, j) using a set of Gaussian basis functions [30], [43]. Let bk \u2208 Rn\u00d7n denotes one of the Gaussian basis functions. The element (i, j) of this function can be expressed as:\nbk(i, j) = Gk (aij\u03c6(i, j) + \u03b2ij \u2013 \u03bc\u03ba,\u03c3\u03ba), (6)\nwhere aij, Bij, \u03bc\u03ba, and \u03c3\u03ba are learnable parameters, and i and j denote the index of the source and target node, respectively. The result of the basis functions can be represented as B = ||=1bk \u2208 Rn\u00d7n\u00d7K, where || denotes the concatenation operation. All spatial encodings of each node are then summed up along the second dimension and transformed linearly to obtain the geometric structure encoding.\nh = xWx + ziWz + ri, zi = \u2211=1Bi,j,k, (7)\nwhere i denotes the node index, and Wz \u2208 RK\u00d7d is a learnable projection function. Additionally, we incorporate the spatial encoding as a bias term into the softmax attention, which will help the model properly capture the spatial correlations.\nOur proposed spatial encoding matrix is directed, which is inconsistent with the assumption of a symmetric adjacency matrix [13], [16]. Using directed connections provides the model with greater expressive power because the correlation between node pairs (i, j) and (j, i) may differ. Since we assume nodes are fully connected, we avoid specific assumptions about inter-channel correlations and learn the functional correlations between nodes through encoding. Let I denote the model depth, and i denote the index of multi-head attention. Therefore, the brain functional encoding can be represented as:\nAli = Softmax (k\u221ad + BWB (8)\nwhere W, W and Wg are learnable parameters, and d' denotes the feature dimension size of the l-th layer. This encoding method integrates temporal, frequency, and spatial domain features into the model, enhancing its expressive power. We compute the attention scores between nodes using embedded vectors, representing the semantic correlations between different nodes from multiple perspectives. Finally, the attention scores are added to the spatial geometric encoding to obtain the correlations between channels."}, {"title": "D. Implementation Details of MVGT", "content": "In this section, we describe the overall architecture of the model, including spatial encoding and the Transformer encoder, as illustrated in Fig. 1. For better optimization, we first apply GraphNorm [44] to normalize the input features between 0 and 1. Subsequently, we perform geometric and regional structure encoding to obtain multi-domain embeddings.\nX' = GraphNorm(X), (9)\nH(0) = SpatialEncoding + Proj(X'), (10)\nWe employ a Pre-LN Transformer structure, applying layer normalization (LN) before the multi-head attention (MHA) and the FFN. Recent study suggests that the Pre-LN structure yields more stable gradients and is more favorable for optimizer, enabling faster convergence [45] compared to Post-LN. Additionally, we utilize dropout to mitigate overfitting. This process is represented as follows:\nH\u2032(1) = MHA(LN(H(1\u22121))) + H(1\u22121), (11)\nH(1) = FFN(LN(H'(t))) + H\u2032(1), (12)\nInspired by [30], [46], we feed the outputs recursively into the same modules, denoted as recycling in Fig. 1. The iterative refinement progressively refines the model's ability to discriminate encoded information and understand emotional patterns, thereby helping the model capture more effective details."}, {"title": "V. EXPERIMENTS", "content": "For our experiments, we selected the SEED [5] and SEED-IV [47] datasets to evaluate the effectiveness of our model. These datasets consist of EEG signals recorded from subjects while they watched emotion-eliciting videos.\nSEED dataset comprises data from 15 subjects who participated in three sessions, each separated by at least one week. Each sessions consists of 15 trials capturing emotional labels, with the emotion labels being positive, negative, and neutral.\nSEED-IV dataset is constituted by EEG signals from 15 subjects across three separate sessions conducted at different times, using the same device as the SEED dataset. This dataset encompasses four emotion labels: neutral, sad, fear, and happy. In each session, each subject underwent 24 trials.\nB. Settings\nTo prevent potential data leakage that could arise from segment-wise shuffling, we split the training and test sets at the trial level. Following the settings of previous studies [5], [10], [12]\u2013[14], [16], [31], [47], we use pre-computed differential entropy (DE) features for the recognition task. For the SEED dataset, we use the first 9 trials of each subject as the training set and the last 6 trials as the test set, as done in previous research. The DE features are computed using five frequency bands extracted from 1s nonoverlapping windows. The model performance is evaluated based on the average accuracy and standard deviation across all subjects over two sessions of EEG data. Similarly, for the SEED-IV dataset, we use the first 16 trials as the training set and the last 8 trials as the test set. The DE features for SEED-IV are calculated using 4s windows. The performance of our model is assessed using data from all three sessions.\nFor input data, we use overlapping sliding windows of size T along the time axis to extract sample fragments, with T being set to 5. During experiments, the hidden dimension is set to 64 and the number of Gaussian basis functions is 32. The number of MHA layers is 4 and the number of attention heads is 2. The iterative refinement process is performed three times. We set the batch size to 32 and the learning rate within the range of 3e-5 to 3e-3. Cross-entropy is used as the loss function, and AdamW [48] is employed as the optimizer with a weight decay rate of 0.1.\nC. Baseline Models\n\u2022 DGCNN [12]: A dynamic graph neural network method based on Chebyshev polynomials dynamically learns inter-channel relations in emotion recognition.\n\u2022 BiHDM [10]: This model employs a pairwise subnetwork to capture the discrepancy between the left and right hemispheres of the brain.\n\u2022 R2G-STNN [11]: A model that captures spatial-temporal features from local to global scales for emotion classification."}, {"title": "A. Datasets", "content": ""}, {"title": "D. Results Analysis", "content": "We compare the classification results based on the SEED and SEED-IV datasets with recent state-of-the-art models, as shown in Table 1. It is evident that our proposed model significantly outperforms the baseline models under the same experimental settings. In the experiments on the SEED dataset, the model adopting the FRONTAL scheme achieved the best performance, with a classification accuracy of 96.45%. The LOBE scheme also achieved a slightly superior accuracy of 95.36%, compared to other models. For the SEED-IV dataset, the classification accuracy under the GENERAL scheme was 93.57%, reaching the best performance compared to baseline models. The MVGT model also achieved commendable results under other division schemes. Overall, our model achieved the best recognition accuracy compared to the baselines. The results suggest that selecting the specific division scheme relevant to the emotion task could enhance the expressive power of MVGT."}, {"title": "E. Ablation Study", "content": "To validate the effectiveness of spatial encodings, we conducted ablation experiments on the SEED and SEED-IV datasets, as presented in Table II. By removing both types of spatial encoding, we repeated the aforementioned experiments under the same experimental settings. On the SEED dataset, the model achieved an accuracy of 93.79% with a standard deviation of 7.15%. Compared to MVGT-F, the accuracy decreased by 2.66% and the standard deviation increased by 2.75% after removing spatial encodings. For the SEED-IV dataset, the accuracy dropped by 4.08%, resulting in 89.49%, with the standard deviation rising by 1.80% to 10.40%, when compared to MVGT-G. The experiments demonstrate that incorporating spatial structure information benefits the model performance in emotion recognition tasks. Under experimental settings that consider only geometric structure or brain region structure, the model's classification accuracy improved over the plain model without any spatial encoding. Evidently, when considering both types of spatial structures simultaneously, the model performance significantly surpassed that of the plain model and models using only single spatial information. This indicates the effectiveness of our proposed spatial encodings and confirms that the expressive"}, {"title": "F. Visualization of Inter-channel relations", "content": "To better illustrate the correlations between channels, we visualized the inter-channel relations of MVGT-F on the SEED and MVGT-G on the SEED-IV. Given that the inter-channel relations might vary among different subjects, we calculated the average weights across all subjects. We focused on the last iteration of iterative refinement and selected the 10 strongest connections of channel pairs. Fig. 4 shows the visualization results, where the rows represent the attention heads and the columns represent the layers of the MHA.\nThe parameters based on the SEED dataset indicate that emotion patterns are reflected in the activities of multiple brain regions. In the first layer of MVGT-F, the channels in the left frontal region had higher participation in the first attention head, while the channels in the right frontal region were more involved in the second head, potentially corresponding to positive and negative emotion patterns [9], respectively. In the second layer of the model, the parietal and occipital regions showed higher involvement, which aligns with the findings on emotion patterns in [49]. As the model depth increases, the symmetrical connections in the lateral temporal regions of both hemispheres are enhanced, consistent with previous research by [5], [13], [16]. For the SEED-IV dataset, the connections in the frontal, parietal, and occipital regions are the most active, consistent with the findings of [13]. In the first attention head of MVGT-G, the strongest correlation was between Ol and PO3, followed by P4 and P2. Other connections were mainly distributed in the temporal and frontal regions. In the second head, the channel pairs (O1, PO5), (CB1, PO7), and (PO5, PO7) contributed the most to emotion recognition. Additionally, the connection between AF3 and FP1 provided important information for emotion processing, which aligns with the conclusions of [13], [16].\nOverall, our model does not focus solely on the local information of a single brain region but comprehensively considers intra-regional and inter-regional information. This confirms that emotional states result from interactions among widely distributed functional networks in the brain, as discussed by [50]."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we propose a multi-view Graph Transformer based on spatial relations for EEG emotion recognition. This model integrates information from multiple perspectives, including temporal, frequency and spatial domains. We incorporate spatial geometric encoding and brain region encoding to enhance the Graph Transformer's ability to perceive spatial structures. Additionally, the model adaptively learns inter-channel relationships through an attention mechanism and the encoding of channel geometry. Extensive experiments on public emotion recognition datasets demonstrate that our proposed model outperforms other competitive baseline models.\nFurthermore, analysis of channel correlations indicates that emotional activities in the brain are not confined to a single local region but result from the coordinated action of multiple brain areas. The frontal, parietal, occipital, and lateral temporal lobes all contribute to the emotion recognition tasks in varying degrees.\nIn future work, we will focus on the following aspects: (1) designing more optimal structural encodings, such as data-driven methods for adaptive structural encoding; (2) attempting to combine various handcrafted features and exploring the possibility of extracting effective EEG features through neural networks; (3) investigating emotion recognition methods based on multimodal physiological signals."}]}