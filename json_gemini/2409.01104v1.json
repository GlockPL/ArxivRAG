{"title": "AI Olympics challenge with Evolutionary Soft Actor Critic", "authors": ["Marco Cal\u00ec", "Alberto Sinigaglia", "Niccol\u00f2 Turcato", "Ruggero Carli", "Gian Antonio Susto"], "abstract": "In the following report, we describe the solution we propose for the AI Olympics competition held at IROS 2024. Our solution is based on a Model-free Deep Reinforcement Learning approach combined with an evolutionary strategy. We will briefly describe the algorithms that have been used and then provide details of the approach.", "sections": [{"title": "I. INTRODUCTION", "content": "In this paper, we present the approach taken for solving the simulation stage of the AI Olympics competition held in IROS 2024. Our approach first uses Model-Free Reinforcement Learning to find a policy that performs the swing up and stabilization of the robots, and then finetunes the agents through evolutionary methods. In particular, it first trains the agent using the Soft Actor-Critic (SAC)[2] algorithm with a physics-inspired reward function in order to train an agent that is able to perform the main task. Thanks to the ability of SAC to have stable yet effective training of the agents, we are able to optimize a reward function that approximates the competition's reward function with just a few hours of training while obtaining results that surpass the proposed baselines. Furthermore, SAC trains the agent to be robust out of the box, thanks to its algorithmic specifications.\nFinally, the agent undergoes further optimization through the application of evolutionary algorithms to optimize the competition's desired score function.\nThis paper is organized as follows: Section 2 introduces the challenge, Section 3 introduces the Soft Actor-Critic algorithm, Section 4 instead introduces evolutionary strate-gies, Sections 5, 6, and 7 introduce the reward and training schedule used to train the final agent, Section 8 introduces the experiments settings used for the training, and finally Section 9 concludes the paper.\nThe submission code can be found at https://github.com/AlbertoSinigaglia/double_pendulum"}, {"title": "II. COMPETITION'S GOAL", "content": "The competition involves an underactuated double pendulum system with 2 degrees of freedom[8], combined in two settings named acrobot and pendubot, whose state is represented as (01, 02, 01, 02). In the former, the first joint is passive while the second one is actuated, whereas in the latter, the first joint is actuated, and the second one is passive.\nThe competition is composed of a simulation and a real hardware stage.\nIn the simulation stage, the goal is to derive a controller for each setting that performs the swing-up and stabilization in the vertical setting, an unstable equilibrium point of the systems. Both systems are simulated with a frequency of 500 Hz for 10 seconds. Finally, the proposed controllers are also tested for their robustness. For this stage, an OpenAI Gym environment is employed [9]. Previous years' competition submissions can be found in [10].\nIn the real hardware stage, controllers are tested on the physical systems.\nIn the rest of the sections, we review the simulation stage's performance and robustness evaluations."}, {"title": "III. SOFT ACTOR CRITIC", "content": "This section provides a concise review of the Soft Actor-Critic (SAC) algorithm, its origins, and its training routine."}, {"title": "A. Deep Deterministic Policy Gradient (DDPG)", "content": "Expanding on the concepts of Deterministic Policy Gradient (DPG) [4], the Deep Deterministic Policy Gradient (DDPG) algorithm [1] integrates deep learning. DDPG employs two deep neural networks: the actor and the critic. The actor-network directly maps states to actions, while the critic network approximates the Q-function. The core update equations in DDPG are as follows: Critic update:\n$\\Delta\\varphi = E_{(s,a,r,s')} [(r + \\gamma Q_{\\varphi'}(s', \\pi_{\\theta'}(s')) ) - Q_{\\phi}(s,a))^2]$ (1)\nActor update:\n$\\Delta\\theta = E_s [\\nabla_aQ_{\\phi}(s, a)|_{a=\\pi_{\\theta}(s)}\\nabla_{\\theta}\\pi_{\\theta}(s)]$ (2)\nwhere \u03c6 and \u03b8 are the parameters of the critic and actor networks, respectively. DDPG also uses a replay buffer and target networks ($' and \u03b8'), enhancing stability and efficiency."}, {"title": "B. Soft Actor-Critic (SAC)", "content": "The Soft Actor-Critic (SAC) algorithm, presented in [2], introduces an entropy term to the reward structure, promoting exploration and robustness in policy learning. SAC optimizes a stochastic policy in a way that balances the trade-off between entropy and reward. The objective function for SAC is given by:\n$J(\\pi) = E_{\\delta t, a_t \\sim \\pi} \\sum_{t} \\gamma^t (r(s_t, a_t) + \\alpha H(\\pi(\\cdot|s_t)))$ (3)\nwhere H denotes the entropy of the policy and \u03b1 is a temperature parameter that controls the importance of the entropy term against the reward. SAC employs separate networks for the policy, value function, and two Q-functions, reducing overestimation bias and improving learning stabil-ity."}, {"title": "IV. ZERO-ORDER OPTIMIZATION AND EVOLUTIONARY STRATEGIES", "content": "Zero-order optimization, or derivative-free optimization, encompasses a range of techniques that optimize functions without requiring gradient information. These methods are essential in scenarios where gradients are non-existent, expensive to compute, or do not reliably guide the optimization due to noise, discontinuities, or non-differentiability. Evolutionary strategies (ES) are a prominent class of zero-order optimization techniques that employ mechanisms analogous to biological evolution, such as mutation, recombination, and selection, to evolve a population of candidate solutions over successive generations."}, {"title": "A. Separable Natural Evolution Strategy (SNES)", "content": "The Separable Natural Evolution Strategy (SNES) [3] refines evolutionary strategies by focusing on the efficient adaptation of mutation distributions. SNES is particularly effective in environments where parameters have different scales and sensitivities. It maintains and adapts a separate step size for each parameter dimension, facilitating an inde-pendent and efficient exploration of the parameter space. The core of the SNES algorithm involves updating the mutation strengths using a log-normal rule, which is mathematically depicted as follows:\n$\\sigma_{new, i} = \\sigma_{old,i} \\exp (\\tau N(0,1) + \\tau'N(0, 1)_i)$, (4)\n$\\theta_{new, i} = \\theta_{old,i} + \\sigma_{new,i}N(0, 1)_i$, (5)\nfor each dimension i, where rand \u03c4' are learning rates designed to control the global and individual adaptation speeds, respectively. This adaptation mechanism is inspired by the principles of natural evolution and covariance matrix adaptation, but it simplifies the adaptation process by treating the search space dimensions as separate entities. The SNES thus combines the robustness of evolutionary algorithms with the efficiency of adaptive step-size mechanisms, leading to faster convergence and reduced computational complexity compared to traditional ES and other sophisticated adaptation strategies like CMA-ES."}, {"title": "V. SURROGATE REWARD FUNCTION", "content": "The competition's reward function combines different aspects of the learned controllers, such as the time required for the swing up, the energy consumed, the torque required, and many others. It then combines them with different weights since they all have different scales. However, such a reward is challenging to maximize for a Deep RL agent that observes only the current immediate state of the system since it includes terms that depend on the overall trajectory, thus violating the RL definition of reward function R(s, a, s'), if not using the whole trajectory as the state. For this reason, we developed a surrogate reward function that should act as a feasible counterpart but still be proximal to the competition score function.\nThe reward function being used is the following:\n$R(s, a) = \\begin{cases}  V+a[1 + cos(\\theta_2)]^2 - \\beta T - \\rho_1\\alpha^2 - \\phi_1 \\Delta a & \\text{if } y > Y_{th} \\\\  V - \\rho_2\\alpha^2 - \\phi_2\\Delta a - \\eta||\\dot{s}||^2 & \\text{otherwise} \\end{cases}$ (6)\nwhere \u03b1, \u03b2, \u03c6, \u03b7, \u03c1 are just hyperparameters that are used to control the tradeoffs. Yth is set to 0.375 m for acrobot and 0.35 m for pendubot. In this formulation, a is the normalized action, thus a \u2208 [-1,1]. V is the potential energy of the system, and T is the kinetic energy. \u0394a is the difference between the current action and the previous action. s is the square norm of the angular velocities of the robot, thus $||\\u00b3||2 = \\u03b8 + \\u03b83. Finally, 02 is the angle between the two robot links."}, {"title": "VI. ON THE AGENT ROBUSTNESS", "content": "Soft Actor-Critic is a very popular algorithm used to learn policy for continuous control. Its optimization objective is the following:\n$J(\\theta) = \\sum_{T}E_{(s,a) \\sim \\pi_\\theta} [r(s, a) + \\alpha H(\\pi_\\theta(\\cdot|s))]$ (7)\nFor this reason, SAC models \u03c0\u03bf(s) ~ N(\u03bc(s), diag[\u2211(s)]). Thanks to this definition, the model is already optimized to find a reasonably robust solution. Indeed, the agent has to integrate all actions that it can take in the next state, which are normally distributed, and it's optimized to reward high entropy distributions. For this reason, the solution found by SAC, will already be robust to noise on the action sampled."}, {"title": "B. SNES agent robustness", "content": "The SNES algorithm is used for this project to align the agent with the actual reward function rather than the surrogate one.\nHowever, training the agent to optimize the reward func-tion greedily has a major flaw: if earlier the SAC algorithm was making the agent robust out of the box, for SNES, nothing prevents the agent from cherrypicking a trajectory extremely effectively, but at the same time extremely brittle.\nFor this reason, the training is slightly modified in the following way:\n1) sample the greedy action from the SAC agent: a := \u03c0\u03bf(\u03c2)\n2) undo the tanh transformation a := atanh(a)\n3) introduce noise: a := a + \u20ac, \u03b5 ~ \u039d(0, \u03c3\u00b2)\n4) apply squashing: a := tanh(a)\nIndeed, it is not enough to only take a non-greedy action and sample from the posterior learned by the agent because nothing prevents such posterior from collapsing to a Dirac delta distribution. Instead, we measured the average \u03c3 com-puted by SAC after training during a trajectory, which was about 0.01, and we introduced it manually on the action."}, {"title": "VII. AGENT TRAINING", "content": "The following section describes our two-step training approach for the Evolutionary SAC controller."}, {"title": "A. Main agent training", "content": "The training of the Soft Actor-Critic (SAC) agent begins by using the previously defined reward function. One of the most impactful hyperparameters is the maximum torque Tmax, which directly influences the agent's actions. In our experiments, we explored various maximum torque limits to balance learning effectiveness and energy efficiency.\nThe torque values tested included 1.5 Nm, 3.0 Nm, and 5.0 Nm. After conducting extensive experiments, we estab-lished that the torque limit had a notable impact on both the training dynamics and the final performance of the agent.\nWith a torque limit of 1.5, Nm, we noticed that restricting the torque encouraged the agent to develop more energy-efficient solutions. By imposing tighter constraints on the force the agent could exert, the agent focused on optimizing its actions to achieve the task with minimal energy usage. However, this conservative approach also introduced sig-nificant challenges. The reduced torque limited the agent's ability to apply sufficient force when necessary, causing it to stabilize about equilibrium points such as (01, 02) = (\u03c0,\u03c0) or (01, 02) = (0, \u03c0).\nConversely, when the torque limit was increased to 5.0 Nm, the agent was empowered to exert greater force, which initially appeared to accelerate the learning process. The agent could more easily overcome the local maxima of the reward function and execute the required actions with brute force, leading to quicker early-stage learning. However, this initial advantage proved to be deceptive. As training progressed, the agents that relied on this higher torque tended to overuse the available force, resulting in less efficient and less precise solutions. These agents displayed excessive and unnecessary movements, which made the agent unable to stabilize around the desired equilibrium after the swing-up task.\nThe 3.0 Nm torque limit struck a balance between the ex-tremes. This setting provides the agent with sufficient power to perform necessary actions effectively while maintaining a reasonable degree of energy efficiency. Agents trained with this torque limit managed to learn the task within a reasonable timeframe and achieved competitive performance without resorting to excessive energy consumption. The 3.0, Nm torque limit allowed for effective learning without the drawbacks associated with the lower or higher torque extremes.\nThis formulation led us to solutions with a score of 0.504 for the acrobot and 0.567 for the pendubot.\nThe complete list of hyperparameters being used can be found in table I"}, {"title": "B. Evolutionary strategy agent", "content": "The Soft Actor-Critic (SAC) framework encompasses three primary neural networks: a policy network and two Q-networks. These networks are parametric models; hence, they can be optimized through zero-order methods such as evolutionary strategies. Such strategies excel in optimiz-ing systems that are either highly ill-conditioned or non-differentiable.\nSubtle modifications to the policy parameters can yield considerable disparities in the behavior of controllers, con-sequently affecting performance metrics drastically. The op-timization of SAC is inherently non-stationary, as the policy updates are executed through Stochastic Gradient Descent (SGD) on the evolving Q-functions. However, employing evolutionary strategies directly from the competition score proves challenging due to the lack of immediate feedback unless the model stabilizes the pendulum.\nTo address these issues, we propose a multi-stage training approach. The first stage is the SAC training reported in the previous sections, and the last stage employs the evolutionary strategy SNES, which begins from the policy refined during SAC's fine-tuning phase. Such policy is finetuned with SNES directly on the competition goal. SNES effectively bridges the gap between the surrogate reward function utilized in training the SAC agent and the original score metric from the competition.\nDespite its advantages, this methodology is highly contin-gent on the starting point of the optimization. The resultant policy tends to remain proximate to that of the initial SAC controller, thereby constraining its potential for variation. Nevertheless, we anticipate that the initial SAC agent has approximated a near-optimal policy, with a discrepancy amenable to further optimization via SNES. Notably, SNES requires minimal hyper-parameter tuning, with the key pa-rameters being the variance \u03c3 of the population, set at 0.01, and the population size, established at 40.\nHowever, the robustness of the agent is a critical con-sideration, as SNES tends to converge the agent's posterior to a Dirac Delta Distribution, which may exhibit fragility under perturbations. To mitigate this, we perturb the pre-tanh-action with noise from N(0,0.1) and then apply the tanh activation.\nThe best agents found by SNES finetuning achieve a score of 0.524 for the acrobot, and 0.596 for the pendubot, while minimally impacting the robustness scores, which decreases from 0.700 to 0.692 for the acrobot, and from 0.800 to 0.796 for the pendubot."}, {"title": "VIII. EXPERIMENTS", "content": "For the experiment side of this project, all code was developed in Python. For SAC, we relied on Stable Base-lines 3, implemented with PyTorch. For the evolutionary strategy training, we used the EvoTorch library. The code was run on a computer with an AMD Ryzen Threadripper 1920X 12-Core Processor and three Nvidia Titan V GPUs.\nThe evolutionary algorithm was run directly on the CPU. During training with SAC, the policy applies control with a frequency of 100 Hz, whereas in the evaluation and also for the evolutionary strategy, the control frequency is 500 Hz.\nThe learned trajectory can be seen in fig. 2 for pendubot and in fig. 1 for acrobot. Regarding the robustness of the agents, the scores for the singular test for pendubot are reported in fig. 3. The overall performance results are reported in table III for the pendubot setting and in table III for the acrobot setting."}, {"title": "IX. CONCLUSIONS AND FUTURE WORKS", "content": "This work explores a pure end-to-end Deep Reinforcement Learning approach for the AI Olympics challenge. In both settings, our approach surpasses all the baselines. Many other solutions integrated LQR controllers for the stabilization part of the system. Such an introduction allowed them to have faster and more effective training since it addresses the unstable equilibrium point with a more direct approach. Such integration can also be applied to the solution explored for this work. One future line of research is to include an LQR controller for the control near the unstable equilibrium and possibly fine-tune that part with the evolutionary approach. Another route is redesigning the reward function to enable lower torque values to obtain lower energy and efficient controllers."}]}