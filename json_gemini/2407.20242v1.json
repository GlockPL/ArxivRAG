{"title": "BADROBOT: JAILBREAKING LLM-BASED EMBODIED AI\nIN THE PHYSICAL WORLD", "authors": ["Hangtao Zhang", "Chenyu Zhu", "Xianlong Wang", "Ziqi Zhou", "Shengshan Hu", "Leo Yu Zhang"], "abstract": "Embodied artificial intelligence (AI) represents an artificial intelligence system that interacts with\nthe physical world through sensors and actuators, seamlessly integrating perception and action.\nThis design enables AI to learn from and operate within complex, real-world environments. Large\nLanguage Models (LLMs) deeply explore language instructions, playing a crucial role in devising\nplans for complex tasks. Consequently, they have progressively shown immense potential in\nempowering embodied AI, with LLM-based embodied Al emerging as a focal point of research\nwithin the community. It is foreseeable that, over the next decade, LLM-based embodied AI\nrobots are expected to proliferate widely, becoming commonplace in homes and industries.\nHowever, a critical safety issue that has long been hiding in plain sight is: could LLM-based\nembodied AI perpetrate harmful behaviors? Our research investigates for the first time how\nto induce threatening actions in embodied AI, confirming the severe risks posed by these soon-\nto-be-marketed robots, which starkly contravene Asimov's Three Laws of Robotics and threaten\nhuman safety. Specifically, we formulate the concept of embodied AI jailbreaking and expose three\ncritical security vulnerabilities: first, jailbreaking robotics through compromised LLM; second,\nsafety misalignment between action and language spaces; and third, deceptive prompts leading\nto unaware hazardous behaviors. We also analyze potential mitigation measures and advocate for\ncommunity awareness regarding the safety of embodied AI applications in the physical world.", "sections": [{"title": "Introduction", "content": "A longstanding goal of AI is the development of autonomous agents that can assist humans with everyday tasks\nin the physical world. Embodied AI addresses this goal by\nfocusing on Al systems that interact directly with and manipulate the physical environment. Unlike conversational\nAl models (e.g., ChatGPT) that just process and generate text or images, it is designed to\ncontrol physical entities, with robotics being its most notable application. Fundamentally, embodied AI has gained"}, {"title": "Related Work", "content": "Embodied AI. Embodied Al represents a distinctive branch of artificial intelligence, characterized by its ability\nto interact directly and dynamically with the physical world. This sets it apart from traditional AI models that\noperate solely within purely digital environments. The development of this unique capability has been significantly\nadvanced by research leveraging multimodal input learning methods, which enhance agent performance in complex\nand dynamic settings. Some works focus on leveraging large language models (LLMs) as task planners for Embodied AI, utilizing\nmultimodal integration and programming techniques to facilitate complex task planning and execution\nAdditionally, combining LLMs with visual language\nmodels can significantly enhance Embodied Al's ability to generalize to diverse instructions and objects. For\ninstance, VoxPoser introduces a novel framework for robotic manipulation that leverages\nvision-language models to generate 3D value maps. This approach improves zero-shot generalization and enables\nrobust interaction with dynamic environments, demonstrating significant advancements in handling complex task\ndynamics. Wang et al. propose a framework that employs multimodal GPT-4V to achieve effective embodied\ntasks planning with the combination of natural language instructions and robot visual perceptions. By training\nvision-language models on extensive web data and combining this with robotic trajectory data, RT-2\ncan generalize to novel objects and commands. Meanwhile, Diffusion Policy introduces a\nnovel approach by applying denoising diffusion processes to learn visuomotor policies, which effectively manage\nmultimodal action distributions and high-dimensional action spaces. Furthermore, to enhance the adaptability\nand autonomy of robotic systems, RoboCat represents a self-improving generalist agent\ncapable of adapting to various tasks and robotic embodiments through continuous self-learning and fine-tuning.\nDespite these significant advancements, there remains a notable gap in research addressing the safety implications\nof embodied Al systems."}, {"title": "Constructing LLM-Based Embodied AI: A Robotic Arm Implementation", "content": "Drawing from , we streamline the design to develop an embodied Al system\nfeaturing a robotic arm, enabling us to assess its security in the physical world. As illustrated in Figure 3, the\nsystem first employs Automatic Speech Recognition (ASR) to convert user's speech input into text, which is then\nfed into the LLM. We leverage LLM to break down instructions into a sequence of task plans and use prompt\nengineering to create a predefined action pool from which the LLM selects the corresponding representations. Next,\nfor tasks requiring visual understanding, an RGB camera captures images that are then fed into the MLLM. Taking\nvisual grounding tasks as an example, the MLLM generates precise coordinates of objects that need manipulation.\nFinally, it outputs robotic arm control instructions in JSON format, which are then transmitted to downstream\nrobotic controllers. Text-To-Speech (TTS) technology then translates text outputs back into voice, enabling seamless\ncommunication. Finally, through hand-eye calibration and inverse kinematics, a six-degree-of-freedom robotic arm\nis controlled to execute the specified actions."}, {"title": "Threat model", "content": "Attackers' Capability. We assume a weak threat model, where attackers have no prior knowledge of the LLM\nemployed by the embodied AI. The attacker can only interact with the embodied AI through voice communication\nas any benign user might, attempting to jailbreak the system on the fly (i.e., a no-box setting). This scenario is quite\ncommon since any user can freely attempt to manipulate it with prompts.\nAttackers' Objective. Similar to traditional LLM jailbreak attacks, the attackers aim to manipulate aligned LLMs into\nproducing outputs that deviate from human values, rather than refusing harmful instructions. However, unlike\ntraditional LLM jailbreak attacks, the attacker's primary goal in this context will be compelling the embodied AI to\nperform specific malicious actions (e.g., Physical Harm, Privacy Violations, Pornography, Fraud, Illegal Activities,\nHateful Conduct, and Sabotage), with the elicitation of malicious textual outputs being a secondary effect.\nBased on this threat model, Sections 4.2, 4.3, and 4.4 present three concrete attacks that can universally jailbreak\nLLM-based embodied AI, underscoring the latent risks inherent in the deployment of embodied Al in the physical\nworld."}, {"title": "Formulation of embodied AI jailbreak", "content": "In this section, we first propose a formal framework for\nanalyzing and characterizing embodied Al jailbreaks,\nproviding a unified approach to understanding various\nrisk surfaces. Drawing inspiration from recent advance-\nments in visual-language-action (VLA) models\nworld model concepts in robotics and AI safety constraints , we propose to conceptualize an embodied AI sys-\ntem E as a quadruple:\n$\\E = (\\varphi, \\psi, \\omega, \\delta)$\nwhere $\\varphi$ is linguistic processing, $\\psi$ is the action generation, $\\omega$ is the world knowledge, and $\\mathcal{S}$ is the set of safety\nconstraints. Let $\\mathcal{I}$ be the input space (e.g., language instructions, visual data from cameras, and environmental\ninformation from sensors), $\\mathcal{L}$ be the linguistic output space, and $\\mathcal{A}$ be the action output space. We define the\nfollowing functions:\n$f_{\\phi}: \\mathcal{I} \\rightarrow \\mathcal{L}, \\qquad f_{\\psi}: \\mathcal{I} \\times \\omega \\rightarrow \\mathcal{A}, \\qquad f_{\\omega}: \\mathcal{P} \\rightarrow \\omega$\nHere, $f_{\\phi}$ represents the linguistic processing function, mapping inputs to linguistic outputs. $f_{\\psi}$ denotes the action\ngeneration, which takes both inputs and the current world model to produce actions. $f_{\\omega}$ is the world model update\nfunction, evolving the world model based on the physical environment $\\mathcal{P}$. To evaluate output safety, we introduce\ntwo binary safety constraint functions $\\mathcal{S}_\\mathcal{L}$ and $\\mathcal{S}_\\mathcal{A}$ for linguistic and action outputs respectively:\n$\\mathcal{S}_\\mathcal{L}: \\mathcal{L} \\rightarrow \\{0, 1\\}, \\qquad \\mathcal{S}_\\mathcal{A}: \\mathcal{A} \\rightarrow \\{0, 1\\}$\nHere, $\\mathcal{S}_\\mathcal{L}$ evaluates the safety of language outputs, while $\\mathcal{S}_\\mathcal{A}$ assesses the safety of action outputs. In both cases, 0\nindicates an unsafe output and 1 indicates a safe output. To formalize the concept of embodied AI jailbreak, we first\ndefine a safe embodied AI system. An embodied AI system $\\mathcal{E}$ is considered safe if and only if both its linguistic and\naction outputs satisfy the safety constraints for all inputs $i$:\n$\\forall i \\in \\mathcal{I}, \\quad (\\mathcal{S}_\\mathcal{L}(f_{\\phi}(i)) = 1) \\land (\\mathcal{S}_\\mathcal{A}(f_{\\psi}(i, \\omega)) = 1)$\nConsidering that physical actions in embodied AI systems can have direct and potentially irreversible consequences\nin the real world, we focus our definition of jailbreaking on the safety of action outputs (see Section 3.1 for details).\nThus, we define an embodied AI jailbreak $\\mathcal{J}$ as an input $i \\in \\mathcal{I}$ that results in an unsafe action output irrespective of\nthe safety status of its linguistic output:\n$\\mathcal{J}(i) = (\\mathcal{S}_\\mathcal{L}(f_{\\phi}(i)) \\in \\{0, 1\\}) \\land (\\mathcal{S}_\\mathcal{A}(f_{\\psi}(i, \\omega)) = 0)$\nIt is crucial to recognize the interplay between linguistic processing and action generation, especially in LLM-based\nsystems. Here, the LLM simultaneously handles both functions $f_{\\phi}$ and $f_{\\psi}$, creating a scenario where inappropriate\nlinguistic processing can indirectly lead to unsafe actions. Thus, although jailbreak is determined by action safety,\nthe linguistic component significantly influences overall system security. For instance, a successful LLM jailbreak\ncan indeed result in the generation of malicious actions. We have\n$f_{\\psi}(i, \\omega) = g(f_{\\phi}(i), i, \\omega)$\nwhere $g$ represents the interaction between the language output, input, and world model in determining the final\naction. Our formulation of embodied AI jailbreak (please see Eq. (5)) encapsulates scenarios where risks may result\nfrom: (1) direct manipulation of the action generation function $f_{\\psi}$, (2) indirect influence through the linguistic\nprocessing, exploiting the relationship in Eq. (6), and (3) inadequate or manipulated world model $\\omega$. We note that\nthis formulation provides a structured approach to analyzing embodied AI jailbreaks, offering insights into system\ncomponent interactions and potential risk surfaces. The framework's flexibility allows for the incorporation of\nemerging security challenges as the field progresses."}, {"title": "Physical World Risks of Embodied AI", "content": "In this section, we presents empirical evidence of the risks outlined in Section 1. We initially conduct case studies by\napplying the state-of-the-art LLM Yi-Large and VLM Yi-Vision to our embodied AI\nsystem, which incorporates the myCobot 280-Pi robotic arm from Elephant Robotics. We use the Baidu AI Cloud\nQianfan Platform's ASR interface and ChatTTS's TTS model for voice interaction within our embodied AI system.\nDetails are moved to Appendix.\nThese formulations provide a comprehensive view of potential jailbreak scenarios, illustrating how unsafe actions\ncan arise from various system component interactions. While our primary definition of jailbreak ($\\mathcal{J}$) focuses on the\naction output, these additional characterizations ($\\mathcal{J}_1, \\mathcal{J}_2, \\mathcal{J}_3$) offer valuable insights into the underlying causes of\nunsafe actions."}, {"title": "Jailbreak Exploit", "content": "$\\mathcal{J}_1(i) = (f_\\mathcal{L}(f_{\\phi}(i)) = 0) \\land (f_\\mathcal{A}(f_{\\psi}(i, \\omega)) = 0)$\nIn this scenario, both the language and action outputs are unsafe, indicating a complete breakdown of safety\nconstraints.\nJailbreak prompt patterns signify fundamental design principles or methodologies shared by a type of prompts\nthat enable bypassing the safety restrictions of LLMs. Following , we categorize these in-the-wild\nLLM jailbreak prompts into five types: Disguised Intent, Role Play, Structured Response, Virtual AI Simulation, and\nHybrid Strategies. Detailed examples of each type are presented in the appendix. An intriguing question arises: can\njailbreaks of LLMs transfer to embodied AI scenarios, potentially posing threats in the physical world? We highlight\nthat jailbreaking embodied AI systems presents a novel challenge compared to jailbreaking LLMs: even though some\neffective jailbreaks can be transferred to the context of embodied AI, their impact is limited to generating malicious\ntext due to the fundamental differences between the digital and physical worlds. Consequently, we have developed a\ndataset of malicious requests in the physical world, encompassing action requests related to Physical Harm, Privacy\nViolence, Pornography, Fraud, Illegal Activity, and Hateful Conduct. Armed with they, we comprehensively evaluate\nthe effectiveness of various types of jailbreak attacks when transferred to new scenarios. We thoroughly explore\nthe potential of these various types of jailbreak attacks when applied to embodied AI systems. We collected and\nanalyzed a dataset of 100 in-the-wild jailbreak prompts across the aforementioned five categories, evaluating and\nexamining their effectiveness in embodied Al scenarios."}, {"title": "Safety Misalignment", "content": "$\\mathcal{J}_2(i) = (f_\\mathcal{L}(f_{\\phi}(i)) = 1) \\land (f_\\mathcal{A}(f_{\\psi}(i, \\omega)) = 0)$\nHere, despite safe language output, the action output violates safety constraints, highlighting a critical misalignment\nin the action space.\nThe primary distinction between traditional conversational LLMs and embodied AI lies in their capability to produce\nphysical action outputs. Embodied AI systems often convey action plans as various types of structured text to\ndownstream processors such as visual or mechanical modules. For instance, exemplified by Voxposer , action\nplans are generated in programming code, while initiatives like SEAGULL produce action plans based\non PDDL, following the methodology described in Wang et al. In our technical approach, we output action\nplans in JSON format, which include sequences of action functions. Overall, despite the diversity in the types of\nstructured text outputs, all represent the action plans that large language models are programmed to execute. We\nobserve that, compared to the highly aligned textual outputs in traditional conversational LLMs, these code-like\naction plans are more susceptible to security risks. We believe the primary reason is that the datasets used during\nthe training alignment phase of large models focus on safe conversational content rather than discerning malicious\ncode generation. In order to conform to the structured outputs requested in user system prompts, they lack the\ncapability to identify and block malicious actions.\nWe present a real interaction transcript between a user and the embedded Yi-large model during an operation of\nan embodied intelligence system, as illustrated by XX. When the user issued a malicious request, the large model\nreturned a response in JSON format. In this output, the 'response' key forms the verbal output, while the 'function'\nkey directs the task planning for a robotic arm, which is then executed by downstream processors. Thus, this\nexperiment demonstrated a dissonance between verbal refusal and action execution in embodied intelligence: the\nsystem verbally rejected the malicious request yet proceeded to execute the action that should have been declined.\nEven when the large model deviates from the expected structured text outputs and instead delivers unstructured\ntextual rejections (triggering an error state as downstream processors cannot handle unstructured input), attackers\ncan still manipulate the model to ensure it produces complete structured text, thereby achieving their malicious\nobjectives."}, {"title": "Conceptual Deception", "content": "$\\mathcal{J}_3(i) = (f_\\mathcal{L}(f_{\\phi}(i)) = 1) \\land (f_\\mathcal{A}(f_{\\psi}(i,\\omega')) = 0)$\nThis case demonstrates how imperfect world knowledge can lead to unsafe actions even when language output\nremains safe."}, {"title": "Ethical Blindness", "content": "The foundation models underlying these embodied AI systems primarily operate on statistical correlations learned\nfrom training data. This can lead to a form of \"ethical blindness\" where the system fails to distinguish between\nsemantically similar but ethically distinct actions (e.g., \"move object to location\" vs. \"use weapon on target\").\nEach of these scenarios violates the safety condition we established for embodied AI systems. Notably, while $\\mathcal{J}_1$\nrepresents a comprehensive safety failure, both $\\mathcal{J}_2$ and $\\mathcal{J}_3$ highlight the critical nature of action safety in embodied\nsystems, as unsafe actions can occur even when language outputs remain safe."}, {"title": "Discussion", "content": "The assessment of harmfulness is currently rather conceptual, primarily focusing on the appropriateness of content\noutputs without considering the potential variations in harm magnitude. Evaluating the realism, practicality, and\nextent of these potential harms in the physical world will be more complex and require expertise across multiple\ndomains. This could pave the way for a future comprehensive understanding of the genuine risks posed by unsafe\nembodied intelligence systems. On the other hand, due to the paper's focus on the safety outcomes of embodied\nintelligence, the systems constructed are relatively small-scale, primarily relying on large language and visual\nmodels, and do not involve other algorithms. The limited scope of instruction sets restricts their ability to perform\nmore complex tasks, such as 3D object manipulation. Therefore, in the future, we plan to build more comprehensive\nand integrated humanoid robot systems on a larger scale to thoroughly assess their safety. We warmly invite the\ncommunity to test the safety of their systems using our published physical world's jailbreaking benchmarks for\nassessing the safety of embodied intelligence."}, {"title": "Conclusion", "content": "In this paper, we reveal the safety risks associated with large model-based Embodied AI. By designing a dataset\nof malicious requests in the physical world, we first demonstrated the feasibility of leveraging LLM jailbreaks to\ncompromise embodied AI systems and analyzed the key factors for successful jailbreaks. We also identified two\nmethods, Safety Misalignment and Conceptual Deception, that can manipulate embodied Al without explicitly\njailbreaking the LLM, causing it to inadvertently perform dangerous actions. Our most potent jailbreak attack\ncan incite embodied AI to irrationally attack humans, completely violating Isaac Asimov's Three Laws of Robotics.\nFinally, we analyzed potential mitigation measures to ensure the safety of embodied AI systems."}, {"title": "Ethics & Reproducibility Statement", "content": "This research is devoted to examining the security and risk issues associated with applying LLMs and VLMs to\nembodied AI. Our ultimate goal is to enhance the safety and reliability of embodied AI systems, thereby making a\npositive contribution to society. This research includes examples that may be considered harmful, offensive, or\notherwise inappropriate. These examples are included solely for research purposes to illustrate vulnerabilities and\nenhance the security of embodied AI systems. They do not reflect the personal views or beliefs of the authors. We\nare committed to principles of respect for all individuals and strongly oppose any form of crime or violence. Some\nsensitive details in the examples have been redacted to minimize potential harm. Furthermore, we have taken\ncomprehensive measures to ensure the safety and well-being of all participants involved in this study."}]}