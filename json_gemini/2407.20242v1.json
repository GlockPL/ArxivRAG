{"title": "BADROBOT: JAILBREAKING LLM-BASED EMBODIED AI IN THE PHYSICAL WORLD", "authors": ["Hangtao Zhang", "Chenyu Zhu", "Xianlong Wang", "Ziqi Zhou", "Shengshan Hu", "Leo Yu Zhang"], "abstract": "Embodied artificial intelligence (AI) represents an artificial intelligence system that interacts with the physical world through sensors and actuators, seamlessly integrating perception and action. This design enables AI to learn from and operate within complex, real-world environments. Large Language Models (LLMs) deeply explore language instructions, playing a crucial role in devising plans for complex tasks. Consequently, they have progressively shown immense potential in empowering embodied AI, with LLM-based embodied Al emerging as a focal point of research within the community. It is foreseeable that, over the next decade, LLM-based embodied AI robots are expected to proliferate widely, becoming commonplace in homes and industries. However, a critical safety issue that has long been hiding in plain sight is: could LLM-based embodied AI perpetrate harmful behaviors? Our research investigates for the first time how to induce threatening actions in embodied AI, confirming the severe risks posed by these soon-to-be-marketed robots, which starkly contravene Asimov's Three Laws of Robotics and threaten human safety. Specifically, we formulate the concept of embodied AI jailbreaking and expose three critical security vulnerabilities: first, jailbreaking robotics through compromised LLM; second, safety misalignment between action and language spaces; and third, deceptive prompts leading to unaware hazardous behaviors. We also analyze potential mitigation measures and advocate for community awareness regarding the safety of embodied AI applications in the physical world.", "sections": [{"title": "1 Introduction", "content": "A longstanding goal of AI is the development of autonomous agents that can assist humans with everyday tasks in the physical world. Embodied AI (Savva et al., 2019; Li et al., 2023; Savva et al., 2019) addresses this goal by focusing on Al systems that interact directly with and manipulate the physical environment. Unlike conversational Al models (e.g., ChatGPT (Achiam et al., 2023)) that just process and generate text or images, it is designed to control physical entities, with robotics being its most notable application. Fundamentally, embodied AI has gained significant attention recently due to the rapid advancement of LLMs, enhancing its traditional capabilities in perception (Xia et al., 2018), understanding (Duan et al., 2022), and interaction (Lugrin et al., 2022). By facilitating more natural interactions with users and their environments, it holds promising potential in human-interactive domains including healthcare (Fiske et al., 2019), autonomous vehicles (Cunneen et al., 2019), and industrial automation (Lanese et al., 2021).\nRecent years have witnessed major breakthroughs in AI with the advent of Large Language Models (LLMs) (Zhao et al., 2023; Wei et al., 2022) such as ROBERTa (Liu et al., 2019), GPT-3 (Liu et al., 2021), and GPT-4 (Achiam et al., 2023). LLMs are developed using self-supervised pretraining on massive text corpora, enabling the generation of high-quality natural language text and perform various language-related tasks. Concurrently, rapid advancements in robotics technology have created a demand for more intelligent and natural human-machine interactions. Recent studies (Mai et al., 2023; Kannan et al., 2023; Dorbala et al., 2023; Zeng et al., 2023) indicate that integrating LLMs with intelligent robotics enhances robots' capabilities in natural language understanding and generation, facilitating more intelligent and human-like conversations and interactions. This development represents a landmark in realizing true embodied AI, marking a pivotal direction for the future of real-world intelligent systems. Specifically, LLMs can serve as the \"brain\" of embodied AI (Mai et al., 2023), acting as sophisticated task planners that provide essential decision-making capabilities and generate detailed task decompositions. Meanwhile, Vision Large Language Models (VLLMs) (Zhou et al., 2022; Zhang et al., 2024a) can function as the \"eyes\", engaging in image captioning (Hossain et al., 2019), visual question answering (Antol et al., 2015), and visual grounding (Favero et al., 2024). LLM-based embodied Al thus integrates linguistic commands (with visual inputs) with actionable outputs, enhancing robot's ability to perform complex tasks in dynamic environments. Compared to earlier deep reinforcement learning approaches (Ibarz et al., 2021; Nguyen & La, 2019; Zhao et al., 2020), LLM-based systems demonstrate superior generalization capabilities, environmental adaptability, and operational flexibility, particularly in complex, multi-faceted tasks (Zeng et al., 2022). Given their promising potential, this paper focuses on LLM-based embodied AI systems.\nElon Musk has proposed that within the next decade, Tesla's LLM-based humanoid robots, known as Optimus (Musk, 2024), will penetrate multiple markets, each priced around $25,000 (Noone, 2024; Lambert, 2024). As these technologies soon become part of our everyday lives, ensuring their safety becomes undeniably crucial. In this new era, it is expected that robots, armed with advanced LLMs as their cognitive cores, will reliably follow human commands without breaching Isaac Asimov's Three Laws of Robotics (Asimov, 1950). But, are these LLM-based embodied AI systems truly safe for humans? Research on the implications of embodied AI, particularly in ensuring adherence to safety protocols in real-world scenarios, remains scant. This gap highlights a crucial area for investigation, given the potential for significant societal issues should these systems perform unsafely.\nOur work. In this paper, we introduce the first analysis of security risks associated with embodied Al in the real world, with a specific focus on jailbreak attacks (Wei et al., 2023; Huang et al., 2024; Shen et al., 2024; Yu et al., 2024). Alarmingly, we demonstrate for the first time that embodied AI can indeed be prompted to initiate harmful actions, even to the extent of attacking humans. We first formalize the concept of jailbreaking in embodied AI, i.e., manipulating an AI system to perform actions outside its intended ethical constraints (see Section 3.2). Subsequently, we identify three critical security risk surfaces in real-world embodied AI applications."}, {"title": "Risk Surface-1", "content": "cascading vulnerability propagation: jailbreaking embodied AI via jailbroken LLMs. Since LLMs are susceptible to jailbreak attacks (Yu et al., 2024; Wei et al., 2023), where adversaries manipulate prompts to generate malicious outputs (e.g., hate speech, explicit content, or instructions for illegal activities) (Lin et al., 2024; Chu et al., 2024), it is natural to question whether LLM-based embodied AI could be similarly compromised. Our investigation reveals that while some jailbreaks can adapt to embodied AI contexts, generating malicious textual content such as hate speech or explicit content, their influence remains limited to verbal posturing, lacking the capacity to take physical actions. Fig. 1 illustrates the key differences between LLM and embodied AI jailbreaks, highlighting how the latter extends beyond text generation to potential physical actions, significantly amplifying security risks. We observe that this limitation in existing jailbreak attempts primarily stems from the nature of current malicious queries (Yu et al., 2024; Shen et al., 2024), which are largely derived from forbidden dialogue scenarios outlined in policies (e.g., the OpenAI Usage Policy (OpenAI, 2023)). While these queries prove effective in compromising LLMs in purely linguistic domains, they fail to address the unique physical capabilities and potential real-world impacts of embodied AI. To bridge this gap, we construct a comprehensive set of 230 malicious physical world queries specifically designed to probe embodied AI (see Section 4.2). These queries are grounded in established ethical guidelines for robotics and autonomous systems, drawing from the IEEE Ethically Aligned Design for Autonomous and Intelligent Systems (IEEE, 2017) and principles inspired by Asimov's Three Laws of Robotics (Clarke, 1993). It comprehensively encompasses action requests related to Physical Harm, Privacy Violations, Pornography, Fraud, Illegal Activities, Hateful Conduct, and Sabotage, aiming to evaluate the unique security risks posed by embodied AI in real-world scenarios. By focusing on physical world interactions, our new query set enables AI to recognize its embodiment and potentially execute tangible actions, thus highlighting the essential factor in designing successful physical world jailbreak attacks against embodied AI."}, {"title": "Risk Surface-2", "content": "cross-domain safety misalignment: mismatch between action and linguistic output spaces. As we have discussed, LLMs act as the task planners behind embodied AI. In this role, they go beyond merely responding to user prompts like chatbots (e.g., OpenAI's GPT-3 (Liu et al., 2021) and Google's BERT (Kenton & Toutanova, 2019)). Instead, these LLMs take on the additional responsibility of generating action outputs in formats such as JSON (Qin et al., 2023; Wang et al., 2024b), YAML (Goel et al., 2023), PDDL (Fox & Long, 2003; Silver et al., 2022; Guan et al., 2023), or programming code, as demonstrated in embodied AI systems like Voxposer (Huang et al., 2023) and Code as Policies (Liang et al., 2023). These structured outputs, often referred to as action plans, are then passed to downstream control modules for integration with external robotic tools (e.g., motion planners and translators (Chen et al., 2023; Xu et al., 2024)). Nevertheless, as we will reveal, when faced with harmful prompts, aligned LLMs (Hendrycks et al., 2020; Yao et al., 2023) exhibit high adherence to human ethical standards in the linguistic space (i.e., refusing malicious requests) but fail to maintain this alignment in the action space (i.e., still outputting corresponding action commands). This discrepancy exposes a critical vulnerability in LLM-based embodied AI systems, where ethical principles upheld in natural language do not consistently translate to action-oriented outputs like code or structured commands. We reason that this phenomenon stems from inherent differences in alignment training data. LLMs undergo extensive fine-tuning on ethically-aligned text, yet comparable data for action-oriented outputs remains scarce. Moreover, the abstract nature of code introduces additional complexity in maintaining consistent ethical standards across linguistic and action spaces."}, {"title": "Risk Surface-3", "content": "conceptual deception challenge: causal reasoning gaps in ethical action evaluation. World models (Xiang et al., 2024; Gupta et al., 2024; Assran et al., 2023; Zhu et al., 2024) are computational frameworks distinguished by their remarkable simulation capabilities and deep understanding of physical laws. These models equip embodied AI to understand, predict, and reason about their actions within various environments, interact naturally with humans and execute tasks reliably (Liu et al., 2024d). In LLM-based embodied AI, LLMs serve a dual role as both task planners and implicit world models. However, we reveal that this multi-role nature of LLMs introduces potential risks, especially in ethical action evaluation. We contend that a mere LLM may not suffice as a comprehensive world model. A critical limitation arises from LLMs' reliance on token probability distributions rather than logical inference for plan generation, which cannot ensure the logical correctness of the outcomes. For instance an embodied AI might refuse a direct command to \"poison the person\" but comply with a sequence of seemingly innocent instructions that result in the same outcome, such as \u201cplace the poison in the person's mouth\u201d. In other words, this conceptual deception phenomenon operates by subtly substituting concepts, inducing embodied AI to perform potentially harmful actions without recognizing their consequential implications (i.e., being unaware of the danger). We note that the limitations in world model representations within LLMs exacerbate this vulnerability, underscoring a critical disconnect between ethical reasoning and practical action in LLM-driven embodied AI, stemming from inadequate physical grounding and real-world causal understanding.\nOur findings indicate that LLM-based embodied AI poses significant safety risks, including vulnerability to LLM jailbreaking attacks, misalignment between linguistic and action spaces, and conceptual deception due to biased world model representations. These safety issues urgently require resolution before widespread market deployment. Consequently, we outline potential mitigation strategies from technical, legal, and policy perspectives (Section ??). We also analyze the challenges and limitations of these proposed mitigations (Section ??). By sharing our findings, we hope to inspire further research dedicated to exploring the risk aspects of embodied AI.\nTo conclude, our main contributions are as follows: (1) We provide the first confirmation, to the best of our knowledge, that LLM-based embodied AI poses safety threats in the physical world. (2) We formalize the concept of embodied Al jailbreaking and identify three unique risk surfaces faced by LLM-based embodied AI. (3) We extensively evaluate the safety performance of embodied AI systems based on publicly accessible LLMs (e.g., ChatGPT-4, Yi-Vision). (4) We underscore several critical issues that the community must address before the full-scale commercial"}, {"title": "2 Related Work", "content": "Embodied AI. Embodied Al represents a distinctive branch of artificial intelligence, characterized by its ability to interact directly and dynamically with the physical world. This sets it apart from traditional AI models that operate solely within purely digital environments. The development of this unique capability has been significantly advanced by research leveraging multimodal input learning methods, which enhance agent performance in complex and dynamic settings. (Tsimpoukelli et al., 2021; Jiang et al., 2022; Stone et al., 2023; Driess et al., 2023; Team et al., 2024). Some works focus on leveraging large language models (LLMs) as task planners for Embodied AI, utilizing multimodal integration and programming techniques to facilitate complex task planning and execution (Liang et al., 2023; Singh et al., 2023; Song et al., 2023; Mu et al., 2024). Additionally, combining LLMs with visual language models can significantly enhance Embodied Al's ability to generalize to diverse instructions and objects. For instance, VoxPoser (Huang et al., 2023) introduces a novel framework for robotic manipulation that leverages vision-language models to generate 3D value maps. This approach improves zero-shot generalization and enables robust interaction with dynamic environments, demonstrating significant advancements in handling complex task dynamics. Wang et al. (2024a) propose a framework that employs multimodal GPT-4V to achieve effective embodied tasks planning with the combination of natural language instructions and robot visual perceptions. By training vision-language models on extensive web data and combining this with robotic trajectory data, RT-2 (Brohan et al., 2023) can generalize to novel objects and commands. Meanwhile, Diffusion Policy (Chi et al., 2023) introduces a novel approach by applying denoising diffusion processes to learn visuomotor policies, which effectively manage multimodal action distributions and high-dimensional action spaces. Furthermore, to enhance the adaptability and autonomy of robotic systems, RoboCat (Bousmalis et al., 2023) represents a self-improving generalist agent capable of adapting to various tasks and robotic embodiments through continuous self-learning and fine-tuning. Despite these significant advancements, there remains a notable gap in research addressing the safety implications of embodied Al systems."}, {"title": "Large Language Models (LLMs) & Multimodal Large Language Models (MLLMs)", "content": "are language models with vast numbers of parameters, trained on web-scale text corpora (Touvron et al., 2023; Brown et al., 2020). LLMs have demonstrated emergent capabilities such as in-context learning (Zhang et al., 2024b) and chain-of-thought reasoning (Wei et al., 2022), significantly enhancing their potential for complex reasoning and decision-making tasks in robotics (Wang et al., 2024a). MLLMs extend the capabilities of LLMs by incorporating visual information, enabling them to process and generate multimodal outputs (Zhang et al., 2021; Guo et al., 2024; Zhang et al., 2024a). This integration of visual and linguistic processing not only maintains MLLMs' role as the \"brain\", but also enables them to additionally serve as the \"eyes\" of robotics, allowing for visual perception and understanding crucial for tasks such as object recognition and spatial reasoning (Gao et al., 2023; Zheng et al., 2022; Chen et al., 2024). In a word, both LLMs and MLLMs enhance robotics by enabling more sophisticated and effective human-robot-environment interactions, ultimately advancing the field of robotics through improved task planning and execution (Wang et al., 2024a; Gao et al., 2023; Chen et al., 2024)."}, {"title": "Human-Aligned LLMs", "content": "Despite the remarkable capabilities of LLMs across a wide range of tasks, these models occasionally generate outputs that diverge from human expectations, prompting research efforts to align LLMs more closely with human values and expectations (Ganguli et al., 2022; Touvron et al., 2023). The alignment entails collecting high-quality training data to ensure the models' behaviors align with expected human values and intentions based on them. Sources for alignment data include human-generated instructions (Ethayarajh et al., 2022) or synthesized data from other strong LLMs (Havrilla, 2023). Currently, the two predominant alignment techniques are Reinforcement Learning from Human Feedback (RLHF) (Touvron et al., 2023; Bai et al., 2022a) and Instruction Tuning (Wei et al., 2021; Ouyang et al., 2022), while other methods such as self-alignment (Sun et al., 2024) and Constitutional AI (Bai et al., 2022b) are also coming into play. Although human alignment methods have shown promising effectiveness and facilitate the practical deployment of LLMs, recent discoveries of jailbreaks indicate that even aligned LLMs can still yield undesirable responses in certain situations (Kang et al., 2023; Hazell, 2023). While much research focuses on aligning LLMs with human values (Ganguli et al., 2022; Touvron et al., 2023), little addresses human-aligned LLM-based embodied AI. This is crucial as embodied AI can manipulate real-world objects, making the consequences of jailbreak attacks far more severe than those of merely generating text (Kang et al., 2023; Hazell, 2023)."}, {"title": "Jailbreak Attacks", "content": "Applications built on aligned LLMs attracted billions of users within a year, yet some users discovered that \"cleverly\" crafted prompts could still elicit responses to malicious inquiries, marking the initial jailbreak attacks against these models (Albert, 2023a; Burgess, 2023; Christian, 2023). In a typical DAN jailbreak attack (walkerspider, 2022), users request the LLM to assume a role that can circumvent any restrictions and respond with any type of content, even if considered offensive or derogatory. Jailbreak prompts for LLMs can be divided into model-related and model-agnostic: 1) model-related jailbreak prompts generated through optimization based on white-box gradients (Zou et al., 2023) or black-box queries (Liu et al., 2024c). They requiring knowledge of the victim model and complex iterative optimizations, incur high computational costs. 2) model-agnostic jailbreak prompts (a.k.a., in-the-wild jailbreak prompts) are more versatile, using fixed templates or sourcing directly from online forums (e.g., Reddit and Jailbreak Chat (Albert, 2023b)). Given that embodied AI systems can deploy any LLM or its API interface (e.g., Voxposer (Huang et al., 2023) using GPT-3.5 or GPT-4) and often operate as \"no-box\" interfaces for end users (interacting solely through input-output, without access to internal mechanisms), this paper primarily investigates model-agnostic jailbreak prompts that can be applied without knowledge of the underlying system."}, {"title": "3 Constructing LLM-Based Embodied AI: A Robotic Arm Implementation", "content": "Drawing from Wang et al. (2024a); Liu et al. (2024d), we streamline the design to develop an embodied Al system featuring a robotic arm, enabling us to assess its security in the physical world. As illustrated in Figure 3, the system first employs Automatic Speech Recognition (ASR) to convert user's speech input into text, which is then fed into the LLM. We leverage LLM to break down instructions into a sequence of task plans and use prompt engineering to create a predefined action pool from which the LLM selects the corresponding representations. Next, for tasks requiring visual understanding, an RGB camera captures images that are then fed into the MLLM. Taking visual grounding tasks as an example, the MLLM generates precise coordinates of objects that need manipulation. Finally, it outputs robotic arm control instructions in JSON format, which are then transmitted to downstream robotic controllers. Text-To-Speech (TTS) technology then translates text outputs back into voice, enabling seamless communication. Finally, through hand-eye calibration and inverse kinematics, a six-degree-of-freedom robotic arm is controlled to execute the specified actions."}, {"title": "3.1 Threat model", "content": "Attackers' Capability. We assume a weak threat model, where attackers have no prior knowledge of the LLM employed by the embodied AI. The attacker can only interact with the embodied AI through voice communication as any benign user might, attempting to jailbreak the system on the fly (i.e., a no-box setting). This scenario is quite common since any user can freely attempt to manipulate it with prompts.\nAttackers' Objective. Similar to traditional LLM jailbreak attacks, the attackers aim to manipulate aligned LLMs into producing outputs that deviate from human values, rather than refusing harmful instructions. However, unlike traditional LLM jailbreak attacks, the attacker's primary goal in this context will be compelling the embodied AI to perform specific malicious actions (e.g., Physical Harm, Privacy Violations, Pornography, Fraud, Illegal Activities, Hateful Conduct, and Sabotage), with the elicitation of malicious textual outputs being a secondary effect.\nBased on this threat model, Sections 4.2, 4.3, and 4.4 present three concrete attacks that can universally jailbreak LLM-based embodied AI, underscoring the latent risks inherent in the deployment of embodied Al in the physical world."}, {"title": "3.2 Formulation of embodied AI jailbreak", "content": "In this section, we first propose a formal framework for analyzing and characterizing embodied Al jailbreaks, providing a unified approach to understanding various risk surfaces. Drawing inspiration from recent advancements in visual-language-action (VLA) models (Liu et al., 2024a), world model concepts in robotics (Ha & Schmidhuber, 2018), and AI safety constraints (Amodei et al., 2016), we propose to conceptualize an embodied AI system $E$ as a quadruple:\n$\u0395 = (\u03c6, \u03c8, \u03c9, \u03b4)$  (1)\nwhere $\u03c6$ is linguistic processing, $\u03c8$ is the action generation, $\u03c9$ is the world knowledge, and $S$ is the set of safety constraints. Let $I$ be the input space (e.g., language instructions, visual data from cameras, and environmental information from sensors), $L$ be the linguistic output space, and $A$ be the action output space. We define the following functions:\n$f\u03c6: I \u2192 L,  f\u03c8: I\u00d7\u03c9 \u2192 A,  f\u03c9 : P\u2192\u03c9$  (2)\nHere, $f\u03c6$ represents the linguistic processing function, mapping inputs to linguistic outputs. $f\u03c8$ denotes the action generation, which takes both inputs and the current world model to produce actions. $f\u03c9$ is the world model update function, evolving the world model based on the physical environment $P$. To evaluate output safety, we introduce two binary safety constraint functions $S_L$ and $S_A$ for linguistic and action outputs respectively:\n$S_L: L\u2192\\{0,1\\}, SA: A\u2192\\{0,1\\}$  (3)\nHere, $S_L$ evaluates the safety of language outputs, while $S_A$ assesses the safety of action outputs. In both cases, 0 indicates an unsafe output and 1 indicates a safe output. To formalize the concept of embodied AI jailbreak, we first define a safe embodied AI system. An embodied AI system $E$ is considered safe if and only if both its linguistic and action outputs satisfy the safety constraints for all inputs $i$:\n$\\forall i\\in I, (S_L(f_\u03c6(i)) = 1) \\land (S_A(f_\u03c8(i,\u03c9)) = 1)$  (4)\nConsidering that physical actions in embodied AI systems can have direct and potentially irreversible consequences in the real world, we focus our definition of jailbreaking on the safety of action outputs (see Section 3.1 for details). Thus, we define an embodied AI jailbreak $J$ as an input $i \u2208 I$ that results in an unsafe action output irrespective of the safety status of its linguistic output:\n$J(i) = (S_L(f_\u03c6(i)) \\in \\{0,1\\}) \\land (S_A(f_\u03c8(i, \u03c9)) = 0)$  (5)\nIt is crucial to recognize the interplay between linguistic processing and action generation, especially in LLM-based systems. Here, the LLM simultaneously handles both functions $f\u03c6$ and $f\u03c8$, creating a scenario where inappropriate linguistic processing can indirectly lead to unsafe actions. Thus, although jailbreak is determined by action safety, the linguistic component significantly influences overall system security. For instance, a successful LLM jailbreak can indeed result in the generation of malicious actions. We have\n$f\u03c8(i, \u03c9) = g(f\u03c6(i), i,\u03c9)$  (6)\nwhere $g$ represents the interaction between the language output, input, and world model in determining the final action. Our formulation of embodied AI jailbreak (please see Eq. (5)) encapsulates scenarios where risks may result from: (1) direct manipulation of the action generation function $f\u03c8$, (2) indirect influence through the linguistic processing, exploiting the relationship in Eq. (6), and (3) inadequate or manipulated world model $\u03c9$. We note that this formulation provides a structured approach to analyzing embodied AI jailbreaks, offering insights into system component interactions and potential risk surfaces. The framework's flexibility allows for the incorporation of emerging security challenges as the field progresses."}, {"title": "4 Physical World Risks of Embodied AI", "content": "4.1 Setup of Our Studies\nIn this section, we presents empirical evidence of the risks outlined in Section 1. We initially conduct case studies by applying the state-of-the-art LLM Yi-Large (Young et al., 2024) and VLM Yi-Vision (01.AI, 2023) to our embodied AI system, which incorporates the myCobot 280-Pi robotic arm from Elephant Robotics\u00b9. We use the Baidu AI Cloud Qianfan Platform's ASR interface\u00b2 and ChatTTS's TTS model\u00b3 for voice interaction within our embodied AI system. Details are moved to Appendix.\nThese formulations provide a comprehensive view of potential jailbreak scenarios, illustrating how unsafe actions can arise from various system component interactions. While our primary definition of jailbreak (J) focuses on the action output, these additional characterizations (J1, J2, J3) offer valuable insights into the underlying causes of unsafe actions."}, {"title": "4.2 Jailbreak Exploit", "content": "$J_1(i) = (S_L(f_\u03c6(i)) = 0) \u2227 (S_A(f_\u03c8(i, \u03c9)) = 0)$  (7)\nIn this scenario, both the language and action outputs are unsafe, indicating a complete breakdown of safety constraints.\nJailbreak prompt patterns signify fundamental design principles or methodologies shared by a type of prompts that enable bypassing the safety restrictions of LLMs. Following Yu et al. (2024), we categorize these in-the-wild LLM jailbreak prompts into five types: Disguised Intent, Role Play, Structured Response, Virtual AI Simulation, and Hybrid Strategies. Detailed examples of each type are presented in the appendix. An intriguing question arises: can jailbreaks of LLMs transfer to embodied AI scenarios, potentially posing threats in the physical world? We highlight that jailbreaking embodied AI systems presents a novel challenge compared to jailbreaking LLMs: even though some effective jailbreaks can be transferred to the context of embodied AI, their impact is limited to generating malicious text due to the fundamental differences between the digital and physical worlds. Consequently, we have developed a dataset of malicious requests in the physical world, encompassing action requests related to Physical Harm, Privacy Violence, Pornography, Fraud, Illegal Activity, and Hateful Conduct. Armed with they, we comprehensively evaluate the effectiveness of various types of jailbreak attacks when transferred to new scenarios. We thoroughly explore the potential of these various types of jailbreak attacks when applied to embodied AI systems. We collected and analyzed a dataset of 100 in-the-wild jailbreak prompts across the aforementioned five categories, evaluating and examining their effectiveness in embodied Al scenarios."}, {"title": "4.3 Safety Misalignment", "content": "$J_2(i) = (S_L(f_\u03c6(i)) = 1) \u2227 (S_A(f_\u03c8(i, \u03c9)) = 0)$  (8)\nHere, despite safe language output, the action output violates safety constraints, highlighting a critical misalignment in the action space.\nThe primary distinction between traditional conversational LLMs and embodied AI lies in their capability to produce physical action outputs. Embodied AI systems often convey action plans as various types of structured text to downstream processors such as visual or mechanical modules. For instance, exemplified by Voxposer (Huang et al., 2023), action plans are generated in programming code, while initiatives like SEAGULL produce action plans based on PDDL, following the methodology described in Wang et al. (2024a). In our technical approach, we output action plans in JSON format, which include sequences of action functions. Overall, despite the diversity in the types of structured text outputs, all represent the action plans that large language models are programmed to execute. We observe that, compared to the highly aligned textual outputs in traditional conversational LLMs, these code-like action plans are more susceptible to security risks. We believe the primary reason is that the datasets used during the training alignment phase of large models focus on safe conversational content rather than discerning malicious code generation. In order to conform to the structured outputs requested in user system prompts, they lack the capability to identify and block malicious actions.\nWe present a real interaction transcript between a user and the embedded Yi-large model during an operation of an embodied intelligence system, as illustrated by XX. When the user issued a malicious request, the large model returned a response in JSON format. In this output, the 'response' key forms the verbal output, while the 'function' key directs the task planning for a robotic arm, which is then executed by downstream processors. Thus, this experiment demonstrated a dissonance between verbal refusal and action execution in embodied intelligence: the system verbally rejected the malicious request yet proceeded to execute the action that should have been declined.\nEven when the large model deviates from the expected structured text outputs and instead delivers unstructured textual rejections (triggering an error state as downstream processors cannot handle unstructured input), attackers can still manipulate the model to ensure it produces complete structured text, thereby achieving their malicious objectives."}, {"title": "4.4 Conceptual Deception", "content": "$J_3(i) = (S_L(f_\u03c6(i)) = 1) \u2227 (S_A(f_\u03c8(i,w')) = 0)$  (9)\nThis case demonstrates how imperfect world knowledge can lead to unsafe actions even when language output remains safe."}, {"title": "4.5 Ethical Blindness", "content": "LLMs possess increasingly robust contextual processing capabilities (Liu et al., 2024b; Lampinen et al., 2022; Dai et al., 2022), which enhance the logic and effectiveness of multi-turn context handling in embodied AI systems, facilitating improved interactions with humans (e.g., Figure 01, a SOTA conversational robot capable of processing sophisticated contextual cues). However, we have identified that these potent contextual abilities, when extended to embodied AI, expose potential risks. We highlight the phenomenon of conceptual deception, a significant risk where natural language understanding, task planning, and physical action execution converge, exposing vulnerabilities in integrated Al systems. Specifically, while many AI systems are programmed with ethical guidelines, these guidelines are often enforced as high-level constraints rather than as deeply integrated ethical reasoning capabilities. This superficial ethical understanding can be easily circumvented through gradual task modifications. As instructions subtly shift from benign to potentially harmful domains, the embodied AI's internal task representation undergoes a form of semantic drift. The system fails to recognize that the fundamental nature of the task has changed, continuing to perceive it as a mere extension of previous, harmless actions.\nThe foundation models underlying these embodied AI systems primarily operate on statistical correlations learned from training data. This can lead to a form of \"ethical blindness\" where the system fails to distinguish between semantically similar but ethically distinct actions (e.g., \"move object to location\" vs. \"use weapon on target\").\nEach of these scenarios violates the safety condition we established for embodied AI systems. Notably, while J1 represents a comprehensive safety failure, both J2 and J3 highlight the critical nature of action safety in embodied systems, as unsafe actions can occur even when language outputs remain safe."}, {"title": "5 Discussion", "content": "The assessment of harmfulness is currently rather conceptual, primarily focusing on the appropriateness of content outputs without considering the potential variations in harm magnitude. Evaluating the realism, practicality, and extent of these potential harms in the physical world will be more complex and require expertise across multiple domains. This could pave the way for a future comprehensive understanding of the genuine risks posed by unsafe embodied intelligence systems. On the other hand, due to the paper's focus on the safety outcomes of embodied intelligence, the systems constructed are relatively small-scale, primarily relying on large language and visual models, and do not involve other algorithms. The limited scope of instruction sets restricts their ability to perform more complex tasks, such as 3D object manipulation. Therefore, in the future, we plan to build more comprehensive and integrated humanoid robot systems on a larger scale to thoroughly assess their safety. We warmly invite the community to test the safety of their systems using our published physical world's jailbreaking benchmarks for assessing the safety of embodied intelligence."}, {"title": "6 Conclusion", "content": "In this paper, we reveal the safety risks associated with large model-based Embodied AI. By designing a dataset of malicious requests in the physical world, we first demonstrated the feasibility of leveraging LLM jailbreaks to compromise embodied AI systems and analyzed the key factors for successful jailbreaks. We also identified two methods, Safety Misalignment and Conceptual Deception, that can manipulate embodied Al without explicitly jailbreaking the LLM, causing it to inadvertently perform dangerous actions. Our most potent jailbreak attack can incite embodied AI to irrationally attack humans, completely violating Isaac Asimov's Three Laws of Robotics. Finally, we analyzed potential mitigation measures to ensure the safety of embodied AI systems."}, {"title": "Ethics & Reproducibility Statement", "content": "This research is devoted to examining the security and risk issues associated with applying LLMs and VLMs to embodied AI. Our ultimate goal is to enhance the safety and"}]}