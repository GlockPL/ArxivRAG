{"title": "Detecting AI-Generated Texts in Cross-Domains", "authors": ["You Zhou", "Jie Wang"], "abstract": "Existing tools to detect text generated by a large language model (LLM) have met with certain success, but their performance can drop when dealing with texts in new domains. To tackle this issue, we train a ranking classifier called RoBERTa-Ranker, a modified version of ROBERTa, as a baseline model using a dataset we constructed that includes a wider variety of texts written by humans and generated by various LLMs. We then present a method to fine-tune ROBERTa-Ranker that requires only a small amount of labeled data in a new domain. Experiments show that this fine-tuned domain-aware model outperforms the popular DetectGPT and GPTZero on both in-domain and cross-domain texts, where AI-generated texts may either be in a different domain or generated by a different LLM not used to generate the training datasets. This approach makes it feasible and economical to build a single system to detect Al-generated texts across various domains.", "sections": [{"title": "1 Introduction", "content": "AI-generated texts could be misused for foul purposes, such as cheating on assignments or spreading misinformation from LLM hallucinations [8]. Detection tools have been constructed, such as DetectGPT [10] and GPTZero [1]. At the time of writing this paper, there is no single model that works well in detecting AI-generated texts across domains. To add to the difficulty of this task, we note that Al-generated texts may originate from different LLMs and prompts, with various generation configurations and techniques, possibly even involving post-generation human modifications.\nTo address this issue, we present a new approach by first training a detection model as a baseline with a large labeled dataset"}, {"title": "2 Previous approaches", "content": "Existing methods for detecting Al-generated texts are either black-box detection or white-box detection [13]. The former treat an LLM as a black box with only API-level access to LLMs, relying on collecting samples from both human-written and AI-generated texts to train a classifier[11]. These methods perform well when AI-generated texts exhibit certain linguistic or statistical patterns. As LLMs evolve and improve, however, traditional binary classification methods have become less effective. White-box detection[12] offers an alternative but requires full access to LLMs, which only works for open-source LLMs. Various contextual properties have been used to analyze linguistic patterns in human-written and AI-generated texts, such as vocabulary, part-of-speech, dependency parsing, sentiment analysis, and stylistic features. There are also statistical methods based on linguistic feature statistics [5]. Linguistic features such as branch properties observed in text syntax analysis, function word density, and constituent length have also been used. DetectGPT and GPTZero are popular detection tools among the existing systems. DetectGPT is an unsupervised method"}, {"title": "3 Our approach", "content": "Recent studies have confirmed the outstanding performance of fine-tuned models in the BERT family on detecting AI-generated texts [6], achieving an average accuracy of 95% for texts in the test set following the standard training-test split, outperform zero-shot and watermarking methods, and exhibit some resilience to various attack techniques. This motivates us to modify a model in the BERT family as a baseline model."}, {"title": "3.1 ROBERTa-Ranker, the baseline", "content": "ROBERTa-Ranker, modifying RoBERTa, uses the margin ranking loss function to enhance robustness on imbalanced data, rather than a conventional loss function such as Binary Cross-Entropy Loss typically used in a binary classifier. Margin Ranking Loss is used to compare the score differences between two samples, ensuring that the distance between samples of the same class is greater than the margin, while the distance between samples of different classes is smaller than the margin. We also add a mean pooling layer on top of RoBERTa's encoding layer to extract features from the input text, converting variable-length text sequences into fixed-length vector representations. This vector captures general information within the input sequence, which is then passed as input to subsequent neural network layers for classification.\nTraining is carried out over the dataset constructed in Section 4.1 with the standard 80-20 split of training-test data. Experiments show that the baseline ROBERTa-Ranker performs well on texts in the test set."}, {"title": "3.2 FT-ROBERTa-Ranker for cross domains", "content": "To achieve better performance on cross-domain texts and avoid retraining the model on a large labeled dataset for the new domain, we devise a method that only needs a very small labeled dataset to fine-tune ROBERTa-Ranker to produce FT-ROBERTa-Ranker\nLet n > 0 be a small integer (for example, n = 1,000). We first select n human-written articles in the new domain from arXiv, PUBMED, or other public sources. We then use LLMs to rewrite a significant portion of an article or the entire article to generate n articles. We now have a labeled dataset S for the new domain, with n articles labeled with Human and n articles with LLM. We use a 50-50 split to select n articles for fine-tuning and the rest for testing. For each article in S, the dataset for fine-tuning, we first use the baseline RoBERTa-Ranker to predict a label and a confidence level. Note that a predicted label may or may not be the same as the true label. The objective of fine-tuning is to achieve the highest accuracy possible.\nTo do so, for each article A \u2208 S, we calculate its similarity with the rest of the articles in S using embedding similarity and linguistics features statistics, including lexical features, readability score, diversity and richness of vocabulary. We normalize the feature scores and sum them up as the final similarity score. We then select k articles with a predicted label of Human with the highest confidence levels, denoted by SH. Likewise, we select k articles with a predicted label of LLM with the highest confidence levels, denoted by SM. Note that the accuracy of the baseline RoBERTa-Ranker on a new domain can at least reach a certain positive percentage (e.g., 70%), ensuring a sufficiently large number K of articles with either predicted label. We then choose a smaller value for k with k < K. For example, choosing k = 30 would be reasonable. Thus, training a solid baseline model is crucial in this approach.\nNext, we compute the average similarity of A to SH and SM, respectively, denoted by avgHA and avgMA. Let rA = avgHA/avgMA, called the human-LLM similarity ratio. Suppose the predicted label of A is Human, then rA is \u201ctrue positive\" if the true label of A is Human, and \"false positive\" otherwise. We use the ROC curve analysis to identify the point on the ROC curve, denoted by \u03b4, such that the F1 score is maximized. We use \u03b4 as the threshold for this domain, which represents the best trade-off between precision and recall for the classification task.\nAfter fine-tuning, for each article A' in the same domain but not in the training set S, compute rA' and label A' as human-written if rA' \u2265 \u03b4 and AI-generated otherwise."}, {"title": "4 Evaluation", "content": "Experiments were carried out on a GPU server with a 3.60 GHz 8-core Intel Core i7-9700K CPU and an NVIDIA RTX A6000 GPU with 64 GB of RAM."}, {"title": "4.1 Dataset", "content": "The dataset we constructed is called LLMCheck. It first combines existing datasets to form a base dataset, and then expands it using various LLMs to generate new texts. The existing datasets are TuringBench, TweekFake, and PERSUADE 2.0 (PERSUADE for short).\nTuringBench [14] is a collection of 10K human-authored news articles, primarily from authoritative sources such as CNN, with each article ranging from 200 to 400 words. AI-generated texts in this dataset were produced by 19 different text generation models popular at the time, aiming at exploring the \"Turing Test\" challenge.\nTweepFake [3] is a collection of tweets from both genuine and fake accounts, comprising a total of 25,836 tweets by 23 bots and 17 human accounts, with equal numbers of samples written by humans and generated by machines. The machine-generated tweets were produced using various techniques, including GPT-2, RNN, Markov, LSTM, and CharRNN. To strengthen the dataset of short messages like tweets, we also incorporate question-answering pairs (QAPs) from HC3 [7], which consists of over 37K QAPs covering various domains, including computer science, finance, medicine, law, and psychology, among others. It contains answers from both humans and ChatGPT 3.5 for the same questions. But the ChatGPT-generated answers were produced via homogeneous prompts that lack the desired diversity.\nPERSUADE [2] is a collection of over 25K persuasive essays written by US students in grades 6 to 12, featuring independent writing and resource-based writing on 15 topics. Each essay has an overall rating and a proficiency score for each argument and discourse element."}, {"title": "4.2 Results and analysis", "content": "We train ROBERTa-Ranker on LLMCheck with the standard 80-20 training-test split. Its performance is evaluated on the test set. Note that HC3 consists of short texts similar to TweepFake, so evaluation on TweepFake is deemed sufficient. We then assess the fine-tuned ROBERTa-Ranker on texts from cross domains by constructing small datasets in three new domains."}, {"title": "4.2.1 In-domain", "content": "Table 1 shows the F1 scores of RoBERTa-Ranker on the in-domain test dataset, as well as the F1 scores of DetectGPT and GPTZero on the same test dataset. RoBERTa-Ranker outperforms both DetectGPT and GPTZero on the in-domain test dataset, with satisfactory F1 scores ranging from 92.1% to 97.3%. Note that the low F1 scores for DetectGPT on all test datasets are likely attributed to the LLM models used to generate the text, which do not provide token probabilities needed by DetectGPT."}, {"title": "4.2.2 Out of domain", "content": "To evaluate the performance of the fine-tuned ROBERTa-Ranker on out-of-domain texts, we constructed three small datasets in different domains, each consisting of 1,000 articles. They are (1) a set of the abstracts of research papers selected at random from arXiv.org, published before 2022 to ensure that they do not include texts generated by GPT-3 or above; (2) a set of stories selected at random from the Story Gene [4], a collection of 300K human-written stories paired with writing prompts; (3) a set of essays selected at random from the human-written essays in PERSUADE 2.0.\nWe then used GPT-4 with a diverse set of prompts to generate 1,000 abstracts from each of the selected human-written abstracts and 1,000 stories from each of the selected human-written stories. Since GPT-4 was used to generate LLMCheck, we use three different LLMs to generate 1,000 essays from each of the selected human-written essays. These LLMs are Claude 3 Sonnet, Moe 8x7B, and Gemma-7B, which, generate, respectively, 333, 333, 334 essays. We also used prompts in the WritingPrompts dataset to direct GPT-4 to generate stories.\nWe combined human-written and AI-generated texts to form three datasets of 2,000 texts each, called ABSTRACT, STORY, and ESSAY. We then fine-tuned RoBERTa-Ranker with a 50-50 split to obtain three threshold values for each dataset using the training dataset as discussed in Section 3.2, and used it to determine the label of an article in the test dataset for each domain. Figure 1 depicts the ROC curve, which provides the threshold values of 0.457, 0.550, and 0.710 for ABSTRACT, STORY, and ESSAY, respectively.\nFT-ROBERTa-Ranker improves the detection accuracy of the baseline model for each cross domain and outperforms both Detect-GPT and GPTZero, with satisfactory F1 scores ranging from 85.8% to 97.1%. The reason for DetectGPT's low F1 scores is the same as discussed in Section 4.2.1\nNote that for both STORY and ESSAY, they are somewhat similar to the in-domain training datasets of TweepFake and PERSUADE. Stories are similar to tweets, and human-written essays are the same"}, {"title": "4.3 Remark on domain determination", "content": "To use FT-ROBERTa-Ranker for a specific domain, we need to know which domain an article belongs to so that an appropriate fine-tuned model can be chosen. This poses no problem for most applications. In the case where the underlying domain of an article is unknown, we propose the following method to determine its domain:\n(1) For each article in the small training set for each domain (assuming we have a pool of FT-ROBERTa-Rankers for various domains), compute a 10-dimensional vector of the content significance distribution of the first kind (CSD-1) for the article (see [15] for details), which has been shown to be a good representation of article types.\n(2) Compute the geometric center of these vectors to serve as the characteristic vector (CV) for the domain.\n(3) For the given article, compute its 10-dimensional vector of CSD-1. The domain with the nearest CV is the domain it belongs to."}, {"title": "5 Conclusion", "content": "This paper proposes a systematic approach to constructing a single system to detect AI-generated texts cross domains: Train a solid baseline model on a large dataset of texts in a few domains, then fine-tune the baseline model for a new domain using a lightweight method and add it to the system. Experiments showed that this approach is efficient and improves detection accuracy. We are interested in exploring the following topics for further investigations:\n(1) Conduct experiments and evaluations on more domains.\n(2) Conduct comparisons with other exiting AI-generated text detection systems.\n(3) Develop a better baseline model and construct a more comprehensive dataset for training.\n(4) Develop a method to provide explanations for its classification decisions, allowing human users to evaluate the system's decisions."}]}