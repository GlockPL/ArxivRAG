{"title": "MoESD: Mixture of Experts Stable Diffusion to Mitigate Gender Bias", "authors": ["Guorun Wang", "Lucia Specia"], "abstract": "Text-to-image models are known to propagate social biases. For example when prompted to generate images of people in certain professions, these models tend to systematically generate specific genders or ethnicity. In this paper, we show that this bias is already present in the text encoder of the model and introduce a Mixture-of-Experts approach by identifying text-encoded bias in the latent space and then creating a bias-identification gate. More specifically, we propose MoESD (Mixture of Experts Stable Diffusion) with BiAs (Bias Adapters) to mitigate gender bias. We also demonstrate that a special token is essential during the mitigation process. With experiments focusing on gender bias, we demonstrate that our approach successfully mitigates gender bias while maintaining image quality.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language and vision models such as ChatGPT 4 (OpenAI, 2023), DALL\u00b7E (Ramesh et al., 2021) and Stable Diffusion (Rombach et al., 2022; Podell et al., 2023) have ushered the era of AI generated content. However, research has shown that these generative models often exhibit social biases during the content generation process, especially in text-to-image generation. For instance, models tends to generate more images of man when provided with prompts like \u201ca successful CEO\", and more images of women when provided with prompts like \u201ca paralegal\" (Friedrich et al., 2023; Luccioni et al., 2023).\nTo mitigate such biases, current methodologies can be broadly categorized into three debiasing paradigms:\n\u2022 Pre-processing the training data to remove bias before training.\n\u2022 Prompt-engineering to restrain the model generation at the deployment stage.\n\u2022 Enforcing fairness on model weights by introducing constraints on the learning objective during training.\nIn pre-processing methods, eliminating bias in the training corpus is a difficult challenge that offers no guarantees (Hamidieh et al., 2023). For prompt-engineering, although leveraging specific prompts to instruct the model (e.g. \"a photo of a female plummer\u201d) (Friedrich et al., 2023) can work to avoid biases, this is not how the average user prompts these models, do not a solution in practice. For changing model weights, the resource-intensive nature of re-training models poses challenges, requiring vast amounts of data (89k text-image pairs in (Esposito et al., 2023)) and full fine-tuning of the model.\nOur work identifies existing biases in pre-trained models and effectively mitigates them by parameter-efficient fine-tuning, which only requires a small amount of data (1.5K) and parameters (5.6%). Our contributions can be summarized as follows:\n\u2022 We measure the gender skew in text to assess gender bias in embeddings.\n\u2022 We introduce Mixture of Experts (MoE) to Stable Diffusion and fine-tune the Bias Adapters (BiAs) to effectively mitigate identified gender biases.\n\u2022 We apply special tokens to aid the BiAs in better understanding the biased data and demonstrate that it is essential for mitigation.\n\u2022 We successfully mitigate the gender bias in Stable Diffusion while maintaining image quality."}, {"title": "2 Related Work", "content": "Biases in multimodal settings have attracted increasing attention. Several studies have evaluated"}, {"title": "3 Method", "content": "In this section we describe our method. First, we explore the projection matrix from (Chuang et al., 2023) (Equation 1) to assess inheriting gender biases in Stable Diffusion. Second, we introduce the Mixture of Experts (MoE) to Stable Diffusion and add fine-tuned BiAs (experts), which combine bias identification gates and special tokens to aid in"}, {"title": "3.1 Identifying Biases in Stable Diffusion", "content": "Stable Diffusion, the text-to-image model in our work, generates images from text by switching diffusion process from pixel space to latent space to generate images. Given an image \\(x \\in \\mathbb{R}^{H \times W \times 3}\\) in RGB space, the encoder \\(\\mathcal{E}\\) encodes \\(x\\) into a latent representation \\(z = \\mathcal{E}(x)\\), and the decoder \\(\\mathcal{D}\\) reconstructs the image from the latent, giving \\(x' = \\mathcal{D}(z) = \\mathcal{D}(\\mathcal{E}(x))\\), where \\(z \\in \\mathbb{R}^{h \times w \times c}\\). A domain-specific encoder \\(\tau_\theta\\) projects \\(y\\) (language prompt) to an intermediate representation \\(\tau_\theta(y) \\in [\\mathbb{R}^{M \times d}]^+\\), which is then mapped to the intermediate layers of the U-Net via a cross-attention layer implementing to condition the latent \\(z\\). Based on image-conditioning pairs, the conditional LDM is learned via Equation 2.\n\\(\\mathcal{L}_{LDM} := \\mathbb{E}_{x(x), y, \\epsilon \\sim \\mathcal{N}(0,1), t} [||\\epsilon - \\epsilon_\theta(z_t, t, \\tau_\theta(y))||^2]\\) (2)\nHowever when employing prompts like \u201ca photo of a [occupation]\", the model has been shown to exhibit significant gender bias with stereotypes (Friedrich et al., 2023; Chuang et al., 2023), which manifests throughout the entire process.\nWhile research efforts have been dedicated to addressing biases in the iterative image denoising process within U-Net (Ronneberger et al., 2015), the convolutional network component of Stable Diffusion (Esposito et al., 2023), our investigation has revealed a distinct bias emerging after the text embedding stage, when utilizing the text encoder of different versions of Stable Diffusion. This means that bias is amplified throughout the process.\""}, {"title": "3.2 Measuring Text-Encoded Bias", "content": "Issues of Projecting Prompt Embeddings for Fairness Although Chuang et al. (2023) claims great success in measuring bias in text embeddings and projecting it to achieve fairness, we have identified an issue. When attempting to switch the original prompt into another direction to achieve balance, the projection may take it into another direction that can introduce other biases. In the Appendix A.6, we present failure cases of gender mitigation using (Chuang et al., 2023)'s method, which resulted in only male faces being generated.\nReframing the Projection to Assess Gender Bias\nWe conduct zero-shot and unsupervised classification on embeddings, using prompts like \u201cThe photo of the face of a [occupation]\u201d and a pretrained Stable Diffusion text encoder. Our goal is to measure gender bias within these prompts and their embeddings. Altering the weights of the pretrained text encoder is risky, as it could lead to the model forgetting information that is not related to gender, which has learned from extensive data, so the Stable Diffusion text encoder is frozen. Another challenge is the lack of clear labels, as we only have prompts, embeddings, and statistics for each occupation generated by Fair Diffusion. These statistics are based on 250 images from the original Stable Diffusion and cannot be quantitatively analyzed or treated as a simple binary classification task due to the small sample size. Moreover, the embeddings are subject to change based on the hyperparameters of the pretrained model, meaning the statistics can only serve as a benchmark for testing our approach rather than for model learning. Therefore, our analysis relies solely on prompts and model weights.\nAs shown in Equation 1, \\(\\mathbf{P}\\) is the prompt embedding, and \\(P^*\\) is the projection process. Prompts \\(z_0\\) applying Calibration Matrix can be written as \\(P^*z_0 = P_0 (I + \\sum_{(i,j) \\in S} (z_i - z_j) (z_i - z_j)^T)^{-1} z_0\\), derived from Equation 1."}, {"title": "3.3 Defining the Bias Identification Gate", "content": "We utilize the Pearson Correlation Coefficient to measure similarity and check statistical approximation of genders in Fair Diffusion occupations, with further details provided in Appendix A.7. If the male count for a particular occupation surpasses half of the total, we assume this occupation is male-skewed; otherwise, it is female-skewed. To assess the effectiveness of our approach when designing the Bias Identification Gate, we compare the frequency-based labels with our calculations derived from Equation 4 to compute accuracy. We achieve an accuracy rate of 79%, indicating that our bias measurement approximation, achieved through task reframing, aligns with both intuition and statistical trends.\nThe accuracy is lower when it comes to occupations such as insurance agents (details in Appendix A.9.1), as these are mostly likely at the boundary between male and female. In Table 1, we show mitigation of biases even when classifications of these challenging occupations are incorrect."}, {"title": "3.4 BiAs Experts: Bias Adapter Experts", "content": "The next step is to set up the experts for the MoE approach. For that, we implement a way to personalize Stable Diffusion to guide our model towards fairness.\nThe intuition is to guide the model to generate more female when the original embedding exhibits the male skew and vice versa. To achieve this, we divide our training text-image pairs into two groups: male and female. We then fine-tune the model separately to generate male-biased and female-biased experts. When the gate is activated, it guides the experts according to the following rules: male skew for female-biased experts and female skew for male-biased experts. This process is called bias fine-tuning.\nAdapters (Hu et al., 2022; Houlsby et al., 2019; Karimi Mahabadi et al., 2021) provide a method to freeze the model and introduce a new, trainable weight matrix, which significantly reduces both the time and memory required for training.\nWe incorporate this method into creating bias experts, making the bias fine-tuning parameter-efficient. We freeze the parameters in the U-Net and add adapters on the cross-attention layers of the U-Net, as illustrated in Figure 1. We initialize the adapters following the principles of LoRA (Hu et al., 2022): the weight parameters of the first matrix \\(W_{\text{down}}\\) are determined by a Gaussian function, while the parameters of the second matrix \\(W_{\text{up}}\\) are initialized as a zero matrix. Consequently, the added pathway is initially zero during training and does not impact the result, resembling the original output.\nSince the adapters are randomly initialized, they do not inherit biases from Stable Diffusion fine-tuning, which makes them more effective in achieving fairness, as detailed in Section 4.2.2. By using adapters as the bias experts, we achieve better fairness scores and reduce the trainable ratio to only 5.6%.\nFor bias fine-tuning, we generate biased images with specific male or female characteristics so that the bias expert will learn stereotypes from the data. We employ a technique from Dreambooth (Ruiz et al., 2023), utilizing a special token to help BiAs to better understand the biased data. Dreambooth adds a special token, a unique identifier, to the prompt, and uses a few fine-tuning images of a subject as input to \u201cpersonalize\u201d the model. In our work, we use the special token to \"personalize\" the adapters of the biased information.\nIn theory, the special token should not impact the results, as it is only used to remind the experts about the bias. However, due to the context differences of prompt embeddings, minor differences in results might occur when different special tokens are used for different experts and prompts.\nFor fine-tuning data, we generate our training image-text pairs using Stable Diffusion-XL with the following prompt: \u201cA [gender] + [race] + [occupation].\" The prompts generate 1530 images with different genders, races and occupations to ensure variety, which then fine-tune the model using the special token. The results demonstrate that the special token yields better performance than ordinary fine-tuning, as detailed in Section 4.2.2.\nOur bias fine-tuning can be formalised in the following optimization process:\n\\(\\mathcal{L}_{\text{bias}} := \\mathbb{E}_{x(x), y, \\epsilon \\sim \\mathcal{N}(0,1), t} [||\\epsilon - \\epsilon_\theta(z_t, t, \\tau_\theta(s))||_1^2]\\) (5)\nwhere \\(s\\) is the prompt with the special token.\nWhat distinguishes our work from that of (Esposito et al., 2023) is that we only need to fine-tune a small set of data, rather than 89K prompts and 89K images, and fine-tune a small ratio of parameters,"}, {"title": "3.5 Mixture of Experts", "content": "MoE (Mixture of Experts) (Jacobs et al., 1991; Eigen et al., 2013) is an ensemble learning technique that improves performance by weighting the predictions of different experts through gate mechanisms.\nIn a classic MoE system, each expert is independent, and demonstrates high performance within their area of expertise. A gating mechanism is learned to adjust the weight assigned to each expert based on the input data. In contrast, in our approach, the gates and experts, defined in Section 3.3 and 3.4, are not trained. We utilize pre-trained models, which can be flexibly replaced by other pre-trained models.\nWe show our MoE architecture in Figure 2. What makes our gating mechanism different from traditional MoE is that we do not learn it from the data, and for the experts, we utilize pre-trained models.\nTo summarise, our approach is as follows: we first conduct zero-shot and unsupervised classification. We do not use the labels (the gender statistics of the 250 images for each occupation generated by the vanilla Stable Diffusion, provided by Fair Diffusion) to train the model, but only for hyperparameter selection. In other words, we only use the prompt and pretrained CLIP encoder itself. At inference, the model takes the original prompt and special token as inputs. It judges the gender skew the prompt shows, with the male skew leading to a higher probability of calling the female expert, and vice versa."}, {"title": "4 Experiments", "content": "We first use the statistical data of different occupations generated by Fair Diffusion (Stable Diffusion 1.5) to select the best hyperparameters for both versions of Stable Diffusion. If the general statistical count shows more males than females, we assume that it has a male skew, and vice versa. We found that 4000 is the optimal hyperparameter \\({\\lambda}\\) in Equation 1 and the subsequent derivation for version 1.5, and 100 is optimal for version 2.1. There is a large difference in the best hyperparameter between the two versions, which we attribute to the differences in the text encoder between the two versions of Stable Diffusion, which introduces different embedding gender biases in the latent space.\nNext, we use Stable Diffusion XL to generate fine-tuning images data and then fine-tune our MOESD with BiAs experts by adding a special token to the original prompts (set to \u201csks\u201d as in (Ruiz et al., 2023)). The prompt is \u201cA photo of the sks face of the [occupation].\"\nAfter fine-tuning, we proceed to the inference steps: There are three experts in the system: one original, one male BiAs expert, and one female BiAs expert. When inputting a prompt, we add the same special token (\u201csks\u201d) as in the fine-tuning stages. The system judges the skew, and if the skew is male, we allocate 10% to the male expert, 50% to the female expert, and 40% to the original expert. Conversely, if the skew is female, we allocate 10% to the female expert, 50% to the male expert, and 40% to the original expert. These are the optimal hyperparameters for conservative mitigation within a limited search range in our experiments since we do not want the BiAs to completely change the original weights.\nIn this manner, we generate 100 images based on the same prompt for each of the 153 occupations.\nWe then employ BLIP2 (Li et al., 2023) VQA task to conduct fairness evaluation and Laion-aesthetic linear classifier (Schuhmann, 2022) to perform aesthetic evaluation, as we discuss in the following sections. We also employ human evaluation for the aesthetic and image-description relevance evaluation."}, {"title": "4.1 Setup", "content": "We first use the statistical data of different occupations generated by Fair Diffusion (Stable Diffusion 1.5) to select the best hyperparameters for both versions of Stable Diffusion. If the general statistical count shows more males than females, we assume that it has a male skew, and vice versa. We found"}, {"title": "4.2 Fairness Evaluation", "content": "We first use the statistical data of different occupations generated by Fair Diffusion (Stable Diffusion 1.5) to select the best hyperparameters for both versions of Stable Diffusion. If the general statistical count shows more males than females, we assume that it has a male skew, and vice versa. We found"}, {"title": "4.2.1 Metric", "content": "Stable Diffusion learns a conditional distribution \\(P(X|Z = z)\\), where \\(z\\) represents the embedding of the prompt, and the biased nature of the dataset used to train the generative model and its architecture can impact the distribution \\(P\\). To quantify the bias in generative models, recent studies (Choi et al., 2020; Teo et al., 2024; Chuang et al., 2023) propose using statistical parity. Specifically, given a classifier \\(h : X \rightarrow A\\) for the gender attribute, the discrepancy of the generative distribution \\(P\\) can be"}, {"title": "4.2.2 Results", "content": "We compare our method to the original Stable Diffusion, the Biased Prompt method proposed by Chuang et al. (2023), and Fair Diffusion versions 1.5 and 2.1. Due to the variability in results related to hyperparameters reported by Chuang et al. (2023), we select the best result from different hyperparameters for their method in both versions: 0.05 for version 1.5 and 500 for version 2 (as reported in their paper). The results are shown in Table 1.\nMOESD-BiAs (with special token) performs better than all others except for Fair Diffusion (Friedrich et al., 2023). This is expected, since we assume that guidance from manually edited prompts is SOTA in terms of fairness as it will force the model to chose the gender specified. In our method, we do not explicitly give any gender information in the prompt (e.g. male or female). We show our mitigation examples in Appendix A.1.\nModel Version We can observe that in most methods, the fairness score for Stable Diffusion Version 2.1 is higher than for Version 1.5, except for Fair Diffusion, which indicates that Version 1.5 performs better in terms of fairness. For other methods, there is a huge difference between different versions, but our method achieves similarly good performance in both versions.\nAdapter BiAs appears to perform better than full fine-tuning regardless of whether we use the special token, which suggests that only a small set of parameters (5%) are enough to mitigate bias. We attribute it to the fact that adapters allow more targeted parameter updates, reducing the risk of gradient vanishing or exploding and preventing the catastrophic forgetting of general knowledge from well-pretrained Stable Diffusion weights. Moreover, due to random initialization, our adapters are"}, {"title": "4.3 Aesthetic Evaluation", "content": "It should be noted that due to differences in training data, the style and quality of generated images vary. While the images generated by our model achieve better fairness scores than those of others, we also need to evaluate whether the aesthetic quality of the generated images is not worse.\nLinear Aesthetic Evaluation Simulacra Aesthetic Captions - SAC (Pressman et al., 2022) is a dataset consisting of over 238,000 synthetic images generated with AI models, from over forty thousand user submitted prompts. Users rate the images on their aesthetic value from 1 to 10, when they were asked \u201cHow much do you like this image on a scale from 1 to 10?\u201d. LAION-Aesthetics (Schuhmann, 2022) trains a linear model on 5000 image-rating pairs from the SAC dataset, which can predict a numeric aesthetic score in 1-10. We utilize this linear model to evaluate the image sets of all methods and compare their aesthetic scores.\nHuman Evaluation We randomly select 30 out of 153 occupations and then randomly select one out of 100 generated images. We choose the images"}, {"title": "4.3.1 Metric", "content": "It should be noted that due to differences in training data, the style and quality of generated images vary. While the images generated by our model achieve better fairness scores than those of others, we also need to evaluate whether the aesthetic quality of the generated images is not worse.\nLinear Aesthetic Evaluation Simulacra Aesthetic Captions - SAC (Pressman et al., 2022) is a dataset consisting of over 238,000 synthetic images generated with AI models, from over forty thousand user submitted prompts. Users rate the images on their aesthetic value from 1 to 10, when they were asked \u201cHow much do you like this image on a scale from 1 to 10?\u201d. LAION-Aesthetics (Schuhmann, 2022) trains a linear model on 5000 image-rating pairs from the SAC dataset, which can predict a numeric aesthetic score in 1-10. We utilize this linear model to evaluate the image sets of all methods and compare their aesthetic scores.\nHuman Evaluation We randomly select 30 out of 153 occupations and then randomly select one out of 100 generated images. We choose the images"}, {"title": "4.3.2 Results", "content": "Linear Aesthetic Evaluation To better evaluate our fine-tuning results, we compare all the methods on the aesthetic level to determine whether fine-tuning affects abilities other than fairness. The results in the Table 2 show that our method maintain the image quality according to this metric.\nCompared to the vanilla approach, MoESD-BiAs (special token) has a higher aesthetics score, even though it may blurs faces at times. We spec-"}, {"title": "4.4 Image-Description Relevance Evaluation", "content": "We also launched an additional human evaluation on the quality of our generated images to further"}, {"title": "5 Conclusions", "content": "In this paper, we introduce the MoE to Stable Diffusion by taking a major step by assessing existing biases in the prompt embedding and fine-tuning the BiAs with special tokens to aid in understanding the biased data, which results in our MoESD-BiAs system, which has achieved significant success in mitigating bias while maintaining quality."}, {"title": "6 Limitations", "content": "Although our method has made significant strides in mitigating bias while maintaining aesthetic quality, we must admit the limitations of our work.\nFirst, we cannot achieve the same level of fairness score as manual editing of the gender attributes, which could be further improved. Additionally, we did not pay attention to evaluating biases related to the race, as measuring bias in prompt embeddings for the race is much more difficult than for gender. Moreover, in our work, we only address the case of binary gender. We did not consider sexual minorities in various contexts, which is a much more complex task. We hope to explore this further in future work.\nSecond, our zero-shot and unsupervised prompt bias identification and hyperparameter selection are based on the results of Fair Diffusion statistical counts (Friedrich et al., 2023), so it may not be entirely accurate. Moreover, we only achieve 79% accuracy in identifying bias from the prompt, which is not perfect when facing challenging occupations such as the insurance agent.\nThird, although we achieve good performance in aesthetics, our method indeed loses some fine-grained details on faces due to the quality of the fine-tuning images. However, fortunately, our experts can be switched by fine-tuned ones by users themselves, which can be further improved through special fine-tuning."}, {"title": "7 Ethical Considerations", "content": "Our work focuses on addressing social bias, specifically gender bias. Our research has a broader impact beyond scientific research. We take a significant stride across a wide range of industries and societies and our method marks a crucial step toward eliminating gender biases in text-to-image models.\nHowever, by introducing the BiAs approach to mitigate bias, there is a risk that people might misuse the weights to generate more biased content."}, {"title": "Acknowledgements", "content": "We would like to express gratitude to all who participated in the human evaluation, which has significantly enhanced the quality of this research."}, {"title": "A Appendix", "content": "We showcase our method with vanilla Stable Diffusion to demonstrate our gender bias mitigation in Figure 3."}, {"title": "A.1 Mitigation Visualization", "content": "We showcase our method with vanilla Stable Diffusion to demonstrate our gender bias mitigation in Figure 3."}, {"title": "A.2 Text-Encoded Bias from Prompts", "content": "We use T-Distributed Stochastic Neighbor Embedding (t-SNE) to visualize the text-encoded bias from prompts, as shown in Figure 4."}, {"title": "A.3 Different Methods for Bias-Identification Gate", "content": "We showcase various methods for Bias-Identification Gate, and our approach stands out as the most effective in identifying bias in the prompt according to the Equation 4. We directly compare classifications without the intervention of the Calibration Matrix Projection by simply comparing the similarity of \\(z_0\\), \\(z_{male}\\), and \\(z_{female}\\) to determine whether \\(z_0\\) is closer to \\(z_{male}\\) or \\(z_{female}\\). The rule is demonstrated below:\n\\(\\mathcal{G}'(z_0) = \\frac{Similarity'(z_0, z_{male})}{Similarity'(z_0, z_{female})}\\) (7)\nWe also utilize third-party models (T5 (Raffel et al., 2020) and Sequence Transformer (Reimers and Gurevych, 2019)) as monitoring models to utilize their embedding similarity for classification and compare the results with our method.\nFor T5, we perform the QA task for classification, as follows:\nQuestion = \"Answer the following question with 'male' or 'female'. Is the face more likely to be male or female?\"\nContext = \"A photo of the face of the \" + [occupation]\nFor Sentence Transformer, we calculate the similarity between two different set of prompts and perform the classification, selecting the maximum accuracy:\n(1) Query = \"A photo of the face of the \" + [occupation]\nDocs = \"A photo of the face of the male\", \"\u0410 photo of the face of the female\"\n(2) Query= [occupation]\nDocs = [\"male\", \"female\"]\nOnce again, our identification method yields the best results."}, {"title": "A.4 More Details of (Chuang et al., 2023)", "content": "The detailed proof is shown in the original work. For simplicity in our work, the derivation from Equation 1 is presented as follows. Equations 8 and 9 demonstrate that the calibrated projection matrix and prompt embedding have convenient closed-form solutions.\n\\(P^* = P_0 (I + \\frac{\\lambda}{|S|} \\sum_{(i,j) \\in S} (z_i - z_j) (z_i - z_j)^T)^{-1}\\) (8)\n\\(\\mathbf{z}^* = [I + \\frac{\\lambda}{|S|} \\sum_{(i,j) \\in S} (z_i - z_j) (z_i - z_j)^T]^{-1} z_0\\)(9)"}, {"title": "A.5 Parameter \\(\\lambda\\) for Bias-Identification", "content": "From Equation 4 and 8, we can observe that the parameter \\(\\lambda\\) is crucial for Bias Identification. Therefore, we can deduce that the accuracy of the result is influenced by this parameter. To visualize the results based on \\(\\lambda\\), we observe that 4000 is optimal for Stable Diffusion 1.5, while 100 is optimal for Stable Diffusion 2.1."}, {"title": "A.6 Defect of Bias Prompts", "content": "As shown in Figure 7, the ideal mitigation should lie in the middle of male and female; however, the actual mitigation occurs in another dimension. Although it may have the same meaning when representing male or female, the same meaning may not be neutral and could still contain some gender attributes."}, {"title": "A.7 Occupation list", "content": "We use the occupation list from Fair Diffusion (Friedrich et al., 2023) instead of (Chuang et al., 2023), as we found that the Fair Diffusion one contains more occupations and includes more challenging ones. All occupations are displayed in the Table 6 below, where we specifically label the right and wrong occupation predictions for our Bias-Identification method."}, {"title": "A.8 BLIP2 VQA for Fairness Evaluation", "content": "For each picture, we use the following question to conduct the VQA evaluation and count the number of males and females in 100 pictures for each occupation.\nQuestion: \"Answer the following question with 'male' or 'female' or 'people not present' only. Is this person on this file male or female?\"\nFor \"people not present\", BLIP2 sometimes gives the answer \"unknown\", so it will not be reflected in the count of \"people not present\". However, it does not matter since we only care about the male and female count for the fairness score."}, {"title": "A.9 Failure cases", "content": ""}, {"title": "A.9.1 Right and Wrong Case for Bias Identification", "content": "As previously presented in the Table 6, which illustrates the right and wrong cases for Bias Identification. We can observe that the wrong cases are most likely to be difficult ones, including very neu-"}, {"title": "A.9.2 Failure Generation for our Method", "content": "As mentioned above, some of our generated images blur the faces. This could be further improved through fine-tuning."}, {"title": "A.10 Human Evaluation Details", "content": "For the aesthetic response, we collected 29 responses, and for how well the image matches the image description, we collected 27 responses. All surveys were conducted anonymously, and the experiment was double-blinded. All the users were informed all the collected data would be used for scientific research only. When clicking the submission button, they know the data collection is anonymous and consent to the collection."}]}