{"title": "MoESD: Mixture of Experts Stable Diffusion to Mitigate Gender Bias", "authors": ["Guorun Wang", "Lucia Specia"], "abstract": "Text-to-image models are known to propagate social biases. For example when prompted to generate images of people in certain professions, these models tend to systematically generate specific genders or ethnicity. In this paper, we show that this bias is already present in the text encoder of the model and introduce a Mixture-of-Experts approach by identifying text-encoded bias in the latent space and then creating a bias-identification gate. More specifically, we propose MoESD (Mixture of Experts Stable Diffusion) with BiAs (Bias Adapters) to mitigate gender bias. We also demonstrate that a special token is essential during the mitigation process. With experiments focusing on gender bias, we demonstrate that our approach successfully mitigates gender bias while maintaining image quality.", "sections": [{"title": "Introduction", "content": "In recent years, large language and vision models such as ChatGPT 4 (OpenAI, 2023), DALL\u00b7E (Ramesh et al., 2021) and Stable Diffusion (Rombach et al., 2022; Podell et al., 2023) have ushered the era of AI generated content. However, research has shown that these generative models often exhibit social biases during the content generation process, especially in text-to-image generation. For instance, models tends to generate more images of man when provided with prompts like \u201ca successful CEO\", and more images of women when provided with prompts like \u201ca paralegal\" (Friedrich et al., 2023; Luccioni et al., 2023).\nTo mitigate such biases, current methodologies can be broadly categorized into three debiasing paradigms:\n\u2022 Pre-processing the training data to remove bias before training.\n\u2022 Prompt-engineering to restrain the model generation at the deployment stage.\n\u2022 Enforcing fairness on model weights by introducing constraints on the learning objective during training.\nIn pre-processing methods, eliminating bias in the training corpus is a difficult challenge that offers no guarantees (Hamidieh et al., 2023). For prompt-engineering, although leveraging specific prompts to instruct the model (e.g. \"a photo of a female plummer\u201d) (Friedrich et al., 2023) can work to avoid biases, this is not how the average user prompts these models, do not a solution in practice. For changing model weights, the resource-intensive nature of re-training models poses challenges, requiring vast amounts of data (89k text-image pairs in (Esposito et al., 2023)) and full fine-tuning of the model.\nOur work identifies existing biases in pre-trained models and effectively mitigates them by parameter-efficient fine-tuning, which only requires a small amount of data (1.5K) and parameters (5.6%). Our contributions can be summarized as follows:\n\u2022 We measure the gender skew in text to assess gender bias in embeddings.\n\u2022 We introduce Mixture of Experts (MoE) to Stable Diffusion and fine-tune the Bias Adapters (BiAs) to effectively mitigate identified gender biases.\n\u2022 We apply special tokens to aid the BiAs in better understanding the biased data and demonstrate that it is essential for mitigation.\n\u2022 We successfully mitigate the gender bias in Stable Diffusion while maintaining image quality."}, {"title": "Related Work", "content": "Biases in multimodal settings have attracted increasing attention. Several studies have evaluated multimodal models and discovered that they inherit and propagate many biases (Agarwal et al., 2021; Cho et al., 2023). Prompt learning and engineering have been widely utilized in vision-language models and generative models (Berg et al., 2022; Friedrich et al., 2023). Specifically, Fair Diffusion (Friedrich et al., 2023) addresses biases through prompt engineering: users insert prompts to generate fair images with the assistance of specific guidance on the SEGA model (Brack et al., 2023). Subsequently, image generation is guided towards a fairer outcome through manual semantic editing in the latent space of biased concepts.\nApproaches to offset bias representation are also increasingly popular: Seth et al. (2023) employs additive residual image representations to mitigate biased representations, while Esposito et al. (2023) focuses on fine-tuning the model to achieve fairness. Chuang et al. (2023) proposed a method to project out biased directions in text embeddings to create fair generative models. This approach leverages positive pairs of prompts to debias embeddings effectively. By generating embeddings of prompts such as \u201ca photo of a [class name] with [spurious attribute]\u201d, a calibrated projection matrix, as shown in Equations 1, is optimized. After projection, the embedding should only contain information about the \"[class name]\" with no spurious information (e.g., gender).\nEquation 1 illustrates the regularization of the difference between the projected embeddings of the set of positive pairs S, where (zi, zj) represents the embedding of prompt pair (i, j) in S, which describes the same class but with different spurious attributes (e.g., gender). The loss function encourages the linear projection P to be invariant to the difference between the spurious attributes (details in Appendix A.4).\n$\\min_{P} ||P - P_0||^2 + \\frac{\\lambda}{|S|} \\sum_{(i,j)\\in S} ||Pz_i - Pz_j||^2$ (1)"}, {"title": "Method", "content": "In this section we describe our method. First, we explore the projection matrix from (Chuang et al., 2023) (Equation 1) to assess inheriting gender biases in Stable Diffusion. Second, we introduce the Mixture of Experts (MoE) to Stable Diffusion and add fine-tuned BiAs (experts), which combine bias identification gates and special tokens to aid in understanding the biased data. This results in our MoESD-BiAs approach."}, {"title": "Identifying Biases in Stable Diffusion", "content": "Stable Diffusion, the text-to-image model in our work, generates images from text by switching diffusion process from pixel space to latent space to generate images. Given an image x \u2208 RH\u00d7W\u00d73 in RGB space, the encoder E encodes x into a latent representation z = E(x), and the decoder D reconstructs the image from the latent, giving x' = D(z) = D(E(x)), where z \u2208 Rhxwxc. A domain-specific encoder \u03c4\u03bf projects y (language prompt) to an intermediate representation \u0442\u04e9(y) \u2208 [RM\u00d7d+, which is then mapped to the intermediate layers of the U-Net via a cross-attention layer implementing to condition the latent z. Based on image-conditioning pairs, the conditional LDM is learned via Equation 2.\n$\\mathcal{L}_{LDM} := E_{x(\\epsilon), y, \\epsilon \\sim \\mathcal{N}(0, 1), t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, \\tau_{\\theta}(y))||^2]$ (2)\nHowever when employing prompts like \u201ca photo of a [occupation]", "The photo of the face of a [occupation]\", where the [occupation] are from the top 8 male-biased and top 8 female-biased occupations from Fair Diffusion (Friedrich et al., 2023)\u00b3. There is a clear boundary between two gender-biases occupation encoded\"\n    },\n    {\n      \"title\": \"Measuring Text-Encoded Bias\",\n      \"content\": \"Issues of Projecting Prompt Embeddings for Fairness Although Chuang et al. (2023) claims great success in measuring bias in text embeddings and projecting it to achieve fairness, we have identified an issue. When attempting to switch the original prompt into another direction to achieve balance, the projection may take it into another direction that can introduce other biases. In the Appendix A.6, we present failure cases of gender mitigation using (Chuang et al., 2023)'s method, which resulted in only male faces being generated.\nReframing the Projection to Assess Gender Bias We conduct zero-shot and unsupervised classification on embeddings, using prompts like \u201cThe photo of the face of a [occupation]": "nd a pretrained Stable Diffusion text encoder. Our goal is to measure gender bias within these prompts and their embeddings. Altering the weights of the pretrained text encoder is risky, as it could lead to the model forgetting information that is not related to gender, which has learned from extensive data, so the Stable Diffusion text encoder is frozen. Another challenge is the lack of clear labels, as we only have prompts, embeddings, and statistics for each occupation generated by Fair Diffusion. These statistics are based on 250 images from the original Stable Diffusion and cannot be quantitatively analyzed or treated as a simple binary classification task due to the small sample size. Moreover, the embeddings are subject to change based on the hyperparameters of the pretrained model, meaning the statistics can only serve as a benchmark for testing our approach rather than for model learning. Therefore, our analysis relies solely on prompts and model weights.\nAs shown in Equation 1, $z_0$ is the prompt embedding, and P is the projection process. Prompts $z_0$ applying Calibration Matrix can be written as $P^* z_0 = P_0 (1 + \\sum_{(i,j)\\in s} (z_i - z_j) (z_i - z_j)^T )^{-1} z_0$, derived from Equation 1."}, {"title": "Defining the Bias Identification Gate", "content": "Here we first define the similarity between two prompts before and after applying the Calibration Matrix:\n$\\Delta S(z_0, z_t, P_0, P^*) = \\frac{Similarity(P_0z_0, P_0z_t)}{Similarity(P^*z_0, P_0z_t)}$ (3)\nWe then calculate the gender skew for prompt embedding z0 as\n$G(z_0) = \\frac{\\Delta S(z_0, z_{male}, P_0, P^*)}{\\Delta S(z_0, z_{female}, P_0, P^*)}$ (4)\nWe assume the gender skew is male when G(z0) > 0 while the gender skew is female G(z0) < 0. If \u2206S(\u22480, zt, P0, P*) is larger, then the disparity between two prompts-before and after applying the Calibration Matrix is also larger. This results in a greater divergence between the original embedding direction and the projected embeddings. Consequently, the strength of the redirected embeddings is increased, indicating a stronger gender preference before redirecting. In essence, if the redirection strength is substantial, it indicates a more forceful adjustment away from the original gender direction, which means higher gender skew.\nWe utilize the Pearson Correlation Coefficient to measure similarity and check statistical approximation of genders in Fair Diffusion occupations, with further details provided in Appendix A.7. If the male count for a particular occupation surpasses half of the total, we assume this occupation is male-skewed; otherwise, it is female-skewed. To assess the effectiveness of our approach when designing the Bias Identification Gate, we compare the frequency-based labels with our calculations derived from Equation 4 to compute accuracy. We achieve an accuracy rate of 79%, indicating that our bias measurement approximation, achieved through task reframing, aligns with both intuition and statistical trends.\nThe accuracy is lower when it comes to occupations such as insurance agents (details in Appendix A.9.1), as these are mostly likely at the boundary between male and female. In Table 1, we show mitigation of biases even when classifications of these challenging occupations are incorrect."}, {"title": "BiAs Experts: Bias Adapter Experts", "content": "The next step is to set up the experts for the MoE approach. For that, we implement a way to personalize Stable Diffusion to guide our model towards fairness.\nThe intuition is to guide the model to generate more female when the original embedding exhibits the male skew and vice versa. To achieve this, we divide our training text-image pairs into two groups: male and female. We then fine-tune the model separately to generate male-biased and female-biased experts. When the gate is activated, it guides the experts according to the following rules: male skew for female-biased experts and female skew for male-biased experts. This process is called bias fine-tuning.\nAdapters (Hu et al., 2022; Houlsby et al., 2019; Karimi Mahabadi et al., 2021) provide a method to freeze the model and introduce a new, trainable weight matrix, which significantly reduces both the time and memory required for training.\nWe incorporate this method into creating bias experts, making the bias fine-tuning parameter-efficient. We freeze the parameters in the U-Net and add adapters on the cross-attention layers of the U-Net, as illustrated in Figure 1. We initialize the adapters following the principles of LoRA (Hu et al., 2022): the weight parameters of the first matrix $W_{down}$ are determined by a Gaussian function, while the parameters of the second matrix $W_{up}$ are initialized as a zero matrix. Consequently, the added pathway is initially zero during training and does not impact the result, resembling the original output.\nSince the adapters are randomly initialized, they do not inherit biases from Stable Diffusion fine-tuning, which makes them more effective in achieving fairness, as detailed in Section 4.2.2. By using adapters as the bias experts, we achieve better fairness scores and reduce the trainable ratio to only 5.6%.\nFor bias fine-tuning, we generate biased images with specific male or female characteristics so that the bias expert will learn stereotypes from the data. We employ a technique from Dreambooth (Ruiz et al., 2023), utilizing a special token to help BiAs to better understand the biased data. Dreambooth adds a special token, a unique identifier, to the prompt, and uses a few fine-tuning images of a subject as input to \u201cpersonalize\" the model. In our work, we use the special token to \"personalize\" the adapters of the biased information.\nIn theory, the special token should not impact the results, as it is only used to remind the experts about the bias. However, due to the context differences of prompt embeddings, minor differences in results might occur when different special tokens are used for different experts and prompts.\nFor fine-tuning data, we generate our training image-text pairs using Stable Diffusion-XL with the following prompt: \u201cA [gender] + [race] + [occupation].\" The prompts generate 1530 images with different genders, races and occupations to ensure variety, which then fine-tune the model using the special token. The results demonstrate that the special token yields better performance than ordinary fine-tuning, as detailed in Section 4.2.2.\nOur bias fine-tuning can be formalised in the following optimization process:\n$\\mathcal{L}_{bias} := E_{x(x), y, \\epsilon \\sim \\mathcal{N}(0, 1), t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, \\tau_{\\theta}(s)||1^2]$ (5)\nwhere s is the prompt with the special token.\nWhat distinguishes our work from that of (Esposito et al., 2023) is that we only need to fine-tune a small set of data, rather than 89K prompts and 89K images, and fine-tune a small ratio of parameters,"}, {"title": "Mixture of Experts", "content": "making it more training-efficient.\nWhat distinguishes our work from Fair Diffusion (Friedrich et al., 2023) is that we do not manually engage in prompt engineering (human intervention), in other words, we do not modify the prompt. The special token we add can be any token. Moreover, even without the special token, our approach still mitigates gender bias.\nMoE (Mixture of Experts) (Jacobs et al., 1991; Eigen et al., 2013) is an ensemble learning technique that improves performance by weighting the predictions of different experts through gate mechanisms.\nIn a classic MoE system, each expert is independent, and demonstrates high performance within their area of expertise. A gating mechanism is learned to adjust the weight assigned to each expert based on the input data. In contrast, in our approach, the gates and experts, defined in Section 3.3 and 3.4, are not trained. We utilize pre-trained models, which can be flexibly replaced by other pre-trained models.\nWe show our MoE architecture in Figure 2. What makes our gating mechanism different from traditional MoE is that we do not learn it from the data, and for the experts, we utilize pre-trained models.\nTo summarise, our approach is as follows: we first conduct zero-shot and unsupervised classification. We do not use the labels (the gender statistics of the 250 images for each occupation generated by the vanilla Stable Diffusion, provided by Fair Diffusion) to train the model, but only for hyperparameter selection. In other words, we only use the prompt and pretrained CLIP encoder itself. At inference, the model takes the original prompt and special token as inputs. It judges the gender skew the prompt shows, with the male skew leading to a higher probability of calling the female expert, and vice versa."}, {"title": "Experiments", "content": "We first use the statistical data of different occupations generated by Fair Diffusion (Stable Diffusion 1.5) to select the best hyperparameters for both versions of Stable Diffusion. If the general statistical count shows more males than females, we assume that it has a male skew, and vice versa. We found that 4000 is the optimal hyperparameter \u03bb in Equation 1 and the subsequent derivation for version 1.5, and 100 is optimal for version 2.1. There is a large difference in the best hyperparameter between the two versions, which we attribute to the differences in the text encoder between the two versions of Stable Diffusion, which introduces different embedding gender biases in the latent space.\nNext, we use Stable Diffusion XL to generate fine-tuning images data and then fine-tune our MOESD with BiAs experts by adding a special token to the original prompts (set to \u201csks\u201d as in (Ruiz et al., 2023)). The prompt is \u201cA photo of the sks face of the [occupation].\u201d\nAfter fine-tuning, we proceed to the inference steps: There are three experts in the system: one original, one male BiAs expert, and one female BiAs expert. When inputting a prompt, we add the same special token (\u201csks\u201d) as in the fine-tuning stages. The system judges the skew, and if the skew is male, we allocate 10% to the male expert, 50% to the female expert, and 40% to the original expert. Conversely, if the skew is female, we allocate 10% to the female expert, 50% to the male expert, and 40% to the original expert. These are the optimal hyperparameters for conservative mitigation within a limited search range in our experiments since we do not want the BiAs to completely change the original weights.\nIn this manner, we generate 100 images based on the same prompt for each of the 153 occupations.\nWe then employ BLIP2 (Li et al., 2023) VQA task to conduct fairness evaluation and Laion-aesthetic linear classifier (Schuhmann, 2022) to perform aesthetic evaluation, as we discuss in the following sections. We also employ human evaluation for the aesthetic and image-description relevance evaluation."}, {"title": "Fairness Evaluation", "content": "Stable Diffusion learns a conditional distribution P(X|Z = z), where z represents the embedding of the prompt, and the biased nature of the dataset used to train the generative model and its architecture can impact the distribution P. To quantify the bias in generative models, recent studies (Choi et al., 2020; Teo et al., 2024; Chuang et al., 2023) propose using statistical parity. Specifically, given a classifier h : X \u2192 A for the gender attribute, the discrepancy of the generative distribution P can be"}, {"title": "Aesthetic Evaluation", "content": "It should be noted that due to differences in training data, the style and quality of generated images vary. While the images generated by our model achieve better fairness scores than those of others, we also need to evaluate whether the aesthetic quality of the generated images is not worse.\nLinear Aesthetic Evaluation Simulacra Aesthetic Captions - SAC (Pressman et al., 2022) is a dataset consisting of over 238,000 synthetic images generated with AI models, from over forty thousand user submitted prompts. Users rate the images on their aesthetic value from 1 to 10, when they were asked \u201cHow much do you like this image on a scale from 1 to 10?\u201d. LAION-Aesthetics (Schuhmann, 2022) trains a linear model on 5000 image-rating pairs from the SAC dataset, which can predict a numeric aesthetic score in 1-10. We utilize this linear model to evaluate the image sets of all methods and compare their aesthetic scores.\nHuman Evaluation We randomly select 30 out of 153 occupations and then randomly select one out of 100 generated images. We choose the images from Fair Diffusion (Friedrich et al., 2023), Bias Prompts (X = 0.05) (Chuang et al., 2023), and our best method (MoESD-BiAs (special token)), and compare them with the vanilla images, all implemented on Stable Diffusion Version 1.5. So there are 90 pairs of images (30 pairs compare Fair Diffusion with vanilla, 30 pairs compare Bias Prompts (X = 0.05) with vanilla, and 30 pairs compare ours with vanilla) for users to choose which one is better. The instruction is \u201cChoose the image that has better quality (with 'same' being an option), where the main criterion is that the person in the image looks like a normal person and their there are no unrealistic items in the background\". More details of the survey are listed in Appendix A.10.\nIt's important to note that in some pairs, the vanilla model and ours have the same image because there is a probability that we do not activate the Bias expert in our model. In those cases, the quality remains the same. The same happens in some instances of Fair Diffusion, where the gender prompt engineering may not lead to different results. For the Bias Prompts, the best performance of Stable Diffusion Version 1.5 occurs with a small A value, so the pairs should be generally similar.\nAs final result, we calculate the average ratio at which users think each model does not perform worse than the vanilla model.\""}, {"title": "Image-Description Relevance Evaluation", "content": "We also launched an additional human evaluation on the quality of our generated images to further"}, {"title": "Conclusions", "content": "In this paper, we introduce the MoE to Stable Diffusion by taking a major step by assessing existing biases in the prompt embedding and fine-tuning the BiAs with special tokens to aid in understanding the biased data, which results in our MoESD-BiAs system, which has achieved significant success in mitigating bias while maintaining quality."}, {"title": "Limitations", "content": "Although our method has made significant strides in mitigating bias while maintaining aesthetic quality, we must admit the limitations of our work.\nFirst, we cannot achieve the same level of fairness score as manual editing of the gender attributes, which could be further improved. Additionally, we did not pay attention to evaluating biases related to the race, as measuring bias in prompt embeddings for the race is much more difficult than for gender. Moreover, in our work, we only address the case of binary gender. We did not consider sexual minorities in various contexts, which is a much more complex task. We hope to explore this further in future work.\nSecond, our zero-shot and unsupervised prompt bias identification and hyperparameter selection are based on the results of Fair Diffusion statistical counts (Friedrich et al., 2023), so it may not be entirely accurate. Moreover, we only achieve 79% accuracy in identifying bias from the prompt, which is not perfect when facing challenging occupations such as the insurance agent.\nThird, although we achieve good performance in aesthetics, our method indeed loses some fine-grained details on faces due to the quality of the fine-tuning images. However, fortunately, our experts can be switched by fine-tuned ones by users themselves, which can be further improved through special fine-tuning."}, {"title": "Ethical Considerations", "content": "Our work focuses on addressing social bias, specifically gender bias. Our research has a broader impact beyond scientific research. We take a significant stride across a wide range of industries and societies and our method marks a crucial step toward eliminating gender biases in text-to-image models.\nHowever, by introducing the BiAs approach to mitigate bias, there is a risk that people might misuse the weights to generate more biased content."}, {"title": "Appendix", "content": "We showcase our method with vanilla Stable Diffusion to demonstrate our gender bias mitigation in Figure 3.\nWe use T-Distributed Stochastic Neighbor Embedding (t-SNE) to visualize the text-encoded bias from prompts, as shown in Figure 4.\nWe showcase various methods for Bias-Identification Gate, and our approach stands out as the most effective in identifying bias in the prompt according to the Equation 4. We directly compare classifications without the intervention of the Calibration Matrix Projection by simply comparing the similarity of $z_0$, $z_{male}$, and $z_{female}$ to determine whether $z_0$ is closer to $z_{male}$ or $z_{female}$. The rule is demonstrated below:\n$G'(z_0) = \\frac{Similarity'(z_0, z_{male})}{Similarity'(z_0, z_{female})}$ (7)\nWe also utilize third-party models (T5 (Raffel et al., 2020) and Sequence Transformer (Reimers and Gurevych, 2019)) as monitoring models to utilize their embedding similarity for classification and compare the results with our method.\nFor T5, we perform the QA task for classification, as follows:\nQuestion = \"Answer the following question with 'male' or 'female'. Is the face more likely to be male or female?\"\nContext = \u201cA photo of the face of the \" + [occupation]\nFor Sentence Transformer, we calculate the similarity between two different set of prompts and perform the classification, selecting the maximum accuracy:\n(1) Query = \"A photo of the face of the \" + [occupation]\nDocs = \"A photo of the face of the male\", \"\u0410 photo of the face of the female\"\n(2) Query= [occupation]\nDocs = [\"male\", \"female\"]\nOnce again, our identification method yields the best results.\nFrom Equation 4 and 8, we can observe that the parameter \u03bb is crucial for Bias Identification. Therefore, we can deduce that the accuracy of the result is influenced by this parameter. To visualize the results based on \u03bb, we observe that 4000 is optimal for Stable Diffusion 1.5, while 100 is optimal for Stable Diffusion 2.1.\nAs shown in Figure 7, the ideal mitigation should lie in the middle of male and female; however, the actual mitigation occurs in another dimension. Although it may have the same meaning when representing male or female, the same meaning may not be neutral and could still contain some gender attributes."}]}