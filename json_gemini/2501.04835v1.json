{"title": "Do Code LLMs Understand Design Patterns?", "authors": ["Zhenyu Pan", "Xuefeng Song", "Yunkun Wang", "Rongyu Cao", "Binhua Li", "Yongbin Li", "Han Liu"], "abstract": "Code Large Language Models (LLMs) demonstrate great versatility in adapting to various downstream tasks, including code generation and completion, as well as bug detection and fixing. However, Code LLMs often fail to capture existing coding standards, leading to the generation of code that conflicts with the required design patterns for a given project. As a result, developers must post-process to adapt the generated code to the project's design norms. In this work, we empirically investigate the biases of Code LLMs in software development. Through carefully designed experiments, we assess the models' understanding of design patterns across recognition, comprehension, and generation. Our findings reveal that biases in Code LLMs significantly affect the reliability of downstream tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "LLMs transform traditional paradigms across various fields [6]. Among these [7], Code LLMs, trained on extensive code data, demonstrate great versatility in code-related tasks. They facilitate automated software development through downstream tasks such as code generation and completion, bug localization and repair, and security detection and enhancement, which significantly improves developer productivity.\nHowever, there are still considerable challenges in applying code LLMs in real-world scenarios, especially regarding specific aspects of software engineering. For instance, in complex repository environments, it is difficult to extract the necessary background knowledge from a large amount of information and follow project-specific conventions during code generation. Current code LLMs often fail to properly understand the existing design patterns and coding styles of a project, leading to generated code that does not meet project requirements. This mismatch creates an additional burden for developers, such as requiring extensive code revisions and post-processing.\nRecent empirical studies primarily evaluate code LLMs on tasks like code generation, bug detection, and code summarization, often using benchmarks such as CodeXGLUE [10]\u2013[12]. While these analyses have provided insights into generation accuracy and contextual understanding, the role of design patterns in software engineering has been largely overlooked. Design patterns are essential for maintaining software quality, particularly in object-oriented development. Emphasizing bias analysis is crucial, especially regarding potential biases in code LLMs under different design patterns, as such biases may impact the reliability of downstream tasks.\nTo bridge this gap, we propose an empirical study to evaluate code LLMs' understanding and generation of design patterns, focusing on whether these models can classify and generate code while adhering to established patterns like Singleton and Factory. For our dataset, we manually selected high-quality repositories from GitHub for both Python and Java, each corresponding to 12 design patterns, with two repositories chosen for each pattern in each language. Based on this, we designed three experiments to assess the capabilities of Code LLMs in recognition, generation, and comprehension of design patterns. For recognition, we conducted a Design Pattern Classification experiment to classify the design patterns in a given code file. For generation and comprehension, we designed two sets of experiments involving code completion and function generation, both with and without prior information about the design pattern. Based on the evaluation of these three capabilities, we provide an analysis of the strengths and weaknesses of Code LLMs in handling design patterns.\nThe main contributions of our work are : (1) We propose an evaluation framework for recognizing, generating, and understanding design patterns, (2) Through experiments, we demonstrate that code LLMs exhibit biases when adhering to specific design patterns. (3) We illustrate how these biases affect the reliability of code generation and the additional workload imposed on developers. (4) We emphasize the significance of our work for future research and practical applications, such as improving model training and guiding models to better follow development standards."}, {"title": "II. RELATED WORK", "content": "Code LLMs demonstrate notable advancements, each with distinct strengths and limitations. The GPT-4 series excel in reasoning, multi-modal tasks, and code generation, but requires high computational resources and shows variability across updates [1]. Claude 3.5 focuses on safety and ethics in code generation, performing well in simple tasks but struggling with complex logic [2]. CodeQwen is efficient in domain-specific tasks like structured retrieval and technical analysis but lacks adaptability for general use [3]. The LLaMA 3.1 series are open-source, emphasizing efficiency and accessibility, but their reliance on public datasets limits generalization in niche"}, {"title": "III. DESIGN PATTERN EVALUATION", "content": "A. Experiment design\n1) Overview: We evaluate the design pattern comprehension ability of code LLMs in different tasks across two programming languages, Python and Java: (i) Design Pattern Classification, which assesses the model's ability to identify and classify different object-oriented design patterns, such as Singleton, Factory, and Observer, within given code snippets. The objective is to determine if the model can accurately recognize structural and behavioral characteristics that define each design pattern in both Python and Java; (ii) Line Completion, where the model completes missing lines within a code snippet that follows a particular design pattern. This task tests the model's understanding of the context and requirements of specific design patterns, as it must generate lines that adhere to the pattern's conventions in each language; and (iii) Function Generation, which evaluates the model's ability to generate entire functions with given description in with/without given design pattern, assessing whether the model can recognize and apply structural and procedural elements of the pattern to fulfill functional requirements effectively in both Python and Java.\n2) Models: Our study draws upon a wide range of state-of-the-art large language models (LLMs), each selected to showcase distinct strengths across various model families and architectural designs. We employ general-purpose models, such as GPT-4 and Claude, alongside code-focused models like GLM and DeepSeek, which are tailored for tasks involving code understanding and generation. Additionally, we include models from specific series, such as Qwen and LLaMA, and incorporate multimodal and multilingual capabilities through options like Yi and Mistral. This diverse model set enables a robust evaluation of LLM performance on code-centric tasks, with a particular focus on assessing capabilities in design pattern comprehension within domain-specialized applications.\n3) Datasets: We manually selected 48 high-quality repositories from GitHub, with 24 in Python and 24 in Java, representing 12 design patterns. Each pattern was represented by two repositories per language, and the code files were categorized into three levels based on complexity: simple, moderate, and difficult. For the Design Pattern Classification task, we provided code files to LLMs to determine the design pattern. In the Line Completion task, we randomly removed three separate lines of effective code and asked the LLMs to complete it based on the context. For the Function Generation task, we extracted a function, used GPT-4 to write a description, and provided the context and description to the LLMs to generate the function. We test both with and without given design pattern to evaluate the models' understanding.\n4) Metrics: For the classification results, we used simple accuracy to evaluate the models' performance. For the code completion and generation tasks, we used two metrics: code similarity (CS) and code edit similarity (ES). CS measures the overlap between the generated code and the reference code. We use difflib's SequenceMatcher, which employs the Ratcliff/Obershelp algorithm to finds the Longest Matching Subsequence (LCS) between two sequences and then recursively processes the unmatched parts to calculate the similarity ratio. ES measures the minimal number of edit operations (insertion, deletion, substitution) required to transform the generated code into the reference code, normalized by the length of the reference."}, {"title": "B. Evaluation on Design Pattern Classification", "content": "1) Empirical Analysis: As shown in Table I, GPT-40 and Llama-31-70B achieved the highest overall accuracy, both scoring 38.81% across all datasets. In the Java datasets, Llama-31-70B performed best overall with 53.33% accuracy on Java All, and it excelled in the Java Hard subset with 55.56%. Both GPT-40 and Llama-31-70B achieved the highest accuracy on Java Easy at 71.43%. For the Python datasets, GPT-40 led with an overall accuracy of 29.73% on Python All. On Python Easy, several models, including CodeQwen, Mistral-123B, Yi-34B, and GLM-3-6B, matched high accuracy levels at 44.44%. A general trend across all models is a decrease in performance from Java to Python and from Easy to Hard datasets. This suggests that the LLMs find Python code and more complex code samples more challenging, possibly due to differences in language syntax or less exposure to complex patterns during training. In summary, while GPT-40 and Llama-31-70B stand out in overall performance, all models show room for improvement, especially in handling complex Python code.\nFrom the heat map shown in Figure 1, when using LLMs to identify design patterns in code, Singleton and Factory patterns are often over-predicted or misattributed due to their prevalence and recognizable structure. Singleton involves centralized object management, which overlaps with patterns like Facade, Proxy, or Command, leading to frequent misclassification. Factory, similarly, is confused with Abstract Factory, Builder, or Strategy due to shared object creation logic. Frameworks like Spring (Java) or Python's dependency injection further blur these distinctions, making it hard for LLMs to discern intent. Real-world implementations often combine patterns, adding to the challenge. LLMs heuristically rely on static methods, single-instance management, or object creation cues, which can align with Singleton or Factory but are not exclusive to them. Ambiguity in natural language prompts and insufficient training data also contribute to frequent misclassifications. Addressing these issues requires improved training data, clearer distinctions, and refined contextual understanding of overlapping features in code. LLMs also struggle with the Facade pattern due to its abstract nature and contextual dependency. Unlike Singleton or Factory, which have distinct structural markers, Facade simplifies access to complex subsystems via a unified interface, making its identification challenging. Its reliance on naming conventions and subtle structural cues, along with integration with other dominant patterns, often masks its intent. This complexity, combined with insufficient training examples, makes Facade one of the harder patterns for LLMs to reliably identify.\nInsights: The classification results indicate that even the best-performing models, GPT-4o and Llama-31-70B, achieved only 38.81% overall accuracy, highlighting substantial room for improvement in design pattern recognition. A consistent decline in performance from Java to Python and from Easy to Hard datasets suggests that LLMs find Python code and complex samples more challenging, due to syntax differences and less exposure during training. The heatmaps reveal that Singleton and Factory patterns are often over-predicted or misclassified. Their prevalence and distinctive structures make them default predictions, but overlaps with patterns like Facade or Strategy lead to confusion. LLMs particularly struggle with the Facade pattern due to its abstract nature and lack of explicit structural markers, making it difficult to identify without contextual understanding. These findings underscore the need for enhanced training data, clearer distinctions between"}, {"title": "C. Evaluation on Line Completion and Function Generation", "content": "1) Empirical Analysis: As shown in Table II, we find: (1) CS and ES Relationship: Higher CS correlates with higher ES, meaning that code closely matching the original typically requires less effort to adapt. However, this relationship isn't always linear. For instance, GPT-4o achieves high CS without design pattern knowledge, but its ES scores suggest significant edits may still be required, especially in line completion tasks. (2) Impact of Design Pattern Knowledge and Model Variability: Several models, including GPT-40 and Llama-31-405B, perform better without design pattern information, implying reliance on internalized patterns from training. Claude35 consistently excels in generating accurate, modifiable code, while models like CodeQwen show lower performance. Function generation often yields higher similarity scores than line completion, suggesting better model handling of larger code segments. Overall, improving design pattern integration, practical utility, and task-specific optimization could enhance LLM reliability and reduce manual editing efforts.\nTable III highlights that providing design pattern improves model performance, as seen with patterns like Singleton, where GPT-4 achieved higher Code Similarity (33.64%) with pattern knowledge than without (13.20%). However, exceptions exist; some models, such as GPT-40, performed better without design pattern knowledge in patterns like Observer, suggesting effective reliance on internalized pattern knowledge. Performance varied across patterns, with Template and Decorator yielding higher scores, while Builder, Bridge, and Facade proved more challenging. A nuanced relationship between CS and ES was observed. Higher CS often corresponds with higher ES, suggesting that code closer to the original requires less effort to edit. However, there are cases where this relationship is not strong. For example, in the Decorator pattern, GPT-4 with design pattern knowledge had CS of 31.44% and ES of 35.37%, but without design pattern knowledge, CS increased and ES decreased. Model performance variability was evident, with GPT-40 and Llama-31-70B excelling in multiple patterns, while models like CodeQwen struggled.\nInsights: Results reveal a nuanced relationship between CS and ES, indicating that high CS does not always equate to reduced editing effort for programmers. While some LLMs generate code resembling the original, these outputs may still require significant modifications due to underlying issues. It highlights the need for future Code LLM to prioritize better integration of design pattern knowledge and the generation of code that minimizes manual edits. Enhancing both accuracy and practical utility makes LLMs more reliable in software development, saving time and reducing error potential."}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "This paper evaluates Code LLMs' ability to understand and generate code following design patterns. Experiments on classification, line completion, and function generation reveal that while design pattern knowledge often improves performance, some models rely on internalized patterns effectively. The study highlights challenges with complex patterns like Builder and nuances in the relationship between Code Similarity and Edit Similarity, emphasizing the need for improved design pattern integration and practical utility to enhance LLM reliability and reduce developer effort. We plan to expand the dataset to cover more object-oriented languages and incorporate more high-quality repositories. Additionally, we will observe evaluation results from more dimensions to ensure the reliability and practical guidance of our analyses."}]}