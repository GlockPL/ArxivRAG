{"title": "RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation", "authors": ["Soroush Nasiriany", "Sean Kirmani", "Tianli Ding", "Laura Smith", "Yuke Zhu", "Danny Driess", "Dorsa Sadigh", "Ted Xiao"], "abstract": "We explore how intermediate policy representations can facilitate generalization by providing guidance on how to perform manipulation tasks. Existing representations such as language, goal images, and trajectory sketches have been shown to be helpful, but these representations either do not provide enough context or provide over-specified context that yields less robust policies. We propose conditioning policies on affordances, which capture the pose of the robot at key stages of the task. Affordances offer expressive yet lightweight abstractions, are easy for users to specify, and facilitate efficient learning by transferring knowledge from large internet datasets. Our method, RT-Affordance, is a hierarchical model that first proposes an affordance plan given the task language, and then conditions the policy on this affordance plan to perform manipulation. Our model can flexibly bridge heterogeneous sources of supervision including large web datasets and robot trajectories. We additionally train our model on cheap-to-collect in-domain affordance images, allowing us to learn new tasks without collecting any additional costly robot trajectories. We show on a diverse set of novel tasks how RT-Affordance exceeds the performance of existing methods by over 50%, and we empirically demonstrate that affordances are robust to novel settings.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, we have seen the rise of large pretrained models for learning robot policies. Vision-language-action (VLA) models [8, 32], pretrained with large-scale robot data on top of vision-language models (VLMs) [48] come with the promise of generalization to new objects, scenes, and tasks. However, VLAs are not yet reliable enough to be deployed outside of the narrow lab settings on which they are trained. While these shortcomings can be mitigated by expanding the scope and diversity of robot datasets, this is highly resource intensive and challenging to scale.\nAlternatively, there are various ways of interfacing with the policy that can potentially facilitate generalization by providing useful guidance on how to perform manipulation tasks. Examples of these policy representations include language specifications [4, 53], goal images [6], goal sketches [45], and trajectory sketches [22]. These interfaces introduce mid-level abstractions that shield the policy from reasoning in a higher dimensional input space leading to policies that can generalize over these intermediate rep-resentations. While one of the most common policy rep-resentations is conditioning on language, in practice most robot datasets are labeled with underspecified descriptions of the task and language conditioning does not reveal enough guidance on how to perform the task. Alternatively, goal image-conditioned policies provide detailed spatial context about the final goal configuration of the scene. However, goal-images are high-dimensional, which presents learn-ing challenges due to over-specification issues [40, 45]. Furthermore, providing goal images at evaluation time is cumbersome for human users. This has lead to exploration of other intermediate representations trajectory or goal sketches [22, 45], or keypoints [19, 52] that attempt to provide spatial plans for the policy. While these spatial plans are informative, they still lack sufficient information for the policy on how to manipulate e.g. what pose of the gripper should take when picking up a clothes hanger.\nIn this work, we seek a policy representation that provides expressive yet lightweight abstractions for learning robust manipulation polices. We propose RT-Affordance, which is a policy conditioned on both language specifications and visual affordances. The visual affordances show the pose of the robot end effector at key stages of the task, visually projected onto the image input of the policy. By conditioning on affordances, the robot will have access to precise yet concise guidance on how to manipulate objects. To allow a seamless experience for the human user, we employ a hierarchical model that only requires task language from the user. The model first predicts the affordances given a task"}, {"title": "II. RELATED WORK", "content": "Affordances for robot manipulation. Affordances [2] and grasp pose predictions have been heavily leveraged in robotics research for motion planning, grasping, and hierarchical control. Modern data-driven methods [35, 46] build upon prior works which leverage optimization-based approaches, and achieve performant grasp pose prediction capabilities given large-scale grasping datasets [18] and point-cloud [34] or geometry based inductive biases [17]. More recently, robot manipulation systems propose combining vision-language models (VLMs) with affordance or grasp prediction models and downstream control policies [16, 24, 25, 47]. In contrast, our method RT-Affordance does not rely on large-scale offline grasp pose specific datasets, 3D point clouds at training or test time, or simulation-based geometric planning.\nLearning pre-trained representations from non-action data. Similar to trends seen in scaling up VLMs [49], there has also been exploration in robotics on adapting large-scale internet data for improving perception and reasoning capabilities [15] which are important for downstream robot policy learning, particularly with the usage of vision-language-action (VLA) models [8]. Non-robotics interaction datasets have been particularly of interest, due to the substantial cost of real-world robotics action data such as teleoperated expert demonstrations [31, 50]; representation learning methods which learn affordance prediction from internet data and human videos [13, 21] have been proposed [3, 5, 41]. Most similar to our method is RoboPoint [52], which proposes fine-tuning a VLM to predict points which represent spatial affordances by leveraging procedural 3D scene generation in simulation. Our method RT-Affordance also studies predicting spatial affordances, but proposes a more descriptive affordance representation beyond a single point, and also does not require large-scale simulated scene generation.\nIntermediate representations for policy conditioning. Prior works have studied how multi-task robot manipulation policies can be conditioned on various types of representations and interfaces to perform different manipulation skills. Popular interfaces have included one-hot task vectors [30], latent skill or task embeddings [23, 27, 39], templated or natural language [9, 28, 36, 42, 53], object-centric representations [19, 29, 43, 44], trajectories [22, 51], goal images or sketches [6, 7, 10-12, 37, 45], and videos [14, 20, 26]. Our method leverages affordances represented visually or textually as an interface, which strikes a balance between flexibility, expressivity, and data efficiency"}, {"title": "III. RT-A: AFFORDANCE-BASED POLICY LEARNING", "content": "Our goal is to implement an intermediate policy interface that (1) is an expressive yet compact representation for a broad set of manipulation tasks, (2) can effectively bridge knowledge from external datasets and facilitate generaliza-tion, and (3) enables learning novel tasks through cheap, in-"}, {"title": "A. Affordance-conditioned policies", "content": "We are given a dataset of robot trajectories $D = \\{\\ell, \\{(o_i, e_i, g_i, a_i)\\}_{i=0}^n\\}$; each trajectory consists of a language instruction $\\ell$ and a sequence of images $o_i$, actions $a_i$, end-effector poses $e_i$ and gripper states $g_i$. We learn an affordance-conditioned policy $\\pi(a | l, o, q)$ that generates actions given the language instruction $l$, current image $o$, and additionally the affordance plan $q$. We define the affordance plan as the sequence of robot end effector poses correspond-ing to key timesteps in the trajectory, $q = (e_{t_1}, e_{t_2}, ..., e_{t_n})$. These timesteps capture critical stages in the task execution, for example when the robot is about to come in contact with objects or encounters bottleneck states. We can employ a variety of approaches to extract these timesteps. In practice we adopt a simple and scalable solution: we automatically extract from proprioception data timesteps when the gripper state changes from open to close ($g_{i-1} > 0$ and $g_i < \\alpha$ for some constant $\\alpha$) or vice versa from close to open, or the final timestep of the trajectory. This implicitly captures object-centric interactions corresponding to the stages in the task when the robot contacts, grasps, pushes, or lets go of objects. Compared to conditioning on language as in prior work [8], the affordance plans in RT-A policies reveal precise spatial information about how to manipulate objects. These affordance plans not only reveal the position of the robot end effector but also orientation, which is critical for fine-grained manipulation. However, solely conditioning on affordance plans may not reveal full context about the task, and we thus opt to condition the policy on both affordance plans and language. This ensures that we retain the full expressiveness of language-conditioned policies, while benefiting from the additional context provided by affordance plans.\nWe train the affordance-conditioned policy via behavioral cloning and additionally co-train on web datasets, in a similar manner as in RT-2. We can represent these affordances either as tokenized text values passed as input to the policy, or by overlaying them onto the image using a visual operator $\\psi(o, q)$, following similar techniques in prior work [22, 38]. In our implementation we visually project the outline of the robot hand at the poses $e_i$ onto the image. Specifically given $e_i$ we compute the 3D positions of the leftmost end effector tip, rightmost finger tip, top of end effector, and arm, and project these points onto the 2D image. We connect these points to make an outline. See Figure 3 for an illustration. We designate unique colors to each of the affordances overlaid onto the image to capture temporal ordering. Note that this projection step assumes knowledge of the robot camera intrinsics and extrinsics which is readily available for many robot platforms. If this information is not available, we can opt to condition the policy on the affordance plan directly as tokenized text values."}, {"title": "B. Learning to predict affordances", "content": "We can deploy the affordance-conditioned policy by ask-ing the human user to provide affordance plans and language goals to the policy at inference time. The affordance plans can be provided easily by marking them visually onto the image using a UI interface, without moving the robot of changing the scene. Compared to prior approaches such as conditioning on goal images or trajectory sketches, affor-dance plans are lower dimensional, making them easier to provide. We can also learn models to predict affordance plans automatically, sidestepping the need for humans to provide affordances at all at test time.\nWe learn an affordance prediction model $\\phi(q | l, o)$ which predicts the affordance plan given the language task instruc-tion $l$ and initial image of the scene $o$. To train the model we extract $(o,l, q)$ tuples from the same robot dataset $D$ used to train $\\pi$ and we also co-train the model with web datasets. In applications where we have access to camera information we predict the xy pixel locations of the end effector points, allowing us to better transfer knowledge with other existing web datasets such as object detection. In some applications, training on these datasets may not yield adequate performance and we may seek additional training data to further improve the capabilities of the model. Instead of collecting additional demonstrations through expensive robot teleoperation, we can collect a set of images with corresponding task labels, ie. $D_{aug} = \\{(o_i,l_i)\\}_{i=0}^n$. We can collect hundreds or thousands of these images at a fraction of the cost compared to costly teleoperation. After this data"}, {"title": "C. Model Inference", "content": "We are given the initial image of the scene $o_{init}$ and a natural language task instruction l. We can either prompt a human or the affordance prediction model $(q|l, o_{init})$ to obtain the affordance plan q. The affordance plan is projected onto the image, i.e., $\\psi(o,q)$ and we prompt the policy $\\pi(a | l, \\psi(o, q))$ with the language instruction and annotated image to execute the task. We can optionally replan updated affordance plans at fixed or adaptive intervals to handle novel scenarios that arise during the execution of the policy."}, {"title": "IV. EXPERIMENTS", "content": "In our experiments we are interested in exploring the following questions:\n\u2022 Are affordance-conditioned polices broadly useful for performing diverse manipulation tasks?\n\u2022 Can affordances enable efficiently learning novel tasks, without costly additional robot teleoperation?\n\u2022 How well do affordance prediction models generalize to novel objects, camera views, and backgrounds?"}, {"title": "A. Experiment implementation", "content": "We use the robot manipulator from RT-1 [9]. The arm is controlled via Cartesian end-effector control. The robot ob-serves the environment from a single head-mounted camera. Our robot demonstration datasets comprise three phases of data collection: (1) the RT-1 dataset [9] which focuses on basic manipulation skills, (2) the MOO dataset [43] which focuses on picking diverse objects, and (3) an additional set of trajectories targeting more dexterous tasks. We use the same web datasets from RT-2 for co-training. We adopt PaLM-E 2 [1, 15] as the underlying model and use the 1-billion parameter variant, unless otherwise noted. We train and evaluate vision-language-action models (VLAs) which share the same underlying model but adopt different policy input interfaces. All methods are trained on the same number of robot trajectories and same web datasets. We train the affordance prediction model with the hindsight affordance labels from the robot trajectory datasets, in addition to a set of ~750 cheap-to-collect images manually annotated with affordance labels. We collect these images by placing diverse objects on the table in front of the robot and taking snapshots of the scene. These images include the tasks and objects from our grasping tasks and additional tasks beyond grasping which we will outline the tasks in the following sections."}, {"title": "B. Learning to grasp novel objects efficiently", "content": "In our first experiment we investigate how affordances facilitate learning to grasp novel objects. Grasping is a ubiquitous skill demanded across a wide range of tasks, and it is important that robots can grasp diverse objects in a robust manner. We design a benchmark of picking diverse household objects, including dustpans, kettles, pots, boxes, and headphones. In contrast to simple objects with rigid convex shapes, we selected these objects as they encompass complex shapes and require fine-grained part-level reasoning in order to successfully grasp them. Note that our benchmark focuses on unseen object categories, meaning that they are not present in any of our robot trajectory datasets. We place the object on a tabletop in addition to two or three distractor objects coming from a wide range of object categories. We run comprehensive evaluations comparing our method to prior state-of-the-art approaches. We evaluate each method across five rollouts per object category and record the task success rates.\nFirst we compare to RT-2 [8], a state-of-the-art language-conditioned robot policy learning model notable for its impressive capabilities in understanding novel semantic con-cepts and objects. Despite these capabilities, we find that it struggles on our suite of evaluations, achieving an average success rate of just 28%. We observe that the policy is generally capable in identifying the correct object on the table and reaching the vicinity of the object but is unable to grasp the object at the appropriate location. For example, the robot attempts to grasp at the center of the dustpan rather than the handle, resulting in an unsuccessful grasp. Similarly with picking the pot the robot tries to grasp around the base of the pot rather than handle. However, it is generally capable of picking boxes. We also tried to prompt the policy with specific language instructions indicating how to grasp the"}, {"title": "C. Beyond object picking", "content": "We demonstrate that these findings are not exclusive to grasping tasks but can be extended to a range of manipulation tasks. We compare RT-A to the next best baseline from the previous experiments, the language-conditioned RT-2 model, on an additional set of manipulation tasks. We consider tasks involving placing objects into receptacles (eg. \"place apple into pot\", \"place bell pepper into basket\"), and articulated manipulation (\"close the cubby\"). Again, we highlight that these tasks are unseen in the robot trajectory datasets and demand precise spatial reasoning and execution. results. Surprisingly, the RT-2 baseline performs quite poorly in this setting achieving only 3% success rate. We observe a range of failures, including unreliable grasping of objects, freezing after grasping the object, placing the object next to the receptacle rather than into the receptacle. Using the same underlying VLA architecture but additionally conditioning on affordances, and we employ the affordance conditioned model trained to predict affordances on a handful of images annotated with affordance labels. We see a signif-icant improvement of performance, with 70% success rate using our affordance prediction model. These results show that affordances are a flexible form of task specification that can describe a broad set of tasks. In cases where the user provides oracle affordances at evaluation, we can solve novel tasks without any additional data, and training our affordance prediction model to infer affordances automatically only incurs a small budget to collect and annotate images. In contrast, improving the capabilities of language-conditioned or goal-conditioned policies would require fine-tuning on dozens or hundreds of additional robot demonstrations col-lected through teleoperation [4, 33], which is significantly more expensive and less scalable."}, {"title": "D. Robustness to out of distribution factors", "content": "Next, we perform an analysis of the affordance prediction model. In order for the affordance prediction model to be useful it needs to be robust to a wide range of out-of-distribution (OOD) settings. To better understand this, we"}, {"title": "E. Ablation study", "content": "We perform an ablation study on our affordance prediction model, where we study the impact of different data sources on the model. Our model is trained on the full data mixture including (1) robot trajectories, (2) web datasets, and (3) the 750 additional augmented affordance images we collected. We perform ablations where we (a) exclude the augmented data (No aug data) and (b) exclude web datasets (No web data). We compare these settings on the same in-distribution evaluation suite outlined in Section IV-D, and we report results. We see that removing these sources of data leads to a large drop in performance. We hypothesize that large web datasets play an important role for training robust models, and that our augmented data is needed to train performant models for specific downstream tasks."}, {"title": "V. CONCLUSION", "content": "We have presented RT-Affordance, a hierarchical method that uses affordances as an intermediate representation for policies. Affordances provide precise spatial guidance on how to preform a manipulation task, and they are easy for hu-mans to specify. We have shown empirically that affordance-conditioned policies can perform a wide range of novel tasks without requiring additional human demonstrations. Additionally, we have shown how we can learn models to predict affordances during deployment using cheap-to-collect images, and that these models are robust. One limitation that we observed is that our policy did not exhibit generalization to completely novel motions or skills. This has been noted in other VLA works [8] as well, and we are interested in exploring this in future work. In addition we are interested in exploring the complementary strengths of different policy representations and combining their capabilities into a single model that can share knowledge across representations."}]}