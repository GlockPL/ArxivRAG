{"title": "Variable-Agnostic Causal Exploration for Reinforcement Learning", "authors": ["Minh Hoang Nguyen", "Hung Le", "Svetha Venkatesh"], "abstract": "Modern reinforcement learning (RL) struggles to capture real-world cause-and-effect dy-\nnamics, leading to inefficient exploration due to extensive trial-and-error actions. While\nrecent efforts to improve agent exploration have leveraged causal discovery, they often make\nunrealistic assumptions of causal variables in the environments. In this paper, we introduce\na novel framework, Variable-Agnostic Causal Exploration for Reinforcement Learning (VAC-\nERL), incorporating causal relationships to drive exploration in RL without specifying envi-\nronmental causal variables. Our approach automatically identifies crucial observation-action\nsteps associated with key variables using attention mechanisms. Subsequently, it constructs\nthe causal graph connecting these steps, which guides the agent towards observation-action\npairs with greater causal influence on task completion. This can be leveraged to generate\nintrinsic rewards or establish a hierarchy of subgoals to enhance exploration efficiency. Ex-\nperimental results showcase a significant improvement in agent performance in grid-world, 2d\ngames and robotic domains, particularly in scenarios with sparse rewards and noisy actions,\nsuch as the notorious Noisy-TV environments.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) is a machine learning paradigm wherein agents learn to improve\ndecision-making over time through trial and error [25]. While RL has demonstrated remarkable\nsuccess in environments with dense rewards [15, 23], it tends to fail in case of sparse rewards where\nthe agents do not receive feedback for extended periods, resulting in unsuccessful learning. Such\nscarcity of rewards is common in real-world problems: e.g., in a search mission, the reward is only\ngranted upon locating the target. Prior studies tackle this problem by incentivizing exploration\nthrough intrinsic rewards [26, 3], motivating exploration of the unfamiliar, or with hierarchical\nreinforcement learning (HRL) [13, 31, 17]. However, these methods encounter difficulties when\nscaling up to environments with complex structures as they neglect the causal dynamics of the\nenvironments. Consider the example of a search in two rooms (Fig. 1(a, b)), where the target is\nin the second room, accessible only by opening a \"door\" with a \"key\" in the first room. Traditional\nexploration methods might force the agent to explore all corners of the first room, even though\nonly the \"key\" and \"door\u201d areas are crucial. Knowing that the action \"pick up key\" is the cause\nof the effect \"door opened\" will prevent the agent from aimlessly wandering around the door\nbefore the key is acquired. Another challenge with these approaches is the Noisy-TV problem\n[3], where the agent excessively explores unfamiliar states and actions that may not contribute\nto the ultimate task. These inefficiencies raise a new question: Can agents effectively capture\ncausality to efficiently explore environments with sparse rewards and distracting actions?\nInspired by human reasoning, where understanding the relationship between the environmen-\ntal variables (EVs) helps exploration, causal reinforcement learning (CRL) is grounded in causal\ninference [29]. CRL research often involves two phases: (i) causal structure discovery and (ii)\nintegrating causal knowledge with policy training [29]. Recent studies have demonstrated that\nsuch knowledge significantly improves the sample efficiency of agent training [8, 22, 30]. However,\ncurrent approaches often assume assess to all environmental causal variables and pre-factorized\nenvironments [22, 8], simplifying the causal discovery phase. In reality, causal variables are not\ngiven from observations, and constructing a causal graph for all observations becomes a non-\ntrivial task due to the computational expense associated with measuring causality. Identifying\nEVs crucial for downstream tasks becomes a challenging task, thereby limiting the effectiveness\nof CRL methods. These necessitate the identification of a subset of crucial EVs before discovering\ncausality.\nThis paper introduces the Variable-Agnostic Causal Exploration for Reinforcement\nLearning (VACERL) framework to address these limitations. The framework is an iterative\nprocess consisting of three phases: \u201cCrucial Step Detection\u201d, \u201cCausal Structure Discovery\u201d, and\n\u201cAgent Training with Causal Information\". The first phase aims to discover a set of crucial\nobservation-action steps, denoted as the SCOAS. The term \"crucial observation-action step\"\nrefers to an observation and an action pair stored in the agent's memory identified as crucial for\nconstructing the causal graph. We extend the idea of detecting crucial EVs to detecting crucial\nobservation-action steps, motivated by two reasons. Firstly, variables in the environment are\nassociated with the observations, e.g., the variable \"key\" corresponds to the agent's observation\nof the \"key\". Secondly, actions also contribute to causality, e.g., the agent cannot use the \"key\"\nwithout picking it up. One way of determining crucial observation-action steps involves providing\nthe agent with a mechanism to evaluate them based on their contribution to a meaningful\ntask [9]. We implement this mechanism using a Transformer architecture, whose task is to\npredict the observation-action step leading to the goal given past steps. We rank the significance\nof observation-action steps based on their attention scores [28] and pick out the top-ranking\ncandidates since the Transformer must attend to important steps to predict correctly.\nIn Phase 2, we adapt causal structure learning [10] to discover the causal relationships among\nthe observation-action steps identified in the discovered SCOAS set, forming a causal graph G.\nThe steps serve as the nodes of the causal graph, while the edges can be identified through a\ntwo-phase iterative optimization of the functional and structural parameters, representing the\nStructure Causal Model (SCM). In Phase 3, we train the RL agent based on the causal graph\nG. To prove the versatility of our approach in improving the sample efficiency of RL agents,\nwe propose two methods to utilize the causal graph: (i) formulate intrinsic reward-shaping\nequations grounded on the captured causal relationship; (ii) treat the nodes in the causal graph as\nsubgoals for HRL. During subsequent training, the updated agent interact with the environments,\ncollecting new trajectories for the agent memory used in the next iteration of Phase 1.\nIn our experiments, we use causally structured grid-world and robotic environments to empir-\nically evaluate the performance improvement of RL agents when employing the two approaches in\nPhase 3. This improvement extends not only to scenarios with sparse rewards but also to those\""}, {"title": "2 Related Work", "content": "Causal Reinforcement Learning (CRL) is an emerging field that integrates causality and\nreinforcement learning (RL) to enhance decision-making in RL agents, addressing limitations\nassociated with traditional RL, such as sample efficiency and explainability [29]. CRL methods\ncan be categorized based on their experimental setups, whether they are online or offline [29].\nOnline-CRL involves real-time interaction with the environment [5, 8, 30, 22], while Offline-CRL\nrelies on learning from a fixed previously collected dataset [24, 18]. Our framework operates\nonline, using trajectories from an online policy for an agent training while simultaneously con-\nstructing the underlying causal graph. Prior works in CRL have focused on integrating causal\nknowledge into RL algorithms and building causal graphs within the environment. Pitis et al.,\n[18] use Transformer model attention weights to generate counterfactual data for training RL\nagents, while Coroll et al., [5] use causal effect measurement to build a hierarchy of control-\nlable effects. Zhang et al., [32] measure the causal relationship between states and actions with\nthe rewards and redistribute the rewards accordingly. For exploration purposes, CRL research\nintegrates causal knowledge by rewarding the agents when they visit states with higher causal\ninfluence [22, 30] or treating the nodes of the causal graph as potential subgoals in HRL [8].\nZhang et al., [30] measure the average causal effect between a predefined group of variables and\nuse this as a reward signal, meanwhile, Seitzer et al., [22] propose conditional mutual information\nas a measurement of causal influence and use it to enhance the exploration of the RL agent. Hu\net al., [8] introduce a continuous optimization framework, building a causal structure through\na causality-guided intervention and using it to define hierarchical subgoals. Despite advance-\nments, previous methods often assume prior knowledge of EVs and the ability to factorize the\nenvironment accordingly. Our framework autonomously detects crucial steps associated with the\nkey EVs, enabling causal structure learning without predefined EVs, thus, distinguishing it from\nprevious methods. The causal graph uncovered by VACERL is versatile and can complement\nexisting RL exploration methods, such as intrinsic reward motivation or as hierarchical subgoals.\nIntrinsic Reward Motivation addresses inefficient training in sparse reward RL environ-\nments; an issue associated with random exploration techniques like e-greedy [2]. The core idea\nunderlying these motivation strategies is to incorporate intrinsic rewards, which entail adding\nbonuses to the environment rewards to facilitate exploration [2, 26, 3]. These methods add\nbonuses to environment rewards to encourage exploration, either based on prediction error [3] or\ncount-based criteria [2, 26]. However, they struggle to scale to complex structure environments,\nespecially in the scenario of Noisy-TV, where the agent becomes excessively curious about unpre-\ndictable states and ignores the main task [3]. VACERL tackles this by incorporating a mechanism\nto identify essential steps for the primary task and construct the causal graph around these steps,\nthus, enabling the agent to ignore actions generating noisy-TV.\nGoal-conditioned Hierarchical Reinforcement Learning (HRL) is another approach\nthat is used to guide agent exploration. Levy et al., [13] propose a multilevel policies framework,\nin which each policy is trained independently and the output of higher-ranking policies are used\nas subgoals for lower-level policies. Zhang et al., [31] propose an adjacency constraint method\nto restrict the search space of subgoals, whereas, Pitis et al., [17] introduce a method based on\nmaximum entropy gain motivating the agent to pursue past achieved goals in sparsely explored\nareas. However, traditional HRL methods often rely on random subgoals exploration, which has\nshown inefficiency in learning high-quality hierarchical structures compared to causality-driven\napproaches [8, 7]. Hu et al., [8] operate under the assumption of pre-availability and disentangle-\nment of causal EVs from observations, using these EVs as suitable subgoals for HRL. However,\nthey overlook cases where these assumptions are not applicable, e.g., the observation is the im-\nage. In our apprroach, subgoals are determined by abstract representations of the observation\nand action, thereby, extending the applications of causal HRL to unfactorized environments."}, {"title": "3 Methods", "content": "3.1 Background\nRL Preliminaries.\nWe are concerned with the Partially Observable Markov Decision Process (POMDP) framework,\ndenoted as the tuple (S, A, O, P, Z, r, y). The framework includes sets of states S, actions A,\nobservations O providing partial information of the true state, a transition probability function\nP(s' | s,a), and an observation model Z denoted as Z(o | s,a), indicating the probability of\nobserving o when taking action a in states.r:S\u00d7A \u2192 R is a reward function that defines\nthe immediate reward that the agent receives for taking an action in a given state, and discount\nfactor y. The objective of the RL agent is to maximize the expected discounted cumulative\nreward \u0395\u03c0,\u03a1 [\u03a3t=0\u03bdt\u03b3tr (st, at)], over a policy function mapping a state to a distribution over\nactions.\nCausality.\nCausality is explored through the analysis of relationships among variables and events [16]. It can\nbe described using the SCM framework [16]. SCM, for a finite set V comprising M variables, is\nVi := fi(PA(Vi)(G), Ui), \u2200i \u2208 {1, . . ., M}, where F = {f1, f2, \u2026, fm } denotes the set of generating\nfunctions based on the causal graph G and U = {U1, U2, ..., UM} represents the set of noise in\nthe model. The graph G = {V, E} provides the edge eij \u2208 E, representing variable Vi causes on\nvariable Vj, where eij = 1 if Vj \u2208 PA(Vi), else, eij = 0. The SCM framework can be characterized\nby two parameter sets: the functional parameter &, representing the generating function f; the\nstructural parameter \u03b7 \u2208 RM\u00d7M, modelling the adjacency matrix of G [10].\n3.2 Variable-Agnostic Causal Exploration Reinforcement Learning Frame-\nwork\n3.2.1 Overview.\nThe primary argument of VACERL revolves around the existence of a finite set of environment\nvariables (EVs) that the agent should prioritize when constructing the causal graph. We provide\na mechanism to detect these variables, aiming to reduce the number of nodes in the causal\ngraph mitigating the complexity of causal discovery. Initially, we deploy an agent to randomly\nexplore the environment and gather successful trajectories. Once the agent accidentally reaches\nthe goal a few times, we initiate Phase 1, reformulating EVs detection into finding the \u201ccrucial\nobservation-action steps\u201d (COAS) from the collected trajectories. The agent is equipped with\nthe ability to rank the importance of these steps by employing the Transformer (TF) model's\nattention scores (as). Top-M highest-score steps will form the crucial set SCOAS. Subsequently,\nin Phase 2, we identify the causal relationships among steps in SCOAS to learn the causal graphs\nG of the environment. In Phase 3, where we extract a hierarchy causal tree from graph G and\nuse it to design two approaches, enhancing the RL agent's exploration capability. We then utilize\nthe updated agent to gather more successful trajectories and repeat the process from Phase 1.\nSee Fig. 1(c) for an overview of VACERL and detailed implementation in Supp. A 1."}, {"title": "3.2.2 Phase 1: Crucial Step Detection.", "content": "We hypothesize that important steps (a step is a pair of observation and action) are those in\nthe agent's memory that the agent must have experienced to reach the goal. Hence, these steps\nshould be found in trajectories where the agent successfully reaches the goal. We collect a buffer\nB = ({ot1, at1}t=1T1, {ot2, at2}t=1T2,..., {otn, atn}t=1Tn), where n is the number of episodes wherein the\nagent successfully reaches the goal state, ot and at is the observation and action, respectively at\nstep t in an episode, and Tk is the number of steps in the k-th episode. We train the TF model,\nwhose input consists of steps from the beginning to the second-to-last step in each episode and\nthe output is the last step. The reasoning behind choosing the last step as the prediction target\nis that it highlights which steps in the trajectories are crucial for successfully reaching the goal.\nFor a training episode k-th sampled from B, we predict (otTk, atTk) = TF({otk, atk}t=1Tk\u22121). The model\nis trained to minimize the loss L TF = Ek [MSE ((otTk, atTk), (otTk, atTk))], where MSE is the mean\nsquare error. Following training, we rank the significant observation-action steps based on their\nattention scores as (detailed in Supp. A)\nand pick out the top-M highest-score steps. We argue that the top-attended steps should\ncover crucial observations and actions that contribute to the last step prediction task, associated\nwith meaningful causal variables. For instance, observing the key and the action of picking it up\nare linked to the variable \"key\".\nIn continuous state space, the agent may repeatedly attend to similar steps involving the\nsame variable. For example, the agent might select multiple instances of observing the key, from\ndifferent positions where the agent is located, and picks it up. As a result, the set SCOAS will be\nfilled with similar steps relating to picking up the key and ignoring other important steps. To\naddress this, we introduce a function is_sim to decide if two steps are the same:\n\u2022 For discrete action space environments, is_sim ((o, a), (o', a')) = 1\nif cos (o, o') > \u222esim and a = a', else 0.\n\u2022 For continuous action space environments, is_sim ((o, a), (o', a')) = 1 if cos ((o, a), (o', a')) >\nOsim, else 0.\nwhere Cos (o, o') = \\frac{o \\cdot o'}{\\|o\\| \\cdot \\|o'\\|} and Osim is a similarity threshold. Intuitively, if the agent has\ntwo observations with a high cosine similarity and takes the same action, these instances are\ngrouped. The score as for a group is the highest as among the steps in this group. The proposed\nis_sim method will also be effective in noisy environments, particularly when the observations\nare trained representations rather than raw pixel data. Subsequently, we add the steps with the\nhighest as to SCOAS. We define an abstract function I to map a pair (ota, ata) to an element i in\nScoas: i = I ((otaata)) \u21d4 is_sim ((ota, ata), (o, a)) = 1 and collect a new buffer B*, where:\nB* = B {(ot, at):(I((ot, at)) \u2208 Scoas}\nHere, B* is B removing steps that are unimportant (not in SCOAS)."}, {"title": "3.2.3 Phase 2: Causal Structure Discovery.", "content": "Inspired by the causal learning method proposed by Ke et al., [10], we uncover the causal rela-\ntionships among M steps identified in the SCOAS set. Our approach optimizes the functional\nparameter & and the structural parameter \u03b7 associated with the SCM framework. The optimiza-\ntion of these parameters follows a two-phase iterative update process, wherein one parameter is"}, {"title": "3.2.4 Phase 3: Agent Training with Causal Information.", "content": "We extract a refined hierarchy causal tree from graph G with an intuition to focus on steps that\nare relevant to achieving the goal. Using the goal-reaching step as the root node of the tree, we\nrecursively determine the parental steps of this root node within graph G, and subsequently for\nall identified parental steps. This causal tree is used to design causal exploration approaches.\nThese approaches include (i) intrinsic rewards based on the causal tree, and (ii) utilizing causal\nnodes as subgoals for HRL. For the first approach, we devise a reward function where nodes closer\nto the root are deemed more important and receive higher rewards, preserving the significance of\nthe reward associated with the root node and maintaining the agent's focus on the goal. In the\nsecond approach, subgoals are sampled from nodes in the causal tree, with nodes closer to the\nroot sampled more frequently. We present the detailed implementations and empirically evaluate\nthese approaches in Sec. 4.1 and Sec. 4.2."}, {"title": "4 Experiments", "content": "4.1 VACERL: Causal Intrinsic Rewards - Implementation and Evaluation\nCausal Intrinsic Reward.\nTo establish the relationship where nodes closer to the goal hold greater importance, while\nensuring the agent remains focused on the goal, we introduce intrinsic reward formulas as follows:\nr causal (o, a) = rg \u2212 \\frac{(d \u2212 1)}{h} ro \u2200(o, a) \u2208 Da\nwhere rg is the reward given when the agent reach the goal, rcausal (o, a) is the intrinsic reward\ngiven to a node (o,a), Da is the set of nodes at depth d of the tree, ro = a(rg/h) with a\nis a hyperparameter and h is the tree height. In the early learning stage, especially for hard\nexploration environments, the causal graph may not be well defined and thus, rcausal may not\nprovide a good incentive. To mitigate this issue, we augment rcausal with a count-based intrinsic\nreward, aiming to accelerate the early exploration stage. Intuitively, the agent is encouraged to\nvisit never-seen-before observation-action pairs in early exploration. Notably, unlike prior count-\nbased methods [2], we restrict counting to steps in Scoas, i.e., only crucial steps are counted.\nOur final intrinsic reward is:\nr + causal = \\frac{1}{\\sqrt{n(o,a)t}} r causal (ota, ata)\nwhere n(o,a)t is the number of time observation o and action a is encountered up to time step\nt. Starting from zero, this value increments with each subsequent encounter. We add the final\nintrinsic reward to the environment reward to train the policy. The total reward is r (st, at) =\nrenv (St, at) +r+ rcausal (St, at), where renv is the extrinsic reward provided by the environment.\nEnvironments.\nWe perform experiments across three sets of environments: FrozenLake (FL), Minihack (MH),\nand Minigrid (MG). These environments are tailored to evaluate the approach in sparse reward\nsettings, where the agent receives a solitary +1 reward upon achieving the goal (detailed in Supp.\nB)\nFL includes the 4x4 (4x4FL) and 8x8 (8x8FL) FrozenLake environments (Supp Fig. B.1(d,e))\n[27]. Although these are classic navigation problems, hidden causal relationships exist between\nsteps. The pathway of the agent can be conceptualized as a causal graph, where each node\nrepresents the agent's location cell and its corresponding action. For example, moving right from\nthe cell on the left side of the lake can be identified as the cause of the agent falling into the\nlake cell. We use these environments to test VACERL's efficiency in discrete state space, where\nis sim is not used.\nMH includes MH-1 (Room), MH-2 (Room-Monster), MH-3(Room-Ultimate) and MH-4\n(River-Narrow) [20]. These environments pose harder exploration challenges compared to FL\ndue to the presence of more objects. Some environments even require interaction with these ob-\njects to reach the goal, such as killing monsters (MH-2 and MH-3) or building bridges (MH-4).\nFor this set of environments, we use pixel observations.\nMG is designed based on Minigrid Environment [4], with escalating causality levels. These\ninclude the Key Corridor (MG-1) (Supp. Fig. B.1(a)) and 3 variants of the BlockUnlockPickUp:"}, {"title": "4.2 VACERL: Causal Subgoals - Implementation and Evaluation", "content": "Causal subgoals sampling.\nIn HRL, identifying subgoals often relies on random exploration [13, 31], which can be inefficient\nin large search spaces. We propose leveraging causal nodes as subgoals, allowing agents to\nactively pursue these significant nodes. To incorporate causal subgoals into exploration, we\nsuggest substituting a portion of the random sampling with causal subgoal sampling. Specifically,\nin the HRL method under experimentation where subgoals are randomly sampled 20% of the\ntime, we replace a fraction of this 20% with a node from the causal tree as a subgoal, while\nretaining random subgoals for the remainder. Eq. 7 denotes the probability of sampling a node\ni at depth d > 0 (excluding the root node as this is the ultimate goal) from the causal tree:\np(i) = \\frac{(di)^{-1/2}}{\\sum_{i=1}^{N} (di)^{-1}}\nwith di is the depth of node i and N is the number of nodes in the causal tree.\nEnvironments.\nWe use FetchReach and FetchPickAndPlace environments from Gymnasium-Robotics [11]. These\nare designed to test goal-conditioned RL algorithms. We opt for sparse rewards settings, in which\nonly a single reward of 0 is given if the goal is met, otherwise \u22121 (detailed in Supp. C).\nBaselines.\nHAC [13], a goal-conditioned HRL algorithm, serves as the backbone and a baseline. HAC is\nimplemented as a three-level DDPG [14] with Hindsight Experience Replay (HER) [1], where\nthe top two levels employ a randomized mechanism for subgoal sampling. We also evaluate\nour performance against the standard DDPG+HER algorithm [1] on the FetchPickAndPlace\nenvironment, as this is the more challenging task [19] and for comprehensiveness."}, {"title": "4.3 Ablation Study and Model Analysis", "content": "We use MG-2 (Fig. 1(a)) task and causal intrinsic reward for our analysis.\nCrucial Step Detection Analysis: We investigate how the the Transformer model TF's per-\nformance changes with varying buffer B sizes. As depicted in Fig. 4(a,b), increasing the number\nof trajectories in B enhances the framework's accuracy in detecting important steps through\nattention. Initially, with 4 trajectories (Fig. 4(a)), TF attends to all actions in the top-left grid.\nHowever, after being trained with 40 trajectories (Fig. 4(b)), TF correctly attends to pick-up"}, {"title": "5 Conclusion", "content": "This paper introduces VACERL, a framework that enhances RL agent performance by analyzing\ncausal relationships among agent observations and actions. Unlike previous methods, VACERL\naddresses causal discovery without assuming specified causal variables, making it applicable\nto variable-agnostic environments. Understanding these causal relationships becomes crucial\nfor effective agent exploration, particularly in environments with complex causal structures or\nirrelevant actions, such as the Noisy-TV problem. We propose two methods to leverage the\nidentified causal structure. Future research could explore other methods utilizing this structure.\nEmpirical evaluations in sparse reward navigation and robotic tasks demonstrate the superiority\nof our approach over baselines. However, a limitation is the introduction of new hyperparameters,\nwhich require adjustment for different settings."}, {"title": "A Details of Methodology", "content": "A.1 VACERL Framework\nThe detailed processing flow of the VACERL framework is described in Algo. 1. Buffer B is\ninitialized using the process from lines 2-6, using a random policy to collect successful trajectories\n(Note: as long as the agent can accidentally reach the goal and add 1 trajectory to B, we\ncan start the improving process). We, then, start our iterative process (the outer loop). In\nPhase 1, \"Crucial Step Detection\" (lines 8-21), the process commences with the training of the\nTransformer model TF using Algo. 2. Subsequently, we collect the dictionary D that maps\n(of, a) to attention score as. D is sorted based on as. We, then, define is_sim function (line 9)\nand abstract function I (line 10) to handle similar observation-action steps, and add the top M\n(of, a) steps to the set Scoas using the process from lines 12-20. After collecting ScoAs, we\napply Eq. 1 to acquire the new buffer B*. With buffer B*, we initiate Phase 2 (line 22) called\n\"Causal Structure Discovery\". We optimize the two parameters \u03b4 and \u03b7 using Algo. 3 and collect\nthe causal graph G. Using graph G, we collect the causal tree relative to the goal-reaching step\nto create a hierarchy of steps. We use this hierarchy to calculate the intrinsic reward associated\nwith (o, a) using Eq. 6 or to calculate subgoals sampling probability using Eq. 7. Finally, we\ntrain the policy \u03c0\u03b8 and adding new successful trajectories to buffer B, summarizing Phase 3\n(lines 23-29) called \u201cAgent Training with Causal Information\u201d. The process starts again from\nPhase 1 using the updated buffer B.\nA.2 Transformer Model Training\nDetailed pseudocode for training the Transformer model is provided in Algo. 2. We utilize the\nTransformer architecture implemented in PyTorch 2, for our TF model implementation. This\nimplementation follows the architecture presented in the paper [28], thus, the attention score\nas for a step is computed using the self-attention equation: softmax(\\frac{QKT}{\\sqrt{dk}}), where Q = XWQ\nrepresents the query vector, K = XWK represents the key vector, X is the learned embedding\nof a step (o, at), and WQ, WK are trainable weights. The values of as are extracted from the\nencoder layer of the TF model during the last training iteration. We use a step (f, a) as the\nkey in dictionary D that maps to an associated as, described in the process in lines 5-11 (Algo.\n2).\nA.3 SCM Training\nThe detailed pseudocode is provided in Algo. 3. Our approach involves a two-phase iterative\nupdate process, inspired by the causal learning method proposed by Ke et al., [10]. This process\noptimizes two parameters: the functional parameter d of generating function f and the structural\nparameter n of graph G, representing a Structural Causal Model (SCM). In Phase 1 of the\nprocess, we want to keep the structural parameter \u03b7 fixed and update the functional parameter\nd, whereas in Phase 2, we keep & fixed and update n. Both sets of parameters underwent training\nusing the buffer B* (Eq. 1). The generating function f is initialized as a 3-layer MLP neural"}, {"title": "A.4 Causal Tree Extraction", "content": "We extract a tree from the resultant causal graph G, focusing on the steps relevant to achieving\nthe goal. We use the goal-reaching step as the root of the tree and recursively determine the\nparental steps of this root node within graph G and add them to the causal tree as new nodes,\nand subsequently, we will determine the parental steps for all these identified nodes. However, to\navoid cycles in the tree, we need to add an order of ranking, thus, we use the ranking of attention\nscore as. So, for edge eij from variable Xi to variable Xj, we will remove the edge eij if as of Xi\nis smaller than the as of Xj, even if eij=1 according to the graph G."}, {"title": "B Setting to Test VACERL Causal Intrinsic Reward", "content": "B.1 Environments\nFrozen Lake Environments\nThese tasks involve navigating the FrozenLake environments of both 4x4 (4x4FL) and 8x8\n(8x8FL) [27]. Visualizations for these environments can be found in Fig. B.1(d,e). The goal of\nthe agent involves crossing a frozen lake from the starting point located at the top-left corner\nof the map to the goal position located at the bottom-right corner of the map without falling"}, {"title": "B.2 Baseline Implementations", "content": "The backbone algorithm is Proximal Policy Optimization (PPO). We use the Pytorch implemen-\ntation of this algorithm on the open-library Stable Baselines 33. To enhance the performance\nof this backbone algorithm, we fine-tuned the entropy coefficient and settled on a value of 0.001\nafter experimenting with [0.001, 0.005, 0.01, 0.05]. All other parameters were maintained as per\nthe original repository. Subsequently, we incorporated various intrinsic reward baselines on top\nof the PPO backbone, including Count-Based, Random Network Distillation (RND), ATTEN-"}, {"title": "C Setting to Test VACERL with Causal Subgoal for HRL", "content": "C.1 Environments\nBoth of the environments used in this section are available in Gymnasium-Robotics [11] and\nare built on top of the MuJoCo simulator [33]. The robot in question has 7 degrees of freedom\n(DoF) and a two-fingered parallel gripper. In FetchReach, the state space is S C R10, and\nin FetchPickAndPlace, the state space is S C R25. In both environments, the action space is\nAC [-1,1]4, including actions to move the gripper and opening/closing of the gripper. In\nFetchReach, the task is to move the gripper to a specific position within the robot's workspace,\nwhich is relatively simpler compared to FetchPickAndPlace. In the latter, the robot must grasp\nan object and relocate it.\nIn both cases, user are given two values which are \"achieved_goal\" and \"desired_goal\". Here,\n\"achieved_goal\" denotes the final position of the object, and \"desired_goal\" is the target to\nbe reached. In FetchReach, these goals represent the gripper's position since the aim is to\nrelocate it. While in FetchPickAndPlace, they signify the block's position that the robot needs\nto manipulate. Success is achieved when the Euclidean distance between \"achieved_goal\" and\n\"desired_goal\" is less than 0.05m.\nSparse rewards are employed in our experiments, wherein the agent receives a reward of -1\nif the goal is not reached and 0 if it is. The maximum number of timesteps allowed in these"}, {"title": "C.2 Baseline Implementations", "content": "We utilize the PyTorch implementation of DDPG+HER from the Stable Baselines 3 6 open-\nlibrary as one of our baselines. The hyperparameters for this algorithm are set to benchmark\nvalues available in RL-Zoo 7. We assess the performance of this baseline against the results pre-\nsented in the original robotic paper by Plappert et al. [19], noting similarities despite differences\nin environment versions.\nFor our HAC implementation, the core algorithm of our approach, we adopt a publicly\navailable code repository 8 (MIT License) by the author of the original paper [13]. We modify this\ncode to align with our environments, where the goal position and the goal condition are supplied"}, {"title": "C.3 Additional Experiment Results", "content": "To validate our assertion that causal subgoals can effectively narrow down the search space\nfor an HRL agent to significant subgoals, thus enhancing HRL sample efficiency in Robotic-\nEnvironments, we present an additional experiment along with the visualization of subgoals'\naverage coordinates selected by VACERL and Vanilla HAC in this experiment (Fig. C.1). The\nexperiment was conducted in the FetchReach environment, with the causal graph re-evaluated\nevery 2,000 steps, mirroring our main experiments. We specifically chose a run where initial\nsubgoals of HAC and VACERL exhibited similar average coordinates (x, y, z) for fairness. In\nthis run, the goal (indicated by a red + marker) was positioned at coordinates (1.38294353,\n0.68003652, 0.396999).\nAs illustrated in Fig. C.1(a), despite the initial subgoals' average coordinates being very\nsimilar (represented by blue markers) \u2013 (0.727372811, 0.501121846, 0.28868408) for HAC and\n(0.7377534, 0.521795303, 0.2345436) for VACERL \u2013 VACERL swiftly converges to subgoals much\ncloser to the goal after just one iteration of causal discovery learning, while, Vanilla HAC struggles\nto converge. We plot the weighted average coordinates of nodes in the causal graph after this\niteration (indicated by a grey + marker), with weights determined by the probability of node\nsampling according to Eq. 7; higher probabilities correspond to higher weights. We choose to\nplot the values of this iteration because it represents the instance where VACERL undergoes\nthe most significant shift in subgoals' coordinates. The results indicate that the coordinates of\nnodes in the causal graph closely align with the coordinates of subgoals sampled by the top-\nlevel policy. This supports our intuition that causal subgoals contribute to the improvement in"}, {"title": "D Architecture and Hyperparameters of VACERL", "content": "The default hyperparameters (if not specified in accompanying tables then these values were\nused) are provided in Table. 2. The definitions and values for hyperparameters, which require\ntuning and may vary across different environments, are specified in the accompanying tables.\nThe system's architecture and the explanation for tuning of hyperparameter are outlined below:\nArchitecture\n\u2022 TF model's architecture: num_encoder_layers = 2, num_decoder_layers = 2, hidden_size =\n128, dropout = 0.1.\n\u2022 Functional model fs's architecture: 3-layer MLP, hidden_size = 512.\n\u2022 PPO: Stable Baselines 3's hyperparameters with entropy coefficient = 0.001.\n\u2022 DDPG+HER: RL-Zoo's architecture and hyperparameters for FetchReach and FetchPickAnd-\nPlace environments."}]}