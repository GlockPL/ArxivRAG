{"title": "Variable-Agnostic Causal Exploration for Reinforcement Learning", "authors": ["Minh Hoang Nguyen", "Hung Le", "Svetha Venkatesh"], "abstract": "Modern reinforcement learning (RL) struggles to capture real-world cause-and-effect dynamics, leading to inefficient exploration due to extensive trial-and-error actions. While recent efforts to improve agent exploration have leveraged causal discovery, they often make unrealistic assumptions of causal variables in the environments. In this paper, we introduce a novel framework, Variable-Agnostic Causal Exploration for Reinforcement Learning (VAC-ERL), incorporating causal relationships to drive exploration in RL without specifying environmental causal variables. Our approach automatically identifies crucial observation-action steps associated with key variables using attention mechanisms. Subsequently, it constructs the causal graph connecting these steps, which guides the agent towards observation-action pairs with greater causal influence on task completion. This can be leveraged to generate intrinsic rewards or establish a hierarchy of subgoals to enhance exploration efficiency. Experimental results showcase a significant improvement in agent performance in grid-world, 2d games and robotic domains, particularly in scenarios with sparse rewards and noisy actions, such as the notorious Noisy-TV environments.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) is a machine learning paradigm wherein agents learn to improve decision-making over time through trial and error [25]. While RL has demonstrated remarkable success in environments with dense rewards [15, 23], it tends to fail in case of sparse rewards where the agents do not receive feedback for extended periods, resulting in unsuccessful learning. Such scarcity of rewards is common in real-world problems: e.g., in a search mission, the reward is only granted upon locating the target. Prior studies tackle this problem by incentivizing exploration through intrinsic rewards [26, 3], motivating exploration of the unfamiliar, or with hierarchical reinforcement learning (HRL) [13, 31, 17]. However, these methods encounter difficulties when scaling up to environments with complex structures as they neglect the causal dynamics of the environments. Consider the example of a search in two rooms (Fig. 1(a, b)), where the target is in the second room, accessible only by opening a \"door\" with a \"key\" in the first room. Traditional exploration methods might force the agent to explore all corners of the first room, even though only the \"key\" and \"door\u201d areas are crucial. Knowing that the action \"pick up key\" is the cause"}, {"title": "Variable-Agnostic Causal Exploration for Reinforcement Learning", "content": "of the effect \"door opened\" will prevent the agent from aimlessly wandering around the door before the key is acquired. Another challenge with these approaches is the Noisy-TV problem [3], where the agent excessively explores unfamiliar states and actions that may not contribute to the ultimate task. These inefficiencies raise a new question: Can agents effectively capture causality to efficiently explore environments with sparse rewards and distracting actions?\nInspired by human reasoning, where understanding the relationship between the environmental variables (EVs) helps exploration, causal reinforcement learning (CRL) is grounded in causal inference [29]. CRL research often involves two phases: (i) causal structure discovery and (ii) integrating causal knowledge with policy training [29]. Recent studies have demonstrated that such knowledge significantly improves the sample efficiency of agent training [8, 22, 30]. However, current approaches often assume assess to all environmental causal variables and pre-factorized environments [22, 8], simplifying the causal discovery phase. In reality, causal variables are not given from observations, and constructing a causal graph for all observations becomes a non-trivial task due to the computational expense associated with measuring causality. Identifying EVs crucial for downstream tasks becomes a challenging task, thereby limiting the effectiveness of CRL methods. These necessitate the identification of a subset of crucial EVs before discovering causality.\nThis paper introduces the Variable-Agnostic Causal Exploration for Reinforcement Learning (VACERL) framework to address these limitations. The framework is an iterative process consisting of three phases: \u201cCrucial Step Detection\u201d, \u201cCausal Structure Discovery\u201d, and \u201cAgent Training with Causal Information\". The first phase aims to discover a set of crucial observation-action steps, denoted as the $SCOAS$. The term \"crucial observation-action step\" refers to an observation and an action pair stored in the agent's memory identified as crucial for constructing the causal graph. We extend the idea of detecting crucial EVs to detecting crucial observation-action steps, motivated by two reasons. Firstly, variables in the environment are associated with the observations, e.g., the variable \"key\" corresponds to the agent's observation of the \"key\". Secondly, actions also contribute to causality, e.g., the agent cannot use the \"key\" without picking it up. One way of determining crucial observation-action steps involves providing the agent with a mechanism to evaluate them based on their contribution to a meaningful task [9]. We implement this mechanism using a Transformer architecture, whose task is to predict the observation-action step leading to the goal given past steps. We rank the significance of observation-action steps based on their attention scores [28] and pick out the top-ranking candidates since the Transformer must attend to important steps to predict correctly.\nIn Phase 2, we adapt causal structure learning [10] to discover the causal relationships among the observation-action steps identified in the discovered $SCOAS$ set, forming a causal graph $G$. The steps serve as the nodes of the causal graph, while the edges can be identified through a two-phase iterative optimization of the functional and structural parameters, representing the Structure Causal Model (SCM). In Phase 3, we train the RL agent based on the causal graph $G$. To prove the versatility of our approach in improving the sample efficiency of RL agents, we propose two methods to utilize the causal graph: (i) formulate intrinsic reward-shaping equations grounded on the captured causal relationship; (ii) treat the nodes in the causal graph as subgoals for HRL. During subsequent training, the updated agent interact with the environments, collecting new trajectories for the agent memory used in the next iteration of Phase 1.\nIn our experiments, we use causally structured grid-world and robotic environments to empirically evaluate the performance improvement of RL agents when employing the two approaches in Phase 3. This improvement extends not only to scenarios with sparse rewards but also to those"}, {"title": "Variable-Agnostic Causal Exploration for Reinforcement Learning", "content": "influenced by the Noisy-TV problem. We also investigate the contributions of the core com-ponents of VACERL, analyzing the emerging learning behaviour that illustrates the captured causality of the agents. Our main contributions can be summarized as:\n\u2022 We present a novel VACERL framework, which autonomously uncovers causal relationships in RL environments without assuming environmental variables or factorized environments.\n\u2022 We propose two methods to integrate our framework into common RL algorithms using intrinsic reward and hierarchical RL, enhancing exploration efficiency and explaining agent behaviour.\n\u2022 We create causally structured environments, with and without Noisy-TV, to evaluate RL agents' exploration capabilities, demonstrating the effectiveness of our approach through extensive experiments."}, {"title": "2 Related Work", "content": "Causal Reinforcement Learning (CRL) is an emerging field that integrates causality and reinforcement learning (RL) to enhance decision-making in RL agents, addressing limitations associated with traditional RL, such as sample efficiency and explainability [29]. CRL methods can be categorized based on their experimental setups, whether they are online or offline [29]. Online-CRL involves real-time interaction with the environment [5, 8, 30, 22], while Offline-CRL relies on learning from a fixed previously collected dataset [24, 18]. Our framework operates"}, {"title": "Variable-Agnostic Causal Exploration for Reinforcement Learning", "content": "online, using trajectories from an online policy for an agent training while simultaneously con-structing the underlying causal graph. Prior works in CRL have focused on integrating causal knowledge into RL algorithms and building causal graphs within the environment. Pitis et al., [18] use Transformer model attention weights to generate counterfactual data for training RL agents, while Coroll et al., [5] use causal effect measurement to build a hierarchy of control-lable effects. Zhang et al., [32] measure the causal relationship between states and actions with the rewards and redistribute the rewards accordingly. For exploration purposes, CRL research integrates causal knowledge by rewarding the agents when they visit states with higher causal influence [22, 30] or treating the nodes of the causal graph as potential subgoals in HRL [8]. Zhang et al., [30] measure the average causal effect between a predefined group of variables and use this as a reward signal, meanwhile, Seitzer et al., [22] propose conditional mutual information as a measurement of causal influence and use it to enhance the exploration of the RL agent. Hu et al., [8] introduce a continuous optimization framework, building a causal structure through a causality-guided intervention and using it to define hierarchical subgoals. Despite advance-ments, previous methods often assume prior knowledge of EVs and the ability to factorize the environment accordingly. Our framework autonomously detects crucial steps associated with the key EVs, enabling causal structure learning without predefined EVs, thus, distinguishing it from previous methods. The causal graph uncovered by VACERL is versatile and can complement existing RL exploration methods, such as intrinsic reward motivation or as hierarchical subgoals.\nIntrinsic Reward Motivation addresses inefficient training in sparse reward RL environ-ments; an issue associated with random exploration techniques like e-greedy [2]. The core idea underlying these motivation strategies is to incorporate intrinsic rewards, which entail adding bonuses to the environment rewards to facilitate exploration [2, 26, 3]. These methods add bonuses to environment rewards to encourage exploration, either based on prediction error [3] or count-based criteria [2, 26]. However, they struggle to scale to complex structure environments, especially in the scenario of Noisy-TV, where the agent becomes excessively curious about unpre-dictable states and ignores the main task [3]. VACERL tackles this by incorporating a mechanism to identify essential steps for the primary task and construct the causal graph around these steps, thus, enabling the agent to ignore actions generating noisy-TV.\nGoal-conditioned Hierarchical Reinforcement Learning (HRL) is another approach that is used to guide agent exploration. Levy et al., [13] propose a multilevel policies framework, in which each policy is trained independently and the output of higher-ranking policies are used as subgoals for lower-level policies. Zhang et al., [31] propose an adjacency constraint method to restrict the search space of subgoals, whereas, Pitis et al., [17] introduce a method based on maximum entropy gain motivating the agent to pursue past achieved goals in sparsely explored areas. However, traditional HRL methods often rely on random subgoals exploration, which has shown inefficiency in learning high-quality hierarchical structures compared to causality-driven approaches [8, 7]. Hu et al., [8] operate under the assumption of pre-availability and disentangle-ment of causal EVs from observations, using these EVs as suitable subgoals for HRL. However, they overlook cases where these assumptions are not applicable, e.g., the observation is the im-age. In our apprroach, subgoals are determined by abstract representations of the observation and action, thereby, extending the applications of causal HRL to unfactorized environments."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Background", "content": ""}, {"title": "RL Preliminaries.", "content": "We are concerned with the Partially Observable Markov Decision Process (POMDP) framework, denoted as the tuple $(S, A, O, P, Z, r, \\gamma)$. The framework includes sets of states $S$, actions $A$, observations $O$ providing partial information of the true state, a transition probability function $P(s' | s,a)$, and an observation model $Z$ denoted as $Z(o | s,a)$, indicating the probability of observing $o$ when taking action $a$ in state $s$. $r: S \\times A \\rightarrow R$ is a reward function that defines the immediate reward that the agent receives for taking an action in a given state, and discount factor $\\gamma$. The objective of the RL agent is to maximize the expected discounted cumulative reward $E_{\\pi,P} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$, over a policy function $\\pi$ mapping a state to a distribution over actions."}, {"title": "Causality.", "content": "Causality is explored through the analysis of relationships among variables and events [16]. It can be described using the SCM framework [16]. SCM, for a finite set $V$ comprising $M$ variables, is $V_i := f_i(PA(V_i)(G), U_i), \\forall i \\in \\{1, ..., M\\}$, where $F = \\{f_1, f_2, \u2026, f_m \\}$ denotes the set of generating functions based on the causal graph $G$ and $U = \\{U_1, U_2, ..., U_M\\}$ represents the set of noise in the model. The graph $G = \\{V, E\\}$ provides the edge $e_{ij} \\in E$, representing variable $V_i$ causes on variable $V_j$, where $e_{ij} = 1$ if $V_j \\in PA(V_i)$, else, $e_{ij} = 0$. The SCM framework can be characterized by two parameter sets: the functional parameter $\\delta$, representing the generating function $f$; the structural parameter $\\eta \\in R^{M \\times M}$, modelling the adjacency matrix of $G$ [10]."}, {"title": "3.2 Variable-Agnostic Causal Exploration Reinforcement Learning Frame-work", "content": ""}, {"title": "3.2.1 Overview.", "content": "The primary argument of VACERL revolves around the existence of a finite set of environment variables (EVs) that the agent should prioritize when constructing the causal graph. We provide a mechanism to detect these variables, aiming to reduce the number of nodes in the causal graph mitigating the complexity of causal discovery. Initially, we deploy an agent to randomly explore the environment and gather successful trajectories. Once the agent accidentally reaches the goal a few times, we initiate Phase 1, reformulating EVs detection into finding the \u201ccrucial observation-action steps\u201d (COAS) from the collected trajectories. The agent is equipped with the ability to rank the importance of these steps by employing the Transformer (TF) model's attention scores ($\\alpha_s$). Top-M highest-score steps will form the crucial set $SCOAS$. Subsequently, in Phase 2, we identify the causal relationships among steps in $SCOAS$ to learn the causal graphs $G$ of the environment. In Phase 3, where we extract a hierarchy causal tree from graph $G$ and use it to design two approaches, enhancing the RL agent's exploration capability. We then utilize the updated agent to gather more successful trajectories and repeat the process from Phase 1. See Fig. 1(c) for an overview of VACERL and detailed implementation in Supp. A 1."}, {"title": "3.2.2 Phase 1: Crucial Step Detection.", "content": "We hypothesize that important steps (a step is a pair of observation and action) are those in the agent's memory that the agent must have experienced to reach the goal. Hence, these steps should be found in trajectories where the agent successfully reaches the goal. We collect a buffer $B = \\{(\\textbf{o}^1_t, \\textbf{a}^1_t)_{t=1}^{T_1}, (\\textbf{o}^2_t, \\textbf{a}^2_t)_{t=1}^{T_2},..., (\\textbf{o}^n_t, \\textbf{a}^n_t)_{t=1}^{T_n}\\}$, where n is the number of episodes wherein the agent successfully reaches the goal state, $\\textbf{o}_t$ and $\\textbf{a}_t$ is the observation and action, respectively at step t in an episode, and $T_k$ is the number of steps in the k-th episode. We train the $TF$ model, whose input consists of steps from the beginning to the second-to-last step in each episode and the output is the last step. The reasoning behind choosing the last step as the prediction target is that it highlights which steps in the trajectories are crucial for successfully reaching the goal. For a training episode k-th sampled from $B$, we predict $(\\textbf{o}^k_T, \\textbf{a}^k_T) = TF(\\{(\\textbf{o}^k_t, \\textbf{a}^k_t)\\}_{t=1}^{T_k-1})$. The model is trained to minimize the loss $L_{TF} = E_k[MSE ((\\textbf{o}^k_T, \\textbf{a}^k_T), (\\hat{\\textbf{o}}^k_T, \\hat{\\textbf{a}}^k_T))]$, where $MSE$ is the mean square error. Following training, we rank the significant observation-action steps based on their attention scores $\\alpha_s$ (detailed in Supp. A)\nand pick out the top-M highest-score steps. We argue that the top-attended steps should cover crucial observations and actions that contribute to the last step prediction task, associated with meaningful causal variables. For instance, observing the key and the action of picking it up are linked to the variable \"key\".\nIn continuous state space, the agent may repeatedly attend to similar steps involving the same variable. For example, the agent might select multiple instances of observing the key, from different positions where the agent is located, and picks it up. As a result, the set $SCOAS$ will be filled with similar steps relating to picking up the key and ignoring other important steps. To address this, we introduce a function $is\\_sim$ to decide if two steps are the same:\n\u2022 For discrete action space environments, $is\\_sim ((\\textbf{o}, a), (\\textbf{o}', a')) = 1$\nif $cos (\\textbf{o}, \\textbf{o}') > \\varpi_{sim}$ and $a = a'$, else 0.\n\u2022 For continuous action space environments, $is\\_sim ((\\textbf{o}, a), (\\textbf{o}', a')) = 1$ if $cos ((\\textbf{o}, a), (\\textbf{o}', a')) > \\varpi_{sim}$, else 0.\nwhere $Cos (\\textbf{o}, \\textbf{o}') = \\frac{\\textbf{o}.\\textbf{o}'}{|\\textbf{o}||\\textbf{o}'|}$ and $\\varpi_{sim}$ is a similarity threshold. Intuitively, if the agent has two observations with a high cosine similarity and takes the same action, these instances are grouped. The score $\\alpha_s$ for a group is the highest $\\alpha_s$ among the steps in this group. The proposed $is\\_sim$ method will also be effective in noisy environments, particularly when the observations are trained representations rather than raw pixel data. Subsequently, we add the steps with the highest $\\alpha_s$ to $SCOAS$. We define an abstract function $I$ to map a pair $(\\textbf{o}_t, \\textbf{a}_t)$ to an element i in $Scoas$: $i = I ((\\textbf{o}_t, \\textbf{a}_t))  is\\_sim ((\\hat{\\textbf{o}}_t, \\hat{\\textbf{a}}_t), (\\textbf{o}, a)) = 1$ and collect a new buffer $B^*$, where:\n$B^* = B \\setminus \\{(\\textbf{o}_t, \\textbf{a}_t): I((\\textbf{o}_t, \\textbf{a}_t)) \\notin SCOAS\\}$  (1)\nHere, $B^*$ is $B$ removing steps that are unimportant (not in $SCOAS$)."}, {"title": "3.2.3 Phase 2: Causal Structure Discovery.", "content": "Inspired by the causal learning method proposed by Ke et al., [10], we uncover the causal rela-tionships among M steps identified in the $SCOAS$ set. Our approach optimizes the functional parameter $\\delta$ and the structural parameter $\\eta$ associated with the SCM framework. The optimiza-tion of these parameters follows a two-phase iterative update process, wherein one parameter is"}, {"title": "Variable-Agnostic Causal Exploration for Reinforcement Learning", "content": "fixed while the other is updated. Both sets of parameters are initialized randomly and undergo training using the buffer $B^*$ (Eq. 1). Our intuition for training the SCM is that the \"cause\" step has to precede its \"effect\" step. Therefore, we train the model to predict the step at timestep t using the sequence of steps leading to that particular timestep.\nIn the first causal discovery phase, we fix $\\eta$ and optimize $\\delta$. For a step t in the trajectory k-th, we formulate f as:\n$\\hat{(\\textbf{o}_t^k, \\textbf{a}_t^k)} = f_{\\delta,I((\\textbf{o}_{t-1}^k, \\textbf{a}_{t-1}^k))}(\\{\\hat{(\\textbf{o}_{t'}^k, \\textbf{a}_{t'}^k)}\\}^t_{t'=1} \\text{ and } I((\\textbf{o}_{t'}^k, \\textbf{a}_{t'}^k)) \\in PA (I(\\textbf{o}_t^k, \\textbf{a}_t^k))|G)$ (2)\nwhere {$\\hat{(\\textbf{o}_{t'}^k, \\textbf{a}_{t'}^k)}$} is the sequence of steps from 1 to t \u2212 1 that belong to the parental set of $PA (I(\\textbf{o}_t, \\textbf{a}_t))$, as defined by the current state of G parameterized by $\\eta$. We use MSE as the loss function:\n$L_{\\delta,G} = E_{t,k} [MSE ((\\textbf{o}_t^k, \\textbf{a}_t^k), (\\hat{\\textbf{o}}_t^k, \\hat{\\textbf{a}}_t^k))]$ (3)\nIn the second phase, we fix $\\delta$ and optimize the parameter $\\eta$ by updating the causality from variable $X_j$ to $X_i$ as $\\eta_{ij} = \\eta_{ij} - \\beta \\sum_{h} (\\sigma(\\eta_{ij}) - e^{(h)}_{ij})\\frac{\\partial}{\\partial \\eta_{ij}} L_{\\delta,G}^{(h),i}(X_i)$, where h indicates the h-th drawn sample of causal graph G, given the current parameter $\\eta$, and $\\beta$ is the update rate. $e^{(h)}_{ij}$ is the edge from variable $X_j$ to $X_i$ of $G^{(h)}$, and $\\sigma$ is the sigmoid function. $L_{\\delta,G}^{(h),i}(X_i)$ is the MSE loss in Eq. 3 for specific variable $X_i$ of current function $f_{\\delta,X_i}$ under graph $G^{(h)}$. After updating parameter $\\eta$ for a number of steps, we repeat the optimization process of parameter $\\delta$. Finally, we use the resulting structural parameter $\\eta$ to construct the causal graph G. We derive edge $e_{ij}$ of graph G, using:\n$e_{ij}=\\begin{cases}1 & \\text{if } \\eta_{ij} > \\eta_{ji} \\text{ and } \\sigma(\\eta_{ij}) > \\Phi_{causal} \\\\0 & \\text{otherwise}\\end{cases}$ (4)\nwhere $\\Phi_{causal}$ is the causal threshold."}, {"title": "3.2.4 Phase 3: Agent Training with Causal Information.", "content": "We extract a refined hierarchy causal tree from graph $G$ with an intuition to focus on steps that are relevant to achieving the goal. Using the goal-reaching step as the root node of the tree, we recursively determine the parental steps of this root node within graph $G$, and subsequently for all identified parental steps. This causal tree is used to design causal exploration approaches. These approaches include (i) intrinsic rewards based on the causal tree, and (ii) utilizing causal nodes as subgoals for HRL. For the first approach, we devise a reward function where nodes closer to the root are deemed more important and receive higher rewards, preserving the significance of the reward associated with the root node and maintaining the agent's focus on the goal. In the second approach, subgoals are sampled from nodes in the causal tree, with nodes closer to the root sampled more frequently. We present the detailed implementations and empirically evaluate these approaches in Sec. 4.1 and Sec. 4.2."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 VACERL: Causal Intrinsic Rewards - Implementation and Evaluation", "content": ""}, {"title": "Causal Intrinsic Reward.", "content": "To establish the relationship where nodes closer to the goal hold greater importance, while ensuring the agent remains focused on the goal, we introduce intrinsic reward formulas as follows:\n$r_{causal}(o, a) = r_g - \\frac{(d - 1) r_0}{D_0} \\forall (o, a) \\in D_a$  (5)\nwhere $r_g$ is the reward given when the agent reach the goal, $r_{causal}(o, a)$ is the intrinsic reward given to a node (o,a), $D_a$ is the set of nodes at depth $d$ of the tree, $r_0 = \\alpha (r_g/h)$ with $\\alpha$ is a hyperparameter and $h$ is the tree height. In the early learning stage, especially for hard exploration environments, the causal graph may not be well defined and thus, $r_{causal}$ may not provide a good incentive. To mitigate this issue, we augment $r_{causal}$ with a count-based intrinsic reward, aiming to accelerate the early exploration stage. Intuitively, the agent is encouraged to visit never-seen-before observation-action pairs in early exploration. Notably, unlike prior count-based methods [2], we restrict counting to steps in $Scoas$, i.e., only crucial steps are counted. Our final intrinsic reward is:\n$r^+_{causal} = (1/\\sqrt{n(o,a)}) r_{causal}(o_t, a_t)$ (6)\nwhere $n(o,a)_t$ is the number of time observation o and action a is encountered up to time step t. Starting from zero, this value increments with each subsequent encounter. We add the final intrinsic reward to the environment reward to train the policy. The total reward is $r (s_t, a_t) = r_{env} (s_t, a_t) +r^+ r_{causal} (s_t, a_t)$, where $r_{env}$ is the extrinsic reward provided by the environment."}, {"title": "Environments.", "content": "We perform experiments across three sets of environments: FrozenLake (FL), Minihack (MH), and Minigrid (MG). These environments are tailored to evaluate the approach in sparse reward settings, where the agent receives a solitary +1 reward upon achieving the goal (detailed in Supp. B)\nFL includes the 4x4 (4x4FL) and 8x8 (8x8FL) FrozenLake environments (Supp Fig. B.1(d,e)) [27]. Although these are classic navigation problems, hidden causal relationships exist between steps. The pathway of the agent can be conceptualized as a causal graph, where each node represents the agent's location cell and its corresponding action. For example, moving right from the cell on the left side of the lake can be identified as the cause of the agent falling into the lake cell. We use these environments to test VACERL's efficiency in discrete state space, where $is\\_sim$ is not used.\nMH includes MH-1 (Room), MH-2 (Room-Monster), MH-3(Room-Ultimate) and MH-4 (River-Narrow) [20]. These environments pose harder exploration challenges compared to FL due to the presence of more objects. Some environments even require interaction with these ob-jects to reach the goal, such as killing monsters (MH-2 and MH-3) or building bridges (MH-4). For this set of environments, we use pixel observations.\nMG is designed based on Minigrid Environment [4], with escalating causality levels. These include the Key Corridor (MG-1) (Supp. Fig. B.1(a)) and 3 variants of the BlockUnlockPickUp:"}, {"title": "Variable-Agnostic Causal Exploration for Reinforcement Learning", "content": "2 2x2 rooms (MG-2 Fig. 1(a)), 2 3x3 rooms (MG-3) and the 3 2x2 rooms (MG-4) (Supp Fig. B.1(b,c)). The task is to navigate and locate the goal object, in a different room. These environments operate under POMDP, enabling us to evaluate the framework's ability to construct the causal graph when only certain objects are observable at a timestep. In these environments, the agent completes the task by following the causal steps: firstly, remove the obstacle blocking the door by picking it up and dropping it in another position, then, pick up the key matching the colour of the door to open it; and finally, pick up the blue box located in the rightmost room, which is the goal. In MG-3, distracting objects are introduced to distract the agent from this sequence of action. In any case, intrinsic exploration motivation is important to navigate due to reward sparsity; however, blind exploration without an understanding of causal relationships can be ineffective.\nNoisy-TV setting is implemented as an additional action (action to watch TV) and can be incorporated into any of the previous environments, so the agent has the option to watch the TV at any point while navigating the map [12]. When taking this watching TV action, the agent will be given white noise observations sampled from a standard normal distribution. As sampled randomly, the number of noisy observations can be conceptualized as infinite."}, {"title": "Baselines.", "content": "PPO [21], a policy gradient method, serves as the backbone algorithm of our method and other baselines. Following Schulman et al., [21], vanilla PPO employs a simple entropy-based explo-ration approach. Other baselines are categorized into causal and non-causal intrinsic motivation. Although our focus is causal intrinsic reward, we include non-causal baselines for comprehen-siveness. These include popular methods: Count-based [2, 26] and RND [3]. Causal motivation baselines include ATTENTION and CAI, which are two methods that have been used to mea-sure causal influence [22, 18]. We need to adapt these methods to follow our assumption of not knowing causal variables. The number of steps used to collect initial successful trajectories and to reconstruct the causal graph (denoted as Hs and Ts respectively) for VACERL and causal baselines are provided for each environment in Supp. D. However, not all causal methods can be adapted, and as such, we have not conducted comparisons with approaches, such as [8]. Ad-ditionally, as we do not require demonstrating trajectory from experts, we do not compare with causal imitation learning methods [6, 24]."}, {"title": "Results.", "content": "In this section, we present our empirical evaluation results of VACERL with causal intrinsic rewards.\nDiscrete State Space: Table 1 illustrates that our rewards improve PPO's performance by approximately 30%, in both 4x4FL and 8x8FL environments. Notably, VACERL outperforms both causal baselines, ATTENTION and CAI. Specifically, VACERL surpasses ATTENTION by 67% and 39% in 4x4FL and 8x8FL. CAI fails to learn the tasks within the specified steps due to insufficient trajectories in the agent's memory for precise causality estimation between all steps. In contrast, our method, incorporating a crucial step detection phase, requires fewer trajectories to capture meaningful causal relationships in the environment. VACERL also performs better than Count-based by 66% in 4x4FL and 100% in 8x8FL, and RND by 51% in 4x4FL and 31% in 8x8FL. We hypothesize that Count-based and RND's intrinsic rewards are unable to encour-age the agent to avoid the trapping lakes, unlike VACERL's are derived from only successful"}, {"title": "Variable-Agnostic Causal Exploration for Reinforcement Learning", "content": "trajectories promoting safer exploration.\nMG-2 Learning Curve Analysis: We conduct experiments with 2 types of observation space (image and vector) and visualize the learning curves in Fig. 2(a) and Supp. Fig. B.4. Results demonstrate that VACERL outperforms vanilla PPO, causal baselines, and RND in both types of observation space. While VACERL shows slightly slower progress than Count-based in early steps, it quickly catches up in later stages, ultimately matching optimal performance. We at-tribute this to VACERL requires a certain number of training steps to accurately acquire the causal graph before the resulting causal rewards influence the agent's training a phenomenon observed in other causal baselines as well.\nContinuous State Space: Table 1 summarizes the testing results on 8 continuous state space environments (MH-1 to MG-4). In most of these environments, VACERL demonstrates superior performance. It only ranks as the second-best in MH-4, MG-1 and MG-2 with competitive returns. In MG-3 environment, at 30 million steps, VACERL achieves the best result with an average return of 0.77, outperforming the second-best Count-based by 10%, while other baselines show little learning. Notably, in the hardest task MG-4, only VACERL can show signs of learning, achieving an average score of 0.29 after 50 million steps whereas other baselines' returns remain zero. Additional learning curves and results are provided in Supp. B.\nUnder Noisy-TV: Fig. 2(b, c), showing the results on MG-2 environment under Noisy-TV setting, confirm that our reward exhibits greater robustness in Noisy-TV environments compared to traditional approaches. Count-based, CAI, and RND fail in this setting as they cannot differentiate noise from meaningful novelty, thus, getting stuck watching Noisy-TV. While the noise less impacts ATTENTION and naive PPO, their exploration strategies are not sufficient for sparse reward environments. Overall, VACERL is the only method performing well across all settings, with or without Noisy-TV."}, {"title": "4.2 VACERL: Causal Subgoals - Implementation and Evaluation", "content": ""}, {"title": "Causal subgoals sampling.", "content": "In HRL", "31": "which can be inefficient in large search spaces. We propose leveraging causal nodes as subgoals", "tree": "n$p(i) = \\frac{(\\sqrt{d_i})^{-1}}{\\sum_{l=1}^{N}(\\sqrt{d_l})^{-1}}$ (7)\nwith $d_i"}]}