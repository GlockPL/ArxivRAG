{"title": "Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples in Constrained Access Environments", "authors": ["Heeyoung Lee", "Hoyoon Byun", "Changdae Oh", "JinYeong Bak", "Kyungwoo Song"], "abstract": "Accessing machine learning models through remote APIs has been gaining prevalence following the recent trend of scaling up model parameters for increased performance. Even though these models exhibit remarkable ability, detecting out-of-distribution (OOD) samples remains a crucial safety concern for end users as these samples may induce unreliable outputs from the model. In this work, we propose an OOD detection framework, MixDiff, that is applicable even when the model's parameters or its activations are not accessible to the end user. To bypass the access restriction, MixDiff applies an identical input-level perturbation to a given target sample and a similar in-distribution (ID) sample, then compares the relative difference in the model outputs of these two samples. MixDiff is model-agnostic and compatible with existing output-based OOD detection methods. We provide theoretical analysis to illustrate MixDiff's effectiveness in discerning OOD samples that induce overconfident outputs from the model and empirically demonstrate that MixDiff consistently enhances the OOD detection performance on various datasets in vision and text domains.", "sections": [{"title": "1 Introduction", "content": "Recent developments in deep neural networks (DNNs) opened the floodgates for a wide adaptation of machine learning methods in various domains such as computer vision, natural language processing and speech recognition. As these models garner more users and widen their application area, the magnitude of impact that they may bring about when encountered with a failure mode is also amplified. One of the causes of these failure modes is when an out-of-distribution (OOD) sample is fed to the model. These samples are problematic because DNNs often produce unreliable outputs if there is a large deviation from the in-distribution (ID) samples that the model has been validated to perform well.\nOOD detection is the task of determining whether an input sample is from ID or OOD. This work focuses on semantic shift [35] where distribution shift is manifested by samples of unseen class labels at test time. Several studies explore measuring how uncertain a model is about a target sample relying on the model's output [10, 19]. While these methods are desirable in that they do not assume access to the information inside the model, they can be further enhanced given access to the model's internal activations, [28] or its parameters [12]. However, the access to the model's internal states is not always permitted. With the advent of foundation models [25, 24], users often find themselves interacting with the model through remote APIs [26]. This limits the utilization of rich information inside the model [13], as well as the modification possibilities [27] that can be effectively used to detect OOD samples. In this work, we explore ways to bypass this access restriction through the only available modification point, namely, the models' inputs.\nData samples in the real world may contain distracting features that can negatively affect the model's performance. Sometimes these distractors may possess characteristics resembling a class that is different from the sample's true label. In this case, the model's predictions for an ID sample could become uncertain as it struggles to decide which class the sample belongs to. Similarly, the model could put too much emphasis on a feature that resembles a certain in-distribution characteristic from an OOD sample, outputting an overconfident prediction, even though the sample does not belong to any of the classes that the model was tasked to classify.\nWe start from the intuition that the contributing features in a misclassified sample, either misclassified as ID or OOD, will tend to be more sensitive to perturbations. In other words, these features that the model has overemphasized will be more brittle when compared to the actual characteristics of the class that these features resemble. Take as an example the image that is at the top left corner of Figure 1a. This sample is predicted to be a bus with a high confidence score, despite it belonging to an OOD class train. When we exact a perturbation to this sample by mixing it with some other auxiliary sample, the contribution of the regions that led to the model's initial prediction is significantly reduced as can be seen by the change in the class activation maps (CAM) [4]. However, when the same perturbation is applied to an actual image of a bus, the change is significantly less abrupt. The model's prediction scores show a similar behavior.\nTo experimentally verify the intuition, we collect OOD samples that induce high confidence scores from the model and compute CAMs for these samples before and after perturbation. Two versions of CAMs are computed with a zero-shot image classifier using CLIP model [25]. One with respect to the predicted class of the sample and the other with respect to the ground truth class of the sample."}, {"title": "2 Related work", "content": "Output-based OOD scoring functions Various works propose OOD scoring functions measuring a classifier's uncertainty from its prediction scores. Some of these methods rely solely on the model's prediction probability. Maximum softmax probability (MSP) [10] utilizes the maximum value of the prediction distribution. Thulasidasan et al. [31] use Shannon entropy as a measure of uncertainty, while GEN [19] proposes a generalized version of the entropy score. KL Matching [11] finds the minimum KL divergence between the target and ID samples. D2U [36] measures the deviation of output distribution from the uniform distribution. If we take a step down to the logit space, maximum logit score (MLS) [11] utilizes the maximum value of the logits. Energy score [18] takes LogSumExp over the logits for the OOD score. MCM [21] emphasizes the importance of temperature scaling in vision-language models [25]. While these output-based methods are desirable in that they take a relaxed assumption on model accessibility, they suffer from the model's overconfidence issue [22]. This motivates us to investigate the perturb-and-compare approach as a calibration measure.\nEnhancing output-based OOD scores Another line of work focuses on enhancing the aforementioned output-based OOD scores to make them more discriminative. ODIN [17] utilizes Softmax temperature scaling and gradient-based input preprocessing to enhance MSP [10]. ReAct [28] alleviates the overconfidence issue by clipping the model's activations if they are over a certain threshold. BAT [42] uses batch normalization [14] statistics for activation clipping. DICE [27] leverages weight sparsification to mitigate the overparameterization issue. Recently, methods that are based on activation [6] or weight pruning [1] approaches also have been proposed. These approaches effectively mitigate the overconfidence issue. However, all of these methods require access to either gradients, activations or parameters; hence limits their applicability in remote API environments. Our work stands out as an OOD score enhancement method in constrained access environments, where models' gradients, activations, and parameters are not accessible, leaving the model inputs as the only available modification point.\nUtilization of deeper access for more discriminative OOD scores Several studies exploit the rich information that the feature space provides when designing OOD scores. Olber et al. [23], Zhang et al. [39] utilize ID samples' activations for comparison with a target sample. Models' inner representations are employed in methods that rely on class-conditional Mahalanobis distance [16]. ViM [33] proposes an OOD score that complements the energy score [18] with additional information from the feature space. Sun et al. [29] use the target sample's feature level KNN distance to ID samples. GradNorm [13] employs the gradient of the prediction probabilities' KL divergence to the uniform distribution. Zhang and Xiang [41] show that decoupling MLS [11] can lead to increased detection performance if given access to the model parameters. However, these methods are not applicable to black-box API models where one can only access the model's two endpoints, namely, the inputs and outputs."}, {"title": "3 Methodology", "content": "In this section, we describe the working mechanism of MixDiff framework. MixDiff is comprised of the following three procedures: (1) find ID samples that are similar to the target sample and perturb these samples by performing Mixup with an auxiliary sample; (2) perturb the target sample by performing Mixup with the same auxiliary sample; (3) measure the model's uncertainty of the perturbed target sample relative to the perturbed ID samples. We now provide a detailed description of each procedure.\nOracle-side perturbation We feed the given target sample, $x_t$, to a classification model $f(\u00b7)$ and get its prediction scores for $K$ classes, $O_t$, and the predicted class label, $\\hat{y}_t$, as shown in Equation 1.\n\n$O_t = f(x_t) \\in \\mathbb{R}^K, \\quad \\hat{y}_t = \\arg \\max(O_t)$\n\nNext, we assume a small set of $M$ labeled samples, $\\Omega_k = \\{(x, y)\\}_{m=1}^M$, for each class label $k$. We refer to these samples as the oracle samples. From these, we take the samples that are of the same label as the predicted label $\\hat{y}_t$. Then, we perturb each oracle sample, $x_m$, by performing Mixup with an auxiliary sample, $x_i \\in \\{x_i\\}_{i=1}^N$, with Mixup rate $\\lambda_r$.\n\n$x_{mir} = \\lambda_r x_m + (1 - \\lambda_r)x_i, \\quad \\text{where } y = \\hat{y}_t$\n\nWe feed the perturbed oracle sample to the classification model $f(\u00b7)$ and get the model's prediction scores, $O_{mir} = f(x_{mir}) \\in \\mathbb{R}^K$. Then, we average the perturbed oracle samples' model outputs, to get $\\tilde{O} = \\frac{1}{M} \\sum_{i=1}^M O_{mir}$. Finally, we compute the perturbed oracle samples' OOD score, $s_r \\in \\mathbb{R}$, with an arbitrary output-based OOD scoring function $h(\u00b7)$ such as MSP or MLS, i.e., $s_r = h (\\tilde{O}) \\in \\mathbb{R}$.\nTarget-side perturbation We perturb the target sample $x_t$ with the same auxiliary samples $\\{x_i\\}_{i=1}^N$, as $x_{ir} = \\lambda_r x_t + (1 - \\lambda_r)x_i$, and compute the OOD scores of the perturbed target sample as follows:\n\n$O_{ir} = f(x_{ir}) \\in \\mathbb{R}^K, \\quad S_{ir} = h(O_{ir}) \\in \\mathbb{R}$\n\nComparison of perturbed samples' outputs From the perturbed target's and oracles' uncertainty scores, $(s_{ir}, S_{ir})$, we calculate the MixDiff score for the target sample, $x_t$, as shown in Equation 4. It measures the model's uncertainty score of the target sample relative to similar ID samples when both undergo the same Mixup operation with an auxiliary sample $x_i$, then takes the average of the differences over the auxiliary samples and the Mixup ratios. We provide descriptions and illustrations of the overall procedure in Algorithm 1 and Figure 2.\n\n$\\text{MixDiff} = \\frac{1}{RN} \\sum_{r=1}^R \\sum_{i=1}^N (S_{ir} - s_{ir})$\n\nWe calibrate the base OOD score for the target sample, $h(f(x_t))$, by adding the MixDiff score with a scaling hyperparameter $\\gamma$ to it so as to mitigate the model's over- or underconfidence issue.\nPractical implementation The oracle-side procedure can be pre-computed since it does not depend on the target sample. The target-side computations can be effectively parallelized since each perturbed target sample can be processed by the model, independent of the others. We organize the perturbed target samples in a single batch in our implementation (see Appendix F for details on practical implementation). Further speedup can be gained in remote API environments as API calls are often handled by multiple nodes."}, {"title": "3.1 Theoretical analysis", "content": "To better understand how and when our method ensures performance improvements, we present a theoretical analysis of MixDiff. We use a similar theoretical approach to Zhang et al. [40], but towards a distinct direction for analyzing a post hoc OOD scoring function. Proposition 1 reveals the decomposition of the OOD score function into two components: the OOD score of the unmixed clean target sample and the supplementary signals introduced by Mixup."}, {"title": "4 Experiments", "content": "4.1 Experimental setup\nWe elaborate on the implementation details and present the descriptions on baselines. Other details on datasets and evaluation metrics are provided in Appendix G. See Appendix O for code.\nImplementation details Following a recent OOD detection approach [7, 21, 34] that utilizes vision-language foundation models' zero-shot classification capability, we employ CLIP ViT-B/32 model"}]}