{"title": "Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples in Constrained Access Environments", "authors": ["Heeyoung Lee", "Hoyoon Byun", "Changdae Oh", "JinYeong Bak", "Kyungwoo Song"], "abstract": "Accessing machine learning models through remote APIs has been gaining prevalence following the recent trend of scaling up model parameters for increased performance. Even though these models exhibit remarkable ability, detecting out-of-distribution (OOD) samples remains a crucial safety concern for end users as these samples may induce unreliable outputs from the model. In this work, we propose an OOD detection framework, MixDiff, that is applicable even when the model's parameters or its activations are not accessible to the end user. To bypass the access restriction, MixDiff applies an identical input-level perturbation to a given target sample and a similar in-distribution (ID) sample, then compares the relative difference in the model outputs of these two samples. MixDiff is model-agnostic and compatible with existing output-based OOD detection methods. We provide theoretical analysis to illustrate MixDiff's effectiveness in discerning OOD samples that induce overconfident outputs from the model and empirically demonstrate that MixDiff consistently enhances the OOD detection performance on various datasets in vision and text domains.", "sections": [{"title": "1 Introduction", "content": "Recent developments in deep neural networks (DNNs) opened the floodgates for a wide adaptation of machine learning methods in various domains such as computer vision, natural language processing and speech recognition. As these models garner more users and widen their application area, the magnitude of impact that they may bring about when encountered with a failure mode is also amplified. One of the causes of these failure modes is when an out-of-distribution (OOD) sample is fed to the model. These samples are problematic because DNNs often produce unreliable outputs if there is a large deviation from the in-distribution (ID) samples that the model has been validated to perform well.\nOOD detection is the task of determining whether an input sample is from ID or OOD. This work focuses on semantic shift [35] where distribution shift is manifested by samples of unseen class labels at test time. Several studies explore measuring how uncertain a model is about a target sample relying on the model's output [10, 19]. While these methods are desirable in that they do not assume access to the information inside the model, they can be further enhanced given access to the model's internal activations, [28] or its parameters [12]. However, the access to the model's internal states is not always permitted. With the advent of foundation models [25, 24], users often find themselves interacting with the model through remote APIs [26]. This limits the utilization of rich information inside the model [13], as well as the modification possibilities [27] that can be effectively used to detect OOD samples. In this work, we explore ways to bypass this access restriction through the only available modification point, namely, the models' inputs.\nData samples in the real world may contain distracting features that can negatively affect the model's performance. Sometimes these distractors may possess characteristics resembling a class that is different from the sample's true label. In this case, the model's predictions for an ID sample could become uncertain as it struggles to decide which class the sample belongs to. Similarly, the model could put too much emphasis on a feature that resembles a certain in-distribution characteristic from an OOD sample, outputting an overconfident prediction, even though the sample does not belong to any of the classes that the model was tasked to classify.\nWe start from the intuition that the contributing features in a misclassified sample, either misclassified as ID or OOD, will tend to be more sensitive to perturbations. In other words, these features that the model has overemphasized will be more brittle when compared to the actual characteristics of the class that these features resemble. Take as an example the image that is at the top left corner of Figure 1a. This sample is predicted to be a bus with a high confidence score, despite it belonging to an OOD class train. When we exact a perturbation to this sample by mixing it with some other auxiliary sample, the contribution of the regions that led to the model's initial prediction is significantly reduced as can be seen by the change in the class activation maps (CAM) [4]. However, when the same perturbation is applied to an actual image of a bus, the change is significantly less abrupt. The model's prediction scores show a similar behavior.\nTo experimentally verify the intuition, we collect OOD samples that induce high confidence scores from the model and compute CAMs for these samples before and after perturbation. Two versions of CAMs are computed with a zero-shot image classifier using CLIP model [25]. One with respect to the predicted class of the sample and the other with respect to the ground truth class of the sample."}, {"title": "2 Related work", "content": "Output-based OOD scoring functions Various works propose OOD scoring functions measuring a classifier's uncertainty from its prediction scores. Some of these methods rely solely on the model's prediction probability. Maximum softmax probability (MSP) [10] utilizes the maximum value of the prediction distribution. Thulasidasan et al. [31] use Shannon entropy as a measure of uncertainty, while GEN [19] proposes a generalized version of the entropy score. KL Matching [11] finds the minimum KL divergence between the target and ID samples. D2U [36] measures the deviation of output distribution from the uniform distribution. If we take a step down to the logit space, maximum logit score (MLS) [11] utilizes the maximum value of the logits. Energy score [18] takes LogSumExp over the logits for the OOD score. MCM [21] emphasizes the importance of temperature scaling in vision-language models [25]. While these output-based methods are desirable in that they take a relaxed assumption on model accessibility, they suffer from the model's overconfidence issue [22]. This motivates us to investigate the perturb-and-compare approach as a calibration measure.\nEnhancing output-based OOD scores Another line of work focuses on enhancing the aforementioned output-based OOD scores to make them more discriminative. ODIN [17] utilizes Softmax temperature scaling and gradient-based input preprocessing to enhance MSP [10]. ReAct [28] alleviates the overconfidence issue by clipping the model's activations if they are over a certain threshold. BAT [42] uses batch normalization [14] statistics for activation clipping. DICE [27] leverages weight sparsification to mitigate the overparameterization issue. Recently, methods that are based on activation [6] or weight pruning [1] approaches also have been proposed. These approaches effectively mitigate the overconfidence issue. However, all of these methods require access to either gradients, activations or parameters; hence limits their applicability in remote API environments. Our work stands out as an OOD score enhancement method in constrained access environments, where models' gradients, activations, and parameters are not accessible, leaving the model inputs as the only available modification point.\nUtilization of deeper access for more discriminative OOD scores Several studies exploit the rich information that the feature space provides when designing OOD scores. Olber et al. [23], Zhang et al. [39] utilize ID samples' activations for comparison with a target sample. Models' inner representations are employed in methods that rely on class-conditional Mahalanobis distance [16]. ViM [33] proposes an OOD score that complements the energy score [18] with additional information from the feature space. Sun et al. [29] use the target sample's feature level KNN distance to ID samples. GradNorm [13] employs the gradient of the prediction probabilities' KL divergence to the uniform distribution. Zhang and Xiang [41] show that decoupling MLS [11] can lead to increased detection performance if given access to the model parameters. However, these methods are not applicable to black-box API models where one can only access the model's two endpoints, namely, the inputs and outputs."}, {"title": "3 Methodology", "content": "In this section, we describe the working mechanism of MixDiff framework. MixDiff is comprised of the following three procedures: (1) find ID samples that are similar to the target sample and perturb these samples by performing Mixup with an auxiliary sample; (2) perturb the target sample by performing Mixup with the same auxiliary sample; (3) measure the model's uncertainty of the perturbed target sample relative to the perturbed ID samples. We now provide a detailed description of each procedure.\nOracle-side perturbation We feed the given target sample, $x_t$, to a classification model $f(\\cdot)$ and get its prediction scores for $K$ classes, $O_t$, and the predicted class label, $\\hat{y}_t$, as shown in Equation 1.\n$\\begin{equation}O_t = f(x_t) \\in \\mathbb{R}^K, \\ \\hat{y}_t = \\arg \\max(O_t)\\end{equation}$\nNext, we assume a small set of $M$ labeled samples, $\\Omega_k = \\{(x, y)\\}_{m=1}^M$, for each class label $k$. We refer to these samples as the oracle samples. From these, we take the samples that are of the same label as the predicted label $\\hat{y}_t$. Then, we perturb each oracle sample, $x_m$, by performing Mixup with an auxiliary sample, $x_i \\in \\{x_i\\}_{i=1}^N$, with Mixup rate $\\lambda_r$.\n$\\begin{equation}x_{mir} = \\lambda_r x_m + (1 - \\lambda_r) x_i, \\text{ where } y = \\hat{y}_t\\end{equation}$\nWe feed the perturbed oracle sample to the classification model $f(\\cdot)$ and get the model's prediction scores, $O_{mir} = f(x_{mir}) \\in \\mathbb{R}^K$. Then, we average the perturbed oracle samples' model outputs, to get $\\bar{O} = \\frac{1}{M} \\sum_{i=1}^M O_{mir}$. Finally, we compute the perturbed oracle samples' OOD score, $s_r \\in \\mathbb{R}$, with an arbitrary output-based OOD scoring function $h(\\cdot)$ such as MSP or MLS, i.e., $s_r = h(\\bar{O}) \\in \\mathbb{R}$.\nTarget-side perturbation We perturb the target sample $x_t$ with the same auxiliary samples $\\{x_i\\}_{i=1}^N$, as $x_{ir} = \\lambda_r x_t + (1 - \\lambda_r) x_i$, and compute the OOD scores of the perturbed target sample as follows:\n$\\begin{equation}O_{ir} = f(x_{ir}) \\in \\mathbb{R}^K, s_{ir} = h(O_{ir}) \\in \\mathbb{R}\\end{equation}$\nComparison of perturbed samples' outputs From the perturbed target's and oracles' uncertainty scores, $(s_{ir}, S_r)$, we calculate the MixDiff score for the target sample, $x_t$, as shown in Equation 4. It measures the model's uncertainty score of the target sample relative to similar ID samples when both undergo the same Mixup operation with an auxiliary sample $x_i$, then takes the average of the differences over the auxiliary samples and the Mixup ratios. We provide descriptions and illustrations of the overall procedure in Algorithm 1 and Figure 2.\n$\\begin{equation}\\text{MixDiff} = \\frac{1}{RN} \\sum_{r=1}^R \\sum_{i=1}^N (S_{ir} - s_{ir})\\end{equation}$\nWe calibrate the base OOD score for the target sample, $h(f(x_t))$, by adding the MixDiff score with a scaling hyperparameter $\\gamma$ to it so as to mitigate the model's over- or underconfidence issue.\nPractical implementation The oracle-side procedure can be pre-computed since it does not depend on the target sample. The target-side computations can be effectively parallelized since each perturbed target sample can be processed by the model, independent of the others. We organize the perturbed target samples in a single batch in our implementation (see Appendix F for details on practical implementation). Further speedup can be gained in remote API environments as API calls are often handled by multiple nodes."}, {"title": "3.1 Theoretical analysis", "content": "To better understand how and when our method ensures performance improvements, we present a theoretical analysis of MixDiff. We use a similar theoretical approach to Zhang et al. [40], but towards a distinct direction for analyzing a post hoc OOD scoring function. Proposition 1 reveals the decomposition of the OOD score function into two components: the OOD score of the unmixed clean target sample and the supplementary signals introduced by Mixup."}, {"title": "Proposition 1 (OOD scores for mixed samples).", "content": "Let pre-trained model $f(\\cdot)$ and base OOD score function $h(\u00b7)$ be twice-differentiable functions, and $x_{i\\lambda} = \\lambda x_t + (1 \u2212 \\lambda)x_i$ be a mixed sample with ratio $\\lambda \\in (0, 1)$. Then base OOD score function of mixed sample, $h(f(x_{i\\lambda}))$, is written as:\n$h(f(x_{i\\lambda})) = h(f(x_t)) + \\sum_{l=1}^{3} \\omega_l (x_t, x_i) + \\varphi_t(\\lambda)(\\lambda \u2212 1)^2 , $\nwhere $lim_{\\lambda \\to 1} \\varphi_t (\\lambda) = 0,$\n$\\omega_1(x_t, x_i) = (\\lambda \u2212 1)(x_t \u2212 x_i)f'(x_t)h'(f(x_t))$\n$\\omega_2(x_t, x_i) = \\frac{(\\lambda \u2212 1)^2}{2}(x_t \u2212 x_i)f''(x_t)(x_t \u2212 x_i)h'(f(x_t))$\n$\\omega_3(x_t, x_i) = \\frac{(\\lambda \u2212 1)^2}{2}(x_t \u2212 x_i)^Tf'(x_t)(x_t \u2212 x_i)^Tf'(x_t)h''(f(x_t)).$\nWe analyze MixDiff using the quadratic approximation of $h(f(x_{i\\lambda}))$, omitting the higher order terms denoted as $\\varphi_t(\\lambda)$ in Equation 5. In Figure 3a, we experimentally verify that the sum of the OOD score of the pure sample and $\\omega$ terms, denoted as $\\Omega (x_t, x_i) = \\sum_{l=1}^{3} \\omega_l (x_t, x_i)$, reasonably approximates the OOD score of the mixed sample in Equation 5. $\\omega (x_t, x_i)$ represents the impact caused by Mixup as can be seen from its increase when $\\lambda$ decreases. Hence, the additional signal from the Mixup can be derived from the first and second derivatives of $f(\\cdot)$ and $h(\\cdot)$ and the difference between the target and auxiliary samples.\nWe argue that perturbing both the target and oracle samples and then comparing the model outputs of the two can help OOD detection even when the target induces a relatively high confidence score from the model, in which case existing output-based OOD scoring functions would result in detection failure. Through Theorem 1, we show the effectiveness of MixDiff by demonstrating the existence of an auxiliary sample with which MixDiff can calibrate the overconfidence of a high confidence OOD sample on a simple linear model setup."}, {"title": "Theorem 1.", "content": "Let $h(x)$ represent MSP and $f(x)$ represent a linear model, described by $w^T x + b$, where $w, x \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$. We consider the target sample, $x_t$, to be a hard OOD sample, defined as a sample that is predicted to be of the same class as the oracle sample, $x_m$, but with a higher confidence score than the oracle sample. For binary classification, $x_t$ is a hard OOD sample when $0 < f(x_m) < f(x_t)$ or $f(x_t) < f(x_m) < 0$. There exists an auxiliary sample $x_i$ such that\n$h(f(x_t)) \u2212 h(f(x_m)) + \\sum_{l=1}^{3}( \\Omega_l(x_t, x_i) \u2212  \\Omega_l(x_m, x_i)) > 0.$\nTheorem 1 provides a theoretical ground for our approach\u2019s effectiveness in discerning OOD samples that may not be detected by existing output-based OOD scores. Figures 3b to 3d illustrate examples of such auxiliary samples using synthetic data. Proof and details of Proposition 1 and Theorem 1 are in Appendix B and C, respectively. We also show that Theorem 1 holds for MLS and Entropy in Appendix C. While we take a linear model as the classifier for simplicity of analysis, the prevalence of linear probing from foundation models\u2019 embeddings brings our analysis closer to real-world setups (see Section 4.5 for experimental validation)."}, {"title": "4 Experiments", "content": "We elaborate on the implementation details and present the descriptions on baselines. Other details on datasets and evaluation metrics are provided in Appendix G. See Appendix O for code.\nImplementation details Following a recent OOD detection approach [7, 21, 34] that utilizes vision-language foundation models' zero-shot classification capability, we employ CLIP ViT-B/32 model [25] as our classification model without any finetuning on ID samples. We construct the oracle set by randomly sampling $M$ samples per class from the train split of each dataset. For a given target sample, we simply use the other samples in the same batch as the auxiliary set. Instead of searching hyperparameters for each dataset, we perform one hyperparameter search on Caltech101 [8] and use the same hyperparameters across all the other datasets, which is in line with a more realistic OOD detection setting [17]. We provide full description of the implementation details in Appendix G.\nBaselines We take MSP [10], MLS [11], energy score [18], Shannon entropy [31] and MCM [21] as output-based training-free baselines. We also include methods that require extra training for comparison. ZOC [7] is a zero-shot OOD detection method based on CLIP [25] that requires training a separate candidate OOD class name generator. CAC [20] relies on train-time loss function modification and shows the best performance among the train-time modification methods compatible with CLIP [7]. We take CAC trained with the same CLIP VIT-B/32 backbone as a baseline (CLIP+CAC)."}, {"title": "4.2 Logits as model outputs", "content": "First, we assume a more lenient access constraint whereby logits are provided as the model $f(\\cdot)$'s outputs. This setup facilitates validation of MixDiff's OOD score enhancement ability on both the logit-based and probability-based scores. Note that, in this setup, the perturbed oracle samples' probability-based OOD scores are computed after averaging out $M$ perturbed oracle samples in the logit-space, i.e., $\\bar{O}_r = \\frac{1}{M} \\sum_{i=1}^M O_{mir}$. The consistent improvements across all datasets and methods in Table 1 indicate that MixDiff is effective in enhancing output-based OOD scores, to a degree where one of the training-free methods, MixDiff+MCM, outperforming a training-based method CLIP+CAC. Equipping MixDiff with the best performing non-training-free method, ZOC, also yields performance improvements."}, {"title": "4.3 Prediction probabilities as model outputs", "content": "We now take a more restricted environment where the only accessible part of the model is its output prediction probabilities. To the best of our knowledge, none of the existing OOD score enhancement methods are applicable in this environment. Logits are required in the case of Softmax temperature scaling [17]. ODIN's gradient-based input preprocessing [17] or weight pruning methods [27] assume an access to the model's parameters. The model's internal activations are required in the case of activation clipping [28] and activation pruning [6].\nWe take a linear combination of entropy and MSP scores with a scaling hyperparameter tuned on the Caltech101 dataset as a baseline (Entropy+MSP). The results are presented in Table 2. Even in this constrained environment, MixDiff effectively enhances output-based OOD scores, as evidenced by MixDiff+Entropy outperforming MCM (in Table 1), a method that assumes an access to the logit space, while MSP score fails to provide entropy score any meaningful performance gain. Figure 4a shows that MixDiff's performance gain can be enjoyed with as little as two additional forward passes (R = 1, N = 2). Figure 4b illustrates the discriminative edge provided by MixDiff score when the base OOD score's values are almost identical. We observe that the performance gain is more pronounced when the outputs contain more limited information as can be seen in the case of MSP where only the predicted class's probability value is utilized."}, {"title": "4.4 Prediction labels as model outputs", "content": "We push the limits of the model access by assuming that only the predicted class labels are available without any scores attached to them. We apply MixDiff by representing the model's predictions as one-hot vectors and taking the difference between the perturbed target's predicted label and the corresponding perturbed oracles' average score for that label in Equation 4. As there is no base OOD score applicable in the environment, we use the MixDiff score alone. The results in Table 2 show that MixDiff is applicable even in this extremely constrained access environment."}, {"title": "4.5 Last layer activations as model outputs", "content": "We relax the model access constraint by permiting access to the model's activations from the last layer, i.e., image embeddings in CLIP model. In this setup, instead of input-level Mixup, we utilize embedding-level Mixup. More specifically, embeddings of target (or oracle) are perturbed by mixing them with auxiliary sample's embeddings, after which logits are computed from the perturbed embeddings and fed to an output-based OOD scoring function h(\u00b7) such as entropy. As auxiliaries' and oracles' embeddings are precomputed, the computational overhead introduced by MixDiff is almost nil. The assumption of linear model in theoretical analysis is more closely followed in embedding-level Mixup since they can be viewed as linear probing of foundation models' activations. Bottom block of Table 2 shows that MixDiff can enhance OOD detection performance even with negligible compute overhead in this relaxed setup. We use random ID samples as auxiliaries in the embedding Mixup experiments."}, {"title": "4.6 Robustness to adversarial attacks", "content": "In adversarial attack on an OOD detector, the attacker creates a small, indistinguishable modification to the input sample with the purpose of increasing the model's confidence of a given OOD sample or decreasing the model's confidence of a given ID sample [2]. These modifications can be viewed as injection of certain artificial features, specifically designed to induce more confident or uncertain outputs from the model. Our motivation in Section 1 suggests that these artificial features may also be less robust to perturbations. We test this by evaluating MixDiff under adversarial attack. The results in Table 3 indicate that the contributing features that induce ID/OOD misclassification are less robust to perturbations and that MixDiff can effectively exploit such brittleness."}, {"title": "4.7 Experiments on out-of-scope detection task", "content": "Out-of-scope detection We take the MixDiff framework to out-of-scope (OOS) detection task to check its versatility in regard to the modality of the input. To reliably fulfill users' queries or instructions, understanding the intent behind a user's utterance forms a crucial aspect of dialogue systems. In intent classification task, models are tasked to extract the intent behind a user utterance. Even though there has been an inflow of development in the area for the improvement of classification performance, there is no guarantee that a given query's intent is in the set of intents that the model is able to classify. OOS detection task [3], concerns with detection of such user utterances.\nMixDiff with textual input Unlike images whose continuousness lends itself to a simple Mixup operation, the discreteness of texts renders Mixup of texts not as straightforward. While there are several works that explore interpolation of texts, most of these require access to the model parameters [15]. This limits the MixDiff framework's applicability in an environment where the model is served as an API [26], which is becoming more and more prevalent with the rapid development of large language models [24]. Following this trend, we assume a more challenging environment with the requirement that Mixup be performed on the input level. To this end, we simply concatenate the text pair and let the interpolation happen while the pair is inside the model [9]."}, {"title": "5 Liminations and future work", "content": "Dependency on model's performance We construct a low-confidence oracle set by limiting the oracle pool to contain the top p% of most uncertain ID samples. Fig. 5 shows MixDiff's dependency on the model's ability to assign minimal confidence on the oracle. The experiments are performed with CIFAR10 dataset using the other oracle samples of the predicted class of the target as auxiliaries."}, {"title": "Time and space complexity", "content": "MixDiff is effective at bypassing a black-box model's access restriction for OOD detection, but bypassing the access restriction comes with a certain computational overhead. For each target sample $x_t$, MixDiff requires processing of $N \\times R$ mixed samples. While these samples can be effectively processed in parallel and the MixDiff framework outperforming the baselines only with small values of R and N, it nonetheless remains as a drawback of the MixDiff framework. Further research is called for reducing the computational and space complexity of MixDiff framework."}, {"title": "Selection of auxiliary samples", "content": "In Section 4, we experiment with three auxiliary sample selection methods, one using the in-batch samples and the other two using the oracle or random ID samples as the auxiliary samples. Figure 4a shows reduced performance gain when the number of auxiliary samples, N, is too small. We hypothesize that this is due to the fact that while on average MixDiff can effectively discern the overemphasized features, there is a certain degree of variance in the MixDiff score, requiring N and, to some degree, R to be over a certain value for reliable performance. There may be an auxiliary sample that is more effective at discerning an overemphasized feature of a given target sample, but this is subject to change depending on the target sample. We leave the exploration of better auxiliary sample selection methods, either by careful curation of auxiliary samples or by making the procedure more instance-aware and possibly learnable, as future work."}, {"title": "Other forms of inputs", "content": "MixDiff framework can be easily extended to incorporate inputs from other modalities. The experiments on the out-of-scope detection task serve as an example of these kinds of extensions. This input-level Mixup makes the framework applicable to environments where the access to the model parameters cannot be assumed. It also grants the freedom to design better Mixup methods that are specific to the format of the input or the task at hand. But this freedom comes at the cost of having to devise a Mixup mechanism for each input format and task. For example, the simple concatenation of samples that we have utilized on out-of-scope detection task has the limitation that it cannot be applied if the input sequence is too long due to the quadratic time and space complexity of Transformers [32]."}, {"title": "Other types of distribution shifts and broader categories of models", "content": "This work deals with detecting label shift with classifier models. However, there are other types of distribution shifts such as domain shift and broader range of models other than classifiers, e.g., image segmentation models. Extensions of the perturb-and-compare mechanism to more diverse types of shifts and tasks would be a valuable addition to the black-box OOD detection field."}, {"title": "6 Conclusion", "content": "In this work, we present a new OOD detection framework, MixDiff, that boosts OOD detection performance in constrained access scenarios. MixDiff is based on the perturb-and-compare approach that measures how the model's confidence in the target sample behaves compared to a similar ID sample when both undergo an identical perturbation. This provides an additional signal that cannot be gained from the limited information of the target sample's model output alone. We provide theoretical grounds for the framework's effectiveness and empirically validate our approach on multiple degrees of restricted access scenarios. Our experimental results show that MixDiff is an effective OOD detection method for constrained access scenarios where the applicability of existing methods is limited."}, {"title": "A Notation", "content": "Notation Definition\nf() Classifier model.\nh() Arbitrary output-based OOD score function.\nM The number of oracle samples of each class.\nR The number of Mixup ratios.\nN The number of auxiliary samples that will be mixed with the oracle or target samples.\n$\\Omega_k$ Set of oracle sample and label pairs for the k-th class.\n$\\Omega$ Set of oracle sample and label pairs of all classes, $\\{k\\}_1^K$\n$\\lambda_r$ r-th Mixup ratio.\n$x_t$ The target sample.\n$x_{ir}$ Mixed sample from the target $x_t$ and i-th auxiliary sample with Mixup ratio of $\\lambda_r$.\n$x_{mir}$ Mixed sample from the m-th oracle sample i-th auxiliary sample with Mixup ratio of $\\lambda_r$.\n$O_{ir}$ The prediction scores from the mixture of the target and i-th auxiliary sample with the Mixup ratio $\\lambda_r$.\n$O_{mir}$ The prediction scores from the mixture of the m-th oracle and i-th auxiliary sample with the Mixup ratio $\\lambda_r$.\n$\\bar{O}_r$ The mean of $\\{O_{mir}\\}_{M=1}^M$ along the subscript m.\n$s_{ir}$ OOD score induced by $O_{ir}$.\n$S_r$ OOD score induced by $\\bar{O}_r$.\n$\\gamma$ The scaling hyperparameter to which the MixDiff score will be multiplied."}]}