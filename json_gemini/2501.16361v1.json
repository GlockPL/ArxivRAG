{"title": "Large Language Models Meet Graph Neural Networks for Text-Numeric Graph Reasoning", "authors": ["Haoran Song", "Jiarui Feng", "Guangfu Li", "Michael Province", "Philip Payne", "Yixin Chen", "Fuhai Li"], "abstract": "In real-world scientific discovery, human beings always make use of the accumulated prior knowledge with imagination pick select one or a few most promising hypotheses from large and noisy data analysis results. In this study, we introduce a new type of graph structure, the text-numeric graph (TNG), which is defined as graph entities and associations have both text-attributed information and numeric information. The TNG is an ideal data structure model for novel scientific discovery via graph reasoning because it integrates human-understandable textual annotations or prior knowledge, with numeric values that represent the observed or activation levels of graph entities or associations in different samples. Together both the textual information and numeric values determine the importance of graph entities and associations in graph reasoning for novel scientific knowledge discovery. We further propose integrating large language models (LLMs) and graph neural networks (GNNs) to analyze the TNGs for graph understanding and reasoning. To demonstrate the utility, we generated the text-omic(numeric) signaling graphs (TOSG), as one type of TNGs, in which all graphs have the same entities, associations and annotations, but have sample-specific entity numeric (omic) values using single cell RNAseq (scRNAseq) datasets of different diseases. We proposed joint LLM-GNN models for key entity mining and signaling pathway mining on the TOSGs. The evaluation results showed the LLM-GNN and TNGs models significantly improve classification accuracy and network inference. In conclusion, the TNGs and joint LLM-GNN models are important approaches for scientific discovery.", "sections": [{"title": "Introduction", "content": "The problem: the need of text-numeric graph (TNG) for real-world scientific discovery. In the real world, many complex systems can be represented using interconnected graph-structured data, such as knowledge graphs, cell signaling graphs, and chemical structure graphs. Graph Al, based on graph neural networks (GNN) models, is one effective approach for graph understanding and reasoning. A major limitation of the current existing graph-based reasoning is that they are developed for either text-attributed1\u20134 or numeric-attributed5-7 graphs. It is not consistent with and not efficient for the real-world scientific knowledge discoveries. In real-world scientific discovery, human beings always make use of the accumulated prior knowledge with imagination pick select one or a few most promising hypotheses from large and noisy data analysis results. Therefore, the prior knowledge and numeric data should be integrated during scientific discovery and knowledge reasoning. Herein, we introduce a new type of graph structure, the text-numeric graph (TNG), which is defined as graph entities and associations have both text-attributed information and numeric information. The TNG is an ideal data structure model for novel scientific discovery via graph reasoning because it integrates human-understandable textual annotations or prior knowledge, with numeric values that represent the observed or activation levels of graph entities or associations in different samples. Together both the textual information and numeric values determine the importance of graph entities and associations in graph reasoning for novel scientific knowledge discovery. We further propose integrating large language models (LLMs) and graph neural networks (GNNs) to analyze the TNGs for graph understanding and reasoning.\nApplication scenarios of LLM-GNN for TNG: To demonstrate the utility, we generated the text-omic(numeric) signaling graphs (TOSG), as one type of TNGs, in which all graphs have the same entities, associations and annotations, but have sample-specific entity numeric (omic) values using single cell RNAseq (scRNAseq) datasets of different diseases. We proposed joint LLM-GNN models for key entity mining and signaling pathway mining on the TOSGs. The evaluation"}, {"title": "Methodology", "content": "Notations\nA gene graph is denoted as $G = (V, E)$, where $V$ is the set of gene nodes with $|V| = n$, $E$ is the set of edges and $E \u2286 V \u00d7 V$. The node embedding set is denoted by $X = [x_1, x_2, ..., x_n]^T \u2208 R^{nxd}$, where $x_u \u2208 R^d$ is the embedding vector of the node $u$. The graph structure is defined by an adjacency matrix $A \u2208 [0,1]^{n\u00d7n}$, where $A_{uv} = 1$ indicate there is an edge from the node $u$ to node $v$ and $A_{uv} = 0$ otherwise. Further, a set of paths sampled from a graph is denoted as $P = {P_1, P_2, \u2026, P_p}$, where $p_m$ is the $m$-th path, which is a list to store the nodes of the path in order.\nPaths can have different lengths, and we denote the length of path $m$ be $l_m$.\nThe proposed LLM-GNN on TNG models.\nThe overview of the proposed model is shown in Figure 1. The model consists of three parts: gene encoder, path encoder, and graph encoder. For the gene and path encoder, we add sentence embedding generated from LLMs to obtain additional knowledge about genes and paths from the training corpus of the LLMs. The sentence embedding will be combined with gene expression for the prediction of the cell condition and output important paths. In the following, we discuss each module respectively.\nSentence embedding from LLM\nTo acquire knowledge about genes and paths from LLM, we utilize LLM to generate sentence embedding for genes and paths. Specifically, for each gene/protein, the textual prior knowledge is available from the four datasets NCBI Gene, GeneCards, UniProtKB/Swiss-Prot, and Tocris. In addition, we screened these genes through the network to generate gene pairs with biological connections, and converted them into text, such as \"In this pathway: receptor gene ABCA1 connect to target gene DPPA2.\" We sent the converted text to PubMedBert 18 to generate Path-Embedding. The description contains the basic information about genes like their functionality and bioprocess. Next, the description will pass to a well-trained LLM to get the final embedding"}, {"title": "Gene encoder", "content": "This model is a graph-based neural network for processing gene expression data. It combines gene expression data, sentence embedding from LLM, and node connectivity and spatial location information to produce a gene feature map understood by our model. The architecture of the gene encoder is a transformer-based model, which stacks $L$ transformer layer. Additionally, we add inductive bias of gene network structure, we add centrality encoding, spatial encoding, and edge encoding. The input to the gene encoder contains two parts: scRNA expression and the sentence embedding of genes. Let $g_{ev} \u2208 R^1$ be the gene expression of gene $v$. First, the gene expression is integrated with the sentence embedding of gene $v$ through the Expander module. In Expander, the gene expression is first expanded by Multi-Layer Perceptrons (MLPs) and then concatenated"}, {"title": "", "content": "with the sentence embedding. Finally, another MLP is used to reduce the hidden size of the concatenated embedding to the model's hidden size. The equation is shown as the follows:\n$x_v = MLP(Concat(s_{ev}, MLP(g_{ev})))$,\nWhere $Concat$ is the concatenation function to combine multiple vector into one single large vector, $x_v \u2208 R^{h_{emb}}$ and $d$ is the hidden size of the model. Next, the processed embedding of all genes is stacked and input to the transformer layer in the gene encoder.\nLet $H^l \u2208 R^{n\u00d7h_{emb}}$ denotes the stacked embedding of all genes at gene encoder layer $l$, and $H^l_v \u2208 R^{h_{emb}}$ be the embedding of gene $v$ and we have $x_v = H^l_v$. Each gene encoder layer contains a multi-head self-attention module and a point-wise feed-forward network (FFN) with residual connection applied between each part. The computation of multi-head self-attention is :\n$Q^{l,i} = H^{l-1}W^Q_i, K^{l,i} = H^{l-1}W^K_i, V^l = H^{l-1}W^V_i,$\n$head^i = Attention(Q^{l,i}, K^{l,i}, V^{l,i}) = SoftMax(\\frac{Q^{l,i} K^{l,i}^T}{\\sqrt{d_k}})V^{l,i},$\n$O^l = Concat(head^1, ..., head^h)W^O,$\nwhere $W^Q_i, W^K_i, W^V_i \u2208 R^{h_{emb}\u00d7d_k}$, and $W^O \u2208 R^{hd_k\u00d7h_{emb}}$ are all trainable weight matrix, $h$ is the number of heads, $O^l \u2208 R^{n \u00d7h_{emb}}$ is the output from the multi-head self-attention in layer $l$. However, the vanilla transformer cannot be used directly on the graph structure data as it lacks a critical part for encoding the topological information into the model. To deal with this issue, we leverage and modify the method of Graphormer. Specifically, we introduced centrality encoding, spatial encoding, and edge encoding. The centrality encoding is used to embed the graph centrality information into the model. Given the input data $X$, the computation of centrality encoding is:\n$H^0 = X + Z^{-\\{deg^-(G)\\}} + Z^{+\\{deg^+(G)\\}}$,\nwhere the $Z^-,Z^+$ are all trainable embedding vectors and $deg^-(G),deg^+(G):G \u2192 R^n$ are the function to compute the in-degree and out-degree of each node in the graph $G$. The spatial and"}, {"title": "", "content": "edge encoding is used to encode the graph structure into the model. With the spatial and edge encoding, the self-attention is revised as:\n$head^i = SoftMax(\\frac{Q^{l,i} K^{l,i}^T}{\\sqrt{d_k}} + b_i{\\{\u03a6(G)\\}} + c_i) V^{l,i}$,\nwhere $b^i$ is trainable embedding vectors to encode the spatial information at head $i$ and $\u03a6(G): G \u2192 R^{nxn}$ is the function to compute the spatial information. In the proposed model, as each gene is a node in graph and each gene has its unique identity, we directly use it as the spatial encoding function. Specifically, for each gene, we add a unique node ID, and each ID corresponds to a trainable ID embedding. We denote the ID embedding of all genes as $I \u2208 R^{n\u00d7h_{emb}}$. Then, the spatial function $\u03a6(G) = II^T$. $c^i \u2208 R^{n\u00d7n}$ is the edge embedding and $c^{uv}_i = x_{uv}$, where $x_{uv}$ is the edge feature of the edge between node $u$ and node $v$. For node pair without edge, we add a special edge type to it.\nNext, the output $O^l$ will then be fed into a point-wise feed-forward network. The computation of the point-wise feed-forward network is:\n$FFN(x) = ReLu(xW^1_i + b_1)W_2 + b_2$,\nwhere $W^1 \u2208 R^{h_{emb}\u00d7h_{emb}}, W^1 \u2208 R^{h_{emb}\u00d7h_{emb}}, b^1 \u2208 R^{h_{emb}}$, and $b^2 \u2208 R^{h_{emb}}$ are all trainable weight matrix and bias. The output is fed into the next layer of the gene encoder for further processing.\nPath encoder\nThis module further converts gene embeddings into path embeddings by connecting each layer of the gene encoder to the path encoder layer, intending to learn path-level information. In short, it is like encoding the interactions of genes and their roles in specific biological pathways, thereby helping the model understand the specific functions and action pathways of these genes in organisms. Each layer of gene encoder also connect to a path encoder layer. The output gene embedding from the layer are further convert to path embedding and to learn path-level information. Given the output embedding in the graph and the pre-defined path list of the graph."}, {"title": "", "content": "The details of the pre-defined path list are illustrated below. Suppose there are $p$ unique paths in the path list $P$, where the length of the $m$-th path is $l_m$ and the total number of nodes in the path list is $k$ (count repeated nodes in different paths). Denote the node embedding output from the layer $l$ as $H^l$, we first learn a path-specific embedding through:\n$U^l_i = scatter(H^l)W^u_i + b^u_i$,\nwhere $W^u \u2208 R^{h_{emb}\u00d7u}$ and $b^u \u2208 R^u$ are all trainable weight matrix, $scatter$ is a function to reorder and scatter the node in the graph into the order of the pre-defined path list. For example, suppose there are 5 embedding genes output from the node encoder. That is $H^l \u2208 R^{5\u00d7h_{emb}}$. We label each gene from 1 to 5. Suppose there are two paths. The first path is 1->3->4. The second path is 2->3->4->5. Then, the $scatter(H^l)$ will output a new matrix with the size of 7 and each row represents a gene in a path. For instance, the 3rd row is $H^l_1$ since it is the 3rd gene in the first path.\n$U^l \u2208 R^{k\u00d7u}$ is the learned path-specific embedding. For convenience, we denote $U^l_{m,i}$ as the embedding of $i$-th node in the $m$-th path. Then, path positional encoding and path edge encoding are introduced to encode additional information for all paths. Let $\u016a^l$ be the result embedding after the special encodings. We have:\n$\u016a^l_{m,i} = U^l_{m,i} + p^l_i + e^l_{i,i+1}$,\nWhere $p^l_i$ is the learnable positional encoding vector and its value only depends on the position $i$, $e^l_{i+1}$ is the learnable edge encoding to encode the edge type between $i$-th node and $i + 1$-th node. Next, the score of each node within the path is computed by:\n$S^l_{m,i} = tanh(\u016a^l_{m,i}W^1_1+b^1_i)W^2_i + b^2_2$\n$S'^l = ScatterSoftMax(S'),$\nwhere the $W_1 \u2208 R^{u\u00d7r}, b_1 \u2208 R^r, W_2 \u2208 R^{r\u00d7r}$, and $b_2 \u2208 R^r$ are all trainable parameters.\nScatterSoftmax is the softmax function working within each path. The $S^l \u2208 R^{k\u00d7r}$ is the final $r$ set"}, {"title": "", "content": "important score for each node in each path. We let the $r \u00d7 u = h_{emb}$ for simplicity. After we obtain the $S'$, the expression aware path embedding is computed by:\n$P^l = Flatten(ScatterSum(S^l * \u016a^l))$\nThe * is the point-wise product working on each set of important scores. That is, for each set of important scores, we do a point-wise product of that set of scores and $U^l$, which results in total $r$ sets. The ScatterSum function is the summation on each path. The Flatten is the function to flatten the embedding of all sets.\nFinally, the sentence embedding of the path is used and integrated with the expression aware path embedding the cross-attention module. Specifically, the computation in the cross-attention module is similar to the self-attention. However, the key and value tensor will be replaced with path sentence embedding and the query tensor is the expression aware path embedding.\nGraph encoder\nFinally, the graph encoder is used to make a prediction and also extract important paths. The graph encoder consists of two parts, the first part is a trainable path weight and the sigmoid function to assign each path with different scores. The second part is the jumping knowledge network to combine the graph embedding in each layer and compute the final embedding.\nIn the model, the graph embedding is learned by integrating all the path embedding from each layer, which requires an important score for each path. Normally, the score is computed based on one sample. However, such a score is not robust and may vary a lot even given a minor variation of the path embedding31,32. To avoid the issue and learn a robust important score across the whole dataset, the trainable path score $M \u2208 R^p$ is introduced. $M$ is identical to all samples and layers and learned through backpropagation. The path important score is computed by:\n$I = Sigmoid(M)$,\nwhere $I \u2208 R^P$ is the important score for each path. Next, the graph embedding of layer $l$ is computed by:"}, {"title": "", "content": "$g^l = IP^l$,\nwhere $g^l$ is the graph embedding of layer $l$. The final step of the graph encoder is to integrate the graph embedding of each layer and learn a final embedding. Here we utilize the idea of JumpingKnowledge network33 and compute the final graph embedding by:\n$G = MaxPooling(Concat(g^1, g^2, ..., g^L))$,\nwhere MaxPooling is the max pooling function and $G\u2208 R^{h_{emb}}$ is the final graph embedding learned by the PathFinder. Finally, the graph embedding is used to classify the cell sample into the corresponding condition (control/test). The prediction is a typical binary prediction computed by:\n$p = SoftMax(GW_p)$,\nWhere $W_p \u2208 R^{h_{emb} \u00d72}$ is the trainable projection matrix and $p$ is the predicted distribution.\nCell importance\nWe defined Cell Importance as a comprehensive indicator of cell frequency changes and gene expression differences between disease and non-disease states:\nCell Importance = \u03b1x ($\\left| f_{diseased} - f_{healthy} \\right|$) + \\sum_{j=1}^{m} \\left| e^{(j)}_{diseased} - e^{(j)}_{healthy} \\right|.\nSpecifically, Cell Importance is calculated using the following formula: where a=0.5 is used to balance the contribution of frequency changes and gene expression differences, $\\left| f_{diseased} - f_{healthy} \\right|$ represents the sum of the differences in cell numbers between disease and non-disease states, reflecting changes in cell frequency; $\\sum_{j=1}^{m} \\left| e^{(j)}_{diseased} - e^{(j)}_{healthy} \\right|$ represents the difference in gene expression, with j denoting individual genes within a single cell, capturing gene expression changes between disease and non-disease states based on scRNA-seq data.\nTrajectory analysis"}, {"title": "", "content": "We propose a cell trajectory analysis pipeline based on the minimum spanning tree (MST) and pruning method. First, we use the model to generate embeddings for cell populations, combining scRNA-seq data and text embedding data of genes and pathways generated by LLM to obtain embedding vectors for each cell population. Then, we calculate the cosine distance between cell populations, defined as:\n$d_{ij} = 1 - \\frac{V_i V_j}{\\left| V_i \\right|\\left| V_j \\right|}$\nIn this context, $v_i$ and $v_j$ represent the embedding vectors of cell populations i and j, respectively, while |vi| is the norm of the vector, and $d_{ij}$ denotes the cosine distance between cell populations i and j. Next, we construct a Minimum Spanning Tree (MST) among these cell populations to find the shortest loop-free path connecting all cell populations. The objective is to find the Minimum Spanning Tree $T \u2286 G$, whose edge set $E_T$ satisfies:\n$\\sum_{(i,j) \\in E_T} d_{ij} = min(\\sum_{(i,j) \\in E'} d_{ij})$\nAmong them, $E_T$ is the edge set $T$, and $E'$ is the edge set of a possible spanning tree. Finally, we perform the pruning step, where we filter out paths that do not meet the time flow requirements based on the time annotations. Specifically, we retain only the paths that flow from a previous time point to the target cell, while filtering out paths with incorrect time flow direction. Given a target cell $C_t$, and $C_i$ represents an intermediate cell along the path to the target cell, we only keep the paths that satisfy the time flow condition, expressed as:\n$E_{pruned} = {(i, j) \u2208 E_t | Time(C_i) < Time(C_j) for C_j = C_t}$\nThrough this process, we ultimately obtained a cell trajectory network that conforms to time flow, represented as:\n$T_{final} = (V, E_{pruned})$"}, {"title": "", "content": ", where $T_{final}$ denotes the final cell trajectory network that adheres to the time flow constraint. It is represented by the graph ($V$, $E_{pruned}$) where $V$ the set of nodes (cells) $E_{pruned}$ is the set of edges that only include paths satisfying the time flow condition.\nDatasets\nscRNA-seq data of human cirrhosis disease\nThe cirrhosis single-cell RNA sequencing dataset is collected from the GEO database, accession number GSE13610316, and will be used to generate pathways and compare with the benchmark for validation. It includes non-parenchymal cells collected from 5 healthy people and 5 patients with cirrhosis. After processing, the single-cell data had a total of 59,854 non-parenchymal single cells. The data were aligned to the GRCh38 and mm10 (Ensembl84) reference genomes as required and processed using the 10X Genomics Cell Ranger v.2.1.0 single-cell software suite to estimate unique molecular identifiers (UMIs). Genes expressed in less than 3 cells, cells expressing less than 300 genes, or cells with mitochondrial gene content exceeding 30% of the total UMI were excluded from the analysis. We isolated three major study cells, including 6197 Endothelial cells, 9173 Macrophages, and 20950 T cells.\nscRNA-seq data of Alzheimer's disease cohort on mice\nAlzheimer's disease scRNA-seq data with the ID number GSE164507 were collected from the Gene Expression Omnibus (GEO) database 17. The raw data were processed using the Seurat R package and followed the previous research methods 17. Specifically, we selected cell samples under two different conditions, called TAFE4_tam and TAFE4_oil. TAFE4_tam refers to mice with APOE4 gene knocked out from microglia, while TAFE4_oil refers to mice with APOE4 gene present. Specifically, we collected excitatory neurons (Ex), microglia (Mic), and astrocytes (Ast) from the TAFE4 group from the dataset, with sample numbers of 13,604, 3,874, and 734, respectively."}, {"title": "", "content": "scRNA-seq data from human of PDAC\nThe scRNA-seq dataset of PDAC tumors included samples collected from 31 patients who underwent standard treatments. The experiments comprised 81 PDAC samples divided into treatment groups: 7 untreated cases, 8 neoadjuvant FOLFIRINOX cases, 4 neoadjuvant gemcitabine + nab-paclitaxel cases, 1 mixed treatment case (FOLFIRINOX and gemcitabine + nab-paclitaxel), and 1 chemo-radiotherapy (Chemo-RT) case. Tumor samples were spatially sampled 2-4 times per tumor, followed by histology, imaging, multi-omics analysis, and bulk RNA sequencing. Single-cell RNA sequencing (scRNA-seq) data were generated for all 73 samples. After quality control, cells with a mitochondrial RNA expression greater than the 75th percentile were excluded, resulting in a total of 232,764 cells retained for downstream analysis.\nText embedding of gene description and path connection from PubMedBert\nWe embedded 33,008 genes in the dataset using the large language model PubMedBert18. First, we crawled the descriptions of all the genes, including those from the four datasets NCBI Gene, GeneCards, UniProtKB/Swiss-Prot, and Tocris. Secondly, we input them into PubMedBert 18 to generate 768-dimensional gene embedding. We obtained a total of 22,173 valid gene embeddings.\nIn addition, we screened these genes through the network to generate gene pairs with biological connections, and converted them into text, such as \"In this pathway: receptor gene ABCA1 connect to target gene DPPA2.\" We sent the converted text to PubMedBert 18 to generate Path-Embedding.\nCell Annotation\nWe used principal component analysis (PCA) to reduce the dimensionality of the raw data. We projected the gene data to 50 dimensions and used the uniform manifold approximation and projection (UMAP) method using the first 25 principal components (PCs). This approach ultimately facilitated the visualization of the data in two-dimensional space (Figure 1A). Using the marker genes in these reference datasets, we identified and labelled 14 cell types with"}, {"title": "", "content": "significant expression, including Endothelial, general CAF, CD4 T cells, cDCs, Macrophages, NK, CD8 T cells, B cells, Plasma, Islet, Tumor, Treg, Fibroblast and Acinar (Figure 1B).\nCell Distribution\nThe distribution of cell types showed significant differences of PDAC patients with treatments and without treatment (Figure 2A).\nImmunosuppression Enhancement: Significant Reduction in T Cells\nCompared with the \u201cwithout treatment\u201d group, the \u201ctreatment\u201d group had a significant decrease in CD8+ T cells and Treg cells. Specifically, CD8+ T cells decreased from 24.9% to 8.1%, while Treg cells decreased from 3.8% to 2%. This observation is consistent with the immunosuppressive nature of PDAC, which lacks immune cells even in their presence, as they often exhibit pro-tumorigenic features. The expansion of CD8+ T cells further supports systemic suppression in PDAC 19. Furthermore, immunosuppressed systemic regions provide a cozy haven for disseminated tumor cells, thereby promoting disease recurrence20 .The decrease in CD8+ T cells in the \u201ctreatment\u201d group further supports the notion that treatment may exacerbate immunosuppression in the tumor microenvironment. Furthermore, Tregs are the most abundant T cell population in PDAC and play a key role in maintaining the immunosuppressive microenvironment. The observed decrease in Treg cells from 3.8% to 2% after treatment may reflect immunoregulatory mechanisms affected by Tregs, as there are increasing reports showing a paradoxical association between tumor infiltration of Tregs and improved patient outcomes 21. These findings suggest that treatment-induced reductions in immune effector cells such as Tregs and CD8+ T cells may enhance the immunosuppressive environment in PDAC.\nAntigen Presentation and Tumor Microenvironment Remodeling: Significant Increase in cDCs and Acinar Cells"}, {"title": "", "content": "A significant increase in cDCs (from 1.6% to 9%) and acinar cells (from 3.3% to 12%) was observed in the \"with treatment\" group compared to the \"without treatment\" group. The rise in CDCs indicates enhanced antigen presentation activity, which could be a result of the treatment\naltering the tumor microenvironment to recruit or activate these dendritic cells. Similarly, the increase in acinar cells might represent changes in stromal cell populations or reprogramming of the tumor microenvironment during treatment. Studies have reported an increase in antigen-presenting cells following immunomodulatory treatments, suggesting their potential role in\nCytokine and Immune Regulation Changes: Increase in Plasma Cells and Helper T Cells\nAn increase in plasma cells (from 4.9% to 12.8%) and CD4+ T cells (from 2.4% to 8.2%) was observed in the \"with treatment\" group compared to the \"without treatment\" group. Plasma cells are known to secrete antibodies, which can modulate immune responses within the tumor microenvironment. The increase in CD4+ T cells suggests a shift in the immune balance, potentially favoring immune regulation or activation of specific immune pathways. Such changes might indicate that treatment induces a compensatory mechanism to regulate the immune response or mitigate inflammation. Previous studies have demonstrated that increased helper T cells and plasma cells correlate with better immune regulation and could enhance therapeutic efficacy.\nResults\nModel evaluation and comparison\nTo evaluate the classification performance of our model, we applied it to the human cirrhosis dataset of Endothelial cells, Macrophages, and T cells separately to predict each cell's Healthy/Cirrhosis status. For each cell type, we repeated the training three times, each time randomly splitting the entire dataset into a training subset, a validation subset, and a test subset at a ratio of 0.7/0.1/0.2. We report the average performance and standard deviation of the test set over all three runs. The detailed experimental setup is described in the Methods section. The detailed results are shown in Table 1. We also applied it to the Alzheimer's disease (AD) scRNA-seq dataset (GSE164507) collected from the Gene Expression Omnibus (GEO) database (Wang et al., 2021). We specifically selected three cell types from the TAFE4 group"}, {"title": "", "content": "for analysis: excitatory neurons (Ex), microglia (Mic), and astrocytes (Ast). The results are summarized in Table 2.\nWe compare our work with random forests, multilayer perceptron, convolutional networks, and our models from previous phases. We find that our model significantly outperforms all other models, including the model we developed in the previous phase. It is worth mentioning that at this stage of our research, we introduced gene embedding and path embedding from the output of the large language model (PubMebBert) as part of our model training, understanding, and applying the information that extracts from the large language model has also become key to the progress of the model. Table 3,4 lists the complete results we achieved in the following tasks. All reported data are from the test set.\nPrediction Tasks of signaling pathway inference\nWe used the golden network proposed by McCalla et al. 24.to explore the rationality of the pathways we generated. The study by McCalla et al. collected multiple experimental regulatory interaction networks from public databases and literature as gold standards for network inference algorithms24. These experiments are based on ChIP-chip, ChIP-seq, or regulatory factor perturbation after global transcriptome analysis. They obtained multiple networks based"}, {"title": "", "content": "on ChIP and transcription factor perturbation experiments for each organism and cell type, which are called \"ChIP\" and \"Perturb\". For human ESC (hESC) cell lines, they used CIS-BP 25, ENCODE26, and JASPAR 27 databases for construction. We used their \u201chESC_ChIP\" and \"hESC_Perturb\u201d gold standard networks to test the results obtained by our model on the human Cirrhosis and human AD datasets.\nSummary and conclusion\nIn conclusion, the TNGs and joint LLM-GNN models are important approaches for scientific discovery. The design and implementation of novel and efficient LLM-GNN and TNGs models are a potentially new field for scientific discovery in the age of Al."}]}