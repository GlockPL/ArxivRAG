{"title": "Weakly Supervised Framework Considering Multi-temporal Information for Large-scale Cropland Mapping with Satellite Imagery", "authors": ["Yuze Wang", "Aoran Hu", "Ji Qi", "Yang Liu", "Chao Tao"], "abstract": "Accurately mapping large-scale cropland is crucial for agricultural production management and planning. Currently, the combination of remote sensing data and deep learning techniques has shown outstanding performance in cropland mapping. However, those approaches require massive precise labels, which are labor-intensive. To reduce the label cost, this study presented a weakly supervised framework considering multi-temporal information for large-scale cropland mapping. Specifically, we extract high-quality labels according to their consistency among global land cover (GLC) products to construct the supervised learning signal. On the one hand, to alleviate the over-fitting problem caused by the model's over-trust of remaining errors in high-quality labels, we encode the similarity/aggregation of cropland in the visual/spatial domain to construct the unsupervised learning signal, and take it as the regularization term to constrain the supervised part. On the other hand, to sufficiently leverage the plentiful information in the samples without high-quality labels, we also incorporate the unsupervised learning signal in these samples, enriching the diversity of the feature space. After that, to capture the phenological features of croplands, we introduce dense satellite image time series (SITS) to extend the proposed framework in the temporal dimension. We also visualized the high-dimensional phenological features to uncover how multi-temporal information benefits cropland extraction, and assessed the method's robustness under conditions of data scarcity. The proposed framework has been experimentally validated for strong adaptability across three study areas (Hunan Province, Southeast France, and Kansas) in large-scale cropland mapping, and the internal mechanism and temporal generalizability are also investigated. The source codes are available at https://github.com/wangyuze-csu/WSFCMI.", "sections": [{"title": "1. Introduction", "content": "Over the past decades, remote sensing observation has played significant roles in large-scale cropland mapping and monitoring. By offering timely and comprehensive images of nearly every part of the Earth's surface, it served as a reliable information source for identifying cropland spatial distribution on a large scale. Moreover, it provides valuable support for various agricultural applications, such as land-use planning, food security, and sustainable agroecology.\nWith the accumulation of remote sensing data, several data-driven methods based on machine learning have been widely applied to the large-scale cropland mapping. The widely adopted approaches are employing traditional machine learning methods to analyze and interpret the remote sensing (RS) images. These methods often rely on a combination of low-level and middle-level visual features, such as texture, spectral, and shape features. However, due to the influence of various factors on cropland, such as climate, geography, and topography, the methods that rely on handcrafted features typically encounter issues of restricted generalization performance and low accuracy. In recent years, Deep Convolutional Neural Networks (DCNNs) attracted great attention in cropland mapping because it can extract high-level visual features that are more representative and distinguishable. However, the performance of DCNNs shows a strong positive correlation with the number and diversity of high-quality labeled samples, leading to high labeling costs. Although various methods designed for sparse labeling conditions can significantly decrease the demand for labeling, the remaining label requirement in large-scale cropland mapping still implies a considerable manual cost. So, reducing the labeling cost while maintaining cropland mapping accuracy is still a great challenge.\nSome methods use existing global land cover (GLC) products, such as GFSAD 30, CCI-LC, FROMGLC , MCD12Q1, Esri , ESA, DyWorld as reference to train the model at a low cost and obtain accurate cropland mapping results. Those methods are known as Automatic Training Sample Generation (ATSG). However, the label obtained from GLC products inevitably contains some errors, due to factors like the diversity of cropland scenes, imaging conditions, and classifier performance. Directly using those labels may lead to instability and over-fitting in the model learning process, causing low-quality cropland mapping. Therefore, identifying the errors of the labels generated from GLC products and preventing their negative impacts on the model training process is one of the significant research topics of ATSG methods.\nAccording to the post-processing ways of error labels, the ATSG methods based on existing GLC products are divided into two categories: discard and re-correct\nDiscard methods establish quality criteria to rate the labels from the GLC products, and exclude low-quality labels before training. Based on the MCD12Q1 product, Zhang and Roy (2017) utilized temporal invariance as a quality criterion to discard the labels that have changes within three years, and excluded the labels with low classification confidence according to the quality assessment layer provided in the product auxiliary information. With the continuous release of GLC products, Li and Xu identified high-quality labels by considering their consistent performance across four GLC products (GFSAD 30, CCI-LC, FROMGLC, and MCD12Q1), and discarded the outlier labels based on the spectral distribution of all corresponding pixels. Considering the spectral mixing problem among vegetation cover, Hermosilla et al. integrated the 3D information from LiDAR data into the quality assessment of the labels from three products (NFI photo plot, EOSD and NWS). By doing so, they could identify and remove the low-quality labels that mismatch the vegetation height.\nRe-correct methods construct filter strategies to obtain high-quality labels based on criteria such as classification consistency across multiple products and spectral consistency among the same land covers. High-quality labels are then obtained and used as references to correct the remaining labels. Considering the spatial continuity and texture consistency within the same land cover, Chen et al. took the sub-regions obtained by Simple Non-Iterative Clustering (SNIC) as the correction unit, and reclassified each unit through voting among filtered high-quality labels, thereby extending high-quality labels into regions with low-quality labels. Zhang et al. considered the phenological attribute of vegetation cover, and used high-quality labels and multi-temporal images to train the classifier to correct low-quality labels. Naboureh et al. selected pure pixels with high-quality labels as reference, and rectified the remaining pixels containing low-quality labels according to their spectral distance from the selected pixels.\nThe above methods still encountered difficulties in achieving high-precision cropland extraction, thus falling short of meeting the localized needs of agricultural applications. The main reasons are as follows:\n1) Insufficient use of samples with low-quality labels: Discard methods can mitigate the model's over-fitting problem on incorrect labels, they also eliminate the samples with diverse features but low-quality labels before training. Furthermore, Discard methods may lead to the imbalance of intra-class distribution for training samples, so the model can only learn typical features and has limited generalization capability. For example, in cropland mapping, these methods often constrain the trained classifier to only extract continuous and large block plain croplands, making it challenging to recognize cropland in complex environments, such as those scattered across hills and mountains.\n2) Over-trust the samples with high-quality labels: Although re-correct methods allow diverse samples to participate in the model training process, they potentially introduce a lot of label noise, misleading the model during the learning process. This occurs for two reasons. First, due to the inherent limitations of the products, the filtering"}, {"title": "2. Materials and process", "content": null}, {"title": "2.1. Study areas and satellite imagery", "content": "We chose three study areas across Asia, Europe, and North America, each representing distinct terrain landscapes, climatic regions, and agriculture systems, including different crop types and phenological attributes. Collectively, these regions encompass a vast area exceeding 620,806 km\u00b2.\nThe general information of the study areas is as follows (Table 1) :\n\u2022 Hunan Province in China: Hunan province is situated in the central-southern part of China, covering a total area of 211,800 km\u00b2. As one of China's largest rice-planting bases, Hunan province ranks among the top ten in national grain production. The province's diverse terrain, ranging from semi-alpine to low mountains, hills, basins, and plains, presents significant challenges for cropland monitoring. The area has a continental subtropical monsoonal humid climate, offering abundant light, heat, and water resources, which is suitable for cultivating a variety of crops including rice, rapeseed, cotton, and tea. The rice types grown here include double-season rice, medium-season rice, and late-season rice, each with distinct sowing and harvesting periods from April to November.\n\u2022 Southwest France: We chose the southwest part of France, encompassing about 195,910 km\u00b2, which represents 2/5 of the entire country, as our study area. This region has several major grain-producing areas such as Aquitaine Basin and Centre-Val de Loire. The landscape is dominated by basins and valleys. It has a temperate oceanic climate, suitable for cultivating a variety of crops, such as spring wheat, soybeans, and olives. Spring wheat and soybeans are typically sown in spring and harvested in late summer, while the olives are planted in early summer and harvested in mid-autumn.\n\u2022 Kansas State in the USA: Kansas state is located in the middle of the USA, covering a total area of 213,096 km\u00b2. It is a major grain-producing region in the United States, with the highest wheat production in the country. Kansas has large plains, suitable for extensive mechanized agricultural activities. The area has a temperate continental climate, favoring growing winter wheat and corn. Winter wheat is sown in mid-September and harvested in late June to early July of the following year. Corn is sown in mid-April and harvested in mid-October.\nWe collected Sentinel-2A and Sentinel-2B Level-2A Bottom of Atmosphere reflectance images (S2 L2A) from Google Cloud, encompassing the three study areas for the period from January 2020 to December 2020. For cropland extraction, we selectively utilized the Blue, Green, Red, and Near-Infrared (NIR) bands (10 m spatial resolution), along with Narrow NIR, Red Edge (RE), and Short-Wave Infrared (SWIR) bands (20 m spatial resolution). Furthermore, we used the Quality Assurance (QA) bands provided by the ESA on Google Earth Engine (GEE) as a reference to select the images with cloud coverage less than 20%. The QA bands were also used to identify cloudy regions in the remaining images, which were filled using the cloud-free images from adjacent time phases. Finally, all SITS were composed of monthly images, where each month's image was obtained by averaging all available images during the month."}, {"title": "2.2. Global Cropland layer and Validation datasets", "content": "To generate training labels, we utilized the cropland layers from three GLC products with 10m spatial resolution: ESA World Cover, Eris Land Cover, and Dynamic World. Firstly, we established a definition of 'cropland' by uniformly mapping from the related categories in various GLC products. Specifically, cropland is defined as fields that are covered by annual crops that are sown or planted and are capable of being harvested at least once within the 12 months after the date of sowing or planting. This type of annual cropland generates an herbaceous canopy, and may occasionally be intermixed with trees or shrubby vegetation.\nSecondly, we collected their data through the GEE platform, and projected their geographic coordinates to the World Geodetic System 1984 (WGS 84), consistent with that of SITS. The details of the three GLC products are as follows:\n\u2022 ESA World Cover (ESA) : It was developed as a part of the ESA WorldCover project, under the 5th Earth Observation Envelope Program (EOEP-5) of the European Space Agency. The product was generated by SITS from Sentinel-1 and Sentinel-2 with multiple random forest classifiers from 2020 to 2021. It contains 11 first-level categories, and we used the 'Cultivated areas' category from the 2020 product to obtain the labels. This category is defined as land covered with annual cropland that is sowed/planted and harvestable at least once within the 12 months after the sowing/planting date. The annual cropland produces an herbaceous cover and is sometimes combined with some tree or woody vegetation.\n\u2022 Esri Land Cover (Esri) : It is a 10-m resolution map of the Earth's land surface, published by the Environmental Systems Research Institute. This map was annually generated from 2017 to 2022 using the composite Sentinel-2 satellite images by deep learning models. The product divides the land cover into 9 categories, and we used the 'cropland' category from the 2020 product to obtain the labels. This category includes crops, human-planted cereals, and other non-tree-height cultivated plants.\n\u2022 Dynamic World (DyWorld) : It was developed by Google and the World Resources Institute. It is a near real-time global land use and cover dataset, updated in sync with the revisit cycle of the Sentinel-2 satellite. The product was generated by deep learning models using available images from 2015 to 2023. We collated all available data from various time phases in 2020 and determined the land cover categories for each pixel based on the mode principle derived from the statistical analysis. It divides land cover into 9 categories, and we used the 'crop' category to obtain the labels. This category is defined as crops humans planted/plotted cereals.\nTo assess the accuracy and completeness of the cropland mapping obtained by the proposed method, we established validation datasets for the three study areas. Considering that random sampling alone may skew the validation datasets toward particular cropland types prevalent in the study areas, potentially compromising the comprehensiveness and objectivity of the cropland extraction assessment in diverse environments. To address this issue, we introduced manual intervention in the selection process for the validation area. Specifically, we classified the entire study area based on the topographical features, and then utilized this result as a foundation to manually refine the sub-areas that were initially selected through random sampling.\nFor the Hunan study area, which is predominantly covered by hills and mountains, we manually excluded certain mountainous and hilly sub-areas and included sub-areas dominated by plains. For the Kansas study area, where plains are the dominant landscape, we excluded some sub-areas of plains, and added sub-areas that include hills and mountains. The validation labels of Hunan and Kansas study areas were obtained by visually interpreting the RGB bands of Sentinel-2 time series images, assisted by high-resolution Google images. In the Southwest France study area, we utilized the S4A crop classification datasets as a basis and modified them for validation purposes. We initially checked all the labels, and selected sub-areas with relatively comprehensive cropland annotations for further supplementation and adjustment. Subsequently, we manually selected the sub-areas to ensure a balance among various types of cropland scenes. The sampling results of the final validation areas are shown in Figure 1.\nFinally, in the Hunan study area, we labeled a total of 978,388 cropland fields, accounting for 18.00% of the validation area. In the Southwest France study area, we identified 520,177 cropland fields, which covered 40.48% of the validation area. In the Kansas study area, we identified 185,250 cropland fields, accounting for 48.24% of the validation area. Note that, 'cropland fields' refer to a piece of cropland separated from one another by identifiable boundaries. Each image-level sample may include several cropland fields."}, {"title": "3. Methodology", "content": "The proposed framework aims to effectively use the prior information from existing GLC products for large-scale cropland mapping. By using this prior information, the model can learn the cropland phenology features from SITS without manual labeling. As shown in Figure 3, the framework consists of three parts. (1) Labels collecting and quality rating: We collect labels from GLC products, and evaluate their quality. These labels are then categorized into high-quality and low-quality parts; (2) Construction of weakly supervised learning signal: we construct the supervised part of the learning signal using high-quality labels, and encode the image intrinsic feature distribution to construct the unsupervised part of the learning signal. By constructing the unsupervised part, we not only incorporate the abundant information contained in the samples with low-quality labels into the model learning process, but also prompt the model to assess and question the reliability of the high-quality labels. Additionally, we extend the weakly supervised signal in the temporal dimension to sufficiently extract the phenology features of cropland. (3) Accuracy assessment: we utilize the well-trained models for large-scale cropland mapping, and establish validation datasets to evaluate their performance."}, {"title": "3.1. Labels collecting and quality rating", "content": "The cropland layers from GLC products are cross-referenced and quality-rated. Consistent parts are considered high-quality labels, and divergent parts are considered low-quality labels. The quality of a given spatial position (i, j) is determined by whether the results of GLC products are the same. Finally, we obtained the high-quality sample mask Mhigh(i, j) and high-quality labels Y(i, j):\nMhigh(i, j) = \\begin{cases}\n1, & \\text{if } P_1(i, j) = P_2(i, j) = ... = P_m(i, j) \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\nY(i, j) = M_{high}(i, j) * \\frac{1}{M} \\sum_{m=0}^{M-1}(P_m(i, j))\nwhere Pm(i, j) is the cropland labels of the m-th product at position (i, j), and M represents the total number of products"}, {"title": "3.2. The construction of weakly supervised learning signal", "content": "The weakly supervised learning signal consists of the supervised part and the unsupervised part. The supervised part is constructed based on high-quality labels, and it can effectively guide the model to learn the cropland features without manual labeling. The unsupervised part is constructed by encoding the visual similarity and spatial aggregation of the same land cover based on the feature space extracted from images. It serves as the regularization term to avoid over-fitting the remaining errors in high-quality labels. In addition, the unsupervised part integrates the abundant information contained in the low-quality labeled samples to enhance the diversity of the model's feature space in the optimization process.\nGiven an image sequence X of size T \u00d7 C \u00d7 H \u00d7 W, we use it as the input of the multi-temporal network to obtain the prediction result P, which is used to construct the supervised part of the learning signal. The intermediate feature maps extracted by the multi-temporal network are fused to form the feature space Z, which is used to construct the unsupervised part of the learning signal. Both the supervised and unsupervised parts are employed in samples with high-quality labels, but only the unsupervised part is employed in samples with low-quality labels. The specific construction process is as follows:\nWe use the high-quality sample mask and prediction result to generate the masked prediction, which is combined with the high-quality labels to construct the supervised part of the learning signal (Figure 4). Given a pixel (i, j), with 0 \u2264 i < H and 0 \u2264 j < W, the masked prediction Mhigh(i, j) * P(i, j) and the high-quality Y(i, j) are used to calculate the supervised cross-entropy loss $Loss_{SL}$ as follows:\n$LOSS_{SL} = -\\sum_{i=0}^{H} \\sum_{j=0}^{W} (Y(i, j) * log (M_{high}(i, j) * P(i, j)))$\nDue to the quality limitation of the products, the high-quality labels may contain some errors. To avoid over-fitting these errors, inspired by Hua et al. and Sabokrou et al., we construct the unsupervised part of the learning signal as the regularization term to constrain the model. The construction is based on two assumptions: (1) In the visual domain, two visually similar samples have a higher probability of belonging to the same semantical concept, which means these samples are adjacent in the model's feature space. (2) In the spatial domain, the land covers are continuous and aggregated which means the adjacent samples with the most similarities should belong to the same category. The regularization term allows the model to enhance the stability of the feature space during the optimization process, which can alleviate cognitive bias caused by the over-fitting of the remaining errors.\nTo enhance the model's generalization ability in large-scale cropland mapping, we also employ the unsupervised part of the learning signal in the low-quality samples. Thus, the information from these samples can be involved in the model optimization process, balancing the intra-class feature distribution of the training samples and improving the diversity of the feature space.\nIn Figure 5, the feature space Z is represented by the fused intermediate feature maps obtained from the multi-temporal network. Given the sample xn, which represents the nth pixel in an image mapped to the high-dimensional feature space Z, where N = W\u00d7H and n \u2208 N. In the visual domain, the sample $x_n^s$ and $x_n^d$ are identified as the samples with the highest similarity and difference, respectively, to xn. These are determined by searching within the same image using the Sorensen-Dice index. We then encourage the feature distance between xn and $x_n^s$/$x_n^d$ to be as small/large as possible during model optimization In the spatial domain, given $x_n^a$ as the sample with the highest similarity to xn among its eight-neighborhood samples, we encourage the model to minimize their feature distance. Ultimately, the constraints from both the visual and spatial domains are integrated to yield the unsupervised loss $Loss_{USL}$:\n$LOSS_{USL} = \\alpha \\sum_{n=0}^{N} D_{KL}[Z(x_n), Z(x_n^s)] - \\beta \\sum_{n=0}^{N} D_{KL}[Z(x_n), Z(x_n^d)] + \\gamma \\sum_{n=0}^{N} D_{KL}[Z(x_n), Z(x_n^a)]$\nwhere \u03b1, \u03b2, and \u03b3 are the priori parameters to measure the importance of different terms. DKL represents the Kullback-Leibler Divergence. At last, the entire model is optimized by minimizing the weakly supervised loss $Loss_{WS}$ that combines the aforementioned supervised and unsupervised parts:\n$Loss_{WS} = Loss_{SL} + Loss_{USL}$"}, {"title": "3.3. The structure of multi-temporal network", "content": "In this paper, considering the temporal features of cropland, we use the multi-temporal network for phenological feature extraction to enhance the separability of cropland in the feature space. Note that, the multi-temporal network is changeable, allowing for different network architectures to be employed or substituted as needed. We select U-Net with Temporal Attention Encoder (U-TAE) to extract multi-scale spatio-temporal features, which enhances the robustness of anomalies and the capture ability of long-term dependencies in temporal features. The U-TAE contains three parts: spatial encoder, temporal encoder, and spatial-temporal decoder.\nIn the spatial encoder, each image in the temporal sequence is embedded by a shared multi-level convolutional spatial encoder E\u00b9. In Equation (6), image sequence X of size T \u00d7 C \u00d7 H \u00d7 W is used as input, The multi-scale spatial feature sequence e' of the temporal image is obtained by continuously down-sampling using sliding convolution at each layer.\ne' = \\begin{cases}\nX,\n& t=0 \\\\\nE'(e_{t-1})^l,\n& for l \\in [1, L]\n\\end{cases}\nwhere 1 is the number of layers and the size of e' is $T \\times C^l \\times H^l \\times W^l$.\nIn the temporal encoder, Lightweight-Temporal Attention Encoder (L-TAE) is used for processing the temporal dimension, which can help the model to capture long-term dependencies and adapt to dynamic temporal features of the cropland. In this process, the model obtains attention a' of the image sequence in the temporal dimension based on the lowest scale spatial feature, and resizes it to $H^l \\times W^l$ for compressing the sequence of spatial features at different scales. Eventually, the feature sequences $F^l$ with size $C^l \\times H^l \\times W^l$ are obtained from multi-scale spatial features $e^l$ and the temporal attention $a^l$ :\na' = \\begin{cases}\nLTAE(e^l),\n& if l=L \\\\\nresize[LTAE(e^l)][:, 0],\n& for l \\in [0, L-1]\n\\end{cases}\n\n$F^l = \\sum_{t=0}^{T} Conv_{1x1}[a_t^l \\odot e_t^l]$, for $l \\in [0, L]$\nwhere $Conv_{1x1}$ is a shared 1 \u00d7 1 convolution layer of width $C^l$ and $ \\odot$ is the term-wise multiplication with channel broadcasting.\nIn the Spatial-Temporal decoding part, a multi-level convolutional decoder D\u00b9 is used to generate single spatial-temporal feature maps on different scales. In detail, the feature map $D^l$ connects with compressed features $F^l$ channel-wise, and continuously up-sampled by transposed convolution $D_{up}$ to get the multi-scale spatial-temporal feature maps:\nD' = \\begin{cases}\nF^L,\n& if l=L \\\\\nD'(D_{up}[D^{(l+1)}], F^l),\n& for l \\in [0, L-1]\n\\end{cases}\n\nP = softmax(Conv(D^L))\nwhere [\u2022] is the channel-wise concatenation. The last feature map $D^L$ with the same size as the original image (H\u00d7W) is processed by the convolution layer Conv(\u2022) and the activation function $sof tmax(\\bullet)$ to get the predictions P.\nUltimately, the predictions P and multi-scale spatial-temporal feature maps D' are used as Z for constructing the supervised and unsupervised part of the learning signal, respectively."}, {"title": "3.4. Mapping and accuracy assessment", "content": "Given that the proposed method does not rely on manual annotations for model training, we are not constrained by the typical limitations of training and validation set independence. Although we refer to the information from GLC products as labels, they more closely resemble pseudo-labels. The overlap between the training and validation sets will not affect the accuracy assessment process. Consequently, we trained the models with each study area, and employed the well-trained models in three study areas (Hunan Province in China, Southeast France, and Kansas State in the USA) to map the cropland meanwhile, we carefully sampled the representative regions of each study area for manual labeling, and constructed the validation set, which is included in the region of the training set. To ensure a comprehensive and objective assessment, the selection process for the validation area was guided by topography to balance different cropland scenes (plains, hills, and mountainous). Details regarding the selection and labeling of the validation area are described in 2.2.\nTo evaluate the accuracy of our cropland mapping results, we employed a set of assessment metrics, including Overall Accuracy (OA), Producer's Accuracy (PA), User's Accuracy (UA), Mean Intersection over Union (mIoU), the macro-average of F1-scores across all (Avg. F1-score), and the F1-score of cropland (Crop. F1-score ) and Non-cropland (Non. crop. F1-score ). The F1-score acts as a harmonic mean that integrates PA and UA to measure the model's precision and recall capabilities effectively."}, {"title": "4. Experiment", "content": null}, {"title": "4.1. Experiment setting", "content": "Experiments used the datasets collected from Hunan province in China, Southeast France, and Kansas state in the USA. We cropped all the images into the size of 256*256 pixels. For the training dataset, we produced 32,318 patches in the Hunan study area, 29,893 in Southeast France, and 32,515 in Kansas. For the validation dataset, we obtained 15,484 patches within the Hunan study area, 6,152 in Southeast France, and 7,697 in Kansas. In the process of cropland mapping, we segmented all the images into multiple patches with a sliding length of 128 pixels, and utilized probabilistic prediction results to integrate the final result.\nAll models were trained using PyTorch on the Ubuntu 16.04 operation system with an NVIDIA GTX3080 GPU (11-GB memory). Each model was trained using the Adam optimizer, with a batch size of 8 and 100 epochs. The learning rate was initially set to 1 \u00d7 10-3 and decrease it to 1 \u00d7 10\u22124 for the last 50 epochs."}, {"title": "4.2. Cropland mapping results", "content": "To demonstrate the effectiveness of our framework, we present the confusion matrix and all accuracy evaluation metrics for our cropland mapping results across the three study regions in Figure 6 and Table 2. In the study areas of Hunan, Southwest France, and Kansas, we attained an Avg.F1-score of 77.91%, 80.50%, and 88.36%, respectively."}, {"title": "4.3. Comparison with other GLC products", "content": "A great challenge for large-scale cropland mapping is to ensure the framework's generalizability across diverse scenarios while maintaining low labeling costs. While our framework provides a promising solution to reduce labeling costs through the proposed weakly supervised learning signals, its generalizability in practical scenarios needs objective assessment. Therefore, we compared the proposed approach with three public GLC products (ESA, Esri, and Dyworld) by a series of comparative experiments across diverse agrosystems, including Hunan, Southwest France, and Kansas. In quantitative terms, the results in Table 3 demonstrate that the proposed framework outperformed the three products. Compared with the highest accuracy of GLC products, our framework achieved improvements in the Avg.F1-score by 5.84%, 0.51%, and 1.40% in the corresponding study area, while achieving improvements in the Crop.F1-score by 11.91%, 0.37%, and 2.18%. The qualitative comparison results are shown in Figure 9, for the Hunan study area, our results are more comprehensive and provide better extraction of fragmented plain and hill croplands. For the Southwest France study area, our results exhibit greater detail. In the Kansas study area, our method is capable of extracting fields with unusual phenological attributes.\nMoreover, the stability of the model's performance is crucial for large-scale cropland mapping in practical application scenarios. However, as shown in Table 3, the accuracy of the three products varies greatly across these study areas. For instance, the ESA performs exceptionally well in Kansas, but it showed the poorest performance in Southwest France. To further evaluate the reliability, we calculated the average accuracy of our proposed framework and the three products in the three study areas (Figure 10). Our framework demonstrates notable improvements in terms of OA, mIoU, and Avg.F1-score, surpassing the best-performing products by 5.52%, 9.70%, and 6.27%, respectively. These results clearly indicated the superior reliability and stability of our proposed framework."}, {"title": "4.4. Comparison with other methods", "content": "To demonstrate the superiority of the proposed method, we compared it with the following three types of methods based on Automatic Training Sample Generation (ATSG) :\nDiscard methods: We used classical single- and multi-temporal networks, including Deeplabv3+ , Unet-3D and LSTM, and U-TAE as classifiers, relying solely on"}, {"title": "5. Discussion", "content": "In this section, we conducted ablation experiments across all three study areas to comprehensively analyze the input setting, and further discussed the limitations and effects of using temporal information under our framework. We discussed four questions: the impacts of using different GLC product combinations as inputs, the temporal"}, {"title": "5.1. Analysis of Employing Varied GLC Product Combinations as Inputs", "content": "In the proposed framework, the use of different GLC products as inputs results in diverse high-quality labels, which directly affect the supervised part of the learning signal. To analyze the impacts of these inputs, we used different combinations of ESA, Esri, and Dyworld as inputs, and calculated the number and accuracy of the obtained high-quality labels. The results in Table 5 demonstrate that the combination of all three products yields the best performance across all the study areas, with the label accuracy showing the closest correlation with the final prediction outcome. In the Hunan study areas, although the combination of the Dyworld and Esri gets the highest label ratio of 72.02%, its prediction F1-score is the lowest at 72.99%, influenced by its lowest label accuracy of 75.33%. In the study areas of Southwest France, the combination of Dyworld and ESA demonstrates the highest label ratio of 90.66%, but with the lowest label accuracy of 77.28%, which leads to the lowest prediction F1-score of 73.49%. In the Kansas study areas, the combination of all three GLC products has a label ratio of 72.85%, but it has higher prediction accuracy than the rest combinations, because of its label accuracy of 91.65%. The reason is that our framework uses unsupervised learning signals to incorporate the samples without high-quality labels into the model learning process, reducing the model's high dependence on the label number."}, {"title": "5.2. Assessment of temporal generalizability", "content": "A key challenge in large-scale cropland mapping is the generalization capacity of the framework. Influenced by different agrosystems and climates, the cropland exhibits various phenological attributes on SITS for different spatial and temporal coverage. Section 4.3 provides a comprehensive analysis of the model's spatial generalization capability"}, {"title": "5.3. Exploring the Imperative Role of Time-series Information", "content": "To analyze the necessity of incorporating temporal information, we used temporal data from each season to generate the seasonal composite images. These images were used as input to train the model, following the proposed framework but excluding the temporal encoding part. when compared with the highest accuracy obtained by using only the seasonal composite images, the extraction results from the entire year's SITS exhibit improvement of 3.47%, 5.22%, and 4.26% in Hunan, Southwest France, and Kansas, respectively. The main reasons for this phenomenon are as follows:\nConsidering cropland as a whole, it displays significant visual variations across different periods due to its planting status. This poses challenges for accurately delineating the extent of cropland, as it can resemble other land covers during some periods. For instance, in the Hunan area, where rice cultivation is prevalent, the planting season falls in early summer and harvesting occurs in autumn. Thus, the cropland exhibits similar visual features to bare land in spring and winter, leading to relatively low extraction accuracy during these two seasons:\nInside one cropland, there may be multiple crop types with different phenological patterns, which leads to large intra-class diversity in a specific period. In this case, the models only relying on textural, spectral, and spatial features in a single time phase can hardly recognize every part of the cropland. For instance, in Hunan, oilseed rape is often planted in the autumn, coinciding with the harvest of rice. Although the harvested rice field can be easily distinguished by its unique textural feature, oilseed rape may visually resemble other vegetation covers like shrubs or lawns during"}, {"title": "5.4. Robustness Analysis of Cloud Cover Scenarios", "content": "In practical application scenarios", "follows": "in the spatial dimension, we added cloud masks of various sizes to the images, ranging from 0.00% to 40.00% in increments of 10.00%. In the temporal dimension, we simulated data-missing situations by randomly dropping images. The dropping rate was set from 0% to 83"}]}