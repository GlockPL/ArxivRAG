{"title": "AD-Net: Attention-based dilated convolutional residual network with guided decoder for robust skin lesion segmentation", "authors": ["Asim Naveed", "Syed S. Naqvi", "Tariq M. Khan", "Shahzaib Iqbal", "M. Yaqoob Wani", "Haroon Ahmed Khan"], "abstract": "In computer-aided diagnosis tools employed for skin cancer treatment and early diagnosis, skin lesion segmentation is important. However, achieving precise segmentation is challenging due to inherent variations in appearance, contrast, texture, and blurry lesion boundaries. This research presents a robust approach utilizing a dilated convolutional residual network, which incorporates an attention-based spatial feature enhancement block (ASFEB) and employs a guided decoder strategy. In each dilated convolutional residual block, dilated convolution is employed to broaden the receptive field with varying dilation rates. To improve the spatial feature information of the encoder, we employed an attention-based spatial feature enhancement block in the skip connections. The ASFEB in our proposed method combines feature maps obtained from average and maximum-pooling operations. These combined features are then weighted using the active outcome of global average pooling and convolution operations. Additionally, we have incorporated a guided decoder strategy, where each decoder block is optimized using an individual loss function to enhance the feature learning process in the proposed AD-Net. The proposed AD-Net presents a significant benefit by necessitating fewer model parameters compared to its peer methods. This reduction in parameters directly impacts the number of labeled data required for training, facilitating faster convergence during the training process. The effectiveness of the proposed AD-Net was evaluated using four public benchmark datasets. We conducted a Wilcoxon signed-rank test to verify the efficiency of the AD-Net. The outcomes suggest that our method surpasses other cutting-edge methods in performance, even without the implementation of data augmentation strategies.", "sections": [{"title": "1 Introduction", "content": "Skin diseases indeed represent a substantial health concern, with skin cancer, particularly melanoma, being one of the most life-threatening forms. According to global cancer statistics, skin cancers are among the fastest-growing cancers worldwide [76]. The American Cancer Society (ACS) reports that cancer is the leading cause of mortality globally, accounting for 1.96 million new cancer cases in 2023, and 0.61 million deaths due to cancer. In 2023, 0.098 million new cases of skin cancer from melanoma were reported, with 8. 2% of individuals losing their lives [76]. Skin cancer is commonly classified into two main types: melanoma and non-melanoma [8]. Non-melanoma skin cancers, including squamous cell carcinoma and basal cell carcinoma, are generally considered less life-threatening. However, they may still require painful treatment interventions. In contrast, melanoma represents a highly malignant and deadly form of skin cancer, characterised by a significantly higher mortality rate [89]. In the realm of skin cancer management, prompt diagnosis and timely treatment are important factors for effective control. However, relying solely on visual assessments by medical experts can introduce subjectivity and lead to inconsistent diagnoses, even among experienced professionals. Thus, to enable an early and precise diagnosis, the establishment of effective and automated techniques for the segmentation of skin lesions is essential. These automated methods offer consistent and objective analyses, which improves the reliability and efficiency of skin cancer diagnosis and treatment [61, 64, 80]. Accurate segmentation of skin lesions is a critical prerequisite for effective diagnosis, analysis, and treatment in computer-aided diagnostic (CAD) systems. However, segmentation of dermoscopic images poses distinct challenges, primarily due to variations in colour and texture, as well as the presence of artefacts such as hair and marks [18]. Dermoscopy is a noninvasive imaging technique that enables in vivo observation of pigmented skin lesions and employs optical magnification lenses and specialised illumination to improve the visibility of the underlying features [71]. Dermoscopic images of skin lesions pose further challenges. First, they include the irregular and fuzzy boundaries typically associated with skin lesions. Secondly, distinguishing a skin lesion from its surrounding tissue is often difficult. Third, interpreting the features of skin lesions can be challenging due to their typically irregular shapes and colors. In addition, segmentation is complicated by various interference factors, such as hairs, blood vessels, ruler markings, and ink speckles [23, 27]. The challenges mentioned above are presented in Figure 1."}, {"title": "Traditional image segmentation methods often depend on manually crafted fea- tures [44, 45, 66, 79], which show limited performance when it comes to segmenting complex images such as dermoscopic images[1, 35, 52]. Manually crafted features require domain experience and may not generalise well to the wide variability in the appearance of the lesion [62]. In fact, deep learning techniques have revolutionised the domain by their ability to learn directly from data, yielding remarkable results [46, 47, 55]. Unlike traditional algorithms that often rely on hand-crafted features, deep learning models operate on a data-driven basis, allowing them to automatically extract relevant features and patterns from input data [6, 36, 48, 50, 53]. This data- driven approach contributes to the robustness of segmentation models and builds confidence in their performance [41, 49, 54, 56, 57, 67]. In clinical settings, this con- fidence is crucial, as it serves as an acceptance criterion for the deployment of such models in real-world settings [22, 31, 38, 39]. The U-Net architecture has gathered substantial popularity in the realm of med- ical image segmentation, due to its remarkable performance in capturing fine details of features via its encoder-decoder paths with skip connections [73]. Based on U-Net, researchers have developed several unique architectures, such as U-Net++ [90], Atten- tion U-Net [70], and recurrent residual U-Net [3], which are specifically designed for different segmentation tasks. In recent years, various advanced techniques and modi- fications based on the U-Net framework have emerged to enhance both performance and computational efficiency in tasks such as segmentation of skin lesions [83]. Many researchers proposed methods for the segmentation of skin lesions with dif- ferent approaches. For instance, Lei et al. introduced an approach called the Dual Adversarial Generator and Discriminator Network (DAGAN), which employs dual dis- criminators to analyse the boundaries of objects and contextual relationships [60]. \u03a4\u03bf", "content": "2 Methodology"}, {"title": "achieve better performance, the AS-Net authors introduced a network that blends spa- tial attention with channel attention techniques [33]. Ms RED is an attention-based multiscale feature fusion method that improved the segmentation efficiency of their method by incorporating different components into the proposed approach [18]. FAT- Net, a transformer-based encoder-decoder approach, achieved performance on the skin lesion segmentation task [84]. Some more recent methods such as GREnet [82], CFF- Net [72], and SUNet-DCP [78] also obtained competent results for segmentation of skin lesions. Most of these methods are heavy parameter methods and their perfor- mance is limited due to the learning of redundant features. Figure 2 illustrates the performance of the proposed AD-Net compared to state-of-the-art (SOTA) methods on the ISIC 2017 dataset. The comparison of the Jaccard index values among various SOTA methods, correlated with the number of trainable parameters, is presented. To observe the impact of trainable parameters on model performance, we con- ducted an evaluation using the transformer-based method [13] and the attention-based method [70]. Specifically, we assessed how the variation of the number of parameters influences the Jaccard index value, a key metric to measure segmentation quality. The results are illustrated in Figure 3. Our findings indicate that an increase in the number of trainable parameters does not necessarily result in better performance. In contrast, performance tends to decline as the number of parameters increases, likely due to redundant features in the model learning. This evaluation suggests that an excessive number of parameters can lead to overfitting, where the model captures noise rather than relevant patterns, thereby diminishing its generalization ability. Figure 3 clearly shows this trend, highlighting the inverse relationship between the parameter count", "content": "3 Methodology"}, {"title": "and the Jaccard index beyond a certain point. This comparison is crucial for opti- mising model configurations in skin lesion segmentation tasks. By carefully analyzing these outcomes, we can determine the optimal number of parameters that maximize performance without incurring the drawbacks of overfitting. This balance is essential for developing robust and efficient models that deliver high performance in clinical applications.", "content": "3.1 Overview of the proposed method"}, {"title": "We introduced a method that comprises residual dilated convolutional blocks with different dilatation rates, an attention-based spatial feature enhancement block (ASFEB), and a guided decoder. Dilated convolution is a successful approach to cap- turing contextual information without reducing spatial resolution. Dilated convolution,", "content": "3.2 Attention-based spatial feature enhancement block (ASFEB)"}, {"title": "by introducing gaps between kernel elements, effectively expands the receptive field, allowing the network to capture feature information in a wider scope [29, 85, 86]. This property is advantageous because skin lesions are of different sizes, shapes, and scales. where contextual information and lesion characteristics in different receptive fields are crucial for precise segmentation. In the proposed AD-Net, the dilated residual connections serve to alleviate the problem of vanishing gradients during training. By incorporating the ASFEB into the skip connections, the method can refine the skip connections information and enhance the lesion localization information. The guided decoder strategy facilitates the fast gradient flow and preserves important features, leading to more precise segmentation results. Through extensive experiments on diverse datasets of skin lesion images, we suggest that the proposed approach has effectiveness and improved outcomes. In summary, our contributions include the following.", "content": "3.3 Guided decoder strategy"}, {"title": "\u2022 The dilated convolutional residual blocks with varying dilation rates expand the receptive field without increasing the computational burden significantly. By using varying dilation rates, the model can capture more contextual information, which is crucial for accurately delineating the boundaries, and structures of lesions.\n\u2022 The guided decoder strategy is designed to refine the segmentation outputs progres- sively. It leverages intermediate features and guides the decoding process, ensuring that finer details are preserved and enhancing the overall segmentation performance.\n\u2022 ASFEB is employed to refine feature maps within the skip connections in several ways. It effectively combines feature maps from max and average pooling layers. This combination of features helps in generating a comprehensive representation of the incoming data. By giving attention weights to the input, the model focuses more on relevant regions, such as the lesion areas, while ignoring irrelevant background information. The residual connections within ASFEB help mitigate the vanishing gradient problem and allow for more efficient training.\n\u2022 Achieving SOTA efficiency across multiple datasets like ISIC 2018, ISIC 2017, ISIC 2016, and PH2, without relying on data augmentation, represents a signifi- cant achievement. This highlights the robustness and general applicability of the proposed AD-Net.", "content": "4 Experiments"}, {"title": "The following sections are organised as follows: Section 2 presents a comprehensive review of related work, highlighting key advancements and existing methodologies. In Section 3 the details of AD-Net are explained, including an in-depth discussion of the dilated convolutional residual network, the attention-based spatial feature enhance- ment block, and the guided decoder strategy. Section 4 outlines the experimental setup, including details on the datasets used, the evaluation metrics applied, and the specifics regarding implementation. Section 5 illustrates the experimental outcomes obtained from the test set on four publicly available datasets. Section 6 presents a com- prehensive discussion of the findings, including an examination of the results across different image resolutions, an analysis of computational efficiency, statistical analysis, and a discussion of the limitations of the proposed method. The study concludes in Section 7, providing directions for future research and summarising the predominant contributions and findings.", "content": "4.1 Details of datasets"}, {"title": "The proposed AD-Net consists of four sequentially dilated convolutional residual blocks, with a 2\u00d72 maximum pooling layer after each block. Within the encoder path, the dilated convolutional residual blocks play a crucial role in capturing fea- tures. These blocks employ a 1 \u00d7 1 dilated convolution and batch normalization on the residual path, by enabling the propagation of gradients across the model, our approach effectively mitigates the issue of vanishing gradients and effectively preserves the boundary information [53]. This facilitates the effective propagation of information and enables the model to capture relevant features. The dilated convolution operation", "content": "4.2 Evaluation criteria"}, {"title": "is represented by Eq. 1 [19, 85].", "content": "4.3 Implementation details"}, {"title": "(\\u2061Folk\\u2061)(\\u2061p\\u2061) = \\u2211 s + l t = p F ( s ) k ( t ) , (1)", "content": "4.4 Ablation study on the ISIC 2017 dataset"}, {"title": "In Eq. 1 as previously indicated, the parameter *l serves as a determinant of the dilated convolution level. Specifically, when *l is set to 1, the operation functions similar to a standard convolutional operation. However, the utilization of dilated convolution introduces a distinct advantage by enabling a larger receptive field or a global perspective preserving the resolution of the image while capturing essential details from the input. This approach proves beneficial in capturing a greater amount of contextual information regarding the objects depicted in the images. The initial dilated convolutional residual block comprises 16 feature maps, each with dimensions of 256 \u00d7 256 pixels. Within the encoder path, as we progress through each block, the number of feature channels expands, whereas the dimensions of the feature maps undergo reduction via downsampling operations. This pattern enables the model to effectively capture features and higher-level representations as it pro- gresses through the encoder path. Consequently, in the fourth block of the encoder path, there are 128 feature channels, each with dimensions of 32\u00d732 pixels, as shown in Figure 4. In the first and second encoder blocks, the dilated rate is used 1 for dilated convolution, which implies that the convolution operation is executed with- out any dilation. Consequently, the receptive field of the convolution filters remains confined to their designated size. However, the third encoder block uses a dilated con- volution with a dilation rate of 2. When this dilation rate is used, the convolution filters are enabled to contain a larger receptive field, capturing information from a broader region while preserving the output size identical to the prior blocks. In this context, a dilation rate of 2 represents a one-pixel spacing between the filter values during the application. Similarly, the fourth encoder block employs a dilation rate of 4 for dilated convolution. This dilation rate further enhances the receptive field, enabling them to capture more spatial context information. By increasing the dilation rate in later encoder blocks, the network can incorporate larger context information into the feature maps, enabling it to capture more global patterns and context while maintaining an effortless computational complexity. The encoder blocks and the decoder blocks are connected via the bottleneck layer. It is also implemented by using a dilated convolutional residual block consisting of 256 feature channels, each with a size of 16 \u00d7 16. This layer plays a critical role in squeezing and summarising the encoded information before passing it to the decoder for further processing and reconstruction. Within the residual block of dilated convolution of the bottleneck layer, the dilated convolution layers employ a dilation rate of 4. The decoder part consists of four blocks. Each block has a residual block of dilated convolution followed by a transposed convolutional layer of kernel size 2\u00d72, which is used for upsampling purposes. The transposed convolutional layer facilitates an increase in the spatial size of the feature maps, allowing the decoder to generate high- resolution representations. This is important for recovering fine-grained details during the segmentation process. This block incorporates dilated convolutions with a dilation rate similar to the encoder path, to capture contextual information and maintain spatial resolution.", "content": "5 Results"}, {"title": "In the proposed AD-Net, a skip connection connects each decoder block to its asso- ciated encoder block. These skip connections are crucial for preserving and integrating important spatial information from the encoder to the decoder. This mechanism empowers the decoder by using both low- and high-level features, a crucial aspect for achieving precise segmentation outcomes. Additionally, the proposed AD-Net employs ASFEB at each skip connection. The purpose of ASFEB is to refine the information on skip connections, ensuring that the decoder can efficiently use the information from the corresponding encoder block for precise segmentation output. In the end, a con- volutional layer 1\u00d71 is used followed by a sigmoid activation function to produce the final segmentation output. The key components of the proposed AD-Net are shown in Figure 4.", "content": "5.1 ISIC 2018 dataset comparison with benchmark models"}, {"title": "In deep learning architecture, the pooling operations have several uses, such as reduc- ing the size of the feature map, speeding up computations, and improving the resilience of the feature. For segmentation of skin lesions, capture both local details and global context is crucial, due to factors such as small size, low contrast, and the diverse colours exhibited by skin lesions. To address this, we used ASFEB in skip connections in the proposed AD-Net to enhance feature fusion, attention, spatial features informa- tion, and residual learning. These advantages collectively contribute to achieving more accurate segmentation outcomes, particularly in tasks, where capturing fine details and preserving object boundaries are crucial. Figure 5 contains an illustration of the attention-based spatial feature enhancement block. Operations performed on the input tensor include a convolutional layer 3 \u00d7 3, a batch normalisation layer (BN), and a rectified linear unit layer (ReLU). Furthermore, both max pooling and average pooling operations are performed, and their results are concatenated. This process allows the method to adeptly capture both local details and global feature information, thus enhancing its ability to provide accurate predic- tions. The combined features then go through an additional 3 \u00d7 3 convolutional layer, BN, and ReLU activation. This iterative refinement of features enhances the model's ability to summarise crucial information essential for precise segmentation. In paral- lel, another path is introduced, which incorporates global average pooling, followed by 3 x 3 convolutional layers, BN and sigmoid activation. This path generates atten- tion coefficients that weigh the outputs obtained from the parallel pooling operation. Ultimately, the weighted feature map is combined with the initial inputs, yielding the output. This mechanism helps to integrate the refined features with the original input, contributing to enhancing the segmentation performance. By employing both max pooling and average pooling operations in parallel, while using attention coeffi- cients to balance their contributions, the ASFEB architecture is built to effectively catch local and global features information. The ASFEB technique, as described in the following equations, enhances the representation of features of the given input tensor.", "content": "5.2 ISIC 2017 dataset comparison with benchmark models"}, {"title": "T = RHXWXC (2)", "content": "5.3 ISIC 2016 dataset comparison with benchmark models"}, {"title": "In Equation 2, the symbol T represents the given input, where H denotes height, W denotes width, and C signifies its depth, resulting in dimensions H \u00d7 W \u00d7 C. These dimensions define the size and depth of the input tensor.", "content": "5.4 PH2 dataset comparison with benchmark models"}, {"title": "T1 = ReLU(\u03bc(f3\u00d73(T)), (3)", "content": "6 Discussion"}, {"title": "Equation 3 denotes the output T\u2081, which is acquired by convolving the given input tensor T with a 3\u00d73 filter (f3\u00d73), then batch normalisation (\u03bc) and ReLU activation is applied.", "content": "6.1 Outcomes on different number of image resolutions"}, {"title": "F1 = (Pm(T1)), (4)", "content": "6.2 Computational Analysis"}, {"title": "F2 = (Pa(T1)), (5)", "content": "6.3 Statistical analysis"}, {"title": "Equations 4 and 5 represent F\u2081 and F2, respectively, which are obtained by apply- ing max pooling operation (Pm) and average pooling (Pa) operation with a stride of 3 \u00d7 3 on the given input T\u2081.", "content": "6.4 Limitation"}, {"title": "F3 = F1F2, (6)", "content": "7 Conclusion and Future Work"}, {"title": "In Equation 6, F3 is derived by concatenating the max pooling (F\u2081) and average pooling (F2) features.", "content": "7.1 Conclusion"}, {"title": "F4 = ReLU(\u03bc(f3\u00d73(F3)), (7)", "content": "7.2 Future Work"}, {"title": "Equation 7 shows F4, obtained by applying a convolution operation (f3\u00d73), sub- sequently batch normalisation (\u03bc) and ReLU activation (ReLU) on the combined features F3.", "content": "Conflict of Interest"}, {"title": "F5 = GAP(T), (8)", "content": "Data Availability"}, {"title": "In Equation 8, F5 is derived by performing a global average pooling operation (GAP) on the given input T.", "content": "Funding"}, {"title": "F6 = \u03c3(\u03bc(f3\u00d73(F5))), (9)", "content": "Ethics approval"}, {"title": "Equation 9 shows F6, obtained by applying a convolution operation (f3\u00d73), subsequently (\u03bc) and sigmoid activation (\u03c3) in F5.", "content": "References"}, {"title": "F8F4 F6, (10)", "content": ""}, {"title": "F8 =F4 \u2297 F6, (10)", "content": ""}, {"title": "F = F8T, (11)", "content": ""}, {"title": "Ultimately, the attention features F are acquired by element-wise summation (+) of Fs and the given input T, as delineated in Equation 11.", "content": ""}, {"title": "L.Jaccard(y, y) = 1 \u2212 \u03a3 (Yi Yi) \u03a3 (Yi + Yi \u2013 Yi Yi) (12)", "content": ""}, {"title": "Here, y represents the corresponding ground truth, and \u0177 represents the predicted output. To enable a direct comparison with the ground truth, up-sampling is per- formed on the features of each decoder block to align with the input size of 256 \u00d7 256.", "content": ""}, {"title": "LBCE(Y, \u0177) = \u2211(yilogyi + (1 - Yi)log(1 \u2013 \u0177i)). (13)", "content": ""}, {"title": "L.Jaccard(Y, \u0177) = 1 - 2\u03a3 (Yi Yi) \u03a3+ \u03a3 (14)", "content": ""}, {"title": "LDICE(Y, \u0177) = 1 - \u03a3 (Yi Yi) \u03a3 (Yi Yi) + \u03b1 \u03a3 (yi\u00b7 (1 \u2212 \u0177i)) + \u03b2\u2211((1 \u2212 Yi)\u00b7 \u0177i)\u1fbd (15)", "content": ""}, {"title": "Where y denotes the ground truth, \u0177 represents the predicted output, N is the count of samples, a and \u03b2 are hyperparameters. To convert the Tversky index into a loss function, the complement of the Tversky index can be minimized. The FTL is defined as:", "content": ""}, {"title": "LFTL = \u2211(1 \u2013 TIC), (16)", "content": ""}, {"title": "where y is a hyper-parameter that can be adjusted in the range [1-3]. By incor- porating the focal mechanism, the FTL is designed to assign more weight to false negatives and false positives, which can significantly improve a model's performance, particularly on imbalanced datasets.", "content": ""}, {"title": "L1 = LBCE + LFTL, (17)", "content": ""}, {"title": "The combined loss function for option 1 is defined as:", "content": ""}, {"title": "L2 = LBCE + LDICE, (18)", "content": ""}, {"title": "For Option 2, the loss function is explained as:", "content": ""}, {"title": "L3 = LJaccard, (19)", "content": ""}, {"title": "For the guided decoder, we employed the Jaccard loss after each decoder block. The loss function is expressed as:", "content": ""}, {"title": "LA = C1 + C3, (20)", "content": ""}, {"title": "The overall loss functions (LF) to train the model are defined in Eq. 20 and Eq. 21.", "content": ""}, {"title": "LB = C2 + L3, (21)", "content": ""}, {"title": "Accuracy = TP + TN TP+TN+FP + FN (22)", "content": ""}, {"title": "Sensitivity = TP TP + FN (23)", "content": ""}, {"title": "IOU = TP Tp + FP + FN (24)", "content": ""}, {"title": "Dice/F1 = 2* Tp + Fp + FN 2 * TP (25)", "content": ""}, {"title": "Specificity = TN TN + FP (26)", "content": ""}, {"title": "As described in [28], the performance evaluation of the proposed technique uses five key assessment metrics: accuracy, sensitivity, Jaccard index, dice coefficient, and speci- ficity. The skin lesion segmentation evaluation criteria were selected based on the guidance provided by the ISIC competition leaderboard, a well-known platform. Using these assessment measures, a comprehensive analysis of the performance of the model can be conducted. These measures provide valuable information on various aspects of the segmentation results. The choice to utilize these specific criteria, with a particular emphasis on the Jaccard index (IOU) as the primary metric, aligns with the guidance provided by the ISIC challenge leaderboard, as in [16].", "content": ""}, {"title": "The implementation specifics of the proposed AD-Net are detailed in this section, uti- lizing several widely used benchmark datasets, including PH2, ISIC 2016, ISIC 2017, and ISIC 2018. These datasets are widely recognized as standard benchmarks in the field, enabling the evaluation of our method's performance and its ability to gener- alize across diverse datasets. In our experimental configuration, we standardized the dimensions of all datasets to 256 \u00d7 256 pixels. To train AD-Net, a total of twenty per- cent of the training data was set aside for validation. We utilized the Adam optimizer [58] for training, employing mixed loss functions to enhance the model performance. If the validation set performance does not improve, the Adam optimizer's learning rate decreases by a factor of 0.25 after four epochs, starting at 0.001. We also used the early stop strategy to handle the overfitting issue and to calculate the maximum number of training epochs dynamically. We employed a batch size of 10 for the ISIC 2016 dataset, 8 for the ISIC 2017 dataset, and 8 for the ISIC 2018 dataset. However, AD-Net achieved SOTA performance without requiring additional data. The AD-Net is implemented in Keras and TensorFlow as the back end. All variants of the model are trained on the NVIDIA K80 GPU with the following specifications: An Intel Xeon CPU running at 2.20 GHz, 13 GB of RAM, and a Tesla K80 accelerator with 12 GB of GDDR5 VRAM make up the GPU runtime environment.", "content": ""}, {"title": "We conducted an ablation investigation utilizing various components to evaluate AD- Net's capabilities. Various experiments were performed, including those with single and combined loss functions, using the baseline method. The trainable parameters in millions were also taken into account during the analysis. These experiments aimed to evaluate the contribution and significance of each component in achieving SOTA results. We introduced ASFEB at skip connections and a guided decoder strategy in the proposed method for improved performance. Table 4 shows the results of the ablation study for the ISIC 2017 dataset for each component. Furthermore, Figure 8 illustrates the heat maps for all elements of AD-Net. Observing performance, we used the loss LA to train the model on all data sets. More ablation investigations were conducted to determine the appropriate param- eter values for the segmentation of the skin lesions. Table 5 presents the results, which provide insight into the performance of the proposed method in different combina- tions of parameters. Furthermore, Figure 3 visually shows the connection between the Jaccard index and the count of trainable parameters, highlighting the influence of parameter choices on segmentation performance. These insights are valuable in improving and fine-tuning the proposed method to achieve precise and efficient skin lesion segmentation outcomes. Furthermore, a comparison of two optimizers was con- ducted. The outcomes are displayed in Table 6, indicating that the Adam optimizer performs better compared to stochastic gradient descent (SGD). This implies that the Adam optimizer is better suited for optimizing AD-Net and can enhance segmentation performance.", "content": ""}, {"title": "Table 7 presents a comparison of the proposed AD-Net with various dilation con- volution filter sizes. The results indicate that with 2.92 million trainable parameters, the 3\u00d73 filter size yields the best performance. The choice of filter size in dilated convolutions significantly influences the receptive field, the resolution of the features, the computational efficiency, and the overall performance of the proposed method [29, 85, 86]. By conducting empirical evaluations, we choose the 3 \u00d7 3 filter size for AD-Net.", "content": ""}, {"title": "Based on the evaluation using the ISIC 2018 dataset, our AD-Net was compared with 14 SOTA methods, including LeaNet, CPFNet, DAGAN, FAT-Net, CFF-Net, AS-Net, U-Net 1, SLSN, MS RED, Unet++ 2, ADF Net, Swin-Unet 3, RMMLP and ARU- GD 4. The results presented in Table 8 show that our method achieved improvements of 11.48%, 9.40%, 7.72%, 6.55%, 5.86%, 5.18%, 4.46%, 4.37%, 4.21%, 3.02%, 2.86%, 2.44%, 2.33%, and 1.65% in terms of the Jaccard index compared to these SOTA methods. In addition, a detailed comparison was performed with other widely recognised methods, namely U-Net, Unet++, Swin-Unet and ARU-GD, training them, and the results are summarised in Table 8. Each assessment metric shown in Table 8 consists of the mean and standard deviation of the test data set for the U-Net, Unet ++, Swin-Unet and ARU-GD models. The other methods in Table 8 are the articles cited in the skin lesion segmentation domain. These comparisons provide a comprehensive analysis of the efficiency and performance gains achieved by our AD-Net. In addition, the visual results show different challenges in skin lesion segmen- tation, such as irregular shapes, low contrast, the presence of artefacts, and small lesions. Figure 9 presents these visual results, highlighting the robustness of AD-Net in handling various sizes and irregular shapes of skin lesions, thus demonstrating state-of-the-art performance on unseen test data.", "content": ""}, {"title": "The comparison of AD-Net with 14 SOTA methods on the ISIC 2017 dataset includes U-Net, DAGAN, SUNet-DCP, FAT-Net, TMAHU Net, RMMLP, MS RED, UNet++, ADF Net, LeaNet, AS-Net, ARU-GD, Swin-Unet and CFF-Net. Table 9 compares the results using these SOTA methods. Our AD-Net achieved improvements of 11.65%, 11.29%, 11.20%, 10.43%, 9.60%, 7.89%, 7.58%, 7.55%, 7.08%, 7.07%, 4.97%, 4.63%, 4.60%, and 4.24% in terms of Jaccard index compared to these SOTA methods. Addi- tionally, in Table 9, we contrast the performance of AD-Net with other prominent methods, including U-Net, Unet++, Swin-Unet, and ARU-GD. We conducted exten- sive training and analysis of these methods to thoroughly assess the efficiency of AD-Net. Each assessment metric shown in Table 8 consists of the mean and standard deviation of the test data set for the U-Net, Unet ++, Swin-Unet, and ARU-GD mod- els. The other methods in Table 9 are the articles cited in the skin lesion segmentation domain. In addition, visual results were obtained that show various challenges in skin lesion segmentation, such as irregular shapes, hair, and the presence of artifacts. Figure 10 presents these visual results, illustrating AD-Net's superior performance on unseen test data.", "content": ""}, {"title": "The proposed AD-Net was compared with eleven SOTA methods on the ISIC 2016 dataset, including U-Net, UNet++, DAGAN, ARU-GD, FAT-Net, CFF-Net, Swin- Unet, H2Former, Ms RED, ADF Net, and TMAHU Net. Table 10 compares the results with these methods. Our method demonstrated improvements of 10.48%, 8.57%, 6.50%, 5.63%, 5.40%, 4.90%, 4.83%, 4.00%, 3.31%, 1.73%, and 0.82% in terms of the Jaccard index compared to these cutting-edge approaches. olor In Table 10, we contrast the performance of AD-Net with the cited papers in skin lesion segmenta- tion and with key SOTA methods such as U-Net, Unet++, Swin-Unet and ARU-GD. Comprehensive training and analysis of U-Net, Unet++, Swin-Unet, and ARU-GD were performed to provide a thorough comparison of efficiency against AD-Net. Each assessment metric shown in Table 8 consists of the mean and standard deviation of the test data set for the U-Net, Unet++, Swin-Unet, and ARU-GD models. In addition, visual results were"}]}