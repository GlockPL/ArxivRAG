{"title": "Combining Planning and Diffusion for Mobility with Unknown Dynamics", "authors": ["Yajvan M Ravan", "Zhutian Yang", "Tao Chen", "Tom\u00e1s Lozano-P\u00e9rez", "Leslie Pack Kaelbling"], "abstract": "Abstract-Manipulation of large objects over long horizons\n(such as carts in a warehouse) is an essential skill for deployable\nrobotic systems. Large objects require mobile manipulation\nwhich involves simultaneous manipulation, navigation, and\nmovement with the object in tow. In many real-world situations,\nobject dynamics are incredibly complex, such as the interaction\nof an office chair (with a rotating base and five caster wheels)\nand the ground. We present a hierarchical algorithm for long-\nhorizon robot manipulation problems in which the dynamics are\npartially unknown. We observe that diffusion-based behavior\ncloning is highly effective for short-horizon problems with\nunknown dynamics, so we decompose the problem into an ab-\nstract high-level, obstacle-aware motion-planning problem that\nproduces a waypoint sequence. We use a short-horizon, relative-\nmotion diffusion policy to achieve the waypoints in sequence.\nWe train mobile manipulation policies on a Spot robot that has\nto push and pull an office chair. Our hierarchical manipulation\npolicy performs consistently better, especially when the horizon\nincreases, compared to a diffusion policy trained on long-\nhorizon demonstrations or motion planning assuming a rigidly-\nattached object (success rate of 8 (versus 0 and 5 respectively)\nout of 10 runs). Importantly, our learned policy generalizes to\nnew layouts, grasps, chairs, and flooring that induces more\nfriction, without any further training, showing promise for\nother complex mobile manipulation problems.", "sections": [{"title": "I. INTRODUCTION", "content": "Many robot tasks involve finding and following a path\nwhile interacting with an environment whose dynamics are\nnot known. For example, a robot arm pushing an object\namong obstacles on a table or a mobile robot pushing an\noffice chair among furniture are both facing this type of\nproblem. In this paper, we explore in detail the problem of\nrearranging large objects (comparable to robot size) through\npushing and pulling.\nWe focus in detail on the problem of having a Boston\nDynamics Spot pull a 5-wheeled office chair among other\nfurniture (see fig. 1). This is a challenging instance of finding\nand following a path subject to unknown dynamics, as the\nsurface of the floor may be variable and may have variable\nfriction. Note that the effect of pushing or pulling on the\nchair depends on the (unobservable) orientations of the 5\ncasters on the legs. Also, the robot is holding the top of\nthe chair, which can rotate and incline. The most common\nfailure modes are the robot losing its grasp when making\nsharp turns around obstacles, which involve substantial re-\norientation of the casters, or the chair colliding with/getting\nstuck on another piece of furniture.\nWe seek an approach that (a) allows the robot to be trained\nquickly in the real world, without access to a simulator as\nthe complex dynamics present a large sim-to-real gap, and\n(b) generalizes to somewhat different environments, e.g. a\ndifferent room with a carpet or a different chair. We make two\nsimplifying assumptions. First is that a map of the obstacles\nis available. This can be easily obtained with a simple\nRGB-D scan of the room or existing SLAM algorithms.\nSecond is that the environment dynamics remains roughly\nunchanging along the paths to be followed. Our approach\nis hierarchical: we learn a \"local\" motion control policy\nvia imitation learning and use a \"global\" motion planner\nto define waypoints for the local policy. Lastly, we assume\nthat the details of the local policy will not affect the choice\nof waypoints. We call our approach Planner-Ordered Policy,\nPoPi for short.\nAt the low-level, we learn a short-horizon, diffusion-based\nmanipulation policy that is conditioned on pose estimates of\nthe chair and predicts desired robot motions for reaching a\nwaypoint. We chose this approach for its ability to efficiently\nlearn policies from relatively few suboptimal demonstrations."}, {"title": "II. RELATED WORK", "content": "The class of problems of interest in this paper (manipulat-\ning objects with unknown dynamics) have been investigated\nusing a wide variety of methods, which we categorize as\nfollows: (1) using a simple a priori model of the dynamics\nand applying feedback control; (2) motion planning from\nan approximate or learned model; (3) learning a policy\nvia reinforcement learning; and (4) learning a policy via\nimitation learning. A review of the general area of pushing\nmanipulation is available [1]. Below, we highlight some of\nthe most relevant work.\nFeedback Heins et al. [2] present an example of a mobile\nmanipulation system using motion planning (differential IK\ncontroller) for pushing objects and obstacle avoidance in\nunknown environments. It leverages a simple model of push-\ning and very fast kinematic control. This approach would\nbe applicable to our problem, but it requires substantial\nengineering effort to apply it to a new situation.\nMotion planning There are quite a few motion planning\napproaches that exploit analytic or learned models of the\ndynamics. The closest to our work is Zito et al. [3], who\ndevelop a two-level RRT-based push planner which, like\nour method, uses a high-level planner to generate sub-goals\nfor a lower-level planner. However, their lower level uses\na pushing simulator to implement a kinodynamic RRT to\nreach the sub-goals. In general, the motion planning methods\nrequire a reasonably accurate model of the dynamics. The\ndynamics of our problem are quite complex and difficult to\nmodel, particularly since we cannot observe the state of the\nwheels.\nReinforcement learning The use of hierarchical policies\nfor mobile manipulation is popular. Many prior works use\nreinforcement learning to acquire low-level policies for prim-\nitive skills, tracking end-effector pose, or tracking whole\nbody velocities and combine these with high-level policies\nthat output trajectories of the aforementioned primitives [4],\n[5], [6], [7], [8], [9]. Chappellet et al. [10] use 3D visual\ntracking along with SLAM to execute primitive actions for\nmanipulating large objects such as wheelbarrows and bobbins\ninstead, while Tang et al. [11] use nonlinear model-predictive\ncontrol to achieve stable pushing, without grasping, for non-\nholonomic robots. Several others [4], [12], [7] use behavior\ncloning using human demonstrations on cheap hardware or\nexpert demonstrations from simulation. [6], [13], [8] use\ntask planning [14] to chain together primitives for long-\nhorizon tasks, particularly household object rearrangement.\nYokoyama et al. [15] remove the need for a map, required\nby TAMP, by using expert coordination and skill correction\npolicies. Xia et al. [16] use motion planning for low-level\nmovement and abstract the action space for reinforcement\nlearning to end-effector space rather than joint space to\nachieve long-horizon goals. Most of these works assume\nsimple/known object dynamics, with the notable exception\nof [11]. However, in all cases, these methods require much\nmore extensive experience than our approach.\nImitation Learning Imitation learning has been applied\nextensively to tabletop manipulation. Chi et al. [17] use\na denoising diffusion inference model (DDIM) [18] based\npolicy. Their technique stacks a history of observations and\nproduces a sequence of actions, allowing for multimodality\nand temporal continuity. Their technique learns end-to-end\nmanipulation. Zhou et al. [19] augment imitation learning\nwith score-based online replanning to tackle stochastic and\nlong-horizon tasks. Reuss et al. [20] show the use of imi-\ntation learning to learn goal-conditioned policies from large\ndatasets, while Shen et al. [21] use a non-diffusion based pol-\nicy to achieve object category-level generalization. Our work\nleverages imitation learning to learn a local controller from\nrelatively few demonstrations and couples this controller to\na global planner to enable zero-shot transfer to new settings."}, {"title": "III. PROBLEM FORMULATION", "content": "We focus on moving an attached object with difficult-\nto-characterize dynamics to a specified location that is far\n(multiple meters) from its initial location. In particular,\nwe concentrate on the robot-chair dynamics and chair-floor\ndynamics in the pulling action alone, assuming that the robot\nstarts with a secure grasp of the object.\nInputs\n\u2022 M: A known map of a room-size environment, with\nindicated obstacle regions, possibly gathered via one or\nmore scans of the room with an RGB-D camera.\n\u2022 x: The robot pose within the map, assumed to be\navailable at all times. This can be achieved through\nonline localization. We assume that the robot pose is\nknown accurately (within 5 cm)."}, {"title": "IV. METHODS", "content": "Diffusion-based behavior cloning is very good at repro-\nducing behavior from human demonstrations. However, it\ntends to fail when the horizon is long or if out-of-distribution\nscenarios are encountered, and addressing both requires\ndrastically scaling data. On the other hand, motion planning\nis good at long-horizon tasks, but struggles with planning\nover contact-rich tasks due to complex dynamics. To achieve\ngeneralization and robust, long-horizon reliability for mobile\nmanipulation of unknown objects in a data-efficient manner,\nwe propose Planner-Ordered Policy (PoPi), combining a\nhigh-level motion planner to generate a sequence of way-\npoints with a low-level short-horizon diffusion policy to\ncomplete motion between waypoints.\nA. Planner-Ordered Policy\nWe use motion planning to provide a series of intermediate\ngoals that is tasked with reaching. We assume the simple\nheuristic of holonomic dynamics for the robot and object and\nthat the object's pose relative to the robot remains fixed. We\nfirst collect the environment point-cloud scan M and build\nan offline Roadmap R of object poses (see Section V-C).\nGiven an initial x and goal pose the object xg, we ran A*\nalgorithm [23] on R to generate a long-horizon trajectory\n\u03c4 = {(xk,xk)}k=1 free of obstacle collisions. This long-\nhorizon trajectory is downsampled by factor f to generate\na series of intermediate goals g = {(xkf,xhf)/} for the\ndiffusion policy.\nWe keep a running sequence of robot and object poses in\nthe global frame, and the short-horizon policy is tasked\nwith relative movements towards the next intermediate goal,\nuntil it is sufficiently close (tested via REACHED). We use\nreceding horizon control, with a history length of ho, a\nprediction horizon of ha, and an execution horizon of he.\nPseudocode is depicted in algorithm 1 and the system is\nshown in 2. The function TRANFORM(poses, goal) returns\nthe list of poses in the first argument expressed relative to\nthe goal pose in its second argument. The function STUCK\ndetects if the robot fails to move for a pre-determined time\nperiod."}, {"title": "B. Short-Horizon Diffusion Policy", "content": "We use a diffusion model similar to [17] for con-\nditional short-horizon action generation. The policy \u03c0\u03b8\ntakes in a sequence of waypoint-relative object poses\no  = TRANSFORM({xkt}thot1,g) and waypoint-relative\nrobot poses r = TRANSFORM({xk}thot1,g), then\noutputs a series of waypoint-relative actions at=\nTRANSFORM({xkt}+ha,g), where actions are simply robot\nposes, ho and ha are history length and action horizons\nrespectively, and g is the waypoint.\nThe policy uses a conditional denoising network \u03f5\u03b8 to it-\neratively convert random Gaussian noise a0  into actions\naccording to the equation\nak1=\u03b1k(\u03b1tak\u03b2k\u03f5\u03b8(ak,ot,rt,k))+\u03c3k\u2217N(0,I),\nwhere ak is the denoised action sequence. We use standard\nnoise schedule and hyperparameters \u03b1k,\u03b2k, and \u03c3k [18].\nTo construct our training objective, we take a demonstra-\ntion trajectory \u03c4. For a given point (xt, xq) in the trajectory,"}, {"title": "V. EXPERIMENTS", "content": "We want to answer two questions about our method PoPi:\n1) Can it achieve a higher long-horizon success rate in\nthe training environment compared to baselines?\n2) Can it generalize to environments, objects, and grasp\nposes that are different from those in training?\nA. Task and Metrics\nThe task requires manipulating a five-wheeled office chair\ninto a goal pose in the presence of obstacles. These chairs\nhave many internal degrees of freedom. Notably, the wheels\non the legs rotate passively, and the friction between the\nwheels and the ground is difficult to accurately model and\nsimulate, especially on carpet. The training environment and\nrobot are depicted in fig. 1 and a birds-eye view is shown\nin fig. 4. We focus on the movement only, assuming that\nthe grasping of the chair has already been done and that the\ngrasping point is near the center top on the back of the chair.\nGiven that the policies are trained on trajectories whose\nlength range from 8 m to 18 m, we test goals sampled from\n2 m, 6 m, and 10 m away. Correspondingly, it takes 1, 2,"}, {"title": "D. Baselines and Ablations", "content": "We consider two baselines: pure diffusion policies and\npure motion planning.\nPure Diffusion We use the same demonstrations D to train\na long-horizon diffusion policy in the global frame, similar\nto [24]. In algorithm 2, we simply remove lines 3-4 and 8-9\nand fix t = T, i.e. the goal is fixed to the final position in\nthe trajectory, and there is no restriction on the preceding\naction/observation sequences. However, we note that com-\npared to the method in that paper, we have significantly fewer\ndemonstrations. At evaluation time, there is no planner and\nwe constrain trajectories with the same methods from [24],\ni.e. preventing them from passing through obstacles. Thus,\nthe mapping information is explicitly used by the policy,\nin addition to implicit information about obstacles in the\ntraining data. We apply the same receding horizon control as\na comparison. As an additional diffusion baseline, we also\ncompare our \"local\" short-horizon diffusion policy with no\nmotion planning.\nPure planning As a second baseline, we apply both\nshortest path search using A* in the roadmap and an online\nRRT to navigate between intermediate waypoints of the\nglobal trajectory. We simply replace line 11 in algorithm 1\n(where we call \u03c0) with a call to the respective planner.\nFurthermore, the planners do not use a history, only the\ncurrent pose. The online RRT method takes into account the\ncurrent chair-pose relative to the robot, which may change\nover time, while shortest path search does not. However, both\nassume that straight line, holonomic movements are possible\nand ignore the dynamics of the chair."}, {"title": "E. Training", "content": "We collected 35 demonstrations in the environment shown\nin the top of fig. 1 starting and ending at various places to\ncover all movement within the environment. fig. 4 shows the\ntrajectories used to train both PoPi and a global diffusion\npolicy. The global diffusion policy is trained on whole trajec-\ntories and thus has 35 examples. The diffusion submodule of\nPoPi is trained on relative snippets, which allows substantial\ndata reuse giving 36,000 examples."}, {"title": "F. Long-horizon performance", "content": "We begin by studying these methods in long-horizon tasks\nwith the same conditions (i.e. floor, grasp, chair) as training.\nWe report the success rate, where an evaluation is deemed\nsuccessful if the chair reached within 30 cm of the target\nposition. We tested each method in the training environment\nwith trajectories of varying horizon and curvature with goals\nat 2 m, 6 m, and 10 m distance requiring 1, 2, and 3 turns to\nget around obstacles. For testing, we placed an extra obstacle\nin the center blocking off the narrow passageway so that the\nrobot must take the longer 10 m route with more turns to\nreach its goal. At 2 m, there is minimal obstacle interaction,\nwhile at 10 m, the robot must avoid obstacles for almost\nhalf of the trajectory. The 10 m testing trajectory is depicted\nin fig. 5 in the same environment as fig. 3 along with an\nexample execution (using PoPi). Results are shown in table I.\nWe find that as the horizon increases, the performance of\neach of the methods decreases. The primary failure modes\nare (a) losing grip of the chair due to difficult dynamics and\n(b) collisions with obstacles that prevent chair movement. At\nthe longest horizon, PoPi performs the best, achieving 80%\nsuccess compared to 50% for the next best baseline (RRT).\nThe baseline using A* only achieves 30% success at the\nshort-horizon, and 20% at medium and long-horizon, with\nthe remaining trials failing by losing grasp very quickly. RRT\ndoes better, presumably because it incorporates the current\nrelative pose of the chair in the robot frame, which may be\ndifferent than the rigid pose assumed by A*. However, it\nstill fails to achieve long-horizon robustness, as the simple\ndynamics model leads to failure half of the time.\nWe find that the global diffusion baseline is unable to\nachieve any success. The trajectories generated are sensible,\nhowever, failure by lost grasp occurs almost immediately.\n[24] shows that with substantial amounts of demonstrations\nin constrained distributions, this method generates good\ntrajectories. However, the global diffusion policy is unable\nto learn the dynamics that enable long-horizon goals in\nour more challenging setting, presumably because of lim-ited demonstration data. The pure short-horizon diffusion\nbaseline works very well at 2 m (80% success). As the\ntraining method from section IV-B takes many short snippets\nper demonstration, the effective amount of training data\nis much higher (36,000 snippets), and this leads to robust\nperformance. However, without the motion planning to avoid\nobstacles, it is unable to perform well beyond short horizons."}, {"title": "G. Planner-Ordered Policy Generalizes Better", "content": "We chose a separate testing environment to evaluate the\ngeneralization of our methods to different obstacle config-\nurations. The chosen environment has additional dynamics\ndue to high friction from the carpeted floor that were unseen\nin the training environment. The environment and testing\ntrajectory is depicted in the bottom of fig. 1. To compare\nacross environments, we choose a trajectory with 10 m\ndisplacement and 2 turns in both environments.\nTo test generalizability across objects, we tested manipu-\nlation using a different chair and also varied the initial grasp\npose to test robustness to the obstacle's initial position. Both\nvariations are depicted in fig. 6. Results are shown in table II.\nWe did not attempt the global diffusion baseline in the\nnew environment. The map of the training environment is\nimplicitly encoded in the training distribution, and therefore,\nwe do not expect it to perform with any success.\nWe find that the other methods retain some performance\nas the obstacles change, reflecting that motion planning is\nrobust to changes in obstacle arrangement. We find that\nPoPi generalizes much better than the pure motion planning\nbaselines across environments, with success rate staying\nhigh (70%) even in the new environment. As we change\nto the unseen grasp in the training environment with the\ntraining chair, PoPi maintains 50% success rate, while the\nbaselines achieve only 30%. Interestingly, we note that all\nof the methods, when using the unseen grasp, perform\nbetter in the new environment than the training environment.\nWe conjecture that in the new environment, the carpet's\nadditional friction reduces acceleration of the chair, which\nis a major cause of lost grasps. We note that for the unseen\nchair + training grasp, all methods fail by losing grasp, with\nrare success. The design of the unseen chair made grasping\nthe center more unstable. The unseen grasp, although it did\nnot help in the training environment (with lower friction floor\nand higher acceleration) is much more robust in the new\nenvironment (with higher friction and lower acceleration).\nWe see that PoPi achieves 50% success with the unseen grasp\n+ unseen chair in the new environment compared to 10% and\n30% by the motion planning baselines. Furthermore, PoPi\nconsistently outperforms the baselines in all eight scenarios."}, {"title": "VI. CONCLUSION", "content": "In this paper we describe Planner-Ordered Policy, a hi-\nerarchical algorithm for long-horizon robot manipulation\nproblems where world dynamics are partially unknown. We\nfind that PoPi performs consistently better as the horizon\nincreases, compared to a \"global\u201d diffusion policy or motion\nplanning assuming a rigidly-attached object. Importantly,\nPoPi generalizes to new layouts, grasps, chairs, and even\nflooring, without any further training.\nOne obvious limitation of PoPi is the inability to recover\nfrom complete failure, so incorporating both manipulation"}]}