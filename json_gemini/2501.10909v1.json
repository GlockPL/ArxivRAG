{"title": "Fine-Grained Appropriate Reliance: Human-AI Collaboration with a Multi-Step Transparent Decision Workflow for Complex Task Decomposition", "authors": ["GAOLE HE", "PATRICK HEMMER", "MICHAEL V\u00d6SSING", "MAX SCHEMMER", "UJWAL GADIRAJU"], "abstract": "In recent years, the rapid development of AI systems has brought about the benefits of intelligent services but also concerns about security and reliability. By fostering appropriate user reliance on an AI system, both complementary team performance and reduced human workload can be achieved. Previous empirical studies have extensively analyzed the impact of factors ranging from task, system, and human behavior on user trust and appropriate reliance in the context of one-step decision making. However, user reliance on AI systems in tasks with complex semantics that require multi-step workflows remains under-explored. Inspired by recent work on task decomposition with large language models, we propose to investigate the impact of a novel Multi-Step Transparent (MST) decision workflow on user reliance behaviors. We conducted an empirical study (N = 233) of AI-assisted decision making in composite fact-checking tasks (i.e., fact-checking tasks that entail multiple sub-fact verification steps). Our findings demonstrate that human-AI collaboration with an MST decision workflow can outperform one-step collaboration in specific contexts (e.g., when advice from an Al system is misleading). Further analysis of the appropriate reliance at fine-grained levels indicates that an MST decision workflow can be effective when users demonstrate a relatively high consideration of the intermediate steps. Our work highlights that there is no one-size-fits-all decision workflow that can help obtain optimal human-AI collaboration. Our insights help deepen the understanding of the role of decision workflows in facilitating appropriate reliance. We synthesize important implications for designing effective means to facilitate appropriate reliance on AI systems in composite tasks, positioning opportunities for the human-centered AI and broader HCI communities.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rapid development of artificial intelligence (AI) in recent years, there is a growing recognition of the promising value of AI assistance [6, 14]. AI systems have been used to answer knowledge-intensive questions [59], provide recommendations in e-commerce platforms [113], and even help make critical decisions [96]. While AI systems promise high effectiveness and use across domains, there is no guarantee of their correctness [57, 63]. Thus, accountability and verifiability become a major concern before adopting such systems into existing workflows. To address these concerns, researchers and practitioners are actively exploring the potential of human-AI collaboration [27, 42, 57, 94]. However, human-AI collaboration is not always effective, and there is a growing body of evidence suggesting that in many contexts human-AI team performance is inferior to Al performance alone [9, 27]. To address such issues and ensure complementary team performance (i.e., where the team performance can exceed the individual performance of both team members), users should accept advice from the AI system when it is correct and be able to override it when AI advice is incorrect. Such reliance patterns are denoted as appropriate reliance [93], which has become a focal research topic at the intersection of AI and human-computer interaction. In this context, existing work has explored how user trust and reliance are shaped by different aspects surrounding task characteristics [91], AI systems [82], and user factors [57]. However, most of the research focuses on decision making or data annotation tasks which can be solved in a so-called one-step manner [91]. In such a setting, a decision making task can be solved without requiring any intermediate steps. Herein human-AI collaboration allows humans to contrast their individual decisions against that of the AI, often enriched by further information, e.g., its confidence or explanations on how a decision was derived [57]. The ultimate goal here, is to enable humans to (hopefully) derive correct final decisions, leading to optimal team performance. In contrast to the one-step decision making setting, human-AI collaboration in complex multi-step decision making situations that require a composite semantic understanding and a multi-step workflow (e.g., composite fact checking [5]) is still under-explored.\nIn this work, we address this research gap by investigating the potential benefits and pitfalls of asking decision makers to follow the same multi-step workflow as that of an advisory AI system (i.e., completing a sequence of decomposed sub-tasks) along with fine-grained transparency of AI systems. We consider the context of composite fact-checking due to its growing relevance in the age of LLMs, allowing us to simultaneously draw insights in a timely real-world task. The benefits of such a setup are two-fold. Firstly, such a workflow-based decision process enables us to analyze multi-step user decision making, where user decisions at the intermediate steps affect their final decision and reliance on the AI system. The key idea here is that following the same workflow as the Al system can provide global transparency an overview of the process of the AI system (i.e., task decomposition) \u2013 which allows users to check and verify intermediate steps of the AI system and better inform their reliance on AI advice. Secondly, each intermediate step can be viewed as a sub-task. Compared to global transparency, the sub-task information (i.e., local decision criteria and evidence) entails local transparency (i.e., at the level of a specific sub-task) of the intermediate decisions of the AI system. User reliance on the sub-task information (which is also input to the AI system) provides a fine-grained view to analyze appropriate reliance on the AI system. With the fine-grained transparency by design [31, 72], we denote such a multi-step workflow in our study as multi-step transparent (MST) decision workflow. In this spirit, a multi-step transparent (MST) decision workflow can potentially facilitate appropriate reliance on the AI system and help advance our understanding of fine-grained user reliance.\nAlthough appropriate reliance has been extensively studied in relatively simple tasks [57], it is still unclear how user reliance is shaped by a multi-step decision workflow to solve complex tasks. When intermediate steps are adopted to solve complex tasks, users of the AI system may have more decisions to make sequentially. For example, to verify the claim \u201cGeneral Agreement on Trade in Services is a treaty created to extend the multilateral trading system to service sector and all members of the WTO are parties to the GATS.\u201d, workers need to verify three sub-facts: (1) General Agreement on Trade in Services is a treaty; (2) General Agreement on Trade in Services is created to extend the multilateral trading system to service sector; (3) All members of the WTO are parties to the GATS. In such a multi-step decision workflow, accurate decisions at the intermediate steps can be important. The intermediate steps and intermediate answers generated by the Al system can provide global transparency overall logic of the AI system (i.e., complex fact-checking with task decomposition and answer aggregation). At the same time, the retrieved evidence at each intermediate step enables users to verify the intermediate answers generated by the AI system. In this way, it can increase the transparency of the AI system's intermediate decisions through verifiability [32], which is denoted as local transparency. In this context, we propose to explore appropriate reliance on AI systems at the fine-grained level of intermediate steps and the level of task input of each step. In this paper, we address the following research questions:\n\u2022 RQ1: How does a multi-step decision workflow shape user reliance on the AI system?\n\u2022 RQ2: How do global transparency and local transparency shape user reliance in a multi-step decision workflow?\nTo this end, we conducted an empirical study (N = 233) in a composite fact-checking task (i.e., identifying the factual accuracy of claims based on supporting documents). On the one hand, our findings provide empirical evidence that fine-grained appropriate reliance positively contributes to appropriate reliance at the level of overall task. With an MST decision workflow, users developed a fine-grained appropriate reliance on the intermediate steps, which enabled them to detect misleading AI advice. On the other hand, we found that an MST workflow does not improve human-AI team performance and appropriate reliance on AI advice in comparison to a one-step decision workflow. In contrast to facilitating appropriate reliance globally, the MST decision workflow was effective only in a relatively challenging context, where AI advice is misleading. To encourage more precise intermediate decisions, we asked participants to reflect on the usefulness of supporting documents, which nudge users to carefully work on sub-tasks based on local transparency. We found such an intervention to increase user consideration in the intermediate steps brought about worse team performance and reliance patterns. Combined with the cognitive load feedback across experimental conditions, we infer that such an intervention imposes a high cognitive load on users, limiting its expected impact. However, we found that the MST workflow can help users develop a critical mindset when making final decisions. This can partially explain why participants using an MST workflow showed decreased reliance on the AI system and their confidence decreased after access to the AI advice.\nOur results highlight that the multi-step transparent decision workflow in complex tasks did have some positive impact in facilitating appropriate reliance. Appropriate reliance at the intermediate steps may be a prerequisite to making the MST decision workflow effective. While an MST workflow can help mitigate over-reliance in the presence of misleading AI advice, it may also cause under-reliance without enough explicit considerations in the intermediate steps. We infer that there is no one-size-fits-all decision workflow to achieve optimal team performance in complex tasks. To this end, future work in the human-centered AI and relevant research communities should explore how to dynamically adapt and combine multiple decision workflows according to the contextual requirements of human-AI collaboration. Our findings suggest that apart from the benefits of improving user consideration of the fine-grained transparency with specific interventions, it is important to consider potential trade-offs with concomitant side effects (e.g., a high cognitive load caused by such interventions). Finally, we identify promising future directions that explore how to improve human-AI team performance and promote global appropriate reliance by characterizing fine-grained appropriate reliance. Our work has important theoretical implications for promoting appropriate reliance on AI systems in complex tasks and practical implications for the effective use of interventions to support human-AI collaboration."}, {"title": "2 RELATED WORK", "content": "Our work proposes to analyze fine-grained appropriate reliance on AI systems in handling complex tasks with a multi-step transparent decision workflow. Thus, we position our work in four realms of related literature: trust calibration and appropriate reliance in AI-assisted decision making (2.1), multi-step hybrid workflows for complex tasks (2.2), transparency and verifiability of AI systems in human-AI collaboration (2.3), misinformation and fact-checking (2.4)."}, {"title": "2.1 Trust Calibration and Appropriate Reliance in Al-assisted Decision Making", "content": "Existing empirical studies [9, 57, 70] and theoretical frameworks related to user trust [61] and reliance behavior [93] highlight that users of an AI system need to identify when an AI system is accurate to rely on and when it is inaccurate and should be overridden. Such ideal reliance patterns are recognized as appropriate reliance on the AI system, but have proven to be extremely hard to obtain even by leveraging explainable AI methods [106]. Prior literature has adopted different definitions of trust; interpreting trust as either a subjective attitude or as objective user behavior in different contexts. Following the growing interpretation in AI-assisted decision making [57, 61], we operationalize user trust as a subjective attitude and user reliance as objective behavior in this work. In most empirical studies [39, 57, 110] where AI systems outperform human decision makers by a margin, the team performance has been reported to be typically worse than that of the AI alone. Addressing such challenges, empirical studies in one-step decision making contexts have been proposed to mitigate under-reliance [106] (i.e., disuse of accurate AI advice) and over- reliance [13, 17] (i.e., misuse of misleading AI advice).\nTrust calibration has been extensively analyzed in interactions with AI systems [49, 65] and automation systems [3, 25, 60, 74]. The primary goal is to align or adjust the level of trust that a human places in an Al system or automated technology based on the actual capabilities of that system. Prior work [16, 73, 74] has shown that transparency of the system (e.g., pertaining to uncertainty or the reasoning process behind AI advice) can provide users with more situation awareness, and contribute to trust calibration. In particular, existing research has explored how information about AI performance [82], uncertainty of AI advice [52, 92, 102], and reasoning process [105] affects user trust. As pointed out by Lee and See [61], trust can substantially impact user reliance behaviors. Trust calibration has been shown to play an important role in facilitating appropriate reliance [47], aligning these lines of research.\nAcross multiple domains and diverse setups, researchers have found that many aspects surrounding user factors (like AI literacy [17] and cognitive bias [41]), task characteristics (e.g., task complexity [91] and proxy task [12]), and AI transparency (e.g., explainable AI [106]) have a substantial impact on user reliance. To mitigate the negative impact of these factors, researchers have proposed effective user interventions. User tutorials have been proposed as an intervention that aims at educating users to fill in the knowledge gap [17, 58] and recognize the weaknesses of an AI system [18]. Others have suggested performance feedback [41, 82] through training sessions to calibrate user perceptions of the accuracy of an AI system. Bu\u00e7inca et al. [13] proposed cognitive forcing functions to mitigate the illusion of explanatory depth [20] brought about by explainable Al methods."}, {"title": "2.2 Multi-step Hybrid Workflows for Effective Task Completion", "content": "With the goal to obtain high-quality human annotations in complex tasks, prior crowdsourcing literature [33, 53, 54] has explored how to decompose complex tasks into multiple microtasks. To ensure text generation quality, Bernstein et al. [10] proposed the \u201cFind-Fix-Verify\u201d workflow, which splits complex text writing and editing tasks into a series of generation and review stages. Through empirical studies on writing, brainstorming, and transcription, Little et al. [68] found that both iteration and multiple votes can increase the average quality of responses, which is referred to as the \u201cIterate-and-Vote\u201d workflow. With the rise of conversational agents in recent years, Qiu et al. [81] leveraged conversational microtask workflows to improve worker engagement. However, Retelny et al. [83] argued that workflows can be a bottleneck to the effectiveness of crowdsourcing in complex tasks.\nInspired by such crowdsourcing literature, researchers have also proposed to build Crowd-AI Hybrid workflows [24] to obtain high-quality data services. For example, instead of obtaining fully manual annotations, asking crowd workers to follow a \u201cFind-Fix-Verify\u201d workflow may boost work efficiency and ensure high-quality outcomes [48, 109]. Similar to the \u201cIterate-and-Vote\u201d workflow, in a hybrid crowd-AI system, votes from crowd workers and AI systems can also improve outcome quality [112]. With the rise of large language models (LLMs), there is an increasing exploration of how conversational interaction can boost crowd-AI hybrid intelligence [97, 99]. For instance, users can obtain writing suggestions for a scientific paper using an LLM-powered conversational interface [97]. With a conversational human-AI interaction, users are involved in an implicit multi-step workflow to complete a task. Existing research has explored LLMs to automate exploratory conversations [38] and plan daily tasks [40]. Chaining multiple LLMs can achieve even complex functions entailed in music chatbots and writing assistants [107]. For example, Wu et al. [108] defined primitive operations based on LLMs and chained them to synthesize controllable workflows dynamically. Such AI chains can also be adapted from crowdsourcing workflows [35].\nWe draw inspiration from existing literature on workflows for accomplishing tasks, and propose a multi-step transparent workflow for decision making in a complex fact-checking task. In our study, participants were required to go through intermediate steps of the AI (indicating the step-wise process of the AI), and verify the correctness of the final AI advice. Such a process allows users to develop an understanding of AI advice in a step-wise manner, and make a final decision based on both AI advice and their initial decision. The multi-step transparent workflow is generated and executed by the AI system [80] (i.e., LLMs coupled with retrieval-augmented generation[62]) to provide advice and support participants in the task. Such human-AI collaboration increases the transparency of the AI system. We aim to explore whether the increased transparency in such a process can facilitate appropriate reliance."}, {"title": "2.3 Transparency and Verifiability in Human-Al Collaboration", "content": "Transparency has been recognized as an important goal towards building trustworthy Al systems [2, 50, 63, 67]. Existing work has explored the transparency of AI systems from different angles - transparency in the reasoning process [56], transparency of data collection/curation [43], transparency of limitations (e.g., uncertainty) [100], transparency of social context [27, 34] etc. Explainable AI (XAI) methods, which may be independent of the actual AI system, are also widely adopted to increase the transparency of AI systems in human-AI collaboration [27, 28, 66, 106]. Besides incorporating XAI to increase system transparency, AI transparency is more explored theoretically [67]. Relatively few works have attempted to empirically verify the impact of transparency on human-AI collaboration. With an empirical study, V\u00f6ssing et al. found that providing the transparency of the reasoning process can increase user trust, while providing transparency of system uncertainty can decrease user trust [105].\nDifferent from the transparency of AI systems, verifiability is typically associated with specific AI advice. Within the context of human-AI collaboration, explainable AI methods [1, 90] are widely used to assist human decision makers by providing evidence (e.g., highlighting a part of task input [84]) to support/oppose AI advice [106]. Among the explainable AI methods, causal explanations [19, 75] propose to reason about the causal relationships between the task input and AI advice, which provides a strong verifiability of AI advice. Recently, retrieval-augmented generation [29, 62] has emerged as one popular paradigm to enhance the verifiability of LLMs. With the retrieved evidence (e.g., documents or relevant structure knowledge) as a reference, humans can verify the factual correctness of LLM generation.\nIn this work, we followed the idea of transparency by design [31, 72] to modularize the complex fact-checking task into a series of sub-fact verification steps. With the decomposed sub-facts and sub-fact verification results, we provided users with global transparency of the AI system's overall process of task decomposition. At the same time, we also provide the retrieved documents in each sub-fact verification, which are input to the LLM-based fact verification system. These documents provide local transparency of the intermediate steps (i.e., sub-tasks). Thus, we provided fine-grained transparency of the AI system and explored how user reliance is shaped through the multi-step transparent (MST) decision workflow. To the best of our knowledge, this is the first empirical effort to understand user reliance on an Al system with fine-grained transparency."}, {"title": "2.4 Misinformation and Fact-checking", "content": "From a data mining perspective [98], misinformation is mainly detected based on two criteria: veracity and intentionality. Veracity mainly focuses on whether referred media or an online post is factually false or inaccurate, regardless of intent. \u2018Fact-checking' is a task mainly based on veracity, which assesses whether claims made in written or spoken language are factually correct [37, 88, 101]. Intentionality is another dimension based on the intent of the information creator/provider. For example, hate speech [76] and \u2018fake news' in the political election [36]. Such misinformation, which often uses inflammatory and sensational language to alter people's emotions [46], can be harmful and widespread online [103, 104]. Based on these criteria, different communities have developed deep learning-based methods [37, 44] to automate checking the massive amount of information online. In this work, we focus on the veracity of factual claims and conducted fact-checking tasks in a human-AI collaborative setting.\nWhile deep learning has been widely adopted to manage misinformation online, human partnership is still a crucial factor in this task [78]. In addition to domain experts who are capable of detecting inaccurate or false information, researchers have explored and showcased crowdsourcing as an effective means to conduct fact-checking [4, 7, 51, 77, 87, 89]. Typically, crowdsourced fact-checking involves three steps in a complex workflow [89]: (1) claim selection, which targets selecting check-worthy claims; (2) evidence retrieval, which obtains necessary information sources (e.g., with a web browser); and (3) claim verification, which includes discussion and aggregation of judgment across different crowd workers and further produces explainable, convincing verdicts (i.e., justification production). Prior to the rapid adoption of LLMs, the AI assistant in each stage of the complex workflow was typically trained independently and served different purposes. Such disparity between AI systems in different stages prevents humans from building a coherent and unified mental model when working with these sub-tasks. Recent advances have led researchers to explore leveraging LLMs to enhance all sub-tasks and provide an end-to-end workflow by chaining LLMs [80].\nIt is evident that LLMs bring new opportunities and challenges to the fact-checking task [8, 15, 71]. On the one hand, LLMs have shown powerful natural language understanding and generation capabilities that can help tackle sub-tasks of fact-checking systems [64]. For example, LLMs can retrieve highly relevant information sources [29] and generate explanations to justify the verification process or the results [80]. Furthermore, LLMs can provide an easy way for humans to communicate with the AI system, offering further potential for human-AI interaction in fact-checking tasks [8]. On the other hand, LLMs are known to hallucinate [45], i.e., generating seemingly plausible but incoherent or factually incorrect content. LLMs have been shown to suffer from out-of-distribution data issues [111] and evolving knowledge without external contextual input (e.g., retrieved documents) [29]. Due to the uncertainty brought about by these prevalent flaws and the lack of accountability, human-AI collaborative fact-checking (comprising at least human oversight) is of fundamental importance in the era of LLMs.\nIn a user study of AI-assisted fact-checking, Nguyen et al. [78] found that crowd workers can be easily misled by wrong model predictions, but such errors can be reduced given interactions with the AI system. With dynamic user input and updated AI system predictions, crowd workers make much fewer errors misled by wrong AI predictions. Thus, Nguyen et al. [78] argued that 'transparent models are key to facilitating effective human interaction with fallible AI models. Contributing to existing literature in the area of human-AI collaboration for fact-checking, our work provides a multi-step transparent decision workflow in assisting humans conduct fact-checking with fine-grained retrieved evidence and decomposed sub-steps. Through this, we aim to provide fine-grained transparency and facilitate appropriate reliance of humans on the AI system. Our insights add further empirical evidence and advance our understanding of how transparency of the Al system and decision workflow affects human-AI interaction."}, {"title": "3 TASK AND HYPOTHESIS", "content": "In this section, we describe the composite fact-checking task (i.e., identifying the factual accuracy of claims based on supporting documents), the multi-step transparent workflow (MST), and present our hypotheses, which have all been preregistered before any data collection.\u00b9"}, {"title": "3.1 Composite Fact-checking Task", "content": "To analyze how the MST decision workflow impacts human-AI collaboration in complex tasks, we consider a composite fact-checking task. An example of solving a composite fact-checking task based on the mutli-step transparent workflow is shown in Figure 1. This task asks participants to decide whether a factual claim is True or False using the supporting documents retrieved from Wikipedia. The reasons for selecting the composite fact-checking task as our test bed are three-fold. Firstly, it contains tasks that require composite semantic understanding and can be solved with a workflow. Secondly, the fact-checking task requires evidence-based verification, which provides verifiability in the intermediate steps. Thirdly, due to the practical need for content moderation online (e.g., hate speech, rumors, and hallucinated content from generative AI systems), it is a timely and relevant scenario for human-AI collaboration."}, {"title": "3.2 Multi-step Transparent Workflow", "content": "AI System Setup. In our study, we adopted an LLM-based method called ProgramFC [80] to serve as our Al system. Figure 1 illustrates how ProgramFC provides global transparency and local transparency. The ProgramFC method conducts fact-checking with two stages: (1) Using GPT-3.5 to generate decomposed steps to conduct composite fact-checking. (2) After generation of the decomposed steps, these steps are executed using another LLM, flan-t5-xl [21]. Using the generated decomposed steps (i.e., sub-facts to verify), the execution step generates intermediate answers based on retrieved supporting documents for each sub-fact. The documents are retrieved based on the popular BM25 algorithm [86], which leverages the query terms frequency appearing in documents to achieve a ranking function. All source documents are from Wikipedia, which is provided with the implementation of ProgramFC.\u00b2 Finally, ProgramFC aggregates the intermediate answers to obtain a final prediction of the factual accuracy for the composite fact. The generated decomposed steps, intermediate answers, and retrieved supporting documents form the basis for the multi-step transparent workflow in our study. In our implementation, we selected the aforementioned LLMs due to two reasons: (1) flan-t5-xl are representative open-sourced LLMs that are widely adopted in question answering and fact-checking practice [22], (2) GPT-3.5 is representative of the performance of most open-sourced and commercial LLMs at the time of data collection (i.e., Jan 2024), offering transferable findings and implications within the scope of our empirical study.\nDecision Workflow. In our study, all workflows follow a two-stage decision making setup, a widely adopted design in AI-assisted decision making [17, 41, 70, 106]. In the first stage, participants work on the fact-checking tasks based on the provided supporting documents and the decision workflow. Next, they were given a chance to alter their initial choice following AI advice. In multi-step decision workflows, decomposed steps and intermediate AI predictions are also shown to support user decisions. In the first stage of decision making, if participants do not find useful supporting documents to support or refute the sub-fact / fact, they can choose \u2018Uncertain' beside the label 'True' and \u2018False'. In the second stage of decision making, participants are asked to make a binary decision between \u2018True' and 'False.\u2019"}, {"title": "3.3 Hypotheses", "content": "Our study mainly aims to contribute to understanding user reliance behaviors on AI systems in the context of solving complex tasks in a sequence of decomposed sub-tasks generated by LLMs. To this end, we devised a multi-step decision workflow and catered to transparency along the decomposed steps and AI advice at the intermediate steps. The intermediate steps and answers work in facilitating a global transparency of the AI system, rendering the decision making process visible [105]. Within our study, the decomposed steps generated by LLMs show the overall step-wise process of the AI system, which can potentially cause user trust to increase [105]. Meanwhile, user trust has been found to substantially impact user reliance behaviors [61]. Combining these findings from existing work, we infer that providing decomposed steps generated by LLMs (i.e., global transparency) can increase user reliance on the AI system. Thus, we hypothesize that:\n(H1) Compared to only providing final AI advice, providing the decomposed steps generated by LLMs (i.e., global transparency) will increase user reliance on the AI system.\nWith the multi-step transparent workflow, users follow the same process (i.e., decomposed steps in the same order) to verify how the AI system works on the composite fact-checking task. Throughout this process, the retrieved documents are provided to increase the transparency of each step (i.e., sub-task). Based on local transparency, users make independent judgments about the intermediate steps before being exposed to final AI advice and intermediate answers predicted by the AI system. In comparison, users with a one-step decision workflow do not have the chance to work on the decomposed sub-tasks following the same step-wise process of the AI system. Thus, users with a multi-step transparent workflow may develop a more critical mindset when adopting the final AI advice supported with intermediate steps and answers. In this way, they may be better equipped to recognize when the AI system provides correct advice and when they should rely on their own decisions. Thus, we hypothesize that:\n(H2) Providing users with a multi-step transparent decision workflow of the AI system will result in relatively more appropriate reliance on the AI system, in comparison to a one-step decision workflow with AI advice.\nAlthough the intermediate steps and task input at each step (e.g., retrieved documents) are designed to provide benefits in the decision making process, adequate consideration and appropriate use can be a prerequisite for their effectiveness. If users can properly leverage fine-grained transparency (i.e., developing precise decisions and reflections at the level of intermediate steps and the level of task input in each step), they can benefit from the multi-step decision workflow, thereby calibrating their reliance behaviors on the AI system. Thus, we hypothesize that:\n(H3) Within a multi-step decision workflow, more accurate intermediate user decisions will result in relatively more appropriate reliance on the final Al advice."}, {"title": "4 STUDY DESIGN", "content": "This study was approved by the human research ethics committee of our institution. Our hypotheses and experimental setup had all been preregistered before any data collection."}, {"title": "4.1 Experimental Conditions", "content": "Addressing the aforementioned RQs, we aim to explore the impact of a transparent decision workflow on user reliance on an AI system in a composite decision making task. Considering transparency of the decision workflow as the sole independent variable in our study, we designed a between-subjects study with four experimental conditions (see Table 1). In all conditions, participants follow a two-stage decision making setup (described in Section 3.1). The different experimental conditions are presented below, with each successive condition being a variant of the previous condition by a single factor.\n(1) Control - In this condition, participants follow a one-step fact-checking workflow in the first stage and only have access to the final AI advice in the second stage.\n(2) MST-GT - In this condition, participants can additionally check the intermediate steps from the AI system as global transparency in the second stage.\n(3) MSTworkflow \u2013 In this condition, participants follow a multi-step transparent workflow in the first stage, where they follow the same working logic (i.e., decomposed steps in the same order) of the AI system, and check the retrieved documents (i.e., part of AI input) at each sub-task. In the second stage of decision making, they will be shown the intermediate steps and intermediate answers from both AI systems and themselves (cf. Figure 2).\n(4) MSTworkflow+ \u2014 On top of condition MSTworkflow, participants in this condition are asked to annotate the usefulness of the supporting documents in each intermediate step. Such annotation forces users to carefully check each retrieved document at the intermediate steps and indicate their usefulness in informing their intermediate decisions. This is designed to function similarly to cognitive forcing functions [13], which nudge users towards critical use of AI advice."}, {"title": "4.2 Task Selection", "content": "As described earlier, composite fact-checking is an important avenue for human-AI collaboration (e.g., credibility assessment systems [85]). All data used in our study is from a public fact-checking dataset \u2013 FEVEROUS-S [5]. This dataset is widely used in composite fact-checking, which leverages documents as evidence. The ten selected tasks in our study are shown in Table 2.\nSelection Process. First, we generate decomposed steps for all tasks in the evaluation set of the FEVEROUS-S dataset. The task decomposition is achieved with prompting LLMs (GPT-3.5 in our study, but this can be easily replaced with other LLMs). The prompt is based on the implementation of ProgramFC [80].\u00b3 Next, we considered and retained all tasks that can be solved by verifying 3 sub-facts. This resulted in 1,127 candidate composite tasks. The ProgramFC algorithm achieved 67.7% accuracy on these tasks. Estimating that each fact-checking task could take around 2-3 minutes for participants to complete, we selected ten tasks from these candidates. Considering all possible cases of (Ground Truth, AI Prediction) pairs, we randomly sample 10 tasks for each case (resulting in 40 tasks as candidates). An author of this paper then annotated the correctness of the decomposed steps and manually followed the decomposed steps to annotate both the usefulness of each supporting document and the factual accuracy of each sub-fact in the decomposed steps. After that, 15 tasks, where the decomposed steps were correct to verify the composite fact, were reserved.\nTo balance the label distribution (True/False for the answer of each task), we selected five tasks with ground truth \u201cTrue\u201d and five tasks with ground truth \u201cFalse\u201d (ten tasks intotal). 70% accuracy is adopted when selecting the tasks, the rationale behind is: (1) it is very close to the actual AI accuracy 67.7% (2) With such an accuracy level, the AI system is compatible with crowd workers to provide decision support without risking optimal performance with over-reliance, which makes it suitable to analyze user (appropriate) reliance patterns. To control the difficulty of tasks where AI prediction is wrong, tasks 1, 6, and 7 contain one or two incorrect intermediate steps. Besides, tasks 2 and 9 contain one intermediate step where the supporting documents are not enough to conclude the factual accuracy. In the two tasks, the Al final advice and the intermediate steps are all correct."}, {"title": "4.3 Measures and Variables", "content": "4.3.1 Reliance-based dependent variables. In condition MSTworkflow and MSTworkflow+", "question": "Does this excerpt contain necessary information to verify the sub-fact?", "responses": "Useless: it does not contain any useful information to verify the fact", "Partial support": "it contains some information partially support the sub-fact", "Full support: it contains all necessary information to support the sub-fact": "Contradiction: it contains necessary information to contradict with the sub-fact", "114": "we adopted Team Performance (i.e., average user accuracy based on their final decision) and Team Performance-wid (i.e., average user accuracy where their initial decision disagrees with AI advice) to measure user performance in our study. Following previous work by Yin et al. [110"}, {"114": "we measured user reliance by using the Agreement Fraction and the Switch Fraction. These measures consider the degree to which user final decisions agree with AI advice, and how often they switch to AI advice when their initial decision disagrees with the AI advice. Following prior work [93"}]}