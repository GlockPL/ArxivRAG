{"title": "Universal Actions for Enhanced Embodied Foundation Models", "authors": ["Jinliang Zheng", "Jianxiong Li", "Dongxiu Liu", "Yinan Zheng", "Zhihao Wang", "Zhonghong Ou", "Yu Liu", "Jingjing Liu", "Ya-Qin Zhang", "Xianyuan Zhan"], "abstract": "Training on diverse, internet-scale data is a key factor in the success of recent large foundation models. Yet, using the same recipe for building embodied agents has faced noticeable difficulties. Despite the availability of many crowd-sourced embodied datasets, their action spaces often exhibit significant heterogeneity due to distinct physical embodiment and control interfaces for different robots, causing substantial challenges in developing embodied foundation models using cross-domain data. In this paper, we introduce UniAct, a new embodied foundation modeling framework operating in a tokenized Universal Action Space. Our learned universal actions capture the generic atomic behaviors across diverse robots by exploiting their shared structural features, and enable enhanced cross-domain data utilization and cross-embodiment generalizations by eliminating the notorious heterogeneity. The universal actions can be efficiently translated back to heterogeneous actionable commands by simply adding embodiment-specific details, from which fast adaptation to new robots becomes simple and straightforward. Our 0.5B instantiation of UniAct outperforms 14X larger SOTA embodied foundation models in extensive evaluations on various real-world and simulation robots, showcasing exceptional cross-embodiment control and adaptation capability, highlighting the crucial benefit of adopting universal actions.", "sections": [{"title": "1. Introduction", "content": "In fields like natural language processing and computer vision, foundation models trained on vast and diverse data sources have demonstrated remarkable success and strong generalization ability, highlighting the benefits of learning general-purpose models over task-specific counterparts [1, 5, 40, 62]. Inspired by these successes, developing versatile embodied foundation models that are capable of handling cross-task, cross-environment, and cross-embodiment generalization, offers a promising pathway towards building general-purpose embodied agent [8, 17, 18, 28, 31, 61, 68].\nHowever, significant challenges arise from the substantial heterogeneity of embodied data [20, 49, 64]. Such heterogeneity is evident not only in visual discrepancies caused by variations in camera placements (e.g., wrist or third-person view) and environmental conditions (e.g., lighting or background variations), but more critically, in action heterogeneity [17, 48, 65]. 1) Robots with different embodiment (e.g., different degrees of freedom or distinctions across robotic arms, quadrupeds, and cars) possess entirely distinct action spaces [68]. 2) Furthermore, the diversity in control interfaces (e.g., end effector (EEF) position or velocity controller for robotics arms) leads to fundamentally different physical meanings for action commands [49].\n3) Even when actions are collected from the same robotic platform but by different human manipulators, the multimodality in human behaviors also exacerbates such heterogeneity [14, 34, 41, 55]. Consequently, embodied action data collected across different robots and institutions tend to reside on largely disjoint manifolds within the original physical spaces (e.g., position and rotation of end-effectors) [17, 65], significantly complicating data sharing across different data sources.\nCurrently, no existing solution can adequately address the issue of action heterogeneity. Most prior studies forcibly treat different action spaces as equivalent and apply the same discretization or normalization techniques, leading to a potentially conflicted action space where similar action encodings could represent entirely different physical meanings [31, 49, 61]. While some efforts attempt to design a physically interpretable action space that is applicable across various robotic systems by na\u00efvely aggregating all individual action spaces [17, 41]. This requires extensive human engineering efforts and fails to uncover and leverage the inherent connections across different embodied action spaces, impeding the effective development of general-purpose embodied foundation models.\nIn this paper, we introduce UniAct (Embodied foundation models with Universal Actions), a novel embodied modeling framework that is constructed in the Universal Action Space rather than the troublesome heterogeneous action spaces. The universal actions learned in UniAct encode the generic atomic behaviors that are generalizable across diverse robotics platforms without being constrained to specific embodiment mechanics and control interfaces. For instance, different robots should perform similar behaviors of \"moving forward\" when facing a target directly ahead, despite exhibiting totally different control signals due to their embodiment gaps. This abstraction transcends specific embodiment and control constraints, allowing it to be universally applicable across diverse robotics platforms, providing significant potential to enhance cross-embodiment data utilization and model generalization. In this sense, minimal parameters and data are sufficient to decode the universal action to an embodiment-specific one, since the general motion behaviors have been captured in the universal space and the decoder simply needs to add some embodiment details for each robotic platform, therefore enabling efficient adaptation across new robotic systems during deployment.\nIn details, UniAct employs a shared Vision Language Model (VLM) [6, 32, 35, 39, 40, 72] to construct the universal action space as a vector-quantized codebook [63]. Akin to a learnable skill library [34, 45, 55], each code encapsulates an atomic behavior versatile enough to be executed by diverse robots. This setup acts as a crucial information bottleneck, driving the VLM to identify and exploit shared primitive behaviors across diverse action spaces. This extraction scheme enables effective generalization of behaviors for cross-embodiment control, making our 0.5B instantiation, UniAct-0.5B, surpasses models 14x larger, such as OpenVLA [31] with 7B parameters, in a wide range of tasks. The derived universal actions can be translated into precise, actionable commands for various embodiments through streamlined heterogeneous decoders. These decoders take the universal action as conditional input and augment it with embodiment-specific features from their unique observational data. This allows for flexible customization according to specific requirements, such as the inclusion or exclusion of proprioceptive features or variations in the number of camera views. Fast adaptation to new domains or robotic platforms can be achieved by simply adding new heads as lightweight decoders for new tasks. Our comprehensive evaluations on challenging task settings, including large view changes and an unseen robot not present in the training data, confirm UniAct's remarkable transferability, demonstrating great advantages of developing embodied foundation models within a universal action space over the conventional heterogeneous spaces."}, {"title": "2. Related Work", "content": "Multimodal Foundation Models. Large Language Models (LLMs) [1, 5, 40, 62] have exhibited remarkable capabilities across a variety of tasks, showcasing impressive zero-shot and in-context learning capabilities [16]. Building on this, large Vision Language Models (VLMs) have been developed by integrating vision and language into a unified tokenized space, demonstrating outstanding multimodal instruction-following abilities [6, 21, 32, 35, 58, 59, 72]. Their success is primarily attributed to extensive internet-scale pretraining, which leverages vast and diverse high-quality data corpora from the Internet.\nEmbodied Foundation Models. When developing embodied foundation models, an additional crucial modality-action (the deployable control signals that robots can interpret and execute, e.g., EEF position/velocity) is incorporated during training. State-of-the-art models are often constructed as Vision Language Action models (VLA) [9, 10, 18, 31, 49], integrating visual and linguistic inputs with actionable outputs. However, the action labels collected from different robotics platforms and labs exhibit significant heterogeneity [30, 49, 64], impeding effective data sharing across different sources. To sidestep this challenge, many works employ large-scale action-free vision language data, such as out-of-domain human activities [15, 24, 25], to firstly obtain a good embodied VLMs, then finetune it to a specialized VLA given a small set of action labels from a specific robot platform [3, 4, 7, 10, 12, 19, 22, 29, 36, 37, 42, 47, 66, 67, 69, 72]. While these methods can enhance sample efficiency for specific robots on a narrow set of tasks, they suffer from serious performance bottlenecks towards building a generalist embodied agent [7, 19, 56], as the action data gathered from any single robot platform is far less comprehensive than crowed-sourced data collected globally [20, 30, 49].\nSome recent works leverage the abundant heterogeneous action labels to develop generalist robot policies for cross-embodiment control. RT-X series [49], Octo [61] and Open-VLA [31] leverage data from different 7-DoF robots to enhance generalization over the one trained on single robot source. Step further, CrossFormer [17], RDT [41], \u03c0\u03bf [8] and Yang et al. [68] explore the potential of using data from robots with totally distinct mechanical structures, such as those in manipulation and navigation, and from single-arm versus bi-manual systems. However, existing works either ignore the heterogeneous properties of action spaces of different sources, crudely treating them as equal without considering their inherent conflicts [31, 49, 61], or na\u00efvely aggregate all action spaces together, failing to exploit the underlying shared commonalities across different robots [8, 17, 41, 68].\nEmbodied Models with Latent Action Spaces. Our work aims to extract a versatile universal action space, akin to a latent space but encodes common atomic control behaviors and patterns across various robotic platforms. Some works develop embodied models in latent spaces [4, 13, 34, 45, 54, 55, 69, 70]. Among them, LAPA [69], IGOR[4], and LAPO [54] develop a latent action space through joint self-supervised training of inverse and forward dynamics models on action-free videos [11]. However, the latent actions extracted in this way primarily focus on explaining the changes between video frames, lacking embodiment considerations or direct causal connections to actual control signals. To see why this is problematic, assuming we add a new object in front of the robot, the visual inputs will change but this has nothing to do with the control behavior, an ideal encoded action should not capture such distracted information. BeT [55], VQ-BeT [34] and QueST [45] also build a discrete codebook of actions via K-means clustering [43] or Vector Quantization [33, 44, 63], where each code in the codebook encodes a different clustering center for action labels. These works primarily focus on simpler domains with a single embodiment type, which enhances the ability to model complex human demonstrations with multiple modes, but struggles to address action heterogeneity across different embodiments. In contrast, our universal actions integrate goal information from the embodiment-agnostic language modality with supervision on the actual action signal, providing a versatile and abstracted skill library to facilitate cross-embodiment sharing. Moreover, our research delves into a more complex heterogeneous setting and develops a large embodied foundation model, moving beyond the limited scopes considered in previous studies."}, {"title": "3. The UniAct Framework", "content": "We introduce UniAct, an embodied foundation modeling framework designed to operate in a Universal Action Space, adept at bridging domain gaps and facilitating training on large-scale heterogeneous data. We first discuss the desirable properties of universal actions, and then provide a detailed discussion about the model architecture and learning scheme for extracting and decoding universal actions from heterogeneous cross-embodiment data."}, {"title": "3.1. Universal Action Space", "content": "The desired universal action space is that all movements, driven by heterogeneous control signals from various embodiments, can be distilled into shared latent atomic behaviors, despite their distinct physical meanings. We refer to these abstract behavior representations as universal actions, which are shared across all physical embodiments.\nWe are particularly interested in exploring a discrete universal action space. This is motivated by the robust capabilities of discrete representations in complex reasoning, planning, and predictive learning, as demonstrated by the success of LLMs and VLMs [1, 5, 6, 60] and Vector Quantized Variational Autoencoders [52, 63]. In this paper, we model the universal action space as $U \\in \\mathbb{R}^{N\\times D}$ and implement it with a vector quantized codebook [63], represented as\n$U = (u_1, u_2, u_3...u_n),$\\nwhere N is the space size and each $u_i$ is a D-dimensional vector embedding that represents a generic atomic behavior.\nSeveral prior studies [4, 11, 69] pursued a similar concept of constructing generic, latent actions by inferring them as the dynamic changes observed between two visual states. However, this scheme suffers two key limitations, leading to suboptimal and noisy universal action spaces:\n\\begin{itemize}\n    \\item The observational changes encompass not only the control outcomes of robots, but also external factors (e.g., environment variation, the appearance of new objects, human intervention, etc.) that have no causal connection to the actual control.\n    \\item The interval between two observations critically influences the semantic interpretation of the extracted atomic behaviors, making standardizing behavioral interpretation complicated across different data sources.\n\\end{itemize}"}, {"title": "3.2. Universal Action Extraction", "content": "To derive the desirable universal action space, we propose a new method for extracting universal actions, pivoting away from solely focusing on explaining observational changes, but more on understanding task progression. Specifically, we fine-tune a large vision language model as the universal action extractor, which outputs the likelihood $p(u|o, g)$ of selecting universal action $u$ given observation $o$ and task goal $g$ (e.g., language instruction). We want the corresponding adopted universal action $u^*$ that matches the atomic behavior encoded in the embodied data to satisfy:\n$u^* = \\arg \\max_{u \\in U} p(u|o, g)$\nAkin to planning in the latent space [13, 50, 51], the extractor aims to infer the most relevant universal action for solving a given task $g$ under the observation $o$, thereby crafting universal actions directly related to task progression rather than merely identifying the noisy observational changes. We employ a VLM to achieve this purpose due to its strong visual-language reasoning capability. Moreover, fine-tuning a pre-trained VLM also greatly improves the sample efficiency when learning the universal actions. This extractor creates a crucial information bottleneck for cross-domain generalization, as different robots are forced to use the same discrete codebook $U$ to capture the generic and shared atomic behaviors across all domains.\nTo implement this, however, the non-differentiable $\\arg \\max$ impedes the gradient propagation. So, we use categorical reparametrization during the training, utilizing the Gumbel-Softmax technique to facilitate gradient estimation [27]. Specifically, the forward procedure is as follows:\n$u^* = \\sum_{i=1}^{n} w_i u_i,$\nwhere $w_i$ is the weight for each universal action $u_i$, computed using the Gumbel Softmax function:\n$w_i = \\frac{exp((\\log p(u_i|o, g) + \\epsilon_i)/\\tau)}{\\sum_{k=1}^{n} exp((\\log p(u_k|o, g) + \\epsilon_k)/\\tau)}$\nHere, $\\epsilon_i$ is a Gumbel noise sampled from the Gumbel distribution, and $\\tau$ is the temperature to smooth the probability distribution. To promote parameter space exploration in the early training stage and the stability of model convergence, we gradually decay temperature $\\tau$ during the training process."}, {"title": "3.3. Heterogeneous Decoding", "content": "To efficiently translate the highly abstract behaviors in the universal action space into precise, embodiment-specific control signals, it is crucial to integrate more embodiment detail, such as control type, proprioception, and distinct observations. To address this, we introduce a series of lightweight decoder heads to adapt for each type of embodiment, denoted as $H = (h_1...h_k...h_K)$, K is number of training domains. Each head $h_k$ is specifically designed to learn the mapping from universal action $u^*$ and visual observation $o$ to heterogeneous control signals for the embodiment in domain k. The operation of each decoder head $h_k$ can be formulated as:\n$\\hat{a}^{(k)} = h_k(u^*, o),$\nwhere $\\hat{a}^{(k)}$ is the predicted control signals. As overly complex decoding heads with excessive parameters could overfit to the data distribution of the target domain, all heterogeneous heads are implemented as simple MLP networks that take the $u^*$ and visual features $o$ extracted by a shared vision backbone as inputs. By keeping the decoding heads lightweight, we ensure that the majority of learning is conducted for the universal actions, thereby maximally improving generalization across different embodiments."}, {"title": "3.4. Training Procedure", "content": "The primary learning objective of UniAct is to distill a universal action space that is shared across diverse embodiments, with the critical feature that these universal actions can be precisely translated back into domain-specific control signals. To facilitate this, the model is trained using a comprehensive collection of K heterogeneous datasets $D = (D_1...D_k...D_K)$. Each $D_k$ comprises a set of robotic control trajectories, represented as $T_i^{(k)} = \\{o_t^{(k)}, a_t^{(k)}, g_t\\}_{1\\le t \\le T}$, where $T$ is the i-th trajectory of maximum length T, containing observations, actions, and goals. Concretely, UniAct ingests $o$ and $g$ as inputs to predict the universal action $u^*$ using the universal action extractor, which is then mapped along with o to $\\hat{a}^{(k)}$ using the heterogeneous decoding heads. The overall training objective is as follows:\n$\\min_{\\Theta, \\Phi} \\sum_{k=1}^K  \\mathbb{E}_{a_i^{(k)}, T_i \\in D_k} L_k(\\hat{a}^{(k)}, a_i^{(k)})$\nHere, $L_k$ is the behavior cloning loss, which is customizable based on the nature of the action labels in each dataset, for example, employing Cross-Entropy for discrete actions and MSE, Huber loss, or diffusion loss [14, 26] for continuous actions. We optimize the above objective to learn both the universal action codebook $U$ as well as the parameters $\\Theta$ of the universal action extractor and all heterogeneous decoding heads. Importantly, while $U$ and the universal action extractor are concurrently updated throughout each training iteration, the heterogeneous heads $h_k$ are updated based on the domain-specific sampled training batches. This training strategy mirrors the philosophy in many meta-learning methods [23], which learns both globally shared parameters that allow adaptation to related tasks, as well as task-specific components that guarantee downstream task performance. Through this approach, UniAct strives to refine a robust, adaptive universal action space as well as a decoding strategy that can be seamlessly integrated with diverse embodiments and their specific operational contexts."}, {"title": "4. Experiments", "content": "In this section, we first describe the detailed implementation of the UniAct framework and then present the evaluation experiments conducted to answer the following questions:\n\\begin{itemize}\n    \\item Can universal actions enhance execution performance across various embodiments with large domain gaps?\n    \\item Can universal actions be seamlessly transferred to new, unseen embodiments?\n    \\item Dose UniAct learn a meaningful universal action space?\n\\end{itemize}"}, {"title": "4.1. Experiments Setup", "content": "Implementation Details. In this paper, we build a 0.5B instantiation of UniAct on heterogeneous embodied data sources to explore a universal action space ($U \\in \\mathbb{R}^{256\\times128}$). Specifically, UniAct-0.5B is built upon LLaVA-OneVision-0.5B [35], a well-trained VLM which can provide comprehensive multi-modal representations. The training of UniAct-0.5B is carried out on 64 A100 GPUs with DeepSpeed [53] over a span of 10 days, utilizing 1 million demonstrations gathered from 28 distinct embodiments. The training data combines several open-sourced robot collections, including Open-XEmbodiment [49], Libero [38], and Droid [30], standardized to include third-person visual observations and language instructions while preserving action heterogeneity. For more details about training and data constructions, please refer to the Appendix A.\nBaseline Setup. We select two state-of-the-art open-source vision-language-action models as baselines: Octo [61] and OpenVLA [31]. Octo is a 0.1B diffusion-based policy, and OpenVLA employs a 7B-parameter auto-regressive architecture with discrete actions. Both models are trained on about 1 million carefully curated robot demonstrations without action heterogeneity, such as pre-processing all absolute EEF positions to relative EEF positions and removing joint position actions. In contrast, UniAct-0.5B is trained on a similar scale of data from the same data sources but does not employ such tedious data cleaning. We compare UniAct with the baseline models to demonstrate its effectiveness in extracting universal actions from heterogeneous data."}, {"title": "4.2. Main Results", "content": "To assess the cross-embodiment generalization capabilities of UniAct-0.5B, we conduct \"out-of-the-box\" evaluations on both a real-world WidowX robot [64] and a simulation Franka robot from Liu et al. [38]. Both platforms are commonly used in previous works to test the effectiveness of generalist robot policies [49, 61, 64], and possess substantial domain gaps. Given that our training dataset includes data from these two embodiments, we can leverage the pretrained heterogeneous heads to translate the universal actions seamlessly back into deployable control signals.\nReal-World Robot Evaluation. Following Kim et al. [31], we define a comprehensive set of evaluation tasks for real-world robots, covering several dimensions of generalization: visual (unseen backgrounds/distractions/object appearances); motion (unseen object positions/orientations); physical (unseen object sizes/shapes); semantic (unseen target objects/instructions/concepts from the Internet); and language grounding (manipulate the object specified in language). Overall, each model is evaluated across 190 rollouts, distributed over 19 tasks with 10 trials each. See Appendix B for more details."}, {"title": "4.3. Fast Adaptation to New Embodiment", "content": "Experiment Setup. To assess the fast adaptation ability, we evaluate on a new real-world robot, AIRBOT, with four drastically different controller interfaces: relative/absolute EEF position and relative/absolute joint position. Neither UniAct nor the baselines were pre-trained on AIRBOT data. We collect 100 demonstrations on this new robot platform with the four different types of control interfaces. Considering the significant heterogeneity among these control interfaces, we put a lot of effort into fine-tuning baselines and make sure the model convergence meets the official requirements (e.g., 95% prediction accuracy for OpenVLA). Please refer to Appendix B for details.\nFast Adaptation with UniAct. Unlike baseline models that require extensive training to bridge the adaptation gap across different action types, UniAct can rapidly adapt to new embodiments and control interfaces. Having already learned cross-embodiment behaviors, we facilitate fast adaptation by freezing the codebook and the universal action extractor. Concurrently, we train four heterogeneous decoding heads from scratch for each type of actions with the collected demonstrations. Each newly introduced head is implemented with a simple MLP that takes the $u^*$ and vision features $o$ from a shared vision backbone as inputs.\nEvaluation. We conducted evaluations using both an easy and a hard version of the task \"stack the cube on another cube\". Notably, the number of parameters UniAct-0.5B used for fine-tuning versus the total model size is the smallest (4M / 500M: 0.8%). In comparison, OpenVLA and Octo utilize 1.4% (97M / 7000M) and 2% (2M / 100M) of their total model sizes, respectively. This efficient parameter utilization highlights UniAct's effectiveness and adaptability, showcasing its superior performance in applying learned universal actions to new tasks and embodiments with minimal parameter space expansion."}, {"title": "4.4. In-depth Analysis of Universal Actions", "content": "In this section, we demonstrate that UniAct constructs a meaningful universal action space from two perspectives: 1) Consistent semantical behaviors are encoded as the same universal actions across diverse embodiments; 2) Universal action extractor can efficiently exploit this shared structure in the universal action space across different robots.\nInterpretation of Universal Actions. We manually inspect the decoded behaviors for all 256 universal actions across different robots and observe at least 40% of them exhibit exact consistency. Fig 6 shows that the same universal action can be decoded back to consistent behaviors for different robots even with huge gaps. For instance, different robots with different viewpoints, and even those undergoing huge sim-to-real gaps, can execute similar semantical meaningful behaviors given the same universal action.\nControl with the Universal Action. Therefore, we can directly interact with the robot to perform desired behaviors by choosing a sequence of universal actions. Fig 7 clearly demonstrates that we can control the robot using universal actions without any robotic knowledge, such as learning complex forward/inverse kinematics transformations. This also underscores the potential of utilizing the universal action extractor as an action tokenizer to facilitate future deployments of more advanced embodied foundation models by planning in this discrete universal action space."}, {"title": "5. Conclusion", "content": "We introduce UniAct, an innovative embodied foundation modeling framework that operates in a Universal Action Space to address the challenge of action heterogeneity. This universal action space encodes shareable atomic behaviors across diverse embodied action spaces to significantly enhance cross-domain data utilization and facilitate cross-embodiment generalization, enabling our 0.5B parameter model to outperform SOTA models that are 14 times larger. Also, the learned universal actions can be precisely translated to any embodiment-specific actions with minimal parameters through heterogeneous decoding, thus allowing for fast adaptation to new robots possessing distinct control interfaces and physical properties. Moreover, our learned universal action extractor can also be used as a universal action tokenizer to power the construction of future large-scale embodied foundation models. Currently, UniAct is trained with a 0.5B parameter instantiation and evaluated mostly on single-arm robotics platforms due to resource constraints. Future work will focus on scaling up UniAct to larger models and extending its application to a broader range of embodiments, including bi-manual robots and even autonomous driving, further leveraging its versatile capability and effectiveness in more robotic applications."}, {"title": "A. Training Details", "content": "Training hyper-parameters UniAct-0.5B utilizes the pretrained parameters from LLava-One-Vision-0.5B [35] to initialize the VLM. For visual feature extraction in heterogeneous decoding heads, we deploy an ImageNet pretrained ResNet18, which is commonly employed in vision-based policy learning [64]. This model was jointly trained during the large-scale pre-training process to enhance perceptual capability in manipulation task scenarios. We adopt resolutions of 384x384 for the VLM and 224x224 for the ResNet18, consistent with their original configurations. Image augmentation settings and more hyper-parameters can be found in Table 2 and Table 3, respectively.\nData construction As illustrated in Table 4, we detail the composition of our training data, which includes the number of trajectories, samples, and the control interfaces for the 28 distinct embodiments. Following previous works [31, 61], we assign different sampling rates to each dataset during training to ensure a balanced mix of embodiments, tasks, and scenes. These sampling rates are specified in Table 4. It is important to note that many of the datasets may contain distinctly action spaces such as EEF position and Joint position. In addition, images from different datasets may contain multiple view points. By default, we use only the first third-person perspective following [31, 61] to maintain consistency."}, {"title": "B. Evaluation Setups", "content": "In this section, we delve deeper into the evaluation experiments for the three robotic embodiments used in our study: WidowX Robot, Franka Robot, and AIRBOT. We provide detailed scores for each embodiment to offer a clearer, more intuitive comparison of their performance.\nB.1. WidowX Robot in Real World\nThe experiments on WidowX aim to assess the models' generalization capabilities across five distinct dimensions, as illustrated in Figure 8. Instead of merely reporting task success rates, we calculate scores based on the progress made towards completing the task for some complex tasks. This scoring method can more intuitively reflect the model's understanding and generalization ability [31]. Detailed scores are available in Table 5. In the subsequent sections, we will provide a comprehensive description of the task settings.\nVisual Generalization: This dimension evaluates the model's ability to adapt to different visual environments characterized by variations in lighting, background, and object textures. Following the setups outlined in [31], we design three specific tasks, detailed in Figure 8. Each task is set against a distinct background environment, features different lighting brightness levels, and involves target objects in two colors: red and green. Additionally, we introduce various visual distractors unrelated to the task to assess the model's ability to generalize visually and maintain robustness against such distractions. To ensure a fair comparison, all variables related to the model's testing conditions are kept consistent across all models. All tasks in this suite are assigned binary score: 0 for failure and 1 for success.\nMotion Generalization: Tasks in this dimension aim to evaluate the robot's capability to perform appropriate motions while recognizing the target object's position and orientation, which may not have been encountered during training. Specifically, we have designed two tasks: Lift eggplant and Put carrot on plate, as detailed in Figure 8. The target objects are placed in several predetermined positions and oriented in pre-designed directions. This helps in assessing the robot's adaptability to changes in physical task parameters. All tasks in this suite are assigned binary score: 0 for failure and 1 for success.\nPhysical Generalization: In this dimension, we examine the model's capability to manage physical variations in objects, such as size, weight, and material properties. The tasks are crafted to evaluate the robot's adaptability to these fluctuations, which significantly influence manipulation strategies. Detailed task setups are listed in Figure 8. The objects involved in these tasks, such as carrots and AAA batteries, often have irregular shapes or are placed in unusual positions (e.g., an overturned pot). Successfully handling and completing tasks with these objects requires a generalized policy that can accurately recognize the physical attributes of the target objects and execute appropriate strategies. All tasks in this suite are assigned binary score: 0 for failure and 1 for success.\nSemantic Generalization: In this dimension, we assess the model's ability to understand and generalize across various semantic contexts. Specifically, the tasks especially the target objects included in this dimension have never been encountered during the training procedures for both UniAct and the baseline models. This approach tests the model's capability to interpret and adapt to new instructions and environments, emphasizing its flexibility and learning efficiency. The purple grape is highly smooth, so we assign scores as follows: 0.25 for touching the grape, 0.5 for grasping it, 0.75 for moving it toward the pot, and 1 for successfully completing a pick-and-place. Also, for the stack green cup on red cup task, we assign scores as follows: 0.25 for touching the green cup, 0.5 for grasping it, 0.75 for moving it toward the red cup, and 1 for successfully stacking. Other tasks in this suite are assigned binary scores.\nLanguage Grounding: This dimension evaluates the model's ability to comprehend and execute commands that are grounded in natural language. The focus is on assessing the model's proficiency in following novel instructions to manipulate specific objects as described verbally. While there are similarities with Semantic Generalization in terms of handling unseen scenarios, Language Grounding task distinctly tests the model's capacity to accurately understand and act on language-based directives within potentially misleading environments. An illustrative example is placing two cups of different colors on a table and instructing the model to manipulate one of them using language. The model must accurately ground the language instruction to the correct object in a complex real-world setting. This tests the model's ability to connect linguistic descriptions directly with physical actions in dynamic and visually diverse environments. For all tasks in this suite, we assign scores as follows: 0.5 for correct grounding, 1 for a successful task completion.\nB.2. Franka Robot in Simulation\nWe include 6500 expert demonstrations of 130 different tasks collected with Franka Robot in LIBERO [38] simulation to train our UniAct-0.5B and then follow the LIBERO Benchmark [38] to evaluate our models. The input images are rendered by the emulator and we use the default 128x128 resolution. As the two open-source baseline models, OpenVLA and Octo, were not initially trained with the simulation data, substantial effort was put into fine-tuning them to facilitate fair comparisons with our UniAct framework. The fine-tuning process for OpenVLA and Octo was conducted on 8 A6000 GPUs and 2 4090 GPUs, lasting 7 hours and 4 hours, respectively. Details of the training hyperparameters are provided in Table 6. Notably, we manually cleaned the \"no-op\" data before fine-tuning OpenVLA following its official guidance, a step that proved crucial for achieving convergence. However, as shown in Figure 9, even with an increased number of training steps, the model's action accuracy remained low. In contrast, our UniAct framework demonstrated robustness to noisy data.\nB.3. Fast adaptation to AIRBOT\nTo assess the fast adaptation capability of UniAct-0.5B and baselines, we fine-tune them using newly collected demonstrations on an unseen embodiment during the pretraining phase, AIRBOT. In this section, we provide detailed information about the fine-tuning processes.\nFine-tuning Settings For UniAct: We utilized 4 A100 GPUs to fine-tune UniAct-0.5B with DeepSpeed. Notably, we train a new MLP network from scratch as the heterogeneous head for AIRBOT while keeping the other modules frozen. The fine-tuning was conducted over a span of 1 hours. The training hyper-parameters employed during this process are detailed in Table 7.\nFine-tuning Settings For Baseline Models. The finetuning for the baseline models, OpenVLA and Octo, was executed on 8 A6000 GPUs and 2 4090 GPUs, lasting 1.5"}, {"title": "D. Limitations and Future Works", "content": "In this section", "Embodiments": "In this study", "observation": "despite differences in control interfaces", "question": "Is the commonality of physical movement exclusive to robotic arms or similar embodiments?\nWe argue that the answer is no. Future work will explore the universal actions for more complex embodiments", "truly\u201d universal action space that is capable of encoding movements across ANY physical embodiment while recognizing unique characteristics and prioritizing shared commonalities. This offers a promising direction for future research, with strong potential for enabling cross-embodiment control across diverse systems.\nMore Flexible Network Design": "In our current implementation, UniAct utilizes identical decoding heads-a simple MLP network-for each embodiment to minimize the risk of over-fitting and facilitating training for the universal action extractor. However, it seems more reasonable that the complexity and number of parameters in the decoding heads should be tailored according to the control complexity of each embodiment and the diversity of its training data. For example, a bi-manual robot, with"}]}