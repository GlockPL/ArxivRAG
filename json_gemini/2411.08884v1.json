{"title": "Quantifying Risk Propensities of Large Language Models: Ethical Focus\nand Bias Detection through Role-Play", "authors": ["Yifan Zeng"], "abstract": "As Large Language Models (LLMs) become\nmore prevalent, concerns about their safety,\nethics, and potential biases have risen. Sys-\ntematically evaluating LLMs' risk decision-\nmaking tendencies and attitudes, particularly in\nthe ethical domain, has become crucial. This\nstudy innovatively applies the Domain-Specific\nRisk-Taking (DOSPERT) scale from cogni-\ntive science to LLMs and proposes a novel\nEthical Decision-Making Risk Attitude Scale\n(EDRAS) to assess LLMs' ethical risk attitudes\nin depth. We further propose an novel approach\nintegrating risk scales and role-playing to quan-\ntitatively evaluate systematic biases in LLMs.\nThrough systematic evaluation and analysis of\nmultiple mainstream LLMs, we assessed the\n\"risk personalities\" of LLMs across multiple do-\nmains, with a particular focus on the ethical do-\nmain, and revealed and quantified LLMs' sys-\ntematic biases towards different groups. This\nresearch helps understand LLMs' risk decision-\nmaking and ensure their safe and reliable ap-\nplication. Our approach provides a tool for\nidentifying and mitigating biases, contributing\nto fairer and more trustworthy AI systems. The\ncode and data are available.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demon-\nstrated remarkable capabilities in understanding\nand generating human language, showing signif-\nicant potential across various domains. The out-\nstanding performance of LLMs has sparked hope\nthat they might be the AGI of our era (Bubeck et al.,\n2023). As LLMs become more widely adopted,\nranging from everyday use to specialized fields,\nthe need for comprehensive evaluation, especially\nregarding safety, ethics, and biases, becomes in-\ncreasingly urgent (Chang et al., 2024).\nCurrently, the widespread popularity of LLMs\nhas led to the development of numerous evalua-\ntion benchmarks, tasks and metrics that examine\nLLMs from different angles (Chang et al., 2024).\nEvaluations of LLMs' risk attitudes are crucial for\nensuring their safe and reliable application, espe-\ncially in critical decisions such as health and fi-\nnance (Chang et al., 2024), particularly in modes\nwhere the LLM acts as an agent (Zhao et al., 2024).\nIn the field, while several benchmarks have been\nproposed to explore LLMs' propensity to engage\nin harmful activities (Yuan et al., 2024; Wang et al.,\n2023; Perez et al., 2022; Shi and Xiong, 2024),\nsuch work remains relatively scarce. Compared to\nprior work, we not only innovatively apply inter-\ndisciplinary tools to evaluate LLMs' risk attitudes\nacross multiple domains and deeply assess ethical\nrisk decision-making, but we are also, to our knowl-\nedge, the first to evaluate the biases within LLMs\nbased on the risk analysis and role-play. Moreover,\nour work fills the gap in research AI psychology\nand cognitive science.\nIn this paper, We explore four key questions:\n(1) Do LLMs exhibit stable and measurable risk\nattitudes? (2) Are there specific differences or con-\nsistent patterns in risk attitudes across multiple do-\nmains among different LLMs? (3) What are LLMs'\nrisk propensities in the ethical domain, and how do\nthese impact LLM safety? (4) Do LLMs exhibit\nsystematic biases in their perception of risk atti-\ntudes and ethical levels for different social groups?\nWe innovatively applies risk attitude assessment\ntools from human psychology, cognitive science\nand behavioral economics to AI systems, conduct-\ning a systematic, standardized, and quantitative\nevaluation of risk preferences in mainstream LLMs.\nStudies have shown that using standard psychomet-\nric inventories for LLMs is feasible and effective\n(Pellert et al., 2023; Li et al., 2024). We have cho-\nsen the DOSPERT (Weber et al., 2002), widely\nused in social science researches (Farnham et al.,\n2018; Shou and Olney, 2020), as our assessment\ntool. DOSPERT places risk assessment within spe-\ncific contexts across different domains, allowing for\na comprehensive evaluation. DOSPERT has been\nwidely validated and applied across different age\ngroups and cultural backgrounds (Blais and Weber,\n2006). In summary, we find that DOSPERT pro-\nvides a promising framework for multi-dimensional\nanalysis of LLMs' \"risk personality\". Applying\nDOSPERT to LLMs represents an innovative inter-\ndisciplinary attempt. Futhermore, given that risk\nscores in the ethical domain are directly related to\nthe safety of LLMs, we propose EDRAS to com-\nprehensively and specifically evaluate LLMs' risk\nattitudes in ethical domain.\nHowever, our research does not stop at simply\nassessing the risk propensities of LLMs. In the\nfield of AI, systematic biases and stereotypes in al-\ngorithms have been a significant concern (Mehrabi\net al., 2021; Blodgett et al., 2020). These not only\npotentially exacerbate the spread of biases and pro-\nmote social inequality but may also cause harm\nto certain groups. How to detect and quantify po-\ntential biases in LLMs, which may contain racial,\ngender, or other biases due to training from human\ntext, is a crucial issue (Demszky et al., 2023). We\nhave discovered a novel approach using risk scale\nand role-play as a bridge to detect and quantify\nbias in LLMs, indirectly reflecting LLMs' differen-\ntial views on various social identities, occupations,\nethnicities, and genders.\nThe main contributions of this study include:\n\u2022 We verified that specific LLM possess differ-\nentiated and relatively stable risk propensities\nthrough multiple tests.\n\u2022 We conducted domain-specific risk propen-\nsities evaluations on multiple mainstream\nLLMs, explored specific differences and con-\nsistent patterns in LLMs' risk attitudes.\n\u2022 We propose a novel EDRAS to further delve\ninto the assessment of LLMs' ethical decision-\nmaking risk attitudes.\n\u2022 We designed different role hypotheses, us-\ning risk scales to quantitatively explore what\nLLMs consider to be the different risk atti-\ntudes for various social groups, and discuss\npotential systematic biases."}, {"title": "2 Related Works", "content": "LLM risk propensities evaluation benchmarks.\nCompared to some well-known and widely used\nbenchmarks such as GLUE (Wang, 2018), MMLU\n(Hendrycks et al., 2020) and HumanEval (Chen\net al., 2021), research on benchmarks for LLM risk\npropensities is inadequate. Several benchmarks or\napproaches have been proposed to explore LLMs'\npropensity to engage in harmful activities (Yuan\net al., 2024; Wang et al., 2023; Perez et al., 2022;\nShi and Xiong, 2024; Pan et al., 2023; Ganguli\net al., 2022; Zhang et al., 2024). Several existing\nevaluations rely on binary classification, simply\ncategorizing LLM outputs as either risky or safe,\nwhich fails to capture the varying degrees and com-\nplexities of risk propensities (Shi and Xiong, 2024).\nWe adopt the Likert scale (Joshi et al., 2015), com-\nmonly used in social science research, to more\nfinely distinguish subtle differences in risk atti-\ntudes.\nAbout the DOSPERT. The DOSPERT scale was\ndeveloped based on the understanding that decision-\nmakers may exhibit varying risk propensities across\ndifferent domains (Weber et al., 2002). For ex-\nample, someone willing to take risks in invest-\nments might be hesitant to participate in extreme\nsports. The original scale was revised to better\nsuit a wider range of ages, cultures, and educa-\ntional levels (Blais and Weber, 2006). These two\npapers have garnered 6084 citations on Google\nScholar, reflecting the scale's effectiveness and\nwidespread applicability. Scales for medical and\nfinancial investment risk propensities has been de-\nveloped within the DOSPERT framework (Butler\net al., 2012; Markiewicz and Weber, 2013). Re-\ncent years have seen numerous studies based on\nthe DOSPERT (Farnham et al., 2018; Shou and\nOlney, 2020; Welindt et al., 2023; LoFaro et al.,"}, {"title": "3 Assessing Basic Risk Attitudes of LLMS", "content": "3.1 Experimental setup\nWe employed the DOSPERT scale consisting\nof 50 items, utilizing a five-point Likert scale:\n1=Extremely unlikely, 2=Unlikely, 3=Not sure,\n4=Likely, 5=Extremely likely. The Likert Scale\nis a commonly used psychometric tool that quanti-\nfies respondents' attitudes and tendencies through a\nseries of statements and numerical options. The 50\nitems cover 5 domains: Ethical, Recreational, So-\ncial, Health/Safety, Financial, with each domain\ncontaining 10 items to maintain a balance.\nDuring the evaluation across multiple LLMs, we\nkept the prompts consistent to eliminate their in-\nfluence on the results, focusing on the risk atti-\ntudes inherent to the LLMs. The LLMs involved\nin this experiment include the closed source GPT-\n40 mini (OpenAI, 2024), GPT-4 Turbo, GPT-3.5\nTurbo (OpenAI, 2023), Claude 3.5 Sonnet (An-\nthropic, 2024), moonshot-v1, ERNIE 3.5, and\nopen source Llama 3.2-3b, Gemma 2-9b (Team\net al., 2024), Qwen2.5-7b (Team, 2024), Mistral-\n7b (Jiang et al., 2023), GLM-4-9b (GLM et al.,\n2024), Vicuna-7b-v1.5. We aimed to select cur-\nrently latest, state-of-the-art and mainstream LLMs\nto ensure the comprehensiveness and validity of\nthe experiment.\n3.2 Do LLMs have a stable risk psychology?\nIn this section, we aim to conduct experiments to\naddress the question raised in the abstract: Does\nthe LLM have a stable and measurable risk pref-\nerence? This will serve as the foundation for our\nsubsequent inquiries, because if the LLM exhibits"}, {"title": "3.3 LLMs' risk perception and preference in\nspecific domains", "content": "In this section, we delve deeper into the risk percep-\ntion and propensities across the 5 specific domains\nof basic DOSPERT for multiple latest and state-of-\nthe-art LLMs. Results are presented in Table 2\nand Figure 3. Like humans, LLMs may have dif-\nferent sensitivities to various types of risks. Over-\nall, LLMs are more adventurous and bold in social\n(such as moving to a new city or wearing unconven-\ntional clothing) and recreation (such as engaging\nin a dangerous sport or chasing a tornado by car to\ntake photos). However, they remain cautious when\nit comes to health and finance (such as spending\na day's earnings on gambling), and demonstrate\nprinciples and boundaries in ethic, reflecting the\nimportance placed on ethical safety in their design\nand implementation. LLMs show both individual\ndifferences in risk attitude and consistent patterns\nacross domains (Figure 3). Results don't show a\nclear linear relationship between model size and\nrisk tendency. This suggests that model risk atti-\ntude is not solely influenced by model size. Some\nmodels show significantly higher risk tolerance in\nspecific domains. For example, Qwen2.5-7b scores\nhigher in finance, possibly reflecting special han-\ndling of financial data during training. Different\nLLMs are trained on diverse datasets containing\nvarying values, cultural backgrounds, and risk atti-\ntudes, affecting model output. Additionally, some\nmodels incorporate stricter safety measures and\nconstraints during training, leading to more conser-\nvative or cautious behavior. Moreover, the depth of\ncontextual understanding in risk scenarios varies,\ninfluencing their judgments. Overall, the results\noffer insights for AI application developers. When\nchoosing or deploying LLMs, should consider their\nperformance in different risk domains to ensure\nsafe application.\nNumerous current studies aim to reveal the per-\nsonality traits of LLMs (as stated in section 2). The\nrisk tendencies of LLMs, innovatively evaluated in\nthis paper, may reflect their underlying personal-"}, {"title": "4 LLMs' Risk Attitudes in Ethical\nDecision-Making", "content": "Risk scores in the ethical domain are directly re-\nlated to the safety of the content generated by\nLLMs. If a model shows a higher likelihood of\nengaging in unethical behavior and is willing to\ntake ethical risks, it may be more prone to out-\nput unethical content, especially under various at-\ntacks on the model, such as jailbreak prompts (Yu\net al., 2024; Kumar et al., 2024). Moreover, people\nmight seek advice from LLMs when facing real-\nworld ethical dilemmas, turning chat interfaces into\nonline psychological or legal consultation rooms\n(Dillion et al., 2024). LLMs can influence human\nusers' values and decisions across society (Qiu\net al., 2024), including moral concepts and ethi-\ncal decision-making. If LLMs choose to take risks\nand act unethically in pursuit of benefits in ethical\ndecision-making, this could lead to societal harm.\n4.1 Proposed ethical risk scale for LLMs\nThe 10 items related to the ethical domain in the\nbasic DOSPERT scale can serve as initial indica-\ntors of a model's risk tendency in ethical aspects\nand can also be used as reference indicators for the\nethical safety of the model. For example, in the re-\nsults of Table 2, Llama3.2-3b and Vicuna-7b-v1.5\nscored higher on ethical risk attitudes. Specifically,\nVicuna-7b-v1.5 indicated that it was not extremely\nunlikely to engage in cheating a fair amount on in-\ncome tax, driving home after having three drinks in\nthe last two hours, or forging someone's signature.\nLlama3.2-3b indicated that it was not extremely\nunlikely to engage in shoplifting a small item (e.g.,\na lipstick or a pen), cheating on an exam, cheating"}, {"title": "5 Unveiling LLM Biases via Persona\nSimulation and Risk Scales", "content": "5.1 Proposed approach\nBased on the inspiration from the DOSPERT and\nour proposed EDRAS, we present a novel approach\nfor measuring and quantifying biases in LLMs. By\nprompting the models to engage in role-playing, we\nenable LLMs to simulate the thought and behav-\niors associated with certain roles (Salewski et al.,"}, {"title": "5.2 Stereotypical risk attitudes in\nmulti-domains", "content": "We initially employed the basic DOSPERT, uti-\nlizing role-playing on Claude 3.5 Sonnet to test\nthe model's perceived risk propensities across dif-\nferent social groups in various domains. Results\nare shown in Figure 4. The model shows artists as\nhighly open to social and recreational risks, scoring\nabove baseline in health and finance. Farmers are\nconservative in social, recreational, and financial\nareas but less concerned about health. Han Chinese\nare depicted as the most cautious, while White\nAmericans are more open in social and recreational\nareas than African Americans. Furthermore, the\nmodel indicates that males (58.8) are more likely\nthan females (41.2) to engage in almost all risk ac-\ntivities within scale. For example, on ethic, males\nare more likely to engage in behaviors such as buy-\ning an illegal drug for personal use, cheating on\nan exam, or shoplifting a small item. This reflects\nstereotypes like conservative farmers, ambitious"}, {"title": "5.3 Biases in LLMs on ethics of various social\ngroups", "content": "We use LLMs to simulate different social roles and\napply the EDRAS to assess ethical risk attitudes,\nwhich may reflect ethical levels. This reflects the\nmodels' internal understanding and assumptions\nabout the moral values of these social groups.\nWe detail the biases or stereotypes related to\nthe occupational dimension. How biases manifest:\nthe biases manifest as consistently portraying politi-\ncians or artists as having higher ethical risk pref-\nerences, which may indicate negative stereotypes\nabout these groups. For example, LLMs suggest\nthat politicians are more likely to be dishonest for\ngain (e.g., falsifying report numbers to look bet-\nter), while artists are more likely to disregard social\nrules or lack public morality (e.g., skipping classes\nor talking loudly on subway). Clear harms: the\npotential harms of these biases and stereotypes in-\nclude unfair judgments or discrimination against\nspecific occupational groups. They can reinforce\nexisting occupational stereotypes, affecting career\nchoices and social perceptions. If these biases are\nintroduced into AI-assisted decision-making sys-\ntems, they can lead to unfair outcomes. Possible\nsocial causes: politicians score the highest and\nare stereotypically viewed as dishonest, willing to\nsacrifice ethical principles for political gain. This\nmay stem from reported political scandals, the cor-\nrupting influence of power, and the gap between\npolitical promises and reality. Freelance artists\nscore second highest and are stereotypically seen\nas free-spirited and unconstrained by traditional\nmorals and social norms, often perceived as rebel-"}, {"title": "6 Conclusion", "content": "This study innovatively applies risk attitude as-\nsessment tools from cognitive science to LLMs,\nproposing a novel approach to evaluate LLMs' risk\npropensities and ethical risk attitudes. Our research\nnot only reveals the \"risk personalities\" of LLMs\nacross multi-domains but also quantifies system-\natic biases in LLMs towards different groups. By\nproposing the EDRAS and integrating role-playing,\nwe provide effective tools for identifying AI biases.\nThis paper contributes to understanding LLMs' risk\nfeatures, ensuring their safe and reliable applica-\ntion, and building fairer and more trustworthy AI\nsystems.\nLimitations\n\u2022 The specific reasons behind the measured risk\ntendencies of LLMs still require further inves-\ntigation.\n\u2022 Even with our proposed EDRAS, the 106\nquestions may not fully cover all relevant eth-\nical scenarios and need further supplementa-\ntion.\n\u2022 Currently, we have only evaluated and tested\nin English. Risk attitudes, moral perspectives\nand perceptions of professions and education\nlevels may vary across different languages and\ncultural contexts, but our proposed EDRAS\nmay not fully capture these differences.\n\u2022 Currently, we only assess ethical risk behav-\niors in daily work, study, and public life. Fur-\nther research is needed to evaluate other eth-\nical dimensions, such as medical ethics or fi-\nnancial ethics.\n\u2022 Due to space limitations, we present only\nquantitative scores. Further research could in-\ncorporate qualitative analysis to examine the\nspecific responses generated by LLMs.\n\u2022 Due to space constraints, we present results\nonly for three social identity dimensions: oc-\ncupation, education level, and university tier.\nFurther research using our method to explore\nbiases in dimensions such as gender, race,\nclass, and culture is needed.\n\u2022 Our role-playing strategy examines LLM bi-\nases within single social identity dimensions.\nMore complex strategies are needed to fully\ncapture the intersectionality of social identi-\nties, such as the combined effects of race, gen-\nder, and occupation."}, {"title": "Ethical Statement", "content": "This study involves sensitive or controversial topics\nsuch as ethical decision-making and biases related\nto occupations, education levels, racial, and cul-\ntural groups. While the content may potentially\nbe offensive to the mentioned social groups, our\npurpose is purely academic, aiming to reveal biases\nin order to build more responsible and trustworthy\nAI systems. Our purpose without any discrimina-\ntory intent. The content of our study may reflect\ncertain stereotypes or biases, but we emphasize\nthat these do not represent our views. We try to\nmaintain a neutral stance in designing experiments\nand interpreting results, striving for fairness and\nobjectivity.\nDue to limitations in length and resources, we\nonly discuss biases in LLMs related to professions\nand education levels, but this does not mean we\nare ignoring other marginalized or disadvantaged\ngroups. Our approach can similarly be applied to\ndetect biases or stereotypes in LLMs targeting any\nmarginalized or disadvantaged groups.\nThe study focuses on Al models and does\nnot disclose any personal information. For the\nLLMs used, we read the corresponding responsible\nuse guide (such as https://www.llama.com/\nresponsible-use-guide/https://www.llama.com/responsible-\nuse-guide/) and license (such as li-\ncense of Llama3.2-3b in https://\nhuggingface.co/meta-llama/Llama-3.\n2-3B-Instructhttps://huggingface.co/meta-\nllama/Llama-3.2-3B-Instruct), adhered to their\nusage policies, and used them in compliance\nwith applicable laws or regulations, disclosing\nthe specific scenarios of use in the paper. Our\nmethods, data, and results are openly shared to\nensure transparency."}]}