{"title": "IReCa: Intrinsic Reward-enhanced Context-aware Reinforcement Learning\nfor Human-AI Coordination", "authors": ["Xin Hao", "Bahareh Nakisa", "Mohmmad Naim Rastgoo", "Richard Dazeley"], "abstract": "In human-AI coordination scenarios, human agents usually\nexhibit asymmetric behaviors that are significantly sparse and\nunpredictable compared to those of AI agents. These charac-\nteristics introduce two primary challenges to human-AI co-\nordination: the effectiveness of obtaining sparse rewards and\nthe efficiency of training the AI agents. To tackle these chal-\nlenges, we propose an Intrinsic Reward-enhanced Context-\naware (IReCa) reinforcement learning (RL) algorithm, which\nleverages intrinsic rewards to facilitate the acquisition of\nsparse rewards and utilizes environmental context to enhance\ntraining efficiency. Our IReCa RL algorithm introduces three\nunique features: (i) it encourages the exploration of sparse\nrewards by incorporating intrinsic rewards that supplement\ntraditional extrinsic rewards from the environment; (ii) it im-\nproves the acquisition of sparse rewards by prioritizing the\ncorresponding sparse state-action pairs; and (iii) it enhances\nthe training efficiency by optimizing the exploration and ex-\nploitation through innovative context-aware weights of ex-\ntrinsic and intrinsic rewards. Extensive simulations executed\nin the Overcooked layouts demonstrate that our IReCa RL\nalgorithm can increase the accumulated rewards by approxi-\nmately 20% and reduce the epochs required for convergence\nby approximately 67% compared to state-of-the-art base-\nlines.", "sections": [{"title": "Introduction", "content": "In human-AI coordination scenarios, human agents and AI\nagents leverage each other's capabilities to achieve a shared\ngoal collaboratively (Zhao et al. 2023; Sarkar et al. 2022).\nWhile significant progress has been made in scenarios in-\nvolving multiple AI agents coordinating through reinforce-\nment learning (RL) approaches that can interact with the en-\nvironment smoothly (Vinyals et al. 2019; Berner et al. 2019;\nSilver et al. 2017), these approaches often assume that the\ncoordinators are either optimal or similar to the ego Al agent\nitself (Carroll et al. 2019). This assumption can lead to dis-\ntributional shifts when the well-trained ego Al agent is de-\nployed alongside human coordinators (Zhao et al. 2023), as\nthe ego AI agent has not encountered the asymmetric behav-\niors exhibited by human agents during their training phases.\nDue to the asymmetric behaviors of human and AI agents\nand the dynamic nature of their interactions in the environ-\nment (Goecks et al. 2019), human-AI coordination scenarios\nare inherently complicated (Ghosal et al. 2023; Zhao et al.\n2020). These complexities can intensify the challenge of ob-\ntaining rewards, particularly in tricky environments where\nrewards are sparse. A common approach to mitigate the is-\nsue of sparse reward acquisition in RL is to encourage the AI\nagent to explore its environment more thoroughly (Jaques\net al. 2019; Haarnoja et al. 2018). However, the ingrained\ntrade-off between exploration and exploitation in RL (Sut-\nton and Barto 2018; Mnih et al. 2016; Rashid et al. 2020)\nintroduces a second challenge: the inefficiency of training.\nWhile augmenting exploration can improve the acquisition\nof sparse rewards, it simultaneously reduces exploitation ef-\nficiency that is crucial for training efficiency (Lowe et al.\n2017; Hadfield-Menell et al. 2016; Pathak et al. 2017), there-\nfore problematizing the optimization of human-AI coordina-\ntion.\nTo enhance the effectiveness of obtaining sparse rewards,\nwe design innovative intrinsic rewards that facilitate thor-\nough environmental exploration; whilst to improve the effi-\nciency of the training process, we introduce context-aware\nweights to enhance training efficiency by optimizing the ex-\nploration and exploitation.\nOn the one hand, intrinsic rewards, unlike traditional ex-\ntrinsic rewards obtained from the environment, are usu-\nally derived from agents' observations and can increase the\nlikelihood of identifying sparse state-action pairs for ob-\ntaining sparse rewards. Typical intrinsic rewards, such as\nmaximum entropy (Haarnoja et al. 2018) and causal influ-\nence (Jaques et al. 2019), enhance exploration by focusing\non the actions of the AI agent and its coordinators. How-\never, their effectiveness in sparse human-AI coordination is\nlimited by the high diversity of human actions. While recent\nwork (Zhao et al. 2023) introduces a general approach that\ndoesn't rely on specific human data, the no free lunch theo-\nrems (Wolpert and Macready 1997) suggest that exploiting a\nspecific subclass of problems is necessary for improved per-\nformance (Andrychowicz et al. 2016; Hao et al. 2023a,b).\nIncorporating a logarithmic term of the Kullback-Leibler\n(KL) divergence between the RL policy and the human pol-\nicy as a regularization term has proven effective in enhanc-\ning accuracy in text summarization (Christiano et al. 2017;\nStiennon et al. 2020). Motivated by this strategy, we pro-\npose designing intrinsic rewards tailored to highlight sparse"}, {"title": "Human-AI Coordination Scenario", "content": "One of the primary objectives in human-AI coordination is\nto achieve the shared goal through seamless collaboration\nbetween Al agents and human coordinators (Goecks et al.\n2019; Zhao et al. 2020). Our IReCa RL algorithm aims to\nimprove the effectiveness and efficiency of human-AI co-\nordination by enhancing the AI agent's ability to adapt to\nhuman behaviors and dynamic environments.\nOvercooked environment is a widely recognized bench-\nmark for human-AI collaboration scenarios that present\ncomplicated cooking tasks within a limited time, making\nit ideal for studying coordination dynamics. We experi-\nment with two Overcooked layouts (Carroll et al. 2019).\nThe first layout, Cramped Room, poses challenges related\nto low-level coordination due to its confined space, which\noften leads to agent collisions. In contrast, the second lay-\nout, Asymmetric Advantages, necessitates high-level strate-\ngic planning as agents begin in distinct areas with access to\nmultiple cooking stations."}, {"title": "IReCa Reinforcement Learning", "content": "In this section, we first provide an overview of the IReCa\nRL algorithm. Subsequently, we clarify six key definitions\nintegral to IReCa's detailed design. Finally, we present the\ndesign specifics of our IReCa RL algorithm."}, {"title": "IReCa Overview", "content": "As illustrated in Fig. 1, the core concept of our approach\nlies in the design of the IReCa reward, which is composed\nof three key elements: (1) standard extrinsic rewards de-\nrived from interactions with the environment, (2) innova-\ntive intrinsic rewards based on the actions of both AI and\nhuman agents, and (3) context-aware weights that automat-\nically balance exploration and exploitation by dynamically\nadjusting the importance of extrinsic and intrinsic rewards.\nUsing the IReCa reward, the AI agent leverages an RL al-"}, {"title": "Detailed Design", "content": "Based on the six key concepts clarified above, we present\nthe detailed design of our IReCa RL algorithm.\nExtrinsic Reward In complex human-AI coordination\nscenarios with sparse reward signals, achieving the desired\nsparse rewards often requires leveraging the intermediate-\nstage rewards to guide the exploration and exploitation of\nkey preliminary actions that lead to the target sparse re-\nwards (Ng, Harada, and Russell 1999; Hu et al. 2020). For\nexample, in the Overcooked environment, placing an onion\nin the pot is a crucial preliminary action for earning the\nsparse reward associated with cooking onion soup. Stage re-\nwards are typically present during the early stages of train-\ning but tend to diminish as training progresses. We follow\nthe design outlined in (Carroll et al. 2019), in which the ex-\ntrinsic reward is composed of the target sparse reward, $r^{ES}_t$,\nand a linearly fading stage reward, $r^{EG}_t$, and is given by:\n$r^{E}_t = r^{ES}_t + \\text{max} \\{0, 1 - \\lambda^{EG} \\cdot t\\} r^{EG}_t$, (2)\nwhere $\\text{max}\\{\\}$ is the maximum function, and $\\lambda^{EG}$ is a con-\nstant coefficient of the extrinsic stage reward that controls\nthe fading speed of the stage reward.\nBoth the sparse reward and the stage reward are derived\nfrom the environment and represent the combined contri-\nbutions of both the AI and human agents. Following the\napproach in (Carroll et al. 2019), these rewards can be ex-\npressed as:\n$r^{ES}_t(k) = \\sum_k R^{ES}(o_t(k), a_t(k)),$ (3)\nand\n$r^{EG}_t(k) = \\sum_k R^{EG}(o_t(k), a_t(k)),$ (4)\nwhere $R^{ES}(.)$ and $R^{EG}(.)$ denote the sparse and stage re-\nward functions, respectively, and $o_t(k)$ and $a_t(k)$ repre-\nsent the observation and action of the k-th agent at the t-th\ntimestep.\nIntrinsic Reward While stage rewards can assist in this\nprocess, they also pose the risk of leading the learning pro-\ncess into local optima due to excessive exploitation. To\ncounteract this limitation, intrinsic rewards serve as a pow-\nerful mechanism to enhance the exploration capabilities of\nAI agents (Song et al. 2020; Du et al. 2024).\nIn human-AI coordination scenarios with sparse rewards,\nto effectively collaborate with human coordinators who be-\nhave diversely and sparsely, a focus on the critical state-\naction pairs for obtaining those sparse rewards is necessary.\nDrawing inspiration from the approach in (Stiennon et al.\n2020), which introduced a regularization term that improved\nupon state-of-the-art reward functions, we propose a novel\nmethod. Traditional regularization terms such as entropy,\nKL divergence, and cross-entropy often underestimate the\nimpact of these sparse state-action pairs due to their low\nprobability of occurrence. To address this, we introduce a\nlogarithmic term to emphasize the significance of these low-\nprobability pairs, thereby enhancing the learning process."}, {"title": "Context-aware Weights", "content": "Incorporating stage rewards into\nthe extrinsic reward can enhance the acquisition of sparse\nrewards by guiding exploration. However, it also introduces\nthe risk of the AI agent becoming trapped in local optima,\nparticularly in the early training stages when the stage re-\nward is greater than the sparse reward. While intrinsic re-\nwards can help mitigate this issue by encouraging explo-\nration, excessive exploration can also be detrimental. There-\nfore, a desirable policy must be context-aware \u2013 able to op-\ntimize exploration and exploitation based on the current sit-\nuation. Specifically, the policy should increase exploration\nwhen the AI agent is at risk of converging to suboptimal\nsolutions and favor exploitation when the current policy is\nperforming effectively.\nTo achieve this balance, we introduce context-aware\nweights that adaptively adjust the AI agent's exploration and\nexploitation strategies based on the evolving context. The\ncore idea is to assign greater weight to intrinsic rewards\nwhen the extrinsic reward shows minimal improvement and\nreduce this weight when the extrinsic reward is increasing\nsteadily. To maintain policy robustness, these context-aware\nweights are updated at each epoch, in synchronization with\nthe policy updates.\nWe first define the average episode return to quantify the\nchanges in extrinsic and intrinsic rewards by\n$\\hat{R}^E_n = \\frac{1}{T} \\sum_{t=nT}^{(n+1)T} (r^E_t) = \\frac{1}{T} \\sum_{t=nT}^{(n+1)T} (r^{ES}_t + r^{EG}_t)$,\n$\\hat{R}^{IA}_n = \\frac{1}{T} \\sum_{t=nT}^{(n+1)T} (r^{IA}_t)$,\n$\\hat{R}^{IH}_n = \\frac{1}{T} \\sum_{t=nT}^{(n+1)T} (r^{IH}_t)$, (7)\nwhere $T$ is the number of timesteps in each epoch. In our\ndesign, we assign equal weight to both sparse and stage re-\nturns, based on the premise that stage rewards correspond to\npreliminary actions that ideally lead to the target sparse re-\nwards. Therefore, the ratio between sparse and stage returns\nshould remain consistent, reflecting an effective policy. This\napproach contrasts with the design of the extrinsic reward,\nwhere the stage reward component is linearly faded out over\ntime, as it is not the ultimate objective. The primary goal\nis to maximize the sparse reward, which drives the desired\nbehavior.\nNext, based on the changes in the episode returns, the\ncontext-aware weights are computed using the Softmax\nfunction as\n$\\kappa^E_n,\\kappa^{IA}_n,\\kappa^{IH}_n = \\Lambda^R . \\text{softmax} (\\frac{\\hat{R}^E_{n-1}}{\\hat{R}^E_n}, \\frac{\\hat{R}^{IA}_{n-1}}{\\hat{R}^{IA}_n}, \\frac{\\hat{R}^{IH}_{n-1}}{\\hat{R}^{IH}_n}),$ (8)\nwhere $\\Lambda^R$ is a constant coefficient that controls the magni-\ntude of the context-aware weights. The rationale behind this\ndesign is straightforward: if the episode return is increas-\ning, it suggests that the current policy is effective and re-\nquires minimal updates. Conversely, if the episode return is\ndecreasing, it signals that the policy is underperforming and\nshould be adjusted more significantly.\nTo prevent excessive exploration in later stages of train-\ning, we limit the exploration by applying a threshold on the\nepisode number $N_{th}$. Finally, the context-aware weights are\ndetermined as follows:\n$\\kappa^E = \\kappa^E \\cdot 1\\{n < N_{th}\\} + 1\\{n > N_{th}\\},$\n$\\kappa^{IA} = \\kappa^{IA} \\cdot 1\\{n < N_{th}\\},$\n$\\kappa^{IH} = \\kappa^{IH} \\cdot 1\\{n < N_{th}\\},$ (9)\nwhere $1(.)$ is the indicator function."}, {"title": "Environment", "content": "Our experiments are conducted in the Overcooked environ-\nment (Carroll et al. 2019), where a human agent and an AI\nagent work together to prepare as many onion soups as pos-\nsible within a limited number of timesteps across a vari-\nety of layouts. The goal is to serve the soup in the desig-\nnated area, earning a sparse reward of 20 points for each\nsuccessfully served soup. Achieving this requires complet-\ning a sequence of actions that correspond to stage rewards:\npicking up onions from a specified location, placing three\nonions into a pot, and serving the soup after cooking it for\n20 timesteps. In this collaborative scenario, both sparse and\nstage rewards are equally shared between the agents."}, {"title": "Agent Policies", "content": "For the human agent, we follow the methodology outlined\nin (Carroll et al. 2019) and model the human agent's be-\nhavior by using the behavior cloning policy. Specifically,\nthe human behavior data is split into training and testing\ndatasets (Carroll 2024) and the behavior cloning models\nused in the training and testing phases employed the respec-\ntive datasets.\nFor the AI agent, we utilize the RL algorithm to develop\nits policy since we need to interact with its dynamic hu-\nman coordinator and the environment. The RL algorithm's\nstate space corresponds to the Overcooked grid world, and\nits action space includes six discrete actions: move up, move\ndown, move left, move right, stay, and interact. The \u201cinter-\nact\" action is context-sensitive, enabling the agent to pick\nup or place onions, plates, or soup, depending on the current\nstate of the environment. Our implementation is based on the\nGym-compatible Overcooked environment given in (Carroll\n2024).\nTo evaluate the performance of our proposed IReCa RL\nalgorithm, we compare it with two baseline RL algorithms,\ntraining and testing the AI agent using each of the three poli-\ncies.\nPPOBC is a baseline algorithm employing the traditional\nproximal policy optimization (PPO) policy, where the re-\nward consists of both the sparse reward and a linearly faded\nstage reward, as described in (Carroll et al. 2019).\nCausal is a baseline algorithm that incorporates an intrinsic\ncausal influence reward supplementing the extrinsic reward\nin PPOBC. The causal influence reward encourages the AI\nagent to take actions that can lead to significant changes in\nits coordinator's actions (Jaques et al. 2019).\nIReCa is our proposed IReCa RL algorithm, which incorpo-\nrates our innovative intrinsic rewards and the context-aware\nweights supplementing the extrinsic reward in PPOBC.\nFor fair comparisons, these three RL algorithms share the\nsame neural network architecture and hyper-parameters. The\nPPO structure follows the classical PPO algorithm given\nin (Schulman et al. 2017; Chrysovergis 2024). The archi-\ntecture of both the actor and critic networks comprise three\nconvolutional layers (with filter sizes of 5\u00d75, 3\u00d73, and 3\u00d73,\neach containing 25 filters), followed by two fully connected\nlayers with 64 hidden units each. Key hyper-parameters are\nsummarized in Table 1, and further details for parameter set-\ntings can be found in our GitHub repository."}, {"title": "Results and Analysis", "content": "As shown in Fig. 2, we present experimental results in\ntwo distinct layouts of the Overcooked environment. The\nCramped Room layout shown in Fig. 2a poses significant\nlow-level coordination challenges due to the confined space,\nleading to a high susceptibility to agent collisions. In con-\ntrast, the Asymmetric Advantages layout depicted in Fig. 2b\nevaluates the ability of agents to adopt high-level strategies\nthat leverage their individual strengths.\nFor each layout, the training phase consists of statistical\nresults from 18 independent experiments, with each exper-\niment plotting the episode return over 400 epochs, using a\nhuman model trained on the corresponding dataset. In the\ntesting phase, we provide the average episode return from\n400 independent runs using the human model derived from\nthe testing dataset.\nIn the experimental results, we address following two key\nquestions:\n1) Is our IReCa RL algorithm more effective and efficient\nthan the baseline algorithms?\n2) If so, what factors contribute to the superior performance\nof IReCa compared to the baselines?"}, {"title": "Cramped Room", "content": "Fig. 3 presents the simulation results for\nthe Cramped Room layout of the Overcooked environment.\nAs shown in Fig. 3a, the IReCa algorithm can achieve\napproximately 20% higher average sparse episode returns\ncompared to baseline methods during both the training and\ntesting phases. These results indicate that IReCa is more\neffective than the baselines, and validate that our IReCa\nis more effective in identifying and exploring sparse state-\naction pairs that lead to targeted sparse rewards. Further-\nmore, the superior episode returns observed during testing\nexperiments suggest that IReCa not only enhances explo-\nration but also exhibits robustness and generalization capa-\nbilities, ensuring that the performance gains are due to ef-\nfective exploration rather than overfitting to the training en-\nvironment.\nPrevious studies have identified two primary factors con-\ntributing to performance degradation for AI agents in the\nCramped Room layout: collisions with the human coordina-\ntor (Carroll et al. 2019) and waiting for the human agent to\nvacate the AI agent's preferred path (Zhao et al. 2023). Our\nanalysis extends this by hypothesizing that the AI agent may\nalso become trapped in local optima by prioritizing stage re-\nturns over sparse rewards. This is because the stage episode\nreturns are significantly higher than the sparse episode re-\nturns in the early training epochs, which may lead the AI\nagent to engage in unnecessary and redundant actions aimed\nat optimizing stage rewards.\nFig. 3c further investigates the role of intrinsic rewards\nin this process. As depicted in the figure, these intrinsic re-\nwards are significantly higher than those in the causal base-\nline, helping the AI agent avoid local optima and achieve\nbetter overall performance. Since the primary objective in\nthe Overcooked environment is task completion, indepen-\ndent exploration of the AI agent's action space is crucial.\nThe intrinsic rewards in IReCa facilitate this exploration,\ncomplementing the human agent's actions and improving\nboth coordination and efficiency.\nLastly, Fig. 3d illustrates the evolution of context-aware\nweights during training. In the initial epochs, the weights\nassign higher values to intrinsic rewards and lower values\nto extrinsic rewards. This trend aligns with our expectation\nthat comprehensive early-stage exploration will ultimately"}, {"title": "Asymmetric Advantages", "content": "Fig. 4 presents the experimental\nresults for the Asymmetric Advantages layout (see Fig. 2b).\nIn this layout, agents start in different areas and have access\nto two pots for cooking onion soup, which reduces the like-\nlihood of collisions compared to the Cramped Room layout.\nSimilar to the Cramped Room results, Figs.4a, 4b, and 4c\nshow that IReCa consistently outperforms the baseline algo-\nrithms in sparse, stage, and intrinsic episode returns, respec-\ntively. Interestingly, the gains of the episode returns are less\npronounced during the training phases. This phenomenon is\ndue to the reduced frequency of collisions and waiting times\ncompared with the Cramped Room layout, as the agents are\nspatially separated and have access to more pots.\nDespite this, IReCa reduces the training epochs required\nfor convergence by approximately 67% compared to the\nbaselines. This acceleration is attributed to the context-\naware weights (Fig. 4d), which prioritize updates to extrinsic\nrewards and optimize the AI agent's exploration of its action\nspace. This trend of the context-aware weights is in contrast\nto the Cramped Room layout, where additional exploration\nis necessary to minimize waiting times.\nThe testing results in Figs. 4a and 4b further support\nthat in human-AI coordination scenarios employing the pre-\ntrained human model, IReCa is more effective than tradi-\ntional causal influence rewards that are valuable in purely\nMARL contexts."}, {"title": "Conclusion", "content": "To enhance human-AI coordination, we introduced an In-\ntrinsic Reward-enhanced Context-aware (IReCa) reinforce-\nment learning algorithm. This approach incorporated inno-\nvative intrinsic rewards to facilitate comprehensive explo-\nration and novel context-aware weights to optimize explo-\nration and exploitation, supplementing traditional extrinsic\nrewards. Extensive experimental training results in two lay-\nouts of the Overcooked environment demonstrated that our\nIReCa increased episode returns by approximately 20% and\nreduced the number of epochs required for convergence\nby approximately 67%. Testing experiments further under-\nscored the algorithm's robustness and generalization abili-"}]}