{"title": "IReCa: Intrinsic Reward-enhanced Context-aware Reinforcement Learning\nfor Human-AI Coordination", "authors": ["Xin Hao", "Bahareh Nakisa", "Mohmmad Naim Rastgoo", "Richard Dazeley"], "abstract": "In human-AI coordination scenarios, human agents usually\nexhibit asymmetric behaviors that are significantly sparse and\nunpredictable compared to those of AI agents. These charac-\nteristics introduce two primary challenges to human-AI co-\nordination: the effectiveness of obtaining sparse rewards and\nthe efficiency of training the AI agents. To tackle these chal-\nlenges, we propose an Intrinsic Reward-enhanced Context-\naware (IReCa) reinforcement learning (RL) algorithm, which\nleverages intrinsic rewards to facilitate the acquisition of\nsparse rewards and utilizes environmental context to enhance\ntraining efficiency. Our IReCa RL algorithm introduces three\nunique features: (i) it encourages the exploration of sparse\nrewards by incorporating intrinsic rewards that supplement\ntraditional extrinsic rewards from the environment; (ii) it im-\nproves the acquisition of sparse rewards by prioritizing the\ncorresponding sparse state-action pairs; and (iii) it enhances\nthe training efficiency by optimizing the exploration and ex-\nploitation through innovative context-aware weights of ex-\ntrinsic and intrinsic rewards. Extensive simulations executed\nin the Overcooked layouts demonstrate that our IReCa RL\nalgorithm can increase the accumulated rewards by approxi-\nmately 20% and reduce the epochs required for convergence\nby approximately 67% compared to state-of-the-art base-\nlines.", "sections": [{"title": "Introduction", "content": "In human-AI coordination scenarios, human agents and AI\nagents leverage each other's capabilities to achieve a shared\ngoal collaboratively (Zhao et al. 2023; Sarkar et al. 2022).\nWhile significant progress has been made in scenarios in-\nvolving multiple AI agents coordinating through reinforce-\nment learning (RL) approaches that can interact with the en-\nvironment smoothly (Vinyals et al. 2019; Berner et al. 2019;\nSilver et al. 2017), these approaches often assume that the\ncoordinators are either optimal or similar to the ego Al agent\nitself (Carroll et al. 2019). This assumption can lead to dis-\ntributional shifts when the well-trained ego Al agent is de-\nployed alongside human coordinators (Zhao et al. 2023), as\nthe ego AI agent has not encountered the asymmetric behav-\niors exhibited by human agents during their training phases.\nDue to the asymmetric behaviors of human and AI agents\nand the dynamic nature of their interactions in the environ-\nment (Goecks et al. 2019), human-AI coordination scenarios\nare inherently complicated (Ghosal et al. 2023; Zhao et al.\n2020). These complexities can intensify the challenge of ob-\ntaining rewards, particularly in tricky environments where\nrewards are sparse. A common approach to mitigate the is-\nsue of sparse reward acquisition in RL is to encourage the AI\nagent to explore its environment more thoroughly (Jaques\net al. 2019; Haarnoja et al. 2018). However, the ingrained\ntrade-off between exploration and exploitation in RL (Sut-\nton and Barto 2018; Mnih et al. 2016; Rashid et al. 2020)\nintroduces a second challenge: the inefficiency of training.\nWhile augmenting exploration can improve the acquisition\nof sparse rewards, it simultaneously reduces exploitation ef-\nficiency that is crucial for training efficiency (Lowe et al.\n2017; Hadfield-Menell et al. 2016; Pathak et al. 2017), there-\nfore problematizing the optimization of human-AI coordina-\ntion.\nTo enhance the effectiveness of obtaining sparse rewards,\nwe design innovative intrinsic rewards that facilitate thor-\nough environmental exploration; whilst to improve the effi-\nciency of the training process, we introduce context-aware\nweights to enhance training efficiency by optimizing the ex-\nploration and exploitation.\nOn the one hand, intrinsic rewards, unlike traditional ex-\ntrinsic rewards obtained from the environment, are usu-\nally derived from agents' observations and can increase the\nlikelihood of identifying sparse state-action pairs for ob-\ntaining sparse rewards. Typical intrinsic rewards, such as\nmaximum entropy (Haarnoja et al. 2018) and causal influ-\nence (Jaques et al. 2019), enhance exploration by focusing\non the actions of the AI agent and its coordinators. How-\never, their effectiveness in sparse human-AI coordination is\nlimited by the high diversity of human actions. While recent\nwork (Zhao et al. 2023) introduces a general approach that\ndoesn't rely on specific human data, the no free lunch theo-\nrems (Wolpert and Macready 1997) suggest that exploiting a\nspecific subclass of problems is necessary for improved per-\nformance (Andrychowicz et al. 2016; Hao et al. 2023a,b).\nIncorporating a logarithmic term of the Kullback-Leibler\n(KL) divergence between the RL policy and the human pol-\nicy as a regularization term has proven effective in enhanc-\ning accuracy in text summarization (Christiano et al. 2017;\nStiennon et al. 2020). Motivated by this strategy, we pro-\npose designing intrinsic rewards tailored to highlight sparse\nstate-action pairs in human-AI coordination, to effectively\nfacilitate the acquisition of sparse rewards.\nOn the other hand, to enhance the training efficiency\nof AI agents, we propose a novel approach that incorpo-\nrates context-aware weights into the reward structure. This\nmethod dynamically balances exploration and exploitation\nby adjusting the focus on intrinsic and extrinsic rewards\nbased on their relevant changes over time. Inspired by adap-\ntive weighting in multi-task deep learning, which modifies\ntask importance based on loss changes (Liu, Johns, and\nDavison 2019; Vaswani et al. 2017), our approach ensures\nthat the IReCa RL algorithm effectively manages explo-\nration with intrinsic rewards, reducing unnecessary explo-\nration and thereby improving training efficiency.\nTo facilitate effective human-AI coordination with sparse\nreward signals and asymmetric behaviors of human and\nAI agents, we propose a novel Intrinsic Reward-enhanced\nContext-aware (IReCa) RL algorithm. The contributions of\nthis work are summarized as follows:\n\u2022 To improve the effectiveness of obtaining sparse rewards,\nwe propose to incorporate intrinsic rewards, supplement-\ning the traditional extrinsic reward, motivating the AI\nagent to explore the environment comprehensively, and\navoiding the omission of any potentially existing sparse\nrewards.\n\u2022 We design innovative intrinsic rewards to prioritize\nsparse state-action pairs associated with sparse rewards,\nthe AI agent pays attention to these valuable parts of the\nenvironment.\n\u2022 To enhance training efficiency, context-aware weights are\nintroduced to optimize the exploration and exploitation\nby dynamically adjusting intrinsic and extrinsic rewards\nbased on the changing context of the episode return,\nthereby improving the overall learning process.\nExperimental results demonstrate that our IReCa RL algo-\nrithm outperforms state-of-the-art algorithms in both accu-\nmulated reward and convergence speed, mapping an innova-\ntive path for effective human-AI coordination."}, {"title": "Human-AI Coordination Scenario", "content": "One of the primary objectives in human-AI coordination is\nto achieve the shared goal through seamless collaboration\nbetween Al agents and human coordinators (Goecks et al.\n2019; Zhao et al. 2020). Our IReCa RL algorithm aims to\nimprove the effectiveness and efficiency of human-AI co-\nordination by enhancing the AI agent's ability to adapt to\nhuman behaviors and dynamic environments.\nOvercooked environment is a widely recognized bench-\nmark for human-AI collaboration scenarios that present\ncomplicated cooking tasks within a limited time, making\nit ideal for studying coordination dynamics. We experi-\nment with two Overcooked layouts (Carroll et al. 2019).\nThe first layout, Cramped Room, poses challenges related\nto low-level coordination due to its confined space, which\noften leads to agent collisions. In contrast, the second lay-\nout, Asymmetric Advantages, necessitates high-level strate-\ngic planning as agents begin in distinct areas with access to\nmultiple cooking stations."}, {"title": "IReCa Reinforcement Learning", "content": "In this section, we first provide an overview of the IReCa\nRL algorithm. Subsequently, we clarify six key definitions\nintegral to IReCa's detailed design. Finally, we present the\ndesign specifics of our IReCa RL algorithm."}, {"title": "IReCa Overview", "content": "As illustrated in Fig. 1, the core concept of our approach\nlies in the design of the IReCa reward, which is composed\nof three key elements: (1) standard extrinsic rewards de-\nrived from interactions with the environment, (2) innova-\ntive intrinsic rewards based on the actions of both AI and\nhuman agents, and (3) context-aware weights that automat-\nically balance exploration and exploitation by dynamically\nadjusting the importance of extrinsic and intrinsic rewards.\nUsing the IReCa reward, the AI agent leverages an RL al-\ngorithm, such as proximal policy optimization (PPO), to\nsmoothly interact and collaborate with human partners. We\nfollow the methodology outlined in(Carroll et al. 2019), em-\nploying a human model pre-trained using a behavior cloning\nalgorithm on human data. The use of an RL algorithm is es-\nsential, as it allows the AI agent to learn from its interactions\nwith both the human coordinator and the environment.\nFor symbol simplicity, the following description is based\non one AI agent coordinating with one human agent, but it\ncan be easily extended to scenarios considering more agents.\nThe IReCa reward of the AI agent is given by\n$r_t = \\kappa_t^E r_t^E + \\kappa_t^{IA} r_t^{IA} +  \\kappa_t^{H} r_t^{H},$\nwhere $r_t^E$ is the extrinsic reward obtained from the envi-\nronment in the t-th timestep, $r_t^{IA}$ and $r_t^{H}$ are a pair of in-\ntrinsic rewards in the t-th timestep, which encourages the\nAI agent to explore the environment comprehensively from\nthe distinct behaviors of AI and its human coordinator. The\n$\\kappa_t^E$, $\\kappa_t^{IA}$, and $\\kappa_t^{H}$ are the context-aware weights in the n-\nth epoch, where n = [t/T], t is the current timestep, T is\na predefined constant representing the number of timesteps\nin each epoch. We note that the use of distinguishable su-\nperscripts for the extrinsic reward and its corresponding\ncontext-aware weight is intentional to emphasize their dis-\ntinct design rationale, which will be elaborated in the fol-\nlowing subsections."}, {"title": "Key Definitions", "content": "In RL research, terms such as episode and epoch are often\nused interchangeably, which can hinder precise understand-\ning. To avoid such confusion and ensure clarity within the\ncontext of our IReCa RL algorithm, it is essential to clearly\ndefine these six key terms we use: horizon, episode, epoch,\nreward, episode return, and long-term return. Establishing\nthese precise definitions will facilitate a more accurate inter-\npretation of our detailed design.\nHorizon refers to the sequence of future timesteps that\nan agent considers when planning its policy. The task is\nterminated if the number of timesteps reaches the horizon\nlength (Sutton and Barto 2018). Episode is defined as a se-\nquence of states, actions, and rewards that concludes when\na terminal state is reached (Achiam and OpenAI 2018). It\nrepresents one complete trial of the task and comprises mul-\ntiple timesteps. The number of timesteps in an episode is\nfewer than or equal to the horizon length. Epoch (indexed\nby n in IReCa) refers to a predefined number of timesteps\nused for training the model. In our approach, the policy is\ntypically updated after each epoch.\nReward defines the instantaneous objective in an RL\nproblem, and is a scalar value provided by the environment\nat each timestep. Episode Return is the undiscounted sum-\nmation of all rewards accumulated during a single episode,\nrepresenting the cumulative reward from the start to the\nend of the episode (Achiam and OpenAI 2018). Long-term\nReturn is defined as the infinite-horizon discounted return,\nwhere future rewards are discounted by a factor $\\gamma$ to account\nfor their present value."}, {"title": "Detailed Design", "content": "Based on the six key concepts clarified above", "by": "n$r_t^E = r_t^{ES"}, "max \\{0, 1 - \\lambda^{EG} \\cdot t\\} r_t^{EG},$\nwhere max{} is the maximum function, and $\\lambda^{EG}$ is a con-\nstant coefficient of the extrinsic stage reward that controls\nthe fading speed of the stage reward.\nBoth the sparse reward and the stage reward are derived\nfrom the environment and represent the combined contri-\nbutions of both the AI and human agents. Following the\napproach in (Carroll et al. 2019), these rewards can be ex-\npressed as:\n$r_t^{ES} (k) = \\sum_k R^{ES} (o_t(k), a_t(k)),$\nand\n$r_t^{EG} (k) = \\sum_k R^{EG} (o_t(k), a_t(k)),$\nwhere $R^{ES} (.)$ and $R^{EG} (.)$ denote the sparse and stage re-\nward functions, respectively, and $o_t(k)$ and $a_t(k)$ repre-\nsent the observation and action of the k-th agent at the t-th\ntimestep.\nIntrinsic Reward While stage rewards can assist in this\nprocess, they also pose the risk of leading the learning pro-\ncess into local optima due to excessive exploitation. To\ncounteract this limitation, intrinsic rewards serve as a pow-\nerful mechanism to enhance the exploration capabilities of\nAI agents (Song et al. 2020; Du et al. 2024).\nIn human-AI coordination scenarios with sparse rewards,\nto effectively collaborate with human coordinators who be-\nhave diversely and sparsely, a focus on the critical state-\naction pairs for obtaining those sparse rewards is necessary.\nDrawing inspiration from the approach in (Stiennon et al.\n2020), which introduced a regularization term that improved\nupon state-of-the-art reward functions, we propose a novel\nmethod. Traditional regularization terms such as entropy,\nKL divergence, and cross-entropy often underestimate the\nimpact of these sparse state-action pairs due to their low\nprobability of occurrence. To address this, we introduce a\nlogarithmic term to emphasize the significance of these low-"]}