{"title": "LMUNIT: Fine-grained Evaluation with Natural Language Unit Tests", "authors": ["Jon Saad-Falcon", "Rajan Vivek", "William Berrios", "Nandita Shankar Naik", "Matija Franklin", "Bertie Vidgen", "Amanpreet Singh", "Douwe Kiela", "Shikib Mehri"], "abstract": "As language models become integral to critical workflows, assessing their behavior remains a fundamental challenge \u2013 human evaluation is costly and noisy, while automated metrics provide only coarse, difficult-to-interpret signals. We introduce natural language unit tests, a paradigm that decomposes response quality into explicit, testable criteria, along with a unified scoring model, LMUNIT, which combines multi-objective training across preferences, direct ratings, and natural language rationales. Through controlled human studies, we show this paradigm significantly improves inter-annotator agreement and enables more effective LLM development workflows. LMUNIT achieves state-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and competitive results on RewardBench. These results validate both our proposed paradigm and scoring model, suggesting a promising path forward for language model evaluation and development.", "sections": [{"title": "1 Introduction", "content": "The evaluation of generative language models remains one of the most fundamental challenges in natural language processing (Jones and Galliers, 1995; Deriu et al., 2021; Smith et al., 2022; Chang et al., 2024) it determines how we measure progress and validate improvements, and it ultimately shapes the field's trajectory. As these models transition from research prototypes to production systems, real users increasingly rely on them for critical workflows (Lin et al., 2024), creating an urgent need for evaluation methods that identify response strengths/weaknesses, ensure reliability, detect context-dependent failures, and prevent costly regressions. Yet current approaches fall short: human evaluation is expensive, noisy, and struggles to discern subtle differences among top-performing models (Hosking et al., 2023; Clark et al., 2021; Karpinska et al., 2021), while automated metrics compress the rich, multi-dimensional nature of response quality into coarse, difficult-to-interpret scores (Stent et al., 2005; Liu et al., 2016) that rely on implicitly learned, often biased evaluation criteria (Dubois et al., 2024a; Shankar et al., 2024; Zhang et al., 2024a). As models become more capable and deeply integrated into essential workflows, it is imperative that our evaluation methodologies evolve in tandem, empowering LLM practitioners to reliably detect subtle failures, meaningfully distinguish among top-performing systems, and generate actionable insights that drive sustained improvements in capabilities.\nWe focus on one of the most critical challenges in evaluating language models: measuring response quality. Defining \u201cresponse quality\" is inherently complex what constitutes a good response depends on multiple factors, including factual accuracy, logical coherence, and alignment with user-specific objectives, among other dimensions, all of which vary by domain, application, style, and context (Mehri and Eskenazi, 2020a; Ye et al., 2023; Krishna et al., 2023). Existing evaluation approaches struggle with this complexity, which results in an unreliable evaluation signal: (1) reference-based comparisons are ill-suited to open-ended scenarios and subtle failure modes where no single \"correct\u201d response exists (Liu et al., 2016; Lowe et al., 2017), (2) human evaluations become inconsistent and costly as models grow more capable and errors subtler (Walker et al., 2007; Pan et al., 2024; Christiano et al., 2023), and (3) preference models or prompted LLM judges compress nuanced assessments into opaque, coarse-grained metrics that are difficult to interpret or steer (Dubois et al., 2024b; D'Oosterlinck et al., 2024; Singhal et al., 2023). To address these limitations, we propose natural language unit tests, a paradigm that decomposes response quality into explicit, testable criteria that humans can define, refine, and guide over time (Figure 1). While this approach enhances transparency and adaptability, it introduces a key technical challenge: how to reliably score and integrate these fine-grained assessments in a manner that remains aligned with human values.\nBuilding an effective scoring model for unit tests presents a significant challenge: it must accurately evaluate a wide range of criteria \u2013 ranging from broad notions of quality to detailed rubrics that capture intricate, context-specific requirements. Existing approaches each address part of the problem: prompted LLM judges can be instructed to consider certain criteria (Liu et al., 2023), but their accuracy is limited by generic instruction-following abilities and the inability to learn directly from preference data (Wang et al., 2024b; Zhong et al., 2022); preference models, while closely aligned with human judgments, lack promptability and struggle to handle more granular, human-defined criteria (Singhal et al., 2023; Lambert and Calandra, 2023). To address these challenges, we propose LMUNIT, a unified modeling approach"}, {"title": "2 Related Work", "sections": [{"title": "2.1 Evaluation of Generative Language Models", "content": "As LLM development continues to progress, robust and interpretable evaluation methodologies become increasingly critical. While human evaluation is often considered the gold standard (Ouyang et al., 2022; Touvron et al., 2023), its cost and scalability limitations (Hosking et al., 2023; Schoch et al., 2020) have driven the development of automated approaches. These include word overlap metric (Papineni et al., 2002; Lin, 2004), embedding-based scoring methods (Yuan et al., 2021; Zhang et al., 2019), model-based evaluations (Lowe et al., 2017; Mehri and Eskenazi, 2020b; Zhong et al., 2022; Saad-Falcon et al., 2023), reward modeling (Christiano et al., 2017; Askell et al., 2021; Kim et al., 2023), and LM judges (Zheng et al., 2023; Liu et al., 2023; Es et al., 2023; Ravi et al., 2024; Kim et al., 2024a; Li et al., 2024b). However, automated methods face significant limitations. They often lack interpretability and can exhibit biases that diverge from human evaluations (Shankar et al., 2024; Wang et al., 2023b; Chaudhari et al., 2024). Recent work has defined more fine-grained LM evaluators (Ye et al., 2023; Wang et al., 2024b; Ribeiro et al., 2020; Lin and Chen, 2023; Cook et al., 2024) and unified different paradigms within LM evaluation (Wang et al., 2024b; Kim et al., 2024c; Wu et al., 2023). Furthermore, for code generation tasks, LLM-based unit test generation has led to performance improvements by comparing different responses against compiler-compatible synthetic unit tests (Chen et al., 2022; Yuan et al., 2023; Saad-Falcon et al., 2024)."}, {"title": "2.2 LM Judges", "content": "LLMs can be prompted to evaluate responses without additional training, showing high correlation with human ratings (Liu et al., 2023; Wang et al., 2023a; Fu et al., 2023; Chiang and Lee, 2023; Es et al., 2023; Kocmi and Federmann, 2023). While some approaches focus on in-context examples and evaluation instructions (Fu et al., 2023), others leverage chain-of-thought prompting (Liu et al., 2023) or fine-tune specialized judges (Saad-Falcon et al., 2023; Tang et al., 2024). However, these approaches face key limitations: poor generalization across evaluation tasks (Es et al., 2023; Saad-Falcon et al., 2023; Ravi et al., 2024) and systematic biases in position, verbosity, and self-preference (Chen et al., 2024; Pan et al., 2024; Zheng et al., 2023; Koo et al., 2023)."}, {"title": "2.3 Reward Models", "content": "Reward models have gained widespread adoption for evaluating and aligning language models (Bradley and Terry, 1952; Christiano et al., 2017; Liu and Zeng, 2024). However, these models face fundamental challenges: low inter-annotator agreement in human preference data (e.g., 65% - 75% in early RLHF papers) (Askell et al., 2021; Ouyang et al., 2022; Wang et al., 2024a), preferences can be noisy and inconsistent (Dubois et al., 2024b), and models often learn spurious correlations like favoring longer responses (Lambert and Calandra, 2023; Singhal et al., 2023; Dubois et al., 2024a). Recent advances have shown promise in addressing these limitations: Helpsteer-2 (Wang et al., 2023c) demonstrated improved performance through higher-quality preference data collection, while GenRM-COT (Zhang et al., 2024b) leveraged chain-of-thought reasoning for more reliable evaluation. Despite these improvements, challenges with reward underspecification and alignment persist (Eisenstein et al., 2023; Chaudhari et al., 2024)."}, {"title": "2.4 Fine-Grained Evaluators", "content": "Breaking down complex evaluation problems into simpler components has been a foundational principle in NLP evaluation (Walker et al., 2000), and remains central to solving challenging problems with language models (Saha et al., 2024). While some approaches use fixed evaluation dimensions (Liu et al., 2016; Lowe et al., 2017; Zhong et al., 2022), the emergence of more flexible language models has enabled more dynamic, fine-grained evaluation criteria (Mehri and Eskenazi, 2020a; Lin and Chen, 2023; Ye et al., 2023; Kim et al., 2024b). However, pre-defining criteria likely won't generalize well to real-world settings (Shankar et al., 2024). Most similar to our proposed paradigm is CheckList (Ribeiro et al., 2020), which introduced a structured, behavioral testing paradigm for NLP models, and TICK (Cook et al., 2024), which employed model-generated test criteria to demonstrate the benefits of decomposition. Our work builds on these foundational concepts while extending them in key ways: we train a dedicated scoring model that synthesizes multiple training signals for more accurate and scalable fine-grained evaluation, we conduct broader and more rigorous evaluations across diverse benchmarks, and we validate our paradigm through human studies."}, {"title": "2.5 Unified Evaluators", "content": "Recent work has focused on unifying different evaluation paradigms. DJPO (Wang et al., 2024b) improves human correlation by training LM judges through preference optimization (Rafailov et al., 2023), while Prometheus (Kim et al., 2024a,c) combines direct assessment and pairwise ranking capabilities through model weight merging. These approaches, along with fine-grained reward functions (Wu et al., 2023), show promise in both human and automatic evaluations.\nOur work, LMUNIT, extends these unified approaches while addressing their key limitations. While previous work has made progress in combining different evaluation paradigms, they still face challenges in interpretability, generalization, and fine-grained control. LMUNIT addresses these"}]}, {"title": "3 LMUNIT Methodology", "content": "To enable reliable scoring of natural language unit tests, we develop LMUNIT, a unified modeling approach that combines multi-objective training with natural language rationale generation. The key challenge lies in effectively integrating diverse training signals while maintaining both high accuracy and interpretable outputs. Here we detail our approach to addressing this challenge through careful problem formulation, synthetic data generation, and our training methodology."}, {"title": "3.1 Problem Formulation", "content": "The core challenge in language model evaluation is developing scoring models that can reliably evaluate responses against specific criteria while providing interpretable reasoning. Our formulation centers on unit tests: given a unit test $u$, prompt $p$, and response $r$, we train models to generate both rationales and scores through the mapping $f(u, p, r) \\rightarrow \\text{rationale, score}$.\nOur approach builds on two existing forms of evaluation data: direct rating data $(p, r) \\rightarrow \\text{score}$ and preference data $(p, r_1, r_2) \\rightarrow \\text{preference}$. We extend these into unit test-based formats:\n\nUnit test direct data: $(u, p, r) \\rightarrow \\text{score}$ or $(u, p, r) \\rightarrow \\text{rationale, score}\nUnit test preference data: $(u, p, r_1, r_2) \\rightarrow \\text{pref}$ or $(u, p, r_1, r_2) \\rightarrow \\text{rationale}_1, \\text{rationale}_2, \\text{pref}$\n\nThis formulation leverages two complementary data sources: naturally occurring preference and rating data to capture human preferences and calibrate against absolute quality scales, alongside synthetic data that enables fine-grained evaluation of specific criteria with interpretable rationales. At inference time, LMUNIT can flexibly operate with or without rationale generation."}, {"title": "3.2 Synthetic Data Pipeline", "content": "Our data generation pipeline operationalizes the unit test formulation through three key stages, producing examples scored on a 1-5 scale where higher scores indicate better satisfaction of the criteria:\n\nUnit Test Generation: For each prompt, we generate diverse unit tests targeting fine-grained quality criteria. To encourage focus on response-specific details, we optionally provide one or two responses during generation. We also maintain a set of coarse-grained global tests (see Table 10 for details) to ensure broad coverage of general quality dimensions."}, {"title": "2. Contrastive Response Generation", "content": "For each (u, p, r) triplet, we generate contrastive responses that vary systematically in how well they satisfy the unit test criteria. This creates rich training signal for learning fine-grained quality distinctions."}, {"title": "3. Rationale and Score Generation", "content": "For a subset of examples, we generate chain-of-thought rationales that explicitly reason through the evaluation criteria. Each rationale concludes with a score that must align with any existing seed data scores to maintain consistency.\nWe seed our synthetic data pipeline with prompts, responses, tests and scores from diverse sources including Nectar (Zhu et al., 2024), Prometheus (Kim et al., 2024a), Tulu3 (Lambert et al., 2024a), Complex Instructions (He et al., 2024), Infinity-Instruct (of Artificial Intelligence, BAAI), and HelpSteer2 (Wang et al., 2024d,c)."}, {"title": "3.3 Training", "content": "LMUnit combines the strengths of generative judge models and classifier-based reward models through a unique multi-objective training approach. Given a unit test u, prompt p, and response r, the model outputs a sequence of rationale tokens $rat = (rat_1,...,rat_\\tau)$ followed by a score token s. The probability distribution over possible score values $k \\in 0, 1,...,6$ is:\n$P(s=k | u, p, r, rat) = \\text{softmax}(h_\\tau W_s)_k$\nWe compute a continuous score prediction through a weighted sum:\n$\\hat{y} = \\sum_{k=0}^6 k \\cdot P(s=k | u, p, r, rat)$\nThe training objective combines three losses. First, SFT loss on the rationale and score tokens:\n$L_{sft} = - \\sum_{t=1}^T \\text{log} P(x_t | u, p, r, x_{0:t})$\nwhere $x_{1:t}$ represents tokens in both rationale and score sequences.\nSecond, MSE loss on the continuous score prediction:\n$L_{mse} = (y - \\hat{y})^2$\nThird, preference loss:\n$L_{pref} = -\\text{log}(\\sigma(\\hat{y}_1 - \\hat{y}_2)) \\cdot 1_{\\{pref=y_1\\}} \\\\\n-\\text{log}(\\sigma(\\hat{y}_2 - \\hat{y}_1)) \\cdot 1_{\\{pref=y_2\\}} \\\\\n+(\\hat{y}_1 - \\hat{y}_2)^2 \\cdot 1_{\\{pref=tie\\}}$\nHere, $\\sigma$ is the sigmoid function. The final loss is a weighted combination:\n$L = \\alpha L_{sft} + \\beta L_{mse} + \\gamma L_{pref}$"}, {"title": "3.4 Post-Training of Rationales", "content": "While our initial model learns to generate rationales through imitation learning, there is no guarantee that these rationales actually improve scoring performance. We address this by collecting pairs of desirable and undesirable rationales for direct preference optimization (Rafailov et al., 2023), training the model to prefer rationales that lead to correct scoring over those that don't. We employ several strategies for collecting these pairs. Through the Refined strategy, we collect on-policy rationales from our trained model and use the teacher to refine them through revisions (D'Oosterlinck et al., 2024) that improve scoring accuracy. With the Harmonized strategy, we provide the teacher with two rationales from our trained model from a preference pair and prompt the teacher to harmonize the rationales, ensuring that the independent reasoning traces are consistent with the relative quality of the two samples. In the Teacher-based strategy, we sample multiple rationales from the teacher on samples with known scores and use rationales with correct score outcomes as chosen samples and rationales with incorrect score outcomes as rejected. We compare these approaches in Table 4."}, {"title": "3.5 Bayesian Optimization of Global Unit Tests", "content": "Natural language unit tests decompose evaluation into fine-grained, interpretable criteria through K global tests that assess dimensions like accuracy, safety, and coherence. The aggregation of these individual assessments into an overall score is crucial for valid evaluation. While the standard approach would be to use uniform weighting of test scores, we investigate whether this can be improved through learned weights. Using Bayesian optimization over human preference data, we learn optimal weights $w_1, ..., w_k$ that maximize alignment between weighted test scores and human judgments. This optimization process starts from uniform weights and iteratively updates them based on agreement with held-out human preferences, demonstrating the potential of principled aggregation strategies."}, {"title": "4 Experiments", "content": "We conduct extensive experiments to evaluate LMUNIT and the natural language unit test paradigm. First, we evaluate the performance of LMUNIT on several evaluation benchmarks, comparing to LLMs as judges, reward models, and trained evaluation models. Next, we perform ablations to understand the impact of different methodologies, including loss functions and data mixture choices. Finally, we examine improving rationales through post-training and analyze the impact of decomposition through several unit test strategies."}, {"title": "4.1 Experimental Setup", "sections": [{"title": "4.1.1 Model Configuration and Training Data", "content": "Our training data encompasses a diverse mix of preference judgments, direct scores, and rationales across multiple sources: (i) HELPSTEER 2 (50K pairs with ratings spanning five dimensions), (ii) PROMETHEUS (10K unpaired samples with ratings), (iii) SYNTH NON-RUBRIC (11K pairs with ratings and rationales), (iv) SYNTH RUBRIC (13K unpaired samples with ratings and rationales).\nWe train several variants of LMUNIT initialized from instruction-tuned LLaMa-3.1 models (8B, 70B). We train our models for 2000 steps using fixed weights (i.e., $\\alpha=\\beta=\\gamma=1$) for the different loss components, with a 5x loss multiplier applied to the rationale samples. The training uses the Adam optimizer (Kingma and Ba, 2017) with a learning rate of le-6 and a cosine learning rate scheduler, using a batch size of 64 and a sequence length of 8K."}, {"title": "4.1.2 Evaluation Benchmarks", "content": "We evaluate our models on six evaluation benchmarks spanning different capabilities and evaluation criteria:\n\nRewardBench (Lambert et al., 2024b): A benchmark of pairwise model outputs across chat, reasoning, and safety domains. We measure agreement with human preference judgments.\nLFQA (Xu et al., 2023): A benchmark of long-form question answering responses. We measure agreement with expert preference judgments.\nBiGGen Bench (Kim et al., 2024b): A comprehensive benchmark spanning 77 tasks across instruction-following, content refinement, grounding, and tool usage. We measure correlation with human assessment scores.\nFLASK (Ye et al., 2023): An evaluation framework covering 12 skills across logical thinking, knowledge application, problem handling, and user alignment. We measure correlation with human assessment scores.\nInfoBench (Qin et al., 2024): A collection of instruction-following tasks. Using the expert-validated split, we measure binary classification accuracy against expert consensus."}]}, {"title": "4.2 Key Results", "content": "Our models demonstrate strong performance across diverse evaluation settings (Table 1). On direct assessment tasks, LMUNIT achieves state-of-the-art results with correlations of 72.03 on FLASK and 67.69 on BiGGen-Bench, where fine-grained evaluation is particularly important. In aggregate, LMUNIT achieves strong overall performance with scores of 79.74 (eight weighted global unit tests) and 79.29 (single unit test), outperforming general-purpose models like GPT-4 (78.29) and Claude-3.5 Sonnet (77.78). Even our smaller LMUNITLLAMA3.1-8B variant remains highly competitive with a 74.10 average score. For pairwise ranking tasks, using unweighted global unit tests slightly decreases overall performance to 78.78 (-0.96), but LMUNIT remains stronger than all other baselines. We recover this minor performance loss through Bayesian optimization of the global unit test weights while reaching 93.45 on RewardBench (+2.91) - though we note this weighting is learned on a subset of RewardBench itself, analogous to tuning hyperparameters on the test set (following a similar experimental setup as Wang et al. (2024d)). A more rigorous analysis using a proper held-out evaluation set is provided in Section 4.3.4, confirming the generalization of this method. These strong results across direct assessment, classification, and pairwise ranking tasks validate the effectiveness of our synthetic data pipeline, training setup, and unified scoring methodology, establishing LMUNIT as a state-of-the-art model for reliable evaluation."}, {"title": "4.3 Ablation Studies", "sections": [{"title": "4.3.1 Impact of Loss Functions", "content": "Our ablation studies examine the complementary benefits of different training objectives: SFT, MSE, and preference loss. As shown in Table 2, each additional loss provides measurable improvements across our evaluation benchmarks (+0.5). While we observe incremental gains from each additional loss, there is a meaningful improvement across different evaluation scenarios, validating the effectiveness of our multi-objective training approach."}, {"title": "4.3.2 Data Mixture Effects", "content": "We analyze how different compositions of training data affect LMUNIT's performance to identify the most effective mixture for robust evaluation capabilities. As shown in Table 3, rubric data is essential for strong performance on fine-grained direct assessment and that our synthetic data pipeline provides dramatic performance gains (+3.52) when synthetic rubric data is incorporated. We also observe that non-rubric synthetic data is most effective as preference pairs (+4.04) rather than direct scoring data (-2.75), likely due to the improved contrastive signal."}, {"title": "4.3.3 Impact of Rationales", "content": "Moving beyond simple imitation learning of rationales, we examine strategies to optimize rationale generation for better evaluation. As shown in Table 4, training with rationales improves model performance even when rationales are not used at test time (+0.2). While including rationales during inference initially leads to lower scores, our post-training optimization through DPO helps recover performance, with teacher-based pairs providing the largest gains (+1.1)."}, {"title": "4.3.4 Unit Test Decomposition Analysis", "content": "Our experiments with different unit test strategies on RewardBench (Table 5) reveal two key findings. First, global-level tests significantly outperform query-level tests across all categories, with section-level learned weights achieving the strongest results (+2.4 over unweighted aggregation). Second, the performance of fine-grained query-level tests degrades substantially, particularly on harder examples, though this can be partially mitigated by placing greater weight on earlier tests (+1.5).\nThese results highlight both the promise and challenges of our approach: while global unit tests provide a robust foundation for evaluation, developing effective fine-grained testing criteria remains difficult. The success of weighted global unit tests, coupled with the challenges of query-level decomposition, suggests an important direction for future work in developing more sophisticated test generation and aggregation strategies."}]}, {"title": "5 LMUNIT Human Subject Studies", "content": "We conducted two studies to validate key claims about natural language unit tests: (1) Whether this paradigm, implemented through LMUNIT, provides concrete advantages over traditional LM judges for developers working on real systems, and (2) Whether decomposing evaluation into explicit criteria can improve the quality of human preference data."}, {"title": "5.1 Case Study with LLM Developers", "content": "To evaluate whether decomposed evaluation helps developers better understand and improve language models, we conducted a controlled study with 16 LLM researchers and engineers. Participants were asked to analyze model outputs from Arena Hard Auto (Li et al., 2024a) and a multi-domain commercial benchmark using both LMUNIT and traditional \"LLM as a Judge\u201d.\nLMUNIT enabled substantially more detailed analysis: participants identified 157% more response attributes (10.8 vs 4.2) and 131% more error modes (7.4 vs 3.2), rating both as significantly more important than those found through LM judges. In practice, LMUNIT led to concrete improvements: 13 of 16 researchers made targeted improvements to data selection and preprocessing after identifying critical error modes, with six reporting 10+ point gains in instruction-following and reasoning tasks. Detailed examples and analysis are provided in Appendix A.2."}, {"title": "5.2 Reducing Noise in Human Evaluation", "content": "Human preference data is crucial for training reward models (Christiano et al., 2017; Askell et al., 2021). However, inter-annotator agreement is often low (Wang et al., 2024a), with annotators struggling to weigh different factors consistently and give reliable signal (Howcroft et al., 2020). Since reducing task ambiguity has been shown to help improve agreement (Novikova et al., 2018; Huynh et al., 2021; Rottger et al., 2022), we investigated the benefits of decomposing evaluation into explicit criteria. We conducted an experiment with 15 experienced annotators on expressing judgements with 20 queries, comparing three approaches: unstructured preference judgments (Control), standardized evaluation criteria (Specification), and unit test-based evaluation (Unit Test). The Control group selected their preferred response with no additional guidance, the Specification group assessed each response against a five-point quality specification before selecting their preferred response, and the Unit Test group answered gold-standard targeted unit tests before picking. The gold-standard unit tests were pre-written by trained human annotators to ensure quality. More details on the human-written unit tests are in Appendix A.1.\nAs shown in Figure 3, the Control group showed low inter-annotator reliability (Fleiss' Kappa = 0.04), while the Unit Tests group achieved substantially higher agreement (Fleiss' Kappa = 0.52), demonstrating that structured decomposition significantly improves consistency in human evaluation. Annotators chose their preferred response after completing the unit tests and 89% of the time they selected the response with the largest number of satisfied unit tests. This further shows that answering unit tests guided their preference decisions."}, {"title": "6 Discussion", "content": "Our experiments and analyses reveal several key insights about the effectiveness of our unit test-based evaluation framework and highlight important directions for future work:\n\nLMUnit Shows Benefits of Unified Training: Our empirical results validate the benefits of a unified scoring approach through three key findings: combining multiple training objectives improves performance across all evaluation settings (Table 2), incorporating diverse data types enhances model capabilities (Table 3), and LMUNIT's approach achieves state-of-the-art results on fine-grained evaluation benchmarks like FLASK and BiGGen-Bench (Table 1). These results suggest significant untapped potential in synthesizing different sources of evaluation signal \u2013 from human preferences and ratings to targeted synthetic data - particularly for fine-grained assessment tasks.\nUnit Tests Enable Rich Human-in-the-Loop Evaluation: Language model evaluation frameworks should enable precise human steering while reducing noise and manual effort. Our results show this paradigm achieves both goals: structured criteria dramatically improve evaluation consistency and inter-annotator agreement (Figure 3), while offering multiple meaningful intervention points. Humans can write or refine test criteria, optimize test weights (Table 5), and guide development through decomposed feedback- leading to significantly more detailed error analysis in practice (subsection 5.1). This suggests unit tests can enable deeper, more reliable human-AI collaboration in evaluation.\nRationale Post-Training Improves Task Performance: A fundamental challenge in language models is developing genuine reasoning capabilities rather than simply learning to imitate human-like explanations. While training models to generate rationales through supervised learning can produce plausible-sounding explanations, this doesn't necessarily improve their underlying capabilities. Our work demonstrates two key insights about moving beyond imitation: first, training with rationales improves model performance even when not generating them at inference time (Table 4), and second, post-training optimization of rationales for task performance rather than imitation leads to further gains. This suggests a promising direction for developing better reasoning capabilities: using rationales not just as outputs to mimic but as a trainable intermediate step that can improve task performance while maintaining interpretability and enabling human feedback. Beyond LMUNIT, this approach can be extended to improve general-purpose model reasoning by optimizing rationales for downstream task performance rather than merely imitating ground-truth rationales.\nQuery-Level Unit Test Creation Remains Challenging: While our work advanced scoring and evaluation methodology, generating effective query-specific unit tests proved difficult. Global-level unit tests with learned weights significantly outperform query-level unit tests (Table 5), highlighting the need for better test generation approaches. Future work should explore end-to-end training of test generation, evaluate human-created tests at scale, and investigate when fine-grained decomposition justifies its complexity. These findings collectively point to both the promise and challenges of the unit testing paradigm for language model evaluation. The strong performance of LMUNIT demonstrates the potential of unified training approaches, while our human studies show how structured evaluation can enable more reliable and meaningful human oversight. Though challenges remain in test generation and optimal decomposition strategies, our results suggest this paradigm offers a practical path toward more reliable, interpretable, and human-aligned evaluation of language models."}, {"title": "7 Conclusion", "content": "This paper introduces natural language unit tests, a paradigm for language model evaluation that enables precise assessment through explicit, testable criteria. To implement this paradigm effectively, we develop LMUNIT, a unified scoring model that combines multi-objective training across preferences, direct ratings, and natural language rationales to achieve state-of-the-art performance on major evaluation benchmarks. Our results validate both the broader paradigm of decomposed evaluation and our novel scoring methodology. Looking ahead, this work opens several promising research directions: deeper integration of human feedback loops, enhanced scoring models with improved reasoning capabilities, and end-to-end training of unit test generation and scoring."}, {"title": "A Appendix", "sections": [{"title": "A.1 Assisting Humans with Preference Annotations", "content": "This subsection of the Appendix contains additional information on the experiment to reduce noise in human evaluation of preference data."}, {"title": "A.1.1 Human-Written Unit Tests", "content": "We conducted a pilot study to develop gold-standard unit tests for 20 queries, which were later used to reduce noise in preference data. Four experienced annotators used a Google Sheets interface to create 4-8 unit tests per query. These tests were designed to verify that model responses were both accurate and grounded in the retrieved documents. Annotators wrote specific criteria defining the expected behavior and content that would constitute a complete, well-grounded response to each query."}, {"title": "A.2 LMUNIT Case Study", "content": "This section contains additional details regarding the LMUNIT case study described in Section 5. For the annotation regarding error attributes and failure modes", "LMUNIT": "n\nMotivating LM System Decisions: \"We had suspected for a while that some of our training data was not diverse enough", "Feedback": "With LM judges", "Alignment": "For our project, we noticed a frustrating gap between LM judge evaluations and the feedback from our annotators. The LM judges would pass responses that skipped crucial reasoning steps as long as the final answer was correct but annotators rejected responses for lacking logical progression. After switching to LMUNIT, the alignment with the annotators improved significantly. LMUNIT unit tests flagged responses that missed intermediate steps, just like the annotators. This allowed us to retrain the model with more targeted feedback, leading to better performance in tasks requiring step-by"}]}]}