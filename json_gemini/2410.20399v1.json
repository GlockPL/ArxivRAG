{"title": "Thunder Kittens: Simple, Fast, and Adorable AI Kernels", "authors": ["Benjamin F. Spector", "Simran Arora", "Aaryan Singhal", "Daniel Y. Fu", "Christopher R\u00e9"], "abstract": "The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet their theoretical performance thresholds, even on well-established operations like linear attention. The diverse hardware capabilities of GPUs might suggest that we need a wide variety of techniques to achieve high performance. However, our work explores whether a small number of key abstractions can drastically simplify the process. We present THUNDERKITTENS (TK), a framework for writing performant AI kernels while remaining easy to use and maintain. Our abstractions map to the three levels of the GPU hierarchy: (1) at the warp-level, we provide 16x16 matrix tiles as basic data structures and PyTorch-like parallel compute operations over tiles, (2) at the thread-block level, we provide a template for overlapping asynchronous operations across parallel warps, and (3) at the grid-level, we provide support to help hide the block launch and tear-down, and memory costs. We show the value of TK by providing kernels that match or outperform prior kernels for a range of AI operations. We match CuBLAS and FlashAttention-3 on GEMM and attention inference performance and outperform the strongest baselines by 10-40% on attention backwards, 8\u00d7 on state space models, and 14x on linear attention.", "sections": [{"title": "Introduction", "content": "AI is bottlenecked by the problem of efficiently mapping AI architectures onto accelerated GPU hardware. There has been a Cambrian explosion of ML architectures [21, 25]; however, the performance of these architectures remains substantially below their theoretical potential, despite substantial effort to develop kernels, or GPU implementations. Notably, kernel support has been poor even for softmax attention, which is used throughout industry. FlashAttention-2 [12] suffered a 47% performance degradation when translated to the H100 GPU, and it took over two years from the release of the H100 to develop FlashAttention-3 [37].\nWe are inspired by several approaches to supporting the development of AI kernels. Ideally, we would have a framework that supports high performance for a breadth of primitives, while being easy to use, learn from, and maintain. High performance C++ embedded libraries like NVIDIA CUTLASS/CuTe [29] contain a myriad of nested templates, while compiler based approaches like Triton [39] provide users with simpler interfaces, but fewer optimizations. We ask how broad and fast we can go by choosing a small and opinionated set of abstractions.\nThe main vector of growth for accelerated compute is in specialized matrix multiply units. On the NVIDIA A100 and NVIDIA H100 GPUs, BF16 tensor cores represent 16\u00d7 the FLOPs available relative to general-purpose BF16 / FP32 compute. Consequently, any high performance framework must prioritize keeping tensor cores at high utilization whenever possible. However, all kernels have other operations too (like memory loads or the softmax in attention), and it is crucial to minimize the overhead of non-tensor core operations. This proposition is at the heart of our approach.\nTo understand the complexities and opportunities in building a simple, yet high performance framework, we examine a simplified model of GPU parallelism, further detailed in section 2.1"}, {"title": "GPU fundamentals", "content": "GPU tasks are divided into small programs called kernels. A kernel loads data from high bandwidth memory (HBM), performs work on it, and writes the outputs back to HBM before concluding. Before we explain THUNDERKITTENS's abstractions, we provide background on GPU parallelism including warp, block and grid-level parallelism. We follow NVIDIA's terminology and focus on the H100 SXM GPU, though the principles apply across GPU vendors and generations."}, {"title": "GPU hierarchy", "content": "The GPU software hierarchy closely follows its physical hardware hierarchy (Figure 3). Here, we illustrate several of its most important components and aspects.\n1. Warps consist of groups of 32 nearby threads that operate on data in small but fast register memory. These instructions run on various physical execution units, which are specialized for different compute operations (below) and different threads can occupy different units simultaneously:\n(a) Load and store units, to bring data into and out of registers. Advanced GPUs have also introduced dedicated hardware acceleration for bulk loads and stores.\n(b) General purpose compute pipelines, such as ALU for max, min, FMA for multiplies and adds, and XU for complex operations like exp. Throughput differs across the pipelines.\n(c) Accelerated matrix multiply hardware (tensor cores), which have most of the GPU compute.\n2. Thread blocks are groups of warps which together execute a kernel on a physical core, called a streaming multiprocessor (SM). Although each SM has just four physical execution units, up to 64 software warps can simultaneously run on it (called \"occupancy\"). These collocated warps often contend on hardware resources: registers, shared memory, issue slots, and compute pipelines, but together they can help keep many work streams running at the same time within each execution unit. Warps synchronize at barriers, during which they cannot issue new work.\nImportantly, warps within the same block can quickly communicate through special shared memory (SMEM, 227 KB, 33 TB/s). To improve bandwidth, SMEM is grouped into 32 physical \"banks\" of memory, which can serve memory simultaneously. However, if different threads try to access the same bank at the same time (called a bank conflict), their accesses must be serialized, which both increases access latencies and reduces available bandwidth. Hopper has limit of 255 registers per thread and attempts to request more, results in spills to the L1 cache. SMEM can be reallocated as an L1 cache for fast access to frequently used memory like spilled registers.\n3. Grids of multiple thread blocks are launched to run the kernel. The H100 SXM GPU has 132 physical SM's which can run thread blocks at the same time. Although SM's are capable of collocating multiple thread blocks, most AI kernels can achieve high performance by simply collocating more warps within a single thread block (increasing the occupancy).\nThread blocks on the same GPU share common memory resources: large but slow high-bandwidth memory (80 GB, 3 TB/s), which has both the greatest latency and least bandwidth of all GPU memory, and a smaller but faster L2 cache (50 MB, 12 TB/s).\nThere are overheads to scheduling blocks. First, the block launch incurs setup costs and although this cost must be paid at least once at the initial kernel launch, kernels that continuously launch many large blocks can incur further costs. Second, there are tail effect costs if the grid is sized poorly. If a kernel of 133 blocks is executed on an H100 with 132 physical SMs, the kernel would require two waves to execute, the first with full efficiency, and the second with < 1% efficiency."}, {"title": "Cost model", "content": "Beyond reducing the total amount of work, the other key way to reduce execution time is to overlap multiple kinds of work at once. Summarizing the above components, we provide a simplified cost model for GPU parallelism. We break down the overall kernel execution time $C_{overall}$ as:\n$C_{overall} = max(C_{HBM}, C_{L2}, C_{L1}, C_{Shared}, C_{Tensor}, C_{ALU}, C_{FMA}, C_{XU}) + C_{Setup} + C_{Sync}$\nwhere memory costs are a combination of the latency and bandwidth, and compute costs are a combination of latency and throughput.\nThis model represents the ideal case of perfect overlapping between memory, compute, and tensor core costs. A kernel's actual performance will lie between the max and the sum of these components, depending on the workload properties (i.e., some operations are inherently sequential), as well as the efficiency of its implementation. Nonetheless, our explorations will be guided by trying to (1) reduce these individual costs, and (2) improve their collective overlapping."}, {"title": "GPU programming frameworks", "content": "We are inspired by a number of related efforts to simplify the development of AI kernels, such as NVIDIA CUTLASS/CuTe [29] and Triton [39]."}, {"title": "ThunderKittens", "content": "We present THUNDERKITTENS (TK), a framework designed to simplify the development of high-performance AI kernels while leveraging the full capabilities of modern GPUs. This section (1) introduces our key programming abstractions and (2) shows how they can help developers navigate the tradeoffs between different types of parallelism. Section 3.1 focuses on warp level, Section 3.2 on thread block level, and Section 3.3 on grid level parallelism.\nAs running examples in this section, we show how TK helps optimize attention [41] and GEMM kernels. Section 4 demonstrates how the principles yield performant kernels for a breadth of AI operations (e.g., attention variants, convolution, SSM, rotary)."}, {"title": "Warp parallelism with familiar data structures and operations", "content": "At its core, THUNDERKITTENS is built on two fundamental abstractions - tile data structures at each level of the memory hierarchy and bulk operands on tiles akin to the familiar suite of operations in PyTorch and NumPy. We first define the abstractions, and then show they can help developers navigate tradeoffs between the tile sizes and efficiency.\nProgramming abstractions TK is heavily inspired by PyTorch and NumPy, given their familiarity to ML audiences [31]. We provide a concise set of parallel compute operations, based on the suite of operations in PyTorch (e.g., in Figure 2). The operations are executed by a \u201cworker\u201d abstraction, or a warp or warpgroup (4 warps) of threads that collaboratively own and operate on a piece of data. TK uses a 16 \u00d7 16 matrix tile as its basic data structure, designed to maximize compatibility with tensor cores. We provide tiles for each level of the memory hierarchy:\n1. Register tiles and vectors, which are templated by type, shape, and layout. In Figure 2 we initialize a bfloat16 type tile with a column-major layout, height 16, width 64.\n2. Shared tiles and vectors, which are templated by type and shape.\n3. Global layout descriptors: We set up HBM loads and stores as indexing into 4D tensors (similar to {batch, head, length, and embed} in PyTorch). Dimensions can be known at compile-time or runtime. Compile-time dimensions can be stored in the instruction cache, saving registers.\nAn advantage of these tile-based abstractions is that they enable TK to statically check layouts and operations, which is important because GPU kernels are often difficult to debug. For example, an in-register tensor core multiply $mma_AB$ requires A to be in a row-major layout, and B to be in a column-major layout, and TK can raise compile-time errors if these conditions are not met.\nChoosing a memory layout Layouts specify how logical data elements are mapped to physical thread ownership. Different execution units, tile sizes and types, and hardware-accelerated instructions benefit from different layouts. A poor choice of layout can lead to bank conflicts (CShared, Section 2). Our goals are:\n\u2022 We want our register tiles (the fastest GPU memory) to by-default keep memory in the layouts required by tensor core units (the fastest GPU compute units). Shown in Figure 1 (Left), where each color represents"}, {"title": "Block parallelism with a generalized asynchronous template", "content": "THUNDERKITTENS helps developers reduce overheads by coordinating how workers in a thread block asynchronously overlap execution. Though the GPU hierarchy might suggest that we need a wide variety of techniques, we propose a single concise template that we find enables high performance on a surprisingly broad range of AI workloads. We first define the template, which has four steps - load-compute-store-finish (LCSF for short) \u2013 and builds on the classical producer-consumer paradigm [7, 16]. We then show how the LCSF template can help developers navigate the tradeoffs between occupancy and efficiency.\nProgramming abstractions As discussed in Section 2, the general pattern of an AI kernel is to load tiles of large tensors from HBM to SRAM, perform computation in fast memory, store the result for the tile back to HBM, and repeat this for the next tiles. To use the LCSF template, the developer writes four functions:\n1. Load function: The load function specifies the data that load workers should load from HBM to shared memory, and when to signal to compute workers that this memory is ready for use.\n2. Compute function: This function specifies the kernel instructions that compute workers should execute, using the tile data structure and operation primitives from Section 3.1.\n3. Store function: The store function specifies what data needs to be stored to HBM by store workers.\n4. Finish function: At the end of the kernel, the workers store any final state and exit.\nTK provides abstractions to help the developer manage worker overlapping and synchronization.\n1. Multi-stage buffer: The template maintains N-stage pipelined buffers in shared memory, which are used for loads and stores from HBM. Load/store workers add/remove tiles of data from the buffers, based on the status of compute workers. With a single stage, load workers would need to wait for all compute workers to finish executing before replacing the input tile. A 2-stage buffer can hide the HBM load (store) latency since the next tile can asynchronously load, while the compute workers execute on the current tile. Deep buffers can reduce the amount of synchronization required across compute workers, allowing them to operate on different tiles concurrently.\nTK lets the user set a single number to specify the number of stages, and manages the setup and use of these buffers for the user. We demonstrate this in Section 3.2, where we vary the number of stages N \u2208 {1,2,3,4} for our GEMM kernel.\n2. Synchronization barriers: Load/store workers need to alert compute workers when new memory is written to the input buffer. Compute workers need to alert load/store workers when tiles are written to the output buffer, or when input tiles can be evicted from the input buffer. Within the TK template, we provide an arrive function for workers to signal that they have finished their stage.\n3. Asynchronous I/O: We wrap synchronous and asynchronous load and store instructions, including cp.async and TMA, in the same interface. We automate tensor map descriptor creation for TMA hardware-accelerated address generation for our global layout descriptors (gl).\nTradeoffs between occupancy and efficiency\nTK parametrizes the number of load/store and com- pute workers (or occupancy) providing a simple way for developers tune their kernels. As discussed in Sec- tion 2, higher occupancy increases overlapping, but creates contention over limited hardware resources (e.g., registers). With fewer registers, workers need to operate on smaller tiles of data, resulting in more instruction issues, SRAM to register I/O, and potentially higher synchronization costs due to the increased data partitioning across workers."}, {"title": "Grid parallelism with block launch scheduling", "content": "Next, at the grid level, we explore how to coordinate thread block launches. TK's template does not explicitly choose grid structures for the user, however we provide a tradeoffs study of two key opportunities: reducing the setup and tear-down costs for each thread block (CSetup), and encouraging memory reuse between thread blocks to avoid slow HBM accesses (CHBM).\nBlock launch costs We first explore the use of a persistent grid, where we launch thread blocks on the full set of 132 SMs upfront, and simply load the next chunk of work for the kernel within the existing block, instead of launching a new block. We also explore the idea of having the block load shared memory into the input stage of our template's memory buffer to prepare for the next chunk of work, while the thread block runs the finish stage for the prior chunk of work."}, {"title": "Experiments", "content": "In experiments, we validate that THUNDERKITTENS speeds up a broad range of ML primitives. We compare to well-optimized kernels from prior work, written in alternate frameworks such as CutLass, CuBLAS, general CUDA, and Triton. We compare our kernels for the \"workhorse\" operations in AI, GEMM and attention, as well as kernels for emerging AI architectures, such as linear attention and state space models (Section 4.1). We profile the kernels to understand TK's role in achieving high performance in Section 4.2. Kernel listings, in the TK template, are in Appendix B."}, {"title": "TK enables simple and performant AI kernels", "content": "This section shows a suite of kernels that we develop in the TK framework. We benchmark the kernels on NVIDIA H100 80GB SXM GPUs using CUDA 12.6 and report the average TFLOPS.\nWorkhorse kernels for AI Industry teams and researchers have made significant investments into optimizing GEMMs and attention over the past several years [8, 15, 30, 37, inter alia.], two workhorse operations that power the Transformer architecture [41]. Despite the investment, TK kernels written entirely in the TK abstractions and LCSF template can match or outperform the strongest baselines:\n\u2022 GEMM We compare to the strongest available baseline: CuBLAS for GEMMs [30]. We show a single matrix multiply kernel, with just 40 lines of device code, can compete with CuBLAS\n\u2022 Attention. We support multiple variants of attention: causal, non-causal, and grouped query attention [3] at head dimensions 64 and 128. We compare to the strongest available baseline, which is concurrent to our work: FlashAttention-3 (FA3) [37]. TK competes with FA3 across sequence lengths on the non-causal forwards pass, and outperforms FA3 on the causal and non-causal backwards pass by over 40% at short sequences and 10% at longer sequences.\nWe find that TK makes it easy to use the GPU effectively by simplifying the choice of memory layouts, exploration of grid patterns for L2 reuse, and selection of occupancy and pipeline depth. The baseline kernels successfully use specialized H100 instructions and manage memory. However, the existing kernels are relatively complex: FlashAttention-3 proposes a \"ping-pong scheduler\" for workers, and the CuBLAS library is >600MB in CUDA 12.6 (Appendix A), containing many tuned GEMM variants and logic to select the best option at runtime [36]. With TK, we remove the ping-pong and maintain FA3-level efficiency, and we compete"}, {"title": "Kernels for emerging AI architectures", "content": "In addition to supporting peak performance on popular operations like GEMMs and attention, TK is also designed for to be extensible to emerging AI workloads. We release a family of kernels across newer machine learning primitives, including linear attention [27], FFT convolutions [11], and state space models [22].\n\u2022 Linear attention We optimize two different classes of linear attention architectures, polynomial-based feature maps as in [4, 5, 26, 28] and learned feature maps as in [45, 46]. In Figure 9, we compare to the strongest available baselines: the popular Flash Linear Attention (FLA) CUDA kernels [44], which are written in Triton. We show TK outperforms FLA's polynomial-based linear attention by 14x. TK outperforms FLA's learned map linear attention by 6.5x.\n\u2022 State space models The long convolution, implemented with Fourier transforms using the convolution theorem, is the key primitive in popular state space modeling architectures such as S4, H3, and Hyena [2, 19, 22, 23, 33, inter alia.]. In Figure 9, we compare to the strongest available baseline: the FlashFFTConv CUDA kernels in Fu et al. [20] and show TK outperforms the prior work by 4.7\u00d7 at sequence length 4096 and 7.9x at 1024. TK outperforms PyTorch's FFT operations by up to 8.7x."}, {"title": "Comparing kernel implementations", "content": "To further compare TK and baseline kernels, we profile the kernels using NVIDIA's NSight Compute (NCU) tool. In Table 4, we give NCU profiles for both the emerging long convolution primitive and the well-optimized attention backwards pass, comparing to the strongest respective baselines.\n\u2022 Long convolution We profile FlashFFTConv (FC) and TK long convolution kernels at B, D,N = 16, 1024, 4096 in NCU. We find TK helps both with overlapping the workers (indicated by higher issue slots and fewer memory stalls) and in tensor core utilization (4.1\u00d7 increase). This is enabled by our TK template, and use of TK warpgroup operations (which saves registers and establishes a SMEM to register memory pipeline through warpgroup matrix-multiply-add (WGMMA) operations).\n\u2022 Attention backwards We consider FA3 and TK at B, H, N, D = 16, 16, 3072, 128. The methods match in tensor core utilization, but TK gives higher issue slot utilization, suggesting the occupancy may be better-tuned. In HBM costs, TK gives higher memory throughput and correspondingly incurs 10% fewer stalled cycles on HBM waits. For shared memory, TK incurs 85% fewer stalled cycles we find TK has no bank conflicts, but NVIDA's NCU profiler reports up to 9.6-way bank conflicts in FA-3.\nThe kernel profiles highlight the difficulty of simultaneously managing each type of GPU parallelism, and we hope TK can help reduce the effort. We provide example TK kernels listings in Appendix B."}, {"title": "Conclusion", "content": "Given the challenge of mapping AI architectures to GPU hardware, our work asks how far we can get with a few easy to use GPU programming abstractions. In THUNDERKITTENS, we give an abstraction for each level of the GPU hierarchy: tiles with managed layouts at the worker level and a asynchronous execution LCSF template at the thread block level. We highlight options and tradeoffs for persistent block launches and L2 reuse at the grid level. The natural question is whether we sacrifice anything in performance when we write kernels with so few abstractions. We implement a breadth of AI kernels in TK and excitingly find that our abstractions are both general and consistently meet or exceed state-of-the-art. We are optimistic about the potential for simple and accessible ways of programming AI hardware."}, {"title": "Related work", "content": "We provide an initial discussion of related work in Section 2 and extended discussion here. We discuss frameworks that support AI kernel development and prior work to develop hardware-aware AI algorithms.\nCPP embedded libraries Towards raising the level of abstraction and supporting simpler programs, NVIDIA maintains the CuTe and CUTLASS libraries, which are CUDA primitives library for graphics, scientific computing, and ML. As discussed in Section 2, both CUTLASS and TK can support the same kernels, since both are C++ embedded libraries. Developers can use the power of the full C++ library, including raw CUDA, when using these frameworks. Distinct from CUTLASS's objectives, we specifically explore how broad and fast we can go, just using a small number of opinionated abstractions.\nCompiler-based libraries Many machine learning frameworks employ high-level computational graph representations for optimizations, such as TVM [10], TensorFlow XLA [1], Glow [35], and DLVM [43]. TVM, for instance, incorporates a flexible tensor expression language and automated schedule optimization. Building upon Halide's [34] separation of algorithm and schedule, TVM introduces new primitives such as tensorization. These frameworks' approach differs from THUNDERKITTENS in that they provide a full end-to-end stack that optimizes both graph-level and operator-level transformations, while THUNDERKITTENS concentrates specifically on kernel-level optimizations.\nTriton [39] builds on existing approaches to deep learning compilation while introducing novel techniques for efficient tiled computations on GPUs. Recently, tools such as Flex Attention also provide easy to use interfaces to write kernels for attention variants, and compile down to Triton [24]. Unlike XLA and Glow, which use tensor-level IRs and predefined templates, or Tensor Comprehensions [40] and Diesel [17], which rely on polyhedral models, Triton introduces a C-like language (Triton-C) and an LLVM-based IR (Triton-IR) centered around parametric tile variables. This approach supports non-affine tensor indices that polyhedral models struggle with. Triton's JIT compiler (Triton-JIT) implements tile-level optimizations and an auto-tuner, often enabling high performance on par with hand-tuned libraries. A key difference between TK and Triton is that TK is embedded within CUDA, and as a result its abstractions fail gracefully. In contrast, operations in Triton must be written entirely within its framework.\nHardware-aware AI architectures We are inspired by the success of several prior works that introduce systems-level innovations to improve ML efficiency such as FlashAttention [12, 15, 37] and other optimized attentions [8], FlashFFTConv [20], linear attention kernels [13, 32, 42, 44]. Given the effort required to independently optimize each new architecture, we ask whether a small set of opinionated GPU programming abstractions can obtain kernels for a broad range of AI operations."}, {"title": "ThunderKittens kernels", "content": "This section first recaps our benchmarking methodology for the results and provides a set of kernels written in the TK LCSF template and tile abstractions:\n1. Appendix B.1 GEMM kernel\n2. Appendix B.2 Long convolution kernel\n3. Appendix B.3 Attention kernel\n4. Appendix B.4 Rotary kernel\nTo introduce the template components, we describe the GEMM kernel in detail in Appendix B.1.\nBenchmarking approach Our kernels in Section 4 are benchmarked on an NVIDIA H100 80GB SXM GPU with 10 warmup and 10 timed iterations using timings measured in C++. We also provide Python-bound kernels and benchmarking infrastructure in our repository for reference."}, {"title": "Matrix multiply", "content": "First we show and describe a TK GEMM kernel in the LCSF template.\nEach compute warpgroup is responsible for computing 64M-row, 64N-column chunk of the resulting output matrix. Each compute worker identifies the coordinates for its chunk, zeros its accumulator registers, repeatedly runs large asynchronous matrix multiplies (compute), and finally stores out its tile in the end (finish). The load workers also compute their coordinates, and then repeatedly load chunks of the input matrices (load). Store workers perform asynchronous stores when the compute workers are finished with the chunks (stores).\nTuning the number of workers and pipeline stages The computation is divided into stages, with each stage processing 64 elements along the reduction dimensions of the input matrices. The input pipeline is automatically sized by THUNDERKITTENS if the user does not specify a value. For common configurations of either a (2 compute warpgroup) 128 \u00d7 256 or (3 compute warpgroups) 192 \u00d7 192 output tile per block, it generates a 4-stage pipeline.\nTuning the grid order The greatest complexity of this kernel is in setting the grid parameters. This kernel adopts a 3D stride over the input matrices, which has a significant effect for large matrices which do not fit in L2 cache. The order in which blocks execute strongly influences cache locality and thus available memory bandwidth. To illustrate the magnitude of the effect, comparing the presented scheme versus a naive grid (in which blocks are executed in row-major order) a 4096 \u00d7 4096 \u00d7 4096 matrix multiply only drops from 767 TFLOPs to 735 TFLOPs, but a 16384 \u00d7 16384 \u00d7 16384 matrix multiply drops from 797 TFLOPs to 387 TFLOPs, a > 50% performance degradation."}, {"title": "Long convolution", "content": "This section shows the long convolution kernel for sequence length 4096, written in the TK abstractions. We use the FFT convolution algorithm, computed via Monarch Matrices, for our long convolution kernel [11, 14, 18]."}, {"title": "Attention", "content": "This section shows non-causal attention at head dimensions 64, 128, in the TK abstractions."}, {"title": "Rotary positional encodings", "content": "This section shows the rotary kernel for head dimension 128, written in the TK abstractions."}, {"title": "Shared memory layouts", "content": "To illustrate some of the choices available in shared memory layouts, this appendix outlines six different shared memory layouts for GPU tiles: a naive row-major layout, a padded layout, a simple swizzled layout, and three more specialized swizzled layouts. We are particularly interested in which memory banks (numbered from 00 to 31) store each element of the tile; for each layout, we color and label the element of the tile accordingly. We illustrate all layouts using a 32 \u00d7 64 16-bit tile."}, {"title": "Naive layout", "content": "A row-major layout, illustrated in figure 14, is among the simplest layouts. It has the benefit of accurately reflecting tensor layouts in HBM. Furthermore, for access patterns that access row-wise, it has no bank conflicts. But when loading or storing tensor core register layouts, it suffers 8-way bank conflicts, and is thus extremely slow.\n```\nbf16* naive_layout(bf16 *data, int r, int c) {\n return &data[r * columns + c];\n}\n```"}, {"title": "Padded layout", "content": "A common solution to these bank conflicts is to \"pad\" each row by one memory bank, thereby introducing an offset to shift consecutive elements of a column into different memory banks. This eliminates bank conflicts, but creates misaligned addresses which interferes with fast instructions that require aligned addresses.\n```\nbf16* padded_layout(bf16 *data, int r, int c) {\n return &data[r * (columns+1) + c];\n}\n```"}, {"title": "Naive Swizzled Layout", "content": "A third option is to \"swizzle\" the memory, in which progressive rows are reshuffled to alter their banking. This layout accomplishes this by xor'ing the index with the row, which eliminates bank conflicts and has aligned addresses. However, this layout lacks hardware support for HGMMA and UTMA instructions, which are particularly important on H100 GPUs for achieving high performance. We illustrate a simple swizzling pattern here:\n```\nbf16* row_swizzled_layout(bf16 *data, int r, int c) {\n uint64_t addr = (uint64_t) &data[r * columns + c];\n return (bf16*) (addr^ (r << 2));\n}\n```"}, {"title": "32 byte swizzling", "content": "32 byte swizzling is the first of a family of layouts (of which we will examine three), where instead of swizzling the index with the row, the memory address is instead swizzled directly with itself. This layout is defined by the following C code:\n```\nb"}, {"title": "64 byte swizzling", "content": "64 byte swizzling is a layout similar to 32 byte swizzling with a more aggressive pattern:\n```\nbf16* swizzled_layout_64B(bf16 *data, int r, int c) {\n uint64_t addr = (uint64_t) &data[r * columns + c];\n return (bf16*) (addr^ (((addr % (64*8)) >> 7) << 4));\n}\n```\n64 byte swizzling suffers from just 2-way bank conflicts, but is only valid for tiles whose width is a multiple of 32 (for half-precision types, or 16 for full-precision)."}, {"title": "128 byte swizzling.", "content": "128 byte swizzling is a further extension of its kin:\n```\nbf16* swizzled_layout_128B(bf16 *data, int r, int c) {\n uint64_t addr = (uint64_t) &data[r * columns + c];\n return (bf16*) (addr^ (((addr % (128*8)) >> 7) << 4));\n}\n```\nFinally, 128 byte swizzling has no bank conflicts, but is only valid for half-precision tiles whose width is a multiple of 64."}, {"title": "ThunderKittens", "content": "After substantial evaluation of these layouts, we concluded that the three final layouts were the three most important, because HGMMA and UTMA instructions are critical to high performance, and furthermore that they are good enough to yield high performance across many kernels. Correspondingly, depending on the width of the tile at compile time we select the highest level of swizzling possible to minimize bank conflicts."}]}