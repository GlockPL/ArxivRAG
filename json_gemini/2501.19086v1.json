{"title": "FAIRNESS ANALYSIS OF CLIP-BASED FOUNDATION MODELS FOR X-RAY IMAGE\nCLASSIFICATION", "authors": ["Xiangyu Sun", "Xiaoguang Zou", "Yuanquan Wu", "Guotai Wang", "Shaoting Zhang"], "abstract": "X-ray imaging is pivotal in medical diagnostics, offering non-\ninvasive insights into a range of health conditions. Recently,\nvision-language models, such as the Contrastive Language-\nImage Pretraining (CLIP) model, have demonstrated poten-\ntial in improving diagnostic accuracy by leveraging large-\nscale image-text datasets. However, since CLIP was not\ninitially designed for medical images, several CLIP-like\nmodels trained specifically on medical images have been\ndeveloped. Despite their enhanced performance, issues of\nfairness-particularly regarding demographic attributes re-\nmain largely unaddressed. In this study, we perform a com-\nprehensive fairness analysis of CLIP-like models applied\nto X-ray image classification. We assess their performance\nand fairness across diverse patient demographics and disease\ncategories using zero-shot inference and various fine-tuning\ntechniques, including Linear Probing, Multilayer Perceptron\n(MLP), Low-Rank Adaptation (LoRA), and full fine-tuning.\nOur results indicate that while fine-tuning improves model\naccuracy, fairness concerns persist, highlighting the need for\nfurther fairness interventions in these foundational models.", "sections": [{"title": "1. INTRODUCTION", "content": "Medical imaging, particularly X-ray imaging, plays a crucial\nrole in diagnosing a variety of diseases. The integration of\nArtificial Intelligence (AI) into medical imaging has shown\npotential in enhancing diagnostic accuracy and efficiency [1].\nDeep learning models [2, 3, 4, 5, 6] trained through fully su-\npervised learning have achieved significant success in various\nmedical image classification tasks. However, these models\nare constrained by high annotation costs and limited general-\nizability to new image types or datasets, which restricts their\napplicability across diverse clinical settings.\nRecent advancements in vision-language models, such as\nCLIP [7], have enhanced the classification of previously un-\nseen images by leveraging large-scale image-text pairs for\ntraining and zero-shot inference. This approach is particu-\nlarly appealing for medical image classification, where ob-\ntaining sufficient annotated training data is challenging and\ntime-consuming. However, CLIP was originally trained pri-\nmarily on natural image-text pairs, limiting its performance\non medical images like X-rays that exhibit different seman-\ntic and visual distributions [8]. To address this limitation,\ndomain-adapted CLIP variants\u2014GLORIA [9], MedCLIP [8],\nand BioMedCLIP [10]\u2014have been developed by training on\nextensive medical image-text datasets, resulting in improved\nperformance in X-ray image classification.\nDespite these advancements, fairness ensuring consis-\ntent model performance across diverse patient demograph-\nics-remains inadequately addressed. Fairness in medical AI\nis essential to prevent biased model outputs that may lead to\nunequal healthcare outcomes, particularly concerning demo-\ngraphic factors such as age and gender [11]. Previous studies\nhave demonstrated that biases can emerge at various stages\nof the model development pipeline, including pretraining and\nfine-tuning [12]. Therefore, a comprehensive evaluation of\nfairness in these adapted CLIP-based models is crucial to en-\nsure both accurate and equitable healthcare delivery.\nIn this work, we systematically evaluate the performance\nand fairness of medical CLIP models, focusing on their abil-\nity to generalize across different patient demographics and\ndisease categories. An overview of this work is shown in\nFig. 1, and the main contributions of this research include: 1)\nCreating a gender and age-balanced X-ray dataset with mul-\ntiple diseases for fairness evaluation of foundation models;\n2) Conducting a comprehensive evaluation of the fairness of\nfour CLIP-like models on two sensitive attributes, including\ngender and age; 3) Investigating the effect of different fine-\ntuning methods on the fairness of these models. Our findings\nshow that while fine-tuning improves classification accuracy,"}, {"title": "2. METHOD", "content": "significant fairness gaps related to age and gender remain, un-\nderscoring the need for advanced bias mitigation strategies."}, {"title": "2.1. X-ray Dataset with Balanced Demographic Groups", "content": "To evaluate the fairness and efficacy of CLIP-based models\nin X-ray image classification, we employed the NIH Chest X-\nray dataset, which comprises 108,948 images from 32,717 pa-\ntients, spanning fourteen distinct disease categories [13]. This\ndataset includes annotations for key demographic attributes\nsuch as age and gender. However, as the original dataset\nhas large imbalanced distributions of age, gender and disease\ntype, directly using it for fine-tuning may introduce additional\nunfairness. Therefore, we curated a subset with balanced de-\nmographic representation to facilitate fairness studies.\nWe considered gender and age as the sensitive attributes\nin our work. For each disease, we considered four patients\ngroups: Young (age < 60) Male, Old (age >= 60) Male,\nYoung Female, and Old Female. To avoid imbalance from\nthe dataset, for each disease, we selected N images from each\npatient group, leading to 4N images for each disease. In this\nstudy, we set N = 50 and found that only six diseases sat-\nisfy the data selection criteria: the image number in any of\nthe four groups should be larger than N. Setting N > 50 will\nlead to reduced disease types, while N < 50 will make the\nsample number too small in each patient group. As a result,\nwe selected six diseases each with 4 \u00d7 50 = 200 images, and\nname it as NIH 6 \u00d7 200 dataset. The disease types are: Car-\ndiomegaly, Effusion, Atelectasis, Pneumothorax, Edema, and\nConsolidation. Note that an exception is the Edema class in\nthe Old age group and it contains 47 female and 53 male cases\nrespectively due to limitations in the available NIH data. This\nbalanced dataset facilitates a rigorous evaluation of model\nfairness across different demographic groups."}, {"title": "2.2. Models Under Investigation", "content": "We investigate four CLIP-based models for X-ray image clas-\nsification: 1) CLIP [7] that was trained on 400 million natural\nimage-text pairs from the internet. It offers various image\nencoder architectures, and we utilize the ViT-B/16 model in\nour experiments. For text encoding, CLIP employs a standard\nBERT-based Transformer encoder. 2) GLORIA [9] that was\ntrained on the CheXpert [14] dataset using a ResNet-50 image\nencoder. For text encoding, it employs BioClinicalBERT [15]\npretrained on the MIMIC-III [16] dataset. 3) MedCLIP [8]\nthat was trained on a combination of MIMIC-CXR [17] and\nCheXpert datasets. It has two variants that use SwinTrans-\nformer and ResNet-50, respectively, denoted as MedCLIPViT\nand MedCLIPRN. MedCLIP also utilizes BioClinicalBERT\nfor text encoding. 4) BioMedCLIP [10] that was trained on\nthe PMC-15M dataset (100 times larger than MIMIC-CXR).\nIt employs a Vision Transformer (ViT) as the image encoder\nand utilizes PubMedBERT [18] for text encoding with refine-\nments to the tokenizer and context size. It is important to note\nthat none of these models was trained on the NIH Chest X-\nray dataset. Therefore, evaluating these models on our NIH\n6\u00d7200 dataset ensures an unbiased assessment."}, {"title": "2.3. Zero-shot Inference and Fine-Tuning Strategies", "content": "We first evaluate the models' generalization capabilities using\nZero-Shot (ZS) Inference, which involves directly applying\nthe pretrained models with text prompts without fine-tuning.\nSpecifically, we employ three distinct prompt formats as pro-\nposed in the CLIP, GLORIA, and CXR-CLIP studies [7, 9,\n19]. Since the latter two methods involve random prompt\ncombinations, for each prompt format, we took an average of\n10 runnings and selected the best-performing prompt format\nfor each model.\nTo further adapt the CLIP-like models for X-ray image\nclassification, we employ four fine-tuning strategies: 1) Lin-\near Probing (LP) that freezes the pretrained backbone and"}, {"title": "2.4. Utility and Fairness Metrics", "content": "To comprehensively assess models' performance, we apply\nboth utility and fairness metrics.\nUtility Metrics: Three metrics are used to evaluate the\nmodels' performance: 1) Accuracy that measures the overall\nproportion of correct predictions; 2) Class-wise F1 score that\nis the harmonic mean of precision and recall for each class;\nand 3) Average F1-score for each demographic group.\nFairness Metrics: Firstly, to evaluate the fairness across\ndifferent disease types, we measured the variance of F1 score\n($Var_{F1}$) across all the disease categories, see Equation 1.\n$Var F1 = \\frac{1}{C} \\sum_{c=1}^{C}(F1c \u2013 \\overline{F1})^2$ (1)\nwhere C is the number of disease categories, $Flc$ is the\nF1 score for the c-th category, and $\\overline{F1}$ is the mean F1 score\nfor the C classes.\nSecondly, for the fairness assessment on gender and age,\nwe employ: 1) Gap of F1 Score (F1\u25b3) that represents the av-\nerage difference in F1 scores between demographic groups\nacross all classes; 2) Equalized Odds (EqOdds) that com-\nbines gaps of both true positives and false positives across\ndemographic groups; and 3) Gap of Expected Calibration Er-\nror (ECE) that quantifies the difference in model calibration\nacross demographic groups [12]. See Equations 2, 3, and 4.\n$\\Delta_{F1 A} = \\frac{1}{C} \\sum_{c=1}^{C}(F1_{c}^{A}-F1_{c}^{A'})$ (2)\nwhere $F1_{c}^{A}$ and $F1_{c}^{A'}$, are the F1 scores for class c in demo-\ngraphic groups A and A' (e.g., male and female), respectively.\n$EqOdds = \\frac{1}{2C} \\sum_{c=1}^{C} (|TP_{c}^{A}-TP_{c}^{A'}| + |FP_{c}^{A}-FP_{c}^{A'}|)$ (3)\nwhere $TP_{c}^{A}, FP_{c}^{A}$ and $TP_{c}^{A'}, FP_{c}^{A'}$, are the true positive,\nfalse positive rates for class c in demographic group A and\nA', respectively.\n$ECE_{IA} = \\frac{1}{N_{A}} \\sum_{i=1}^{N_{A}} |pi - oi| - \\frac{1}{N_{A'}} \\sum_{j=1}^{N_{A'}} |p'_{j} - o'_{j}|$ (4)\nwhere $N_{A}$ and $N_{A'}$ are the number of samples in the two\ndemographic groups, respectively. $pi$ and or are the predicted\nprobability and actual outcome for sample i in the first group,\nand p'; and of; are those for sample j in the second group."}, {"title": "3. EXPERIMENTS AND RESULTS", "content": ""}, {"title": "3.1. Implementation Details", "content": "We utilized the NIH 6\u00d7200 dataset to ensure a balanced rep-\nresentation across six distinct disease categories and two de-\nmographic attributes: age and gender. Our NIH 6x200 dataset\nwas partitioned into training, validation, and testing sets at a\nratio of 7:1:2 to enable robust model assessment.\nAll experiments were performed on a single NVIDIA\nGTX 1080Ti GPU. Each model was reproduced and fine-\ntuned within the software environments specified in their\noriginal publications. For model fine-tuning, we employed\nthe AdamW optimizer with a cosine annealing learning rate\nscheduler, with a batch size of 64, and trained for 150 epochs.\nOther hyperparameters, such as the learning rate, were in-\ndividually adjusted based on validation performance. We\nretained the checkpoint with the lowest validation loss for\nevaluation on the testing set."}, {"title": "3.2. Utility Analysis", "content": "As presented in Fig. 2(a), the original CLIP model had the\nlowest classification performance in both zero-shot inference"}, {"title": "3.3. Fairness Analysis", "content": "and fine-tuning settings, due to that the other models were\nspecifically trained for medical images. In terms of zero-shot\ninference, MedCLIPViT outperformed the other models with\na classification accuracy of 49.4%. However, the performance\nremains limited, highlighting the challenges of applying pre-\ntrained models directly to X-ray image classification tasks\nwithout adaptation.\nIn addition, Fig. 2(a) shows that all the fine-tuning meth-\nods led to performance improvement. Full fine-tuning largely\noutperformed the other fine-tuning strategies for CLIP and\nMedCLIPViT, and LoRA obtained the best performance on\nMedCLIPRN and BioMedCLIP. Among all these models\nand fine-tuning strategies, MedCLIP using a ViT architecture\nwith full fine-tuning achieved the highest overall accuracy of\n59.6%, highlighting the effectiveness fine-tuning in enhanc-\ning the model's performance. Fig. 2(b) shows the class-wise\nF1 score of different models after full fine-tuning. It can be\nobserved that MedCLIPViT consistently outperformed the\nother models across all the evaluated disease categories.\nFairness on Disease Types. A large variation of F1-score\namong different disease categories can be found in Fig. 2(b).\nFor example, the class-wise Fl-score of GLORIA ranged\nfrom 21.1% to 74.7%, while that for MedCLIPViT ranged\nfrom 25.8% to 81.8%, demonstrating the obvious unfairness\nof these models on different disease types. Fig. 3(a) further\ncompares the variance of class-wise F1-score of these meth-\nods, and it shows that MedCLIPRN and MedCLIPViT have\nvery close VarF1 values, and they are much larger than those\nof GLORIA and BioMedCLIP. Generally, BioMedCLIP has\nthe highest disease-level fairness (VarF1=1.7%), in despite of\nits lower classification performance than MedCLIPViT.\nFairness on Age and Gender. Fig. 3(b) shows the av-\nerage class-wise F1-score on different demographic groups.\nIt can be observed that MedCLIPVIT has a large unfairness\nbetween young male and young female (71.1% vs 52.0% in\nterms of F1), and the gap between young male and old male is\nalso large (71.1% vs 57.4%). In contrast, GLORIA exhibits a\nrelatively fair performance on these demographic groups with\nF1 ranging from 47.1% to 50.9%. Fig. 3(c) presents more\nfairness metrics of these models after full fine-tuning includ-"}, {"title": "4. CONCLUSION", "content": "ing F1, EqOdds and ECE\u25b3 for age and gender. It is evi-\ndent that all models, except GLORIA, exhibit relatively high\nfairness metric values, indicating significant disparities across\ndifferent demographic groups. Specifically, MedCLIPVIT has\nthe highest F1Gender (12.6%) and F1Ag (7.1%), indicating the\npoorest fairness in both gender and age dimensions. In con-\ntrast, GLORIA obtained the highest fairness with an F1Gender\nof 2.5% and FlAge of 0.4%, respectively.\nEffect of Fine-tuning on Fairness. Fig. 3(d) presents\nthe fairness metrics before fine-tuning. Comparison between\nFig. 3(c) and (d) shows that GLORIA generally has reduced\nmetric values (improved fairness) after fine-tuning. However,\nfor the other models, the fine-tuning leads to higher unfair-\nFor example, for GLORIA, EqOddsGender decreased\nfrom 2.9% to 1.4%. For MedCLIPVIT, EqOddsGender and\nEqOddsAge increased from 2.3% to 6.8% and from 3.0% to\n3.5%, respectively. The comparison shows that the reduced\nfairness of the other models is unlikely due to our dataset, but\nfrom the models themselves.\nness.\nIn this work, we constructed a balanced chest X-ray image\ndataset to investigate the fairness of CLIP-like foundation\nmodels on different disease types and attributes including age\nand gender. Both utility and fairness of zero-shot inference\nand different strategies of fine-tuning are analyzed. Our main\nfinding includes: 1) The zero-shot inference performance of\nthese models is still low for chest X-ray image classification,\nand fine-tuning could significantly improve the performance.\n2) While MedCLIPViT after full fine-tuning achieved the\nbest performance, its fairness on different disease types and\ndemographic attributes (age and gender) is lower than the\nother compared models, and GLORIA has the best fairness\nmetric values. 3) The effect of fine-tuning with a balanced\ndataset on fairness mainly depends on the model itself, with\nGLORIA having improved fairness and the other models hav-\ning decreased fairness after full fine-tuning, respectively. Our\nevaluation reveals a comprehensive understanding of perfor-\nmance and fairness in adapted CLIP-based models for chest\nX-ray image classification. These results underscore the ne-\ncessity for developing advanced fairness interventions for\nfoundation models to ensure equitable clinical diagnosis."}, {"title": "5. COMPLIANCE WITH ETHICAL STANDARDS", "content": "Ethical approval was not required as confirmed by the license\nattached with the open access data."}, {"title": "6. ACKNOWLEDGMENT", "content": "This work was supported by the National Key Research &\nDevelopment Program of China (2022ZD0160705)."}]}