{"title": "FAN: Fourier Analysis Networks", "authors": ["Yihong Dong", "Ge Li", "Yongding Tao", "Xue Jiang", "Kechi Zhang", "Jia Li \u01a1", "Jing Su", "Jun Zhang", "Jingjing Xu"], "abstract": "Despite the remarkable success achieved by neural networks, particularly those represented by MLP and Transformer, we reveal that they exhibit potential flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize the periodic data rather than genuinely understanding the underlying principles of periodicity. However, periodicity is a crucial trait in various forms of reasoning and generalization, underpinning predictability across natural and engineered systems through recurring patterns in observations. In this paper, we propose FAN, a novel network architecture based on Fourier Analysis, which empowers the ability to efficiently model and reason about periodic phenomena. By introducing Fourier Series, the periodicity is naturally integrated into the structure and computational processes of the neural network, thus achieving a more accurate expression and prediction of periodic patterns. As a promising substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in various models with fewer parameters and FLOPs. Through extensive experiments, we demonstrate the effectiveness of FAN in modeling and reasoning about periodic functions, and the superiority and generalizability of FAN across a range of real-world tasks, including symbolic formula representation, time series forecasting, and language modeling.", "sections": [{"title": "Introduction", "content": "The flourishing of modern machine learning and artificial intelligence is inextricably linked to the revolutionary advancements in the foundational architecture of neural networks. For instance, multi-layer perceptron (MLP) (Rosenblatt, 1958; Haykin, 1998) plays a pivotal role in laying the ground-work for current deep learning models, with its expressive power guaranteed by the universal approximation theorem (Hornik et al., 1989). Recent claims about the impressive performance of large models on various tasks are typically supported by Transformer architecture (Vaswani et al., 2017; Touvron et al., 2023; OpenAI, 2023). In this context, the community's enthusiasm for research on neural networks has never diminished. Some emerged neural networks demonstrate notable capabilities in specific fields (Gu & Dao, 2023; Liu et al., 2024), sparking widespread discussion within the community.\nBeneath the surface of apparent prosperity, we uncover a critical issue that remains in existing neural networks: they struggle to model the periodicity from data. We showcase this issue through an empirical study as illustrated in Figure 1. The results indicate that existing neural networks, including MLP (Rosenblatt, 1958), KAN (Liu et al., 2024), and Transformer (Vaswani et al., 2017), face difficulties in fitting periodic functions, even on a simple sine function. Although they demonstrate"}, {"title": "Preliminary Knowledge", "content": "Fourier Analysis (Stein & Weiss, 1971; Duoandikoetxea, 2024) is a mathematical framework that decomposes functions into their constituent frequencies, revealing the underlying periodic structures within complex functions. At the heart of this analysis lies Fourier Series (Tolstov, 2012), which expresses a periodic function as an infinite sum of sine and cosine terms. Mathematically, for a function $f(x)$, its Fourier Series expansion can be represented as:\n$f(x) = a_0 + \\sum_{n=1}^{\\infty} \\left( a_n \\cos \\left( \\frac{2 \\pi n x}{T} \\right) + b_n \\sin \\left( \\frac{2 \\pi n x}{T} \\right) \\right);$ \nwhere $T$ is the period of the function, and the coefficients $a_n$ and $b_n$ are determined by integrating the function over one period:"}, {"title": "Fourier Analysis Network (FAN)", "content": "In this section, we first construct a simple neural network modeled by the formula of Fourier Series, and then on this basis, we design FAN and provide its details. Finally, we discuss the difference between the FAN layer and the MLP layer.\nConsider a task involving input-output pairs ${x_i, y_i}$, with the objective of identifying a function $f(x): \\mathbb{R}^{d_x} \\rightarrow \\mathbb{R}^{d_y}$ that approximates the relationship such that $y_i \\approx f(x_i)$ for all $x_i$, where $d_x$ and $d_y$ denote the dimensions of $x$ and $y$, respectively. To build a simple neural network $f_s(x)$ that represents Fourier Series expansion of the function, specifically $\\mathcal{F}\\{f(x)\\}$, as described in Eq. (1), we can express $f_s(x)$ as follows:\n$f_s(x) \\approx a_0 + \\sum_{n=1}^{N} \\left( a_n \\cos \\left( \\frac{2 \\pi n x}{T} \\right) + b_n \\sin \\left( \\frac{2 \\pi n x}{T} \\right) \\right).$\n$\\qquad = a_0 + \\sum_{n=1}^{N} (w_{cn} \\cos (w_n x) + w_{sn} \\sin (w_n x)),$\n$\\qquad = B + [w_1, w_2, ..., w_N] \\cos([w_1||w_2||...||w_N]x)$\n$\\qquad + [w_1, w_2, ..., w_N] \\sin([w_1||w_2||...||w_N]x)$\n$\\qquad = B + W_c \\cos(W_{in}x) + W_s \\sin(W_{in}x),$\n$\\qquad = B + W_{out} [\\cos(W_{in}x)|| \\sin(W_{in}x)],$\nwhere $B \\in \\mathbb{R}^{d_y}$, $W_{in} \\in \\mathbb{R}^{N \\times d_x}$, and $W_{out} \\in \\mathbb{R}^{d_y \\times 2N}$ are learnable parameters, (I) follows that the computation of $a_n$ and $b_n$ computed via Eq. (2) is definite integral, (II) and (III) follows the equivalence of the matrix operations, $[\\cdot||\\cdot]$ and $[\\cdot,\\cdot.]$ denotes the concatenation along the first and second dimension, respectively.\nTo fully leverage the advantages of deep learning, we can stack the aforementioned network $f_s(x)$ to form a deep network $f_D(x)$, where the i-th layer, denoted as $l_i(x)$, retains the same structural design as $f_s(x)$. Therefore, $f_D(x)$ can be formulated as:\n$f_D(x) = l_L \\circ l_{L-1} \\circ \\cdots \\circ l_1 \\circ x,$\nwhere $l_i \\circ x$ denotes the application of the left function $l_i$ to the right input $x$, that is $l_i(x)$. However, we discover that the direct stacking of $f_s(x)$ results in the primary parameters of the model $f_D(x)$ focusing on learning the angular frequency $(w_n = \\frac{2 \\pi n}{T})$, thereby neglecting the learning of the Fourier coefficients ($a_n$ and $b_n$), as follows:\n$f_D(x) = l_L(l_{L-1} \\circ l_{L-2} \\circ \\cdots \\circ l_1 \\circ x)$\n$\\qquad = B_L + W_{out} [\\cos(W_{in} (l_{1:L-1} \\circ x)|| \\sin(W_{in} (l_{1:L-1} \\circ x))]$\nwhere $l_{1:L-1} \\circ x$ is defined as $l_{L-1} \\circ l_{L-2} \\circ \\cdots \\circ l_1 \\circ x$, $W_{in}(l_{1:L-1} \\circ x)$ is used to approximate the angular frequencies, and $W_{out}$ is used to approximate the Fourier coefficients. Therefore, the capacity of $f_D(x)$ to fit the Fourier coefficients is independent of the depth of $f_D(x)$, which is an undesirable outcome."}, {"title": "Experiments", "content": "In this section, we first introduce the baselines and implementation details of our experiments. Next, we verify the effectiveness of FAN in modeling and reasoning about periodic functions (Section 4.1). Finally, we demonstrate the superiority and generalizability of FAN across a range of real-world tasks, including symbolic formula representation (Section 4.2), time series forecasting (Section 4.3), and language modeling (Section 4.4).\nBaselines. In our experiments, we mainly compare FAN with the following baselines. 1) MLP (Rosenblatt, 1958): the most classic model, which is widely used in the backbone of various models. 2) Transformer (Vaswani et al., 2017): a prevalent model known for its self-attention mechanism, which achieves outstanding performance on various tasks. 3) KAN (Liu et al., 2024): an emerged model specialized for symbolic formula representation, which uses the b-spline functions instead of fixed activation functions. 4) LSTM (Hochreiter & Schmidhuber, 1997): a well-known recurrent neural network (RNN) that can capture long-term dependencies on sequential data. 5) Mamba (Gu & Dao, 2023): an emerged selective state space model (SSM) that achieves competitive performance on some tasks with sequential inputs. Moreover, we also include the following variants of FAN into our comparisons: I) FAN (Gated): a variant of FAN that adds gates to control the tendency of the layer, with the formula defined as $g(x) = [g\\cdot\\cos(W_px)||g\\cdot\\sin(W_px)||(1 - g)\\cdot \\sigma(B_p + W_px)]$, where g is a learnable parameter. II) Transformer with FAN and Transformer with FAN (Gated): we replace each MLP layer in Transformer with the FAN layer computed via Eq. (9) and the layer of FAN (Gated), respectively.\nImplementation Details. We conduct our experiments on a single GPU of Tesla A100-PCIe-40G. Unless otherwise specified, we use the following hyperparameters in the experiments. The model architecture consists of 3-12 layers, the activation function \u03c3 is set to GELU (Hendrycks & Gimpel, 2016), and the dimension of the projection matrix $W_p$ is set to $d_p = d_h$, where $d_h$ denotes the dimension of the hidden layers. We employ the AdamW optimizer (Loshchilov & Hutter, 2019) for the model's training process. More experimental details and comprehensive setups of each task can be found in Appendix C."}, {"title": "Periodicity Modeling", "content": "Setup. In periodic modeling tasks, we select periodic functions with practical significance and compare the models' performance in learning the underlying principles of periodicity. Specifically, we generate data from periodic functions over a large domain, using a portion of this domain as training data and the entire domain as test data, i.e., a part of test data would be out of the domain of training data. In this task, we compare FAN and its variant, FAN (Gated), with MLP, KAN, and Transformer. The input of each task is a scalar.\nResults. Figure 3 illustrates the performance of FAN and other baselines in periodicity modeling. The results indicate that existing neural networks, including MLP, KAN, and Transformers, exhibit notable deficiencies in their ability to model periodicity. Although they attempt to fit these periodic functions, their ability limits their performance in modeling a large domain of periodicity. In contrast, FAN significantly outperforms the baselines in all these tasks of periodicity modeling."}, {"title": "Symbolic Formula Representation", "content": "Setup. Symbolic formula representation is a common task in both mathematics and physics. We follow the experiments conducted in KAN's paper (Liu et al., 2024), adhering to the same tasks, data, hyperparameters, and baselines. In addition to the original baselines, we also include Transformer for comparison in this task.\nResults. Figure 5 demonstrates the performance of different models applied to four common functions in mathematics and physics. From Figure 5, we can observe that while KAN remains competitive with FAN when the number of parameters is small, its performance declines significantly as the number of parameters increases. In contrast, as the number of parameters grows, FAN consistently"}, {"title": "Time Series Forecasting", "content": "Setup. Time series forecasting plays a critical role in various real-world applications. In our experiments, we employ four public datasets of this task to assess the model performance on time series forecasting, including Weather (Wu et al., 2021a), Exchange (Lai et al., 2018), Traffic (Wu et al., 2021a), and ETTh (Zhou et al., 2021) datasets. For each dataset, we input 96 previous time steps and forecast the subsequent time steps of {96, 192, 336, 720}. In this task, we choose the sequence models as baselines, including LSTM, Mamba, Transformer, Transformer with FAN, and Transformer with FAN (Gated).\nResults. As presented in Table 2, we compare the performance of Transformer with FAN and other sequence models for time series forecasting tasks on four public datasets. In most cases, Transformer with FAN and its gated version achieves the best performance on these tasks, compared to LSTM, Mamba, and the standard Transformer. The improvements of Transformer with FAN and FAN (Gated) over the standard Transformer are notable, with the average relative improvements ranging from 14.3% to 15.0% for MSE and from 7.6% to 7.9% for MAE. These results suggest that incorporating explicit periodic pattern encoding within neural networks improves time series forecasting performance in real-world applications."}, {"title": "Language Modeling", "content": "Setup. Language modeling is a fundamental task in natural language processing. In this experiment, we conduct language modeling using the SST-2 (Socher et al., 2013) dataset and evaluate the model's performance on its test set, as well as on the related datasets such as IMDB (Maas et al., 2011), Sentiment140 (Sahni et al., 2017), and Amazon Reviews (Linden et al., 2003). These four classic datasets all belong to the field of sentiment analysis. In this task, the comparisons are between Transformer with FAN and FAN (Gated), along with other sequence models, including LSTM, Mamba, and Transformer.\nResults. We report the performance comparison between different sequence models across four sentiment analysis datasets, as shown in Table 3. We can find that our proposed Transformer with FAN demonstrates significantly superior performance compared to the standard Transformer and other baselines, such as LSTM and Mamba, especially for zero-shot cross-domain performance on IMDB, Sentiment140, and Amazon Reviewers datasets. Transformer with FAN achieves the relative improvements up to 14.65% and 8.50% in terms of Loss and Accuracy respectively, while reducing the number of parameters by about 14.16M. The result indicates the potential of periodicity modeling to enhance both effectiveness and generalization on cross-domain language modeling and sentiment analysis tasks."}, {"title": "Related Work", "content": "In this section, we outline the two most relevant directions and associated papers of this work.\nLearning Periodicity with Neural Networks. Periodic functions are one of the most basic functions of importance to human society and natural science (Newton, 1687; Osborn & Sensier, 2002; Kwasnicki, 2008; De Groot & Franses, 2012; Zhang et al., 2017). However, commonly used neural networks, such as MLPs and transformers, struggle to extrapolate periodic functions beyond the scope of the training data. This limitation arises from the lack of inherent \u201cperiodicity\" in their inductive biases. Some previous works (Silvescu, 1999; Liu, 2013; Parascandolo et al., 2016; Uteuliyeva et al., 2020) proposed merely using standard periodic functions themselves or their linear combinations as activation functions, which only work well on some shallow and simple models. On this basis, work (Liu et al., 2020) introduced the Snake function, i.e., $x + sin^2(x)$, as the activation function. However, we observed that it can fit periodic functions to a certain extent, but its effect is limited, as demonstrated in Appendix D. Therefore, although some previous studies have attempted to integrate the periodic information into neural networks, their actual performance and range of applications remain heavily constrained.\nFourier-based Neural Network. Previous studies have explored Fourier-based neural networks to enhance the computational tasks (Zuo & Cai, 2005; Tan, 2006; Zuo & Cai, 2008; Zuo et al., 2008; Li et al., 2021; Chen et al., 2022). Fourier Neural Networks (Silvescu, 1999; Ngom & Marin, 2021) are shallow feedforward networks that employ cosine activation functions to map inputs to their Fourier decompositions. Work (Lee et al., 2021) directly utilized the Fourier Series constructed by a shallow neural network for generating periodic signals. In addition, work (Jiang et al., 2022) introduces Fourier Series at the end of models to embed periodic components within the network. These approaches generally possess a similar principle as Eq. (3), using a neural network to simulate the formula of Fourier Series. However, this leads to the same problem as in Eq. (5), i.e., they are hard to serve as building blocks for deep neural networks, which limits these approaches' capabilities.\nIn this paper, we design FAN to address these challenges, which performs exceptionally well on periodicity modeling and a range of real-world tasks."}, {"title": "Discussion", "content": "In this section, we mainly discuss the expressive power and application scope of FAN as follows.\nFirst, FAN theoretically possesses the same expressive power as MLP as it also adheres to the universal approximation theorem, which ensures its capacity for functional approximation. Moreover, FAN introduces an important enhancement by explicitly incorporating periodicity, a feature absent in traditional MLPs. Through this design, FAN not only retains the capabilities of MLP but also enhances its ability to capture periodic characteristics in data. Therefore, FAN can be seen as a strong alternative to MLP.\nSecond, beyond tasks that explicitly require periodicity modeling, FAN also has utility in a broader range of applications. This has been evidenced by our experiments on real-world tasks, such as symbolic formula representation, time series forecasting, and language modeling, where FAN outperforms MLP and other baselines. In fact, many machine learning tasks may harbor hidden forms of periodicity, even without explicit requirements to include periodicity, such as mathematical operations and logic reasoning. If the neural network lacks the ability to model periodic components, it could impair its learning efficiency. From a deeper perspective, periodicity is not just a data feature but reflects a form of structural knowledge-one that allows for the transfer and reuse of abstract rules and principles across different contexts."}, {"title": "Conclusion", "content": "In this paper, we have proposed Fourier Analysis Network (FAN), a novel neural network architecture for tackling the problem of periodicity modeling, which utilizes Fourier Series to facilitate capturing the underlying principles within data and reasoning. Experimental results demonstrate that FAN can successfully fit a variety of both basic and complex periodic functions, whereas other"}, {"title": "MLP", "content": "The MLP layer \u03a6(x) is defined as:\n\u03a6(x) = \u03c3(Bm + Wmx),\nwhere $B_m \\in \\mathbb{R}^{d_m}$ and $W_p \\in \\mathbb{R}^{d_x\\times d_m}$ are learnable parameters with the hyperparameter $d_m$ indicating the first dimension of $W_m$, \u03c3 denotes the activation function, and MLP can be defined as the stacking of the MLP layer \u03a6(x):\nMLP(x) = \u03a6L \u00b0 \u03a6L\u22121 \u00b0 \u00b7 \u00b7 \u00b7 \u00b0 \u03a61 \u00b0 x,\nwhere\n\u03a6l(x) =\n{\n\u03c3(Bm + Wmx), if l < L,\nBL + WLx, if l = L,\n}"}, {"title": "Additional Experiments", "content": null}, {"title": "Experimental Details", "content": null}, {"title": "Setup of Periodicity Modeling", "content": "In periodicity modeling tasks, FAN, MLP, and KAN each consist of three layers with comparable FLOPs, while the Transformer model comprises twelve layers. For consistency, we set the hidden layer dimension ($d_p$) to 2048 for FAN, MLP, and Transformer. In the case of KAN, we follow its original paper (Liu et al., 2024), where the spline order (K) and the number of spline intervals (G) are set to 3 and 50, respectively. We apply a learning rate of 1 \u00d7 10-5 for training all models. We ensured that the data density of each period in tasks was consistent, meaning that each cycle contained a fixed quantity of 10,000 training data points."}, {"title": "Setup of Symbolic Formula Representation", "content": "In symbolic formula representation tasks, we used the create_dataset function from the official KAN repository to generate the datasets. Each dataset contains 3000 training samples and 1000"}, {"title": "Setup of Time Series Forecasting", "content": "In the time series forecasting task, we implement our model based on the codebase by (Wu et al., 2021b). Each model comprises 2 encoder layers and 1 decoder layer. We fix the hidden size for both the Transformer and our model at 512, with the feedforward dimension set to 2048 (four times the hidden size). The parameter sizes detailed in the main text correspond to the Exchange dataset; variations in the number of variables across different datasets influence the linear layers in the model. We adjust the hidden sizes of the other models to align with the Transformer parameters for fairness."}, {"title": "Setup of Language Modeling", "content": "In language modeling task, we employ the BERT tokenizer (Devlin et al., 2018) and an embedding layer with a dimensionality of 768, except for Mamba, which adheres to its default settings as specified in the original paper (Gu & Dao, 2023). The architecture features 4, 24, and 12 layers with hidden sizes of 1800, 768, and 768 for LSTM, Mamba, and Transformers, respectively. To mitigate training stagnation in deeper LSTM models, we reduce the number of layers while increasing the hidden size to balance the parameters. Importantly, Mamba's layer count is twice that of a similarly sized Transformer, as each layer consists of two Mamba blocks (Multihead attention block + MLP block)."}, {"title": "Comparison of FAN and Snake Activation Function", "content": null}]}