{"title": "Single cell resolution 3D imaging and segmentation\nwithin intact live tissues", "authors": ["G. Paci", "P. Vicente-Munuera", "I. Fernandez-Mosquera", "A. Miranda", "K. Lau", "Q. Zhang", "R. Barrientos", "Y. Mao"], "abstract": "Epithelial cells form diverse structures from squamous spherical organoids to densely packed\npseudostratified folded tissues. Quantification of cellular properties in these contexts requires\nhigh-resolution deep imaging and computational techniques to achieve truthful three-\ndimensional (3D) structural features. Here, we describe a detailed step-by-step protocol for\nsample preparation, imaging and deep-learning-assisted cell segmentation to achieve\naccurate quantification of fluorescently labelled individual cells in 3D within live tissues. We\nshare the \"lessons learned\u201d through troubleshooting 3D imaging of Drosophila wing discs,\nincluding considerations on the choice of microscopy modality and settings (objective, sample\nmounting) and available segmentation methods. In addition, we include a computational\npipeline alongside custom code to assist replication of the protocol. While we focus on the\nsegmentation of cell outlines from membrane labelling, this protocol applies to a wide variety\nof samples, and we believe it will be valuable for studying other tissues that demand complex\nanalysis in 3D.", "sections": [{"title": "Introduction", "content": "Epithelial cells can display an astounding variety of cell shapes, from very thin and flat cells in\nsquamous epithelia to extremely tall and tortuous in densely packed pseudostratified epithelia.\nTechnological development in microscopy including 2- and 3-photon excitation, adaptive\noptics and improved objectives now allow imaging these tissues at higher resolution and depth\n(1, 2), unlocking the possibility of single-cell resolution quantification. Artificial Intelligence (AI)\nis in vogue, and its impact on cell biology is equally significant (3, 4). The use of Al algorithms\naims at reducing the time for human annotations and corrections. In particular, the process of\nidentifying individual cells (semantic segmentation) in 3D can be very time consuming (5).\nDeep learning algorithms (a subfield within Al) has proven useful to analyse tissues in 3D (6-\n8). However, the accuracy of deep learning models on a given dataset relies on its similarity\nto the dataset used to train it. To tackle this, transfer learning has been used, where only the\nlast layer of weights is re-trained on a pre-trained neural network. However, to utilize transfer\nlearning, annotated ground truth data is required and generating it via manual annotation is\nvery time-consuming. A solution to this has been proposed: human-in-the-loop (9, 10) system\nthat combine the advantages of Al-assisted prediction with minimal expert user input, which\nis the inspiration for the pipeline proposed here."}, {"title": "Methods", "content": "1. Materials\nFly stocks\nDrosophila melanogaster membrane-labelled lines yw; Ubi-GFP-CAAX (CAAX-GFP, DGRC\n109824 FBID: FBtp0011013) or NubGal4, UAS-myrGFP (nubbin-Gal4 FBID: FBti0016825\nrecombined with myrGFP, gift of Thompson group).\nReagents\nCorning Cell-Tak Cell and Tissue Adhesive, 1 mg (product number 354240)\nRound plastic dish \u2013 60 Thermo Scientific Nunc Cell Culture Dishes (product number 150462)\nFluorescent beads - PS-Speck Microscope Point Source Kit (Invitrogen, product number\nP7220)\nEquipment\nMicroscopy\nLeica Microsystems Stellaris 8 DIVE microscope equipped with a 25x water immersion HC\nIRAPO L motCORR objective (NA=1) and a dual-beam Coherent Discovery multiphoton laser\nwith a tuneable laser line for excitation.\nWorkstations\nTwo computers were used to perform the segmentation protocol: a Windows and an Ubuntu\nLinux machine. Both with the following hardware:\nBase: Dell Precision 5820 Tower XCTO.\nProcessor: Intel\u00ae Xeon\u00ae W-2223 (8.25 MB cache, 4 cores, 8 threads, 3.60 GHz to\n3.90 GHz Turbo, 120 W).\nGraphic card: NVIDIA\u00ae RTXTM A5000, 24 GB GDDR6, 4 DP.\nRAM memory: 64 GB, 4 x 16 GB, DDR4, 2933 MHz, ECC.\nTwo hard drives: 2 TB, 7200 RPM, 3.5-inch, SATA, HDD; 512 GB, M.2, PCIe NVMe,\nSSD, Class 40.\nSoftware\nHyugens Professional version (Scientific Volume Imaging, The Netherlands, http://svi.nl),\nsection 3.1.\nCellpose3 (14, 17) https://www.cellpose.org, sections 3.1, 4.1, and 4.5.\nNapari (16) https://napari.org/stable/ with the following collection of plugins installed: 'devbio-\nnapari' https://github.com/haesleinhuepf/devbio-napari, sections 4.2, and 5; EpiTools (18, 19)\nhttps://github.com/epitools/epitools, section 4.3.\nFIJI: Fiji is just ImageJ (20) https://imagej.net/software/fiji/, sections 4.4, and 5.1.\nTrackMate (15) https://imagej.net/plugins/trackmate/, section 4.4."}, {"title": "Drosophila melanogaster husbandry, wing disc dissection\nand mounting", "content": "Fly husbandry:\n1. Raise fly stocks of the desired genotype on standard cornmeal molasses fly food\nmedium at 25\u00b0C. Per 1L, the fly food contained 10g agar, 15g sucrose, 33g glucose,\n35 g years, 15g maize meal, 10g wheat germ, 30g treacle, 7.22g soya flour, 1g nipagin,\n5mL propionic acid. To visualise cell membranes, we used flies of the genotype ubi-\nCAAX-GFP or NubGal4, UAS-MyrGFP.\n2. 5-6 days before the target experiment day, flip the flies onto a new fresh food vial.\nWing disc dissection and mounting:\n1. Remove third instar larvae gently from the food vials, rinse them in PBS and transfer\nthem into a glass dissection well. Dissect wing imaginal discs using forceps in culture\nmedia: Shields and Sang M3 media (Merck) supplemented with 2% FBS (Merck), 1%\npen/strep (Gibco), 3 ng/ml ecdysone (Merck) and 2 ng/ml insulin (Merck).\n2. Prepare dishes for mounting: pipette a thin horizontal stripe of Cell-Tak at the centre\nof a plastic round cell culture dish and place it on a heated plate for 10 min to dry.\n3. Mount wing discs: fill the dish with 5 mL of culture media, aspirate the wing discs with\na 20 \u00b5L pipette and transfer them into the dish, away from the Cell-Tak. By moving the\nforceps rapidly in proximity to the discs (whisking motion), gently move the discs\nwithout touching them by causing local media flow. In this way, discs float slightly and\ncan be moved to the desired position onto the Cell-Tak strip, letting them settle down.\n4. Bring samples to the microscope to be imaged immediately after mounting.\n**As an alternative mounting option, discs can be mounted between a coverslip and\nrectangular coverglass using two strips of double-sided tape to create a small vertical channel\nfilled with 10 \u00b5L of medium. This is suitable for imaging on both upright and inverted\nmicroscopes."}, {"title": "Imaging", "content": "For imaging, the steps are as follows:\n1. Bring the mounted dish to the upright microscope and gently dip the objective lens in\nthe media, use brightfield imaging to identify and focus on the wing disc to be imaged.\n**In our experience, objective choice and optimal sample mounting is critical to\nminimise signal loss at depth: a dipping objective minimises refractive index\nmismatches and multiphoton laser attenuation.\n2. Tune the laser line to a wavelength of 924 nm for two-photon GFP imaging. Adjust\nlaser intensity to avoid saturation of the signal on the apical side while maximising the\nsignal at deeper planes. Adjust zoom to the region of interest to be imaged, we found\na zoom of 8 to be a good compromise.\n**We found that the fluorophore excitation peaks set by default in the microscope\nsoftware are not always optimal so recommend initially testing a set of wavelengths to\nmaximise the signal obtained."}, {"title": "Pre-processing: image restoration", "content": "Deconvolution is a powerful approach that aims to restore an image degraded by blurring and\nnoise to improve image quality while preserving the structures of interest. We tested two\ndeconvolution/deblurring strategies to see whether they could improve image quality and\nsegmentation outcomes.\nDeconvolution with a measured PSF\nFor the most accurate results, an experimental point spread function (PSF) needs to be\nmeasured using fluorescent beads on the same microscope used to acquire the images to be\nsegmented:\n1. Prepare a sample of fluorescent beads and image it using the same microscopy\nsettings used for the samples to be segmented.\n2. In Hyugens, distil a PSF using the beads image.\n3. In Hyugens, deconvolve the samples to be segmented using the measured PSF.\n4. Once you have obtained a deconvolved image, you can drag and drop it into Cellpose.\nIn Cellpose, segment the image using the steps explained in the \u201cInitial segmentation:\nCellpose\" section.\n** We used Hyugens software but many alternative deconvolution software are available.\nDenoising with Cellpose 3.0\n1. Denoise/deblur/upscale your images using the 'one_click3' model. Go to the GUI and\npress 'One_click3'.\n2. Press 'run model' to obtain the segmentation.\n**You can find more technical instructions in the '3D-deep-segmentation-protocol' jupyter\nnotebook (section \u201c2. Improving raw images for segmentation: Denoising\u201d).\nOverall, while both image restoration methods improved the visual appearance of the images\ntested, this did not correspond to improved segmentation results. In particular, running the\nsegmentation pipeline on deconvolved images resulted in highly fragmented labels. We did\nhowever find that acquiring images of beads was a useful reference for checking the PSF\nquality and helped us fine-tune some microscopy parameters, for example the objective\ncorrection collar."}, {"title": "Segmentation", "content": "To obtain a 3D instance segmentation at a single cellular level (each cell individually\nsegmented), we have tried different software and pipelines (see Discussion). The approach\nthat gave best results starting with no ground truth data is the following (Figure 1c):\n1) Obtain an initial segmentation with Cellpose, using the pre-trained 'cyto3' model.\n2) Manually correct the segmentation of each individual 2D slice.\n3) Use TrackMate to correct 3D stitching issues automatically and manually correct the\nremaining issues.\n4) Re-train the 'cyto3' model with the corrected ground truth dataset.\n5) Repeat 1-4 with the new model re-trained for every new image to be segmented.\nTo reproduce these steps and the following sections, we have developed a jupyter notebook:\nhttps://colab.research.google.com/drive/1ToQjW9W42gO1wZmQa4qYr4GbYbPGOS-w\n**For a first attempt, we recommend using Google Colab to get an intuition on how things\nwork. Google Colab is an on-line jupyter notebook that requires no setup and is free to use.\nFor longer fine-tuning training sessions, Google Colab has a policy that tasks cannot be run\nfor longer than 1 day. Thus, we recommend using a local computer for longer fine-tuning\nsessions.\n**We observed that Google Colab may close your session if you have exceeded its (not public)\nusage."}, {"title": "Initial segmentation: Cellpose", "content": "1. Install Cellpose, according to your operating system, from:\nhttps://github.com/mouseland/cellpose?tab=readme-ov-file#option-1-installation-\ninstructions-with-conda\n2. In the same conda prompt or terminal where you installed your Cellpose, type:\npython -m cellpose --Zstack\n**You need this step to use the graphical user interface (GUI) to make sure Cellpose\nreads the stack correctly. You can try the default GUI, but there might be instances\nthat the image is not formatted correctly and it will open it as a 2D image.\nhttps://cellpose.readthedocs.io/en/latest/do3d.html#input-format\n3. Once the GUI is open, you can drag and drop your stack of images as a single file or\nuse File > Load image > Select your image.\n4. We recommend the Graphical User Interface (GUI) for the initial exploration of\nparameters that fit the most.\n**For this initial exploration, we are looking for the least time-consuming experience\nwhilst obtaining the best segmentation possible. Here, a good cell segmentation\nmeans that as many cells as possible are visually represented correctly: cells occupy\nall their cytoplasm space including the edge membrane without any empty space; the\nsame cell is identified correctly throughout the stack; cells are not fragmented into\ndifferent objects (or identifiers/colours).\n5. Calibrate automatically the cell diameter by pressing 'calibrate' under 'Segmentation'\non the left side panel. The number next to the button may change from 30 to the auto\ncalibrated. You can adjust it further based on the segmentation output.\n**Images are rescaled to match the images the model was trained from.\n**If cells are looking fragmented, it means you might need to increase the cell diameter.\nIf cells are merged together, it might mean you should decrease this parameter."}, {"title": "Manual correction of labels", "content": "1. Install napari (16) in a new environment following these instructions:\nhttps://napari.org/stable/tutorials/fundamentals/installation.html#napari-installation\n2. Open napari by typing 'napari' in the terminal window. Install the following plugins:\n\"devbio-napari\" through the user interface under Plugins > Install/Uninstall plugins\n**This plugin is a compilation of very useful packages; you can look for relevant plugins\non the napari hub https://www.napari-hub.org\n** Note that the first time napari is opened it might take a couple of minutes to launch.\n3. Drag and drop the raw file fluorescence image, and the segmented image to be\ncorrected into the napari window.\n4. Right-click on the segmented image > Convert to labels (in case it is not already loaded\nas a labels layer). Select that layer by clicking on it. Decrease its opacity to see better\nthe 'raw' image (recommended value of 0.25). Enhance the contrast of the raw image\nuntil you see the edges of the cells, by sliding the left side button towards the left side.\n**You should be able to see both the labels image and the cells' real edges.\n**You can also quickly toggle the visibility of a layer to check if the segmentation\nmatches the cells' shape.\n5. Manually correct the cells using the following tools:\na. Dropper or \u2018Pick model'. Use it to identify the ID of the cell. When you click on\na cell, the label will change to a number.\nb. 'Shuffle colours'. Use it to check if cells represented by the same colour are a\nsingle cell, which you would have to split into two, or different ones.\n**Note that a given colour might be representing two different cells due to the\nreduced number of colours in the palette. Using 'Shuffle colours' will help you\nwith that.\nc. 'Paintbrush'. To edit the segmented labels, simply pick a cell's ID to paint. Use\nthe brush to modify regions belonging to that cell.\nd. 'Fill bucket'. Use it to fill empty (background) spaces with the selected cell ID.\ne. 'Eraser'. Remove regions of segmented areas transforming them into\nbackground.\nf. 'n edit dim'. You can use the 'Paintbrush', 'Fill bucket', and 'Eraser' in 2D or 3D.\nIf 'n edit dim' is 2, these tools will only affect the selected z-slice. If it is 3, you\nwill be editing in three dimensions (layers above or below, based on the brush\nsize).\ng. 'contiguous'. (only \u2018Fill bucket') If checked, it will only change pixels whose\nvalues are the same as the selected one and connected to it. If not checked,\nand if (for instance) you pick the background ID, all the background will be filled.\nh. 'preserve labels'. If checked, only the background pixels will be modified by the\n'Paintbrush', 'Fill Bucket', and 'Eraser'.\ni. 'Show selected'. It will only show the selected cell 'ID'.\n**Very useful to see if there are fragmented cells.\nj. 'Plugins > napari-segment-blobs-and-things-with-membranes > Manually\nmerge labels'.\ni. Create a 'New points layer' by clicking on the button most to the left.\nii.\nSelect 'Add point' on the top of your left bar.\niii.\nClick in the centre of your cells to put them together as one. Press 'run\u2019.\n**You should see how the cells are now the same.\n**Delete the points layer afterwards by clicking on the bin.\nk. 'Plugins > napari-segment-blobs-and-things-with-membranes > Manually split\nlabels'.\ni.\nCreate a 'New points layer' by clicking on the button most to the left.\nii.\nSelect 'Add point' on the top of your left bar.\niii.\nClick in the centre of your cells to be split. Press 'run'."}, {"title": "Segmentation quality assessment", "content": "**You can do this process with multiple cells.\n**You might need to correct the splitting in 3D.\n**Both functions perform the split and merging in 3D. If you just want to perform\nin 2D for a single slice, we recommend using the brush.\n6. Time of corrections depends on the quality of the segmentation obtained after\nautomatic corrections and the quality of the raw image.\n** For us, on average, a week of a person working 37.5 hours per week on a full stack\nwith 100 cells.\n** We found that it's best to first manually correct labels on each 2D slice, then perform 3D\nautomatic corrections with TrackMate. If, for example, there are cells fused together or split\ninto multiple parts in 2D, the 3D stitching in TrackMate will be impacted. Thus, we recommend\nfirst having a good 2D segmentation and then, focusing on improving its 3D stitching (see\nsection 4.4).\nTo reproduce this section, check this section from the following jupyter notebook here.\n4.3. Segmentation quality assessment\nFor heterogeneous cell shapes along the z axis, we found that most manual correction was\nneeded to improve the 3D stitching rather than the 2D x-y planes. In addition to visually\nassessing the quality of segmentation, we introduce a new metric to evaluate the 3D stitching\nquality: Cell Persistence score, which assesses how many labels (cells) are followed through\na minimum % of z planes. A perfect Cell Persistence score would be achieved when each cell\nis tracked throughout the whole stack with no interruption.\n**\n1. Use your previously set up napari environment to install the plugin 'EpiTools'\nhttps://github.com/epitools/epitools.\n2. To calculate the Cell Persistence score, click on 'Plugins > Calculate Cell Persistence\nscore'. A widget will open on the right-hand side of the GUI.\n3. Select your raw image in 'image', and your segmented image in 'labels'.\na. The percentage of z-slices indicate the number of z-planes required for cells to\nbe counted as successfully stitched in 3D.\n** threshold values can be adjusted depending on how strict users want the\nstitching to be, for example we used 80%.\nb. Show overlay. If selected, it will create another labels layer with the cells that\nfit the criteria.\nc. Run metrics. If ticked, it will run our module to calculate cell statistics on only\nthe corrected cells.\n4. Press 'Run'. After a while, it will output the number of good cells that have been\nsuccessfully segmented continuously in the selected percentage of z-planes.\n** Use this function to overlay a selection of the best reconstructed cells in 3D, to help\nidentify problem areas that need further correcting. If a patch of successfully\nreconstructed cells is identified and has sufficient N number, the exported layer can be\nused for quantification ticking 'run metrics' or exported for further analysis.\nAlternatively, you can compute the Cell Persistence Score in a Python console or Google\nColab.\nThis step is quite important and specific to pseudostratified or columnar tissues, where we\nexpect most of the cells to be present in all layers from the top to the bottom of the tissue."}, {"title": "Tracking as custom z-stitching", "content": "For very tall tissues where cells change shape and position along z, we found that the built-in\nstitching in Cellpose was not sufficient to correctly follow and stitch the cells along z. We found\nthat the tracking package TrackMate could be used as a powerful custom z-stitching algorithm.\nThe main advantage is the possibility of setting multiple custom parameters, such as the\nnumber of planes an object is allowed to \u201cdisappear\u201d and the distance that the object can travel\nbetween frames.\n1. Download FIJI (Fiji is just ImageJ) from https://imagej.net/software/fiji/downloads. It is\na portable application, so you only need to unpack and open it by double-clicking on\nthe FIJI executable.\n2. Open your 3D segmented image to be corrected.\n3. Click on Plugins > Tracking > TrackMate. There would be a pop-window asking you to\nswap the z and time-axis. The answer should be 'Yes'.\n**If you do not swap these axes, you would see circles around the cells instead of the\nboundaries of each cell in pink.\n4. Click 'next'. Select the 'Label image detector' which is providing the segmented file and\nclick 'Next'.\n**You can preview to check if the IDs are ok.\n5. It will process the labels. Once it is finished, click 'Next'.\n6. Do not change the initial threshold, click 'Next'. It will process it again, click 'Next'.\n7. Select a 'tracker', we used 'Kalman tracker'. 'Next'.\n8. Set the tracker parameters: we found that slightly increasing the search radius\ncompared to default parameters improved the tracking: set 'Initial radius' to 20, 'Search\nradius' to 25 and 'Number of missing frames' to 2. Click 'Next'. Once it is processed,\nclick 'Next' again and a second time.\n9. In the 'Set filters on tracks' window, pick relevant tracking metrics (e.g., distance\ntravelled, splitting events) to apply a colour coding and evaluate the tracking. If needed,\nincorrect tracks can be filtered out at this stage.\n10. Check what you want to display and click 'Next', and 'Next' again.\n11. Export your new labelled image by picking 'Export label image' and clicking \u2018Execute'.\nLeave it as is and click 'OK'.\n12. You can now save the new segmented image.\n**Note that IDs might differ from the original IDs. Thus, the cell with (e.g.) ID 1 in the\noriginal image might not be the cell with ID 1 in the new segmented image."}, {"title": "Fine-tuning initial segmentation", "content": "1. Cellpose pre-trained models were trained using 2D images. Therefore, to re-train any\nof the available models, 2D images should be used as input. To do that, 3D images\nneed to be transformed into XY, YZ, and XZ slices.\na. We have developed a code to generate 3D sections (section 5 \u201cRefining the\nsegmentation: Cellpose fine-tuning\u201d).\nb. Code from Cellpose is also now available to do this.\nhttps://github.com/MouseLand/cellpose/blob/main/cellpose/gui/make train.py\n2. Split the dataset into training and test sets either manually or with a python function\nlike 'train_test_split' from Scikit-learn (by default 25% test, 75% training). This is a step\nrequired to train your data.\n3. Re-train a given model with your ground truth data (transfer learning). We recommend\nkeeping the default parameters.\n**Based on our quantifications, depending on the number of images used, more\nepochs will give a better prediction, but it takes longer (3468 2D slices images, 2000\nepochs: 151393s seconds; 100 epochs: 7757s seconds).\n**Estimated time: dependent on number of 2D images; for 3468 2D images, approx. 2\ndays of computing time with the workstations described above). For additional\ninformation, please check:\nhttps://cellpose.readthedocs.io/en/latest/benchmark.html\nAdditionally, we encountered some common issues with this step and provide possible\nsolutions:\na. Problem with the minimum number of labels on an image.\n**Fix: use '--min_train_masks' with 0.\nb. Overfitting issues. If you re-train a model with images with all cells and no\nbackground, when you input images with some empty space (background) that\nspace will be filled with cells.\n**Fix: use images with some black background or empty space (no labels) in\nyour images.\nc. Even though we recommend to re-train your model using python, you can also\nuse the GUI to re-train it. However, you might get: 'IndexError: list index out of\nrange'.\n**Fix: you would need to obtain the_seg.npy files beforehand in python. **Note:\nthis error will only happen using the GUI of cellpose.\n4. Predict images with your new model.\na. Click Models > Add custom torch model to GUI > Pick the model file you have\ncreated.\nb. Under 'Other models', you should find your new model by clicking on the drop-\ndown button 'custom models'. Pick your model.\nc. Click 'run'.\n**This code can be reproduced at \u20183D-deep-segmentation-protocol' jupyter notebook (section\n\u201cRefining the segmentation: Cellpose fine-tuning\u201d)."}, {"title": "3D Analysis and visualisation with napari", "content": "1. Use the same environment you used for the 'manual segmentation' (see \u201cManual\nsegmentation\" section).\n2. Open napari and drag and drop your raw image.\n3. Drag and drop your segmented image. If it is not seen as a 'labelled image', right-click\non the layer corresponding to the segmentation and select 'convert to labels'.\n**If you do not see your labels layer, it could be hidden behind another layer. The first\nimage in the list is the top layer and layers below will be only visible if opacity allows.\n4. To visualise the cells in 3D, click on the button (2nd starting from the left, left-bottom\npanel) to swap from 2D to 3D view.\n**You may see that your cells do not look as expected (e.g., much flatter). If so, you\nmay have to update the voxel size to reflect the z-slicing of the data.\n5. To change the voxel size of both your images: 'Tools > Utilities > Set voxel size of all\nlayers'. Change the values to be like your acquired image properties. **We typically\nopen an image, 'FIJI > Image > Properties' and copy the voxel height, width and depth\nto napari.\n**Because you have changed the view, your image may have disappeared. Reset to\nits original view (house icon, left-bottom panel) and then, click on the button to change\nto 3D view.\n6. Click 'Run'. You should see your cells with the correct aspect ratio.\n**We recommend visualising your raw image with the rendering engine\n\"Attenuated_mip\u201d, where you can adjust the attenuation depending on your images. If\nyou want crispier images, you may change its interpolation.\n7. You can now export your cells as displayed by clicking on \u2018File > Save screenshot...' \n8. (Optional) To save a 3D animated video of your 3D stack, you can use the napari-\nanimation plugin https://github.com/napari/napari-animation."}, {"title": "Analysis", "content": "1. To obtain cellular features, we recommend using the 'regionprops' function in napari:\n'Tools > Measurements table > Regionpops'. It will appear on the top-right as a new\npanel with the settings.\n2. Make sure your raw image is selected on 'image' and segmented on 'labels'.\n3. Tick or untick the features of interest and click on 'Run'.\n**Depending on your question of study you might pick different measurements, but we\nrecommend to tick: 'size', 'intensity', 'perimeter', and 'shape'.\n4. On the bottom side you will see a table with each individual segmented cell as a row\nand the columns as features.\n**Note that the columns are named with the same name as in 2D, but they are 3D\nfeatures. Thus, 'area' would be 'volume', and so on.\nhttps://github.com/haesleinhuepf/napari-skimage-regionprops\n5. To save the table you can either copy to the clipboard (clicking on that button) and\npaste it on any processing software sheet (e.g. Microsoft Excel), or you can save it as\na comma-separated-values (or csv) file.\n**You can use this table to search for possible outliers, like cells with very small/large\nvolumes or weird shapes. You can display cells that look odd by using the 'show\nselected' feature (as explained before) and correct it if necessary."}, {"title": "Discussion", "content": "Here we have detailed an end-to-end protocol covering all aspects from sample preparation\nand imaging to 3D segmentation and quantification, which has been optimised to analyse\nindividual cells in Drosophila wing discs. Improvements in sample mounting and imaging have\nbeen essential for our ability to segment epithelial cells individually within a 50 \u00b5m thick tissue.\nThe largest enhancements have been using 2-photon for excitation and imaging the wing discs\nwith a water dipping objective, which minimizes refractive index mismatches between the\nsample and the objective. For this, we describe an optimised sample mounting method that\nleverages on a biocompatible adhesive to affix wing discs to the bottom of a cell culture dish,\nfilled with growth media. This mounting method is suitable for a wide variety of tissues and\ntissue slices.\nIn terms of the image analysis aspects, we presented a human-in-the-loop pipeline that\nleverages on existing open-source software. In terms of pre-processing, we tested two image\nrestoration method and found that, unexpectedly, they did not aid in cell segmentation even if\nthe visual appearance of images improved. In particular, Cellpose models appeared to perform\nworse on deconvolved data, probably because they were trained on raw, noisy images. For\nsegmentation, we find that Cellpose pre-trained models work reasonably well 'out-of-the-box'.\nCrucially, the quality of the segmentation substantially increases if we fine-tune the model\nusing our dataset, even starting from a single image stack containing hundreds of cells. While\nwe acknowledge the powerful properties of fine-tuning, we also want to highlight that it can be\ncounterproductive. As explained in section 4.4, if a model is fine-tuned with images completely\nfilled by cells (no background areas), the newly trained model might always predict cells, even\nin background regions, an indication of model overfitting.\nIn complex pseudostratified tissues like the Drosophila wing disc, a main challenge is stitching\nof the 2D segmentations along the z-axis and we found that commonly used quality metrics\n(e.g. intersection over union (21)) were not useful to assess this when ground truth data is not\navailable. Thus, we introduced the Cell Persistence Score, which quantifies how well the cells\nare being tracked in the Z axis. We have found that using the Cell Persistence score aids us\nto accurately assess how the segmentation improves during manual correction and to identify\nproblematic regions in the image.\nWe expect our protocol to shed light into the challenging task of analysing thick tissues at a\ncellular level, offering a robust pipeline adaptable to other complex biological systems. In the\nfuture, aided by advancements in computational tools and imaging techniques, we expect to\nsee more quantitative analysis of tissues across a wide range of biological contexts."}]}