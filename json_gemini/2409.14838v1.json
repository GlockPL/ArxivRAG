{"title": "MICSim: A Modular Simulator for Mixed-signal Compute-in-Memory based Al Accelerator", "authors": ["Cong Wang", "Zeming Chen", "Shanshi Huang"], "abstract": "This work introduces MICSim, an open-source, pre-circuit simulator designed for early-stage evaluation of chip-level software performance and hardware overhead of mixed-signal compute-in-memory (CIM) accelerators. MICSim features a modular design, allowing easy multi-level co-design and design space exploration. Modular-ized from the state-of-the-art CIM simulator NeuroSim, MICSim provides a highly configurable simulation framework supporting multiple quantization algorithms, diverse circuit/architecture designs, and different memory devices. This modular approach also allows MICSim to be effectively extended to accommodate new designs.\nMICSim natively supports evaluating accelerators' software and hardware performance for CNNs and Transformers in Python, lever-aging the popular PyTorch and HuggingFace Transformers frame-works. These capabilities make MICSim highly adaptive when simulating different networks and user-friendly. This work demonstrates that MICSim can easily be combined with optimization strategies to perform design space exploration and used for chip-level Trans-formers CIM accelerators evaluation. Also, MICSim can achieve a 9\u00d7 ~ 32\u00d7 speedup of NeuroSim through a statistic-based average mode proposed by this work.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep neural networks (DNNs) have been pivotal in the development of artificial intelligence in recent years. Early on, CNN-based neural networks achieved significant breakthroughs in the field of computer vision[1]. More recently, Transformer-based models have made remarkable strides [2-4]. Nowadays, both CNNs and Transformers are scaling up in size for advanced performance. Besides the powerful model innovations, the explosion of DNNs also owes to the performance improvement of hardware. However, the processors based on the traditional von Neuman architecture, such as GPUs, face throughput and energy efficiency limitations due to extensive data transfers between the process and storage units.\nThis 'memory wall' problem becomes more and more severe as the scale of model parameters increases. A shift toward memory-oriented architectures promises to break the 'memory wall' bottleneck. Compute-in-memory (CIM) technology, which performs in-place multiply-and-accumulation(MAC) within memory cross-bars, minimizing the need for data movement and demonstrating remarkable energy efficiency for deep learning applications [5]. Among different CIM implementations, the mixed-signal CIM implements the MAC operation in the analog domain and usually embraces advanced non-volatile memory(eNVM) devices, characterized by high density and low energy consumption. Impressive energy efficiencies have been reported for mixed-signal CIM accel-eration of CNNs [6-8] and Transformers[9-11].\nAs an emerging architecture, the design of CIM accelerators evolves rapidly and requires cross-layer optimization of algorithms, architecture, circuits, and devices. Therefore, a simulator for early-stage estimation and exploration is necessary before the timing-and resource-consuming deployment. There are already a bunch of simulator platforms[12-14] that have demonstrated powerful capa-bility at system-level or circuit-level CIM accelerator performance evaluation. However, they still have some limitations in terms of generality and flexibility. First, current mainstream CIM pre-circuit simulators (e.g., NeuroSim [14], NVSim [12]) are focused on CNNs. Extending them to the Transformer needs elaborate and time-consuming work due to the tight coupling between the simulator's code structure and the network type. Second, hardware-software co-design techniques like quantization are essential in modern AI accelerator design. While quantization algorithms are evolv-ing rapidly, quickly applying them-whether hardware-specific or not-to CIM hardware is challenging in current simulators. This dif-ficulty arises because these simulators generally treat quantization algorithms as the inherent property, making it hard to incorporate new or updated methods. Third, current simulators are generally built upon certain architecture/circuit designs, supporting some level of flexibility by parameter tuning. Evaluating new designs or bench-marking across different designs using these simulators needs careful and tedious modification to avoid deviation.\nTo solve the above issues, we propose MICSim, an open-source, pre-circuit, modular simulator that can provide software and hard-ware performance estimation for mixed-signal CIM accelerators with different models. The MICSim is available on GitHub (https:// github.com/ MICSim-official/MICSim_V1.0.git) with main features summarized as follows:\n(1) We base MICSim on NeuroSim but unify the construction models of CNNs and Transformers through modular calcu-lations. This allows MICSim to support both without signifi-cant modifications. For user-friendliness, we use the popular deep learning framework PyTorch for model training and"}, {"title": "2 MICSIM SIMULATOR", "content": "MICSim integrates the parameter quantization and CIM's hardware effects into the neural network layers, such as Convolution/Linear layers, from Pytorch library [15] in a modular manner. This allows for easy integration of CIM operations into various neural networks to validate the software performance. Specifically, for the Trans-formers part, to better integrate into the thriving Transformer-based open-source community, MICSim actively embraces the Trans-formers library [16] from HuggingFace. The corresponding hard-ware overhead is formulated with Python-wrapped circuit mod-ules encapsulating the performance models from silicon validated DNN+NeuroSim V1.3[17].\n2.1 Software Performance Evaluation\nIn the CIM accelerator design, low-precision fixed-point parameters are usually adopted due to the regular structure of the memory array. Also, there will be non-ideal effects, such as limited on/off ratio, cell variation, and ADC quantization loss, introduced in the calculation by mixed-signal operations. All these hardware-specific effects could degrade the software performance if poorly designed. Thus, we modularized the hardware-induced impact of CIM operation to support the combination of different algorithms, architectures, circuits, and devices. The modular modeling is shown in Fig.1.\n2.1.1 Quantizer. Quantizer is used to quantize weight and input from high-precision floating-point (FP) into low-precision integers (INT) according to the quantization schemes. With an empirical observation, the hardware performance will be better with lower pa-rameter precision. Therefore, quantization is always an important"}, {"title": "2.1.2 DigitDecomposer and DigitAssembler.", "content": "Unlike digital systems that typically adopt the binary number system, operands in CIM subarrays could be high-precision digits (k-bit) due to the analog calculation nature. In other words, inputs could be k-bit, repre-sented by $2^k$ voltage levels generated by DACs[8]. And weights could be encoded to different conductance values of 1 - 7-bit [23] based on the types of memory cells. Although CIM operands can be high-precision, the INT outputs from the quantizer may not be directly mapped to the CIM subarray as their precision (N-bit) could be higher than the operands (k-bit) supported by the hard-ware. Thus, we introduce DigitDecomposer to decompose INTs to digits to cover different circuit and device choices. According to the circuit implementation, the conversion from weight INTs to digits could be realized in different ways. In [24, 25], weight parameters are decomposed to digits using a 2'complement-like rule. Other works [6, 26] split weight parameters into positive and negative subarrays and thus decompose them into two groups of unsigned values. Unsigned decomposition could also be achieved by shifting weights by a fixed value [8]. The output of DigitDe-composer could be directly applied in one CIM subarray operation, decoupling the CIM macro from the digital peripherals on-chip. After the MAC operations, decomposed results will be combined by the DigitAssembler block. While all the methods mentioned above for weights could be applied for input INTs decomposition, binary digits are widely used for inputs to eliminate the hardware cost introduced by DACs."}, {"title": "2.1.3 Digit2Cell and DAC.", "content": "Although CIM macros take digits for MAC operation mathematically, the operands are encoded with imperfect analog signals in real implementations. Thus, to mimic the real calculation in the CIM array, we introduce Digit2Cell to leverage imperfection into weight digits. Considering a memory cell with maximum conductance $G_{max}$ and minimum conductance $G_{min}$, to encode k-bit weight digits with the memory cell's conduc-tance, the conductance of each cell is divided into $2^k$ levels between $G_{max}$ and $G_{min}$. Thus, a weight digit $d_i$ will be mapped to cell conductance $G^i$ as below:\n$G^i = d_i \\times \\triangle G + G_{min}, d \\in [0, 2^{k-1}], \\triangle G = \\frac{G_{max} - G_{min}}{2^k-1}$                                                                                                                                                 (1)\nThe conductance is not linearly proportional to $d_n$, due to the $G_{min}$ term, which introduces non-idealities into the CIM operation. In the Digit2Cell module, we normalize the conductance, generalizing weight digit modeling across different devices while maintaining the non-idealities. Thus, weight digit $d_i$ involved in CIM crossbar's analog computation is changed to $d_{cell}^i$ as equation (2). In addition,"}, {"title": "2.1.4 ADC.", "content": "ADC converts the analog MAC results into digital sig-nals. In this digitization process, the quantization loss introduced should be considered. A general process of ADC can be mathemat-ically formulated as a piece-wise function that maps its input to corresponding centers based on configured edges. For a k-bit ADC, it can be represented as:\n$f(x) =  \\begin{cases}\nC_0 & \\text{if } x < ref_0 \\\\\nC_i & \\text{if } ref_{i-1} < x < ref_i, \\text{ for } i=1,..., 2^k-2 \\\\\nC_{2k-1} & \\text{if } x \\geq ref_{2k-2}\n\\end{cases}$                                                                                                                                                                        (3)\nwhere $c_i$ represents the quantized value of each level, and $ref_i$ refers to the reference that defines the edge of each level in the ADC.\nThis general format could represent any ADC by projecting $c_i$ and $ref_i$. MICSim offers two general ADC choices: linear and nonlinear. Linear ADCs have fixed step size between $c_{i-1}$ and $c_i$, while nonlinear ADCs determine their $c_i$ and $ref_i$ based on the statistical distribution of the data. Our modeling of ADC is based on (3) with user-defined $c_i$ and $ref_i$. Thus, it offers great flexibility for users to test their own ADCs without being confined to the existing implementations."}, {"title": "2.2 Hardware Performance Evaluation", "content": "MICSim offers hardware performance evaluation based on a chip architecture similar to NeuroSim. Unlike NeuroSim, which focuses on evaluations of different devices and technology nodes, MICSim is designed for easy implementation of a CIM accelerator with the combination of different algorithms, architecture, circuits, and devices. Thus, flexibility is a key consideration in MICSim's design. By wrapping the circuit modules from NeuroSim, MICSim enables chip-level hardware performance evaluation in an object-oriented and hierarchical manner using Python.\n2.2.1 Chip Architecture. Inheriting the hierarchical chip design from NeuroSim, MICSim features a three-layer architecture consist-ing of Subarray (SA), Processing Engine (PE), and Tile, shown in the left-hand side of Fig.3. This architecture is designed to maximize data reuse for CNNs, but it may not always be efficient for different networks or structures. To address this, MICSim unified the defi-nitions of SA, PE, and Tile, treating all digital circuit modules as objects. This approach allows a chip to be assembled like LEGO,"}, {"title": "2.2.2 Average Data Mode.", "content": "According to [14], the data pattern of the weight/input involved in the computation highly affects the energy consumption and latency of CIM operation. A crucial feature of NeuroSim for accurate hardware overhead estimation is the trace mode, which calculates the latency and energy consumption based on the real data involved in the computation. NeuroSim dumps out real-traced weight/input values during the inference and iterates through them for hardware overhead evaluation. However, this method demands a long runtime and substantial memory usage during simulation, particularly for large models of Transformers. To address this challenge, we propose the average mode, which leverages the statistics of the weight and input data instead of the entire trace.\nFig.4 conceptually illustrates the average mode and its difference from the trace mode. For binarized inputs applied to CIM arrays, a"}, {"title": "3 EXPERIMENT RESULTS", "content": "In this section, we first validate the performance of the average mode and demonstrate its acceleration capabilities. Then, we con-duct a design space exploration using MICSim to showcase the benefits of our modular approach. Finally, we present the evalu-ation of accelerator for transformer-based models using MICSim.\n3.1 Average mode Versus Trace mode\n3.1.1 Performance. As discussed in Section 2.2.2, we adopt the average mode based on data statistics to mimic the trace mode of NeuroSim while improving the calculation efficiency. It is worth noting that the data pattern will not impact the chip area evaluation but will affect the subarray's read latency and dynamic energy. To validate the performance of the average mode, we evaluate hard-ware overhead with the same chip configuration for the two modes. Table 1 compares throughput and energy efficiency for different networks between the average mode and the trace mode. It can"}, {"title": "3.1.2 Runtime.", "content": "Fig.5 shows the runtime comparison between MIC-Sim and NeuroSim for hardware performance evaluation. The av-erage mode in MICSim has less computational load than the trace mode in principal. However, since the MICSim is built in Python for chip-level performance evaluation, the loop through different subarrays is much slower than the C++ loop in NeuroSim. As a result, when evaluating networks with large filters (thus numerous subarrays for weights) but small input feature maps, such as VGG8, MICSim performs even slower than NeuroSim. With the speedup technique of combining subarrays with same memomry untiliza-tion, as described in Section 2.2.2, MICSim improves the estimation of the CNNs up to 9 ~ 32 times faster than NeuroSim despite the slower programming language, as shown in Fig.5. This significant enhancement demonstrates MICSim's efficiency and capability in accelerating mixed-signal compute-in-memory simulations.\n3.2 Design space exploration using MICSim\nMICSim enables cross-layer design space exploration of algorithms (quantization), architectures (digit mapping), circuits (ADC), and de-vices (cell precision, on/off ratios). The decoupled and modularized modeling of these elements allows easy configuration of different designs or extensions to new designs. This section conceptually demonstrates the design space exploration using MICSim for a CNN inference accelerator. For simplicity, we adopt a greedy algorithm for space searching.\n3.2.1 Algorithm level. MICSim supports both post-training quan-tization(PTQ) and quantization-aware training(QAT) of different algorithms in-situ. In this stage, INT output will directly be used for matrix multiplication without introducing any architecture/circuit/ device effect. Thus, the only impact factor on the software per-formance will be quantization. This work explores three QAT al-gorithms for CNN: WAGE, DF, and LSQ, on two CNN networks, VGG8 for Cifar10[27] and ResNet18 for ImageNet(Subset) [28]. Ta-ble 2 shows the minimum precision needed for each network un-der different quantization algorithms. The quantization precision for inputs and weights is consistent, considering the simplicity of"}, {"title": "3.2.2 Architecture and Circuit level.", "content": "As discussed in Section 2.1.2, MICSim supports designs with different architectures for digit map-ping, which not only affect software performance but also impact the hardware performance of the chip. For simplicity, we refer to the three types of circuit architecture from Section 2.1.2 as De-sign1 (2'complement-like), Design2 (positive-negative split), and Design3 (weight shifted). ADC and memory cell precision are also considered while exploring architecture as they are highly coupled. However, an infinite on/off ratio is assumed to eliminate the impact of a certain memory cell. Generally, we prefer lower ADC precision for better hardware performance[29]. Fig.6 shows the software per-formance across different settings. For both VGG8 and ResNet18, Design2 always requires the lowest ADC precision compared to the other two options, while Design3 performs the worst, need-ing higher ADC precision in all cases. Design1's ADC precision requirement is similar to Design2's when cell precision is low, but it increases as cell precision increases.\nTo evaluate the hardware performance, we finalize the ADC precision settings under a 3% accuracy degradation tolerance for different cases (Fig.6). RRAM cells with $R_{on}/R_{off}$ = 6k\u03a9/900k\u03a9 [30] are assumed for hardware performance evaluation, and the results are shown in Fig.7. We can see that when cell precision is 1, Design2 has the largest area overhead. This is because Design2 uses a pair of subarrays to store signed weights, while the other two"}, {"title": "3.2.3 Device level.", "content": "With the accelerator configured from the previ-ous step, we explore some device candidates listed in Table 3. To recapitulate, VGG8 adopts 3-bit input/weights quantized by WAGE while ResNet18 adopts 4-bit input/weights quantized by LSQ. Both employ the architecture based on Design2, with ADC of 3-bit and 5-bit, respectively. Table 3 shows that with a 3% accuracy drop tolerance, only RRAM[30] and FeFET[33] can maintain accuracy at 4-bit precision. The remaining memory cells can only maintain accuracy at a lower bit precision due to the limited on/off ratio. The"}, {"title": "3.2.4 Discussion.", "content": "The above experiments demonstrate a concep-tual process for design space exploration using MICSim. It could be seen that while MICSim decouples the design space into different levels for easy configuration and extension, each sub-level is highly coupled with the others in determining the final software/hardware performance. Thus, we do not explore the design space strictly se-quentially for each sub-level. However, the hardware design choices obtained in Section 3.2.3 could still be sub-optimal. Due to the sim-plicity of configuring MICSim and the short runtime, MICSim could also be integrated into search algorithms such as genetic algorithms or simulated annealing."}, {"title": "3.3 Evaluation of Transformers", "content": "In this section, we demonstrate the evaluation of software and hardware performance of the CIM-based accelerators for Trans-formers using MICSim. The BERT (base) model[3] is fine-tuned on the SST-2 task from the GLUE dataset [35]. All weights and inputs of the model are quantized to 8-bit by the QAT method from the I-BERT[22] method, achieving a baseline accuracy of 90.02%. An RRAM with Ron/Roff = 6k\u03a9/900k\u03a9 with 2-bit per cell[30] is as-sumed for hardware performance evaluation. The other evaluation settings and results are summarized in Table 4. We observe that Design2 needs the least ADC precision, while the ADC requirement of Design3 is strict. This trend is consistent with the conclusions in Section 3.2.2, even though the accelerated model has shifted from CNNs to Transformers. Under these settings, we can see that Design2 has the largest area but achieved the best energy efficiency. In terms of throughput, Design3 achieved the highest throughput, while Design2 is the worst.\nFig.9 provides a breakdown of the hardware overhead of these three designs. We observe that the proportions of overhead for var-ious parts (subarray, buffer, interconnect network (IC)) on the chip"}, {"title": "4 CONCLUSION", "content": "In this work, we present MICSim, a simulator designed to evaluate the software performance and hardware overhead of CIM accelera-tors for CNNs and Transformers. MICSim introduces the average mode, which reduces runtime and memory usage while maintaining accurate hardware overhead estimations. With its modular design, MICSim supports multiple quantization algorithms, and various circuit architecture designs. It could be further extended to new im-plementations and designs easily or be integrated into optimization strategies to effectively performs design space exploration."}]}