{"title": "Unified Lexical Representation for Interpretable Visual-Language Alignment", "authors": ["Yifan Li", "Yikai Wang", "Yanwei Fu", "Dongyu Ru", "Zheng Zhang", "Tong He"], "abstract": "Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's groundbreaking work. Although CLIP performs well, the typical direct latent feature alignment lacks clarity in its representation and similarity scores. On the other hand, lexical representation, a vector whose element represents the similarity between the sample and a word from the vocabulary, is a natural sparse representation and interpretable, providing exact matches for individual words. However, lexical representations is difficult to learn due to no ground-truth supervision and false-discovery issues, and thus requires complex design to train effectively. In this paper, we introduce LexVLA, a more interpretable VLA framework by learning a unified lexical representation for both modalities without complex design. We use DINOv2 as our visual model for its local-inclined features and Llama 2, a generative language model, to leverage its in-context lexical prediction ability. To avoid the false discovery, we propose an overuse penalty to refrain the lexical representation from falsely frequently activating meaningless words. We demonstrate that these two pre-trained uni-modal models can be well-aligned by fine-tuning on modest multi-modal dataset and avoid intricate training configurations. On cross-modal retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset, outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those trained from scratch on even bigger datasets (e.g., 1.1B data, including CC-12M). We conduct extensive experiments to analyze LexVLA.", "sections": [{"title": "1 Introduction", "content": "The development of Vision-Language Alignment (VLA) models has made great progress [38, 43, 19, 23, 6] since the innovative CLIP [30] effectively learns a shared latent space where text and image are well-aligned. This progress has also boosted related fields like vision-language foundation models [17], multi-modal understanding [15], and text-conditional generation [31]. However, CLIP's latent features pose challenges of interpretability issue for analyzing individual factors' impact. Additionally, CLIP's visual model struggles to learn patch-level features, and its text model are trained based on incomplete and biased captions. These challenges reduces its overall effectiveness.\nOn the other hand, the lexical representation is known for its clarity as each dimension corresponds to the similarity between the input and a specific word/token from the vocabulary. Additionally, lexical representation can naturally be made sparse, which means it's efficient for tasks like search or indexing in large-scale retrieval tasks. However, learning lexical representation is difficult. The"}, {"title": "3 LexVLA", "content": "Problem setup Our target is to learn the lexical alignment between text and image modalities. Typically, the lexical representation $s_i \\in \\mathbb{R}^V, i \\in \\{img, txt\\}$ is a score vector, whose element indicates the similarity between the sample and the corresponding word from the vocabulary V with"}, {"title": "3.1 Lexical representation", "content": "Vocabulary Inspired by tokenization techniques [32, 16], we initialize with the tokenizer vocabulary and refine it by removing unused and meaningless tokens, reducing the size from 32,000 to 17,149.\nCodebooks The one-hot embedding is semantic-less and assume the same distance between different words, which is obviously not a good embedding. In this paper, we use the output codebook of language models as a well pre-trained and semantic-rich codebook, inducing lexical codes of 4096-dim vectors. However, the text codebook is not optimized for visual features. Basically, during the pre-training of language model, the visual world is not observable, thus we may have a gap between text-only embeddings and the visual-embeddings. To this end, we propose to use the same vocabulary V but a unique codebook $Z_i, i \\in \\{img, txt\\}$ for text and image modalities, respectively. As the language codebook is pre-trained on large corpus, we freeze $Z_{txt}$ to enjoy the well-trained text embeddings. $Z_{img}$ is initialized with the weights of $Z_{txt}$ and fine-tuned along with the training of lexical encoders to enable the adaptation for visual features.\nSparse representation Thanks to the inherent similarity measurement in lexical representations, it is natural to turn a dense output vector into a sparse lexical representation. Typically we could utilize the following strategies: 1) Top-k thresholding: select the most important k items and discard the rest; 2) Value thresholding: keep items with value above the threshold and discard the rest. In this paper, we use the value thresholding strategy. We select items with values greater than $1/\\sqrt{V}$, which means we consider only the items with above-average signals as informative."}, {"title": "3.2 Lexical encoder", "content": "Given the image as $x_{img}$ and text as $x_{txt}$, the lexical representation is achieved by:\n$s_i := e_i(x_i) = h_i \\odot g_i \\odot f_i(x_i), i \\in \\{img, txt\\}.$   (1)\nWithout loss of generality, we ignore the subscript i in the following definitions unless otherwise specified. We split the lexical encoder in three stages: 1) The single-modal pre-trained feature extractor $f$, takes as input $x$ and as output a feature sequence $y := f(x) \\in \\mathbb{R}^{n \\times d_i}$, where n is the sequence length and $d_i$ is the modal-dependent feature dimension; 2) The projector $g$, projects y to the lexical feature sequence $z := g(y) \\in \\mathbb{R}^{n \\times d}$, where d is the lexical feature dimension; 3) The mapper h, maps z to the lexical representation $s = h(z) \\in \\mathbb{R}^{V}$.\nIn this paper, we restrict the lexical representation to be non-negative and $l_2$-normalized, i.e., $||s_i||_2 = 1, s_i \\geq 0$. Different modalities share the same V with a unique codebook $Z_i \\in \\mathbb{R}^{V \\times d}, i \\in \\{img, txt\\}$."}, {"title": "3.2.1 Lexical text encoder", "content": "Captioning? In VLA training data pair, the text is typically a caption of the corresponding image. However, it is well-known that caption only captures a partial observation of the semantics in the image, and cannot serve as a perfect target to represent the image. As it is the only accessible ground-truth signal, previous approaches mainly train the text encoders to represent the caption embedding via the output hidden state [8, 43, 10, 22] or [CLS] token embeddings [30], and use it as the target for the corresponding image. Hence, the learned alignment is biased and leads to false-elimination of image patterns that are not observed in the captions.\nPredicting! Inspired by the powerful Large Language Models (LLMs), we would like to investigate can we unleash the inherent knowledge of LLMs for lexical representation? The answer is yes.\nAs the auto-regressive LLMs [1, 36] are not naturally to summarize the previous tokens, it is irrational to directly use the predicted token as the embedding for raw text inputs. A straightforward way to activate LLM is to use prompt like \u201csummarize the sentence of [TEXT] in one word:\" to activate"}, {"title": "LLM as lexical predictor", "content": "Given the in-context learning capacity of LLM, we propose to adapt LLM as a lexical predictor. Specifically, our input for the LLM is:\nThe focus of \"The man is riding a white horse.\" lies on important words:\"man\", \"riding\", \"white\", \"horse\". The focus of \" [TEXT]\" lies on important words:\nThe [TEXT] is the input caption. Then we adopt the output token as the text embedding. This prompt includes two parts: 1) One in-context example to guide the LLM to identify the important words in the caption, adapting the LLM to lexical prediction task; 2) The question prompt to ask the LLM the important words of input caption. In this design, the output token of the LLM naturally performs the lexical prediction task to calculate the similarity between the input caption and every word in the vocabulary, and thus serves as a powerful proxy to get the lexical representation.\nRealization we utilize Llama 2 [36] as our text encoder $f_{txt}$. The input text follows the above in-context prompts. We use the output of the predicted token embedding as $y_{txt}$. As our lexical codebook is Llama 2's output codebook, we do not need $g_{txt}$, thus $Z_{txt} = y_{txt}$. To implement $h_{txt}$, we first calculate dot-product attention between each text token $z_{txt} \\in \\mathbb{R}^{1 \\times d}$ and the text lexical codebook $Z_{txt}$. Then we use the $elu1p$ activation [43] to transform the attention score into non-negative values and then normalize it, yielding global lexical representation $s_{txt}$. Formally,\n$s_{txt} = Normalize \\circ elu1p(z_{txt}Z_{txt}^T),$   (2)\nwhere $elu1p(x) = (x + 1) \\cdot \\mathbb{1}_{\\{x>0\\}} + e^{x} \\cdot \\mathbb{1}_{\\{x<0\\}}$ and Normalize is the $l_2$ normalization operator. For word-level lexical representation, we directly use the word code in $Z_{txt}$ followed by Eq. (2)."}, {"title": "3.2.2 Lexical visual encoder", "content": "We adopt DINOv2 [27] as our visual backbone to implement $f_{img}$. Given input $x_{img}$, we first flatten it into patches $x_{img} = [x_1, ..., x_{n-1}]$ and input to $f_{img}$ to get\n$y_{img} = f_{img}([CLS; x_{img}]) \\in \\mathbb{R}^{n \\times d},$   (3)\nwhere CLS is the \u201cclass\u201d token in DINOv2. Subsequently, we implement $g_{img}$ with an adapter includes a self-attention layer and 2 multi-layer perceptions to post-process the DINOv2 features. To implement $h_{img}$, similar to the text encoder but in a more fine-grained manner, we first calculate dot-product attention between each image patch token $z_{img,i} \\in \\mathbb{R}^{1 \\times d}$ and the image lexical code-book $Z_{img}$, followed by the $elu1p$ activation. In the end, we use max-pooling to aggregate patch representations and then normalize, yielding global lexical representation $s_{img}$. Formally,\n$s_{img} = Normalize \\circ Max\\text{-}Pool \\circ elu1p(z_{img}Z_{img}^T).$   (4)\nThe image patch lexical representation follows the same process but replace the max-pooling operator with selecting the corresponding patch location."}, {"title": "3.3 Train LexVLA", "content": "Contrastive objective We use the InfoNCE loss [26] with learnable temperature $\\tau$ for cross-modal retrieval over a batch of N text-image pairs $(X_{img}, X_{txt})$ as the major objective:\n$l_{t2i} = - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{exp(a_{ib}/\\tau)}{\\sum_{j=1}^{N} exp(a_{ijb}/\\tau)}, (a,b) \\in \\{(x_{img}, x_{txt}), (x_{txt}, x_{img})\\}.$   (5)\nOveruse penalty In lexical representation learning, the FLOPs loss [28] is widely used to encourage the output sparsity. Given the modality-specified lexical representation matrix $S = \\{s_{i,j}\\}, i \\in \\{1, ..., N\\}, j\\in \\{1, ..., V\\}$ in current mini-batch, the FLOPs loss is defined as:\n$\\mathcal{L}_{FLOPs} = \\sum_{j=1}^{V} \\sum_{i=1}^{N} s_{i,j}^2,$   (6)"}, {"title": "Objective", "content": "The full objective contains Eq. (5) and Eq. (7) for each modality:\n$\\mathcal{L} = l_{t2i} + l_{i2t} + \\lambda_1 l_{loveruse} + \\lambda_2 l_{r_{overuse}},$   (8)\nwhere $\\lambda_1$ and $\\lambda_2$ are two regularization weights for image and text representations, respectively.\nIncremental fine-tuning For text part, we adopt LoRA adapters [12] to fine-tune the Llama 2. For vision part, we freeze the DINOv2, and only train the projector and vision codebook."}, {"title": "3.4 PatchDis: interpretability metric", "content": "The core target of lexical representation is to achieve an interpretable feature representation for input modality. For the text input, the interpretability is straightforward to check. However, existing literature do not have a quantitative metric to measure the patch-level interpretability of visual lexical representation. In this paper, we propose the PatchDis metric to evaluate the patch-level discrimination tasks, inspired by the patch-level segmentation task but designed for models which are not trained from fine-grained alignment tasks like segmentation or detection.\nBasically, PatchDis evaluates the classification task in the patch-level features. For any given VLA model, we use its text encoder to input the class names and obtain the text embeddings of all classes. Similarly, we input the testing image to the visual encoder to obtain all patch features. Then we could use the model-defined metric to calculate the similarity between each patch feature and the class embeddings, and predict the class of each patch from the largest similarity. Then for each class, we could aggregate all patches of one class as the patch-level prediction for the class. We use the ground-truth segmentation of the class as the target, and use mIoU as the metric to evaluate the patch-level interpretability of the VLA models in the zero-shot patch-level discrimination task."}, {"title": "4 Experiments", "content": "Datasets We use CC-12M [4] for training, a dataset consisting of 12.4 million image-text pairs. We successfully download 9.2M pairs and use this subset as our training set. For evaluation, we use Flickr30k [29] and MSCOCO [20] to evaluate zero-shot cross-modal retrieval tasks.\nImplementation We use DINOv2 [27] base model and Llama 2 [36] 7B model as our backbones. We use Adam optimizer [14] with learning rate 5e - 4 and cosine decayed, batch size of 6,144, precision of BFloat16 for 12 epochs. We initialize \u0442as 0.07, and clip the logits larger than 100 as in [30]. We use 8 A100 GPUs of 40GB memory to train LexVLA. We quadratically warmup A in the fisrt 2k steps and then freeze as [28]. We set \u5165\u2081 as 5e \u2013 4 and \u03bb\u03c4 as le 3. The trainable parameters in LexVLA is 109 M in total, including 70M for vision codebook, 17M for vision projector (19.76% compared with DINOv2), 21M for Llama adaptor (0.30% compared with Llama 2)."}, {"title": "4.1 Zero-shot cross-modal retrieval", "content": "Evaluation We conduct experiments on zero-shot cross-modal retrieval tasks on Flickr30k and MSCOCO. We use the R@K, recall ratio within top-K items, as the evaluation metric.\nCompetitors We compare LexVLA with the latent feature alignment methods, including the original CLIP [30] model trained on different dataset scales and followup CLIP-style alignment models, including FILIP [38], CLIP-BERT [43], DeCLIP [19], SLIP [23], and ProtoCLIP [6]. We also compare with previous lexical representation learning models that focus on zero-shot cross-modal"}, {"title": "4.2 Lexical representation analysis", "content": "In this part, we evaluate if LexVLA learns the accurate lexical repre-sentation. We conduct the following experiments: 1) Quantitatively, we use the proposed PatchDis metric to evaluate the fine-grained patch-level visual lexical representations on MSCOCO 2017 [20]; 2) Qualitatively, we visualize the global lexical representation of im-ages, local lexical representation of image patches, and a PatchDis visualization for the activated patches for the given category."}, {"title": "4.3 Further analysis", "content": "LexVLA under different sparsity One of the targets for lexical representation is to learn a sparse representation for potential benefits in large scale retrieval tasks. To test the robustness of learned"}, {"title": "5 Conclusion", "content": "In this paper, we propose LexVLA, a unified lexical vision-language alignment framework. LexVLA effectively learn a unified lexical representation with unique codebooks for vision and language modalities, with the aid of single-modal pre-trained models. We propose to incremental fine-tune LexVLA with the standard contrastive objective penalized by the proposed overuse objective to prevent meaningless activation and encourage sparse embedding. LexVLA refrains from the complex training configurations, and effectively achieves the patch-level interpretability with only"}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 Model hyperparameters", "content": "Our hyperparamters selection for the detail can be found in Tab. 3."}]}