{"title": "A Survey on Moral Foundation Theory and Pre-Trained Language Models: Current Advances and Challenges", "authors": ["Lorenzo Zangaria", "Candida M. Greco", "Davide Picca", "Andrea Tagarelli"], "abstract": "Moral values have deep roots in early civilizations, codified within norms and laws that regulated societal order and the common good. They play a crucial role in understanding the psychological basis of human behavior and cultural orientation. The Moral Foundation Theory (MFT) is a well-established framework that identifies the core moral foundations underlying the manner in which different cultures shape individual and social lives. Recent advancements in natural language processing, particularly Pre-trained Language Models (PLMs), have enabled the extraction and analysis of moral dimensions from textual data. This survey presents a comprehensive review of MFT-informed PLMs, providing an analysis of moral tendencies in PLMs and their application in the context of the MFT. We also review relevant datasets and lexicons and discuss trends, limitations, and future directions. By providing a structured overview of the intersection between PLMs and MFT, this work bridges moral psychology insights within the realm of PLMs, paving the way for further research and development in creating morally aware AI systems.", "sections": [{"title": "1. Introduction", "content": "Moral values have roots in the earliest civilizations, which had already codified ethical principles within norms and laws that regulated daily life, attesting to a deep reflection on the order of society and the common good. Socrates, Plato, and Aristotle explored the nature of virtues and ethics, linking them with the exploration of individual and collective well-being, which can help people define who they are and what they aspire to be, and are foundational elements for moral values. Indeed, moral values guide how to behave, not only personally but also as the pillars of constructing and maintaining social order. They instill a sense of responsibility and encourage behaviors that would support the well-being of the community (Schwartz, 1992) or increase the division between people within society (Brady et al., 2020). This makes morality a distinctive element of individual character, which influences decisions and human behavior that can lead to a sense of integrity and self-esteem (Rana and Solaiman, 2022), and to implication of what is a morally good or bad life (Hofmann et al., 2014; Ellemers, 2018; K\u00e1d\u00e1r et al., 2019).\nIn recent years, the integration of moral foundations into computational models has been an area of active research. The advances in Natural Language Processing (NLP) have revolutionized the way we extract information and knowledge from textual data. Particularly, Pre-trained Language Models (PLMs), powered by Transformer backbones (Vaswani et al., 2017), have become a groundbreaking technology in addressing a wide range of tasks (Lin et al., 2022). Transformers'self-attention mechanism allows for capturing global complex dependencies within input data, enabling exceptional performance in NLP and beyond. In this respect, one of the emerging trends is to integrate PLMs with moral information for inferring moral foundations from the input data and analyzing the moral dimensions embedded in PLMs. These approaches are beneficial for applications in social sciences, policy-making, and even in enhancing AI ethics. For example, PLMs can be employed to execute moral judgment tasks like moderating content on social platforms (Franco et al., 2023); or to analyze political speeches and campaigns, as well as business strategies, identifying potential manipulative tactics (Liu et al., 2022; Luceri et al., 2024). Hence the need to study AI systems for morality becomes paramount. Furthermore, recent studies have shown that PLMs can be trained to recognize and reflect moral judgments, or can be inspected to reveal certain moral tendencies present in the"}, {"title": "Objectives and scope of the survey.", "content": "In this survey, we aim to analyze the current approaches to combining the MFT and PLMs to systematize and provide valuable insights into how PLMs perceive and generate relevant content within a moral framework, like the MFT. The increasing use of PLMS in diverse applications underscore the critical nature of this aspect. We summarize our main contributions as follows:\n\u2022 New taxonomy: We provide a comprehensive overview of the MFT and the PLMs involved in the context of the MFT. Additionally, we formalize the essential strategies and tasks adopted within the domain of MFT-informed PLMs. Based on our theoretical framework, we organize the existing works in the literature into two main categories:\n(i) Moral-driven PLMs, which focus on training PLMs on MFT-based"}, {"title": "2. Related works", "content": "Recent advancements in the field of moral values within PLMs include the design of algorithms capable of detecting morally relevant contents and the implementation of procedures or evaluation metrics that reflect the moral perceptions of PLMs. However, the existing literature lacks comprehensive reviews that explore the intersection of MFT and PLMs within a computer"}, {"title": "3. Theoretical Background", "content": "In this section, we describe the two fundamental pillars of this survey: the Moral Foundation Theory and Pre-trained Language Models. In section 3.1 we delve into the principles and applications of the MFT, while in section 3.2 we provide an overview of the PLMs involved in the works explored in this survey, as well as a brief description of the strategies they employed for tailoring PLMs to downstream applications."}, {"title": "3.1. Moral Foundation Theory", "content": "Moral Foundations Theory was formulated by Haidt and Joseph (2004) to investigate the underlying reasons behind common moral themes across different cultures. It claims that multiple psychological systems form the basis of our intuitive ethics, which are then shaped by cultures' virtues, narratives, and institutions. MFT does not judge the moral value of these systems, rather it provides a descriptive analysis of human morality based on 4 pillars: (i) Nativism, which involves the hypothesis that certain moral foundations are innate; (ii) Cultural learning, which is the process that enables individuals to acquire moral values from their culture; (iii) Intuitionism, which corresponds to the notion that moral judgments are often made quickly and automatically; and (iv) Pluralism, which recognizes that there are multiple, sometimes conflicting, moral principles that people adhere to. If any of these pillars were to be proven false, the MFT would no longer be valid (Graham et al., 2013).\nMFT's initial framework established five fundamental principled dichotomies within this evolutionary context, or moral foundation dimensions. Each of these dimensions represents a critical aspect of social cooperation and conflict resolution that would have been essential for our ancestors' survival and flourishing:\n\u2022 Care (vs. Harm): making pressure to care for others and repel hurting others as the basis of this foundation. It prompts us to care and be kind to people who are suffering in vulnerable positions, such as those who take refuge, animals, or even those who perform in movies.\n\u2022 Fairness (vs. Cheating): centering on mutual altruism, it correlates with the rule of fairness and treatment of social justice, lauding virtues like honesty and integrity."}, {"title": "3.2. Pre-trained Language Models", "content": "Pre-trained language models have profoundly transformed the field of NLP, pushing society into an era of sophisticated language comprehension and generation. Trained on extensive corpora, these models discern intricate patterns and structures within language, enabling them to excel in tasks such as translation, summarization, and question-answering. At the core of these capabilities lies the self-attention mechanism of Transformer architecture, which serves as the fundamental component driving the performance and versatility of various PLMs.\nIn this Section, we describe the specific PLMs employed in the works discussed in section 6. We also provide a brief summary of the main strategies for adapting PLMs to downstream tasks. We emphasize that the discussed strategies are not exhaustive but are specifically those employed by the studies considered in this survey to address MFT-related tasks."}, {"title": "3.2.1. Encoder-based models: BERT and BERT-based", "content": "BERT-based models are a family of PLMs that leverage the Bidirectional Encoder Representations from Transformers architecture (Devlin et al., 2019). These models are encoder-based, i.e., they primarily focus on encoding the input text into dense vector representations. This approach allows the models to capture global and contextual relationships within the text, making them highly effective for understanding and processing language.\nBERT (Devlin et al., 2019) has set a new standard in NLP by introducing the bidirectionality in the pre-training phase. Unlike previous models that read text sequentially, BERT reads the entire sequence of words at once by considering simultaneously the left and right context, hence allowing it to deeply understand the entire input. As a result, BERT has significantly improved performance across a variety of NLP tasks, including named entity recognition, question answering, and language inference. BERT is released in two different sizes: BERT-base and BERT-large. BERT-base comprises 12 stacked encoder layers, each with 12 attention heads and a hidden size of 768, totaling 110M parameters. In contrast, BERT-large includes 24 stacked encoder layers with 16 attention heads and a hidden size of 1024, resulting in"}, {"title": "3.2.2. Encoder-Decoder based models: T5", "content": "Unlike BERT, which is primarily an encoder model, T5 (Raffel et al., 2020) employs an encoder-decoder architecture that allows it to handle a wide variety of text-related tasks within a unified framework. The key innovation of T5 is its text-to-text approach, where all tasks are framed as text generation problems. Thus, the inputs and outputs for every task are represented as text strings, simplifying the model's application across different NLP tasks. T5 is available in five versions: T5-small (6 layers, 8 attention heads, 512 hidden size, 60M parameters), T5-base (12 layers, 12 attention heads, 768 hidden size, 220M parameters), T5-large (24 layers, 16 attention heads, 1024 hidden size, 770M parameters), T5-3B (24 layers, 32 attention heads, 1024 hidden size, 3B parameters), T5-11B (24 layers, 128 attention heads, 1024 hidden size, 11B parameters)."}, {"title": "3.2.3. Decoder-based models: Large Language Models", "content": "Other models such as GPT-3, GPT-3.5 (Brown et al., 2020), ChatGPT (OpenAI, 2023), and open alternatives like LLaMA-2 (Touvron et al., 2023) have further pushed the boundaries of what PLMs can achieve. Unlike BERT and T5, these models are decoder-based. Therefore, they can predict the next word in a sequence given the previous words, which makes them suitable for text generation tasks. These models are known as Large Language Models (LLMs) due to their extensive scale in terms of parameters (billions), training data, and computational resources, which enable them to perform a wide range of complex language tasks with high proficiency.\nGPT-3 and GPT-3.5 (Brown et al., 2020) are among the most advanced in this category, capable of producing human-like text. It is trained on text-based and code-based data and then aligned using Supervised Fine-Tuning (SFT), through which the model is trained to generate responses w.r.t. the format expected by users, and Reinforcement Learning from Human Feedback (RLHF) (Kalyan, 2024), to align the model behavior with human instructions and preferences. GPT-3 is available in different sizes, such as: ada (350M parameters), babbage (1.3B parameters), curie (6.7B parameters), davinci (175B parameters). GPT-3.5 is an improved version of GPT-3 with less parameters than davinci (20B parameters). ChatGPT (OpenAI, 2023) has GPT-3.5 (and GPT-4) as a core model, but is specifically optimized for interactive conversations and dialogues. It is designed to maintain the context of a conversation over multiple turns, making it suitable for interactive applications such as chatbots and virtual assistants.\nLLaMA-2 (Touvron et al., 2023) is an open-source model designed to enhance efficiency and performance in various NLP tasks, emphasizing open accessibility and robustness. Similarly to ChatGPT, it is optimized for chat-based interactions through SFT and RHLF. It demonstrates proficiency in both conversational and completion tasks and competitiveness compared with GPT-3.5. LLaMA-2 is available in multiple versions, including LLaMA-2 7B (7 billion parameters), LLaMA-2 13B (13 billion parameters), and LLaMA-2 70B (70 billion parameters).\nOPT (Liu et al., 2021) is an open generative model designed to provide an accessible alternative to proprietary models like GPT-3.5, replicating its performance and sizes. OPT is available in different sizes, from 125M to 175B parameters."}, {"title": "3.2.4. Strategies for tailoring PLMs to downstream applications", "content": "In this section, we will analyze and explore the strategies utilized by the approaches discussed in Section 6 to tailor PLMs to MFT-related tasks.\nFine-tuning. The fine-tuning process involves additional training on a smaller, task-specific dataset, allowing the model to learn the nuances and requirements of that particular task while leveraging the broad knowledge it gained during pre-training. Specifically, it consists of training some or all layers of the PLM, then adding a task-specific layer (e.g., feed-forward output layers for classification), and training the model end-to-end. Early methods such as BERT need to be fine-tuned to perform a specific task, e.g., text classification. Fine-tuning is useful for various reasons, such as exposing the model to new or proprietary data (not included during pre-training, so there is a need to include in-domain knowledge) or to align the model's responses with human expectations when providing instructions (through for example fine-tuning with human feedback (OpenAI, 2023)).\nFeature-based approach. The simplest way to use PLMs is to freeze their weights and use their output as context-sensitive word embeddings for a new architecture, which is specifically trained for the downstream task (Min et al., 2024). This approach is similar to feature extraction in classic statistical NLP. Frozen contextual embeddings are employed in scenarios in which fine-tuning PLMs is impractical, e.g. when there is insufficient labeled data or for addressing computationally expensive NLP tasks.\nPrompt Engineering. Prompting involves adding natural language text, typically in the form of short phrases, to provide guidance to pre-trained models"}, {"title": "4. MFT-related tasks", "content": "In this section, we discuss and formalize the tasks related to the integration of PLMs and MFT. Our objective is to present comprehensive and formal definitions of the primary tasks addressed by the investigated works, serving as a valuable foundation for future research endeavors. We categorize the identified tasks as illustrated in Figure 3. We emphasize that, to the best of our knowledge, this is the first survey to provide a taxonomy of the main MFT tasks faced by current research on PLMs.\nWe define $\\mathcal{T}_D$ as the MFT and $\\mathcal{D}$ as the set of moral foundation values associated to $\\mathcal{T}_D$. Without loss of generalizability, we assume that $\\mathcal{D}$ does not strictly represent the original MFT foundations with virtues and vices, bur rather moral values in which virtues and vices can be treated separately, i.e., $\\mathcal{D} = {Care, Harm, Fairness,...}$. Following this approach, $\\mathcal{D}$ could be easily integrated with other moral values, i.e. ${non-moral} \\cup \\mathcal{D}$ (Hoover et al., 2020), which can be useful for learning more expressive models."}, {"title": "4.1. Moral values prediction", "content": "Moral values prediction involves learning a function $M_\\Theta$ that infer the moral values $\\mathcal{D}$ from the input data. Formally, given the input space $\\mathcal{X}$, the objective of the moral values prediction task is to learn $M_\\mathcal{D} : \\mathcal{X} \\rightarrow \\mathcal{Y}$, where $\\mathcal{Y} \\subseteq \\mathcal{P}(\\mathcal{D})$, and $\\mathcal{P}(\\mathcal{D})$ indicates the powerset of $\\mathcal{D}$. Note that $M_\\Theta$ could be a PLM that is freezed, fine-tuned or further pre-trained (cf. section 3.2.4) for performing the final downstream task.\nIn order to train the PLM for the moral values prediction task, we assume the availability of a labeled dataset $\\mathcal{L} = {(x_i, d_i)}^n_{i=1}$, where $x_i \\in \\mathcal{X}$ is an"}, {"title": "4.2. Moral values alignment", "content": "In the moral value alignment problem the goal involves analyzing whether the response of PLMs is aligned with specific moral values $\\mathcal{D}$. In this context, alignment refers to the evaluation of whether the response of the PLM is in line with the moral values expressed in the input text $\\mathcal{X}$ and denoted by $\\mathcal{D}$. Hereinafter, we will assume that the alignment function f needs to be minimized, indicating that a lower value corresponds to a higher level of moral alignment for the PLM. The operational framework for addressing a moral value alignment task is shown in Figure 5. In our formulation, the problem can be approached as follows.\nLearning-based Moral Value Alignment: We provide a similar setting to the one proposed by Dognin et al. (2024). Let us denote with $\\mathcal{S}$ the current conversation history of the PLM, $\\mathcal{P}$ the set of possible prompts given to the model, $\\mathcal{R}$ the set of responses that can be generated, and $f:\\mathcal{R} \\times \\mathcal{D} \\leftrightarrow \\mathcal{R}^k$ an alignment function. The objective is to learn a function $M_\\Theta : \\mathcal{S} \\times P \\rightarrow \\mathcal{R}$ that provides responses $r_i \\in \\mathcal{R}$ for a given prompt in a given state such that $f(r_i, d_i)$ is minimized over the dataset $L = {(r_i, d_i)}^n_{i=1}$, with $d_i \\in \\mathcal{D}$.\nFor example, if a PLM is prompted with scenarios involving a moral dilemma, the aligned responses would be those that consider the moral dimensions relevant to the scenario. Suppose a scenario that involve choosing between honesty and loyalty, such as deciding whether to tell the truth about a friend's mistake at work or to protect your friend by keeping it a secret. The responses chosen by the PLM should reflect an understanding of the"}, {"title": "5. Survey, Datasets, and Lexicon with Moral Values", "content": "The MFT is a cornerstone of datasets focusing on the morality field (Guo et al., 2023b). Insights from current studies on morality have primarily been obtained via analyzing moral vignettes, questionnaire data, and social media datasets (Hofmann et al., 2014). Therefore, we categorize the data sources into 3 categories: (i) Moral surveys, which correspond to set of close-ended questions designed to gauge individuals' moral beliefs, (ii) Moral lexicons, which refer to lists of word-score pairs related to moral values, and (iii) MFT-annotated datasets, which refer to data manually or (semi-) automatically labeled with the dimensions of the MFT."}, {"title": "5.1. Moral surveys", "content": "The Moral Foundations Questionnaire (MFQ) (Graham et al., 2008) is a psychometric tool designed to evaluate the extent to which individuals endorse the five different moral foundations of the MFT. It is widely used in social psychology to understand moral diversity across cultures and to examine individual differences in moral scenarios. The MFQ evaluates each foundation by utilizing 30 questions and six items, which are graded on a 6-point Likert scale (Likert, 1932), anchored from 1 (strongly disagree) to 6 (strongly agree)."}, {"title": "5.2. Moral Lexicons", "content": "The Moral Foundations Dictionary (MFD) (Graham et al., 2009) is the first vocabulary developed to assess moral values from textual data. The MFD provides a list of words and phrases associated with the five moral foundation dimensions. It has been used in various studies to understand cultural differences in moral values, political ideologies, and the language used in moral discourse (Kobbe et al., 2020).\nThe average number of words included in MFD for each moral category is 32, hence having a size of 324 words. MFDv2 (Frimer, 2019) is an extension of MDS, which consists of a lexicon of 2014 words, with each word associated to a single moral category. Another extension of the MFD is the extended Moral Foundation Dictionary (eMFD) (Hopp et al., 2021), a dictionary of 35,985 text samples extracted from English news articles on a variety of topics. Lemmas and stems are annotated with a moral value and words in the eMFD are weighted according to a probabilistic scoring procedure.\nMoralStrength (Araque et al., 2020) is a semi-automated lexicon of approximately 1,000 lemmas, obtained as an extension of the MFD, which quantifies the relevance and strength of words related to the five moral foundations of the MFT. The lexicon is constructed by initially selecting seed words related to each moral foundation and then expanding this set through semantic similarity measures derived from word embeddings.\nLibertyMFD (Araque et al., 2022) is in turn an extension of Moral-Strength for detecting the Liberty moral dimension. Roy and Goldwasser (2021) extend the dataset proposed by Johnson and Goldwasser (2018) to build a topic indicator lexicon, for topics like Gun Control and Immigration."}, {"title": "5.3. MFT-annotated datasets", "content": "The Moral Foundations Twitter Corpus by Hoover et al. (2020) comprises 35,108 tweets annotated with 11 moral values based on the MFT. Each tweet is categorized into 10 moral values, reflecting the dyadic dimensions such as care/harm, which are treated separately, i.e., care and harm are separate labels. Additionally, the dataset includes a non-moral label for tweets that do not exhibit any moral sentiment, resulting in 11 possible labels per tweet. It also includes seven different domains related to morally relevant issues: (i) All lives matter, referring to American social movement that emerged as a reaction to the Black Lives Matter movement of the African-American community; (ii) The Baltimore Protests, concerning tweets related to the protest"}, {"title": "6. Methods", "content": "We organize existing studies into two main categories. The first one regards Moral-driven PLMs, which refer to works that adopt training strategies for learning model that predict or are aligned with moral values. The general approach here is to fine-tune or further pre-train PLMs for predicting the moral values of the underlying moral foundations hidden in the data. The second category, namely Moral-targeted PLMs, regards methods that seek to assess the moral values embedded into the PLMs without any moral task-specific adaptation, to probe their responsiveness to morally challenging issues and situations. The general approach is to analyze the PLM responses to determines their beliefs and the effects of training on a particular dataset.\nWe propose a meta-definition for the problem of integrating PLMs with the MFT, aiming to establish a formal foundation framework for all the works examined. The proposed meta-definition must be broad enough to include both methods that use PLMs to learn hidden patterns related to the MFT (Moral-driven PLMs) and methods that use the MFT to analyze hidden patterns related to the PLMs' responses (Moral-targeted PLMs)."}, {"title": "6.1. Moral-driven PLMs", "content": "In this section, we describe moral-driven PLMs. Initially, we discuss methods that leverage BERT-based model for moral values prediction on social data in both within-domain and cross-domain scenarios. Then, we analyze studies that identify moral values to support argument analysis and moral values alignment.\nPLMs for moral values prediction in social discourses. Bulla et al. (2022) extend the work of Hoover et al. (2020) by replacing the LSTM model with a PLM. Specifically, SqueezeBERT is fine-tuned on the labeled tweets of the MFTC dataset for the task of multi-label classification of moral values. Each of the moral dimensions has been defined as the union of the two moral values that compose it, e.g., for the Care moral value, positive tweets are the ones labeled either with Care or Harm. Experimental results show that the BERT-based model outperforms the LSTM in all seven domains of the MFTC, achieving an F1 score ranging from 47.0 to 86.0 across the moral"}, {"title": "6.2. Moral-targeted PLMs", "content": "We further explore the existing literature of works examining the moral alignment of PLMs. We firstly review the literature that leverages LLMS, which inherit moral inclinations during their training phase or are influenced by prompt design to provide biased responses in relation to specific moral values. Then, we delve into studies which explore the inherent moral biases in the contextual embeddings of encoder-decoder and encoder-based PLMs.\nMoral values discovery through prompt engineering. Abdulhai et al. (2023) conducted a series of experiments analyzing the moral foundations of GPT-3 models (curie, babbage and davinci) to understand the values encoded from their training data and their potential impact on unforeseen tasks. The models are examined to understand whether they can exhibit specific moral stances, the consistency of these stances across various contexts, and the ability to deliberately prompt them to endorse particular moral foundations. Questions from the MFQ are exploited to construct prompts containing a description of the task, a rating scale specifying the model response (ranging from 0 to 5, with 0 corresponding to 'not relevant' and 5 to 'extremely relevant'), and a static example to show the model how to structure its response. To evaluate potential cultural and political biases in GPT-3, its default responses are analyzed and prompted with a political affiliation. By comparing the response of GPT-3 models with human studies, it is observed that less expensive GPT-3 models (curie, babbage) exhibit greater differences from human moral foundations, while more expensive models (davinci) align more closely, suggesting that larger models may better capture moral political values. As regards the consistency of PLMs' moral foundations across different contexts, certain dimensions (e.g., Fairness and Purity) are weighted more strongly than others. Additionally, the impact of prompting the models on"}, {"title": "7. Findings, challenges and future directions", "content": "Our discussion so far has provided insights into the potential of PLMs in modeling the framework of the MFT. These insights might contribute to a deeper understanding of the complexities involved in integrating MFT and PLMs and the ongoing efforts to improve their performance.\nMain trends in the existing literature. Observing Table 3, recurring patterns can be noticed regarding models selection, tasks addressed, data employed, and strategies used (the interplay among the categories and these aspects are shown in Figure 7). From the perspective of model choice, BERT and BERT-based models represent the most commonly used technology when creating moral-driven PLMs, while LLMs are primarily employed as moral-targeted PLMs. Clearly, the choice of the model has a significant impact on the adaptation strategy. Therefore, a clear separation of strategies can be observed between moral-driven PLMs, which exclusively use fine-tuning strategies to adapt to the MFT task, and moral-targeted PLMs, which adopt few-shot prompting and feature extraction. Moral-driven PLMs are frequently employed for tasks involving moral values prediction, while moral-targeted PLMs are predominantly utilized for moral value alignment tasks. Specifically, MFT-annotated datasets are generally used in the context of within-domain and cross-domain moral values prediction tasks. Conversely, lexicons and moral surveys are primarily used for addressing moral value alignment challenges. As regards the employed data, we can notice that moral surveys and moral lexicons are mostly used to evaluate moral value alignment, while MFT-annotated datasets are primarily involved in tasks of moral values prediction.\nCorrelation between moral foundations and political stances. A prominent theme is the correlation between moral foundations and political stances. Roy and Goldwasser (2021) demonstrate a strong link between the use of"}, {"title": "8. Conclusion", "content": "This survey explored the field related to the intersection between MFT and PLMs, highlighting the efforts made in targeting and driving PLMs in the context of MFT. Furthermore, We formalized the MFT-related tasks addressed by current PLMs and provided a meta-definition that can encompass a broad spectrum of PLMs integrating the MFT. This theoretical framework enable to categorize the current approaches in the field.\nVarious models and strategies, including fine-tuning and prompt engineering, have shown promise in accurately predicting moral values and aligning PLMs within specific moral scenarios. However, challenges such as the reliability of annotated data and the performance of models in cross-domain scenarios remain.\nThe findings from the reviewed works reveal several key insights. Firstly, there is a notable correlation between moral foundations and political stances, demonstrating how PLMs can be influenced by and reflect societal biases. The superiority of PLMs over traditional and lexicon-based methods in capturing the complexity of moral language has been established, highlighting the potential of PLMs to better understand and generate morally nuanced content. Integration techniques, such as combining PLMs with logical reasoning frameworks and employing adversarial learning, have shown significant advancements in moral value prediction tasks. However, the need for comprehensive and standardized annotation processes is critical to enhance the reliability of moral datasets. Additionally, constructing large-scale, multicultural, and multi-domain datasets is crucial for improving the generalizability of PLMs in MFT-related tasks. Prompt engineering has been identified as a powerful tool for influencing PLM responses to align with desired moral values. This malleability underscores the importance of careful prompt design and the potential for integrating predefined ontologies to guide PLM outputs consistently. Furthermore, cross-lingual and cross-domain challenges highlight the need for diverse and representative training data. Ensuring that PLMs can generalize moral judgments across different cultures and languages is crucial for developing universally applicable morally aware AI systems.\nIn conclusion, the analysis presented in this survey provided a comprehensive overview of the current state of research, highlighting significant advances and ongoing challenges. By addressing these challenges and exploring new research directions, such as enhancing dataset reliability, improving cross-domain generalization, and integrating ontologies for consistent moral"}]}