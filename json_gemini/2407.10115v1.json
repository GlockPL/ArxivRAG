{"title": "A Bag of Tricks for Scaling CPU-based Deep FFMs to\nmore than 300m Predictions per Second", "authors": ["Bla\u017e \u0160krlj", "Benjamin Ben-Shalom", "Grega Ga\u0161per\u0161i\u010d", "Adi Schwartz", "Ramzi Hoseisi", "Naama Ziporin", "Davorin Kopi\u010d", "Andra\u017e Tori"], "abstract": "Field-aware Factorization Machines (FFMs) have emerged as a pow-\nerful model for click-through rate prediction, particularly excelling\nin capturing complex feature interactions. In this work, we present\nan in-depth analysis of our in-house, Rust-based Deep FFM im-\nplementation, and detail its deployment on a CPU-only, multi-\ndata-center scale. We overview key optimizations devised for both\ntraining and inference, demonstrated by previously unpublished\nbenchmark results in efficient model search and online training.\nFurther, we detail an in-house weight quantization that resulted in\nmore than an order of magnitude reduction in bandwidth footprint\nrelated to weight transfers across data-centres. We disclose the\nengine and associated techniques under an open-source license to\ncontribute to the broader machine learning community. This paper\nshowcases one of the first successful CPU-only deployments of\nDeep FFMs at such scale, marking a significant stride in practical,\nlow-footprint click-through rate prediction methodologies.", "sections": [{"title": "1 INTRODUCTION", "content": "Design and development of machine learning approaches for the\ndomain of recommendation systems revolves around the interplay\nbetween scalability and approximation capability of classification\nand regression algorithms. Currently, many deployed recommen-\ndation engines rely on factorization machine-based approaches;\nthis is mostly due to good trade-offs when it comes to scalability,\nmaintainability and data scientists' involvement in building such\nmodels. Even though contemporary recommenders started to in-\ncreasingly rely on language model-based techniques [18], utilizing\nfactorization machines remains de facto solution for large-scale\n\"screening\" of candidates that are to be served. Such candidates can\ninclude from unseen items (online stores), to movie recommenda-\ntions, to ads [6, 19]. Scalability of factorization machines enables\ncreation of real-time systems that handle hundreds of millions of\nrequests in predictable and maintainable manner. In recent years,\ntwo main branches of methods have emerged. Approaches based\non frameworks such as TensorFlow [1] and PyTorch [12] enabled\nconstruction of highly expressive architectures that often require\nspecialized hardware for efficient productization [5, 7, 11, 16]. CPU-\nonly, single instance - single pass alternatives are fewer, and revolve\naround highly optimized C++ or Rust-based approaches that ex-\nploit consumer hardware as much as possible. The latter is the main\nfocus of this paper (overview in Figure 1)."}, {"title": "2 FWUMIOUS WABBIT (FW) - AN OVERVIEW", "content": "We proceed with a discussion of Fwumious Wabbit (FW), an in-\nhouse, Rust-based factorization machine-based system currently\nused in production for large-scale recommendation\u00b9."}, {"title": "2.1 Origins of FW and Vowpal Wabbit (VW)", "content": "The FW derives from Vowpal Wabbit (VW) [3], a high-performance,\nscalable open-source ML system recognized for its efficiency on\nlarge datasets \u00b2. While VW primarily uses logistic regression for\ntasks like click-through rate prediction, it lacks readily available\nadvanced extensions found in the domain of factorization machines.\nOne of the more expressive variations of factorization machines\nare the Field-aware Factorization Machines (FFMs), described in\ndetail in the works of Juan et al. [9, 10]. Building on this foundation,\nwe enhanced the FFM architecture by integrating elements of deep\nlearning. Specifically, a multi-layer perceptron (MLP)-like structure\nin conjunction with the traditional FFM (and logistic regression)\ncomponents. The architecture's computational complexity, a no-\ntable challenge, contributes to its rarity in existing benchmarks.\nWhen implemented in standard frameworks like TensorFlow, the\narchitecture struggles to scale effectively for practical use.\nDespite these challenges, our deep learning-extended FFM method\ndemonstrated significant performance gains over other tested al-\ngorithms in internal assessments. However, scaling this method\nwas not straightforward. It was only through invoking BLAS [4],\nthat we achieved critical performance enhancements, allowing for\npractical full-scale deployment\u00b3. An overview of the architecture is\nshown in Figure 2.. Key parts of the architecture are\n$LR(w, x) = \\sum_{j}^{n} wj. xj + b; FFM(w, x) = \\sum_{j1=1}^{n} \\sum_{j2=j1+1}^{n} (Wine Wja.fi) Xj1X j2$.\nNeural part (matrix form),\n$FFNN (W1,2,...,n, X) = an (... a2 (a1 (XW1) W2)...). Wn,$\ntakes as input both FFM and LR's outputs, i.e.\n$DFFM(W1,2,...,n. Wb, Wc, X) =FFNN(W1,2,...,n, MergeNormLayer(LR(wb, x), DiagMask(FFM(wc, x))).$"}, {"title": "2.2 Criteo, Avazu and KDD2012 - a benchmark\nand stability analysis", "content": "Even though we evaluated FW extensively on internal data sets\n(and online, in A/B tests), where it showed consistent dominance,\nresults on published data sets such as Criteo are also of relevance\nfor dissemination of engines' behavior and overall performance.\nIn this section we overview a benchmark we conducted to assess\ngeneral behavior of VW and FW. We also implemented DCNv2 [15,\n17], a Tensorflow-based strong baseline5. For considered data sets\n(Criteo6, Avazu and KDD20128), log transform of continuous fea-\ntures was conducted and no additional data pruning (rare values\netc.) was conducted (as is done in our system). The hyperparam-\neters considered include power of t, learning rates for different\ntypes of blocks (ffm, lr), regularization amount (L2 norm, VW). For\nDCNv2 we considered different learning rates, cross layer numbers,\ndropout rates and beta parameters. Results of the benchmark are\nsummarized in Figure 3. For each data set, algorithms considered\nare visualized as AUC scores computed in a rolling window of 30k\ninstances 10.\nThe trace in each plot represents the average performance (95%\nCI), and light-gray regions represent model evaluations that were\nout-of-distribution - this aspect is particularly relevant for under-\nstanding stability of different approaches and their sensitivity to\nhyperparameter configurations. For example, we observed that\nadding deep layers to VW models in most cases resulted in worse\nperformance. Carefully tuned VW hyperparameters yielded suf-\nficient performance, however, indicate potentially cumbersome\nmodel search (when considering new use cases/data) in practice.\nSimilar behavior was observed for DCNv2. The dotted black lines\nrepresent the overall best single-window performance, and per-\nformance on a given data set's test set11 Overall, initial phases\nof learning revealed VW's capability to adapt with less data, the\nDeepFFMs dominate after enough data is seen by the engines. Supe-\nrior performance was observed by DCNv2 on Criteo, yet not other\ndata sets (all features considered). The benchmark demonstrates\nthat progressively more complex architectures tend to result in\nbetter modeling capabilities, and with them, better AUCs in this\nbenchmark. In terms of runtime, on the same hardware, Criteo\ndata set could be processed on average in 32min by VW, and 31min\nby FW (linear model vs. DeepFFM). Deep VW variations took sub-\nstantially longer, around 65min on average (batch size of 2k). This\nresult indicates that FW enables more powerful models with same\ntime bounds for training. The DCNv2 (CPU) baseline was 30%-50%\nslower compared to DeepFFM runs. These tatistics were obtained\nbased on tens of thousands of runs that represented different al-\ngorithm configurations (both hyperparameters and field specifica-\ntions). Being CPU-based, the described approaches enable seamless\nscaling to commodity hardware, resulting in lower training and\ninference costs in practice."}, {"title": "3 FW IN PRACTICE: SERVICE\nARCHITECTURE OVERVIEW", "content": "This section aims to facilitate understanding of subsequently dis-\ncussed optimizations that were put in place to enable scaling of\nDeep FFMs. The implemented FW contains both training and infer-\nence logic. The training logic is relevant for incrementally training\nmore than a hundred models, online, every n minutes (depends\non the model). Training jobs are separate deployments that au-\ntomatically query for relevant chunks of data, download, update\nbased on existing weights and send the weights to the serving layer.\nServing layer on-the-fly reconstructs the final inference weights\nvia a patching mechanism discussed in Section 6, and exposes the\nweights as part of the serving service that handles millions of re-\nquests with new data. Based on the effect of predictions, data is\nstreamed back to the system as training data (a feedback loop).\nThe training jobs are Python-based services that interact with the\nbinary via process invocations. Serving binds the inference capabil-\nities with the serving (Java) service directly via a foreign function\ninterface (ffi)12. The architecture enables separation of concerns\ntraining jobs are separate to inference jobs, albeit at the cost of\nneeding to send the updated weight data between services; this is\none of the key performance bottlenecks that was addressed in this\nwork."}, {"title": "4 MODEL TRAINING IMPROVEMENTS", "content": "We next discuss main improvements implemented at the level of\ntraining jobs and offline research."}, {"title": "4.1 Speeding up model warm-up phase", "content": "Model warm-up corresponds to a phase in model training where\nmodel starts with past data, and \"catches up\" with present data as\nfast as possible. We identified efficient data pre-fetching as a crucial\noptimization for speeding up this process. By implementing async\nlearning cycles, multiple rounds of \"future\" data can be downloaded\nupfront, making sure the learning engine has constant influx of data.\nData pre-fetch in practice results in up to 4x faster pre-warming.\nWithin the cloud environment where the jobs are deployed, we\ncan control machine \"taints\", i.e. signatures that determine their\nhardware profile. Pre-warm jobs have dedicated taints, which in\npractice results in machines that are newer and stronger."}, {"title": "4.2 Hogwild-based training", "content": "An optimization that significantly improved model pre-warm time\nis the previously reported Hogwild-based model training[13], im-\nplemented also for Fwumious framework (as part of this work).\nHere, weight overlaps/overrides are allowed as the trade off for\nmulti-threaded updates. By tuning Hogwild capacity to tainted\nmachines, we observed multi-fold speedups in model warm-up. In\npractice, the times for bigger models went from multiple weeks\nto days, and in most cases around a day of training (to catch up).\nWeight degradation due to Hogwild was A/B tested and does not\nappear to cause any noticeable RPM drops. Summary of Howgild-\nbased training compared to control (no such training) is shown\nin Table 2. Utilization of hogwild has shown substantial benefits\nalso when utilized during online training (e.g., every 5min), and\nenabled scaling 100% bigger models. To the best of our knowledge,\nthis is one of the first demonstrations of consistent Hogwild-based\ntraining improvements for Deep FFMs."}, {"title": "4.3 Sparse weight updates", "content": "The next discussed optimization is related to how gradients are\naccounted for during model optimization itself. We observed that\ndeep layers, albeit being parameter-wise in minority compared to\nFFM part, take up considerable amount of time during optimiza-\ntion. To remedy this shortcoming, we identified an optimization\nopportunity that is a combination of activation function used in\nmost models, $f(x) = max(x, 0)$, and the specific implementation of\nFW. By realizing that we can identify zero global gradient scenar-\nios upfront, prior to updating any weights, we could skip whole\nbranches of computation with no impact on learning. The perfor-\nmance (speed) of training, however, was across-the-board improved"}, {"title": "5 MODEL SERVING IMPROVEMENTS", "content": "A considerable optimization we observed could take place in our\nsystem is context caching. Each request can be separated into con-\ntext and candidates. For all candidates in the request, the context is\nthe same, even though the recommended content's features differ\n- this implies part of the feature space is very consistent for each\ncandidate batch. To exploit this property, a dedicated serving-level\ncaching scheme was put in place. FW at this point does an addi-\ntional pass only with the context part, where it identifies and caches\nfrequent parts of the context. On subsequent candidate passes it\nreuses this information on-the fly instead of re-calculating it for\neach context-candidate pair. Deployment impact of context caching\nis shown in Figure 413. We next discuss (SIMD) Instruction-aware\nforward pass. Another optimization that is particular to infer-\nence is proper exploitation of SIMD intrinsics. These hardware\ninstruction level optimizations, however, needed to be carefully\nimplemented as the space of serving hardware is not homogeneous,\nmeaning that on-the-fly instruction detection, and subsequent uti-\nlization of appropriate binary needed to be put in place. SIMD\nintrinsics were successfully used to speed up forward pass (infer-\nence) with no loss in RPM performance, and resulted in a consistent\n20% speedup for all serving14. Real-life example of deployed SIMD-\nbased FW vs. the control (no SIMD) is shown in Figure 5. Up to\n25% faster inference (and with it lower resource utilization) were\nobserved."}, {"title": "6\nSTORAGE AND TRANSFER OPTIMIZATION", "content": "As discussed in previous sections, training and serving jobs are\nseparated. This separation of concerns, albeit easier to maintain,\ncontributes to a major drawback: weight sending across the net-\nwork. Model weights need to be constantly updated, which incurs\nsubstantial bandwidth costs. For example, hundreds of live mod-\nels that take up to 10G of memory (per update) are constantly\ntransferred across the network, resulting in a substantial bandwidth\noverhead to ensure low-latency online serving.\nModel patching. The first improvement we implemented is\nthe concept of model patching. This process is inspired by appli-\ncation of software patches (in general), albeit tailored to internal\nstructure of FW's weights. Each trained model consists of training\nweights and the optimizer's weights. The latter are not required for\nactual inference, which immediately reduces the required space by\nhalf. Further, each subsequent inference weights update (inference\nweights can be multiple GB) first computes model diff - byte-level\ndifference between old and new weights. This is possible due to\na consistent memory-level structure of weight files. The diffs are\ncompressed, sent to the serving layer, unpacked and applied to pre-\nvious weights file to obtain the new set of weights (inference). This\nprocess takes tens of seconds, however, further reduces memory\nfootprint on the network by more than 100% (less than a GB of\nupdates per model after patching Deep FFMs).\nFirst, instead of storing absolute indices of bytes that change,\nrelative locations are stored, resulting in a considerable storage\nsaving. Next, small integers denoting these differences are stored as\na custom integer type - instead of storing whole ints, compressed\nversions (small ints are impacted the most) are stored, leading to fur-\nther improvements15. As patcher works at the level of bytes, we also\nsuccessfully tested it for internal Tensorflow-based flows (reduced\nbandwidth for sending models). Inspired by recent weight quanti-\nzation advancements in the field of large language models [2, 14],\nwe implemented a variation of 16b weight quantization that,\nwhen combined with the byte-level patching mechanism, offered\nconsiderable bandwidth and model storage improvements. The\nquantization algorithm was designed to account for the following\nuse-case specific properties. First, by ensuring consistently small\nweight patches, the quantization ensures consistently smaller net-\nwork load. Second, the quantization and dequantization proce-\ndures must be fast, as they need to happen within a designated time\nwindow after each training round (procedure has tens of seconds\nat most at its disposal for full weight space). Finally, the algorithm\nneeds to be able to dynamically select viable weight ranges, as\nwe observed considerable variation in weight update sizes based\non e.g., time of the day (traffic amount). The final version of the\nalgorithm can be summarized as follows. For each online model\nupdate (e.g., 5min window), weights are first traversed to obtain\nthe minimum and maximum values (weights). These statistics are\nrequired to dynamically determine the range of relevant weight\nbins, as the amount of possible values for 16b representation is\nsmall (around 65k). Let $W = {W1, W2,..., Wn Wi \u2208 R}$ denote the\nset of all (n) weights and bmax denote the number of possible weight\nbuckets. Once the minimum and maximum are obtained, the bucket\nsize is computed as\n$BUCKETS = \\frac{max(W).round(\u03b1) \u2013 min(W).round(\u03b2)}{bmax}$.\nNote that minimum and maximum are rounded to \u03b1 and \u03b2 deci-\nmals. This consideration stems from empirical results that indicated\nthat considering full precision bounds results in less stable patch\nsizes 16. When constraining minimum and maximum to certain\nprecision, behavior stabilized whilst preserving performance and\nonline behavior. In the second pass, weights are quantized \u2013 for\neach weight, its 16b representation is computed and stored. This\nresults in computing\n$((wi - min(W)/BUCKETS).round().castTo16b().convertToBytes(),\ni.e. a set of bytes that represent a certain weight bucket. Bytes\nare stored in FW weight format and re-used during inference. An\nimportant detail also concerns metadata required to perform this\ntype of quantization; the original weights file is enriched with a"}, {"title": "7\nCONCLUSIONS AND OPEN PROBLEMS", "content": "In this paper, we presented a collection of implementation details\nfor scaling CPU-based DeepFFMs to operate at a multi-data-center\nscale, capable of handling hundreds of millions of predictions per\nsecond. We delved into both the offline and online components of\nour system. In the offline phase, we covered the complete workflow,\nincluding model architecture, enhancements to system warm-up\nprocesses, and bandwidth optimization strategies. Within the on-\nline phase, we describe two novel modifications to the inference\nlayer that have yielded significant speed improvements. Our main\nalgorithms, concepts, and performance benchmarks were discussed\nin detail, open-source implementations of key components were\nmade freely available. The implementation is extensible to other\nFFM-based variants. As further work, on the inference side, imple-\nmenting quantization techniques could accelerate the forward pass\nby using integer-based operations [8]. Improved weight sharing\nand memory mapping could offer training improvements."}]}