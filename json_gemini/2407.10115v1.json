{"title": "A Bag of Tricks for Scaling CPU-based Deep FFMs to more than 300m Predictions per Second", "authors": ["Bla\u017e \u0160krlj", "Benjamin Ben-Shalom", "Grega Ga\u0161per\u0161i\u010d", "Adi Schwartz", "Ramzi Hoseisi", "Naama Ziporin", "Davorin Kopi\u010d", "Andra\u017e Tori"], "abstract": "Field-aware Factorization Machines (FFMs) have emerged as a powerful model for click-through rate prediction, particularly excelling in capturing complex feature interactions. In this work, we present an in-depth analysis of our in-house, Rust-based Deep FFM implementation, and detail its deployment on a CPU-only, multi-data-center scale. We overview key optimizations devised for both training and inference, demonstrated by previously unpublished benchmark results in efficient model search and online training. Further, we detail an in-house weight quantization that resulted in more than an order of magnitude reduction in bandwidth footprint related to weight transfers across data-centres. We disclose the engine and associated techniques under an open-source license to contribute to the broader machine learning community. This paper showcases one of the first successful CPU-only deployments of Deep FFMs at such scale, marking a significant stride in practical, low-footprint click-through rate prediction methodologies.", "sections": [{"title": "1 INTRODUCTION", "content": "Design and development of machine learning approaches for the domain of recommendation systems revolves around the interplay between scalability and approximation capability of classification and regression algorithms. Currently, many deployed recommendation engines rely on factorization machine-based approaches; this is mostly due to good trade-offs when it comes to scalability, maintainability and data scientists' involvement in building such models. Even though contemporary recommenders started to increasingly rely on language model-based techniques [18], utilizing factorization machines remains de facto solution for large-scale \"screening\" of candidates that are to be served. Such candidates can include from unseen items (online stores), to movie recommendations, to ads [6, 19]. Scalability of factorization machines enables creation of real-time systems that handle hundreds of millions of requests in predictable and maintainable manner. In recent years, two main branches of methods have emerged. Approaches based on frameworks such as TensorFlow [1] and PyTorch [12] enabled construction of highly expressive architectures that often require specialized hardware for efficient productization [5, 7, 11, 16]. CPU-only, single instance - single pass alternatives are fewer, and revolve around highly optimized C++ or Rust-based approaches that exploit consumer hardware as much as possible. The latter is the main focus of this paper (overview in Figure 1)."}, {"title": "2 FWUMIOUS WABBIT (FW) - AN OVERVIEW", "content": "We proceed with a discussion of Fwumious Wabbit (FW), an in-house, Rust-based factorization machine-based system currently used in production for large-scale recommendation\u00b9."}, {"title": "2.1 Origins of FW and Vowpal Wabbit (VW)", "content": "The FW derives from Vowpal Wabbit (VW) [3], a high-performance, scalable open-source ML system recognized for its efficiency on large datasets \u00b2. While VW primarily uses logistic regression for tasks like click-through rate prediction, it lacks readily available advanced extensions found in the domain of factorization machines. One of the more expressive variations of factorization machines are the Field-aware Factorization Machines (FFMs), described in detail in the works of Juan et al. [9, 10]. Building on this foundation, we enhanced the FFM architecture by integrating elements of deep learning. Specifically, a multi-layer perceptron (MLP)-like structure in conjunction with the traditional FFM (and logistic regression) components. The architecture's computational complexity, a notable challenge, contributes to its rarity in existing benchmarks. When implemented in standard frameworks like TensorFlow, the architecture struggles to scale effectively for practical use.\nDespite these challenges, our deep learning-extended FFM method demonstrated significant performance gains over other tested algorithms in internal assessments. However, scaling this method was not straightforward. It was only through invoking BLAS [4], that we achieved critical performance enhancements, allowing for practical full-scale deployment\u00b3. An overview of the architecture is shown in Figure 2.. Key parts of the architecture are\n$LR(w, x) = \\sum_{j}^{n} w_j.x_j + b; FFM(w, x) = \\sum_{j_1=1}^{n}\\sum_{j_2=j_1+1}^{n} (W_{j_1}i_1 \\cdot W_{j_2}i_2) X_{j_1}X_{j_2}.$\nNeural part (matrix form),\n$FFNN(W_{1,2,...,n}, X) = a_n (... a_2 (a_1 (XW_1) W_2)...). W_n,$\ntakes as input both FFM and LR's outputs, i.e.\n$DFFM(W_{1,2,...,n}. W_b, W_c, X) =FFNN(W_{1,2,...,n}, MergeNormLayer (LR(w_b, x), DiagMask(FFM(w_c, x))).$"}, {"title": "2.2 Criteo, Avazu and KDD2012 - a benchmark and stability analysis", "content": "Even though we evaluated FW extensively on internal data sets (and online, in A/B tests), where it showed consistent dominance, results on published data sets such as Criteo are also of relevance for dissemination of engines' behavior and overall performance. In this section we overview a benchmark we conducted to assess general behavior of VW and FW. We also implemented DCNv2 [15, 17], a Tensorflow-based strong baseline5. For considered data sets (Criteo6, Avazu and KDD20128), log transform of continuous features was conducted and no additional data pruning (rare values etc.) was conducted (as is done in our system). The hyperparameters considered include power of t, learning rates for different types of blocks (ffm, lr), regularization amount (L2 norm, VW). For DCNv2 we considered different learning rates, cross layer numbers, dropout rates and beta parameters. Results of the benchmark are summarized in Figure 3. For each data set, algorithms considered are visualized as AUC scores computed in a rolling window of 30k instances 10.\nThe trace in each plot represents the average performance (95% CI), and light-gray regions represent model evaluations that were out-of-distribution - this aspect is particularly relevant for understanding stability of different approaches and their sensitivity to hyperparameter configurations. For example, we observed that"}, {"title": "3 FW IN PRACTICE: SERVICE ARCHITECTURE OVERVIEW", "content": "This section aims to facilitate understanding of subsequently discussed optimizations that were put in place to enable scaling of Deep FFMs. The implemented FW contains both training and inference logic. The training logic is relevant for incrementally training more than a hundred models, online, every n minutes (depends on the model). Training jobs are separate deployments that automatically query for relevant chunks of data, download, update based on existing weights and send the weights to the serving layer.\nServing layer on-the-fly reconstructs the final inference weights via a patching mechanism discussed in Section 6, and exposes the weights as part of the serving service that handles millions of requests with new data. Based on the effect of predictions, data is streamed back to the system as training data (a feedback loop). The training jobs are Python-based services that interact with the binary via process invocations. Serving binds the inference capabilities with the serving (Java) service directly via a foreign function interface (ffi)12. The architecture enables separation of concerns - training jobs are separate to inference jobs, albeit at the cost of needing to send the updated weight data between services; this is one of the key performance bottlenecks that was addressed in this work."}, {"title": "4 MODEL TRAINING IMPROVEMENTS", "content": "We next discuss main improvements implemented at the level of training jobs and offline research."}, {"title": "4.1 Speeding up model warm-up phase", "content": "Model warm-up corresponds to a phase in model training where model starts with past data, and \"catches up\" with present data as fast as possible. We identified efficient data pre-fetching as a crucial optimization for speeding up this process. By implementing async learning cycles, multiple rounds of \"future\" data can be downloaded upfront, making sure the learning engine has constant influx of data. Data pre-fetch in practice results in up to 4x faster pre-warming. Within the cloud environment where the jobs are deployed, we can control machine \"taints\", i.e. signatures that determine their hardware profile. Pre-warm jobs have dedicated taints, which in practice results in machines that are newer and stronger."}, {"title": "4.2 Hogwild-based training", "content": "An optimization that significantly improved model pre-warm time is the previously reported Hogwild-based model training[13], implemented also for Fwumious framework (as part of this work). Here, weight overlaps/overrides are allowed as the trade off for multi-threaded updates. By tuning Hogwild capacity to tainted machines, we observed multi-fold speedups in model warm-up. In practice, the times for bigger models went from multiple weeks to days, and in most cases around a day of training (to catch up). Weight degradation due to Hogwild was A/B tested and does not appear to cause any noticeable RPM drops. Summary of Howgild-based training compared to control (no such training) is shown in Table 2. Utilization of hogwild has shown substantial benefits also when utilized during online training (e.g., every 5min), and enabled scaling 100% bigger models. To the best of our knowledge, this is one of the first demonstrations of consistent Hogwild-based training improvements for Deep FFMs."}, {"title": "4.3 Sparse weight updates", "content": "The next discussed optimization is related to how gradients are accounted for during model optimization itself. We observed that deep layers, albeit being parameter-wise in minority compared to FFM part, take up considerable amount of time during optimization. To remedy this shortcoming, we identified an optimization opportunity that is a combination of activation function used in most models, $f(x) = max(x, 0)$, and the specific implementation of FW. By realizing that we can identify zero global gradient scenarios upfront, prior to updating any weights, we could skip whole branches of computation with no impact on learning. The performance (speed) of training, however, was across-the-board improved"}, {"title": "5 MODEL SERVING IMPROVEMENTS", "content": "A considerable optimization we observed could take place in our system is context caching. Each request can be separated into context and candidates. For all candidates in the request, the context is the same, even though the recommended content's features differ \u2014 this implies part of the feature space is very consistent for each candidate batch. To exploit this property, a dedicated serving-level caching scheme was put in place. FW at this point does an additional pass only with the context part, where it identifies and caches frequent parts of the context. On subsequent candidate passes it reuses this information on-the fly instead of re-calculating it for each context-candidate pair. Deployment impact of context caching is shown in Figure 413. We next discuss (SIMD) Instruction-aware forward pass. Another optimization that is particular to inference is proper exploitation of SIMD intrinsics. These hardware instruction level optimizations, however, needed to be carefully implemented as the space of serving hardware is not homogeneous,"}, {"title": "6 STORAGE AND TRANSFER OPTIMIZATION", "content": "As discussed in previous sections, training and serving jobs are separated. This separation of concerns, albeit easier to maintain, contributes to a major drawback: weight sending across the network. Model weights need to be constantly updated, which incurs substantial bandwidth costs. For example, hundreds of live models that take up to 10G of memory (per update) are constantly transferred across the network, resulting in a substantial bandwidth overhead to ensure low-latency online serving.\nModel patching. The first improvement we implemented is the concept of model patching. This process is inspired by application of software patches (in general), albeit tailored to internal structure of FW's weights. Each trained model consists of training weights and the optimizer's weights. The latter are not required for actual inference, which immediately reduces the required space by half. Further, each subsequent inference weights update (inference weights can be multiple GB) first computes model diff - byte-level difference between old and new weights. This is possible due to a consistent memory-level structure of weight files. The diffs are compressed, sent to the serving layer, unpacked and applied to previous weights file to obtain the new set of weights (inference). This process takes tens of seconds, however, further reduces memory footprint on the network by more than 100% (less than a GB of updates per model after patching Deep FFMs).\nFirst, instead of storing absolute indices of bytes that change, relative locations are stored, resulting in a considerable storage saving. Next, small integers denoting these differences are stored as a custom integer type - instead of storing whole ints, compressed versions (small ints are impacted the most) are stored, leading to further improvements15. As patcher works at the level of bytes, we also successfully tested it for internal Tensorflow-based flows (reduced bandwidth for sending models). Inspired by recent weight quantization advancements in the field of large language models [2, 14], we implemented a variation of 16b weight quantization that, when combined with the byte-level patching mechanism, offered considerable bandwidth and model storage improvements. The quantization algorithm was designed to account for the following use-case specific properties. First, by ensuring consistently small weight patches, the quantization ensures consistently smaller network load. Second, the quantization and dequantization procedures must be fast, as they need to happen within a designated time window after each training round (procedure has tens of seconds at most at its disposal for full weight space). Finally, the algorithm needs to be able to dynamically select viable weight ranges, as we observed considerable variation in weight update sizes based on e.g., time of the day (traffic amount). The final version of the algorithm can be summarized as follows. For each online model update (e.g., 5min window), weights are first traversed to obtain the minimum and maximum values (weights). These statistics are required to dynamically determine the range of relevant weight bins, as the amount of possible values for 16b representation is small (around 65k). Let $W = \\{W_1, W_2,..., W_n W_i \\in R\\}$ denote the set of all (n) weights and bmax denote the number of possible weight buckets. Once the minimum and maximum are obtained, the bucket size is computed as\n$BUCKETS = \\frac{max(W).round(\\alpha) \u2013 min(W).round(\\beta)}{bmax}$\nNote that minimum and maximum are rounded to \u03b1 and \u03b2 decimals. This consideration stems from empirical results that indicated that considering full precision bounds results in less stable patch sizes 16. When constraining minimum and maximum to certain precision, behavior stabilized whilst preserving performance and online behavior. In the second pass, weights are quantized \u2013 for each weight, its 16b representation is computed and stored. This results in computing\n$((wi - min(W)/BUCKETS).round().castTo16b().convertToBytes(),$\ni.e. a set of bytes that represent a certain weight bucket. Bytes are stored in FW weight format and re-used during inference. An important detail also concerns metadata required to perform this type of quantization; the original weights file is enriched with a"}, {"title": "7 CONCLUSIONS AND OPEN PROBLEMS", "content": "In this paper, we presented a collection of implementation details for scaling CPU-based DeepFFMs to operate at a multi-data-center scale, capable of handling hundreds of millions of predictions per second. We delved into both the offline and online components of our system. In the offline phase, we covered the complete workflow, including model architecture, enhancements to system warm-up processes, and bandwidth optimization strategies. Within the online phase, we describe two novel modifications to the inference layer that have yielded significant speed improvements. Our main algorithms, concepts, and performance benchmarks were discussed in detail, open-source implementations of key components were made freely available. The implementation is extensible to other FFM-based variants. As further work, on the inference side, implementing quantization techniques could accelerate the forward pass by using integer-based operations [8]. Improved weight sharing and memory mapping could offer training improvements."}]}