{"title": "Optimization of Autonomous Driving Image Detection Based on RFAConv and Triplet Attention", "authors": ["Zhipeng Ling", "Qi Xin", "Yiyu Lin", "Guangze Su", "Zuwei Shui"], "abstract": "YOLOv8 plays a crucial role in the realm of autonomous driving, owing to its high-speed target detection, precise identification and positioning, and versatile compatibility across multiple platforms. By processing video streams or images in real-time, YOLOv8 rapidly and accurately identifies obstacles such as vehicles and pedestrians on roadways, offering essential visual data for autonomous driving systems. Moreover, YOLOv8 supports various tasks including instance segmentation, image classification, and attitude estimation, thereby providing comprehensive visual perception for autonomous driving, ultimately enhancing driving safety and efficiency. Recognizing the significance of object detection in autonomous driving scenarios and the challenges faced by existing methods, this paper proposes a holistic approach to enhance the YOLOv8 model. The study introduces two pivotal modifications: the C2f_RFAConv module and the Triplet Attention mechanism. Firstly, the proposed modifications are elaborated upon in the methodological section. The C2f_RFAConv module replaces the original module to enhance feature extraction efficiency, while the Triplet Attention mechanism enhances feature focus. Subsequently, the experimental procedure delineates the training and evaluation process, encompassing training the original YOLOv8, integrating modified modules, and assessing performance improvements using metrics and PR curves. The results demonstrate the efficacy of the modifications, with the improved YOLOv8 model exhibiting significant performance enhancements, including increased MAP values and improvements in PR curves. Lastly, the analysis section elucidates the results and attributes the performance improvements to the introduced modules. C2f_RFAConv enhances feature extraction efficiency, while Triplet Attention improves feature focus for enhanced target detection.", "sections": [{"title": "Introduction", "content": "With the rapid development of autonomous driving technology, the accurate perception and understanding of the surrounding environment of the vehicle has become crucial. Image detection, as the core component of the automatic driving system, directly affects the decision-making and safety performance of the vehicle. In recent years, deep learning, especially convolutional neural networks (CNNs), have made remarkable progress in image detection tasks, such as the [1]YOLO series models (You Only Look Once), which perform well in real-time and detection accuracy. However, faced with the complex and changeable traffic environment, existing methods still face challenges in feature extraction and multi-scale information processing. In practical applications of autonomous driving, image detection needs to deal with a large number of complex scenes, including different lighting conditions, weather changes, and dynamic targets. These factors present the following challenges to detection models:\n1. Multi-scale target detection: Targets in the traffic environment vary in size, ranging from distant pedestrians to nearby vehicles, requiring detection models to have strong multi-scale perception abilities.\n2. Real-time processing: The automatic driving system requires efficient computing performance to ensure real-time processing and response, thereby avoiding safety hazards caused by delays.\n3. Robustness: [2]The model needs to maintain stable performance in various extreme conditions, including rain, night, and occlusion.\n4. Accuracy of feature extraction: In complex traffic scenes, accurate extraction of useful features is crucial for target recognition and classification.\nTherefore, to address the above challenges, this paper proposes an improved YOLOv8 model that combines the RFAConv and Triplet Attention mechanisms. RFAConv (Receptive Field Attention Convolution) and the Triplet Attention mechanism are introduced to optimize the performance of the YOLOv8 model and improve the image detection capability of the automatic driving system. It is demonstrated that the YOLOv8 model combined with RFAConv and the Triplet Attention mechanism can not only extract and represent key information in images more accurately but also enhance the image detection performance of the automatic driving system while ensuring real-time performance. This paper will detail the design and implementation of these two mechanisms and verify their effectiveness through experiments."}, {"title": "Related Work", "content": "2.1. Automatic driving image detection technology\nNormally, autopilot uses eight cameras to identify objects in the real world. The images captured by the cameras include pedestrians, vehicles, animals, and obstacles, which are important not only for the safety of the drivers of unmanned autonomous vehicles, but also for others. It is important that the camera is able to identify these objects in a timely and accurate manner.\nThe following is the automatic driving image recognition framework layer"}, {"title": "YOLOv8 model", "content": "The YOLO (You Only Look Once) series of models became very famous in the field of computer vision. YOLO is famous because it has a fairly high accuracy while maintaining the size of a small model. YOLO models can be trained on a single GPU, which makes them suitable for a wide range of developers. Machine learning practitioners can deploy it at low cost on edge hardware or in the cloud. Since it was first released by Joseph Redmond in 2015, YOLO has been on the radar of the computer vision community. In earlier versions (versions 1-4), YOLO was maintained in C code in a custom deep learning framework called Darknet written by Redmond.\nThe object detection and tracking model YOLOv8 can quickly and accurately identify and locate multiple objects in an image or video frame, as well as track their movement and classify them. In addition to detecting objects, YOLOv8[5] can also distinguish the exact contours of objects, perform instance segmentation, estimate human posture, and help identify and analyze specific patterns in medical images, among other computer vision tasks.\nThe main functions of YOLOv8 model include:\n1. High-speed target detection: YOLOv8 continues to maintain the high-speed detection characteristics of YOLO series models, capable of real-time processing of video streams or high-speed analysis of targets in static images.\n2. High-precision recognition: Through the improved algorithm and network structure, YOLOv8 improves the accuracy of target detection, including better boundary frame positioning and classification accuracy.\n3, multi-platform compatibility: YOLOv8[6] supports deployment through a variety of formats such as ONNX, OpenVINO, CoreML and TFLite, enhancing the availability and compatibility of the model, enabling it to run on a variety of hardware and platforms.\n4. Multi-task capability: In addition to target detection, YOLOv8 also supports tasks such as instance segmentation, image classification and pose estimation, providing a one-stop solution for a variety of visual recognition needs.\nApplication scenarios of YOLOv8 model:\nObject detection: YOLOv8 is able to quickly and accurately identify and locate multiple objects in an image or video frame. This is particularly useful for security monitoring, traffic flow monitoring,"}, {"title": "Triplet Attention mechanism principle", "content": "The recent proliferation of attention mechanisms across various computer vision tasks underscores their efficacy in leveraging interdependencies among channels or spatial positions. In this paper, we explore a lightweight yet effective attention mechanism termed Triplet Attention, which captures cross-dimensional interactions using a tripartite structure to compute attention weights. Triplet Attention establishes dependencies between dimensions through a residual transformation following rotational operations on input tensors, encoding information across channels and spatial locations with negligible computational overhead. Our method is straightforward and efficient, seamlessly integrable as an additional module into classic backbone networks. We validate the effectiveness of our approach on challenging tasks including image classification on ImageNet-1k and object detection on [8]MSCOCO and PASCAL VOC datasets. Furthermore, through intuitive examination of GradCAM and GradCAM++ results, we offer deeper insights into the performance of Triplet Attention. Empirical evaluations corroborate our intuition, highlighting the importance of capturing cross-dimensional dependencies when computing attention weights."}, {"title": "Triplet Attention and other simple attention mechanisms", "content": "In the realm of attention mechanisms for enhancing neural network performance in computer vision tasks, various modules have been proposed. Each module employs distinct strategies to compute attention weights and amplify the significance of salient features within input data. Here, we delineate the key characteristics and operational methodologies of four prominent attention modules, including Squeeze Excitation (SE)[9], Convolutional Block Attention Module (CBAM), Global Context (GC), and our proposed Triplet Attention module:\n1. Squeeze Excitation (SE) Module:\n\u2022 Utilizes global average pooling to generate channel descriptors.\n\u2022 Employs two fully connected layers (1x1 Conv) with ReLU activation function, followed by a Sigmoid function to produce channel-wise weights.\n2. Convolutional Block Attention Module (CBAM):\n\u2022 Integrates global average pooling and global max pooling (GAP+GMP), followed by convolutional layers and ReLU activation, culminating in a Sigmoid function to compute attention weights."}, {"title": "Global Context (GC) Module", "content": "3. Global Context (GC) Module:\n\u2022 Initiates with a 1x1 convolutional layer, followed by normalization using Softmax function.\n\u2022 Subsequently, another 1x1 convolutional layer is applied, followed by LayerNorm and final 1x1 convolution, which are combined with the original feature map using broadcast addition.\n4. Triplet Attention :\n\u2022 Comprises three branches, each dedicated to distinct types of feature interactions.\n\u2022 The upper branch computes attention weights for channel and spatial dimensions (C and W) using Z pooling followed by a convolutional layer and Sigmoid activation.\n\u2022 The middle branch captures dependencies between channel dimensions (C) and spatial dimensions (H and W) using similar Z pooling and convolution operations followed by Sigmoid activation.\n\u2022 The lower branch focuses on capturing dependencies among spatial dimensions (H and W) while maintaining the identity of the input, executing max pooling and convolution operations, followed by Sigmoid activation.\n\u2022 After generating attention weights, each branch permutes the input, and their outputs are aggregated via average pooling to yield the final output of Triplet Attention.\nThis unique architecture of Triplet Attention, with its tailored operations across multiple dimensions and efficient computation, enables comprehensive feature representation and enhances a network's ability to discern critical features in various visual tasks. Additionally, its modular design facilitates seamless integration into existing network architectures, thereby augmenting their understanding and processing capabilities for complex data structures.\nIn summary, Triplet Attention stands out among other attention mechanisms in computer vision tasks due to its unique architecture and efficient computation. By leveraging a tripartite structure to capture cross-dimensional interactions, Triplet Attention effectively computes attention weights, enhancing a network's ability to focus on relevant features across different dimensions. This approach facilitates comprehensive feature representation and improves the model's performance in various visual tasks. Furthermore, Triplet Attention's modular design enables seamless integration into existing network architectures, augmenting their understanding and processing capabilities for complex data structures. This comparative analysis sets the stage for further exploration of Triplet Attention's effectiveness in the experimental section."}, {"title": "Methodology", "content": "3.1. Focus on the spatial characteristics of receptive field\nThe receptive field spatial feature refers to the local region of input data that the convolutional layer can \"see\" in the convolutional neural network[10] (CNN). In CNN, the output of each convolution operation is a small window, or a local receptive field, based on the input data. This receptive field defines the size and range of input data that the convolution kernel can access.\nThe concept of receptive fields is crucial to understanding how CNNS extract features from input data. In the primary layer of the network, the receptive field is usually small, allowing the model to capture subtle local features such as edges and corner points. As the data passes through more convolutional layers, the receptive field gradually expands by stacking layers on top of each other, allowing the network to perceive larger areas and capture more complex features such as textures and parts of objects."}, {"title": "Convolutional network layer realizes k processing", "content": "The figure 4 above shows a 3x3 convolution operation. In this operation, the features are obtained by multiplying the convolution kernel with a receptive field slider of the same size and then summing it. Specifically, every 3x3 region (i.e. receptive field) on the input image X is processed by a 3x3 convolution kernel K. Each element in the receptive field, Xij(where and represents the position in the receptive field), is multiplied by the weight of the corresponding position in the convolution kernel K, Kij, and then these products are summed to give a new eigenvalue F. This process is carried out by sliding over the entire input image to generate a new feature map. This standard convolution operation emphasizes the concept of local join and weight sharing, i.e. the weight of the convolution kernel to the entire input graph.\nIn the context of CNN, receptive field spatial features refer to the features in the input image region that are perceived by each convolution operation. These features can include basic visual elements such as color, shape, and texture. In traditional convolutional networks, the receptive field is usually fixed, and each position is treated the same way. However, if the network can adapt the processing of the receptive field to the different characteristics of each region, then the network's understanding of the features will be more refined and adaptive.\nTherefore, in this paper, we propose the RFAConv module, which aims to focus on the spatial characteristics of receptive fields. By introducing an adaptive receptive field mechanism, the RFAConv module can dynamically adjust the size and range of receptive fields according to local features of input data, thus capturing important spatial features more efficiently. Specifically, the RFAConv module utilizes an adaptive receptive field mechanism to weight the input data at each location to increase attention to important features, thereby enhancing the model's ability to understand complex scenes.\nWith the RFAConv module, we are able to extract key information from input data more accurately, providing a more reliable feature representation for subsequent target detection and recognition tasks."}, {"title": "Mechanism to solve parameter sharing problem", "content": "RFAConv convolution solves the problem of parameter sharing by introducing an attention mechanism that allows the network to assign a specific weight to each perception. In this way, the convolution kernel can dynamically adjust its parameters according to different features within each receptive field, rather than treating all regions equally.\nSpecifically, RFAConv uses spatial attention to determine the importance of each position in the receptive field and adjusts the weight of the convolution kernel accordingly. In this way, each receptive field has its own unique convolution kernel, rather than all receptive fields sharing the same kernel. This approach enables the network to learn local features in images in more detail, which helps improve overall network performance.\nIn this way, RFAConv improves the expressiveness of the model, allowing it to more accurately adapt and express the features of the input data, especially when dealing with complex or variable image"}, {"title": "Improve the efficiency of large-size convolution kernel", "content": "RFAConv dynamically adjusts the weights of convolutional kernels by leveraging receptive field attention mechanism, providing customized attention for feature extraction in each region. This allows even large-sized convolutional kernels to effectively capture and process important spatial features without allocating excessive computational resources to less relevant information.\nSpecifically, the RFAConv method enables the network to identify and emphasize the more important regions in the input feature map and adjust the weights of convolutional kernels accordingly. This means that the network can reweight critical features, allowing large-sized convolutional kernels to not only capture a wide range of information but also concentrate computational resources on more informative features, thereby enhancing overall processing efficiency and network performance. This addresses the common phenomenon in standard convolution operations where feature overlap leads to weight sharing issues, implying that different receptive fields may use the same attention weights for the same input features.\nIn the illustration, F1, F2, FN represent the feature outputs within different receptive field sliders, obtained through element-wise multiplication of the input features X with corresponding attention weights A and convolutional kernel weights K. For instance, F1 is computed by multiplying X11 by the corresponding attention weight A11 and convolutional kernel weight K1, and so forth.\nThe diagram emphasizes that the parameters of convolution operations within each receptive field slider should not be entirely shared, but rather adjusted based on the features and corresponding attention weights in each specific region. This adjustment allows the network to handle each local region more finely, better capturing and responding to specific features of the input data rather than simply applying the same weights to the entire image. Such an approach enhances the network's understanding and representation of features, thereby improving learning and prediction outcomes.\nIn summary, through this approach, [13]RFAConv enhances the model's expressive power, allowing it to more accurately adapt to and represent the features of input data, especially when dealing with complex or variable image content. This flexible parameter adjustment mechanism provides a new pathway for improving the performance and generalization capability of convolutional neural networks."}, {"title": "Experimental Procedure", "content": "4.1. Experimental design\nThe main objective of this experiment is to improve the performance of YOLOv8 in target detection by improving its network structure. We use C2f_RFAConv to replace the C2f module in the original YOLOv8 network, and introduce RFAConv and Triplet Attention modules. We will verify the improvement effect by comparing the performance indicators before and after the improvement.\n1. Experimental environment:\n\u2022 Hardware: GPU (e.g. NVIDIA Tesla V100)\n\u2022 Software: Python 3.x, PyTorch, YOLOv8 framework\n2. Data set:\n\u2022 Training and testing using the COCO dataset.\n4.2. Experimental Model\n1.Original YOLOv8 network training:\nUsing the original YOLOv8 network structure, we trained on the COCO dataset, saving the model and log files during the training process. After the training, we recorded the final performance indicators with MAP(50) values of 0.326 and MAP(50-95) values of 0.187. In order to visually demonstrate the detection performance of the model, the precision rate-recall ratio (PR) curve of the original YOLOv8 was drawn and saved in this experiment."}, {"title": "Introduction of C2f RFAConv module", "content": "Replace C2f module in YOLOv8 network with C2f_RFAConv module to improve model performance. The concrete implementation steps include: First, we replace the Bottleneck structure in C2f with RFAConv, and then build a new C2f_RFAConv module. The implementation code for the C2f_RFAConv module is provided, which contains detailed definitions of the C2f_RFAConv and RFAConv classes. RFAConv makes convolution operations more efficient by introducing attention mechanisms and multi-scale feature extraction. We integrated these new modules into the YOLOv8 network to make sure the code was up and running. Through training on the COCO dataset, we verify the correctness and validity of the model."}, {"title": "To introduce the Triplet Attention module", "content": "The improved YOLOv8 network structure is used to train on the COCO dataset, and the training results are recorded, including the final MAP(50) value of 0.385 and MAP(50-95) value of 0.217. To visualize the performance of the improved network, we drew and saved a new PR curve. These curves show the relationship between accuracy rate and recall rate under different detection thresholds. The overall curve should be closer to the upper right corner than the original model, indicating that the improved model has significantly improved both accuracy and recall rate. These results and charts comprehensively evaluate the improved network performance and provide a basis for further optimization.\n4.Performance comparison and analysis:\nBy comparing the performance indicators of the original YOLOv8 and the improved YOLOv8, we mainly focus on the changes of MAP(50) and MAP(50-95). The results show that the improved YOLOv8's MAP(50) has been improved from 0.326 to 0.385, and MAP(50-95) has been improved from 0.187 to 0.217. After calculating the performance improvement of each category, it is found that the detection accuracy of all categories has been improved in different degrees. The reasons for the"}, {"title": "Experimental Result", "content": "4.3. Experimental Result\n\u2022 Original YOLOv8: MAP(50): 0.326-MAP(50-95): 0.187\n\u2022 Improved YOLOv8-C2f_RFAConv-Triplet Attention-P2: MAP(50): 0.385-MAP(50-95): 0.217\nComparing the performance indicators of the original YOLOv8 and the improved YOLOV8-C2F_RfaconV-triplet Attention, the MAP(50) of the improved model has been improved from 0.326 to 0.385, with an increase of 0.059, and the MAP(50-95) has been improved from 0.187 to 0.217. An increase of 0.030. By analyzing the PR curve, the improved PR curve moves towards the upper right corner as a whole, showing that the accuracy rate and recall rate have been improved under various detection thresholds, indicating that the improved model has significantly improved the detection performance in each category. This trend further validates the effectiveness of introducing"}, {"title": "Improvement Analysis", "content": "4.4. Improvement Analysis\nIn this experiment, the addition of detection head P2 significantly improved the detection performance of YOLOv8, especially in handling small targets, alleviating the impact of scale variance, enhancing robustness and reducing computation burden. P2 layer usually has higher resolution and is able to capture more details of small size targets, helping to improve small target detection by providing richer spatial information. At the same time, Layer P2 is located in the shallower layer of the network and can capture more fine-grained features, which is crucial for understanding the shape and texture of small targets. [14-15]This makes the detection ability of the model to small targets significantly improved. The detection head derived from layer P2 combined with the original detection head can effectively alleviate the negative impact of scale variance, so that the model has better adaptability and accuracy when dealing with different scale targets. In addition, P2 detection head has stronger anti-interference ability, can better adapt to different scenes and sizes of detection targets, improve the robustness of the algorithm.\nIn the improved analysis, the C2f_RFAConv module significantly improves the performance of the network through more efficient feature extraction and weighting mechanisms. C2f_RFAConv uses weighted feature extraction in RFAConv to capture important feature information more accurately and improve the expression ability of feature maps. The multi-scale feature extraction mechanism makes the model more flexible when dealing with objects of different sizes, and enhances the overall detection ability of the model.\nThe Triplet Attention module also plays an important role in feature extraction. By applying Attention mechanisms on both spatial and channel dimensions, Triplet Attention is able to focus more accurately on important feature areas, ignoring disturbing information. This dual attention mechanism greatly improves the accuracy of target detection in complex background. Attention in the spatial dimension can ensure that the model focuses on the local details of the target, while attention in the channel dimension optimizes the information interaction between the feature channels, thus improving the discriminant ability of the feature graph. In short, the combination of C2f_RFAConv and Triplet Attention module greatly enhances the feature extraction capability and detection accuracy of YOLOv8, making the model perform better in various detection tasks."}, {"title": "Conclusion", "content": "In conclusion, YOLOv8 stands as a cornerstone technology in the realm of autonomous driving, offering rapid and precise target detection capabilities vital for ensuring road safety and efficiency. This study has presented a comprehensive enhancement approach for the YOLOv8 model, addressing its crucial role and the challenges faced in autonomous driving scenarios. By introducing the C2f_RFAConv module and the Triplet Attention mechanism, significant improvements have been achieved in feature extraction efficiency and feature focusing, respectively. Through meticulous experimentation and evaluation, the effectiveness of these modifications has been demonstrated, as evidenced by notable increases in MAP values and enhancements in PR curves. Ultimately, the combination of these modifications results in a marked enhancement in the overall performance of the YOLOv8 model, promising improved safety and efficacy in autonomous driving systems.\nIn summary, the findings of this study underscore the pivotal role of advanced computer vision techniques, such as those integrated into the YOLOv8 model, in advancing the capabilities of autonomous driving systems. The successful implementation of the C2f_RFAConv module and the Triplet Attention mechanism has not only demonstrated the adaptability and versatility of the YOLOv8 framework but has also showcased its potential for further refinement and optimization. By leveraging these enhancements, the improved YOLOv8 model exhibits enhanced feature extraction efficiency and improved target detection accuracy, paving the way for safer and more efficient autonomous driving experiences in real-world scenarios."}]}