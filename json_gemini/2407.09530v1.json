{"title": "Optimization of Autonomous Driving Image Detection Based\non RFAConv and Triplet Attention", "authors": ["Zhipeng Ling", "Qi Xin", "Yiyu Lin", "Guangze Su", "Zuwei Shui"], "abstract": "YOLOv8 plays a crucial role in the realm of autonomous driving, owing to its high-speed target\ndetection, precise identification and positioning, and versatile compatibility across multiple\nplatforms. By processing video streams or images in real-time, YOLOv8 rapidly and accurately\nidentifies obstacles such as vehicles and pedestrians on roadways, offering essential visual data\nfor autonomous driving systems. Moreover, YOLOv8 supports various tasks including instance\nsegmentation, image classification, and attitude estimation, thereby providing comprehensive\nvisual perception for autonomous driving, ultimately enhancing driving safety and efficiency.\nRecognizing the significance of object detection in autonomous driving scenarios and the\nchallenges faced by existing methods, this paper proposes a holistic approach to enhance the\nYOLOv8 model. The study introduces two pivotal modifications: the C2f_RFAConv module\nand the Triplet Attention mechanism. Firstly, the proposed modifications are elaborated upon in\nthe methodological section. The C2f_RFAConv module replaces the original module to enhance\nfeature extraction efficiency, while the Triplet Attention mechanism enhances feature focus.\nSubsequently, the experimental procedure delineates the training and evaluation process,\nencompassing training the original YOLOv8, integrating modified modules, and assessing\nperformance improvements using metrics and PR curves. The results demonstrate the efficacy\nof the modifications, with the improved YOLOv8 model exhibiting significant performance\nenhancements, including increased MAP values and improvements in PR curves. Lastly, the\nanalysis section elucidates the results and attributes the performance improvements to the\nintroduced modules. C2f_RFAConv enhances feature extraction efficiency, while Triplet\nAttention improves feature focus for enhanced target detection.", "sections": [{"title": "Introduction", "content": "With the rapid development of autonomous driving technology, the accurate perception and\nunderstanding of the surrounding environment of the vehicle has become crucial. Image detection, as\nthe core component of the automatic driving system, directly affects the decision-making and safety\nperformance of the vehicle. In recent years, deep learning, especially convolutional neural networks\n(CNNs), have made remarkable progress in image detection tasks, such as the [1]YOLO series models\n(You Only Look Once), which perform well in real-time and detection accuracy. However, faced with\nthe complex and changeable traffic environment, existing methods still face challenges in feature\nextraction and multi-scale information processing. In practical applications of autonomous driving,\nimage detection needs to deal with a large number of complex scenes, including different lighting\nconditions, weather changes, and dynamic targets. These factors present the following challenges to\ndetection models:\n1. Multi-scale target detection: Targets in the traffic environment vary in size, ranging from distant\npedestrians to nearby vehicles, requiring detection models to have strong multi-scale perception abilities.\n2. Real-time processing: The automatic driving system requires efficient computing performance to\nensure real-time processing and response, thereby avoiding safety hazards caused by delays.\n3. Robustness: [2]The model needs to maintain stable performance in various extreme conditions,\nincluding rain, night, and occlusion.\n4. Accuracy of feature extraction: In complex traffic scenes, accurate extraction of useful features is\ncrucial for target recognition and classification.\nTherefore, to address the above challenges, this paper proposes an improved YOLOv8 model that\ncombines the RFAConv and Triplet Attention mechanisms. RFAConv (Receptive Field Attention\nConvolution) and the Triplet Attention mechanism are introduced to optimize the performance of the\nYOLOv8 model and improve the image detection capability of the automatic driving system. It is\ndemonstrated that the YOLOv8 model combined with RFAConv and the Triplet Attention mechanism\ncan not only extract and represent key information in images more accurately but also enhance the image\ndetection performance of the automatic driving system while ensuring real-time performance. This paper\nwill detail the design and implementation of these two mechanisms and verify their effectiveness\nthrough experiments."}, {"title": "Related Work", "content": ""}, {"title": "Automatic driving image detection technology", "content": "Normally, autopilot uses eight cameras to identify objects in the real world. The images captured by\nthe cameras include pedestrians, vehicles, animals, and obstacles, which are important not only for the\nsafety of the drivers of unmanned autonomous vehicles, but also for others. It is important that the\ncamera is able to identify these objects in a timely and accurate manner.\nThe following is the automatic driving image recognition framework layer"}, {"title": "YOLOv8 model", "content": "The YOLO (You Only Look Once) series of models became very famous in the field of computer\nvision. YOLO is famous because it has a fairly high accuracy while maintaining the size of a small\nmodel. YOLO models can be trained on a single GPU, which makes them suitable for a wide range of\ndevelopers. Machine learning practitioners can deploy it at low cost on edge hardware or in the cloud.\nSince it was first released by Joseph Redmond in 2015, YOLO has been on the radar of the computer\nvision community. In earlier versions (versions 1-4), YOLO was maintained in C code in a custom deep\nlearning framework called Darknet written by Redmond.\nThe object detection and tracking model YOLOv8 can quickly and accurately identify and locate\nmultiple objects in an image or video frame, as well as track their movement and classify them. In\naddition to detecting objects, YOLOv8[5] can also distinguish the exact contours of objects, perform\ninstance segmentation, estimate human posture, and help identify and analyze specific patterns in\nmedical images, among other computer vision tasks.\nThe main functions of YOLOv8 model include:\n1. High-speed target detection: YOLOv8 continues to maintain the high-speed detection\ncharacteristics of YOLO series models, capable of real-time processing of video streams or high-speed\nanalysis of targets in static images.\n2. High-precision recognition: Through the improved algorithm and network structure, YOLOv8\nimproves the accuracy of target detection, including better boundary frame positioning and classification\naccuracy.\n3, multi-platform compatibility: YOLOv8[6] supports deployment through a variety of formats such\nas ONNX, OpenVINO, CoreML and TFLite, enhancing the availability and compatibility of the model,\nenabling it to run on a variety of hardware and platforms.\n4. Multi-task capability: In addition to target detection, YOLOv8 also supports tasks such as instance\nsegmentation, image classification and pose estimation, providing a one-stop solution for a variety of\nvisual recognition needs.\nApplication scenarios of YOLOv8 model:\nObject detection: YOLOv8 is able to quickly and accurately identify and locate multiple objects in\nan image or video frame. This is particularly useful for security monitoring, traffic flow monitoring,"}, {"title": "Triplet Attention mechanism principle", "content": "The recent proliferation of attention mechanisms across various computer vision tasks underscores\ntheir efficacy in leveraging interdependencies among channels or spatial positions. In this paper, we\nexplore a lightweight yet effective attention mechanism termed Triplet Attention, which captures cross-\ndimensional interactions using a tripartite structure to compute attention weights. Triplet Attention\nestablishes dependencies between dimensions through a residual transformation following rotational\noperations on input tensors, encoding information across channels and spatial locations with negligible\ncomputational overhead. Our method is straightforward and efficient, seamlessly integrable as an\nadditional module into classic backbone networks. We validate the effectiveness of our approach on\nchallenging tasks including image classification on ImageNet-1k and object detection on [8]MSCOCO\nand PASCAL VOC datasets. Furthermore, through intuitive examination of GradCAM and\nGradCAM++ results, we offer deeper insights into the performance of Triplet Attention. Empirical\nevaluations corroborate our intuition, highlighting the importance of capturing cross-dimensional\ndependencies when computing attention weights."}, {"title": "Triplet Attention and other simple attention mechanisms", "content": "In the realm of attention mechanisms for enhancing neural network performance in computer vision\ntasks, various modules have been proposed. Each module employs distinct strategies to compute\nattention weights and amplify the significance of salient features within input data. Here, we delineate\nthe key characteristics and operational methodologies of four prominent attention modules, including\nSqueeze Excitation (SE)[9], Convolutional Block Attention Module (CBAM), Global Context (GC),\nand our proposed Triplet Attention module:\n1. Squeeze Excitation (SE) Module:\n\u2022 Utilizes global average pooling to generate channel descriptors.\n\u2022 Employs two fully connected layers (1x1 Conv) with ReLU activation function, followed by a\nSigmoid function to produce channel-wise weights.\n2. Convolutional Block Attention Module (CBAM):\n\u2022 Integrates global average pooling and global max pooling (GAP+GMP), followed by convolutional\nlayers and ReLU activation, culminating in a Sigmoid function to compute attention weights."}, {"title": "Methodology", "content": ""}, {"title": "Focus on the spatial characteristics of receptive field", "content": "The receptive field spatial feature refers to the local region of input data that the convolutional layer\ncan \"see\" in the convolutional neural network[10] (CNN). In CNN, the output of each convolution\noperation is a small window, or a local receptive field, based on the input data. This receptive field\ndefines the size and range of input data that the convolution kernel can access.\nThe concept of receptive fields is crucial to understanding how CNNS extract features from input\ndata. In the primary layer of the network, the receptive field is usually small, allowing the model to\ncapture subtle local features such as edges and corner points. As the data passes through more\nconvolutional layers, the receptive field gradually expands by stacking layers on top of each other,\nallowing the network to perceive larger areas and capture more complex features such as textures and\nparts of objects."}, {"title": "Convolutional network layer realizes k processing", "content": "The figure 4 above shows a 3x3 convolution operation. In this operation, the features are obtained\nby multiplying the convolution kernel with a receptive field slider of the same size and then summing\nit. Specifically, every 3x3 region (i.e. receptive field) on the input image X is processed by a 3x3\nconvolution kernel K. Each element in the receptive field, Xij(where and represents the position in the\nreceptive field), is multiplied by the weight of the corresponding position in the convolution kernel K,\nKij, and then these products are summed to give a new eigenvalue F. This process is carried out by\nsliding over the entire input image to generate a new feature map. This standard convolution operation\nemphasizes the concept of local join and weight sharing, i.e. the weight of the convolution kernel to the\nentire input graph.\nIn the context of CNN, receptive field spatial features refer to the features in the input image region\nthat are perceived by each convolution operation. These features can include basic visual elements such\nas color, shape, and texture. In traditional convolutional networks, the receptive field is usually fixed,\nand each position is treated the same way. However, if the network can adapt the processing of the\nreceptive field to the different characteristics of each region, then the network's understanding of the\nfeatures will be more refined and adaptive.\nTherefore, in this paper, we propose the RFAConv module, which aims to focus on the spatial\ncharacteristics of receptive fields. By introducing an adaptive receptive field mechanism, the RFAConv\nmodule can dynamically adjust the size and range of receptive fields according to local features of input\ndata, thus capturing important spatial features more efficiently. Specifically, the RFAConv module\nutilizes an adaptive receptive field mechanism to weight the input data at each location to increase\nattention to important features, thereby enhancing the model's ability to understand complex scenes.\nWith the RFAConv module, we are able to extract key information from input data more accurately,\nproviding a more reliable feature representation for subsequent target detection and recognition tasks."}, {"title": "Mechanism to solve parameter sharing problem", "content": "RFAConv convolution solves the problem of parameter sharing by introducing an attention\nmechanism that allows the network to assign a specific weight to each perception. In this way, the\nconvolution kernel can dynamically adjust its parameters according to different features within each\nreceptive field, rather than treating all regions equally.\nSpecifically, RFAConv uses spatial attention to determine the importance of each position in the\nreceptive field and adjusts the weight of the convolution kernel accordingly. In this way, each receptive\nfield has its own unique convolution kernel, rather than all receptive fields sharing the same kernel. This\napproach enables the network to learn local features in images in more detail, which helps improve\noverall network performance.\nIn this way, RFAConv improves the expressiveness of the model, allowing it to more accurately\nadapt and express the features of the input data, especially when dealing with complex or variable image"}, {"title": "Improve the efficiency of large-size convolution kernel", "content": "RFAConv dynamically adjusts the weights of convolutional kernels by leveraging receptive field\nattention mechanism, providing customized attention for feature extraction in each region. This allows\neven large-sized convolutional kernels to effectively capture and process important spatial features\nwithout allocating excessive computational resources to less relevant information.\nSpecifically, the RFAConv method enables the network to identify and emphasize the more\nimportant regions in the input feature map and adjust the weights of convolutional kernels accordingly.\nThis means that the network can reweight critical features, allowing large-sized convolutional kernels\nto not only capture a wide range of information but also concentrate computational resources on more\ninformative features, thereby enhancing overall processing efficiency and network performance. This\naddresses the common phenomenon in standard convolution operations where feature overlap leads to\nweight sharing issues, implying that different receptive fields may use the same attention weights for\nthe same input features.\nIn the illustration, F1, F2, FN represent the feature outputs within different receptive field sliders,\nobtained through element-wise multiplication of the input features X with corresponding attention\nweights A and convolutional kernel weights K. For instance, F1 is computed by multiplying X11 by the\ncorresponding attention weight A11 and convolutional kernel weight K1, and so forth."}, {"title": "Experimental Procedure", "content": "The diagram emphasizes that the parameters of convolution operations within each receptive field\nslider should not be entirely shared, but rather adjusted based on the features and corresponding attention\nweights in each specific region. This adjustment allows the network to handle each local region more\nfinely, better capturing and responding to specific features of the input data rather than simply applying\nthe same weights to the entire image. Such an approach enhances the network's understanding and\nrepresentation of features, thereby improving learning and prediction outcomes.\nIn summary, through this approach, [13]RFAConv enhances the model's expressive power, allowing\nit to more accurately adapt to and represent the features of input data, especially when dealing with\ncomplex or variable image content. This flexible parameter adjustment mechanism provides a new\npathway for improving the performance and generalization capability of convolutional neural networks."}, {"title": "Experimental design", "content": "The main objective of this experiment is to improve the performance of YOLOv8 in target detection\nby improving its network structure. We use C2f_RFAConv to replace the C2f module in the original\nYOLOv8 network, and introduce RFAConv and Triplet Attention modules. We will verify the\nimprovement effect by comparing the performance indicators before and after the improvement.\n1. Experimental environment:\n\u2022 Hardware: GPU (e.g. NVIDIA Tesla V100)\n\u2022 Software: Python 3.x, PyTorch, YOLOv8 framework\n2. Data set:\n\u2022 Training and testing using the COCO dataset."}, {"title": "Experimental Model", "content": "1.Original YOLOv8 network training:\nUsing the original YOLOv8 network structure, we trained on the COCO dataset, saving the model\nand log files during the training process. After the training, we recorded the final performance indicators\nwith MAP(50) values of 0.326 and MAP(50-95) values of 0.187. In order to visually demonstrate the\ndetection performance of the model, the precision rate-recall ratio (PR) curve of the original YOLOv8\nwas drawn and saved in this experiment."}, {"title": "Introduction of C2f RFAConv module:", "content": "Replace C2f module in YOLOv8 network with C2f_RFAConv module to improve model\nperformance. The concrete implementation steps include: First, we replace the Bottleneck structure in\nC2f with RFAConv, and then build a new C2f_RFAConv module. The implementation code for the\nC2f_RFAConv module is provided, which contains detailed definitions of the C2f_RFAConv and\nRFAConv classes. RFAConv makes convolution operations more efficient by introducing attention\nmechanisms and multi-scale feature extraction. We integrated these new modules into the YOLOv8\nnetwork to make sure the code was up and running. Through training on the COCO dataset, we verify\nthe correctness and validity of the model."}, {"title": "To introduce the Triplet Attention module:", "content": "The improved YOLOv8 network structure is used to train on the COCO dataset, and the training\nresults are recorded, including the final MAP(50) value of 0.385 and MAP(50-95) value of 0.217. To\nvisualize the performance of the improved network, we drew and saved a new PR curve. These curves\nshow the relationship between accuracy rate and recall rate under different detection thresholds. The\noverall curve should be closer to the upper right corner than the original model, indicating that the\nimproved model has significantly improved both accuracy and recall rate. These results and charts\ncomprehensively evaluate the improved network performance and provide a basis for further\noptimization."}, {"title": "Performance comparison and analysis:", "content": "By comparing the performance indicators of the original YOLOv8 and the improved YOLOv8, we\nmainly focus on the changes of MAP(50) and MAP(50-95). The results show that the improved\nYOLOv8's MAP(50) has been improved from 0.326 to 0.385, and MAP(50-95) has been improved from\n0.187 to 0.217. After calculating the performance improvement of each category, it is found that the\ndetection accuracy of all categories has been improved in different degrees. The reasons for the"}, {"title": "Experimental Result", "content": "Comparing the performance indicators of the original YOLOv8 and the improved YOLOV8-\nC2F_RfaconV-triplet Attention, the MAP(50) of the improved model has been improved from 0.326 to\n0.385, with an increase of 0.059, and the MAP(50-95) has been improved from 0.187 to 0.217. An\nincrease of 0.030. By analyzing the PR curve, the improved PR curve moves towards the upper right\ncorner as a whole, showing that the accuracy rate and recall rate have been improved under various\ndetection thresholds, indicating that the improved model has significantly improved the detection\nperformance in each category. This trend further validates the effectiveness of introducing"}, {"title": "Improvement Analysis", "content": "In this experiment, the addition of detection head P2 significantly improved the detection\nperformance of YOLOv8, especially in handling small targets, alleviating the impact of scale variance,\nenhancing robustness and reducing computation burden. P2 layer usually has higher resolution and is\nable to capture more details of small size targets, helping to improve small target detection by providing\nricher spatial information. At the same time, Layer P2 is located in the shallower layer of the network\nand can capture more fine-grained features, which is crucial for understanding the shape and texture of\nsmall targets. [14-15]This makes the detection ability of the model to small targets significantly\nimproved. The detection head derived from layer P2 combined with the original detection head can\neffectively alleviate the negative impact of scale variance, so that the model has better adaptability and\naccuracy when dealing with different scale targets. In addition, P2 detection head has stronger anti-\ninterference ability, can better adapt to different scenes and sizes of detection targets, improve the\nrobustness of the algorithm.\nIn the improved analysis, the C2f_RFAConv module significantly improves the performance of the\nnetwork through more efficient feature extraction and weighting mechanisms. C2f_RFAConv uses\nweighted feature extraction in RFAConv to capture important feature information more accurately and\nimprove the expression ability of feature maps. The multi-scale feature extraction mechanism makes the\nmodel more flexible when dealing with objects of different sizes, and enhances the overall detection\nability of the model.\nThe Triplet Attention module also plays an important role in feature extraction. By applying\nAttention mechanisms on both spatial and channel dimensions, Triplet Attention is able to focus more\naccurately on important feature areas, ignoring disturbing information. This dual attention mechanism\ngreatly improves the accuracy of target detection in complex background. Attention in the spatial\ndimension can ensure that the model focuses on the local details of the target, while attention in the\nchannel dimension optimizes the information interaction between the feature channels, thus improving\nthe discriminant ability of the feature graph. In short, the combination of C2f_RFAConv and Triplet\nAttention module greatly enhances the feature extraction capability and detection accuracy of YOLOv8,\nmaking the model perform better in various detection tasks."}, {"title": "Conclusion", "content": "In conclusion, YOLOv8 stands as a cornerstone technology in the realm of autonomous driving,\noffering rapid and precise target detection capabilities vital for ensuring road safety and efficiency. This\nstudy has presented a comprehensive enhancement approach for the YOLOv8 model, addressing its\ncrucial role and the challenges faced in autonomous driving scenarios. By introducing the\nC2f_RFAConv module and the Triplet Attention mechanism, significant improvements have been\nachieved in feature extraction efficiency and feature focusing, respectively. Through meticulous\nexperimentation and evaluation, the effectiveness of these modifications has been demonstrated, as\nevidenced by notable increases in MAP values and enhancements in PR curves. Ultimately, the\ncombination of these modifications results in a marked enhancement in the overall performance of the\nYOLOv8 model, promising improved safety and efficacy in autonomous driving systems.\nIn summary, the findings of this study underscore the pivotal role of advanced computer vision\ntechniques, such as those integrated into the YOLOv8 model, in advancing the capabilities of\nautonomous driving systems. The successful implementation of the C2f_RFAConv module and the\nTriplet Attention mechanism has not only demonstrated the adaptability and versatility of the YOLOv8\nframework but has also showcased its potential for further refinement and optimization. By leveraging\nthese enhancements, the improved YOLOv8 model exhibits enhanced feature extraction efficiency and"}]}