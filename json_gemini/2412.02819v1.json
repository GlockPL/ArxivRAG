{"title": "CNNSum: Exploring Long-Conext Summarization with Large Language Models in Chinese Novels", "authors": ["Lingxiao Wei", "He Yan", "Xiangju Lu", "Junmin Zhu", "Jun Wang", "Wei Zhang"], "abstract": "Large Language Models (LLMs) have been well-researched in many long-context tasks. However, due to high annotation costs, highquality long-context summary datasets for training or evaluation are scarce, limiting further research. In this work, we introduce CNNSum, a new multi-scale Chinese long-context novel summarization benchmark, including four subsets, length covering 16k~128k, 695 samples in total, the annotations are human-driven. We evaluate commercial and open-source models on CNNSum and conduct a detailed analysis. Based on the observations, we further conduct fine-tuning exploration with short-context summary data. In our study: (1) GPT-40 underperformed, due to excessive subjective commentary. (2) Currently, long-context summarization mainly relies on memory ability, small LLMs with stable longer context lengths are the most cost-effective. Using long data concatenated from short-context summaries makes a significant improvement. (3) Prompt templates may cause a large performance gap but can be mitigated through fine-tuning. (4) Fine-tuned Chat or Instruction versions may harm the Base model and further fine-tuning cannot bridge performance gap. (5) while models with RoPE base scaling exhibit strong extrapolation potential, their performance may vary significantly when combined with other interpolation methods and need careful selection. (6) CNNSum provides more reliable and insightful evaluation results than other benchmarks. We release CNNSum to advance research in this field.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) 's context length has been significantly improved (Pawar et al., 2024; Wang et al., 2024b). Long-text summarization is a crucial task for assessing the long-context ability of LLMs and forms the basis for understanding complex contextual facts (Pagnoni et al., 2021; Bhandari et al., 2020; Kim et al., 2024). Unlike QA or reasoning tasks, summarization is more subjective, without an objective gold standard. Obtaining high-quality summary data is challenging and annotation is costly. Especially for long texts, which require a global understanding and memory of the context, even human experts struggle to annotate directly (Qiu et al., 2024). Therefore, longtext summarization datasets for training or evaluation are scarce, particularly in Chinese. The latest open-source book-length dataset remains BookSum (Kry\u015bci\u0144ski et al., 2021). Recent studies on book-length summarization (Chang et al., 2023; Kim et al., 2024; Song et al., 2024) focus on evaluating sentence coherence or faithfulness, most are expensive in human annotation. However, if outputs are disorganized, meaningless, or fail to follow instructions by generating context-related content but not a summary, these evaluations hold limited significance. The core is how LLMs handle long-context summarization tasks. Due to a lack of datasets, there is still insufficient systematic research and guidance.\nRecently, many benchmarks for long-context LLMs evaluation have emerged (An et al., 2023; Bai et al., 2023b; Ni et al., 2024; Qiu et al., 2024; Zhang et al., 2024b). However, their summarization tasks seem to be not valued and carefully constructed. Regardless of language and domain, we summarize their shortcomings: (1) Built based on previous short summarization datasets (Fabbri et al., 2019; Huang et al., 2021; Sharma et al., 2019; Wu et al., 2023; Kry\u015bci\u0144ski et al., 2021), leading to high data leakage risk (Xu et al., 2024; Li et al., 2024). (2) The sample size is very small, may only dozens. (3) The average and maximum lengths are short (e.g., shorter than 16k). (4) Absence of multi-scale subsets based on length, limited evaluating LLMs in different context lengths. (5) Annotations are collected from web or synthesized by LLMs.\nWeb-collected annotations carry a high data leakage risk and low quality, while LLM-generated book-length summaries tend to have issues like coherence errors (Chang et al., 2023), factual errors (Song et al., 2024; Kim et al., 2024). These benchmarks each have at least two of the above.\nTo better explore and evaluate long-context summarization in LLMs and address the scarcity of Chinese datasets, we introduce Chinese Novel Summarization (CNNSum), a newly constructed, multi-scale benchmark that improved all the shortcomings above. Based on a newly collected Chinese novel corpus and preset target token sequence lengths, we built four subsets: L (16k), XL (32k), 2XL (64k), and 3XL (128k), 695 samples in total. The annotations were completed by human experts with assistance from LLMs. Details see Section 3. (Sclar et al., 2024)demonstrate that prompt templates significantly impact LLMs' performance, (Han et al., 2024; Liu et al., 2024c) shows beginning and ending tokens are crucial in long contexts. For summarization, the key is the relative position of context and \"summary instruction\". We defined prompt types: Instruction at the Beginning (Prompt-IB) and Instruction at the End (Prompt-IE). Using prompts in E.1, we benchmark many commercial and open-source LLMs on CNNSum. According to the results, GPT-40 (OpenAI, 2024b) unexpectedly underperformed compared to other commercial models, we analyzed specific cases to identify the reasons. The two prompts can result in a significant quality gap in LLM outputs. and the Chat or Instruction version may undermine Base model's long-context summarization ability. The memory capacity with longer context length is critical for performance, as large LLMs may struggle to fully leverage their logical reasoning and comprehension abilities, suggesting that small LLMs are cost-effective.\nBased on observations from CNNsum benchmark, we further explored fine-tuning LLMs specific for summarization. However, training directly on long-context summary data is costly and challenging due to data scarcity. (Xiong et al., 2023; Fu et al., 2024) demonstrate that training on short data can obtain longer-context ability, requiring a few long data to activate. Therefore, we redesigned prompts E.2 and concatenated summary data into longer ones for training. Our findings indicate that: (1) The performance gap caused by prompt templates can be mitigated, and Base models are better for further fine-tuning and extrapolation. (2) Fine-tuning with short-context summary data can significantly improve long-context summarization performance. (3) Current open-source models widely use Adjusted Base Frequency (ABF) (Xiong et al., 2023), improving the extrapolation potential of original RoPE, and maintaining decent performance on several times the training length. However, when combined with other interpolation methods, their extrapolation characteristics change, showing interesting differences depending on RoPE base. (4) The only similar work to CNNSum is CLongEval-LStSum (Qiu et al., 2024), constructed by translating BookSum (Kry\u015bci\u0144ski et al., 2021) and merging short annotations with GPT-4. We also used it in fine-tuning experiments. It also has multi-scale subsets with a wide range of sample lengths and are linearly uniformly distributed. While this approach covers a wide length range easily, we find it may lead to misleading results, especially in extrapolation performance evaluation. In contrast, CNNSum employs a more rigorous and reasonable sampling strategy for multi-scale subsets, providing more reliable and insightful evaluation results.\nIn summary, we present CNNSum, a multiscale Chinese long-context novel summarization benchmark, which significantly improves existing datasets' shortcomings in design and construction. We evaluate and analyze factors affecting longcontext summarization tasks for LLMs. We further explore fine-tuning for long-context summarization LLMs and demonstrating the advantages of CNNSum in evaluation. We hope to advance research in this field and provide valuable insights."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Long-Context Extension for LLMs", "content": "ROPE (Su et al., 2024) is commonly used in LLMs, but its extrapolation performance beyond the training length is limited. Position Interpolation (Chen et al., 2023b) achieves efficient extension within the pretraining range but still needs fine-tuning on the target length. NTK-Aware (u/LocalLLaMA, 2023) and DynamicNTK (r/LocalLLaMA, 2024) mitigate the loss of high-frequency information caused by interpolation, enabling extrapolation directly but causing an overall perplexity increase. YaRN (Peng et al., 2023) combines these methods and achieves 2x extrapolation. ResonanceRoPE (Wang et al., 2024a) can integrate with others further enhancing performance. CLEX (Chen et al., 2023a) is a more efficient dynamic scaling method, LongROPE (Ding et al., 2024) leverages ROPE's nonuniformity to search for better interpolations, both achieving 4x or 8x extrapolation, but their iteration process is complex. Above methods are mostly based on Llama (Touvron et al., 2023a) or Llama2 (Touvron et al., 2023b) experiments, with good performance. The Adjusted Base Frequency (ABF) (Xiong et al., 2023) has been widely used in advanced LLMs (Yang et al., 2024; Team, 2024; Young et al., 2024; Cai et al., 2024), significantly improving extrapolation potential. (Liu et al., 2023) further proposes scaling laws for RoPE base. There has been no systematic research on the performance of applying other interpolation methods on this.\nEfficient attention is another direction of focus. Such as StreamingLLM (Xiao et al., 2023) supports infinite inputs but may be less suitable for summarization tasks due to lack of long-term memory. methods like LM-Infinite (Han et al., 2024; Xiao et al., 2024a; Jiang et al., 2024b) for infinite-length have same limitation of being unable to access full context. LongLoRA (Chen et al., 2023c) and (Zhang et al., 2024a; Han et al., 2023; Xiao et al., 2024b) can be used in fine-tuning phase. However, the impact on the model's extrapolation ability needs further investigation. (Liu et al., 2024b; Liu and Abbeel, 2023; Ding et al., 2023; Ao et al., 2024) designed for large-scale hardware training."}, {"title": "2.2 Long-Context Summarization Evaluation", "content": "ROUGE (Lin, 2004)remains the most popular for summarization tasks in benchmarks due to Implementation being simple and low cost. It measures the information overlap between outputs and the reference summaries. (Chang et al., 2023)proposes a protocol for evaluating coherence of book-length summarization generated by LLMs and implements an LLM-based automatic evaluation method. This work does not focus on faithfulness, thus it does not rely on gold-standard summary but advanced LLMs like GPT-40 (OpenAI, 2024b). (Kim et al., 2024) are the first to conduct a large-scale evaluation of faithfulness and content selection, demonstrating that the faithfulness of LLMs summarization still needs improvement. However, this study mainly focuses on commercial models, while many open-source models still struggle with generating normal outputs for long-context summarization. Both studies have highly costly human annotations. (Krishna et al., 2023)proposes guidelines for human evaluation of faithfulness of long-form Summarization but does not extend this to automatic evaluation. (Song et al., 2024)introduces a fine-grained summarization evaluator through prompts construction, capable of assessing faithfulness, completeness and so on. It also relies on advanced LLMs for reliable evaluation. All these studies are based on English."}, {"title": "2.3 Long-Context LLMs for Chinese", "content": "Although the Chinese language domain possesses one of the largest and richest corpora in the world, earlier open-source LLMs often provided poor support for Chinese. A specific issue is that their tokenizers exhibit low efficiency when converting Chinese text into tokens, meaning more tokens are required to represent Chinese text. Since the context length in LLMs is defined at the token level, this leads to a limitation in the actual length of Chinese text that can be processed. Moreover, as the Attention mechanism has a computational complexity of $n^2$, more token means significantly increased training and inference costs for LLMs when processing Chinese text.\nWe performed a rough calculation of the Chinese encoding efficiency of several open-source LLMs based on the Chinese book corpus we collected, which is show in Table 1. Such as Llama (Touvron et al., 2023a) and Llama2 (Touvron et al., 2023b) have extremely low Chinese encoding efficiency, with the effective number of Chinese characters they can process being limited to around 2k. The derived Mistral (Jiang et al., 2023, 2024a) and LWM-Text (Liu et al., 2024a) support context lengths of up to 128k and 1M, respectively, but still exhibit low Chinese encoding efficiency. Recently released Llama3.1 (Dubey et al., 2024) and Ministral (AI, 2023) support a 128k context length, benefiting from a substantial increase in vocabulary size to over one hundred thousand, resulting in significantly improved Chinese encoding efficiency, though still not ideal. Bilingual LLMs Yi (Young et al., 2024) and ChatGLM3 (Du et al., 2021; Zeng et al., 2022), as well as InternLM (Cai et al., 2024), all developed by Chinese teams, achieve higher Chinese encoding efficiency with vocabularies of fewer than one hundred thousand, supporting context lengths ranging from 128k to 1M. Baichuan2 (Yang et al., 2023) has the highest Chinese encoding efficiency, with a vocabulary size exceeding one hundred thousand, but it mainly supports Chinese and English. Qwen (Bai et al."}, {"title": "3 CNNSum Construction", "content": ""}, {"title": "3.1 Chinese Book Corpus Collection", "content": "We collected approximately 100 Chinese books from open-source data on the Chinese internet, each featuring a clear chapter structure. Initially, we excluded books composed of multiple short stories that lacked a fixed or explicit main storyline, as their context did not exhibit long-distance dependencies or causal relationships, making it difficult to create a cohesive summary. We then randomly selected multiple chapters from the remaining books and utilized the Qwen2-7B-Instruct to generate summaries for these chapters. If the model identified the book it belonged to or included additional, unrelated book information, we eliminated these \"popular books\" to ensure the fairness and validity of our benchmark construction. This was necessary because these books are likely to have been extensively leaked in the training data of other advanced LLMs. Ultimately, we filtered out about 20 books.\nOur collection includes many web-serialized novels, often exhibiting the author's unique writing habits and formatting styles. Common cases include non-standard punctuation usage, randomly interspersed interactive content with the reader, or additional annotations. These distinctive formats and contents may increase the understanding difficulty for LLMs or disrupt the coherence of the context as illustrated in the case presented in Appendix D.1. We formatted these web-serialized novels via designing regular expressions, supplemented by manual checks, to obtain cleaner corpus."}, {"title": "3.2 Multi-Scale Sampling", "content": "We use Yi (Young et al., 2024) tokenizer to process all chapters in the book corpus and set a tokenlevel target length $T$ for each subset. We allow the data length to fluctuate within a specified range around $T$. Each target length $T$ has a distinct lower boundary to avoid scoring bias from overly short data, while the upper boundary is uniformly set as $T + 2k$, with $k = 1024$, allowing limited extrapolation without causing score degradation. The specific settings for each of the four subsets are as follows:\n$T_L = 16k,  Ranger_L = [16k - 4k, 16k + 2k]$\n$T_{XL} = 32k,  Range_{XL} = [32k - 6k, 32k + 2k]$\n$T_{2XL} = 64k,  Range_{2XL} = [64k-10k, 64k+2k]$\n$T_{3XL} = 128k, Range_{3XL} = [128k \u2013 16k, 128k + 2k]$\nEach book in the corpus has a chapter structure. A book $B$ composed of $i$ chapters $c$ can be represented as $B = {c_0, c_1, ..., c_i}$. For a target interval length $R$ and a book $B$, the sampling method is as follows: (1) initialize a variable sliding window $w$ starting from chapter $c_0$, expanding chapter by chapter. (2) when $w = [c_j, c_{j+1},..., c_{k\u22121}, c_k]$ ($j < k < i$):\n\u2022 If the length of $w < infR$, continue expanding with chapter $c_{k+1}$.\n\u2022 If the length of $w \u2208 R$, it is considered a sample and is recorded. Begin a new sampling from chapter $c_{k+1}$.\n\u2022 If the length of $w > supR$, attempt to remove chapters sequentially from $c_j$ onward until $w$ falls within or below $R$.\n(3) follow the above procedure until there are no remaining chapters in $B$, then proceed to the next book. This sampling method is applied for each interval length $R$ across our book corpus, generating a candidate set for each range.\nWe perform a final screening based on the candidate set. Although the volume of the candidate set"}, {"title": "3.3 Summary Annotation", "content": "(Chang et al., 2023)summarize the methods for generating long-text summaries using LLMs, named Hierarchical merging (Wu et al., 2021) and Incremental updating while claiming their limitations. These methods trust LLMs' ability of short-context summarization (e.g., 4k or 8k). Based on these methods, our approach involves more human guidance. For one sampled data, we first generate a \"plot synopsis\" for each chapter via commercial LLMs with more of the original plot and character information. The prompt in Appendix E.3. Annotators then read each synopsis select and retain key plots based on their judgment, and merge them into a final summary. If find conflicting content, annotators will ask LLMs to locate related plots in the original chapter and correct them manually. We require annotators to not only delete LLMs' output and merge the rest but also rewrite in their own words. We suggest annotators avoid adding subjective commentary, focusing instead on objective content. Although this may sacrifice some coherence, it increases the density of effective information. For L and XL, we limit the maximum to 500 words and 600 for 2XL and 3XL. On one hand, adding more words for coherence is not cost-effective; on the other hand, as observed in Section 4, most models do not generate overly lengthy summaries. We use crowdsourcing for annotation to improve efficiency and reduce individual bias."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Baselines", "content": "Commercial Models (1) OpenAI's flagship model GPT-40, the GPT-40-2024-08-06 (OpenAI, 2024b) and the lite version GPT-40-mini-2024-0718 (OpenAI, 2024a), both have 128k context length. (2) The Moonshot-v1-128k\u00b9. (3) The Qwen-plus2024-09-19\u00b2 with a 128k context length. (4) The Doubao-pro-128k\u00b3. (5) DeepMind's Gemini-1.5pro (Team et al., 2024a), with maximum context length up to 2 million.\nOpen-source Models (1) Yi (Young et al., 2024) extends context via ABF (Xiong et al., 2023). Yi6B and Yi-6B-Chat default to 4k context, consistent with the training phase, theoretically support extrapolation. Yi-6B-200K and Yi-34B-200K using larger RoPE Base to extend context to 200k. Yi-1.5-34B-32K and Yi-1.5-34B-Chat-16K are the largest long-context models in the Yi-1.5 series. (2) InternLM2.5 (Cai et al., 2024) also extends the context by adjusting the RoPE Base (Liu et al., 2023) and defaults to using DynamicNTK (r/LocalLLAMA, 2024). For context length, InternLM2.520B defaults to 256k, InternLM2.5-20B-Chat to 32k, and the special version InternLM2.5-7BChat-1M supports 1 million. (3) ChatGLM36B-128K is 128k version of ChatGLM3-6B (Du et al., 2021). GLM4-9B-Chat-1M is special version of GLM4 (GLM et al., 2024) using LongAlign (Bai et al., 2024) and ABF. (4) Llama3.1 series"}, {"title": "4.2 Experimental Setup", "content": "Based on the statistics of annotation summary lengths, we set maximum generation to 400 tokens for L and XL and 500 tokens for 2~3XL. For automatic evaluation, we first process the Chinese outputs using jieba\u2074 for word segmentation. Then, we calculate the ROUGE-L (Lin, 2004) to measure the information overlap between the outputs and the reference summaries. All experiments were conducted on NVIDIA A100 (80GB) GPUs.\nBaseline Evaluation The prompt templates we used are shown in Appendix E.1. For commercial model APIs, we set temperature = 0 to reduce randomness in generation. GPT-40 is limited by Chinese encoding efficiency and can only complete up to 2XL. Gemini and Qwen have strict content safety checks, which block 29% and 2% of samples, respectively. Doubao excluded 8% of samples on 3XL due to excessive length. These cases all caused score bias. For open-source models, we apply greedy sampling for deterministic results. We use vLLM (Kwon et al., 2023) for inference and modify the source code related to positional embedding for correct extrapolation. For models that default to using RoPE scaling methods, we keep their original settings. Qwen series Instruct models can optionally enable YaRN for long context. For consistency and extrapolation evaluation, we keep default RoPE settings.\nFine-tuning Datasets Our training dataset has about 9,000 short novel excerpt summaries, which are non-overlapping with CNNSum's corpus. Using Yi tokenizer, their lengths range from 2k~4k tokens. The prompt templates are shown in Appendix E.2. We randomly concatenated the samples, similar to Section 3.2, setting the lengths to 14k~18k and 30k~34k, with average lengths of about 16k and 32k. The main motivation is that longer sequences enable LLMs to adapt to more positions and activate extrapolation ability. Besides, a recent study proposed a similar data augmentation strategy (Tian et al., 2024). It proved this strategy trains LLMs to focus more on relevant contexts, enhancing long-context ability.\nFine-tuning Experiments We follow findings from (Chen et al., 2023c), use a LORA (rank=8) (Hu et al., 2021), and unfreeze embedding and normalization layers. We used Flash Attention 2 (Dao, 2024), DeepSpeed ZeRO2 / ZeRO3 with CPU offloading (Ren et al., 2021), and Adam (Loshchilov, 2017) with a learning rate 2e-5, including a linear warmup of 30 steps. Inference also used vLLM. For fine-tuning concatenated data with an average length of 16k, we set global batch size to 16. Due to the varying convergence rates of RoPE-based scaling methods with different scale s (Peng et al., 2023), we evaluated multiple checkpoints between 400 and 500 steps and selected the best result. For fine-tuning on concatenated data with an average length of 32k, we set the global batch size to 8 and started from a checkpoint that had been fine-tuned for 300 steps on data with an average length of 16k. We continued fine-tuning for another 200 to 300 steps and selected the best result. Each experiment was repeated at least three times to minimize randomness as much as possible."}, {"title": "4.3 Main Results on CNNSum", "content": "The detailed results are shown in Appendix Table 7. We calculated the average scores and used the mean squared error (MSE) to measure the performance differences caused by prompts, as shown in Table 3. Overall, commercial models significantly outperform most open-source models. The lower MSE indicates minimal performance differences between two prompts, demonstrating more stable long-context summarization ability. Moonshot-v1 performed the best, with the advantage more evident from L to 2XL. However, its performance decreased by 32.1% at 3XL. Gemini-pro, Qwenplus, and Doubao-pro had similar performance. Doubao-pro experienced the most significant decrease in performance from L to 3XL, at 32.7%, while Gemini-pro showed the smallest decrease, at 24.4%. Unexpectedly, GPT-40 performed the worst, lagging behind other models by 20% to 30% on average from L to 2XL.\nSeveral open-source models demonstrated strong competitiveness. GLM4-9B-Chat-1M performed the best overall, comparable to or better than commercial models across all subsets, with only an 18.9% performance decrease from L to 3XL. InternLM2.5-20B-Chat showed robust extrapolation capability, with a performance decrease of 24.8%. Among the Qwen series, Qwen2.5-72BInstruct performed best, but performance decreased by 31.6% when extrapolated to 3XL. Other models performed similarly on L and XL, with Qwen2 series decreasing 20% to 40% on 3XL. Qwen1.5 had only a few meaningful outputs on 2XL. Due to lower Chinese encoding efficiency, Llama3.1 and Ministral required extrapolation at 3XL, with Llama3.1 showing an average performance decrease exceeding 40% and Ministral reaching as high as 78.3%. Yi-200K series demonstrated weak generation capability. Yi-34B-200K generally output very short summaries, just one or two sentences, or meaningless repetitions. Yi-6B-200K performed even worse. LWM-Text-1M's insufficient Chinese capability led to mostly meaningless outputs."}, {"title": "4.4 Analysis and Case Study", "content": "Why Did GPT-40 Fail? One obvious reason is that, according to Table 1 and Table 2, GPT-40's Chinese encoding efficiency is relatively lower, it needs processing more tokens leading to a decrease in performance. Despite this, GPT-40's performance on L still trails other commercial models on XL by an average of 16%. We compared the outputs and find that GPT-40 and Qwen-plus generated brief summaries, while Moonshot-v1 and Gemini-pro generated longer ones. Moonshot-v1's outputs are significantly longer than all other models, with many samples being able to continue generating until reaching the maximum limit without repetition. Longer outputs contain richer information, more likely to overlap with reference summaries, leading to higher ROUGE scores. For GPT-40, we find that its already shorter outputs often include \"commentary\" on the novel's plot, examples are shown in Appendix D.2. These contents are extensions of the plot rather than the plot itself, often not overlapping with the reference summary. This reduces the valuable information in the output and led to lower ROUGE scores. Other models rarely exhibit this and focus on generating content related to the plot, regardless of the output length.\nPrompt-IB vs Prompt-IE Several open-source models exhibited large MSE values between the scores of two prompts (Marked in red). Detailed results of these models are shown in Table 4. Yi-1.5-34B and Qwen1.5-7B have relatively short context lengths, yet both performed much better with Prompt-IE. We further checked their outputs, due to the \"summary instruction\" in Prompt-IB being placed before the novel excerpt, they often failed to follow instruction after processing such a long context. Most outputs copied the excerpt or exhibited meaningless repetition. Using Prompt-IE within the default context length can generate many high-quality, clean summaries. see Appendix D.3. This advantage persisted in extrapolation on 2XL but was completely ineffective on 3XL. Yi-1.5-34B-Chat-16K is a further fine-tuned version that outperformed when using Prompt-IB, possibly because the fine-tuning data contained similar summarization tasks. However, fine-tuning somewhat damaged the Base model's continuation ability when using Prompt-IE, although the ROUGE also improved. We find the summaries generated by Yi-1.5-34B-32K tend to be more concise and clean. The chat version would generate more content, but sometimes more disorganized, prone to repetition, and unable to stop properly. This observation also occurred in models with lower MSE, such as InternLM2.5-20B and its Chat version. Examples are shown in Appendix D.4.\nQwen2-72B have higher overall output quality. Unlike Yi-1.5 and Qwen1.5, when using PromptIE, they were more likely to copy content from the excerpt rather than generate a summary. Notably, when using Prompt-IB and extrapolating on 3XL, Qwen2-72B-Instruct had gradually forgotten instruction, randomly outputting other content related to the excerpt, leading to the ROUGE score being overtaken by Prompt-IE. Appendix D.5. Llama3.1 Base models have relatively poor overall output quality. The gap stems from the poor continuation ability with Prompt-IE, with some outputs just repeating the last sentence of the prompt, usually the \"summary instruction\" Appendix D.6. As the context length increases, such meaningless repetitions occur more frequently. With Prompt-IB, it did not happen at all. However, since the model isn't instruction-tuned, it generates content related to the excerpt but is usually disorganized. Besides, the repetition is partly caused by greedy sampling strategy. (Meister et al., 2023; Xu et al., 2022)\nRise of Smaller Long-Context LLMs In opensource models, except GLM4-9B-Chat-1M leads in performance, ChatGLM3-6B-128K has the smallest parameter size but outperformed larger LLMs on 2~3XL. For the InternLM2.5, 7B-Chat-1M achieved 94% of 20B-Chat's average performance, slightly outperforming 20B Base version. Similar observations were in Llama3.1 and Qwen, smaller Instruct or Chat models can match the performance of larger Base models. We summarized the main points: (1) A stable and sufficiently long context length is most important for long-context summarization. (2) Models with instruction fine-tuning may better recognize and remember instruction after reading a long-context novel.\nDespite larger LLMs being better at reasoning,"}, {"title": "4.5 Ablation Study for Fine-Tuning", "content": "We have analyzed factors affecting long-context summarization based on general LLMs. However, how to configure these factors for fine-tuning LLMs specifically for summarization requires further exploration. Using reconstructed prompts E.2, we fine-tuned Yi-6B and Qwen1.5-7B on concatenated datasets with an average length of 16k. Results are shown in Table 5.\nExtrapolation Potential Compared to the results in Table 3, the performance of Yi-6B on L and XL significantly improved by 2 to 3 times, while Qwen1.5 also improved by approximately 50%. We summarized two main points: (1) During pre-training, the RoPE base was scaled up via ABF (Xiong et al., 2023; Liu et al., 2023), endowing the model with greater potential for longcontext. Fine-tuning on long-text data further activated this ability. (2) The model's ability to follow instructions for summarization tasks was enhanced, reducing the meaningless outputs. For Yi-6B, with its short default context length, both points contributed to the improvement. In contrast, Qwen1.5, having limited long-context ability, benefited more from point (2). With the largest RoPE base of open-source baselines 4.1, Yi-6B shows more extrapolation potential than Qwen1.5 on 2~3XL.\nChat vs Base Within each model series, the best results for each subset were almost always achieved by base models. Fig 1 illustrates the performance on CNNSum, indicating that the base models demonstrated better extrapolation potential and were better suited for fine-tuning summarization LLMs.\nPrompt-IB vs Prompt-IE In Table 4, Qwen1.57B shows the most significant gaps when using different prompts. After fine-tuning, these differences nearly disappeared. Evaluations on CLongEvalLStSum further confirmed this. Fig 1 intuitively shows that a model fine-tuned with a specific prompt has consistent long-text summarization performance, regardless of the prompt template. we also carefully checked and compared the model outputs but did not observe the quality difference mentioned in 4.4."}, {"title": "4.6 Reliable Evaluation of Extrapolation", "content": "Models' scores on CLongEval Large (50k~100k) and 2XL (64k) and higher than on 3XL (128k) in Table 5. We find the output quality has already declined obviously on the 2XL. Many samples generated some valid content but then degenerated into meaningless repetition and was worse on 3XL. In CLongEval Large, shorter samples have much better output, shown in Fig 2, leading to misleading final scores. To further explore long-context summarization extrapolation and the importance of reliable evaluation, we fine-tuned Yi-"}]}