{"title": "Population Aware Diffusion for Time Series Generation", "authors": ["Yang Li", "Han Meng", "Zhenyu Bi", "Ingolv T. Urnes", "Haipeng Chen", "William & Mary", "Virginia Tech", "Generated Health"], "abstract": "Diffusion models have shown promising ability in generat-ing high-quality time series (TS) data. Despite the initial success, existing works mostly focus on the authenticity of data at the individual level, but pay less attention to pre-serving the population-level properties on the entire dataset. Such population-level properties include value distributions for each dimension and distributions of certain functional de-pendencies (e.g., cross-correlation, CC) between different di-mensions. For instance, when generating house energy con-sumption TS data, the value distributions of the outside tem-perature and the kitchen temperature should be preserved, as well as the distribution of CC between them. Preserving such TS population-level properties is critical in maintaining the statistical insights of the datasets, mitigating model bias, and augmenting downstream tasks like TS prediction. Yet, it is of-ten overlooked by existing models. Hence, data generated by existing models often bear distribution shifts from the orig-inal data. We propose Population-aware Diffusion for Time Series (PaD-TS), a new TS generation model that better pre-serves the population-level properties. The key novelties of PaD-TS include 1) a new training method explicitly incor-porating TS population-level property preservation, and 2) a new dual-channel encoder model architecture that better cap-tures the TS data structure. Empirical results in major bench-mark datasets show that PaD-TS can improve the average CC distribution shift score between real and synthetic data by 5.9x while maintaining a performance comparable to state-of-the-art models on individual-level authenticity.", "sections": [{"title": "1 Introduction", "content": "Time series data exists in a broad spectrum of real-world domains, spanning healthcare (Kaushik et al. 2020; Morid, Sheng, and Dunbar 2023), energy (Priesmann et al. 2021; Deb et al. 2017), finance (Zeng et al. 2023; Masini, Medeiros, and Mendes 2023), and many more. TS models have been used in these domains for effective data analy-sis and prediction tasks. Developing such models requires rich and high-quality TS datasets, which unfortunately may not exist in many data-scarce domains like healthcare and energy. Various data augmentation techniques (e.g., jitter-ing, scaling, permutation, time warping, and window slic-ing) have been developed to enhance the original datasets with synthetically generated TS data. Synthetic data can po-tentially create observations that do not exist but are close to the original dataset (Coletta et al. 2023; Esteban, Hyland, and R\u00e4tsch 2017). With a newly augmented dataset, we can further enhance TS models for data analysis, while protect-ing the privacy and confidentiality of the original data and potentially enhancing data sharing.\nHow do we measure the quality of synthetic TS data? First, it should be authentic on the individual level. Given two samples one from the original set and another from the generated set we should not be able to identify which is fake or real. Second, it should preserve the TS popula-tion-level properties of the original data. Such population-level properties include distributions for each dimension of the data and distributions of certain functional dependencies (e.g., CC) between different dimensions of the data. Taking house energy consumption TS as an example (Candanedo, Feldheim, and Deramaix 2017), the value distribution of the outside temperature and the kitchen temperature, as well as the CC distribution between them should be preserved. As shown in Figure 1, previous methods trust the model to estimate the CC distribution between them which fails to preserve the same distribution in generated TS. Preserving both the individual- and population-level properties is cru-cial in maintaining the standalone and statistical insights of the original data, hence reducing model bias and further aug-menting downstream tasks such as prediction.\nWe revisit the TS generation problem with an emphasis on preserving the TS population-level properties. There have been extensive studies on TS generation using generative adversarial networks (GANs) (Yoon, Jarrett, and Van der Schaar 2019; Liao et al. 2020; Mogren 2016; Esteban, Hy-land, and R\u00e4tsch 2017; Pei et al. 2021) and variational au-toencoders (VAEs) (Desai et al. 2021; Naiman et al. 2023). Though they have achieved reasonable results, it is known that GANs suffer from unstable training because of the need to interactively and iteratively train both the generator and discriminator, while VAEs normally generate lower-quality samples due to optimizing an approximate objective via the evidence lower bound (ELBO). Moreover, both GANs and VAEs may struggle with the mode collapse issue. Diffusion models (DMs) emerge as another class of powerful genera-tive models that are robust against mode collapse, and show state-of-the-art performances in domains such as image gen-eration (Ho, Jain, and Abbeel 2020; Nichol and Dhariwal 2021; Peebles and Xie 2023; Sohl-Dickstein et al. 2015), text-to-image generation (Ramesh et al. 2022; Nichol et al. 2021), and text-to-video generation (Brooks et al. 2024). In light of this, recent studies have developed DM-based TS generation models (Coletta et al. 2023; Yuan and Qiao 2024) that yield better results than GANs and VAEs for TS gener-ation. Despite the initial success, most existing works pay less attention to the preservation of population-level proper-ties and hence may suffer from the generation distribution shift.\nWe hypothesize that the distribution shift of existing TS generative models comes from two sources: 1) TS population-level property preservation is not explicitly in-corporated into the training process, as they only try to cap-ture TS population-level properties by minimizing the value distance between synthetic and original samples. InfoVAE (Zhao, Song, and Ermon 2017) tackles a similar issue in image generation by penalizing the distribution shift in the single encoded latent space with a regularization term in its training loss. However, this cannot be directly applied to DMs, as DMs follow an iterative generation framework usually with an extremely long series of latent spaces. 2) Model architectures of existing works cannot fully capture the multivariate TS data information. Cross-dimensional in-formation has been shown to be critical (Liu et al. 2024) for TS prediction, as it can yield great performance using only dimension information. Previous methods either ne-glect cross-dimension features (Yuan and Qiao 2024; Yoon, Jarrett, and Van der Schaar 2019; Desai et al. 2021) or try to capture them in a shared block with temporal feature extrac-tions (Coletta et al. 2023; Tashiro et al. 2021), which may not be sufficient to capture all cross-dimension features.\nWe propose PaD-TS, a new DM that addresses the above issues. PaD-TS comes with a new DM training objective that penalizes population-level distribution shifts. This is en-abled by a new sampling strategy (during training) that en-forces the comparison of two distributions to the same dif-fusion step in a mini-batch. In addition, we design a new transformer (Peebles and Xie 2023; Vaswani et al. 2017) encoder-based dual-channel architecture that can better cap-ture the population-level properties.\nOur main contributions are as follows. 1) We are the first to study DM-based TS generation that explicitly considers TS population-level property preservation, along with new metrics to evaluate it. 2) We propose PaD-TS, a novel DM that addresses the technical challenges of this problem. 3) We conduct extensive empirical evaluations of our model. The empirical results show that PaD-TS achieves state-of-the-art performance in population-level property preserva-tion and comparable individual-level authenticity."}, {"title": "2 Related Work", "content": "GANs (Goodfellow et al. 2014) are generative models that usually consist of a generator and a discriminator. The gen-erator generates plausible data to fool the discriminator, whereas the discriminator tries to distinguish the synthetic data from real data. Due to their ability to generate high-quality synthetic data (Mogren 2016; Yoon, Jarrett, and Van der Schaar 2019; Liao et al. 2020), GANs have been extensively studied for TS generation (Mogren 2016; Yoon, Jarrett, and Van der Schaar 2019; Liao et al. 2020) as well as functional dependency persevation tables (Chen et al. 2019). However, GANs suffer from inherent instability, resulting from the interactive and iterative training process on both the generator and the discriminator. This often leads to non-converging models that oscillate or vanishing gradient is-sues that prevent the generator from learning meaningful patterns.\nVAEs (Kingma and Welling 2022) are another family of generative models based on the encoder-decoder archi-tecture. The encoder in VAEs encodes the data in a latent space following a Gaussian distribution. The decoder sam-ples from learned latent space as prior information to gener-ate synthetic data. TimeVAE (Desai et al. 2021) utilizes con-volution structures to capture temporal correlations. KOVAE (Naiman et al. 2023) uses a linear Koopman-based prior and a sequential posterior to further improve the performance. However, VAEs are known for their low generation quality and susceptibility to mode collapse, which limits their suc-cess in TS generation.\nDMs (Ho, Jain, and Abbeel 2020; Sohl-Dickstein et al. 2015) emerge as a new generative framework that learns to generate data by gradually reversing a noising process ap-plied to the training data. Being theoretically grounded with connections to score-based generative modeling (Song et al. 2020), and having robustness against mode collapse com-pared to other generative models like GANs and VAEs, they"}, {"title": "3 Problem Statement", "content": "Given a multivariate TS dataset $D_{orig} = {X_n}_{n=1}^N$ with N samples. Each data sample $x_{1:L;1:F} \\in R^{L\\times F}$ is a multivariate TS, where L is the sequence length and F is the number of features/dimensions (e.g., we can denote ${x_{l;i}}$ for all $l\\in [1, L]$ as values in the l-th dimension). Our task is to generate synthetic dataset $D_{syn} = {\\hat{x}_n}$ such that the synthetic data is similar to the original data $D_{orig}$ in individual level and follows original population-level property distributions. To evaluate the generation quality at the individual level, we have the following metric:\n(1) Discriminative Accuracy (DA) (Yoon, Jarrett, and Van der Schaar 2019) is based on the post-hoc machine learning classifier (clf) trained with the training set from original and synthetic datasets (with label real=1; synthetic = 0). Then DA is the model performance on the test set with size S using the following equation:\n$DA = \\frac{1}{2S} | \\sum_{n=1}^{S} (0 = clf(\\hat{x}_n)) + \\sum_{n=1}^{S} (1 = clf(x_n))  - 0.5|$  (1)\nwhere $\\hat{x}_n$ and $x_n$ are test samples. To evaluate the generation quality in terms of TS population-level property preservation, we propose two new metrics:\n(2) Value distribution shift (VDS):\n$VDS = \\frac{1}{F} \\sum_{i=1}^{F} D(P_i, Q_i)$ (2)\nwhere D stands for a certain distribution distance measure (e.g., KL divergence); $P_i$ is the value distribution of i-th dimension over the original data ; and $Q_i$ is the counterpart distribution for the synthetic dataset.\n(3) Functional dependency distribution shift (FDDS):\n$FDDS = \\frac{1}{M} \\sum_{m=1}^{M} D(P_{FD}^m, Q_{FD}^m)$  (3)"}, {"title": "4 Approach", "content": "In this section, we introduce PaD-TS which addresses population-level preservation problems. Building on top of diffusion models, PaD-TS consists of two novel compo-nents: a new population-aware training process, and a new dual-channel encoder model architecture.\nPreliminary: Diffusion Models\nWe briefly review the formulations of the denoising diffu-sion probabilistic models (DDPMs) (Ho, Jain, and Abbeel 2020; Nichol and Dhariwal 2021) which have iterative for-ward and reverse processes.\nGiven original data $x^0 ~ q(x)$, the forward process q is a Markov process which adds noise $\\epsilon_t$ iteratively based on a fixed variance scheduler $\\beta_t$ and sampled diffusion step t. Normally, t is uniformly sampled from [1, T \u2013 1] for each sample data.\n$q(x_{1:T}|x^0) = \\prod_{t=1}^T q(x_t|x_{t-1})$ (4)\n$q(x_t|x_{t-1}) = N(\\sqrt{1-\\beta_t}x_{t-1}, \\beta_tI)$\nThe reverse process ($p_{\\theta}$) gradually removes noises from $p(x_T) ~ N(0, I)$ and tries to recover $x^0$ with $x^0(\\theta)$.\n$p_{\\theta}(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_{\\theta}(x_{t-1}|x_t)$ (5)\n$p_{\\theta}(x_{t-1}|x_t) = N(\\mu_{\\theta}, \\Sigma_{\\theta})$\nwhere mean $\\mu_{\\theta}$ and variance $\\Sigma_{\\theta}$ are learnable parameters. To reduce complexity, DDPMs set $\\Sigma_{\\theta} = \\sigma_tI$ where $\\sigma_{\\theta} = \\beta_t$ follows the same variance scheduler as the forward process. With fixed $\\Sigma_{\\theta}$, one can effectively approximate the reverse process by training a model that predicts $\\mu_t$ with:\n$\\hat{\\mu}_t(x_t, x^0) = \\frac{1}{\\sqrt{\\alpha_t}} ( \\frac{\\sqrt{\\alpha_t} (1-\\alpha_{t-1})}{\\alpha_t}\\epsilon_t + \\frac{\\sqrt{\\alpha_{t-1}}(1-\\alpha_t)}{1-\\bar{\\alpha}_t} x_t)$ (6)\nwhere $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$ are both constants. Note that the only unknown in $\\hat{\\mu}_t(x_t, x^0)$ is $x^0$. A neural network can directly model towards $x^0$ or $\\epsilon_t$ when applying reparametrization trick $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t$. With target $x^0$, DDPMs have the following training objective:\n$L_0(\\theta) = E_{t,x_0} [|| x^0 - x^0(\\theta) ||^2]$ (7)"}, {"title": "Algorithm 1: PaD-TS training procedure", "content": "As mentioned above, existing DMs show promising per-formance in individual-level authenticity but exhibit sub-optimal performance in preserving the TS distribution of population-level properties. One hypothesis is that the origi-nal DDPM training process focuses on the value distance be-tween model input and output, and overlooks TS population-level properties preservation. A naive resolution to this is to penalize distribution shifts by regularizing the loss func-tion. However, this turns out to be not directly feasible be-cause of the iterative DM generation process. To address this technical challenge, we propose a new DM training pro-cess that enables applying any regularization of interest for DMs which consists of two components: population aware training (PAT) objectives and same diffusion step samplings (SSS).\nAlgorithm 1 shows our new DM training procedure for PaD-TS: (1) We first sample a mini-batch of data from the original TS dataset in Line 2. (2) In Lines 3-4, we use the SSS for the mini-batch diffusion step t. (3) In Lines 5-8, we use the PAT objective to update the model parameter $\\theta$. The details of the training procedure are as follows.\nPopulation Aware Training Objective This objective considers preserving the TS population-level property dis-tribution rather than simple statistical measures (e.g., mean, variance, etc). Thus it is crucial to use the right distribu-tion distance. The distribution of property at the population level may come in arbitrary parametric forms, thus distri-bution shift measures such as Kullback-Leibler divergence (Zhao, Song, and Ermon 2017; Liu and Wang 2016) and the Wasserstein distance (Arjovsky, Chintala, and Bottou 2017) are inappropriate, as in practice they usually either have assumptions toward the underlying distributions and/or is computationally expensive, especially with high dimen-sional data. Inspired by InfoVAE (Zhao, Song, and Ermon 2017), we use the Maximum Mean Discrepancy (MMD) (Gretton et al. 2012) as the distribution distance. It is com-monly used in deep learning tasks such as image generation (Zhao, Song, and Ermon 2017; Li et al. 2017) and domain adaptation (Yan et al. 2017; Cao, Long, and Wang 2018).\nWith a pair of arbitrary distributions (P and Q), MMD compares all their moments using the selected kernels. Us-ing a Radial Basis Function (RBF) kernel on multiple win-dow sizes as an example, the MMD distance can be effi-ciently estimated as follows:\n$MMD_w(Q||P) = \\sum_{w_i \\in W} E_{Q,Q} [RBF_{w_i}(Q, Q)] + \\sum_{w_i \\in W} E_{Q,P} [RBF_{w_i}(Q, P)] + \\sum_{w_i \\in W} E_{P,P} [RBF_{w_i}(P, P)]$ (8)\nwhere $RBF_{w_i}$ stands for a RBF kernel with window $w_i$.\nWe use cross-correlation (CC, see definition in Appendix A) as an example of TS functional dependency, as CC is often a critical property in TS data. For each TS data sample $x_{1:L;1:F}$, we can calculate $M = \\frac{F(F-1)}{2}$ unique CC values. We use $P_{FD}^{ij}$ to represent the distribution of CC between i-th and j-th dimension in the original data, and use $Q_{FD}^{ij}$ to represent its counterpart for the synthetic data. By considering all possible pairs of CC distributions, the regularization loss term can be defined as:\n$L_{pop} = \\frac{1}{M} \\sum_{m=1}^{M} MMD_W (P_{FD}^{m}, Q_{FD}^{m})$ (9)\nHence, the PAT objective can be formally defined as follows:\n$L_{total} = L_0 + \\alpha * L_{pop}$ (10)\nwhere $\\alpha$ is a hyperparameter that controls the weight of the population aware loss.\nSame Diffusion Step Sampling How do we empirically compute a meaningful $L_{pop}$ in DMs? As mentioned, the DM framework is an iterative generation process (i.e., gradually removing noise with a variance scheduler $\\beta_t$) that be-haves differently at each diffusion step t. A common uni-form diffusion step sampling strategy (Yuan and Qiao 2024; Ho, Jain, and Abbeel 2020; Coletta et al. 2023; Peebles and Xie 2023) and importance sampling (Nichol and Dhariwal 2021) will produce mixed diffusion step sampling within a mini-batch. If the resulting diffusion steps of one of the two strategies are applied, DM will yield generations from mixed diffusion steps. Although it works for value distribu-tion preservation, it will be problematic to compare func-tional dependency distributions because of different behav-iors at different diffusion steps. To ensure a reasonable func-tional dependency distribution comparison in DM, we intro-duce the SSS strategy.\nGiven a mini-batch training procedure with b samples. We have diffusion step vector $t = [t_1, t_2,..., t_b]$, where each $t_i \\in [0,T-1]$. SSS first samples $t_1 \\in [1,T-1]$ and duplicates the diffusion step $t_1$ to fill the vector t. Thus we have the SSS-based diffusion step vector $t = [t_1, t_1, ..., t_1]$. For a mini-batch sample, SSS ensures the distribution com-parison is on the same diffusion step. Compared to uniform sampling, SSS has one obvious limitation: less coverage of diffusion steps. By increasing the number of training epochs, each diffusion step in [0, T \u2013 1] will eventually be sampled."}, {"title": "Model Architecture", "content": "Our model in Figure 2 is based on transformer encoders including vanilla transformer (Vaswani et al. 2017) encoders and diffusion transformer (DiT) blocks (Peebles and Xie 2023). To fully capture temporal and cross-dimensional in-formation, we propose a dual-channel architecture where each part of the information is processed separately. Each channel (temporal and cross-dimensional) passes a dense layer to encode channel representation, a vanilla transformer encoder, a few residual connected DiT blocks, and a dense layer revert to its original shape.\nTemporal and cross-dimensional representation can be learned via a linear dense layer (Liu et al. 2024). Given a mini-batch TS $x \\in R^{b\\times L\\times F}$ and its corresponding diffusion steps t, where b stands for the number of samples in a mini-batch, L stands for sequence length, and F stands for the number of features. By permuting L and F separately, we obtain temporal first input $X^T \\in R^{b\\times L\\times F}$ and feature first inputs $X^D \\in R^{b\\times F\\times L}$. For the temporal first input, we have an additional learned positional embedding pos($X^T$). This process can be formulated as follows:\n$h_T = (W_T X^T + b_T) + pos(X^T)$ (11)\n$h_D = W_D X^D + b_D$ (12)\nwhere $W_T$ and $W_D$ represent dense layer parameters; $b_T$ and $b_D$ represent bias terms; and has outputs $h_T \\in R^{b\\times L \\times H}$ and $h_D \\in R^{b\\times F \\times H}$.\nVanilla transformer encoder (Enc) is used to analyze TS at each diffusion step. The transformer encoder block is based on multi-head attention which is commonly used for pattern recognition and feature extraction (Peebles and Xie 2023; Liu et al. 2024; Yuan and Qiao 2024; Coletta et al. 2023; Tashiro et al. 2021). We use one transformer block for each channel to extract relative information:\n$H_T = Enc(h_T)$ (13)\n$H_D = Enc(h_D)$ (14)\nDiT blocks with residual connections are the final layers in the model. Compared to the vanilla encoder blocks, Pee-bles and Xie (2023) design DiTs which perform well in the diffusion framework with two advantages: high throughput and the way conditional information is introduced. Trans-formers are naturally with high-throughput due to the multi-head attention mechanism that can be processed in parallel. Unlike many methods that add conditional information be-fore each block, DiTs introduce partial conditional embed-ding at each layer. More details can be found in Appendix B. In our study, conditional embedding is the diffusion step embedding $t_{emb}$ which is learned via dense layers. We use DiT blocks for the generation process which can be formally described as:\n$\\hat{O} = l_{i=0}^{l \\leq N}DiT(\\hat{O}^{i-1} , t_{emb}) +  l_{i=1}^{l \\leq N}DiT(O^{i-1} + H , t_{emb})  + l_{i=2}^{l \\leq N}DiT(O^{i-1} + \\hat{O}^{i-2} , t_{emb}) $ (15)\nwhere $O^i$ is the i-th DiT block output, H is the encoded information from the previous section, t is the diffusion conditional embedding (i.e., diffusion step), and $l_i$ is an indica-tor function with condition c. For the different channels, we simply replace $H$ with $H_D$ or $H_T$ to obtain $O^D$ or $O^T$.\nThe final output can be obtained by adding all DiT blocks output from cross-dimension and temporal modules with a dense layer that converts to its original shape:\n$\\hat{X}_{out} = (W_B \\sum_{i=0}^{N} O^D ) + (W_B \\sum_{i=0}^{N} O^T )$ (16)"}, {"title": "5 Experiments", "content": "In this section, we describe our experiment settings and evaluate the TS generation quality of PaD-TS across different domains and sequence lengths. The experiment results con-sist of quantitive and qualitative results in terms of individ-ual authenticity and population-level property preservation. We also perform an ablation study to demonstrate the effec-tiveness of each proposed component and the effect of the hyperparameter \u03b1."}, {"title": "Ablation Study", "content": "To further understand our model, we conduct two abla-tion studies to evaluate: (1) the effectiveness of each model component in terms of population-level property (i.e., CC) preservation, and (2) the effect of the population aware train-ing objective hyperparameter \u03b1.\n(1) In the first ablation study, we train PaD-TS variants by taking out one of the four components of the full PaD-TS as follows: (1) without (w/o) temporal channel, (2) w/o"}]}