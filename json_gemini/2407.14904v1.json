{"title": "Large-vocabulary forensic pathological analyses via prototypical cross-modal contrastive learning", "authors": ["Chen Shen", "Chunfeng Lian", "Wanqing Zhang", "Fan Wang", "Jianhua Zhang", "Shuanliang Fan", "Xin Wei", "Gongji Wang", "Kehan Li", "Hongshu Mu", "Hao Wu", "Xinggong Liang", "Jianhua Ma", "Zhenyuan Wang"], "abstract": "Forensic pathology is critical in determining the cause and manner of death through post-mortem examinations, both macroscopic and microscopic. The field, however, grapples with issues such as outcome variability, laborious processes, and a scarcity of trained professionals. This paper presents SongCi, an innovative visual-language model (VLM) designed specifically for forensic pathology. SongCi utilizes advanced prototypical cross-modal self-supervised contrastive learning to enhance the accuracy, efficiency, and generalizability of forensic analyses. It was pre-trained and evaluated on a comprehensive multi-center dataset, which includes over 16 million high-resolution image patches, 2,228 vision-language pairs of post-mortem whole slide images (WSIs) and corresponding gross key findings, along with 471 distinct diagnostic outcomes. Our findings indicate that SongCi surpasses existing multi-modal Al models in many forensic pathology tasks, performs comparably to experienced forensic pathologists and significantly better than less experienced ones, and provides detailed multi-modal explainability, offering critical assistance in forensic investigations. To the best of our knowledge, SongCi is the first VLM specifically developed for forensic pathological analysis and the first large-vocabulary computational pathology (CPath) model that directly processes gigapixel WSIs in forensic science. The source code will be released on https://github.com/shenxiaochenn/SongCi.", "sections": [{"title": "Introduction", "content": "The medicolegal autopsy, commonly known as a post-mortem examination, is conducted by a forensic pathologist to examine the body of a deceased individual meticulously 1,2. The purpose of this examination is to determine the cause and manner of death and identify any diseases or injuries present\u00b3. As a cornerstone of forensic science, autopsies are crucial for both legal and medical analysis4. Within the criminal justice system, they provide essential evidence that may implicate or exonerate individuals5. Additionally, autopsies significantly advance medical knowledge regarding various health conditions6.\nForensic pathologists conduct autopsies using a comprehensive methodology that spans from macroscopic to microscopic analysis. This process encompasses external inspection of the body's surface,\ninternal examination via dissection to assess organ systems, toxicological analysis of bodily fluids, and histopathological analysis of tissue samples1,7. The insights gleaned from these assessments, including macroscopic observations at the organ level and microscopic details at the cellular level, are integral to generating precise and dependable autopsy reports. These reports are essential not only for determining the cause and manner of death but also for estimating the time since death8.\nHowever, conducting precise post-mortem examinations, especially in forensic pathology, poses substantial challenges. The accuracy of these examinations relies heavily on the expertise and subjective assessments of forensic pathologists, leading to considerable variability in outcomes9, 10. Such variability can result in inconsistent findings, even among forensic pathologists with similar training, and is especially marked in complex cases11,12. Forensic pathology is also a labor-intensive and time-consuming discipline, requiring experienced pathologists to invest significant time in analyzing a single whole slide image (WSI) 13. The complexity increases when multiple organ analyses are necessary. Furthermore, the stringent standards of forensic pathology, combined with a shortage of skilled professionals, exacerbate these issues, impacting the overall efficiency and precision of the field14,15.\nIn recent years, computational pathology (CPath), augmented by artificial intelligence (AI), has demonstrated significant potential in a range of clinical pathology tasks, including cancer diagnosis and subtyping 16, 17, metastasis detection18, and patient survival prediction\u00b99. The typical approach involves training deep neural networks driven by specific tasks on carefully labeled samples. However, the arduous and expensive process of sample collection and labeling, particularly for WSIs, restricts the availability of training data, thereby limiting the scalability and generalizability of these AI models. Inspired by the remarkable advancements in self-supervised learning (SSL) and foundation models within the broader machine learning community, recent CPath studies have begun to pre-train models using a variety of unlabeled data, subsequently fine-tuning them for specific downstream tasks, exemplifying the SSL-based transfer learning paradigm20,21. To enhance generalization and robustness, some innovative studies have integrated pathological images with linguistic information (e.g., pathologists' reports, scholarly articles, or medical textbooks) to pre-train visual-language models (VLMs)22\u201324. These models capitalize on the critical semantic information and in-depth domain knowledge contained in textual descriptions to improve the contextual interpretation of histopathological images, resulting in more prosperous and nuanced feature representations. Such advancements in CPath research have significantly refined the workflow and contributed to the advancement of clinical pathology, offering valuable insights for forensic pathological analysis.\nNevertheless, the direct application of clinical-focused models to forensic pathology presents chal- lenges due to the unique characteristics of forensic samples and tasks. Forensic data often exhibit complex post-mortem changes that are absent in clinical biopsies. Moreover, forensic analyses typically encompass a more comprehensive range of conditions (e.g., trauma, disease, and postmortem changes) and organs (e.g., brain, heart, and lung), whereas clinical biopsies usually concentrate on a single organ and task25,26. Consequently, forensic CPath displays more extensive large-vocabulary attributes. These distinctions necessitate the development of specialized models for forensic CPath, which requires forensic-specific pre- training data and more sophisticated SSL approaches to acquire fine-grained, multi-modal representations from such demanding data.\nAddressing these deficiencies, this paper introduces a novel VLM tailored for extensive forensic pathology lexicons. It is pre-trained using cross-modal self-supervised contrastive learning methods on a heterogeneous collection of multi-organ post-mortem WSIs paired with descriptive texts. The model was christened SongCi in tribute to Song Ci, the trailblazer of forensic science during the Southern Song dynasty (Note: Song Ci was a distinguished forensic medical scientist from the Southern Song dynasty, renowned as the inaugural forensic entomologist. His judicial case examinations and experiences are"}, {"title": "Results", "content": "Visualization of post-mortem WSI prototypes across different organs\nIn a task-agnostic fashion, SongCi employs a prototypical contrastive learning strategy to derive generalizable image representations from post-mortem WSIs of various organs, as depicted in Fig. 1c. Each WSI is segmented into a collection of patches, and an image encoder extracts patch-level representations. These are then projected into a low-dimensional space defined by shared prototypes across WSIs. In our study, we learned a total of 933 prototypes using this SSL method. The organization of these prototypes was visualized using the two-dimensional UMAP technique 28 and bar plot, as illustrated in Fig. 2a and Supplementary Fig. 1, where each dot signifies a prototype, color-coded according to the organ type of the nearest patches. The post-mortem WSIs encompass nine distinct organ types, each represented by a unique color: brain, adrenal gland, heart, gastrointestinal tract, kidney, liver, lung, pancreas, and spleen. Patches from the WSIs are associated with their closest prototype, imparting the corresponding color to the prototype. Figure 2a reveals distinct clustering patterns among the prototypes, with some exhibiting uniform colors, denoting intra-tissue prototypes that encode tissue-specific features, such as myocardial hypertrophy and pneumorrhagia (see Fig. 2c). Conversely, prototypes with mixed colors represent inter- tissue prototypes that encapsulate standard histopathological features across different organs, including\nautolysis, inflammation, fibrosis, and hemorrhage (refer to Fig. 2b). These findings suggest that the prototype representations encapsulate both tissue-specific and cross-tissue-shared characteristics from high-resolution WSIs, establishing a versatile foundation for downstream tasks.\nPatch-level generation of synthetic post-mortem WSIs\nTo assess the generalizability of the prototype and instance representations generated by prototypical contrastive learning, we devised a downstream task focused on patch-level post-mortem image synthesis. We employed representation-conditional diffusion models29 to generate WSI patches from random noise. These patches were conditioned on the prototypes (see Fig. 2d) and instance representations (see Fig. 2e). Figure 2b displays synthesized images based on four distinct inter-tissue prototypes, which exhibit high- fidelity representations of post-mortem phenomena. The images correspond to various states: extensive autolysis with cellular structure loss, inflammation with abundant lymphocytes, fibrosis, and hemorrhage with dense erythrocytes. These prototypes, learned by SongCi, effectively encapsulate patterns common"}, {"title": "Self-supervised segmentation of post-mortem WSIs", "content": "Tissue and cell segmentation are fundamental steps in CPath. Beyond image generation, we have utilized pre-trained prototypical WSI encoders for self-supervised WSI segmentation without fine-tuning. Specifically, we treat each prototype learned by the SongCi as a semantic mask, enabling efficient labeling of all patches in a WSI by matching them with their closest prototypes based on the cosine similarity of their representations. Figure 3 showcases self-supervised segmentations of WSIs from four different organs: the spleen, brain, heart, and liver. These are compared with segmentations obtained using an iterative clustering algorithm typically employed in histopathological analysis30-32. We can observe that, compared with the conventional iterative clustering (Fig. 3b), SongCi (Fig. 3c) led to much better segmentations in all cases (more samples see Supplementary Fig. 3). The segmentations by SongCi exhibit\nmarked superiority, as evidenced by the precise differentiation between parenchyma and mesenchyme within splenic and hepatic tissues, which are delineated by the red and various other colored sections, respectively. Furthermore, SongCi achieves a level of granularity in brain and myocardial segmentation that aligns closely with cellular structures, minimizing the introduction of noise. In contrast, conventional clustering often results in noisy segmentations that incorrectly separate identical tissues, as indicated by the black boxes in Fig. 3b.\nIt is worth noting that SongCi offers two significant technical advantages. First, it is more efficient, eliminating the need for iterative optimization and enabling one-shot labeling of patches in ultra-high- resolution WSIs, which may contain over ten thousand patches. Second, it is more flexible and robust, as it does not require pre-determination of cluster numbers. Traditional methods rely heavily on this hyper-parameter, which must be carefully adjusted outside the optimization process. In contrast, SongCi dynamically learns prototypes in a data-driven manner through contrastive SSL. These benefits position SongCi as a generalizable tool for self-supervised or zero-shot segmentation of post-mortem WSIs."}, {"title": "Large-vocabulary forensic diagnosis, with comparisons to existing VLMs", "content": "Leveraging the pre-trained WSI encoder and a pathology-dedicated language model (i.e., PLIP27), SongCi designs a cross-modal contrastive learning strategy to learn from multi-modal inputs (i.e., gross key findings and WSIs) a gated-attention-empowered VLM for zero-shot forensic diagnosis, with the schematic diagram shown in Fig. 1e. First of all, we applied this model to the internal cohort and two external cohorts with significantly different data distributions (Fig. 1d), and compared the performance with six state-of-the-art VLMs from both the medical and general machine-learning communities, including IRENE33, BottleNeckFusion34, DLNMS35, GIT36, Perceiver37, and MCAT19. The quantitative results in terms of three metrics (i.e., Recall, Precision, and IOU) obtained by these methods across different organs are summarized in Fig. 4 and Supplementary Table. 1. We can see that, on average, SongCi consistently outperformed all other competing VLMs by large margins in terms of all three metrics. Specifically, on the internal cohort, external cohort I, and external cohort II, SongCi got a mean Recall of 0.823\u00b10.119, 0.778\u00b10.091, and 0.770\u00b10.132, a mean Precision of 0.814\u00b10.164, 0.773\u00b10.131, and 0.742\u00b10.117, and a mean IOU of 0.682\u00b10.145, 0.617\u00b10.086, and 0.594\u00b10.070, respectively. Compared with other VLMs, the average improvements range between 10% and 20% in most cases. To go into more detail, we can observe that SongCi performed significantly better than other VLMs on the two external cohorts in handling post-mortem WSIs from most organs, e.g., brain, heart, lung, and liver, which are closely related to issues such as cause of death determination and manner of death identification.\nFurthermore, to comprehensively assess the performance in large-vocabulary forensic pathological"}, {"title": "Comparisons between SongCi and forensic pathologists", "content": "We compared SongCi with five forensic pathologists with varying expertise levels, i.e., two senior forensic pathologists (SP) with more than 15 years of experience, two junior pathologists (JP) with more than five years of experience, and a pathologist assistant (PA). Specifically, considering the time-consuming and demanding process of forensic pathology analysis, we selected 100 samples with an unambiguous diagnosis from the two external cohorts and assigned them to these forensic pathologists. We also distributed the internal cohort and ground-truth labels to these experts. Each forensic pathologist analyzed these external samples and made their predictions independently using the internal cohorts as the reference. The results quantified on such an external subset are summarized in Table 2 and Fig. 5. From the precision-recall (PR) curve shown in Fig. 5 and the metric values in Table 2, we can see that SongCi's performance aligns closely with an SP and significantly surpasses the other SP, two JPs, and the PA. Notably, besides the accuracy matching up with the seasoned pathologists, SongCi outperforms in efficiency (i.e., 0.37 versus 7 hours in Table 2), which could significantly reduce the workload of forensic pathology."}, {"title": "Multi-modal explainability in forensic pathological analysis", "content": "SongCi aggregates WSI prototypes and gross key findings to align them with the diagnostic outcomes by learning word/prototype-level attention scores via cross-modal contrastive learning. By nature, these scores also provide critical multi-modal explanation factors for fine-grained analysis of the model's final"}, {"title": "Ablation studies of SongCi's vision and language encoders", "content": "SongCi has two critical components: a pre-trained text encoder to encode gross key findings and a vision encoder trained via prototypical contrastive learning for WSI embedding. The selection of the text encoder and the training strategy of the vision encoder determines the quality of textual and imaging representations, thus the effectiveness of multi-modal fusion. We conducted ablation experiments regarding our specific implementations of SongCi.\nFirst, in SongCi, PLIP27 was used as the text encoder, a language model pre-trained on paired pathology image-text collected from Twitter. We replaced it with the following alternatives: PubmedBERT38, i.e., an encoder pre-trained on medical text data only; CLIP39, i.e., an encoder pre-trained on natural image-text pairs; BiomedCLIP40, i.e., an encoder pre-trained on medical paper illustrations for image-text pairing, and QUILT-1M41, an encoder pre-trained on the pathology image-text pairs grasped from YouTube. The large-vocabulary forensic diagnoses obtained by these variants are summarized in Table 3, from which we can have two observations. That is, we can see that foundation models pre-trained by image-text pairs (e.g., CLIP, BiomedCLIP, QUILT-1M, and PLIP) perform better than those that use only texts (e.g., PubmedBERT), implying the merit of multi-modal learning. In addition, we can see that the two pathology- dedicated LLMs (i.e., QUILT-1M and PLIP) outperformed others by large margins, demonstrating the power of domain knowledge in cross-modal information alignment and fusion. Such domain knowledge encoded in texts is critical in improving generalizability and reducing the reliance on large training datasets. The PLIP used in SongCi led to the best performance in most cases.\nSecond, to pre-train the vision encoder by task-agnostic SSL, SongCi designed sophisticated regu- larization terms to constrain the prototypical space. To check the efficacy of these regularization terms, we further conducted an ablation study to remove them from the loss function and quantify the influence on the diagnosis performance, with the results summarized in Table 4. As can be seen, the baseline (i.e., the model pre-trained by using $L_{ins} + L_{pro}$) has the worst performance, without the three regularization terms on the prototypical space (i.e., $L_{me-max}$, $L_{ipc}$, and $L_{ipd}$). The inclusion of these terms significantly improved the diagnostic outcomes. This demonstrates the effectiveness of the constraints designed in SongCi for learning complete prototypical space, allowing the encoder to establish better the mapping relationship between instance embeddings and prototypical embeddings."}, {"title": "Discussion", "content": "This study presents a generalizable and explainable vision-language model (VLM), i.e., SongCi, dedicated to forensic pathology. To build SongCi, we curated one of the largest post-mortem, multi-modal datasets, gathering 2,228 paired WSI-text samples from three forensic centers, nine organs, and 471 diagnostic outcomes. By leveraging cutting-edge self-supervised learning techniques, the pre-trained SongCi was evaluated on a broad spectrum of downstream tasks in forensic pathological analyses, demonstrating exciting accuracy, generalizability, and explainability compared with state-of-the-art VLMs and forensic pathologists. Our research addresses a significant gap in the availability of multimodal AI tools for complex diagnostic and predictive tasks in forensic pathology. This discipline has traditionally depended on expert judgment and is marked by subjectivity, inconsistency, and inefficiency.\nA primary challenge in building forensic VLMs is extracting and aligning multimodal representations from challenging post-mortem data for large-vocabulary analyses. For this purpose, one major technical strength of SongCi lies in customizing a prototypical contrastive learning algorithm to pre-train a powerful image encoder for fine-grained feature extraction from post-mortem WSI with atypical and varying appearances. In a task-agnostic fashion, it maps millions of patches from WSIs into a low-dimensional space spanned by limited prototypes, where similar instances or patches are grouped tightly from intrinsic semantic views across different organs. The joint visualization of the learned prototypical and instance representations via the UMAP technique provides an intuitive way to understand their organization and relationships among various tissue types. Such visualization revealed that SongCi learns to partition the complex post-mortem WSI space into interpretable clusters corresponding to distinct histopathological entities or disease states, either inter-tissue shared or intra-tissue specific. Two downstream tasks, post- mortem image generation at the patch level and self-supervised semantic segmentation at the WSI level, evaluated the utility of the pre-trained image encoder. The image generation results demonstrate that SongCi's pre-trained image encoder can provide strong guidance to boost advanced diffusion models, generating controllable image patches with high fidelity that present detailed post-mortem changes and organ-specific lesions. Based on the learned prototypes, the self-supervised semantic segmentation results show that a gigapixel WSI can be efficiently segmented as meaningful and essential areas of interest under remarkable precision. These evaluations suggest the generalizability and reliability of SongCi's image encoder as a CPath tool in forensic pathology, even without fine-tuning.\nTo seamlessly align multi-modal, post-mortem data, another key innovation of SongCi is the design of a cross-modal contrastive learning algorithm to establish a dedicated fusion block empowered by gated-attention mechanisms. It plays a pivotal role in integrating macroscopic observations at the organ level (i.e., gross key findings) with microscopic cues at the tissue level (i.e., WSIs) to produce higher- level representations encoding multi-modal knowledge for accurate and coherent forensic pathological analyses. The downstream applications demonstrate that this design led to much better large-vocabulary diagnostic performance than other open-sourced VLMs in forensic pathology. More importantly, its accuracy has matched that of experienced forensic pathologists while significantly improving efficiency, further implying the meaning of SongCi in the autopsy practice. The gated-attention mechanisms in this fusion block assign relative importance or scores to each input element, i.e., each patch in a WSI and each word in the textual description of the gross key findings, by which SongCi straightforwardly focuses on the most salient aspects of both modalities. This mimics the workflow of a forensic pathologist, who typically makes his/her judgments by analyzing organ-level autopsy findings in conjunction with microscopic assessments. The results of explainability analysis demonstrate that such attention mechanisms can capture fine-grained cross-modal factors to uncover how SongCi makes a particular prediction given specific multi-modal inputs, which is very critical considering that AI tools for forensic pathology by nature have exceptionally high requirements in reliability and trustiness.\nStrong zero-shot learning capability is a standout advantage of SongCi, indicating its adaptability and generalizability to novel scenarios, a featured challenge in forensic pathology, considering that autopsies typically involve a broad array of investigations across multiple organs and varying conditions. The success of this zero-shot transfer learning capability lies in SongCi's robust cross-modal fusion and alignment layer, which effectively combines information across modalities to create an aligned representation space for both textual and visual data. Specifically, in the inference stage, given the post-mortem WSI and gross key findings of a particular subject, an operator can list a set of suspicious diagnoses (in texts) as candidates, which could be new cases that have not yet been seen in the pre-training stage. Then, SongCi calculates the cosine similarity between the multi-modal fusion representations and the embeddings of the provided candidate diagnoses, based on which the most likely outcome can be ranked out, together with detailed explanation factors pinpointing specific aspects of the multi-modal data that significantly influence the model's decision. This empowers a forensic pathologist to assess in detail the relationships between a given sample and various potential diagnoses by providing a quantitative measure of confidence, thus assisting pathologists in making accurate assessments and potentially reducing diagnostic errors or inconsistencies. The large-vocabulary forensic diagnostic results across the two external cohorts, especially the off-set and low-frequency quantifications presented in Table 1, demonstrate the superior zero-shot"}, {"title": "Methods", "content": "SongCi\nSongCi is a multi-modal deep learning model tailored for forensic pathological analyses. The archi- tecture consists of three main parts: an imaging encoder for WSI feature extraction, a text encoder for embedding gross key findings and diagnostic queries, and a multi-modal fusion block that integrates the embeddings of WSI and gross key findings to align with those of the diagnostic queries. Specifically, we used an open-sourced, pathology-dedicated language model, i.e., PLIP, in SongCi as the textual encoder directly. To deal with post-mortem data with varying conditions, we designed two novel self-supervised learning (SSL) algorithms to build the imaging encoder and multi-modal fusion block in a task-agnostic fashion. In inference, SongCi can flexibly conduct large-vocabulary (or even open-vocabulary) diagnosis, as an operator only needs to provide a set of candidate outcomes, based on which the model ranks out the possible diagnosis associated with detailed explanation factors identified from the multi-modal inputs.\nPrototypical WSI encoder\nWe propose a hybrid contrastive learning algorithm to learn from gigapixel, post-mortem WSIs fine- grained representations generalizable across different organs (Fig. 1c). The algorithm is built upon a straightforward assumption that image patches (i.e., instances) from different spatial locations, organs, and conditions, are grouped as meaningful clusters in the desired representation space that captures both intra-tissue-specific and inter-tissue-specific information; besides, in each cluster, instance representations present a certain degree of variance to preserve detailed patch-wise specificity. Accordingly, such a hybrid SSL algorithm consists of an instance contrastive learning part and a prototypical contrastive learning part."}, {"title": "The instance contrastive learning part aims to build a vision transformer (ViT)", "content": "42, which leverages the local patches of a WSI as the input and learns instance-level representations via self-distillation. In line with DINO43, contrastive learning is achieved by a teacher-student strategy. A batch of (say totally N) instances is first transformed by a series of data augmentation operations, including multi-cropping44, random resizing, flipping, color jittering, solarization, and Gaussian blurring, which produces two different views for each input instance (say X and X for the n-th instance). Then, X and X are fed into the student and teacher branches, respectively, and we want the corresponding predictions by the two branches to be cross-view consistent. Specifically, the student branch consists of a ViT $f_s(\u00b7)$, a projector $g_{pro}(\u00b7)$, and a predictor $g_{pre}(\u00b7)$. Compared with the student branch, the teacher branch also contains a ViT $f_t(\u00b7)$ and a projector $g_{pro}(\u00b7)$, while the last component is replaced by a sharpening & centering module $J_t(\u00b7)$, which adjusts the distribution of the instance representations to avoid mode collapse. The model parameters of the student branch are optimized by gradient back-propagation, based on which the teacher branch is updated via the exponential moving average (EMA)45. To this end, we quantify the predictions from the two branches via soft-max normalization, such as equation (1) and equation (2)\nPs(X) = \\frac{exp(g_{pre}(g_{pro}(f_s(X_n^s)))/\\tau_s)}{\\sum_{n=1}^N exp(g_{pre}(g_{pro}(f_s(X_n^s)))/\\tau_s)} (1)\nPt(X) = \\frac{exp(J_t(g_{pro}(f_t(X_n^t)))/\\tau_t)}{\\sum_{n=1}^N exp(J_t(g_{pro}(f_t(X_n^t)))/\\tau_t)} (2)\nwhere \\tau_s and \\tau_t are two hyperparameters that control the sharpness of the output probability dis- tributions, respectively. After that, the model parameters are iteratively optimized by minimizing the cross-entropy loss that encourages cross-view consistency, such as equation (3)\nL_{ins} = - \\sum_n p_t(X_n^t) log(p_s(X_n^s)) (3)\nThe prototypical contrastive learning part further distills from the instance representations a more abstract and generalizable feature space spanned by a set of learnable prototypes shared across different WSIs, organs, and conditions. Similar to the instance contrastive learning, here we have two different views for an input instance after data augmentation; the difference in this prototypical learning procedure is that the learnable parameters of the student branch are updated via back-propagation and the teacher branch shares weights with the student branch. Both branches contain a ViT followed by a projector. Let their output embeddings for N input instances be $Z_s \\in \\mathbb{R}^{D \\times N}$ and $Z_t \\in \\mathbb{R}^{D \\times N}$, respectively. We want to learn a set of M prototypical embeddings, say $P \\in \\mathbb{R}^{D \\times M}$, for which we can find a linear mapping $C_t \\in \\mathbb{R}^{M*N}$ that maximizes their similarities with $Z_t$, i.e., minimizes the Sinkhorn distances of the associated optimal-transport problem46, defined as equation (4) and equation (5)\nmax_{C_t} Tr(C_t P^T Z_t) + \\epsilon h(C_t) (4)\ns.t. C_t \\in \\mathbb{R}^{M*N}, C_t 1_M = 1_N, C_t 1_N = \\frac{1}{M} 1_M (5)\nwhere $1_N$ and $1_M$ denote the N- and M- dimensional all-ones vectors, respectively. The function $Tr(\u00b7)$ stands for the matrix trace operation, $h(C_t) = - \\sum_{i,j} C_t[i, j] log(C_t[i, j])$ quantifies the entropy of the linear mapping, and \u025b is a tuning parameter controls its influence. Given P and $Z_t$, a $C_t$ qualified for the above objective realizes that all instances in $Z_t$ can be matched up to the prototype space P and each prototype is"}, {"title": "44,46", "content": "selected at least $\\frac{N}{M}$ times on average. Such a constrained optimization problem can have an approximate solution by using the iterative Sinkhorn-Knopp algorithm , with each iteration defined as equation (6):\n\\hat{C_t} = diag(u) exp(\\frac{P Z_t^T}{\\epsilon}) diag(v) (6)\nwhere $u \\in \\mathbb{R}^M$ and $v \\in \\mathbb{R}^N$ are re-normalization vectors, and $diag(\u00b7)$ formulate them as the diagonal matrices. Following SwAV44, the number of iterations was set as three in our study.\nTo establish the cross-view consistency, we use $\\hat{C_t}$ from the teacher branch as the pseudo label to constrain the prediction in the student branch. That is, the student branch predicts the mapping matrix as equation (7)\nC_s = \\frac{exp(P^T Z_s/\\tau_{prototype})}{\\sum_{m=1}^M exp(P^T Z_s/\\tau_{prototype})} (7)\nwhere $\\tau_{prototype}$ is a element-wise scaling parameter. Then, the main loss function for the prototypical contrastive learning is defined as equation (8)\nL_{prototype} = -C_t log(C_s) (8)\nFurthermore, to stabilize the learning of the prototypical representations, we design three regularization terms in addition to the main loss function. Ideally, the prototype-instance mapping matrix should be sparse rather than dense, by which the prototypes are encouraged to encode diverse information and each instance tends to have the most similar prototype(s). To this end, we impose an instance-prototype cross-entropy loss ($L_{ipc}$) and an instance-prototype distance loss ($L_{ipd}$), which are defined as equation (9), equation (10) and equation (11).\nL_{ipc} = - \\sum_{i=1}^N log(\\frac{exp(z_i^T p_i)}{\\sum_{j=1}^M exp(z_i^T p_j)}) (9)\nL_{ipd} = \\frac{1}{N} \\sum_{i=1}^N || z_i - p_j ||^2 = \\sum_{i=1}^N (2 - 2 z_i^T p_j) (10)\np'_i = max(\\frac{1}{i} { z_i^T p_j } - 1) (11)\nwhere $z_i$ and $p_j$ denote the $L_2$-normalized D-dimensional representation of the i-th instance and j-th prototype, respectively, and $p'_i$ is the nearest prototype of $z_i$. As a result, $L_{ipc}$ encourages inter-prototype differences, and $L_{ipd}$ encourages intra-cluster consistency, i.e., instances close to a particular prototype should be grouped tightly. In addition, to obtain fine-grained representations by the prototypes, we encourage the prototype-instance mapping to fully use all these prototypes, for which a mean-entropy maximization regularization47 is attached, such as equation (12)\nL_{me-max} = \\frac{1}{M} \\sum_{i=1}^N C_i log(C_i) + log(M) (12)\nwhere $C_i = \\frac{1}{i} \\sum_{j=1}^N C_s[i, j]$. Therefore, the global loss for the hybrid contrastive learning is defined as equation (13)\nL_{final} = \\lambda_1 L_{ins} + \\lambda_2 L_{prototype} + \\lambda_3 L_{ipc} + \\lambda_4 L_{ipd} + \\lambda_5 L_{me-max} (13)"}, {"title": "where \u03bb\u2081 was set as 0.6", "content": "\u03bb3 was 0.1, and \u03bb2 = \u03bb4 = \u03bb5 = 1.0 in our implementation. After the contrastive learning, the ViT, the projector, and the prototypes are frozen to be the pre-trained prototypical WSI encoder. All the patches' nearest prototypes determine the WSI-level representation.\nGated-attention-boosted multi-modal fusion block\nWe propose a multi-modal fusion block empowered by gated attention mechanisms for the adaptive fusion of gross key findings and WSI information, which produces multi-modal representations encoding macroscopic and microscopic cues for large-vocabulary forensic pathological analyses.\nThis gated-attention-boosted fusion block adopts initial imaging embeddings of a WSI and textual embeddings of the paired gross key findings as the multi-modal input. The pre-trained prototypical WSI encoder produces the initial imaging embeddings. That is, each patch of a WSI is denoted by the nearest prototype according to the cosine similarity between instance and prototypical representations, and the combination of all patches' nearest prototypes forms the WSI-level prototypical feature embedding, say $z_{pro}$. Moreover, it is worth noting that the frequency of a particular prototype occurring in a WSI could encode critical information regarding the WSI's specific patterns. For example, in a pathology section of brain autolysis, the proportion of normal brain tissue varies due to the degree of autolysis, which can be reflected by the frequency of the post-mortem autolysis-related prototype selected by the pathological image. To encode such key information, we further quantify the numbers of occurrences of each prototype for a WSI and use them as the WSI's prototypical num embedding, say $z_{num}$. On the other hand, the initial word-wise textual embeddings of gross key findings (say $z^l$) are produced by PLIP, a pathology-dedicated LLM. To better align PLIP with forensic pathology, we attach a simple but effective adaptation layer, i.e., a text-to-image adapter $f_{adapter}(\u00b7)$ onto PLIP, which fine-tunes $z^l$ in a few-shot knowledge transfer fashion48,49. Furthermore, considering that the initial mono-modal feature embedding brings inevitable information loss that could cause a cross-modal mismatch, we follow C-MCR50 to update these initial embeddings by adding a small amount of Gaussian noise to improve their robustness for subsequent cross-modal fusion. More specifically, given the initial embeddings of the paired WSI (i.e., $z_{pro}$ and $z_{num}$) and gross key findings (i.e., $z^l$), they are first refined before feeding into the fusion block, such as equation (14) and equation (15)\n\\hat{z} = \\frac{z_{pro} + \\epsilon^i}{\\parallel z_{pro} + \\epsilon^i \\parallel_2} + z_{num} (14)\n\\hat{z^l} = f_{adapter}(\\frac{z^l + \\epsilon^l}{\\parallel z^l + \\epsilon^l \\parallel_2}) (15)\nwhere \u025b and \u025b denote the random Gaussian noises, and $\\parallel . \\parallel_2$ stands for $L_2$-normalization.\nGiven the refined multi-modal inputs (i.e., $\\hat{z}$ and $\\hat{z^l}$), the fusion block designs gated cross-attention and feed-forward network (FFN) layers to update the representations of each modality by considering the complementary information from the other modality. The gated mechanism51 adaptively controls the inter-modal information transfer, thus balancing the contributions of different modalities during cross- modal communication, which has been proven to have typically better performance than conventional cross-attention strategies19,33. Specifically, by using $\\hat{z^l}$ as the guidance, the WSI embedding $\\hat{z}$ is updated by the gated knowledge-guided cross-attention layer followed by a gated FFN, which can be formulated as equation (16) and equation (17)\n\\tilde{z} = \\hat{z} + tanh(A_{att}) \\cdot softmax(\\frac{W_q \\hat{z} (W_k \\hat{z"}]}