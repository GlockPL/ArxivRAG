{"title": "Large-vocabulary forensic pathological analyses via prototypical cross-modal contrastive learning", "authors": ["Chen Shen", "Chunfeng Lian", "Wanqing Zhang", "Fan Wang", "Jianhua Zhang", "Shuanliang Fan", "Xin Wei", "Gongji Wang", "Kehan Li", "Hongshu Mu", "Hao Wu", "Xinggong Liang", "Jianhua Ma", "Zhenyuan Wang"], "abstract": "Forensic pathology is critical in determining the cause and manner of death through post-mortem examinations, both macroscopic and microscopic. The field, however, grapples with issues such as outcome variability, laborious processes, and a scarcity of trained professionals. This paper presents SongCi, an innovative visual-language model (VLM) designed specifically for forensic pathology. SongCi utilizes advanced prototypical cross-modal self-supervised contrastive learning to enhance the accuracy, efficiency, and generalizability of forensic analyses. It was pre-trained and evaluated on a comprehensive multi-center dataset, which includes over 16 million high-resolution image patches, 2,228 vision-language pairs of post-mortem whole slide images (WSIs) and corresponding gross key findings, along with 471 distinct diagnostic outcomes. Our findings indicate that SongCi surpasses existing multi-modal Al models in many forensic pathology tasks, performs comparably to experienced forensic pathologists and significantly better than less experienced ones, and provides detailed multi-modal explainability, offering critical assistance in forensic investigations. To the best of our knowledge, SongCi is the first VLM specifically developed for forensic pathological analysis and the first large-vocabulary computational pathology (CPath) model that directly processes gigapixel WSIs in forensic science. The source code will be released on https://github.com/shenxiaochenn/SongCi.", "sections": [{"title": "Introduction", "content": "The medicolegal autopsy, commonly known as a post-mortem examination, is conducted by a forensic pathologist to examine the body of a deceased individual meticulously 1,2. The purpose of this examination is to determine the cause and manner of death and identify any diseases or injuries present\u00b3. As a cornerstone of forensic science, autopsies are crucial for both legal and medical analysis. Within the criminal justice system, they provide essential evidence that may implicate or exonerate individuals5. Additionally, autopsies significantly advance medical knowledge regarding various health conditions6.\nForensic pathologists conduct autopsies using a comprehensive methodology that spans from macro- scopic to microscopic analysis. This process encompasses external inspection of the body's surface,\ninternal examination via dissection to assess organ systems, toxicological analysis of bodily fluids, and histopathological analysis of tissue samples1,7. The insights gleaned from these assessments, including macroscopic observations at the organ level and microscopic details at the cellular level, are integral to generating precise and dependable autopsy reports. These reports are essential not only for determining the cause and manner of death but also for estimating the time since death8.\nHowever, conducting precise post-mortem examinations, especially in forensic pathology, poses substantial challenges. The accuracy of these examinations relies heavily on the expertise and subjective assessments of forensic pathologists, leading to considerable variability in outcomes9, 10. Such variability can result in inconsistent findings, even among forensic pathologists with similar training, and is especially marked in complex cases11,12. Forensic pathology is also a labor-intensive and time-consuming discipline, requiring experienced pathologists to invest significant time in analyzing a single whole slide image (WSI) 13. The complexity increases when multiple organ analyses are necessary. Furthermore, the stringent standards of forensic pathology, combined with a shortage of skilled professionals, exacerbate these issues, impacting the overall efficiency and precision of the field14,15.\nIn recent years, computational pathology (CPath), augmented by artificial intelligence (AI), has demonstrated significant potential in a range of clinical pathology tasks, including cancer diagnosis and subtyping 16, 17, metastasis detection18, and patient survival prediction\u00b99. The typical approach involves training deep neural networks driven by specific tasks on carefully labeled samples. However, the arduous and expensive process of sample collection and labeling, particularly for WSIs, restricts the availability of training data, thereby limiting the scalability and generalizability of these AI models. Inspired by the remarkable advancements in self-supervised learning (SSL) and foundation models within the broader machine learning community, recent CPath studies have begun to pre-train models using a variety of unlabeled data, subsequently fine-tuning them for specific downstream tasks, exemplifying the SSL-based transfer learning paradigm20,21. To enhance generalization and robustness, some innovative studies have integrated pathological images with linguistic information (e.g., pathologists' reports, scholarly articles, or medical textbooks) to pre-train visual-language models (VLMs)22\u201324. These models capitalize on the critical semantic information and in-depth domain knowledge contained in textual descriptions to improve the contextual interpretation of histopathological images, resulting in more prosperous and nuanced feature representations. Such advancements in CPath research have significantly refined the workflow and contributed to the advancement of clinical pathology, offering valuable insights for forensic pathological analysis.\nNevertheless, the direct application of clinical-focused models to forensic pathology presents chal- lenges due to the unique characteristics of forensic samples and tasks. Forensic data often exhibit complex post-mortem changes that are absent in clinical biopsies. Moreover, forensic analyses typically encompass a more comprehensive range of conditions (e.g., trauma, disease, and postmortem changes) and organs (e.g., brain, heart, and lung), whereas clinical biopsies usually concentrate on a single organ and task25,26. Consequently, forensic CPath displays more extensive large-vocabulary attributes. These distinctions necessitate the development of specialized models for forensic CPath, which requires forensic-specific pre- training data and more sophisticated SSL approaches to acquire fine-grained, multi-modal representations from such demanding data.\nAddressing these deficiencies, this paper introduces a novel VLM tailored for extensive forensic pathology lexicons. It is pre-trained using cross-modal self-supervised contrastive learning methods on a heterogeneous collection of multi-organ post-mortem WSIs paired with descriptive texts. The model was christened SongCi in tribute to Song Ci, the trailblazer of forensic science during the Southern Song dynasty (Note: Song Ci was a distinguished forensic medical scientist from the Southern Song dynasty, renowned as the inaugural forensic entomologist. His judicial case examinations and experiences are"}, {"title": "Results", "content": "Visualization of post-mortem WSI prototypes across different organs\nIn a task-agnostic fashion, SongCi employs a prototypical contrastive learning strategy to derive generalizable image representations from post-mortem WSIs of various organs, as depicted in Fig. 1c. Each WSI is segmented into a collection of patches, and an image encoder extracts patch-level representations. These are then projected into a low-dimensional space defined by shared prototypes across WSIs. In our study, we learned a total of 933 prototypes using this SSL method. The organization of these prototypes was visualized using the two-dimensional UMAP technique 28 and bar plot, as illustrated in Fig. 2a and Supplementary Fig. 1, where each dot signifies a prototype, color-coded according to the organ type of the nearest patches. The post-mortem WSIs encompass nine distinct organ types, each represented by a unique color: brain, adrenal gland, heart, gastrointestinal tract, kidney, liver, lung, pancreas, and spleen. Patches from the WSIs are associated with their closest prototype, imparting the corresponding color to the prototype. Figure 2a reveals distinct clustering patterns among the prototypes, with some exhibiting uniform colors, denoting intra-tissue prototypes that encode tissue-specific features, such as myocardial hypertrophy and pneumorrhagia (see Fig. 2c). Conversely, prototypes with mixed colors represent inter- tissue prototypes that encapsulate standard histopathological features across different organs, including\nautolysis, inflammation, fibrosis, and hemorrhage (refer to Fig. 2b). These findings suggest that the prototype representations encapsulate both tissue-specific and cross-tissue-shared characteristics from high-resolution WSIs, establishing a versatile foundation for downstream tasks.\nPatch-level generation of synthetic post-mortem WSIs\nTo assess the generalizability of the prototype and instance representations generated by prototypical contrastive learning, we devised a downstream task focused on patch-level post-mortem image synthesis. We employed representation-conditional diffusion models29 to generate WSI patches from random noise. These patches were conditioned on the prototypes (see Fig. 2d) and instance representations (see Fig. 2e). Figure 2b displays synthesized images based on four distinct inter-tissue prototypes, which exhibit high- fidelity representations of post-mortem phenomena. The images correspond to various states: extensive autolysis with cellular structure loss, inflammation with abundant lymphocytes, fibrosis, and hemorrhage with dense erythrocytes. These prototypes, learned by SongCi, effectively encapsulate patterns common\nacross different tissues. Additionally, Fig. 2c illustrates that SongCi can also capture organ-specific patterns, such as myocardial hypertrophy and cerebral edema, through intra-tissue prototypes. The diffusion model then generates high-fidelity patches for these specific tissue types. The examples in Fig. 2b and 2c demonstrate that SongCi's prototypes encode essential post-mortem features of diverse intra-tissues and inter-tissues (more samples see Supplementary Fig. 2). Moreover, Fig. 2f showcases image patches generated using specific instance representations as the conditions. These images retain intricate details of the original instances, highlighted by yellow boxes in Fig. 2f, including renal tubules with hemorrhage, normal renal tubules, liver fat particles (undissolved), and splenic trabeculae. This indicates that SongCi's instance embeddings are highly detailed, laying a robust groundwork for distilling generalizable prototypes across various organs.\nSelf-supervised segmentation of post-mortem WSIs\nTissue and cell segmentation are fundamental steps in CPath. Beyond image generation, we have utilized pre-trained prototypical WSI encoders for self-supervised WSI segmentation without fine-tuning. Specifically, we treat each prototype learned by the SongCi as a semantic mask, enabling efficient labeling of all patches in a WSI by matching them with their closest prototypes based on the cosine similarity of their representations. Figure 3 showcases self-supervised segmentations of WSIs from four different organs: the spleen, brain, heart, and liver. These are compared with segmentations obtained using an iterative clustering algorithm typically employed in histopathological analysis30-32. We can observe that, compared with the conventional iterative clustering (Fig. 3b), SongCi (Fig. 3c) led to much better segmentations in all cases (more samples see Supplementary Fig. 3). The segmentations by SongCi exhibit\nmarked superiority, as evidenced by the precise differentiation between parenchyma and mesenchyme within splenic and hepatic tissues, which are delineated by the red and various other colored sections, respectively. Furthermore, SongCi achieves a level of granularity in brain and myocardial segmentation that aligns closely with cellular structures, minimizing the introduction of noise. In contrast, conventional clustering often results in noisy segmentations that incorrectly separate identical tissues, as indicated by the black boxes in Fig. 3b.\nIt is worth noting that SongCi offers two significant technical advantages. First, it is more efficient, eliminating the need for iterative optimization and enabling one-shot labeling of patches in ultra-high- resolution WSIs, which may contain over ten thousand patches. Second, it is more flexible and robust, as it does not require pre-determination of cluster numbers. Traditional methods rely heavily on this hyper-parameter, which must be carefully adjusted outside the optimization process. In contrast, SongCi dynamically learns prototypes in a data-driven manner through contrastive SSL. These benefits position SongCi as a generalizable tool for self-supervised or zero-shot segmentation of post-mortem WSIs.\nLarge-vocabulary forensic diagnosis, with comparisons to existing VLMs\nLeveraging the pre-trained WSI encoder and a pathology-dedicated language model (i.e., PLIP27), SongCi designs a cross-modal contrastive learning strategy to learn from multi-modal inputs (i.e., gross key findings and WSIs) a gated-attention-empowered VLM for zero-shot forensic diagnosis, with the schematic diagram shown in Fig. 1e. First of all, we applied this model to the internal cohort and two external cohorts with significantly different data distributions (Fig. 1d), and compared the performance with six state-of-the-art VLMs from both the medical and general machine-learning communities, including IRENE33, BottleNeckFusion34, DLNMS35, GIT36, Perceiver37, and MCAT19. The quantitative results in terms of three metrics (i.e., Recall, Precision, and IOU) obtained by these methods across different organs are summarized in Fig. 4 and Supplementary Table. 1. We can see that, on average, SongCi consistently outperformed all other competing VLMs by large margins in terms of all three metrics. Specifically, on the internal cohort, external cohort I, and external cohort II, SongCi got a mean Recall of 0.823\u00b10.119, 0.778\u00b10.091, and 0.770\u00b10.132, a mean Precision of 0.814\u00b10.164, 0.773\u00b10.131, and 0.742\u00b10.117, and a mean IOU of 0.682\u00b10.145, 0.617\u00b10.086, and 0.594\u00b10.070, respectively. Compared with other VLMs, the average improvements range between 10% and 20% in most cases. To go into more detail, we can observe that SongCi performed significantly better than other VLMs on the two external cohorts in handling post-mortem WSIs from most organs, e.g., brain, heart, lung, and liver, which are closely related to issues such as cause of death determination and manner of death identification.\nFurthermore, to comprehensively assess the performance in large-vocabulary forensic pathological\nanalysis, we applied SongCi to the challenging tasks of off-set and low-frequency diagnosis (Table 1). Specifically, the off-set samples are a subset of each external cohort whose ground-truth diagnostic labels do not exist in the internal cohort for pre-training. The low-frequency samples have labels occurring less than ten times in the corresponding cohorts. Table 1 shows that SongCi consistently led to the best Recall in the off-set and low-frequency diagnosis tasks compared to other VLMs. Notably, these results were obtained by the zero-shot inference via the pre-trained multi-modality models, demonstrating the promising generalizability of SongCi.\nComparisons between SongCi and forensic pathologists\nWe compared SongCi with five forensic pathol- ogists with varying expertise levels, i.e., two senior forensic pathologists (SP) with more than 15 years of experience, two junior pathologists (JP) with more than five years of experience, and a pathologist assistant (PA). Specifically, considering the time-consuming and demanding process of forensic pathology analysis, we selected 100 samples with an unambiguous diagnosis from the two external cohorts and assigned them to these forensic pathologists. We also distributed the internal cohort and ground-truth labels to these experts. Each forensic pathologist analyzed these external sam- ples and made their predictions independently using the internal cohorts as the reference. The results quan- tified on such an external subset are summarized in Table 2 and Fig. 5. From the precision-recall (PR) curve shown in Fig. 5 and the metric values in Table 2, we can see that SongCi's performance aligns closely with an SP and significantly surpasses the other SP, two JPs, and the PA. Notably, besides the accuracy matching up with the seasoned pathologists, SongCi outperforms in efficiency (i.e., 0.37 versus 7 hours in Table 2), which could significantly reduce the workload of forensic pathology.\nMulti-modal explainability in forensic pathological analysis\nSongCi aggregates WSI prototypes and gross key findings to align them with the diagnostic outcomes by learning word/prototype-level attention scores via cross-modal contrastive learning. By nature, these scores also provide critical multi-modal explanation factors for fine-grained analysis of the model's final"}, {"title": "Discussion", "content": "This study presents a generalizable and explainable vision-language model (VLM), i.e., SongCi, dedicated to forensic pathology. To build SongCi, we curated one of the largest post-mortem, multi-modal datasets, gathering 2,228 paired WSI-text samples from three forensic centers, nine organs, and 471 diagnostic outcomes. By leveraging cutting-edge self-supervised learning techniques, the pre-trained SongCi was evaluated on a broad spectrum of downstream tasks in forensic pathological analyses, demonstrating exciting accuracy, generalizability, and explainability compared with state-of-the-art VLMs and forensic pathologists. Our research addresses a significant gap in the availability of multimodal AI tools for complex diagnostic and predictive tasks in forensic pathology. This discipline has traditionally depended on expert judgment and is marked by subjectivity, inconsistency, and inefficiency.\nA primary challenge in building forensic VLMs is extracting and aligning multimodal representations from challenging post-mortem data for large-vocabulary analyses. For this purpose, one major technical strength of SongCi lies in customizing a prototypical contrastive learning algorithm to pre-train a powerful image encoder for fine-grained feature extraction from post-mortem WSI with atypical and varying appearances. In a task-agnostic fashion, it maps millions of patches from WSIs into a low-dimensional space spanned by limited prototypes, where similar instances or patches are grouped tightly from intrinsic\nsemantic views across different organs. The joint visualization of the learned prototypical and instance representations via the UMAP technique provides an intuitive way to understand their organization and relationships among various tissue types. Such visualization revealed that SongCi learns to partition the complex post-mortem WSI space into interpretable clusters corresponding to distinct histopathological entities or disease states, either inter-tissue shared or intra-tissue specific. Two downstream tasks, post- mortem image generation at the patch level and self-supervised semantic segmentation at the WSI level, evaluated the utility of the pre-trained image encoder. The image generation results demonstrate that SongCi's pre-trained image encoder can provide strong guidance to boost advanced diffusion models, generating controllable image patches with high fidelity that present detailed post-mortem changes and organ-specific lesions. Based on the learned prototypes, the self-supervised semantic segmentation results show that a gigapixel WSI can be efficiently segmented as meaningful and essential areas of interest under remarkable precision. These evaluations suggest the generalizability and reliability of SongCi's image encoder as a CPath tool in forensic pathology, even without fine-tuning.\nTo seamlessly align multi-modal, post-mortem data, another key innovation of SongCi is the design of a cross-modal contrastive learning algorithm to establish a dedicated fusion block empowered by gated-attention mechanisms. It plays a pivotal role in integrating macroscopic observations at the organ level (i.e., gross key findings) with microscopic cues at the tissue level (i.e., WSIs) to produce higher- level representations encoding multi-modal knowledge for accurate and coherent forensic pathological analyses. The downstream applications demonstrate that this design led to much better large-vocabulary diagnostic performance than other open-sourced VLMs in forensic pathology. More importantly, its accuracy has matched that of experienced forensic pathologists while significantly improving efficiency, further implying the meaning of SongCi in the autopsy practice. The gated-attention mechanisms in this fusion block assign relative importance or scores to each input element, i.e., each patch in a WSI and each word in the textual description of the gross key findings, by which SongCi straightforwardly focuses on the most salient aspects of both modalities. This mimics the workflow of a forensic pathologist, who typically makes his/her judgments by analyzing organ-level autopsy findings in conjunction with microscopic assessments. The results of explainability analysis demonstrate that such attention mechanisms can capture fine-grained cross-modal factors to uncover how SongCi makes a particular prediction given specific multi-modal inputs, which is very critical considering that AI tools for forensic pathology by nature have exceptionally high requirements in reliability and trustiness.\nStrong zero-shot learning capability is a standout advantage of SongCi, indicating its adaptability and generalizability to novel scenarios, a featured challenge in forensic pathology, considering that autopsies typically involve a broad array of investigations across multiple organs and varying conditions. The success of this zero-shot transfer learning capability lies in SongCi's robust cross-modal fusion and alignment layer, which effectively combines information across modalities to create an aligned representation space for both textual and visual data. Specifically, in the inference stage, given the post-mortem WSI and gross key findings of a particular subject, an operator can list a set of suspicious diagnoses (in texts) as candidates, which could be new cases that have not yet been seen in the pre-training stage. Then, SongCi calculates the cosine similarity between the multi-modal fusion representations and the embeddings of the provided candidate diagnoses, based on which the most likely outcome can be ranked out, together with detailed explanation factors pinpointing specific aspects of the multi-modal data that significantly influence the model's decision. This empowers a forensic pathologist to assess in detail the relationships between a given sample and various potential diagnoses by providing a quantitative measure of confidence, thus assisting pathologists in making accurate assessments and potentially reducing diagnostic errors or inconsistencies. The large-vocabulary forensic diagnostic results across the two external cohorts, especially the off-set and low-frequency quantifications presented in Table 1, demonstrate the superior zero-shot\nlearning performance of SongCi. Furthermore, the comprehensive comparisons with the existing VLMs and forensic pathologists show that SongCi is a generalizable, explainable, and, more importantly, forensic pathology-dedicated AI tool, adept at efficiently amalgamating various data sources, thereby enhancing the efficiency, accuracy, and consistency of forensic diagnoses across varying cases and organs.\nThis work has several limitations that deserve continual research in the future. Although an un- precedently large dataset of paired post-mortem WSIs and gross key findings was collected for the self-supervised learning of SongCi, more data with significant diversities are needed to improve its robust- ness and generalizability further. Considering that data collection in forensic pathology is practically more complicated than in clinical applications, and the former has more obvious large-vocabulary properties, national or even international collaborations on this topic are urgently necessary. The current version of SongCi is a VLM, which could neglect critical forensic information in other data modalities. In practice, an autopsy includes multiple steps, which produce a broad spectrum of data formats like textual, multi- omics, and imaging data. Intuitively, the fusion of this multi-modal information could further improve the outcomes in AI-empowered forensic pathology. Although we have justified the efficacy of SongCi through a series of downstream tasks, some other applications still need further investigation, such as predicting post-mortem time and simulating longitudinal post-mortem changes conditioned on varying organs, environments, and death causes. A more comprehensive design of downstream evaluations is vital in enhancing the practical usage of SongCi. Moreover, using English-based LLMs may limit SongCi's applicability in non-English communities, necessitating adaptations for broader adoption.\nIn summary, both the presented applications and SongCi's existing limitations pronounce the signifi- cance of continuous research and evaluation to advance and better understand the strengths and practical usage of cross-modal self-supervised pre-training (or even so-called multi-modal foundation models) for forensic pathology."}, {"title": "Methods", "content": "SongCi\nSongCi is a multi-modal deep learning model tailored for forensic pathological analyses. The archi- tecture consists of three main parts: an imaging encoder for WSI feature extraction, a text encoder for embedding gross key findings and diagnostic queries, and a multi-modal fusion block that integrates the embeddings of WSI and gross key findings to align with those of the diagnostic queries. Specifically, we used an open-sourced, pathology-dedicated language model, i.e., PLIP, in SongCi as the textual encoder directly. To deal with post-mortem data with varying conditions, we designed two novel self-supervised learning (SSL) algorithms to build the imaging encoder and multi-modal fusion block in a task-agnostic fashion. In inference, SongCi can flexibly conduct large-vocabulary (or even open-vocabulary) diagnosis, as an operator only needs to provide a set of candidate outcomes, based on which the model ranks out the possible diagnosis associated with detailed explanation factors identified from the multi-modal inputs.\nPrototypical WSI encoder\nWe propose a hybrid contrastive learning algorithm to learn from gigapixel, post-mortem WSIs fine- grained representations generalizable across different organs (Fig. 1c). The algorithm is built upon a straightforward assumption that image patches (i.e., instances) from different spatial locations, organs, and conditions, are grouped as meaningful clusters in the desired representation space that captures both intra-tissue-specific and inter-tissue-specific information; besides, in each cluster, instance representations present a certain degree of variance to preserve detailed patch-wise specificity. Accordingly, such a hybrid SSL algorithm consists of an instance contrastive learning part and a prototypical contrastive learning part.\nThe instance contrastive learning part aims to build a vision transformer (ViT)42, which leverages the local patches of a WSI as the input and learns instance-level representations via self-distillation. In line with DINO43, contrastive learning is achieved by a teacher-student strategy. A batch of (say totally N) instances is first transformed by a series of data augmentation operations, including multi-cropping44, random resizing, flipping, color jittering, solarization, and Gaussian blurring, which produces two different views for each input instance (say X and X for the n-th instance). Then, X and X are fed into the student and teacher branches, respectively, and we want the corresponding predictions by the two branches to be cross-view consistent. Specifically, the student branch consists of a ViT fs(\u00b7), a projector gro(.), and a predictor gre (.). Compared with the student branch, the teacher branch also contains a ViT f\u2081(\u00b7) and a projector gro(.), while the last component is replaced by a sharpening & centering module J\u2081(\u00b7), which adjusts the distribution of the instance representations to avoid mode collapse. The model parameters of the student branch are optimized by gradient back-propagation, based on which the teacher branch is updated via the exponential moving average (EMA)45. To this end, we quantify the predictions from the two branches via soft-max normalization, such as equation (1) and equation (2)\n$P_{s}(X_{i}^{s})=\\frac{exp(g_{pre}^{s}(g_{pro}^{s}(f_{s}(X_{i}^{s}))))/\\tau_{s})}{\\sum_{n=1}^{N}exp(g_{pre}^{s}(g_{pro}^{s}(f_{s}(X_{n}^{s}))))/\\tau_{s})}(1)$\n$P_{t}(X_{i}^{t})=\\frac{exp(J_{t}(g_{pro}^{t}(f_{t}(X_{i}^{t}))))/\\tau_{t})}{\\sum_{n=1}^{N}exp(J_{t}(g_{pro}^{t}(f_{t}(X_{n}^{t}))))/\\tau_{t})}(2)$\nwhere \u03c4\u03c2 and \u03c4\u06c1 are two hyperparameters that control the sharpness of the output probability dis- tributions, respectively. After that, the model parameters are iteratively optimized by minimizing the cross-entropy loss that encourages cross-view consistency, such as equation (3)\n$L_{ins} = - \\sum_{i}^{N} P_{t}(X_{i}^{t})log(P_{s}(X_{i}^{s}))(3)$\nThe prototypical contrastive learning part further distills from the instance representations a more abstract and generalizable feature space spanned by a set of learnable prototypes shared across different WSIs, organs, and conditions. Similar to the instance contrastive learning, here we have two different views for an input instance after data augmentation; the difference in this prototypical learning procedure is that the learnable parameters of the student branch are updated via back-propagation and the teacher branch shares weights with the student branch. Both branches contain a ViT followed by a projector. Let their output embeddings for N input instances be Zs \u2208 RD\u00d7N and Z\u2081 \u2208 RD\u00d7N, respectively. We want to learn a set of M prototypical embeddings, say P \u2208 RD\u00d7M, for which we can find a linear mapping C\u2081 \u2208 RM*N that maximizes their similarities with Z\u2081, i.e., minimizes the Sinkhorn distances of the associated optimal-transport problem46, defined as equation (4) and equation (5)\n$\\underset{C_{t}}{max}Tr(C_{t}P^{T}Z_{t})-\\epsilon h(C_{t}))(4)$\ns.t. $C_{t} \\in R^{M*N}, C_{t}1_{M} = 1_{N}^{T}, C_{t}^{T}1_{N} = \\frac{1}{M}1_{M}$(5)\nwhere 1N and 1M denote the N- and M- dimensional all-ones vectors, respectively. The function Tr(\u00b7) stands for the matrix trace operation, h(C\u2081) = \u2212 \u2211ij C\u2081[i, j]log(C\u2081[i, j]) quantifies the entropy of the linear mapping, and \u025b is a tuning parameter controls its influence. Given P and Z\u0142, a C\u2081 qualified for the above objective realizes that all instances in Z\u2081 can be matched up to the prototype space P and each prototype is\nselected at least $\\frac{N}{M}$ times on average. Such a constrained optimization problem can have an approximate solution by using the iterative Sinkhorn-Knopp algorithm44,46, with each iteration defined as equation (6):\n$\\hat{C}_{t} = diag(u)exp(\\frac{P^{T}Z_{t}}{\\epsilon})diag(v))(6)$\nwhere u \u2208 RM and v \u2208 RN are re-normalization vectors, and diag(\u00b7) formulate them as the diagonal matrices. Following SwAV44, the number of iterations was set as three in our study.\nTo establish the cross-view consistency, we use \u0108\u2081 from the teacher branch as the pseudo label to constrain the prediction in the student branch. That is, the student branch predicts the mapping matrix as equation (7)\n$C_{s} = \\frac{exp(P^{T}Z_{s}/\\tau_{prototype})}{\\sum_{m=1}^{M}exp(P^{T}Z_{s}/\\tau_{prototype})}(7)$\nwhere $\\tau_{prototype}$ is a element-wise scaling parameter. Then, the main loss function for the prototypical contrastive learning is defined as equation (8)\n$L_{prototype} = - \\sum_{i}^{N} C_{t}log(C_{s})(8)$\nFurthermore, to stabilize the learning of the prototypical representations, we design three regularization terms in addition to the main loss function. Ideally, the prototype-instance mapping matrix should be sparse rather than dense, by which the prototypes are encouraged to encode diverse information and each instance tends to have the most similar prototype(s). To this end, we impose an instance-prototype cross-entropy loss (Lipc) and an instance-prototype distance loss (Lipd), which are defined as equation (9), equation (10) and equation (11).\n$L_{ipc} = - \\sum_{i}^{N} log (\\frac{exp(z_{i}^{T}p_{j})}{\\sum_{j=1}^{M}exp(z_{i}^{T}p_{j})}) (9)$\n$L_{ipd} = \\sum_{i=1}^{N} \\left || z_{i} - p'_{j} \\right || ^{2} = \\sum_{i=1}^{N} (2 - 2z_{i}^{T}p'_{j}) (10)$\n$p'_{j} = \\underset{p_{j}}{max} \\{ (z_{i}^{T}p_{j})\\}_{j=1}^{M}(11)$\nwhere zi and pj denote the L2-normalized D-dimensional representation of the i-th instance and j-th prototype, respectively, and p' is the nearest prototype of zi. As a result, Lipc encourages inter-prototype differences, and Lipd encourages intra-cluster consistency, i.e., instances close to a particular prototype should be grouped tightly. In addition, to obtain fine-grained representations by the prototypes, we encourage the prototype-instance mapping to fully use all these prototypes, for which a mean-entropy maximization regularization47 is attached, such as equation (12)\n$L_{me-max} = \\sum_{i=1}^{N} C_{i}log(C_{i}) + log(M)(12)$\nwhere C = $\\frac{1}{N} \\sum_{j=1}^{M}C_{s}[i, j]$. Therefore, the global loss for the hybrid contrastive learning is defined as equation (13)\n$L_{final} = \\lambda_{1}L_{ins} + \\lambda_{2}L_{prototype} + \\lambda_{3}L_{ipc} + \\lambda_{4}L_{ipd} + \\lambda_{5}L_{me-max}(13)$\nwhere \u03bb\u2081 was set as 0.6, 23 was 0.1, and 22 = 24 = 25 = 1.0 in our implementation. After the contrastive learning, the ViT, the projector, and the prototypes are frozen to be the pre-trained prototypical WSI encoder. All the patches' nearest prototypes determine the WSI-level representation.\nGated-attention-boosted multi-modal fusion block\nWe propose a multi-modal fusion block empowered by gated attention mechanisms for the adaptive fusion of gross key findings and WSI information, which produces multi-modal representations encoding macroscopic and microscopic cues for large-vocabulary forensic pathological analyses.\nThis gated-attention-boosted fusion block adopts initial imaging embeddings of a WSI and textual embeddings of the paired gross key findings as the multi-modal input. The pre-trained prototypical WSI encoder produces the initial imaging embeddings. That is, each patch of a WSI is denoted by the nearest prototype according to the cosine similarity between instance and prototypical representations, and the combination of all patches' nearest prototypes forms the WSI-level prototypical feature embedding, say Zpro. Moreover, it is worth noting that the frequency of a particular prototype occurring in a WSI could encode critical information regarding the WSI's specific patterns. For example, in a pathology section of brain autolysis, the proportion of normal brain tissue varies due to the degree of autolysis, which can be reflected by the frequency of the post-mortem autolysis-related prototype selected by the pathological image. To encode such key information, we further quantify the numbers of occurrences of each prototype for a WSI and use them as the WSI's prototypical num embedding, say zhum. On the other hand, the initial word-wise textual embeddings of gross key findings (say z\u00b9) are produced by PLIP, a pathology-dedicated LLM. To better align PLIP with forensic pathology, we attach a simple but effective adaptation layer, i.e., a text-to-image adapter fadapter(\u00b7) onto PLIP, which fine-tunes z in a few-shot knowledge transfer fashion48,49. Furthermore, considering that the initial mono-modal feature embedding brings inevitable information loss that could cause a cross-modal mismatch, we follow C-MCR50 to update these initial embeddings by adding a small amount of Gaussian noise to improve their robustness for subsequent cross-modal fusion. More specifically, given the initial embeddings of the paired WSI (i.e., zpro and Znum) and gross key findings (i.e., z), they are first refined before feeding into the fusion block, such as equation (14) and equation (15)\n$\\hat{z}_{pro} = \\frac{z_{pro} + \\epsilon^{i}}{\\left ||z_{pro} + \\epsilon^{i}\\right ||_{2}} + \\frac{\\hat{z}_{num}}{\\left ||z_{pro} + \\epsilon^{i}\\right ||_{2}}(14)$\n$\\hat{z}^{t} = f_{adapter}(\\frac{z^{t} + \\epsilon^{t}}{\\left ||z^{t} + \\epsilon^{t}\\right ||_{2}})(15)$\nwhere \u025b and \u025b denote the random Gaussian noises, and || . ||2 stands for L2-normalization.\nGiven the refined multi-modal inputs (i.e., 2 and 2), the fusion block designs gated cross-attention and feed-forward network (FFN) layers to update the representations of each modality by considering the complementary information from the other modality. The gated mechanism51 adaptively controls the inter-modal information transfer, thus balancing the contributions of different modalities during cross- modal communication, which has been proven to have typically better performance than conventional cross-attention strategies19,33. Specifically, by using as the guidance, the WSI embedding z is updated by the gated knowledge-guided cross-attention layer followed by a gated FFN, which can be formulated as equation (16) and equation (17)\n$\\hat{z}^{v}_{l} = \\hat{z}^{v} + tanh(\\alpha^{v}_{att}) \\cdot softmax(\\frac{W_{q}z^{t}(W_{k}\\hat{z}^{v})^{T}}{\\sqrt{d_{k}}})W_{v}\\hat{z}^{v}, (16)$\n$\\hat{z}_{l+1}^{v} = \\hat{{v} + tanh(\\alpha_{ffn}). FFN(\\hat{z}_{l}^{v})(17)$\nwhere $W_{q}, W_{k}$, and $W_{v}$ stand for learnable matrices for linear mappings, $\\alpha_{att}^{v}$ and $\\alpha_{ffn}^{v}$ are learnable scalars (i.e., the gated-attention coefficients), and tanh(\u00b7) denotes the tanh activation. Similarly, by using z as the guidance, 2 is updated by the gated prototype-guided cross-attention layer followed by a gated FFN; equation (18) and equation (19):\n$\\hat{z}^{t}_{l} = \\hat{z}^{t} + tanh(\\alpha^{t}_{att}) \\cdot softmax(\\frac{W_{q}\\hat{z}^{v}(W_{k}\\hat{z}^{t})^{T}}{\\sqrt{d_{k}}})W_{v}\\hat{z}^{t}, (18)$\n$\\hat{z}_{l+1}^{t} = \\hat{z}^{t} + tanh(\\alpha^{t}_{ffn}). FFN(\\hat{z}_{l}^{t})(19)$\nwhere $\\alpha_{att}^{t}$ and $\\alpha_{ffn}^{t}$ are the respective gated-attention coefficients.\nAfter that, we concatenate $\\hat{z}_{l+1}^{v}$ and $\\hat{z}_{l+1}^{t}$, and apply a gated Transformer-based encoder (with two layers) to update these representations. They are further processed by a modality projector to obtain the multi-modal representations, say $z^{f} \\in R^{D*M}$, where M denotes the number of tokens, and D stands for feature dimensionality.\nConsidering the large-vocabulary property of forensic pathological analysis, the flexibility in aligning the multi-modal representation $z^{f}$ with the diagnostic outcomes is a key issue. Practically, an autopsy investigation could lead to multi-label outcomes from different views (or according to different downstream needs). For instance, given a post-mortem liver case, one could have the diagnoses of liver autolysis, cirrhosis, and rupture from the perspectives of post-mortem changes, diseases, and injury patterns, respectively. Thus, the multi-modal representation $z^{f}$ should be able to seamlessly align with each of them without bias to any particular outcome, a common limitation of many existing VLMs, like CLIP39,52\u201354. It is intuitive to assume that different diagnostic outcomes (after the embedding by a text encoder) are associated with varying parts of $z^{f}$, based on which we design an adaptive alignment strategy to flexibly align between the representations of multi-modal inputs and large-vocabulary outcomes. Specifically, any particular diagnostic outcome is regarded as a caption of the corresponding case and is mapped by the frozen PLIP and a learnable linear projector to obtain its caption embedding, say $z^{d} \\in R^{D*1}$. By calculating the cosine similarities between the normalized $z^{d}$ and $z^{f}$ across different tokens, we can obtain the attention scores (say $z \\in R^{1*M}$) indicating the contribution of each token to this diagnosis, such as equation (20):\n$z = \\frac{exp(\\frac{z^{d}|z^{f}}{\\left ||z^{d}\\right ||_{2} \\left ||z^{f}\\right ||_{2}})}{\\sum_{i=1}^{M}exp(\\frac{z^{d}|z^{f}}{\\left ||z^{d}\\right ||_{2} \\left ||z^{f}\\right ||_{2}})}(20)$\nwhere || . ||2 stands for L2-normalization. Finally, we leverage z to aggregate $z^{f}$, yielding $\\hat{z}^{f} = z^{t}(z^{f})^{T}$ to align with $z^{d}$. Notably, changing $z^{d}$ leads to changing $z^{t}$, thus different $\\hat{z}^{f}$.\nFinally, given $\\hat{z}^{f}$ and $z^{d}$ for training samples, we minimize the InfoNCE loss55 to update the learnable parts of this multi-modal fusion block, such as equation (21), equation (22) and equation (23):\n$L_{fd} = - \\sum_{i=1}^{N}log(\\frac{exp((\\frac{z^{d}|\\hat{z}^{f}}{\\left ||z^{d}\\right ||_{2} \\left ||\\hat{z}^{f}\\right ||_{2}})/\\lambda)}{\\sum_{j=1}^{N}exp((\\frac{z^{d}|\\hat{z}^{f}}{\\left ||z^{d}\\right ||_{2} \\left ||\\hat{z}^{f}\\right ||_{2}})/\\lambda)}(21)$\n$L_{d \\rightarrow f} = \\sum_{i=1}^{N} E_{log(\\frac{z^{d}|z^{f}}{\\left ||z^{d}\\right ||_{2} \\left ||z^{f}\\right ||_{2}}})}(22)$\n$L_{all} = \\frac{L_{fd} + L_{d \\rightarrow f}}{2}(23)$\nwhere N is the batch size and \u039b is the scaling temperature parameter.\nInference process\nThe pre-trained SongCi model serves as an auxiliary tool for forensic pathologists, facilitating diagnosis and analysis through zero-shot learning39. When presented with a case that includes multi-modal inputs, such as WSIs and macroscopic gross findings from various organs, a forensic pathologist can propose multiple diagnostic hypotheses from different perspectives, including post-mortem alterations, disease classifications, and injury patterns. SongCi then computes organ-specific multi-modal feature embeddings and evaluates their cosine similarity with each proposed diagnosis. The model ranks the most probable diagnoses based on normalized similarity scores, applying a predefined threshold to identify the top outcomes. In our research, we established the threshold at 0.88, corresponding to the point on the precision-recall (PR) curve where precision and recall are optimally balanced. In addition to the possible diagnostic results, fine-grained interpretable analytical factors are also available to forensic pathologists. This process enables them to scrutinize the interpretable results and refine their assessments.\nImplementation details\nA total of 3.15 million patches were extracted from the WSIs(the internal cohort) to train the pro- totypical whole slide image encoder. The network's backbone was initialized using DEiT-small56 and subsequently fine-tuned over 250 epochs with a mini-batch size of 768. Both the instance and proto- type projectors (projectorins & projectorpro) and the instance predictor (predictorins) share an identical architecture, comprising two fully connected layers, a batch normalization layer57, and a Gaussian error linear unit (GELU) layer58. These components were trained from scratch using Xavier initialization. The AdamW59 optimization algorithm was employed, starting with an initial learning rate of le \u2013 6 and concluding with a final rate of 1e \u20137. A cosine annealing strategy was applied to decay the learning rate. The prototype vectors were initially set to a dimensionality of 1,024, with their parameters being frozen at the outset and later included in the training after 5,000 iterations. Post-training, each patch of WSIs in the internal cohort was matched to the most similar prototype in the prototype space using only resizing for data augmentation. Upon reviewing all 3.15 million patches, it was found that 933 prototypes had corresponding patches.\nThe multi-modal fusion layer underwent training for 500 epochs, utilizing a mini-batch size of 64. The architecture comprises one text-to-image adapter, two cross-attention layers, two feedforward neural networks (FFNs), one transformer encoder, and one modality projector layer. The depth of the transformer encoder is 2. Following the settings in CLIP, the word embedding length is set to 77, while the prototype embedding length is 128. Embeddings are truncated or padded to maintain these specified lengths. To improve the generalization and robustness of the fusion block, random noise drawn from a normal distribution with a mean of 0 and a standard deviation of 0.5 was introduced to the embeddings (word and prototype). All training and experimental procedures were performed on a PC Server with eight NVIDIA GEFORCE RTX 3090 GPUs.\nPrototype visualization & patch-level post-mortem WSI generation\nWe adopted the Umap method28 to visualize in 2-D the prototypes learned by SongCi, as shown in Fig. 2a. Each prototype was marked by one or multiple color(s) according to the ratio of organ types of nearest image patches. To check the generalizability of the patch and prototypical embeddings produced by SongCi, we conducted a downstream patch-level post-mortem image generation task. Specifically, a diffusion model60 was trained to generate image patches conditioned on a prototype or patch embedding. Consistent with DDPM61 and DDIM62, the training of our diffusion model contains a forward process to add noises and a reverse process that learns to predict added noises. In the noise prediction, we incorporate conditions by combining learned embedding and time embedding:\n$t_{c} = f(e_{time-embedding})+g(e_{learned-embedding})(24)$\n$\\hat{X}_{t} = \\sqrt{\\bar{\\alpha_{t}}}x_{0} + \\sqrt{1 - \\bar{\\alpha_{t}}} \\epsilon(25)$\n$X_{t-1} = \\sqrt{\\bar{\\alpha_{t-1}}} ( \\frac{X_{t} - \\sqrt{1 - \\bar{\\alpha_{t}}} \\epsilon_{\\theta}(X_{t},t_{c})}{\\sqrt{\\bar{\\alpha_{t}}}}) + \\sqrt{1 - \\bar{\\alpha_{t-1}}} - \\sigma_{t}^{2} \\epsilon_{\\theta} + \\sigma Z (26)$\nwhere \u025b ~ N(0,1),\u03b5\u0473 is a Unet63, and f(.) and g(-) are two fully connected networks. The hyperpa- rameter & controls the generation process62, and a controls the accumulation of noise intensity at the moment (t) of the diffusion process.\nSelf-supervised WSI segmentation & diagnostic explainability analysis\nBased on the prototypes produced by SongCi, we conducted a downstream task of self-supervised post-mortem WSI segmentation. Specifically, we assume that each prototype encodes specific semantic information, and then each patch of a WSI was assigned a semantic label according to its nearest prototype. Notably, for a particular WSI, if the resulting prototype categories exceed a predefined threshold, we further cluster64 the number of prototypes to be equal to the threshold for comparison with other methods. In our experiments, the threshold was set as seven.\nIn the zero-shot diagnosis process, for a candidate outcome, the cross-modal fusion block of SongCi assigns respective attention scores to each patch of the input WSI as well as each word of the gross key findings, which provide fine-grained, cross-modal explanations regarding the network prediction. Specifically, we visualized the patches and words with the top five attention scores in our experiments.\nComparative analysis study\nIn our study, SongCi was compared with six state-of-the-art multi-modal fusion methods in the medical domain. For a fair comparison, all these methods were implemented under the same configurations with SongCi, such as the batch size and number of epochs, etc. Consistent with SongCi, these multimodal fusion methods adopt the embeddings of a WSI and respective gross key findings as the multimodal input, from which they learn fused representations in different ways to align with the diagnostic outcomes. These competing methods include:\n\u2022 Multimodal Co-Attention Transformer (MCAT)19: MCAT is a transformer-based model that designs a genomic-guided co-attention layer to fuse multimodal information. In this study, we replaced the input of genomic embedding with gross key findings. Other operations remained the same as in the original implementation of MCAT."}]}