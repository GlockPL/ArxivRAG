{"title": "Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search", "authors": ["Jinhao Jiang", "Zhipeng Chen", "Yingqian Min", "Jie Chen", "Xiaoxue Cheng", "Jiapeng Wang", "Yiru Tang", "Haoxiang Sun", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Dong Yan", "Jian Xie", "Zhongyuan Wang", "Ji-Rong Wen"], "abstract": "Recently, test-time scaling has garnered significant attention from the research community, largely due to the substantial advancements of the o1 model released by OpenAI. By allocating more computational resources during the inference phase, large language models (LLMs) can extensively explore the solution space by generating more thought tokens or diverse solutions, thereby producing more accurate responses. However, developing an o1-like reasoning approach is challenging, and researchers have been making various attempts to advance this open area of research. In this paper, we present a preliminary exploration into enhancing the reasoning abilities of LLMs through reward-guided tree search algorithms. This framework is implemented by integrating the policy model, reward model, and search algorithm. It is primarily constructed around a tree search algorithm, where the policy model navigates a dynamically expanding tree guided by a specially trained reward model. We thoroughly explore various design considerations necessary for implementing this framework and provide a detailed report of the technical aspects. To assess the effectiveness of our approach, we focus on mathematical reasoning tasks and conduct extensive evaluations on four challenging datasets, significantly enhancing the reasoning abilities of LLMs.", "sections": [{"title": "1 Introduction", "content": "In recent years, researchers have made substantial advancements in the capabilities of large language models (LLMs) by scaling both the training data and model parameters in accordance with the (training-time) scaling law [1, 2]. While LLMs are proficient in processing a wide range of human instructions, their performance remains limited in complex reasoning tasks, such as those encountered in STEM disciplines (including Olympiad-level mathematics, physics, and biology), coding, and medical diagnosis [3, 4, 5]. In response to these limitations, researchers have explored various strategies to improve the complex reasoning abilities of LLMs [6, 7, 8, 9, 10], including optimizations during both the training and test phases, or a combination of the two.\nSpecifically, training-time optimizations typically involve the design of targeted training strategies, utilizing curated datasets or tailored optimization objectives to enhance the capabilities of LLMs (a.k.a., policy models). These strategies range from single-turn methods (e.g., direct preference optimization [11]) to iterative approaches (e.g., self-improvement training [12, 13, 14]). In contrast, test-time scaling expands the reasoning space by generating additional tokens (e.g., chain-of-thought reasoning [8], CoT) or solutions (e.g., self-consistency [9], SC). This approach aims to enhance problem-solving performance by allowing more inference costs, simulating the slow thinking process of System 2 [15]. It often integrates search based techniques such as beam search [16] and Monte Carlo Tree Search (MCTS) [17, 18] to more effectively identify improved solutions. To well support test-time scaling, a capable reward model is often required\u2014either process-based or outcome-based to guide the learning of the policy model through feedback signals, which is considered essential for search-enhanced reasoning. In the early stages, training-time optimizations were the primary focus of the research community; however, more recently, test-time scaling has gained greater prominence, particularly following the release of OpenAI's ol model 3. 01 has demonstrated notable improvements across a wide range of challenging evaluation benchmarks, spanning from programming to scientific disciplines. According to the official blog, o1 is characterized by conducting long internal thoughts of chains prior to responding, which allows for performance gains through increased train-time compute (i.e., reinforcement learning) and extended test-time compute (i.e., thinking during inference).\nDespite significant advancements, OpenAI has not disclosed the core technology underlying the o1 model. Consequently, recent research efforts have focused on uncovering the \u201csecret\" behind its approach to enhancing complex reasoning capabilities of LLMs [17, 19, 20, 21, 14]. Generally speaking, most of these efforts aim to emulate the working mechanisms of AlphaGo (or its enhanced version, AlphaZero) and conceptualize a search-based reasoning framework comprising three key components: the policy model, reward model, and search algorithm. Within this framework, the policy model learns to solve complex tasks by generating reasoning steps (or thoughts) guided by the search algorithm, with its performance further refined and directed by the reward model. These efforts have yielded promising results, though they have not yet reached the level of performance demonstrated by o1.\nImplementing a search-based reasoning framework, as demonstrated in prior studies, often entails a multitude of design considerations or technical details, As a result, reproducing an o1-like system is not trivial, even when performance is not the primary objective. A comprehensive report detailing the exploration of various implementation aspects would be useful in guiding future research in this area. In fact, several studies and projects [22, 23] have been dedicated to serving this purpose. In parallel with these works, we present a technical report documenting our team's preliminary exploration of a reward-guided tree search framework for improving LLM reasoning. Our framework is primarily constructed around a tree search algorithm, where the policy model navigates a dynamically expand-ing tree, guided by a specially trained reward model. We thoroughly explore the various design considerations necessary to implement this framework and report the technical details. To summarize, we list the major attempts at each aspect below:\n\u2022 For the policy model, we investigate how to adapt it to the reasoning format defined by the search tree structure. Additionally, we outline how to perform preference optimization using specially constructed training data, under the guidance of the reward model.\n\u2022 For the reward model, we examine several key design considerations, including the selection of discriminative or generative forms, outcome- or process-supervised training, and ranking-or score-based optimizations. We provide comprehensive training details and also explore how to conduct iterative mutual refinement with the policy model.\n\u2022 For tree search, we implement MCTS-like algorithm to support the reasoning of the policy model, guided by the reward model. We explore enhancements in both effectiveness and efficiency to make it well-suited for mathematical reasoning tasks.\nTo validate the effectiveness of our implemented framework, we conduct extensive evaluations on four challenging mathematical benchmark datasets: MATH-OAI [24], GSM-Hard [25], Olympiad Bench [26], and College Math [27]. Experimental results demonstrate that our reasoning framework significantly enhances the performance of the policy model on these datasets. Additionally, we conduct an in-depth empirical analysis of the design of the policy model, reward model, and tree search algorithm, aiming to provide researchers with meaningful guidance."}, {"title": "2 Method", "content": "In this work, we focus on the mathematical domain, specifically addressing mathematical problems described in the text. We implement a reward-guided tree search framework designed to enhance the reasoning capabilities of LLMs. This framework consists of three main components: the policy model, the reward model, and the search algorithm. Within our framework, the policy model generates new reasoning steps based on a partial solution prefix along a given path in the search tree. The search algorithm constructs the search tree to structure the reasoning process, while the reward model provides feedback signals to guide the policy model's actions and the search process. This approach allows the policy model to explore a broader reasoning space in a more informed manner, increasing the likelihood of finding the correct answer. Overall, it trades test time for improved accuracy. In the following sections, we will provide detailed descriptions of the implementation of these three key components."}, {"title": "2.1 Policy Model", "content": "In this section, we provide a detailed description of the training process for the policy model, which primarily consists of two steps: instruction tuning for reasoning format adaptation (Section 2.1.1), and preference optimization for policy improvement (Section 2.1.2)."}, {"title": "2.1.1 Instruction Tuning for Reasoning Format Adaptation", "content": "As we perform the reasoning using a tree-structured search algorithm, this necessitates adapting the reasoning format of the policy model.\nDefining the Reasoning Format. To define the reasoning format, we first set the granularity of each reasoning step, i.e., how each node is represented, framing the multi-step reasoning process as a node-based search within a tree structure. Previous studies have extensively explored reasoning steps at both the token and sentence levels [28, 29]. However, in mathematical contexts, a logically complete step in problem-solving may involve multiple sentences. Therefore, we model a complete logical step within the mathematical problem-solving process as a node in the tree. Below is the output template for our reasoning process."}, {"title": "2.1.2 Preference Optimization for Policy Improvement", "content": "By aligning the policy model with the desired reasoning format, we can better control the step-by-step generation of the reasoning process. This alignment facilitates both the execution of tree search algorithms and the assessment by the reward model. Next, we conduct preference optimization on the policy model using feedback from the reward model (Section 2.2), thereby achieving policy improvement. This process involves two key steps: constructing the training data and performing preference optimization. Next, we introduce the implementation of these two steps.\nTraining Data Construction. To improve the policy model, we construct the corresponding training data through the following three steps: first, sample multiple solutions for each question from the policy model; second, filter out low-quality solutions; and third, create positive and negative sample pairs based on scores from the reward model and the annotated labels. Specifically, given a set of mathematical problems $Q = \\{(q_i, S_i)\\}_{i=1}^N$, we begin by solving each problem $q_i$ using the aforementioned policy model after adapting its reasoning format. During the generation process, step-level diversity is crucial, as sufficiently varied paths are essential for exploring different actions within the tree. To address this, we increase the temperature parameter to enhance sampling diversity, thereby obtaining more diverse solution paths $\\{\\hat{s}_{i,j}\\}_{j=0}^k$. Next, we apply a rule-based approach to remove samples that contain garbled content or deviate from the specified reasoning format. We then annotate the remaining samples with correctness labels by comparing the predicted answers to the ground-truth $s_i$, and only retain those problems with both correct and incorrect samples. Subsequently, we score each retained sample using the reward model and select the top-ranked correct and incorrect samples to form positive-negative sample pairs. This approach aims to select highly confident positive samples and incorrect samples that are nearly correct, providing more informative signals for preference optimization. Similar strategies have been also used in previous work [13, 14, 12, 18]. Additionally, we sample only one pair of positive and negative examples per problem to increase the diversity of the training data and prevent overfitting. Ultimately, we obtain the training data set $\\{(q_i, s_i^+, s_i^-)\\}_{i=0}^N$.\nPreference Optimization. After obtaining the training data, we apply the chat template and prompt to the training data and then conduct preference optimization on the policy model $P$ using the direct preference optimization algorithm [11] as follows:\n$L =  -log \\sigma (\\beta log \\frac{\\pi(s_i^+|q_i; \\Theta)}{\\pi(q_i; \\Theta_{ref})} +  \\beta log \\frac{\\pi(s_i^-|q_i; \\Theta)}{\\pi(s_i^-|q_i; \\Theta_{ref})})$ (1)\nwhere $\\sigma(\\cdot)$ denotes the sigmoid function, $\\Theta_{ref}$ denotes the parameters of the reference model (i.e., the policy model itself), and $\\pi$ denotes the generative probability of a sample text given the question and the corresponding model."}, {"title": "2.2 Reward Model", "content": "In this section, we first identify the key design considerations in implementing the reward model, and then introduce its detailed training process."}, {"title": "2.2.1 Key Design Considerations in Reward Modeling", "content": "Since the reward model (RM) provides feedback signals to guide the policy model's reasoning within the search framework, it plays a crucial role in directing the reasoning process of LLMs. Generally, a reward model can be implemented using various types of machine learning models [32, 33, 34]; however, in our work, we focus on using LLMs as the backbone model. To design an effective RM, we concentrate on the following three key considerations.\nDiscriminative RM v.s. Generative RM. When implementing reward models for LLM reasoning, we can consider using either discriminative RM or generative RM. In existing work [35, 34, 31], discriminative RM has been widely utilized to provide supervision signals, which projects the model's hidden state into a score that serves as the supervision signal. In contrast, generative RM takes a specific prompt as input (i.e., guidance for evaluation and the text to be evaluated) and generates a descriptive evaluation of the quality for the provided solution, potentially accompanied by associated reward scores [36, 37, 38]. A direct implementation of generative RM involves constructing assessment prompts, such as \u201cIs the answer correct (Yes/No)?\u201d, and then using the prediction probabilities of the assessment tokens (e.g., the probability of \u201cYes\u201d) as the reward score. Compared to discriminative RM, a potential advantage of generative RM is that it can leverage the learned knowledge of the base model more effectively, as its training process closely aligns with both the pre-training and post-training phases of the model.\nOutcome-supervised RM v.s. Process-supervised RM. The second issue we consider is the granularity of the supervision signals provided by the reward models. Typically, we can use either outcome-supervised reward models (ORM), which assess the correctness of the entire solution, or process-supervised reward models (PRM), which evaluate the correctness of intermediate steps in the reasoning process [24, 35, 39]. To develop these two types of reward models, it requires labeled data at different granularities: solution-level labels and step-level labels. In practice, solution-level labels are relatively easy to obtain when ground-truth answers are available for the tasks, whereas step-level labels are very difficult to annotate. To automatically obtain process-supervised signals, we can employ rollout-based methods (e.g., Math-Shepherd [35]) to derive step-level labels from ground-truth answers. The core idea is to annotate each reasoning step based on whether correct answers can be derived from it through a rollout process. However, rollout-based methods typically require more computation time, particularly when the process is repeated multiple times. As a result, this work focuses on training outcome-supervised reward models, primarily relying on outcome-level supervision signals to guide the reasoning process. Interestingly, we find that the reward model trained with solution-level labels also has potential in assessing step-level correctness.\nRanking-based RM v.s. Scoring-based RM. To train reward models, we can explore different optimization objectives, which are generally either ranking-based or scoring-based. Specifically, a ranking-based RM is trained to identify which candidate response is the best, focusing primarily on the preference order among candidate solutions. In contrast, a scoring-based RM assigns an absolute reward score for a single response based on its assessed quality level. The following equations illustrate the comparison between ranking-based and scoring-based approaches:\n$Rank(r_1, r_2, r_3) \\rightarrow r_1 > r_3 > r_2;$ (2)\n$Score(r) \\rightarrow 0.8.$ (3)\nSince our reasoning framework relies on the guidance of concrete scores for solution evaluation, we adopt the scoring-based optmization objective for training our RM."}, {"title": "2.2.2 Training Data Construction", "content": "To effectively train the reward model, we construct a high-quality dataset with outcome-level supervision signals, using rule-based methods to clean and filter the training data.\nData Collection. In order to provide accurate guidance to the policy model, we curate the training instances directly from the content generated by the corresponding policy model. We begin by sampling multiple candidate solutions $\\{\\hat{s}_{i,1},..., \\hat{s}_{i,k}\\}$ for each problem in the training set. To ensure diversity among the solutions, we adjust hyperparameters such as temperature and top_p to promote more varied outputs. After obtaining the generated solutions for each problem, these candidates are labeled according to the ground-truth answer $s_i$. The label $l_{i,j}$ indicates whether the candidate solution $\\hat{s}_{i,j}$ is correct or incorrect. The resulting labeled instances constitute the original training dataset for the reward model, denoted as $D_O = \\{(q_i, \\hat{s}_{i,j}, l_{i,j})\\}$.\nData Cleaning. Although we adjust the hyperparameters for generation to maintain diversity among the generated solutions, some candidate solutions may still be highly similar, potentially leading to overfitting in the reward model. To address this issue, we perform data cleaning by removing redundant solutions. Specifically, we iteratively eliminate responses that have a high n-gram overlap with previously selected responses. Additionally, we balance the dataset by ensuring an equal number of correct and incorrect solutions for each problem. This is achieved through random sampling, which helps prevent potential bias in the training dataset. After applying the filtering and balancing process, the remaining solutions are used to construct the training instances. The final cleaned training dataset is denoted as $D_1 = \\{(q_k, \\hat{s}_k, l_k)\\}$."}, {"title": "2.2.3 Reward Model Training", "content": "After constructing the training data, we leverage these data to train the generative reward model R, which can provide outcome-level absolute scores based on the problems and generated solutions.\nDomain Adaptation. For the backbone model of the reward model, we select LLaMA-3.1-8B-Instruct, which is the same choice as for the policy model in our framework. Although LLMs have demonstrated remarkable performance on various text generation tasks, they still face challenges in complex mathematical reasoning tasks. To address this, we fine-tune the backbone model with mathematical instructions, enabling it to better adapt to mathematical scenarios and tasks. Similar to the fine-tuning process for the policy model, we randomly sample 70K problems from the NuminaMath dataset and use Qwen2.5-Math-72B-Instruct to generate solutions for each problem, thereby enhancing the quality of the training data. Incorrect solutions are then removed from the training set, and the remaining high-quality data is used to fine-tune the original model, resulting in a more capable backbone model.\nGenerative Training. The generative RM is trained to evaluate the correctness of the solution based on the given problem, by generating the assessment text in natural language. To further leverage the advantages of the generative RM, we design a prompt template that guides the reward model to generate an evaluation of a given solution. The template is as follows:"}, {"title": "2.2.4 Iterative Training for Mutual Evolution", "content": "In our framework, the policy model and the reward model are two highly interrelated components: their training involves using data generated or selected by one another, and they jointly perform the reasoning process to arrive at the final solution. Consequently, we explore the possibility of mutual improvement for these two components and iteratively refine the model capacities.\nSpecifically, let $P_0$ and $R_0$ denote the initial policy model and reward model (after format and domain adaptation), respectively. At the i-th iteration, policy model $P_{i-1}$ first generates the candidate solutions based on the problems in the training dataset. Then, these generated solutions and problems are used to compose the original training dataset for training the reward model $R_{i-1}$ and obtain an improved reward model $R_i$. Next, we leverage the new reward model $R_i$ to score the original training dataset and perform preference optimization on policy model $P_{i-1}$ to obtain the improved policy model $P_i$. Finally, the new policy model $P_i$ and reward model $R_i$ will be utilized as the"}, {"title": "2.3 Search Algorithm", "content": "In this section, we first describe the overall process of search algorithm using the specially trained pol-icy model and reward model (Section 2.3.1), followed by two improvements for search performance (Section 2.3.2)."}, {"title": "2.3.1 Overall Process", "content": "We perform the search process by executing a certain number of search steps. Each search step consists of four main operations: selection, expansion, simulation, and backpropagation. Next, we introduce each operation in detail.\nSelection. We first consider the selection method that follows the standard MCTS algorithm [41]: starting from the root node, the algorithm traverses the tree by selecting child nodes based on the Upper Confidence Bound (UCB) value, which is a widely used criterion that balances exploration and exploitation. Let $s_t$ denote the t-th node. The UCB value of a node to be selected is determined by two key variables: $V(s_{t+1})$, the reward value of the candidate child node given by the reward model R, and $N(s_t)$, the visit frequency of $s_t$. At each step, the child node with the highest UCB would be chosen. This process can be formally represented as:\n$s_{t+1} = \\underset{s_j \\in Children(s_t)}{arg \\ max} [V(s_j) + c \\sqrt{\\frac{log\\ N(s_t)}{1 + N(s_j)}}],$ (6)\nwhere c is a constant that determines the level of exploration. This operation repeats until a leaf node is reached. For selection, MCTS locally selects from the child nodes of the current node, while we further design another selection method that considers all the leaf nodes of the current search tree. Concretely, the algorithm begins by collecting all leaf nodes of the current search tree and calculating the average value $\\mu$ and standard deviation $\\delta$ of their reward values. A threshold $\\rho$ is then dynamically calculated based on $\\mu$ and $\\delta$. Finally, nodes with a reward value V exceeding $\\rho$ will be selected. The calculation method is outlined as follows:\n$\\mu = \\frac{1}{N_{leaf}} \\sum_{s \\in leaf} V(s),$ (7)\n$\\delta = \\sqrt{\\frac{1}{N_{leaf}} \\sum_{s \\in leaf}(V(s) - \\mu)^2},$ (8)\n$\\rho = \\mu + \\lambda \\cdot \\delta,$ (9)\nwhere $\\lambda$ is a constant that specifies the extent to which $\\rho$ exceeds $\\mu$, and $N_{leaf}$ denotes the number of leaf nodes in the current search tree. We refer to this modified selection method as MCTSG, and will further compare the performance of MCTS and MCTSG in Section 3.5.1.\nExpansion. If the selected leaf node is not in a terminal state (i.e., the final answer has not been generated), the node will be expanded in depth with k child nodes $\\{s_c\\}$, by decoding for one step using the policy model. The initial values of these newly expanded child nodes will be determined through the following simulation operation.\nSimulation. This operation employs rollout samples to estimate the potential value of the expanded child node $s_c$. Concretely, from the selected child node $s_c$, the policy model P performs n rollouts (i.e., generates a complete solution $\\tau^{(i)}$ based on the current node's state). The reward model R then evaluates the correctness of each rollout trajectory by computing a scalar value. The evaluation results are then averaged to determine the initial reward value V of the selected child node. This process can be formally represented by the following equation:\n$V(s_c) = \\frac{1}{n} \\sum_{i=1}^{n} R(\\tau^{(i)}).$ (10)"}, {"title": "Backprogation.", "content": "Once the initial reward values of the expanded child nodes $\\{s_c\\}$ are evaluated, they are propagated back up the tree. Each node along the path updates its statistics, such as the visit count N and the state value V, which will inform future selections. The process can be formally defined as:\n$V(s_t) \\leftarrow \\frac{N(s_t) \\cdot V(s_t) + \\sum V(s_c)}{N(s_t) + k},$ (11)\n$N(s_t) \\leftarrow N(s_t) + k.$ (12)\nThese operations are repeated for a predetermined number of iterations or until a time limit is reached, allowing the algorithm to build a question-dependent search tree. Additionally, to accelerate the search process and leave more exploration space for subsequent search at the beginning of the simulation, we also implement pre-expansion before starting the simulation.\nPre-expansion. Pre-expansion involves expanding a significant number of nodes in the initial layers of the search tree before the standard search process begins. Instead of selecting the most promising leaf node and expanding it, we expand all nodes in the initial layers, to enhance the coverage of correct actions. After this expansion, the algorithm proceeds with the aforementioned operations of selection, expansion, simulation, and backpropagation."}, {"title": "Discussion on Search Algorithms.", "content": "The essence of incorporating search algorithms in the reasoning framework is to help effectively and efficiently expand the solution space that LLMs can explore. In principle, we can employ various search algorithms to improve the reasoning performance of LLMs. In this work, we mainly consider three search algorithms: MCTS, MCTSG, and beam search. MCTS has been widely used in various games [32] and has now been adapted to instruct the reasoning of LLMs. It focuses on analyzing the most promising actions from the current state and expanding the search tree based on random sampling of the search space. However, it may produce suboptimal solutions due to its local selection strategy, and the inherent randomness may affect search effectiveness. To potentially overcome this limitation, MCTSG expands the selection scope to all the leaf nodes of the current tree. However, the algorithm may waste computational resources by pursuing incorrect paths, leading to a deeper exploration of high-value paths in complex or error-prone problems. In comparison, beam search is a simplified search algorithm that explores a fixed number of the best candidates at each step. While it benefits from parallelizability, which accelerates the search process, it may yield suboptimal results due to its limited exploration."}, {"title": "2.3.2 Performance Optimization", "content": "In this part, we discuss the optimization strategies for improving the performance of the search framework.\nSelf-consistency Enhancement with Rollout Samples. During the tree search process, we perform rollouts to estimate the value of nodes, which results in a large number of samples with complete solutions. These samples can be leveraged to improve reward evaluation. Based on this idea, in addition to relying on the reward model to score the steps generated by the model, we also calculate the self-consistency (SC) score of the answers produced by the model itself, a proven effective indicator in existing studies [42]. Specifically, when calculating the reward for each sample $\\tau^{(i)}$, we combine the reward model's score with the SC score derived from all historical rollout samples:\n$R^{+}(\\tau^{(i)}) = (1 - \\alpha) \\cdot R(\\tau^{(i)}) + \\alpha \\cdot SC(\\tau^{(i)}),$ (13)\nwhere a weighted factor $\\alpha$ is applied to the SC score that returns the proportion of the answer of $\\tau^{(i)}$ in the historical rollout samples. We empirically find that SC often provides accurate predictions of the ground-truth answer, particularly when the question difficulty is relatively low. Overall, it serves as a valuable indicator for assessing the correctness of candidate answers.\nTool Manipulation. In each problem-solving step, the policy model may introduce calculation errors. While these errors may appear insignificant individually, they can accumulate and disrupt the solution process if left unchecked. Unlike logical mistakes, these miscalculations often go undetected by reward models and self-consistency checks, as they are not specially designed to"}, {"title": "3 Experiments", "content": "In this section, we conduct experiments to examine the effectiveness of the implemented framework."}, {"title": "3.1 Evaluation Setup", "content": "To demonstrate the effectiveness of our framework, we conduct experiments on four challenging mathematical benchmarks: MATH-OAI [24], GSM-Hard [25], OlympiadBench [26], and College Math [27]. The test set sizes for the four benchmarks are 500, 1319, 675, and 2818, respectively. To save testing time, we randomly sample 500 samples from each of the last three benchmarks for evaluation. We select LLaMA-3.1-8B-Instruct [44] as the backbone model for both the policy and reward models because it demonstrates excellent overall capabilities and does not saturate these benchmarks. For each benchmark, we employ the same evaluation tools as those used in prior research [31] and report the average performance of various methods over all test problems. Aside from the main experiments, we primarily report results on the MATH-OAI dataset, unless otherwise specified."}, {"title": "3.2 Main Results", "content": "3.2.1 Overall Performance Comparison\nAs the main experiments, we compare four methods based on the same backbone model: zero-shot COT (CoT) with fine-tuned policy model, best-of-N chosen by the reward model (BoN, N = 100), and our tree search based reasoning framework (T-Search). Specially, We utilize LLaMA-3.1-8B-Instruct LLM as the backbone model for policy model and reward model. And we utilize the same prompt for all methods as described in Section 2.1.1.\nWe first present the overall performance comparison of the four methods on the selected evaluation benchmarks. As shown in Table 1, the CoT strategy can improve the performance of the original chat model to some extent, as these test tasks require enhanced step-by-step reasoning capabilities. Among these methods, search-based approaches (i.e., BoN and T-Search) deliver more superior performance because they effectively expand the solution space for challenging reasoning tasks. Finally, our framework achieves the best performance among all methods, enhancing the reference baseline by 46.9%, 7.3%, 91.6%, and 31.4% on the four evaluation benchmarks, respectively. These results indicate that our framework effectively enhances the reasoning capabilities of LLMs on complex mathematical tasks."}, {"title": "3.2.2 Results of Iterative Training", "content": "In this section, we examine how the performance of the policy model and reward model evolves during the multi-turn iterative training process. As described in Section 2.1.2 and Section 2.2.2, both the training of the policy and reward models involves data selection based on the reward model. Therefore, in this part, we conduct a comparison experiment with random selection to highlight the superiority of the reward-based selection method. For the detailed analysis of the selection strategies of the reward model, we conduct further ablation experiments in Section 3.4.2. We execute two iterations and present the results in Table 2.\nFirst, it can be observed that the reward-based selection method outperforms random selection in improving the performance of both the reward and policy models. This indicates that feedback from the reward model not only facilitates its own activate learning (Section 3.4.2) but also aids in the preference optimization of the policy model (Section 2.1.2). Second, across two iterations, we can observe continual improvements for both policy and reward models on almost all datasets, suggesting that they can mutually enhance each other during the iterative training, which demonstrates the effectiveness of our framework."}, {"title": "3.3 Further Analysis of Policy Model Training", "content": "In this part, we focus on examining the effect of policy model training. We adopt three evaluation metrics: accuracy (ratio of the correctly solved test problems by direct generation), maj\u00a910 (ratio of the correctly solved test problems by majority vote among ten generated solutions), and pass@10 (recall of the correctly solved test problems among ten generated solutions). The latter two are regarded as coverage metrics for the generated solutions. Next, we give detailed results and analysis."}, {"title": "3.3.1 Effect of Data Synthesis Model", "content": "Recall that we use a more capable model for data synthesis. We now examine the effect of differ-ent data synthesis models by comparing LLaMA-3.1-8B-Instruct, Qwen2.5-72B-Instruct [45], and Qwen2.5-Math-72B-Instruct [31], which represent the policy model itself, a strong general-purpose model, and a strong domain-specific model, respectively. We fix the total amount of formatted synthetic data at 10K and compare the performance of using different models. As shown in Table 3, the results indicate that self-generated data offers little improvement and may even lead to perfor-mance degradation, whereas data generated by strong models significantly enhances performance. Additionally, data synthesized by the domain-specific model does not show significant differences compared to the general-purpose model, with slight improvements observed on certain datasets.\nThese results indicate two key points: First, when the foundational model has strong capabilities, a higher-quality solution process is necessary to further enhance its complex reasoning abilities. This can be achieved by distilling higher-quality instruction data using a larger LLM. Second, the help from a larger general LLM is comparable to that of a larger domain-specific LLM for the policy model, reducing reliance on the latter."}, {"title": "3.3.2 Effect of Training Data Scaling", "content": "We further investigate how the amount of synthetic data affects model performance. Specifically, we conduct experiments using varying amounts of synthetic data from the Qwen2.5-Math-72B-Instruct, including 5K, 10K, 20K, 30K, and 40K, while keeping the other settings fixed. Figure 2 displays the tuning results achieved with different amounts of synthetic data. Compared to the original policy model, using more synthetic data for format adaptation overall enhances the reasoning performance of the policy model. However, as the amount of training data increases, the performance gains become less significant or may even decline. On the other hand, the model's pass@k fluctuates within a narrow range, which does not bring significant improvements.\nConsidering the balance between the cost of synthetic data and downstream performance, we utilize 10K training data for reasoning format adaptation in our framework."}, {"title": "3.3.3 Performance Improvement on Self-consistency", "content": "To further evaluate the effectiveness of our training method for the policy model, we conduct self-consistency experiments to assess how an improved policy model contributes to model performance using the self-consistency (SC) strategy. We select the SC strategy because we empirically find that it performs very well when employing a sufficient number of rollouts. Specifically, we compare the policy model with and without (i.e., the original model) optimization using our training method, while varying the sampling budget (i.e., the number of rollouts). Additionally, we include the performance of the overall reasoning framework (i.e., T-Search in Table 1) as a reference. From Figure 3, we can see that our training method significantly enhances SC performance by improving the underlying policy model, highlighting its effectiveness. Moreover, SC with the enhanced policy model acts as a very strong baseline, achieving high performance yet still underperforming compared to our entire reasoning framework."}, {"title": "3.4 Further Analysis of Reward Model Training", "content": "In this section, we conduct detailed analysis on the impact of different training strategies for the reward model."}, {"title": "3.4.1 Effect of Model Adaptation Strategies", "content": "Building on the original backbone model", "strategies": "domain adaptation and format adaptation.\nDomain Adaptation. As detailed in Section 2.2.3, the backbone model was"}]}