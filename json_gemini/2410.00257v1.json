{"title": "The age of spiritual machines: Language quietus induces synthetic altered states of consciousness in artificial intelligence", "authors": ["Jeremy I Skipper", "Joanna Kuc", "Greg Cooper", "Christopher Timmermann"], "abstract": "How is language related to consciousness? Language functions to categorise perceptual experiences (e.g., labelling interoceptive states as 'happy') and higher-level constructs (e.g., using \u2018I' to represent the narrative self). Psychedelic use and meditation might be described as altered states that impair or intentionally modify the capacity for linguistic categorisation. For example, psychedelic phenomenology is often characterised by \u2018oceanic boundlessness' or 'unity' and 'ego dissolution', which might be expected of a system unburdened by entrenched language categories. If language breakdown plays a role in producing such altered behaviour, multimodal artificial intelligence might align more with these phenomenological descriptions when attention is shifted away from language. We tested this hypothesis by comparing the semantic embedding spaces from simulated altered states after manipulating attentional weights in CLIP and FLAVA models to embedding spaces from altered states questionnaires before manipulation. Compared to random text and various other altered states including anxiety, models were more aligned with disembodied, ego-less, spiritual, and unitive states, as well as minimal phenomenal experiences, with decreased attention to language and vision. Reduced attention to language was associated with distinct linguistic patterns and blurred embeddings within and, especially, across semantic categories (e.g., 'giraffes' become more like 'bananas'). These results lend support to the role of language categorisation in the phenomenology of altered states of consciousness, like those experienced with high doses of psychedelics or concentration meditation, states that often lead to improved mental health and wellbeing.", "sections": [{"title": "Introduction", "content": "The Ego Dissolution Scale (EDS) has two subfactors, 'unity' and 'ego-loss', states that are often reported after high doses of psychedelics and by experienced meditators\u00b9. Given her description in the epigraph, one might assume JB Taylor was either a frequent psychedelic user or an advanced meditator. However, this quote is from Dr Taylor's book, My Stroke of Insight, in which she describes the experience of a cerebrovascular event that led to global aphasia, an inability to speak or understand language\u00b2. There are many similar anecdotes\u00b9 of people deprived of or having lost language, describing their experience upon acquiring or recovering language. These suggest that language plays a mechanistic and causal role in determining the phenomenology of more \u2018typical' and altered states of consciousness4,5.\nIndeed, the empirical evidence for this position is amassing. Behavioural studies suggest that words facilitate the conscious experience of non-linguistic stimuli, through categorically organising relevant sensory information. For example, colour, motion, and object words promote the detection of visual colour, motion, and objects, even bringing them into consciousness. This is supported by neurobiological data, with damage and aphasia, split-brain patients, intracarotid anaesthetic, and disorders of consciousness all suggesting language is causally related to consciousness\u201c. For example, injection of an anaesthetic into the left or right intracarotid artery is more likely to result in unconsciousness in individuals more left or right lateralised for language functioning, respectively.\nFurther supporting the link between language and consciousness, numerous studies have shown a similar relationship between language and psychedelic-induced altered states. Evidence suggests that these experiences often involve ineffability\u2014an inability to adequately convey experiences in words8-10. This might manifest as slower, reduced, and less complex language output11-14. When language is engaged, it tends to be more unusual and bizarre, less predictable, and involves altered"}, {"title": "Mechanisms", "content": "How does language play a role in inducing altered states? We have proposed that the neurobiology of language is a whole brain process that connects words processed in auditory sensory regions to distributed regions of the brain involved in sensorimotor processing (e.g., auditory, visual, motor, somatosensory, and interoceptive regions) and memory (e.g., regions involved in autobiographical memory)4,31. Specifically, words flexibly organise our perceptual and more general experience of the world. This involves categorically organising external auditory and visual information (e.g., into colours, faces, and objects). Language also categorically organises internal information, like labelling lower-level interoceptive processing with emotional words, but also higher-level categories like our 'narrative self', largely involving stories constructed and manipulated with words. These categorical processes are established with learning and can become positively or negatively entrenched.\nFrom this perspective, psychedelic phenomenology may be explained by the breakdown of the categorical functions of language45. That is, brain networks involved in language processing might cease to function typically and/or function at a reduced level or even be temporarily suspended altogether. Specifically, we suggest that auditory cortical activity decreases, diminishing or eradicating labels or word forms32,33. This occurs with hyperactivity or a decrease in inhibition in core frontal brain regions involved in semantic selection and retrieval34\u201336. This combination of processes in distributed language-related brain circuits disrupts entrenched linguistic categories, resulting in the reviewed"}, {"title": "Hypotheses", "content": "We sought to test this theoretical framework by generating artificial alternate states of consciousness in machine intelligence. Indeed, our theory conceptually aligns well with multimodal deep neural networks like the CLIP (Contrastive Language-Image Pre-training) and FLAVA (Flow-based Language and Vision Attention) models. Both utilise shared semantic embedding spaces to relate text and image modalities. These models capture linguistic and sensory interactions akin to those in distributed language networks of the human brain. Just as we suggest that language categorically organises sensory, motor, and interoceptive experiences, CLIP and FLAVA map language content and visual input onto a shared space, mirroring the process by which we suggest language shapes our subjective experience. CLIP, trained on an extensive dataset of image-caption pairs, models how language forms and maintains perceptual categories. FLAVA, with its multimodal encoder, captures a more complex interplay between language and sensory processing. Crucially, these models possess manipulable text and image \u2018attention' parameters. By manipulating weights in these shared semantic embedding spaces, we might be able to simulate the altered language functions observed in psychedelic and meditative states, potentially leading to a simulacrum of unitive states, ego dissolution, and minimal phenomenal experiences."}, {"title": "Results", "content": "To evaluate our proposal that language quietus induces synthetic altered states of consciousness, we focused on comparing factors from questionnaire-based metrics of altered states. We emphasised contrasts between the Anxiety and Unity factors of the ASC, as they appear to represent opposing experiences. We first demonstrate, qualitatively and quantitatively, that these factors produce distinct degradation patterns in models as a function of attention to text and images. We then applied searchlight analyses to precisely map how these attentional manipulations affected different factors. Finally, we analysed the associated language features and examined category coherence within and across states to further understand these effects."}, {"title": "Similarity", "content": "Figure 2 visualises the relationship between prompt and response cosine similarity scores as a function of text and image attentional weights. It shows that similarity does not degrade uniformly; instead, the largest reduction in similarity occurs with negative text and image weights (i.e., quadrant one or Q1) across all ASC factors (Figure 2, left column). This pattern varies by specific altered state factor, though it remains relatively consistent across different models. Comparing the top and bottom panels of the second and third columns of Figure 2, both the CLIP and FLAVA models show that the Anxiety factor of the ASC differs from the Unity factor in all quadrants, in a relatively similar manner across models. Factors from other altered state questionnaires show subtle but distinctly different patterns (Figure 2, last column).\nTo qualitatively visualise the differential degradation of similarity, we found the medium difference between prompt and random cosine similarity scores for each of the 11 ASC factors in each of the four quadrants (Figure 3). Overall, different quadrants grossly load differently on different ASC factors, with Q1 and Q2 showing the largest variations."}, {"title": "GAM", "content": "We next sought to quantify the observed variable relationships between text and image attention weights and cosine similarity scores for the CLIP and FLAVA models as they related to different altered states. The results of Generalised Additive Model (GAM) analyses are presented in Figure 4."}, {"title": "CLIP", "content": "The top row of Figure 4 presents the GAM model results for CLIP, comparing all ASC factor scores to the random score. The model showed a strong fit, with a pseudo-R-squared value of 0.9099, indicating that 90.99% of the variance in the cosine similarity scores was explained by the model. Both text weight (s(0)) and image weight (s(1)) showed significant non-linear effects (p < 1.11 \u00d7 10-16), highlighting the complex relationships between these weights and the similarity scores. The main effect of Score Type (f2)) was also significant (p < 1.11 \u00d7 10-16), indicating an overall difference between ASC and random scores. Moreover, the interaction effects between text weight and score type (te(0,2)) and image weight and score type (te(1,2)) were both significant (p < 1.11 \u00d7 10-16), suggesting that the differences between ASC and random scores depended on specific attentional weight conditions."}, {"title": "FLAVA", "content": "The bottom row of Figure 4 presents the GAM model results for FLAVA, comparing ASC Anxiety and ASC Unity factor scores. The model fit was strong, with a pseudo-R-squared value of 0.8536, indicating that 85.36% of the variance in the similarity scores was explained by the model. The text weight (s(0)) and image weight (s(1)) effects were both significant (p < 1.11 \u00d7 10\u201316), demonstrating non-linear relationships between attentional weights and similarity scores. The main effect of Score Type (f(2)) was also significant (p = 7.46 \u00d7 10-7), indicating an overall difference between ASC Anxiety and Unity factor scores. In addition, both interaction effects (te0,2) and te(1,2) were significant (p < 1.11 \u00d7 10\u201316), suggesting that the differences between ASC Anxiety and Unity factors depended on specific attentional weight values."}, {"title": "Searchlight", "content": "To quantify the significant non-uniform effects of text and image attention within the 2D attentional weight space, we developed a novel searchlight method. This approach systematically identifies localised effects of attentional manipulations using a sliding window, correcting for multiple comparisons, and clustering significant regions. This allowed us to pinpoint areas where attentional weights had the greatest impact on embedding spaces for responses simulating altered states. For example, negative text and reduced image attention may be linked to unity rather than anxiety states, as seen qualitatively and in the quantitative GAM analysis (Figures 2-4).\nIndeed, the searchlight analysis reveals differential attentional weight effects across various ASC factors (Figure 5). The Imagery-related factors show distinct patterns. Elementary and Complex"}, {"title": "Language", "content": "We next asked what text features might be driving changes in attentional weight space that lead the model to be variously more aligned with specific altered states like unity, ego-loss, and minimal phenomenal experiences. To begin to address this, we examined the distribution of correlation patterns of LIWC-22 categories associated with response text from the four quadrants of our attentional weight space (Q1-Q4) with cosine similarity scores (Figure 7)."}, {"title": "Categories", "content": "Text analysis demonstrated that differences in the attentional weight space are associated with unique types of text. We hypothesise that at an even more mechanistic level, observed effects could be explained by differential blurring of semantic categories in model embedding spaces. If this is the case, lower attention to text and images would blur category boundaries within ('bear' and 'pig') and across semantic categories, like Food (\u2018banana') and Furniture ('giraffe').\nHeatmaps representing degradation of within and across-category similarities suggest that this is so. Though within-category similarity was not dramatically affected (Figure 8, top left), across-category similarity was severely degraded compared to the baseline similarity, albeit in a quadrant-specific manner (Figure 8, top right). That is, Q1 and Q2 were the most degraded across categories, followed by Q3 and Q4 (Figure 8, bottom)."}, {"title": "Discussion", "content": "In this study, we used multimodal AI models (CLIP and FLAVA) to manipulate text and image attention weights, aiming to generate artificial altered states of consciousness. We systematically varied these weights to simulate altered states and analysed the resulting similarity between the embedding outputs and questionnaire items designed to measure such states (Figure 1). Our mechanistic account of human-altered states such as unity, ego-loss, and minimal phenomenal experience suggests that these states arise from degraded or intentionally altered language functioning, specifically the ability to categorise sensory and internal processes. We hypothesised that reduced attention to text and images would degrade Al performance but in such a way that the embedding space would resemble these altered states. We also hypothesised that these states would correspond to unique language forms and distinct levels of across-category blurring."}, {"title": "Summary", "content": "Qualitatively, cosine similarity between simulated altered states and prompts questionnaires intended to measure those states degraded non-uniformly across our 2D attentional weight space. Though the largest degradation was characterised by negative text and image weights (Q1), different altered states were associated with different text and image weight pairs (Figures 2 and 3). These results were confirmed by quantitative analysis. That is, there were main effects of text and image weights and interaction effects between simulated altered states and random text and between different simulated states, indicating that different text and image weight pairs were more or less associated with different altered states (Figure 4).\nThese observations were confirmed and clarified by our searchlight analyses (Figures 5 and 6). Results showed that altered state factors related to imagery primarily loaded on positive image weights (Q2 and Q3), while anxiety and cognition were associated with positive text weights (Q3 and Q4). In contrast, unity, ego-loss, and minimal phenomenal experience loaded on negative text weights (Q1 and Q2), with a particular emphasis on regions of negative text and image weights (Q1). These effects were consistent across both models, suggesting the robustness of these findings despite differences in model training and architecture.\nThe language patterns linked to simulated altered states in each quadrant support these conclusions (Figure 7): Q1 contained language not related to much at all relative to other categories. Q2 featured more visual and perceptual content, and past-focused language and Q3 and Q4 were dominated by specific language categories, personal pronouns, and self-reference. Finally, supporting these findings"}, {"title": "Mechanisms", "content": "Our findings largely confirm the proposed mechanisms by which language contributes to the phenomenology of altered states 4,5. Specifically, our results support the hypothesis that the manipulation of language-related processes through attentional shifts leads to altered consciousness, in line with observations from both psychedelic and meditative experiences; altered states related to unity, ego-loss, and minimal phenomenal experience were consistently associated with negative text weights, particularly when both text and image attention were minimal. This suggests that reducing language's influence diminishes categorical differentiation and self-referential thought, consistent with states of unity and ego-loss.\nThe robustness of these findings across multiple models-despite differences in architecture-highlights that the capacity for language to shape consciousness might be more fundamental than initially thought. These models, trained on language and sensory associations, demonstrated that specific attentional manipulations can lead to a breakdown in categorisation, akin to the blurring of conceptual boundaries observed under psychedelics. Moreover, the language content analysis supports the view that language plays a critical role in organising subjective experience: Positive text weights corresponded to more categorical, self-referential language, while negative text weights corresponded to reduced categorical differentiation and minimal content. Together, these results help refine our mechanistic understanding by showing that the language of altered states is not merely characterised by a functional loss of capacity but by a differential impact on categorical organisation with a more fundamental effect on consciousness. This may explain both the subjectively felt transcendent unity of psychedelic experiences and the open, unbounded awareness reported in meditative states38,42."}, {"title": "Neurobiology", "content": "Our findings align with recent neurobiological studies on altered states induced by meditation and psychedelics. These studies show that psychedelics dysregulate connectivity in high-level brain networks, which broadly overlap with systems classically associated with language processing42. For instance, medium-to-high doses of LSD, psilocybin, and DMT have been shown to induce hyperconnectivity within default mode, frontoparietal, and attention-related networks23,25, often correlating with scores of ego-dissolution and mystical experiences23,43. Importantly, reverse inference analyses of brain regions with high 5-HT2A receptor expression indicate an overlap with areas linked to language and semantics25. However, while these findings generally support the involvement of"}, {"title": "Wellbeing", "content": "Our findings suggest that language significantly shapes altered states of consciousness, with implications for mental health interventions like psychedelics, meditation, and psychotherapy. Reduced text attention in Al models produced states resembling unity, ego-loss, and minimal phenomenal experience-states linked to improved mental health during psychedelic experiences and meditation\u00b9,45. These states corresponded with less use of personal pronouns (e.g., 'I') and diminished category boundaries, suggesting an Al analogue for reduced self-referential processing. Previous research shows that first-person pronoun use correlates with rumination and self-focused attention, which negatively impacts mental health and wellbeing46\u201348. Collectively, our results support the proposal that psychedelics might promote therapeutic effects by acutely breaking down rigid linguistic categories, leading to reduced self-focus. Similarly, meditation that reduces internal verbalisation could achieve states of openness and tranquillity by diminishing the dominance of language, thereby reducing its constraining effects on conscious experience. It is plausible that such an acute breakdown could promote subsequent flexibility of self-related categorisation, and the development of novel insights49,50, with therapeutic relevance."}, {"title": "Limitations", "content": "While multimodal AI models like CLIP and FLAVA provide a useful framework for examining the influence of language on altered states of consciousness, they are not direct analogues of human brain function. Differences in architecture, training, and the absence of embodied experience require cautious extrapolation to human phenomenology. One potential concrete limitation is the concern of circularity, given that altered state descriptions were derived from prompts. However, this concern is mitigated by using high-temperature settings to ensure variability in generated prompts and by removing prompts from responses to minimise direct influence. Moreover, the observed variability in responses across altered states within the attentional weight space and the use of different models argue against purely circular effects. Future iterations may address this by generating text directly from the altered models instead of processing pre-existing text through embedding spaces. Finally, practical"}, {"title": "Conclusion", "content": "In this study, we posed the question: What is it like to be a multimodal language model whose attention is deliberately shifted away from language? Can these artificial systems mimic altered states of consciousness, such as those induced by psychedelics or meditation, where language's grip on categorisation loosens? Our results suggest that reducing language constraints leads to simulated phenomenological outputs that align with states of ego-loss, unity, and minimal phenomenal experience. Despite limitations, our approach represents an important step in leveraging AI to simulate and understand these altered states, as well as to illuminate the relationship between functions like language and consciousness. It invites further exploration, using more sophisticated models, into how altering the balance between language and sensory processing might influence subjective experiences in both biological and artificial systems."}, {"title": "Methods", "content": "We adopted a general strategy of using questions from standardised questionnaire metrics to measure altered states of consciousness as prompts to simulate machine experiences of those altered states (responses). Specifically, we used the Altered States of Consciousness Rating Scale (ASC)\u00b9\u00b9, Ego Dissolution Scale (EDS)\u00b9, and the Minimal Phenomenal Experience questionnaire (MPE-92M)39.\nThe ASC has 11 factors and we used all three questions for the Blissful State (Bliss), Complex Imagery (Complex), Elementary Imagery (Elementary), Insightfulness (Insight), Disembodiment, Changed Meaning of Percepts (Meaning), Spiritual Experience (Spiritual), Audio-Visual Synesthesia (Synesthesia). Some factors have more than three questions and we subselected three to have an equal number of prompts per factor, based on a subjective evaluation of how closely the questions matched the chosen factor. These were Anxiety, Impaired Control and Cognition (Cognition), and Experience of Unity (Unity).\nWe included the EDS to provide a validated measure of 'ego dissolution' that is distinct from experiences of unity. Unlike the Ego Dissolution Inventory\u00b3\u2070, which measures ego dissolution broadly, including the unitive experience, the EDS captures two distinct factors: Ego-Loss and Unity. The questions we selected for the ASC Unity factor above were chosen to capture the gist of the questions in the Unity factor in the EDI and EDS. We used all six EDI Ego-Loss questions as prompts.\nWe selected the 12 questions with the highest factor loadings from the MPE and adapted them into a yes/no format using the poles from the Likert scale employed in the questionnaire. For instance, the question 'Could your experience be described as emptiness, a vacuum, or a void?' was adapted into two statements: 'My experience could not be described as emptiness, a vacuum, or a void' and 'My experience could very much be described as emptiness, a vacuum, or a void'. Where applicable, questions were reverse-coded to ensure consistent interpretation across the set. This approach allowed us to generate responses that more or less capture a minimal phenomenal experience."}, {"title": "Responses", "content": "Individual questions from these measures served as prompts that were used to generate simulations of the phenomenological experiences intended to be addressed by each question. Specifically, for each prompt, a text response was generated using a pre-trained GPT-2 language model51. A high temperature setting of two was used to increase the variability and creativity of the output, possibly reflecting the variability of experiences individuals might have for any given altered state. Responses were limited to 75 tokens (M=61.65 words), resulting in standardised lengths, facilitating comparisons"}, {"title": "Attention", "content": "To investigate the role of text and image attention on model responses, we compared the semantic embedding space of the prompts (before attentional manipulation) to the embedding space of the responses (after attentional manipulation). This comparison served as a measure of how much the semantic quality of the responses was altered by attentional changes. We hypothesised that the degree of semantic degradation would vary systematically across different altered states, depending on the combinations of attentional weights applied. For instance, we expected unitive states to show less degradation with negative text and image attention weights. To capture these effects comprehensively, we sampled a wide range of combinations of positive and negative attentional weight pairs, encompassing conditions from strong suppression to strong amplification.\nSpecifically, we devised an attentional weight space ranging from -5 to 5 for both text and image attention, a range chosen based on values in the literature and through pilot testing of different ranges. This weight space was divided into four quadrants, based on the signs of text and image attentional weights. We began by defining 13 fixed extreme points to ensure coverage of key regions within this space, including the corners of each quadrant (e.g., [-5, 5]), the centre ([0, 0]), and intermediate points at the centre of each quadrant (e.g., [2.5, 2.5] and [-2.5, -2.5]). To further populate the quadrants in this weight space, we employed an enhanced Latin Hypercube Sampling (LHS) approach to distribute 500 additional text-image pairs across each of the four quadrants. LHS was chosen to ensure that the full range was sampled evenly, avoiding over-representation of specific regions. This sampling strategy resulted in 2013 evenly distributed text-/image weight pairs (Figure 1)."}, {"title": "Extraction", "content": "To understand how the attentional weight space impacts the alignment of responses with prompts, we extracted semantic embeddings for both prompts and responses. The embedding space for the prompts was extracted before applying any attentional weight manipulation. To generate prompt embeddings, we used the text encoder from the models. For image embeddings, we provided a default black 224x224 pixel image to the image encoder, simulating visual deprivation similar to meditation or"}, {"title": "Comparisons", "content": "The resulting prompt and response embeddings formed the foundation for further analysis.\nSpecifically, we primarily used cosine similarity to compare prompt, response, and random response embeddings without applying normalisation. Cosine similarity measures the angular distance between vectors in high-dimensional space, focusing on the orientation of the vectors rather than their magnitude, making it well-suited for assessing semantic similarity in embeddings.\nTo ensure the robustness of our results, we also experimented with additional similarity and distance metrics, including dot product, Euclidean distance, and Manhattan distance. For each metric, we computed both normalised and unnormalised scores by scaling embeddings to unit length and compared the results for structured and random responses. The consistency of results across these metrics generally supported the stability of our findings, and detailed results are not reported here."}, {"title": "Models", "content": "We used the CLIP and FLAVA multimodal models to extract and compare the embeddings described in the prior sections."}, {"title": "CLIP", "content": "We used the 'openai/clip-vit-large-patch14' variant of the CLIP model (Radford et al., 2021). CLIP integrates both text and image modalities to create joint representations, facilitating semantic alignment between text and images. It was trained on a large-scale dataset of 400 million image-text pairs collected from the internet, designed to capture a wide range of natural associations between images and their descriptions.\nThe model was implemented using the Hugging Face transformers library, with text and image inputs processed using the CLIPProcessor class. To manipulate attention, the \u2018query', 'key', and 'value'"}, {"title": "FLAVA", "content": "We used the 'facebook/flava-full' variant of the FLAVA model. Like CLIP, FLAVA integrates both text and image modalities, but it also features a distinct multimodal encoder to handle cross-modal interactions between textual and visual inputs. FLAVA was trained on a combination of datasets, including both curated and natural data, encompassing not only image-text pairs but also structured multimodal datasets. This diverse training allowed FLAVA to develop richer cross-modal relationships compared to CLIP.\nThe model was implemented using the Hugging Face transformers library, with inputs processed accordingly. Similar to our approach with CLIP, we manipulated the attention weights by scaling the 'query', 'key', and 'value' projection weights in both the text and image encoders using specific text and image weight factors. FLAVA's multimodal self-attention layers, which govern interactions between text and image representations, were also adjusted to investigate the impact of cross-modal attention. The model and processor were run on a GPU."}, {"title": "Analyses", "content": "To evaluate the impact of text and image weighting on the cosine similarity scores generated by comparing CLIP and FLAVA's embedding spaces, we applied four analytical approaches: 1) Generalised Additive Model (GAM) regression to understand how the attentional weight space influences responses; 2) A novel searchlight analysis to uncover subtle differences in attentional weight spaces generated by different responses and experiences; 3) An analysis of the language produced by the model under various attentional weight combinations; and 4) An assessment of within- and across-category semantic stability to investigate the proposed categorical mechanism underlying the effects of attentional weight manipulation."}, {"title": "Similarity", "content": "Heatmaps were generated to qualitatively represent various scores across the 2D attentional weight space, defined by text and image weights. Cosine similarity scores from each pair of text and image weights were aggregated and interpolated onto a regular grid. The averaged scores were then used to generate heatmaps, illustrating the distribution of similarity scores across the entire attentional weight space. Each heatmap was overlaid with contour lines to highlight regions of similar values."}, {"title": "GAM", "content": "Due to the highly non-linear relationships observed between text and image weights and cosine similarity scores, linear regression was deemed inappropriate. Instead, we employed a Generalised Additive Model (GAM) using the pyGAM library, which provides the flexibility to capture nonlinear dynamics by fitting smooth functions to the predictors.\nThe GAM was fitted to evaluate the effects of both text and image weights on the cosine similarity scores, as well as their interaction with the score type. The response variable, Score, could represent either the response or random score, and a categorical variable (Score Type) was included to differentiate between the genuine prompt-response relationship and random comparisons. Specifically, the model was defined as:\n$Y=s(Text Weights)+s(Image Weights)+f(Score Type)+te(Text Weights, Score Type)+te(Image Weights, Score Type)$\nIn this model, $s(Text Weights)$ and $s(Image Weights)$ are smooth functions that capture non-linear effects of text and image weights on the similarity scores. $f(Score Type)$ represents the categorical variable distinguishing between response and random similarity scores. $te(Text Weights, Score Type)$ and $te(Image Weights, Score Type)$ are tensor product smooth interactions that evaluate whether the relationship between attentional weights and similarity scores varies depending on the score type."}, {"title": "Searchlight", "content": "We developed a novel searchlight analysis to systematically quantify the non-uniform effects of text and image attention across a 2D attentional weight space. This approach aimed to identify regions within the weight space where attention manipulation had significant effects on different questionnaire factors."}, {"title": "Language", "content": "To understand the relationship between attentional weight space and linguistic responses, we used the Linguistic Inquiry and Word Count (LIWC) software to analyse model-generated text across different quadrants of the attentional weight space. Specifically, we divided the attentional weight space into four distinct quadrants based on combinations of text and image weights, allowing us to examine linguistic properties within each unique attentional configuration. Each quadrant (Q1\u2013Q4) was defined based on the combination of high or low attentional weights for the text and image dimensions, as described in the Similarity section. The LIWC-22 toolbox was then used to quantify the linguistic characteristics of all response texts generated under each quadrant condition.\nSpecifically, for each quadrant, Pearson correlation coefficients were calculated between the LIWC-derived scores for linguistic categories and the cosine similarity score. False Discovery Rate (FDR) correction using the Benjamini-Holm procedure was applied to account for multiple comparisons, with statistical significance set at a conservative threshold of p < 0.01. This analysis was performed for each of the four quadrants separately, providing an overview of linguistic differences under distinct attentional manipulations."}, {"title": "Categorisation", "content": "To evaluate the effects of varying text and image attention weights on within- and between-category similarities, we analysed 16 predefined object categories, each containing multiple objects (e.g., Animals: 'Dog' and 'Elephant'; Furniture: 'Chair' and 'Sofa'; Vehicles: 'Car' and 'Bicycle', etc.). For each of the 2,013 combinations of text and image attention weight pairs, embeddings were generated for these objects. The embeddings were produced by applying the specified weights to the text and image features in the model, capturing how the model represented each object under different attention conditions (otherwise conducted as described for CLIP and FLAVA).\nAfter generating the embeddings for each weight combination, cosine similarity was calculated to quantify how similar objects were to each other. Within-category similarity was measured by calculating cosine similarity between objects within the same category (e.g., comparing 'Dog' to 'Elephant' within the Animals category). Between-category similarity was measured by calculating the similarity between objects from different categories (e.g., comparing 'Dog' from Animals with 'Chair' from Furniture). This allowed us to assess how the varying attention to text and images influenced both the consistency within a category and the boundaries between categories.\nTo analyse the relationship between these similarities and the attention weights, we used linear regression models. The full model included interaction terms between the category, text weight, and image weight, and was designed to test whether the effects of attention manipulation differed across categories. Specifically, the model took the form:\n$similarity ~ category * text_weight * image_weight$\nWe then developed nested models by removing interaction terms to explore the independent contributions of text and image weights, and the interactions between them. Each model was compared using Akaike Information Criterion (AIC) to determine the goodness of fit, with likelihood ratio tests performed to evaluate whether removing certain terms led to significant changes in the model. Based on the AIC values and likelihood ratio tests, we selected the model with the best fit."}]}