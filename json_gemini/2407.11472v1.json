{"title": "DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems", "authors": ["Kaibo He", "Chenhui Zuo", "Chengtian Ma", "Yanan Sui"], "abstract": "Learning an effective policy to control high-dimensional, overactuated systems is a significant challenge for deep reinforcement learning algorithms. Such control scenarios are often observed in the neural control of vertebrate musculoskeletal systems. The study of these control mechanisms will provide insights into the control of high-dimensional, overactuated systems. The coordination of actuators, known as muscle synergies in neuromechanics, is considered a presumptive mechanism that simplifies the generation of motor commands. The dynamical structure of a system is the basis of its function, allowing us to derive a synergistic representation of actuators. Motivated by this theory, we propose the Dynamical Synergistic Representation (DynSyn) algorithm. DynSyn aims to generate synergistic representations from dynamical structures and perform task-specific, state-dependent adaptation to the representations to improve motor control. We demonstrate DynSyn's efficiency across various tasks involving different musculoskeletal models, achieving state-of-the-art sample efficiency and robustness compared to baseline algorithms. DynSyn generates interpretable synergistic representations that capture the essential features of dynamical structures and demonstrates generalizability across diverse motor tasks.", "sections": [{"title": "1. Introduction", "content": "In the evolution of embodied intelligence, researchers have used reinforcement learning (RL) algorithms to train controllers across diverse robotic platforms, yielding notable advancements in motor control. These agents can acquire robust and generalizable policies through iterative trial and error within large-scale simulations, subsequently deploying them onto real-world robots via sim-to-real methodologies (Rudin et al., 2022; Duan et al., 2022; Radosavovic et al., 2023). Overactuation and redundancy can often enhance the safety and robustness of embodied intelligent systems, mitigating the risk of sudden control failures (Hsu et al., 1989; Schneiders et al., 2004; Tohidi et al., 2016). However, overactuation will increase the complexity of the controlled object, particularly by enlarging the dimensionality of the action space, making it difficult for the deep RL controllers to achieve motor control.\nA common example of overactuated embodied systems is musculoskeletal systems in nature. In contrast to existing DRL agents, the motor control intelligence of vertebrates can control musculoskeletal systems through the central nervous system, exhibiting the ability to generalize across a variety of motor tasks while maintaining stability even under large disturbances, such as external force interference and drastic changes of environmental parameters. Exploring musculoskeletal motion control can help address the control challenges posed by high-dimensional, overactuated systems, thereby advancing our progress towards embodied intelligence. Nevertheless, a musculoskeletal model of human possesses characteristics that pose significant challenges for motor control by a RL agent. Firstly, the muscle control parameter space is high-dimensional, where over 600 skeletal muscles control hundreds of joints (Bernstein, 1966). Secondly, the system is overactuated, as multiple muscles actuate one joint and multiple joints may be affected by one muscle (Ting et al., 2012). Thirdly, the dynamic of neuromuscular actuators is non-linear and inconstant (Zajac, 1989; Wolpert & Ghahramani, 2000), and these actuators can only generate tension and no reverse forces.\nHow does motor control in vertebrates effectively address the challenge of redundant actuation? In neuroscience, there is a hypothesis known as muscle synergies. It proposes that coordinated recruitment of groups of muscles serves as a modular framework for biological motor control, simplifying the generation of motor commands. Studies have demonstrated that various motor behaviors can be reconstructed with high fidelity using a basic set of coordinated"}, {"title": "2. Related Work", "content": "Over-redundant actuation control. Overactuated systems, often observed in the motor control of vertebrates, such as musculoskeletal systems, present a challenge for controllers trained by RL algorithms. A low-dimensional representation can be used to evaluate the quality of the control (Sui et al., 2017). A series of attempts aimed on tackling the problem of learning control policies for locomotion and manipulation tasks with musculoskeletal models (Kidzi\u0144ski et al., 2020; Caggiano et al., 2022b; 2023b). Leading solutions of these challenges include heavy curriculum training approaches, with reward shaping or demonstration imitation (Song et al., 2021). Recently, a hierarchical reinforcement learning algorithm combined with imitation learning was applied to a 346-muscle musculoskeletal model (Lee et al., 2019). Generative models like variational autoencoders were utilized to control this musculoskeletal model to generate diverse behaviors (Park et al., 2022; Feng et al., 2023). In addition to human models, a musculoskeletal model of ostrich was constructed using the MuJoCo physics engine and controlled by TD4 deep reinforcement learning algorithm (La Barbera et al., 2021). Recent works, such as DEP-RL (Schumacher et al., 2023) and Lattice (Chiappa et al., 2023), have shown that employing better exploration techniques in reinforcement learning can help address the problem. Multi-task learning method is used in dexterous physiological control on a human hand model (Caggiano et al., 2023a). Bio-inspired approaches (Cheng et al., 2019;"}, {"title": "3. System Dynamical Features", "content": "In this study, we aim to generate synergistic representations of actuators based on the dynamical characteristics of overactuated systems. Overactuation is common in natural musculoskeletal systems controlled by multi-articulation and pull-only actuations, making their motion control much harder than conventional torque-controlled robots (Caggiano et al., 2023a; Berg et al., 2023). In this section, we will introduce the neuro-musculoskeletal control of a full-body model as an example and outline the problem formulation.\n3.1. Physiological Neuro-Musculoskeletal Control\nFull body musculoskeletal model. As shown in Figure 2(a), a full body musculoskeletal model MS-HUMAN-700 (Zuo et al., 2023) is used. The model has 90 rigid body segments,"}, {"title": "3.2. Problem Formulation", "content": "A motor control task of musculoskeletal models and robots can be formulated as a Markov decision process, denoted by \\(M = (S, A, r, p, \\rho_0, \\gamma)\\), where \\(S \\subseteq \\mathbb{R}^n\\) represents the continuous space of all valid states, and \\(A \\subseteq \\mathbb{R}^m\\) represents the continuous space of all valid actions. \\(r : S \\times A \\rightarrow \\mathbb{R}\\) is the reward function. The state transfer probability function \\(p(s' | s, a)\\) describes the probability of an agent taking an action \\(a\\) to transfer from the current state \\(s\\) to the state \\(s'\\). \\(\\rho_0\\) is the probability distribution of the initial state with \\(\\sum_{s_0 \\in S} \\rho_0(s_0) = 1\\) and \\(\\gamma \\in [0, 1)\\) is the discount factor. In the reinforcement learning paradigm, the goal of the agent is to optimize the policy parameter \\(\\theta\\) that maps from states to a probability distribution over actions \\(\\pi_{\\theta} : S \\rightarrow P(A)\\). The policy seeks to maximize the discounted returns, \\(\\pi_{\\theta}(a | s) = \\arg \\max_{\\theta} {\\mathbb{E}[\\sum_{t=0}^T \\gamma^t r(s_t, a_t)]}\\). The details of our action space and state space are described in Appendix B."}, {"title": "4. Dynamical Synergistic Representation", "content": "As the action space enlarges, the sample efficiency of DRL algorithms sharply decreases. Researchers have explored various aspects of a typical example of these problems, human musculoskeletal system control, by means including refining exploration strategies (Schumacher et al., 2023; Chiappa et al., 2023) and the utilization of hierarchical learning approaches (Lee et al., 2019). Efforts has been made to learn synergistic action representations from trajectories in pre-training stage to expedite training, which is highly reliant on pre-training outcomes (Berg et al., 2023). As shown in Figure 3, we observe that actuators with similar functions exhibit structural similarities. Hence, we employ a dynamics-based method which is able to rapidly generate interpretable synergistic representation. We then propose a novel algorithm to use these representations for further learning process.\n4.1. Representation Generation\nAs illustrated in Algorithm 1, We employ a similarity-based grouping method for the dynamical synergistic representation generation. Firstly, we generate muscle length trajectories of length \\(N_s\\) through applying random control to the joint space of a musculoskeletal model \\(M\\). Here, the control signal is the joint velocity \\(\\dot{q}\\) of the musculoskeletal model, and this signal is sampled randomly from a uniform distribution \\(Unif[-A_c, A_c]\\) every \\(T\\) time intervals. Upon obtaining the muscle length trajectory \\(T \\in \\mathbb{R}^{N_s \\times N_m}\\), we calculate the correlation between length changes for each pair of muscles according to Equation 2. \\(N_m\\) is the dimension of actions (the number of muscles). Based on the correlation matrix \\(R \\in \\mathbb{R}^{N_m \\times N_m}\\), and a predetermined number of groups \\(N_g\\), we employ the K-Medoids (Park & Jun, 2009) clustering algorithm to generate the closest clustering results, forming grouping bins \\(G\\).\n\\[R_{i,j} = \\frac{1}{N-1} \\sum_{k=0}^{N-1} S_c([\\Tau_{i}(t_k:t_{k+1})], [\\Tau_{j}(t_k:t_{k+1})])\\]\nThe correlation matrix is calculated by Equation 2. We divide the trajectories into \\(N\\) segments with respect to time and then average the similarity of each segmented trajectory."}, {"title": "4.2. State-dependent Representation", "content": "Using the above algorithm, functionally similar actuators will be categorized into a group, and assigned with same actions. This prevents DRL algorithms from assigning opposite actions to functionally similar actuators during the learning process, thereby enhancing effective exploration in high-dimensional action spaces. However, merely assigning same actions to all actuators within a group may result in unnatural movements. Therefore, we propose the algorithm illustrated in Figure 4. While the actuators within a group perform shared actions, state-dependent correction weights are produced for each actuator to facilitate fine-tuned adjustments.\nBased on the SAC algorithm (Haarnoja et al., 2018), our algorithm generates a unified action \\(a_g\\) for each group, along with state-dependent correction weights \\(w\\) for each actuator on top of the unified action \\(a_g\\). \\(a_g\\) and \\(w\\) are written as\n\\[a_g = \\tanh(u_g), u_g \\sim N(\\mu_a, \\sigma_a)\\]\n\\[w = \\tanh(\\hat{w}), \\hat{w} \\sim N(\\mu_w, \\sigma_w),\\]\nwhere \\(\\mu_a, \\sigma_a\\) represent the mean and variance of the unified actions, \\(\\mu_w\\) is the mean of state-dependent correction weights and \\(\\sigma_w\\) is the state-independent variance of the weights. By default, the first actuator in each group is assumed to have a correction weight of 1, and \\(N_m - N_g\\) correction weights are to be determined. The final action is computed using the following equations:\n\\[a_1 = IS(a_g) \\odot clip(k w, -c, c)\\]\n\\[c = \\min(\\max(k p_t + a_p, 0), k)\\]\n\\[a = clip([a_g, a_r], -1, 1),\\]\nwhere \\(clip(x, l, h)\\) is a function that restricts the value of \\(x\\) to the interval \\([l, h]\\). The Index Selection function (IS function), selects corresponding unified actions \\(a_g\\) according to indices determined by grouping results. \\(\\odot\\) represents element-wise multiplication, and \\([\\cdot, \\cdot]\\) denotes concatenation. \\(a_r\\) is the individual action. \\(k_p\\), \\(a_p\\) and \\(k\\) are the hyperparameters of the weight boundary, which relaxes gradually as the training timesteps \\(t\\) increase. During training, the state-dependent adaptation of representations will start at \\(a_D\\) considering the stability of learning. Finally, the policy \\(\\pi\\) will be updated according to the following formula:\n\\[\\pi^* = \\arg \\max_{\\pi} \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{t=0}^T \\gamma^t (R(s_t, a_t) + \\alpha H(\\pi(a_t | s_t)))]\\]"}, {"title": "5. Experiments", "content": "We demonstrate our method's efficiency during learning and its generalization ability in overactuated motor control benchmarks built in MuJoCo. In this part, we will introduce the benchmarks, the learning process of DynSyn and the details of baselines.\n5.1. Environments\nWe create reinforcement learning environments of various models and tasks to test our algorithm. Additionally, two tasks from MyoSuite (Caggiano et al., 2022a) are taken into account. We use Model-Task pair to label the environments, as shown in Figure 5. A complete description including the action space, the state space and the reward function of each environment is given in Appendix B.\nHuman Motion Imitation: In FullBody-Gait, we expect the full body MS-HUMAN-700 model with 206 joints actuated by 700 muscles (described in Section 3.1) to mimic a motion-capture walking trajectory. During training, the model may be initialized at any time step throughout a trajectory cycle.\nHuman Locomotion: In Legs-Walk, a 20-DoF Legs model actuated by 100 muscles is used. In MyoLegs-Walk, the MyoLegs model in MyoSuite with 20 DoF and 80 muscles is used. Both models are expected to walk forward robustly, driven by biomechanically inspired reward functions.\nHuman Manipulation: In Arm-Locate, an Arm model of 28 DoF and 81 muscles, with wrist and fingers is used. The agent is trained to learn to grasp a bottle, relocate it to the random target position and orientation. In MyoHand-Reorient100, the MyoHand model in MyoSuite with 23 DoF and 39 muscles needs to rotate a set of 4 objects, each with 25 different geometries, to predetermined orientations.\nAnimal Locomotion: In Ostrich-Run, an Ostrich model (La Barbera et al., 2021) with 50 joints actuated by 120 muscles is used. The model is trained to run horizontally as fast as possible by rewarding its velocity.\nGeneration Tasks: We test various terrain conditions (MyoLegs-Walk-Terrain) and different walking targets (Legs-Walk-Fast, Legs-Walk-Diagnal) to demonstrate the generalization capability of the dynamical synergistic representation across various physical conditions, as well as the robustness of generated motor behaviors.\nThe Legs model and the Arm model are obtained by removing irrelevant degrees of freedom from the full body model. For the locomotion task and the manipulation task, two environments are tested for each task to demonstrate the robustness of the algorithm, given that there are several variations between the models and the environments."}, {"title": "5.2. Learning Dynamical Synergistic Representation", "content": "Before training, dynamical synergistic representations (i.e. the grouping of actuators) are generated for each model. We impose random control on joint velocities for 5e5 simulation frames to collect sequences of actuators' features. For muscle-tendon units, the collected feature is length. The grouping of actuators is then obtained according to Section 4.1. We choose an appropriate number of groups where the difference between maximum and minimum distances among cluster centers are large enough. This process is further detailed in Appendix C.2. To demonstrate that the representation generated by our method can stably capture the dynamical features of the models, we repeat the generation for 10 times on each model and calculate the mean value and variance. The same representation of a single model are retained in tasks with changed conditions to verify the generalization ability of the algorithm. Furthermore, a series of ablation experiments are presented to prove that our choice of the number of groups is reasonable and helpful for the learning of motor control (see Section 6.2)."}, {"title": "5.3. Baselines", "content": "We compared DynSyn with current DRL methods in over-actuated systems: SAC (Haarnoja et al., 2018), SAR (Berg et al., 2023) and DEP-RL (Schumacher et al., 2023). SAR collects low-dimensional representations from a pre-training collection stage over M time steps and its training stage is over another N time steps. Other methods are directly trained over M + N time steps. It should be noted that DEP-RL is an exploration method which can be integrated into our algorithm. DynSyn are based on the RL library Stable-Baselines 3 (Raffin et al., 2019). Hyperparameters and implementation details in the experiments are summarized in Appendix C.4. All results are averaged across 5 random seeds."}, {"title": "6. Results and Analysis", "content": "In this section, we present the experimental results, demonstrating that DynSyn effectively facilitates motor control across various tasks involving different models, exhibiting state-of-the-art sample efficiency and high stability. Additionally, we illustrate that the dynamical synergistic representations extracted from models exhibit good performance in terms of convergence and interpretability. This allows the model to leverage the representations in learning motor control across diverse tasks, even under varying conditions such as terrain and training targets.\n6.1. Efficient Learning\nFigure 6 illustrates that DynSyn achieves higher returns in fewer training steps across all standard experimental environments. This implies that DynSyn efficiently produces robust motor control (refer to Figure 1) in various overactuated models and motor tasks. Notably, the performance of baseline agents significantly deteriorates as the number of action dimensions increases, whereas DynSyn performs well even in a very high-dimensional action space of 700 dimensions.\n6.2. Synergies Generalization\nWhen the same representations are applied to tasks with additional environmental conditions or changed targets, such as rugged terrains and walking direction, DynSyn maintains good performance (see Figure 7). This suggests that the generated synergistic representations of the same model can effectively generalize across different tasks. Figure 8(a) shows the average results of 10 preliminary group extractions for the Legs model, showing a high probability of obtaining the same grouping result (close to 1) and furthermore, the stability of the extraction process.\nFor ablation study, we utilize a random grouping approach to create 40 clusters for comparison. As shown in Figure 9, our method consistently yields performance improvements. However, randomly generated representations outperform the SAC algorithm, possibly due to the influence of our state-dependent algorithm. The high standard deviation of the learning curve of randomly generated representation shows a decrease in stability. We also attempt to generate representations for varying numbers of clusters. The results demonstrate that our cluster number selection scheme ensures performance stability with a lower standard deviation of the learning curve (see Figure 9). Each grouping mode is"}, {"title": "6.3. Physiological Analysis", "content": "In accordance with the left-right symmetry in Legs modeling, 50 indices, from 0 to 49, are assigned to muscles of each leg symmetrically. In the grouping matrix in Figure 8(a), the upper left quadrant signifies the correlation between left-leg muscles, and the lower right quadrant represents the correlation between right-leg muscles. The remaining part of Figure 8(a) depicts correlation near 0 between pairs of muscles from both legs. Notably, our representation generation method identifies inherent symmetries in the musculoskeletal model. Only groupings within the same leg are observed. Furthermore, the groupings of muscles from the left and right legs are symmetrical. For improved visualization, the muscle grouping result of the right leg is expanded and depicted in Figure 8(b), detailing two representative muscle grouping examples in the musculoskeletal model (i.e., Psoas Major and Thigh Adductors). From a biomechanical perspective, this is evident that muscles grouped together exhibit similar effects, highlighting our method's ability to capture fundamental dynamical characteristics in the system."}, {"title": "7. Conclusion and Discussion", "content": "We introduce DynSyn, a deep RL method that generates synergistic representations of actuators from dynamical structures of overactuated systems and make task-specific, state-dependent adaptation to the representations, thereby expediting and stabilizing motor control learning. Applying DynSyn to musculoskeletal locomotion and manipulation tasks, we demonstrate its superior learning efficiency compared to all baselines. Additionally, we illustrate the robust generalization ability of the extracted synergistic representations across various motor tasks with the same model. In conclusion, our work offers an efficient, generalizable, and interpretable approach to controlling high-dimensional redundant actuation systems. The generation method of synergistic representations can help deepen the understanding of motor intelligence. This research aims to facilitate the training of motor control policies for use in artificial intelligence, robotics and medicine, contributing to the development of a versatile embodied agent.\nDespite the promising outcomes, distinctions persist between real embodied motor intelligence and the musculoskeletal model simulation employed in our study. For instance, current simulation methods primarily leverage proprioception (joint position and velocity), whereas in the real world, an animal receives additional sensory inputs, including vision and touch (Patla, 1997; 1998; Jeka et al., 2000). To enhance customization for specific applications, further work on biomechanically realistic simulations is essential. Other significant limitations include the multiple potential solutions in overactuated systems, and our method can only generate one of the numerous high-dimensional combinations to control the system. Future research may need to consider establishing a solution space of control patterns."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning, especially Embodied Intelligence. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Neuro-muscle dynamics", "content": "The input control signal of muscle-tendon units is the neural excitation ctrl, and the muscle activation act is calculated by a first-order nonlinear filter as follow:\n\\[\\frac{d \\text{act}}{dt} = \\frac{\\text{ctrl} - \\text{act}}{\\tau(\\text{ctrl}, \\text{act})},\\]\n\\[\\tau(\\text{ctrl}, \\text{act}) = \\begin{cases} \\tau_{\\text{act}} (0.5 + 1.5 \\text{act}) & \\text{ctrl} > \\text{act} \\\\ \\tau_{\\text{deact}} / (0.5 + 1.5 \\text{act}) & \\text{ctrl} < \\text{act} \\end{cases}\\]\n(\\( \\tau_{\\text{act}}, \\tau_{\\text{deact}} \\)) is a time constant to activate or deactivate latency of defaults (10ms, 40ms). The force produced by a single muscle-tendon unit can be formulated as\n\\[f_m(\\text{act}) = f_{\\text{max}} \\cdot [F_l(l_m) \\cdot F_v(v_m) \\cdot \\text{act} + F_p(l_m)],\\]\nwhere \\(f_{\\text{max}}\\) stands for the maximum isometric muscle force and act,\\(l_m\\), \\(v_m\\) respectively stand for the activation, normalized length and normalized velocity of the muscle. \\(F_l\\) and \\(F_v\\) represent force-length and force-velocity functions fitted using data from biomechanical experiments (Millard et al., 2013)."}, {"title": "B. Environment Details", "content": "For all environments, the simulation time step is 0.01s. The action space consists of muscle excitations ctrl (i.e. motor neuron signals). The dimensions of action and state spaces, number of joints and episode length of all the environments are summarized in Table A.1. Task and reward parameters are summarized in Table A.2.\nFullBody-Gait We expect the full body MS-HUMAN-700 model with 206 joints actuated by 700 muscles (Section 3.1) to mimic a motion-capture walking trajectory. During training, the model may be initialized at any time step throughout a trajectory cycle. The state space consists of simulation time, joint positions, joint velocities, muscle forces, muscle lengths, muscle velocities, muscle activation and reference joint positions. The reward function is:\n\\[R = w_v R_v + w_q R_q + w_h R_{\\text{alive}}\\]\n\\[R_v = \\exp(-||\\vec{v}_{\\text{com}} - \\vec{v}^t||^2) + \\exp(-(\\vec{v}_{\\text{com}} - \\vec{v}^t)^2)\\]\n\\[R_q = - ||q - q^*||^2\\]\nwhere q is actual joint positions, \\(q^*\\) is the reference joint positions, \\{\\vec{v}_{\\text{com}}, \\vec{v}^t\\} is the velocity of the center of mass and \\{\\vec{v}^t, \\vec{v}^*\\} is the desired velocity. \\(w_v\\), \\(w_q\\) and \\(w_h\\) are the weights.\nMyoLegs-Walk The MyoLegs model in MyoSuite with 20 DoF and 80 muscles is used. The model is expected to walk forward robustly, driven by biomechanically inspired reward functions:\n\\[R = w_v R_v - w_c R_c + w_r R_r + w_j R_j - w_a R_a - w_d R_d\\]\n\\(R_a = \\{ f_{\\text{alled}} \\}\\), imposes a penalty when the model falls. The weights \\(w_v\\), \\(w_c\\), \\(w_r\\), \\(w_j\\), \\(w_a\\), and \\(w_d\\) determine the importance of each reward term. The other terms are as follow:\n\\[R = \\exp(-\\sqrt{v - v_x}^2) + \\exp(-\\sqrt{v - v_y}^2)\\]\n\\(v\\) and \\(v\\) represent the desired and actual velocity of the center of mass. \\(R_v\\) represents the velocity reward.\n\\[R_c = ||[0.8\\cos(\\phi \\times 2\\pi + \\pi), 0.8 \\cos(\\phi \\times 2\\pi)] - [q_{rhip}, q_{lhip}]||\\]\n\\(\\phi\\) is the phase percentage of the pre-define gait period. \\(q_{rhip}\\) and \\(q_{lhip}\\) are the hip flexion angle of both legs. \\(R_c\\) encourages rhythmic hip movements.\n\\[R_r = \\exp(-5||(\\vec{q}_{pelvis} - \\vec{q}_{pelvis}^{\\text{init}}||)\\]\n\\(\\vec{q}_{pelvis}\\) and \\(\\vec{q}_{pelvis}^{\\text{init}}\\) are the quaternions of pelvis and its initial value when reset. \\(R_r\\) encourages the model to follow a predetermined rotation pattern.\n\\[R_j = \\exp(-5 \\sum_{i=1}^{N} |q_i| / N)\\]"}, {"title": "C. Implementation Details", "content": "C.1. Action normalization\nOur preliminary experiments reveal that in MyoSuite, the action space, originally [0, 1], is normalized to [-1, 1] using Equation 30. This normalization method enhances training effectiveness. Consequently, we apply this normalization approach in all our environments and algorithm comparison experiments.\n\\[\\hat{a} = \\frac{1}{1 + e^{-5(a - 0.5)}}\\]"}, {"title": "C.2. Dynamical synergistic representation generation", "content": "In the process of representation generation, determining the number of groups is crucial. In Figure 11(a), we illustrate the maximum and minimum values of the distance among cluster centers for different group configurations. The algorithm exhibits robust and explainable performance when we choose an appropriate number of clusters where the difference between maximum and minimum distances are large enough. In Figure 11(b), it is evident that when the selected number of groups is 40, the t-SNE visualization maintains symmetry and interpretability.\nAs illustrated in Figure 12 and Figure 13, the grouping results are shown to converge to their final grouping with a data point quantity as low as 25,600. It's also displayed that even if we have only 100 data points, the grouping result is similar to the final result."}]}