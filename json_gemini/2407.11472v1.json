{"title": "DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems", "authors": ["Kaibo He", "Chenhui Zuo", "Chengtian Ma", "Yanan Sui"], "abstract": "Learning an effective policy to control high-dimensional, overactuated systems is a significant challenge for deep reinforcement learning algorithms. Such control scenarios are often observed in the neural control of vertebrate musculoskeletal systems. The study of these control mechanisms will provide insights into the control of high-dimensional, overactuated systems. The coordination of actuators, known as muscle synergies in neuromechanics, is considered a presumptive mechanism that simplifies the generation of motor commands. The dynamical structure of a system is the basis of its function, allowing us to derive a synergistic representation of actuators. Motivated by this theory, we propose the Dynamical Synergistic Representation (DynSyn) algorithm. DynSyn aims to generate synergistic representations from dynamical structures and perform task-specific, state-dependent adaptation to the representations to improve motor control. We demonstrate DynSyn's efficiency across various tasks involving different musculoskeletal models, achieving state-of-the-art sample efficiency and robustness compared to baseline algorithms. DynSyn generates interpretable synergistic representations that capture the essential features of dynamical structures and demonstrates generalizability across diverse motor tasks.", "sections": [{"title": "1. Introduction", "content": "In the evolution of embodied intelligence, researchers have used reinforcement learning (RL) algorithms to train controllers across diverse robotic platforms, yielding notable advancements in motor control. These agents can acquire robust and generalizable policies through iterative trial and error within large-scale simulations, subsequently deploying them onto real-world robots via sim-to-real methodologies. Overactuation and redundancy can often enhance the safety and robustness of embodied intelligent systems, mitigating the risk of sudden control failures. However, overactuation will increase the complexity of the controlled object, particularly by enlarging the dimensionality of the action space, making it difficult for the deep RL controllers to achieve motor control.\n\nA common example of overactuated embodied systems is musculoskeletal systems in nature. In contrast to existing DRL agents, the motor control intelligence of vertebrates can control musculoskeletal systems through the central nervous system, exhibiting the ability to generalize across a variety of motor tasks while maintaining stability even under large disturbances, such as external force interference and drastic changes of environmental parameters. Exploring musculoskeletal motion control can help address the control challenges posed by high-dimensional, overactuated systems, thereby advancing our progress towards embodied intelligence. Nevertheless, a musculoskeletal model of human possesses characteristics that pose significant challenges for motor control by a RL agent. Firstly, the muscle control parameter space is high-dimensional, where over 600 skeletal muscles control hundreds of joints. Secondly, the system is overactuated, as multiple muscles actuate one joint and multiple joints may be affected by one muscle. Thirdly, the dynamic of neuromuscular actuators is non-linear and inconstant, and these actuators can only generate tension and no reverse forces.\n\nHow does motor control in vertebrates effectively address the challenge of redundant actuation? In neuroscience, there is a hypothesis known as muscle synergies. It proposes that coordinated recruitment of groups of muscles serves as a modular framework for biological motor control, simplifying the generation of motor commands. Studies have demonstrated that various motor behaviors can be reconstructed with high fidelity using a basic set of coordinated"}, {"title": "2. Related Work", "content": "Over-redundant actuation control. Overactuated systems, often observed in the motor control of vertebrates, such as musculoskeletal systems, present a challenge for controllers trained by RL algorithms. A low-dimensional representation can be used to evaluate the quality of the control. A series of attempts aimed on tackling the problem of learning control policies for locomotion and manipulation tasks with musculoskeletal models. Leading solutions of these challenges include heavy curriculum training approaches, with reward shaping or demonstration imitation. Recently, a hierarchical reinforcement learning algorithm combined with imitation learning was applied to a 346-muscle musculoskeletal model. Generative models like variational autoencoders were utilized to control this musculoskeletal model to generate diverse behaviors. In addition to human models, a musculoskeletal model of ostrich was constructed using the MuJoCo physics engine and controlled by TD4 deep reinforcement learning algorithm. Recent works, such as DEP-RL and Lattice , have shown that employing better exploration techniques in reinforcement learning can help address the problem. Multi-task learning method is used in dexterous physiological control on a human hand model. Bio-inspired approaches"}, {"title": "3. System Dynamical Features", "content": "In this study, we aim to generate synergistic representations of actuators based on the dynamical characteristics of overactuated systems. Overactuation is common in natural musculoskeletal systems controlled by multi-articulation and pull-only actuations, making their motion control much harder than conventional torque-controlled robots. In this section, we will introduce the neuro-musculoskeletal control of a full-body model as an example and outline the problem formulation."}, {"title": "3.1. Physiological Neuro-Musculoskeletal Control", "content": "Full body musculoskeletal model. As shown in Figure 2(a), a full body musculoskeletal model MS-HUMAN-700 is used. The model has 90 rigid body segments, 206 joints, and 700 muscle-tendon units, and is implemented in the MuJoCo physics simulator. The base segment of the model is pelvis, which can translate and rotate in full degrees of freedom. The body parts can interact with the environment during simulation because the mesh files of their bones are used for collision calculation. The dynamics of the human musculoskeletal system can be formulated with the Euler-Lagrangian equations as\n\n\\(M(q)\\ddot{q} + c(q, \\dot{q}) = J_m f_m(act) + J_c^\\intercal f_c + T_{ext}\\)  (1)\n\nwhere q represents the generalized coordinates of joints, M(q) represents the mass distribution matrix, and \\(c(q, \\dot{q})\\) stands for Coriolis and gravitational forces. \\(f_m(act)\\) is the vector representing muscle forces generated by all muscle-tendon units, and is determined by muscle activations (act). \\(f_c\\) is the constraint force. \\(J_m\\) and \\(J_c\\) are Jacobian matrices that map forces to the space of generalized coordinates. \\(T_{ext}\\) is external torque. The input control signal of muscle-tendon units is the neural excitations, which determine muscle activations. With the employment of the Hill-type muscle model, the activation-contraction dynamics of muscles exhibit non-linearity and temporal delay, thereby posing challenges to neuromuscular control (see Appendix A).\n\nIn Section 5, we apply DynSyn to several local models of human body (such as an arm model in Figure 2(b)) and a model of ostrich. These local models of human body are part of the MS-HUMAN-700"}, {"title": "3.2. Problem Formulation", "content": "A motor control task of musculoskeletal models and robots can be formulated as a Markov decision process, denoted by M = (S, A, r, p, \\rho_0, \\gamma), where \\(S \\subset \\mathbb{R}^n\\) represents the continuous space of all valid states, and \\(A \\subseteq \\mathbb{R}^m\\) represents the continuous space of all valid actions. r : S \u00d7 A \u2192 R is the reward function. The state transfer probability function \\(p(s'|s, a)\\) describes the probability of an agent taking an action a to transfer from the current state s to the state s'. \\rho_0 is the probability distribution of the initial state with \\(\\sum_{s_0 \\in S} \\rho_0(s_0) = 1\\) and \\(\\gamma \\in [0,1)\\) is the discount factor. In the reinforcement learning paradigm, the goal of the agent is to optimize the policy parameter \\(\\theta\\) that maps from states to a probability distribution over actions \\(\\pi_\\theta : S \\rightarrow P(A)\\). The policy seeks to maximize the discounted returns, \\(\\pi_\\theta(a|s) = arg\\max_\\theta E [\\sum_{t=0}^T \\gamma^t r(s_t, a_t)]\\). The details of our action space and state space are described in Appendix B."}, {"title": "4. Dynamical Synergistic Representation", "content": "As the action space enlarges, the sample efficiency of DRL algorithms sharply decreases. Researchers have explored various aspects of a typical example of these problems, human musculoskeletal system control, by means including refining exploration strategies and the utilization of hierarchical learning approaches. Efforts has been made to learn synergistic action representations from trajectories in pre-training stage to expedite training, which is highly reliant on pre-training outcomes. As shown in Figure 3, we observe that actuators with similar functions exhibit structural similarities. Hence, we employ"}, {"title": "4.1. Representation Generation", "content": "As illustrated in Algorithm 1, We employ a similarity-based grouping method for the dynamical synergistic representation generation. Firstly, we generate muscle length trajectories of length \\(N_s\\) through applying random control to the joint space of a musculoskeletal model M. Here, the control signal is the joint velocity \\(\\dot{q}\\) of the musculoskeletal model, and this signal is sampled randomly from a uniform distribution Unif[-Ac, Ac] every T time intervals. Upon obtaining the muscle length trajectory \\(T \\in \\mathbb{R}^{N_s \\times N_m}\\), we calculate the correlation between length changes for each pair of muscles according to Equation 2. \\(N_m\\) is the dimension of actions (the number of muscles). Based on the correlation matrix \\(R \\in \\mathbb{R}^{N_m \\times N_m}\\), and a predetermined number of groups \\(N_g\\), we employ the K-Medoids  clustering algorithm to generate the closest clustering results, forming grouping bins G.\n\n\\(R_{i,j} = \\frac{1}{N-1} \\sum_{k=0}^{N-1} S_c(\\tau_{[t:(t+1)]}, \\tau_{[t:(t+1)]})\\) (2)\n\nThe correlation matrix is calculated by Equation 2. We divide the trajectories into N segments with respect to time and then average the similarity of each segmented trajectory."}, {"title": "4.2. State-dependent Representation", "content": "Using the above algorithm, functionally similar actuators will be categorized into a group, and assigned with same actions. This prevents DRL algorithms from assigning opposite actions to functionally similar actuators during the learning process, thereby enhancing effective exploration in high-dimensional action spaces. However, merely assigning same actions to all actuators within a group may result in unnatural movements. Therefore, we propose the algorithm illustrated in Figure 4. While the actuators within a group perform shared actions, state-dependent correction weights are produced for each actuator to facilitate fine-tuned adjustments.\n\nBased on the SAC algorithm , our algorithm generates a unified action \\(a_G\\) for each group, along with state-dependent correction weights w for each actuator on top of the unified action \\(a_G\\). \\(a_G\\) and w are written as\n\n\\(a_G = tanh(u_G), u_G \\sim N(\\mu_\\alpha, \\sigma_\\alpha)\\) (3)\n\\(w = tanh(\\tilde{w}), \\tilde{w} \\sim N(\\mu_w, \\sigma_w),\\) (4)\n\nwhere \\(\\mu_\\alpha, \\sigma_\\alpha\\) represent the mean and variance of the unified actions, \\(\\mu_w\\) is the mean of state-dependent correction weights and \\(\\sigma_w\\) is the state-independent variance of the weights. By default, the first actuator in each group is assumed to have a correction weight of 1, and \\(N_m - N_g\\) correction weights are to be determined. The final action is computed using the following equations:\n\n\\(a_i = IS(a_G) \\odot clip(k_w, -c, c)\\) (5)\n\\(c = min(max(k_p t + a_p, 0), k)\\) (6)\n\\(a = clip([a_G, a_r], -1, 1)\\) (7)\n\nwhere clip(x, l, h) is a function that restricts the value of x to the interval [l, h]. The Index Selection function (IS function), selects corresponding unified actions \\(a_G\\) according to indices determined by grouping results.\n\\odot represents element-wise multiplication, and [,] denotes concatenation. \\(a_r\\) is the individual action. \\(k_p, a_p\\) and k are the hyperparameters of the weight boundary, which relaxes gradually as the training timesteps t increase. During training, the state-dependent adaptation of representations will start at \\(\\alpha_D\\) considering the stability of learning. Finally, the policy \\(\\pi^*\\) will be updated according to the following formula:\n\n\\(\\pi^* = arg \\max_{\\pi} E_{\\tau \\sim \\pi} [\\sum_{t=0}^T \\gamma^t (R(s_t, a_t) + \\alpha H(\\pi(a_G|s_t)))]\\) (8)"}, {"title": "5. Experiments", "content": "We demonstrate our method's efficiency during learning and its generalization ability in overactuated motor control benchmarks built in MuJoCo. In this part, we will introduce the benchmarks, the learning process of DynSyn and the details of baselines."}, {"title": "5.1. Environments", "content": "We create reinforcement learning environments of various models and tasks to test our algorithm. Additionally, two tasks from MyoSuite are taken into account. We use Model-Task pair to label the environments, as shown in Figure 5. A complete description including the action space, the state space and the reward function of each environment is given in Appendix B.\n\nHuman Motion Imitation: In FullBody-Gait, we expect the full body MS-HUMAN-700 model with 206 joints actuated by 700 muscles (described in Section 3.1) to mimic a motion-capture walking trajectory. During training, the model may be initialized at any time step throughout a trajectory cycle.\n\nHuman Locomotion: In Legs-Walk, a 20-DoF Legs model actuated by 100 muscles is used. In MyoLegs-Walk, the MyoLegs model in MyoSuite with 20 DoF and 80 muscles is used. Both models are expected to walk forward robustly, driven by biomechanically inspired reward functions.\n\nHuman Manipulation: In Arm-Locate, an Arm model of 28 DoF and 81 muscles, with wrist and fingers is used. The agent is trained to learn to grasp a bottle, relocate it to the random target position and orientation. In MyoHand-Reorient100, the MyoHand model in MyoSuite with 23 DoF and 39 muscles needs to rotate a set of 4 objects, each with 25 different geometries, to predetermined orientations.\n\nAnimal Locomotion: In Ostrich-Run, an Ostrich model with 50 joints actuated by 120 muscles is used. The model is trained to run horizontally as fast as possible by rewarding its velocity.\n\nGeneration Tasks: We test various terrain conditions (MyoLegs-Walk-Terrain) and different walking targets (Legs-Walk-Fast, Legs-Walk-Diagnal) to demonstrate the generalization capability of the dynamical synergistic representation across various physical conditions, as well as the robustness of generated motor behaviors.\n\nThe Legs model and the Arm model are obtained by removing irrelevant degrees of freedom from the full body model. For the locomotion task and the manipulation task, two environments are tested for each task to demonstrate the robustness of the algorithm, given that there are several variations between the models and the environments."}, {"title": "5.2. Learning Dynamical Synergistic Representation", "content": "Before training, dynamical synergistic representations (i.e. the grouping of actuators) are generated for each model. We impose random control on joint velocities for 5e5 simulation frames to collect sequences of actuators' features. For muscle-tendon units, the collected feature is length. The grouping of actuators is then obtained according to Section 4.1. We choose an appropriate number of groups where the difference between maximum and minimum distances among cluster centers are large enough. This process is further detailed in Appendix C.2. To demonstrate that the representation generated by our method can stably capture the dynamical features of the models, we repeat the generation for 10 times on each model and calculate the mean value and variance. The same representation of a single model are retained in tasks with changed conditions to verify the generalization ability of the algorithm. Furthermore, a series of ablation experiments are presented to prove that our choice of the number of groups is reasonable and helpful for the learning of motor control (see Section 6.2)."}, {"title": "5.3. Baselines", "content": "We compared DynSyn with current DRL methods in over-actuated systems: SAC, SAR and DEP-RL . SAR collects low-dimensional representations from a pre-training collection stage over M time steps and its training stage is over another N time steps. Other methods are directly trained over M + N time steps. It should be noted that DEP-RL is an exploration method which can be integrated into our algorithm. DynSyn are based on the RL library Stable-Baselines 3 . Hyperparameters and implementation details in the experiments are summarized in Appendix C.4. All results are averaged across 5 random seeds."}, {"title": "6. Results and Analysis", "content": "In this section, we present the experimental results, demonstrating that DynSyn effectively facilitates motor control across various tasks involving different models, exhibiting"}, {"title": "6.1. Efficient Learning", "content": "Figure 6 illustrates that DynSyn achieves higher returns in fewer training steps across all standard experimental environments. This implies that DynSyn efficiently produces robust motor control (refer to Figure 1) in various overactuated models and motor tasks. Notably, the performance of baseline agents significantly deteriorates as the number of action dimensions increases, whereas DynSyn performs"}, {"title": "6.2. Synergies Generalization", "content": "When the same representations are applied to tasks with additional environmental conditions or changed targets, such as rugged terrains and walking direction, DynSyn maintains good performance (see Figure 7). This suggests that the generated synergistic representations of the same model can effectively generalize across different tasks. Figure 8(a) shows the average results of 10 preliminary group extractions for the Legs model, showing a high probability of obtaining the same grouping result (close to 1) and furthermore, the stability of the extraction process.\n\nFor ablation study, we utilize a random grouping approach to create 40 clusters for comparison. As shown in Figure 9, our method consistently yields performance improvements. However, randomly generated representations outperform the SAC algorithm, possibly due to the influence of our state-dependent algorithm. The high standard deviation of the learning curve of randomly generated representation shows a decrease in stability. We also attempt to generate representations for varying numbers of clusters. The results demonstrate that our cluster number selection scheme ensures performance stability with a lower standard deviation of the learning curve (see Figure 9). Each grouping mode is"}, {"title": "6.3. Physiological Analysis", "content": "In accordance with the left-right symmetry in Legs modeling, 50 indices, from 0 to 49, are assigned to muscles of each leg symmetrically. In the grouping matrix in Figure 8(a), the upper left quadrant signifies the correlation between left-leg muscles, and the lower right quadrant represents the correlation between right-leg muscles. The remaining part of Figure 8(a) depicts correlation near 0 between pairs of muscles from both legs. Notably, our representation generation method identifies inherent symmetries in the musculoskeletal model. Only groupings within the same leg are observed. Furthermore, the groupings of muscles from the left and right legs are symmetrical. For improved visualization, the muscle grouping result of the right leg is expanded and depicted in Figure 8(b), detailing two representative muscle grouping examples in the musculoskeletal model (i.e., Psoas Major and Thigh Adductors). From a biomechanical perspective, this is evident that muscles grouped together exhibit similar effects, highlighting our method's ability to capture fundamental dynamical characteristics in the system."}, {"title": "7. Conclusion and Discussion", "content": "We introduce DynSyn, a deep RL method that generates synergistic representations of actuators from dynamical structures of overactuated systems and make task-specific, state-dependent adaptation to the representations, thereby expediting and stabilizing motor control learning. Applying DynSyn to musculoskeletal locomotion and manipulation tasks, we demonstrate its superior learning efficiency compared to all baselines. Additionally, we illustrate the robust generalization ability of the extracted synergistic representations across various motor tasks with the same model. In conclusion, our work offers an efficient, generalizable, and interpretable approach to controlling high-dimensional redundant actuation systems. The generation method of synergistic representations can help deepen the understanding of motor intelligence. This research aims to facilitate the training of motor control policies for use in artificial intelligence, robotics and medicine, contributing to the development of a versatile embodied agent.\n\nDespite the promising outcomes, distinctions persist between real embodied motor intelligence and the musculoskeletal model simulation employed in our study. For instance, current simulation methods primarily leverage proprioception (joint position and velocity), whereas in the real world, an animal receives additional sensory inputs, including vision and touch. To enhance customization for specific applications, further work on biomechanically realistic simulations is essential. Other significant limitations include the multiple potential solutions in overactuated systems, and our method can only generate one of the numerous high-dimensional combinations to control the system. Future research may need to consider establishing a solution space of control patterns."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning, especially Embodied Intelligence. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Neuro-muscle dynamics", "content": "The input control signal of muscle-tendon units is the neural excitation ctrl, and the muscle activation act is calculated by a first-order nonlinear filter as follow:\n\n\\(\\frac{d_{act}}{dt} = \\frac{ctrl - act}{\\tau(ctrl, act)}\\), \n\\(\\tau(ctrl, act) = \\begin{cases}\n\\tau_{act} / (0.5 + 1.5 act) & ctrl > act\\\\\n\\tau_{deact} / (0.5 + 1.5 act) & ctrl \\le act\n\\end{cases}\\) (9)\n\n(\\( \\tau_{act}, \\tau_{deact} \\)) is a time constant to activate or deactivate latency of defaults (10ms, 40ms). The force produced by a single muscle-tendon unit can be formulated as\n\n\\(f_m(act) = f_{max} \\cdot [F_l(l_m) \\cdot F_v(v_m) \\cdot act + F_p(l_m)],\\) (10)\n\nwhere \\(f_{max}\\) stands for the maximum isometric muscle force and \\(act, l_m, v_m\\) respectively stand for the activation, normalized length and normalized velocity of the muscle. \\(F_l\\) and \\(F_v\\) represent force-length and force-velocity functions fitted using data from biomechanical experiments ."}, {"title": "B. Environment Details", "content": "For all environments, the simulation time step is 0.01s. The action space consists of muscle excitations ctrl (i.e. motor neuron signals). The dimensions of action and state spaces, number of joints and episode length of all the environments are summarized in Table A.1. Task and reward parameters are summarized in Table A.2.\n\nFullBody-Gait We expect the full body MS-HUMAN-700 model with 206 joints actuated by 700 muscles (Section 3.1) to mimic a motion-capture walking trajectory. During training, the model may be initialized at any time step throughout a trajectory cycle. The state space consists of simulation time, joint positions, joint velocities, muscle forces, muscle lengths, muscle velocities, muscle activation and reference joint positions. The reward function is:\n\nR=wvRv + wqRq + Whalive (11)\nRv = exp(-(vc-v)2) + exp(-(vcom \u2013 vt)2) (12)\nRq = - ||qq||2 (13)\n\nwhere q is actual joint positions, q\" is the reference joint positions,  is the velocity of the center of mass and {v, v} is the desired velocity. ww, wq and wh are the weights.\n\nMyoLegs-Walk The MyoLegs model in MyoSuite with 20 DoF and 80 muscles is used. The model is expected to walk forward robustly, driven by biomechanically inspired reward functions:\n\nR = wvRv - wcRc + wrRr + WjRj - WaRa - wdRd (14)\n\nRa = {falled}, imposes a penalty when the model falls. The weights wv, wc, wr, Wj, Wa, and wa determine the importance of each reward term. The other terms are as follow:\n\nR = exp(-vx - Ux) + exp(-v - Vy) (15)\n\nv and v represent the desired and actual velocity of the center of mass. R\u2082 represents the velocity reward.\n\nRc = ||[0.8 cos(\u00a2 \u00d7 2\u03c0 + \u03c0), 0.8 cos(\u00a2 \u00d7 2\u03c0)] \u2013 [qrhip, qlhip]|| (16)\n\n$ is the phase percentage of the pre-define gait period. qrhip and qlhip are the hip flexion angle of both legs. R, encourages rhythmic hip movements.\n\nRr = exp(-5||(qpelvis - qpelvis)||) (17)\n\nqpelvis and q_\\text{init}^{\\text{pelvis}}\n is the quaternions of pelvis and its initial value when reset. Rr encourages the model to follow a predetermined rotation pattern.\n\nRj = exp(-5|qi|/N) (18)\n\ni=1\""}, {"title": "", "content": "In the environment, N = 4 and q\u2081 are the hip abduction and rotation angles of both legs. Rj penalizes deviations from desired joint angles.\n\nRa = ||act||/Na (19)\n\nact is the muscle activation vector, Na is the number of muscles, and Ra promotes efficient actuator usage by computing the norm of the action divided by the number of actuators.\n\nThe state space consists of simulation time, joint positions (except x and y positions for the base segment), joint velocities, muscle forces, muscle lengths, muscle velocities and muscle activations. The task-specific observations include time phase percentage of the gait, velocity and height of the center of mass, torso angle, the height of the feet and their positions relative to the pelvis. The diverse terrain conditions including slopes and rough ground can be added to the task (see Figure 10)."}, {"title": "", "content": "Legs-Walk A 20-DoF Legs model actuated by 100 muscles is used. The observations and reward function are the same as MyoLegs-Walk environment except that there is no Ra term. In addition, we apply diverse walking speed targets in the simulation.\n\nArm-Locate An Arm model of 28 DoF and 81 muscles with wrist and fingers is used. The agent is trained to learn to grasp a bottle, relocate it to the target position and reorient it to the target orientation. The position and orientation of target are randomized when reset. The reward function is:\n\nR = wpRp \u00d7 woRo + wrRr - WaRa (20)\n\nRa = ||act||/Na, promotes efficient actuator usage. We use multiplication to enforce the relocation and reorientation simultaneously. The weights wp, Wo, Wr and wa determine the importance of each reward term. The other terms are as follow:\n\nRp = exp(-10/||Ptarget - Pobject||) (21)\n\nPtarget and Pobject represent the positions of the target and the object, respectively. Rp represents the reward for relocation.\n\nRo = exp(-2||Otarget - Oobject||) (22)\n\nOtarget and Oobject represent the orientation of the target and the object (in Euler angle, except for the rotation of the bottle around the vertical axis). Ro represents the reward for reorientation.\n\nRr = exp(-10||Ppalm - Pobject||) (23)\n\nPpalm and Pobject represent the positions of the palm of the arm model and the position of object. R, encourages the model to grab the object.\n\nThe state space consists of simulation time, joint positions, joint velocities, muscle forces, muscle lengths, muscle velocities and muscle activations. The task-specific observations include the positions of the object and the target, the orientations of the object and the target, the error of position and orientation, the position of the model's palm and the distance between the palm and the object.\n\nMyoHand-Reorient100 MyoHand model in MyoSuite with 23 DoF and 39 muscles needs to rotate a set of 4 objects, each with 25 different geometries, to a given orientation without dropping them. This set of 100 objects is randomly presented,"}, {"title": "", "content": "and initialized onto the hand. The reward function is:\n\nR = -wpRp + woRo - waRa - WaRa + wbRb (24)\n\nRa = ||act||/Na, promotes efficient actuator usage. The weights wp, wo, wa, Wa and wb determine the importance of each reward term. The other terms are as follow:\n\nRp = ||Ptarget - Pobject|| (25)\n\nPtarget and Pobject represent the positions of the target and the object. Rp keeps the object at its initial position (i.e. onto the palm).\n\nRo = cos(Otarget - Oobject) (26)\n\nOtarget and Oobject represent the orientations of the target and the object (in vector). Ro represents the reward for reorientation.\n\nRd = (||Ptarget - Pobject|| > 0.075) (27)\n\nRd represents the penalty for dropping objects.\n\nRb = (cos(Otarget - Oobject) > 0.9) \u00d7 (||Ptarget \u2013 Pobject|| < 0.075)\n+5 \u00d7 (cos(Otarget - Oobject) > 0.95) \u00d7 (||Ptarget - Pobject|| < 0.075) (28)\n\nRb represents the bonus reward for simultaneous rotational and positional alignment above a threshold.\n\nThe state space consists of simulation time, joint positions, joint velocities, muscle forces, muscle lengths, muscle velocities and muscle activations. The task-specific observations include the positions of the object and the target, the orientations of the object and the target, the velocities of objects, and the error of position and orientation.\n\nOstrich-Run An Ostrich model with 50 joints actuated by 120 muscles is used. It needs to run horizontally as fast as possible, rewarded only by the velocity of the center of mass projected to the x-axis.\n\nR = \u03c9\u03c5\u03bd\u03b1 (29)\n\nThe state space consists of joint positions (except x position for the base segment), joint velocities, muscle forces, muscle lengths, muscle velocities and muscle activations. The task-specific observations include the height of ostrich torso, the height of the feet and the horizontal velocity."}, {"title": "C. Implementation Details", "content": "C.1. Action normalization\nOur preliminary experiments reveal that in MyoSuite, the action space, originally [0, 1], is normalized to [-1, 1] using Equation 30. This normalization method enhances training effectiveness. Consequently, we apply this normalization approach in all our environments and algorithm comparison experiments.\n\n\\(a = \\frac{1}{1 + e^{-5(a - 0.5) }}\\) (30)"}, {"title": "C.2. Dynamical synergistic representation generation", "content": "In the process of representation generation, determining the number of groups is crucial. In Figure 11(a), we illustrate the maximum and minimum values of the distance among cluster centers for different group configurations. The algorithm exhibits robust and explainable performance when we choose an appropriate number of clusters where the difference between maximum and minimum distances are large enough. In Figure 11(b), it is evident that when the selected number of groups is 40, the t-SNE visualization maintains symmetry and interpretability.\n\nAs illustrated in Figure 12 and Figure 13, the grouping results are shown to converge to their final grouping with a data point quantity as low as 25,600. It's also displayed that even if we have only 100 data points, the grouping result is similar to the final result."}, {"title": "C.3. Baselines", "content": "We compare our algorithm with the best performing baselines, SAC, SAR and DEP-RL. SAR and DEP-RL algorithms are implemented using the official released code. SAC, SAR and DynSyn all adopt the DRL framework Stable baselines3, and DEP-RL adopts the Tonic framework. All algorithms adopt SAC as the basic algorithm. The specific algorithm parameters of SAR and DEP-RL are those reported in the original papers, and for models with similar complexity we use the same parameters. See Table A.3 and Table A.4 for details."}, {"title": "C.4. Hyperparameters of DynSyn and baselines", "content": "Algorithm hyperparameters are summarized in Table A.3 and A.4."}]}