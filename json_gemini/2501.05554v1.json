{"title": "LLMQuoter: Enhancing RAG Capabilities Through Efficient Quote Extraction From Large Contexts", "authors": ["Yuri Fa\u00e7anha Bezerra", "Li Weigang"], "abstract": "We introduce LLMQuoter, a lightweight, distillation-based model designed to enhance Retrieval-Augmented Generation (RAG) by extracting the most relevant textual evidence for downstream reasoning tasks. Built on the LLaMA-3B architecture and fine-tuned with Low-Rank Adaptation (LoRA) on a 15,000-sample subset of HotpotQA, LLMQuoter adopts a \"quote-first-then-answer\" strategy, efficiently identifying key quotes before passing curated snippets to reasoning models. This workflow reduces cognitive overhead and outperforms full-context approaches like Retrieval-Augmented Fine-Tuning (RAFT), achieving over 20-point accuracy gains across both small and large language models. By leveraging knowledge distillation from a high-performing teacher model, LLMQuoter achieves competitive results in a resource-efficient fine-tuning setup. It democratizes advanced RAG capabilities, delivering significant performance improvements without requiring extensive model retraining. Our results highlight the potential of distilled quote-based reasoning to streamline complex workflows, offering a scalable and practical solution for researchers and practitioners alike.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have revolutionized natural language processing by enabling robust performance across diverse tasks such as open-domain question answering, summarization, and conversational AI. However, as these models grow in size and capability, their computational demands and inefficiencies in handling large contexts have become an active area of research, driving efforts to develop innovative strategies for improvement (Lin et al., 2024; Jin et al., 2024; An et al., 2024). This challenge is particularly pronounced in scenarios requiring complex reasoning and retrieval of specific information from extensive textual data.\nRetrieval-Augmented Generation (RAG) has become a powerful paradigm for improving model performance by seamlessly integrating retrieval mechanisms with generative capabilities. By integrating external knowledge sources, RAG enables models to access and utilize relevant information dynamically, enhancing their domain-specific performance without necessitating extensive retraining. Despite its potential, RAG still faces limitations, especially in smaller models, which often struggle to reason effectively when confronted with large or noisy contexts (Mirzadeh et al., 2024; Hu et al., 2024a).\nTo address these limitations, we propose LLMQuoter, a lightweight model designed to enhance RAG workflows by adopting a \u201cquote-first-then-answer\" strategy. Instead of reasoning over an entire context, LLMQuoter extracts relevant textual snippets that directly support downstream reasoning tasks. This approach reduces cognitive load on reasoning models, enabling both small and large models to achieve superior accuracy with lower computational overhead.\nOur methodology builds on recent advances in knowledge distillation, where compact models are trained to emulate the capabilities of high-performing teacher models. By leveraging distillation techniques and fine-tuning a LLaMA-3B model with Low-Rank Adaptation (LoRA) (Hu et al., 2021), we demonstrate that LLMQuoter can effectively identify and extract key quotes from large contexts. These extracted quotes are then passed to reasoning models, enabling a \"quote-first-then-answer\" strategy. This approach departs from traditional full-context techniques and frameworks like Retrieval-Augmented Fine-Tuning (RAFT) (Zhang et al., 2024a), where the model quotes, thinks, and answers in a single step. By decoupling these stages, LLMQuoter simplifies the reasoning process and reduces cognitive overhead for downstream models, offering an alternative and efficient pathway for retrieval-augmented generation (RAG) pipelines.\nThis paper evaluates LLMQuoter using the DSPy framework (Khattab et al., 2023) for semantic evaluation, leveraging a 15,000-sample subset of the HotpotQA dataset (Yang et al., 2018). This dataset is commonly used for RAG problems and was also utilized in RAFT, the solution that served as our starting point. The results show that LLMQuoter is a standout solution, holding its own against RAG techniques like RAFT. It delivers impressive accuracy gains without compromising on computational efficiency, making it both effective and resource-friendly. Furthermore, the lightweight nature of LLMQuoter democratizes access to advanced RAG capabilities, providing a scalable solution for researchers and practitioners with limited resources. The rest of this paper is organized as follows: Section 2 delves into the background, covering LLM reasoning, knowledge distillation, the RAFT approach, and evaluation methods for LLMs. Section 3 outlines the methodology behind the proposed solution. In Section 4, we detail the experimental setup and key insights from the process, while Section 5 discusses the results and highlights the advantages of quote-based reasoning. Finally, Section 6 wraps up the paper with conclusions and potential avenues for future research."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 LLM Reasoning", "content": "Reasoning remains a core challenge for Large Language Models (LLMs), with both large and small models facing distinct limitations. Large models excel in generalization but struggle with intricate logical reasoning and multi-step problem-solving. Rather than genuinely reasoning, they often replicate patterns from their training data, which can lead to significant performance drops when faced with tasks requiring clause integration or minor input variations, as highlighted in the GSM-Symbolic study (Mirzadeh et al., 2024). Smaller models, while resource-efficient, suffer from capacity constraints, making them prone to losing context in reasoning-intensive tasks, as demonstrated in experiments with MiniCPM, which attempts to match the reasoning performance of larger models (Hu et al., 2024a).\nTo mitigate these challenges, split-step reasoning has emerged as a promising solution. By dividing reasoning tasks into distinct phases, such as problem decomposition and problem-solving, smaller models can focus on manageable subtasks, improving their generalization and inference efficiency (Wu et al., 2024). Advanced techniques like Generative Context Distillation (GCD) and task-specific fine-tuning further enhance reasoning accuracy while preserving computational efficiency (Fu et al., 2024b). This approach has also been successfully applied in specific domains such as sarcasm detection, where frameworks like chain-of-contradictions outperform holistic reasoning approaches, particularly in smaller models (Yao et al., 2024).\nSelf-correction mechanisms provide an additional boost to reasoning capabilities. Training pipelines that incorporate self-correction, where models generate critiques of their incorrect answers, have proven effective, particularly when pairing small models with strong verifiers (Zhang et al., 2024b). Domain-specific approaches, such as multi-modal assistants integrating textual and visual reasoning, further demonstrate that smaller models can achieve sophisticated reasoning abilities when tailored strategies are employed (Zhu et al., 2024).\nThese studies underscore the importance of split-step reasoning and specialized training frameworks to address the limitations of both large and small LLMs. By leveraging strategies that combine task decomposition, fine-tuning, and self-correction, researchers can design models that effectively balance scalability and reasoning performance across diverse applications."}, {"title": "2.2 Knowledge Distillation in LLMs", "content": "Knowledge distillation has become a vital technique for reducing the computational demands of large language models (LLMs) while preserving their advanced capabilities. The process transfers knowledge from a high-capacity teacher model to a more efficient student model, enabling smaller models to perform complex tasks such as reasoning, recommendation, and maintaining factual accuracy, all with significantly lower resource consumption. Techniques like Generative Context Distillation (GCD), for instance, streamline inference by internalizing prompts rather than relying on explicit ones, enhancing efficiency (Fu et al., 2024a). Similarly, rationale-based approaches help compact models achieve state-of-the-art performance in recommendation tasks by improving model profiling (Hu et al., 2024b).\nThe applications of LLM distillation are diverse, ranging from mitigating hallucinations to enhancing recommendation systems and enabling cost-effective deployment. Techniques such as temperature scaling and intermediate layer matching have been shown to reduce hallucination rates while improving accuracy (Gogate et al., 2024). In the domain of recommendation systems, rationale-based and importance-aware ranking distillation techniques allow smaller models to effectively learn from user-item interactions, striking a balance between computational efficiency and performance (Cui et al., 2024). Task-specific strategies, such as dividing reasoning tasks into problem decomposition and solution phases, further improve the generalizability and inference efficiency of compact models (Wu et al., 2024).\nDespite its advantages, knowledge distillation faces challenges, including capacity gaps between teacher and student models and semantic divergence in embedding spaces. Solutions like collaborative embedding distillation and ranking distillation have been developed to bridge these gaps, allowing smaller models to align more closely with their larger counterparts in semantic reasoning (Cui et al., 2024). As researchers continue to optimize these methods, they push the boundaries of LLM distillation, making smaller, efficient models increasingly viable for a broad spectrum of AI applications."}, {"title": "2.3 RAFT (RAG + FT)", "content": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted technique for domain-specific applications, offering an efficient way to leverage pre-trained LLMs without requiring extensive retraining. RAG is particularly advantageous as it allows models to retrieve relevant information from external knowledge bases or documents, making it an effective approach in fields such as healthcare, legal analysis, and technical documentation (Su et al., 2024; \u0130rican et al., 2024). Additionally, RAG is increasingly being utilized as an external memory mechanism for LLMs (Alonso et al., 2024; Jim\u00e9nez Guti\u00e9rrez et al., 2024).\nHowever, not all models especially smaller ones are capable of effectively handling large contexts and reasoning simultaneously (Zhang et al., 2024b; Chen et al., 2024). Even when provided with the appropriate context, these models often fail to generate coherent or accurate answers, revealing a gap in their ability to integrate retrieval with reasoning. To address these limitations, techniques like RAFT, Retrieval-Augmented Fine-Tuning, have emerged, aiming to fine-tune LLMs specifically for RAG in domain scenarios (Zhang et al., 2024a; Di Oliveira et al., 2024).\nRAFT focuses on training models to \u201cthink while quoting,\" combining reasoning with the ability to extract and reference relevant portions of the retrieved context. This dual focus enables the model to dynamically identify key parts of the input text, synthesize their meaning, and produce a well-reasoned final answer. By teaching models to reason and quote in tandem, RAFT enhances their ability to operate effectively in RAG settings, bridging the gap between retrieval and generation to deliver more accurate, context-aware responses. In summary, RAFT trains a model to perform chain-of-thought (CoT) reasoning, transforming a task structured as context + question into a comprehensive output of reasoning, relevant quotes, and the final answer (Figure 1)."}, {"title": "2.4 Semantic Evaluation", "content": "Semantic evaluation of Large Language Models (LLMs) has emerged as a critical area of research as these models continue to excel in tasks such as text summarization, question answering, and open-domain generation. With their growing sophistication, there has been a corresponding rise in prompt-based systems designed work with LLM as the product brain (Liu et al., 2024; Garcia et al., 2024; T Wijesiriwardene, 2024). However, evaluating the effectiveness of LLM-generated outputs objectively remains a significant challenge (Hu and Zhou, 2024). Traditional metrics often fall short in capturing the nuanced semantics and creative aspects of these models' outputs, necessitating the development of more refined evaluation frameworks (Khattab et al., 2023).\nTraditional metrics such as BLEU and ROUGE are limited by their reliance on surface-level token overlaps, failing to capture the semantic depth and creativity of LLM-generated outputs(van Schaik and Pugh, 2024). This limitation has driven the development of more robust and adaptable evaluation frameworks, including LLMs themselves as evaluators. Frameworks like GPTScore (Fu et al., 2023) and AlpacaEval (Dubois et al., 2024) exemplify this shift:"}, {"title": "3 METHODOLOGY", "content": "With the goal of developing an efficient language model for extracting relevant quotes from contexts to properly answer questions about it, this section details the methodology employed in training and evaluating the distilled LLM. The process involves leveraging a high-performing LLM for dataset creation, fine-tuning a smaller LLM, and validating the approach with task-specific metrics.\nWe begin with a formalization of the distillation problem in Section 3.1, followed by an overview of the fine-tuning process in Section 3.2. Finally, the evaluation framework and metrics used to validate the model's performance are described, along with a simple approach to demonstrate the benefits of extracting relevant quotes instead of using the large content itself."}, {"title": "3.1 Problem Formalization", "content": "Let us consider a dataset of text samples, denoted by $D = \\{(C, Q,A)\\}$, where:\n\u2022 C: a large text context.\n\u2022 Q: a specific question.\n\u2022 A: the expected answer.\nThe task is to train a model capable of extracting relevant quotes from C that support A in response to Q. To achieve this, we employ a distillation process in which a large LLM generates high-quality training data, and a smaller LLM is fine-tuned on this dataset to efficiently replicate the behavior of the larger model."}, {"title": "3.2 LLM Distillation", "content": "The dataset creation process can be formalized as follows: Given a high-performance language model $f_{high}$, such as ChatGPT or Gemini, the task is to extract quotes R from a context C that directly support an answer A in response to a question Q. Formally, this process can be represented as:\n$f_{high}: (Q,A,C) \\rightarrow R$\nFor each data point (Q,A,C), the high-performance model $f_{high}$ generates the set of quotes R, which serve as the ground truth:\n$D_{gold} = \\{(Q,A,C,R) \\vert R = f_{high}(Q,A,C)\\}$\nThe result is a high-quality dataset $D_{gold}$, consisting of tuples (Q,A,C, R), where R represents the relevant quotes extracted by $f_{high}$. This dataset is then used to train and evaluate the smaller distilled model $f_{small}$."}, {"title": "3.3 Fine-Tuning LLM with LoRA", "content": "The smaller model $f_{small}$ is fine-tuned on the $D_{gold}$ dataset using Low-Rank Adaptation (LoRA) for task-specific learning in the extraction of relevant quotes. The fine-tuning process is defined as:\n$f_{small}: (Q,C) \\rightarrow R$\nwhere Q represents the question, C is the textual context, and R is the set of relevant quotes generated by the fine-tuned model. The training process is described in the following steps:\n1. Input: Data from the $D_{gold}$ dataset in the form of tuples (Q, C), where Q is the question, C is the textual context.\n2. Output: The fine-tuned model $f_{small}$ is optimized to predict R, replicating the behavior of the larger model $f_{high}$, but without knowing the answer."}, {"title": "3.4 Evaluation Framework and Metrics", "content": "The model's performance is evaluated using the DSpy framework, which computes task-specific metrics tailored to LLM outputs. Precision and recall are redefined for the quote extraction task using an LLM Judge to assess semantic relevance between model predictions and ground truth."}, {"title": "3.5 Proving the Benefit of Using Quotes", "content": "Let $f_{base}$ represent base models without any fine-tuning to establish a baseline for comparison. Two experimental setups are defined to demonstrate the advantage of using relevant quotes R instead of the full context C:\n1. Providing only the gold quotes R from $D_{gold}$ to the base models $f_{base}$ to answer the questions:\n$f_{base}: (Q, R_{gold}) \\rightarrow A_{base}$\n2. Providing the full context C instead of the quotes R to the same base models $f_{base}$ to answer the questions:\n$f_{base}: (Q,C) \\rightarrow A_{base}$\nFor both setups, Q represents the question, $R_{gold}$ is the set of gold quotes extracted from the $D_{gold}$ dataset, C is the entire context, and $A_{base}$ is the base models answers.\nThe accuracy of the answers produced by $f_{base}$ is measured using Semantic Accuracy ($S_{acc}$), which evaluates the alignment between the model-generated answers $A_{base}$ and the expected answers $A_{gold}$. Semantic Accuracy is defined as:\n$S_{acc} = \\frac{\\sum_{a \\in A_{base}} Judge (a, A_{gold})}{\\vert A_{gold} \\vert}$\nwhere Judge(a, $A_{gold}$) is a semantic similarity function scoring the alignment between a model-generated answer a and the ground truth $A_{gold}$, with scores ranging from 0 (no match) to 1 (perfect match)."}, {"title": "4 EXPERIMENTS", "content": "This section describes the experimental setup used to analyze the performance of the proposed methodology. It begins with details of the datasets used for training and evaluation, followed by an explanation of the training configurations, including hyper-parameters and computational resources. An overview of the entire process, from data distillation to evaluation, is illustrated in Figure 2. Finally, the experiments designed to validate the effectiveness of using relevant quotes instead of full context are presented (Figure 3 illustrates the process). The code utilized in this work is available on GitHub\u00b9. Concrete examples of the experimental results can be found in the appendix for further clarification."}, {"title": "4.1 Datasets", "content": "Our method was evaluated on the HotpotQA dataset (Yang et al., 2018), an open-domain question-answering benchmark derived from Wikipedia, with a focus on common knowledge topics such as movies, sports, and general trivia. The dataset consists of three columns: question, context, and answer, where each sample pairs a question with a large textual context and its corresponding answer.\nDue to resource constraints, a random subset of 15,000 samples was selected from the original dataset to serve as the basis for applying the distillation process. From this subset, 600 samples were set aside for evaluation purposes, forming the test set. This test set was used to measure the model's performance during the evaluation phase and to validate the benefit of using extracted quotes as opposed to the entire context"}, {"title": "4.3 Fine-Tuning Process", "content": "The fine-tuning process was applied to the smaller LLM, LLAMA 3.2 3B, using the Low-Rank Adaptation (LoRA) technique to optimize the model for the quote extraction task. LLAMA 3.2 3B was chosen as the base model due to its balance between computational efficiency and task-specific adaptability. The fine-tuning process was completed over a single epoch, ensuring efficient adaptation without overfitting.\nThe fine-tuning process was conducted on a NVIDIA A100-SXM4-40GB GPU, with a maximum memory capacity of 39.564 GB. The specific resource utilization and training parameters are summarized below:"}, {"title": "4.4 Evaluation and Proving the Benefits", "content": "The evaluation of the extracted quotes was performed using the DSpy framework in conjunction with Ope"}, {"title": "5 RESULTS AND DISCUSSION", "content": "This section presents the experimental results obtained by evaluating the quote extraction model (quoter) and validating the benefit of using quotes over full context in open-domain question-answering tasks. The results demonstrate the effectiveness of the proposed method in improving the performance of both small and large language models in RAG (retrieval-augmented generation) scenarios."}, {"title": "5.1 Evaluation of the Quoter Model", "content": "The performance of the quoter model was evaluated using the metrics described in Section 4.3. The recall, precision, and F1-score were measured both before and after fine-tuning the smaller LLM using the LORA approach. The results are summarized in Table 3."}, {"title": "5.2 Benefits of Using Quotes Over Full Context", "content": "To validate the benefit of using quotes instead of full context, a comparison was performed using original models without any training. Both the gold quotes and the full context were provided as inputs to different models: LLAMA 1B, LLAMA 3B, and GPT-3.5 Turbo. The accuracy of the answers generated by each model in these two configurations is summarized in Table 4."}, {"title": "5.3 Discussion", "content": "The results validate the hypothesis that using extracted quotes instead of full context significantly improves model performance in open-domain question-answering tasks. This finding aligns with the original RAFT approach, which involves reasoning and answering directly over the full context. However, our experiments demonstrate that separating the tasks-first extracting quotes with a simple quoter and then reasoning over the concise data can lead to comparable or better outcomes with lower computational overhead."}, {"title": "6 CONCLUSIONS AND FUTURE WORK", "content": "This study demonstrates the effectiveness of data distillation and lightweight training for enhancing Retrieval-Augmented Generation (RAG) systems. By leveraging a high-performing teacher model to distill relevant quotes and fine-tuning a compact model, we achieved significant improvements in model performance. The fine-tuning process required minimal resources, with just 5 minutes of training on an NVIDIA A100 GPU, yet delivered robust results.\nThe experiments validate that an efficient quoter model can substantially enhance RAG performance by reducing the cognitive load on the reasoning process. By focusing the model's efforts on the answer rather than processing and reasoning over large contexts, we eliminate the need for extensive training while improving accuracy. This approach aligns with the principle of \u201cdivide and conquer,\" where the reasoning task is simplified and made more manageable for even small models. Ultimately, our results demonstrate that high-quality quote extraction can democratize access to high-performing RAG capabilities across a range of computational constraints.\nWhile this work has established a strong foundation for quote-based RAG, several avenues for future research remain open:\n\u2022 Expanded Datasets: Testing the methodology on a wider range of datasets, including those with different domains and levels of complexity, and utilizing larger samples from each dataset will provide a more comprehensive evaluation of the approach.\n\u2022 Reinforcement Learning: Incorporating rein"}]}