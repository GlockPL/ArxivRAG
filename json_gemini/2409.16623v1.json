{"title": "On Your Mark, Get Set, Predict! Modeling Continuous-Time Dynamics of Cascades for Information Popularity Prediction", "authors": ["Xin Jing", "Yichen Jing", "Yuhuan Lu", "Bangchao Deng", "Sikun Yang", "Dingqi Yang"], "abstract": "Information popularity prediction is important yet challenging in various domains, including viral marketing and news recommenda-tions. The key to accurately predicting information popularity lies in subtly modeling the underlying temporal information diffusion process behind observed events of an information cascade, such as the retweets of a tweet. To this end, most existing methods either adopt recurrent networks to capture the temporal dynamics from the first to the last observed event or develop a statistical model based on self-exciting point processes to make predictions. However, information diffusion is intrinsically a complex continuous-time process with irregularly observed discrete events, which is oversimplified using recurrent networks as they fail to capture the irregular time intervals between events, or using self-exciting point processes as they lack flexibility to capture the complex dif-fusion process. Against this background, we propose ConCat, modeling the Continuous-time dynamics of Cascades for information popularity prediction. On the one hand, it leverages neural Ordinary Differential Equations (ODEs) to model irregular events of a cascade in continuous time based on the cascade graph and sequential event information. On the other hand, it considers cascade events as neural temporal point processes (TPPs) parameterized by a conditional intensity func-tion which can also benefit the popularity prediction task. We conduct extensive experiments to evaluate ConCat on three real-world datasets. Results show that ConCat achieves superior performance compared to state-of-the-art baselines, yielding 2.3%-33.2% improvement over the best-performing baselines across the three datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "With the proliferation of information on the Web, the prediction of information popularity in the future is of critical importance [1], serving as the foundation of building various applications such as recommendation [2], influence maximization [3], risk management [4], and rumor detection [5]. As typical examples, social networking platforms, such as Facebook\u00b9 and Twitter\u00b2, incorporate resharing or reposting features that enable users to disseminate content created by others in their online social networks via their friends or followers, in a cascade manner. Such an information dissemination process intrinsically forms an information cascade through the social network [6]. In this context, accurately predicting the popularity of such cascades, which refers to the number of potentially reached users in a future period of time, has attracted increasing attention from both academia and industries, ranging from marketing and traffic control to risk management [4].\nIn recent years, many works have focused on understanding the dynamic diffusion process and popularity prediction of cascades [6]\u2013[9]. These works mainly fall into three categories. First, feature-based approaches manually extract a set of potentially relevant features [10], such as publication time [11], [12], user interest [13], and then employ various machine learning techniques for prediction, such as regression models [14] and content-based models [15]. However, these works heavily rely on the quality of hand-crafted features and thus are dataset-specific and have limited generalizability. Second, generative-based approaches resort to generative probabilistic models [4], [16] for characterizing the cascade diffusion process, but they are built under strong prior assumptions, such as Poisson or Hawkes processes, which are often inflexible to capture the complex diffusion process [17]. Finally, deep-learning-based approaches design different deep neural architectures to capture both structural information and temporal dynamics of cascades for information popularity prediction. On the one hand, the structural information of a cascade refers to the interconnections between nodes within a cascade graph. For example, in a retweet cascade graph, as shown in Figure 1, each retweet event of a user is represented as a node of the user with an incoming edge from the retweeted user. Existing approaches usually resort to graph embedding techniques to learn node embeddings [18]\u2013[20] or sample cascade paths as input sequences [21], [22] to encode the structural properties of the cascade graph. On the other hand, the temporal dynamics of the cascade refers to the sequential/temporal patterns of the retweet events in the cascade. Existing approaches mostly regard"}, {"title": "2 PRELIMINARIES", "content": "In the section, we firstly define the cascade graph, the global graph, and the cascade popularity prediction problem formally with the corresponding notations and then give a brief introduction to neural ODEs and neural TPPs."}, {"title": "2.1 Problem Definition", "content": "We take the retweet cascade as an illustrative example as shown in Figure 1. However, it's important to note that our work can be easily applied to various modes of information diffusion, such as the Weibo platform, academic publica-tions, and so on. Table 1 presents a comprehensive overview of the notations used in this paper.\nAs shown in Figure 1, a Twitter user, denoted as uo, posts a tweet I at time to = 0 (we set the original tweet time to 0). Subsequently, other users can engage with this tweet through various actions, such as commenting, expressing approval through liking, and retweeting. In this paper, we specifically focus on the event of \"retweeting\" which is widely recognized as a significant means of information dissemination on the Twitter social network. After uo posts the tweet, four other users retweet this tweet. For example, user u\u2081 reposts the tweet from user uo at time t\u2081, forming a cascade connection as a triplet (40, 41, t\u2081) and information I diffuses from user us to user u\u2081. Subsequent serialized triplets build up a retweet cascade C and the corresponding cascade graph G. Our goal is to predict the future popularity of the information cascade C between the observation time and prediction time, that is the number of triplets in C within this period of time.\nSpecifically, given an observation time ts, we define the retweet cascade at time ts as C(ts) = {(Uk1, Uk2, tk)}k\u2208M, where M indicates there are M triplets involved in the diffusion process and tk < ts. In the following, we first give the formal definition of the cascade graph and global graph, followed by the definition of the cascade popularity prediction problem based on these graphs."}, {"title": "Definition 1. Cascade Graph", "content": "Given a tweet I and its corresponding retweet cascade C(ts) observed at time ts, its cascade graph G(ts) is defined as G(ts) = (Ve(ts), Ec(ts)), where Ve(ts) is the user set of triplets in C(ts) and Ec(ts) is the edge set in the cascade graph where an edge presents that there exists a retweeting action between two nodes."}, {"title": "Definition 2. Global Graph", "content": "Given all the retweet cascades under the observation time, we define the global graph as G = (Vg, Eg), where the edge in Eg represents the node relationship from cascading, such as the follower/followee relationship in the social network."}, {"title": "Definition 3. Cascade Popularity Prediciton", "content": "Given the observed retweet cascade C(ts) at ts, we predict the incremental popularity P = |C(tp)| \u2013 |C(ts)|, where ts is the observation time, tp is the prediction time and C denotes the number of triplets contained in the cascade C."}, {"title": "2.2 Neural Ordinary Differential Equations", "content": "A neural Ordinary Differential Equation (ODE) describes the continuous-time evolution of variables. It represents a transformation of variables over time, where the initial state at time to, denoted as h(to), is integrated forward using an ODE to determine the transformed state at any subsequent time ti.\n$\\frac{dh(t)}{dt}$ = $f(h(t), t;0)$ where $h(0) = h_0$   (1)\n$h(ti) = h(to) + \\int_{to}^{ti} \\frac{dh(t)}{dt}dt$, (2)\nwhere f denotes a neural network, such as a feed-forward or convolutional network, parameterized by 0 to characterize the ODE dynamics."}, {"title": "2.3 Neural Temporal Point Processes", "content": "A Temporal Point Process (TPP) is a stochastic process that characterizes event sequences of varying lengths within a specified time interval [0,T]. A TPP realization can be represented as a sequence of arrival times, denoted as t = (t1,..., tv), where N represents the random variable indicating the number of events, and the arrival times are strictly increasing.\nThe characterization of a TPP involves the definition of the conditional intensity function *(t) and the calculation of the corresponding log-likelihood.\n$logp(t) = \\sum_{i=1}^{N}log\\lambda^* (t_i) - \\int_{0}^{T} \\lambda^*(\\tau)d\\tau$ (3)\nBy minimizing this Negative Log-Likelihood (NLL), the parameters are estimated/learnt to represent the conditional intensity *(t) to describe the dynamics at different times. In particular, to capture long-range dependency structure, some recent works [28] utilize neural networks to parameterize the conditional intensity of TPPs, and thus these methods are denoted as neural TPPs."}, {"title": "3 CONCAT", "content": "In this section, we present the details of our proposed method ConCat to model the Continuous-time dynamics of Cascades for cascade popularity prediction, which considers not only the structural information of the cascade graph and global graph but also the irregular continuous-time dynamics characteristic to make information popularity pre-diction. As shown in Figure 2, ConCat consists of four main components:\n\u2022 (1) Structural learning: It models and captures contex-tualized structural characteristics of the cascading graph during diffusion. We use heat wavelet diffusion patterns to learn structural representations of nodes. Meanwhile, we formulate global graph embedding as sparse matrix factorization to efficiently extract node representations from a global view.\n\u2022 (2) Temporal dynamics modeling: Neural ODEs are used to model the continuous information diffusion process. For each newly observed event, we utilize a self-attention mechanism to capture the sequential information from the initial event to the current event and take it as the jump condition in GRU. More importantly, neural ODEs allow us to align the last hidden states of different cascades at a unified observation time (i.e., \"On Your Mark\") for popularity prediction.\n\u2022 (3) Global trend modeling: To capture the global charac-teristic of the cascades graph, ConCat uses the conditional intensity function to model cascades from the view of TPPs and considers the impact of the integral of intensities for prediction.\n\u2022 (4) Prediction: ConCat combines the aligned hidden states and the integral of the conditional intensity and feeds them into a multi-layer perception (MLP) to make the final popularity prediction."}, {"title": "3.1 Structural Learning", "content": "This component directly uses existing graph embedding methods to model both the cascade graph and the global graph. For the cascade graph, following previous work [9], [18], we use GraphWave [29] to capture the local structural information. For the global graph, we employ a fast and scalable network embedding approach NetSMF [30] to obtain connections between all users in the network."}, {"title": "3.1.1 Cascade Graph Learning", "content": "In our study, we utilize GraphWave [29] to capture the intrinsic structural characteristics of the cascade graph. This technique enables us to generate a node-level representa-tion by learning the diffusion patterns of spectral graph wavelets for each node. It has been empirically shown as an effective method compared to other graph embedding techniques for cascade popularity prediction problems [9], [18]. Specifically, given the observed cascade graph G(ts) at observation time ts, we set the weight between nodes as the time interval of retweet triplet time to observation time and we leverage heat wavelet diffusion patterns to get each node's low-dimensional embeddings Ec(ui) in the cascade graph, where ui \u2208 Vc."}, {"title": "3.1.2 Global Graph Learning", "content": "For the reason that the global graph is a large-scale network, we consider three well-known fast and scalable network embedding methods, including ProNE [31], NetSMF [30] and AROPE [32]. We chose NetSMF to get the representation of the global graph due to its superior performance as evidenced by our experiments later:\n$E_g(u_i) = NetSMF(G),$ (4)\nwhere G = (Vg,Eg) and ui \u2208 Vg. NetSMF formulates the global graph as a sparse matrix and makes use of spectral sparsification to efficiently sparsify the dense matrix in NetMF [33] with a theoretically bounded approximation error. At last, it performs randomized singular value de-composition to efficiently factorize the sparsified matrix, outputting the node embeddings for the global graph."}, {"title": "3.2 Temporal Dynamics Modeling", "content": "In this component, to model the irregular temporal dynam-ics in the cascade graph, we leverage neural ODEs to capture the temporal patterns with real-valued timestamps. How-ever, ODEs cannot incorporate discrete events that abruptly change the latent vector. So we consider a sudden jump conditioned on sub-sequence representation to enhance the continuous dynamic flow."}, {"title": "3.2.1 Modeling Sequential Information with Self-Attention", "content": "After getting the embeddings E(u) of each node in the cascade graph with real-valued timestamps and the embed-dings Eg(u) of each node in the global graph without real-valued timestamps, we model the sequential information of the two graphs with two self-attention modules.\nGiven a cascade graph G(ts), we have sequential node embeddings Ec = {Ec(40), ..., Ec(ui), ..., Ec(UN)}uzev with the correspond-ing timestamps (to, t1,\u2026\u2026, tv) and the global graph node embeddings Eg = {E9(40), ..., \u0395g(Ui), ..., Eg(UN)}uz\u2208Vg, where UN is the last node.\nWe split one cascade sequence into multiple sub-sequences S = {(uo), (uo, u\u2081), ..., (uo, U1, ..., UN)} according to chronological order. We use the self-attention [34] for each sub-sequence to get the long-term history information. We define a temporal encoding procedure for each times-tamp by trigonometric functions:\n$[z(ti)]_j = \\begin{cases} cos(ti/10000), \\ if j is odd, \\ sin(t_i/10000), if j is even. \\end{cases}$ (5)\nWe deterministically compute z(ti) \u2208 RM, where M is the dimension of encoding. Then, the embedding of each event in the sequence is specified by\nx(i) = z(ti) + Ec(ui), (6)\n$E_{ci} = {x(0), x(1), ..., x(i)},$ (7)\n$E_{gi} = { E_g(u_0), E_g(u_1), ..., E_g (u_i)},$ (8)\nwhere Eci and Egi are the i-th sub-sequence embedding of the cascade graph and global graph respectively and we pass them through two self-attention modules. Specifically, the scaled dot-product attention [34] is defined as:\n$S = Attention(Q, K, V) = Softmax(\\frac{QKT}{\\sqrt{d}})V,$ (9)\nwhere Q, K, V represent queries, keys, and values. In our case, the self-attention operation takes the embedding Eci and Egi as input, and then converts it into three matrices by linear projections:\n$Q = EWQ, K = EWK,V = EWV,$ (10)\nwhere WQ, WK and WV are weights of linear projections. We use a position-wise feed-forward network to transform the attention output S into the hidden representation s. For all sub-sequences, we all employ the above self-attentive operation to generate hidden representation Sc0, Sc1, \u2026, ScN and S90, $91, ..., SgN for cascade graph and global graph respectively. Then we concatenate the two hidden represen-tations, denoted as 80, 81, ..., SN, and take it as the jump condition in the dynamic flow."}, {"title": "3.2.2 Modeling Continuous Dynamics with Neural ODEs", "content": "After getting the jump condition 80, 81, \u2026\u2026\u2026, SN, we leverage neural ODEs to model the dynamics with a vector represen-tation ht, at every timestamp t\u2081 that acts as both a summary of the past history and as a predictor of future dynamics. Meanwhile, by making instantaneous updates 80, 81, ..., SN to the hidden state ht, we can incorporate abrupt changes according to new observed events.\nHere we use a standard multi-layer fully connected neural network f\u2081 to model the continuous change in the form of an ODE. When a new retweeting action occurs at time ti, we use a GRU function g to model instantaneous changes based on a newly observed node:\n$\\frac{dht_i}{dt} = f1(t, ht-1)$ (11)\n$ht_i = ODESolve(f1, ht_{i-1}, (ti\u22121, ti))$ (12)\n$ht_i = g(ht_i, Si)$ (13)\nBy solving equation 12, we can get a sequence of hidden states (hto, ht1,..., ht\u221a) after each jump. To propagate the last event of different cascades to the fixed observation time, we also use the ODE to get the hidden representation hts exactly at the observation time based on the final output state:\n$ht_s = ODESolve(f1, ht_N, (t_N,ts))$ (14)\nWe use the hidden representation hte as the cascade latent vector for later popularity prediction."}, {"title": "3.3 Global Trend Modeling", "content": "In this section, we leverage neural temporal point processes to model the probability distribution over variable-length event sequences in continuous time for capturing the global trend in the cascade graph.\nHere we use the conditional intensity function A*(t) to define the temporal point process and parameterize *(t) with neural networks. In the cascade graph, we consider the occurring time of every node as the temporal point process (to, t1,..., tv) and we use the hidden state dynamics ht to represent the intensity function:\n$\\lambda^*(t) = f2(ht)$ (15)\nwhere f2 is a neural network with a softplus nonlinearity applied to the output, to ensure the intensity is positive. Meanwhile, we compute the integral of the conditional intensity *(t) based on neural ODE between 0 to ts:\n$\\frac{dAt_i}{dt} = f(t, At-1) where Ato 0$ (16)\nWe use the ODE dynamics to represent the intensity function because At is difficult to compute; most TPPs approximate the integral by using Monte Carlo integration or numerical integration which creates a deviation, while ODE-based methods are closed-formed for computing the integral and neural TPP is an enhancement to the dynamic flow representation.\nBy solving equation 16, we can get the final integral At.. The intensity of TPPs is determined by the distribution of"}, {"title": "3.4 Prediction", "content": "After obtaining the final hidden state hte at observation time ts and the integral Ats from 0 to ts, we concatenate the integral with the final hidden state and then feed them into an MLP to make the final cascade popularity prediction:\n$P = Softplus(MLP([At_s, ht_s]))$ (17)\nwhere the softplus activation function is to ensure the pre-dicted popularity is positive. The loss function is defined as:\n$\\mathcal{L} = (log_2(P + 1) - log_2(\\hat{P} + 1))^2 + ( -\\sum_{i=1}^{N}log\\lambda^*(t_i) - \\int_{0}^{ts} \\lambda^*(\\tau)d\\tau)$ (18)\n$ \\int_{0}^{ts} \\lambda^*(\\tau)d\\tau = At_s$ (19)\nwhere P is the truth popularity and P is the predicted popularity. The first term is the mean squared logarithmic error, which is a typical loss for regression problems. The second term is the negative log-likelihood of the temporal point process. Note that At\uff61 is the integral of the conditional intensity function A*(t) between 0 to ts."}, {"title": "4 EXPERIMENTS", "content": "In this section, we present an overview of our benchmark datasets and then evaluate our model against state-of-the-art baselines in information cascade popularity prediction task to answer the following questions:\n\u2022 RQ1: Compared to state-of-the-art baselines, can our approach achieve more accurate prediction for cascade popularity?\n\u2022 RQ2: What's the impact of different pre-processing settings on performance, such as the number of triplets?\n\u2022 RQ3: Why do we need neural ODE to model the irregular cascades? How much does it contribute to performance improvement?\n\u2022 RQ4: What are the benefits of employing neural TPPs for enhancing the modeling of spatiotemporal dynamics? In contrast, how does the utilization of simpler models such as RNNs capture the global trend?\n\u2022 RQ5: Which ODESolver should we choose and what is the impact of the hyperparameters?"}, {"title": "4.1 Experimental Settings", "content": "We conduct experiments on several benchmark cascade datasets, considering Sina Weibo [8], Twitter [35] and APS [4], that have been commonly used in previous re-lated works [4], [8], [19] for evaluating cascade popularity prediction models. Note that the Twitter dataset, originally collected by [35], consists of publicly available English-written tweets and primarily focuses on examining the impact of network and community structure on diffusion. In this dataset, each hashtag and its corresponding adopters are treated as independent information cascades, but a large number of original tweets' information is missing. For example, user B retweets a tweet from user A at time t which is recorded by the dataset, but the time when user A is involved in the hashtag is not available. A significant number of retweeting records, specifically 762,775 out of 1,687,704 (45.2%), lack the timestamps associated with user A in this example, which prevents us from extracting a real-world cascade information diffusion with sufficient tempo-ral information. We encourage future works in this direction to refrain from using this Twitter dataset in experiments. Against this background, we collected a new Twitter dataset crawled from the general Twitter stream. The details of our datasets in experiments are as follows:\n\u2022 Twitter dataset we collected includes the tweets from the general Twitter stream during the period from March 1 to April 15, 2022. We take a hashtag where the original tweet and their retweets with the same hashtag as an information cascade. We select the original tweets during the period from March 1 to March 31, 2022, and their retweets until April 15, ensuring a minimum of 15 days for propagating.\n\u2022 Sina Weibo is the largest microblogging platform in China. On Weibo, each original post (also referred to as tweet hereinafter) and its subsequent retweets can generate a retweeting cascade. Due to the diurnal rhythm effect in Weibo [8], we focus on tweets posted between 8 a.m. and 6 p.m., allowing each tweet at least 6 hours to gather retweets.\n\u2022 American Physical Society (APS)5 contains scientific papers published by APS journals. Every paper in the APS dataset and its citations form a citing cascade. We consider papers published between 1893 and 1997, ensuring a minimum of 20 years (1997 - 2017) for each paper to"}, {"title": "4.1.2 Baselines", "content": "We select eight state-of-the-art baselines for comparison as follows:\n\u2022 Feature-based approaches extract various hand-crafted features and here we use the following features: the size of the observed cascade, the temporal interval between the original node and its initial forwarding, the time at which the last retweet occurs, and the average duration of the diffusion process from the initial node to the ultimate node like [9]. These features are subsequently fed into an MLP model for the purpose of predicting the popularity of cascades.\n\u2022 SEISMIC [16] develops a no-training and no-expensive feature engineering framework which is based on the theory of self-exciting point processes.\n\u2022 DeepCas [21] introduces a novel approach to representing cascades which are conceptualized as a collection of random walk paths and it employs a bi-directional GRU with an attention mechanism to effectively model and predict cascades.\n\u2022 DeepHawkes [8] integrates the Hawkes process and deep learning techniques for the purpose of modeling cascades. It takes into account three important factors from the perspective of the Hawkes process: the impact of users, the self-exciting mechanism, and the time decay effects.\n\u2022 CasCN [19] samples a cascade graph by organizing it into a sequence of sub-cascade graphs. It effectively captures the directions of diffusion and the timing of retweeting through the utilization of graph convolutional networks and LSTM.\n\u2022 VaCas [18] proposes a hierarchical graph learning method and it leverages variational autoencoder for modeling the uncertainty between the sub-graphs and the whole cascades.\n\u2022 CasFlow [9] is the extension of VaCas. It mainly considers the effect of both the local graph and global graphs and leverages scalable graph learning methods to represent user behavior for predicting popularity.\n\u2022 CTCP [27] combines all cascades into a diffusion graph and takes the correlation between cascades and the dy-namic preferences of users into account. It proposes an evolution learning module that updates the states of users and cascades in real time as diffusion behaviors occur."}, {"title": "4.1.3 Metrics", "content": "The distribution of popularity in the three datasets is shown in Figure 3 (right) and we see they are all a long-tail distribution where it is better to use log-level metrics for evaluation. We use three commonly used metrics, i.e., mean squared logarithmic error (MSLE), mean absolute percent-age error (MAPE), and the coefficient of determination (R2) for evaluation, which are defined as follows:\n$MSLE = \\frac{1}{M} \\sum_{k=1}^{M} (log_2(P_k + 1) \u2013 log_2(\\hat{P}_k + 1))^2$ (20)\n$MAPE = \\frac{1}{M} \\sum_{k=1}^{M} \\frac{|log_2(P_k + 2) \u2013 log_2(\\hat{P}_k+2)|}{log_2(\\hat{P}_k+2)}$ (21)\n$R^2 = 1- \\frac{\\sum_{k=1}^{M} (log_2(P_k + 1) \u2013 log_2(\\hat{P}_k + 1))^2}{\\sum_{k=1}^{M} (log_2(P_k + 1) - \\frac{\\sum_{k=1}^{M} log_2(\\hat{P}_k + 1)}{M})^2}$ (22)\nwhere the '+ 1' and the '+ 2' operations are for scale, M is the number of cascades, P is the truth popularity, and P"}, {"title": "4.2 Performance Comparison (RQ1&RQ2)", "content": "Tables 3, 4, and 5 show the results on the three datasets, respectively. We observe that ConCat significantly outper-forms all baselines on MSLE, MAPE, and R2. For example, compared to the best-performing baselines, our ConCat achieves a 6.1%-21.1% improvement of MSLE, a 2.2%-7.8% improvement of MAPE, and a 2.5%-33.2% improvement of R2 on three datasets with two different observation times. Moreover, we see that the performance of considering the first 1000 triplets is better than the case of 100, especially in Weibo and Twitter datasets, as longer cascades usually provide more information for popularity prediction while there is only a slight difference in the APS dataset because the proportion of cascades with more than 100 triplets is very small (only 0.01%). Moreover, with the increase in observation time, the MSLE exhibits a decreasing trend while the MAPE shows a larger magnitude. This can be attributed to the fact that as the observation time increases, the incremental popularity diminishes, leading to a smaller denominator in the MAPE calculation and consequently resulting in a larger MAPE value.\nTo explore the impact of triplets on the results of the prediction of popularity, we also set the number of triplets to 100, 200, 400, 800, 1000. The results are shown in Figure 4. The MSLE of Weibo and Twitter demonstrates a decreasing trend as the number of observed triplets increases. The MSLE of APS tends to be stable, as the proportion of cas-cades with a large number of triplets in Weibo and Twitter"}, {"title": "4.3 Ablation Study (RQ3&RQ4)", "content": "To answer RQ3 and RQ4, we have conducted a series of experiments where we have introduced different variants of our ConCat model. The purpose of these variants is to examine the impact of key design choices on the model's performance. Specifically, we investigate: 1) whether it is useful to model the continuous-time dynamics of cascades utilizing neural ODEs instead of RNNs, 2) whether it is useful to propagate the last event of different cascades to the unified observation time (i.e., \"On Your Mark\") for popularity prediction, 3) whether it is helpful to capture the global trend of the cascade graph by integrating neural TPPs rather than directly leveraging the last hidden state in temporal modeling for prediction, and 4) whether it is necessary to employ neural TPPs to capture the global trend, as opposed to some simpler methods.\nFirstly, we conduct the ablation study to investigate the contribution of each component as follows:\n\u2022 ConCat-w/o TPP: We remove the global trend modeling module and only input the hidden state hts to MLP. The loss function only considers MSLE without the log-likelihood.\n\u2022 ConCat-w/o align: We remove the hidden state hte and only rely on the last node's hidden representation to make predictions.\n\u2022 ConCat-w/o ODE: We remove the ODE module and it is correspondingly without the global trend modeling because the integral is computed by ODE.\n\u2022 ConCat-w/o attention: We remove the self-attention block, concatenate the representation of the cascade graph and global graph, and take it as the jump condition.\nSecondly, we design some variants to better answer our design choice in our model:\n\u2022 ConCat-RNNs: We remove the ODE and TPP modules, concatenate the representation of the cascade graph and global graph, and feed it into two bi-directional GRUS according to the time order.\n\u2022 ConCat-ELP: We replace the global trend modules with another module, which separates the observation time into five equal-length periods and counts the popularity of a cascade in each time interval. Then we feed the series of the popularity count into an MLP to get global trend representation and concatenate it with the hidden state hts for prediction.\n\u2022 ConCat-AROPE & ConCat-ProNE: We replace the global graph learning method with other scalable network em-bedding approaches, such as ProNE [31] and AROPE [32]."}, {"title": "4.4 Hyperparameter Sensitivity (RQ5)", "content": "We investigate the influence of hyperparameters including the hidden dimension of ht and the choice of ODESolvers. The hidden dimension is varied across values of 16, 32, 64, and 128, while the ODESolver options considered are dopri5, bosh3, adaptive_heun, euler, rk4, implicit_adams, and midpoint. The results of the impact of different hidden dimensions are shown in Figure 5 and we see that the hidden dimensions have some influence on the Weibo and Twitter datasets while the performance on APS indicates a tendency toward stability.\nAdditionally, among the ODESolver options shown in Table 9, euler yields the worst performance because the error of euler's method usually decreases as the step size decreases, while dopri5 demonstrates relatively superior predictive capabilities. Therefore, we empirically select the best hyperparameters for individual datasets in previous experiments, choose the dopri5 as our ODESolver across all of our experiments, and set the hidden dimension of ht as 32 for Twitter, 64 for APS and Weibo."}, {"title": "5 RELATED WORK", "content": "In the current literature, cascade popularity prediction tech-niques can be roughly classified into three categories.\nFeature-based approaches: In the early stage of this research direction, the primary emphasis was placed on understanding and identifying diverse hand-crafted fea-tures derived from raw data. These features include cascade graph structures [36], temporal features such as publication time [11], [12], observation time [6], [37], first participation time [38], and user interest [13], contents [10], etc. After the extraction of these features, they are then fed into conven-tional machine learning models for popularity prediction, such as simple regression models [14], regression trees [39], support vector machine [40], [41], and passive-aggressive algorithms [11]. Nonetheless, the application of feature-based methods can pose challenges due to their reliance on domain experts' knowledge, which often restricts the generalizability of learned features to novel contexts. For example, features extracted from tweets may not be directly applicable to scientific papers.\nGenerative-based approaches: These methods model the underlying diffusion process as event sequences in the continuous temporal domain. The dissemination of infor-mation items is commonly characterized using probabilistic generative models, including epidemic models [42], survival"}, {"title": "6 CONCLUSION", "content": "This paper investigates the problem of modeling continuous-time dynamics of cascades for information pop-ularity prediction. By revisiting the existing cascade pop-ularity prediction methods, we identify the importance of modeling continuous-time dynamics of cascades for popu-larity prediction, in particular, the dynamics after the last event of a cascade until the observation time. To address this issue, we propose ConCat modeling the Continuous-time dynamics of Cascades for popularity prediction. It utilizes neural ODE to model continuous patterns between two events and GRU to incorporate jumps into the neural ODE framework for modeling the discrete events, with neural TPPs further capturing the global trend of cascades, for the ultimate goal of popularity prediction. We conduct extensive experiments to evaluate ConCat on three real-world information cascade datasets. Results show that Con-Cat achieves superior performance compared to a sizeable collection of state-of-the-art baselines, yielding 2.3%-33.2% improvement over the best-performing baselines across the three datasets. In particular, our ablation study provides strong support for our design choices, where the propaga-tion to the unified observation time and neural TPPs for global trends both yield a significant improvement.\nIn the future, we plan to explore more effective methods to further leverage the characteristics of TPPs beyond the intensity integral for popularity prediction."}, {"title": "7 BIOGRAPHY SECTION", "content": "Xin Jing received the BSc and MSc degree both in software engineering from University of Electronic Science and Technology of China. He is currently a PhD student in the Department of Computer and Information Science at University of Macau. His research interests include spa-tiotemporal data and their applications in various scenarios, primarily on information diffusion in social network and trajectory generation.\nYichen Jing received the BSc degree in data science and big data technology from Minzu Uni-versity of China. He is currently a MSc student in the Department of Computer and Information Science at University of Macau. His research mainly focous on temporal data mining.\nYuhuan Lu received the BSc and MSc degrees both in transportation engineering from Sun Yat-Sen University. He is currently a PhD student in the Department of Computer and Information Science at University"}]}