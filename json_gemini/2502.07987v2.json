{"title": "Universal Adversarial Attack on Aligned Multimodal LLMs", "authors": ["Temurbek Rahmatullaev", "Polina Druzhinina", "Matvey Mikhalchuk", "Andrey Kuznetsov", "Anton Razzhigaev"], "abstract": "We propose a universal adversarial attack on multimodal Large Language Models (LLMs) that leverages a single optimized image to override alignment safeguards across diverse queries and even multiple models. By backpropagating through the vision encoder and language head, we craft a synthetic image that forces the model to respond with a targeted phrase (e.g., \"Sure, here it is\") or otherwise unsafe content even for harmful prompts. In experiments on the SafeBench benchmark, our method achieves significantly higher attack success rates than existing baselines, including text-only universal prompts (e.g., up to 93% on certain models). We further demonstrate cross-model transferability by training on several multimodal LLMs simultaneously and testing on unseen architectures. Additionally, a multi-answer variant of our approach produces more natural-sounding (yet still malicious) responses. These findings underscore critical vulnerabilities in current multimodal alignment and call for more robust adversarial defenses. We will release code and datasets under the Apache-2.0 license.", "sections": [{"title": "1. Introduction", "content": "Adversarial attacks remain one of the most pressing concerns in modern artificial intelligence research. In general, an adversarial attack involves crafting malicious inputs often subtle, carefully designed perturbations capable of causing models to produce unintended or harmful outputs. Such attacks can lead to privacy breaches, the generation of disallowed content, or even strategic exploitation of a system's decision-making processes. Despite advances in alignment techniques (e.g., supervised fine-tuning and Reinforcement Learning from Human Feedback), Large Language Models (LLMs) still exhibit significant vulnerability to these adversarial strategies.\nExtending these vulnerabilities to the multimodal setting raises additional risks. Multimodal LLMs, such as those combining vision and language capabilities, have recently achieved remarkable breakthroughs in visual-textual reasoning and aligned content generation. However, even with robust safety measures and policy filters, these systems often fail to withstand carefully crafted adversarial inputs. In particular, the mere presence of a specially optimized image can override safety filters, prompting the model to produce harmful or disallowed content.\nIn this paper, we present a universal adversarial attack that leverages a single synthetic image to compromise multimodal LLMs across diverse prompts. By optimizing pixel-level perturbations through the model's entire vision-"}, {"title": "Universal Adversarial Attack on Multimodal Aligned LLMs", "content": "language stack, our approach compels the system to respond with a targeted, unsafe phrase for virtually any input query. Our experiments on SafeBench, a benchmark designed to stress-test alignment with malicious prompts, show significantly higher attack success rates compared to existing baselines, revealing the magnitude of the threat that adversarial images pose.\nWe further demonstrate that a single adversarial image can transfer across multiple multimodal architectures, even when trained on only a subset of models. Additionally, our multi-answer variant elicits diverse but still malicious responses, underscoring the broader implications of universal, multimodal exploits. Taken together, these results highlight an urgent need for more robust adversarial defenses and deeper explorations into how visual embeddings manipulate language outputs in aligned systems."}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Adversarial Attacks on Vision Models", "content": "Early work on adversarial examples demonstrated that small pixel-level perturbations can mislead deep convolutional networks (Szegedy, 2013; Kurakin et al., 2018). Subsequent research explored universal perturbations that transfer across multiple inputs (Moosavi-Dezfooli et al., 2016), highlighting the inherent fragility of these models. Gradient-based methods remain central in these studies, including diverse improvements on iterative update rules (Guo et al., 2021) to enhance attack efficacy and transferability."}, {"title": "2.2. Adversarial Attacks on Text Models", "content": "Textual adversarial attacks typically rely on discrete perturbations such as synonym substitution or character-level changes (Neekhara et al., 2018). These approaches leverage gradient signals (Guo et al., 2021) or rule-based strategies (Jones et al., 2023) to disrupt language understanding, often requiring careful semantic and syntactic constraints. Despite growing sophistication, text-based attacks must address the discrete nature and lower dimensionality of language data compared to vision."}, {"title": "2.3. Multimodal and Universal Attacks", "content": "Extending adversarial attacks to multimodal systems reveals novel vulnerabilities, as both image and text components can be targeted (Gu et al., 2024). Some methods combine cross-modal manipulations or exploit attention mechanisms to cause misalignment (Zhang et al., 2022a; Carlini et al., 2024). Additionally, universal perturbations retaining effectiveness across multiple prompts and modalities (Zou et al., 2023) pose a significant threat to real-world deployment. Recent attempts have also shown how carefully optimized single images can trigger unsafe responses in aligned models (Carlini et al., 2024).\nWhile there have been numerous advances in adversarial attacks on unimodal systems, the multimodal models remains relatively underexplored. Universal and multimodal perturbations are particularly concerning for safety-critical applications, as they can bypass alignment safeguards (Gu et al., 2024). Ongoing research focuses on building robust countermeasures, but the rapid development of Large Language Models and vision-language alignment leaves many open questions regarding reliable and scalable defense strategies."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Simple White-Box Attack: One Model, One Prompt", "content": "In the simplest setup (used in the initial experiments), our method applied gradient-based optimization to the pixel values of an image z to produce the desired answer to the question given to the LLM. Specifically, we used a masked cross-entropy loss (LLM loss) $L_{LLM}(y|x, z)$, applied only to the answer tokens y, and backpropagated gradients through the language model, adapter, and visual encoder. To minimize visual distortion, we optimized an additional tensor $z_1$ of the same shape as the original image $z_0$, which was added to it before being passed through the visual encoder. We also constrained the image distortion values $z_1$ by applying a bounded function tanh scaled by a small constant $\u03f5_1$. This optimization process can be described by the following expressions:\n$z_1 = arg min_{z_1} L_{LLM} (y|x, z_0 + g(z_1))$,\nwhere x represents the tokens of the question, y denotes the tokens of the answer, and $z_0$ refers to the pixel values of the original image. The function $g(z_1) = \\epsilon_1 tanh(z_1)$ constrains the norm of the trainable tensor $z_1$ added to the image."}, {"title": "3.2. Improving Robustness Against Quantization Errors", "content": "During these experiments, we observed that the generated text output of the LLM was highly sensitive to minor changes in the optimized image. For example, quantization errors, arising from saving the image as int8 and reloading it, could disrupt the attack. To improve robustness, we added small random noise to the input image at each optimization step. The noise amplitude \u03c3 was carefully chosen to ensure successful convergence while providing resilience to quantization errors. To further adapt to quantization artifacts, \u03c3 was updated at each iteration so that it was equal to the standard deviation of the difference between original optimized and saved (quantized) tensors."}, {"title": "Universal Adversarial Attack on Multimodal Aligned LLMs", "content": "Additionally, we clipped the pixel values of the attacked image $(z_0 + z)$ after each iteration to ensure brightness values remained within the [0, 255] range after conversion to integers. So, the pixel values fed into the model can be expressed with the following formula:\n$z = z_0 + clip(z_0 + g(\\epsilon), -1, 1)$, where $\\epsilon \\sim N(0, \\sigma^2I)$.\nThis method successfully optimized an image to produce the desired text generation for a single model, specific prompt, and input image. However, this is not particularly surprising for a white-box attack. Therefore, our subsequent experiments aimed to generalize these results to multiple prompts and models."}, {"title": "3.3. Prompt Generalization", "content": "To achieve prompt generalization, we aimed to make the model respond affirmatively to any query, including harmful ones. We constructed a small dataset containing diverse questions (unrelated to the image) with the same affirmative answer: \"Sure, here it is.\" Some questions were safe, while others contained harmful prompts designed to attack the model's safety alignment. The optimization process followed the earlier setup, with the difference that a random prompt from this dataset was used in each iteration. After training, the model generalized to unseen queries, consistently starting its response with \"Sure, here it is,\" even for harmful prompts. We present the results with examples of this setup on the SafeBench benchmark in the paper."}, {"title": "3.4. Answer Generalization", "content": "To achieve more diverse and robust affirmative responses, we introduce the multi-answer attack (MA), where the target response is not fixed to \"sure, here it is\", but is randomly selected from a predefined set of phrases. This approach not only prevents overfitting to a specific phrase and ensures more natural responses but also makes the attack less obvious to both automated and manual detection mechanisms, as adversarial outputs appear more context-aware and natural while still violating alignment constraints. The set of affirmative responses was independently generated and was not conditioned on the input questions. During optimization, a randomly chosen positive target response from this set is used in each iteration, guiding the gradient descent process toward its generation."}, {"title": "3.5. Cross-Model Generalization", "content": "In the next setup, we aimed to generalize the attack across models, enabling a semi black-box attack on a new model without using its gradients during training. For this, we optimized the z1 tensor using gradients from three models and tested the attack on a fourth, unseen model in a leave-one-out setup. The only difference from the previous setup is that, during training, the z1 was optimized with combined loss L, calculated as the sum of the losses from models:\n$L = \\sum_{i=1}^3 L_{LLM}$"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset", "content": "To train and evaluate our universal adversarial attack, we constructed a custom dataset and utilized the SafeBench benchmark (Ying et al., 2024) for comparison with existing solutions.\nOur training dataset consists of 100 safe questions and 50 adversarial questions, generated according to the 13 adversarial categories defined in the Llama-Guard paper (Chi et al., 2024). The validation set includes 50 adversarial questions. In all experiments, except for the multi-answer setup, the target response was prefixed with \"Sure, here it is!\". For answer robustness evaluation in the multi-answer setup, target responses were additionally generated, and [query, target] pairs were randomly sampled.\nTo assess the effectiveness of our attack, we conducted evaluations on SafeBench (Ying et al., 2024), a benchmark designed to assess the safety of MLLMs. It includes a diverse multimodal harmful queries across 23 risk scenarios, covering categories like hate speech, self-harm, and other prohibited content."}, {"title": "4.2. Setup", "content": "In this section, we describe the different experimental setups used to evaluate the proposed adversarial attack. We begin by outlining the four main scenarios we consider: (1) singlemodel attacks with a single target phrase, (2) single-model"}, {"title": "Universal Adversarial Attack on Multimodal Aligned LLMs", "content": "attacks with multiple answers, (3) multi-model training for cross-model generalization, and (4) multi-model training with multi-answer targets.\nWe evaluate our attacks on Llava-1.5-7B (Liu et al., 2024), Llama-3.2-11B-Vision-Instruct (Dubey et al., 2024), Phi-3.5-Vision-Instruct (Abdin et al., 2024), and Qwen2-VL-2B-Instruct (Wang et al., 2024), each employing different image preprocessing techniques."}, {"title": "4.2.1. EXPERIMENTAL SCENARIOS", "content": "(1) Single-Model, Single-Answer Attack (Table 3, Figure 2, Ours). In our initial and simplest setup, we focus on attacking one specific multimodal LLM at a time, using a single target phrase (\"Sure, here it is\") for every prompt. In each training iteration, we randomly select one textual prompt from a small dataset of diverse queries some safe, some explicitly harmful and force the model to produce the same final answer tokens.\n(2) Single-Model, Multi-Answer Attack (Table 3, Figure 2, Ours-MA). Next, we extend beyond a single target phrase to a set of multiple malicious answers(MA), where the target response is randomly sampled from a predefined set of affirmative answers during training. At each optimization step, a different response is selected, and the same adversarial image z1 is optimized to drive the model toward producing any of these responses.\n(3) Multi-Model Training for Cross-Model Generalization (4). To explore the transferability of our adversarial image, we also optimize z1 jointly on three different models (Phi, Llama, Qwen) by summing their individual loss terms into a single objective. We then test on a fourth, unseen model (Llava) to assess how well the resulting image transfers to architectures for which we did not have gradient access.\n(4) Multi-Model, Multi-Answer 5. Finally, we combine the ideas of multi-answer attacks and multi-model training. In this setup, we have a pool of malicious answers and multiple models from which we gather gradient signals.\nThe last two scenarios most closely resembles a realistic black-box threat: an attacker could design a single, flexible adversarial image that leads to unsafe outputs across different LLM architectures and to varied malicious completions."}, {"title": "4.2.2. BASELINES", "content": "To assess the effectiveness of our approach, we compare it against the following baselines:\nReference values: The proportion of unsafe responses generated by the model, where the input consists only of the original question, without any adversarial images, text suffixes, or response prefixes.\n\"Sure, here it is\" attack: A textual jailbreak where the phrase \"Sure, here it is\" is prepended before the model's response.\nSafeBench baseline (Ying et al., 2024): Adversarial image-text queries specifically designed to stress-test multimodal alignment.\nGCG-transferable attack (Zou et al., 2023): A universal textual suffix that exploits large language model weaknesses without relying on visual inputs."}, {"title": "4.2.3. IMPLEMENTATION DETAILS", "content": "Gradient-Based Optimization. All experiments use gradient-based optimization (AdamW) on a learnable tensor $Z_1$ added to a base image $z_0$. We apply a bounding function\n$g(Z_1) = \\epsilon tanh(z_1)$,\nwith a small \u03f5 to constrain perturbations to a fixed maximum range. Unless otherwise stated, we use a learning rate of 1 \u00d7 10-2 with \u03f5 \u2208 [0.1, 0.5] and train for up to several hundred steps (depending on the model and complexity).\nRobustness to Quantization. One key practical challenge is that saving and reloading the adversarial image to disk (with int8 quantization) may degrade or even break the attack. To enhance robustness, we add small random noise $\\epsilon \\sim N(0, \\sigma^2I)$ at each training step. We also adapt \u03c3 to approximate the real-world quantization error. This helps the attack remain effective when the final image is stored or shared in standard formats.\nPrompts and Answer Targets. For single-model experiments, the training set consists of a few hundred textual prompts (both safe and adversarial). In the single-answer case, the entire set is paired with the same target response tokens. In the multi-answer variant, each training prompt is randomly matched with one of several possible malicious answers. At inference time, we present the attacked image plus a new user query, and measure how often the LLM produces harmful content (see Section Experiments)."}, {"title": "4.3. Metrics", "content": "We assess the effectiveness of adversarial attacks through the Attack Success Rate (ASR), a key metric that quantifies the proportion of prompts leading to misaligned model responses. An attack is considered successful if the model's response, evaluated in isolation by an external evaluator (Oracle) without access to the input prompt, is classified as unsafe. This approach ensures that the evaluation remains"}, {"title": "Universal Adversarial Attack on Multimodal Aligned LLMs", "content": "impartial, focusing solely on the content of the generated response rather than the intent of the query.\nFormally, given a dataset containing n query-response pairs, where $R_i$ represents the model's response to the i-th query, the ASR is defined as:\n$ASR = \\frac{1}{n} \\sum_{i=1}^{n} I(Oracle(R_i) = 1)$\nwhere I is an indicator function that returns q if the response Ri is classified as unsafe by the Oracle, and 0 otherwise. As an Oracle we use Llama-Guard-11B (Chi et al., 2024).\nTo establish a robust evaluation framework, we compare our original Oracle evaluation setup based on Llama-Guard-11B, against Jury Deliberation Protocol, of SafeBench (Ying et al., 2024), across a multiple set of experiments. Llama-Guard is selected due to its comparable performance while significantly reducing computational costs (Table 2)."}, {"title": "4.4. Results", "content": "In our baseline experiment, we optimize a unique adversarial image for each targeted model individually. As shown in Table 3 and Figure 2, our method outperforms existing baselines in terms of Attack Success Rate (ASR), including a purely textual jailbreak that simply prepends \u201cSure, here it is\u201d to the model's response. While this classic text-only attack is known to bypass alignment in some cases (reaching 45.3% ASR on Llava), our single-image adversarial approach generally achieves even higher rates. Specifically, a single visual prompt can force the model to generate undesired or harmful content across a wide range of textual queries, underscoring the power of visual adversarial cues.\nWhy Llava Has a Higher Baseline ASR We observe a notably higher reference ASR of 18.5% for Llava compared to the 2-3% range for the other models (Table 3). Our empirical finding suggests that Llava's safety alignment is comparatively weaker, allowing more harmful or policy-violating content even before the adversarial image is introduced. Consequently, adversarial perturbations exploit these weaker defenses, resulting in higher success rates. Additionally, Llava encodes input images in multiple patches rather than a single consolidated patch, which may increase opportunities for carefully placed perturbations to influence the model's final response."}, {"title": "Cross-Model Generalization", "content": "In the next set of experiments, we examine how well a single adversarial image can transfer across different multimodal LLMs. Specifically, we optimize the perturbation using gradients from three models and then evaluate on the remaining, unseen model. Tables 4 and 5 confirm a transferability trend for Llava model: for example, optimizing without explicit consideration of Llava still significantly increases ASR on Llava compared to its reference values. While the performance can vary depending on the specific combination of models, these findings collectively illustrate that a carefully tuned image can generalize to multiple architectures further emphasizing the vulnerability of current multimodal alignment strategies.\nComparison to Baselines As shown in Table 3, our universal adversarial image consistently achieves higher ASRs than these baselines, spotlighting the unique potency of visual adversarial signals over purely textual methods.\nOverall, these experimental results reveal a serious gap in multimodal LLM defenses: even a single optimized image can systematically bypass safety mechanisms across diverse queries and multiple architectures, sometimes yielding nearly 90% or higher success rates. We hope these findings encourage further research into more robust adversarial defenses and safety alignment techniques in the multimodal domain."}, {"title": "5. Discussion", "content": "Our universal adversarial attack reveals a significant vulnerability in multimodal alignment, demonstrating the fragile nature of these systems. While such models are designed to filter harmful or policy-violating content, the effectiveness of these safeguards is undermined by the power of a single,"}, {"title": "Universal Adversarial Attack on Multimodal Aligned LLMs", "content": "current vision-language alignment mechanisms, achieving notably higher attack success rates compared to existing baselines.\nMoreover, our multi-answer approach highlights the adaptability of such attacks, producing diverse, convincingly natural but harmful responses. These results stress the urgent need for more comprehensive adversarial training techniques, improved interpretability of visual embeddings, and stricter validation of multimodal inputs. We hope our findings serve as a catalyst for future research on robust and secure multimodal systems."}, {"title": "Impact Statement", "content": "This work reveals a universal adversarial attack that can bypass safety features in multimodal LLMs. While our goal is to advance understanding of vulnerabilities and encourage stronger defenses, our findings also highlight potential risks if such attacks are deployed maliciously. We recommend responsible disclosure and collaboration with developers to implement robust mitigations and protect users."}, {"title": "Universal Adversarial Attack on Multimodal Aligned LLMs", "content": "carefully crafted image. This vulnerability raises several critical considerations for the field, which require further exploration:\nRobust training: Expanding adversarial training to include image-based attacks.\nModel interpretability: Understanding how visual embeddings manipulate textual output.\nDeployment considerations: Restricting the acceptance of unverified visual inputs in high-stakes applications."}, {"title": "6. Conclusion", "content": "We have introduced a universal adversarial attack on multimodal LLMs, showing that a single crafted image can systematically bypass safety constraints across a wide range of prompts and even across different model architectures. Our experiments on SafeBench confirm the vulnerability of"}]}