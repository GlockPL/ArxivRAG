{"title": "DRIVING BY THE RULES: A BENCHMARK FOR INTEGRATING TRAFFIC SIGN REGULATIONS INTO VECTORIZED HD MAP", "authors": ["Xinyuan Chang", "Maixuan Xue", "Xinran Liu", "Zheng Pan", "Xing Wei"], "abstract": "Ensuring adherence to traffic sign regulations is essential for both human and autonomous vehicle navigation. While current benchmark datasets concentrate on lane perception or basic traffic sign recognition, they often overlook the intricate task of integrating these regulations into lane operations. Addressing this gap, we introduce MapDR, a novel dataset designed for the extraction of Driving Rules from traffic signs and their association with vectorized, locally perceived HD Maps. MapDR features over 10,000 annotated video clips that capture the intricate correlation between traffic sign regulations and lanes. We define two pivotal sub-tasks: 1) Rule Extraction from Traffic Sign, which accurately deciphers regulatory instructions, and 2) Rule-Lane Correspondence Reasoning, which aligns these rules with their respective lanes. Built upon this benchmark, we provide a multimodal solution that offers a strong baseline for advancing autonomous driving technologies. It fills a critical gap in the integration of traffic sign rules, contributing to the development of reliable autonomous navigation systems.", "sections": [{"title": "INTRODUCTION", "content": "The emergence of autonomous vehicles and intelligent transportation systems has highlighted the critical need for accurate and reliable navigational data. High-Definition (HD) maps \u00b9, with their detailed representation of the road environment, have become indispensable for these advanced systems. Traffic signs, as the visual language of the road, are essential for conveying driving rules such as speed limits, lane usage restrictions, and right-of-way rules. For autonomous vehicles, accurate recognition and interpretation of these signs are not just advantageous but essential for safe and compliant operation on public roads. However, current online HD map construction for autonomous driving mainly focuses on accurately depicting the types and positions of map elements in BEV space using point sequences, neglecting the driving rules conveyed by traffic signs and their relation to lanes. Beyond mere recognition, effective autonomous navigation demands a deeper integration of traffic signs into the vehicle's HD map, as depicted in Figure 1. The conventional researches of sign detection and classification Behrendt et al. (2017); Stallkamp et al. (2012); Fregin et al. (2018); Zhu et al. (2016); Yu et al. (2020), which often rely on single labels, are inadequate for capturing the detailed requirements of lane-level driving rules. A single traffic sign often represents multiple rules applicable to various lanes, each with distinct attributes such as lane direction and speed limitations. The challenge lies in binding these lane-level rules to the corresponding lanes within the HD map. Achieving this level of integration is essential for developing HD map that can robustly support autonomous driving.\nDespite the critical role that traffic sign integration plays in autonomous driving, there has been a noticeable lack of focused research in this area. The CTSU dataset Guo et al. (2021), for instance, takes an initial step by encoding traffic signs in {key: value} pairs, yet it does not effectively link the semantic content of signs to specific lanes. Other efforts, such as OpenLaneV2 Wang et al. (2023) and VTKGG Guo et al. (2023) have attempted to establish connections between traffic signs and lanes. However, they have not fully addressed the structural interpretation of the multifaceted attributes of lane-level rules.\nTo address this gap, we introduce MapDR, the first dataset specifically designed for driving rules extraction from traffic signs and association with vectorized HD maps. MapDR provides an extensive collection of over 10,000 video clips that explore the correlation between lanes and driving rules extracted from traffic signs. For more details on the proposed dataset, please refer to Section 4.\nMapDR introduces two innovative sub-tasks aimed at bolstering research in this domain: 1) Rule Extraction from Traffic Sign: This sub-task is dedicated to developing algorithms that can extract specific lane-level rules from traffic signs, including their attributes and the lanes to which they apply. It is an essential step for understanding the intricate details of traffic signs and their navigational implications. 2) Rule-Lane Correspondence Reasoning: This sub-task focuses on establishing a precise relationship between the extracted rules and the corresponding lanes in the HD maps. This process is vital for autonomous systems to accurately contextualize and apply lane-level rules to their driving path. For detailed descriptions of the proposed tasks and metrics, please refer to Section 3.\nBased on the proposed tasks and dataset, we leverage multimodal models to design a solution that integrating traffic sign regulations into vectorized HD maps. This provides a strong baseline for future research work. We hope to inspire more researchers to focus on this task and drive the development of related industries.\nTo sum up, our contributions are as follows:\n\u2022 For the first time, we introduce the task of extracting lane-level rules from traffic signs and integrating them into vectorized HD maps. Additionally, we present the MapDR dataset and specific metrics for benchmarking this task.\n\u2022 MapDR comprises an extensive collection of images from three representative Chinese cities, captured over a quarter year at various times of the day. This dataset includes over 10,000 video clips, at least 400, 000 front-view images, and more than 18,000 lane-level rules. All annotations are carefully validated, with all data newly collected."}, {"title": "TASK DEFINITION: INTEGRATING TRAFFIC SIGN REGULATIONS INTO HD MAPS", "content": "The ability to discern rules from traffic signs and to associate them with specific lanes is pivotal for autonomous navigation. As depicted in Figure 2, traffic signs are primary indicators of lane-level rules. Our proposed task involves two core sub-tasks: 1) Extracting lane-level rules from traffic signs, and 2) Establishing correspondence between these rules and centerlines. Generally, vehicles follow the center of lanes, i.e., centerlines, to drive on the road Wang et al. (2023). Therefore, we use centerlines to represent lanes. This approach mirrors human drivers' instinct to observe traffic signs and then relate the indicated rules to the lanes they govern."}, {"title": "RULE EXTRACTION FROM TRAFFIC SIGN", "content": "As shown in Step 2 of Figure 2, this task involves extracting multiple rules \\(R = \\{r_i\\}_{i=1}^m\\) from a series of image sequences \\(X = \\{x_i\\}_{i=1}^n\\), where \\(m\\) is the number of rules and \\(n\\) is the number of frames. Each rule \\(r_i\\) is a set of pre-defined properties in \\(\\{key : value\\}\\) pairs. The rule extraction model, denoted as \\(M\\), can be expressed as \\(R = M(X)\\). To facilitate this challenging task, existing algorithm results for sign detection and OCR, represented as \\(B\\) and \\(T\\) respectively, can be utilized, making the rule extraction process \\(R = M(X, [B], [T])\\), \\[\\cdot\\] indicates optional input."}, {"title": "RULE-LANE CORRESPONDENCE REASONING", "content": "As shown in Step 3 of Figure 2, the reasoning process establishes the correspondence between centerlines \\(L = \\{l_i\\}_{i=1}^k\\) and all rules \\(R\\), where \\(k\\) is the number of centerlines. We denote the correspondence reasoning model as \\(\\mathcal{T}\\), and this process can be described as \\(E = \\mathcal{T}(R, L)\\), where \\(E \\in \\{0, 1\\}^{m \\times k}\\) and the element \\(E_{ij}\\) in the \\(i\\)-th row and \\(j\\)-th column of matrix \\(E\\) represents the corresponding status between \\(r_i\\) and \\(l_j\\). The final reasoning result forms a bipartite graph \\(\\mathcal{G} = (R \\cup L, E)\\), which means corresponding relationships only exist between rules and centerlines ."}, {"title": "EVALUATION METRICS", "content": "We evaluated the two sub-tasks separately and then assessed the overall task performance. Methods are supposed to be ranked according to the overall AP."}, {"title": "Rule Extraction (R.E.)", "content": "Given the ground truth \\(R\\) and predicted rules \\(\\hat{R}\\), we propose to calculate the Precision (\\(P_{R.E.}\\)) and Recall (\\(R_{R.E.}\\)) to evaluate the capability of rules extraction as defined in Equation equation 1, where \\(r_i = \\hat{r_i}\\) represents all the properties are predicted correctly.\n\\begin{equation}\nP_{R.E.} = \\frac{|\\hat{R} \\cap R|}{|\\hat{R}|} \\quad \\quad R_{R.E.} = \\frac{|\\hat{R} \\cap R|}{|R|}\n\\end{equation}"}, {"title": "Correspondence Reasoning (C.R.)", "content": "Given the ground truth of correspondence bipartite graph \\(\\mathcal{G} = (R \\cup L, E)\\) and predicted graph \\(\\hat{\\mathcal{G}} = (R \\cup L, \\hat{E})\\), we propose to calculate Precision (\\(P_{C.R.}\\)) and Recall (\\(R_{C.R.}\\)) of edge set \\(E\\) to evaluate the capability of correspondence reasoning individually."}, {"title": "ARCHITECTURE", "content": "Vision-Language Encoder. Inspired by vision-language frameworks Li et al. (2021; 2022a); Radford et al. (2021); Kim et al. (2021); Bao et al. (2022), we designed a vision-language fusion model named VLE, following Li et al. (2021). As shown in Figure 6, VLE uses ViT-b16 as the"}, {"title": "IMPLEMENTATION", "content": "We utilize VLE and MEE as backbones to integrate multiple modalities and address these two sub-tasks. The specific procedures are detailed as follows:"}, {"title": "Rule Extraction from Traffic sign", "content": "To clarify the objectives of model, we first cluster symbols and texts into groups. As shown in the upper part of Figure 6, the VLE is used to encode OCR results and images. By calculating the cosine similarity between \\([STC]\\) tokens, different symbols and texts are clustered into groups. This process is supervised by contrastive loss during training. Next, using grouped OCR results as text input and maintaining the VLE structure, we extract lane-level rules. We employ a multi-classification head (understanding head) for the \\([CLS]\\) token to predict the corresponding value for each attribute of the rules. This process allows us to express all rules inside a traffic sign as \\(\\{key : value\\}\\) pairs."}, {"title": "Evaluation Metric", "content": "To evaluate the entire task, capability of both sub-tasks should be considered jointly. There- fore the predicted results are supposed to be the combination of two sub-tasks. Given the predicted rules, correspondence should be reasoned between \\(\\mathcal{R}\\) and \\(\\mathcal{L}\\) which means the prediction of entire task is \\(\\hat{\\mathcal{G}} = (\\hat{R} \\cup L, \\hat{E})\\) and the ground truth is consistent \\(\\mathcal{G} = (R \\cup L, E)\\). We evaluate Precision \\(P_{all}\\) and Recall \\(R_{all}\\) using the sub-graph \\(\\mathcal{G}^s\\), where \\(\\mathcal{G}^s = \\{(r_{ij}, l_{ij})\\}_{i=1,j=1}^{m,k}, g_{ij} = (\\{r_i, l_j\\}, e_{ij})\\). In set of sub-graph \\(\\mathcal{G}^s\\), \\(m\\) is the number of rules, and \\(k\\) is the number of centerlines. Furthermore, we propose the average precision (AP) for the final benchmark ranking. Metrics are defined in Equation equation 3, AP score is the area under the precision-recall curve, where \\(p\\) and \\(r\\) denote \\(P_{all}\\) and \\(R_{all}\\) respectively. We provide an example of calculating the Overall metrics in appendix I.\n\\begin{equation}\nP_{all} = \\frac{|\\mathcal{G}^s \\cap \\hat{\\mathcal{G}^s}|}{|\\hat{\\mathcal{G}^s}|} \\quad \\quad R_{all} = \\frac{|\\mathcal{G}^s \\cap \\hat{\\mathcal{G}^s}|}{|\\mathcal{G}^s|} \\quad \\quad AP = \\int_0^1 p(r)dr\n\\end{equation}"}, {"title": "A BASELINE METHOD FOR MAPDR", "content": "To tackle the multimodal information interaction involving images, texts, and vectors, we develop a Vision-Language Encoder (VLE) and a Map Element Encoder (MEE). The following sections detail their structures and applications, as well as the experimental results on MapDR."}, {"title": "CONCLUSION", "content": "We introduce MapDR, a dataset with more than 10, 000 video clips, over 400, 000 images, and at least 18,000 driving rules. This work defines the task of integrating traffic sign regulations into vectorized HD map, proposes a viable solution and establishes an effective baseline. With the emergence of MLLMS, we will explore their potential to tackle this complex comprehending task in future work.\nLimitation. In our dataset, we do not consider the impact of dynamic elements, such as traffic lights, on driving rules, as these scenarios have already been discussed in previous works like OpenLaneV2 Wang et al. (2023). Instead, we focus on the impact of lane-level rules on driving, a topic often overlooked in previous datasets. In the future, we plan to incorporate these dynamic elements to create a more comprehensive dataset."}]}