{"title": "CycleGuardian: A Framework for Automatic Respiratory Sound classification Based on Improved Deep clustering and Contrastive Learning", "authors": ["Yun Chu", "Qiuhao Wang", "Enze Zhou", "Ling Fu", "Qian Liu", "Gang Zheng"], "abstract": "Auscultation plays a pivotal role in early respiratory and pulmonary disease diagnosis. Despite the emergence of deep learning-based methods for automatic respiratory sound classification post-Covid-19, limited datasets impede performance enhancement. Distinguishing between normal and abnormal respiratory sounds poses challenges due to the coexistence of normal respiratory components and noise components in both types. Moreover, different abnormal respiratory sounds exhibit similar anomalous features, hindering their differentiation. Besides, existing state-of-the-art models suffer from excessive parameter size, impeding deployment on resource-constrained mobile platforms. To address these issues, we design a lightweight network CycleGuardian and propose a framework based on an improved deep clustering and contrastive learning. We first generate a hybrid spectrogram for feature diversity and grouping spectrograms to facilitating intermittent abnormal sound capture. Then, CycleGuardian integrates a deep clustering module with a similarity-constrained clustering component to improve the ability to capture abnormal features and a contrastive learning module with group mixing for enhanced abnormal feature discernment. Multi-objective optimization enhances overall performance during training. In experiments we use the ICBHI2017 dataset, following the official split method and without any pre-trained weights, our method achieves Sp: 82.06%, Se: 44.47%, and Score: 63.26% with a network model size of 38M, comparing to the current model, our method leads by nearly 7%, achieving the current best performances. Additionally, we deploy the network on Android devices, showcasing a comprehensive intelligent respiratory sound auscultation system.", "sections": [{"title": "1 Introduction", "content": "According to the World Health Organization (WHO), the five major respiratory diseases [1]-lung cancer, tuberculosis, asthma, acute lower respiratory tract infections (LRTI), and chronic obstructive pulmonary disease (COPD) collectively result in over 3 million deaths worldwide annually [2] [3]. Pulmonary diseases have become the third leading cause of death globally [4]. These respiratory ailments significantly impact healthcare systems and adversely affect people's lives. Prevention, early diagnosis, and treatment are recognized as key factors in reducing the adverse effects of these deadly diseases.\nPulmonary auscultation [5] is a crucial method for examining respiratory system diseases. By listening to lung sounds, experts can identify adventitious sounds during the respiratory cycle, including crackles and wheezes. These non-periodic and non-stationary sounds are primarily categorized into two groups: normal (vesicular) and abnormal (adventitious) [6]. Normal respiratory sounds are observed in the absence of respiratory diseases, with frequencies ranging from 100 to 2000 Hz. Abnormal respiratory sounds can be further classified as continuous or discontinuous, with wheezes and crackles being the most common types [7], respectively. Wheezing is a significant symptom of asthma and COPD, with its main frequency concentrated around 400 Hz and a duration exceeding 80 ms. In contrast, crackles are discontinuous anomalies often associated with obstructive pulmonary diseases, including COPD, chronic bronchitis, pneumonia, and pulmonary fibrosis. Crackles comprise fine and coarse crackles, with fine crackles having a frequency of approximately 650 Hz and a duration of around 5 ms, while coarse crackles have a frequency of about 350 Hz and a duration of about 15 ms. These abnormal respiratory sounds can be characterized based on frequency, pitch, energy, and differentiation from normal lung sounds.\nHowever, even for experts, identifying differences between various abnormal respiratory sounds poses challenges, as different experts may introduce subjectivity in diagnostic interpretations [8]. Furthermore, in many underdeveloped regions lacking experienced physicians, delays in disease diagnosis and treatment occur. In such circumstances, people have begun using stethoscopes to collect respiratory sounds, followed by computer-aided automatic classification of these sounds. Existing research primarily focuses on addressing the following aspects: dealing with class imbalance in respiratory sound data, feature extraction, and network design.\nClass imbalance, prevalent in medical datasets, presents a scarcity of abnormal samples compared to normal ones, a pattern also evident in respiratory sound datasets. [9] addresses this by employing data augmentation, concatenating audio samples to generate new abnormal instances. Similarly, [10] utilizes mixup techniques on spectrogram features to augment abnormal data at the feature level. [11] introduces focal loss to mitigate class imbalance by assigning higher weights to hard samples. [12] utilized generative models to synthesize abnormal data and applied adversarial fine-tuning techniques to align the features between the generated and real data, addressing the issue of data imbalance. [13] proposed an audio-level enhancement technique to improve the feature representation. [14] introduced a multi-label learning method to handle the scarcity of abnormal samples.\nIn feature extraction from respiratory sounds, some methods decompose the signal into subsequent signals. [15] decomposes audio signals into three components, extracting spectrograms from each with different window sizes. [16] employs empirical mode decomposition to derive intrinsic mode function coefficients, then applies wavelet transform for feature extraction. [17] combines respiratory sounds with lung volume using MVMD to predict diseases. On the other hand, feature extraction directly from the original respiratory sound signal is a more common approach in research. [18], [19] improves the S-transform to obtain spectrograms, while [20] and [21] utilize STFT and WT simultaneously for spectrogram extraction. [22] and [23] directly extract MFCC coefficients and MEL spectrograms. [24], [25] employs gammatone filter banks, and [26] analyzes second-order spectral function to obtain bispectrogram features. Direct spectrogram extraction simplifies feature extraction, enabling end-to-end classification with neural networks. However, both normal and abnormal samples contain common information, predominantly normal components and noise. Extracting spectrograms in image form hinders the network's ability to discern abnormal information effectively. Early classification relied on empirical thresholds, evolving to deep neural networks [27], [28], [29], often utilizing pre-trained weights for transfer learning. Challenges arise in distinguishing complex symptom abnormal features from single symptom ones due to their strong similarities.\nIn summary, for the task of abnormal respiratory sound classification, the key issues that need to be ad-"}, {"title": "CycleGuardian", "content": "dressed are as follows: 1. There exists class imbalance among different categories in the dataset. 2. Distinguishing between normal and abnormal respiratory sounds presents a challenge due to the concurrent presence of normal respiratory components and noise components in both categories. 3. Different types of abnormal respiratory sounds often share similar anomalous features, particularly between complex symptomatic patterns and single-symptom presentations, further complicating their discrimination. What's more, the utilization of current state-of-the-art models [30] is hindered by excessive parameter and model size, impeding their deployment [31], [32] on resource-constrained mobile platforms.\nTo address these issues, we propose a method based on deep clustering and contrastive learning and design a lightweight network specifically for handling grouped respiratory spectrogram, named CycleGuardian. Fig.1 illustrates the overall design concept of this paper. The framework mainly consists of three components: 1) Grouped spectrogram feature generation. 2) Deep clustering and contrastive learning. 3) Multi-loss optimization strategy. The workflow of the entire framework is described below.\nWe first group and encode the spectrogram features to capture intermittent abnormal respiratory sounds effectively. Unlike patching methods [33], our method directly groups spectrograms in the time dimension at the frame level, as illustrated in Stage 1 of Fig.1 (Section 3.1). Following the encoding of grouped features, we propose an improved deep clustering approach to cluster these features and introduce a constraint loss based on cosine similarity to reduce similarity between cluster features. On the other hand, to enhance discriminability between complex and single abnormal features, we introduce a contrastive learning branch, this branch randomly replaces a portion of grouped features with features from other samples in the batch, facilitating contrastive learning between original and mixed samples. We employ a contrastive loss based on the similarity between global vectors (gloori and glomix) (Section 3.2). Furthermore, to optimize network performance, we utilize joint optimization with multiple objective losses. Different optimizers and weights are assigned to these losses for joint optimization (Section 3.3).\nTo the best of our knowledge, the work to combine deep clustering with contrastive learning and apply it to respiratory sound classification is currently rarely explored. The main contributions of this paper are summarized as follows:\n1) Comparing to conventional methods of directly encoding spectrograms or applying patch-based techniques, we explore the approach of grouping spec-"}, {"title": "3", "content": "trogram features, which is advantageous for capturing abnormal information from normal respiratory sounds, and augment the network's ability to differentiate between normal and abnormal respiratory sounds.\n2) We design a lightweight network architecture model, CycleGuardian, and propose an improved method of deep clustering and contrastive learning. By combining them, the network's capacity to distinguish between different abnormal respiratory sounds is improved. During training, we introduce a multi-objective loss joint optimization strategy to simultaneously optimize the clustering module, contrastive learning, and classification module, thus enhancing the overall performance of the network.\n3) Extensive experiments on the ICBHI2017 dataset demonstrate the effectiveness and superiority of our method. CycleGuardian achieves state-of-the-art performance without pretrained weights, utilizing a compact architecture of only 38MB model size. Additionally, we conduct preliminary deployment on smartphones, paving the way for an intelligent respiratory sound diagnosis system."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Contrastive learning", "content": "Contrastive learning is a self-supervised learning technique that learns effective feature representations by contrasting positive and negative pairs without the need for explicit data labeling, which is particularly important when labeled data is scarce or costly to obtain. It enables models to extract feature representations from the structure of the data itself, which can be applied to various downstream tasks such as classification, detection, and segmentation.\nRecent work has applied contrastive learning to abnormal classification tasks in respiratory sounds. Combining contrastive learning with multiple instance learning, studies [33] [34] divide each sample into multiple patches, categorizing patches into different subclasses based on the types of respiratory sounds they contain. Since patch-level labels are often unavailable, they estimate patch-level labels using a multiple instance learning (MIL) approach, minimizing the distance between features of patches from the same subclass and maximizing the distance between features of patches from different subclasses, forming a contrastive loss for learning. Another paper [30] proposes the use of pre-trained weights on large-scale visual and audio datasets and introduces a Patch-Mix contrastive learning method to improve respiratory sound classification performance."}, {"title": "2.2 Deep Clustering", "content": "Clustering aims to group given objects such that the similarity among objects within the same cluster is higher than those in other clusters. It is an unsupervised learning method widely applied in various practical applications like image classification and data visualization. Traditional clustering methods such as K-means (KM), Gaussian Mixture Model (GMM), and Spectral Clustering (SC) are known for their speed and broad applicability. However, these methods often rely on predefined similarity measures, which become ineffective when dealing with high-dimensional data. To address this issue, researchers have employed dimensionality reduction techniques to project the original data into a lower-dimensional space before clustering, thereby facilitating effective clustering. Nevertheless, the limited expressive power of low-dimensional features restricts the representation ability of the data.\nCompared to traditional methods, deep neural networks (DNNs) excel in extracting high-level features through nonlinear embeddings, benefiting clustering tasks. Approaches like Deep Embedded Clustering (DEC) [37] and Deep K-means (DKM) [38] typically employ a two-stage learning process, separating feature extraction and clustering. However, this separation overlooks potential relationships, limiting performance. To address this, researchers explore joint unsupervised learning [39] and propose models like deep adaptive clustering (DAC) [40] and Adaptive Self-Paced Clustering (ASPC) [41]. These models prioritize high-confidence samples and incorporate constraints into deep clustering processes to enhance performance. Constraints include pairwise, instance hardness, triplet, and cardinality constraints [42]. Additionally, instance-level must-link and cannot-link constraints [43] leverage pairwise distances between instances to enhance constrained clustering capability.\nInspired by the aforementioned works, this paper integrates deep clustering with contrastive learning and bring them into the task of respiratory sound classification. Unlike previous studies, we introduce a cosine similarity-based constraint loss in deep clustering to further reduce inter-cluster correlations and enhance clustering performance. In contrastive learning, we modify the encoding of spectrograms by grouping them directly at the frame level instead of patch encoding. The features of different samples within the same batch are mixed using group mix to generate new mixed samples for contrastive learning."}, {"title": "3 Proposed method", "content": "In this section, we introduce the generation of multi-channel spectrogram features and their grouping encoding to obtain group features. Subsequently, we present the two main components of CycleGuardian: the deep clustering module for clustering group features into different clusters, and the contrastive learning module, where group features from different samples are mixed to generate new mixed sample features for contrastive learning. Finally, we perform joint optimization of deep clustering and contrastive learning, detailing the proposed method."}, {"title": "3.1 Multi Channel Spectrogram Grouping", "content": "To enhance the diversity of spectrogram features, we construct multi-channel spectrogram features consisting of Mel spectrograms, CQT, and gamma spectrograms."}, {"title": "3.1.1 Multi Channel Spectrogram Generation", "content": "In this method, Mel spectrograms and gamma spectrograms are generated by designing respective filters based on the cochlear effect, providing higher resolution for low-frequency components and lower resolution for high-frequency components. They are both formed based on Fast Fourier Transform (FFT). Given a fixed-length audio segment x[n], the FFT is used to transform the time-domain signal into the frequency domain. Mel spectrograms extract frequency bands through a set of Mel filters and scale the power spectrum logarithmically to form Mel spectrograms, denoted as $S_0$. The conversion between Mel frequency and Hertz frequency is given by $f_{Mel} = 2595 log_{10}(1 + \\frac{f}{700})$. Gamma spectrograms, denoted as $S_1$, are formed by applying different enhancements to different frequency bands of the frequency-domain signal using gamma-tone filter banks and logarithmically scaling the filter outputs.\nUnlike Mel and gamma spectrograms, Constant Q Transform (CQT) is another method of transforming signals into time-frequency domains, providing a constant Q factor as frequency varies. Here, $Q = \\frac{f_k}{\\Delta f_k}$, where $f_k$ is the center frequency of the k-th bin and"}, {"title": "N-1", "content": "$\\Delta f_k$ is the bandwidth. Each bin's frequency signal is obtained through $X[k]_{cqt} = \\sum_{n=0} x[n] \\cdot e^{-j2\\pi k\\frac{n}{N}}$, followed by a logarithmic transformation to obtain the CQT spectrogram, denoted as $S_2$.\nAfter obtaining the three different types of spectrograms, we generate multi-channel spectrogram features using the following equation (1).\n$S = S_0 \\oplus S_1 \\oplus S_2$,\nHere, $S \\in \\mathbb{R}^{c \\times m \\times n}$ represents the three channels of spectrograms generated, all of which are $m \\times n$ matrices. Here, m represents the frequency dimension, n represents the number of frames, and c represents the number of channels. By using the $\\oplus$ operation, we concatenate them along the channel dimension to generate a three-channel spectrogram feature."}, {"title": "3.1.2 Network Basic Unit", "content": "As shown in Fig.2, four basic units are employed in our network model. The CBR unit consists of Convolution, BatchNorm, and ReLU activation functions. The CGL unit comprises convolution, GELU, and LayerNorm. LGL is composed of Linear, GELU, and LayerNorm. The Group Feature Embedding (GFE Unit) module cascades with three CBR unit and the linear layers. Using these basic units, we construct the improved Deep Embedding Cluster module (IDEC) and"}, {"title": "the Cluster Projection Fusion Module (CPF Module), forming the entire architecture of CycleGuardian.", "content": ""}, {"title": "3.1.3 Group Feature Embedding", "content": "The right side of Fig.2 illustrates the concept of group encoding. Initially, the multi-channel hybrided spectrogram features $S \\in \\mathbb{R}^{c \\times m \\times n}$ are grouped, with each group containing information from 20 frames of spectrograms. These grouped spectrograms are then inputted into the Group Feature Embedding (GFE Unit) module to obtain encoded features $g_i$ for each group. Group encoding of spectrograms enhances the capturing ability for intermittent abnormal respiratory sound such as crackles.\n$g_i = G_\\Theta(group_i), i \\in {0, 1, ...(N_g \u2013 1)},$\n$G_\\Theta() = GFEunit(),$\nIn (2), $group_i$ represents the grouped spectrograms, $G_\\Theta()$ denotes the GFE unit, $\\Theta$ represents the learnable parameters in the module, and $g_i$ represents the output of the group encoding module used to represent group encoding features. Let $T_{frames}$ denote the total number of frames in each sample. By sliding $S_{frames}$ frames to the right each time, the number of groups obtained for each sample is denoted as $N_g = \\frac{T_{frames}}{S_{frames}}$. Additionally, the sliding frames $S_{frames}$ must be less than the number of frames included in each group, $G_{frames}$, to en-"}, {"title": "sure approximately 25% overlap between each grouped spectrogram.", "content": ""}, {"title": "3.2 Improved Contrastive Learning and Deep Clustering", "content": "In this section, we first introduce the contrastive learning module based on group mix, which blends group features from each sample to generate new mixed sample features for contrastive learning against the original samples. Next, we elucidate the deep clustering module based on similarity constraints, used to cluster multiple group features from each sample into five clusters. Subsequently, we provide an overview of the overall architecture and operation flow of CycleGuardian."}, {"title": "3.2.1 Contrastive learning based on group mix", "content": "Considering that directly mixing the original audio may result in mislabeling of audio samples containing short segments of crackles or wheezes, the labels of the newly generated audio samples should ideally be marked as crackle or wheeze. However, since we cannot directly determine whether the mixed-in audio segment is normal or abnormal during the mixing process, making it difficult to assign labels to the newly generated samples, therefore, our study elects to implement group mixing at the feature level.\nWe proposed a contrastive learning based on group mix, as shown in Fig.3, each sample's spectrogram is divided into multiple groups, with each group containing 20 frames of spectrogram information. These grouped spectrograms are then inputted into the Group Feature Embedding (GFE) Unit to obtain encoding features $g_i$ for each group. Within a batch, each sample generates multiple group features $g_i$. By using group mix, new mixed sample features are generated. The proportion of original sample's group features retained in the mixed sample feature is denoted as $\\lambda$, with the replacement ratio being 1 \u2212 $\\lambda$. Based on this approach, contrastive learning is performed between the original samples and the mixed samples.\nAs shown in equation (3), $z_i$ represents the global feature of the current original sample, while $\\tilde{z_i}$ denotes the global feature of the current sample after group mix. Since in one batch contains multiple samples, $z_m$ refers to the global feature generated by other samples and is also another part groups of the source for generating $\\tilde{z_i}$. The similarity between $z_i$ and $z_i$, as well as between $\\tilde{z_i}$ and $z_m$, is controlled by the parameter $\\lambda$ and 1 \u2212 $\\lambda$. The main idea behind this contrastive learning is that if the mixed sample feature retains the proportion of original sample group features as $\\lambda$, then theoretically, the similarity between the mixed global feature and the original global features should also be approximately $\\lambda$.\n$L_{con} = -\\frac{1}{I}\\sum_{i=1}^{I} [\\lambda \\cdot (h(z_i) \\cdot h(\\tilde{z_i})) + (1-\\lambda) \\cdot (h(\\tilde{z_i}) \\cdot z_m)]$,\nHere, h() is a mapping module composed of two MLP layers with ReLU and BN layers. I represents the number of samples contained in one batch, T controls the sharpness of cosine similarity, and $\\lambda$ is sampled from the \u1e9e distribution. Additionally, all global features, such as $z_i$ or $h(\\tilde{z_i})$, are normalized before the dot product."}, {"title": "3.2.2 Improved Deep clustering based on similarity constraints", "content": "In traditional clustering algorithms like K-means, each data point or feature is hard-assigned to a single cluster. However, in many real-world scenarios, data points or features may belong to multiple clusters or reside in fuzzy regions between clusters, such as complex symptoms containing both abnormal features like crackles and wheezes. To address this limitation, we introduce the idea of Deep Embedding Clustering (DEC) in our approach, which employs a soft assignment strategy. This soft assignment is implemented using a Student's t-distribution, where each group encoding feature is associated with a probability distribution over clusters. In equation (4), $e_i$ represents the group encoding feature generated after dimensionality reduction by an autoencoder. $\\mu_j$ is the centroid of cluster j. $q_{ij}$ models the relationship between $e_i$ and $\\mu_j$. Considering the high dimensionality and nonlinearity of the embedding space, directly optimizing $q_{ij}$ may be challenging. Hence, an auxiliary target distribution $p_{ij}$ is introduced, as shown in equation (5). During training, $p_{ij}$ serves as a reference for $q_{ij}$, providing a more stable and effective optimization process for the entire clustering.\n$q_{ij} = \\frac{(1 + ||e_i - \\mu_j||^2/a)^{-1}}{\\sum_{j'=1}^k(1 + ||e_i \u2013 \\mu_{j'}||^2/a)^{-1}}$,\n$p_{ij} = \\frac{q_{ij}^2/\\sum_i q_{ij}}{\\sum_{j'}(q_{ij'}^2/\\sum_i q_{ij'})}$,\nHere, i \u2208 I represents the number of group features contained in each sample and it's value range from [0, $N_g$ - 1], k is the total number of clusters. a denotes the degrees of freedom of the Student's t-distribution,"}, {"title": "controlling the concentration of the distribution. In DEC, a is typically set to a predefined value (e.g., 1), but it can also be learned. qij is termed as soft assignment, interpreting the probability of the clustering module assigning the i-th group encoding feature ei to the j-th cluster.", "content": ""}, {"title": "In contrast to previous Deep Embedding Clustering(DEC) approaches, we propose an Improved Deep Embedding Clustering (IDEC) to enhance the differentiation between cluster features. The principle is to introduce a constraint based on cosine similarity on top of DEC during the clustering process. As illustrated in Fig.4, the IDEC include two part which are DEC module and Cluster Projection Fusion (CPF) module. In the DEC module, the group encoding features $g_i$ are dimensionally reduced to $e_i$, the number of clusters is set to five which are representing normal, noise, and three types of abnormal features, through clustering, each $e_i$ is assigned to the cluster with the highest probability. After determining each group's cluster assignment, the CPF module maps all group features belonging to the same cluster into a cluster feature $c_i$. It's worth to note that the cluster features $c_i$ are generated using the group encoding features $g_i$ before dimensionality reduction. After obtaining the cluster features for each cluster, these five cluster features are fused into a global feature z through weighted fusion.\nAs defined in equation (6), the similarity between any two cluster features $c_i$ is quantified using soft cosine similarity, reducing the correlation between these 5 cluster representation features, forming a constrained deep clustering process.", "content": ""}, {"title": "3.2.3 Network Architecture", "content": "The network architecture of CycleGuardian, as depicted in Fig.5, operates as follows. Firstly, in Stage 1, the spectrograms of each sample are divided into multiple groups. Secondly, in Stage 2, each group's spectrogram is input into the GFE unit to obtain each group's encoding feature $g_i$. Next, all the $g_i$ from the same batch are fed into two branches. In the lower branch, $g_i$ undergoes the group mix module, which replaces the $g_i$ from the original sample with those from other samples at a certain ratio, generating new mixed sample"}, {"title": "features input into the DEC module. The DEC module first reduces each $g_i$ to $e_i$, then clusters all $e_i$ into five clusters, ensuring similar $e_i$ are assigned to the same cluster. Subsequently, in the CPF module, all group encoding features $g_i$ belonging to the same cluster are mapped into one cluster feature $c_i$. After generating five cluster features $c_i$, they are fused into a tensor $\\hat{z_i}$ using adaptive weighting, representing the global feature formed by the mixed sample features.\nMeanwhile, in the upper branch, the group encoding features $g_i$ bypass the group mix operation and are directly input into the DEC module. They then pass through the CPF module to generate the original sample's global feature $z_i$. The DEC and CPF modules in both branches share the same weight parameters. In stage 3, the newly generated mixed sample feature obtains a global feature $\\tilde{z_i}$ and the original sample generates a global feature $z_i$. Contrastive loss is constructed between the global features of mixed samples $\\tilde{z_i}$ and the global features of other samples in the same batch.", "content": ""}, {"title": "3.3 Multi object loss optimization", "content": "At this point, three types of objectives need to be optimized in the entire framework: the contrastive learning module based on group mix, the deep embedding clus-"}, {"title": "AbsSoftCossim(ci, cj) = \\frac{c_i^T S c_j}{\\sqrt{(c_i^T S c_i)(c_j^T S c_j)}}$,\nci = Fo(gi), i, j\u2208 [1, ..k],", "content": "Here, in the numerator, $c_i^T S c_j$ representing the weighted dot product between the features ci and cj, where the weights are provided by the similarity matrix S. In the denominator, $\\sqrt{(c_i^T S c_i)(c_j^T S c_j)}$ is the product of the norms of the feature ci and cj, adjusted by the similarity matrix which is composed by learnable parameters. Fo() represents the cluster feature fusion (CPF) module, where \u0398 denotes the learnable parameters of this module. The use of absolute value || is due to the cosine similarity values ranging from (-1,1). Since this similarity will be used as a loss term, taking the absolute value helps prevent positive and negative values from canceling each other during optimization."}, {"title": "tering module, and the final classification module. In the previous section, we have elaborated on the composition of the loss in the contrastive learning module. Here, we will discuss the composition of the clustering loss and classification loss.\nThe clustering loss mainly consists of two aspects. Firstly, using the soft assignment $q_{ij}$ and the auxiliary target distribution $p_{ij}$, we construct a clustering objective loss based on the KL divergence, denoted as $L_{clu}$, as shown in (7). On the other hand, we impose a constraint based on soft cosine similarity to reduce the similarity between any pair of the five cluster features, $Cossim (Ci, Cj)$ representing the cluster vector similarity loss $L_{cos}$, as shown in (8):\n$L_{clu} = \\sum_{i=1}^{I}\\sum_{j=1}^{k} p_{ij} ln \\frac{p_{ij}}{q_{ij}}$,\n$L_{cos} = \\sum_{i=1}^{k}\\sum_{j=1}^{k} Abs SoftCossim(C_i, C_j)$,\nWhere (ci, cj) represents the generated cluster vector representations, k denotes the total number of clusters, set to 5 in this paper. Additionally, for the classification loss between predictions and labels, we employ cross-entropy as the loss function, denoted as Lcls, as shown in (9):\nLcls = - \\frac{1}{N}\\sum_{n=1}^N \\sum_{c=1}^C yclogy(n),\nWhere N represents the number of samples, C represents the number of classes, yc and \u0177c represent the original class labels and predicted labels, respectively. With this, we have constructed the components of the entire framework's optimization objectives, which include the contrastive loss, clustering loss based on soft cosine similarity constraints, and classification loss. The overall loss is denoted as Ltotal, as shown in (10):\nLtotal = Lcon + \u03b1Lclu + \u03b3Lcos + Lcls,\nWhere \u03b1 and \u03b3 are hyperparameters used to balance the weights of clustering and similarity losses in the overall loss function. Lcon, Lclu, Lcos, and Lels represent the contrastive loss, clustering loss, cluster similarity constraint loss, and classification loss, respectively, as discussed earlier.", "content": ""}, {"title": "4 Experiments", "content": "In this section, we provide a detailed description of the dataset, experimental setup, training process, and parameter configurations. We present qualitative and quantitative experimental results along with analysis."}, {"title": "4.1 Dateset and Environment", "content": "Our study is based on the ICBHI2017 dataset, the largest open-source dataset available to date for respiratory sound analysis. The majority of research on respiratory sounds is conducted using this dataset, facilitating experimental comparisons. The dataset comprises 920 audio files and corresponding annotated texts from 126 participants, including adults and children. Each audio recording is segmented based on annotated time intervals to extract respiratory sound cycles. Consequently, a total of 6898 respiratory sound cycles are generated. According to Tablel, the dataset exhibits class imbalance issues. To address this, we perform data augmentation at two levels: raw audio and spectrogram features. We alleviate this problem by applying different data augmentation techniques randomly to each batch during training."}, {"title": "4.2 Data pre processing and parameter setting", "content": ""}, {"title": "4.2.1 Data pre processing", "content": "Sample Length Alignment: The original dataset comprises over 6800 samples with varying durations ranging from 0.2s to 16.2s. Approximately 65% of respiratory sound cycles are less than 3s, 33% have lengths between 4-6s, with an average duration of 2.7s. To ensure uniformity in the length of respiratory sounds used in experiments and considering a typical respiratory cycle lasts 2-3s, we follow the approach suggested by [9] to fix all respiratory sound cycles to 8s. For samples shorter than 8s, we randomly extract segments from the sample itself and concatenate them to reach a length of 8s. If a sample exceeds 8s, we truncate it to retain only the initial 8s.\nData Augmentation: As mentioned earlier, to address data imbalance, data augmentation is performed at two levels: raw audio and spectrogram features. During preprocessing, various augmentation techniques such as adding noise, shifting, stretching, and VTLP are randomly applied to audio samples within each batch. Additionally, during training, random masking is applied to the generated spectrogram features in either the frequency or time dimension.\nData Normalization: Conventionally, entire spectrograms are normalized. In contrast, this study normalizes each row of the spectrogram individually. Considering that each row in the spectrogram represents information within a specific frequency range and different rows represent different frequency bands, normalizing rows aids in identifying frequency variations within each row. Moreover, as respiratory sound features predominantly occur in the low-frequency part of the spectrogram, normalizing columns would potentially mask minor anomalies present in the high-frequency part. Therefore, this study adopts row-wise normalization for respiratory sound feature normalization."}, {"title": "4.2.2 Parameter setting", "content": "During preprocessing, all audio is resampled to 10 kHz and fixed to a duration of 8 seconds. Parameters used in the Short-Time Fourier Transform (STFT) for converting raw audio to spectrograms are set as follows: the number of FFT points n_fft = 1024, window length in"}, {"title": "points win_len = 1000, hop length in points hop_len = 128. The lowest and highest cutoff frequencies in spectrogram features are set to f min = 32.7 and f_max = 3000 respectively, with a uniform filter bank number of 84. Each spectrogram generated contains approximately 625 frames per sample, with a frame length of 20 frames per group and a 5-frame overlap between groups. These grouped spectrograms are then fed into the GFE module for encoding.\nIn the training process, we use the same optimizers and learning rates for the deep clustering module and contrastive learning module, the optimizer used the Adam with an initial learning rate of 0.01. The learning rate is decayed by a factor of 0.33 every 150 epochs to ensure training stability over the course of 600 epochs. As show in Fig.6, the upper subfigure demonstrates the steady reduction in contrast loss and classification loss over the course of training, indicating that the contrast learning as well as the classification head learns the respiratory sounds features better. The values of similarity loss and clustering loss are stable in an interval, which shows that the clustering module does play the role of auxiliary constraints between different cluster features. The bottom subfigure demonstrates the steady reduction in total loss.", "content": ""}, {"title": "4.3 Comparative study", "content": ""}, {"title": "4.3.1 Comparing with other method", "content": "To validate the effectiveness and superiority of the proposed method, we utilized the official 6-4 validation split and compared the classification performance of various current methods on the ICBHI2017 dataset,\nFor the Specificity metric, we bolded scores above 80. Among these works, models achieving high scores without the influence of pre-trained weights employed an attention mechanism-based approach for feature fusion. This suggests that, for the entire spectrogram, selective feature extraction is more beneficial than extracting all information, as it enhances the model's ability to distinguish between normal and abnormal respiratory sounds.\nRegarding the Sensitivity metric, we bolded values above 41. The patch-based method, without pre-trained weights, achieved a sensitivity score of around 30. However, with pre-trained weights [33], [30], the model's sensitivity significantly increased, indicating that pre-trained weights on large-scale datasets help capture subtle feature differences. Additionally, in the [22] literature, although they did not use a patch-based approach,"}, {"title": "they employed a 7x7 convolution kernel, which approximates patch extraction. This further demonstrates that extracting local features from the spectrogram aids in"}]}