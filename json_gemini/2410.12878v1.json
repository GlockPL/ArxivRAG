{"title": "Towards More Effective Table-to-Text Generation: Assessing In-Context Learning and Self-Evaluation with Open-Source Models", "authors": ["Sahar Iravani", "Tim O. F. Conrad"], "abstract": "Table processing, a key task in natural language processing, has significantly benefited from recent advancements in language models (LMs). However, the capabilities of LMs in table-to-text generation, which transforms structured data into coherent narrative text, require an in-depth investigation, especially with current open-source models. This study explores the effectiveness of various in-context learning strategies in LMs across benchmark datasets, focusing on the impact of providing examples to the model. More importantly, we examine a real-world use case, offering valuable insights into practical applications. To complement traditional evaluation metrics, we employ a large language model (LLM) self-evaluation approach using chain-of-thought reasoning and assess its correlation with human-aligned metrics like BERTScore. Our findings highlight the significant impact of examples in improving table-to-text generation and suggest that, while LLM self-evaluation has potential, its current alignment with human judgment could be enhanced. This points to the need for more reliable evaluation methods.", "sections": [{"title": "1 Introduction", "content": "In today's data-driven world, the ability to make informed decisions increasingly depends on our capacity to process and interpret structured data. As we engage with diverse forms of such data across various domains, there is a growing demand for methods that can transform complex, structured information into clear and accessible content [11, 18, 22]. An area that has received significant attention is table processing [4, 5, 12], driven by advances in language models (LMs), which have revolutionized numerous natural language processing (NLP) tasks [27, 12, 3, 33]. These models, trained on vast amounts of text, excel at recognizing patterns in language and applying them to a variety of applications. Among the most promising techniques for handling table-related tasks is the fine-tuning of pre-trained LMs on datasets designed for specific tasks [6, 20, 42]. However, identifying or developing a dataset that is appropriately tailored to the capabilities and requirements of LMs can be challenging. In response, researchers have explored in-context learning [3], a strategy where models are guided by tailored prompts and minimal examples, improving performance across a wide range of NLP challenges. This approach has been particularly effective in table-based tasks like question answering and fact-checking [5, 23]. Building on this, chain-of-thought prompting methods [35, 37] further empower LMs to perform complex reasoning tasks, enabling them to tackle challenges that require multi-step logic and in-depth understanding [13, 43, 39].\nDespite these advancements, the potential of recent open-source LMs in generating narrative text from tabular data or the table-to-text generation task remains underexplored. This task focuses in particular on extracting and verbalizing insights from tabular data into narrative text. Automating this process has far-reaching implications for fields such as healthcare, business intelligence, and academic research, especially in automated report generation and personalized data summaries. It minimizes manual effort, ensures real-time updates at lower costs, and enables more informed decision-making."}, {"title": "1.1 Investigating In-Context Learning for Table-to-Text Generation", "content": "In this study, our objective is to comprehensively explore the performance of in-context learning in the table-to-text generation task. We investigate a real-world use-case scenario of generating biographies for mathematicians. For this use-case, structured (tabular) data is available from sources such as WikiData or the Mathematical Research Data Initiative consortium (MaRDI) which focuses on systematic management and utilization of mathematical research data [8].\nWe focus on optimizing prompting strategies, including zero-shot, single-shot, and few-shot approaches, to enhance table-to-text generation. By carefully selecting examples, we aim to improve model performance, leveraging recent advancements in in-context learning [5]. To evaluate the presented approach, we run experiments on two benchmark datasets: WikiBio [15] and ToTTo [26]. Here, table-to text generation is investigated in two scenarios: WikiBio, where the text is the description of a table, and ToTTo, where a sentence description is presented for a specific cell of the table. We also consider the concern that LMs may have been exposed to portions of public datasets, such as Wikipedia-based datasets, during training, which could inadvertently bias their performance and lead to inflated results. To address this, we evaluate the models on recent Wikipedia biography pages created after the public release of the language models used in this study. This approach mitigates the risk of data overlap, providing a more accurate reflection of the models' unbiased capabilities in transforming structured data into narrative text.\nIn addition to these datasets, we explore a specialized use case involving the MaRDI Portal [8] which contains structured information on mathematicians and their contributions. This investigation provides a unique opportunity to apply table-to-text generation in a real-world scenario. The MaRDI use case is particularly challenging because the available (structured) data about a person can be incomplete, due to the way data are collected from various sources. We present how the choice of examples in the few-shot approach can lead to fewer hallucinations in a real-world table-to-text task, biography generation from tabular data. This comprehensive study allows us to better understand the strengths and limitations of LMs in diverse and complex environments, ultimately contributing to more effective and reliable table-to-text generation applications. Although many high-performing open-source language models exist, in this work we focus on two models: Llama 3 [10] and Phi-3 [1] which are good representative of the current landscape and allow us to allocate resources for comprehensive evaluations, including various in-context learning strategies and real-world use cases. Llama 3, as one of the largest open-source and most advanced models, provides insights on large-scale architectures in table-to-text generation. In contrast, Phi-3, a more lightweight model, allows us to examine performance relative to model size and efficiency. This combination allows us to explore a spectrum of capabilities and offers a balanced perspective on the effectiveness of model scales in practical applications."}, {"title": "2 Methodology", "content": "In this study, our objective was to explore the effectiveness of in-context learning in table-to-text generation tasks with current open-source capable language models including Llama 3 and Phi-3, particularly focusing on the performance differences between zero-shot, single-shot, and few-shot prompting strategies. Our investigation was conducted through two distinct table-to-text generation tasks: (1) generating a concise one-line description from a specific table and (2) generating a comprehensive biography from a given table.\nIn-context Learning: To investigate the impact of in-context learning, we employed three approaches: zero-shot, single-shot, and few-shot prompting. For the zero-shot prompting, we provided the model with a simple instruction to generate text from the table without any examples. This approach allowed us to assess the model's ability to perform the task without prior examples. In the single-shot approach, we added one example input and corresponding output before generating the text for the target table. In the few-shot approach, we supplied the model with three sets of examples. These approaches were used to evaluate how providing contextual examples influenced the model's performance in generating accurate and contextually relevant text.\nInspired by the study by Zhou et al. [44], we generated the initial prompts for our experiments using the well-established GPT-4 system. For each task, we began by providing GPT-4 with a simple task description and a single example from the dataset to generate initial prompts for other language models. However, the output from GPT-4 was not used without modification in all cases. The complexity of the task and the specific characteristics of the dataset often necessitated iterations on the generated prompts. We refined the prompts to ensure that the results were more closely aligned with our desired results. An example of this refinement within our MaRDI use case is as follows: one row in the dataset contains gender-related information, such as [ sex or gender | male]. Our initial experiment with the GPT4 generated prompt, resulted in generated text where the gender was explicitly linked to the scientist's name, such as: Douglas Bate is a male researcher., which was not aligned with our objectives. Since we opted not to preprocess the data to remove such patterns, we instead added a refinement to the prompt: Generate the biography without directly mentioning the gender. This adjustment mitigated the issue without altering the dataset. Additionally, in the cases of single-shot and few-shot prompting, we manually selected examples from the ground truth samples to further guide the models. After evaluating the model's performance across different samples in a zero-shot setting, we selected one high-quality example where the model performed well to serve as the prompt for single-shot learning. For the few-shot setup, we supplemented this optimal example with two additional examples where the model demonstrated moderate performance in the zero-shot scenario, as well as one example where the model performed poorly. This selection strategy ensured that a diverse range of samples was represented, allowing the model to generalize better across varying input complexities.\nWe integrated these examples with natural language instructions. Since LMs process input in a linear, sequential text format, a common method, as highlighted in previous research [30, 31], is to linearize tables into a markdown structure. This is achieved by separating rows with new lines and using column separators (e.g., \"|\") between individual cells to maintain the table's structure within the text sequence."}, {"title": "2.1 Experimental Setup", "content": "The experiments were conducted on a Linux based system having 16 CPU cores, 64 GB RAM, and a NVIDIA A100 GPU for model inference tasks. Language models were deployed using the Ollama framework (version 0.1.44) for model management and inference. We used Python 3.11 with the Langchain module (langchain_community), enabling seamless loading and interaction with Ollama. We conducted experiments using two recent open-source language models, Llama 3 [10] with 70B parameters as a large language model (LLM) and Phi-3 [1] with 14B parameters as a small language model (SLM). This selection enabled a robust evaluation across different scales. LLAMA 3, developed by Meta AI, is known for its high optimization and performance across diverse NLP tasks. It employs a unique 128K tokenizer and grouped query attention to enhance inference speeds. The model is trained on 15 trillion tokens, emphasizing multilingual capabilities and advanced attention mechanisms. Microsoft's Phi-3 is a compact language model optimized for both efficiency and high performance. Renowned for employing high-quality training datasets, Phi-3 utilizes a curated dataset that includes heavily filtered web data, curated educational materials, and synthetic data created by larger models. Despite its reduced parameter count, this model consistently surpasses larger counterparts in standard benchmarks, demonstrating its superior design and training methodology."}, {"title": "2.2 Measurements", "content": "BLEU [25] (Bilingual Evaluation Understudy) score evaluates the quality of machine-generated text by comparing them to one or more reference text. It works by calculating the overlap between small sequences of words, called n-grams, from the generated text and the reference texts. BLEU considers multiple n-gram lengths (e.g., single words, pairs of words, etc.) to capture different levels of similarity. To avoid giving too much credit for repeated words, BLEU uses a method called clipping, which limits how often n-grams are counted. Additionally, a brevity penalty is applied to gnerated text that are too short. The final BLEU score is the geometric mean of the n-gram precisions, weighted equally, and scaled by the brevity penalty. The BLEU score was initially introduced for the evaluation of machine translation but has since been widely adopted across various natural language processing (NLP) tasks. However, it exhibits limitations in effectively capturing semantic nuances and may not consistently correlate with human judgment, particularly in more complex NLP tasks [28, 14].\nBERTScore [41] evaluates the semantic similarity between a candidate and reference sentence using to-ken embeddings from pre-trained models like BERT. Unlike BLEU, BERTScore captures semantic mean-ing rather than exact word matches, providing a more nuanced assessment of text quality. To calculate BERTScore, both sentences are tokenized and embedded, and then cosine similarity is computed between each token in the candidate and every token in the reference. Precision is calculated by averaging the max-imum similarity for each candidate token with respect to the reference, while recall averages the maximum similarity for each reference token with respect to the candidate. The final BERTScore is the harmonic mean (F1-score) of precision and recall.\nLLM Self Evaluation employs LLMs to evaluate the quality of a text based on a set of pre-defined criteria. We tested a LLM self-evaluation strategy inspired by the G-EVAL framework [21], which incorporates LLMs with chain-of-thought (CoT) reasoning [37]. We specifically tailored the evaluation template, originally developed for summarization tasks, to suit biography evaluation. G-Eval employed GPT-4 to generate CoT prompts and evaluate generated text based on four key criteria: fluency, relevance, consistency, and coherence. In this study, we instead employed Llama 3 to assess the generated biographies based on the GPT-4 generated criteria. We were particularly interested in determining whether LLM self-evaluation could be effectively used for table-to-text generation, especially for datasets like MaRDI that lack reference data to measure performance. Our goal was to assess the reliability of this evaluation method in scenarios where traditional metrics could not be applied, ensuring a comprehensive assessment even in the absence of benchmark data."}, {"title": "2.3 Datasets", "content": "2.3.1 WikiBio\nThe WikiBio dataset [16] consists of approximately 700,000 pairs of Wikipedia infoboxes and their corresponding biographical introductions. Each infobox is a structured collection of key-value pairs representing factual information (e.g., name, birth date, occupation), while the corresponding biography provides a nat-ural language summary of these facts. The dataset is widely used in natural language generation tasks, specifically for table-to-text generation. WikiBio presents challenges such as maintaining factual fidelity, ensuring text fluency, and handling varying levels of detail across infoboxes, making it a suitable resource for evaluating table-to-text generation models.\n2.3.2 Recent Wikipedia pages\nTo ensure a fair evaluation of the latest language models, Llama 3 and Phi-3, we specifically designed a dataset similar to WikiBio, but with a critical distinction: it exclusively comprised Wikipedia biography pages created after June 2024. This selection criterion was pivotal because it coincides with the timeline after the public releases of Llama 3 in April 2024 and Phi-3 in May 2024. Consequently, this approach decreased the likelihood that the models had previously encountered the data during training (unless the recent Wikipedia pages were generated by these models themselves). By focusing on these newly generated pages, we aimed to provide a more accurate and unbiased assessment of the models' capabilities in executing table-to-text generation tasks.\n2.3.3 MaRDI\nIn our use case, we focused on generating biographical content from structured mathematical research data housed within the MaRDI Portal [8]. This portal featured a rich knowledge graph that incorporated data from various mathematical sources such as DLMF, CRAN, PolyDB, swMATH, and zbMATH, which were still in the partial stages of integration. The objective was to automate the generation of detailed biographies for mathematicians and researchers by harnessing this structured data, thereby facilitating a more dynamic presentation of their contributions and impact within the mathematical community. As we do not have any reference biographies for this data, we evaluated the outcomes manually and with the LLM self-evaluation method across relevance, fluency, consistency, and coherence metrics.\nFor this dataset, we implemented the zero-shot prompting strategy, similar to our approach with other datasets. However, since we lacked existing examples for single-shot and few-shot prompting, we adopted this strategy: first, we generated initial outputs using the zero-shot method. We then manually modified selected samples. For the single-shot prompting, we used one example of a well-constructed biography to provide a clear standard for the model to follow when generating new outputs. Our experiments revealed that most hallucinations occurred in cases where the information was incomplete or missing. Therefore, to select the most effective examples for few-shot prompting, we included both the samples where the model performed well, to serve as a template for replicating success in other cases, and those samples that contained significant errors, requiring substantial editing to serve as an example. By including both successful outputs and those with significant errors, we aimed to guide the model on how to handle such situations and cover variety in the data.\n2.3.4 \u03a4OTTO\nThe ToTTo dataset [26] is a large-scale corpus for table-to-text generation, containing over 120,000 examples of Wikipedia tables paired with human-written descriptions. It focuses on generating fluent and factually accurate natural language summaries from specific subsets of tables, where relevant cells are annotated to guide the content selection process. ToTTo addresses key challenges such as content selection, fluency, and factual accuracy, making it a critical benchmark for models designed to convert structured table data into coherent text. Our aim in running the experiment on this task was to observe how well LMs could understand where to focus within the table to produce accurate and relevant descriptions."}, {"title": "3 Results", "content": "In this section, we present the experimental results, benchmarking the performance of our pipeline across multiple datasets. Figures 3,4, and 5 provide illustrative examples of the pipeline's output, demonstrating its effectiveness across diverse datasets. Additionally, Figure 7 highlights the impact of example selection in few-shot prompting, showcasing how carefully chosen examples enhance the relevance of the generated outputs. In the following subsections, we provide a detailed walkthrough of the benchmarking results, underscoring the robustness and versatility of the pipeline in benchmark datasets and a real-world application."}, {"title": "3.1 WikiBio", "content": "Table 1 presents the comparison study of the zero-shot, single-shot and few-shot prompting on the Wik-iBio dataset. BLEU scores and BERTScores show incremental performance improvement with Llama 3 by employing examples in the prompt. However, Phi-3, as a SLM, exhibited comparatively less benefit. We also conducted a manual evaluation of 10 randomly selected samples. While Llama 3 demonstrated slight improvements and a reduction in hallucinations, no enhancement in the quality of biographies was observed when examples were provided to the Phi-3 model. To ensure that this difference in performance was not due to Phi-3 having already been exposed to the data during its training (which could diminish the impact of additional examples) we conducted experiments on recently added Wikipedia pages. The LLM self-evaluation results in Table 1 across fluency, relevance, consistency, and coherence did not show any agreement with the quantitative measurements. In many cases, the self-evaluation scores were close to the maximum possible values, providing little meaningful differentiation between the different prompting strategies. We were more interested to see if these metrics had any agreement with BERTScores that have been shown to correlate well with human judgment. As evident, the self-evaluation metrics do not show any agreement with Bertscores. We further examined this discrepancy using recent Wikipedia pages to gain a better understanding of its applicability in table-to-text generation tasks."}, {"title": "3.2 Recent Wikipedia pages", "content": "Table 2 presents the comparison study of the zero-shot, single-shot, and few-shot prompting on recent Wikipedia pages. According to this table, the few-shot approach generally outperforms zero-shot and single-shot approaches across most metrics in both Llama 3 and Phi-3 models. This suggests that incorporating a few examples into the model's training or inference process could enhance its performance. The performance improvements are notably more pronounced with Llama 3 compared to Phi-3. By manually comparing the 10 randomly selected samples we also noticed more considerable improvement with Llama 3 than Phi-3."}, {"title": "3.3 MaRDI", "content": "Figure 4 presents a table data extracted from MaRDI Portal on the first row and the generated biography using few-shot prompting with Llama 3 on the second row. This example demonstrates the model's ability to accurately interpret the properties and values without generating hallucinations. The resulting biography is coherent, consistent, and easy to read and understand and covers the information provided in the table.\nWe manually observed that the few-shot approach, when provided with well-crafted demonstrative examples significantly reduced hallucinations in generated text. This reduction was evident not only when compared to the zero-shot approach but also when compared to the single-shot approach, where the model was only shown one pair of table and biography. Notably, Llama 3 demonstrated a lower incidence of hal-lucinations, particularly in cases of missing information. This finding underscores the importance of careful selection of demonstrative examples to cover similarity and diversity. Figure 7 illustrates an example of how different prompting strategies can lead to better outcomes when dealing with incomplete information. Compared to the example shown in Figure 4, the structured data in this example contains only a few properties. In the zero-shot scenario, the model generated content that is not relevant to the provided table, leading to hallucinations. The single-shot approach mitigates these hallucinations, guided by a well-crafted example that sets a clear standard for what a good biography should look like. The few-shot approach, however, performed best, as it effectively avoids adding extraneous information. This success is likely due to one of the examples in the few-shot prompt including missing information, which helped the model learn to avoid generating unrelated content. However, we manually observed that Phi-3 was less responsive to these examples and continued to generate unrelated text in similar cases.\nThe improvement with Llama 3 based on our manual evaluation contrasts with the results presented in table 3, which shows the LLM self-evaluation on the MaRDI dataset. This table shows that the differ-ences between zero-shot, single-shot, and few-shot strategies across various metrics are relatively small. We highlighted this contrast in second row of Figure 7 for the corresponding MaRDI example in the first row. Despite the evident improvement in the generated output using the few-shot prompt using Llama 3, the LLM self-evaluation does not reflect these differences."}, {"title": "3.4 TOTTO", "content": "Table 8 demonstrates the comparison study of the zero-shot, single-shot, and few-shot prompting on the ToTTo dataset using Llama 3 and Phi-3 models. The results indicate that Llama 3 benefits more from the few-shot approach compared to Phi-3, which aligns with findings from other experiments. The reduction in Phi-3's performance with the few-shot method, as opposed to single-shot, mirrors a similar trend observed in recent Wikipedia pages as well. This reduction can likely be attributed Phi-3's sensitivity to context length. Including multiple examples in the prompt might create a context that is too lengthy for Phi-3, as an SLM, to effectively process and understand. Our manual evaluation of 10 randomly selected samples further supports this, as the samples generated using the few-shot approach with Llama 3 showed noticeable improvements in the quality. We mentioned earlier that we modified the human-crafted prompts for the language models using GPT-4, and for ToTTo dataset we made an attempt to compare the original prompt with the modified prompt in Figure 9. The results indicate that the performance of both Llama 3 and Phi-3 models improved with GPT-4-modified prompts compared to the human-crafted ones."}, {"title": "4 Related Work", "content": "The notable advancements in LMs [34, 2, 1, 32] have driven a major shift in the fields of controllable text generation and data interpretation. Leveraging these developments, recent research has explored table processing across various scenarios, including question answering [7], fact checking [9], and real-world information seeking [43]. While fine-tuning models [6, 20, 42] on specific tasks remains popular, it often re-quires substantial amounts of high-quality data. Moreover, these table-based tasks encompass a wide variety of input-output formats and domains, presenting significant variability across task types. Studies such as UnifiedSKG [38] have aimed to standardize diverse table-based tasks by converting them into a unified text-to-text format. Instruction-tuning is another promising approach, as explored by models like TableLlama [40] and Table-GPT [19]. This approach involves constructing instruction tuning datasets and continuing the pre-training LMs, such as Llama 2 (7B) [34] in TableLlama and GPT-3.5 in Table-GPT. While instruction-tuning has demonstrated promising results and enhanced generalization capabilities, the process of curating new datasets and pretraining large models comes with significant computational and resource costs. Moreover, instruction-tuning does not consistently outperform the fine-tuning of smaller, pre-trained language models in terms of accuracy. Previous research [17] suggests that across various metrics and domains, there is no definitive advantage of LLM-based solutions over pre-LLM approaches. Although Table-GPT demonstrates superior performance on table-related tasks compared to GPT-3.5 and ChatGPT through continued training on GPT-3.5, the associated training costs remain a prohibitive factor for many enterprises looking to deploy models privately. Despite these efforts, recent findings [5, 22] indicate that LMs, through in-context learning, can outperform fine-tuned models on table-processing tasks. In contrast to fine-tuning, in-context learning (1) requires only a few annotations or demonstrations as prompts, and (2) conducts inference without mod-ifying the model parameters, reducing the need for extensive fine-tuning. The effectiveness of in-context learning largely depends on how well demonstrative examples are selected and organized within the prompt. For instance, Nan et al. [24] suggest that to improve the text-to-SQL capabilities of LMs, it is important to consider not only similarity but also diversity among the examples. We explored this potential in our MaRDI use case. In-context learning approaches heavily studied the superiority of GPT families (GPT3, 3.5, 4 and ChatGPT) [5, 43] for various table processing tasks. We based our experiment on open-source models that can be run locally, alleviating concerns around privacy and the high costs. Additionally, we specif-ically targeted the table-to-text generation task, which remains surprisingly underexplored in the current literature. Another factor of the effectiveness of in-context learning is the unpredictability of plain language prompts. Plain language prompts may fail to produce desired results, requiring users to experiment with various instructions [29, 36]. This is due to the lack of transparency in how LMs process the instructions for the human. We modified human-crafted prompts using stronger LMs like GPT-4 to instruct other LMs and explored its potential in ToTTo data analysis to improve the outcomes. This approach does not raise concerns about data privacy or higher costs, as the prompts are only used once and solely for describing the task to the other LMs."}, {"title": "5 Discussion & Conclusion", "content": "In this study, we employed a comprehensive prompting strategy across multiple datasets to evaluate two language models, an LLM and an SLM, for table-to-text generation tasks. Our aim was to assess the models' effectiveness in both standard and real-world applications, including the WikiBio, ToTTo datasets, and our use case MaRDI data. This study suggests that language models can benefit from in-context learning by providing a set of proper examples to the model for table-to-text generation.\nWe investigated the potential influence of prior data exposure using the WikiBio dataset. To address this, we retrieved recent Wikipedia pages and formatted them similarly to the WikiBio dataset, then conducted experiments on this newly retrieved data. The key finding from this study was that we observed a more significant improvement in biography generation on the recent Wikipedia pages compared to the WikiBio dataset. We attribute this effect to the likelihood that the language models had not encountered the recent data during training, making the impact of providing examples in the prompts more pronounced. This conclusion was supported by both automated metrics and manual evaluations.\nWe compared the performance of Llama 3, as an LLM, with Phi-3, an SLM. As expected, Llama 3 generally outperformed Phi-3 across various settings and datasets, reflecting the advantages of larger models in table-to-text tasks. However, the primary goal of this comparison was to analyze how both models responded to few-shot prompting. While both improved with the inclusion of examples, Llama 3 showed consistently greater benefits from additional examples. Notably, in our experiments with the MaRDI dataset, a real-world application, Llama 3 demonstrated a significant reduction in hallucinations, an improvement not observed with Phi-3.\nWe studied the potential of LLM self-evaluation with CoT reasoning for measuring the table-to-text generation task using Llama 3, a powerful open-source LLM. Specifically, we aimed to assess the correlation of this metric, which does not require a ground truth for comparison, with BERTScore, which is known to align well with human judgment. Interestingly, despite the potential of this strategy in other tasks , our study revealed that the self-evaluation strategy showed weak or no meaningful correlation with BERTScore . To investigate further, we analyzed the MaRDI dataset and manually observed a significant reduction in hallucinations when comparing the few-shot approach to the zero-shot method. However, despite this noticeable improvement, the LLM self-evaluation metrics showed minimal variation and failed to reflect the relevance between the provided table and the corresponding generated biography . These results suggest that the current state-of-the-art LLM self-evaluation strategy, even with advanced open-source models like Llama 3, is not yet reliable for measuring table-to-text generation tasks. This can be explained by either the LLM favors their own generated outputs or failing to establish a meaningful connection between the table and the generated text. In future work, we will explore providing observatory examples for evaluation model that might improve the alignment between LLM self-evaluation and human judgment.\nTable processing studies have often demonstrated LM performance on benchmark datasets that are known to be well-structured. However, current public datasets, such as WikiBio and ToTTo, commonly used for benchmarking, have inherent limitations for table-to-text generation. For example, Wikipedia-based datasets like WikiBio often contain inconsistencies between infoboxes and the associated text. Similarly, the ToTTo dataset presents challenges where descriptions sometimes focus solely on a specific table cell, while in other cases, additional information from the table is included. This mismatch complicates accurate model evaluation. This is where our study of the MaRDI use case, representing a real-world application, offers valuable insights. It built a foundation on how to design prompts and how to prepare examples where there is no ground truth in real-world table-to-text generation scenarios (section 2.3.3)."}]}