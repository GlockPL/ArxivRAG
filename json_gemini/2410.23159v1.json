{"title": "Fourier Amplitude and Correlation Loss: Beyond Using L2 Loss for Skillful Precipitation Nowcasting", "authors": ["Chiu-Wai Yan", "Shi Quan Foo", "Van Hoan Trinh", "Dit-Yan Yeung", "Ka-Hing Wong", "Wai-Kin Wong"], "abstract": "Deep learning approaches have been widely adopted for precipitation nowcasting in recent years. Previous studies mainly focus on proposing new model architectures to improve pixel-wise metrics. However, they frequently result in blurry predictions which provide limited utility to forecasting operations. In this work, we propose a new Fourier Amplitude and Correlation Loss (FACL) which consists of two novel loss terms: Fourier Amplitude Loss (FAL) and Fourier Correlation Loss (FCL). FAL regularizes the Fourier amplitude of the model prediction and FCL complements the missing phase information. The two loss terms work together to replace the traditional L2 losses such as MSE and weighted MSE for the spatiotemporal prediction problem on signal-based data. Our method is generic, parameter-free and efficient. Extensive experiments using one synthetic dataset and three radar echo datasets demonstrate that our method improves perceptual metrics and meteorology skill scores, with a small trade-off to pixel-wise accuracy and structural similarity. Moreover, to improve the error margin in meteorological skill scores such as Critical Success Index (CSI) and Fractions Skill Score (FSS), we propose and adopt the Regional Histogram Divergence (RHD), a distance metric that considers the patch-wise similarity between signal-based imagery patterns with tolerance to local transforms. Code is available at https://github.com/argenycw/FACL.", "sections": [{"title": "Introduction", "content": "Precipitation nowcasting refers to the task of predicting the rainfall intensity for the next few hours based on meteorological observations from remote sensing instruments such as weather radars, satellites and numerical weather prediction (NWP) models. The development of a precise precipitation nowcast algorithm is crucial to support weather forecasters and public safety, as it could facilitate timely alerts or warnings on severe precipitation and mitigate their impact on the community through early preventive actions. Sharp precipitation nowcast imagery that is perceptually similar to the actual observations (such as radar images) is equally important for weather forecasters to comprehend how the severity of precipitation will evolve in space and time, as well as to diagnose the rapid evolution of the underlying weather systems in real-time forecasting operations.\nBesides the traditional optical-flow and NWP models, deep learning models have also been widely explored and adopted for precipitation nowcasting in recent years. The research community generally formulates the task as a spatiotemporal prediction problem, where a sequence of input radar or satellite maps is given, and the future sequence needs to be predicted or generated. Although multiple previous attempts proposed solid improvements to the model to grasp the spatiotemporal dynamics,"}, {"title": "Related Works", "content": "Previous works generally formulate precipitation nowcasting as a spatiotemporal predictive learning problem. Given a sequence of observed tensors with length t: X1, X2, ..., Xt, the problem is to predict the future k tensors formulated as follows:\nBased on this formulation, numerous variations of convolutional RNN models were proposed to model both spatial and temporal relationships in the data. ConvLSTM [2] first proposed to integrate convolutional layers into LSTM cells, with the recurrence forming an encoder-forecaster architecture. PredRNN [3] replaced the ConvLSTM units with ST-LSTM units and modified the structure such that the hidden states flow in both spatial and temporal dimensions in a zigzag pattern. MIM [4] replaced the forget gate in ST-LSTM with another RNN unit, forming a memory-in-memory structure to learn"}, {"title": "A Non-deterministic Perspective on Atmospheric Instability", "content": "Traditional models can result in blurry predictions at longer lead times, causing difficulty in forecasting operations. To address it, recent works leverage generative models such as GANs and diffusion models to promote realistic forecasts which could bring more insightful observation to forecasting operations. DGMR [1] utilizes a GAN framework with discriminators in both the spatial and temporal dimensions to ensure that the predicted images are sufficiently realistic and cohesive. LDCast [16] uses latent diffusion to generate a diverse set of outputs for ensemble forecasting. Meanwhile, the literature in video generation strives to generate realistic output frames with generative models. PreDiff [17] introduces a knowledge alignment mechanism with domain-specific constraints while adopting a latent diffuser for quality forecasts. DiffCast [18] appends a diffusion component as an auxiliary module to improve the realisticity of the forecasts. It is worth mentioning that the literature in video generation [1, 19, 20, 21, 22] also exhibits potential in generating high-quality nowcastings despite not specifically being designed to handle precipitation. Unlike works in video prediction, instead of evaluating the output quality with pixel-wise similarity, perceptual metrics such as LPIPS [23] and Fr\u00e9chet Video Distance (FVD) [24] are predominantly used.\nThese works usually formulate the task as an unsupervised or semi-supervised learning problem with the results being non-deterministic based on a random prior, enabling the possibility of ensemble prediction. However, studying each prediction individually is less reliable as the prediction is unexplainably affected by the random prior. Furthermore, the inference efficiency of the diffusion model is poor due to the iterative nature of the reverse diffusion sampling process. Concerning the drawbacks of generative models, our method is proposed to be efficient, deterministic, and accurate at both the pixel and perceptual levels, bridging the advantages of both probabilistic video prediction and non-deterministic video generation."}, {"title": "Supervised Learning Problems That Utilize Fourier Transform", "content": "Spectral analysis in the Fourier space is a common practice for DNNs to study the features in terms of frequency. Rahaman et al. [25] proposed a property known as the spectral bias, which causes DNN models to be biased towards low-frequency functions. A follow-up study [26] theoretically showed that DNN models have a much slower convergence rate toward high-frequency components. Such observations motivate subsequent works to apply Fourier-based loss terms extensively in tasks such as super-resolution (SR) where fine details are crucial.\nDespite the existence of works that apply Fourier transform amid the model feed-forward pipeline [27, 28, 29, 30, 31], here we focus on works that utilize spectral transform in the loss function or as a regularization term. Inspired by the JPEG compression mechanism, the Frequency Domain Perceptual Loss [32] compares the Discrete Cosine Transform (DCT) of the model"}, {"title": "Our Methods", "content": "In this section, we start by arguing why a naive implementation of the Fourier loss does not benefit the model compared with the MSE loss in the image space. Then, we will discuss the motivation and details of our proposed FACL."}, {"title": "Preliminaries", "content": "An image X can be interpreted as a 2D matrix with the transformed Fourier series, F. The orthonormalized Discrete Fourier Transform (DFT) output and its corresponding inverse Discrete Fourier Transform are formulated as:\nwhere M and N are the height and width, respectively, of the image X.\nTo constrain model convergence via the spatial frequency components of its prediction, one naive design is to regularize the L2 norm of the displacement vector between the ground truth and prediction in the Fourier space apart from the image space. Parseval's Theorem shows that such design is linearly proportional to the spatial MSE loss, and the detailed proof can be found in Appendix B.\nSince this straightforward regularization does not differ from the MSE loss in the image space, the common adaptations from previous works are either to apply weighting on different frequencies or to decompose the Fourier features into amplitude |F| and phase \\(\\Theta_F\\) with the following definitions:"}, {"title": "Fourier Amplitude Loss (FAL)", "content": "As the spectral bias indicates the lack of attention to the high-frequency components, we encourage the model to consider high-frequency patterns by applying a loss on the amplitude of each frequency band. Similar to previous works, we first apply DFT to obtain the spectral information. Using Equation (3), we extract only the Fourier amplitudes (|F|) in the Fourier space and compare them in L2:"}, {"title": "Fourier Correlation Loss (FCL)", "content": "To remedy the missing information resulting from FAL, there are several approaches to take the image structure into account. A straightforward way is minimizing the difference of the Fourier phase between the prediction and the label, but it fails as \\(\\Theta_F\\) obtained under DFT is discontinuous. Another approach is to compute the cosine distance in the Fourier domain without extracting \\(\\Theta_F\\) directly. However, our preliminary experiments reveal that such formulation is unstable in reconstructing the image structure. Ultimately, we propose to implement the correlation between the generated output and ground truth in the Fourier domain and adopt it as the Fourier Correlation Loss (FCL) in our proposed loss:"}, {"title": "Proposed Approach: Random Selection between FAL and FCL", "content": "While it is straightforward to apply the overall loss function as a linear combination of FAL and FCL, we find it tricky to determine the weighting of the components in our preliminary studies. Instead, we offer a more controllable solution - to alternate FAL and FCL as shown below:\nwhere p is sampled randomly and uniformly in [0, 1] and P(t) is a pre-defined threshold decreasing during the training process as shown in Figure 1. P(t) always decreases from 1 to 0 such that the model is first trained with 100% FCL that takes image structure into account, and then the models are more frequently trained with FAL which improves the image sharpness.\nSince FCL loses information on the overall brightness, the model could not achieve proper brightness at the early stage where FCL dominates the learning objective. To address it, we append a sigmoid function in the output layer of the model. This constrains the model output in the range [0, 1] to prevent the model from converging to a sub-optimal state with an undesirable range of output values.\nOverall, the following modifications are applied to the models:"}, {"title": "A New Metric: Regional Histogram Divergence (RHD)", "content": "Previous works in video prediction tend to use pixel-wise met-rics such as MSE and MAE to measure the difference between the prediction and labels. Such a choice of metrics might not fit spatiotemporal data for two reasons: (1) reasonable pixel shifts are highly penalized, and (2) the overall distribution of values is ignored. This encourages the models to output blurry pre-dictions while regional uncertainty diffuses outward over time. By inverse, deep perceptual metrics such as LPIPS, Inception Score (IS) and Fr\u00e9chet Video Distance (FVD) suffer from the knowledge bias between multi-channel pictures (as pre-trained on ImageNet) and monotonic signal-based intensities.\nOne of the metrics that consider both the previous two factors is the Fractional Skill Score (FSS), which is widely used in meteorology. After splitting the image into Nx \u00d7 Ny smaller patches, where Nx and Ny control the shift of precipitation events we tolerate, we obtain the FSS score as follows:\nwhere Fi,j and Oi,j refer to the fraction of predicted positives and fraction of observed positives, respectively, of the patch in the i-th row and j-th column. Based on this formulation, the intensities are free to reposition within the patch window, granting tolerance to translation and deformation. Nevertheless, one drawback of FSS is that the pixel range is only categorized into two classes: positives and negatives. For a threshold of 0.5, a pixel value of 0 is treated the same as a pixel value of 0.49, resulting in a huge error when viewing the per-patch precision. This means that the choice of threshold induces a bias in evaluating the forecasting performance of models.\nTo improve the representation, we propose the Regional Histogram Divergence (RHD), a variation of FSS that exhibits smaller errors within a class. Instead of categorizing the pixel values into \u2018hits\u2019 and \u2018misses\u2019, we divide the values into n bins and count the frequency of each bin, obtaining a histogram for each patch. Next, we compare the average Kullback-Leibler (KL) divergence on the histograms. Mathematically, the RHD between two sets of bins can be expressed as:\nwhere \\(F_{i,j}\\) and \\(O_{i,j}\\) correspond to the predicted and observed discrete probability distributions, respectively, among the set of bins X of the patch in the i-th row and j-th column.\nDifferent from FSS where the proportions of positives are subtracted directly, RHD instead compares the distributional difference in the context of the histograms. This not only increases the precision of each class/bin, but also heavily penalizes blurs since blurring forms a Gaussian-like distribution in the histograms while sharp intensities should have an \u2018M-shape\u2019 bimodal distribution. If the patch-wise distribution of the two images is identical, the corresponding RHD is 0. The larger the RHD is, the more different the two sets of patches behave. Furthermore, RHD is formulated to be a mean KL divergence so it is always non-negative.\nFor simplicity and consistency, we choose the number of bins to be 10, divided uniformly within the range [0, 1] for all datasets in our experiments. In real-life applications, non-uniform division can be applied for data in non-linear scales such as radar echo (in dBZ) to highlight specific ranges of values. When we compute the histograms, as 0 dominates in the imagery, we apply a threshold \\(\\epsilon = 10^{-5}\\) to exclude all small intensities."}, {"title": "Experiments", "content": "We evaluate the performance of our proposed method on a synthetic dataset and three radar echo datasets, namely, Stochastic Moving-MNIST, SEVIR [41], MeteoNet [42] and HKO-7 [43]. A more detailed description for each dataset can be found in Appendix A. To show that our method is effective and generic, we selected ConvLSTM [2] and PredRNN [3] (reported in Appendix I), two RNN-based models with different recurrence paths; SimVP [44], a CNN-based model and Earthformer [10], a transformer-based model. We trained the models with two variants: conventional MSE and FACL (as formulated in Eq. (6)). To compare with generative models as references, we also report the results of LDCast [16] (latent diffusion) and MCVD [21] (denoising diffusion) for all datasets. For Stochastic Moving-MNIST, we report two more models, i.e., PreDiff [17] (latent diffusion) and STRPM [45] (GAN-based). Appendix M reports the detailed setup and hyper-parameters.\nIn the upcoming sections, we will first present the setup and experimental results on the Stochastic Moving-MNIST dataset. After that, we will test the models with three real-world radar echo datasets. Extra studies on our methods are reported in the Appendix. Specifically, we report the ablation study of FAL, FCL and \u03b1 in Appendix F, the running time of FACL in Appendix G, experiments on additional datasets in Appendix H, comparison with other potential loss functions in Appendix J, the performance when applying FACL to generative models in Appendix K, and characteristic analysis of RHD in Appendix L. To demonstrate the advantages of our method against counterparts for precipitation nowcasting, video prediction and video generation, we evaluate the models with a union of metrics from the areas. Specifically, we report the MAE and SSIM to show the pixel-wise and structural accuracy; LPIPS and FVD to show the deep perceptual similarity to ground truth; FSS and RHD to measure the similarity of the intensity distribution in different regions. For radar echo datasets, we further include the CSI and pooled CSI to reveal the models' capability of identifying potential extreme weather. Such a combination of metrics is believed to facilitate a comprehensive understanding of the pros and cons of the current state-of-the-art in precipitation nowcasting."}, {"title": "A Stochastic Modification of Moving-MNIST", "content": "The Moving-MNIST dataset has been a common benchmark to evaluate how well a model could predict motion in spatial preservation and temporal extrapolation. However, the nature of the Moving-MNIST is highly deterministic, which does not resemble the chaotic nature of the atmospheric system. Previous adaptations attempted to simulate the physical dynamics by introducing a set of complex motions such as rotation and scaling [10] or by applying an external force on collision [46]. We argue that the fundamental reason causing the blur in precipitation nowcasting is the intrinsic stochasticity of the motion caused by external factors unseen in the weather dataset, such as orographic effects, vertical wind shear, interaction with other weather systems, etc. Trained with such stochasticity, regular models with pixel-wise loss could consistently fail to provide quality prediction in the future lead time.\nTo verify our claim, we introduce a simplistic modification to the Moving-MNIST dataset. The standard Moving-MNIST dataset contains handwritten digits sampled from the MNIST dataset moving and bouncing with a constant velocity (uo, vo) on the 64 \u00d7 64 image plane. To introduce stochasticity, we perturb the velocity with a random Gaussian noise \\(\\epsilon\\) at each time step. Details of the perturbation are shown in Appendix A. In the upcoming sections, we dub this dataset Stochastic Moving-MNIST and apply the experimental setting to this synthetic dataset. The performance of combinations of different losses and models can be found in Table 1 and qualitative visualizations of the corresponding methods are shown in Figure 2 and Appendix N. Note that the Stochastic Moving-MNIST is used in both training and evaluation to ensure that the models are well exposed to motion randomness.\nIn Table 1, our modification drastically improves the sharpness and realisticity for all tested models, as reflected by the vast reduction in LPIPS and RHD. In particular, FACL reduces up to 57% of LPIPS and 71% of RHD for the ConvLSTM model. The pixel-wise and structural metrics between the two losses are comparable. On the other hand, generative models result in much poorer MAE and SSIM, with skill scores like FSS still being worse than most of the baseline models. In Figure 2, we can observe that the model trained with MSE cannot reconstruct a clear spatial pattern, especially in the subsequent frames, while the model trained with FACL yields much sharper and higher quality"}, {"title": "Performance on Radar-based Datasets", "content": "In this section, we extend the previous setup to general radar-based datasets. Apart from the distance metrics used in the last section, we further report the CSI with different pooling sizes. Following the previous works [43, 10], we measure multiple CSI scores with different thresholds ({16, 74, 133, 160, 181, 219} for SEVIR, {12, 18, 24, 32} for MeteoNet and {84, 117, 140, 158, 185} for HKO-7).\nThe visualizations can be found in Figure 3 and more in Appendix N.\nThe results of Table 2 are similar to the observations in Table 1. Compared with the MSE baselines, FACL always improves the perceptual and skill scores. For sharp forecasts, the pooled CSI increases with the pooling size while for blurry forecasts, CSI shows no apparent difference based on pooling size. For some models, we observe a tiny decay in pixel-wise and structural metrics. For example, the Earthformer model trained with FACL on SEVIR has a 6.4% increase in MAE, which is believed to be a natural trade-off since pixel-wise metrics have no tolerance for spatial transformation. Despite poorer pixel-wise performance, the perceptual metrics and skill scores always improve, as further illustrated by Figure 3 that only FACL predicts fine-grained extreme values. Regarding those generative models, although they could perform the best in deep perceptual scores like LPIPS and FVD, we still observe that they usually result in poorer skill scores. Moreover, it is noteworthy that FACL does not add any new parameters to the model. The change in the metrics solely indicates that FACL induces a shift of focus from pixel-wise accuracy to image quality and prediction skillfulness."}, {"title": "Conclusion", "content": "In this paper, we proposed the Fourier Amplitude and Correlation Loss (FACL). The two loss terms, Amplitude Loss (FAL) and Fourier Correlation Loss (FCL), encourage the model to focus on the Fourier frequencies and image structure correspondingly. Besides, we proposed a new metric, Regional Histogram Divergence (RHD), to measure the patch-wise similarity between two spatiotemporal patterns. We widely tested our methods on a synthetic dataset and three more real-life radar echo datasets, measured by metrics considering accuracy, realisticity and skillfulness. Extensive experiments reflect that our method yields sharper, more realistic and skillful forecasts with limited degradation in pixel-wise similarity.\nDespite the remarkable performance of the FACL loss, our methods still have room for improvement. First, we assumed the data to be monotonic radar echo, which might not generalize well to multi-modal datasets featured in medium-range forecasts. Besides, our loss provides no regularization on temporal consistency, which may lead to the misalignment of temporal features between frames. The solution to these issues, however, will be open for future work."}, {"title": "Details of the Datasets", "content": "Stochastic Moving-MNIST. Based on the same method generating vanilla Moving-MNIST, we further update the velocity of the digits at time t:\nEach unit corresponds to a pixel in the 64 \u00d7 64 image. Note that the expected trajectory is unchanged under this modification. However, the biased trajectory is exposed to the model as a stochastic factor influencing model training. Due to such behavior, models trained with the MSE loss are expected to exhibit a motion blur pattern along the direction of motion.\nN-body-MNIST. To forge the chaotic nature of the Earth system, N-body-MNIST [10] was pro-posed to study the effectiveness of deep learning models. Rather than linear translation as in the conventional Moving-MNIST, digits in N-body-MNIST follow the N-body motion pattern, exerting an attractive force between digits and causing each other to circulate. Following the default setting of the original paper, the frame size is set to be 64 \u00d7 64 with N = 3. We use the same training, validation and test sets which contain 20000, 1000 and 1000 sequences respectively provided by the official repository.\nSEVIR. The SEVIR dataset [41] is a spatiotemporally aligned dataset containing over 10,000 weather events in a 384km \u00d7 384km region in the US spanning a period of four hours from 2017 to 2019. Among the five channels provided, we extract the NEXRAD Vertically Integrated Liquid (VIL) data product for precipitation nowcasting. Following previous works [10, 47], we predict the future VIL up to 60 minutes (12 frames) from 65 minutes of input frames (13 frames). We sample the test set from June 2019 to December 2019, leaving the remaining as the training set.\nMeteoNet. MeteoNet [42] is an open meteorological dataset containing satellite and radar imagery in France. The data covers two geographic areas of 550km \u00d7 550km on the Mediterranean and Brittany coasts, respectively, from 2016 to 2018. The time interval between consecutive frames is 5 minutes. Since there are missing values labeled as -1 in the raw rectangular data in shape (565, 784), we preprocess it by filling 0 to the missing values, followed by a linear scaling of pixel values to the range [0, 1]. After that, we downsample the images to (256, 256) using bilinear interpolation. The task is to predict the next sequences of radar echoes in an hour (12 frames) from the given 20-minute radar echoes (4 frames). The data in 2016 and 2017 are sampled as the training set and those in 2018 are sampled as the test set.\nHKO-7. The HKO-7 dataset [43] is a collection of radar reflectivity image data from 2009 to 2015 based on the radar product, namely, the Constant Altitude Plan Position Indicator (CAPPI) at an altitude of 2 kilometers with a radius of 256 kilometers centered at Hong Kong. No prior data cleansing was applied to the HKO-7 dataset so it may consist of noises commonly found in radar imagery due to sea or ground clutters and anomalous propagation, and blind sectors due to blockage of microwave signals. Moreover, the sub-tropical climate of Hong Kong, mesoscale weather development which is caused by the land-sea contrast and complex terrain over the territory and the adjacent coastal areas lead to changeable weather and limited predictability of severe convective precipitation beyond the next couple of hours. Overall, the HKO-7 dataset is known to be much more difficult to model precisely, which could better highlight the effectiveness of our methods. We predict the next 2-hour radar echoes (20 frames) from that of the past 30 minutes (5 frames). The data from 2009 to 2014 are used as the training set and those in 2015 are used as the test set."}, {"title": "Triviality of L2 Loss in the Fourier Space", "content": "In this section, we prove that the L2 distance between ground truth and prediction in both the Fourier domain and image domain are equivalent from both the forward and gradient aspects."}, {"title": "Showing that L2(F, F) = L2(X, X)", "content": "Parseval's Theorem (or the general one: Plancherel Theorem) describes the unitarity of the Fourier transform under proper normalization. Without normalization, we have the following relationship:\nThis refers to the 1D case where F is the Fourier transformed output of X, and N is the vector length of both F and X. In the 2D case with F orthonormalized in Eq. (2), we have instead\nwhere \\(\\sum\\) is used as a shorthand summing every element in the following 2D matrix. When we apply the L2 loss to two orthonormalized Fourier matrices, we obtain\ndue to the linearity of the Fourier transform and the use of Parseval's Theorem."}, {"title": "Showing that \u2207 L2(F,F) = \u2207 L2(X,X)", "content": "From Eq. (2), we continue and derive the gradient of the Fourier transform output F with respect to image X:\nFor every complex vector, the multiplication of itself and its conjugate is equal to the square of its amplitude, that is FF* = |F|2. Thus,\nWith the inverse Fourier transform defined in Eq. (2) and the assumption that X is always real, we can obtain:\nConsequently, the gradients of L2(F, F) and L2(X, Y) with respect to Xkl are equivalent. This result indicates that implementing L2 in the Fourier domain without any weighting as a loss function does not affect the model performance."}, {"title": "FAL in the Gradient Aspect", "content": "In Section 3.2, we claim that FAL works as a regularizer to maintain the frequency amplitude in the Fourier domain, but not the full loss function. In this section, we study the reason behind this statement from the perspective of gradient feedback. Before that, we start with deriving the derivative of |F| with respect to Xkl:\nwhere \\(F_{pq} = |F_{pq}|e^{i\\phi_{pq}}\\) and \\(\\alpha_{pq,kl} = 2\\pi(\\frac{kp}{M} + \\frac{lq}{N})\\).\nFrom Eq. (4), we further derive its derivative with respect to Xkl and get:\nwhere |Fpq| and |\\hat{F}_{pq}| are the Fourier amplitudes of the ground truth and prediction corresponding to the frequency (p, q) while \\(\\hat{\\phi}_{pq}\\) is the Fourier phase of prediction corresponding to the frequency (p,q).\nFrom Eq. (10), we note that \\(\\hat{\\phi}_{pq}\\), which corresponds to the position or the image structure of the ground truth frequencies (object) in the image space, is missing. In other words, the model never gets its parameters updated based on the phase of the ground truth. As a result, FAL only encourages the model to predict what has the same amplitude distribution in the Fourier domain, without considering the image structure. This theoretically shows the infeasibility of reconstructing the ground truth based on FAL alone, motivating us to adopt a second loss term to maintain the image structure.\nTo effectively make use of FAL as a regularizer, we have to ensure that the model has sufficient time to learn the general image structure (the low-frequency pattern) such that FAL could provide better guidance on the remaining frequency components by exposing it more to FCL in the beginning of training process. In contrast, if the model cannot learn the low-frequency components before FAL becomes the dominant learning objective, the model will likely converge to a trivial solution. This claim is also empirically verified by our ablation study experiments where using FAL alone results in poor performance as shown in Appendix. F."}, {"title": "Further Analysis of FAL", "content": "This section discerns FAL and a naive L2 loss in the Fourier space. By definition, the major difference between the two is whether the complex pattern or the amplitude is used in the comparison. The FAL loss term can be expanded as follows:\nApart from the L2 component which is equivalent to Eq. (9), we also obtain two extra terms, shorthanded as \\(\\sum 2XX \\) and \\(\\sum2|F||\\hat{F}|\\). To study the empirical effect of the two factors on the high-frequency components, we performed a simple experiment: we sampled an image from the Moving-MNIST dataset and performed two modifications over time: (1) applying Gaussian blur with a standard deviation of \\(\\sigma\\) to the sample, and (2) translating the sample along the direction (t, t). Then we observed the trend of increment of the two factors, as shown in Figure 4.\nThe figure reflects a couple of characteristics of the FAL loss term. First, in the case of blurring, it behaves similarly to the standard L2 loss with a tiny difference when \\(\\sigma\\) gets very large. Intriguingly, FAL does not exhibit a different degree of sensitivity to different frequencies. However, for translation, the absolute difference \\(\\sum 2XX - \\sum 2|F||\\hat{F}|\\) is almost equivalent to and thus cancels out the L2 loss, causing the final FAL loss term to become very small. It is also noteworthy that when the two samples X and \\(\\hat{X}\\) are identical, both \\(\\sum 2XX\\) and \\(\\sum 2|F||\\hat{F}|\\) are zero and thus FAL is also zero. From the observations above, FAL is invariant to global translations and robust to one-directional local translation compared to L2. Because of such invariance, FAL is more robust against the spectral bias and could better fine-tune the frequencies of the output. Utilizing this behavior, the model could focus on the reconstruction of a clear signal without suffering from the influence of the randomness in translations. Moreover, it also shows that an arbitrary scaling between L2 and the factor \\(\\sum 2XX - \\sum 2|F||\\hat{F}|\\) could not result in a desired effect, since this causes the two terms to no longer overlap in the plot over t, leading to an increase in sensitivity to translation."}, {"title": "Further Analysis of FCL", "content": "To understand how FCL affects the model during training, we derive its derivative with respect to \\(\\hat{X}_{kl}\\) and obtain an interesting result with the aid of Plancherel's theorem:\nFrom Eq. (11), the ratio \\(\\hat{X}_{kl}\\) to \\(X_{kl}\\) highly depends on the summation over the image domain, providing global information to \\(\\hat{X}_{kl}\\), unlike the element-wise or pixel-wise relationship between X and \\(\\hat{X}\\) in the conventional MSE loss, \\(L_2(X, \\hat{X})\\).\nTo have an intuitive understanding of the conclusion above, we design a thought experiment to understand how FCL is different from \\(L_2(X, \\hat{X})\\) here. Consider the case where the prediction has the same image structure as the ground truth but with different intensity, for instance, \\(\\hat{X} = \\beta X\\), where \\(\\beta\\) is an arbitrary non-zero constant.\nSubstituting \\(\\hat{X}\\) into Eq. (11), we have\nMeanwhile, it is straightforward that:\nWith the above discrepancy, the behavior of FCL is substantially different from MSE in regard to overall brightness. That is, MSE is affected by both the image structure and the overall brightness but FCL is affected by the image structure only. Therefore, with FCL alone, we lose the pixel intensity. While applying the sigmoid function is one method to alleviate the drawback of the missing information, incorporating FAL which focuses on the intensity in particular could be viewed as a parallel complement to further stabilize the models."}, {"title": "Ablation Study on FAL and FCL", "content": "In previous sections, we showed that the Fourier phase of the ground truth, \\(\\theta\\), is missing in the gradient of FAL. Hence, we claim that FAL alone is insufficient to be a reconstruction loss. Similarly, in the thought experiment conducted in Appendix E, we conclude that FCL does not consider the image intensity and sharpness. As a result, the two loss terms FAL and FCL have to be used together as a full reconstruction loss. Here, we report the empirical results of the ablation study for FAL and FCL in Table 3.\nIn Table 3, the model trained with FAL does not produce meaningful output, as reflected by the abnormal values of the metrics and the faulty predictions in Figure 5. This agrees with our statement that models trained with FAL alone cannot converge to proper local minima. Meanwhile, the model trained with FCL only exhibits behaviors similar to MSE as shown in Figure 5. To sum up, either using FAL or FCL alone does not empirically produce the desired effect. However, combining the two loss terms together achieves a huge improvement to most of the metrics, which agrees with our theoretical analysis.\nNext, we study the effect of \\(\\alpha\\), which controls the length of the fine-tuning process with FAL. The fine-tuning process encourages the models to predict sharper and brighter predictions. At the same"}, {"title": "Running Time of FACL", "content": "In the previous sections, we showed that the method is both effective and generic of models. In this section, we discuss the running time of FACL. Theoretically, FACL utilizes DFT, which has time complexity O(n\u00b2) in the 1D case with vector length n. By leveraging the 2D Fast Fourier Transform (FFT), we could improve the time complexity to O(MN(log(M) + log(N))) for each pair of frames, where M and N correspond to the height and width of the samples. With the aid of deep learning frameworks such as PyTorch, such operations can be run in parallel and supported by GPU. Therefore, the computational load for FACL is light compared to the deep network architectures. During inference, the only difference between models trained with MSE and FACL is that the FACL one consists of a sigmoid layer at the end. Running in parallel, again, this operation is negligible.\nTo test the actual speed of FACL, we report the running time during model training and model inference for the experimented models in Table 6. For the training stage, we report the mean of the training time for the first 5 epochs. The table shows that the running time of FACL is negligible compared to the MSE counterpart, with the model selected being the most dominant factor for the running time. With the inference time reported, we could also notice the advantage of staying with video prediction models over generative models. The diffusion models are much slower than traditional predictive models. Our slowest model (FACL on PredRNN) is still 50X faster than MCVD. Note that such a difference scales with the image size, causing some of the generative models infeasible to apply to large-size radar imagery."}, {"title": "Evaluation on N-Body-MNIST", "content": "Apart from the proposed Stochastic Moving-MNIST, previous works proposed N-Body-MNIST [10"}]}