{"title": "Zero-shot Cross-lingual Transfer Learning with Multiple Source and Target Languages for Information Extraction: Language Selection and Adversarial Training", "authors": ["Nghia Trung Ngo", "Thien Huu Nguyen"], "abstract": "The majority of previous researches addressing multi-lingual IE are limited to zero-shot cross-lingual single-transfer (one-to-one) setting, with high-resource languages predominantly as source training data. As a result, these works provide little understanding and benefit for the realistic goal of developing a multi-lingual IE system that can generalize to as many languages as possible. Our study aims to fill this gap by providing a detailed analysis on Cross-Lingual Multi-Transferability (many-to-many transfer learning), for the recent IE corpora that cover a diverse set of languages. Specifically, we first determine the correlation between single-transfer performance and a wide range of linguistic-based distances. From the obtained insights, a combined language distance metric can be developed that is not only highly correlated but also robust across different tasks and model scales. Next, we investigate the more general zero-shot multi-lingual transfer settings where multiple languages are involved in the training and evaluation processes. Language clustering based on the newly defined distance can provide directions for achieving the optimal cost-performance trade-off in data (languages) selection problem. Finally, a relational-transfer setting is proposed to further incorporate multi-lingual unlabeled data based on adversarial training using the relation induced from the above linguistic distance. Experimental results on two practical multi-lingual IE tasks demonstrate our method significantly outperforms baselines across tasks and languages simultaneously. Additionally, by carefully designing the multi-lingual training to utilize data from relevant languages, we can achieve a substantial boost in generalization ability with reasonable labor cost for the additional data collection.", "sections": [{"title": "Introduction", "content": "The objective of Information extraction (IE) is to identify and extract structure information, such as entities, relations, and events, from natural unstructured text. IE plays an important role in various downstream applications, including Question Answering, Knowledge Graph Construction, News Analysis, etc. Solving IE tasks pose significant challenges for NLP models as they often require the understanding of complex features of natural languages. For example, to extract relations within a sentence, models first need to learn specialized structures of the corresponding language to identify entities mentioned in the given text. Next, a deep understanding of context is required to correctly classify the relations between these entities. These challenges are further exacerbated in multilingual settings, where datasets are collected from multiple languages, each of which contains language-specific characteristics and structures.\nThe rapid development around English-based datasets has pushed machine performance to be on par with human ability in English tasks, prompting recent works to explore NLP research in other languages (Liang et al., 2020; Ruder et al., 2021). However, despite advanced large-scale architectures and high English results, current models notably under-perform in new languages, especially those that are considered low-resource and lack high-quality datasets for fine-tuning. Cross-lingual Transfer, as a result, becomes one of the most important directions in the field. Given a particular task, the goal of Cross-lingual Transfer is to train multilingual models over high-resource source languages that can solve textual tasks in new target languages despite the shifts in linguistic origin.\nCurrently, the most popular and practical approach for IE involves Zero-Shot Cross-Lingual (ZSCL) transfer (Conneau et al., 2020; Goyal et al., 2021). These methods fine-tune Transformer-based multilingual Language Models (mLMs), which were pre-trained using unlabeled text from hundreds of languages, for downstream tasks using high-resource source-language labeled datasets"}, {"title": "Related Work", "content": "Zero-shot Cross-lingual Transfer The majority of recent ZSCL works (Fang et al., 2021; Chi et al., 2021) follow single-transfer setting, using comprehensive multi-lingual multi-task benchmarks such as XTREME (Ruder et al., 2021), or XGLUE (Liang et al., 2020). These datasets provide English training data for fine-tuning the pre-trained MMLMs, which are then evaluated on translated test sets in different languages. As a result, English becomes the dominant source language for transfer in following ZSCL researches (Phang et al., 2020), which is suboptimal due to the linguistic diversity of languages. Specifically, (Keung et al., 2020) discovers that model's Engish dev accuracy does not correlate with its performance of other languages, and (Lauscher et al., 2020) demonstrates the limitation of using English to transfer to low-resource languages. Furthermore, (Turc et al., 2021) find that other high-resource languages such as German and Russian often transfer more effectively when the set of target languages is diverse or unknown a priori. Our work builds on these findings and provides a more complete view of language relations and ZSCL performances of mLMs.\nLinguistic Diversity By probing the learned representation of mLMs, (Pires et al., 2019; Limisiewicz et al., 2020) have found syntactical information implicitly encoded in layers of the multi-lingual models (typically at middle-level layers of the architectures). (Xu et al., 2022a) shows that the pre-training and fine-tuning processes transform these features and directly impact model multi-lingual performances. Further investigation by (Lin et al., 2019) demonstrates that distances"}, {"title": "Datasets", "content": "Information extraction tasks extract structured contextual information from unstructured text, thus requiring model's comprehension of both syntactic and semantic knowledge in multilingual documents. In this paper, to demonstrate to the heterogeneity of ZSCL for multi-lingual IE problems in practice, we experiment on two recent datasets that provide training and evaluation data in a wide range of languages.\nMINION: Multi-lingual Event Detection (MINION) (Pouran Ben Veyseh et al., 2022) annotates event triggers for 8 typologically different languages. The goal event detection task is to identify the word(s) that describe the occurrence of an event the best from a given text, also referred to as he"}, {"title": "Linguistic Relations", "content": "To illustrate a comprehensive picture of the linguistic relations among the available languages, we consider different base linguistic features and how to compare them, into a total of 14 distance metrics\nFollowing the standard approach, We use five different linguistic features provided by the URIEL Typological Database (Littell et al., 2017), including a phylogeny feature, a geography feature, and three typological features (syntax, phonology, and inventory):\nPhylogeny (fam): the membership in language families derived from the world language family tree in Glottolog (Hammarstr\u00f6m et al., 2022)\nGeography (geo): the language location based on Glottolog, more specifically the orthodromic distance between the language and a fixed point on the surface of the earth.\nSyntax: the language syntactic structures derive from either WALS (Dryer and Haspelmath, 2013) or Ethnologue (Lewis, 2009)\nPhonology: the phonology features extracted in a similar manner from WALS and Ethnologue\nInventory: the phonetic features derived from PHOIBLE's phonetic inventories (Moran et al., 2014)\nEach of the above linguistic features is represented by a multi-dimensional binary vector for every language, where a value 0 (1) in each dimension represents the absence (presence) of a particular linguistic phenomenon for that language."}, {"title": "Distance Metrics", "content": "To calculate the linguistic distance between languages based on the above feature vectors, previous works only consider cosine or Euclidean distance between the binary vectors. However, numerous binary similarity measures have been proposed and play a critical role in many problems in various fields. These binary metrics are distinguished by their unique synthetic properties (negative matches, count differences, correlation, etc.), and applying an appropriate one is the key to more accurate data analysis results. Based on the categorization in (Choi et al., 2009) which survey over 76 binary similarity measures, we decide to focus on the following 4 representative distances: Hamming, Jaccard, Inner-product, and Anderberg."}, {"title": "Zero-shot Cross-lingual Single-transfer", "content": "To answer the first research question, we evaluate ZSCL-S scores for every language pair of each task, in three model scales: small (MiniLM (Wang et al., 2020)), base (XLM-Roberta-base (Conneau et al., 2020)), and large (XLM-Roberta-large). Next, Pearson correlations are computed between the transfer performances and linguistic distances to identify the degree to which the relations in the linguistic landscape determine a model's cross-lingual transfer ability.\nExperimental Setup In ZSCL-S, given a pair of source and target languages, the model is trained using labeled data from source language. The ZSCL-S score is then defined as the zero-shot evaluation of the trained model on the test set of target language.\nTransfer Performance Detailed transfer scores are provided in figures 6 and 7 in appendix A. While the language-wise order of the transfer scores is maintained across different model sizes, it is not clear, however, if language identities alone are able to determine model cross-lingual transfer ability. This is due to the significant difference between the results of the two tasks. Even more unexpectedly, model transfer scores do not increase linearly with its number of parameters"}, {"title": "Linguistic Correlation", "content": "We determined if any of the linguistic distances defined in section 4 can explain the heterogeneity of resulting transfer performances, across all settings.\nDistance-Transfer Correlation We compute the Pearson correlation between the transfer score and distance vector between each language pair. The detailed results are presented figures 8 and 9 in appendix A. While there are several distances that achieve a correlation score of over 0.7, effectively predicting the corresponding transfer performances, none of the linguistic relations are highly correlated with the transfer scores for both tasks. In particular, syntax and inventory features have above-average correlation scores for SMiLER, whereas only phonology-based distances are effective for the event detection task.\nCombined Metric In order to achieve our objective of creating a universal metric that can be applied across different practical settings, we define a combined metric as a weighted average of all relevant linguistic features. For each task, the optimal weights are the solution of a simple constrained correlation linear maximization (the weights are constrained to be non-negative and sum to 1). Figure 2 compares the resulting weight importances between the two tasks across model scales. Similar to the above assessment, there is a divergence between MED and SMilER on how the linguistic features are weighted in the optimal combined metric.\nFrom these observations, we propose a joint combined metric that involves all three of the typological features as follows: $d_{comb} = 0.4* d_{ander-syntax} + 0.2*d_{inner-phonology}+0.4*d_{ander-inventory}$. To demonstrate the adaptability of the new distance, we provide the mean correlation scores (across all languages) of all computed linguistic distances in figure 3. Not only $d_{comb}$ achieves the highest correlation with transfer performances overall (above 0.6 for every setting), the combined metric also greatly lessens the score's variability amid tasks and scales of models. This implies that $d_{comb}$ has the potential to be a general metric to approximate ZSCL performances prior to model training. The following sections will use this combined distance for guiding the language selection and adversarial training in multi-transfer setting."}, {"title": "Zero-shot Cross-lingual Multi-transfer", "content": "We address question Q2 by evaluating multi-transfer performances between two sets of languages. In particular, we define a transfer configuration as an experimental setting that specifies languages inside the source and target sets, and a transfer run as an actual experimental evaluation of a transfer configuration. As the number of configurations is exponential in terms of the number of languages, it is computationally impossible to evaluate every configuration, even more so for different tasks and model scales. Therefore, we focus solely on the resource-constrained scenario, which is also equivalent to the setting with the minimum number of source languages. Based on the result from the previous section, we propose to limit the configuration scope using the general combined distance metric as follows."}, {"title": "Language Selection", "content": "In the resource-constrained setting, our goal is to select the minimal set of source languages $D_s$ that can maximally transfer to a given target language set $D_t$. We further restrict our attention to transfer configurations with $D_t$ as set of closely related languages in terms of cross-lingual transfer. Assuming pair-wise transfer is highly correlated with"}, {"title": "Transfer Performance", "content": "Detailed results of multi-transfer performances are provided in figures 10 and 11 in appendix A, from which we can observe a clear improvement over single-transfer setting owning the additional training data. Our main interest here is how effective $d_{comb}$ is in guiding the language selection for ZSCL-M. Table. 2 shows the differences in transfer scores of Inter-cluster (medoids*) and Intra-cluster ([medoid_lang]*) configurations over Random configuration. Specifically, medoids* measures inter-cluster transfer capacity of the set $D_s$ consisting of every medoid from each cluster, to the target set $D_t$ of all considered languages. On the other hand, [medoid_lang]* measures intra-cluster performance of a randomly sampled set $D_s$"}, {"title": "Zero-shot Cross-lingual Relational-transfer", "content": "Due to limited access to multi-lingual annotators, gathering labeled data across languages is difficult. Previous section address this by careful language/data selection to optimize cost-effect. In contrast, unlabeled data is easy to collect, but leveraging it correctly for ZSCL is non-trivial. This section investigates the effectiveness of adversarial training approach for the more general ZSCL-M setting, and the possibility of further improving multi-lingual transfer through explicitly integrating our transfer-correlated linguistic relations."}, {"title": "Experimental Setup", "content": "We follow the same setup as in ZSCL-M, but each language is accompanied by an unlabeled dataset which can be used for training. The model use labeled data from the source cluster to learn the task, whereas unlabeled data from another cluster is used to help transfer source performance to that target cluster. The goal is to bridge the performance between 2 different language clusters with the aid of given unlabeled text."}, {"title": "Adversarial Language Adaptation", "content": "A typical method use for ZSCL-S is adversarial language adaptation (ALA) (Chen et al., 2018; Nguyen et al., 2021) which employs a language discriminator that takes an encoded representation from mLMs as its input and predicts its origin (language). By pushing the encoder to both minimize the downstream loss and maximally misdirect the language predictor (adversarial training), the resulting representation can be indiscriminate with respect to the shift between the languages while also discriminative for the main learning task. Apply ALA for ZSCL-M setting is equivalent to applying DANN for a single joint source domain to a single joint target domain (the union of every languages in $D_s$ and $D_t$, respectively)."}, {"title": "Zero-shot Cross-lingual Relational-transfer", "content": "We extend ALA to the case of multiple source and target languages through Graph-relational domain adaptation (GrDA) (Xu et al., 2022b), a generalization of DANN to multi-domains adaptation setting by introducing a domain graph that captures heterogeneous relations among domains. GrDA relaxes the strict uniform alignment of DANN to allow flexible and effective adaptation between distant domains. We use the language clustering graphs on the right of figure 4 as domain graphs for GrDA. Noted that additional direct connections between medoid languages are introduced (red edges) to facilitate inter-cluster transfer. We refer to this adversarial learning process for ZSCL that directly embeds the linguistic relations into the representations as Zero-shot Cross-lingual Relational-transfer (ZSCL-R)."}, {"title": "Transfer Performance", "content": "Performances of the baseline DANN and the proposed adversarial training method ZSCL-R are provided in table. 3, which follows the same format as table. 2. However, instead of comparing against Random configurations, they are compared directly with results of ZSCL-M runs of the corresponding configurations, but only in terms of inter-cluster transfer. The negative results of DANN confirm that strictly aligning language representations uniformly is not effective in ZSCL-M setting. As the model scale gets smaller, model's representation becomes less expressive, whereas the language-invariant feature of all languages is harder to capture as the number of languages grows. Thus, the adverse effect gets significantly worse on small models for SMiLER task (-12 points on average). In contrast, ZSCL-R provides consistent improvements over ZSCL-M for most configurations. Due to the flexibility of GrDA alignment, ZSCL-R performs even better as the number of languages increases, effectively leveraging the additional unlabeled data to help improve inter-cluster transfer ability of models."}, {"title": "Conclusion", "content": "We explored the general cross-lingual transfer learning setting where multiple source and target languages are involved. Our experiments on two practical information extraction tasks across different model scales and languages reveal new general insights on cross-lingual transfer learning: (1) There is a correlation between linguistic distances and single-transfer performances; however, simplistic measures based on syntax features are only sufficient for syntactic-based tasks. We develop a combined distance based on various metrics and linguistic features that achieves a high correlation with cross-lingual transfer score robustly across all settings. (2) The proposed combined metric provides useful directions for language clustering and selection to achieve optimal cost-performance trade-off in multi-transfer to a specific group of languages. (3) Finally, linguistic relations can be leveraged with unlabeled data for adversarial training to help generalize to a new group of languages with minimum additional annotation cost. Our findings collectively suggest multi-transfer as a new baseline for cross-lingual learning, and provide a baseline for efficient and effective multi-transfer together with promising directions that future work can further improve upon."}, {"title": "Limitations", "content": "Compared to prior cross-lingual transfer papers (Srinivasan et al., 2022), our work aims to demonstrate generalization across various hyperparameters and design choices that affect the results of previous investigations on the topic. Consequently, this has led to significant computational demands, forcing us to limit and simplify some aspects of our"}]}