{"title": "Fast Vision Mamba: Pooling Spatial Dimensions for Accelerated Processing", "authors": ["Saarthak Kapse", "Robin Betz", "Srinivasan Sivanandan"], "abstract": "State Space Models (SSMs) with selective scan (Mamba) have been adapted into efficient vision models. Mamba, unlike Vision Transformers, achieves linear complexity for token interactions through a recurrent hidden state process. This sequential processing is enhanced by a parallel scan algorithm, which reduces the computational time of recurrent steps from L sequential steps to log(L) parallel steps with respect to the number of input tokens (L). In this work, we propose Fast Vision Mamba (FastVim), that further reduces the computational time of the SSM block by reducing the number of recurrent steps in Vision Mamba models while still retaining model performance. By alternately pooling tokens along image dimensions across Mamba blocks, we obtain a 2\u00d7 reduction in the number of parallel steps in SSM block. Our model offers up to 72.5% speedup in inference speed compared to baseline Vision Mamba models on high resolution (2048x2048) images. Our experiments demonstrate state-of-the-art performance with dramatically improved throughput in a range of tasks such as image classification, cell perturbation prediction, segmentation, and object detection.", "sections": [{"title": "1. Introduction", "content": "Recent developments in neural network architectures for computer vision tasks have used State Space Models [20] (SSM) with selective scan (Mamba [19]) to enhance computational efficiency by replacing the quadratic complexity of self-attention in transformers [60] with Mamba's linear complexity while retaining context dependence unlike recent SSMs [20]. Models like Vision Mamba [70] (Vim) and VMamba [39] have shown that they can outperform their transformer-based counterparts, like Vision Transformer [17] (ViT) and Swin [40], in vision tasks, while being particularly advantageous for tasks involving high-resolution images due to its efficient scaling. While Mamba supports content-based reasoning through selective scan in State Space Models (SSM), it cannot utilize efficient convolutions, necessitating a sequential recurrent approach that limits parallel processing. To address this, Mamba incorporates a parallel scan algorithm [5], reducing sequential steps to a lower bound of logarithmic scale [57] with respect to the number of tokens. While this approach significantly reduces sequential steps, in the vision domain, the number of tokens scale quadratically with increasing resolution. Consequently, this results in a quadratic increase in the number of sequential recurrent steps translating to 2x increase in the number of parallel steps when using parallel scan, which challenges throughput in high-resolution imaging.\nIn this work, we explore the possibility of scaling the number of recurrent computations in Vision Mamba linearly with image resolution, as opposed to scaling quadratically. We do this by applying average pooling across one dimension of the 2D token grid before the recurrent SSM block. This raises the question: Can we reduce the number of recurrent computations in Vision Mamba without compromising model performance?\nThe answer is yes. In this paper, we utilize a simple, parameter-free technique of average pooling to reduce the number of recurrent steps in Vision Mamba while maintaining strong predictive power. An important consideration is alternating the pooling dimensions (as shown in Fig. 1) across stacked Mamba blocks. Pooling tokens across column (Poolcol) prevents interaction of tokens in a row and similarly across row pooling (Poolrow) prevents interaction of tokens in a column; thus, alternation ensures all tokens interact implicitly across multiple blocks. We empirically demonstrate that this alternation is a necessity for achieving high performance in visual encoding, not just a desirable feature.\nOur method, FastVim, is a purely Mamba-based neural network architecture (built on Vim) that uses pooling to accelerate contextualization in SSM scans. As shown in Fig. 2, in each forward and backward scan branch, mean pooling is applied after a 1D convolution layer to compress tokens across rows or columns, resulting in a one-dimensional token grid. These compressed tokens are projected to input-dependent parameters via a linear layer for selective scan, followed by interaction in the SSM module. The output is then repeated to restore the original token grid before the skip connection, followed by norm layer. Thus, across blocks, the number of tokens remains unchanged: they are compressed with pooling before the SSM scan and decompressed with repetition afterward, as shown in Fig 2. To alternate pooling dimensions and align 1D convolution direction with the SSM scanpath direction, we transpose the token grids every block. We further investigated whether our pooling approach is effective in ViT, and experimental results demonstrate that while it works in Vim, it fails in ViT, highlighting the need to further study Mamba's contextualizing capabilities versus Transformers.\nTo extend our approach across other domains, we propose the following two adaptations of FastVim: FastMaskVim - incorporating masking in FastVim for applications like Mask Autoencoders [24] (MAE), DINOv2 [46], and pathology datasets [9] having non-regular grids, and FastChannelVim - utilizing per-channel tokenization as introduced by ChannelViT [3], beneficial for datasets like microscopy cell [8] and satellite imaging. Remarkably, we trained the most effective pure Mamba-based monolithic visual encoder using our pooling method with MAE on ImageNet-1K [16] to date, achieving state-of-the-art performance (SOTA). In per-channel tokenization task, FastChannelVim showed phenomenal gains in accuracy over ChannelViT baselines on microscopy image classification task, demonstrating the benefits of our method for long token sequences in vision. In summary, our main contributions are:\n\u2022 FastVim, a Vim-based architecture that utilizes average pooling, achieving 1\u00d7 parallel steps in logarithmic scale from 2x, translating to 72.5% speedup in overall framework.\n\u2022 FastVim is adapted into FastMaskVim and FastChannelVim, extending its utility to applications with irregular grids and multi-channel imaging, respectively.\n\u2022 Our methods set a new SOTA of 86.7% on ImageNet-1k [16] (with MAE pretraining) with a Mamba-based encoder and show substantial improvements over transformer baselines in long token sequence modeling in per-channel tokenization on microscopy imaging by 8.3%."}, {"title": "2. Preliminaries", "content": "State Space Models (SSMs) are mathematical frameworks that model continuous-time sequences by transforming an input sequence x(t) \u2208 R into an output sequence y(t) \u2208 R via a hidden state h(t) \u2208 RN, as governed by:\nh'(t) = Ah(t) + Bx(t),\ny(t) = Ch(t) + Dxt,\n                                                       (1)\nwhere A \u2208 RNXN describes how the current state evolves, B \u2208 RN\u00d71 describes how the input influences the state, C \u2208 R1XN describes how the current state translates to the output, D \u2208 R describes how the input directly influences the output, acting as a skip connection, and N being number of states.\nTo be applied on discrete sequence datasets, SSMs are discretized using zero-order hold over a sampling interval \u0394, resulting in discrete parameters A and B:\n A = e^(\u0394A) , B = (\u0394A)^(-1) (e^(\u0394A) \u2013 I) \u0394B  (2)\nThe discrete-time SSM equations are then modified as:\n ht = A h_(t-1) + B x_t,  Y_t = C h_t + D x_t (3)\nThe above equation can be computed like a recurrent neural network, viewing ht as a hidden state with transition matrix A. However, since it is not practical for training on modern hardware due to sequential processing, the well-known connections between linear time-invariant (LTI) SSMs (eq. 3) and continuous convolutions can be used. Specifically, the eq. 3 can be computed as y = K * x, where K is the SSM convolution kernel. However, computing K is computationally prohibitive due to repeated matrix multiplication of A (for more depth, please see Linear State Space Layer (LSSL) [21]). To address this, the Structured State Space sequence model (S4) [20] proposed a novel parameterization of A, making it significantly faster and less memory consuming, while exceeding the LSSL's performance empirically.\nSelective State Space Models termed as Mamba [19] enhance S4 further by allowing input-dependent parameters, enabling the model to capture richer contextual information. Mamba modifies the parameters B, C, and \u0394 as functions of the token sequence x \u2208 RB\u00d7L\u00d7D, where B is the batch size, L is the sequence length, and D is the token embedding dimension, as follows:"}, {"title": "3. Method", "content": "In this section, we present the details of our Fast Vision Mamba (FastVim), providing an overview in Fig. 2. A detailed description of our proposed pooling method, designed to accelerate contextualization in Vision Mamba, is provided in Sec. 3.1. In Sec. 3.2, we explore extensions to masking paradigms, whereas in Sec. 3.3 we illustrate the extension to the domain of per-channel tokenization modeling [3].", "subsections": [{"title": "3.1. Spatial Pooling for faster contextualization", "content": "In this paper, we propose a novel method to reduce the number of recurrent steps in Vim through spatial pooling (FastVim). Specifically, as detailed in Algorithm 1, we propose mean pooling across one spatial dimension of a 2-D image's token grid (x). Suppose we have a square grid where h = w and L = h \u00d7 w = h\u00b2 (where h = H/P and w = W/P are spatial dimensions of token sequence x before flattening), this pooling reduces the sequence length to h from h\u00b2, resulting in a 1\u00d7 parallel steps (when using parallel scan) in FastVim (log(h)) compared to 2\u00d7 parallel steps in Vim (log(h\u00b2)). Note that we use a square grid for a simpler example, but FastVim is generalizable to any image dimensions. This approach fits within the sparse contextualization paradigm because, instead of all tokens interacting, only pooled tokens interact with each other across one spatial dimension. Following the scan operation, the output is repeated to get back the sequence of size h\u00b2. Average pooling is used as a default; variants with max and attention pooling are in the Supplement.\nIntuitively, pooling might lead to inadequate contextualization of tokens in a row when pooling tokens across columns (Poolcol), and similarly for tokens in a column when pooling tokens across rows (Poolrow). We address this issue by alternating the pooling operation across rows and columns across layers in FastVim. This enhances effective interactions among pooled tokens in different rows"}, {"title": "3.2. FastMaskVim: Incorporating Masking", "content": "So far, we have described FastVim in the context of a regular token grid of size h \u00d7 w. However, this approach cannot be directly utilized when faced with an irregular grid, a situation often encountered in scenarios involving masked tokens such as in Masked Autoencoders [4, 24] (MAE) and DINOv2 [46], or in multiple instance learning [28, 30] (MIL) in pathology, where tissue samples can contain gaps. To enable FastVim to function effectively in such domains, we need to modify the pooling and transpose operations.\nSpecifically, instead of using a simple transpose operation on the token grid, we employ advanced indexing techniques to transpose a sparse token grid of shape h \u00d7 w, but only including the unmasked tokens. For pooling, we sum the tokens in each row and then divide by the number of columns, i.e., w, instead of naively performing mean pooling (see Fig. 12), as mean pooling could result in the loss of information regarding the number of tokens present in the row. These simple modifications have proven effective, as is demonstrated by the MAE-pretrained FastMaskVim in Sec. 4.3."}, {"title": "3.3. FastChannelVim: Per-Channel tokenization", "content": "In 2-D imaging datasets, a region of size P \u00d7 P \u00d7 C is typically projected into a single token of dimension D, where P is the patch size and C is the number of channels, thus forming a token sequence x \u2208 RL\u00d7D for L tokens. However, this tokenization approach has been shown to be inadequate for certain imaging modalities where per-channel information is highly complementary, such as in microscopy cell imaging and satellite imaging, unlike the RGB channels in natural images. As established by ChannelViT [3], per-channel tokenization can address this limitation, though at the cost of increasing the number of tokens by a factor of C, thus forming a token sequence x \u2208 R(L.C)\u00d7D. In this paradigm, channel embedding is added along with position embedding to preserve order information.\nBuilding on the benefits (performance and efficiency) of Mamba over Transformers in long sequence settings, we introduce an extension of Vim with per-channel tokenization, which we term ChannelVim. To implement this extension, we must address two key considerations due to the sequential nature of SSM scan in Mamba, in contrast to the set-like, permutation-invariant nature of self-attention in transformers. First, for the scan path, as illustrated in Fig. 14, we have two options: we can either traverse across all spatial tokens within a channel and then proceed to the next channel (spatial-first approach), or we can traverse across all channels at a given spatial position and then move to the next spatially adjacent position and repeat (channel-first approach). Second, it has been shown that hierarchical channel sampling (HCS), where some channels are randomly dropped during training, improves performance [3]. We incorporate such HCS in ChannelVim. However unlike the original implementation, the output of the HCS module needs to be sorted, as order of channels matters in sequential modeling. We provide thorough evaluation of the effect of both above mentioned considerations in the Supplement. 10.\nFinally, we adapt our FastVim to this domain, which we term FastChannelVim. In the main paper, we explore compressing tokens only across the spatial dimensions (see Fig. 13). Thus, for each scan operation, we input either hx C (see Fig. 15) or w \u00d7 C tokens, instead of the entire h \u00d7 w \u00d7 C tokens. In Supplement 10, we also explore compressing across the channel dimension."}]}, {"title": "4. Experiments and Results", "content": "...", "subsections": [{"title": "4.1. Image Classification", "content": "..."}, {"title": "4.2. Efficiency Analysis", "content": "..."}, {"title": "4.3. Self-Supervised Learning: MAE", "content": "..."}, {"title": "4.4. Cell imaging: JUMP-CP", "content": "..."}, {"title": "4.5. Semantic Segmentation", "content": "..."}, {"title": "4.6. Object Detection and Instance Segmentation", "content": "..."}]}, {"title": "4.7. Ablation Study", "content": "...", "subsections": [{"title": "Effect of Alternating Dimension Pooling on FastVim.", "content": "..."}, {"title": "Exploring pooling in ViT.", "content": "..."}]}, {"title": "5. Related work", "content": "Vision Mamba. VMamba [39] introduced visual state space blocks that combine Mamba with 2D convolution layers and a hierarchical design similar to the Swin transformer [40], employing tricks like reducing the SSM states and expansion ratio to optimize throughput. EfficientVMamba [47] enhances VMamba by using an atrous-based selective scanning strategy for efficient global feature extraction, integrating SSMs with convolution branches. GroupMamba [55] addresses scalability and stability with a Modulated Group Mamba layer featuring multi-directional scanning and enhanced cross-channel communication. MambaVision [22] reconfigures Mamba by incorporating convolutional layers and interleaved Mamba-Transformer blocks, achieving a new state-of-the-art in accuracy and throughput. Our average pooling in FastVim can be readily applied to these advancements.\nSparse contextualization methods. Inspired by efforts to enhance efficiency in ViTs, numerous studies [6, 25, 35, 50, 53, 54] have examined the reduction of tokens across layers through merging or pruning. Similarly, Famba-V [56] utilizes a token fusion technique to consolidate similar tokens in Vim, thereby reducing training and inference time. Vim-prune [66, 67] addresses the challenges of naive pruning in Mamba due to its sequential nature by introducing pruning-aware hidden state alignment to stabilize neighborhoods. Our FastVim method aligns with this sparse contextualization approach, offering simplicity and maintaining performance beyond the tiny model size, in contrast to Famba and Vim-prune. Additionally, our proposed extension, FastMaskVim, can be seamlessly integrated with Vim-prune or Famba for further speedup, albeit with potential performance trade-offs."}, {"title": "6. Conclusion and Future Work", "content": "We presented FastVim, which enhances Vim's efficiency by reducing its computational complexity and increasing practical throughput speed. Remarkably, FastVim achieves this without any performance degradation compared to the baseline Vim model across multiple tasks, even though it contextualizes significantly fewer tokens in the SSM scan at each layer. By using pooling, our method delivers up to 72.5% overall throughput speedup (while reducing the parallel steps in SSM scan by 2\u00d7), with the gap widening at higher resolutions (longer token sequences). Our FastMaskVim sets the new state-of-the-art performance of 86.7% on ImageNet-1k for Mamba-based encoders and ranks among the top 12 visual encoders (when only ImageNet-1k is used). Additionally, it achieves substantial improvements over Transformer baselines in microscopy imaging. Beyond extending to Mamba-2 [13], future work will also explore applying FastVim in gigapixel imaging, such as histopathology [18, 44, 63], as well as in video domain [1, 15, 33] aligning with FastChannelVim."}]}