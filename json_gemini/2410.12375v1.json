{"title": "PREFLEXOR: PREFERENCE-BASED RECURSIVE LANGUAGE MODELING FOR EXPLORATORY OPTIMIZATION OF REASONING\nAND AGENTIC THINKING *", "authors": ["Markus J. Buehler"], "abstract": "We introduce PRefLexOR (Preference-based Recursive Language Modeling for Exploratory Optimization of\nReasoning), a framework that combines preference optimization with concepts from Reinforcement Learning\n(RL) to enable models to self-teach through iterative reasoning improvements, to create synthetic intelligence\nwith enhanced scientific reasoning capabilities. Central to PRefLexOR is a recursive approach that engages\nthe model in multi-step reasoning, revisiting, and refining intermediate steps before producing a final output\nin both training and inference phases. The foundation of PRefLexOR lies in multi-stage training, where the\nmodel first learns to align its reasoning with scientifically accurate decision paths by optimizing the log odds\nbetween preferred and non-preferred responses through a novel in-situ dataset generation algorithm. For on-\nthe-fly training data generation, PRefLexOR builds a dynamic knowledge graph by generating questions from\nrandom text chunks and utilizing retrieval-augmentation to contextualize relevant details from across the entire\ncorpus, resulting in rigorous reasoning chains. In a second stage, preference optimization strategies further\nenhances model performance by using rejection sampling to fine-tune reasoning quality by continually producing\nin-situ training data while masking the reasoning steps to focus on discovery of novel mechanisms to achieve\ncorrect answers. This hybrid approach mirrors key aspects of RL, where the model is continuously guided by\nfeedback to improve decision-making and reasoning, and the adaptive process enables the model to self-teach\nas it continually improves through real-time feedback and recursive processing. Our method does not use\npre-generated datasets and instead trains the model to continuously adapt and improve in real time. Recursive\noptimization within special thinking tokenization introduces iterative feedback loops, where the model refines its\nreasoning, much like policy refinement in RL, achieving deeper coherence, consistency, and adaptability. By\nrecursively optimizing reasoning through feedback-driven learning, PRefLexOR achieves significant flexibility\nin its ability to handle complex tasks, learning and evolving its cognitive abilities autonomously. PRefLexOR's\nrecursive optimization mirrors how biological systems adapt and evolve. By using feedback loops to refine\nreasoning pathways during training and/or inference, it emulates nature's resilience and adaptability, enhancing\nits decision-making capabilities. Implemented in very small language models with only 3 billion parameters,\nwe showed that even tiny models can iteratively teach themselves to reason with greater depth and reflectivity,\nakin to an RL-based self-improving system capable of solving open-domain problems with superior reasoning\ndepth and logic. Our implementation is straightforward and can be incorporated into any existing pretrained\nLLM. We focus our examples on applications in biological materials science, and demonstrate the method in a\nvariety of case studies that range form in-domain to cross-domain applications. We explore several reasoning\nstrategies that include both thinking and reflection modalities to construct a multi-agent recursive self-improving\nmodel that can successively improve responses via repeated sampling during inference, offering flexibility and\nintegration into larger agentic systems.", "sections": [{"title": "1 Introduction", "content": "Generative artificial intelligence (AI) models, such as Large Language Models (LLMs) and many variants [1, 2, 3, 4, 5, 6]\nhave not only impacted the landscape of natural language processing (NLP) but also unlocked the potential for\nscientifically-focused models that may ultimately be able to reason, think, and generate insight across an unparalleled\nrange of disciplines. From general-purpose tasks to highly specialized domains like materials science and engineering [7,\n8, 9, 10, 11, 12, 13, 14, 15], a grand challenge remains to develop strategies that yield more sophisticated scientific\nreasoning engines capable of performing tasks previously thought to be far beyond the reach of machines.\nEarlier work has resulted in attempts towards that goal, such as LLMs that were being taught to reason, not simply by\nbrute force or through rote memorization, but by leveraging structured approaches that mimic human thought processes.\nChain-of-thought prompting [16], for instance, guides models to break complex problems into clear, manageable steps,\nmimicking the logical progression that human minds follow when faced with a challenging task. Similarly, few-shot\nlearning methods [17] give models the ability to handle new tasks with minimal examples, enabling them to generalize\nand adapt their reasoning capabilities to novel scenarios.\nYet, applying these powerful models in technical fields like biomateriomics [18, 19] presents unique challenges. The\nintricacies of biomaterials design\u2014where insights are drawn from multiscale, cross-disciplinary knowledge-require\nLLMs to go beyond surface-level understanding. In biomateriomics, researchers seek to explore and model biological\nsystems at different scales, identifying how nature's building blocks can inspire new materials [19, 20, 21, 22, 23, 7, 24].\nModels of synthetic intelligence that capture scientific processes used in the analysis of such systems should offer a\ncoherent and integrative strategy for solving cross-disciplinary problems, making them indispensable tools in fields like\nbiomaterials research, where the ability to think, reason, and innovate is crucial. We posit that such advances can be\nachieved by developing models that can achieve several key objectives, including the ability to ingest rich, diverse and\ndisparate information from varied sources by forming rigorous internal knowledge representations that can be used to\npredict actionable outcomes (Figure 1a). To reach this goal, models need to be developed that go beyond conventional\npredictions without situational awareness (Figure 1b) towards more sophisticated models that encompass a higher\ndegree of situational awareness, realized through capabilities of self-reflection, error correction, and exploration of a\nwide space to predict novel solutions (Figure 1c)."}, {"title": "1.1 Modeling reasoning, thinking and more", "content": "As AI systems advance, the need for models capable of reasoning with greater depth, consistency, and adaptability has\nbecome increasingly critical. Traditional large language models (LLMs) have shown a certain level of proficiency in\ngenerating text, answering questions, and handling a wide range of natural language tasks. However, their reasoning\ncapabilities-especially when it comes to reflecting on complex tasks or iterating over ideas to refine thought processes-\nremain limited, and are often only achieved in very large models.\nIn many current AI systems (especially in scientific applications), reasoning often follows a single-pass approach,\nwhere the model generates outputs without reflecting on the steps that led to its conclusions. This leads to challenges in\nsolving open-domain or multi-step problems where deep cognitive engagement is required. Furthermore, the lack of\nflexibility in reasoning, adaptation to new challenges, and real-time learning means that these models struggle to handle\ntasks that require evolving, recursive reasoning strategies.\nTo address these challenges, we propose PRefLexOR (Preference-based Recursive Optimization and Refinement), a\nframework that combines preference optimization with recursive reasoning inspired by Reinforcement Learning (RL)\nprinciples (Figure 1c). PRefLexOR enables models to self-teach by iterating over thought processes, refining reasoning,\nand continuously learning from both preferred and rejected outputs. This approach represents a shift towards a more\nreflective and flexible learning paradigm, where the model improves its decision-making in real time.\nIn PRefLexOR, the dynamic data generation process allows us to build a complex graph of interactions that facilitates\nrecursive reasoning and refinement. For instance, when using a corpus of data sourced from scientific papers [13, 14, 15],\nthe process begins by generating a question from a randomly selected piece of text, which acts as the initial node in the\ngraph. To answer the question, we employ Retrieval-Augmented Generation (RAG), which queries the entire corpus,\nretrieving and integrating contextually relevant information from multiple sources.\nThis interaction between the question and the retrieved data forms a graph of knowledge, where nodes represent pieces\nof text, and edges represent the relationships between them. The embedding model plays a key role in this process by\nensuring that similar pieces of information are mapped to adjacent nodes within the graph, facilitating efficient retrieval\nand reasoning. As the model continues to refine its reasoning across recursive cycles, this graph evolves, reflecting the\ncomplex interconnections between various pieces of knowledge and how they contribute to the model's final output."}, {"title": "1.2 Motivation and Challenges", "content": "Traditional methods of training LLMs rely heavily on supervised fine-tuning, where models are trained on static datasets\nwith fixed inputs and outputs. While this allows for the learning of broad patterns, it lacks the ability to dynamically\nadapt to new reasoning tasks. Furthermore, these models are limited in their capacity to engage in multi-step reasoning\nand reflection, often leading to outputs that lack coherence or depth when faced with complex, multi-faceted problems.\nTo overcome these limitations, recent advances have introduced preference optimization techniques, such as Odds\nRatio Preference Optimization (ORPO) [25] and Direct Preference Optimization (DPO) [26, 27], or variants of these\nmethods [28]. These methods guide the model to align its outputs with certain preferences (in the context of our\nparticular application, scientific accuracy as identified using the raw corpus of data) by optimizing the log odds between\npreferred and rejected responses. However, existing implementations do not fully leverage the potential of recursive\nthinking and iterative refinement."}, {"title": "1.3 PRefLexOR Framework", "content": "PRefLexOR addresses these challenges by integrating preference optimization with a recursive reasoning mechanism\ndriven by thinking tokens, which explicitly mark phases of reasoning within the model's output. This allows the model\nto:\n1. Generate initial reasoning steps.\n2. Revisit and refine those steps through recursive processing, ensuring that reasoning is consistent, coherent, and\ndeeply aligned with scientifically accurate processes and resulting final answers.\n3. Adapt its decision-making by generating new tasks and feedback during training, enabling real-time learning.\nThe algorithm features two major phases, complemented by agentic inference. We first focus on training strategies and\nmove on to inference methods towards the end of the paper. The first phase is Structured Thought Integration Training,\nfollowed by Independent Reasoning Development and ultimately a Recursive Reasoning Algorithm.\nAt the core of PRefLexOR's approach is an initial alignment phase achieved using ORPO, which ensures that the model\nconsistently aligns its reasoning with desired outcomes by directly optimizing preference odds. In a second phase,\npreference optimization strategies are then layered on to handle fine-tuning through rejection sampling, capturing more\nsubtle distinctions in preference and further refining the model's output. This layered approach, combined with recursive\nreasoning, makes the model capable of handling open-domain tasks with greater reasoning capacity and adaptability.\nThe recursive reasoning and iterative feedback loops in PRefLexOR closely resemble Reinforcement Learning (RL)\nmethods, where models learn by refining policies based on rewards and feedback. In PRefLexOR, the model is\ncontinually provided with feedback in the form of preferred and rejected responses, which it uses to improve its thought\nprocess. This self-teaching mechanism is akin to the policy refinement seen in RL, where iterative feedback loops allow\nthe model to explore, evaluate, and improve its decision-making in real-time.\nThe dynamic task generation in PRefLexOR introduces an active learning component, wherein the model generates\ntasks, reasoning steps, and negative examples on-the-fly during training. This method allows the model to handle\nmore nuanced reasoning challenges, evolving its cognitive abilities without the need for extensive pre-curated datasets.\nThe ability to recursively refine thoughts leads to a model that can continuously evolve and adapt to novel, complex\nproblems, effectively teaching itself to reason more deeply and align its outputs with preferred outcomes that align with\nthe ground truth data.\nWhile Quiet-STaR focuses primarily on recursive reflection and iterative reasoning, it can be enhanced through\npreference optimization techniques like ORPO and preference optimization. By incorporating these techniques, the\nmodel can align its reflective reasoning with preferences rooted in training data, such as scientific papers or simulation\nresults, ensuring that its refined thoughts and decisions meet desired outcomes. The recursive cycles in Quiet-STaR,\nfor instance, can be viewed as a form of policy refinement in reinforcement learning, where the model's reasoning\npolicy is continually updated based on feedback. When combined with preference optimization, these cycles ensure"}, {"title": "1.4 Outline of this paper", "content": "We first present the overall modeling strategy, focusing on the training phases and key aspects such as special tokens\nand other considerations. We present various inference examples with an in-depth technical analysis of the results.\nWe then proceed to an experimental feature by incorporating multiple phases that feature both thinking and reflection.\nUsing the reflection phase we implement a recursive algorithm that allows us to improve responses iteratively by scaling\ninference compute. We conclude with a detailed discussion of strengths, weaknesses, and future results."}, {"title": "2 Results and Discussion", "content": "The training of the model consists of two distinct phases, each designed to progressively enhance its reasoning\ncapabilities and ability to handle structured prompts and enhanced reasoning, here exemplified for domain-targeted\nstructured thinking processes."}, {"title": "2.2 Expanding the Analysis to incorporate Thinking and Reflection for Recursive Improvement in Agentic\nModeling", "content": "To show the flexibility of the method, we experiment also with other reasoning mechanisms such as combining\n<|thinking|>,<|/thinking|> with a second stage of reflection, triggered by <|reflection|> and <|/reflec-\ntion>. In this phase the model reviews earlier responses and is encouraged to critique, improve and otherwise enhance\nthe responses before the final answer is produced. Figure 5 depicts an overview of this approach."}, {"title": "2.2.1 Recursive Reasoning Algorithm", "content": "The existence of reflection allows us to implement a Recursive Reasoning Algorithm, a method designed to enhance the\nquality and depth of responses generated by the reasoning model by iteratively improving its reasoning steps in the\nthinking phase based on the reflection feedback.\nThe algorithm utilizes a multi-agent format, and exploits a synergistic interaction between two distinct models: The\nfine-tuned reasoning model and a general-purpose critic model. The reasoning model, specialized through careful\nfine-tuning, excels in generating structured, logical responses to given prompts. It not only produces initial responses\nbut also demonstrates the capability to iteratively improve its outputs based on feedback. Complementing this, the critic\nmodel serves as an evaluator and improved, analyzing the reasoning model's outputs and improving it based on the\nfeedback received.\nAt the heart of the algorithm lies an iterative process shown in Figure 6, forming the core mechanism for continuous im-\nprovement of responses. This process begins with the Reasoning Model generating an initial response to a given prompt.\nSubsequently, this response undergoes a cycle of refinement. Each iteration involves a thorough analysis of the current\nresponse, from which a reflection is extracted (indicated via <|reflect|>..<|/reflect|>). The critic model then\nutilizes this reflection to suggest improvements to the thinking process indicated via <|thinking|>..<|/thinking|>.\nBased on these suggestions, the model generates a new, improved response, incorporating the insights gained from the\ncritic's analysis. This is done by feeding the improved thinking process to the model, which then uses that to generate a\nnew reflection mechanism (to be used in the next iteration, if another one is done) and then the next answer.\nThis cycle of generation, evaluation, and refinement continues for a predetermined number of iterations or until the\nalgorithm achieves a response that meets specified quality criteria. The iterative nature of this process allows for the\nprogressive enhancement of responses, with each cycle building upon the improvements of the previous one.\nUpon completion of the iterative process, the algorithm presents two options for final output selection. The first option\nis to select the response from the final iteration as the definitive output. Alternatively, the algorithm offers the capability\nto integrate all generated responses into a comprehensive final answer, potentially capturing a broader range of insights\nand perspectives developed throughout the iterative process.\nThis approach combines the structured thinking of the specialized reasoning model with the broader perspective of the\ncritic model. The result is a system capable of producing responses that are not only logically sound but also nuanced\nand comprehensive. By incorporating elements of self-reflection and iterative improvement, the recursive response\nalgorithm strives to emulate human-like reasoning and problem-solving processes, potentially leading to higher-quality\noutputs in various natural language processing tasks."}, {"title": "2.2.2 Comparison with Non-fine-tuned Model", "content": "When inferencing the questions as covered above using the non-fine tuned model, we find that the responses are not\naligned to the application domain (here: biological materials) and do not feature thinking sections. Text Box 7 shows\nan example that illustrates the different, non-domain specific and more generic response without any thinking and\nreflection section, which we contrast to the results shown in Text Box 5."}, {"title": "3 Conclusions", "content": "This study addressed the challenge of fine-tuning generative models of synthetic intelligence, such as LLMs, to a\nspecific domain, while endowing it with particular reasoning capabilities for enhanced modeling of scientific thinking\n(Figure 1c). Inspired by biological systems' adaptability and evolution, PRefLexOR's recursive optimization approach\nmimics the processes through which natural materials achieve resilience and complexity. Just as biological systems self-\norganize and adapt to achieve optimal performance [19], PRefLexOR uses iterative feedback loops to refine and evolve\nits reasoning pathways. This bioinspired approach allows the model to autonomously enhance its decision-making\nabilities, achieving coherence and adaptability reminiscent of nature's design principles, particularly in applications\ninvolving biological materials and cross-domain scientific discovery.\nWe view this as an extension of more conventional physics or data-driven models that typically feature only forward\ncapabilities without situational awareness. In other words, conventional models cannot assess the quality of their\nown predictions. For example, a Partial Differential Equation (PDE) will confidently predict solutions to boundary\nvalue problems whether or not the model actually captures the underlying physics), true for both physics-based or\ndata-driven models (Figure 1b). In conventional scientific methods, humans will assess the quality of predictions using\na host of methodologies, specifically logical assessment, additional data collection, comparison with literature, and\nmore. Our quest to expand the reference to a model to include not only its forward capabilities but much broader\nsituational awareness is, in our opinion, an important area of research that can benefit greatly from synthetic, or artificial\nintelligence [24], especially in applications to solving inverse materials design problems [20, 35, 36, 7]. PRefLexOR\noffers one possible avenue to overcome these limitations through a multi-stage training and inference strategy, as\nvisualized in Figure 8."}, {"title": "3.1 Enhancing the Algorithm by Invoking Multidisciplinary Concepts from the Glass Bead Game", "content": "The invocation of the Glass Bead Game [33] goes beyond the use as a test case to probe the model's generalization\ncapabilities, but forms also an analogy to what advanced reasoning models can do. In his novel, Hermann Hesse\npresents a game that synthesizes knowledge from various fields\u2014such as mathematics, music, and philosophy-into\na higher-order conceptual framework, with players combining ideas in ways that reveal deeper patterns and insights.\nWithin the scope of PRefLexOR, this game becomes a metaphor for how thinking and reflection processes in reasoning\nmodels, operate. Just as players of the game engage in an iterative exploration of connections between disparate\ndisciplines, LLMs with thinking and reflection phases mimic this recursive synthesis. The \u201cthinking\u201d and \u201creflection\"\nphases, along with recursive agentic self-improvement, allows the model to explore multiple layers of reasoning and\nrefinement for cohesive responses (see, e.g., Figure 6, much like the Glass Bead Game connects concepts across\ndomains. The structured interplay of thought and reflection in reasoning models echoes the intellectual depth and\ncomplexity of Hesse's game, suggesting that, like the Glass Bead Game, such models may be capable of uncovering\nrich, interdisciplinary insights when guided by sophisticated reasoning strategies. This capacity to connect and reflect\nupon diverse ideas highlights the potential of LLMs to act as powerful tools for understanding, much like the characters\nin The Glass Bead Game use their symbolic play to explore the essence of knowledge itself as it resembles connections\nbetween bits of information, as shown in Figure 1a.\nSpecifically, the Glass Bead Game as proposed in the novel [33], is a symbolic system that serves as \u201ca kind of synthesis\nof human learning.\" The game represents a means of integrating and refining knowledge from diverse disciplines, such\nas mathematics, music, and philosophy. Players engage in an iterative process, continuously refining and revisiting\nconcepts to discover deeper relationships between them. Similarly, the algorithm in this method employs a recursive\napproach where a fine-tuned Reasoning Model generates an initial response, which is then subjected to reflection and\nimprovement through multiple iterations.\nThe process begins with the generation of an initial response, analogous to the first move in the Glass Bead Game,\nwhere the players begin with basic knowledge. As in the game, where players continually refine their moves through\nreflective thought, the algorithm extracts reflections from the initial response, enhancing the reasoning behind it. The\nCritic Model plays a role much like the intellectual rigor imposed by the rules of the Glass Bead Game, providing an\nevaluative framework that helps guide the refinement of responses. Through this iterative process, the model improves\nits output, cycling between generating new responses and reflecting on previous iterations until an optimal or integrated\nsolution is reached.\nThis recursive thinking and reflection model mirrors the way the Glass Bead Game synthesizes diverse strands of\nknowledge into a cohesive whole. Just as the game is meant to model a kind of synthesis of human learning, the\nalgorithm integrates reasoning and reflection to create responses that combine multiple iterations of thought into a\nmore comprehensive final answer. In this way, the recursive algorithm not only produces more refined outputs but\nalso illustrates how generative AI can emulate deep, interdisciplinary reasoning, much like the intellectual pursuit\nportrayed in Hesse's game. Figure 9 depicts a possible flowchart of such an algorithm that merges ideas proposed in the\nPRefLexOR framework with the process introduced in the Glass Bead Game.\nIn the integrated framework, a simple Reasoning Model is replaced with a set of Collaborative Agents, each acting\nas an individual reasoning engine with specialized expertise or perspectives (or a single model with distinct sets of\nspecial tokens to induce a particular type of reasoning specialty). This transformation allows the algorithm to simulate\na community of thinkers, reflecting the collective intellectual exploration emphasized in the Glass Bead Game [33].\nBy incorporating multiple reasoning models as collaborative agents, the algorithm harnesses diverse viewpoints and\nmethodologies, enhancing creativity, depth, and robustness in problem-solving. Each agent contributes unique insights,\nchallenges others' ideas, and collaboratively refines responses through iterative dialogue, much like the scholars in the\nGlass Bead Game who engage in symbolic synthesis across disciplines.\nSimilarly, the Critic is replaced with an Interdisciplinary Knowledge Base Model, serving as a rich repository of\ninformation from various fields that all agents can access and utilize. This shift moves the focus from evaluation\nto synthesis, aligning the algorithm with the game's emphasis on the unity of knowledge and deep contemplation\nby finding new connections [37]. The knowledge base enables agents to draw connections across different domains,\nfostering holistic understanding and allowing for more profound insights. By integrating this shared resource, the\nalgorithm encourages collaborative synthesis rather than hierarchical critique, mirroring the Glass Bead Game's practice\nof unifying arts and sciences through collective intellectual endeavor.\nThese revisions emulate collective intellectual endeavors at high levels of integrated societal scales, simulating\na community of thinkers enhances the algorithm's ability to explore complex problems from multiple angles. It\nincorporates diverse expertise via agents with specialized knowledge contribute to a more comprehensive and nuanced\nunderstanding. This is believed to improve problem-solving as collaborative refinement leads to innovative solutions"}, {"title": "3.2 Future Work, Challenges and Opportunities", "content": "Several avenues for future work offer exciting opportunities to enhance the capabilities of our model. Key directions\ninclude exploring agentic reasoning strategies, such as AutoGen [38] and high degrees of agentic modeling via\nswarm-based approaches, and scaling to larger models for increased performance. Additionally, testing the model's\ngeneralizability across diverse domains and incorporating multiple thinking sections with partial masking are promising\nmethods for improving reasoning efficiency.\nWhile PRefLexOR demonstrates promising results in enhancing AI reasoning capabilities, particularly in biological\nmaterials science, several limitations warrant further investigation. The framework's increased computational cost,\nespecially in its recursive phases, may limit real-time applications, necessitating optimization strategies. However,\nin cases where compute is not an issue, such as scientific discovery, this may not present a significant burden. Its\ncurrent focus on specific domains suggests that future work should explore other areas of applications including broader,\nmulti-disciplinary training.\nFuture work may also focus on refining reasoning strategies, including more structured outputs (e.g. additional steps to\ndiscovery reasoning categories from data) and integrating other methods, potentially mixing various approaches for\noptimal outcomes. One direction is to trigger different reasoning strategies based on task type or allow the model to\nautonomously detect the best approach. For example, logic-based questions might follow a distinct reasoning pathway\ncompared to materials design or regression tasks. The use of symbolic reasoning may further enhance generalization\ncapabilities, perhaps combined with graph theoretic concepts such as isomorphic analysis as was suggested in other\nwork [37]. This adaptability offers remarkable flexibility and precision in addressing diverse challenges.\nMore sophisticated agentic modeling can be another promising next step, where reasoning or reflection stages are\ncritiqued or assessed for feasibility, particularly in areas such as physical design or materials science. By incorporating\nreflective critique, the model can continuously refine its reasoning processes. For example, reasoning steps could be\ncritiqued based on real-world constraints, such as physical feasibility or design limitations, to ensure solutions are not\nonly theoretically sound but practically viable."}, {"title": "4 Materials and Methods", "content": "4.1 Special Tokens for Reasoning\nIn this work, several special tokens were introduced to improve the structured reasoning and reflection capabilities of\nthe model. These tokens are integrated into the tokenizer of the Llama 3.2 model [6, 39] and help guide the model in\ngenerating specific types of outputs, such as thinking steps, reflective improvements, and final answers, while providing\nstructured reasoning pathways during the training process.\nThe following special tokens were added:\n\u2022 <|response|> and <|/response |> - Used to demarcate the boundaries of the final answer or response\nprovided by the model.\n\u2022 <|reflect|> and <|/reflect|> - Used to mark the reflection phase, where the model evaluates and\nimproves upon its initial reasoning.\n\u2022 <|thinking|> and <//thinking|> - Used to denote the thinking phase, where the model generates its\nreasoning steps.\n\u2022 </scratchpad|> and <|/scratchpad |> - Optionally used to provide a scratchpad for interim steps, allowing\nthe model to store intermediary calculations or thoughts during inference.\nThese tokens allow for a clear delineation of different reasoning processes and phases within the model's output,\nenabling it to engage in reflective and structured thinking. Below is a summary of these tokens and their properties,\nincluding their token IDs in the customized tokenizer:"}, {"title": "4.2 On-the-fly dataset generation via in-situ knowledge extraction", "content": "The algorithm is designed to questions from a given context and provide both correct and incorrect answers. The\nprocess is conducted in-situ during training and consists of several key steps, which are described below."}, {"title": "4.2.1 Context Enhancement with Retrieval-Augmented Generation during Dataset Generation", "content": "The context is enriched using Retrieval-Augmented Generation (RAG). This process involves querying the index with\nthe generated question to retrieve additional relevant information and reasoning, which is appended to the original\ncontext.\nWe build an index of text embeddings to facilitate efficient retrieval-augmented generation (RAG). It transforms each\ntext chunk T\u2081 from a corpus of original raw data into a dense vector representation vi using the embedding model:\n$V_i = f_{embed}(T_i)$\nwhere fembed is the embedding function. When a query Q is generated, it is similarly encoded into a vector vq:\n$V_q = f_{embed}(Q)$\nLlama Index then computes the cosine similarity between vq and each v\u2081 in the index:\n$similarity(v_q, v_i) = \\frac{v_q . v_i}{\\left \\|v_q \\right\\| \\left \\|v_i \\right\\|}$\nThe most relevant vectors are selected based on this similarity measure, retrieving the corresponding text chunks, Tj,\nwhich are then appended to the original query context. This expanded context allows the LLM to generate a response\nthat incorporates both the retrieved information and the pre-existing knowledge, improving the depth and relevance of\nthe output.\nWe use the BAAI/bge-large-en-v1.5 text embedding model in RAG implemented in Llama Index [32]."}, {"title": "4.2.2 Raw data used for training", "content": "We use 500 scientific papers from the domain of biological and bio-inspired materials as the training data, as reported in\nearlier work [14]. To construct the raw corpus of text, we convert all PDFs into Markup language and then create text\nchunks. We use the LlamaIndex SentenceSplitter function with chunk size of 1024 tokens with chunk overlap of 20\ntokens."}, {"title": "4.2.3 Context Retrieval", "content": "The algorithm first retrieves relevant context information from a pre-constructed index of nodes. When a specific topic\nis provided, it selects nodes related to that topic; otherwise, it retrieves a random set of nodes (n = 3 in the work\nreported here). The text from the selected nodes is concatenated into a single context, which serves as the basis for\nquestion generation. The token length of the concatenated context is computed using a tokenizer."}, {"title": "4.2.4 Question Generation", "content": "A domain-specific question is generated based on the provided context using a text generation model. The question is\nformulated to capture an important aspect of the context, without referring to specific studies, papers, or authors. The\nquestion is intended to be challenging, requiring expert-level knowledge to answer."}, {"title": "4.2.5\nCategory-Based Information Extraction", "content": "The algorithm extracts structured information from the context based on several predefined categories [40]. These\ncategories include reasoning steps, relevant materials, and design principles, among others. For each category, the\nmodel generates a well-reasoned, concise explanation, which contributes to a deeper understanding of the question. The\npredefined categories are listed in Table 4."}, {"title": "4.2.6 Thinking Section for Reasoning", "content": "The extracted information from each category is assembled into a \u201cThinking Section for Reasoning\" This section\nis designed to aid in the reasoning process by providing structured, logical insights. The Thinking Section includes\nkey pieces of information from each category, which help guide the construction of the correct answer. It serves as a\nstructured reasoning framework for answering the question."}, {"title": "4.2.7 Correct and Incorrect Answer Generation", "content": "The correct answer is generated using the context and the Thinking Section. The reasoning included in the Thinking\nSection helps to formulate a well-structured and comprehensive response. Additionally, an incorrect (rejected) answer\nis generated either by a trained model or through a prompt-based approach. The rejected answer lacks logical reasoning\nand does not reference the correct context."}, {"title": "4.2.8 Final Output", "content": "The algorithm outputs three elements:\n\u2022 The generated question with an instruction to include the Thinking Section for Reasoning.\n\u2022 The correct answer, which is enhanced with the structured Thinking Section for Reasoning.\n\u2022 The rejected answer, which is designed to be incorrect and devoid of proper reasoning (in ORPO phase) or an\nanswer generated based on the current trained state of the model (in DPO/EXO phase)."}, {"title": "4.2.9 Reflection Section", "content": "When we use an additional reflection section, we introduce an introspective step in the algorithm that critiques the\nreasoning process used to generate an answer. This function asks the model to evaluate the thinking behind the generated\nanswer and suggest improvements. The reflection process is guided by the following prompt:"}, {"title": "4.2.10 Models used for Dataset Generation", "content": "We use the mistralai/Mistral-Nemo-Instruct-2407 model for dataset generation. We also experimented with\nmeta-llama/Llama-3.1-8B-Instruct for some training runs, which works well also. Alternatively, a host of\nother models can be used including more sophisticated models (e.g. gpt-40, Claude Sonnett3.5, etc.) but we\ndeliberately focused on small-scale open-source models for this study."}, {"title": "4.3 Handling Reasoning Tokens in Preference Alignment Loss Computation", "content": "In our algorithm we revise conventional preference optimization frameworks to handle learning intermediate reasoning\nsteps, referred to as \u201cthinking tokens.\u201d These tokens represent the model's internal reasoning processes and are enclosed"}, {"title": "4.3.1 Masking of Thinking Tokens in Multiple Sections", "content": "In this approach, all tokens between <|thinking|> and <|/thinking|> are masked, meaning they are excluded from\nthe log-probability computation and the subsequent loss calculation. However, the <|thinking|> and <|/thinking|> tokens themselves are included in the loss calculation to ensure that the model learns to produce these tokens correctly.\nFor a sequence of token IDs t = [t1, t2,..., tn] and log-probabilities p = [P1,P2,..., Pn], a boolean mask m\n[M1, M2,..., mn] is applied, where:\n$m_i = \\begin{cases}\n0 \\text{ if } t_i \\text{ is an inner thinking token}, \\\\\n1 \\text{ otherwise (including <|thinking|> and <//thinking|>)}.\\\n\\end{cases}$\nThe masked log-probabilities pmasked are computed as:\n$P_{masked} = p \\odot m,$\nwhere \u2299 denotes element-wise multiplication.\nThis approach ensures that the inner thinking tokens are ignored during the DPO loss computation, while the model is\nstill incentivized to generate the correct reasoning markers. The DPO loss is then calculated as:\n$L_{DPO} = log \\sigma (\\beta . (P_{masked, chosen} - P_{masked, rejected})),$\nwhere \u1e9e is a temperature parameter, Pmasked, chosen are the masked log-probabilities for the chosen response, and\nPmasked, rejected are those for the rejected response.\nAdditionally, we introduce flexibility by allowing a fraction of the inner thinking tokens to be masked, controlled by a\nparameter a. For a sequence of n thinking tokens, [a\u00b7n] tokens are randomly selected for masking, where 0 \u2264 a \u2264 1.\nSetting a = 0 results in no masking, while a = 1 masks all inner thinking tokens."}, {"title": "4.3.2 Dynamic Final Answer Detection via Masked Thinking Tokens", "content": "To provide the model with more flexibility in producing variable-length thinking periods, we introduce a dynamic\nfinal answer detection approach. Instead of masking tokens, this approach dynamically identifies the final answer by\ndetecting the last occurrence of the <"}]}