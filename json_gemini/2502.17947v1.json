{"title": "DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI 01, and o3-mini\nin Bilingual Complex Ophthalmology Reasoning", "authors": ["Pusheng Xu, MD", "Yue Wu, MD", "Kai Jin, MD, PhD", "Xiaolan Chen, MD", "Mingguang He, MD, PhD", "Danli Shi, MD, PhD"], "abstract": "Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and three other\nrecently released large language models (LLMs) in bilingual complex ophthalmology cases.\nMethods: A total of 130 multiple-choice questions (MCQs) related to diagnosis (n = 39) and\nmanagement (n = 91) were collected from the Chinese ophthalmology senior professional title\nexamination and categorized into six topics. These MCQs were translated into English using\nDeepSeek-R1. The responses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI ol and o3-mini were\ngenerated under default configurations between February 15 and February 20, 2025. Accuracy\nwas calculated as the proportion of correctly answered questions, with omissions and extra\nanswers considered incorrect. Reasoning ability was evaluated through analyzing reasoning\nlogic and the causes of reasoning error.\nResults: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862 in Chinese\nMCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI ol, and OpenAI 03-mini attained\naccuracies of 0.715, 0.685, and 0.692 in Chinese MCQs (all P<0.001 compared with DeepSeek-\nR1), and 0.746 (P=0.115), 0.723 (P=0.027), and 0.577 (P<0.001) in English MCQs, respectively.\nDeepSeek-R1 achieved the highest accuracy across five topics in both Chinese and English\nMCQs. It also excelled in management questions conducted in Chinese (all P<0.05). Reasoning\nability analysis showed that the four LLMs shared similar reasoning logic. Ignoring key positive\nhistory, ignoring key positive signs, misinterpretation medical data, and too aggressive were\nthe most common causes of reasoning errors.\nConclusion: DeepSeek-R1 demonstrated superior performance in bilingual complex\nophthalmology reasoning tasks than three other state-of-the-art LLMs. While its clinical\napplicability remains challenging, it shows promise for supporting diagnosis and clinical\ndecision-making.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), such as OpenAI GPT series[1] and Google's Gemini series,[2\n3] have revolutionized the field of artificial intelligence (AI) by demonstrating impressive\ncapabilities in natural language understanding and reasoning. These models exhibit significant\npotential in the medical domain, including personalized health consultations, research and\nclinical decision support, surgical planning assistance, and the facilitation of telemedicine.[4]\nHowever, their performance and safety must undergo rigorous evaluation before they can be\nresponsibly integrated into clinical workflows.[5]\nIn ophthalmology, researchers have tried to use the LLMs to integrate massive amounts of\nophthalmic medical literature, guidelines, and patients' data to assist doctors in making more\naccurate diagnoses and facilitating clinical decision support.[6-12] However, existing LLMs\nhave not yet meet the rigorous standards required for clinical adoption in ophthalmic disease\ndiagnosis.[13] For example, Bahir et al. found that Gemini Advanced only got a 66% accuracy\nrate in an ophthalmology residency exam.[14] Similarly, Zhang et al. found that GPT-40\nexhibited significantly lower accuracy in primary diagnosis compared to human\nophthalmologists in twenty-six glaucoma cases.[15] Notably, while LLMs may perform well\non recall tasks, they face challenges when handling complex medical cases that require\nreasoning. For example, GPT-4 achieved mean accuracies of only 48.0% and 63.0% in\ndiagnosing and determining the next step in reasoning tasks derived from JAMA\nOphthalmology's Clinical Challenges section.[16]\nRecently, the DeepSeek team released its latest cost-effective open-source model, DeepSeek-\nR1.[17] By incorporating multi-stage training and cold-start data prior to large-scale\nreinforcement learning (RL), DeepSeek-R1 achieved performance on reasoning tasks\ncomparable to OpenAI-01-1217. However, its accuracy and clinical applicability in complex\nophthalmology reasoning tasks, particularly in a bilingual context, remain uncertain. Bilingual\ncapabilities are essential in clinical ophthalmology, as medical professionals often need to\ninterpret patient records, guidelines, and research findings across multiple languages, especially\nin multilingual regions or international collaborations. Accurate language comprehension is\ncritical for reducing misinterpretations and ensuring precise diagnoses and treatment\nrecommendations.\nThis study aims to evaluate the performance of state-of-the-art (SOTA) LLMs, including\nDeepSeek-R1, Gemini 2.0 Pro, OpenAI ol, and OpenAI 03-mini, in bilingual complex\nophthalmology reasoning. By assessing their accuracy and reasoning ability, we seek to\ndetermine their potential for real-world clinical applications and identify areas for future\nimprovement."}, {"title": "Methods", "content": "Data sources\nTo prevent potential data leakage\u2014where test data is used in model training-we didn't use the\npublicly accessible USMLE questions that had been utilized in previous studies. Instead,\nwe collected 130 multiple-choice questions (MCQs) designed for the Chinese ophthalmology\nsenior professional title examination from VIP documents on Baidu Wenku. These MCQs were\nreviewed for validity and reliability by an ophthalmologist with over six years of clinical\nexperience. The questions assess diagnostic (including differential diagnosis, n=39) and\nmanagement (n=91) aspects across various ophthalmic subspecialties. We categorized them\ninto six main topics: anterior segment diseases (n=25), external eye/orbital diseases (n=24),\nglaucoma (n=21), ocular trauma (n=32), refractive disorders/strabismus (n=17), and retinal\ndiseases (n=11). Each question contains 5 to 9 answer choices, with the number of correct\nanswers ranging from 1 to 6. An overview of this study is presented in Figure 1.\nTranslation of MCQs\nConsidering that the same question in different languages may affect the performance of\nLLMs. We used DeepSeek-R1 to generate the English version of these MCQs. To mitigate\nthe risk of the model unintentionally memorizing the correct answers during translation, only"}, {"title": "LLM Access", "content": "the questions and answer options were input, excluding the reference answers. The prompt used\nfor this translation process is provided in Supplemental Table 1A.\nWe accessed DeepSeek-R1 via the Application Programming Interface (API) provided by\nVolcengine, a cloud service platform under ByteDance, as the official DeepSeek server has\nbeen experiencing attacks, overload pressure, and usage limitations. Gemini 2.0 Pro (Gemini-\n2.0-pro-exp) and OpenAI 03-mini were accessed through their official APIs. Due to OpenAI's\nrestrictions on API access for ol, we were unable to retrieve responses via the official API and\ninstead obtained them through the official chat user interface (UI).\nTo ensure linguistic consistency between the responses and the corresponding MCQs, we used\nprompts aligned with the language of the questions (Supplemental Tables 1B and 1C). All\nresponses were generated under default configurations between February 15 and February 20,\n2025."}, {"title": "Reasoning Ability Analysis", "content": "Reasoning ability was assessed by analyzing reasoning logic and identifying the causes of\nreasoning errors. The analysis of reasoning logic involved examining and comparing the\nreasoning processes used to answer questions correctly across all models. The causes of\nreasoning errors were analyzed separately according to question type. For diagnostic-related\nquestions, errors were classified into five categories: 1. Ignoring key positive history. 2.\nIgnoring key negative history. 3. Ignoring key positive signs. 4. Ignoring key negative signs. 5.\nInadequate differential consideration. For management-related questions, errors were also\nclassified into five categories: 1. Diagnostic error, which means the answer was chosen based\non an incorrect diagnosis. 2. Too conservative. For example, ignoring the role of adjunctive\ntherapy or recommending conservative treatment when surgery should be chosen. 3. Too\naggressive. For example, recommending a more expensive or higher-risk test or treatment when\na cheaper or safer alternative is available. 4. Misinterpreting medical data. For example,\nmisinterpreting disease characteristics and complications, or misunderstanding the indications\nand contraindications of medications or surgeries. 5. Misunderstanding the question. For\nexample, providing multiple answers when the question asks for the most important or most\nurgent action."}, {"title": "Statistical Analysis", "content": "The final answers chosen by the LLMs were manually verified based on the text responses of\nthe models. Accuracy was calculated as the ratio of correctly answered questions to the total\nnumber of questions. Since some MCQs contained multiple correct answers, both omitted and\nextra answers were considered incorrect in this study. A 95% confidence interval was calculated\nusing the Clopper-Pearson method. P-values were computed using McNemar's test, with P<\n0.05 considered statistically significant. Statistical analyses were performed using Stata/MP\n17.0 (StataCorp, College Station, TX, USA). Radar charts, grouped bar charts and stacked bar\ncharts were created with Origin 2025 (OriginLab Corporation, Northampton, MA, USA)."}, {"title": "Results", "content": "Overall performance of the four LLMs\nAs shown in , DeepSeek-R1 demonstrated a leading performance in Chinese complex\nophthalmology reasoning tasks, achieving an overall accuracy of 0.862 (95% CI: 0.790-0.916;\nall P<0.001 when compared with three other LLMs). Gemini 2.0 Pro ranked second, with an\noverall accuracy of 0.715 (95% CI: 0.630-0.791); however, its performance was comparable to\nOpenAI ol and o3-mini, as no statistically significant differences were observed.\nAlthough DeepSeek-R1 performed lower accuracy in English reasoning tasks than in Chinese\n(Table 1B), it still ranked first, achieving an overall accuracy of 0.808 (95% CI: 0.729-0.872;\nP = 0.115, 0.027 and <0.001 when compared with Gemini 2.0 Pro, OpenAI ol and o3-mini,\nrespectively). Additionally, both Gemini 2.0 Pro and OpenAI ol exhibited higher accuracy in\nEnglish reasoning tasks. In contrast, OpenAI 03-mini demonstrated worse performance in\nEnglish (P=0.017, Supplemental Table 2), with an accuracy of only 0.577 (95% CI: 0.487-\n0.633), placing it in fourth position."}, {"title": "Performance of LLMs in different topics", "content": "In the Chinese MCQs, DeepSeek-R1 achieved the highest accuracy in five topics (Figure 2A),\nincluding glaucoma (0.952, 95% CI: 0.762-0.999), refractive disorders/strabismus (0.941, 95%\nCI: 0.713-0.999), external eye/orbital diseases (0.875, 95% CI: 0.676-0.973), ocular trauma\n(0.843, 95% CI: 0.672-0.947), and anterior segment diseases (0.840, 95% CI: 0.639-0.955).\nHowever, only statistically significant when compared with Gemini 2.0 Pro in the topic of\nglaucoma (Supplemental Table 3A). Gemini 2.0 Pro achieved the highest accuracy in retinal\ndisease topic with an accuracy of 0.727 (95% CI: 0.390-0.940).\nIn the English MCQs, DeepSeek-R1 also ranked first in five topics (Figure 2B): refractive\ndisorders/strabismus (0.941, 95% CI: 0.713-0.999), glaucoma (0.905, 95% CI: 0.696-0.988),\nexternal eye/orbital diseases (0.875, 95% CI: 0.676-0.973), ocular trauma (0.781, 95% CI:\n0.600-0.907), and anterior segment diseases (0.720, 95% CI: 0.506-0.879). However, statistical\nsignificance was only observed when compared with OpenAI 03-mini in the topics of glaucoma\nand ocular trauma (Supplemental Table 3B). Gemini 2.0 Pro shared the highest accuracy in\nthree topics, while OpenAI ol shared the highest accuracy in two topics. All models exhibited\npoor performance in retinal diseases, with OpenAI 03-mini achieving the lowest accuracy of\n0.182 (95% CI: 0.023-0.518)."}, {"title": "Performance of LLMs in diagnostic and management questions", "content": "As shown in , DeepSeek-R1 exhibits superior performance compared to OpenAI 03-"}, {"title": "Reasoning logic analysis", "content": "mini and achieves comparable results to Gemini 2.0 Pro and OpenAI o1 in bilingual diagnostic\nquestions. Besides, in management questions conducted in Chinese, DeepSeek-R1 outperforms\nthe three other LLMs, with all comparisons reaching statistical significance (Figure 3B).\nAll four LLMs correctly answered the same 63 questions in both Chinese and English MCQs.\nAn example is provided in Supplemental Table 4. All models exhibited similar reasoning logic.\nFirst, they identified a history of herpes zoster as the most critical clue, recognizing it as a\nknown causative factor of acute retinal necrosis (ARN). Second, they analyzed key positive\nclinical signs supporting the diagnosis of ARN. Finally, they systematically ruled out incorrect\noptions by eliminating alternative diagnoses.\nThere were four questions that only DeepSeek-R1 answered correctly in both Chinese and\nEnglish MCQs. An example is presented in Supplemental Table 5. Although DeepSeek-R1,\nGemini 2.0 Pro, and OpenAI ol recognized that the key to this question was differentiating\nbetween an inflammatory pseudotumor of the lacrimal gland and acute dacryoadenitis, their\ndiagnostic approaches differed. DeepSeek-R1 selected blood routine test and ocular ultrasound,\nwhereas Gemini 2.0 Pro and OpenAI ol opted for magnetic resonance imaging (MRI). In\ncontrast, OpenAI 03-mini deviated by initially focusing on the exclusion of Graves'\nophthalmopathy."}, {"title": "Reasoning error analysis", "content": "Diagnostic errors resulting from ignoring key positive history and positive signs ranked the top-"}, {"title": "Discussion", "content": "2 across all LLMs in both Chinese and English diagnostic questions (Figures 4A and 4B). In\nbilingual management questions, misinterpretation of medical data and too aggressive were the\ntwo most common errors across all LLMs (Figures 4C and 4D).\nCompared to the three other SOTA LLMs, DeepSeek-R1 achieved the best performance in\nChinese complex ophthalmology reasoning tasks and performed comparably to Gemini 2.0 Pro\nin English. DeepSeek-R1 also had the highest number of topics with the highest accuracy and\nexcelled in management questions conducted in Chinese. Reasoning ability analysis showed\nthat the four LLMs shared similar reasoning logic. Ignoring key positive history, ignoring key\npositive signs, misinterpretation of medical data, and being too aggressive were the most\ncommon causes of reasoning errors across all LLMs.\nIn this study, DeepSeek-R1 demonstrated excellent performance in bilingual complex\nophthalmology reasoning tasks, which may be attributed to its innovative training methodology.\nThe DeepSeek team incorporated thousands of high-quality Chain-of-Thought (CoT) data as"}, {"title": "Limitations", "content": "cold-start data. They observed that fine-tuning the model with cold-start data at the initial phase\nof reinforcement learning (RL) significantly improved the readability of its output.[17]\nAdditional strategies, including reasoning-oriented RL, rejection sampling, and supervised\nfine-tuning, were also implemented in DeepSeek-R1. It is likely that this innovative training\napproach has enabled DeepSeek-R1 to excel in reasoning tasks such as those in this dataset,\nwhich require complex and extended logical chains. Several preprint studies have compared\nthe performance of DeepSeek-R1 and other LLMs in medical contexts. For instance, Mondillo\net al. reported that OpenAI ol outperformed DeepSeek-R1 in pediatric MCQs, achieving a\ndiagnostic accuracy of 92.8% compared to 87.0%.[21] Zhou et al. found that DeepSeek-R1\ngenerated more readable responses than ChatGPT-4o in patient education materials for spinal\nsurgeries. [22] Mikhail et al. observed that DeepSeek-R1 and OpenAI ol demonstrated\ncomparable performance on an English ophthalmology MCQ dataset collected from\nStatPearls.[23] However, the questions in their dataset consist of one correct answer and three\ndistractor options, making them less challenging than those in our dataset. Furthermore, as their\ndataset is publicly available, it may have been utilized in the pre-training or fine-tuning of one\nor more models. To the best of our knowledge, this is the first study to evaluate bilingual\ncomplex ophthalmology reasoning performance across DeepSeek-R1 and three other SOTA\nLLMs.\nDeepSeek-R1 performed slightly better in Chinese than in English on our MCQs, which may\nbe attributed to its higher proportion of Chinese pretraining data. Although DeepSeek-R1 has\nnot disclosed the exact proportion of Chinese and English data, its earlier version, DeepSeek-\nV2, contained 1.12 times more Chinese tokens than English tokens.[24] Gemini 2.0 Pro and\nOpenAI ol exhibited superior performance in English, a finding consistent with previous\nresearch. However, the reason for OpenAI 03-mini's poorer performance in English\nMCQs remains unclear. In this study, the accuracy of OpenAI ol was slightly lower than\nreported in previous studies, which may be attributed to the higher difficulty level of the\nquestions used. For instance, in the topic of retinal diseases, where all models demonstrated\nsuboptimal performance, the 11 MCQs included varying numbers of answer choices: 3\nquestions had five options, 3 had six options, and 5 had eight options. Additionally, 4 questions\nhad more than two correct answers, further increasing the complexity of the task.\nAll models demonstrated strong reasoning abilities and exhibited similar analytical logic in\nophthalmology case analysis. For instance, in diagnostic questions, they first summarize the\nmedical history to identify key clues. Next, they highlight critical positive clinical signs and\nintegrate these with the medical history to formulate a preliminary diagnosis along with a\nrationale. They then conduct a differential diagnosis for each option, systematically analyzing\nboth supporting and non-supporting factors. Finally, they determine the most appropriate final"}, {"title": "Further Work", "content": "answer (Supplemental Figure 1). This logical reasoning sequence in case analysis closely\naligns with the diagnostic approach used by human physicians. Even in cases where errors in\nreasoning occurred, they were not due to fundamental flaws in logical structure but rather a\nfailure to identify the most critical clue or select the most appropriate method. For example, as\nshown in Supplemental Table 5, DeepSeek-R1, Gemini 2.0 Pro, and OpenAI ol correctly\nrecognized that the key to the question was differentiating between an inflammatory\npseudotumor of the lacrimal gland and acute dacryoadenitis. Although the reference answer\naligns with DeepSeek-R1's choice of blood routine tests and ocular ultrasound, it is undeniable\nthat the MRI selected by Gemini 2.0 Pro and OpenAI ol can also effectively differentiate\nbetween the two conditions, but its higher cost prevents it from being the first-line choice.[28]\nIgnoring key positive history and positive signs was found to be the top two sources of\nreasoning error. Interestingly, this result aligns with the causes of diagnostic errors observed in\nhuman clinicians. Misinterpretation of medical data and recommendation of too aggressive\nmeasures may result from wrong information in the training data,[30] variations in reference\nstandards, or the dynamic evolution of medical guidelines. For example, when answering the\ndiagnostic criteria for dry eye using Schirmer's test, the four LLMs failed to reach a consistent\nconclusion on whether the threshold should be a filter paper wetting length of less than 10mm\nor less than 5mm within 5 minutes, which may be related to changes in the diagnostic standards\nfor dry eye.\nThis study has several limitations. First, similar to other studies comparing the performance of\ndifferent LLMs in ophthalmology,  the MCQs used in this study were published before\nthe models' knowledge cutoff date, making it impossible to ensure that these questions were\nnot included in the models' training data. However, the questions were sourced from VIP\ndocuments, and Baidu Wenku has implemented various anti-crawling measures for such\ndocuments, including asynchronous loading and data encryption, which reduce the likelihood\nof these documents being included in the training data. Second, due to the lack of reference\nanswers for the reasoning process, we did not quantify the models' reasoning ability. In studies\nwhere human answers serve as reference reasoning processes, emerging metrics such as\nconsistency (invariance to logically equivalent inputs), generalization (performance on out-of-\ndistribution data), and explainability (clarity of reasoning steps) can be measured.[34] Although\ndeductive reasoning, inductive reasoning, abductive reasoning, and analogical reasoning are all\ncrucial to the reasoning capabilities of LLMs, abductive reasoning is more commonly used in\nthe medical domain.[35] Therefore, future medical research could prioritize the evaluation of\nthis specific ability.\nIn summary, compared to the three other LLMs, DeepSeek-R1 exhibited the best performance"}, {"title": "Conclusion", "content": "in bilingual complex ophthalmology reasoning tasks. Although its direct application in clinical\npractice remains challenging, it holds significant potential for assisting in diagnosis and\nsupporting clinical decision-making."}]}