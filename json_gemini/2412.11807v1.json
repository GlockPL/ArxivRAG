{"title": "PhysAug: A Physical-guided and Frequency-based Data Augmentation for Single-Domain Generalized Object Detection", "authors": ["Xiaoran Xu", "Jiangang Yang", "Wenhui Shi", "Siyuan Ding", "Luqing Luo", "Jian Liu"], "abstract": "Single-Domain Generalized Object Detection (S-DGOD) aims to train on a single source domain for robust performance across a variety of unseen target domains by taking advantage of an object detector. Existing S-DGOD approaches often rely on data augmentation strategies, including a composition of visual transformations, to enhance the detector's generalization ability. However, the absence of real-world prior knowledge hinders data augmentation from contributing to the diversity of training data distributions. To address this issue, we propose PhysAug, a novel physical model-based non-ideal imaging condition data augmentation method, to enhance the adaptability of the S-DGOD tasks. Drawing upon the principles of atmospheric optics, we develop a universal perturbation model that serves as the foundation for our proposed PhysAug. Given that visual perturbations typically arise from the interaction of light with atmospheric particles, the image frequency spectrum is harnessed to simulate real-world variations during training. This approach fosters the detector to learn domain-invariant representations, thereby enhancing its ability to generalize across various settings. Without altering the network architecture or loss function, our approach significantly outperforms the state-of-the-art across various S-DGOD datasets. In particular, it achieves a substantial improvement of 7.3% and 7.2% over the baseline on DWD and Cityscape-C, highlighting its enhanced generalizability in real-world settings.", "sections": [{"title": "Introduction", "content": "Object detection is a fundamental task in computer vision aiming to localize and recognize objects in natural images (Ren et al. 2015; Redmon et al. 2016; Tian et al. 2022; Zou et al. 2023). In past decades, object detection has achieved significant progress due to the advancement of deep learning, which assumes that the training data and test data come from the same distribution (Zhao et al. 2019). However, distribution shifts in various real-world applications make deep learning-based object detectors impractical. For instance, when trained on high-quality source data, object detectors for autonomous driving tend to perform poorly on different distributions of target data due to the changes in weather, illuminations, and object appearances. Such performance degradation hinders its wide deployment in safety-critical areas.\nTo tackle this issue, single-domain generalized object de-"}, {"title": "Related Works", "content": "To enhance model performance in unseen target domains, recent advancements in this field have explored various approaches. These approaches could be categorized into three main types: data augmentation, losses for domain generalization, and network architecture improvements. Most existing methods address this task through data augmentation. Those methods generate new domains via various transformations, thereby converting the S-DGOD problem into a domain adaptation object detection challenge. (Vidit, Engilberge, and Salzmann 2023) leveraged the vision-language model CLIP for semantic augmentation with textual prompts, enhancing the diversity of features extracted by the model. (Lee et al. 2024) employs an object-aware augmentation approach that enables the model to better detect and focus on potential regions of interest. DivAlign (Danish et al. 2024) similarly opts for diversity in the source domain and additionally aligns losses based on the class prediction results from the detector. In addition to data augmentation, domain-invariant feature distillation and neural architecture search have also been successfully employed to enhance model generalization. CDSD (Wu and Deng 2022) adopts a cyclic self-decoupling method to extract domain-invariant representations from the training data. G-NAS (Wu et al. 2024) searches for the most general architecture to prevent the model from overfitting. Unlike other methods, we propose PhysAug from the perspective of physical modeling, incorporating real-world prior knowledge into the augmentation process."}, {"title": "Single-Domain Generalization for Object Detection", "content": "tection (S-DGOD) has emerged as a prominent learning paradigm to alleviate the domain-shift impact, which aims at generalizing an object detector trained on a single source domain to multiple unseen target domains. Along this line of research, data augmentation is an active research direction for the S-DGOD problem to enrich the diversity of source domain. Several works propose to design the compositional policies of pre-defined visual corruptions applied on the global image or object-level regions (Lee et al. 2024) for expanding training samples. SRCD (Rao et al. 2023) introduces a mixing strategy of the global image with texture of its patches to enforce the model to focus on semantic content. UFR (Liu et al. 2024) adapts a combination of frequency-based and spatial-based transformations to augment local objects. However, these methods do not explicitly consider the formation mechanism of non-ideal imaging conditions, preventing the capture of real-world variations. Fig. 1(a) depicts the optical imaging process with three light propagation paths through the atmosphere. Among these paths, as shown in Fig. 1(b), interactions between light and various particles in the atmosphere are the primary causes of image degradation.\nIn this paper, we first formulate a universal perturbation model from the theory of atmospheric optics. This perturbation model aims to explicitly characterize the inferior cases of optical imaging resulting from light propagation properties in the atmosphere. Guided by this physical-based model, we introduce our data augmentation method, namely PhysAug. Specifically, PhysAug is implemented from a fourier perspective: we adopt simple frequency-dependent filters to augment low-frequency components of the image, simulating the global non-uniform illumination caused by light scattering and attenuation. Furthermore, fourier basis functions are randomly combined and projected onto the spatial domain of the image, generating local occlusion effects due to non-transparent particles in the wild. We conduct experiments on two S-DGOD datasets, Diverse Weather Dataset (DWD) and Cityscapes-C. The results show that PhysAug significantly outperforms existing state-of-the-art methods, achieving improvements of 7.3% and 7.2% over the baseline methods on the corrupted domains of DWD and Cityscapes-C, respectively. Besides, our analysis indicates that PhysAug enables models to learn better domain-invariant representations than non-physical based augmentations, proving its superiority. In summary, our contributions are as follows:\n\u2022 We establish a universal perturbation model based on atmospheric optics, which is the first attempt to guide the design of data augmentation from the perspective of physical principles.\n\u2022 We introduce PhysAug, a frequency-based data augmentation method tailored for the S-DGOD problem. This method accurately simulates naturally-occurring visual degradation throughout the full trajectory of light propagation, enriching real-world variations in training data.\n\u2022 Comprehensive experiments and analysis validate the effectiveness of our method. Our method achieves new state-of-the-art performance on two standard S-DGOD datasets, outperforming other approaches by a large margin."}, {"title": "Data Augmentation for Domain Generalization", "content": "The most commonly used method in domain generalization is data augmentation in image domain (Zhou et al. 2022; Shorten and Khoshgoftaar 2019; Zhao et al. 2020; Rebuffi et al. 2021), there are several widely applied methods, including MixUp (Zhang et al. 2017), Cutout (DeVries and Taylor 2017), and AugMix (Hendrycks et al. 2019). Although image domain data augmentation has achieved significant success in enhancing model generalization and robustness. (Yin et al. 2019) found that models trained with visual transformations could be susceptible to noise affecting specific parts of the frequency spectrum. AmpMix (Xu et al. 2023) linearly interpolates the amplitudes of two images while keeping the phase information unchanged to enhance the model's ability to generalize. Furthermore, some studies attempt to combine image domain and frequency domain augmentation methods. PRIME (Modas et al. 2022) employs a combined augmentation approach that integrates both frequency domain and image domain techniques. AFA (Vaish, Wang, and Strisciuglio 2024) builds upon the approach of PRIME and further expands the integration of image domain and frequency domain augmentation techniques."}, {"title": "Preliminary", "content": "Single-domain generalization for object detection (S-DGOD) aims to train the model on a single source domain while ensuring it performs well across multiple target domains (Wang et al. 2021). Given a labeled source domain $D_s = \\{(x,y)\\}_{i=1}^{N_s}$ and multiple unseen target domains $M = \\{D_1, D_2,\\ldots,D_t\\}$, where $D_t = \\{(x)\\}_{i=1}^{N_t}$, $N_s$ and $N_t$ denote the size of the source and target datasets respectively. The S-DGOD task is to train an object detector $F$ on the source domain $D_s$, and test its performance across target domains $M$."}, {"title": "Problem Formulation", "content": "Single-domain generalization for object detection (SDGOD) aims to train the model on a single source domain while ensuring it performs well across multiple target domains (Wang et al. 2021). Given a labeled source domain $D_s = \\{(x, y)\\}_{i=1}^{N_s}$ and multiple unseen target domains $M = \\{D_1, D_2,\\ldots,D_t\\}$, where $D_t = \\{(x)\\}_{i=1}^{N_t}$, $N_s$ and $N_t$ denote the size of the source and target datasets respectively. The S-DGOD task is to train an object detector $F$ on the source domain $D_s$, and test its performance across target domains $M$."}, {"title": "Evaluation Metrics", "content": "We follow the settings established by (Michaelis et al. 2019), using mean average precision (mAP) as metrics to evaluate the performance of our method on the clean domain. Furthermore, we also use mean performance under corruption (mPC) established by (Lee et al. 2024) as metrics to evaluate our method's performance on corrupted domains, which represents the average of mean average precision (mAP) across all corrupted domains and severity levels, the calculation is as follows.\n$mPC = \\frac{1}{N_c} \\sum_{C=1}^{N_c}(\\frac{1}{N_s} \\sum_{S=1}^{N_s} P_{c,s})$, (1)\n$P_{c,s}$ is the target domain corrupted by corruption $C$ at severity $S$, $N_s$ and $N_c$ means the number of severity and corruption. For DWD, the values for $N_s$, $N_c$ are 1 and 4, for Cityscapes-C, the values are 5 and 15."}, {"title": "Methodology", "content": "In this section, we introduce the design and implementation of PhysAug. First, we formulate the universal perturbation model from the theory of atmosphere optics. Based on the model, we derive our data augmentation method from a fourier perspective to explicitly simulate real-world variations for augmented data."}, {"title": "A Physical-based Perturbation Model", "content": "Physical imaging models based on atmospheric optics have achieved great success in various low-level computer vision tasks (Li, Cheong, and Tan 2019). Retinex theory (Land and McCann 1971) is introduced to simplify the optical imaging process as a combination of the incident light image $L(x)$ and the reflected light image $R(x)$, concentrating on the light propagation of Path1 and Path2 in Fig. 1(a). This can be represented by:\n$I (x, y) = L (x, y) \\cdot R (x, y),$\nwhere $I(x)$ is the observed image, $(x, y)$ is the image point. Retinex-based algorithms are designed to eliminate the effects of incident light to restore image quality, making them widely used for low-light enhancement tasks. The atmospheric scattering model (Nayar and Narasimhan 1999) is another popular approach to model physical imaging in foggy conditions. Its mathematical form is as follows:\n$I (x, y) = t (x, y) \\cdot J (x, y) + A \\cdot [1 \u2212 t (x, y)],$ (3)\nwhere $J(x, y)$ represents the haze-free image, $A$ denotes the atmospheric light and $t(x, y)$ is the transmission function. The first term of Eq. (3) indicates light scattering effects along the reflected light path, corresponding to Path2 in Fig. 1(a). The second term refers to changes in atmospheric light path (e.g., Path3). The above modeling methods focus on special cases of the physical imaging process for image restoration. In practice, the quality of optical imaging is influenced by the interactions of light with different types of particles along all three propagation paths. So we formulate a perturbation model for covering general cases of inferior"}, {"title": "The Design of PhysAug", "content": "In this section, we introduce the implemental details of PhysAug. Based on Eq (4), we define PhysAug as follows:\n$PhysAug (x) := Q \\cdot (\\hat{h}_g (x) + \\hat{h}_o (x)) + A,$\nwhere $\\hat{h}_g(\\cdot)$ and $\\hat{h}_o(\\cdot)$ denote the estimates of $h_g(\\cdot)$ and $h_o(\\cdot)$, respectively. To simplify notation, we use A to replace A. $t_w (x, y)$. Fig. 2 depicts the overall structure of PhysAug consisting of four components: The most significant components are non-uniform illumination $h_g$ and local occlusions $h_o$ along the reflected light path, as most visual degradation generates in this path. We adopt frequency-based pipelines to simulate these two components, as shown in Fig. 2. Besides, $Q$ and $A$ correspond to illumination changes from the incident light and atmospheric light path."}, {"title": "Overview", "content": "In this section, we introduce the implemental details of PhysAug. Based on Eq (4), we define PhysAug as follows:\n$PhysAug (x) := Q \\cdot (\\hat{h}_g (x) + \\hat{h}_o (x)) + A,$ (7)\nwhere $\\hat{h}_g(\\cdot)$ and $\\hat{h}_o(\\cdot)$ denote the estimates of $h_g(\\cdot)$ and $h_o(\\cdot)$, respectively. To simplify notation, we use A to replace A. $t_w (x, y)$. Fig. 2 depicts the overall structure of PhysAug consisting of four components: The most significant components are non-uniform illumination $h_g$ and local occlusions $h_o$ along the reflected light path, as most visual degradation generates in this path. We adopt frequency-based pipelines to simulate these two components, as shown in Fig. 2. Besides, $Q$ and $A$ correspond to illumination changes from the incident light and atmospheric light path."}, {"title": "Global Non-uniform Illumination", "content": "Recent studies have shown that illumination information exhibits excellent separation properties in the image frequency domain, and is mainly concentrated in the low-frequency component (Vaish, Wang, and Strisciuglio 2024). So we apply the convolution with random kernels to the image, adjusting the amplitude spectrum of the low-frequency components to generate a diverse range of non-uniform illumination conditions. This operation is defined as:\n$\\hat{h}_g (x, y) := J (x, y) * G$\n$:= F^{\\circledR -1} \\{F \\{J (x,y)\\} \\cdot F\\{G\\}\\}.$ (8)\nwhere $F$ and $F^{-1}$ represent the Fourier transform and the inverse Fourier transform, respectively. $G$ denotes a convolutional filter with a kernel size of $n \\times n$, and its parameters are drawn from a gaussian distribution. $*$ is the convolution operation."}, {"title": "Partical-induced Local Occlusion", "content": "It is well known that visible light consists of components of different wavelengths. Here, we consider that local occlusions are generated from a composite of single-wavelength light which undergoes reflection and absorption by interacting with stmospheric particles. Inspired by previous fourier-based analysis (Vaish, Wang, and Strisciuglio 2024; Chen et al. 2021; Xu et al. 2023), we utilize the sinusoidal planar wave functions to simulate the occlusion case of single-wavelength light. The real part of planar wave function is defined as:\n$SP (x,y) = \\Re \\left( C \\cdot e^{i2\\pi f \\cdot \\Tau(\\omega,\\phi)}\\right),$\n$\\Tau(\\omega, \\phi) = x \\cos (\\omega) + y \\sin (\\omega) + \\phi,$\nwhere $C$ is a constraint to enforce a unit l2-norm of the planar wave. $\\Re(\\cdot)$ denotes the real part. $\\Tau(\\cdot)$ is a regular function. $f$ and $\\omega$ represent the frequency and phase, respectively. $\\phi$ is set to a constant of $n\\pi$, where $n \\in \\mathbb{N}$. $P$ denotes light of the specific wavelength, and in practice, we set $P\\in [R, G, B]$. Hence, we use $S(x,y) = [S_R(x, y), S_G (x, y), S_B(x, y)]$ to represent the operations on R,G,B channels. Finally, we introduce the composite function to construct local occlusions from multiple types of particles:\n$\\hat{h}_o (x, y) = \\sum_{i=0}^{n} M_i \\cdot S_i (x, y),$ (11)\nwhere $i$ is the particle type. $M_i$ denotes a random matrix of the same size as the image for diversifying the inconsistent properties of local occlusions. According to Eq. (7), we implement PhysAug to increase the diversity of augmented data. Its parameter settings are provided in the implementation details."}, {"title": "Experiments", "content": "In this section, we first introduce the datasets and implement details. Then, we present our detailed experimental results on two standard S-DGOD datasets."}, {"title": "Datasets", "content": "We evaluate our approach on two large-scale object detection datasets, Diverse Weather Dataset (DWD) and Cityscapes-C, following standard S-DGOD settings. DWD is an urban-scene detection dataset, which consists of five weather conditions, i.e., Daytime Sunny, Night Sunny, Night Rainy, Dusk Rainy and Daytime Foggy (Wu and Deng 2022). This Dataset is collected from multiple autonomous driving benchmarks (Yu et al. 2020; Sakaridis, Dai, and Van Gool 2018; Hassaballah et al. 2020). Daytime Sunny serves as a source domain containing 18,205 images for training and 8,313 for evaluation. Night Sunny, Night Rainy, Dusk Rainy and Daytime Foggy are used as unseen target domains, with 26, 158, 2,494, 3,501, and 3,775 images, respectively. Cityscapes-C is a robust detection benchmark, which synthesizes 15 types of common corruptions with five severity levels based on the validation set of Cityscapes (Michaelis et al. 2019). These corruptions are categorized into four groups: Noise, Blur, Digital, and"}, {"title": "Implementation Details", "content": "We first describe the implementation details of the proposed PhysAug. We set the filter $G$ in Eq. (8) as the size of $3 \\times 3$ with a gaussian distribution $N(0,4)$. The frequency $f$ and phase $\\omega$ in Eq. (9) are both drawn from a uniform distribution $(-512,512)$, and $\\phi$ is $n \\pi$. The matrix $M_i$ in Eq. (11) is sampled from 2D gaussian distribution $N(0, \\Sigma)$, where $\\Sigma$ is an identical matrix with the same size as the image. Q is set to 1 to indicate a constant incident light coefficient. Following the atmospheric scattering model, A represents the attenuation effects of atmospheric light, given by $A = L_{\\infty}(1 \u2212 e^{(-d)})$, and $L_{\\infty}$ is the atmospheric light at infinity, which is set to $10^{-1}$ (Nayar and Narasimhan 1999). We sample $d$ from a uniform distribution (0, 10). For DWD dataset, we adopt the setting up in CDSD (Wu and Deng 2022) and OA-DG (Lee et al. 2024). Specifically, we use Faster R-CNN (Ren et al. 2015) with a ResNet101 (He et al. 2016) backbone, and employ the SGD optimizer with a momentum of 0.9 and a weight decay of 0.0001. The learning rate is set to 0.001, and the batch size is 2. All other configurations align with those in the CDSD and OA-DG. For Cityscapes-C, we use Faster R-CNN (Ren et al. 2015) with a ResNet50 (He et al. 2016) backbone and feature pyramid networks (FPN) (Lin et al. 2017) as the baseline model. The learning rate is set to 0.01, with a batch size of 8. The SGD optimizer is employed with a momentum of 0.9 and a weight decay of 0.0001. All experiments are conducted on NVIDIA RTX 3090 GPU."}, {"title": "Performance on Diverse Weather Conditions", "content": "We evaluate PhysAug and other the state-of-the-art methods on DWD dataset, including SW (Pan et al. 2019), IBN-Net (Pan et al. 2019), IterNorm (Huang et al. 2019), ISW (Choi et al. 2021), SHADE (Zhao et al. 2022), Clip the Gap (Vidit, Engilberge, and Salzmann 2023), OA-DG (Lee et al. 2024), DivAlign (Danish et al. 2024), and UFR (Liu et al. 2024). Note that Div and OA-Mix are data augmentation methods in DivAlign and OA-DG, respectively. Table 1 reports their performances on five different scenarios. We can see that PhysAug achieves the best detection accuracy of 60.2 mAP in Daytime Sunny, 44.9 mAP in Night Sunny, 41.2 mAP in Dusk Rainy, and 40.8 mAP in Daytime Foggy, respectively. Overall, our method obtains the largest improvement of 7.3 mPC compared with the baseline detector in adverse weather conditions."}, {"title": "Performance on Common Corruptions", "content": "To further prove the generality of our proposed PhysAug, we also compare PhysAug with popular data augmentation approaches on Cityscapes-C, including Cutout (DeVries and Taylor 2017), PhotoDistort (Redmon and Farhadi 2018), AutoAug (Zoph et al. 2020), AugMix (Hendrycks et al. 2019), StylizedAug (Geirhos et al. 2018). Besides, we select other three methods that combine data augmentation with loss function, SupCon (Khosla et al. 2020), FSCE (Sun et al. 2021) and OA-DG (Lee et al. 2024) for comparison. These results are summarized in Table 2. It is noted that our proposed PhysAug achieves the state-of-the-art performance, improving by an average of 7.2 mPC over the baseline detector. Besides, we can find that our method performs better on Noise and Blur corruption types, with the best result of 17.0 mAP on Shot Noise, and 25.5 mAP on Motion Blur, respectively."}, {"title": "Ablation Studies", "content": "In this section, we conduct ablation studies to analyze the efficacy of the proposed PhysAug and highlight the benefits of guidance from physical-based modeling."}, {"title": "Dicussion", "content": "In this section, we conduct ablation studies to analyze the efficacy of the proposed PhysAug and highlight the benefits of guidance from physical-based modeling."}, {"title": "PhysAug", "content": "We now perform additional ablation experiments to study the impact of different components of the proposed PhysAug. Specifically, we compare the use of global non-uniform illumination in Eq. (8) and particle-induced local occlusions in Eq. (10). Table 3 reports on our results on DWD and Cityscapes-C dataset. For DWD dataset, each component can still obtain improvements of above 5 mPC in adverse weather conditions, indicating their versatility. The best performance is achieved with all components activated. One can see similar improvements of each component in Cityscape-C dataset, which validates the efficacy of the proposed method."}, {"title": "Physical-based Modeling", "content": "This experiment aims to validate the benefits of physical-based modeling. So we construct two non-physical models for comparison. These models are as follows:\n$I (x, y) =Q \\cdot [h_g (x, y) + h_o (x, y)],$ (12)\n$I (x, y) =\\lambda \\cdot Q \\cdot [h_g (x, y) + h_o (x, y)] + (1 - \\lambda) \\cdot J (x, y) + L$ (13)\nwhere $\\lambda$ is the mixing factor. we name the models derived from Eq. (12) and Eq. (13) as NPM1 and NPM2. NPM1 neglects the negative effect of the atmospheric light path, and NPM2 follows the MixUp-style (Zhang et al. 2017) to mix the non-corruption image based on Eq. (4). We compare the performance of PhysAug with NPM1 and NPM2 on the Cityscapes-C dataset, as shown in Table 4. The results show that PhysAug achieves the largest performance gains on corrupted domains, with 0.4-3.2 mPC improvements on NPM1 and NPM2, respectively."}, {"title": "Qualitative Analysis", "content": "In this section, we further perform visualizations and feature assessment of our method to see how exactly it works."}, {"title": "Visualization Analysis", "content": "In this section, we further perform visualizations and feature assessment of our method to see how exactly it works. In Fig. 3, we provide visualized object detection examples of the baseline method, OA-Mix and the proposed PhysAug on four adverse weather"}, {"title": "Conclusion", "content": "In this paper, we propose PhysAug, a novel data augmentation method for single-domain generalized object detection. To achieve this goal, we introduce a universal perturbation model from the theory of atmospheric optics to describe the general case of the inferior imaging process. Guided by this model, PhysAug simulates real-world variations caused by atmospheric particles to diversify training data. We conduct extensive qualitative and quantitative experiments to demonstrate the effectiveness of our method for the S-DGOD task. Results also show that PhysAug outperforms the state-of-the-art by a large margin."}]}