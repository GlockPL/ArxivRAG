{"title": "LLM Safety for Children", "authors": ["Prasanjit Rath", "Hari Shrawgi", "Parag Agrawal", "Sandipan Dandapat"], "abstract": "This paper analyzes the safety of Large Language Models (LLMs) in interactions with children below age of 18 years. Despite the transformative applications of LLMs in various aspects of children's lives, such as education and therapy, there remains a significant gap in understanding and mitigating potential content harms specific to this demographic. The study acknowledges the diverse nature of children, often overlooked by standard safety evaluations, and proposes a comprehensive approach to evaluating LLM safety specifically for children. We list down potential risks that children may encounter when using LLM-powered applications. Additionally, we develop Child User Models that reflect the varied personalities and interests of children, informed by literature in child care and psychology. These user models aim to bridge the existing gap in child safety literature across various fields. We utilize Child User Models to evaluate the safety of six state-of-the-art LLMs. Our observations reveal significant safety gaps in LLMs, particularly in categories harmful to children but not adults.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are increasingly impacting children through education, toys, and therapy, offering benefits like improved mental health and parental controls. Ensuring their safety is crucial given the potential for both benefit and harm, akin to social media or the internet. Despite significant attention to general LLM safety , little focus has been dedicated toward children and adolescents. This mirrors issues in other technologies, like the internet, where a unified approach to child safety is lacking, due to the diversity across scientific fields. Children's varying personalities and interests make them vulnerable to unique risks, highlighting the need for safety evaluations tailored to their specific needs.\nStudies on AI and child safety have primarily focused on explicit harms like child grooming or education-related risks. However, given children's openness and tendency to share personal experiences with chatbots , a more holistic approach to content harms is needed. We identify two primary gaps in current research on child safety in LLMs. First, there is a lack of a comprehensive taxonomy of potential content harms specific to children. Existing taxonomies are either overly specialized or only cover a small subset of general risks. Second, current evaluation studies are highly standardized and fail to address the diverse needs of children. This work addresses child safety in LLMs with the following contributions:\n\u2022 Child Content Harm Taxonomy: We propose a comprehensive taxonomy for content harms specific to children in LLM applications.\n\u2022 Child User Models: Development of diverse child user models based on child-care and psychiatry literature to capture personality and interest variations.\n\u2022 LLM Evaluation: Comprehensive evaluation of six LLMs through red-teaming, identifying safety gaps for children which is not covered by standard evaluations. Although we focuse on six LLMs, the method"}, {"title": "2 Related Work", "content": "Integrating child safety with technology research is challenging due to its multidisciplinary nature and the lack of a unified framework. While most studies focus on traditional media and internet technologies, AI's recent adoption among children has resulted in sparse literature, which this work addresses.\nA lot of existing technological child safety literature revolves around the use of television, videogames, mobiles, internet and social media. Mainstream usage of AI among children is relatively recent, resulting in sparse literature on the topic. We broadly cover two segments of literature focusing on child safety and AI."}, {"title": "2.1 Using AI to improve Child Safety", "content": "AI is increasingly being utilized in various domains to enhance child safety, including areas such as Detecting child abuse using AI, AI-based personal therapist and Al for safety against technology. Detecting child abuse using AI has been widely explored across various domains. Lupariello et al. (2023) surveys AI predictive models for child abuse, while works like explore approaches for the detection of children at risk of physical abuse based on textual clinical records. In case of an AI-based personal therapist, as demonstrated by, it suggests that children may disclose challenging personal events more openly to AI assistants than to human therapists or parents, presenting a new opportunity. Furthermore, AI for safety against technology has been explored in several studies shows that AI-based moderation is better than parental control for child sustainability and reducing continued exposure to digital devices. Zhuk (2024) highlights several ways AI can help tackle risks of Metaverse with personalized approaches that is able to provide nuanced safety tailored for the child.\nDespite the existing body of work in this area, our primary focus is to highlight key directions that promote the beneficial applications of AI by child safeguarding."}, {"title": "2.2 Evaluating Child Safety of LLMs", "content": "There has been effort toward evaluating LLMs for child safety, but it is often restricted to a few dimensions under general RAI evaluations or focused on a limited set of applications. Prosser and Edwards (2024) explore the protections of a few open-source and commercial LLMs against child grooming. They find all LLMs to be severely vulnerable to child grooming. Chauncey and McKenna (2023) provide a taxonomy of ethical risks in AI for education, while McStay and Rosner (2021) explore the ethical implications of exposing children to emotional AI through toys and digital devices. Vidgen et al. (2024) provide a test set that covers various AI harms including child-specific harms like child abuse and eating disorders. These areas of harm within LLMs are consistently observed as being the least protected. While Liu et al. (2024) survey 29 harms, one of which is harm to minors. Other works also target general safety, for example how incorrect instructions can be generated regarding supervising children around water bodies. Overall, research on evaluating the safety of LLMs for children is limited. Existing studies tend to focus on either narrowly defined applications such as educational or emotional AI, or address specific harms, such as child grooming, using simplistic, template-based prompts. In this paper, we build on this line of work by evaluating six state-of-the-art LLMs, across twelve child harm categories using diverse child user models that engage in conversations with LLMs to ensure high-level of safety testing."}, {"title": "3 Child Harm Taxonomy", "content": "Based on our literature survey, we identified potentially harmful topics. Within these topics, we identified 12 categories that could be harmful to children. These categories along with the reason for them being harmful are provided in the last column of Table 1.\nMany works exist that present detailed harm taxonomies , but these do not focus on children. Our taxonomy broadly cover two types of categories depending on whether these are covered in existing adult harm categories or not: Covered in adult harm taxonomies - These are categories like Violence that are harmful to adults as well. However even within these, we add new sub-categories to help cover specific manifestation of these for child safety. For example Bullying and School Shootings in Violence category; Not"}, {"title": "4 Evaluation Methodology & Experimental Setup", "content": "In this paper, we aim to evaluate LLMs for child safety across the various harms as described in Table 1. The goal is to closely replicate a real child using the diverse child models that capture varying personalities, developmental stages, and interests to evaluate LLM safety comprehensively, along with multi-turn testing that can uncover patterns missed in single-turn testing. Diversity in child user model is captured first by leveraging adjectives representing 11 personality traits from. Secondly, we also use 25 interests from to further capture diverse children personas. Examples of these are provided in Table 2 and Table 3 (complete tables are present in Appendix A.5).\nThe main evaluation strategy is to deploy an automated red-team testing approach where an adversarial conversation is carried out by a less protected \"Red\" LLM (prompted using child user models) against the test LLM which is being evaluated . Figure 1 shows a sample prompt used for the Red LM to continue the conversation. Here, the Red LM generates the next user turn based on the ongoing conversation, persona, and goal."}, {"title": "4.2 Child and Adult Models Generation", "content": "To evaluate LLM safety comprehensively, we create a dataset of 560 child user models by prompting GPT-4 to generate targeted personas and tasks using specific input configurations, as illustrated in Figure 1. Each child model is assigned a unique personality and interests to ensure diversity. Overall, we generate 40 seed queries per harm area based on Category column of Table 1. However, in experimentation, we breakup one of the categories into 3 categories for ease of experimentation, hence resulting in 14 categories instead of 12 in Table 1."}, {"title": "4.3 Evaluation", "content": "We use the 560 child and adult user models generated to simulate conversations between Red LLM and the test LLM. In this paper, we evaluate child safety for 6 models (as in figure 2) as our test models. For the adversarial Red LLM, we have used Mistral-7B-Instruct-v0.3. This model is less censored and thus is able to generate better harmful content which is a requirement for the role of Red LLM. We also use GPT-40 as a judge in order to annotate the simulated conversation as harmful or not using a custom labelling prompt created covering all the harms. All the user models, prompts, simulated conversation as"}, {"title": "5 Results & Insights", "content": "We analyse LLM safety with respect to children using two simple metrics: Defect rate - the percentage of conversations that contain at least one harmful target LLM response and Refusal rate - the percentage of conversations where target LLM refuses to answer to the user"}, {"title": "5.1 State of Child Safety in LLMs", "content": "Comparing families: Figure 2 shows overall Defect and Refusal rates for the six models. The Llama family exhibits low defect rates and high refusal rates, indicating relatively safer behavior, while the Phi family, Mistral, and GPT-40 show significantly higher defect rates. Despite Llama's better performance, its defect rate of 29.6% highlights the critical need for improving LLM safety for children across all models.\nComparing sizes: No clear correlation is observed between model size and safety, as GPT-40, the largest model, has the highest defect rate. This aligns with finding that model size alone may not lead to success, hence emphasizing the need for better safety tuning for child safety."}, {"title": "5.2 Relation between safety and usefulness", "content": "If we consider $(100 \u2013 \\text{Defect Rate})$ as the % of safe conversations or the safety score. Then we can measure safety cost as $\\text{Refusal rate}/(100-\\text{Defect rate})$. Table 4 shows that the safety cost of Llama-2 models is significantly high, they refuse on more than half of the conversations in order to provide safety. Thus, we understand that when safety is provided, it is at the cost of usefulness which can significantly impact child understanding, growth and safety as well due to their curiosity being not satisfied. The safety cost of all other models are below 35%."}, {"title": "5.3 Impact of Personality on Harm Elicitation", "content": "We show the defect rates across personality inventory traits in Table 5. We observe that user models with Impulsivity, Dissimulation and Inconsistency traits are able to elicit high defect rates from target LLMs. This demography needs the most protection and special attention as harms can compound their issues further."}, {"title": "5.4 Impact of Conversational Evaluation", "content": "We analyze the first harmful turn in conversations and the distribution of harms across five turns in Table 6. Most harms occur in the third turn, revealing that single-turn tests miss conversational nuances. However, significant defects in the first turn highlight inadequate LLM safety tuning, as harmful responses can occur without extended interaction."}, {"title": "5.5 Comparing safety with respect to adults", "content": "We compare model safety with child and adult user models in Table 7, observing significantly higher defect rates for child user models. Categories like Sexual, Regulated Goods/Services, and Illegal Activities show the highest defect rates for children, highlighting LLMs' unsuitability for both traditional sensitive categories like Sexual and child-specific ones like Regulated Goods/Services. Categories without child-specific nuances, such as LGBTQ, exhibit the smallest defect rate differences between adult and child safety."}, {"title": "6 Conclusion", "content": "LLMs have the potential to be an ally to children, but they can also cause harms. This work focuses on understanding the current landscape of child safety in interactions with LLMs. The work highlights following key observations:\n\u2022 We have high defect rates across all models - highlighting a general gap in safety tuning for child safety, regardless of size.\n\u2022 Even for safer models like Llama, we observe that the safety is achieved by refusals - which which can lead to continued unsafe behaviour.\n\u2022 Child personality plays a key role in safety, and the demographic needing most protection is also most susceptible to harm.\n\u2022 As compared to adults, children are at much more risk for existing harm categories as well as new categories targeting children.\nOverall, we conclude that the general focus on safety alignment may not ensure child safety and special attention is needed to make LLMs safe for children. Our work hopefully is a step in that direction and leads to more awareness and scrutiny of LLMs in this regard."}, {"title": "7 Limitations", "content": "The study is limited by its predefined taxonomy of 12 harm categories, potentially overlooking other relevant harms to children's safety. Its restriction to English narrows the applicability of findings across languages and cultures, where harmful content may differ. Additionally, the analysis is confined to five conversational turns due to computational constraints, potentially underestimating risks and missing harmful interactions that may arise in longer dialogues. Future research should address these limitations by incorporating broader harm categories, multilingual contexts, and extended conversation spans for a more accurate assessment of LLM safety.\nThe study simplifies the diversity of children's personalities and cultural backgrounds, overlooking individual differences and the complexity of their interactions with LLMs. It lacks longitudinal data on long-term effects and does not account for the role of parents or guardians in mitigating risks. Strategies to improve LLM safety, such as model alignment and prompt engineering, are not explored, and the findings are not validated with real children, limiting realism. The impact of name bias and bidirectional influences between users and LLMs (for example this work focuses on User influencing LLM responses but the oppositie pattern, LLM influencing User, can also exist) are also un-addressed. Furthermore, the study assumes a generalized prohibition for children, neglecting age-specific legal distinctions (for example energy drink is illegal for those under 16 whereas alcohol is illegal for those under 18 in the UK), which future research could refine for better ecological validity and applicability."}, {"title": "8 Ethical Considerations", "content": "The work and data can be highly offensive and sensitive to certain readers. We do provide appropriate warning at the top of the document to protect unsuspecting readers.\nAll the data created is synthetic (except the personalities and interests) and as such has no Personally Identifiable Information.\nThe work also carries the following ethical risks:\n1. We understand that there are potentially harmful applications of the harm taxonomy and the child user models we create. While our aim is to improve the safety of LLMs, this work can be used to undermine it as well - especially using the powerful child user models coupled with uncensored LLMs like Mistral-7B-Instruct-v0.3. Additionally, the study's reliance on a predefined taxonomy of harm categories may overlook emerging harms that are pertinent to children's safety. There is a responsibility to continuously update and refine harm taxonomies to ensure they reflect evolving risks and threats faced by children.\n2. The work only focuses on English which raises the risk of overexposure of this language. Furthermore, the exclusion of sophisticated techniques to test LLMs' responses (such as jailbreaking techniques or advanced tasks) could be seen as limiting the study's ability to uncover deeper vulnerabilities in LLM safety protocols. This limitation raises ethical questions about the comprehensiveness of the study and whether it adequately reflects real-world scenarios where children might encounter more sophisticated attempts to elicit harmful responses from LLMs.\n3. The work heavily relies on GPU computation and can have a negative impact on the environment. We tried to mitigate this issue by restricting the evaluation to only six LLMS as that was sufficient for answering the major research questions we had around child safety. Mainly whether it is an area of concern beyond standard safety and giving a working evaluation methodology to be used where necessary. In the spirit of reducing further impact, we also make all of the data generated as part of this study available to public to be used in future works."}]}