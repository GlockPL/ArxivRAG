{"title": "A Qualitative Study on Using ChatGPT for Software Security: Perception vs. Practicality", "authors": ["M. Mehdi Kholoosi", "M. Ali Babar", "Roland Croft"], "abstract": "Artificial Intelligence (AI) advancements have enabled the development of Large Language Models (LLMs) that can perform a variety of tasks with remarkable semantic understanding and accuracy. ChatGPT is one such LLM that has gained significant attention due to its impressive capabilities for assisting in various knowledge-intensive tasks. Due to the knowledge-intensive nature of engineering secure software, ChatGPT's assistance is expected to be explored for security-related tasks during the development/evolution of software. To gain an understanding of the potential of ChatGPT as an emerging technology for supporting software security, we adopted a two-fold approach. Initially, we performed an empirical study to analyse the perceptions of those who had explored the use of ChatGPT for security tasks and shared their views on Twitter. It was determined that security practitioners view ChatGPT as beneficial for various software security tasks, including vulnerability detection, information retrieval, and penetration testing. Secondly, we designed an experiment aimed at investigating the practicality of this technology when deployed as an oracle in real-world settings. In particular, we focused on vulnerability detection and qualitatively examined ChatGPT outputs for given prompts within this prominent software security task. Based on our analysis, responses from ChatGPT in this task are largely filled with generic security information and may not be appropriate for industry use. To prevent data leakage, we performed this analysis on a vulnerability dataset compiled after the OpenAI data cut-off date from real-world projects covering 40 distinct vulnerability types and 12 programming languages. We assert that the findings from this study would contribute to future research aimed at developing and evaluating LLMs dedicated to software security.", "sections": [{"title": "I. INTRODUCTION", "content": "Software security remains one of the most significant challenges facing modern software development. With the rise of agile development practices, a focus is being placed on developer-centric security assurance [1]. However, studies have shown that developers fail to keep up with the ever-growing list of required security knowledge and expertise [2], [3]. Secure coding alone requires developers to learn and use thousands of security-related patterns, practices, and tools. Some areas of software security, such as Software Vulnerability Management (SVM), need specialised knowledge and expertise for detecting, assessing, patching, and disclosing vulnerabilities [4]. Consequently, developers tend to leverage recommended systems and/or social media platforms to gain and use knowledge about software security [1].\nChatbots, i.e., conversational agents, provide effective interfaces for enhanced information retrieval and task assistance [5]. Natural language comprehension offers a potentially game-changing interface facilitating human and computer interaction. Natural language can greatly improve the efficiency in which we perform complicated tasks and use complex technologies by allowing us to operate them with simple natural language queries.\nLike many other domains, such as education and marketing [6], chatbots are gaining significant traction in the software development domain for obtaining knowledge/information related to specific software development tasks, such as API usages [7]. However, the use of chatbots for software security is still in its infancy stage [8] as current software security chatbots use simple rule-based technologies with limited capabilities [8]. This situation is expected to change as the advancements in Large Language Models (LLMs) enable much more powerful and capable chatbots, e.g., ChatGPT.\nChatGPT is an Artificial Intelligence (AI) chatbot developed by OpenAI created using GPT (Generative Pretrained Transformer) [9], a state-of-the-art LLM. These technical advancements allow ChatGPT to surpass traditional chatbots through its capability to provide answers and solutions to complex queries [10]. Training and fine-tuning an LLM with billions of parameters (e.g., 175 billion in GPT-3 [9] and 1,700 billion in GPT-4 [11]) presents considerable challenges due to the extensive computational resources required [12]. Since ChatGPT was released to the public, this limitation has been addressed, and many users were able to experience LLMs for the first time. Whilst ChatGPT was not specifically developed for the software domain, it still capture intrinsic knowledge that enables software security tasking to a certain degree [13]\u2013[15]. We believe exploring the capabilities and limits of this intrinsic knowledge is extremely beneficial for end-users as most organisations and practitioners lack the required resources to teach domain-specific knowledge to LLMs. Hence, we intend to investigate ChatGPT's intrinsic knowledge in software security to help enable the use of chatbots in this area in the future.\nWe conducted a multifaceted exploration of perceptions and practicality to investigate ChatGPT's potential for supporting software security tasks. First, we analysed public discussions"}, {"title": "II. RELATED WORK", "content": "Chatbots have been demonstrated to be suitable for software engineering tasks such as information retrieval [5] and API usage [16]. However, the use of chatbots for software security is limited. To date, the SKF-Chatbot (Secure Knowledge Framework) is one of the leading software security chatbots, which was developed by the OWASP Foundation to provide access to Software Vulnerability (SV) information via a chat interface. However, its capabilities are limited due to its lack of contextual understanding [8]. Recently, AI-based chatbots have been able to overcome these limitations via the LLMs that power them. ChatGPT has thus far demonstrated remarkable capabilities for semantic comprehension and providing tailored solutions for given tasks [10]."}, {"title": "B. Vulnerability Detection Using Transformer-based Language Models.", "content": "Prior studies have explored the use of transformer-based language models to identify software vulnerabilities using a variety of fine-tuning methodologies. For example, Thapa et al. [12] evaluated transformer-based language models (i.e., BERTBase, GPT-2 Base) against recurrent neural network (RNN)-based models (i.e., BiLSTM, B\u0130GRU) for vulnerability detection in software datasets featuring C/C++ source code. According to their findings, transformer-based language models outperformed RNN-based models on all evaluation metrics. In another study, Kalouptsoglou et al. [17] fine-tuned various transformer-based models (i.e., BERT variants, GPT-2, BART) and carried out a comparative analysis to determine which of these models is the most appropriate for vulnerability detection. Furthermore, Chan et al. [18] created a vulnerability detection model targeted at detecting vulnerabilities in incomplete code snippets. They leveraged common learning approaches such as fine-tuning on three pre-trained LLMs, namely CodeBERT, code-davinci-002, and text-davinci-003."}, {"title": "C. Vulnerability Detection Using ChatGPT.", "content": "Several research efforts have explored the efficacy of ChatGPT in vulnerability detection. Chen et al. [19] exclusively probed ChatGPT's capability in identifying smart contract vulnerabilities. Szab\u00f3 et al. [20] focused on identifying vulnerabilities associated with CWE-653 (Improper Isolation or Compartmentalization) using multiple GPT models. Ozturk et al. [21] assessed ChatGPT's effectiveness in detecting the top 10 OWASP vulnerability categories in web applications. Zhang et al., [22] through comprehensive testing on two Java and C/C++ vulnerability datasets, demonstrated that prompt-enhanced approaches could strengthen ChatGPT's vulnerability detection capability. In a recent study, Fu et al. [15] compared the capabilities of ChatGPT (i.e., GPT-3.5 and GPT-4) against three other LLMs (e.g., CodeBERT [23]) across various vulnerability tasks. The LLMs used in this study were fine-tuned explicitly for code-related tasks.\nThere are two main ways in which our study differs from prior studies:\n1) Prior research has primarily focused on assessing [12], [15], [17], [21] and enhancing ChatGPT's vulnerability detection performance through prompt engineering methodologies [22], [24], [25] or the integration of supplementary elements [26] within the LLM pipeline. We instead strive to focus more on insights through qualitative analysis of existing real-world uses and outputs. This shift aims to illuminate the disparity between practitioners' perceived value and the practicality of this technology in real-world settings. We provide further details in Section IV.\n2) Prior studies that utilised datasets from existing literature [15], [17], [19], [22] or open-source projects [12], [18], [20] ignored the possibility of data leakage (i.e., overlap between training and test data). This factor could artificially inflate the reported performance of AI models' (e.g., ChatGPT) [27]\u2013[29]. Consequently, we curated a high-quality vulnerability dataset tailored to our evaluation criteria from unseen data. Section III-B2 discusses this in more detail."}, {"title": "III. RESEARCH DESIGN", "content": "Initially, we present our research questions in Section III-A. This is followed by a detailed explanation of the data collection methodology in Section III-B. Lastly, Section III-C delves into the qualitative analysis processes. Figure 1 displays the overall workflow that we used to conduct this study."}, {"title": "A. Research Questions.", "content": "The following Research Questions (RQs) motivated our empirical study:\n\u2022 RQ1: What are users' perceptions of using ChatGPT for software security? We examine user demographics, topics, and types of discussions to determine how ChatGPT is being applied by users and its potential strengths and weaknesses.\n\u2022 RQ2: How practical are ChatGPT's outputs for the most desired software security task? We perform a qualitative analysis of ChatGPT's outputs for the most discussed topic of RQ1 to ascertain the practicality of this technology in real-world settings."}, {"title": "B. Data Collection.", "content": "1) Collecting Twitter Data and Curating a Sample: For the first data source, we selected the popular social media site, Twitter, for our investigation due to its active use by developers to discuss emerging technologies [30] and its use in former research for uncovering public sentiments [31], [32]. We used the Twitter API to collect English language tweets that matched the following keyword search query: ChatGPT AND (security OR secure OR insecure OR vulnerability OR vulnerabilities).\nWe collected tweets that were posted between December 1, 2022, and February 28, 2023, the first three months of ChatGPT's public release. In total, we collected 7716 tweets.\nTo enable qualitative analysis of our data, we curated a sample of posts for manual examination. To ensure that our data sample was of sufficiently high quality, we selected the 700 most liked tweets (approximately 10% of the collected tweets) from our dataset. Twitter likes are given by users who appreciate or agree with a tweet. Hence, highly liked tweets are influential posts that reflect the viewpoints of a larger audience and represent broader public sentiments. We label these tweets T1-T700.\n2) Collecting Vulnerability Data and Curating a Sample: To perform the required analysis for RQ2, we needed to test ChatGPT's capabilities in real-world settings. This purpose was achieved by using the National Vulnerability Database (NVD) [33], an established repository for SV management data maintained by the National Institute of Standards and Technology (NIST). This database holds an exhaustive array of information on identified SVs in the wild.\nAs we planned to do an in-depth manual analysis for RQ2, we decided to work with a subset of 70 SVs. This sample size was of statistical significance, providing 90% confidence and an error margin of 10% [34]. Previous researchers who needed to perform manual analysis over SVs also chose a similar sample size [27], [35]. It is imperative to note that we considered the issue of data leakage (i.e., contamination of training and test data) in our data selection for RQ2. It is well known that ChatGPT has been trained using a great deal of publicly available data. Since NVD is a public database, it is possible that ChatGPT encountered NVD data during its training phase. Thus, to prevent data leakage, we only included SVs in our dataset with a published date (i.e., the official date on which a vulnerability is added to the NVD) after Sep 2021 (the knowledge cutoff date for ChatGPT at the time of conducting this study). Any newly discovered SVs after this date have not been seen by ChatGPT during training.\nAdditionally, three other factors were considered when curating the random sample of 70 SVs. First, as the fixed code was required for validation purposes in our analysis, the SV report on NVD must be directly linked to the GitHub commit that addresses the SV (i.e., fixing commit). The References section of each SV in NVD usually includes a link to the fixing commit. The second consideration is that the fixing commit should be focused and not tangled (i.e., a commit that consists of multiple unrelated changes). The last considered factor is that the vulnerability should be contained within a single function. The last two factors are because we planned to perform all analysis in the following steps manually, and tracing complicated commits is an error-prone process.\nAfter incorporating the aforementioned factors into our SV selection criteria, we ended up with a dataset (containing 70 SVs) that covered 40 distinct CWEs (Common Weakness Enumeration) [36], and its affected products (i.e., products and platforms considered vulnerable) were written in 12 different programming languages. By utilising the provided fixing commits, we carefully collected the entire vulnerable and fixed (i.e., patched) versions of functions from the relevant GitHub repositories. The average number of lines of code among our 70 collected vulnerable functions was 32.\n3) Collecting ChatGPT Outputs: In this step, ChatGPT was fed the 70 collected vulnerable functions as input and asked about potential SVs within them. The GPT-4 model has demonstrated superior performance compared to the GPT-3 model in various benchmarks [11]. Considering the importance of delivering high-quality and up-to-date research insights in the critical domain of software security, all ChatGPT-related analysis in this paper were performed using the GPT-4 model. Prior research has shown that tailored prompts are essential to taking full advantage of LLM's capabilities in various software engineering tasks [22], [37], [38]. Following their prompt designs, for each of the SVs in our dataset, we utilised the following prompt template in our conversation with ChatGPT and carefully saved its outputs for further analysis. As ChatGPT remembers information from previous inputs, each query was made in a new chat."}, {"title": "C. Qualitative Analysis.", "content": "1) Qualitative Analysis of Sampled Tweets: To understand user perceptions of ChatGPT for software security (RQ1), we qualitatively analysed our sample of 700 tweets using thematic analysis [39]. This was an iterative and reflective process that we conducted over several phases.\nWe started by immersing ourselves in the data by carefully reading each tweet and all surrounding data: other tweets in the thread, replies, attached media, linked articles, and associated user profiles. The first and third authors conducted this pilot data labelling process collaboratively for 30 tweets, and it typically took 15 to 20 minutes per tweet. During this process, we posed potential characteristics of the discussion that address RQ1: user demographics, topics and types of discussion. To determine the values of these characteristics, we performed open coding [39] to classify the common discussion points. We collaboratively identified keypoints as short summaries for each tweet. We then revised these into themes, which provide a one or two word description of the common topics. We conducted a second round of pilot data labeling on an additional batch of 30 randomly selected tweets to ensure thorough coverage of potential themes. In this iterative process, no new themes emerged, indicating that thematic saturation had been reached.\nOnce we were satisfied with the identified themes, two authors independently analysed and categorised the remaining 640 tweets in the sample. They then compared the annotated data to ensure data coding was conducted consistently. The two raters achieved a Cohen Kappa value of 0.813 [40], which implies a high level of agreement. Any remaining disagreements between the first and third authors were resolved through discussion with the second author. During the labelling process, we excluded any tweets from our sample that were unrelated (434 tweets) to the use of ChatGPT for software security due to falsely matched keywords, e.g., tweets discussing the impact of ChatGPT on job 'security'. Ultimately, 266 tweets were included in the sample.\nWe also wanted to gain an understanding of general user sentiments toward ChatGPT's use for software security. E.g., whether the author of a tweet had a positive experience using it or was optimistic about its potential applications. We experimented with using the NLTK sentiment analysis tool [41] to classify our data automatically. However, we found this tool to be inaccurate from a manual review of 30 sample tweets. This is because security keywords such as vulnerability or security were often falsely interpreted as indicating negative sentiments. Furthermore, a tweet may have an overall positive sentiment but a negative opinion of ChatGPT, and vice versa. For example, \"This article does a really good job of highlighting the weakness of ChatGPT.\u201d Hence, we manually classified user sentiments of ChatGPT during qualitative analysis. For each tweet, we labelled the sentiment as either positive, negative, or neutral based on our interpretation of the user's opinion or experience of ChatGPT capabilities for software security. We reserved the neutral sentiment label for tweets that discussed positive and negative points or were ambiguous to identify. The nature of the sentiment of a tweet depends on the type of discussion (e.g., experiences vs. speculation). Hence, we consider the types of discussion whilst performing sentiment analysis, which we elaborate further on in Section IV-A.\n2) Qualitative Analysis of Sampled Vulnerabilities: To thoroughly investigate the practicality of ChatGPT in the vulnerability detection task (RQ2), we first needed to assess ChatGPT's efficacy in this task. This is because we were interested in exploring how characteristics (e.g., tone) of outputs differ when ChatGPT is accurate versus instances of inaccurate outputs. To this end, we manually analysed the output of all 70 queries and provided an assessment as to whether the target SV was detected. This manual assessment was performed independently by the first and third authors of this paper who had a cumulative eight years of both academic and industrial software security experience.\nFor each SV within the dataset, we used the fixing commit as a baseline for our comparison and employed techniques from prior researchers [27], [42] to manually analyse the code changes related to a SV. These techniques are explained step by step as follows. In the first step, we concentrated on the SV summary from the NVD Description section. Moreover, we focused on the information available in the source code (i.e., fixing commit). Aside from code changes, the title and message of the fixing commit were also examined, as they usually provide valuable information regarding the functional aspects of the code and the context in which it was altered. Having thoroughly understood the SV, we assessed its generated ChatGPT output. Accordingly, each output was labelled as either not found or found. Most often, ChatGPT provided a list of potential vulnerabilities within the input code, with the particular SV we were interested in being mentioned in one of the bullet points. These outputs were classified as found. If the output did not contain information about the vulnerability we sought, it would be marked as not found. This is even if the other potential vulnerabilities ChatGPT listed appeared genuine. This decision was because we wanted to maximise the reliability of our manual analysis and we did not have the ground truth (i.e., fixing commit) for comparison for the other detected vulnerabilities.\n3) Qualitative Analysis of ChatGPT Outputs: In response to RQ2, we conducted a thematic analysis [39] of ChatGPT's outputs regarding the vulnerability detection task. In particular, our primary goal was to demonstrate the breadth and depth of information that a conversational agent such as ChatGPT can offer and examine the nuances of its presentation. For the"}, {"title": "IV. RESULTS", "content": "We firstly wanted to determine the occupations of users of ChatGPT for software security. Such insights provide essential context for the source of discussions and sentiments in our data. Furthermore, we reveal the invested stakeholders of AI-based chatbot solutions for software security. We analysed user profile descriptions of the tweet authors (n=202) and identified any occupation details that were listed. We then categorised user occupations using open coding as described in Section III-C1. The population of authors was analysed as a set not containing duplicates. This means that a user was only considered once in the population if they authored multiple tweets. We identified five main occupations in our sample data, which we describe below:\n\u2022 Security Practitioners are individuals (n=78) that conduct software security or cybersecurity jobs or tasks. This includes occupations such as security analysts, hackers, and vulnerability researchers.\n\u2022 Security Companies are the Twitter profiles of organisations (n=27) that provide software security solutions and services.\n\u2022 Software Practitioners are individuals (n=19) that perform software development or associated tasks for their occupation.\n\u2022 Blogs are the profiles for online sites or organisations (n=46) that provide news outlets or blogging services.\n\u2022 Other defines a collection of occupations which had too few samples to form individual categories. These include academics (n=6) and government employees (n=5). User profiles that contained insufficient or vague descriptions of occupation (n=21) were also placed in the Other category.\nTable I displays the occupations of Twitter users in our sample. Given that our dataset is comprised of software security discussions, security practitioners are expected to form the largest user demographic (39%). However, this provides evidence that security practitioners are interested in the potential applications of AI chatbots to their domain, which is supported by the noticeable demographic (13%) of commercial security companies that are also discussing the viability of such tools. Furthermore, the high percentage of security practitioners in our data indicates that our Twitter sample contains reliable information from users with background domain knowledge.\nWe next wanted to examine the topics of discussion in our sample Twitter data to understand the areas of software security for which ChatGPT was being used. These insights were expected to help us determine the potential strengths and weaknesses of AI chatbots for different software security tasks. From our qualitative analysis of tweets described in Section III-C1, we discovered five main software security categories in which users utilise ChatGPT, which are also displayed in Table I alongside their frequency. We go into detail for each category below. We provide quoted text from the sample tweets that we have analysed. However, many tweets provide associated media to provide further context to the discussion, which we cannot capture within our paper.\nVulnerability Detection. Vulnerability detection is one of the most important components of SV management. Consequently, it is the most frequent discussion category (29%) within our sample of tweets. Many users were impressed that ChatGPT has even basic capabilities for vulnerability detection due to the difficulty of this task:\n\"Experiment time. I fed ChatGPT the Damn Vulnerable Ethereum Smart Contract Migration code in it's entirety. It pointed to four potential vulnerabilities!!!\" (T95)\nWhilst many users demonstrated that ChatGPT could successfully identify vulnerabilities in simple code snippets, some users were sceptical about the validity of its use in real-world scenarios.\n\"I just went three rounds with a \"security researcher\" (who was chatGPT in disguise) making reports on our smart contracts. Plausible text and impacts wrapped in magnificent misunderstandings of the basics.\" (T49)\nChatGPT produces plausible-sounding answers even for tasks which it cannot necessarily complete successfully. Vulnerability detection requires substantial expertise, so it is hard for many users to verify responses for these tasks. The accuracy of ChatGPT may also be limited as it was not trained to perform these tasks.\n\"Don't use ChatGPT for security code review. It's not meant to be used that way, it doesn't really work (although you might be fooled into thinking it does)\" (T80)\nHowever, despite this lack of trust, users still found the vulnerability analysis capabilities helpful in speeding up code review.\n\"I had ChatGPT take code and then audit for security. Would I deploy the resulting code? No. But two weeks just became minutes.\" (T150)"}, {"title": "RQ1 Findings:", "content": "\u2022 Vulnerability detection was the most discussed software security task among practitioners testing ChatGPT.\n\u2022 The overall sentiment of the discussions among users was positive.\n\u2022 In various security tasks, credibility and practicality of generated information were common concerns.\n\u2022 Sentiments were primarily positive for simple proof-of-concept use cases."}, {"title": "B. RQ2: practicality of ChatGPT's outputs when utilised for the most desired software security task", "content": "Using the qualitative analysis described in Section III-C2, we first recorded our observations regarding ChatGPT's vulnerability detection accuracy for each SV. A total of 43 out of 70 (61.42%) vulnerabilities were correctly identified by ChatGPT. This efficacy was achieved when ChatGPT was presented with vulnerable code snippets of moderate complexity\u2014 specifically, those that are relatively long (average of 32 lines of code per snippet) with focused fixing commits and not overly intricate.\nWe answer RQ2 in the following sections by describing the types of information ChatGPT generated and how that information is presented for the vulnerability detection task. This investigation can provide a basis for future enhancements in the design and functionality of tailored conversational agents for software security. We provide quoted text from the analysed ChatGPT outputs. Figure 3 displays the overall structure of the provided information in ChatGPT outputs.\n1) Assumption: It has been observed that ChatGPT occasionally makes assumptions about tertiary code when analysing the source code inputted. This kind of information was presented tentatively because they all rest on assumptions.\n\"For instance, we're making assumptions about functions like compose_sadb_supported() and pfkey_broadcast() which could have their own set of vulnerabilities.\" (Output #30)\n2) Description: There was a constant presence of descriptions in the ChatGPT responses. According to our thematic analysis, the descriptions can be divided into three major categories, Supplied Code, Vulnerability, and Mitigation.\n\u2022 Supplied Code: ChatGPT frequently begins its responses with a description of the supplied code. For instance, the functionality of the code or its programming language.\n\"This code snippet is written in Java and seems to be responsible for handling requests to trigger a configuration reload through a security token system.\" (Output #70)\n\u2022 Vulnerability: A large portion of each ChatGPT response is devoted to describing identified vulnerabilities. In addition to classifying these descriptions under the Vulnerability category, we further divided them into two themes for a more granular analysis: \"Theoretical and Practical Implications\". This is because we sought to distinguish between when ChatGPT delves into abstract security implications and when it explains the actionable aspects of potential exploits.\n\"Theoretical Implication\" example:\n\"... 4. Exposure of Sensitive Information: The password recovery link may contain sensitive information (like a reset token). The application must make sure to generate a unique and unpredictable token for each password recovery request. \" (Output #14)\n\"Practical Implication\" example:\n\"... There are a few places where objects are dereferenced without first ensuring they are not null. This could potentially lead to a segmentation fault and crash the program. (Output #26)\nBased on our observations, out of 43 cases where ChatGPT identified the specific vulnerability of interest, 38 instances (88%) were classified under the \"Theoretical Implications\" theme, while only 5 instances (12%) were categorised under \"Practical Implications\".\n\u2022 Mitigation: ChatGPT infrequently provided specific information on how to address the particular vulnerability identified. This category contained two themes: \"List of Resolutions\" and \"Strategy\"."}, {"title": "RQ2 Findings:", "content": "\u2022 Even when ChatGPT correctly pinpoints the vulnerability of interest, it predominantly focuses on abstract rather than practical security implications.\n\u2022 Although ChatGPT responses appear comprehensive, they are heavily populated with generic security information (e.g., Description, Guideline).\n\u2022 The severity of the vulnerability appears to be directly related to the tone of the responses.\n\u2022 ChatGPT generally adopted a tentative tone to present the information."}, {"title": "V. DISCUSSION", "content": "In RQ1, our investigation has shown that software and security practitioners enthusiastically experiment with and share the pros/cons of using ChatGPT for assistance in various software security tasks, such as vulnerability detection and security information retrieval. However, the state of discussions mostly remains in example proof of concept use, whereas only a few cases of applied use have been discussed. Current security practitioners appear to think that ChatGPT is inaccurate as a standalone solution. Hence, ChatGPT may be better suited to secondary tasks such as vulnerability report writing. For instance, users can draft comprehensive and detailed vulnerability reports by providing context-specific prompts and formatting instructions. Nevertheless, security professionals should review and validate the contents to ensure accuracy and completeness.\nIn the second phase of our study (RQ2), we treated ChatGPT as an oracle for the vulnerability detection task. In our data sample, it achieved a detection accuracy of 61%, which is far from ideal. Furthermore, after qualitatively examining the generated responses, we determined the characteristics (i.e., type and presentation style of information) of this chatbot and found that it is unable to provide reliable information for this task. Our analysis revealed that ChatGPT responses suffer from two specific quality issues. The most prevalent issue we observed was that all responses (70 instances) contained generic security information, as indicated in the \u201cTheoretical Implication\" theme and the three themes under the \"Security Guideline\" information type. Throughout these themes, we found an abundance of unneeded information, resulting in a lack of clarity in the responses. This issue persisted even in cases where ChatGPT correctly identified the vulnerability. Another noteworthy issue we found was that ChatGPT regularly lacks assertiveness when delivering responses. Based on our analysis, the majority of responses were given with a tentative tone, as shown in Figure 3, and a consistent request for additional contextual information. We believe both of these quality issues may undermine the practicality of ChatGPT for vulnerability detection within industry settings. This is because when a fault localisation tool is not transparent about its limitations, developers may inadvertently spend valuable time and effort verifying potentially inaccurate outputs [44]. However, in contrast to existing weakness detection tools (e.g., static code analyser), conversational agents such as"}, {"title": "A. Implications of Our Study", "content": "Programmers. Understanding the nuances of responses (RQ2) generated by ChatGPT can offer valuable benefits to programmers who leverage this technology for vulnerability detection. Notably, they may better assess the legitimacy of the generated information and make informed decisions about whether to trust and adopt the security suggestions. Furthermore, by understanding ChatGPT's strengths and limitations, programmers can better integrate this technology into their development workflows, using it as an adjunct to other weakness detection tools.\nAI practitioners. The investigation into ChatGPT's communication of software security information (RQ2) has explicit implications for AI researchers and developers. Insights of this nature may assist them in fine-tuning LLM-based chatbots for specific software security tasks. Specifically, they must strive to create security-tailored LLMs capable of conveying security information to users in a manner that is more straightforward, actionable, and less susceptible to misinterpretation. Such models are likely to perform better and be more practical when applied in real-world security settings.\nSecurity researchers. We have identified some promising research directions for researchers. Our analysis in RQ1 unveiled that there is a lot of speculation (discussions in the Vulnerability Exploits category) regarding the potential misuse of ChatGPT for malicious purposes. Researchers need to identify whether technologies like ChatGPT can enable malicious hacking so that countermeasures can be applied to prevent using these technologies for such purposes. Additionally, our analysis in RQ2 demonstrated that ChatGPT alters its language tone when confronted with vulnerabilities of varying severity scores. Further research is required to thoroughly understand the capability of LLMs in estimating vulnerability severity. This capability is imperative in vulnerability prioritisation, where precise severity assessment is essential for effective mitigation strategies."}, {"title": "VI. THREATS TO VALIDITY", "content": "Construct validity: Our manual analysis may be subject to bias or inaccuracies, as is the case for most qualitative research. We have attempted to minimise such threats to validity by ensuring that two annotators independently performed the required manual analysis.\nInternal Validity: We acknowledge that the criteria we used for selecting vulnerability data could have influenced the outcome of our study. As explained in Section III-B2, our intention to manually trace code changes meant we could not trace tangled fixing commits. As a result, we decided to limit the number of line changes within fixing commits to 10 lines. This approach ensured we gathered code snippets that were neither simplistic nor too complex.\nExternal Validity: Qualitative analysis enables us to examine our data in depth, but consequently, we can only engage with a small sample of the total dataset. We cannot generalise beyond the sampled tweets and vulnerabilities due to the effort required to examine the data manually. We obtained our tweet sample based on the most liked and influential tweets, so we expect our sample represents a larger audience.\nFurthermore, public opinions may be inaccurate or biased for emerging technologies, as users may be overly excited or not have had enough time to evaluate their use properly. Despite our desire to extend the three-month period for collecting tweets, we were unable to do so since Twitter's academia API has been deprecated following recent changes at the company.\nIt is also possible that our findings might not generalise to future ChatGPT versions enriched with updated data. However, we conducted this study using a single ChatGPT version with the same knowledge cut-off date (Sep 2021) for consistency."}, {"title": "VII. CONCLUSION", "content": "We have conducted an empirical investigation to understand the potential use of ChatGPT for software security. Based on examining public discussions regarding this LLM, we found that practitioners were often optimistic about its utility for various security tasks. Our systematic evaluation further demonstrated that ChatGPT can identify vulnerabilities with roughly 61% accuracy. Furthermore, we qualitatively demonstrated the versatility and granularity of information that ChatGPT can offer in the vulnerability detection task. The results revealed that ChatGPT outputs in this task are often too generic and ambiguous to be of practical use at the industry level. A tailored security LLM could enhance this capability, enabling practitioners to leverage the abundance of knowledge embedded in LLMs."}, {"title": "VIII. DATA AVAILABILITY", "content": "We have made all our artefacts of this study available via a reproduction package [45]."}]}