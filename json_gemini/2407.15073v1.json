{"title": "Multi-Agent Causal Discovery Using Large Language Models", "authors": ["Hao Duong Le", "Xin Xia", "Zhang Chen"], "abstract": "Large Language Models (LLMs) have demonstrated significant potential in causal discovery tasks by utilizing their vast expert knowledge from extensive text corpora. However, the multi-agent capabilities of LLMs in causal discovery remain underexplored. This paper introduces a general framework to investigate this potential. The first is the Meta Agents Model, which relies exclusively on reasoning and discussions among LLM agents to conduct causal discovery. The second is the Coding Agents Model, which leverages the agents' ability to plan, write, and execute code, utilizing advanced statistical libraries for causal discovery. The third is the Hybrid Model, which integrates both the Meta Agents Model and Coding Agents Model approaches, combining the statistical analysis and reasoning skills of multiple agents. Our proposed framework shows promising results by effectively utilizing LLMs' expert knowledge, reasoning capabilities, multi-agent cooperation, and statistical causal methods. By exploring the multi-agent potential of LLMs, we aim to establish a foundation for further research in utilizing LLMs multi-agent for solving causal-related problems.", "sections": [{"title": "1 Introduction", "content": "Understanding causal relationships is crucial across scientific fields. While statistical causal inference is widely used, it heavily relies on assumed causal graphs. To address this limitation, data-driven methods have evolved, leading to statistical causal discovery (SCD) approaches and the creation of datasets for evaluation. Despite advancements in SCD algorithms, data-driven causal graphs without domain knowledge can be inaccurate. This inaccuracy is often due to a mismatch between SCD algorithm assumptions and real-world phenomena (Reisach et al. [2021]). Incorporating expert knowledge can mitigate this issue, but it is costly.\nThe advent of Large Language Models (LLMs), trained on vast amounts of data, has enabled them to acquire extensive knowledge, from common sense to specific domains such as math and science. Recent studies suggest that complex behaviors, such as writing code, generating long stories, and even reasoning capabilities, can emerge from large-scale training (Wei et al. [2023]; Rozi\u00e8re et al. [2024]; Zhao et al. [2023b]; Yao et al. [2023a]). LLMs present a promising alternative for obtaining expert knowledge more accessible and affordable. Recent research (K\u0131c\u0131man et al. [2023];Choi et al. [2022]; Long et al. [2024]) has attempted to leverage these capabilities for causal discovery based on metadata and knowledge-based reasoning, akin to human domain experts."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Prompt Engineering", "content": "Several advanced techniques for leveraging large language models (LLMs) have been identified. Zero-shot prompting, introduced by Radford et al. [2019], guides LLMs to perform novel tasks using carefully crafted prompts without the need for training data, allowing the model to leverage its existing knowledge to generate predictions. Few-shot prompting, as described by Brown et al. [2020], enhances model performance by providing a few input-output examples, though it requires more tokens and careful example selection to mitigate biases. For reasoning and logic, Wei et al. [2023] introduced Chain-of-Thought (CoT) prompting, which guides LLMs through step-by-step reasoning processes, significantly improving accuracy in complex tasks such as math and commonsense reasoning. Building on this, Wang et al. [2023] proposed self-consistency, a strategy that generates diverse reasoning chains and identifies the most consistent final answer, further enhancing accuracy. Additionally, Yao et al. [2023b] developed ReAct, enabling LLMs to generate reasoning traces and task-specific actions concurrently, thereby improving performance in question answering, fact verification, and interactive decision-making by enhancing the synergy between reasoning and action. In this work, due to deliberate thinking such as analyzing the problem or code writing, we integrate ReAct as one of our main prompting techniques and also one-shot prompting when the agents are required to output in a certain way."}, {"title": "2.2 LLMs' Agentic Workflow", "content": "A general LLM agent framework consists of core components: user request, agent/brain, planning, memory, and tools. The agent/brain acts as the main coordinator, activated by a prompt template. It can be profiled with specific details to define its role, using handcrafted, LLM-generated, or data-driven strategies. Planning employs techniques like Chain of Thought and Tree of Thoughts, and for complex tasks, feedback mechanisms like ReAct Yao et al. [2023b] and Reflexion Shinn et al. [2023] refine plans based on past actions and observations. Memory stores the agent's logs, with short-term memory for the current context and long-term memory for past behaviors. Hybrid memory combines both to enhance reasoning and experience accumulation. Tools enable interaction with external environments, such as APIs and code interpreters. Frameworks like MRKL Karpas et al. [2022], Toolformer Schick et al. [2023], Function Calling OpenAI [2024], and HuggingGPT Shen et al. [2023] integrate tools to solve tasks effectively.\nHowever, for more complex problems where a single LLM agent may struggle, LLM-MA (multi-agent) systems excel. Current LLM-MA systems primarily employ three communication paradigms: Cooperative, Competitive, and Debating. In the Cooperative paradigm, agents collaborate towards a shared goal, typically exchanging information to enhance a collective solution Qian et al. [2023], Chen et al. [2024b]. In the Competitive paradigm, agents work towards their own goals, which might conflict with those of other agents Zhao et al. [2023a]. The Debating paradigm involves agents engaging in argumentative interactions, where they present and defend their viewpoints or solutions while critiquing those of others. This approach is ideal for reaching a consensus or a more refined solution (Li et al. [2023]; Liang et al. [2023]; Xiong et al. [2023]). In this work, the debating paradigm will be implemented, as the nature of causal discovery problems requires diverse and potentially conflicting opinions to approach the truth."}, {"title": "2.3 Statistical and LLM-based Causal Methods", "content": "Traditional methods of statistical causal inference often depend heavily on assumed causal graphs to identify and measure causal impacts. To overcome this limitation, data-driven algorithmic approaches have been developed into statistical causal discovery (SCD) methods, encompassing both non-parametric (e.g., Spirtes et al. [2000]; Chickering [2002]; Silander and Myllym\u00e4ki [2006]; Yuan and Malone [2013]; Huang et al. [2018]; Xie et al. [2020]) and semi-parametric (e.g., Shimizu et al. [2006]; Hoyer et al. [2009]; Shimizu et al. [2011]; Rolland et al. [2022]; Tu et al. [2022]) techniques. Many SCD algorithms can be systematically augmented with background knowledge and have accessible software packages. For example, the non-parametric and constraint-based Peter-Clerk (PC) algorithm (Spirtes et al. [2000]) in \u201ccausal-learn\" integrates background knowledge of mandatory or forbidden directed edges. \u201cCausal-learn\" also includes the Exact Search algorithm (Shimizu et al. [2011]; Yuan and Malone [2013] ), a non-parametric and score-based SCD method that can incorporate background knowledge in the form of a super structure matrix of forbidden directed edges. Furthermore, the semi-parametric DirectLiNGAM (Shimizu et al. [2011]) algorithm can use prior knowledge of causal order (Inazumi et al. [2010]) in the \u201cLiNGAM\" project (Ikeuchi et al. [2023]).\nIn the context of knowledge-driven approaches using large language models (LLMs), applying LLMs for causal inference is relatively new. There have been a few significant efforts to use LLMs for causal inference among variables by merely prompting with the variable names, without going through the traditional SCD process with benchmark datasets (K\u0131c\u0131man et al. [2023]; Ze\u010devi\u0107 et al. [2023]). Jiralerspong et al. [2024] even uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries to have a higher efficiency. Researchers have focused on using LLMs to minimize error metrics in score-based SCD results (Ban et al. [2023]) and to improve causal effect estimation by determining the causal order from triplet subgraphs derived from non-parametric SCD results (Vashishtha et al. [2023]). Another approach focuses on constructing background knowledge based on the response probability of the LLM to reflect the credibility of its decisions with SCP and details achieving statistical validity and natural interpretation in semi-parametric SCD methods like LINGAM using causal coefficients and bootstrap probabilities (Takayama et al. [2024])."}, {"title": "3 MAC: Multi-Agent Causality Framework", "content": ""}, {"title": "3.1 Multi-Agent Causality Modules", "content": ""}, {"title": "3.1.1 Meta-Debate Module", "content": "The Meta-Debate Module is an advanced system comprising three intelligent agents: two causal debaters (an affirmative one and a negative one) and one causal judge. This structure emulates the dynamic and rigorous nature of human debate, specifically within the realm of causal discovery. The design of this debating module is meticulously crafted to ensure a thorough examination of critical elements in causal discovery, such as understanding the temporal order necessary to establish cause-and-effect relationships and identifying potential confounding variables that could distort perceived relationships between primary variables. Each side engages in active disagreement by presenting different opinions and viewpoints, fostering a comprehensive and robust debate. Additionally, each agent within Meta-Debate Module utilizes the ReAct prompting technique. This technique integrates the ability to dynamically formulate, modify, and refine action plans based on new information or insights gained during the debate. This integration allows the agents to engage in more sophisticated and adaptive reasoning processes, closely mimicking human-like debate and decision-making 3.\nThe debating process begins with a meta-question. Initially, the causal affirmative side presents its answer along with the supporting rationale. Subsequently, the causal negative side offers diverse or conflicting perspectives by providing alternative viewpoints on the same question. The causal judge then evaluates the responses from both sides, determining either a winner or identifying the need for additional clarification. If further information is required, the causal judge poses specific follow-up questions, prompting the debaters to clarify and elaborate on their initial propositions. The causal judge reaches a final verdict once all relevant information has been thoroughly examined.\nFor meta-questions, questions might address aspects such as the appropriate algorithm for a particular problem, the causal relationship between two variables, or a step-by-step approach for solving a causal problem, as illustrated in Figure 1. The affirmative side initiates the debating process based"}, {"title": "3.1.2 Debate-Coding Module", "content": "The Debate-Coding Module leverages statistical algorithms to achieve precise causal discovery through a structured two-phase process. This group consists of four agents, divided into two phases: debating the algorithm and executing the algorithm.\nIn the initial phase, three agents engage in a debate format, similar to the workflow of Meta-Debate module at Section 3.1.1. However, the difference is that the affirmative and negative agent are pre-prompts the information with 3-5 statistical causal algorithms. Additionally, the meta-question posed to these agents is specifically curated to determine which algorithm should be used given the metadata, which includes the description and structure of the data. After the debate, the output is the most suitable algorithm for the particular dataset and question. Compared to the Meta-Debate Module, an additional step in this process is that the agent participating in the debate (whether affirmative or negative) will provide a step-by-step plan for implementing the selected algorithm.\nIn the second phase, the causal coding executor receives the plan from the previous phase along with the observational data. The causal coding executor is responsible for writing, executing, and debugging the code. It pre-prompts the functions and provides parameters within a specific Python library\u00b9 based on the algorithm selected by the debaters in the initial stage. This pre-prompting is crucial because LLMs can call functions from their training dataset, which may be outdated or incorrect, leading to errors and excessive debugging (details of prompting design can be found in the"}, {"title": "3.2 Implementations of MAC", "content": "In this section, we will elaborate on the detailed implementation of the three models regarding their input and basic workflow."}, {"title": "3.2.1 Meta Agents Model", "content": "The algorithm for the Meta Agents Model aims to identify direct causal relationships between variables in a dataset. It starts with the input data  \ud835\udc4b = [\ud835\udc65\u2081, ..., \ud835\udc65\ud835\udc5b] . The output of the algorithm is a graph  \ud835\udc3a , with its edges  \ud835\udc3a\ud835\udc56\ud835\udc57 , where  \ud835\udc56  and  \ud835\udc57  are indices in the set of variables  \ud835\udc5b\nThe algorithm proceeds by iterating through each variable  \ud835\udc56  from 1 to the number of variables in  \ud835\udc4b . For each  \ud835\udc56 , it checks all  \ud835\udc57  values less than  \ud835\udc56  (i.e.,  \ud835\udc57  ranges from 1 to  \ud835\udc56  \u2013 1). It queries the Meta-Debate Module to check if there is a direct causal relationship from  \ud835\udc4b [\ud835\udc56]  to  \ud835\udc4b [\ud835\udc57]  and stores the result in  \ud835\udc3a\ud835\udc56\ud835\udc57 . Then, it checks all  \ud835\udc57  values greater than  \ud835\udc56  (i.e.,  \ud835\udc57  ranges from  \ud835\udc56  + 1 to the number of labels in  \ud835\udc4b ). Those queries are meta-questions that use the Meta-Debate Module function to check if there is a direct causal relationship from  \ud835\udc4b [\ud835\udc56]  to  \ud835\udc4b [\ud835\udc57]  and store the result in  \ud835\udc3a\ud835\udc56\ud835\udc57 . The algorithm concludes by returning the constructed graph  \ud835\udc3a\ud835\udc56\ud835\udc57 . A detail of the function Meta-Debate Module has been described in section 3.1.1"}, {"title": "3.2.2 Coding Agents Model", "content": "The inputs to the algorithm are a meta-question  \ud835\udc44\ud835\udc4b  and observational data  \ud835\udc42\ud835\udc4b , and the output is the construction of a causal graph  \ud835\udc3a\ud835\udc56\ud835\udc57 . The algorithm for the Coding Agents Model aims to determine causal relationships by first generating a causal analysis plan using a debate format. After yielding the plan, it will then be executed with the observational data using a causal code executor. The detailed implementation of the function causal code executor can also refer to the section 3.1.2"}, {"title": "3.2.3 Hybrid Model", "content": "There are two combinations of Hybrid Group: Coding-Debating Hybrid and Debating-Coding Hybrid. They are fundamentally identical to the Meta Agents Model and Coding Agents Model in terms of their internal architectures, algorithms, and outputs. The difference lies in their inputs.\nFor Coding-Debating Hybrid, the initial result will be obtained from the Coding Agents Model of Algorithm 2 given the input of a meta-question and observational data. The graph and the proposed algorithm for achieving the final graph will also be extracted and fed into the Meta Agents Model 1. For example, the proposed algorithm in the Coding Agents Model is PC, and the initial graph \u011c, when both of them were input to the Meta Agents Model, the query would change slightly at line 8 and 11 in the algorithm 1 which is illustrated by the box below. The final return output is a matrix representation of a causal graph.\nFor the Debating-Coding Hybrid, the initial graph comes from the Meta Agents Model of algorithm 1. This output is then considered as prior knowledge or background knowledge. It is aggregated with the meta-question, and input to the Coding Agent Model of algorithm 2. It will select a suitable statistical causal discovery algorithm and plan. This plan is then given to the code executor to implement, resulting in a matrix representation of a causal graph."}, {"title": "4 Experiment and Results", "content": ""}, {"title": "4.1 Experimental Setup", "content": "We use GPT-3.5-turbo API for most of our experiments with the temperature set at 0. We experiment on three different datasets that adopted from the work of Takayama et al. [2024]\n1. Auto MPG dataQuinlan [1993] This dataset consists of the variables around the fuel consumption of cars. With five variables: \u201cWeight\u201d, \u201cDisplacement\u201d, \u201cHorsepower\u201d, \u201cAcceleration\" and \"Mpg\u201d(miles per gallon)"}, {"title": "4.2 Metrics", "content": "For the evaluation metrics, we assess the adjacency matrix obtained from LLMs or a code executor using structural hamming distance (SHD), false positive rate (FPR), false negative rate (FNR), precision, and F1 score, following the methodology of Takayama et al. [2024]. Additionally, we use the Normalized Hamming Distance (NHD) as described by K\u0131c\u0131man et al. [2023]."}, {"title": "4.3 Results", "content": "This experiment uses several metrics to evaluate different methods for constructing graph adjacency matrices from datasets with continuous variables. We compare with some classical statistical causal discovery algorithms such as PC, Exact Search, and DirectLiNGAM. We also compare with LLMs-based statistical methods from the work of Takayama et al. [2024] (run on the GPT-3.5-turbo first pattern): Peter-Clark (PC) LLM-KBCI, Exact Search (ES) LLM-KBCI, and DirectLiNGAM LLM-KBCI. Lastly, we also compare with single-agent zero-shot Chain-of-Thought with GPT-3.5-turbo and GPT-40 (checkpoint on 13 May 2024).\nFor the Auto MPG data, the Coding Agents Model achieves the best performance in terms of SHD (4), NHD (0.48), FNR (0), Precision (1), Recall (0.357), and F1 score (0.526). The Meta Agents Model also performs well with an SHD of 5, NHD of 0.2, FNR of 0.4, FPR of 0.15, Precision of 0.5, Recall of 0.6, and F1 score of 0.545. The traditional methods and other advanced methods show higher SHD values, indicating lower structural accuracy. For example, PC, DirectLiNGAM, and Single-agent zero-shot prompting (GPT-40) all have SHD values of 8, demonstrating less accurate structural learning compared to the Coding agents.\nFor the DWD climate data, Meta Agents Model outperforms other methods with the lowest SHD of 5, NHD of 0.194, FNR of 0.833, FPR of 0.2, Precision of 0.333, Recall of 0.166, and F1 score of 0.222. This indicates strong structural accuracy and a balanced performance in terms of precision and recall. The traditional methods such as PC and DirectLiNGAM have higher SHD values (9 and 10, respectively) and lower overall performance in other metrics. The Single-agent zero-shot prompting methods, including GPT-40, show higher SHD values (10 and 11, respectively) and less competitive performance across the other metrics.\nFor Sachs' protein data, Single-agent zero-shot prompting (GPT-40) stands out with an SHD of 18, the lowest among all methods, indicating the highest structural accuracy. This method also excels in the False Negative Rate (FNR), achieving the best result of 0.098, which signifies a minimal number of missed detections. Its FPR is 0.214, placing it in a competitive position among the top-performing methods. However, while its Precision is 0.230 and F1 score is 0.187, these values reflect a moderate balance between precision and recall. For the GPT-3.5-turbo method, the Coding-Debating Hybrid Group demonstrates notable performance with an SHD of 23, the second-best result in the table. This method also achieves the best FPR of 0.198 and an impressive FNR of 0.068, highlighting its effectiveness in minimizing both false positives and false negatives. The precision of this group is 0.222, and the F1 score is 0.142, indicating a robust overall performance.\nIn summary, classical stastitcal methods like the PC algorithm, Exact Search, and DirectLiNGAM exhibit higher SHD values, ranging from 24 to 31, and varied performance across FPR, FNR, Precision, and F1 score. Methods integrating LLMs with KBCI, such as PC LLM-KBCI, ES LLM-KBCI, and DirectLiNGAM LLM-KBCI, show moderate improvements but do not surpass the results achieved by Single-agent zero-shot prompting (GPT-40). Finally, our proposed multi-agent framework has the best performance."}, {"title": "5 Conclusion and Future Research", "content": "In this study, a novel framework MAC that integrates agentic workflows of large language models (LLMs) with data-driven methods is introduced. To the best of our knowledge, this is the first work to explore these agentic workflows of LLMs in a causal context. Our framework enhances causal discovery by combining the capabilities of LLMs with empirical data analysis. We acknowledge the need for further research to explore different cooperative workflows, various prompting techniques, and applications in domains such as healthcare, economics, and social sciences. Additionally, recognizing the potential bias in LLMs, more experiments with diverse datasets are required to verify our findings. Preliminary results demonstrate the potential of our framework to improve the accuracy and interpretability of causal discovery. We hope that our work lays a foundational stone for future research, inspiring advancements in the integration of LLMs with causal inference methodologies and contributing to more informed decision-making and policy development."}]}