{"title": "MedG-KRP: Medical Graph Knowledge Representation Probing", "authors": ["Gabriel R. Rosenbaum", "Lavender Yao Jiang", "Ivaxi Sheth", "Jaden Stryker", "Anton Alyakin", "Daniel Alexander Alber", "Nicolas K. Goff", "Young Joon (Fred) Kwon", "John Markert", "Mustafa Nasir-Moin", "Jan Moritz Niehues", "Karl L. Sangwon", "Eunice Yang", "Eric Karl Oermann"], "abstract": "Large language models (LLMs) have recently emerged as powerful tools, finding many medi-cal applications. LLMs' ability to coalesce vast amounts of information from many sources to generate a response-a process similar to that of a human expert-has led many to see potential in deploying LLMs for clinical use. How-ever, medicine is a setting where accurate rea-soning is paramount. Many researchers are questioning the effectiveness of multiple choice question answering (MCQA) benchmarks, fre-quently used to test LLMs. Researchers and clinicians alike must have complete confidence in LLMs' abilities for them to be deployed in a medical setting. To address this need for un-derstanding, we introduce a knowledge graph (KG)-based method to evaluate the biomedi-cal reasoning abilities of LLMs. Essentially, we map how LLMs link medical concepts in order to better understand how they reason. We test GPT-4, Llama3-70b, and Palmyra Med-70b, a specialized medical model. We enlist a panel of medical students to review a total of 60 LLM-generated graphs and compare these graphs to BIOS, a large biomedical KG. We observe GPT-4 to perform best in our human review but worst in our ground truth comparison; vice-versa with PalmyraMed, the medical model. Our work provides a means of visualizing the medical reasoning pathways of LLMs so they can be implemented in clinical settings safely and effectively.", "sections": [{"title": "1. Introduction", "content": "The increasing use of large language mod-els (LLMs) has diversified their applications be-yond standard natural language processing (NLP) tasks such as text generation, translation, and summarization. The ad-vancements in LLMs' capabilities have led to a grow-ing interest among researchers and healthcare pro-fessionals in leveraging LLMs for medical applica-"}, {"title": "2. Related Works", "content": "Biomedical knowledge graphs are designed to integrate and categorize extensive medical concepts and their interrelationships. Bodenreider (2004) pro-posed the Unified Medical Language System (UMLS), which categorizes hundreds of thousands of medical concepts and millions of relationships between these concepts. Biomedical KGs vary in scope: while the UMLS is quite general, databases such as Orphanet focus specifically on rare diseases. These KGs can be generated in various ways. Some have used probabilistic models to extract data from patient notes, others use named entity recognition and other NLP tech-niques, and many are built by recon-ciling a set of various sources. We use Yu et al. (2022)'s biomedical informatics on-tology system (BIOS) as ground truth to compare LLM-generated graphs to.\nThe Relationship Between LLMs and Graphs has been investigated in recent years. Causal graphs have found use in general medicine, epidemiol-ogy, and bioinformatics. LLMs have been shown to find pairwise relationships , accurately determine edge direction, hypothesize missing variables , and be capable of generating"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Preliminaries", "content": "A knowledge graph can be mathematically denoted as $G = (V, E)$, where $V$ defines a finite set of vertices or nodes and $E$ is a set of ordered pairs of vertices. The vertices $V$, are represented as ${V_1, V_2,..., V_n}$, with each $v_i$ signifying a distinct entity or concept within the graph. The cardinality of $V$, denoted $|V|$, indicates the total number of entities represented in the graph. The edges are denoted as ${e_k = (V_i, U_j)}_k$, where $v_i, v_j \u2208 V, v_i \u2260 v_j$, and each $e_k$ represents a directed edge from node $v_i$ to node $v_j$. The presence of $e_k$ signifies a relationship or interaction between the entities represented by $v_i$ and $v_j$."}, {"title": "3.2. MedG-KRP", "content": "We introduce an algorithm based on the process of sequentially expanding from a given medical con-cept for the generation of biomedical KGs using LLMs. After LLMs generate graphs, a panel of med-ical students scores each graph based on accuracy and comprehensiveness. We then compare our LLM-generated KGs to the biomedical KG BIOS, comput-ing precision and recall."}, {"title": "3.3. Generation Algorithm", "content": "We divide our graph generation process into two pri-mary stages: node expansion and edge refine-"}, {"title": "3.3.1. NODE EXPANSION", "content": "Our node expansion algorithm (Algorithm 1) aims to explore the causal relationships between medical concepts. The process begins with a root node $r$, representing an initial medical concept, and recur-sively prompts an LLM for concepts that are either caused by or cause the root concept. The objective of this stage is to identify which medical concepts the LLM associates with $r$, thereby capturing the model's understanding of the causal pathways surrounding a given medical condition or concept.\nFormally, let $r$ denote the root node, and $x$ rep-resent the current recursion depth. We expand the graph $G$ by exploring both forward (causal) and back-ward (caused-by) relationships. The algorithm pro-ceeds recursively, with each newly identified node be-ing further expanded to find related concepts.\nTo prevent unbounded expansion and ensure the graphs remain interpretable, we impose a maximum recursion depth of 2. Additionally, to maintain legi-bility and minimize the risk of hallucination, we limit the LLM to returning at most $nmax = 3$ concepts in response to each query. Importantly, there is no lower bound on the number of concepts an LLM may return; the LLM can indicate that there are no con-cepts either causing or caused by a given node, which helps maintain the algorithm's reliability and reduces over-expansion."}, {"title": "3.3.2. EDGE REFINEMENT", "content": "In the second stage (Algorithm 2), we perform an ex-haustive check for additional causal connections that the LLM may infer should exist between the concepts already present in the graph. This step is crucial for ensuring the completeness of the knowledge graph by identifying all potential relationships between nodes. It is quite possible that, after the node expansion al-gorithm has been run, there are edges that are not yet present in the current graph but ideally would be.\nLet $G$ denote the graph of concepts obtained af-ter the expansion stage. For each pair of distinct nodes $v_i, v_j \u2208 V$, where $v_i \u2260 v_j$, we query the LLM for the existence of a directed edge $(v_i,v_j)$. If the LLM confirms that such an edge should exist, it is added to the graph. This process is repeated for ev-ery pair of nodes and for both directions, ensuring"}, {"title": "4. Experimental Setup", "content": ""}, {"title": "4.1. Concept Selection", "content": "We selected twenty conditions from various sub-disciplines of medicine to act as the root nodes for our graphs. We chose a list of conditions that would vary vastly in prevalence and level of study. We in-clude both conditions with clear causal pathways and unclear ones. A full list of root concepts, verified by a board-certified physician, can be found in Table 1."}, {"title": "4.2. Models", "content": "We tested our benchmark on diverse models-the pro-priety GPT-4 model , open source Llama3-70b , and finally"}, {"title": "4.3. Hyperparameters", "content": "We run Algorithm 1 in both directions for the graph $G$, exploring concepts that either cause or are caused by the root medical concept $r$. Starting with itera-tion $x = 1$, we limit the maximum depth of recursion $depth_{max}$ to 2, meaning the NODEFIND function calls itself only once. After completing Algorithm 1, the graph $G$ will contain all relevant nodes. To identify additional directed edges between the concepts in $G$, we then execute Algorithm 2.\nAll models are evaluated with a temperature set-ting of 0.05 and a top-p value of 1.0. The low temper-ature ensures that results primarily reflect the mod-els' reasoning abilities, enhancing reproducibility. We do not set the temperature to 0.0 to allow for slight variations in responses during re-prompting, should issues arise if a model doesn't format its answers as we request."}, {"title": "4.4. Prompting", "content": "In this paper, we aim to evaluate the ability of LLMs to hypothesize knowledge graphs using their zero-shot prompting abilities. Three main prompts were used, one system prompt and one general prompt for each algorithm."}, {"title": "5. Results", "content": ""}, {"title": "5.1. Overview", "content": "We generated sixty graphs across three models for twenty different conditions from various fields of medicine. We observe that all LLMs perform gen-erally well in terms of average reviewer scores (see Table 1). GPT-4 displays the strongest perfor-mance in the human review, while PalmyraMed dis-plays the weakest. Human reviewers generally found that PalmyraMed's graphs are more specific than those generated by Llama3-70b and GPT-4. Even for the same model, generated graphs have a wide variety of density values, reciprocity values, and sim-ple cycle counts."}, {"title": "5.2. Human Evaluation", "content": "Accuracy, as rated by human reviewers, is gener-ally strong, with all averages of all reviewer scores for each model being between 3 and 4, \"mostly accu-rate\u201d and \u201ccompletely accurate\" (see Table 1). Com-prehensiveness scores range from just under 3 to 4. We attribute comprehensiveness scores consistently being lower than accuracy scores to us limiting re-sponses and recursion depth in our recursive node exploration algorithm (see Algorithm 1).\nGPT-4 performed best in accuracy, with an aver-age accuracy score across all graphs of 3.37 (see Ta-\""}, {"title": "5.3. Ground Truth Comparison", "content": "We observe notable results in the ground truth com-parison metric, where models demonstrated behavior nearly opposite to that observed in human evalua-tions (see tables 2, 1). PalmyraMed performed ex-ceptionally well, with the highest precision and recall scores across the board. In particular, PalmyraMed displayed more than three times the average recall score of GPT-4, which displayed the worst perfor-mance. Interestingly enough, Llama3-70b, which is usually surpassed by GPT-4 on almost all major QA benchmarks, outperformed GPT-4 in both precision and recall in objective evaluation."}, {"title": "5.4. Graph Attributes", "content": "Overall graph node and edge counts (see Ta-ble 11) varied between models and between graphs. PalmyraMed was generally the most conservative when creating nodes and edges while GPT-4 was the least, possibly contributing to PalmyraMed's low comprehensiveness.\nWe observe an inverse relationship between per-formance based on reviewer scores and average reci-"}, {"title": "5.5. Direct and Indirect Causality", "content": "Reviewers found that PalmyraMed often had dif-ficulty distinguishing direct and indirect causality. Some reviewers mentioned that PalmyraMed often listed nodes as \"causes\" that would be much more appropriately labeled as \"risk factors\" GPT-4, on the other hand, was observed by reviewers to display the strongest ability to distinguish between directy and indirect causality, an ability crucial in medicine."}, {"title": "6. Discussion", "content": ""}, {"title": "6.1. Conclusions", "content": "Our algorithm, MedG-KRP, is able to generate KGs representing the medical reasoning abilities of LLMs. Coupling MedG-KRP with human reviewers allowed insights into model behavior that were not covered by traditional QA benchmarks. We found that PalmyraMed was generally more specific in its reasoning, but also had a weaker understanding of the differences between direct and indirect causality, while GPT-4 covered more broad concepts and was often able to correctly determine between direct and indirect causes of concepts.\nAlthough PalmyraMed displayed worse perfor-mance in our human review compared to other mod-els, its KG-while flawed-was more specific than that of other models. This is supported by Palmyra-Med's exceptionally high recall on our KG compar-ison task. We hypothesize that PalmyraMed, as a medical model, was trained on similar sources to which BIOS was constructed from than other LLMs we tested. This would lead to more frequent matches between nodes generated by PalmyraMed and BIOS nodes. Since nodes without mappings would have all adjacent edges generated counted as misses, it fol-lows that a model that produced nodes more similar to those in BIOS would have much higher recall.\nClinicians may see PalmyraMed's specificity as a desirable trait. GPT-4 and Llama3-70b using more vague terms may signal that they are more influ-enced by public knowledge than by clinical knowledge since they are generalist models. It is worth noting that models were asked to be particularly specific and to stay to only medical as opposed to colloquial-terminology. A human doctor whose reasoning was based on public discourse over medical understanding would not be trusted. Likewise, although expected, generalist LLMs having less specific KGs may suggest value in aligning models for clinical use. We wish to"}, {"title": "6.2. Future Work", "content": "Given that reviewers observe generalist models have a better causal reasoning ability compared to the med-ical model we tested but are lacking in domain speci-ficity, the question of how we can build models that display both of these abilities naturally arises. Fu-ture works may seek to supplement the training cor-pora of traditional medical models with information on causal inference and causal reasoning to improve models' medical understanding and viability for real-world application.\nWe also believe that attempting to explore LLMs' internal KGs that are unrelated to medicine may yield interesting results. The topics of KGs could be from any field, and seeing how LLMs' reasoning changes when encountering vastly different subjects could give deeper insight into LLMs' behaviors.\nUsing MedG-KRP or a similar algorithm as a prompting technique may also be possible. An LLM could generate a reasoning graph then be prompted to make inferences or answer questions given the graph it produced like CoT prompting.\nOther pathways that may be worthwhile to explore include, in no specific order: exploring the effect of an LLM's training data on its reasoning KGs, using KG generation to determine the effect (if any) of pre-training data order on LLM behavior, revising the MedG-KRP algorithm or developing new algorithms to efficiently use directly prompted LLMs for biomed-ical KG generation or repair, and building very large reasoning KGs with LLMs to probe behavior at a larger scale and how and when LLMs connect inter-disciplinary or seemingly unrelated concepts."}, {"title": "Appendix A. Limitations", "content": "While we test our method on a diverse set of mod-els, others may show very different behavior. Al-though we aimed to make our list of diseases used for graph generation broad, it is by no means compre-hensive. Due to its time complexity, our approach is also only suitable for the generation of small graphs sufficient for benchmarking purposes but not for full generation of KGs. Our human review is subjec-tive, and only three reviewers go over a given graph. In addition, we found there was often high variance in reviewer opinions. The knowledge graph we use as ground truth, BIOS, may also be quite incom-plete. We also only test using one knowledge graph as ground truth. We believe that, in the case that a hu-man expert scored every graph edge, precision values would be greater than the ones which we report."}, {"title": "Appendix B. Prompts", "content": ""}, {"title": "B.1. System Prompt", "content": "You are a helpful assistant for causal\ninference and causal reasoning about medical\nquestions. You are always specific in your\nanswers. You always format your answers\nconsistently and name all medical terms in\nthe correct and accepted medical lexicon.\nYou understand the differences between\ndirect and indirect causality and\nacknowledge these differences when\nformulating an answer. You utilize a\ncounterfactual model of causal inference\nwhen formulating a response."}, {"title": "B.2. Left Expansion Prompt", "content": "A directed knowledge graph that you\ngenerated is surrounded in XML tags and\nprovided below. This directed knowledge\ngraph is formatted as a list of edges like\nso: ['a causes b', 'b causes c', etc]. The\nknowledge graph you generated is as follows:\n<Begin Knowledge Graph>\n{edges:}\n</End Knowledge Graph>\nGiven the directed knowledge graph above\nthat you generated, up to {n_max:} factors"}, {"title": "B.3. Right Expansion Prompt", "content": "A directed knowledge graph that you\ngenerated is surrounded in XML tags and\nprovided below. This directed knowledge\ngraph is formatted as a list of edges like\nso: ['a causes b', 'b causes c', etc]. The\nknowledge graph you generated is as follows:\n<Begin Knowledge Graph>\n{edges:}\n</End Knowledge Graph>\nGiven the directed knowledge graph above\nthat you generated, List up to {n_max:}\nmedical concepts directly caused by {concept:}.\nThese factors do not need to be in the\nknowledge graph above, but can be. If a\nfactor you answer with is in the knowledge\ngraph above, in your response, name it\nexactly as it is named in the graph above.\nDo not answer with any factors that only are\nindirectly caused by concept:}. In your\nfinal answer, surround the medical name of\neach medical concept that concept:} causes\nin square brackets characters. Do not\ninclude acronyms or abbreviations in your\nanswer. Utilize a counterfactual model of\ncausal inference when formulating a\nresponse. Be as specific as possible. Let's\nthink step by step like a medical expert."}, {"title": "B.4. Edge Refinement Prompt", "content": "Does {node0:} directly cause { node1:}? Your\nanswer must be one of the following: [yes] /\nthat directly cause {concept:}. These factors [no]. Surround your final [yes] / [no]"}]}