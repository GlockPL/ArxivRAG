{"title": "Scaling LLM Inference with Optimized Sample Compute Allocation", "authors": ["Kexun Zhang", "Shang Zhou", "Danqing Wang", "William Yang Wang", "Lei Li"], "abstract": "Sampling is a basic operation in many inference-time algorithms of large language models (LLMs). To scale up inference efficiently with a limited compute, it is crucial to find an optimal allocation for sample compute budgets: Which sampling configurations (model, temperature, language, etc.) do we use? How many samples do we generate in each configuration? We formulate these choices as a learning problem and propose OSCA, an algorithm that Optimizes Sample Compute Allocation by finding an optimal mix of different inference configurations. Our experiments show that with our learned mixed allocation, we can achieve accuracy better than the best single configuration with 128x less compute on code generation and 25x less compute on 4 reasoning tasks. OSCA is also shown to be effective in agentic workflows beyond single-turn tasks, achieving a better accuracy on SWE-Bench with 3x less compute than the default configuration. Our code and generations are released at https://github.com/LeiLiLab/OSCA.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) solve more problems with more inference compute. Different ways of scaling up LLM inference include sampling (Chen et al., 2021), self-consistency (Wang et al., 2023c), tree search (Yao et al., 2024), and multi-agent systems (Du et al., 2023), etc. Among these, sampling is the most basic and serves as an atomic operation needed in all other more complicated methods. Therefore, it is crucial to do it well.\nPrevious studies (Wang et al., 2023b) have investigated how to find the optimal sampling configuration, such as the best temperature, model, and prompt. While these methods are effective, they miss one key fact: Not all problems require"}, {"title": "2 Related Work", "content": "Inference Time Algorithms. Following the tax-onomy of Welleck et al. (2024) on inference-time algorithms, chained meta-generators run multiple LLM calls sequentially and use the output sample from each call as the input to the next one (Dohan et al., 2022; Schlag et al., 2023). Parallel meta-generators samples multiple candidates for a problem and selects the best candidate (Wang et al., 2023c; Chen et al., 2022; Jiang et al., 2023; Zhang et al., 2023; Huang et al., 2023). Step-level search methods regards problem-solving as a multi-step process and sample candidate next steps at each intermediate state, using algorithms like tree search (Yao et al., 2024), graph search, and Monte Carlo Tree Search (Lample et al., 2022; Tian et al., 2024; Chi et al., 2024). Refinement-based meth-ods samples candidate solutions sequentially, rely-ing on some feedback to revise the next candidate (Madaan et al., 2024; Shinn et al., 2024). Although these algorithms scale up inference differently, they all need LLM sampling as a basic operation.\nScaling Inference. Many studies investigate how scaling in inference affects LLM performance. AlphaCode (Li et al., 2022; Leblond et al., 2023) scales up the sample number and finds the solve rates scale log-linearly with more samples. Brown et al. (2024) improves LLMs' performance on math problems by repetitively sampling candidate solu-tions with a high temperature. Wu et al. (2024) and Snell et al. (2024) study the scaling behaviors of various inference time algorithms, reward models, and the model sizes. In this paper, we investigate the allocation algorithm between various inference configurations for scaling up inference.\nInference Compute Optimization. There are two typical ways to optimize inference compute. One is to search for a single optimal configuration. For ex-ample, Wang et al. (2023a) proposes EcoOptiGen to optimize the inference hyperparameter under a limited compute budget, and Wang et al. (2024) finetunes LLMs to self-regularize its generation with the best hyperparameter set. The other is to find the optimal allocation between various infer-ence configurations (Graves, 2016; Dehghani et al., 2019). Damani et al. (2024) allocates the compute budget based on the estimation of problem diffi-culty. Here, we optimize the compute allocation to different inference configurations."}, {"title": "3 Method", "content": "3.1 Problem: Sample Compute Allocation\nLLM Problem-Solving. We study how to im-prove LLM-based solvers for problems with binary correctness values. Formally, a problem is a pair (x, v), where x \u2208 X is the problem specifications, v : X \u00d7 Y \u2192 {True, False} is the verifier that maps a solution of the problem to a truth value, and y is the space of solutions. An LLM solver is given the problem specification and produces a distribution PrLM(y|x) over the solution space conditioned on the problem specification.\nCompute Budget and pass@C. We evaluate LLM-based problem solvers with the average solve rate of test set problems given a fixed compute budget C, which can be defined as the maximum num-ber of samples, the maximum number of tokens, the maximum FLOPs, etc. For any definition of compute budget C, we generate as many samples as possible from PrLM(y|x) within the compute limit. The solver can be evaluated using pass@C, which is defined as the probability of at least one sample among the candidates being correct. In our experiments, we use the number of samples as a metric for C.\nSampling Configurations. Given a problem spec-ification x, there are multiple inference hyperpa-rameters to decide when solving it with an LLM \u2013 the model to use, inference temperature, language of the output, prompt, etc. For the i-th hyper-parameter, we use Hi to denote the set of fea-sible values and hi to denote an element in Hi that is actually chosen. Assuming the number of tunable hyperparameters is d, we call an d-tuple"}, {"title": "3.2 Why Mixed Allocation?", "content": "We show an example that demonstrates why it is sometimes necessary to use a mixed allocation of sample compute. Consider two problems x1 and X2, and two configurations h\u2081 and h2. We use Pr(pass|h, x) to denote the probability of generat-ing a correct solution to the problem x under con-figuration h. Let's assume that Pr(pass|h1, x1) = 10%, Pr(pass|h2,x1) = 1%, Pr(pass|h1, x2) = 1%, Pr(pass|h2, x2) = 10%. In other words, h\u2081 is better at solving x\u2081 and h2 is better at solving x2.\nConsider the allocation of 10 samples per prob-lem to these two configurations. If we use a pure allocation, all 10 samples will be entirely given to either h\u2081 or h2, the expected pass@10 would be\n$\\frac{1 \u2013 (1 \u2013 0.1)^{10} + 1 \u2212 (1 \u2013 0.01)^{10}}{2} = 37.3\\%$.\nOn the other hand, if we use a mixed allocation and split 10 samples evenly between h\u2081 and h2, the expected pass@10 would be\n$1 \u2212 (1 \u2212 0.1)^5 \u00d7 (1 \u2013 0.01)^5 = 43.8\\%$,\nwhich is significantly higher than the pure alloca-tion's result."}, {"title": "3.3 Our Algorithm", "content": "The proposed OSCA is described in Algorithm 1.\nEstimating pass probability. For each sampling configuration hi and each problem xj in the train-ing set, we estimate a probability matrix P with |H| \u00d7 |D| elements. Each element pij indicates the probability of a sample from hi solving xj. We estimate pij by sampling Co times from hi and computing the frequency of correct solutions,\n$P_{ij}=\\frac{C_{ij}}{C_0}$.\nwhere cij is the number of correct samples gener-ated for the problem xj with the configuration hi. Note that to save compute, the estimation compute budget Co is much smaller than the actual compute budget C.\nMaximizing expected pass@C on training set. Assuming that the test set is i.i.d. with the train-ing set, we optimize \u03c0 to maximize pass@Con training data. For a single problem xj and a sin-gle configuration hi \u2208 H, its pass@Ci is defined as it probability of being solved with compute Ci, which can be derived as\n$Pr(x_j solved with C_i samples from h_i)= 1 \u2013 (1 \u2013 P_{ij})^{C_i}$.\nBy aggregating all problems and all configurations, we can obtain this optimization problem:\n$\\max_\\pi E[pass@C]=\\frac{1}{|D|} \\sum_{j=1}^{|D|} \\prod_{i=1}^{|H|} 1-(1-P_{ij})^{\\pi_i}$ s.t. 0 < \u03c0i < C,$\\sum_{i=1}^{|H|} \\pi_i = C, \\pi_i \\in N$.\nNote that if we remove the integral constraint \u03c0i \u0395 N from Problem (3), the relaxed problem is convex (Proof in Appendix A.1) that can be solved optimally by hill climbing algorithm (Russell and Norvig, 2016). We start from a randomly picked distribution of compute \u03c0(0). At each iteration t, we examine all the neighbors of the current distribution that differ slightly from \u03c0(t) and \u201cclimb\u201d to the neighbor if it's better than the current distribution to obtain \u03c0(t+1). The algorithm stops once there is no better neighbor. Though the algorithm is not guaranteed to produce global optima for integral solutions, it works well empirically.\nThis algorithm can be extended to problems with real scores with some slight modification, as dis-cussed in Appendix A.4."}, {"title": "4 Evaluation on Single-Turn Tasks", "content": "We first evaluate OSCA on single-turn tasks, where a solution to a problem can be generated with a single LLM call. For these tasks, the only way to scale up LLM inference is to sample more solutions.\n4.1 Baselines\nWe consider three baselines in our experiments \u2013 default pure allocation, optimal pure allocation, and uniform mixed allocation:\nDefault pure allocation is the one used to pro-duce the leaderboard results on the benchmarks. These benchmarks usually run LLM inference once for each problem instance, with a very basic prompt and a low temperature for reproducibility and fair comparison. Therefore, the default pure allocation is not optimized for scaling up.\nOptimal pure allocation is the one that has the highest pass @Co on the training set given a com-pute budget of Co. Since the actual C is larger than Co in most cases, we use pass@Co to select the optimal configuration. Comparison between the optimal pure allocation and the default pure allo-cation indicates whether it is necessary to search for a good set of inference hyperparameters, which is what existing hyperparameter optimization tech-niques do.\nUniform mixed allocation naively distributes the compute budget evenly to every sampling con-figuration hi in H. We compare the uniform mixed allocation with our learned mixed allocation to ex-amine whether it is necessary to optimize sample compute allocations for them to perform well.\n4.2 Tasks and Benchmarks\nWe evaluate OSCA on five tasks from two bench-marks \u2013 LiveCodeBench (Jain et al., 2024) and LiveBench (White et al., 2024). These two bench-marks are built with periodically released exami-nations and competitions so that the possibility of contamination can be minimized. To further avoid contamination, we choose the earlier released prob-lems as training set Dtrain.\nLiveCodeBench (Jain et al., 2024) collects LeetCode-style problems from weekly held on-line programming competitions such as LeetCode, Codeforces, and AtCoder. Each problem comes with a natural language specification that includes"}, {"title": "4.3 Key Observations", "content": "We present the pass rates for different sample com-pute allocations in Table 2. For allocations other than the default pure, we show the difference be-tween their pass rate and that of default pure. Sev-eral key observations can be made from these re-sults:\nPure allocation is not enough, mixed alloca-tion is necessary. By finding the optimal pure allocation on the training set, we get much better accuracy than the default allocation, highlighting the importance of a suitable inference configuration. However, pure allocation is not enough on Live-CodeBench. We find that code problems require a more diverse solution set, making the pure con-figuration not enough. On LiveCodeBench, just by allocating sample compute evenly across inference settings, we achieve a pass@8 of 73.3%, which is better than the pass rate of optimal pure allocation with 512 samples.\nUniform mixed allocation is not enough, OSCA's optimized allocation is necessary. On LiveBench, uniform mixed allocation does not out-perform optimal pure, suggesting that we can't always opt for uniform mixed. However, OsCA's optimized allocation does. With 8 samples, OSCA's pass rate comes to 67%, which is higher than the pass rate of the default pure allocation with 128 samples. In fact, OSCA outperforms all others in all but two cases.\nOSCA scales well with larger compute bud-gets. Although the sample compute budget (Co)"}, {"title": "4.4 Ablation Studies", "content": "We conduct ablation experiments on LiveBench to answer the following questions.\nHow many problems do we need in training to learn a good mixed allocation? We run OSCA on different proportions of the original training set and plot the results in Figure 3. Since there are multiple ways of subsampling the training set, we run it multiple times and compute the average. For reference, we also plot the results when we train on the test set, which should be the upperbound of OSCA. We observe that with the full training data, the allocation learned is very close to the upperbound. At smaller sample compute C, the allocation trained from less data is not much worse than the allocation from full training data. However, as C gets to over 26, the allocation gets much worse with less training data. We hypothesize that this is because when C is large, only the hard problems"}, {"title": "Which hyperparameter in sampling configu-rations needs mixed allocation?", "content": "To study whether mixed allocation is needed for the two hyperpa-rameters temperature or model, we consider two limited versions of OSCA by banning it from using more than one temperature or more than one model. As shown in Figure 5, when OSCA is not allowed to use multiple models, its accuracy degrades to that of optimal pure. When OSCA is not allowed to use multiple temperatures, its accuracy degrades to be worse than optimal pure. These results suggest that in order for OSCA to perform well, we need more diverse sampling configurations."}, {"title": "How general is OSCA's learned allocation?", "content": "In Table 2, we report the overall performance on LiveBench, because OSCA's training data is a com-bination of 4 subtasks from LiveBench \u2013 math, reasoning, language and data analysis. To evaluate the generality of the learned allocation, we examine its performance on the 4 tasks separately. As Ta-ble 3 shows, OSCA is the best in 7/12 cases and the second best in the remaining 5. This indicates that the OSCA's learned allocation is domain-specific to some extent. Example problems of these 4 cate-gories can be found in Appendix A.3."}, {"title": "Can we learn the best configuration at in-stance level?", "content": "Ideally, there is no need to use mixed allocation of sampling compute because there is only one optimal configuration for a specific prob-lem. Therefore, we investigate whether it is pos-sible to learn instance-level optimal configuration. We conduct this experiment on LiveCodeBench, as it has a larger training set and more homogeneous problems and is thus easier to learn. We propose a k-nearest neighbor algorithm under the assumption that similar problems need similar sampling config-urations. For each test problem x, we retrieve the k problems in the training set that are most similar to x, and allocate compute according to the distribu-tion of optimal configuration over these problems. We use OpenAI's text-embedding-3-large to"}, {"title": "5 Evaluation on Agentic Tasks", "content": "Optimizing sample compute allocation is not just useful for its own purpose, it is also useful for more complicated LLM inference algorithms, such as tree search and agentic workflows, because sam-pling is a basic operation in these. To demonstrate such usefulness, we apply OSCA to an agentic workflow for SWE-Bench, which needs 2 stages and 7 steps involving numerous LLM calls to re-solve software engineering issues in repository-level code.\n5.1 Benchmark and Workflow\nSWE-Bench (Jimenez et al.) collects real-world software engineering issues from open-source GitHub repositories such as django and matplotlib. Each issue is paired with human-written unit tests. To resolve an issue, one needs to understand the issue description, examine the codebase (often hun-dreds of thousands of lines long), locate where to make changes and make necessary modifications by generating a patch. Decent solutions on this benchmark often takes an agentic approach by giv-ing an LLM multiple tools to use and multiple actions to take and guiding it through a multi-stage workflow. We consider a subset of SWE-Bench cu-rated by the authors called SWE-Bench Lite, which contains 300 issues.\nAgentless (Xia et al., 2024) is one of the best-performing open-source solutions to SWE-Bench."}, {"title": "5.2 Results", "content": "As demonstrated in Figure 1, OSCA can also im-prove the performance of Agentless by improving one of the steps in its agentic workflow. Compared to both the pure allocation (Temperature is set to 1 in both default and optimal pure allocation) and uni-form mixed allocation, OSCA's learned allocation can get a similar accuracy with fewer samples. The gap between OSCA and the optimal pure allocation enlarges as sample compute gets larger, while uni-form mixed allocation approaches and outperforms optimal pure with larger sample sizes.\nThese findings further demonstrate the necessity of optimizing sample budget allocation, as it can be used in more complicated workflows to make them more efficient."}, {"title": "6 Conclusion", "content": "In conclusion, this paper presents OSCA, an algo-rithm designed to optimize sample compute allo-cation for large language models (LLMs) during inference. Through various experiments on both single-turn and agentic tasks, the study demon-strates that OSCA significantly improves accuracy with reduced compute resources compared to tra-ditional methods, such as pure or uniform mixed allocations. By leveraging a mixed allocation al-location, OSCA balances different inference con-figurations, proving particularly effective in code generation and reasoning tasks, as seen in bench-marks like LiveCodeBench and LiveBench. This work demonstrates the importance of adapting sam-pling allocation to the specific characteristics of the problem at hand. Furthermore, OSCA's application to more complex workflows, such as multi-step agentic tasks, shows potential for broader utility in improving the efficiency of LLMs in various real-world applications. However, future work could explore optimizing additional hyperparameters and testing the scalability of the method with larger compute budgets.\nLimitation\nAlthough OSCA demonstrates an effective way to allocate sample compute there are still sev-eral limitations. First, this paper mainly focuses on four representative inference hyperparameters: model types, temperatures, response languages, and prompts. In addition to these aspects, there are other hyperparameters such as top k, top p, rep-etition penalty, etc. Combining these hyperparam-eters can make the sample configuration set more diverse. Besides, due to the computation limitation, we limited our inference compute budget to 512. It would be interesting to see how further scaling up will affect the performance.\nEthics Statement\nWe acknowledge that there might be some ethical considerations in enhancing LLMs' capability such as the OSCA presented in this paper. However, we believe that none must be specifically highlighted here."}, {"title": "A.1 Proof of Convexity", "content": "We aim to minimize the following objective function:\n$\\min O(\\pi) = \\sum\\prod(1 \u2013 P_{ij})^{\\pi_i}$\n$\\pi$j=1 i=1\nsubject to:\n\u2022 0 \u2264 \u03c0i < C, with \u03c0i \u0395N\n\u2022$\\sum_{i=1}^{m} \u03c0_i = C$\nTo simplify the analysis, we take the negative logarithm of the objective function, which pre-serves convexity properties:\n$\\min O(t) = - \\sum_{j=1}^{m}exp(\\sum_{i=1}^{n} \u03c0_i ln(1 \u2013 P_{ij})$\nDefine:\n$gj(\\pi) = \\sum_{i=1}^{n} \u03c0_i ln(1 \u2013 P_{ij})$\nSince gj(\u03c0 is a linear combination of \u03c0i, it is an affine function in \u03c0. As affine functions are both convex and concave, gj(\u03c0) is convex.\nThe exponential function, exp(z), is convex. Since the composition of a convex function with an affine function remains convex, it follows that:\nhj(\u03c0) = exp(gj(\u03c0))\nis convex in \u03c0.\nThus, the overall objective function:\nf(\u03c0) = \\sumhj(\u03c0)\nj=1\n= \\sumexp\\sum\u03c0i ln(1 \u2013 Pij)\nj=1i=1\nis a sum of convex functions, implying that f(\u03c0) is convex."}, {"title": "A.3 Example Problems from LiveBench", "content": "Math Problem\nReal numbers x and y with x, y > 1 satisfy logx(y) = logy(x\u00b2y) = 10. What is the value of xy? Please think step by step, and display the answer at the very end of your response. The answer is an integer consisting of exactly 3 digits (including leading zeros), ranging from 000 to 999, inclusive.\nGround Truth: 025"}, {"title": "A.4 Mathematical Formulas for Evaluating Results with Fractional Scores", "content": "Probability Mass Function (PMF) for the Maxi-mum Score When Sampling k Scores\nTo calculate the probability that the maximum score Xmax among k samples is exactly x:\n$P(X_{max} = x) = \\frac{\\binom{C_{<=x}}{k} - \\binom{C_{<x}}{k}}{\\binom{m}{k}}$\nWhere:\n\u2022 m = \u2211x Cx is the total sample size.\n\u2022 C<x = \u2211y\u2264x Cy is the cumulative count of scores less than or equal to x.\n\u2022 C<x = \u2211y<x Cy is the cumulative count of scores strictly less than x.\nExpected Maximum Score Across Multiple Settings with Known PMF\nThe expected value E[X] of the maximum score across multiple settings is:\n$E[X] = \\sum x * P(X=x)$\nWhere P(X = x) is computed as:\n$P(X=x)=\\prod_{j=1}^{s}P_j(X_j \\le x) - \\prod_{j=1}^{s}P_j(X_j < x)$\nHere, Pj(Xj \u2264 x) represents the probability that the maximum score in setting j is less than or equal to x. The difference between the products isolates the probability that the maximum score is exactly x.\nExpected Maximum Score with Excess Sam-ples\nThe expected value of the maximum score when sampling n times is estimated by:\n$E[X] = \\sum(x - (C + f_x)^n - c^n)$\nWhere:\n\u2022n is the number of samples,\n\u2022 fx is the probability density for score x,\n\u2022 c is the cumulative probability up to score x.\nThe term (c + fx)n \u2013 cn reflects the probability of selecting exactly x as the maximum score from n samples"}]}