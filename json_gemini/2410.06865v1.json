{"title": "Students' Perceptions and Use of Generative AI Tools for Programming Across Different Computing Courses", "authors": ["HIEKE KEUNING", "ISAAC ALPIZAR-CHACON", "IOANNA LYKOURENTZOU", "LAUREN BEEHLER", "CHRISTIAN K\u00d6PPE", "IMKE DE JONG", "SERGEY SOSNOVSKY"], "abstract": "Investigation of students' perceptions and opinions on the use of generative artificial intelligence (GenAI) in education is a topic gaining much interest. Studies addressing this are typically conducted with large heterogeneous groups, at one moment in time. However, how students perceive and use GenAI tools can potentially depend on many factors, including their background knowledge, familiarity with the tools, and the learning goals and policies of the courses they are taking.\nIn this study we explore how students following computing courses use GenAI for programming-related tasks across different programs and courses: Bachelor and Master, in courses in which learning programming is the learning goal, courses that require programming as a means to achieve another goal, and in courses in which programming is optional, but can be useful. We are also interested in changes over time, since GenAI capabilities are changing at a fast pace, and users are adopting GenAI increasingly.\nWe conducted three consecutive surveys (fall '23, winter '23, and spring '24) among students of all computing programs of a large European research university. We asked questions on the use in education, ethics, and job prospects, and we included specific questions on the (dis)allowed use of GenAI tools in the courses they were taking at the time.\nWe received 264 responses, which we quantitatively and qualitatively analyzed, to find out how students have employed GenAI tools across 59 different computing courses, and whether the opinion of an average student about these tools evolves over time. Our study contributes to the emerging discussion of how to differentiate GenAI use across different courses, and how to align its use with the learning goals of a computing course.", "sections": [{"title": "1 INTRODUCTION", "content": "An increasing number of studies have investigated students' perceptions and opinions on the use of generative artificial intelligence (GenAI) within education in general, and specifically for computing students (e.g. [18, 21, 23]). These studies are typically conducted either by inviting a large body of computing students to complete a survey once, or within a specific computing course. How generative AI is used by students in their studies might change depending on the learning goals of the course they are taking. Students taking an introductory programming course might use GenAI for explaining compiler error messages, or explaining a programming concept. They would not learn much from generating the solution for each programming exercise, but some of them probably do that anyway, for different reasons. In follow-up courses this might be different; if the learning goal is to build a website using a framework, generating small snippets for subtasks might be acceptable and not impede their learning. In this study we explore further how students use and perceive GenAI across different programs and courses: Bachelor and Master, courses in which learning programming is the learning goal, and courses in which programming is a means to achieve another goal. We also investigate changes over time, since GenAI capabilities are changing at a fast pace, and users are adopting GenAI increasingly. We conducted three consecutive surveys among students of all programs of the Information and Computer Science department of a large European research university, capturing their perceptions on job prospects, policies and ethics, and classroom use. The survey also included a course-specific part to investigate to what extent students use it for programming-related tasks, and their beliefs on whether that should be allowed, or not. We conducted a quantitative and qualitative analysis to answer the following research questions:\nRQ1: How are GenAI tools currently used by students across different types of programming courses?\nRQ2: How have students' opinions on GenAI changed over the course of an academic year?\nThe contributions of this paper are: 1) an updated snapshot of students' perspectives from the academic year 2023-24, 2) an overview of changes over time, and 3) a fine-grained analysis of GenAI use cases and rationale for programming- related tasks within different computing courses. The insights provide a background to the increasing adoption of GenAI by students, and the emerging discussion of how to differentiate generative AI use across different courses, and how to align its use with the learning goals of computing courses."}, {"title": "2 BACKGROUND", "content": "Since the first paper that discussed GenAI's implications for computing education [11], many studies have appeared. While the first papers mainly focused on GPT's and Codex's abilities to generate solutions to standard programming problems, the topics expanded to educational content generation [22], student's use of GenAI in the classroom [13, 15- 17], and its implications for computing education [2, 3, 18]. Some of these studies look at the introduction of GenAI from the perspective of an educator. Becker et al., for example, identified several challenges and opportunities related to introductory programming courses in winter 2022-23 [3]. The challenges that were identified relate (among others) to the risk of over-reliance by students on tools (impeding learning of fundamental concepts or problem solving ability), and academic misconduct in the re-use of code. Other studies have found these proposed challenges to be realistic. Gooch et al. [12] analyzed a large number of university student assessments in the summer of 2022 and 2023, and noticed an increase in material flagged as AI-generated. Although the authors had expected a larger percentage to be AI-flagged, it clearly shows that students are using GenAI tools actively. Next to the challenges discussed earlier, Becker et al. [3] also identified opportunities for programming education, including the ability to quickly generate example solutions to problems, support in reviewing of students' code, and generating exercises. Examining the opportunities for enriching education more closely, Cambaz and Zhang [7] conducted a systematic literature review, identifying teaching and learning practices that use GenAI. The learning practices they found are: \u2018generate practice exercises', 'generate exemplar solutions', 'generate alternative solutions', 'improve student code', 'clarify error messages and provide suggestions', 'support conceptual understanding, 'provide syntax tips', and 'code explanation'. In the context of software engineering education, Daun and Brings [10] discuss that GenAI can offer numerous benefits, as long as students are supported in the use of these tools."}, {"title": "3 METHOD", "content": "We conducted a survey among students of the department of Information and Computing Sciences (ICS) at Utrecht University. The survey was distributed at three instances during the academic year 2023-2024: after Period 1 (November 2023), 2 (February 2024), and 3 (May 2024). The academic year at Utrecht University is divided into two semesters, having two periods each.\nUtrecht University is a large research university in the Netherlands. The ICS department offers Bachelor (BSc) and Master's (MSc) programs to over 2,000 students. These programs cover disciplines such as Computing Science, Information Science, Game and Media Technology, Data Science, Human-Computer Interaction, Business Informatics, and AI. The Bachelor program and its courses are mostly offered in Dutch, and the Master programs are in English with"}, {"title": "3.1 Survey design", "content": "The survey was largely based on the survey conducted as part of the 2023 ITiCSE working group on Generative AI in computing education [18], as mentioned in Section 2. We made some minor adjustments to this survey, e.g. we included our BSc and MSc programs, removed a few questions we considered irrelevant in the context of the current study, and improved some phrasings. However, we kept most questions to be able to compare results. We added some questions about the students' (perceived) programming proficiency (Q6) and importance for their future career (Q7). We divided the survey questions into two parts: part 1 (general) contained demographics and general questions on student's perceptions of GenAI, and part 2 (course-specific) contained questions on students' GenAI use and rationale for that, from which we expect the answers will vary for different courses. At the end of part 1 of the survey, we added a question on which courses the student had followed in the period before. For each selected course, the survey then dynamically offered the set of course-specific questions.\nBecause we conducted the survey multiple times, some students might participate more than once. To prevent students from abandoning the survey when confronted with a large set of questions they answered before, we included a question about former participation. If they stated they did the survey before, they would skip the general part of the survey and go directly to the course-specific questions.\nThe participants were asked for consent to join the survey, and to agree that the results (including anonymized quotes) may be used and shared in academic and non-academic publications. The consent form also states that participation is voluntary, participants have the right to withdraw at any point, responses will be kept completely confidential, stored on a secure server, and data will be deleted 5 years after the conclusion of the project. We also gave contact information of the education director and privacy officer, who were not involved in the study.\nThe final survey can be found in the supplemental materials. The survey was created using Qualtrics, hosted on a university server, and the data was later stored on a protected research data drive."}, {"title": "3.2 Distribution", "content": "For each period, we assembled a list of the courses offered within the study programs of the ICS department. We excluded thesis courses and supporting courses with very little credits (e.g. seminars). We labeled each course regarding the role of programming in the course. We focused on textual languages that can be easily generated by GenAI, therefore including languages such as SQL, CSS, and HTML. We distinguish between the following types of courses:\n\u2022 PROGCRS. Programming course. These are core programming courses, in which learning to program is the major learning goal. Examples are 'imperative programming' and 'advanced functional programming'.\n\u2022 PROGREQ. Although not a learning goal in itself, programming is required for this course. Examples are software development project (capstone) courses, computer graphics, Al courses, algorithms, etc.\n\u2022 PROGOPT. Programming is optional. Programming knowledge is not needed, but can be used (by students who want to) in the course, for example to do some data analysis in Python. This category also includes courses in which programming is needed for a group project, but not all students have to contribute to the coding part. Lastly, this category includes a course using a low-code platform, since some programming could be useful there.\n\u2022 NOPROG. No programming component. Examples of these are courses about research methods, or ethics within computing.\nWe asked teachers to choose the right label for their own course. If we did not receive a reply, we looked at the course description and used our own knowledge of courses to determine the label. We include the final list of courses for which we received responses in the supplemental materials.\nAt the end of each period, we sent a request to the teachers of the courses for that period (except those without any programming component), asking them to distribute a message to students in their course to participate in the survey. Teachers could use email, the learning management system, or another channel used in the course to invite the students. We decided to distribute the survey in the context of a course, because of the more personal approach. A downside of this was that not all teachers acted upon our request. However, because students could select all courses they had followed in the survey, we could still get some data from courses through which the survey was not distributed.\nFor each survey, we randomly selected three participants to win a \u20ac 15 gift voucher. They had to leave an email address if they wanted to join this raffle. These addresses were collected in a separate part at the end of the survey, not connected to their responses."}, {"title": "3.3 Data analysis", "content": "We cleaned the received survey data by removing responses that only contained answers to the demographic questions, and some repeated responses flagged as invalid (ballot stuffing). We added a course type label for each course-specific response, distinguishing between BSc and MSc courses, resulting in 8 groups. We updated a few course types only after the survey to NoProg, which we decided to keep. Based on the number of responses for each of the 8 groups, we merged groups with few responses, resulting in 5 new groups: PROGCRS-BSC, PROGREQ-BSC, PROGREQ-MSC (merged with ProgCrs-MSc), PROGOPT-BSC (merged with NoProg-BSc), and PROGOPT-MSC (merged with NoProg-MSc).\nFor this paper, we do not present an exhaustive descriptive analysis of all survey questions. Instead, we focus on the questions that contribute to answering our research questions."}, {"title": "3.3.1 Quantitative analysis", "content": "We analyzed the responses to all closed questions from the general part of the survey (Q8 - Q13) over the three periods. For each question, we conducted a temporal analysis to observe changes and trends over time. To facilitate a clear and comprehensive visualization, we created plots depicting the responses across the three periods. Given that the majority of our data is ordinal, derived from Likert-scale questions, we employed the Mann-Whitney U test to determine if there were statistically significant differences between the periods.\nFor the course-specific section, we focused on two questions (Q17 and Q19). In addition to analyzing these questions over time, we examined the data across different course levels (BSc and MSc) and the five course types. We applied both the Mann-Whitney U test and the Kruskal-Wallis H test to account for more than two groups. Furthermore, we utilized Python as a supporting tool for the analysis."}, {"title": "3.3.2 Qualitative analysis", "content": "We analyzed the responses to two open questions that the students had to answer for each course they had taken, namely \"Describe the ways you currently use GenAI tools in this course for code generation (e.g.: debugging, writing, etc.)?\" (Q22) and \"Can you elaborate on when you believe GenAI should be allowed or disallowed in this course?\" (Q20). These questions contained 269 and 284 student answers respectively. To analyze these, we followed the principles of Design Thinking, employing affinity diagramming to organize and categorize the qualitative data. Design thinking was chosen as a human-centered, collaborative, and iterative method that is particularly effective in"}, {"title": "4 RESULTS", "content": "Respondents included 47 females, 206 males, 1 non-binary individual, 2 participants who self-described, and 8 who did not indicate their gender. Of the responses, 163 were from BSc students (across 3 programs from the ICS department and 9 programs from other departments), 99 from MSc students (across 7 programs from the ICS department and 3 programs from other departments), and 2 from PhD students. Students have a diverse range of (self-estimated) programming proficiency, with BSc students averaging 6.4 out of 10, and MSc students averaging 7.3. The survey also shows that BSc students place high importance on programming for their future careers, rating it on average 7.9 out of 10. MSc students have a slightly higher perception, with an average rating of 8.2."}, {"title": "4.1 General impressions and trends", "content": "Overall, we observe small variations in student perceptions and usage over time for the general questions. However, these differences are not statistically significant, with a few exceptions.", "eqautions": []}, {"title": "4.2 Perceiving GenAl as a source of support", "content": "Q17 asked students to rank six potential sources of support according to the order in which they would resort to them in the case of a problem. These sources were: GenAI, Course discussion forum, Online search, Friend, Online forum (e.g. StackOverflow), and Course teacher or a TA. This question is particularly interesting, as we can estimate the role GenAI tools now play in computing courses compared to all other traditional practices of help seeking. results of the two analyses we conducted over the 340 responses students provided to this question."}, {"title": "4.2.1 Trend over time", "content": "First, we wanted to see if students' opinions regarding the usefulness of GenAI as a learning tool changes over time. Over three terms, it moved from the close forth to the close third choice, surpassing teacher/TA in the process. In fact, GenAI is the only source of help that has been consistently gaining prominence at the expense of other sources. This is not surprising; as the GenAI tools improved and more students learned about them and learned how to use them, their popularity grew."}, {"title": "4.2.2 Group analysis", "content": "We have also looked at how differently students rank GenAI as a support tool / help-seeking strategy across different course types. This analysis has revealed two main results. When comparing MSc versus BSc courses, we have found that MSc students rank GenAI significantly higher as a support tool than BSc students do; the results of the Mann-Whitney test for course level are U = 9459.5, p < 0.001. Looking at different course categories with each level, we did not see any difference for MSc-level courses, however, we did observe a significant difference on the BSc-level. Students of the introductory programming courses (ProgCrs-BSc) prioritized GenAI tools significantly lower as a help-seeking strategy than students of the BSc courses where programming was a required (ProgReq-BSc) or an optional (ProgOpt-BSc) component. The results of the Kruskal-Wallis test for course category on the BSc-level are as follows $\\chi^{2}(2) = 6.20$, p = 0.045. These two findings are consistent with each other. BSc students are less proficient in programming than MSc students, and BSc students taking introductory programming are the least proficient of all. As students' programming expertise grows, they are more ready to engage with GenAI tools."}, {"title": "4.3 Acceptance of GenAl for programming assignments", "content": "For each course the students took, we asked if they believed GenAI should be allowed or disallowed for programming assignments (Q19). Students could choose from three options: always allowed, allowed in some assignments but disallowed in others (mixed acceptance), or always disallowed."}, {"title": "4.3.1 Trend over time", "content": "We aimed to investigate whether the acceptance of GenAI for programming assignments has changed over time in the BSc and MSc courses. At the BSc level, the distribution appears stable across periods. This observed stability was confirmed with a Kruskal-Wallis test ($\\chi^{2}(2) = 1.53$, p = 0.465). In contrast, for the MSc level, there is an increasing trend over the periods. However, the results of the Kruskal-Wallis test indicate that these changes are not statistically significant ($\\chi^{2}(2) = 5.12$, p = 0.077). These findings suggest that despite the increasing use of GenAI among students (as noted in Q8-see Section"}, {"title": "4.3.2 Group analysis", "content": "The observed difference between MSc and BSc students becomes more evident when we aggregate the responses by course level and course type. Consistent with our earlier findings, MSc students generally exhibit a higher acceptance of GenAI usage compared to BSc students. Specifically, a larger proportion of MSc students support allowing GenAI either always or in a mixed manner, while a significant portion of BSc students prefer to disallow its use. This trend persists across different types of courses, indicating a broader acceptance of GenAI among more advanced students. Statistical tests confirmed this difference: a Mann-Whitney test for course level (U = 9667.5, p < 0.0001) and a Kruskal-Wallis test for course type ($\\chi^{2}(4) = 22.76$, p < 0.001)."}, {"title": "4.4 Using GenAl for programming across different courses", "content": "In this section we dive deeper into how and why students (do not) use GenAI tools for a diverse range of programming tasks, across different courses. There were several tasks from the original taxonomy that we did not find in the student responses, such as all subcategories of process, which contain activities related to the development process (e.g. release planning, automating the creation of commits, PRs, and issues). From the feature implementation category we only kept finding an API for a given task and implementing a new feature. For the latter category, we identified two subcategories: generic uses of GenAI, and using GenAI to create boilerplate code, code templates, and other structures. As expected in an educational setting, the learning category played a bigger role for students, therefore we modified the subcategories and identified a new subcategory for responses that talked about understanding/starting a task.\nIn the following subsections, we discuss the responses for the different types of courses."}, {"title": "4.4.1 Programming courses", "content": "The most prominent use of GenAI in programming courses regarding software quality was for debugging and fixing code. While most students simply mentioned debugging, three students explicated that they used debugging \u201conly when I had a bug i had tried to fix for a long time but couldn't succeed\". Two students added details on how they used the generated debugging help: \u201cI have used it for fixing a bug, but I did not use the code it provided and only used the idea\". Five students used GenAI for refactoring and no responses mentioned testing.\nSeveral students used GenAI for learning. Asking for help on how to apply certain functions or libraries and on how certain programming concepts work was mentioned most. Few students generated code examples for exploration or asked GenAI to explain code they did not understand. Others used GenAI as a teaching assistant or for a head-start by either helping them in understanding the assignment or providing help on how to continue when completely stuck. Interestingly, the usage of GenAI for feature implementation seems to be scarce in programming courses. Only two students generated boilerplate code and two used GenAI for auto-completion and general code generation. The rationale"}, {"title": "4.4.2 Courses in which programming is required", "content": "In courses in which programming is required, but not a learning goal in itself, we see a shift in use regarding feature implementation. In several of these courses, both BSc and MSc students report using GenAI to generate boilerplate code (e.g. \"generate simple getters\", \"bulk work\u201d, \u201cbasic skeleton of test functions\"), and other generic code generation uses (\u201cwriting\u201d, \u201cusing a coding buddy\u201d). Some students specifically mention they only use it for single lines, or \"max 5 lines of code\". The breadth of uses cases also increases for these courses: we see new uses such as finding an API (\"finding out if some functions exist that i do not have enough knowledge of\"), generating/manipulating data (\"fill database\"), development environment tasks (\"get some Docker template\u201d), documentation (\u201cadd comments\u201d), and testing, while all learning subcategories are also still relevant.\nAs an example, in a MSc course in the Applied Data Science program, in which students learn methods to extract, link and prepare data from medical systems, a student mentions \"writing a lot of code that just needs to be ran once anyway\", but also for \"throwing a database at it and letting it discover what's what\". Generating test code was mostly mentioned by students taking a capstone software development course: \"to generate simple unit tests\u201d and \u201cto help me use mocking in my tests\". Refactoring code was also a topic we noticed often in courses requiting programming, for example, we noticed several responses from students doing a course on data wrangling using GenAI to \"improve\", \"tidy up\u201d, and \u201crestructuring\u201d code.\nAcross all course types, students mention that it should be allowed for explaining and as an aid in understanding the material, but not for actually writing the entire code. In another context, a Data Mining course, a student gave an example on decision trees: \u201cGenAi could be used to explain Decision Trees in more detail, but copy and pasting a generated function to create a decision tree should not be used\".\nStill, some students from these courses argue disallowing GenAI. A student noted that the reason they propose disallowing GenAI is that students should take responsibility for their own learning: \"For learning skills, people should be able to come up with results on their own. Don't give responsibility away to AI!\". Many students pointed out the need to disallow GenAI for tasks where understanding is central.\nOther students made a connection with the course nature. For courses that require students to develop more \"real- world\" and complex code, such as project-based and challenge-based learning courses where the programming task is"}, {"title": "4.4.3 Courses in which programming is helpful", "content": "For courses where programming is considered helpful but not required for its goals, students were more unanimously than the other categories in favor of allowing the use of GenAI. We have also seen this in Figures 4(c) & 4(d), where MSc students were more in favor of always allowing it than BSc students for these courses, and have the same lower percentage of always disallowed. They base their rationale on the fact that if GenAI is used for a task that is not central to the course goals, then it should be allowed.\nAs an example, for a MSc course on Crowd simulation, a student mentioned that GenAI was encouraged, and they even received extra points for it, under the condition it was disclosed. Other responses for these courses often mention that these courses are not about learning programming (\"the assignments are about figuring out strategies and not engineering\" in the Multi-Agent systems course), so it should be allowed for these tasks. Similarly, for a course on Software ecosystems security a student noted that \"The course is not about our ability to write clean code but rather the quality of our research\u201d. For the course Personalisation for media, a student notes \u201cYou're not teaching programming here, so just let people use AI\"."}, {"title": "5 DISCUSSION", "content": "5.1 Answers to our RQs\nFor RQ1 we investigate how GenAI tools are currently used by students across different types of programming courses. In the analysis we distinguish between programming courses (where the fundamentals of programming are taught), courses where programming is a required component, and courses where programming is optional. Of the 269 responses on how students use GenAI tools (Q22), 115 responses indicated no use. Looking at the instances where GenAI was used, we found some differences between the different types of courses. Students relied more on GenAI for implementing features in courses where learning to program was not the main goal. This is also reflected in the answers of students when asked when they believe use of GenAI should be allowed, and can be connected to the differences we found in the acceptance of GenAI use between MSc and BSc students. MSc students found it more acceptable to use GenAI tools for programming assignments in their courses. A reason for this could be that these MSc courses were almost all programming required or programming optional courses, but not fundamental programming courses in which students were still learning the programming basics. The results for the BSc students also point towards this. The results show a trend where the use of GenAI tools is seen as more appropriate for courses where learning to program is not the goal but applying programming is. This finding aligns with the results of Margulieux et al. [16], where the perceived usefulness of AI in an introductory programming course decreased over time. In the study, the findings indicate that students are still interested in learning, with some expressing skepticism toward the Al's responses and recognizing when its outputs are inaccurate or unhelpful.\nOur second research question (RQ2) focuses on how students' opinions on GenAI have changed over the course of an academic year. Looking at the changes over time, we found a few significant results. Notably, one question asked students whether it is unethical to generate an entire solution to an assignment and hand this in. Prather et al. [18] reported a percentage of 95 on this question in the summer of 2023. In our survey, 99.2% of respondents found this"}, {"title": "5.2 Limitations", "content": "First, this study was conducted in a single institute in the Netherlands. Policies as stated by the institute and university could have influenced the perceptions and behavior of students. It would be interesting to replicate this longitudinal study at other universities and in other countries.\nSecondly, the students who responded to the survey might have been interested in GenAI more than the average student, leading them to feel the need to voice their opinions and experiences. This could impact the results in multiple ways. For example, these students might have stronger opinions, either more positive or more negative, about GenAI. This could affect how representative our findings are of the broader student population.\nThird, students' reports may not accurately reflect reality. They might be reluctant to admit using GenAI for unapproved purposes in a course due to fear of retaliation, even though the responses were treated confidentially.\nFourth, students could skip the first part of the survey if they had already completed that part. Therefore, we cannot track individuals' opinions over time. We can only investigate the overall changes in opinion of the respondents as a group. Because we invited students from the same study programs, we do not believe this poses a threat for the interpretation of our results.\nFinally, the distribution of the different types of courses over the periods was uneven. The core programming courses were mainly represented in Period 1, while the other periods contained more responses for programming required and programming optional courses."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this study, we aimed to explore the current use of GenAI tools in various programming courses and how students' opinions on GenAI evolve over time. To achieve this, we conducted a survey among students from the Department of Information and Computing Sciences at a large European research university at three different moments during the 2023-2024 academic year. The survey, based on a previously conducted international study, was divided into two sections: general opinions on the use of GenAI and course-specific questions. We received 264 responses for the first"}]}