{"title": "Memory-efficient Continual Learning with Neural Collapse Contrastive", "authors": ["Trung-Anh Dang", "Vincent Nguyen", "Ngoc-Son Vu", "Christel Vrain"], "abstract": "Contrastive learning has significantly improved representation quality, enhancing knowledge transfer across tasks in continual learning (CL). However, catastrophic forgetting remains a key challenge, as contrastive based methods primarily focus on \u201csoft relationships\u201d or \u201csoftness\" between samples, which shift with changing data distributions and lead to representation overlap across tasks. Recently, the newly identified Neural Collapse phenomenon has shown promise in CL by focusing on \"hard relationships\" or \"hardness\" between samples and fixed prototypes. However, this approach overlooks \u201csoftness\", crucial for capturing intra-class variability, and this rigid focus can also pull old class representations toward current ones, increasing forgetting. Building on these insights, we propose Focal Neural Collapse Contrastive (FNC2), a novel representation learning loss that effectively balances both soft and hard relationships. Additionally, we introduce the Hardness-Softness Distillation (HSD) loss to progressively preserve the knowledge gained from these relationships across tasks. Our method outperforms state-of-the-art approaches, particularly in minimizing memory reliance. Remarkably, even without the use of memory, our approach rivals rehearsal-based methods, offering a compelling solution for data privacy concerns.", "sections": [{"title": "1. Introduction", "content": "Unlike human learning, which is adaptive and ongoing, current deep neural networks (DNNs) are typically trained in batches, using all available data at once. To emulate human learning, these models need to support continual learning (CL), which involves acquiring new knowledge while retaining previously learned information. However, this process is hindered by the phenomenon known as \"catastrophic forgetting\" (CF) [36, 44], where learning new tasks with fresh data causes the model to overwrite its prior knowledge, leading to a drastic decline in performance on older tasks. To tackle this challenge, CL has gained significant attention in recent years [5, 27, 37, 41, 45, 48, 53]. CL aims to develop methods that enable models to learn from a continuous stream of data by balancing the retention of prior knowledge with the ability to adapt to new information. Achieving this balance, known as the stability-plasticity trade-off, is crucial for preventing performance loss when integrating new tasks.\nCurrent CL methods are predominantly based on supervised strategies, which have proven effective in preserving knowledge [3, 50, 57]. Notably, approaches that decouple representation learning from classifier training have shown greater resistance to forgetting compared to joint training methods [5, 12, 34, 53]. Building on this decoupling, several supervised contrastive representation learning methods have delivered strong results [5, 53]. However, most of these methods depend on replay buffers for storing past samples, limiting their use when data privacy is a concern. Another limitation is their reliance on inter-sample relationships, which can lead to representation drift and overlap with new tasks-one of the main causes of forgetting in CL. Indeed, this issue has been highlighted in both DNNS [4, 58] and neuroscience [10].\nNeural collapse (NC), a recently discovered phenomenon characterized by highly structured and aligned neural network features, has attracted considerable attention in the deep learning community [11, 15, 20, 33, 49, 63]. It shows particular promise for CL by reducing representation overlap, enhancing class separation, and mitigating CF. NC achieves this by aligning feature representations with fixed prototypes, which act as optimal class-specific reference points and remain constant throughout training, significantly enhancing class separation. Additionally, prototypes can serve as class representatives, and their integration into contrastive learning reduces reliance on memory buffers. Leveraging these advantages, several CL methods [46, 55, 56] have incorporated NC. However, focusing only on sample-prototype relationships can reduce diversity, disrupt within-class data distribution, and lead to forgetting as older representations shift toward current task prototypes. To address this, we propose a supervised method using"}, {"title": "2. Related Work", "content": "CL approaches can be broadly classified into three main categories. Rehearsal-based approaches [3, 5, 34, 53] store a small amount of data in a memory buffer and replay them to prevent forgetting. Regularization-based methods [12, 21, 39, 48, 60] penalize changes in network parameters of the current task with respect to the previous task. Meanwhile, instead of using shared parameters, the architectural-based approaches [28, 45, 57] construct task-specific parameters and allow network expansion during CL. This work focuses on regularization-based methods by devising a specific regularization loss to align the current model with the previous one. Additionally, our approach is capable of performing well in both replay-based and memory-free scenarios.\nMost of the current CL methods target primarily to achieve a balance between acquiring new tasks (plasticity) and retaining knowledge of previous tasks (stability) [5, 12, 47, 53]. \u03a4To attain this balance, regularization-based approaches typically employ knowledge distillation (KD) [18], which aims to transfer knowledge from the previous trained model (teacher) to the current one (student). Recent works based on KD directly minimize the divergence between their intermediate representations [19] or final outputs [6, 12]. Additionally, several methods have explored relational KD, which enhances knowledge retention by leveraging the sample-to-sample relationships, such as Instance-wise Relation Distillation (IRD) [5, 53]. Other approaches, such as [2, 26], focus on relationships between learnable class prototypes and individual samples, rather than between samples themselves. In this work, we utilize KD through IRD, alongside a prototype-based distillation method. Our approach differs from [2, 26] in both the use of fixed rather than learnable prototypes, and in how the sample-prototype relationships are applied.\nBeyond balancing plasticity and stability, recent research emphasizes the importance of cross-task consolidation for improving representation and reducing forgetting [29, 61]. In this work, we implicitly address this by designing a plasticity loss function crafted to enhance the cross-task separability."}, {"title": "2.2. Contrastive Learning", "content": "Contrastive learning has emerged as a prominent representation learning approach, demonstrating its SoTA for different downstream tasks [8, 22, 51]. Numerous contrastive methods have been proposed and widely applied in both unsupervised [8, 9, 16, 51] and supervised settings [22].\nIn the context of CL, many studies [5, 12, 26, 35, 53] have shown that contrastive learning is highly effective in acquiring task-invariant representations, which significantly mitigates the primary cause of forgetting data imbalances between previous and current tasks. Among these, Co2L [5] is the first method to apply supervised contrastive learning in CL. Subsequently, CILA [53] improved Co2L by analyzing the importance of coefficients for the distillation loss. Recently, CCLIS [26] emerged as a SoTA method by preserving knowledge through importance sampling to recover previous data distributions. However, unlike Co\u00b2L and CILA, CCLIS cannot operate without a memory buffer, which limits its efficiency in many real-world applications where data privacy is a concern.\nContrastive learning generates augmented views of each sample, bringing positive pairs closer and pushing negative pairs apart, promoting representation invariance to augmentations. Most contrastive methods focus on learning representations through relationships between samples, referred to as soft relationships or \"softness\". In CL, relying solely on softness preserves class diversity and data distribution but can cause task representation overlap, as shown in Fig. 1a. In this paper, we propose a novel supervised contrastive loss for CL that reduces memory dependence and resolves this overlap while maintaining class distribution."}, {"title": "2.3. Neural Collapse (NC)", "content": "A recent study [38] identified neural collapse (NC) phenomenon, where, at the end of training on a balanced dataset, class features collapse to their class means, aligned with a simplex equiangular tight frame (ETF). This finding has led to further research showing that NC represents global optimality in balanced training with cross-entropy [11, 14, 20, 33, 64] and mean squared error [15, 40, 49, 63] loss functions. Inspired by NC, studies such as [24] have used fixed simplex ETF points with modified contrastive loss to achieve NC, while [54] induced NC under imbalanced data conditions by fixing the classifier.\nInducing NC for CL. Building on [13], which showed that NC persists when transferring models to new samples or classes, several CL studies have leveraged NC to mitigate forgetting [55, 56]. These works pre-assign a group of classifier prototypes as a simplex ETF for all tasks and then align sample representations to their corresponding prototypes. The relationship between samples and prototypes, which this learning approach focuses on, is referred to as hard relationships, or \u201chardness\". For instance, [55, 56] employed the dot-regression (DR) loss proposed in [54] for NC-based CL, which is considered a hard learning method. Since the prototype group remains consistent, these methods prevent task overlap but risk reducing diversity and disrupting within-class distribution, potentially leading to forgetting. Our method addresses this issue by integrating NC directly into the loss function, combining both softness and hardness to preserve data distribution and avoid task overlap in CL.\nThe concept of NC can be presented as follows.\nDefinition 1. A simplex Equiangular Tight Frame (ETF) is a collection of K vectors: $Q = {qk}_{k=1}^{K}$, each vector $qk \u2208 R^d, K < d + 1$, which satisfies:\n$q_i^T q_j = \\begin{cases}\n\\frac{K}{K-1} \\text{ if } i = j \\\\\n-\\frac{1}{K-1} \\text{ if } i \\neq j\n\\end{cases} = \\frac{K}{K-1} (I_K - \\frac{1}{K}1_K1_K^T)$, (1)\nwhere $U \u2208 R^{d\u00d7K}$ is an orthogonal basis and $U^T U = \u0406\u043a, IK$ is an identity matrix and 1K is an all-ones vector. Each vector qe has the same 12 norm, and any two distinct vectors consistently produce an inner product of $-\\frac{1}{K-1}$, which is the lowest possible cosine similarity for K equiangular vectors in Rd. This geometric relationship can be described as\n$q_i^T q_j = \\frac{K}{K-1} - \\frac{1}{K-1} \u03b4_{i, j}, \\forall i, j \u2208 [1, K]$, (2)\nwhere di,j = 1 in case of i = j, and 0 otherwise.\nAfter that, the NC phenomenon can be formally characterized by the following four attributes [38]:\nNC1: Features from the last layer within the same class converge to their intra-class mean, such that the covariance \u03a3\u2081 \u2192 0. Here, \u03a3\u2081 = $Avg_{i}\\{(v_{k,i} \u2212 \u03bc_{k})(\u03bd_{k,i} \u2212 \u03bc_{k})^T\\}$, where vk,i is the feature of sample i in class k, and uk is the intra-class feature mean.\nNC2: After centering by the global mean, intra-class means align with simplex ETF vertices, i.e., {$\u03bc_\u03ba$}, 1 \u2264 k \u2264 K satisfy Eq. (2), where $\u03bc_{\u03ba} = (\u03bc_\u03ba - \u03bc_G)/||\u03bc_\u03ba - \u03bc\u03b1||$ and global mean $\u03bc_G = \\frac{1}{K} \u03a3_{k=1}^{K} \u03bc\u03b5$;\nNC3: Intra-class means centered by the global mean align with their classifier weights, leading to the same simplex ETF, i.e., \u03bc\u03ba = wk/||wk||, where 1 \u2264 k \u2264 K and wk is the classifier weight of class k;\nNC4: When NC1-NC3 hold, model predictions simplify to selecting the nearest class center, represented as argmax(z, wk) = argmink||z \u2013 \u03bck||, where \u3008\u00b7,\u00b7) denotes the inner product operator, and z is the output of the model."}, {"title": "3. Preliminaries", "content": "In the general supervised CL scenario, we have a sequence of training datasets, which is drawn from non-stationary data distributions for each task. Namely, let t be the task index, where t\u2208 {1,...,T}, and T represents the maximum number of tasks. The dataset for the t-th task, denoted by Dt, consists of Nt supervised pairs: $D_t = {(x_i, y_i)}_{i=1}^{N_t}$, along with the set of classes Ct. CL comprises a variety of scenarios; however, in this work, we focus specifically on two popular settings: class-incremental learning (Class-IL) and task-incremental learning (Task-IL). In both settings, there is no overlap in class labels across tasks, ensuring Ct\u2229Ct' = \u00d8 for any two distinct tasks t\u2260 t'. For Task-IL, the learned model additionally has access to the task label during the testing phase."}, {"title": "3.2. Supervised Contrastive Learning", "content": "This section details the SupCon algorithm [22], which inspires the \"softness\" component of our approach. Suppose that in each batch B of N samples, SupCon firstly creates two randomly augmented versions of each sample in the batch, making each batch now contain 2N views: B = 2N. After that, given the feature extractor f, each view x\u2081 in the batch is mapped into a unit d-dimensional Euclidean sphere through a linear projector gas zi = h(xi), where h = go f. Consequently, generic presentations are learned through the minimization of the following loss:\n$L_{SupCon} = \\sum_{i=1}^{2N} \\frac{-1}{|P(i)|} \\sum_{j \\in P(i)} log( \\frac{e^{(Z_iZ_j)/\u03c4}}{ \\sum_{k \\in A\\\\{i\\}} e^{(Z_iZ_k)/\u03c4}})$, (3)\nwhere \u3008\u00b7,\u00b7\u3009 is the cosine similarity, \u03c4 > 0 is the temperature factor, A(i) = {1..2N} \\{i}, and P(i) is the index set of positive views with the anchor xi, denoted as:\nP(i) = {p \u2208 {1...2N}|Yp = Yi, p\u2260 i} (4)\nFocal contrastive learning. Despite their advantages, contrastive learning methods often struggle to reduce intra-class feature dispersion. They rely heavily on positive/negative pairs, but most samples are easy to contrast, resulting in minimal loss gradients and scattered intra-class samples. To address this issue, inspired by focal loss [30], [62] introduced the focal contrastive loss. This approach emphasizes hard positive views those with low cosine similarity to the anchor and thus lower prediction probability. Since hard positives are more influential in contrastive loss, they lead to clearer class clustering. Building on these insights, we propose focal contrastive loss in the context of NC for CL, as detailed in Sec. 4.3."}, {"title": "4. Methodology", "content": "Our method uses a two-stage learning process, as in [5]. First, we learn a representation, which is then used to train the classifier. The main objective of CL is to balance two goals: learning new tasks (plasticity) and preserving the knowledge from previous tasks (stability). The overall loss can be described as:\n$L_{overall} = L_{plasticity} + L_{stability}$ (5)\nRemark. While plasticity and stability are central to most CL algorithms, some very recent works have highlighted the importance of cross-task consolidation [29, 61]. Although our approach does not explicitly incorporate a cross-task consolidation term, we will show that it is implicitly accounted for in our Lplasticity\u00b7\na. Plasticity. Previous contrastive learning approaches, like SupCon [22], use sample relationships but allow class representations to shift, as shown in Fig. 1a, causing overlap with current clusters and leading to forgetting. We term these methods \"soft plasticity\u201d. To address this issue, inspired by Neural Collapse, recent works [55, 56] propose using fixed, equidistant prototypes as the optimal class means at the end of training, as shown in Fig. 1b. These methods, which we term \"hard plasticity\", focus on aligning sample representations with their assigned prototypes.\nWhile these \"hard\" methods tightly align representations with prototypes, they have drawbacks. First, they neglect sample relationships, leading to representations clustered only around prototypes, which is not ideal since some samples share characteristics with multiple classes and should lie between them. Second, tightly aligning samples with current prototypes can pull old class representations towards the current task's prototypes, causing forgetting, as shown in Fig. 1b. To harness the benefits of NC while avoiding the pitfalls of hard learning, we propose to integrate both soft and hard learning. This approach maintains cluster distribution within each class and preserves separation from clusters of both current and past classes, as illustrated in Fig. 1c.\nb. Stability. Co2L, the pioneering work in continual contrastive learning, employs IRD as Lstability. The \u201csoft stability\" IRD is designed to preserve the relationships between samples in the old and new feature spaces. However, as training progresses, its effectiveness diminishes\""}, {"title": "4.2. Overview of the proposed method", "content": "Fig. 3 provides an overview of our method (see Sec. 3.1 for setting details). In our approach, memory is optional, and when used, we employ the Reservoir sampling strategy [52] to fill a fixed-size buffer M. After each task beyond the first, current samples are combined with buffered samples, and each sample is drawn independently with equal probability for the mini-batch. We first predefine a set of fixed equidistant prototypes as the vertices of an ETF. We denote this prototype set as P = {$p_i$}{1, K is the number of prototypes, corresponding to the number of classes. These prototypes are used as equidistant optimal points in the feature space. We utilize these prototype points in both learning new tasks and distilling old tasks through their direct use in the corresponding loss functions.\nThe overall objective of our method is:\nL = LFNC2 + LHSD (6)"}, {"title": "4.3. Hardness-Softness Plasticity", "content": "Inspired by NC phenomenon [38] and the concept of focal loss for addressing batch imbalance during training, we introduce a novel loss called Focal Neural Collapse Contrastive (FNC2), defined as:\n$L_{FNC2} = \\frac{1}{2N} \\sum_{i=1}^{2N} \\{- \\frac{1}{|P_i| + 1} \\sum_{j \\in P_i} (1 - C_{ij})^{\\gamma} log(C_{ij}) + (1 - r_i)^{\\gamma} log(r_i)\\}$, (7)\nwhere\n$C_{ij} = \\frac{e^{(Z_iZ_j)/\u03c4}}{\\sum_{k \\neq i} e^{(Z_iZ_k)/\u03c4} + \\sum_{p_l \u2208 P_{1:t-1}} e^{(Z_iP_l)/\u03c4}} \\quad \\text{(cross task)}$ (8)\n$r_i = \\frac{e^{(Z_iP_z)/\u03c4}}{\\sum_{k \\neq i} e^{(Z_iZ_k)/\u03c4} + \\sum_{p_l \u2208 P_{1:t-1}} e^{(Z_iP_l)/\u03c4}}$. (9)\nHere, P(i) is the set of positive indexes for each anchor, as defined in Eq. (4), |P(i)| represents its cardinality, and y is the focusing hyperparameter. Additionally, pz, is the prototype (specifically, the ETF vertex corresponding to label yi), and P1:t-1 represents the set of prototypes used in all previous tasks.\nIntuitively, for each sample in the current task, we pull positive samples closer to the anchor and push negative samples away, forming clusters that are then pulled towards their optimal prototypes, as shown in Eq. (8) and Eq. (9). This approach helps the model learn both hardness and softness information.\nBy incorporating (1 \u2013 Cij) and (1 \u2013 ri), LFNC2 emphasizes hard samples - those that are positive but far from the anchor or far from their prototype. These hard samples are crucial in contrastive learning, as they significantly affect intra-class sample distribution, unlike easy-to-contrast samples. A larger y further focuses the model on training from samples that are distant from their positive views and prototypes. To reduce reliance on memory, we use prototypes from previous tasks as representative points for past samples and include their cosine similarity with the anchor in Cij and ri. This allows our method to use old prototypes as negative points, similar to cross-task consolidation as described in [29, 61], aiding in distinguishing between current and old classes. This approach effectively mimics stored samples and facilitates robust learning even without"}, {"title": "4.4. Hardness-Softness Distillation", "content": "Our new distillation loss, which capitalizes on both hardness and softness plasticity during training, is described as:\n$L_{HSD} = (1 - \u03b1) \\sum_{i=1}^{2N} -o_{t-1}(x_i) log(o_t(x_i)) + \u03b1 \\sum_{i=1}^{2N} -q_{t-1}(x_i; P_{1:t}) log(q(x_i; P_{1:t}))$ (10)\nwith a = max(0, $\\frac{e-e_0}{E}$), e is the epoch index, eo is the number of epochs used for the warm-up period, and E is the number of epochs (details are provided in the Appendix).\nHere, LIRD (used in [5]) represents \u201csoft stability"}, {"title": "5. Experiments", "content": "Datasets. As in other continual contrastive learning works, we use thee datasets: Seq-Cifar-10, Seq-Cifar-100, and Seq-Tiny-ImageNet for all experiments. We consider the class-IL and task-IL settings, described in Sec. 3.1. Seq-Cifar-10 is created from Cifar-10 [23] and divided into five tasks with two classes for each task, Seq-Cifar-100 contains five tasks (20 classes/task) built from Cifar-100 [23], and Seq-Tiny-ImageNet has 10 tasks (20 classes/task) built from Tiny-ImageNet [25].\nImplementation details. We train our method using ResNet-18 [17] as the backbone, and we remove the last layer of the backbone as [5, 12]. Similar to previous works [5, 8, 59], we add a two-layer projection MLP on top of the backbone, followed by ReLU activation functions to map the output of the backbone into d-dimension embedding space, where d = 128 with Seq-Cifar-10 and Seq-Cifar-100, d = 256 with Seq-Tiny-ImageNet. With a batch size of 512, we train the backbone with 500 epochs for the initial task and 100 epochs for other tasks as in [5] for all datasets. Besides, we run with buffer sizes 0, 200 and 500 to evaluate the performance of the model with different memory settings. Regarding the value of y in LFNC2, we choose \u03b3 = 1 for Seq-Cifar-10, and y = 4 for all other datasets. Details about the selection of y and other hyperparameters are provided in the Appendix. Since Co2L [5] does not report results on Seq-Cifar-100 and results in the memory-free setting on Seq-Tiny-ImageNet, we run Co\u00b2L on these cases for comparison. For evaluation, we train a classifier on top of the learned backbone using last-task samples and buffered samples with 100 epochs.\nEvaluation metrics. Similar to [5, 53], we evaluate the quality of the learned encoder ft by training a classifier ht on top of the frozen encoder ft, using only the current training dataset Dt and the samples from the memory M. As defined in [7] as well as in other CL methods [5, 12, 53], we compute the Average Accuracy (AA) on the test dataset and all accuracies AT,k of each task k after learning the final task T. The equation for AA is defined as:\n$AA = \\frac{1}{T} \\sum_{i=1}^{T} A_{T,i}$ (11)\nConsidered methods. To focus on learning ability with limited or no memory, we compare our method only with recent approaches that can operate in both settings. Since CCLIS [26] and LODE [29] cannot operate in memory-free scenarios, their results are excluded in Tab. 2. As shown in Tab. 1, only Co2L [5], the recent method CILA [53], and ours can run without a memory buffer. Additionally, we compare our results with other well-known supervised methods, including ER [43], iCaRL [42], GEM [31], GSS [1], DER [3], Co\u00b2L [5], GCR [50], and CILA [53]."}, {"title": "5.3. Ablation Studies", "content": "Effectiveness of FNC2. To evaluate the efficiency of learning plasticity via LFNC2, we compare it with the asymmetric version of SupCon loss (LoupCon) [5], in combination with different using distillation methods scenario in the Class-IL setting. Note that with buffer size 0, the asymmetric loss Lasymcon become the original SupCon loss (LSupCon). The results in Tab. 3 show that when stability loss is absent, the performance with both LFNC2 and Lasymcon drops significantly, with both yielding approximately the same results. When using distillation methods, LFNC2 outperforms LSupCon in all cases, especially in the case of no memory or small memory size (200).\nEffectiveness of HSD. In assessing the ability of LHSD to preserve prior knowledge, we compare cases without and with different distillation methods, incorporating the plasticity loss LFNC2. The results in Tab. 4 demonstrate that using either LIRD [5] or LS-PRD individually yields minimal changes in performance. However, when both methods are combined in LHSD, performance consistently improves across all buffer sizes.\nEffectiveness of pseudo-replay prototypes. We conduct ablation experiments using the class-IL setup on the Seq-"}, {"title": "6. Conclusion", "content": "We explore the roles of hard (\"hardness\") and soft (\"softness\") relationships in NC-based CL for both plasticity and stability. To address these, we propose two loss functions: LFNC2 for plasticity, which uses fixed prototypes to guide representations towards optimal points while emphasizing hard samples and implicitly incorporating cross-task consolidation through pseudo-replay of old prototypes, and LHSD for distilling both hardness and softness over time. Our approach achieves SoTA in memory-free settings across various datasets and remains competitive with limited buffer sizes in memory-based scenarios.\nLimitations and future work. Like other NC-inducing methods in CL, our approach is limited by the need to predefine prototypes, which is impractical when the number of prototypes is unknown. In future work, inspired by recent advancements [32], we will tackle this issue by pre-defining a maximum number of prototypes and learning their distributions, allowing samples to be associated with multiple prototypes and different weights. Additionally, we plan to explore alternative memory-free evaluation methods, as current approaches including Co2L [5], CILA [53] rely on buffers, which are less effective with limited samples for old classes. We also aim to identify easily forgotten samples and focus on distilling only the core knowledge."}, {"title": "APPENDIX", "content": "To select hyperparameters, we employ a grid search strategy, using a randomly drawn 10% of the training data as the validation set. The considered hyperparameters are:\n\u2022 Learning rate (\u03b7)\n\u2022 Batch size (bsz)\n\u2022 Number of start epochs (E1)\n\u2022 Number of epochs of t-th task (Et\u22652)\n\u2022 Temperature for plasticity loss (T): We use the same T for both Focal Neural Collapse Contrastive (FNC2) and Asymmetric SupCon loss [5]\n\u2022 Focusing hyperparameters (\u03b3) for FNC2 loss\n\u2022 Temperature for instance-wise relation distillation loss (LIRD): As in [5], we use different temperature hyperparameters for the past (Kpast) and current (Kcurrent) similarity vectors\n\u2022 Temperature for sample-prototype relation distillation loss (LS-PRD): We utilize \u015apast for the past and \u015acurrent for the current similarity vectors\n\u2022 Number of warm-up epochs in hardness-softness distillation loss (LHSD) (eo)\nThe corresponding search space of these hyperparameters are provided in Tab. 6. The selections of these hyperparameters are based on the average test accuracy over five independent trials, and the final chosen values are detailed in Tab. 7. For the sake of conciseness and to maintain focus, we omit those hyperparameters previously discovered in the literature.\nFocusing hyperparameter (y). In the FNC2 loss function, y plays a crucial role in determining the level of focus on hard samples (i.e., positive samples that are far from"}, {"title": "B. Additional Experiments", "content": "In addition to the results with small buffer sizes (0 and 200), we run experiments with a buffer size of 500 across different datasets to further assess the effectiveness of our method with a larger buffer. As shown in Tab. 8, although our method does not surpass state-of-the-art methods, it achieves results close to them on Seq-Cifar-10 and Seq-Tiny-ImageNet, underperforming only on Seq-Cifar-100 compared to GCR [50]. This further demonstrates that our method, aside from excelling in memory-free and small buffer settings, remains effective with larger buffers."}, {"title": "B.2. Average Forgetting results", "content": "We utilize the Average Forgetting metric as defined in [7] to quantify how much information the model has forgotten about previous tasks, which as\n$F = \\frac{1}{T-1} \\sum_{i=1}^{T-1} maxt\u2208{1,...,T\u22121}(A_{t,i} \u2013 A_{T,i})$ (12)\nTab. 9 report the average forgetting results of our method compared to all other baselines. The results show that our method can effectively mitigate forgetting, especially even without using additional buffers."}, {"title": "A. Hyperparameter selection", "content": "To select hyperparameters, we employ a grid search strategy, using a randomly drawn 10% of the training data as the validation set. The considered hyperparameters are:\n\u2022 Learning rate (\u03b7)\n\u2022 Batch size (bsz)\n\u2022 Number of start epochs (E1)\n\u2022 Number of epochs of t-th task (Et\u22652)\n\u2022 Temperature for plasticity loss (T): We use the same T for both Focal Neural Collapse Contrastive (FNC2) and Asymmetric SupCon loss [5]\n\u2022 Focusing hyperparameters (\u03b3) for FNC2 loss\n\u2022 Temperature for instance-wise relation distillation loss (LIRD): As in [5], we use different temperature hyperparameters for the past (Kpast) and current (Kcurrent) similarity vectors\n\u2022 Temperature for sample-prototype relation distillation loss (LS-PRD): We utilize \u015apast for the past and \u015acurrent for the current similarity vectors\n\u2022 Number of warm-up epochs in hardness-softness distillation loss (LHSD) (eo)\nThe corresponding search space of these hyperparameters are provided in Tab. 6. The selections of these hyperparameters are based on the average test accuracy over five independent trials, and the final chosen values are detailed in Tab. 7. For the sake of conciseness and to maintain focus, we omit those hyperparameters previously discovered in the literature.\nFocusing hyperparameter (y). In the FNC2 loss function, y plays a crucial role in determining the level of focus on hard samples (i.e., positive samples that are far from"}]}