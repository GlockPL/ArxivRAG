{"title": "Cartesian Genetic Programming Approach for Designing Convolutional Neural Network", "authors": ["Maciej Krzywda", "Szymon \u0141ukasik", "Amir H. Gandomi"], "abstract": "The present study covers an approach to neural architecture search (NAS) using Cartesian genetic programming (CGP) for the design and optimization of Convolutional Neural Networks (CNNs). In designing artificial neural networks, one crucial aspect of the innovative approach is suggesting a novel neural architecture. Currently used architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. In this work, we use pure Genetic Programming Approach to design CNNs, which employs only one genetic operation, i.e., mutation. In the course of preliminary experiments, our methodology yields promising results.", "sections": [{"title": "1. Introduction", "content": "Neural Architecture Search (NAS)[1] has gained considerable traction as a fully automated approach in the design of Neural Networks Architecture. The method facilitates the generation of architectures that are not only comparable but often superior in performance to those crafted manually. Essentially, NAS simplifies the traditional process where humans iteratively adjust neural networks"}, {"title": "2. Cartesian Genetic Programming for Convolutional Neural Network Design", "content": "Pure Cartesian Genetic Programming does not include the crossover operation, so the only genetic operation in CGP is a mutation. Using crossover is a hot topic of research to improve GCP, but is still widely investigated [2]. Several types of mutations can be applied in CGP, such as point mutation, gene mutation, and segment mutation. Point mutation involves randomly changing the value of a single gene in the genome, whereas gene mutation involves replacing an entire gene with a new one. Segment mutation involves replacing a genome segment with a new segment that may contain multiple genes[3, 4]. Here, we present the use of Cartesian Genetic Programming (CGP) for designing convolutional neural networks (CNNs). Our goal was to establish the architecture of the first neural network so that it could be compiled. Still, we did not define it according to existing solutions we wanted the network design process to establish solution on its own. In our approach, the evolution process starts with a parent whose genotype is represented by a set of genes and information about which layers in the neural network are active. Then, two offsprings are created by mutating the parent's genotype, where mutations are performed with a specific mutation rate. Children resulting from the mutation process are judged to see whether they are better than their parents, and if so, that child is the parent of the next generation. We have predefined ConvSets with usable layers, each of which is decoded. The randomly"}, {"title": "3. Experiments", "content": ""}, {"title": "3.1. Experimental Setting and Computational Budget", "content": "In our research, we employed various activation functions, including Relu,\nElu, Selu, Sigmoid, Softmax, Softplus, Softsign, Tanh, and Exponential. Addi-\ntionally, we utilized the following pooling methods: GlobalAveragePooling2D,\nMaxPool2D, and AveragePooling2D. We applied a dropout rate of 0.2 and incor-\nporated BatchNormalization with a momentum of 0.99 and epsilon of 0.001. The\nparameters of the GCP algorithm utilized in our experiments include rows set to\n1, Cols equal to 30, Level-Back set to 10, and both Mutation rate and generation\nspecified as values of the list [0.01, 0.05, 0.1] and [10, 25, 50] respectively.\n\nBudget = P \u00d7G\u00d7E\u00d7T\n\nTo compare our solution, we define the computational budget as follows (equa-\ntion 1) where the population (P) was the count of chromosomes representing a\nsingle CNN. The evaluation epoch (E) is a count of the epoch to train the mutated\nCNN in each generation (G) iteration. Training epochs (T) track the best chro-\nmosome trained during the epoch after the last iterate of generation. We set the\npopulation as one. We define our budget, where in our experiments E (Evaluation\nEpochs) is 25. Evaluation Epochs are used in the evolution process when we try\nto decode CNN architectures that can be compiled and all shapes between layers\nare correct; then we train our architecture in 25 epochs, on a tiny part of the data\n(1000 records) (in a 70/30 ratio) and verify fitness. Then we compare the achieved\nfitness value with others achieved in a given generation, and the best individual\n(with the lowest fitness) is trained in 100 epochs (T).\nIn our approach, we use 10, 25, or 50 generations (G) and always 100 epochs\n(T) to train the best convolutional neural network. To avoid the overfitting effect,\nwe implement an Early Stopping mechanism for monitoring loss function (Cat-\negorial Cross Entropy) with patience set as 10. Each of the best neural network\narchitectures was trained 10 times and the final results were averaged and pre-\nsented in Table 2. Data were shuffled each time in 80/20 proportion. Therefore,\nthe possible computational budgets are as follows:\n\u2022 For G=10: Budget = 1 * 10 * 25 * 100 = 25000 = 25K\n\u2022 For G=25: Budget = 1 * 25 * 25 * 100 = 62500 = 62.5K"}, {"title": "3.2. Experimental results", "content": ""}, {"title": "4. Conclusions", "content": "In the last few years, studies on designing Artificial Neural Networks (ANNs)\nhave become an active research field, mainly due to the advanced cost training\nand prototyping of underlying deep learning architectures. Our approach, dis-\ntinguished from more complex solutions [9, 10, 11], focuses on simplicity. We\nemploy basic CNN layers, disregard data dimensionality post flattening, and in-\ncorporate residual connections. Additionally, our method seamlessly integrates\nstate-of-the-art models as input architecture, facilitating the analysis of changes"}]}