{"title": "End-to-End Graph Flattening Method for Large Language Models", "authors": ["Bin Hong", "Jinze Wu", "Jiayu Liu", "Liang Ding", "Jing Sha", "Kai Zhang", "Shijin Wang", "Zhenya Huang"], "abstract": "In recent years, the breakthrough of Large Language Models (LLMs) offers new ideas for achieving universal methods on graph data. The common practice of converting graphs into natural language for LLMs, which refers to graph flattening, exhibits good generalizability and interpretability. However, the poor organization of the textual format results in poor performance in long-distance scenario understanding. Inspired by human cognitive reasoning habits, we propose a novel method for graph flattening to fit LLMs, termed as End-to-End DAG-Path prompting (EEDP). Experiments on real-world datasets show that EEDP enhances the reasoning performance of LLMs in long-distance scenarios while maintaining excellent performance in short-distance scenarios, demonstrating good robustness in the face of distance variations.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent progress in large language models (LLMs) has been remarkable, showcasing strong text processing capabilities across various tasks, including those involving multi-modal data such as graphs [1]. Graphs are crucial in applications like social networks [2], recommender systems [3], knowledge graphs [4], [5], molecular graphs [6], and natural language processing [7], [8]. Leveraging LLMs for graph data aims to achieve a universal method for graph processing [9].\nLLMs have been applied to tasks with implicit graph structures, such as path planning [10] and multi-hop question answering [11]. Recent research has explored LLMs' ability to understand explicit graph structures [12], [13]. Since LLMs cannot directly handle explicit graphs, they use textual descriptions, known as graph-flattening [14], like adjacency lists. Flatten-based methods fit various downstream tasks, demonstrating generalizability and interpretability.\nMany real-world graph-related tasks require handling long-range dependencies, such as textual coherence analysis, inference chains, and time series forecasting. Existing flattened methods are effective in short-distance scenarios but perform poorly in long-distance ones. The textual format of a flattened graph significantly influences LLM performance [15]. Thus, designing an efficient flattened format for graphs is necessary.\nWe explores optimizing graph-flattening by combining human cognitive habits and real-world graph structures. We propose a novel graph representation method, End-to-End DAG-Path (EEDP) prompting, which leverages main backbone paths within a graph to generate textual descriptions. Wang et al. [12] found learning from examples did not occur in complex graph reasoning problems, so we focus on zero-shot performance. We establish two benchmarks based on real-world data: Merged_1000 and ZINC_test_2500. Experiments demonstrate our method's effectiveness in both long-distance and short-distance scenarios."}, {"title": "II. RELATED WORK", "content": null}, {"title": "A. Graph Description Language", "content": "Graph description languages are standardized languages used to define or represent graph-structured data [15]. Himsolt et al. proposed a Python-like language called Graph Modelling Language (GML) [16], while Brandes et al. introduced an XML-based language called Graph Markup Language (GraphML) [17]. In a broader sense, graph description languages include various traditional representations based on 1-hop connections, such as adjacency lists, adjacency matrices, and edge lists.\nIn graph deep learning, processing large-scale graph structures often requires specialized methods. Some approaches employ walk sequences. RandomWalk [18] starts from a node and randomly jumps to a neighbor, generating a sequence after several hops, which is then used to create new node representations. Node2VecWalk [19] optimizes RandomWalk by weighting transition probabilities. GraphSAGE [20] constructs an ego-graph for each node, starting from a central node and sampling neighbors within a k-hop radius."}, {"title": "III. METHODOLOGY", "content": null}, {"title": "A. Problem Definition", "content": "Graph-flattening method generates a textual representation of the input graph G = {V,E}: Gflat = fflat(G).\nEEDP optimizes fflat similarly to human cognition by using the main backbone paths PathEEDP among endpoints Vend within G as the main components of the textual description.\nWe define endpoints as nodes with either zero in-degree or zero out-degree and main backbone paths as the paths connecting each pair of these endpoints.\nIn our method, a special directed acyclic graph DAGEEDP is generated based on G. Then The definition of Vend can be formalized as below:\nVend = {v | v \u2208 DAGEEDP.V, v.in_degree = 0 \nV v.out_degree = 0}.\nFor every simple path p in G, we use p.start and p.end to denote the start node and end node separately. Then the formal definition of PathEEDP is:\nPathEEDP = {p | p \u2208 G.all_simple_paths(),\np.start \u2208 Vend  p.end \u2208 Vend}"}, {"title": "B. End-to-End DAG-Path Prompting", "content": "To build a flattened-graph GEEDP upon Vend and PathEEDP, we introduce our EEDP framework, as shown in Fig. 1. To further enhance the capability of EEDP in short-distance scenarios, we concatenate PathEEDP together with Gadjlst to get the final textual representation GEEDP. An example of how our framework processes a graph is shown in Fig. 1.\n1) Graph Preprocessing Module: To find the endpoints to generate the PathEEDP, We firstly transfer the graph to a special directed acyclic graph DAGEEDP. DAGEEDP has the properties as sharing the same set of nodes V as the input graph G and containing both nodes with zero in-degree and nodes with zero out-degree.\nThese favorable properties of DAGEEDP ensure the existence of PathEEDP. Therefore, generating DAGEEDP from the input G is the most crucial step in the EEDP method.\nTo obtain the desired DAGEEDP while minimizing information loss, we propose EEDP-DAG Algorithm based on BFS, as shown in Algorithm 1. In EEDP-DAG Algorithm, DAGEEDP is initialized as an empty directed graph. The algorithm traverses the edges of the input graph G in breadth-first order and selectively adds edges to DAGEEDP to avoid cycles.\n2) Path Extract Module: In this part, we build the main part of EEDP upon endpoints. As shown by Theorem 1, all the nodes in Vend of the generated DAGEEDP also exist in G.V. Therefore, Path Extract Module first uses the generated DAGEEDP to find the endpoints Vend in the input graph G. After that, we can extract PathEEDP connecting each pair of the endpoints using depth-fist search (DFS).\nTo help LLMs better reasoning in short-distance scenarios, the adjacency list Gadjlst of the input G is introduced. For each head node in G, we set it as a dictionary key and the adjacent tail nodes are grouped into a list and set as the dictionary value. Equation (2) outlines a formalized description of the construction process of Gadjlst. Here is an example of the adjacency list: {0 : [1, 2, 3], 1 : [2, 3], 3: [2]}."}, {"title": "IV. EXPERIMENTS", "content": null}, {"title": "A. Task Definition", "content": "To assess the model's understanding of graph structures, we designed two edge prediction tasks:\n1. Edge Prediction - Connectivity Prediction (EP-CP): Given a pure graph and a pair of nodes, determine whether there is a directed path from the first node to the second node. The LLM must output a binary classification result, either \"yes\" or \"no.\" The answer is considered correct if the model's response matches the ground truth label.\n2. Edge Prediction - Distance Prediction (EP-DP): Given a pure graph and a pair of nodes, determine if there is a directed path from the source node to the target node. If such a path exists, the model must also output the length of the path. If no path exists, the path length is defined as -1. The answer is considered correct if it accurately reflects the length of any one of the simple paths between the given node pair in the graph."}, {"title": "B. Benchmarks", "content": "We construct two benchmarks from real-world graph data: Merged_1000 and ZINC_test_2500. The statistics of them are listed in Table I. We used accuracy as the evaluation metric.\n1) Merged_1000: Merged_1000 is a dataset built upon real-world educational data. We harvest pure graph structures from various educational knowledge graphs. Each graph represents a correlation between a range of knowledge concepts. The full Merged_1000 dataset contains 1,000 graphs.\n2) ZINC_test_2500: We selected the publicly available real-world dataset ZINC [21]. ZINC is a large molecular graph dataset widely used in computational chemistry. The of-ficial standard split of ZINC yielding a test set of 5,000 graphs. We randomly sampled 2,500 graphs from the ZINC_test set.\nFor each graph, we sampled four groups of node pairs based on their shortest path distances: 1, 2, 3, and over 5 hops. We sampled four pairs of nodes for each distance category, resulting in a total of 16 node pairs per graph. If fewer than four pairs were available for a particular distance category, all available pairs were used. These graphs and test cases constitute the test set used in this study."}, {"title": "C. Experimental Setup", "content": "1) LLM Backbone: In our experiments, GPT-4-turbo [22] is selected as the backbone LLM. GPT-4-turbo is one of the most powerful LLMs, recognized for its high performance and widespread usage in research."}, {"title": "V. CONCLUSION", "content": "We propose an end-to-end graph flattening method, End-to-End DAG-Path prompting (EEDP). Our EEDP method draws on the human cognitive process for graph data, optimizing the graph flattening process. We conduct experiments on our proposed dataset Merged_1000 and ZINC_test_2500. The experimental results show that the EEDP method outperforms all baseline methods in zero-shot prediction scenarios for pure graph structures, demonstrating excellent inference performance in both short and long-distance contexts and robustness to varying connection distances."}]}