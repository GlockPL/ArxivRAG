{"title": "AUDIO VISUAL SEGMENTATION THROUGH TEXT EMBEDDINGS", "authors": ["Kyungbok Lee", "You Zhang", "Zhiyao Duan"], "abstract": "The goal of Audio-Visual Segmentation (AVS) is to localize and segment the sounding source objects from the video frames. Researchers working on AVS suffer from limited datasets because hand-crafted annotation is expensive. Recent works attempt to overcome the challenge of limited data by leveraging the segmentation foundation model, SAM, prompting it with audio to enhance its ability to segment sounding source objects. While this approach alleviates the model's burden on understanding visual modality by utilizing pre-trained knowledge of SAM, it does not address the fundamental challenge of the limited dataset for learning audio-visual relationships. To address these limitations, we propose AV2T-SAM, a novel framework that bridges audio features with the text embedding space of pre-trained text-prompted SAM. Our method leverages multimodal correspondence learned from rich text-image paired datasets to enhance audio-visual alignment. Furthermore, we introduce a novel feature, $f^{CLIP}_{CLAP}$, which emphasizes shared semantics of audio and visual modalities while filtering irrelevant noise. Experiments on the AVSBench dataset demonstrate state-of-the-art performance on both datasets of AVSBench. Our approach outperforms existing methods by effectively utilizing pretrained segmentation models and cross-modal semantic alignment.", "sections": [{"title": "1. INTRODUCTION", "content": "Audio-visual segmentation (AVS) is the task of segment-ing sounding source objects from the video frames. With-out pixel-level supervision, early researchers tackled thesounding source localization (SSL) task by leveraging self-supervised learning [1, 2], exploiting the semantic alignmentbetween image and audio pair data. However, this approachresults in coarse localization of sounding objects, whichlimits the application in fields that require fine-grained seg-mentation masks. Zhou et al. [3] introduced the AVSBenchdataset, annotated with a pixel-level segmentation map. Theinitial works on this dataset often fuse audio and image em-beddings [3, 4] and then decode the segmentation mask fromthe fused audio-visual embeddings.\nThe Segment Anything Model (SAM) [5, 6] introducesan innovative approach to segmentation tasks by leveraginglarge-scale pretraining, similar to the way large languagemodels use prompt-based learning [7]. The prompting ideahas been applied to AVS. Mo and Tian [8] pioneered the ideaof audio-prompted SAM by providing audio as a prompt tothe SAM decoder. Liu et al. [9] advanced the audio-promptedSAM by introducing adapters into the image encoder of SAM.Audio-prompted SAM achieved better performance on theAVS task by leveraging the image segmentation foundationmodel, which mitigates the problem of the limited dataset.However, while SAM empowers the capacity of image under-standing, the burden of learning audio-visual correspondencepersists due to the scarcity of labeled data.\nTo address the data scarcity issue, we propose AV2T-SAM, which projects audio prompts into a text embeddingspace. This enables the use of knowledge from pre-trainedtext-prompted SAM models. Text-image models benefitfrom a wealth of text-image paired datasets, surpassing theavailability of audio-visual data. By transforming audio in-puts into a format compatible with pre-trained text-promptedSAM, AV2T-SAM not only capitalizes on SAM's segmenta-tion expertise but also enhances the learning of audio-visualcorrespondence through access to the text embedding space.\nSpecifically, we introduce a novel feature, $f^{CLIP}_{CLAP}$, which aligns audio and visual embeddings into a sharedsemantic space, leveraging the pre-trained cross-modal en-coders CLIP [10] and CLAP [11]. By leveraging modality-specific projections, the model combines these embeddingsthrough element-wise multiplication, capturing the intersec-tion of audio and visual modalities. This design emphasizesshared semantics while reducing irrelevant information, pro-ducing a more robust and contextually enriched representa-tion. Through this approach, our model effectively enhancesaudio-visual correspondence, enabling more accurate and re-liable segmentation performance. Also, we report a visionbias problem in the single sound source dataset (S4). Wefound the vision bias from the dataset by performing betterthan previous state-of-the-art models without using audioinformation.\nIn summary, our contributions are as follows:"}, {"title": "2. RELATED WORKS", "content": "2.1. Multimodal Learning with Segment Anything Model\nSegment Anything Model (SAM) [5] is the first foundationmodel for image segmentation tasks with various prompts.The model is pretrained with more than 11M images and1B masks. Many studies verified capability of SAM ondiverse downstream tasks, including medical image segmen-tation [12] and pose estimation [13].\nBeyond vision tasks, SAM is also widely used in multi-modal learning due to its flexible prompt interactions. Wanget al.[14] employ remote sensing data to prompt SAM,while Liu et al. [9] use audio to segment sounding objects.\nResearchers have also explored text-based prompting forSAM [15]. In this study, we introduce applying a pretrainedtext-prompted SAM on audio-visual segmentation.\n2.2. Audio-Visual Segmentation\nWith the trends of multimodal learning, audio-visual tasksalso got much attention. Aiming for better audio-visual cor-respondence, researchers worked on Sound Source Localiza-tion task [16, 17] that aims to localize the sound source por-tion from the frame. Due to the lack of a labeled dataset,researchers had to rely on Self-Supervised learning, which,while effective in leveraging unlabelled data, comes with lim-ited applications to fine-grained tasks such as segmentation.Zhou et al. [3] introduce Audio-Visual Segmentation with alabeled AVSBench dataset. The AVSBench dataset enabledresearchers to work on the Audio-Visual Segmentation task,which is more fine-grained than the localization task. With thenew dataset, researchers started exploring audio-visual seg-mentation [3, 18]. Gao et al. [4] proposed an architecturethat leverages the transformer in the AVS task. Liu et al. [19]introduced a new framework that separates the segmentationprocess and the verifying sounding object process.\nMo and Tian [8] first explored utilizing the segment any-thing model with an audio prompt. Claiming the two-layerSAM decoder, which was previously prompted, is too shal-low to model the audio-visual correspondence, Liu et al. [9]introduced the adapter that introjects audio information to thefrozen SAM encoder. Seon et al. [20] employed temporalinformation which was not considered on sam-based models.Nguyen and Park [21] advanced the adapter mechanism byintroducing an additional trainable layer to each block of theSAM encoder and utilized the residual audio encoder. In this"}, {"title": "3. METHOD", "content": "3.1. Problem Formation\nThe input video of audio-visual segmentation (AVS) com-prises of a series of visual frames and corresponding audio.The image frames are represented as $V = {v_i}_{i=1}^T$, where $(v_i\\in \\mathbb{R}^{3\\times H_i\\times W_i})$ where $v_i$ denotes the i-th visual frame withspatial dimensions $H_i \\times W_i$, and T is the total number offrames in the video. Each frame $v_i$ is paired with a corre-sponding 1-second audio waveform $A = a_i\\in \\mathbb{R}^S$, where $S$represents the number of audio samples in 1 second. The goalof the AVS task is to segment all sound source objects fromeach frame. The ground truth segmentation map is providedby binary masks $M_n = {m_i}_{i=1}^n$ where $m_i \\in {0, 1}^{H_i\\times W_i}$\n3.2. Text-prompted Segmentation Module\nThe pre-trained text-prompted SAM model is used in ourframework as a backbone to leverage its robust text-visualcorrespondence knowledge, which is from extensive text-image datasets [22, 23, 24]. While this module is inter-changeable, we particularly used EVF-SAM [15]. The modelemploys the multimodal encoder, which is more efficientthan adopting the Large Language model and performs betterthan the simple text encoder as the CLIP. While the originalEVF-SAM model uses text as input, our method does not relyon explicit text. Instead, we map audio-visual features into thelanguage space. To achieve this, we adapt their methodsby removing the text encoder but keeping the multimodalencoder fixed, leveraging its strong adaptability to promptSAM with text embeddings effectively.\n3.3. Projection Module\nThis module consists of 2 Multi-layer Perceptron (MLP)projection layers that connect $f^{CLIP}_{CLAP}$ feature tothe text embedding space required to prompt the pre-trainedtext-prompted segmentation model.\n3.4. Semantically Aligned Features\nTo enhance the semantic alignment between audio and visualmodalities, we propose $f^{CLIP}_{CLAP}$ feature. Let $a \\in \\mathbb{R}^{d_a}$ and $v \\in \\mathbb{R}^{d_v}$ denote the normalized embeddings for audioand visual inputs, respectively, obtained from CLIP[10] andCLAP[11] encoders. These embeddings are projected into ashared embedding space of dimension $d_s$ using learned pro-jection matrices $W_a \\in \\mathbb{R}^{d_s\\times d_a}$ and $W_v \\in \\mathbb{R}^{d_s\\times d_v}$. Theprojected embeddings are computed as follows:\n$f_{CLAP} = W_a a, f_{CLIP} = W_v v,$\nwhere $a'$ and $v'$ represent the transformed embeddings in theshared space. To capture the intersection of semantic infor-mation from both modalities, we calculate:\n$f^{CLIP}_{CLAP} = f_{CLAP} \\odot f_{CLIP},$\nwhere $\\odot$ denotes the Hadamard (element-wise) product, and$f \\in \\mathbb{R}^{d_s}$ is the fused feature representation.\nThis operation ensures that the resulting feature $f$ em-phasizes shared semantics between the two modalities whilefiltering out modality-specific noise. The design leveragesthe complementary nature of audio and visual information,providing a robust and contextually enriched representationthat enhances downstream segmentation tasks. By project-ing both modalities into a unified semantic space and fus-ing them through intersection, our approach effectively alignsand refines cross-modal information, addressing challenges inaudio-visual correspondence learning.\n3.5. Adapters\nThe mask decoder of SAM, which is two transformer layers,is too shallow to fuse the audio and visual information [9].However, fine-tuning the image encoder of SAM requiresconsiderable computing resources. To efficiently fuse au-dio and visual information while not entirely fine-tuning theimage encoder, Liu et al. [9] suggested an audio adaptermechanism that trains only the adapter module which injectsaudio information to the frozen image encoder.\nIn this work, we employ the adapter module in the maskdecoder of SAM with $f^{CLIP}_{CLAP}$ to effectively fuseprompt information to the image encoder. Specifically, forthe i-th layer of a transformer block, the i-th adapter projectsthe prompt information to the output of the transformer blockand repeats the prompt information along the spatial dimen-sions to match the dimensions of the image features, denotedas $P_i \\in \\mathbb{R}^{T\\times H\\times W\\times C}$. The adapted features are then added tothe output of the previous encoder layer, $E_{j-1}$, to form theinput to the j-th layer, $X_j$, as follows:\n$X_j = E_{j-1}(X_{j-1}) + P_i$.\n(1)\nThis approach ensures the efficient fusion of audio and vi-sual information while preserving computational efficiencyby freezing the image encoder during training.\n3.6. Learning Objectives\nFor training the model, we combined the Binary Cross-Entropy (BCE) loss and the Intersection over Union (IoU)loss, following the Liu et al. [9].\n$\\mathcal{L}_{total} = \\mathcal{L}_{BCE} + \\mathcal{L}_{IOU}$\n(2)"}, {"title": "4. EXPERIMENTAL SETUP", "content": "4.1. Dataset\nAVSBench [3] is the pixel-level audio-visual segmentationdataset, which consists of 5-second videos downloaded fromYouTube. AVSBench contains two datasets: Single SoundSource Dataset (S4) and Multi Sound Source Dataset (MS3).\nSingle Sound Source dataset (S4). This dataset includesvideos with only a single sound source. The dataset con-tains 4,932 videos in total over 23 categories and is split intotraining, validation, and test sets with 3,452, 740, and 740samples, respectively. Notably, annotation details vary acrosssubsets: only the first frame of each training video is anno-tated with binary masks, while the validation and test sets in-clude full annotations for all five frames of every video.\nMulti Sound Sources dataset (MS3). The dataset con-tains 424 videos and is split into training, validation, and testsets with 296, 64, and 64 samples, respectively. For all splits,including training, every frame is annotated with the segmen-tation mask.\n4.2. Evaluation Metrics\nFollowing the original work [3], we use the mean Intersectionover Union ($M_J$) and F-score ($M_F$) to evaluate our model.The $M_J$ computes the average IoU of predicted mask andground truth masks over total frames, and $M_F$ computes theharmonic mean of precision and recall.\n4.3. Implementation Details\nWe utilized EVF-SAM2 [15], which integrates SAM-2-L [6]as the SAM backbone and Beit-3-L [25] as the multimodal en-coder for receiving a text prompt. For experiments using theSAM1 [5] as a backbone, we employed EVF-SAM1, whichconsists of SAM-H and Beit-3-L. The input image resolutionwas set to 1024 \u00d7 1024, and We trained the model for 40epochs using the AdamW optimizer."}, {"title": "5. EXPERIMENTS", "content": "5.1. Main Results\nWe compare our model with state-of-the-art methods in Ta-ble 1. Our proposed model achieved new state-of-the-artperformance on both subsets of ASVBench. On the S4dataset, our model outperform the previous state-of-the-art,SAVE [21], with an improvement of 1.56 $M_J$ without intro-ducing extra adapter layers into every image encoder block.Also, on the MS3 dataset, we exceed the previous state-of-the-art, ST-BAVA [20], without utilizing temporal informa-tion, which enabled ST-BAVA to outperform other approachessignificantly.\nFor a more fair comparison, we also run experiments withthe model with SAM1[26] backbone, considering all previousSAM-based approaches employ SAM1 as a backbone. Underthis setting, our model continues to surpass all existing state-of-the-art methods on the S4 dataset. On the MS3 dataset,it falls marginally short of the best result but still maintainscompetitive performance. When using the SAM2 [6] back-bone, many complex components, such as adapters within ev-ery block of the image encoder as in SAVE, and a module fortemporal understanding as in ST-BAVA, are unnecessary.\nFigure 2 compares our segmentation results with those ofexisting models. In the left portion of the figure, we showone example of the results on the S4 dataset. TPAVI [3] cap-tures only part of the guitar, suggesting weak visual objectunderstanding. SAMA-AVS [9] leverages SAM to accuratelysegment the guitar but struggles to separate it from nearby ob-jects. In contrast, our model fully segments the guitar and dis-tinguishes it from its surroundings, made possible by strongaudio-visual correspondence guided by the pretrained text-prompted SAM. A similar pattern emerges in the right por-tion of the figure, which depicts results on the MS3 dataset.Again, TPAVI exhibits partial segmentation of objects. WhileSAMA-AVS identifies the correct objects, it fails to separatethe keyboard from a person. Our model, however, success-fully segments the keyboard and isolates it from other ob-jects. These findings underscore our model's superior abil-ity to detect and correlate audio with its corresponding vi-sual region, thanks to the capabilities of the pre-trained text-prompted SAM."}, {"title": "5.2. Ablation on Adapters", "content": "We present an ablation study of our framework.In general, methods without adapters perform not as well asthose with adapters, and this also applies to our method.\nInterestingly, comparing all the methods that do not useadapters, our proposed method still achieves much better per-formance than the previous method without adapters. More-over, compared to existing methods that rely on adapters,our approach delivers comparable results an outcome notachieved by any prior methods. This highlights the effective-ness of our framework and feature design.\nFrom the result, we can notice that our framework, evenwithout the adapter module, enhances performance dramati-cally on S4, the dataset which requires shallow audio-visualfusion. However, the model without the adapter module stillsuffers on the Multi Sound Sources dataset (MS4), the datasetthat requires more profound audio-visual. This observationshows that while our approach strengthens audio-visual cor-respondence, it cannot replace the adapter module."}, {"title": "5.3. Ablation on Feature", "content": "In this section, we compare the model's performance withdifferent prompt features. As denoted in a Table. 2, we canobserve that $f^{CLIP}_{CLAP}$ perform best on both metricsover both datasets. This implies that $f^{CLIP}_{CLAP}$ fea-ture, which captures information from both audio and visualmodalities, contains richer information as the prompt than justthe CLAP feature, which contains only audio information.\nIt is worth noticing that the model prompted with CLIP [10]features surpasses the previous state-of-the-art on the S4dataset, achieving a score of 86.67 on $M_J$. Given that CLIPlacks any audio-related information, this result suggests thata performance of 86.67 on $M_J$ can be attained using onlyvisual cues. This finding highlights a significant vision biasin the dataset, raising concerns about its effectiveness inevaluating true audio-visual understanding."}, {"title": "6. CONCLUSION", "content": "In this work, we have proposed a novel framework that lever-ages text modality embeddings and pre-trained text-promptedSAM on the audio-visual segmentation task. Also, we intro-duce a new feature that is aware of the intersection of audioand visual modalities for rich semantic information. Com-prehensive experiment results have demonstrated that our ap-proach performs comparable on the AVS task to state-of-the-arts models. Finally, we demonstrate the vision bias issue ofthe S4 dataset by surpassing previous state-of-the-art perfor-mance without using audio information."}]}