{"title": "LiteSearch: Efficacious Tree Search for LLM", "authors": ["Ante Wang", "Linfeng Song", "Ye Tian", "Baolin Peng", "Dian Yu", "Haitao Mi", "Jinsong Su", "Dong Yu"], "abstract": "Recent research suggests that tree search algorithms (e.g. Monte Carlo Tree Search) can dramatically boost LLM performance on complex mathematical reasoning tasks. However, they often require more than 10 times the computational resources of greedy decoding due to wasteful search strategies, making them difficult to be deployed in practical applications. This study introduces a novel guided tree search algorithm with dynamic node selection and node-level exploration budget (maximum number of children) calculation to tackle this issue. By considering the search progress towards the final answer (history) and the guidance from a value network (future) trained without any step-wise annotations, our algorithm iteratively selects the most promising tree node before expanding it within the boundaries of the allocated computational budget. Experiments conducted on the GSM8K and TabMWP datasets demonstrate that our approach not only offers competitive performance but also enjoys significantly lower computational costs compared to baseline methods.", "sections": [{"title": "1 Introduction", "content": "Mathematical reasoning tasks (Amini et al., 2019; Cobbe et al., 2021; Hendrycks et al., 2021; Lu et al., 2022) have long been acknowledged as challenging. These tasks require transforming a question into a sequence of reasoning steps, which are subsequently executed to derive the correct answer. Recently, large language models (LLMs, Achiam et al. 2023; Touvron et al. 2023; Jiang et al. 2024) have demonstrated remarkable potential in addressing them. A pivotal approach is the employment of Chain-of-Thought (CoT) prompting (Wei et al., 2022; Kojima et al., 2022), which prompts LLMs to break down a question solution into a sequence of reasoning steps before reaching an answer.\nDespite their impressive capabilities, LLMs still face challenges when tackling problems with increasing reasoning steps due to the nature of auto-regressive decoding. This can be analogous to the \u201cSystem 1", "System 1\" capability of LLMs by prompt-engineering, such as hierarchical prompting (Suzgun and Kalai, 2024; Zeng et al., 2023) and automatic prompt refine (Madaan et al., 2024; Yang et al., 2024; Zhou et al., 2024). On the other hand, growing research attention is being paid to promote the \u201cSystem 2\" mode of thought (Daniel, 2017) for LLMs, which is characterized by deliberative thinking steps with back-and-forth refinements. These are the key features for solving complex math reasoning tasks. Particularly, prior efforts have studied enhancing LLMs both at inference time and through self-improvement using tree search algorithms (e.g., DFS and BFS, Yao et al. 2024) and Monte Carlo Tree Search (MCTS, Feng et al. 2023; Tian et al. 2024; Zhang et al. 2024; Wang et al. 2024b).\nHowever, these approaches often necessitate the creation of expert-designed utility functions (Tian et al., 2024; Ma et al., 2023; Kang et al., 2024), making them difficult to be adapted to new scenarios. Moreover, they are computationally intensive, especially when tackling problems that require numerous logical steps (Xie et al., 2024). This is because these methods ineffectively manage the expansion budget (the number of nodes to expand) throughout the search process. As a typical example, BFS adopts a constant budget size throughout the search process, overlooking the fact that some tree nodes do not require much expansion. Some MCTS approaches (Tian et al., 2024) take adaptive budget based on the importance of each node, but they still require a large number of simulations or rollouts for accurate statistics to make decisions, and they overlook other important information, such as the depth (progress) of each node. As the result, there is a pressing need to develop more efficient and adaptable methods for enhancing LLMs' \\\"System 2": "easoning capabilities to effectively handle complex reasoning tasks.\nIn this study, we introduce a guided tree search algorithm with dynamic node selection and node-level exploration budget calculation, aiming to maintain the performance at a moderate cost. Concretely, we employ the value score as guidance to select the most promising node for the next action and expand it within a dynamically computed budget size, navigating exploration-exploitation balance for guided tree search. We continue iterating operations of selection and expansion until the resulting trajectory either meets the expected quality score or surpasses the maximum number of iterations. Notably, the computational budget for each node is inversely correlated to its value score. This is inspired by the observation that nodes with higher value scores are more likely to yield the correct solution upon expansion, hence we allocate fewer computational resources to them to prevent unnecessary computation and vice versa. This not only promotes efficient exploitation, facilitating a faster convergence to the final answer, but also guarantees sufficient exploration to cover enough state space for maintaining performance.\nWe conduct experiments on popular GSM8K (Cobbe et al., 2021) and TabMWP (Lu et al., 2022). Results show that our methods offer competitive performance but significantly less computation costs (saving around 5\u00d7) compared to other baselines. Detailed analyses confirm the usefulness of each component and provide more practical options for various settings. Additionally, we also identify the limitations of this research line and suggest possible ways to tackle them."}, {"title": "2 Related Work", "content": "Thanks to the robust capabilities of LLMs, significant advancements have been made in mathematical reasoning tasks, surpassing traditional approaches that rely on semantic parsing (Matsuzaki et al., 2017; Hopkins et al., 2017) or Abstract Syntax Tree (AST) decoding (Li et al., 2019; Qin et al., 2021; Wu et al., 2021).\nSome studies improved the reasoning capabilities of LLMs through further training. These efforts involved either manually annotating or automatically generating feasible and challenging problems to fine-tune the LLMs (Luo et al., 2023; Yu et al., 2023; Liu and Yao, 2024; Toshniwal et al., 2024), as well as devising sophisticated techniques, such as reinforcement learning, for efficient training (Luo et al., 2023; Wang et al., 2023; Lightman et al., 2023; Chen et al., 2024).\nAnother line of research focuses on inference-time improvement. Except for the popular self-consistency (Wang et al., 2022), most of these studies treat this task as a tree search problem and investigate various searching algorithms. Yao et al. (2024) were the first to introduce Tree-of-Thought (ToT), incorporating Depth-First Search (DFS) and Breath-First Search (BFS) to address reasoning problems. Khalifa et al. (2023); Zhu et al. (2024); Xie et al. (2024) applied step-wise Beam Search to math problems, which operates similarly to BFS under certain parameter conditions. To guide the search process, these studies above either directly prompt the LLMs to evaluate the quality of each step (Yao et al., 2024; Xie et al., 2024), or train a verifier on corresponding datasets to achieve better performance (Khalifa et al., 2023; Zhu et al., 2024).\nLater research delved into other sophisticated search algorithms, such as Monte Carlo Tree Search (MCTS, Tian et al. 2024; Zhang et al. 2024; Wang et al. 2024b), A* (Ma et al., 2023), and Levin Tree Search (Kang et al., 2024). Nonetheless, these approaches necessitate more robust verifiers to steer the search procedure. Concretely, Tian et al. (2024) utilize a blend of the value function, Process-supervised Reward Model (PRM), and Outcome-supervised Reward Model (ORM). Ma et al. (2023) and Kang et al. (2024) train their PRM models on PRM800K (Lightman et al., 2023), which offers manual annotations for 800k reasoning steps of problems from MATH (Hendrycks et al., 2021).\nThis study also follows the same research line, yet it concentrates on developing an efficient algorithm to decrease computation costs while maintaining performance. Besides, we employ a naive but more practical value network as the verifier, which is trained solely with the final answer labels as distant supervision."}, {"title": "3 LiteSearch", "content": "Algorithm 1 LiteSearch\nRequire: question q, maximum iterations N, threshold \u03b5, policy \u03c0, value network v\n1: Initialize tree T with q as the root\n2: Set i \u2190 0, \u0177 \u2190 null\n3: while i < N do\n4: Select node s' from T using Eq. 2\n5: Expand s' to obtain its child nodes C using Eq. 3\n6: for c \u2208 C do\n7: S \u2190 return_path(T, c)\n8: if is_terminal(S) and v(S) > \u03b5 then\n9: Set \u0177 \u2190 S\n10: break\n11: end if\n12: end for\n13: i \u2190 i + 1\n14: end while\nEnsure: \u0177\n3.1 Guided Tree Search Algorithm\nTaking each math reasoning question q as a tree search problem, we initialize the root of the search tree with question q, while the other tree nodes represent reasoning steps (e.g., si) generated by an LLM (denoted as policy \u03c0). Concretely, we treat an (incomplete) trajectory q, s1, ..., si as the state S\u03b9. Then, a next step can be sampled from the LLM which consumes S\u03b9:\nsi+1 \u223c LLM(D, si), (1)\nwhere D is the in-context demonstrations made of question-solution pairs.\nAs shown in Alg. 1 and Fig. 2, our algorithm mainly comprises an iterative process of Selection (\u00a73.1.1) and Expansion (\u00a73.1.2) operations. For each loop, we first select the most promising node, and then expand it within the constraints of the computational budget. Both operations are guided by a value network v (\u00a73.2). The algorithm terminates when the generated answers meet the expected value threshold \u03b5 or the number of iterations reaches the limit N."}, {"title": "3.1.1 Selection", "content": "We mainly select the tree node with the highest value for expansion. Besides, we introduce a progress term, denoted as p(S), which quantifies the advancement of a state S towards the goal within the search trajectory. By incorporating this term, we prioritize the exploration of nodes that are expected to lead more rapidly to the final answer.\ns' = maxsi(v(si) + \u03bbp(si)), (2)\nwhere s' denotes the selected node, and \u03bb is introduced to regulate the impact of the progress term. It is non-trivial to estimate the progress of a state, thus we introduce an empirical approach based on the trajectory of greedy decoding. Specifically, we compute the progress by comparing the number of tokens or steps from a given state to those of the corresponding greedy decoding. For example, when using step number as the metric, a state with d steps has progress of d/\u03b4, where \u03b4 denotes the total number of steps in the greedy decoded trajectory."}, {"title": "3.1.2 Expansion", "content": "During the expansion phase, we aim to balance exploitation and exploration by effectively managing the computation budget allocated to the selected node. Intuitively, an appropriate budget size can promote efficient exploitation, facilitating a faster convergence to the final answer, while also guaranteeing sufficient exploration to cover enough state space for reducing uncertainty. In line with this spirit, we further explore two strategies preferring either exploitation or exploration: Incremental Expansion and Batch Expansion."}, {"title": "Budget Computaton", "content": "We define the allocated budget for a node S as the maximum number of its children, denoted as b, which primarily depends on the value v(S) and depth d of that node.\nb = min(\u03b5log(1 \u2212 \u03b5)dlog(1 \u2212 v'(S)), B), (3)\nwhere B denotes the upper bound of the budget and \u03b5 is the expected accuracy, thus a larger \u03b5 (e.g., 0.95) encourages more conservative searching. Besides, we employ the 1/d term, which fosters exploration at the start of searching but encourages exploitation with d increasing to avoid search space explosion. As the value scores of the preceding search steps usually suffer a larger variance due to inefficient learning of delayed and sparse rewards (Sutton and Barto, 2018), confidence estimation of them is relatively not accurate enough. This inevitably influences the computation of suitable budget sizes. Therefore, we propose to further calibrate value scores using the values of corresponding trajectory from greedy decoding (denoted as \u00fb), especially for the first few steps.\nv'(S) = v(S) + \u00fb/d1 + 1/d, (4)\nwhere v'(S) represents the enhanced value calibrated by \u00fb after normalization. We add 1/d term to mainly help the first several steps."}, {"title": "Expansion Strategies", "content": "We propose two expansion strategies that prioritize efficiency and performance, respectively.\n\u2022 Incremental Expansion: This approach incrementally expands one child node after another. If the budget allows, the same node can be reselected until the budget is fully utilized. This method tends to conserve computational resources by carefully managing the budget.\n\u2022 Batch Expansion: In contrast, this strategy consumes the entire budget allocated to a node during each iteration, resulting in the generation of multiple child nodes simultaneously. This method broadens the search space for subsequent iterations, potentially leading to the identification of superior nodes and enhancing overall performance."}, {"title": "3.2 Value Network", "content": "The value network v(S) seeks to approximate the expected cumulative reward starting from state S and following a policy \u03c0 thereafter. This can be represented as v(S) = E\u03c0[Rt | St = S], where Rt is the discounted return starting from state St. Particularly, given a question q and its correct answer y from an expert demonstration dataset. Each trajectory with reasoning steps (e.g., si) and final predicted answer \u0177 is firstly sampled from the LLM (policy \u03c0):\ns1, ..., sn, \u0177 \u223c LLM(D, q). (5)\nThen, we only take the answer correctness as distant supervision for each reasoning step to train the value network via Mean Squared Error (MSE) loss:\nL = (v(Si) \u2212 I[y = \u0177])2, (6)\nwhere I denotes an indicator function.\nIn this work, regardless of the policy used, we simply take Llama3-8B with a regressive head as our value network. This regressive head is a randomly initialized linear layer, which consumes the hidden state of the last input token and returns a scalar within [0, 1]:\nv(Si) = Head(Llama3(P, S\u03b9)[\u22121]), (7)\nwhere P is an instruction to help learning. An example is shown in the Fig. 9."}, {"title": "4 Experiment", "content": "4.1 Setup\nDataset We conduct experiments on two popular mathematical reasoning datasets:\n\u2022 GSM8K (Cobbe et al., 2021): This dataset comprises 7,473 training and 1,319 testing grade school math word problems that take 2 ~ 8 steps to solve. Solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations.\n\u2022 TabMWP (Lu et al., 2022): This dataset features 38,431 tabular math word problems, presented in either free-text or multiple-choice formats. We focus on the more general free-text category, consisting of 17,315 training questions and 1,000 randomly sampled test questions for evaluation. In contrast to GSM8K, reasoning in TabMWP is based on the provided tables. Additionally, it spans a wider range of domains and supports various answer types.\nModels and Hyperparameters We employ Mixtral-8\u00d77B (Jiang et al., 2024) or Llama3-8B as the policy model and train Llama3-8B as the value network. For the policy models, we adhere to the standard approach of utilizing 8/4 shots in-context learning for GSM8K/TabMWP, with a temperature of 0.6. By default, we set N, B, \u03bb, \u03b5, \u03f5 as 100, 10, 0, 0.8, and 0.9, respectively, and investigate other combinations in our analyses. For the value networks, we sample 8 trajectories per training instance, also with a temperature of 0.6. Then, we train the models for 1 epoch across both datasets, employing the AdamW optimizer (Loshchilov and Hutter, 2017) with a learning rate of 5e-6 and a linear learning rate scheduler. Besides, we allocate 5% of the training instances as a development set to select the optimal checkpoints as value networks.\nEvaluation Metrics We adopt answer Accuracy and the number of generated tokens (Tokens (k)) as evaluation metrics for performance and cost, respectively. It should be noted that we do not take into account the cost of executing value networks. This is because a value network only performs the regression task, which incurs significantly lower costs compared to the primary generation task. Besides, it also can be deployed in parallel in practice.\nBaselines We consider the following baselines:\n\u2022 Greedy Decoding: It intuitively selects the most probable next token at each decoding step.\n\u2022 Hard Voting@K (SC, Wang et al. 2022): Known as self-consistency, which ensembles the answers from multiple sampled solutions as the final answer using majority voting. We sample K = {5, 10, 20} times with a temperature of 0.6.\n\u2022 ToT-DFS (Yao et al., 2024): We implement it by capitalizing on guidance from our trained value network. Specifically, we prune a node if its value score falls below a threshold of 0.5 and limit the maximum number of children to 5 to prevent infinite loops.\n\u2022 ToT-BFS/BeamSearch (Khalifa et al., 2023; Yao et al., 2024; Xie et al., 2024; Zhu et al., 2024): These two methods work similarly for this task. Again leveraging our value networks, we ask each node to expand 5 children and only keep 5 nodes with the highest value scores at each depth to avoid search space explosion.\n\u2022 Soft Voting@K: It is an enhancement over hard voting by utilizing our value networks. It softly ensembles the answers of different paths by taking their value scores as weights."}, {"title": "4.2 Development Experiments", "content": "As depicted in Fig. 3, we analyze the alignment between value scores and final correctness. We employ the widely-used Brier score as our evaluation metric, which is the MSE in effect, indicating that lower scores are preferred.\nGenerally, as the number of steps increases, we notice a descending trend. This observation suggests a better correlation between the value scores and the final correctness in subsequent steps. Remarkably, the Brier scores span from 0.21 to 0.14, marking an improvement of merely 0.07. This echoes the conclusion of previous research (Tian et al., 2024) and paves the way for our method by taking value scores as estimated confidence of final correctness."}, {"title": "4.3 Main Results", "content": "Table 1 shows the main test results on GSM8K and TabMWP. We observe the following conclusions:\nValue Guidance Boosts Model Performance In line with prior research (Wang et al., 2022), Hard Voting significantly improves Accuracy. However, its costs also proportionately increase with the growing of sampling size K. With the guidance of our value networks, both Soft Voting and tree search algorithms can further enhance Accuracy without incurring additional costs. Besides, Soft Voting@5 consistently surpasses Hard Voting@20, substantiating the effectiveness of verification as previously discussed in (Cobbe et al., 2021).\nCurrent Tree Search Algorithms Neglect the Performance-Cost Tradeoff Previous methods, TOT-DFS and ToT-BFS, prefer different evaluation metrics. Among the value-guided approaches, ToT-DFS consistently has the lowest cost but achieves suboptimal performance. This is because ToT-DFS focuses mainly on pruning bad nodes and lacks the flexibility to select better nodes for further improvement. In contrast, ToT-BFS tackles this shortcoming of ToT-DFS by maintaining a branch of nodes with the highest values, thereby resulting in better performance. However, it also unnecessarily visits lots of nodes during the search, leading to significantly higher costs.\nDynamic Selection and Expansion Maintain Performance and Decrease Cost By fully utilizing the guidance from value networks, our methods achieve the best tradeoff between performance and cost. Our approach falls within the cost range of ToT-DFS and Soft Voting@5, yet yields significantly better performance. For the two expansion strategies, Incremental saves nearly 20% of costs of Batch and performs even better on TabMWP. However, Incremental performs noticeably worse than Batch on Accuracy on GSM8K, with a 2.6-point lower score. This is due to Batch providing a better comparison among nodes for selection by expanding more nodes each time. It is worth noting that both of our methods often cannot outperform Soft Voting@20 on Accuracy. We will provide detailed analyses in \u00a74.5."}, {"title": "4.4 Ablation Study and Analyses", "content": "Dynamic Budgeting Helps Both Performance and Cost-Efficiency We first study the effectiveness of the dynamic budget size b, which is decided by Eq. 3. The following variants are considered: (1) \u21d2 static budget: We directly set b as B, resulting in each node being expanded with a fixed budget size; (2) w/o depth penalty: We remove the 1/d term from Eq. 3, which previously penalized b as the depth d increased; (3) w/o greedy value: We do not consider Eq. 4 to calibrate value scores with greedy results.\nAs shown in Table 2, we observe that dynamic budgeting helps in both Incremental and Batch by allowing them to maintain higher accuracy with fewer tokens compared to all other variants. Specifically, \u21d2 static budget severely hurts both performance and cost, particularly leading to 3 times computation costs when using Batch Expansion. w/o depth penalty and w/o greedy value perform competitively for Incremental, but still have considerable negative influence on Batch. These results highlight the importance of dynamic budgeting especially in scenarios where Batch Expansion is employed.\nInfluences of Budget Limitation Budget limitation B decides the upperbound of budget size b. As illustrated in Fig. 4, we observe a clear tradeoff between performance and cost. With the growth of B, the computation cost also increases correspondingly because larger budget sizes are allocated to challenging states with lower value scores. Consequently, more problems are correctly solved due to more comprehensive searching. Regarding the two expansion strategies, Incremental perform slightly better than Batch with competitive accuracy but fewer costs when B \u2264 3. This is because it may not use up all budgets when good nodes have been generated during incremental expansion. However, Batch yields better accuracy by taking more costs when B = {5, 10} because it fully utilizes allocated budgets, thus providing larger search space for better selection.\nInfluence of Progress Estimation We then investigate the choice of p(S) and \u03bb in Eq. 2. We consider step number and token number against corresponding results of greedy decoding to estimate p(S). As depicted in Fig. 5, increasing \u03bb improves cost-efficiency by prioritizing nodes with faster progress at the risk of inaccuracy. Comparing step number and token number, the former is relatively better with a modest downward trend. By sacrificing 1.3 points in accuracy, utilizing step number and \u03bb = 0.15 saves nearly 20% computational costs. In contrast, the efficacy of token number is unsatisfactory. This can be attributed to its higher degree of variability, thus yielding less precise estimates of progress.\nHarder Problems are Allocated Larger Budgets Fig. 6 illustrates the correlation between cost and question difficulty. Inspired by (Wang et al., 2024a), we estimate the difficulty of a question by computing the frequency of the gold answer in multiple sampled paths after inversion (\u201c1 \u2013 x\"). We observe that for easier questions, our methods cost competitively to Greedy Decoding. However, as the difficulty escalates, the cost of our method also rises proportionately. Regarding our expansion strategies, Batch consistently takes higher costs and the gap also widens with the difficulty increases.\nMixture-of-Domain Boosts Performance An important future direction is to construct a general value network that can address questions from different domains. To validate the potential of this direction, we conducted experiments using value networks trained with different ratios of TabMWP data and full GSM8K data. Despite the significant difference in question style and answer type, the results in Fig. 7 demonstrate that using a mixture of different domains helps improve search performance, especially when training instances from the target domain are scarce (0.75 vs. 0.78 on Accuracy when using 1% TabMWP data). This highlights the effectiveness of building robust and stronger value networks by collecting various training instances.\""}, {"title": "4.5 Limitations of Guided Tree Search", "content": "Analyses above have shown the effectiveness of our method. However, though much more efficient, guided tree searches often cannot outperform Soft Voting on Accuracy. This section will provide detailed analyses to answer this question.\nLarger Variance of Values at the First Few Steps We first collect the questions that our approach fails to solve, yet are successfully addressed by Soft Voting@20. Fig. 8 displays a lemon pick example. We observe that our value network can select the correct answer when the complete rationale is provided. However, the first two steps in the correct path are scored much lower than the first step of the wrong solution. This results in a reduced priority in exploring the node that is actually of higher quality. We also compute the average best ranking of values for correct solutions across the 20 sampled paths for these questions. Results indicate that the rank for the first step is 3.0, whereas the final step achieves a rank of 1.4. This finding highlights the larger variance of values at the first few steps, which subsequently results in inadequate exploration of high-quality nodes and finally fails to yield the correct answer.\nVoting Helps when Values are Inaccurate Due to the imperfect value network, some incorrect paths may be erroneously scored higher than the correct ones. Guided tree search methods, which search for only one path as the final answer, inevitably fail in these instances. However, Soft Voting can mitigate this issue by leveraging the benefits of both majority voting and the value network. Consequently, even if the highest value is attained by an incorrect path, Soft Voting still has the potential to reach the correct answer with a higher frequency. As demonstrated in Table 3, the use of voting enables Soft Voting@20 to outperform Best@20, highlighting the efficacy of voting in enhancing accuracy.\nInspired by these findings, we further investigate the improvement of our method using voting. Specifically, we discard the answers predicted by our method when their value scores fall below a threshold \u03b1. Generally, these predictions exhibit a higher error rate due to the correlation between value scores and correctness. Subsequently, we employ Soft Voting to address these unresolved questions. The results in Table 3 indicate that accuracy can be significantly improved by increasing \u03b1. However, the associated costs also rise substantially, albeit remaining lower than those of Soft Voting@20."}, {"title": "5 Conclusion", "content": "In this work, we study guided tree search to address math problems, aiming to decrease the computation costs while maintaining the performance. Inspired by the theory of value function, we propose dynamic node selection and expansion strategies, which dynamically determine the priority of nodes to explore and manage the computational budget during expansion. Both procedures are guided by an easy-to-implement value network trained without step-wise supervision. Experiments show that our methods achieve competitive performance with typical baselines but significantly save computation costs. Ablation studies validate the effectiveness of each component, providing more feasible options for various practical scenarios. Besides, we also identify the shortcomings of this research line, and provide a potential strategy for addressing these issues."}]}