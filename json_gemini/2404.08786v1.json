{"title": "NeuroLGP-SM: Scalable Surrogate-Assisted Neuroevolution for Deep Neural Networks", "authors": ["Fergal Stapleton", "Edgar Galv\u00e1n"], "abstract": "Evolutionary Algorithms (EAs) play a crucial role in the architectural configuration and training of Artificial Deep Neural Networks (DNNs), a process known as neuroevolution. However, neuroevolution is hindered by its inherent computational expense, requiring multiple generations, a large population, and numerous epochs. The most computationally intensive aspect lies in evaluating the fitness function of a single candidate solution. To address this challenge, we employ Surrogate-assisted EAs (SAEAs). While a few SAEAs approaches have been proposed in neuroevolution, none have been applied to truly large DNNS due to issues like intractable information usage. In this work, drawing inspiration from Genetic Programming semantics, we use phenotypic distance vectors, outputted from DNNs, alongside Kriging Partial Least Squares (KPLS), an approach that is effective in handling these large vectors, making them suitable for search. Our proposed approach, named Neuro-Linear Genetic Programming surrogate model (NeuroLGP-SM), efficiently and accurately estimates DNN fitness without the need for complete evaluations. NeuroLGP-SM demonstrates competitive or superior results compared to 12 other methods, including NeuroLGP without SM, convolutional neural networks, support vector machines, and autoencoders. Additionally, it is worth noting that NeuroLGP-SM is 25% more energy-efficient than its NeuroLGP counterpart. This efficiency advantage adds to the overall appeal of our proposed NeuroLGP-SM in optimising the configuration of large DNNS.", "sections": [{"title": "I. INTRODUCTION", "content": "Evolutionary Algorithms (EAs) [9] have proven to be effective in both the crafting of architectures and hyperparameter optimisation of Deep Neural Networks (DDNs) [25]. This application is commonly known as neuroevolution, a widely explored field highlighted by the abundance of scientific publications and impactful outcomes [11], and have been applied to numerous problem domains, such as autonomous vehicles [14], face recognition [8]. The pursuit of optimal DNN architectures has led to various methodologies, including EAs [11], reinforcement learning [44], etc. However, a significant challenge persists across these methods: the substantial computational resources required to identify high-performing networks.\nThe rise of GPU-accelerated hardware has helped alleviate some of this computational cost, however, a significant proportion of research in DNNs is based on incremental improvements on DNN algorithms for benchmark problems [33], where there is a significant correlation between network complexity for incremental gains in terms of additional performance. In fact, when looking at very large models of hundreds of billions of parameters, it can cost millions of dollars for a single iteration [26]. This energy consumption is further compounded when considering population-based neuroevolutionary techniques which require many networks to be trained and evaluated in order to find suitable architectures.\nOne way to address this significant issue is with the use of surrogate-assisted evolutionary algorithms (SAEAs). SAEAS can be used to estimate the fitness of DNNs without the need to fully train each network. In particular, surrogate modelling strategies that employ Bayesian optimisation have shown much promise [26]. However, a major challenge remains in how best to deal with the surrogate representation. For instance, using genotype information to build surrogates often requires complex encoding strategies [41], and in some instances, constructing adequate distance metrics to compare different network topologies is not feasible [40]. Using phenotypic information on the other hand has shown some promise [19], [40], but a challenge remains in scaling to deeper and more complex networks which inherently requires a high-dimensional representation [39].\nIn this work, we analyse a novel population-based Neurovolutionary technique, referred to as NeuroLGP, and its surrogate model variant NeuroLGP-SM [37]. Using a robust model management strategy, we use phenotypic distance vectors to estimate the performance of partially trained DNNs. These vectors are comparatively large for the optimisation problem at hand [39] and, as such, we use an approach that is designed for handling high-dimensional data, known as Kriging Partial Least Squares (KPLS). This approach allows for a novel and scalable surrogate-assisted technique that is skilfully adept at handling neuroevolution of DNNs and to the the best of our knowledge, this method of surrogate-assisted neuroevolution has not been studied before.\nThe aim of this study is to apply Surrogate-assisted Evolutionary Algorithms (SAEA) in neuroevolution, using Kriging Partial Least Squares (KPLS) on phenotypic distance vectors inspired by Genetic Programming Semantics. Our key contributions are outlined as follows, 1) Efficient Fitness Estima-"}, {"title": "II. RELATED WORK", "content": "Santos et al. [34] proposed a novel approach that makes use of semantics in neuroevolution. They do so by using Geometric Semantic Genetic Programming, in conjunction with a neuroevolutionary approach called Semantic Learning Machines (SLM) [16], a form of neuroevolution technique. Furthermore, Hagg et al. [19], made the connection between semantic distances in GP and phenotypic distances within the context of surrogate-assisted evolutionary algorithms for neuroevolution. Similarly, Stork et al. [40] extended CGP to use a surrogate-assisted neuroevolution approach that makes use of phenotyic distance vectors. A limitation of Stork's work is the scalability of using Kriging on high-dimensional data.\nFor instance, in the recent work by Stapleton and Galv\u00e1n, highlighted that traditional approaches such as the Kriging approach suffer with high-dimensionality [39] and may not be suitable for DNN architectures.\nFreeze-Thaw Bayesian Optimisation (FBO), proposed by Swersky et al. [43], uses Bayesian optimisation to determine whether a particular neural network that has been partially trained should be fully evaluated. This approach is novel in that the system stops training (or freezing) of less promising networks, instead spending valuable resources on the most promising networks. Of note, is the fact that the FBO approach relies on the phenotypic behaviour of DNNs. It is important to note the majority of works rely on genotypic information when building surrogate models for neuroevolution [10], [17], [41]. For reference, a major work in this regard is that of Sun et al. [41] referred to as the End-to-End Performance Predictor. This approach uses an offline surrogate model based on random forests to accelerate learning of CNN architectures. The CNN architecture is encoded such that it maps to a numerical decision variable, which is passed to the random forest-based surrogate-model. Not only does this approach not require large amounts of training time but, also alleviates the requirement of a smooth learning curve [43]."}, {"title": "III. BACKGROUND", "content": "Surrogate modelling has many use cases but in the context of this work, the aim is to effectively estimate the fitness values for candidate solutions while simultaneously reducing the run time of the evolutionary process. To this end, not only must the surrogate model be well-posed, but also, the evolutionary process must interact with a robust surrogate model management strategy [20]. The surrogate model differs from the parent model that instead of training directly on the data the surrogate model is trained on the design space, where the aim is to identify and further explore regions of this design space that will produce preferable parameters. As such, interpolation-based approaches may be used to simulate the unknown regions of the parameter space, such as Gaussian processes, commonly referred to as Kriging. Another benefit to the Kriging approach is that it allows for estimates of the uncertainty of predictions.\nKriging is an interpolation-based technique that assumes spatial correlation exists between known data points, based on the distance, and variation between these points. We aim to use observations {y(x1), y(x2),...y(xn)} to help estimate an unknown function value \u0177(x') for the unknown data point x', where n is the number of individuals in the training data. A kernel function K(\u00b7) is used to express the spatial correlation between two samples Xi and x as shown in Eq. 1,"}, {"title": "A. Surrogate assisted models: Kriging and Kriging Partial Least Squares", "content": "Surrogate modelling has many use cases but in the context\nof this work, the aim is to effectively estimate the fitness\nvalues for candidate solutions while simultaneously reducing\nthe run time of the evolutionary process. To this end, not\nonly must the surrogate model be well-posed, but also, the\nevolutionary process must interact with a robust surrogate\nmodel management strategy [20"}]}