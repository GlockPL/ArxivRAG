{"title": "Generalizing From Short to Long: Effective Data Synthesis for Long-Context Instruction Tuning", "authors": ["Wenhao Zhu", "Pinzhen Chen", "Hanxu Hu", "Shujian Huang", "Fei Yuan", "Jiajun Chen", "Alexandra Birch"], "abstract": "Long-context modelling for large language models (LLMs) has been a key area of recent research because many real world use cases require reasoning over longer inputs such as documents. The focus of research into modelling long context has been on how to model position and there has been little investigation into other important aspects of language modelling such as instruction tuning. Long context training examples are challenging and expensive to create and use. In this paper, we investigate how to design instruction data for the post-training phase of a long context pre-trained model: how much and what type of context is needed for optimal and efficient post-training. Our controlled study reveals that models instruction-tuned on short contexts can effectively generalize to longer ones, while also identifying other critical factors such as instruction difficulty and context composition. Based on these findings, we propose context synthesis, a novel data synthesis framework that leverages off-the-shelf LLMs to generate extended background contexts for high-quality instruction-answer pairs. Experiment results on the document-level benchmark (LONGBENCH) demonstrate that our proposed approach outperforms previous instruction synthesis approaches and comes close to the performance of human-annotated long-context instruction data.", "sections": [{"title": "1 Introduction", "content": "Recent advances in large language models (LLMs) have significantly extended the length of the context that they are able to ingest by addressing the problems of efficient attention and encoding of positions (Su et al., 2024; Peng et al., 2024; Fu et al., 2024; Dubey et al., 2024). However, for better performance in downstream long-context tasks such as document-level question answering and summarization (Shaham et al., 2023; Bai et al., 2024b;Karpinska et al., 2024), these models still require instruction-tuning. Although previous work has looked at synthetic instructions, we still do not understand how to best leverage existing instructions for optimal use with a long-context model. This is a critical challenge, as it is difficult to create high quality synthetic long-context instructions.\nInitial work on long-context instructions leverages off-the-shelf LLMs to generate instruction-answer pairs from existing long text passages (Bai et al., 2024a; Dubey et al., 2024). While this method prioritizes context length but overlooks other critical aspects of the synthetic data such as quality, difficulty and diversity. These aspects are inherently constrained by an underlying paradox: the synthesized data quality relies on an LLM that can understand the lengthy input text\u2014which is the problem it is tackling in the first place.\nThis paper first presents a pilot study on artificial needle-in-a-haystack tests (Kamradt, 2023; Hsieh et al., 2024) which allows for rigorous control of different aspects of the instruction data. This study yields three key findings: (1) instruction quality plays a crucial role in model performance; (2) models instruction-tuned on short contexts can generalize to much longer ones; (3) training with evidence embedded in distracting content helps models develop robust information extraction abilities."}, {"title": "2 Related Work", "content": "Towards Long-context LLMS To enable LLMs to support longer context, most previous research focuses on the pre-training stage by modifying rotary position embeddings (RoPE) (Su et al., 2024; Peng et al., 2024) along with continued pre-training on longer sequences (Chen et al., 2023; Rozi\u00e8re et al., 2024; Chen et al., 2024a; Peng et al., 2024; Xiong et al., 2024; Fu et al., 2024). However, while previous studies observe that current LLMs often struggle with long-context instruction following (Shaham et al., 2023), few studies systematically investigate the instruction-tuning stage for long-context tasks. In this paper, we aim to understand critical factors in data synthesis for long-context instruction tuning.\nLong-context Instruction Data Synthesis A key challenge for long-context tasks is the scarcity of long-context instruction data. Initially, Chen et al. (2024a) released LongAlpaca, a publicly available long-context instruction dataset, though its annotation process is unclear. Recent works propose \"instruction synthesis\" to prompt LLMs to generate instruction-answer pairs from long documents (Bai et al., 2024a; Xiong et al., 2024; Dubey et al., 2024), exemplified by the open-source dataset LongAlign. However, this approach raises concerns about instruction quality, task complexity (Chen et al., 2024b) and task diversity (Quan et al., 2024), as current LLMs themselves still struggle with understanding long-context messages. Subsequently, Chen et al. (2024b) developed a multi-agent workflow to improve synthesis quality for multi-hop reasoning instruction data (LongMIT). Meanwhile, a contrasting perspective from Gao et al. (2024) suggests that synthetic long-context data offers minimal benefits, arguing that using standard-length general instruction data, e.g., ShareGPT (Chiang et al., 2023a) suffices. Our work, however, reveals limitations in existing instruction synthesis approaches and confirms the importance of long-context instruction data through a novel and effective synthesis framework.\nLong-context Tasks Evaluation Early research focuses on measuring perplexity on long-context"}, {"title": "3 Pilot Study", "content": "We first introduce essential concepts used throughout the discussion. An instruction data instance consists of two components: a prompt and an answer. For general-purpose instruction data, the prompt contains only an instruction that requires the LLM to generate an answer based on its parametric knowledge. For context-aware instruction data, the prompt contains both a context passage and an instruction. Typically, the instruction requests a part of the context (i.e., the relevant or evidence context), while the remaining irrelevant context may distract the model from generating an accurate response. In the following analysis, we systematically control the instruction data to understand the key factors that influence the effectiveness of instruction-tuned LLMs on long-context tasks."}, {"title": "3.1 Concept Definitions", "content": "We first introduce essential concepts used throughout the discussion. An instruction data instance consists of two components: a prompt and an answer. For general-purpose instruction data, the prompt contains only an instruction that requires the LLM to generate an answer based on its parametric knowledge. For context-aware instruction data, the prompt contains both a context passage and an instruction. Typically, the instruction requests a part of the context (i.e., the relevant or evidence context), while the remaining irrelevant context may distract the model from generating an accurate response. In the following analysis, we systematically control the instruction data to understand the key factors that influence the effectiveness of instruction-tuned LLMs on long-context tasks."}, {"title": "3.2 Experiment Design", "content": "Testing Scenario We adopt the needle-in-a-haystack (NIAH) test (Kamradt, 2023) for our analysis, which challenges LLMs to locate evidence buried within long contexts. We also incorporate its extended variants proposed in RULER"}, {"title": "3.3 Empirical Results and Insights", "content": "The Need for Context-aware Instruction Data\nIn Figure 2, we compare the base model with the model trained with only general instruction data (SFT1) and with additional specialized context-aware instruction data (SFT2, SFT3, SFT4). Results show that as the test length increases, SFT1 exhibit significant performance divergence and even under-performs the Base Model on single, multi-key and multi-query NIAH tasks. In contrast, incorporating a pinch of targeted context-aware instruction data (SFT2, SFT3, SFT4) leads to substantial performance improvements. This observation leads to our first insight: unlocking pre-trained long-context LLMs' potential requires specialized instruction data beyond general instruction data.\nShort-Context Training Generalizes to Long Contexts Next, we compare models trained with context-aware instruction data of varying context compositions and data lengths (SFT2, SFT3, SFT4). As shown in Figure 2, training with distracting context (SFT3, SFT4) significantly improves the model's performance with longer contexts. In contrast, models trained solely with relevant context (SFT2) may develop shortcuts, leading to performance degradation when exposed to distracting information. Notably, SFT3 maintains performance above 90% across all NIAH tasks, even when test lengths reach 32k - far beyond its instruction-tuning context length, suggesting a potentially more efficient and cost-effective training approach. This observation leads to our second insight: training with both evidence and distracting contexts, even short ones, is crucial for developing robust generalization to longer contexts.\nTraining with Long Contexts Remains Optimal Although SFT3 approaches the performance of SFT4 with much shorter contexts, SFT4 demonstrate almost perfect performance across all evaluated tasks. The performance gap between the two models is particularly pronounced at the maximum test length in the most challenging task (Multi-value NIAH). This highlights the important role of context length in instruction data for achieving optimal performance. This observation leads to our third insight: training with long-context instruction"}, {"title": "4 Applying Insights to Real-World Tasks", "content": "In real-world scenarios, manually annotating long-context instruction data is both complex and labor-intensive, making the synthesis of context-aware instruction data a critical research challenge. Based on insights from our pilot study, we propose a novel method called context synthesis (\u00a74.2) and discuss the limitations of existing instruction synthesis approach (\u00a74.1). Additionally, we propose an analytic tool for measuring the quality of synthesized data, particularly the coherence between contexts and instructions (\u00a74.3)."}, {"title": "4.1 Previous Approach: Instruction Synthesis", "content": "The existing approach to data synthesis, known as \u201cinstruction synthesis\u201d (Figure 1), starts with long text passages and uses off-the-shelf LLMs to generate instruction-answer pairs based on the given text (Bai et al., 2024a). This method focuses primarily on context length when constructing instruction data for long-context tasks, while overlooking other critical factors such as instruction quality. The automatically generated instructions often lack quality guarantees, and the off-the-shelf LLMs may not have sufficient capacity to effectively process long contexts, compromising the coherence between contexts and synthetic instructions. Furthermore, the source passages may lack complex or contradictory information that could serve as distractors, limiting the model's robustness in handling noisy real-world scenarios."}, {"title": "4.2 Proposed Approach: Context Synthesis", "content": "Unlike the previous approach, our method starts from existing instruction-answer pairs and synthesizes the corresponding context. This approach makes the synthetic content merely part of the input to the model, thus prioritizing the quality of the instruction-answer pairs because they are naturally occurring. Additionally, this design enables control over the context: we can deliberately incorporate complex distractors while maintaining tight coupling between instructions and contexts. Furthermore, by generating manageable moderate-length contexts, our synthesis process avoids the paradox of relying on a strong long-context LLM for instruction data synthesis."}, {"title": "4.3 Quality Measurement for Synthetic Data", "content": "Beyond proposing a framework for data synthesis, we also introduce an analytical tool to verify the quality of synthesized data, especially the coherence between instruction and context. For ideal context-aware instruction data, there should be strong interdependence between the context and instruction. Based on this principle, we propose using the performance gap between context-free and context-included tuning as a quality indicator. If context-free tuning yields results similar to context-included tuning, it indicates potential quality issues in the synthesized data. We apply this analytical tool to reveal limitations in existing instruction synthesis approaches in our experiments (Section 6)."}, {"title": "5 Experiments", "content": "Evaluation Benchmark We conduct evaluations on LONGBENCH, focusing on three representative long-context tasks: single-document question-answering, multiple-documents question-answering and summarization (Table 2). With the"}, {"title": "5.1 Experimental Setting", "content": "Evaluation Benchmark We conduct evaluations on LONGBENCH, focusing on three representative long-context tasks: single-document question-answering, multiple-documents question-answering and summarization (Table 2). With the"}, {"title": "5.2 Main Results", "content": "Context Synthesis Approach Significantly Improves Long-context Performance In Table 3, we first demonstrate the benefits of using long-context instruction data over using general-purpose instruction data alone. We then compare our context synthesis approach against the instruction synthesis approach, while also reporting performance using open-source long-context instruction data for reference, though their construction processes are either transparent or do not allow for a fair comparison. Results show that incorporating our synthesized instruction data significantly improves the model's long-context performance, achieving the highest scores across all tasks.\nContext Synthesis Outperforms Instruction Synthesis on All Evaluated Tasks While instruction synthesis, which prioritizes long text lengths, shows improvement over using general instruction data alone in most cases, it yields suboptimal performance compared to the context synthesis approach (Table 3). Our experiments with LongAlpaca and LongAlign on the cutting-edge LLM (LLAMA3.1) show almost no performance improvement, which aligns with findings in Gao et al. (2024). We attribute this performance gap to the instruction quality, rather than suggesting context-aware instruction data is unnecessary, which will be quantitatively analyzed later (Section 6).\nContext Synthesis Is Closing the Gap with Human-annotated Data The primary objective for synthesizing long-context instruction data is to achieve the effectiveness of human-annotated data with minimal cost. Our experiments also enable a controlled comparison between data synthesis strategies and human-annotated long-context instruction data (Figure 4). Results show that the instruction synthesis approach significantly underperforms human-annotated data. In contrast, our"}, {"title": "6 Analysis", "content": "Revealing Limitations of Instruction Synthesis with Our Analytical Tool As introduced in Section 4.3, we employ context-free tuning as an analytical tool to assess the quality of synthesized instruction data (Figure 4). Notably, augmenting"}, {"title": "Analysis of Length Generalization During Instruction Tuning", "content": "Figure 5 presents a comparative analysis of downstream task performance and context length distribution. Even without context concatenation, our model trained on shorter contexts generalizes effectively to long-context"}, {"title": "Comparative Analysis of Different LLMs as Data Engines", "content": "We evaluate various LLMs as data engines for context synthesis, including both proprietary models like GPT4o-mini and open-source models such as LongWriter-8B and Qwen2.5-72B-instruct. Experimental results in Table 4 demonstrate comparable performance across different engines, validating the robustness of our context synthesis approach."}, {"title": "Evaluating Generalization Capability on Unseen Tasks", "content": "Moreover, we check the generalization capability of our data synthesis approach. We evaluate our instruction-tuned model on tasks that were not seen during training, including the most challenging variant of NIAH task (multi-value"}, {"title": "7 Conclusion", "content": "This work investigates effective data synthesis for long-context instruction-tuning. Through a pilot study on controllable needle-in-a-haystack tasks, we identify that instruction difficulty, context composition, and context length all play crucial roles. Based on these insights, we propose a novel synthesis approach called \u201ccontext synthesis\u201d. Experiment results on document-level question-answering and document-level summarization tasks demonstrate that our method not only outperforms the previous instruction synthesis approach but also"}, {"title": "A Data format for NIAH Test Variants", "content": "In Table 6, we present the data format for NIAH tasks. For more details, please refer to the original paper (Hsieh et al., 2024)."}, {"title": "B Template for Instruction Synthesis", "content": "For instruction synthesis, we adopt the template shown in Figure 6, which prompts the model to generate instruction-answer pairs from given context. While this template does not constrain the instruction type, we also experiment with a task-specific template (Figure 7) that explicitly specifies the instruction type - generating summarization instructions for summarization tasks (GovReport, MultiNews, QMSum), generating multi-hop questions for multi-document QA tasks (2WikiMultihopQA, HotpotQA, Musique) and generating single-hop questions for single-document QA tasks (NarrativeQA, Qasper). As shown in Table 9, the template (Figure 6) we apply in our main experiments produce higher performance."}, {"title": "C Examples of Synthesized Context", "content": "We present examples of our synthesized context from question-answering tasks (Table 7) and summarization tasks (Table 8) to help readers better understand the benefit of our approach. Taking the first example in Table 7, the evidence in the synthesized context is distributed across different parts of the text, while detailed background information"}, {"title": "D Instruction-tuning Details", "content": "We use AdamW optimizer with \u03b2\u2081=0.9, \u03b2\u2082=0.95 for instruction-tuning. The learning rate is set to 2e-5 with a cosine decay schedule and 3% warm-up ratio. The models are fine-tuned for 2 epochs. For training efficiency, we employ DeepSpeed ZeRO-3 (Rasley et al., 2020) alongside a packing strategy with loss weighting (Bai et al., 2024a). Specially, we adopt a packing strategy where training samples are packed together, with loss computed only on the output tokens. Following the default settings in LongAlign codebase, we set the maximum sequence length per batched sam-"}, {"title": "E Experimental Results with ShareGPT", "content": "Table 10 presents the experimental results with ShareGPT on LLAMA3.1-8B. The results demonstrates similar findings to those observed in Table 3."}, {"title": "F Impact of Context Concatenation Size", "content": "In this paper, we set the concatenation size to ten, consisting of one relevant background context and nine irrelevant contexts. Table 11 presents the performance results across different numbers of concatenated contexts. Our results indicate that larger"}, {"title": "G Context-Instruction Coherence Analysis", "content": "With our proposed analytic tool, we measure the context-instruction coherence in synthetic instruction data (LongAlign, LongMIT) by previous instruction sysnthesis approaches. Experimental results are depicted in Figure 8. For LongAlign, we observe minimal difference between context-included and context-free tuning, suggesting poor context-instruction coherence in their synthetic data. While LongMIT enhances the quality of synthetic data through a carefully designed multi-agent workflow for question-answering tasks, it"}, {"title": "H Used Scientific Artifacts", "content": "Below lists scientific artifacts that are used in our work. For the sake of ethic, our use of these artifacts is consistent with their intended use.\n\u2022 LongAlign (Apache-2.0 license), a codebase developed for long-context instruction-tuning.\n\u2022 RULER (Apache-2.0 license), a repository for generating synthetic examples to evaluate long-context language models with configurable sequence length and task complexity.\n\u2022 LongBench (MIT license), a benchmark de-signed for assessing the long-context capabilities of large language models.\n\u2022 ZeroScrolls (MIT license), a benchmark for evaluating the long-context capabilities of large language models.\n\u2022 LLaMA2-7B-64K (Apache-2.0 license), a continued pretrained version of LLAMA2-7B with an extended 64k context window.\n\u2022 LLaMA-3.1 (LLaMA3.1 license), a large language model developed by Meta.\n\u2022 GPT4o-mini (Proprietary license), a large language model developed by OpenAI.\n\u2022 Qwen-2.5-72B (Qwen license), a large language model developed by Qwen."}, {"title": "3.  1 Concept Definitions", "content": "We first introduce essential concepts used throughout the discussion. An instruction data instance consists of two components: a prompt and an answer. For general-purpose instruction data, the prompt contains only an instruction that requires the LLM to generate an answer based on its parametric knowledge. For context-aware instruction data, the prompt contains both a context passage and an instruction. Typically, the instruction requests a part of the context (i.e., the relevant or evidence context), while the remaining irrelevant context may distract the model from generating an accurate response. In the following analysis, we systematically control the instruction data to understand the key factors that influence the effectiveness of instruction-tuned LLMs on long-context tasks."}, {"title": "3.  2 Experiment Design", "content": "Testing Scenario We adopt the needle-in-a-haystack2 (NIAH) test (Kamradt, 2023) for our analysis, which challenges LLMs to locate evidence buried within long contexts. We also incorporate its extended variants proposed in RULER"}, {"title": "4.1 Previous Approach: Instruction Synthesis", "content": "The existing approach to data synthesis, known as \u201cinstruction synthesis\u201d (Figure 1), starts with long text passages and uses off-the-shelf LLMs to generate instruction-answer pairs based on the given text (Bai et al., 2024a). This method focuses primarily on context length when constructing instruction data for long-context tasks, while overlooking other critical factors such as instruction quality. The automatically generated instructions often lack quality guarantees, and the off-the-shelf LLMs may not have sufficient capacity to effectively process long contexts, compromising the coherence between contexts and synthetic instructions. Furthermore, the source passages may lack complex or contradictory information that could serve as distractors, limiting the model's robustness in handling noisy real-world scenarios."}, {"title": "4.2 Proposed Approach: Context Synthesis", "content": "Unlike the previous approach, our method starts from existing instruction-answer pairs and synthesizes the corresponding context. This approach makes the synthetic content merely part of the input to the model, thus prioritizing the quality of the instruction-answer pairs because they are naturally occurring. Additionally, this design enables control over the context: we can deliberately incorporate complex distractors while maintaining tight coupling between instructions and contexts. Furthermore, by generating manageable moderate-length contexts, our synthesis process avoids the paradox of relying on a strong long-context LLM for instruction data synthesis."}, {"title": "4.3 Quality Measurement for Synthetic Data", "content": "Beyond proposing a framework for data synthesis, we also introduce an analytical tool to verify the quality of synthesized data, especially the coherence between instruction and context. For ideal context-aware instruction data, there should be strong interdependence between the context and instruction. Based on this principle, we propose using the performance gap between context-free and context-included tuning as a quality indicator. If context-free tuning yields results similar to context-included tuning, it indicates potential quality issues in the synthesized data. We apply this analytical tool to reveal limitations in existing instruction synthesis approaches in our experiments (Section 6)."}, {"title": "B Template for Instruction Synthesis", "content": "For instruction synthesis, we adopt the template shown in Figure 6, which prompts the model to generate instruction-answer pairs from given context. While this template does not constrain the instruction type, we also experiment with a task-specific template (Figure 7) that explicitly specifies the instruction type - generating summarization instructions for summarization tasks (GovReport, MultiNews, QMSum), generating multi-hop questions for multi-document QA tasks (2WikiMultihopQA, HotpotQA, Musique) and generating single-hop questions for single-document QA tasks (NarrativeQA, Qasper). As shown in Table 9, the template (Figure 6) we apply in our main experiments produce higher performance."}, {"title": "D Instruction-tuning Details", "content": "We use AdamW optimizer with \u03b2\u2081=0.9, \u03b2\u2082=0.95 for instruction-tuning. The learning rate is set to 2e-5 with a cosine decay schedule and 3% warm-up ratio. The models are fine-tuned for 2 epochs. For training efficiency, we employ DeepSpeed ZeRO-3 (Rasley et al., 2020) alongside a packing strategy with loss weighting (Bai et al., 2024a). Specially, we adopt a packing strategy where training samples are packed together, with loss computed only on the output tokens. Following the default settings in LongAlign codebase, we set the maximum sequence length per batched sam-"}]}