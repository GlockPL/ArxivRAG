{"title": "HYPERBOLIC HYPERGRAPH NEURAL NETWORKS FOR\nMULTI-RELATIONAL KNOWLEDGE HYPERGRAPH\nREPRESENTATION", "authors": ["Mengfan Li", "Xuanhua Shi", "Chenqi Qiao", "Teng Zhang", "Hai Jin"], "abstract": "Knowledge hypergraphs generalize knowledge graphs using hyperedges to connect multiple entities\nand depict complicated relations. Existing methods either transform hyperedges into an easier-to-\nhandle set of binary relations or view hyperedges as isolated and ignore their adjacencies. Both\napproaches have information loss and may potentially lead to the creation of sub-optimal models. To\nfix these issues, we propose the Hyperbolic Hypergraph Neural Network (H2GNN), whose essential\ncomponent is the hyper-star message passing, a novel scheme motivated by a lossless expansion\nof hyperedges into hierarchies. It implements a direct embedding that consciously incorporates\nadjacent entities, hyper-relations, and entity position-aware information. As the name suggests,\nH2GNN operates in the hyperbolic space, which is more adept at capturing the tree-like hierarchy. We\ncompare H2GNN with 15 baselines on knowledge hypergraphs, and it outperforms state-of-the-art\napproaches in both node classification and link prediction tasks.", "sections": [{"title": "1 Introduction", "content": "Knowledge hypergraphs are natural and straightforward extensions of knowledge graphs [1\u20133]. They encode high-order\nrelations within diverse entities via hyper-relations and have been widely used in downstream tasks including question\nanswering [4, 5], recommendation system [6, 7], computer vision [8,9] and healthcare [10]. Generally, knowledge\nhypergraphs store factual knowledge as tuples (relation, entity1, ..., entitym), where entities correspond to nodes and\nhyper-relations correspond to hyperedges.\nThe core of representation learning for knowledge hypergraphs lies in embedding hyperedges, which encompass\nmultiple nodes and exhibit diverse types. Existing methods implement the direct approaches [11-13]. These methods\ncommonly view hyperedges as isolated, independently learning embeddings without considering structural information\nwith different hyperedges that share common nodes. For instance, consider the tuple (education, Stephen Hawking,\nUniversity College Oxford, BA degree) and (locate, University College Oxford, Oxford, England). Rather than learning\nin isolation, merging adjacent hyperedges based on the common node University College Oxford can facilitate the\ndeduction of an additional tuple (live in, Stephen Hawking, Oxford, England) within the live in hyper-relation. Therefore,\nskillfully integrating adjacencies becomes essential for capturing underlying relations, prompting us to delve into\nhypergraph neural networks, specialized in modeling structural information.\nExisting hypergraph neural networks [14\u201316], predominantly concentrate on a single hyper-relation, neglecting the\nabundant and diverse multiple hyper-relation types inherent in knowledge hypergraphs. Furthermore, they often\noverlook the importance of incorporating position-aware information. Take (flight, Beijing, Shanghai, Guangzhou) as\nan example, which means the flight takes off from Beijing and passes through Shanghai before landing in Guangzhou.\nAccording to clique expansion-based hypergraph models [14, 15], it will be split into three distinct triples: (Beijing,"}, {"title": "2 Preliminaries", "content": "In the realm of hypergraph neural networks, the endeavor to model multi-relational knowledge hypergraphs necessitates\naddressing novel challenges and opportunities. In this section, our focus is on addressing two fundamental questions."}, {"title": "3 Proposed Method", "content": "In knowledge hypergraphs, each tuple (r, x1, x2, ..., Xm) represents a knowledge fact, where x1, x2, ..., Xm denote the\nentities, and r represents the hyper-relation, where m is called the arity of hyper-relation r. We convert the knowledge\ntuples to hypergraph G = (V, R, E), where V denotes the set of nodes, & is the set of hyperedges, R denotes the set of\nhyper-relations. The goal of representation learning is to obtain embedded representations for each node x \u2208 V and\nhyper-relation r \u2208 R in the knowledge hypergraphs. In this section, we provide a detailed description of H2GNN, and\nthe overall framework is illustrated in Figure 4."}, {"title": "3.1 Hyperbolic Operations", "content": "We denote a hyperboloid space Hn with negative curvature k. In this space, each point is represented as x = [xt; xs],\nwhere the vector in an n-dimensional hyperbolic space is n + 1 dimensional. Here, xt represents the time dimension,\nwhile the remaining n dimensions, denoted as \u00e6s, represent space. (x, y)H = -XtYt+X+Ys. Both linear and nonlinear\noperations in hypergraph neural networks within the hyperbolic space should ensure they preserve the space's integrity.\nTherefore, we introduce two Lorentz transformations in our work.\nLinear transformation learns a matrix M = [v\u00af; W], where v \u2208 Rn+1, W \u2208 Rm\u00d7(n+1), satisfying Fx (M)x E Hm\nand Fx(M) : R(m+1)\u00d7(n+1) \u2192 R(m+1)\u00d7(n+1) transforms any matrix into an appropriate value for the hyperbolic"}, {"title": "3.2 Hyper-Star Message Passing", "content": "General GNNs leverage the feature matrix and graph structure to obtain informative embeddings for a given graph.\nThe node embeddings undergo iterative updates by incorporating information from their neighboring nodes. The\nmessage-passing process in the l-th layer of GNN is formulated as follows:\nx+1 = $(x, {x}j\u2208N\u2082),\nwhere N\u00bf denotes the collection of neighboring nodes of node xi. \u00a2\u00b9 defines the aggregation operation of the l-th layer.\nThe message-passing process in homogeneous hypergraphs can be succinctly expressed as follows:\nShe = $1({xj}j\u2208e),\nxi = $2(xi, {he}ece),"}, {"title": "3.3 Objective Function and Optimization", "content": "For the node classification task, we use the negative log-likelihood loss to optimize our model by minimizing the\ndifference between the predicted log probabilities and the ground truth labels of each node.\nL = -\u03a3\u03a3ly;=j} log P(yi = j|xi),\ni=1 j=1\nwhere N represents the number of entities, C represents the number of labels. The indicator function l{y\u2081=j} takes the\nvalue of 1 when the gold label y\u2081 equals j, and 0 otherwise. P(yi\u2081 = j|xi) represents the model's predicted probability\nthat node i belongs to label j.\nWe train our model on positive and negative instances for the link prediction task. Negative samples are generated\nemploying a methodology akin to HypE [28]. Specifically, we create N * r negative samples for every positive sample\nfrom the dataset by randomly replacing each correct entity with N other entities. Here, N serves as a hyperparameter\nand r denotes the number of entities in the tuple. For any tuple \u00e6 in training dataset Etrain, neg(x) is utilized to generate\na set of negative samples, following the process above. To compute our loss function, we define the cross-entropy loss\nas follows:\nL= \u03a3\nXEEtrain\nexp g(x)\nlog\nexp g(x) + Ex' Eneg(x) exp g(x')\nwhere g(x) signifies our model predicts the confidence score for the tuple x."}, {"title": "4 Experiments", "content": "In this section, we evaluate H2GNN in transductive learning and inductive learning for node classification and link\nprediction tasks. Given a hypergraph G, consisting of node data V and hyperedges &, the node classification and\ninductive learning tasks involve developing a classification function that assigns labels to nodes. The link prediction task\nfocuses on predicting new links between entities within the hypergraph, leveraging the existing connections as a basis."}, {"title": "4.1 Settings", "content": "Dataset. We employ widely used academic Co-citation and Co-author datasets [15], including DBLP, CiteSeer, Pubmed,\nand Cora for node classification tasks. For the link prediction task, our approach is evaluated on two hyper-relation\ndatasets: JF17k [11] and FB-AUTO [34], which consist of both binary and n-ary facts. Further details and statistics of\nthe datasets can be found in Table 1."}, {"title": "4.2 Node Classification Results", "content": "The semi-supervised node classification task aims to predict labels for test nodes, given the hypergraph structure,\nnode features and limited training labels. Table 2 reveals that H2GNN consistently outperforms other methods across\nall datasets, demonstrating both high performance and low standard deviation. This highlights H2GNN's effective\nrepresentation of hypergraph features. Furthermore, when compared to UniSAGE, which also utilizes a two-stage\nmessage-passing schema in the Euclidean space, it becomes evident that hyperbolic space is better suited for modeling\nhierarchical structural information.\nBesides, following the approach [29], we conduct inductive learning on evolving hypergraph, take the historical\nhypergraphs as input, and predict the unseen nodes' labels. Specifically, we use a corrupted hypergraph that randomly\nremoves 40% nodes as unseen data during training. For training, We use 20% of the nodes, reserving 40% for the\ntesting nodes that are seen during training. H\u00b2GNN consistently outperforms other methods across benchmark datasets,\nachieving a remarkable accuracy of 89.7% on DBLP, with a low standard deviation ranging from 0.1% to 1.4%, as\nshown in Table 3. This highlights the effectiveness and robustness of H2GNN for inductive learning on evolving\nhypergraphs. H2GNN can dynamically update the hypergraph structure and node features, leveraging high-order\nneighbor information to enhance node representation."}, {"title": "4.3 Link Prediction Results", "content": "Knowledge hypergraph completion can be achieved by either extracting new facts from external sources or predicting\nlinks between existing facts in the hypergraph. The latter entails inferring new knowledge from the structure of the"}, {"title": "4.4 Ablation Study", "content": "To validate the effectiveness of hyperbolic operation (HO) and position-aware composition operation (CO), we conduct\nan ablation study on H2GNN. In this study, we conduct experiments using m-DistMult as the decoder, where we"}, {"title": "5 Related work", "content": "Graph Neural Networks. Research in graph neural networks serves as the foundational basis for GNN development.\nFor instance, Graph Convolutional Networks (GCNs) [37] leverage node degrees to normalize neighbor information.\nPPNP [38] tackles the over-smoothing problems in GNNs through skip-connections, and AdaGCN [39] integrates a\ntraditional boosting method into GNNs. Heterogeneous graph neural networks [40,41] have made significant strides\nin effectively addressing complex heterogeneity through the integration of message passing techniques. Notably,\nthe heterogeneous graph Propagation Network (HPN) [42] theoretically provides a theoretical analysis of the deep\ndegradation problem and introduces a convolution layer to mitigate semantic ambiguity.\nHyperbolic Graph Neural Networks. Hyperbolic neural networks have demonstrated their ability to effectively model\ncomplex data and outperform high-dimensional Euclidean neural networks when using low-dimensional hyperbolic\nfeatures [43, 44]. While existing hyperbolic networks, such as the hyperbolic graph convolutional neural network [19],\nhyperbolic graph neural network [45] and multi-relation knowledge graphs like M2GNN [46], encode features in\nhyperbolic space, they are not fully hyperbolic since most of their operations are formulated in the tangent space, which\nserves as a Euclidean subspace. In contrast, fully hyperbolic neural networks, such as FFHR [18] define operations that\nare entirely performed in the hyperbolic space, avoiding the complexities of space operations.\nKnowledge Hypergraph Neural Network. Existing knowledge hypergraph modeling methods are derived from\nknowledge graph modeling methods, which can be primarily categorized into three groups: translational distance\nmodels, semantic matching models, and neural network-based models. Translational distance models treat hyper-\nrelations as distances between entities and formulate score functions based on these distances. For instance, models\nlike m-transH [11] and RAE [32] generalize the TransH model. They calculate a weighted sum of entity embeddings\nand produce a score indicating the relevance of the hyper-relation. Neural Network-Based Models, like NaLP [31]\nand NeuInfer [13], represent hyper-relations using main triples and attribute pairs. They calculate compatibility scores\nbetween the main triples and each attribute pair individually using neural networks. The final hyper-relation scores\nare determined based on these computations. Semantic Matching Models, such as HypE [28], assess the semantic\ncorrelation between entities and hyper-relations through matrix products. For instance, HypE incorporates convolution"}, {"title": "6 Conclusion", "content": "In this study, we depict knowledge facts as hypergraphs and bring forward a hyperbolic hypergraph neural network,\ndenoted as H2GNN, for multi-relational knowledge hypergraph representation. H\u00b2GNN pioneers consider the structure-\nbased method and extend GNNs to modeling knowledge hypergraphs, rather than treat the hypergraph as an independent\narray of n-ary facts, without considering the neighborhood information. This unique approach directly encodes adjacent\nentities, hyper-relations, and entity positions within knowledge hypergraphs based on the innovative hyper-star message-\npassing schema, considering both structural and positional information. Moreover, we present the hierarchical structure\nin a Lorentz space. Our H\u00b2GNN encoder yields results comparable to the baselines on knowledge hypergraph link\nprediction and node classification tasks."}]}