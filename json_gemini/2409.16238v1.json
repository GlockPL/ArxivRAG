{"title": "EFFICIENTLY LEARNING PROBABILISTIC LOGICAL MODELS BY CHEAPLY RANKING MINED RULES", "authors": ["Jonathan Feldstein", "Efthymia Tsamoura", "Dominic Phillips"], "abstract": "Probabilistic logical models are a core component of neurosymbolic AI and are important models in their own right for tasks that require high explainability. Unlike neural networks, logical models are often handcrafted using domain expertise, making their development costly and prone to errors. While there are algorithms that learn logical models from data, they are generally prohibitively expensive, limiting their applicability in real-world settings. In this work, we introduce precision and recall for logical rules and define their composition as rule utility a cost-effective measure to evaluate the predictive power of logical models. Further, we introduce SPECTRUM, a scalable framework for learning logical models from relational data. Its scalability derives from a linear-time algorithm that mines recurrent structures in the data along with a second algorithm that, using the cheap utility measure, efficiently ranks rules built from these structures. Moreover, we derive theoretical guarantees on the utility of the learnt logical model. As a result, SPECTRUM learns more accurate logical models orders of magnitude faster than previous methods on real-world datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Motivation. Neurosymbolic AI combines neural networks with (probabilistic) logical models, to harness the strengths of both approaches (d'Avila Garcez et al., 2019; d'Avila Garcez et al., 2022). Neurosymbolic frameworks outperform neural networks in several areas (Manhaeve et al., 2018; Gu et al., 2019; Mao et al., 2019), particularly in interpretability (Mao et al., 2019) and in reducing the need for data (Feldstein et al., 2023a). Unlike neural networks, which are fully trained from data, logical models are typically handcrafted. Thus, developing logical models requires domain expertise, in both the data and the inference task. This process is costly and prone to errors. As a result, there has been increased attention on learning logical models from data. This task is known as structure learning.\nLimitations of state-of-the-art. Examples of probabilistic logical models include Markov logic networks (MLNs) (Richardson & Domingos, 2006), probabilistic soft logic (PSL) (Bach et al., 2017) and probabilistic logical programs (PLPs) (Poole, 1993; Sato, 1995; De Raedt et al., 2007). Numerous structure learning techniques for MLNs (Mihalkova & Mooney, 2007; Kok & Domingos, 2010; Khot et al., 2015; Feldstein et al., 2023b) and especially PLPs (Quinlan, 1990; Muggleton, 1995; Evans & Grefenstette, 2018; Sch\u00fcller & Benz, 2018; Qu et al., 2021; Cheng et al., 2023) have been proposed. However, an overarching limitation of structure learning remains the limited scalability to large datasets. The underlying difficulty is the exponential nature of the problem with respect to the length of possible rules and the number of relations in the data. State-of-the-art structure learning algorithms aim to reduce the complexity of the problem by splitting the task into two"}, {"title": "2 PRELIMINARIES", "content": "First-order logic. In first-order logic, constants represent objects in the domain (e.g. alice, bob). Variables range over the objects (e.g. X, Y, Z). A term is a constant or a variable. A predicate P represents a relation between objects (e.g. FRIENDS). The arity of P is the number of related objects. An atom has the form $P(t_1, ..., t_n)$, where P is an n-ary predicate, and $t_1, ..., t_n$ are terms. A fact is an atom for which each term $t_i$ is a constant (e.g. FRIENDS(alice,bob)). A relational database D is a set of facts. A Datalog rule p, or simply rule, is a first-order logic formula of the form\n$\\forall X.\\bigwedge_{i=1}^{n}P_i(X_i) \\rightarrow P(Y)$,\nwhere $\\bigwedge_{i=1}^{n} P_i(X_i)$ is a conjunction of atoms, $\\rightarrow$ denotes logical implication, X, $X_i$, and Y are tuples of variables, $Y \\subseteq \\bigcup_{i=1}^{n} X_i$, where $X_i \\subseteq X$. Quantifiers are commonly omitted. The left-hand and the right-hand side of a rule are its body and head, respectively, and are denoted by body(p) and head(p). The length of a conjunction is the number of its conjuncts. The length of a rule L(p) is the length of its body plus the length of its head. A grounding of an atom is the atom that results after replacing each occurrence of a variable with a constant. Groundings of conjunctions and rules are defined analogously. A theory p is a set of rules. In probabilistic logic models, the rules would be"}, {"title": "3 PATTERNS", "content": "In this section, we introduce the concept of patterns - commonly recurring substructures within a database graph. From now on, we assume that D is fixed and clear from context. As stated in the introduction, we restrict our discussions to graphs with only unary and binary edges. We use ug(\u03c5) and bg(\u03c5) to denote the set of unary and binary edges in the graph G that are incident to v. Further, we use a to denote the set of atoms that have a grounding in D and $a \\in \\alpha$ to denote a particular atom. We use $\\mathfrak{a}$ to denote the set of all possible groundings of a in D and $a \\in \\mathfrak{a}$ to denote a particular grounding.\nSimilarly to how we can view relational databases as graphs, we can also view conjunctions of atoms as graphs. For a conjunction of atoms $\\varphi := \\bigwedge_{i=1}^{n} P_i(t_i)$, the pattern of $\\varphi$, denoted as $G_{\\varphi} = (V, E)$, is the graph where, for each term $t_i$ occurring in $\\varphi$, V includes a node $t_i$, and, for each atom $P(t_1,..., t_n)$ occurring in $\\varphi$, E includes an edge {$t_1,..., t_n$} with label P. The length of a pattern $G_{\\varphi}$ is the number of atoms in $\\varphi$. Given a rule p, the patterns corresponding to its head and body are denoted by $G_{head(p)}$ and $G_{body(p)}$, respectively. We call $G_{body(p)\\wedge head(p)}$ the rule pattern of p. Rule p is connected if $G_{body(p)\\wedge head(p)}$ is connected; it is body-connected if $G_{body(p)}$ is connected.\nA ground pattern of a conjunction $\\varphi$ is the graph corresponding to a grounding of $\\varphi$ that is satisfied in D. We denote by $\\mathfrak{G}_{body(p)\\wedge head(p)}$ the set of all ground patterns of $body(p) \\wedge head(p)$ in D. For a fact a that is a grounding of $\\alpha = head(p)$, we use $\\mathfrak{G}_{body(p)\\wedge \\alpha}$ to denote the subset of $\\mathfrak{G}_{body(p)\\wedge head(p)}$ which contains only groundings of patterns of the form $G_{body(p)\\wedge \\alpha}$."}, {"title": "4 RULE UTILITY", "content": "In this section, we introduce a measure, that we call utility, for assessing the \"usefulness\" of a theory without requiring inference. The utility itself depends on various criteria, which we present below. The following definitions hold for connected rules that are also body connected.\nDefinition 1 (Precision). The precision of a rule p is defined as\n$P(p) := \\frac{|\\mathfrak{G}_{body(p)\\wedge head(p)}|}{|\\mathfrak{G}_{body(p)}|}$.\nIntuitively, P(p) is thus the fraction of times that the head and body of a rule are both true in the data when the body is true in the data. If one considers cases where the body and head are both true"}, {"title": "5 PATTERN MINING", "content": "In this section, we present our technique for mining rule patterns from relational data, i.e. finding subgraphs in a relational database graph. Since finding all subgraphs is generally a hard problem with no known polynomial algorithm (Bomze et al., 1999), we adopt an approach similar to PRISM (Feldstein et al., 2023b). In particular, we present a non-exhaustive algorithm that has only linear-time complexity in the dataset size but that allows us to compute estimates of utility that are close, in a precise sense, to their true values.\n5.1 ALGORITHM\nThe steps of our technique are outlined in Algorithm 1. The algorithm mines ground patterns by calling a recursive function (NEXTSTEP) from each node vo in the datagraph $G_D$ (lines 1-3). Intuitively, the algorithm searches for ground patterns by rolling out paths in parallel from a starting node. The recursion stops at a user-defined maximum depth D, rolling out up to a maximum number of paths N.\nIn each call of NEXTSTEP, the algorithm visits a node $v \\in V$ of $G_D$. At node v, unary relations $u_G(v)$ are grafted onto previously found ground patterns $G_{previous}$ (lines 6-8). We use $G \\circ e$ to denote the graph that results after adding edge e and the nodes of e to graph G. The resulting ground patterns are stored in $G_{new}$ (line 9). If the maximum recursion depth has not been reached (line 10), a subset of the binary edges of node v is then selected (lines 11-16). The algorithm avoids mining patterns corresponding to tautologies by excluding previously visited binary edges (line 11). To keep the complexity linear, we enforce N to be the maximum number of paths by setting the maximum number of selected binary edges n to be N divided by the number of binary edges selected at each previous stage (lines 13, 16). We graft each chosen binary edge onto the ground patterns in $G_{new}$ (lines 17-20), store the new ground patterns in $G_{final}$ (line 21), and pass $G_{final}$ on to the subsequent call (line 22). $G_{final}$ is passed to the next recursive call to continuously extend previously mined patterns. In the next recursive call, the recursion depth d is increased by 1, thereby expanding the search of grounds patterns to include nodes up to a distance d away from vo. At each depth $d \\in \\{0,..., D - 1\\}$ the algorithm grafts up to one unary and one binary onto the patterns previously discovered along that path. At depth d = D, the algorithm only grafts up to one unary onto the patterns, since it terminates before grafting binaries (line 10). Thus, the patterns found by Algorithm 1 are of maximum length 2D + 1.\nAs a special case, the algorithm also mines patterns that consist of two unary edges on a single node in line 2, to allow constructing rules of the form $P_1(X) \\rightarrow P_2(X)$. DOUBLEUNARYPATTERNS creates all possible patterns that consist of all pairings of distinct unary edges from the set $u_{g_{\\mathfrak{a}}}(v_0)$."}, {"title": "6 UTILITY-BASED STRUCTURE LEARNING", "content": "We now introduce our structure learning pipeline, which we call SPECTRUM (Structural Pattern Extraction and Cheap Tuning of Rules based on Utility Measure), presented in Algorithm 2. In summary, SPECTRUM begins by mining patterns, then checks each mined pattern whether it is a pattern of a \"useful\" rule, and finally sorts the useful rules in a greedy fashion.\n6.1 RESTRICTIONS ON THE MINED RULES\nAs we stated in the introduction, SPECTRUM focuses on learning Datalog rules (as opposed to general first-order logic formulae). In addition, the algorithms restrict the shape of the mined rules:\n(1) Algorithm 1 only mines patterns (and by extension rules) where each term occurs in at most two binary predicates and one unary predicate, except for the special case $P_1(X) \\rightarrow P_2(X)$ (line 2 in Algorithm 1).\n(2) Algorithm 2 restricts to rules that are body-connected and term-constrained, where a rule pis term-constrained if every term occurs in at least two atoms of the head or the body of the rule."}, {"title": "7 EXPERIMENTS", "content": "We compare SPECTRUM on computation time and accuracy of the mined logical theories against state-of-the-art MLN structure learners. Additionally, to test whether the scalability bottleneck of structure learning algorithms has been removed by SPECTRUM, we tested it on large real-world datasets using PSL. Given a dataset with incomplete observations, our goal is to predict the truth values for the missing data. This problem can be framed as missing link prediction in graphs.\nExperiment Setup: For SPECTRUM, we set $N = \\frac{MD}{\\sqrt{\\epsilon}}$, M = 30, \u03b5 = 0.1 and D = 3. We ran the experiments on a Linux machine with 32Gb of RAM and a 12-core 2.60GHz i7-10750H CPU.\n7.1 MLN EXPERIMENTS\nDatasets: To compare our work to state-of-the-art structure learners, we perform experiments on two benchmark datasets adopted for MLNs (Richardson & Domingos, 2006):\n1. IMDB: The dataset is subsampled from the IMDB.com database and describes relationships among movies, actors and directors.\n2. WEBKB: The dataset consists of web pages and hyperlinks collected from four computer science departments.\nEach dataset has five splits. One split is used to test the learnt theories, while the other four are used to train the models. The reported results are the average over all permutations.\nBaselines: We compared against LSM (Kok & Domingos, 2010), BOOSTR (Khot et al., 2015), and PRISM (Feldstein et al., 2023b), using the parameters as suggested by the respective authors. Since PRISM only mines motifs, we used LSM for the remaining steps of the pipeline, in line with Feldstein et al. (2023b).\nResults: We used ALCHEMY (Kok et al., 2005) \u2013 an implementation of an MLN framework \u2013 to perform predictions using the learnt theories. We report in Table 1 the balanced accuracy (ACC) and runtimes. SPECTRUM improves on all fronts: the runtime is reduced to < 1% compared on both IMDB and WEBKB when compared to the most accurate prior art, while also improving accuracy by over 16% on both datasets.\n7.2 PSL EXPERIMENTS\nDatasets: To assess the scalability of SPECTRUM, we also perform experiments on large PSL benchmarks:\n1. Citeseer (6.8k relations): This dataset consists of research papers, categories the papers fall under, and links between papers. Citeseer has the relations HASCAT(P, C) (describing whether a paper P is of a specific category C) and LINK(P1, P2) (describing whether two papers are linked). The dataset has six paper categories."}, {"title": "8 RELATED WORK", "content": "Markov logic networks. State-of-the-art structure learning approaches proceed in two steps \u2013 identification of patterns and optimisation. The different structure learning techniques for MLNS can be split into two groups: (1) methods for which patterns are user-defined and (2) methods for which patterns are identified automatically in the data. The first technique proposed in group (1) was bottom-up structure learning (Mihalkova & Mooney, 2007). This technique relied on user-defined patterns to form simple formulae that compose to form more complex ones. Another technique in this group is BOOSTR (Khot et al., 2015), which simultaneously learns the rules and their weights, using functional gradient boosting (Friedman, 2000). While showing promising scalability, the quality of the theories learnt by BOOSTR drastically decreases in the absence of user-defined patterns (Feldstein et al., 2023b). The inherent limitation of the above line of research is that these frameworks still require domain expertise from the user when defining patterns. The current state-of-the-art that does not require user-defined patterns is LSM (Kok & Domingos, 2010). LSM identifies patterns by running random walks over a hypergraph representation of the data. A major limitation of LSM is that it lacks guarantees on the quality of the mined patterns. PRISM is an efficient pattern-mining technique with theoretical guarantees on the quality of mined patterns, solving the above limitation (Feldstein et al., 2023b). Our empirical results show that SPECTRUM scales significantly better than any of the techniques mentioned above, without requiring any domain expertise as patterns are mined automatically. In addition, inspired by Feldstein et al. (2023b), we provide \u03b5-uncertainty guarantees, which, in contrast to PRISM, are guarantees on the utility of the output theory rather than just the patterns."}, {"title": "9 CONCLUSIONS", "content": "A major point of criticism against neurosymbolic techniques and logical models is the need for domain expertise (Feldstein et al., 2023a; Huang et al., 2021; Li et al., 2023). This work tackles the scalability issue of learning logical models from data, mining accurate logical theories in minutes for datasets with millions of instances, thus making the development of a logical model a simple and fast process. Therefore, we see our work as having the potential to increase the adoption of neurosymbolic frameworks. In addition, learning logical models improves explainability by extracting knowledge from data that is interpretable by a domain expert.\nThere are several directions for future research. First, the pattern mining algorithm could be generalised to relations with higher arity. Second, the pattern mining algorithm could be extended to require fewer restrictions on the shape of the rules. Finally, since the rules we learn are model agnostic, we plan to apply our technique to other logical frameworks, in addition to MLNs and PSL, such as Problog (De Raedt et al., 2007)."}, {"title": "A PROOF OF THEORETICAL PROPERTIES OF PATTERN MINING", "content": "In this section, we prove Theorems 5, 6, and 7 and justify Remark 2. We start with some preliminary definitions before proceeding with the proof.\nDefinitions and notation. The D-neighbourhood of a node $v_i$ is the set of all nodes that are a distance less than or equal to D from $v_i$. The D-neighbourhood length l patterns of a node $v_i$, denoted $P_{D,l}(v_i)$, is the set of all connected patterns of length l that have a grounding that includes $v_i$, and whose remaining nodes in the grounding also occur within the D-neighbourhood of $v_i$. The D-neighbourhood length l pattern distribution of $v_i$ is the function that maps from $P_{D,l}(v_i)$ to the number of groundings of that pattern within the D-neighbourhood of $v_i$ that include $v_i$. The D-neighbourhood length l pattern probability distribution of $v_i$, denoted $P^{(l)}(v_i)$, is the probability distribution obtained by normalising this distribution.\nA.1 PROOF OF THEOREM 5\nProof. We prove this theorem by proving the two statements individually:\nS1: Algorithm 1 finds all patterns of length l < D that only involve the source node and other N-close nodes.\nS2: For patterns involving nodes that are not N-close, Algorithm 1 finds them with a probability larger than when running random walks.\nStatement 1: Consider a node $v'$ that is a distance $l < D$ away from a source node $v_0 = v_{i_0}$ and consider a generic path of length l from $v_{i_0}$ to $v' = v_{i_l}$, $(v_{i_0}, e_{i_0}, ..., e_{i_{l-1}}, v_{i_l})$.\nFirst, notice that if $N > E' = |bg(v_{i_0})|$, then the edge $e_{i_0}$ will certainly be discovered by Algorithm 1 (selection step, lines 11-16). Similarly, for the second edge $e_{i_1}$ to be found, we need the value of n upon reaching node $v_{i_1}$ to satisfy $n \\geq E'$ (selection step, lines 11-16). Note also that $|E'| \\leq |bg(v_{i_1})| \u2013 1$, since the previous incident edge to $v_{i_1}$ is excluded from the set $E'$ (line 11). Therefore, a sufficient condition for the edge $e_{i_1}$ to be included is $N \\geq |bg(v_{i_0})|(|bg(v_{i_1})| \u2013 1)$. Reasoning inductively, a sufficient condition for every edge in the path to be found by Algorithm 1 is\n$N \\geq |bg(v_{i_0})|\\prod_{j=1}^{l-1}(|bg(v_{i_j})| \u2013 1).$\nIf this holds for all possible length I paths between $v_{i_0}$ and $v_{i_l}$, then all of those paths will be discovered. This is equivalent to the statement that $v_{i_l}$ is N-close to $v_{i_0}$. Therefore, for any node that is N-close to $v_{i_0}$ (and is a distance l from $v_{i_0}$), all possible length l paths leading to that node will be found. Since any connected patterns is a subsets of a path, all possible patterns of length $l \\in \\{1,2,..., D\\}$ have been found by Algorithm 1, thus completing the proof.\nStatement 2: First, notice that if a node is not N-close, then there is still a chance that it could be found due to random selection. This is because the smallest value of n is 1 and if $n < |E'|$ then we proceed by choosing the next edge in the path uniform randomly (line 13). Worst-case, $|bg(v_{i_0})| \\geq N$, in which case the algorithm runs N different paths where each edge is chosen at random. This is almost equivalent to running N random walks, with the difference that Algorithm 1 does not allow backtracking or visiting previously encountered nodes, which increases the probability of finding novel nodes compared to independent random walks. Thus, the probability that a node is found using Algorithm 1 is strictly larger than when running N independent random walks from $v_0$.\nA.2 PROOF OF THEOREM 6\nProof. We prove this by partitioning the possibilities into three cases and proving that the upper bound formula is true in all cases."}, {"title": "A.3 PROOF OF THEOREM 7", "content": "Proof. We will prove the theorem in three stages:\nS1: We derive an upper bound, $N(\\epsilon')$, on the number of purely random walks required to achieve $\\epsilon'$-uncertainty of the top M pattern probabilities;\nS2: We derive the corresponding $N(\\epsilon)$ required to achieve $\\epsilon$-uncertainty in the utility of an arbitrary set of rules whose head pattern, body pattern and rule patterns belong to these top M patterns, under purely random walks;\nS3: We prove that running Algorithm 1 with $N = N(\\epsilon)$ leads to a strictly lower $\\epsilon$-uncertainty for this rule utility than when using purely random walks, thus $N(\\epsilon)$ satisfies the theorem claim.\nStage 1 of the proof is an adaptation of a similar proof for $\\epsilon$-uncertainty of path probabilities of random walks on hypergraphs by Feldstein et al. Feldstein et al. (2023b).\nStage 1 Throughout this proof, we will consider pattern probabilities within the D-neighbourhood of nodes, where D is fixed by Algorithm 1.\nGiven a node $v_i \\in D_G$, let $P_D^{(1)}(G_k)$ denote the pattern probability of the kth most common pattern in the D-neighbourhood length l patterns of $v_i$ (note that the constraints we make on rule patterns in Section 6.1 means that we can bound $l < 2D + 1$, where I can exceed D due to the presence of unary predicates in the rule pattern). The Ziphian assumption implies that\n$P_D^{(1)}(G_k) = \\frac{1}{kZ},$ where $Z = \\sum_{k=1}^{|P_{D,l}(v_i)|} \\frac{1}{k}$ is the normalisation constant.\nConsider running N random walks from $v_i$ without backtracking, and up to a maximum depth of D (c.f. Algorithm 1). Since the walks are uniform random, a partial walk up to step $l < 2D + 1$ yields a random sample from the D-neighbourhood length l pattern probability distribution of $v_i$. Denote by $\\hat{C}_N(G_k)$ the number of times that the kth most probable pattern, $G_k$, was sampled after running all N random walks. By independence of the random walks, the quantity $\\hat{C}_N(G_k)$ is a binomially distributed random variable with\n$E [(\\hat{C}_k)] = NP_D^{(1)}(G_k); \\hspace{5mm} Var [(\\hat{C}_k)] = NP_D^{(1)}(G_k) (1 \u2013 P_D^{(1)}(G_k)).$\nIt follows that the pattern probability estimate $P_N^{(1)}(G)) := \\hat{C}_N(G_k)/N$ has fractional uncertainty $\\epsilon(G_k)$ given by\n$\\epsilon (G_k) := \\sqrt{\\frac{Var [P_N^{(1)}(G))]}{E [P_N^{(1)}(G))]} = \\sqrt{\\frac{1-P_D^{(1)}(G_k))}{NP_D^{(1)}(G_k)}} = \\sqrt{\\frac{\\sum_{m=1}^{|P_{D,l}(v_i)|} m^{-1}}{k N}}$,\nwhere in the second line we used the Ziphian assumption equation 3. Suppose further that we require that all pattern probabilities $P_D^{(1)}(G_k)$ up to the $M^{th}$ highest probability for that length have $\\epsilon'$-uncertainty, i.e.\n$\\epsilon' = \\underset{k \\in \\{1,2,...,M\\}}{max} \\epsilon(G_k) = \\epsilon(G_M)$,\nwhere $G_M$ is the $M^{th}$ most probable pattern, and so, upon rearranging,\n$\\frac{1}{N(\\epsilon')} < \\frac{\\epsilon'^2}{M \\sum_{m=1}^{|P_{D,l}(v_i)|} m^{-1}}.$\nWe have\n$N(\\epsilon') \\approx \\frac{M (\\gamma + ln(|P_{D,l}(v_i)|))}{\\epsilon'^2},$ where we used the log-integral approximation for the sum of harmonic numbers $\\sum_{m=1}^{|P_{D,l}(v_i)|} \\frac{1}{m} = \\gamma + ln(|P_{D,l}(v_i)|) + O(\\frac{1}{|P_{D,l}(v_i)|})$, where $\\gamma \\approx 0.577$ is the Euler-Mascheroni constant. Equation equation 6 gives an upper bound on the number of random walks required to achieve $\\epsilon'$-uncertainty of the top M most common pattern probabilities of length l that occur in the 3-neighbourhood of node $v_i$. Note that the exact value of $|P_{D,l}(v_i)|$ depends on the specifics of the dataset, however, in general, it would grow exponentially with the length l due to a combinatorial explosion in the number of patterns Feldstein et al. (2023b). This means that $N(\\epsilon')$ scales as\n$N(\\epsilon') \\sim O(\\frac{MD}{\\epsilon'^2}).$\nIf we want to ensure $\\epsilon'$ uncertainty for patterns of all lengths $l \\in \\{1,2,...,2D+1\\}$ then we conclude that $N(\\epsilon')$ should scale as\n$N(\\epsilon') \\sim O(\\frac{MD}{\\epsilon'^2}).$\nThis concludes stage 1.\nStage 2 Assuming that the top M most common pattern probabilities of length l are $\\epsilon'$-uncertain, for all $l \\in \\{1,2,..., 2D + 1\\}$, we now derive an upper bound for the level of uncertainty of the utility of an arbitrary set of rules whose head patterns, body patterns and rule patterns belong to these top M patterns."}]}