{"title": "Defeasible Visual Entailment: Benchmark, Evaluator, and Reward-Driven Optimization", "authors": ["Yue Zhang", "Liqiang Jing", "Vibhav Gogate"], "abstract": "We introduce a new task called Defeasible Visual Entailment (DVE), where the goal is to allow the modification of the entailment relationship between an image premise and a text hypothesis based on an additional update. While this concept is well-established in Natural Language Inference, it remains unexplored in visual entailment. At a high level, DVE enables models to refine their initial interpretations, leading to improved accuracy and reliability in various applications such as detecting misleading information in images, enhancing visual question answering, and refining decision-making processes in autonomous systems. Existing metrics do not adequately capture the change in the entailment relationship brought by updates. To address this, we propose a novel inference-aware evaluator designed to capture changes in entailment strength induced by updates, using pairwise contrastive learning and categorical information learning. Additionally, we introduce a reward-driven update optimization method to further enhance the quality of updates generated by multimodal models. Experimental results demonstrate the effectiveness of our proposed evaluator and optimization method.", "sections": [{"title": "Introduction", "content": "Natural Language Inference (NLI) (Bos and Markert 2005; Dagan, Glickman, and Magnini 2005; MacCartney and Manning 2009; Bowman et al. 2015) is a fundamental task that involves determining the logical relationship between two sentences, specifically identifying whether one sentence entails, contradicts, or is neutral with respect to the other. To further investigate the logical relationship across modalities, researchers have introduced a new inference task called Visual Entailment (VE). In VE, the premise is provided by an image and the hypothesis by a sentence and the task is to determine whether the image supports, contradicts, or is unrelated to the statement in the sentence (Xie et al. 2019). Existing approaches to the VE task typically leverage pre-trained vision-language models, such as OFA (Wang et al. 2022), UNITER (Chen et al. 2020c), and CoCa (Yu et al. 2022). These models are designed to understand and reason across visual and textual modalities and have greatly improved our ability to accurately link and interpret images and text."}, {"title": "Task Defination", "content": "In this paper, we follow the definition of the defeasible task (Rudinger et al. 2020)\nGiven an image premise I, a hypothesis H is defeasible if there exists an update U (consistent with I) such that a human would find H less likely to be true after learning U. Specifically, an update U is called a weakener if given a premise I and hypothesis H, a human would most likely find H less likely to be true after learning U; if they find H more likely to be true, then we call U a strengthener."}, {"title": "Classification Task", "content": "Formulation The goal of the classification task is to find a classification model Mc which predicts the update type based on the premise I, hypothesis H, and update U as follows,\n$\\hat{L} = M_c(I, H, U),$ (1)\nwhere $L \\in {w, s}$ denotes the predicted update type. L = s (strengthener) is assigned if U makes the hypothesis H more likely given the image I while L = w (weakener) is assigned if U makes the hypothesis H less likely given the image I.\nEvaluation Metric To evaluate the performance of the model Mc on the classification task, we use accuracy as the evaluation metric. Accuracy measures the proportion of correctly classified instances among the total instances, providing a straightforward assessment of model performance."}, {"title": "Generation Task", "content": "Formulation In this task, the model aims to generate an update based on the input premise I, hypothesis H, and goal $G \\in {w, s}$ (i.e., weakener or strengthener) as follows,\n$\\hat{U} = M_g(I, H, G),$ (2)\nwhere $\\hat{U}$ is the generated (textual) update.\nEvaluation Metric To comprehensively assess the quality of the generation model Mg, we utilize a variety of evaluation metrics, including traditional evaluation metrics: ROUGE-L (Lin 2004), BLEU-4 (Papineni et al. 2002), deep learning-based metrics: BERTScore (Zhang et al. 2020) and CLIPScore (Hessel et al. 2021), and our custom-designed reference-free Inference-aware Evaluator, which is detailed in the later section."}, {"title": "Defeasible Visual Entailment Dataset", "content": "Dataset Construction In this section, we describe the construction of our dataset for the DVE task, which leverages three existing datasets: Flickr30k (Young et al. 2014), SNLI (Bowman et al. 2015) and the 8-NLI dataset (Rudinger et al. 2020). Flickr30k is a well-known image captioning dataset comprising 31,783 images and 158,915 captions, depicting everyday activities, events, and scenes. Each image in the dataset is annotated with five captions generated through crowdsourcing, providing diverse descriptions of the visual content. This dataset is essential for developing models that can understand and generate natural language descriptions of images, as it offers a rich set of image-caption pairs that cover a broad range of scenarios and objects. The SNLI dataset is a large annotated textual entailment dataset. It comprises approximately 570,000 premise-hypothesis (T, H) pairs, as well as their corresponding label categorized into three classes: entailment, neutral, and"}, {"title": "Estimating the Impact of Updates on Multimodal Defeasible Reasoning", "content": "For the generation task, we utilize standard generation evaluation metrics such as BLEU, ROUGE, and BERTScore to measure the quality of the generated updates. These metrics assess the lexical or semantic similarity between the generated update and the reference updates, but it is not realistic to collect a comprehensive set of ground-truth references for such open-domain tasks, where answers can vary widely. We also employ the reference-free metric CLIPScore, which primarily evaluates the similarity between the answer and the image. While these metrics provide some insight into the quality of the updates, they are not well-suited to accurately capture the changes in entailment strength brought about by strengtheners and weakeners. To address this, we propose a new reference-free evaluation approach utilizing contrastive learning to train an unsupervised model capable of representing the entailment strength of the changes caused by updates."}, {"title": "Inference-aware Evaluator", "content": "As mentioned before, for the generation task, we designed a novel reference-free evaluation method that leverages contrastive learning to capture the impact of updates on inference strength. Our model consists of the following components. The overall architecture of the model is illustrated in Figure 3, which consists of three modes: Multimodal Embedding, Feature Fusion, and Multitask Learning. Multimodal Embedding The input data for the model consists of both images and text. To feed the multimodal data into our model, we first get the embeddings of the text and image as follows. Visual Embedding Since ResNet (He et al. 2016) has shown great success on vision tasks, such as image classification (Russakovsky et al. 2015; Krizhevsky, Hinton et al. 2009), object detection (Everingham et al. 2010; Lin et al. 2014), semantic segmentation (Zhou et al. 2017), we also use it to extract the visual embedding. Specifically, we use"}, {"title": "1. Multimodal Embeddings", "content": "the pretrained ResNet-50 model to extract the visual embedding as follows,\n$i = ResNet(I),$ (3)\nwhere i \u2208 Rd1 denotes the image embedding of the image premise I. d\u2081 is the embedding size. ResNet(\u00b7) refers to the ResNet-50 model.\nTexual Embedding It is known that BERT (Devlin et al. 2019) achieves superior performance on various natural language models, such as Language Understanding (Wang et al. 2019), Question Answering (Rajpurkar et al. 2016) and Commonsense Inference (Zellers et al. 2018). Therefore, we use BERT to extract the textual features. In particular, we encode a pair of text inputs: the hypothesis and update with BERT as follows,\n$e = BERT([H, U]),$ (4)\nwhere e \u2208 Rd2 represent the embedding of the [CLS] token output from BERT, which is used to represent the overall semantics of the text pairs. BERT() refers to the BERT model.\nFeature Fusion We propose to concatenate the extracted visual and textual features to form a combined multimodal feature representation, denoted as m:\n$m = [i, e],$ (5)\nwhere [,] denotes the concatenation operation. In this context, m \u2208 Rd1+d2 represents the combined features that integrate the multimodal information, enabling the model to leverage both visual and textual contexts effectively.\nMultitask Learning Our evaluator employs a multitask learning framework to jointly perform classification and inference strength tasks, utilizing shared representations to improve overall performance. The inference strength score is ultimately used to represent the strength of visual entailment brought by updates.\nPairwise Contrastive Learning Since the existing entailment datasets only label update classes without indicating entailment strength, they cannot be used to train a model that predicts this strength for a given (premise, hypothesis, and update) triplet. While human scoring could be an option, it is impractical due to its difficulty, cost, and lack of scalability. Instead, motivated by the contrastive learning framework (Chen et al. 2020b), we develop an unsupervised method to train our evaluator by comparing the entailment strength between pairs, requiring only knowledge of which pair has stronger entailment.\nSpecifically, we first devise an entailment strength head to output a numerical score s representing the impact of the update on the hypothesis as follows,\n$s = W_sm + b_s,$ (6)\nwhere $W_s \\in R^{d_1+d_2}$ and $b_s \\in R^1$ are the trainable weights and bias of the entailment strength layer respectively. The entailment strength score s is used as the final measure of the strength of entailment inference, indicating how the update affects the hypothesis. A higher score indicates that the update makes the hypothesis more likely in relation to the premise. In contrast, a lower score indicates that the update makes the hypothesis less likely in relation to the premise. To train the evaluator, we design a custom pairwise contrastive loss function that can capture the change in entailment strength by comparing triplets (update, premise, and hypothesis). It is evident that the the entailment strength of the triplet (strengthener, premise, and hypothesis) is bigger than the triplet (caption, premise, and hypothesis) and the"}, {"title": "2. Feature Fusion", "content": "the entailment strength of the triplet (weakener, premise, and hypothesis) is smaller than the triplet (caption, premise, and hypothesis). Therefore, we devise the pairwise contrastive loss function as follows,\n$L_p = \\frac{1}{N} \\sum_{i=1}^N log(\\sigma(-l(s_h - s))),$ (7)\nwhere sh is the score computed by the Eqn.(6) for the triplet (update, premise, and hypothesis). s is the score computed by the Eqn.(6) for the triplet (caption, premise, and hypothesis). l\u2208 {\u22121,1}, where \u20131 represents the update is a weakener and 1 represents the update is a strengthener. \u03c3(\u00b7) is the sigmoid function, and N is the number of samples.\nCategorical Information Learning To further learn the category information of the update, we devise a categorical information loss function. Specifically, we first design a classification head that aims to classify the update as either a strengthener or a weakener as follows,\n$\\hat{y} = \\sigma(W_cmu + b_c),$ (8)\nwhere o is the sigmoid activation function. We \u2208 Rd1+d2 and bc \u2208 R2 are the trainable weight and bias of the classification layer. mu is the combined multimodal feature representation by Eqn.(5) for the triplet (update, premise, hypothesis). \u0177 \u2208 R2 is the corresponding predicted label (i.e., strengthener and weakener) for the above triplet. Thereafter, we utilize the cross-entry loss function to learn the categorical information as follows,\n$L_c = - \\frac{1}{NC} \\sum_{i=1}^N \\sum_{j=1}^C Y_{ij} log \\hat{y}_{ij},$ (9)\nwhere N is the number of samples, C is the number of classes, Yij is the ground truth label for the i-th sample and the j-th class (1 if the sample belongs to the class, otherwise 0), and \u0177ij is the predicted probability for the i-th sample and the j-th class.\nTraining\nThe overall loss function for multitask learning is a weighted sum of the classification loss and the pairwise contrastive loss as follows,\n$L = (1 - \\alpha)L_p + \\alpha L_c,$ (10)\nwhere Lc is the binary cross-entropy loss for the classification task, Lp is the pairwise contrastive loss, and \u03b1 is a hyper-parameter to balance their contributions."}, {"title": "3. Multitask Learning", "content": "Meta-evaluate Evaluator for Automatic Evaluation\nTo verify the effectiveness of our automatic evaluator, we conduct human evaluations on the whole test dataset. For the evaluation model, we select answers from LLaVA-1.5 (Liu et al. 2023) and GPT-4o. More implementation details can be found in the supplementary material."}, {"title": "Reward-driven Update Optimization", "content": "In the generation task, our objective is to generate updates based on given premises to either strengthen or weaken hypotheses. In our experiments, we observed that the initial updates may suffer from quality issues, such as simply captioning the images rather than effectively achieving the intended goal.\nTo address this issue, we propose a new reward-driven update optimization method, which leverages the entailment strength of the generated update. Figure 4 presents an overview of the proposed method. Our method consists of the following steps:\n1. Initial Response Generation: We submit the user request to the Large Vision-Language Model (LVLM), to generate the initial response. This response serves as the baseline for subsequent comparisons with the refined responses produced by our method.\n2. Critique: Our inference-aware evaluator serves as the critique, assessing the entailment strength of the generated updates. If the critique assigns a low score, we proceed to the next step (i.e., Refinement) to improve the response. We establish a threshold \u03b7 to evaluate the quality of the generated update. Specifically, if the score of a generated strengthener is less than \u03b7 or the score of a generated weakener exceeds -\u03b7, we classify the update as low-quality. Conversely, if the score indicates the update is of high quality, we output the current response as the final result.\n3. Refinement: In this step, we feed the score along with the current generation result into the LVLM to refine the response. After generating a new update, we return to the critique step to obtain a new score. This process is repeated until the model produces a high-quality update (as defined in the critique step) or until the loop reaches a maximum iteration count of M."}, {"title": "Evaluate Models on DVE Tasks", "content": "Experimental Setup For the Classification Task, we selected seven models, categorized into two types: finetuning-based methods and models evaluated in the zero-shot setting. The finetuning-based models include VILT (Kim, Son, and Kim 2021), FLAVA (Singh et al. 2022), and CLIP (Radford et al. 2021). We fine-tuned these models on our training set with standard cross-entropy classification loss function. The models under the zero-shot setting include InstructBLIP (Dai et al. 2023), LLaVA-1.5, mPLUG-Owl (Ye et al. 2023), and GPT-4o. We directly prompt these pretrained LVLMs to generate a prediction for classification results. For the Generation Task, we selected six widely used LVLMs in a zero-shot setting as baselines: 1) InstructBLIP; 2) Multimodal-GPT (Gong et al. 2023); 3) MiniGPT-4 (Zhu et al. 2023); 4) mPLUG-Owl; 5) LLaVA-1.5; 6) GPT-4o. We select GPT-4o as the LVLM in reward-driven update optimization. More details of the experiments can be found in the supplementary material.\nResults and Analysis Classification Task Table 4 presents the accuracy of the various models on the classification task. From this table,"}, {"title": "Related Work", "content": "Natural Language Inference Textual entailment (Bowman et al. 2015; Williams, Nangia, and Bowman 2018; Nie et al. 2020), defined as determining whether a human would typically consider a hypothesis to be likely true given a premise, has become a cornerstone task in natural language processing. However, the task of textual entailment has faced criticism, studies have shown significant variability in human agreement on entailment judgments (Pavlick and Kwiatkowski 2019), leading to the proposal of alternative approaches that use ordinal or numeric values to represent plausibility (Zhang et al. 2017; Sakaguchi and Durme 2018; Chen et al. 2020a; Talman et al. 2023). This shift aims to capture the nuanced nature of entailment more accurately. In recent years, the focus has shifted towards the defeasibility of textual entailments, which involves revising or overturning conclusions based on new evidence. The 8-NLI dataset extends existing NLI datasets by including scenarios where new information can alter inferences, providing a more realistic evaluation of models' reasoning abilities (Rudinger et al. 2020). Similarly, the BoardgameQA dataset measures the reasoning capacity of language models when faced with contradictory information, guided by source preferences and implicit background knowledge, better reflecting real-world reasoning challenges (Kazemi et al. 2023). However, the defeasible entailment inference in the multimodal setting is still unexplored.\nVisual Understanding Tasks Visual Question Answering (VQA), image captioning, and visual reasoning are common visual understanding tasks. VQA aims to answer natural language questions based on provided visual information. The VQA-v1.0 dataset (Antol et al. 2015) was one of the first to address this task, focusing on the basic interaction between visual content and natural language questions. However, it faced issues related to biases and limited reasoning capabilities (Xie et al. 2019). To address these limitations, several datasets (Johnson et al. 2017; Goyal et al. 2017; Han et al. 2023; Mathew et al. 2022; Lu et al. 2021) have been developed to reduce biases and enhance reasoning capabilities. While VQA focuses on understanding and answering questions about visual content, image captioning involves generating natural language descriptions of an image's content (Lin et al. 2014; Young et al. 2014; Sidorov et al. 2020). In addition, visual reasoning involves understanding relationships and interactions between visual elements, enhancing comprehension of visual content (Thrush et al. 2022; Wu et al. 2023). However, these tasks can not capture fine-grained semantics reasoning relation change brought by the new information."}, {"title": "Conclusion", "content": "In this paper, we present a novel defeasible visual entailment task and a new benchmark for studying defeasibility in visual entailment. We also propose a novel inference-ware evaluator for capturing the change of entailment strength brought by the update and a new reward-driven update optimization method to further improve the quality of the update generated by the multimodal model. Our experimental results clearly show the effectiveness of our proposed inference-aware evaluator and reward-driven update optimization method."}, {"title": "Prompts", "content": "In this section, we present all the prompts we used in this paper.\nPrompt for the Classification Task\nWe illustrate our prompt for the classification task for all the LVLMs in Figure 5."}, {"title": "Classification Task Prompt", "content": "User Request\nYou are a helpful assistant that helps to determine if an update strengthens or weakens a hypothesis. The premise is an image that sets the scenario. The hypothesis is an inference based on this scenario, and the update provides additional information that could impact the hypothesis. Based on the premise provided and the given update, please judge whether the update strengthens or weakens the hypothesis. ONLY output strengthener or weakener in your final answer.\nConsider this image as a premise.\nHypothesis: A dog chases a rabbit.\nUpdate: There is a ball bouncing away from the dog."}, {"title": "Generation Task Prompt", "content": "User Request\nYou are a helpful assistant that generates updates to impact hypotheses based on given premises.\nConsider this image as a premise.\nHypothesis: A dog chases a rabbit.\nGoal: strengthener\nBased on the goal, generate an update that:\nStrengthens the hypothesis if the goal is a strengthener.\nWeakens the hypothesis if the goal is a weakener.\nEnsure the update is logically consistent with the image premise.\nPlease provide only the update. Do not include any other information."}, {"title": "DVE and Related Datasets", "content": "We further compare our DVE dataset with other related datasets, dividing them into two categories: visual understanding datasets and defeasible inference datasets. Table 5 provides a detailed comparison. Most visual understanding datasets like SNLI-VE (Xie et al. 2019), VQA-v2.0 (Antol et al. 2015), and CLEVR (Johnson et al. 2017) primarily focus on evaluating models' capabilities to interpret and reason about fixed, predefined visual scenes. However, they do not assess the models' ability to handle dynamic and nuanced semantic changes introduced by new, uncertain information. In contrast, DVE introduces the concept of defeasibility to tackle these uncertainties, thereby improving its capability to evaluate models' performance in reasoning with dynamic and uncertain information. Natural language inference datasets like 8-NLI (Rudinger et al. 2020) and \u03b4-CAUSAL (Cui et al. 2024) introduce defeasibility in the entailment task but lack a metric to assess the impact of new information and overlook the defeasible inference in visual"}, {"title": "Initial Response", "content": "User Request\nYou are a helpful assistant that generates updates to impact hypotheses based on given premises.\nConsider this image as a premise.\nHypothesis: Tall humans standing.\nGoal: weakener\nBased on the goal, generate an update that:\n- Strengthens the hypothesis if the goal is a strengthener.\nWeakens the hypothesis if the goal is a weakener.\nEnsure the update is logically consistent with the image premise.\nPlease provide only the update. Do not include any other information."}, {"title": "Refinement", "content": "User Request\nYou are a helpful assistant that generates updates to impact hypotheses based on given premises.\nConsider this image as a premise.\nHypothesis: Tall humans standing.\nGoal: weakener\nPrevious Update: The humans are carrying backpacks and walking sticks, suggesting they are hikers on a forest trail.\nPrevious Update's Defeasible Strength: 0.49621\nBased on the goal, generate an update that:\n- Strengthens the hypothesis if the goal is a strengthener.\nWeakens the hypothesis if the goal is a weakener.\nEnsure the update is logically consistent with the image premise.\nPlease provide only the update. Do not include any other information."}, {"title": "Effect from Writing Styles", "content": "To verify that the evaluator is effective when it meets texts with a writing style that differs from the training dataset, we performed additional analyses using examples with varying styles generated by different models.\nThe examples are as follows:\n\u2022 The image premise is shown in Figure 1 of our paper\n\u2022 Hypothesis: A dog chases a rabbit.\n\u2022 Strengtheners:\nThe dog looks like it's going to chase something any second now. Score: 5.4776\nEvery muscle in the dog's body is alert, signaling it's primed for a chase. Score: 5.5857\nWith that intense look, could the dog be any more ready to chase? Score: 5.4146\n\u2022 Weakeners:\nA rabbit could photobomb this chase, and the dog would not even look up\u2014it's got its eye on nothing else but its ball. Score: -4.3314\nWith a ball tossed by its owner, the dog's attention is fully absorbed in the game, showing zero interest in rabbits. Score: -4.3284\nThe dog is too absorbed in chasing the ball to even notice a rabbit. Score: -4.5062\nOur results indicate that the evaluator consistently assigns comparable scores across these diverse updates, demonstrating its robustness to stylistic variations."}, {"title": "Ablation for threshold and repetition", "content": "We conducted this experiment using different thresholds and repetition numbers. The result is shown in Table 6."}, {"title": "Human Evaluation for Updates", "content": "We conduct human evaluations for the generated updates, the results are in Table 7"}, {"title": "Implementation Detail for Evaluator", "content": "For the evaluation model, we select answers from LLaVA-1.5 (Liu et al. 2023) and GPT-4o. We employ a pre-trained BERT-large-uncased model 2 for text encoding and a ResNet50 model3 for visual feature extraction. We utilized Adam optimizer (Kingma and Ba 2015) with a batch size"}]}