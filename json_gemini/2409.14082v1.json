{"title": "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL", "authors": ["Ruilin Luo", "Liyuan Wang", "Binghuai Lin", "Zicheng Lin", "Yujiu Yang"], "abstract": "Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problems and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-the-art (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements, mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress.", "sections": [{"title": "1 Introduction", "content": "The Text-to-SQL task involves the automatic generation of SQL statements from natural language and has attracted much attention. Prior research primarily focused on training encoder-decoder models on text corpora and database schemas to capture generation patterns. Given the impressive capabilities of Large Language Models (LLMs) in various Natural Language Processing (NLP) tasks, numerous studies have endeavored to apply LLMs to this task.\nRecent investigations have proposed enhancing the reasoning capabilities of LLMs in the Text-to-SQL task, yielding substantial progress. Diverse methods such as the few-shot Chain-of-Thought (CoT), self-consistency, and the decomposition prompt that emphasizes dissecting complex problems and solving them sequentially have been introduced. A leading method, DIN-SQL, breaks down the task into several subtasks, classifies the complexity based on the nested logic of the problem, and applies different prompt strategies accordingly. However, like other studies, it overlooks the unique characteristics of SQL statements, which differ from math word problems and other code tasks. For calculations involving multiple sets, keywords like 'INTERSECT' or 'UNION' are often used to combine statements of several sub-problems, making these queries naturally suitable for decomposition. Counting and sorting problems typically rely on 'GROUP BY' operations to iden-"}, {"title": "2 Related Work", "content": "LLM Reasoning Nowadays, the development of reasoning models based on LLM has become a popular and critical field. Many efficient prompting methods have been proposed, such as Chain-of-Thought, which guides LLM in step-by-step thinking; Least-to-Most, which makes the model adapt to the difficulty gradient; and Decomposition-based prompting, which breaks down difficult problems to solve them separately. In addition, Self-Consistency demonstrates the overall tendency of LLM towards the correct answer through voting, Self-discover allows the model to make different problem-solving plans according to different types of questions, and Self-refine enables LLM to learn from the feedback of its problem-solving process. Besides, many works also strengthen the weaker aspects of LLM at the code level, such as PAL and PoT.\nLLM-based Text-to-SQL Nowadays, many studies are focusing on utilizing LLMs to complete Text-to-SQL tasks, primarily involving more efficient prompt design and advanced process de-"}, {"title": "3 Pipeline of PTD-SQL", "content": "In this section, we present the process of the PTD-SQL framework as illustrated in Figure 4, which includes: i. The design and implementation of the proposed Query Group Partition (QGP) sub-task; ii. The automatic construction of distinct query group question banks, each containing its unique reasoning process; iii. The inference process."}, {"title": "3.1 Query Group Partition", "content": "In this section, we first provide the definition of the QGP sub-task and then describe the process of fine-tuning the small LLM using PEFT to accomplish the QGP task.\nProblem Formulation SQL queries differ from math word problems and other code problems, such as Python, as their textual labels often contain highly characteristic expressions, making problem group identification convenient. We cluster them based on label keywords: multi-set, combination, filter, and other simple problems. Multi-set problems frequently involve two or more layers of logic and require keywords like 'INTERSECT', 'UNION', and 'EXCEPT' for connection. Combination problems necessitate the use of a 'GROUP BY' operation to group data, followed by sorting, taking extreme values, and other purposeful operations. Filter problems involve constructing condi-"}, {"title": "3.2 Targeted Drilling Bank Auto-construction", "content": "In this section, we explain how to construct targeted drilling banks for different question groups in PTD-SQL, which can be compared to the specialized training and reference ideas and answers designed by teachers for students before examinations. Previous works grade the difficulty based on whether the problem requires nesting and designing corresponding prompt templates. However, this approach only focuses on the surface logic of SQL queries and does not consider the distinct thinking paths required by the essence of different question groups for LLM. Given that selecting irrelevant examples may also be detrimental to LLM's thinking, in PTD-SQL, we can benefit from the proposed QGP. That is, for test queries of specific question groups, we can directly and accurately locate the problem banks with similar thinking paths.\nMulti-set problems often require breaking down a complex problem into multiple subqueries and integrating the different results through connecting keywords. For filtering problems, we can often prompt LLM to first propose the organization of filtering conditions and then process the selection target. Therefore, these two types of problems are naturally suitable for the design inspiration of decomposed prompting. For filtering problems, our decomposition focuses on the division of conditional statements and the extraction of target columns, and the specific prompts are shown in Appendix E.1. It is worth mentioning that we treat schema linking as a byproduct of LLM's thinking process, thereby achieving the purpose of one-time generation, which reduces the query cost.\nFor combination problems and other simple problems, we construct concise CoT templates. For the former, the model is required to distinguish the objects that need to be counted (sorted or taking extreme values) and the groups they belong to, thus improving the ability to organize answers under this question type. For the remaining simple problems, we choose to use the ground truth SQL query directly as the composition of the few-shot prompt without introducing other thinking processes.\nAfter creating four different types of few-shot prompts, we apply them separately to their respective problem groups in the training set to generate the thinking process and the final SQL query. We"}, {"title": "3.3 Few-shot Selection", "content": "Few-shot example construction is a crucial step in prompt engineering because LLMs are sensitive to few-shot samples. In PTD-SQL, we perform QGP on each textual query and then automatically select shots in the corresponding targeted drilling bank.\nSemantic matching Previous work has verified the effectiveness of methods based on semantic vector matching. We calculate and store sentence embeddings for all textual queries in the targeted drilling bank using OpenAI text-embedding-ada-002, resulting in an offline bank matrix M. For test queries, we encode them with text-embedding-ada-002 and calculate the cosine similarity with M to measure the degree of semantic matching as some previous works do.\n$$sim_1(s, s_i) = \\frac{Emb(s)Emb(s_i)^T}{|Emb(s)||Emb(s_i)|}$$\nSyntactic matching Considering that textual SQL queries have strong syntactic features, such as counting problems often having phrases like \"how many\", and extreme value demands often accompanied by comparative adjectives like \"largest\" or \"lowest\". Therefore, we use token overlap counts to rank the syntactic relevance of samples in the corresponding targeted drilling bank.\n$$sim_2(s, s_i) = \\frac{len(set(tokenize(s)) \\& set(tokenize(s_i)))}{len(set(tokenize(s)))}$$\nMix-of-matching Similar to the idea of multi-way recall, we mix an equal amount of examples selected by the two strategies above, for instance, choosing the top 2 most relevant examples from each in a 4-shot scenario, in order to provide as rich and relevant samples as possible within the same problem group, thus guiding effective thinking."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets Spider is the most widely used cross-domain dataset. This dataset has 7,000 training data in the training set and 1,034 data in the development set, covering 200 different databases and spanning 138 domains. Spider-realistic is a more challenging dataset containing 508 test data points, which manually mask the specific column selections in the"}, {"title": "4.2 Main Results", "content": "As shown in Table 2, PTD-SQL + GPT4 achieves the best EX metric on the Spider-dev dataset. Additionally, PTD-SQL surpasses DIN-SQL and DAIL-SQL when using ChatGPT and Deepseek-coder-6.7b-instruct as base models. Compared to the more advanced DAIL-SQL framework, PTD-SQL achieves relative increases of 1.5%, 3.1%, and 1.3% on ChatGPT, GPT-4 and Deepseek-coder-6.7b-instruct, respectively. When compared with previous fine-tuning and prompting methods, PTD-SQL also attains a comparative performance. Be-"}, {"title": "5 More Discussion", "content": "In this section, we investigate the efficacy of PTD-SQL, taking into account both the challenges posed by the database itself (RQ1) and the performance across various problem groups (RQ2). Concurrently, we delve into the insights that PTD-SQL contributes to the LLM-based Text-to-SQL domain. Furthermore, we perform ablation studies on the employed modules, primarily focusing on the effectiveness of introduced QGP task (RQ3), and the influence of shot selection strategies within the same targeted drilling bank (RQ4)."}, {"title": "5.1 RQ1: Performance from a Difficulty-level", "content": "In this subsection, we evaluate the superiority of PTD-SQL over existing state-of-the-art frameworks based on the difficulty levels defined by the database, respectively. As depicted in Table 5, PTD-SQL outperforms DIN-SQL and DAIL-SQL across different base LLMs, particularly at hard and extra difficulty levels, indicating that LLM can specialize in a problem group and demonstrate enhanced targeted reasoning ability after imitating and delving into problems within the same group. Moreover, we illustrate the performance variations of PTD-SQL in comparison to DIN-SQL across different problem types, thereby discerning the disparities between problem group partitioning strategies and difficulty grading strategies. As"}, {"title": "5.2 RQ2: Performance under Problem Groups", "content": "As depicted in Figure 6, PTD-SQL demonstrates a more pronounced advantage in multi-set problems and combination problems when employing three different baseline models. These problem types entail more intricate reasoning and perplexing conditions. Apart from when using GPT-4, the other two models yield very similar results in the filtering"}, {"title": "5.3 RQ3: Effectiveness of QGP", "content": "In this section, we examine the impact of the QGP subtask. As shown in Table 1, the Few-shot method does not align well within a specific context, resulting in weaker performance compared to the fine-tuned model. To further investigate this, we conduct additional experiments involving problem groups classified by ChatGPT, as well as experiments that eliminate the QGP stage and directly recall shots from all targeted drilling banks. The findings presented in Table 6 indicate that a decline in QGP accuracy adversely affects the final outcomes, with a relative decrease of 5.0% when testing on ChatGPT. Besides, ChatGPT exhibits a slight reduction in extra difficulty, while Deepseek demonstrates tolerance for classification accuracy at medium to easy difficulty levels. However, upon removing the QGP, the model surpasses the zero-shot performance, but there is a substantial decline in the results. This observation implies that incorporating various types of questions during similarity retrieval might introduce confusion and burden to the model and also validate the relevance of the QGP stage."}, {"title": "5.4 RQ4: Ablation on Few-shot Selection", "content": "In this section, ablation experiments are conducted for three distinct shot selection strategies within the same problem group. As illustrated in Figure 7, the hybrid strategy demonstrates a favorable integration effect beyond the 'easy' category, resulting in an overall improvement. This finding suggests that considering both query keywords and semantic similarity can yield a more comprehensive prompting effect."}, {"title": "5.5 Ablation on Few-shot Effect", "content": "As a few-shot prompting method, we believe that the number of examples is also an important factor affecting the results. Due to the context limitations we mentioned, we conduct ablation experiments with 4 shots or fewer. For the 1-shot scenario, we selected the single most similar example based on semantic similarity. The performance of PTD-SQL under different numbers of examples, different difficulty levels, and different question types is shown in Table 7 and Table 8, respectively.\nOur results show that when the number of examples is small, it has a greater impact on the final results, and the EX indicator generally shows a growing trend with the increase of examples. This suggests that more examples can stimulate more diverse thinking abilities under relatively limited context constraints. In our framework, more examples mean that the model has done more research on the same type of questions, thus achieving better results."}, {"title": "6 Conclusion", "content": "In this article, a novel method called PTD-SQL is proposed for LLMs to conduct targeted drilling on specific groups of questions after partitioning. This approach addresses the category tendency of SQL queries, which has been overlooked in previous work. By focusing on the thinking logic of specific types, LLM can effectively enhance its reasoning capabilities. Empirical observations from our comprehensive ablation studies reveal that PTD-SQL significantly reduces the likelihood of LLM making errors within its distinct capability range while demonstrating substantial gains across various question groups. Furthermore, it is posited that this approach can be extended to other domains, such as math word problems and different types of code problems, paving the way for future research."}, {"title": "7 Limitations", "content": "The limitations of this article lie in the exploration of its effectiveness on larger-scale databases with a broader domain span. Moreover, even SQL statements with strong structural characteristics may have different types of divisions. Therefore, a more detailed investigation of performance under these different divisions can be further improved and optimized. Besides, as stated in Appendix B.5, for queries with multiple question types, we can also recall example questions from multiple shot banks to comprehensively consider the model and improve the fault tolerance of QGP subtasks. This may be an interesting topic that can be improved in the future. In addition, due to space constraints, this article doesn't optimize for more detailed issues such as schema linking and database content alignment. However, the optimization methods for these issues can be relatively easily integrated into PTD-SQL as a downstream optimization method. Due to our greater focus on the improvement of LLM's reasoning ability for the question answering itself in this article, we are confident that we can achieve better results by adding the aforementioned sub-optimization methods."}, {"title": "A Supplementary Statistics", "content": "On Spider-dev and Spider-realistic datasets, the samples from the four different targeted drilling banks all come from random selections within their respective categories after automated classification in the training set, as shown in Table 9.\nHowever, the BIRD dataset does not provide a training set with regular attributes for generating candidate question banks. Our testing criterion is to randomly divide the Spider-dev dataset into 20% for training and the remaining 80% as a testing benchmark at three different difficulty levels. The training set is used for fine-tuning the classifier and building the targeted drilling bank. The statistical data of the targeted drilling bank on the BIRD-dev dataset is shown in Table A. Additionally, due to the lack of clearly defined multi-set operation queries in the BIRD-dev dataset, we only need to investigate the remaining three question types."}, {"title": "A.1 Statistics of Targeted Drilling Banks", "content": "On Spider-dev and Spider-realistic datasets, the samples from the four different targeted drilling banks all come from random selections within their respective categories after automated classification in the training set, as shown in Table 9.\nHowever, the BIRD dataset does not provide a training set with regular attributes for generating candidate question banks. Our testing criterion is to randomly divide the Spider-dev dataset into 20% for training and the remaining 80% as a testing benchmark at three different difficulty levels. The training set is used for fine-tuning the classifier and building the targeted drilling bank. The statistical data of the targeted drilling bank on the BIRD-dev dataset is shown in Table A. Additionally, due to the lack of clearly defined multi-set operation queries in the BIRD-dev dataset, we only need to investigate the remaining three question types."}, {"title": "B Supplementary Results", "content": ""}, {"title": "E Prompt Design", "content": "In this section, we elaborate on the prompt design employed in our study, which is crucial for the effective application of Large Language Models (LLMs) in Text-to-SQL tasks. The prompts serve as guiding questions or statements that help the LLMs focus on specific aspects of the problem and facilitate their learning process."}, {"title": "E.1 Targeted Drilling Bank Auto-construction on Spider", "content": "This section can be seen as supplementary materials for section 3.2. We provide all four types of shots generation prompts on the Spider dataset, which are leveraged on Spirder-dev and Spider-realistic datasets."}, {"title": "Shots Generation Prompt of Multi-set Problem", "content": "You are a powerful text-to-SQL reasoner. Currently, I am seeking to transform intricate text queries into analytical statements that simplify the creation of SQL statements, leading to the generation of the final SQL query. Our current focus lies in the category of multi-set operations. Please learn from the provided examples, design a detailed plan for the text query, and present the resulting SQL query."}, {"title": "E.2 Targeted Drilling Bank Auto-construction on BIRD", "content": "In this section, we provide the specific shots generation prompt for three types of problems on the BIRD dataset."}, {"title": "E.3 QGP Prompt", "content": "In this section, we demonstrate our few-shot instruction prompt using in QGP sub-task (Table 30)."}]}