{"title": "Three Dogmas of Reinforcement Learning", "authors": ["David Abel", "Mark K. Ho", "Anna Harutyunyan"], "abstract": "Modern reinforcement learning has been conditioned by at least three dogmas. The\nfirst is the environment spotlight, which refers to our tendency to focus on modeling\nenvironments rather than agents. The second is our treatment of learning as finding\nthe solution to a task, rather than adaptation. The third is the reward hypothesis, which\nstates that all goals and purposes can be well thought of as maximization of a\nreward signal. These three dogmas shape much of what we think of as the science\nof reinforcement learning. While each of the dogmas have played an important\nrole in developing the field, it is time we bring them to the surface and reflect on\nwhether they belong as basic ingredients of our scientific paradigm. In order to\nrealize the potential of reinforcement learning as a canonical frame for researching\nintelligent agents, we suggest that it is time we shed dogmas one and two entirely,\nand embrace a nuanced approach to the third.", "sections": [{"title": "On a Paradigm for Intelligent Agents", "content": "In The Structure of Scientific Revolution, Thomas Kuhn distinguishes between two phases of scientific\nactivity (Kuhn, 1962). The first Kuhn calls \"normal science\" which he likens to puzzle-solving, and\nthe second he calls the \"revolutionary\" phase, which consists of a re-imagining of the basic values,\nmethods, and commitments of the science that Kuhn collectively calls a \"paradigm\".\nThe history of artificial intelligence (AI) arguably includes several swings between these two phases,\nand several paradigms. The first phase began with the 1956 Dartmouth workshop (McCarthy et al.,\n2006) and arguably continued up until sometime around the publication of the report by Lighthill\net al. (1973) that is thought to have heavily contributed to the onset of the first AI winter (Haenlein &\nKaplan, 2019). In the decades since, we have witnessed the rise of a variety of methods and research\nframes such as symbolic AI (Newell & Simon, 1961; 2007), knowledge-based systems (Buchanan\net al., 1969) and statistical learning theory (Vapnik & Chervonenkis, 1971; Valiant, 1984; Cortes &\nVapnik, 1995), culminating in the most recent emergence of deep learning (Krizhevsky et al., 2012;\nLeCun et al., 2015; Vaswani et al., 2017) and large language models (Brown et al., 2020; Bommasani\net al., 2021; Achiam et al., 2023).\nIn the last few years, the proliferation of AI systems and applications has hopelessly outpaced our\nbest scientific theories of learning and intelligence. Yet, it is our duty as scientists to provide the\nmeans to understand the current and future artifacts borne from the field, especially as these artifacts\nare set to transform society. It is our view that reflecting on the current paradigm and looking\nbeyond it is a key requirement for unlocking this understanding.\nIn this position paper, we make two claims. First, reinforcement learning (RL) is a good candidate\nfor a complete paradigm for the science of intelligent agents, precisely because \"it explicitly considers\nthe whole problem of a goal-directed agent interacting with an uncertain environment\" (p. 3, Sutton\n& Barto, 2018). Second, in order for RL to play this role, we must reflect on the ingredients of\nour science and shift a few points of emphasis. These shifts are each subtle departures from three\n\"dogmas\", or implicit assumptions, summarized as follows:"}, {"title": "Dogma One: The Environment Spotlight", "content": "The first dogma we call the environment spotlight (Figure 1), which refers to our collective focus on\nmodeling environments and environment-centric concepts rather than agents. For example, the\nagent is essentially the means to deliver a solution to an MDP, rather than a grounded model in itself.\nWe do not fully reject this behaviourist view, but suggest balancing it; after all the classical RL\ndiagram features two boxes, not just one. We believe that the science of AI is ultimately about\nintelligent agents, as argued by Russell & Norvig (1995); yet, much of our thinking, as well as our\nmathematical models, analysis, and central results tend to orbit around solving specific problems,\nand not around agents themselves. In other words, we lack a canonical formal model of an agent.\nThis is the essence of the first dogma.\nWhat do we mean when we say that we focus on environments? We suggest that it is easy to answer\nonly one of the following two questions:\n1. What is at least one canonical mathematical model of an environment in reinforcement learning?\n2. What is at least one canonical mathematical model of an agent in reinforcement learning?\nThe first question has a straightforward answer: the MDP, or any of its nearby variants such as a\nk-armed bandit (Lattimore & Szepesv\u00e1ri, 2020), a contextual bandit (Langford & Zhang, 2007), or a\npartially observable MDP (POMDP; Cassandra et al., 1994). These each codify different versions\nof decision making problems, subject to different structural assumptions\u2014in the case of an MDP,\nfor instance, we make the Markov assumption by supposing there is a maintainable bundle of\ninformation we call the state that is a sufficient statistic of the next reward and next distribution over\nthis same bundle of information. We assume these states are defined by the environment and are\ndirectly observable by the agent at each time step for use in learning and decision making. The\nPOMDP relaxes this assumption and instead only reveals an observation to the agent, rather than\nthe state. By embracing the MDP, we are allowed to import a variety of fundamental results and\nalgorithms that define much of our primary research objectives and pathways. For example, we know\nevery MDP has at least one deterministic, optimal, stationary policy, and that dynamic programming\ncan be used to identify this policy (Bellman, 1957; Blackwell, 1962; Puterman, 2014). Moreover, our\ncommunity has spent a great deal of effort in exploring variations of the MDP such as the Block\nMDP (Du et al., 2019) or Rich Observation MDP (Azizzadenesheli et al., 2016), the Object-Oriented\nMDP (Diuk et al., 2008), the Dec-POMDP (Oliehoek et al., 2016), Linear MDPs (Todorov, 2006), and\nFactored MDPs (Guestrin et al., 2003), to name a few. These models each forefront different kinds of\nproblems or structural assumptions, and have inspired a great deal of illuminating research.\nIn contrast, this second question (\"what is a canonical agent model?\") has no clear answer (Haru-\ntyunyan, 2020). We might be tempted to respond in the form of a specific kind of a popular learning\nalgorithm, such as Q-learning (Watkins & Dayan, 1992), but we suggest that this is a mistake.\nQ-learning is just one instance of the logic that could underlie an agent, but it is not a generic\nabstraction of what an agent actually is, not in the same way that a MDP is a model for a broad\nfamily of sequential decision making problems. As discussed by Harutyunyan (2020), we lack a\ncanonical model of an agent, or even a basic conceptual picture. We believe that at this stage of the\nfield, this is becoming a limitation, and is due in part to our focus on environments.\nIndeed, the exclusive focus on environment-centric concepts (such as the dynamics model, envi-\nronment state, optimal policy, and so on) can often obscure the vital role of the agent itself. But,\nhere we wish to reignite interest in an agent-centric paradigm that can give us the conceptual clarity\nwe need to be able to develop and discover general principles of agency. Without such ground\ncurrently, we struggle to even precisely define and differentiate between key agent families such as\n\"model-based\" and \"model-free\" agents (though some precise definitions have been given by Strehl\net al. 2006 and Sun et al., 2019), or study more complex questions about the agent-environment\nboundary (Jiang, 2019; Harutyunyan, 2020), the extended-mind (Clark & Chalmers, 1998), embedded\nagency (Orseau & Ring, 2012), the effect of embodiment (Ziemke, 2013; Martin, 2022), or the impact\nof resource-constraints (Simon, 1955; Griffiths et al., 2015; Kumar et al., 2023; Aronowitz, 2023) on\nour agents in a general way. Most agent-centric concepts are typically beyond the scope of the basic\nmathematical language of our field, and are consequently not featured in our experimental work.\nThe Alternative: Shine the Spotlight on Agents, Too. Our suggestion is simple: it is important to\ndefine, model, and analyse agents in addition to environments. We should build toward a canonical\nmathematical model of an agent that can open us to the possibility of discovering general laws\ngoverning agents (if they exist), building on the work of Russell & Subramanian (1994), Wooldridge\n& Jennings (1995), Kenton et al. (2023), and echoing the call of Sutton (2022). We should engage in\nfoundational work to establish axioms that characterize important agent properties and families,\nas in work by Sunehag & Hutter (2011; 2015) and Richens & Everitt (2024). We should do this in\na way that is confluent with our latest empirical data about agents, drawing from the variety of\ndisciplines that study agents, from psychology, cognitive science, and philosophy, to biology, AI,\nand game theory. Doing so can expand the purview of our scientific efforts to understand and design\nintelligent agents."}, {"title": "Dogma Two: Learning as Finding a Solution", "content": "The second dogma is embedded in the way we treat the concept of learning. We tend to view\nlearning as a finite process involving the search for\u2014and eventual discovery of\u2014a solution to a\ngiven task. For example, consider the classical problem of an RL agent learning to play a board\ngame, such as Backgammon (Tesauro et al., 1995) or Go (Silver et al., 2016). In each of these cases, we\ntend to assume a good agent is one that will play a vast number of games to learn how to play the\ngame effectively. Then, eventually, after enough games, the agent will reach optimal play and can\nstop learning as the desired knowledge has been acquired.\nIn other words, we tend to implicitly assume that the learning agents we design will eventually\nfind a solution to the task at hand, at which point learning can cease. This is present in many of\nour classical benchmarks, too, such as mountain car (Taylor et al., 2008) or Atari (Bellemare et al.,\n2013), in which agents learn until they reach a goal. On one view, such agents can be understood\nas searching through a space of representable functions that captures the possible action-selection\nstrategies available to an agent (Abel et al., 2023b), similar to the Problem Space Hypothesis (Newell,\n1994). And, critically, this space contains at least one function\u2014such as the optimal policy of an\nMDP-that is of sufficient quality to consider the task of interested solved. Often, we are then\ninterested in designing learning agents that are guaranteed to converge to such an endpoint, at which\npoint the agent can stop its search (and thus, stop its learning). This process is pictured in Figure 2,\nand is summarized in the second dogma.\nThis view is embedded into many of our objectives, and follows quite naturally from the use of the\nMDP as a model of the decision making problem. It is well established that every MDP has at least\none optimal deterministic policy, and that such a policy can be learned or computed through dynamic\nprogramming or approximations thereof. The same tends to be true of many of the alternative\nlearning settings we consider.\nThe Alternative: Learning as Adaptation. Our suggestion is to embrace the view that learning\ncan also be treated as adaptation (Barron et al., 2015). As a consequence, our focus will drift away\nfrom optimality and toward a version of the RL problem in which agents continually improve, rather\nthan focus on agents that are trying to solve a specific problem. Of course, versions of this problem\nhave already been explored through the lens of lifelong (Brunskill & Li, 2014; Schaul et al., 2018),\nmulti-task (Brunskill & Li, 2013), and continual RL (Ring, 1994; 1997; 2005; Khetarpal et al., 2022;\nAnand & Precup, 2023; Abel et al., 2023b; Kumar et al., 2023). Indeed, this perspective is highlighted\nin the introduction of the textbook by Sutton & Barto (2018):"}, {"title": "Dogma Three: The Reward Hypothesis", "content": "The third dogma is the reward hypothesis (Sutton, 2004; Littman, 2015; Christian, 2021; Abel et al., 2021;\nBowling et al., 2023), which states \"All of what we mean by goals and purposes can be well thought\nof as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\"\nFirst, it is important to acknowledge that this hypothesis is not deserving of the title \"dogma\" at all.\nAs originally stated, the reward hypothesis was intended to organize our thinking around goals\nand purposes, much like the expected utility hypothesis before it (Machina, 1990). And, the reward\nhypothesis seeded the research program of RL in a way that has led to the development of many of\nour most celebrated results, applications, and algorithms.\nHowever, as we continue our quest for the design of intelligent agents (Sutton, 2022), it is important\nto recognize the nuance in the hypothesis.\nIn particular, recent analysis by Bowling et al. (2023), building on the work of Pitis (2019); Abel et al.\n(2021) and Shakerinava & Ravanbakhsh (2022), fully characterizes the implicit conditions required\nfor the hypothesis to be true. These conditions come in two forms. First, Bowling et al. provide\na pair of interpretative assumptions that clarify what it would mean for the reward hypothesis\nto be true or false\u2014roughly, these amount to saying two things. First, that \"goals and purposes\"\ncan be understood in terms of a preference relation on possible outcomes. Second, that a reward\nfunction captures these preferences if the ordering over agents induced by value functions matches\nthat of the ordering induced by preference on agent outcomes. Then, under this interpretation, a\nMarkov reward function exists to capture a preference relation if and only if the preference relation\nsatisfies the four von Neumann-Morgenstern axioms (von Neumann & Morgenstern, 1953), and a\nfifth Bowling et al. call y-Temporal Indifference.\nThis is significant, as it suggests that when we write down a Markov reward function to capture a\ndesired goal or purpose, we are forcing our goal or purpose to adhere to the five axioms, and we\nmust ask ourselves if it is always appropriate. As an example, consider the classical challenge on the\nincomparability (or incommeasurability) of values in ethics, as discussed by Chang (2015). That is,\ncertain abstract virtues such as happiness and justice might be thought to be incomparable to one\nanother. Or, similarly, two concrete experiences might be incommeasurable, such as a walk on the\nbeach and eating breakfast\u2014how might we assign measure to each of these experiences in the same\n\"currency\"? Chang notes that two items might not be comparable without further reference to a\nparticular use, or context: \"A stick can't be greater than a billiard ball...it must be greater in some\nrespect, such as mass or length.\" However, the first axiom, completeness, strictly requires that the\nimplicit preference relation assigns a genuine preference between all pairs of experiences. As such, if\nwe take the reward hypothesis to be true, we can only encode goals or purposes in a reward function\nthat reject both incomparability and incommeasurability. It is worth noting that completeness in\nparticular has been criticized by Aumann (1962) due to the demands it places on the individual\nholding the preference relation. Finally, the completeness axiom is not the only one restricting the\nspace of viable goals and purposes; axiom three, independence of irrelevant alternatives, famously\nrejects risk-sensitive objectives as well due to the Allais paradox (Allais, 1953; Machina, 1982).\nThe Alternative: Recognize and Embrace Nuance. Our suggestion is to simply call attention to\nthe limitations of scalar rewards, and to be open to other languages for describing an agent's goals.\nIt is important that we are aware of the implicit restrictions we are placing on the viable goals and\npurposes under consideration when we represent a goal or purpose through a reward signal. We\nshould become familiar with the requirements imposed by the five axioms, and be aware of what\nspecifically we might be giving up when we choose to write down a reward function. On this latter\npoint there is a profound opportunity for future work. It is also worth highlighting the fact that\npreferences are themselves just another language for characterizing goals\u2014there are likely to be\nothers, and it is important to cast a wide net in our approach to thinking about goal-seeking."}, {"title": "Discussion", "content": "We have here argued that the long-term vision of RL should be to provide a holistic paradigm for\nthe science of intelligent agents. To realise this vision, we suggest that it is time to reconcile our\nrelationship with three implicit dogmas that have shaped aspects of RL so far. These three dogmas\namount to over-emphasis (1) on environments, (2) on finding solutions, and (3) on rewards as a\nlanguage for describing goals. Further, we have initial suggestions on how to pursue research that\nmakes subtle departures from these dogmas. First, we should treat agents as one of our central\nobjects of study. Second, we must move beyond studying agents that find solutions for specific tasks,\nand also study agents that learn to endlessly improve from experience. Third, we should recognize\nthe limits of embracing reward as our language for goals, and consider alternatives.\nOpen Questions. Each of these suggestions can be translated into important research questions we\nencourage the community to explore further. First, what is our canonical model of an agent? Several\nrecent proposals have emerged, and agree on many aspects. What are the consequences of adopting\none view, rather than another? Which ingredients of an agent are necessary, rather than extraneous?\nWe suggest that it is important to think carefully about these questions, and adopt conventions for\nthe standard model of an agent. Such a model can be used to clarify old questions, and open new\nlines of study around agent-centric concepts such as the agent-environment boundary (Todd &\nGigerenzer, 2007; Orseau & Ring, 2012; Harutyunyan, 2020), embodiment (Ziemke, 2013; Martin,\n2022), resource-constraints (Simon, 1955; Ortega, 2011; Braun & Ortega, 2014; Ortega et al., 2015;\nGriffiths et al., 2015; Kumar et al., 2023; Aronowitz, 2023), and embedded agency (Orseau & Ring,\n2012). Second, what is the goal of learning when we give up the concept of a task's solution? In other\nwords: how do we think about learning when no optimal solution can be found? How do we begin\nto evaluate such agents, and measure their learning progress? Third, we suggest embracing a wide\nvariety of views about plausible accounts of the objectives of an agent. This includes continuing\nto embrace classical accounts of reward maximization, but also considering varied objectives like\naverage reward (Mahadevan, 1996), risk (Howard & Matheson, 1972; Mihatsch & Neuneier, 2002),\nconstraints (Altman, 2021), logical goals (Littman et al., 2017), or even open-ended goals (Samvelyan\net al., 2023).\nOn the term \"Dogma\". The title of this paper and use of the term \"dogma\" are an homage to \"Two\nDogmas of Empiricism\" by Quine (1951). The term \"dogma\" casts a more negative light on each\nof the principles than we intend (though, as Kuhn (1963) notes, there is a role for dogma in the\nsciences). Indeed, as discussed, the reward hypothesis was originally conceived of as a hypothesis\nas its name suggests. Still, it is a principle that is often taken as a presupposition that frames the\nrest of the field of RL similar to the way that the Church-Turing Thesis frames computation\u2014they\nare both standard pre-scientific commitments that are part of most research programmes (Lakatos,\n2014). The other two dogmas are both implicit rather than conventions we regularly state openly and\nembrace; it is rare to see work in RL actively argue against the importance of thinking about agents\nor agency, for instance. Instead, it is a convention to begin most RL research by framing our research\nquestions around dynamic programming and MDPs. In this sense, the community has been drawn\nto specific well-tread research paths that involve modeling environments first, rather than agents\ndirectly. The same implicit character is true of the second dogma: due to our focus on MDPs and\nrelated models, it also tends to be the case that instances of the RL problem we study have a well\nstructured solution that is known to be discoverable through means such as dynamic programming\nor temporal difference learning. We then often use language involving an algorithm solving a task by\nconverging to an optimal policy, reflecting the influence of the second dogma. It is in this sense that\nwe take the term \"dogma\" to be fitting of the first two: we tend not to question these aspects of our\nresearch programme, yet they influence much of our methods and goals.\nIt is worth noting that it is understandable why the sentiments underlying the three dogmas were\nadopted: by building our study from Markov models, we can make use of the suite of well-understood,\nefficient algorithms based on dynamic programming, thanks to the seminal work by Bellman (1957),\nSutton (1988), Watkins (1989), and others. This is further supported by the way that fundamental\nresults from stochastic approximation (Robbins & Monro, 1951) have influenced many classical\nresults, such as the convergence of Q-learning by Watkins & Dayan (1992) or TD-learning with\nfunction approximation by Tsitsiklis & Van Roy (1996).\nInspiration. We are not the first to suggest moving beyond some of these conventions. The work on\ngeneral reinforcement learning by Hutter (2000; 2002; 2004) and colleagues (Lattimore & Hutter, 2011;\nLeike, 2016; Cohen et al., 2019) has long studied RL in the most general possible setting. Indeed,\nthe stated goal of the original work on AIXI by Hutter (2000) was \"...to introduce the universal AI\nmodel\" (p. 3). Similarly, a variety of work has explicitly focused on agents. For instance, the classical\nAl textbook by Russell & Norvig (1995) defines AI \"as the study of agents that receive percepts from\nthe environment and perform actions\" (p. viii), and frames the book around \"the concept of the\nintelligent agent\" (p. vii). Russell & Subramanian (1994) also feature a general take on goal-directed\nagents that has shaped much of the agent-centric literature that follows\u2014the agent functions there\nintroduced have been more recently adopted as one model of an agent (Abel et al., 2023a;b). Sutton\n(2022) proposes the \"quest for a common model of the intelligent decision maker\", and provides\ninitial suggestions for how to frame this quest. Work by Dong et al. (2022) and Lu et al. (2021)\nhave built on the traditions of agent-centric modeling, providing detailed accounts of the possible\nconstituents of an agent's internal mechanism, similar to Sutton. Further work by Kenton et al. (2023)\nand Richens & Everitt (2024) explore a causal perspective on agents, giving both concrete definitions\nand insightful results. Outside of AI, the subject of agency is an important subject of discourse in its\nown right-we refer the reader to the work by Barandiaran et al. (2009) and Dretske (1999) or the\nbooks by Tomasello (2022) and Dennett (1989) for further insights from nearby communities.\nSimilarly, a variety of work has explored alternative ways to think about goals. For instance, Little &\nSommer (2013) study an agent that learns a predictive model of its environment, and ground this\nstudy using the tools of information theory. This is similar in spirit to the Free-Energy Principle\nadvocated for by Friston (2010), with recent work by Hafner et al. (2020) exploring connections\nto RL. Preferences have also been used as an alternative to rewards, as in preference-based RL\n(Wirth et al., 2017), with a more recent line of work on RL from human feedback (Christiano et al.,\n2017; MacGlashan et al., 2016; 2017) now playing a significant role in the current wave of language\nmodel research (Achiam et al., 2023). Others have proposed the use of various logical languages\nfor grounding goals, such as linear temporal logic (Littman et al., 2017; Li et al., 2017; Hammond\net al., 2021) and nearby structures such as reward machines (Icarte et al., 2022). Another perspective\npresented by Shah et al. (2021) explicitly contrasts the framing of assistance games (Hadfield-Menell\net al., 2016) with reward maximization, and suggests that the former provides a more compelling\npath to designing assistive agents. Lastly, a variety of work has considered forms of goal-seeking\nbeyond expected cumulative reward, as in ordinal dynamic programming (Koopmans, 1960; Sobel,\n1975), convex RL (Zahavy et al., 2021; Mutti et al., 2022; 2023), other departures from the expectation\n(Bellemare et al., 2017; 2023), or by incorporating other objectives such as constraints (Le et al., 2019;\nAltman, 2021) or risk (Mihatsch & Neuneier, 2002; Shen et al., 2014; Wang et al., 2023).\nOther Dogmas. There are many other assumptions inherent to the basic philosophy of reinforcement\nlearning that we did not discuss. For instance, it has been common to focus on agents that learn\nfrom a tabula rasa state, rather than consider other stages of learning. We also tend to adopt the\ncumulative discounted reward with a geometric discounting schedule as the objective, rather than\nusing a hyperbolic schedule (Fedus et al., 2019), or consider the existence of environment-state rather\nthan a partially observable setting (Cassandra et al., 1994; Dong et al., 2022). We take it that reflecting\non these and other perspectives is also important, but that they have already received significant\nattention by the community.\nConclusion. We hope this paper can reinvigorate the RL community to explore beyond our current\nframes. We believe this begins by embracing the vision that RL is a good candidate for a holistic\nparadigm of intelligent agents, and continues with a careful reflection of the values, methods, and\ningredients of our scientific practice that will enable this paradigm to flourish."}]}