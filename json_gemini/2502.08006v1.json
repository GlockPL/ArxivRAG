{"title": "GREED IS GOOD: GUIDED GENERATION FROM A GREEDY PERSPECTIVE", "authors": ["Zander W. Blasingame", "Chen Liu"], "abstract": "Training-free guided generation is a widely used and powerful technique that\nallows the end user to exert further control over the generative process of diffusion\nmodels. In this work, we explore the guided generation from the perspective of\noptimizing the solution trajectory of a neural differential equation in a greedy\nmanner. We present such a strategy as a unifying view on training-free guidance\nby showing that the greedy strategy is a first-order discretization of end-to-end\noptimization techniques. We show that a greedy guidance strategy makes good\ndecisions and compare it to a guidance strategy using the ideal gradients found via\nthe continuous adjoint equations. We then show how other popular training-free\nguidance strategies can be viewed in a unified manner from this perspective.", "sections": [{"title": "1 INTRODUCTION", "content": "Guided generation greatly extends the utility of state-of-the-art generative models by allowing the\nend user to exert greater control over the generative process, ultimately making the tool more useful\nin a wide variety of applications ranging from conditional generation, editing of samples, inverse\nproblems, &c. We focus on guided generation with diffusion/flow models (Song et al., 2021b; Lipman\net al., 2023; Ho et al., 2020) as these models represent the current state-of-the-art across many\ndifferent modalities, e.g., audio (Liu et al., 2023a), images (Rombach et al., 2022), video (Blattmann\net al., 2023), protein generation (Skreta et al., 2024), &c.\nWe can divide the techniques for guided generation into two broad categories: conditional training and\ntraining-free methods. The former of these two requires the training of the underlying diffusion/flow\nmodel on additional conditional information, either as a part of the training or at a later time as\nadditional fine-tuning (Song et al., 2021a; Ho & Salimans, 2021; Hu et al., 2022). The latter category\ninstead makes use of some known guidance function defined on the data distribution and incorporates\nthis information back to the model to influence the generative process.\nThese training-free techniques can be further broken down into two sub-categories, i.e., posterior\nsampling and end-to-end optimization. The former class of techniques uses a simple estimation of\nthe posterior distribution that can be easily found in diffusion models (Chung et al., 2023) and some\nflow models (cf. Lipman et al., 2024, Section 4.8). This simple estimate of the posterior can then be\nfed to a guidance function to construct a gradient which can be brought back into the intermediate\nsampling state. This can then be used to update the ODE solve as a form of classifier guidance (Chung\net al., 2023; Yu et al., 2023). The latter class of techniques, in contrast, performs backpropagation\nthroughout the entire sampling process of the model to update the initial sample used in the solution\ntrajectory via the guidance function. Many of these techniques (Blasingame & Liu, 2024a; Ben-Hamu\net al., 2024; Pan et al., 2024; Marion et al., 2024; Wang et al., 2024) make use of the continuous\nadjoint equations (see Kidger, 2022, Section 5.1.2) to perform this backpropagation, this technique is\nknown as optimize-then-discretize. However, some approaches (Liu et al., 2023c) prefer to perform\nthe more memory-expensive, but potentially more accurate, discretize-then-optimize (see Kidger,\n2022, Section 5.1.1) method, which amounts to performing backpropagation through the entire\ndiscretized sampling process. Finally, some works have viewed this through the lens of optimal\ncontrol and introduce a control signal into the sampling process, which can be optimized through\noptimize-then-discretize (Wang et al., 2024) or discretize-then-optimize (Liu et al., 2023c)."}, {"title": "2 PRELIMINARIES", "content": "Diffusion models seek to learn a mapping from some simple prior distribution p(x) to the data\ndistribution q(x). A forward process from the data distribution to the prior distribution is defined via\nan It\u00f4 Stochastic Differential Equation (SDE):\n$$dxt = f(t)xt dt + g(t) dwt,$$\n(2.1)\non the time interval [0, T] and where {wt}t\u2208[0,T] is the standard Wiener process. The reverse-time\nSDE (Anderson, 1982) is found to be:\n$$dxt = [f(t)xt + g\u00b2(t)\u2207x log pt(xt)] dt + g(t) dWt,$$\n(2.2)\nwhere dt is a negative timestep and {Wt}t\u2208[0,T] is the standard Wiener process in reverse time. The\ndiffusion models then learn the score function, se(x) = \u2207xlog pt(xt), (Song et al., 2021b) or\nsome other closely related quantity, such as noise prediction (Song et al., 2021a; Ho et al., 2020) or\ndata prediction (Kingma et al., 2021). Using this learned score function, we can then sample the\nreverse-time SDE, Equation (2.2), using numerical SDE solvers. Song et al. (2021b) showed that\nthere exists an Ordinary Differential Equation (ODE) known as the probability flow ODE which\nshares the same marginals, pt(x), as Equation (2.2) given by\n$$\\frac{dxt}{dt} = f(t)xt + \\frac{1}{2}g^{2}(t) \\nabla_{x} \\log p_{t}(x_{t}),$$\n(2.3)\nwhich also defined in reverse-time."}, {"title": "2.2 FLOW MODELS", "content": "Flow models (Lipman et al., 2023) are another popular class of generative models that have theoretical\noverlaps with diffusion models. Consider two random variables: Xo ~ p(x) and X\u2081 ~ q(x). Then\nconsider a time-dependent vector field u \u2208 C\u00b9, ([0, 1] \u00d7 Rd; Rd) with r > 1 which determines\u00b2 a\ntime-dependent flow It \u2208 C\u00b9,r ([0, 1] \u00d7 Rd; Rd) which satisfies the ODE\n$$\\frac{d}{dt} \u0424_{t}(x) = u(t, P_{t}(x)),\\qquad  \u0424_{0}(x) = x,  \u0424_{1}(x)  = q(t, P_{t}(x)).$$\n(2.4)\nThis is known as a Cr-flow and this flow is diffeomorphism in its second argument for all t \u2208\n[0, 1]. Applying this flow to the random variable Xo we define a continuous-time Markov process"}, {"title": "2.3 THE CONTINUOUS ADJOINT EQUATIONS", "content": "Let u\u0473 \u2208 C1,1 ([0, 1] \u00d7 Rd; Rd) be a model that models the vector field of some ODE and be Lipschitz\ncontinuous in its second argument. Let \u00e6 : [0, 1] \u2192 Rd be the solution to the ODE with the initial\ncondition xo \u2208 Rd, xt = uo(t, xt). For some scalar-valued loss function L \u2208 C\u00b2(Rd) in x1, let\nax := dL/dxt denote the gradient. Then az and related quantity a\u0473 := dL/80 can be found by\nsolving an augmented ODE of the form,\n$$\n\\begin{aligned}\n\\frac{d \\alpha_{x}}{dt}(t) &= - \\alpha_{x}(t) \\frac{\\partial u_{\\theta}}{\\partial x}(t, x_{t}), \\qquad \\alpha_{x}(1) = \\frac{\\partial \\mathcal{L}}{\\partial x_{1}}\\\\\n\\frac{d \\alpha_{\\theta}}{dt}(t) &= - \\alpha_{x}(t)^T \\frac{\\partial u_{\\theta}}{\\partial \\theta}(t, x_{t}), \\qquad \\alpha_{\\theta}(1) = 0,\n\\end{aligned}\n$$\n(2.5)\nthis system of equations are known as the continuous adjoint equations (Kidger, 2022). N.B., this\ntechnique was first proposed by Pontryagin et al. (1963) and popularized for neural differential\nequations by Chen et al. (2018). Solving the continuous adjoint equations to find the gradients\ndescribed by the adjoint state is known as an optimize-then-discretize method. This is in contrast\nwith discretize-then-optimize methods which first discretize the neural differential equation before\ncalculating the gradients (this can be thought of as \"vanilla\u201d backprop through a neural network). For\na more detailed discussion between these two methods we refer to Kidger's monograph on neural\ndifferential equations (2022, Section 5.1)"}, {"title": "3 RELATED WORKS", "content": "Training-free methods for diffusion/flow models is an active area of research with many different\ntechniques proposed with differing strengths. Diffusion Posterior Sampling (DPS) (Chung et al.,\n2023) is a guidance method that uses Tweedie's formula (Stein, 1981) to estimate the gradient of\nsome guidance function defined in the output state w.r.t. the noisy state, i.e., E[x1|\u00e6t]. Likewise\nthe work of Bansal et al. (2023); Wang et al. (2023); Yu et al. (2023) explore similar concepts by\nemploying Tweedie's formula for diffusion models, with FreeDoM including an additional \u201ctime-\ntravel strategy\" (Wang et al., 2023) to help improve the visual fidelity of the generated images. More\nrelevant to our work, Blasingame & Liu (2024b) explore guidance using posterior sampling from a\ngreedy perspective; however, their analysis is limited as their work is only focused on a particular\nexperimental application. Specifically, they only consider a first-order numerical scheme for the\ndiffusion ODE solver and they do not connect the greedy strategy to other guided generation methods.\nLiu et al. (2023c) provide guidance by adding an additional time-dependent control term to the\nvector field that is optimized using a type of discretize-then-optimize method by finding an efficient\nnon-uniform Euler discretization of the sampling trajectory and perform backpropagation through\nthis discretization to update the control signal.\nSeveral recent works have explored the use of continuous adjoint equations for diffusion/flow models.\nNie et al. (2022) was the first to explore this topic, solving the continuous adjoint equations for\nadversarial purification with diffusion SDEs. Later work by Pan et al. (2024; 2023) explore special\nsolvers for the continuous adjoint equations of VP-type diffusion ODEs. Blasingame & Liu (2024a)\nextends these works by developing bespoke solvers for VP-type diffusion ODEs and SDEs. Marion"}, {"title": "4 A MOTIVATION FOR GREED", "content": "Consider the case of Affine Gaussian Probability Paths (AGPP) and diffusion models in the ODE\nformulation with a data coupling, (X0, X1) ~ p(x0)q(x1) with a Gaussian prior p(x0). We define\nthe new random variable Xt as an affine interpolation,\n$$Xt = \\alpha_{t}X_{1} + \\sigma_{t}X_{0},$$\n(4.1)\nwhere at, \u03c3\u03c4 \u2208 C\u221e([0, 1]; [0, 1]) are smooth functions with boundary conditions \u03b1\u03bf = \u03c3\u2081 = 0 and\n\u03b1\u2081 = \u03c3\u03bf = 1 which define the schedule of {Xt}t\u2208[0,1]. Furthermore, at is strictly monotonically\nincreasing in t; whereas, ot is strictly monotonically decreasing in t (Lipman et al., 2024). Then the\naffine conditional flow is defined as t(x|x1) = AtX1 + otx. It follows that the marginal vector\nfield is given by\n$$u(t, x) = \\mathbb{E}[\\dot{\\alpha}_{t}X_{1} + \\dot{\\sigma}_{t}X_{0}|X_{t} = x],$$\n(4.2)\nwhere at denotes the first derivative of at w.r.t. time. Now assume that we have trained our\ndiffusion/flow model, ue, to zero loss."}, {"title": "4.1 DRAWBACKS OF THE CONTINUOUS ADJOINT EQUATIONS", "content": "A natural choice to solve this problem is to apply the continuous adjoint equations to find the gradient\nL/dx and use that quantity for the optimization problem; however, there are several potential\ndrawbacks to this approach which we will enumerate."}, {"title": "5 A GREEDY PERSPECTIVE", "content": "The computational cost of adjoint methods can be quite high, as is admitted in papers exploring this\nmethod (Ben-Hamu et al., 2024; Blasingame & Liu, 2024a; Pan et al., 2024; Wang et al., 2024). As\nsuch, it is worth considering whether techniques such as FreeDoM (Yu et al., 2023) and DPS (Chung\net al., 2023), which are much cheaper, can be used as a reasonable alternative to techniques that use\nadjoint methods. In other words, we ask the following.\nRather than performing the full optimization back to xo, what if we greedily took an optimal\nstep at each \u00e6t instead?\nThe next question is how to define a locally optimal step at time t. For this purpose we can use\nposterior sampling, which is defined as 21\\t(x) = E[X1|Xt = x], Note that this is also commonly\nreferred to as the denoiser, data prediction model, or x\u2081-prediction model (Lipman et al., 2024). This\nhas the advantage of being easily calculated from the x-prediction model or the model of the vector\nfield, requiring little computation to calculate this quantity. Then using this estimate of the posterior,\nwe can use a guidance function defined in the data space and take the gradient of the loss w.r.t. as the\nlocal step, i.e., use \u2207\u00e6L(21\\t(x)) as guidance. As such, we have our connection between a greedy\nstrategy and posterior sampling approaches."}, {"title": "5.1 GREED AS A UNIFYING THEORY OF GUIDANCE", "content": "By construction we have our connection between the greedy strategy and posterior sampling methods.\nNext, we will establish the connection between posterior sampling methods and the much larger class\nof end-to-end optimization methods. To make our analysis simpler, let us write the flow from s to t in\nterms of the denoiser. The vector field can be written as a function of the denoiser with,\n$$u(t, x) = \\frac{\\sigma_{t}}{\\sigma_{t}} \\dot{\\alpha_{t}} x + \\frac{\\alpha_{t}}{\\sigma_{t}} \\dot{\\sigma_{t}} X_{1|t}(x),$$  \n$$\n= a_{t} x + b_{t} X_{1|t}(x).\n$$\n(5.1)\nThe flow from time s to time t can then be expressed as the integral of the right-hand side of Equa-\ntion (5.1) over time. Thus, the flow is now expressed as a semi-linear integral equation with linear\nterm at\u00e6 and non-linear term bt21\\t(x). Due to this semi-linear structure, we can apply the technique\nof exponential integrators Hochbruck & Ostermann (2010) that has been successfully used to simplify\nnumerical solvers for diffusion models (Lu et al., 2022a; Zhang & Chen, 2023; Gonzalez et al., 2024).\nLet yt = y(t) := at/ot denote the signal-to-noise ratio (SNR), then yt is a monotonically increasing\nsequence in t, due to the properties of (at, \u03c3\u03c4) and thus has an inverse ty such that ty(y(t)) = t.\nWith abuse of notation, we let \u00e6xy := xty(\u03b3) and 21|7(\u00b7) = 21|ty(x)(\u00b7). As such, we can rewrite the\nsolution to the flow model in terms of y by making use of exponential integrators, which we show\nin Proposition 5.1 with the full proof provided in Appendix B."}, {"title": "Proposition 5.1", "content": "Given an initial value of xs at\ntime s \u2208 [0,1] the solution xt at time t \u2208 [0,1] of an ODE governed by the vector field in\nEquation (4.2) is:\n$$X_{t} =  \\frac{\\sigma_{t}}{\\sigma_{s}} X_{s} + \\frac{\\sigma_{t}}{\\sigma_{s}} \\int_{\\gamma_{s}}^{\\gamma_{t}} \\frac{\\alpha_{t}}{\\sigma_{t}} X_{1|\\gamma}(X_{\\gamma}) d \\gamma,$$\n(5.2)\nN.B., our result bears some similarity to the exact solution of diffusion ODEs that use a denoiser\nmodel; however, they integrate w.r.t. one-half of the logarithmic SNR (cf. Lu et al., 2022b)."}, {"title": "Discretize-then-optimize.", "content": "From Proposition 5.1 we can see that using the denoiser to estimate x\u2081 is\nakin to taking a first-order approximation of the flow. More specifically, we can construct a (k \u2212 1)-th\nTaylor expansion of Equation (5.2) with:\n$$X_{t} =  \\frac{\\sigma_{t}}{\\sigma_{s}} X_{s} + \\frac{\\sigma_{t}}{\\sigma_{s}} \\sum_{n=0}^{k-1} \\frac{d^{n}}{d \\gamma^{n}}  \\frac{\\alpha_{t}}{\\sigma_{t}} X_{1|\\gamma}(X_{\\gamma})  \\Big|_{\\gamma = \\gamma_{s}} \\frac{( \\gamma - \\gamma_{s})^{n}}{n!}  + O(h^{k+1}),$$\n(5.3)"}, {"title": "Optimize-then-discretize.", "content": "Next we consider the continuous ideal of the gradients of the solution\ntrajectory, i.e., {ax(t)}t\u2208[0,1]. We then explore how the gradients calculated via the greedy strategy\ncompare to this continuous ideal of the gradient flow. In Theorem 5.2 we show that a greedy strategy\ncan be viewed as the first iteration of a fixed-point method of an implicit Euler discretization of the\ncontinous adjoint equations."}, {"title": "Theorem 5.2", "content": "For some trajectory state xt at time\nthe greedy gradient given by \u2207x+L(21\\t(xt)) is an implicit Euler discretization of the\ncontinuous adjoint equations for the true gradients with step size h = \u00a51\nYt."}, {"title": "Theorem 5.3", "content": "For affine probability paths, if there exists a sequence\nof states xn at time t such that it converges to the locally optimal solution X1\\t(xn) \u2192 x\u2081. Then,\n||P1|t(xn) - x\u2081|| is O(h\u00b2) as n \u2192 \u221e."}, {"title": "5.2 GUIDANCE VIA A CONTROL SIGNAL", "content": "Some works on guidance (Liu et al., 2023c; Wang et al., 2024) have considered the problem from the\nperspective of optimal control. In essence, inject an additional control signal, z \u2208 C1 (R; Rd), to the\nvector field, ue, such that\n$$\\frac{dxt}{dt} = u_{\\theta}(t, x_{t}) + z(t).$$\n(5.5)\nThus, instead of optimizing {xt}t\u2208[0,t] directly, this control signal can instead be optimized, serving\nas one of the key insights in (Liu et al., 2023c; Wang et al., 2024). We can model the gradient to this\nsignal by augmenting the continuous adjoint equations with the adjoint state az(t) := dL/dz(t).\nIn Theorem 5.4 we show that this gradient is simply an integral of the adjoint state ax(t), the full\nproof can be found in Appendix E."}, {"title": "Theorem 5.4", "content": "Let u\u0473 \u2208 C1,1([0, 1] \u00d7\nRd; Rd) be a parameterization of some time-dependent vector field of a neural ODE that\nis Lipschitz continuous in its second argument, and let z \u2208 C\u00b9 ([0, 1]; Rd) be an additional\ncontrol signal such that the new dynamics are given by Equation (5.5). Let az(t) :=\nJL/dz(t) then\n$$\\alpha_{z}(t) = - \\int_{t}^{c_{t}} \\alpha_{x}(s) ds.$$\n(5.6)\nThis result can be viewed as a more generalized version of Wang et al. (2024, Theorem 2) (cf. Liu\net al., 2023c, Equation (8)).\nThe next natural question then is to ask about the behavior of a greedy strategy applied to z(t). To\nsimplify the analysis, we now consider a control signal applied to the posterior model 21|t such that\nit is replaced by x1\\t(xt) + z(t) which amounts to simply rescaling z(t) from Equation (5.5) with\nbt. From this construction it should be clear that the greedy gradient for the control signal is merely\nV1L(x1). If using the original formulation where the control signal is applied to the vector field,\nrather than the denoiser, the gradient is simply scaled by a weighting function dependent on time.\nNote this approach is similar to the greedy approach taken by Blasingame & Liu (2024b); however,\nthey inject the control signal onto the x-prediction model."}, {"title": "6 CONCLUSION", "content": "In this work we show that posterior sampling methods can be viewed as a greedy strategy of end-to-\nend optimization methods, creating a unified framework for examining training-free guided generation\ntechniques. More specifically, we show that posterior sampling methods are simply a single-step first-\norder discretization of either discretize-then-optimize or optimize-then-discretize methods. Hence we\ncan move between the two different strategies by simply altering the number of discretization steps.\nMoreover, we show that a greedy strategy can actually make good decisions, justifying the choice\nof such a strategy over the more computationally intensive end-to-end optimization techniques in\ncertain scenarios. Our hope is that this unified perspective on guided generation can inform future\nresearch directions when applying training-free guided generation to different applications.\nLimitations. In this work we only consider affine probability paths which includes many flow models\nand diffusion models in the ODE formulation; however, we did not explore a greedy strategy for SDE\nbased generative models. With future work, we could likely tighten the bound in the convergence\nproof by incorporating the curvature of the ODE. We did not perform a sensitivity analysis of how\nthe flow changes w.r.t. to the greedy gradients. As this work was theoretical and focused on drawing\nconnections between existing guided generation techniques, we did not include experimental work."}, {"title": "A REVIEW OF OUR NOMENCLATURE", "content": "To enhance the readability of our work, we provide a quick summary of our nomenclature and\npreferred conventions. We use a bold uppercase letter Xt ~ pt(x) to denote a random variable\ndistributed according to pt(x) and xt to denote the particular realization of said random variable. We\nuse the hat symbol to denote a prediction function, e.g., we let\n$$X_{1|t}(x) = \\mathbb{E}[X_{1}|X_{t} = x],$$\n(A.1)\n$$X_{0|t}(x) = \\mathbb{E}[X_{0}|X_{t} = x],$$\n(A.2)\ndenote the x\u2081-prediction (data) and x-prediction (noise) deterministic functions, respectively."}, {"title": "B PROOF OF PROPOSITION 5.1", "content": "We restate Proposition 5.1 here:\n$$\\alpha_{t} = \\frac{\\dot{\\alpha_{t}}}{\\dot{\\sigma_{t}}}.$$\n\nProposition B.1. Given an initial value of xs at time\ns \u2208 [0, 1], the solution xt at time t \u2208 [0, 1] of an ODE governed by the vector field in Equation (4.2)\nis:\n$$X_{t} =  \\frac{\\sigma_{t}}{\\sigma_{s}} X_{s} + \\frac{\\sigma_{t}}{\\sigma_{s}} \\int_{\\gamma_{s}}^{\\gamma_{t}} \\frac{\\alpha_{t}}{\\sigma_{t}} X_{1|\\gamma}(X_{\\gamma}) d \\gamma,$$\n(B.1)\nAdditionally, we assume that the following holds:\nAssumption B.2. The function $$au(t, x) = \\mathbb{E}[\\dot{\\alpha_{t}}X_{1} + \\dot{\\sigma_{t}}X_{0}|X_{t} = x].$$\nThis assumption is necessary for the simplification that we perform with exponential integrators.\nN.B., Ben-Hamu et al. (2024) make the same assumption in their analysis of the continuous adjoint\nequations for affine probability paths.\n\nProof. Recall that we uniquely define a flow model through the vector field u \u2208 C1,1 ([0, 1] \u00d7 Rd; Rd).\nThe vector field which models the affine conditional flow with schedule (at, \u03c3\u03c4), (see Section 4), is\ndefined as\n$$u(t, x) = \\mathbb{E}[\\dot{\\alpha_{t}}X_{1} + \\dot{\\sigma_{t}}X_{0}|X_{t} = x].$$\n(\u0392.2)\nWith some simple algebra we can rewrite the vector field in terms of 21|t,\n$$u(t, x) = a_{t} x + b_{t} X_{1|t}(x),$$\n(B.3)\n$$a_{t} = \\frac{\\dot{\\alpha_{t}}}{\\sigma_{t}},\\qquad b_{t} = \\frac{\\alpha_{t}}{\\sigma_{t}} \\dot{\\sigma_{t}} - \\frac{\\dot{\\alpha_{t}}}{\\sigma_{t}}.$$\nNow using this definition we can rewrite the solution for \u00e6t from \u00e6, in terms of 21|t,\n$$X_{t} = X_{s} + \\int_{s}^{t} u(\\tau, x_{\\tau}) d\\tau,$$\n(B.4)\n$$X_{t} = X_{s} + \\int_{s}^{t} a_{\\tau} x_{\\tau} + b_{\\tau} X_{1|\\tau}(x_{\\tau}) d\\tau.$$\n(B.5)\nNote the semi-linear form of the integral equation. We can exploit this structure using the technique\nof exponential integrators, (see Lu et al., 2022a; Zhang & Chen, 2023; Gonzalez et al., 2024), to\nsimplify Equation (B.5), under Assumption B.2, to\n$$e^{-\\int_{s}^{t} a_{\\mu} d\\mu} X_{t} = e^{-\\int_{s}^{t} a_{\\mu} d\\mu} X_{s} + \\int_{s}^{t}  e^{-\\int_{s}^{\\tau} a_{\\mu} d\\mu} b_{\\tau} X_{1|\\tau}(x_{\\tau}) d\\tau .$$\n(B.6)\nNow, the integrating factor simplifies quite nicely to\n$$\\int_{s}^{\\tau} a_{\\mu} d\\mu = \\int_{s}^{\\tau} \\frac{\\dot{\\alpha_{\\mu}}}{\\sigma_{\\mu}} d\\mu = \\log  \\frac{\\sigma_{s}}{\\sigma_{\\tau}},$$\n(B.7)\nsuch that Equation (B.6) becomes\n$$X_{t} =  \\frac{\\sigma_{t}}{\\sigma_{s}} X_{s} + \\frac{\\sigma_{t}}{\\sigma_{s}} \\int_{s}^{t}  \\frac{b_{\\tau}}{\\sigma_{\\tau}} X_{1|\\tau}(x_{\\tau}) d\\tau .$$\n(\u0392.8)"}, {"title": "C PROOF OF THEOREM 5.3", "content": "We restate Theorem 5.3 here:\nAssumption C.1. Affine probability paths, if there exists a sequence of\nstates xn at time t such that it converges to the locally optimal solution X1\\t(xn) \u2192 x\u2081. Then,\n||P1|t(xn) - x\u2081|| is O(h\u00b2) as n \u2192 \u221e.\nThroughout the proof the norm || || corresponds to the Euclidean norm ||\u00b7 ||2. Additionally, we make\nthe following (mild) regularity assumptions:\nAssumption C.2. The 21-prediction model 21|t : [0,1] \u00d7 Rd \u2192 Rd is Lipschitz in its second\nargument and continuous in its first.\nAssumption C.3. The total derivatives $$\\frac{d b_{t}}{\\sigma_{t}} = \\frac{\\dot{\\alpha_{t}} \\sigma_{t} - \\alpha_{t} \\dot{\\sigma_{t}}}{\\sigma_{t}^{2}} =  \\frac{d}{dt} (\\frac{\\alpha_{t}}{\\sigma_{t}}),$$\n\n\n\n $$\n= \\frac{d}{dt}(\\frac{\\alpha_{t}}{\\sigma_{t}}).$$\n\n exist and are continuous for 0 \u2264 n \u2264 k + 1.\n\nProof. Let x1 = \u03a61\\t(xt). Then by Assumption C.3, we can take a (k-1)-th order Taylor expansion\naround Yt of the flow:\n\n$$X_{1} = \\frac{\\sigma_{t}}{\\sigma_{s}} X_{t} + \\frac{\\sigma_{t}}{\\sigma_{s}} \\int_{\\gamma_{t}}^{1}  \\frac{\\alpha_{t}}{\\sigma_{t}} X_{1|\\gamma}(X_{\\gamma}) d\\gamma = \\frac{\\sigma_{t}}{\\sigma_{s}} X_{t} + \\frac{\\sigma_{t}}{\\sigma_{s}} \\sum_{n=0}^{k-1} \\frac{d^{n}}{d \\gamma^{n}}  \\frac{\\alpha_{t}}{\\sigma_{t}} X_{1|\\gamma}(X_{\\gamma})  \\Big|_{\\gamma = \\gamma_{t}} \\frac{(\\gamma - \\gamma_{t})^{n}}{n!} + O(h^{k+1}),$$\n\n$$\\Big|_{\\gamma = \\gamma_{t}}  \\frac{(\\gamma - \\gamma_{t})^{n}}{n!} + O(h^{k+1}),$$\n$$= -\\frac{\\sigma_{t}}{\\sigma_{s}} \\sum_{n=0}^{k-1} \\frac{d^{n}}{d \\gamma^{n}}  \\frac{\\alpha_{t}}{\\sigma_{t}} X_{1|\\gamma}(X_{\\gamma})  \\Big|_{\\gamma = \\gamma_{t}} + O(h^{k+1}),$$\n\n\n$$ \\frac{h^{n+1}}{(n + 1)!} + O(h^{k+1}),$$\n\n(C.1)\nwhere h := 1 - Yt is the stepsize. Let k = 1, then we have:\n$$X_{1} = \\sigma_{1} x_{t} + \\sigma_{1} X_{1|t}(x_{n}) h + O(h^{2}),$$\n(C.2)\n$$= x_{n} + (\\alpha_{1} - \\sigma_{1}) X_{1|t}(x_{n}) + O(h^{2}).$$\n(C.3)\nBy definition \u03c3\u2081 = 0 and 0\u2081 = 1, then\n$$x =  \\frac{\\sigma_{t}}{\\sigma_{s}} X_{s} + \\frac{\\sigma_{t}}{\\sigma_{s}} \\int_{\\gamma_{t}}^{1}  \\frac{\\alpha_{t}}{\\sigma_{t}} X_{1|\\gamma}(X_{\\gamma}) d\\gamma$$,\n$$X_{1} =  X_{1|t}(x_{n}) + O(h^{2}),$$\n(C.4)\nwhich is equivalent to\n$$||X_{1} -  X_{1|t}(x_{n}) || =  \\frac{\\sigma_{t}}{\\sigma_{s}} X_{s} + \\frac{\\sigma_{t}}{\\sigma_{s}} \\int_{\\gamma_{t}}^{1}  \\frac{\\alpha_{t}}{\\sigma_{t}} X_{1|\\gamma}(X_{\\gamma}) d\\gamma$$,\n$$||X_{1} -  X_{1|t}(x_{n}) ||  \\le C_{1}h^{2},$$\n(C.5)\nfor some constant C\u2081 > 0. Since X1\\t(xn) \u2192 x\u2081 we know that for any \u20ac > 0 there exists some\nn > N such that ||x - X1|t(xn) || < \u0454. Thus,\n\n(C.6)\nThus the error between the optimal solution \u00e6 and the solution from the flow x1 is O(h\u00b2) thereby\nfinishing the proof."}, {"title": "D PROOF"}]}