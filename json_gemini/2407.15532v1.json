{"title": "Large-scale Time-Varying Portfolio Optimisation using\nGraph Attention Networks *", "authors": ["Kamesh Korangi", "Christophe Mues", "Cristi\u00e1n Bravo"], "abstract": "Apart from assessing individual asset performance, investors in financial markets also\nneed to consider how a set of firms performs collectively as a portfolio. Whereas traditional\nMarkowitz-based mean-variance portfolios are widespread, network-based optimisation tech-\nniques have built upon these developments. However, most studies do not contain firms at\nrisk of default and remove any firms that drop off indices over a certain time. This is the first\nstudy to incorporate risky firms and use all the firms in portfolio optimisation. We propose\nand empirically test a novel method that leverages Graph Attention networks (GATs), a sub-\nclass of Graph Neural Networks (GNNs). GNNs, as deep learning-based models, can exploit\nnetwork data to uncover nonlinear relationships. Their ability to handle high-dimensional\nfeatures and accommodate customised layers for specific purposes makes them particularly\nappealing for large-scale problems such as mid- and small-cap portfolio optimization. This\nstudy utilises 30 years of data on mid-cap firms, creating graphs of firms using distance corre-\nlation and the Triangulated Maximally Filtered Graph approach. These graphs are the inputs\nto a GAT model that we train using custom layers which impose weight and allocation con-\nstraints and a loss function derived from the Sharpe ratio, thus directly maximising portfolio\nrisk-adjusted returns. This new model is benchmarked against a network characteristic-based\nportfolio, a mean variance-based portfolio, and an equal-weighted portfolio. The results show\nthat the portfolio produced by the GAT-based model outperforms all benchmarks and is con-\nsistently superior to other strategies over a long period while also being informative of market\ndynamics.", "sections": [{"title": "Introduction", "content": "Portfolio optimisation is crucial in modern risk management, as performance correlations between\nfirms in a portfolio bring unforeseen risks. Given that every investor's risk profile differs, the\nportfolio construction model must also account for different objectives. One such model, based\non the classic work by Markowitz (1952, 1959), is mean-variance optimisation, where returns are\noptimised while volatility is minimised. In this paper, too, we look to optimise with a mean-\nvariance objective, but we do so over a large set of firms that could also default or go bankrupt.\nThis problem must be solved for an investable universe that is expanding, as financial markets\ncontinue to develop, and emerging and private markets are becoming increasingly accessible. All\nthese assets have different risk and liquidity profiles. Against this backdrop, which firms to select\nand what proportion of capital to allocate to each is an increasingly high-dimensional problem.\nThe classical mean-variance measure is not without its problems, and portfolios optimised\nusing it have been shown to have poor out-of-sample performance (Siegel and Woodgate, 2007).\nAssumptions about the normality of returns, absence of transaction costs and presence of regimes\nin markets make the classical model difficult to implement (Guidolin and Ria, 2011). Even if\nthese assumptions are fulfilled, the expected mean of the portfolio and the covariance matrix\ncannot be easily estimated as they are not observed in practice, which means that, instead, the\nsample mean and covariance matrix are commonly used (Ao et al., 2019). Furthermore, it is\nchallenging for such models to cope with high dimensionality, which is a common characteristic of\nmodern portfolios (DeMiguel et al., 2009b). To better address these challenges, different methods"}, {"title": "Related literature", "content": "In this section, we present the relevant literature for the study, focusing on large-scale portfolio\noptimisation studies and GNNs."}, {"title": "Correlation Networks and Portfolio models", "content": "Portfolio optimisation has been the subject of a large body of research, and novel methods, with\nvarious constraints and objectives, continue to be developed (DeMiguel et al., 2009a; Branch et al.,\n2019). Of particular interest to our work are studies that focus on optimising large-scale portfo-\nlios effectively. Perold (1984) was the first to do so, by considering the specific nature of dense\ncovariance matrices, and recommending strategies to make them sparse so they become compu-\ntationally feasible. The first branch of large-scale portfolio optimisation studies folloow a similar\napproach in devising algorithms to efficiently use computational resources, such as computing time\nand memory.\nLater studies on large-scale portfolio optimisation focus on generating weights for the firms and\nmeasuring the model portfolio performance against metrics such as the Sharpe ratio. Bonami and\nLejeune (906) proposed an exact solution approach for a portfolio of up to 200 firms by fixing\nthe expected returns with a probabilistic confidence level and imposed several trading constraints\nto extend the mean-variance framework. On a portfolio of 100 firms within the S&P 500, Ao\net al. (2019) converted mean-variance portfolio optimisation into a sparse linear regression, and\ncombined them with Fama-French factors to study portfolio performance. Liu (2017) used a com-\nbination of efficient computing resources with evolutionary algorithms for portfolio optimisation.\nBian et al. (2020); Dong et al. (2020) applied regularisation methods for portfolio optimisation\nas, without such methods, a large portfolio would lead to overly small allocations; they also esti-\nmated these covariance matrices using factor models depending on size, book-to-market values and\nindustry sector, and found regularization helped to arrive at better optima. Performance-based\nregularization where the sample variance is restricted was also found to perform better on portfo-\nlios from Fama-French data sets (Ban et al., 2018). Recently, Bertsimas and Cory-Wright (2022)\nstudied the size of portfolios of previous large-scale portfolio optimisation studies and proposed\na ridge regression-based regularisation algorithm which could theoretically scale to a portfolio of\nover 3000 firms by converging to a solution. However, we argue that the portfolio scale considered\nthus far is still not large enough when using real data or otherwise studies use simulated data to\nmeasure the efficacy of portfolio optimisation algorithms. Additionally, in all these studies, the\nuniverse of firms or assets to select from is always constant, thus eliminating firms that default,\nget acquired or liquidate, eliminating a large driver of idiosyncratic portfolio risk. Instead, we take\na dynamic approach and allow the portfolio to vary over time, allowing us to use real data and use"}, {"title": "Graph Neural Networks", "content": "GNNs were first proposed by Scarselli et al. (2009) for node classification tasks. Similar to recurrent\nneural networks (RNNs), they employ recursion, which is used to learn higher-order representa-\ntions for a node from its neighbours. As the deep learning field for Euclidean data evolved with\nRNNs for sequential data, Convolutional Neural Networks (CNNs) for primarily image processing,\nand Attention-based models Vaswani et al. (2017) for spatial analysis of unstructured data, non-\neuclidean data models based on GNNs also developed in parallel. Graph Convolutional Networks\n(GCNS) borrow concepts from CNNs, such as kernel filter size and stride, to generate representa-\ntions for graphs (Kipf and Welling, 2017). They produced state-of-the-art results on popular graph\ndatasets such as citation networks and knowledge graphs over semi-supervised or skip-gram-based\ngraph embeddings, label propagation and regularization approaches. Graph Attention Networks\n(GATs) further improved on these results by introducing variable aggregation of neighbours, and\nthey also proved successful in transductive learning tasks where the data is not fully labelled\n(Veli\u010dkovi\u0107 et al., 2018). We refer the reader to a comprehensive survey by Wu et al. (2021) for\na general introduction to GNNs and their various flavours. Thus far, GNNs have been applied in\nsome application areas related to finance or financial markets. For example, in consumer finance,\nthey were used for fraud detection (Xu et al., 2021) and credit risk prediction (\u00d3skarsd\u00f3ttir and\nBravo, 2021) in consumer finance. Avramov et al. (2023) applied other deep learning techniques for\nlisted NYSE firms and found it can produce complex signals that are profitable but become costly\nwith high turnover. Feng et al. (2022) used a variant of GNNs and applied self-attention separately\nfor the stock recommendation out of 738 stocks which is to predict top-3 stocks for the next period.\nWe could have given the fully connected graph as an input to the GAT model. However with such\nhigh-dimensionality with an average 6000 companies to look at every quarter, the training would\nhave been lot complicated but by taking advantage of econometric-based preprocessing of data\nto extract initial relationships we initialize the model with better input. Overall,we hypothesise\nthe deep learning methods are better deployed at large scale with illiquid, risky firms than on a\nsmall set of established firms, as they are better at extracting complex relationships and with a\nlower churn the portfolio which reduces transaction costs, but need large data to arrive at a stable"}, {"title": "Methods", "content": "In this section, we describe the data used for this study, the distance correlation measure and the\ngraph filtering algorithm, which all together create the inputs for the GNN and other benchmark\nmodels."}, {"title": "Data", "content": "We collected the daily closing prices of all mid-cap companies listed in the US over 30 years,\nfrom 1990 to 2021. There are approximately 20,000 firms active for at least part of this period.\nFor portfolio selection, we use a three-year rolling window, so the portfolio allocation problem is\nlimited to around 5,000 firms at any given time point. This number changes substantially over\ntime, however, due to firms entering and leaving the market. We convert the prices to daily returns\nfor each firm, defined as $r_{ut}$ for a firm u at time t. These return series are sequentially divided into\na training (50%), validation(25%), and test set(25%), and serve as the main input to our models.\nTo create the relationships between firms, we also calculate the return-volatility series using\nthe standard deviation of these returns and a 30-day look back period. For a firm u at time t it is\ndefined as $Cut = \\sigma(r_{ut}, r_{ut-1}, .., r_{ut-30})$.\nThe return-volatility series is input to calculate the covariance matrix between firms using\ndistance correlation. We used a three-year look back period between a pair of firms to derive\nthe covariance matrix. This look back period allows for a relatively new firm to also be part of\nthe portfolio, as mid-cap firms generally have some previous trading history as small-cap firms.\nThe return-volatility series also has interesting features compared to return series, such as clus-\ntering where volatilities are asymmetric with short periods of co-movements particularly during\nrisk-off situations in the market, while during benign times markets show more independence in\nrelationships. This series also has strong serial correlation, and also it tracks market investors\nsentiment (e.g.: the volatility index, VIX), which are essential for identifying crises (Huang et al.,\n2019). These are the situations where the performance of different portfolio allocation strategies\ncan diverge substantially.\nWe define the universe of firms as V. We sample them over N periods where $V_t \\subseteq V$ denotes"}, {"title": "Distance correlation", "content": "We quantify the strength of relationships between two firms using the distance correlation mea-\nsure. In this section, we describe this measure. Distance correlation is a generalised measure of\ndependence, which captures non-linear dependencies and performs well in domains such as signal\nprocessing (Brankovic et al., 9 12) and computational biology (Mendes and Beims, 2018). The\ndistance correlation using the estimation procedure is defined below, $dcor(u, v)$ between two firms\nusing the volatility series $(l_u, l_v)$ defined earlier in (2). For each firm, we get the absolute change\nin volatility over all periods and scale these changes by removing the averages to get firm-level\nscaled change matrices, which are compared with other firm-level matrices to derive the distance\ncorrelation measure. First, we define two matrices A and B for a pair of firms u and v.\n$A_{i,j} = ||l_{ui} \u2013 l_{uj}||$\n$b_{i,j} = ||l_{vi} - l_{vj}||$\nwhere i, j = 1, 2, ..., T, and $||\\cdot||$ is the Euclidean distance. This captures times when the firm has"}, {"title": "Graph Filtering", "content": "The filtering technique we chose is the Triangulated Maximum Filtered Graph (TMFG) method\nproposed by Massara et al. (2017) which, like PMFG, imposes a planarity constraint on the graph\nbut is more scalable to larger datasets such as ours. Using the topological features of the graph as\na constraint, a planar graph retains most of the information with fewer edges. A graph is planar\nif no edges cross between the nodes on a sphere. Such graphs have attractive features, making,\nfor example, cluster or community detection easier, which reduces the information need and thus\nmaking large graphs tractable for analysis.\nWe define the dense graph before filtering as $K = (V_t, F_t)$, where $F_t$ is the adjacency matrix.\nWe let an element $f_{uv} \\in F_t$ take the value\n$f_{uv} =\\begin{cases}\n1, & \\text{if } d_{uv} > 0\\\\\n0, & \\text{otherwise}\n\\end{cases}$\nTMFG along with the adjacency matrix $F_t$ uses the distance correlation values from Equation\n(4) to filter the graph $K$ into a sparse planar subgraph $G = (V_t, E_t)$ that is maximal, i.e. the sum\nof edge weights is maximum. Planarity constraints reduce the edges from $|V_t|(|V_t| \u2212 1)/2$ in K to\n$(3(|V_t| - 2))$ in graph G.\nThe TMFG algorithm first identifies a clique of four firms with the highest distance correlation\nmeasure. A clique is when all the distinct vertices in the sub-graph have an edge between them,\ni.e, all members of the clique are connected. In our volatility networks, this happens in most"}, {"title": "Network measures", "content": "The graphs created after TMFG filtering could be used directly for portfolio optimisation, as some\nstudies (Li et al., 2019; Pozzi et al., 2013) have done. We calculate a score for each node in the\ngraph $G = (V_t, E_t)$. This score is transformed as the weight assigned to the firms in the portfolio\nthrough the method that follows.\nWe define the score for a firm $u \\in V_t$ as $p_u$, an average of the three measures of the centrality\nof the networks, the degree centrality, betweenness centrality, and closeness centrality. These\nmeasures have been studied since the 1970s (Freeman, 1977) and are still popular in the network\nliterature (\u00d3skarsd\u00f3ttir and Bravo, 2021). The first measure, degree centrality, is the fraction of\nnodes to which the node u is directly connected to. We count all the nodes adjacent to node \u0438\nand normalise it by the total number of nodes $|V_t|$ \u2013 1. These set of nodes are also the neighbours,\n$N_u$, for node u. Formally, we define neighbours as\n$N_u = \\{v|(u, v) \\in E_t\\}$\nThat is, nodes v that have an edge connecting them to u. This neighbourhood definition is\nalso useful for the description of GNN models, as we will see in Section 4.1 where we define the\nGAT model.\nThe degree centrality $dc_i$ for firm i is defined as\n$dc_i = |N_u|/(|V_t| \u2212 1)$\nor the total number of neighbours divided by the total number of nodes minus one.\nThe betweenness centrality measure specifies the activity that passes through the graph node\nwhen any changes occur in the network. To measure this activity level, we first compute the\nshortest paths between all pairs of nodes. Betweenness centrality is the fraction of the shortest\npaths that pass through a node that does not include the given node. For a firm i, this is defined\nas\n$bc_i = \\sum_{u,v\\in V_t} \\frac{s(u,v/i)}{s(u, v)}$\nwhere s(u, v) is the number of shortest paths between (u, v) and s(u, v|i) is the number of shortest\npaths that pass through i. s(u, v|i) = 0 if i = u or i = v and s(u, v) = 1 if u = v.\nThe final centrality measure we use is closeness centrality $cc_i$. It is the reciprocal of the average\nshortest paths to all the nodes that are reachable from a given node. Higher scores meaning higher\ncentrality.\n$cc_i = \\frac{(n-1)}{(V_t - 1) \\sum_{u=1}^{n-1} d(u, i)}$"}, {"title": "Model performance and loss metric", "content": "We adopt the Sharpe ratio as the final performance measure for the models (Sharpe, 1966). This is\na well-studied metric to measure portfolio performance. There are several studies in the portfolio\nperformance literature on further refinements or limitations of the Sharpe ratio (Lo, 2002; Farinelli\net al., 2008).\nHere, we use the widely accepted form of the metric. From equation (1) for an individual firm\nu, the Sharpe ratio is estimated from the sample mean and variance of the returns. This same\nmethod applies to portfolio returns as well.\n$\\mu_{u} = \\frac{1}{T} \\sum_{i=1}^{T}r_{ui}$\n$\\sigma^{2} = \\frac{1}{T} \\sum_{i=1}^{T}(r_{ui} \u2013 \\hat{\\mu})^{2}$\nFrom these quantities, the Sharpe ratio $SR_u$ can be easily estimated\n$SR_{u} = \\frac{\\mu_{u} - R_{f}}{\\sigma_{u}}$\nwhere $R_f$ is the risk-free interest rate. For ease of calculation,we normalize it to zero as it is a\nconstant baseline for all. So, the Sharpe ratio produced cannot be compared to external studies\non the Sharpe ratio, but the results are comparable between the models in this study. We use this"}, {"title": "Models", "content": "GNNs are applied directly to graph data and successfully exploit the structure within the data\nfor various tasks, such as node classification and edge prediction. In domains with natural graph\ndata, such as protein structures in chemistry or biology, citation networks, or recommendation\nsystems, they produce state-of-the-art results even when compared to other large-scale deep learn-\ning models (Ying et al., 2018; Gilmer et al., 2017). Graph convolutional networks were developed\nalong with convolutional neural networks used for image processing. Unlike images with strict\nstructure, graph data are dynamic and have much more complexity and meaning between connec-\ntions. Graph convolutional networks generate a higher-order representation of input features and\nneighbours by using the neighbourhood degree centrality to weigh the neighbour's features. As\ndegree centrality is a fixed measure, more complex weighing mechanisms are impossible. Graph\nAttention networks solve this problem by using the self-attention mechanism, creating learnable\nparameters to generate the weights for neighbours, making them more dynamic in learning from\nthe neighbourhood features (Veli\u010dkovi\u0107 et al., 2018). Large graphs such as the mid-cap graph we\ncreate in this paper are dynamic and show significant evolution as time passes. Also, the market\nconditions vary for each time period, meaning fixed weighing of neighbours may not perform well"}, {"title": "Graph Attention Networks", "content": "with these changes. Added to this, the Attention operations are more efficient than alternative\napproaches, since they can be parallelizable across node neighbour pairs.\nHere, we formally define the GAT specific to our problem. Given a graph $G = (V_t, E_t)$ as defined\nin section 3.3, and the input features defined in (3), GAT transforms the input features $X_t$ into a\nhigher-order representation $H_t$ given by\n$H_t = [h_1, h_2, .., h_u, h_v, .., h_{|V_t|}]^T$\nwhere $h_u \\in \\mathbb{R}^{T'}$\nSpecifically, for a given firm u, the transformation function from $x_u$ to $h_u$ is where each variant\nof GNN differs.\n$h_u =||_{k=1}^{K} F(x_u, \\sum_{v \\in N_u}a(u, v)(Wx_v))$\n$N_u$ defined in equation (6) are the neighbours of firm u. $W\\in \\mathbb{R}^{(T'\\times T)}$ is a weight parameter\nmatrix of the model that is learnt during training. a(u, v) is the weighted importance score of firm\nv on firm u. A softmax layer does the weighing after a feed-forward network over each neighbour\nfirm v with the present firm u. The function a(u, v) is the one that causes the differences, where\nit is a convolutional function for GCNs and attention for GATS.\n$a(u,v) = softmax (\\sigma(a [Wx_u||Wx_v]))$\nwhere $\\sigma$ is a non-linear function, LeakyRelu in this case. $a \\in \\mathbb{R}^{2T'}$ and $|\\mid$ is the concatenation\noperator.\n$F(\\cdot)$ also applies the non-linearity function (usually a Regularized Linear Unit, ReLU) after\naggregating all weighted outputs. GAT also allows for multi-head attention, where each head\nlearns a different aspect of the input. For a K-multi-head attention, we concatenate each head's\noutputs to construct the final representation. We used $T' = 24$ and K 8 in our experiment.\nThe $H_t$ function is a higher-order representation. We scaled these representations to lower\ndimensions to make them suitable for predicting weights for the portfolio. We apply batch normal-\nisation, which normalises the feature inputs for the next layer, and use two feedforward networks\nwith dropout to scale the representation as we want to generate a single number for each firm\nfinally. The dropout process helps to improve the stability of the training while reducing com-\nplexity. The final feedforward network also uses L1, or LASSO, regularisation to further shrink"}, {"title": "Network index model", "content": "As studies have shown, portfolios invested in peripheral assets outperform portfolios containing\nmore central firms (Pozzi et al., 2013; Giudici et al., 2020). This is possible due to the diversification\nbenefits of peripheral firms, as they are less correlated with market moves. Our model uses the\nperipherality measure defined in Section 3.4 and allocates capital to the portfolio as the inverse of\nthe network index score. We rescale these weights so that they sum up to one. For each node or\nasset i in the network,\n$w_i = 1/p_i$\n$W_i = \\frac{w_i}{\\sum w_j}$"}, {"title": "Benchmark Portfolios", "content": "Markowitz (1952)'s mean-variance model is widely known as part of Modern Portfolio Theory.\nHere, we include the model as one of the benchmark models. For |Vt| firms active at time t, we\nfind the weights wi for each asset in the portfolio such that $\\sum^{|V_{t}|}_{i} w_i = 1$ . Starting with random\nweights, we aim to optimise the weights by maximising the expected return and minimising the\nportfolio's volatility.\nThe expected return and the variance of the portfolio are derived from the sample using the\nsample mean and covariance. The covariance matrix is for a pair of firms, as the system-wide\ncovariance matrix from methods such as VAR decomposition is infeasible for firm numbers with\nilliquid trading patterns (Diebold and Yilmaz, 2012). We use a similar pairs-based method for\ngraph input as well, using distance correlation, which makes this covariance calculation strategy\ncomparable.\n$E(R_i) = \\sum_{i=1}^{V_{t}} w_i \\mu_i$\n$V(R_i) = \\sum_{i} \\sum_{j} w_i w_j cov(i, j)$\nwhere $\\mu_{i}$ is the return of $i^{th}$ asset and cov(i, j) the covariance of the return series between firms i\nand j.\nWe solve the mean-variance problem for $w_i$ using an optimisation library that uses quadratic\nprogramming (Martin, 2021). Intuitively, the model still picks up lowly correlated firms to achieve\ndiversification benefits. However, a high-dimensional portfolio poses challenges to finding an effi-\ncient portfolio, and our model running times increase considerably."}, {"title": "The Equal weight portfolio model", "content": "The market portfolio is an important benchmark for measuring models. More importantly, it is well\nknown in the literature that the performance of such portfolios is challenging to beat (DeMiguel\net al., 2009b). The strategy consists of simply assigning equal weights to all the firms in the\nportfolio. For each firm u in a set of firms $V_t$ the weight $w_u$ corresponds to\n$w_u = |V_t|^{-1}$\nThis allocation may not be feasible when developing a portfolio strategy for many firms, as\nthe transaction costs increase considerably. A modern proxy is to invest a similar amount in the\nmarket index of the firms via an ETF or other similar instrument if they exist. We assume no\ntransaction costs, as is common in these instruments\u00b9, and this equal-weight portfolio serves as\nthe market benchmark in our study. The equal-weighted strategy, though not practical, is difficult\nto outperform, especially as the portfolio size increases because the risk of model misspecification\nerror increases for models that use complicated strategies."}, {"title": "Experimental settings", "content": "The setup of the model from the raw input of the prices of mid-cap firms to the final result is shown\nin Figure 3. All models receive the same data for training in different formats. The Figure 3 shows\nthe three stages in the experiment. The first stage is the prepartion of Inputs. The raw data is\nconverted to returns and volatilities. For each period the volatility data is used to create the dense\ncovriance matrix using distance correlation and which is filtered by the TMFG algorithm to create\na sparse graph. The second stage in the experiment is when the models receive the appropriate\ninputs. The Mean-Variance model and Equal-weighted model get the expected returns data. The\nGAT model, in addition to the return series, also has a filtered graph as an additional input. The\nnetwork model just receives the graph input, which calculates the overall score for each node. All\nmodels generate investment weights at the final Output stage, after which they are all measured\nin a similar method with test, unseen, data, which is two quarters ahead of training data and one\nquarter ahead of the validation data, i.e., an out-of-time sample. The Sharpe ratio is calculated"}, {"title": "Loss function", "content": "Unlike training in classification models, the use of the familiar cross-entropy loss function is im-\npossible, as our model generates weights, not predictions. Our work proposes the negative of the\nSharpe ratio expressed as the Lagrangian as the loss function. This allows us to leverage the\nflexibility of neural networks, so we train the model directly on the desired objective.\nThe inputs to the deep learning models are either the graph network or a time series of historical\nreturns. For a graph constructed with historical data of window size T periods containing $N_t$ firms\nover T periods, the outputs of the models are the weights of the portfolio $w_{it}$ of shape $N_t \u00d7 1$ where\ni = i, 2, ..., Nt. To train the model, we use the true returns of the next h periods, $y_{h}$ of shape\n$N \u00d7 h$.\nThe loss function is calculated as follows. For each batch, we calculate the predicted portfolio\nreturns $Y_{port}$.\n$Y_{port} = w_{it}^{T} * Y_{h}$\nWe calculate the expected returns $E(y_{port})$ and volatility $V(Y_{port})$ and annualise the ratio. To\nnote, we assumed risk-free interest as zero as this assumption is applied for all the models, thus\nthe relative performance we are interested in is unaffected.\n$SR_{port} = E(y_{port})/V(Y_{port})$\nThis procedure ensures that the output $w_i$ of the final layer of the network learns from changes\nin asset i. We averaged the Sharpe ratio of all batches to arrive at a final performance measure."}, {"title": "Training settings", "content": "We first split the returns data into training and test data sets, using 60% for training and the\nremaining as test dataset. The training data set is divided into training and validation data, with\n25% of the remaining data for validation. The models are assessed based on the validation dataset\nto tune the hyperparameters in deep learning models. We also adopt early stopping to prevent\noverfitting of the data, and a patience of 15 epochs is applied to this early stopping to avoid local\nminima."}, {"title": "Evaluation metrics", "content": "The Sharpe ratio is a good measure of portfolio performance. However, this large-scale portfolio\nfurther challenges understanding why the performance is achieved. We use the two centrality\nmeasures discussed earlier, specifically betweenness and degree centrality, to calculate a portfolio-\nlevel score. We take a weighted average of the centrality score of each node and their weight in\nthe portfolio. This shows how peripheral the nodes are selected in each model.\nWe also look at industry-level composition across different portfolios selected by the models.\nAs there are approximately 6,000 firms, the industry-level grouping will give us a better visual of\nany differences. We calculate a weighted industry score by summing up the portfolio weights of\neach company in the industry.\nWe also look at some turnover statistics due to the dynamic nature of the universe of companies\nwe selected, as opposed to selected index constituents, where changes are not common. These mid-"}, {"title": "Results and discussion", "content": "In this section, we first report the performance of each model based on our Sharpe ratio metric\nfor training, validation, and test splits. We compare and discuss the model performance based on\nthe Sharpe ratio of the test dataset. We also present how each model has performed over time in\nthe last 30 years with a rolling window of a one-quarter period. This measure is important as we\nrecalculate the graph every quarter, and the training, validation, and test data all roll ahead.\nWe then report an analysis of the possible reasons why the models performed the way we\nobserved them to. We report the overall peripherality score of each model and compare the\ndifferences between the models. We also look at changes in industry concentration, as different\nstrategies might overweight certain industries. Finally, we look at the turnover of each portfolio\noptimisation model. More turnover typically implies more trades, which increases the costs to run\na portfolio. We define turnover as the number of new positions in the portfolio either created or\nclosed compared to the natural churn in the portfolio over every period."}, {"title": "Model performance", "content": "Table 1 shows the portfolio performance on all data splits. The GAT model has the highest ratio\nof 1.082 over the equal portfolio, where higher results mean higher risk-adjusted performance.\nDeep learning models can learn intricate relationships based on features and the adjacency matrix\ninformation, which can beat an equal portfolio. As earlier studies confirm, it is hard for well-\ndesigned portfolio models to outperform equal portfolios, especially when the portfolio is large"}, {"title": "Strategy differences", "content": "In this section, we look at why such differences exist between the model's performance by looking\nat portfolio-level network scores and industry selection. We chart two scores to understand where\neach model is focusing on. Figure 5 shows each model's weighted average centrality score defined\nearlier. The weights are the portfolio weights of the model. The betweenness centrality (BC) is\nthe first set of bars for each model depicted in yellow. The black line is the standard deviation\nof these scores for each model over time. As we see for the betweenness centrality, the GAT\nmodel and Mean-variance model select similar scored nodes. However, there is a lack of stability\nwith high variability in the Mean-variance model, while GAT models are relatively stable. The\nnetwork models choose the most peripheral nodes as measured by the betweenness centrality score,\nbut they underperform against the GAT models, as they lack diversity compared to them. The\nmid-cap firms which are too peripheral and carry 10% default risk will impact the portfolio and\neliminate the performance gains we have from peripheral nodes. This is a different conclusion\nfrom previous studies, especially those based on topological information for portfolio optimisation\n(Li et al., 2019). However, those studies do not contain firms at-risk of default and are typically\nindexes or constituent firms of well-tracked indices. They also remove any firms that do not have\nthe full data available. This is the first study to incorporate such features, as deep learning models\ncan handle these inputs effectively. From degree centrality scores, we see that GAT models show\nthe smallest variance while selecting relatively central nodes as they are marginally higher than\nthe equal-weighted portfolio."}, {"title": "Conclusion", "content": "In this paper, we have shown a solution for large-scale portfolio optimisation using deep graph\nmodels. We have seen how GAT-based models, a model within the family of deep learning models,\ncan extract intricate relationships that other traditional models cannot find. These models are"}, {"title": "Conclusion", "content": "performance choosing companies that are not too much in the periphery and allocating capital\nto fewer firms. Finally, we observe that the turnover of the GAT models is lower than alterna-\ntives, while maintaining a slightly higher variance. These characteristics result in overall better\nperformance.\nAs for future work, graph neural network models could be further developed to predict aspects\nlike market regimes, or early warning indicators for financial networks. By changing the objectives\nand fine-tuning the loss functions, we could extend the graph neural network-based portfolio op-\ntimisation models for different objectives, such as the construction of Environmental, Social and\nGovernance (ESG) portfolios or other diversification-orientated portfolios on a much larger scale."}]}