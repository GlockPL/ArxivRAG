{"title": "Large-scale Time-Varying Portfolio Optimisation using Graph Attention Networks", "authors": ["Kamesh Korangi", "Christophe Mues", "Cristi\u00e1n Bravo"], "abstract": "Apart from assessing individual asset performance, investors in financial markets also need to consider how a set of firms performs collectively as a portfolio. Whereas traditional Markowitz-based mean-variance portfolios are widespread, network-based optimisation techniques have built upon these developments. However, most studies do not contain firms at risk of default and remove any firms that drop off indices over a certain time. This is the first study to incorporate risky firms and use all the firms in portfolio optimisation. We propose and empirically test a novel method that leverages Graph Attention networks (GATs), a subclass of Graph Neural Networks (GNNs). GNNs, as deep learning-based models, can exploit network data to uncover nonlinear relationships. Their ability to handle high-dimensional features and accommodate customised layers for specific purposes makes them particularly appealing for large-scale problems such as mid- and small-cap portfolio optimization. This study utilises 30 years of data on mid-cap firms, creating graphs of firms using distance correlation and the Triangulated Maximally Filtered Graph approach. These graphs are the inputs to a GAT model that we train using custom layers which impose weight and allocation constraints and a loss function derived from the Sharpe ratio, thus directly maximising portfolio risk-adjusted returns. This new model is benchmarked against a network characteristic-based portfolio, a mean variance-based portfolio, and an equal-weighted portfolio. The results show that the portfolio produced by the GAT-based model outperforms all benchmarks and is consistently superior to other strategies over a long period while also being informative of market dynamics.", "sections": [{"title": "1 Introduction", "content": "Portfolio optimisation is crucial in modern risk management, as performance correlations between firms in a portfolio bring unforeseen risks. Given that every investor's risk profile differs, the portfolio construction model must also account for different objectives. One such model, based on the classic work by Markowitz (1952, 1959), is mean-variance optimisation, where returns are optimised while volatility is minimised. In this paper, too, we look to optimise with a mean-variance objective, but we do so over a large set of firms that could also default or go bankrupt. This problem must be solved for an investable universe that is expanding, as financial markets continue to develop, and emerging and private markets are becoming increasingly accessible. All these assets have different risk and liquidity profiles. Against this backdrop, which firms to select and what proportion of capital to allocate to each is an increasingly high-dimensional problem.\nThe classical mean-variance measure is not without its problems, and portfolios optimised using it have been shown to have poor out-of-sample performance (Siegel and Woodgate, 2007). Assumptions about the normality of returns, absence of transaction costs and presence of regimes in markets make the classical model difficult to implement (Guidolin and Ria, 2011). Even if these assumptions are fulfilled, the expected mean of the portfolio and the covariance matrix cannot be easily estimated as they are not observed in practice, which means that, instead, the sample mean and covariance matrix are commonly used (Ao et al., 2019). Furthermore, it is challenging for such models to cope with high dimensionality, which is a common characteristic of modern portfolios (DeMiguel et al., 2009b). To better address these challenges, different methods to solve the portfolio optimisation problem continue to be developed, borrowing from different techniques in other domains, such as fuzzy programming (Arenas Parra et al., 2001), cluster analysis (Puerto et al., 2020), quantum annealing (Venturelli and Kondratyev, 2019) and deep reinforcement learning (Shi et al., 2022).\nOf particular interest to this work are topological or network studies for portfolio optimisation (Pozzi et al., 2013; Li et al., 2019). Network models exploit graph data structures to identify relationships that are impossible to detect by Euclidean data-based models. In our case, the network nodes are the firms, and each edge measures the relationship between the two firms. More formally, the network at a given time t is represented as an undirected graph G = (V\u2081, E\u2081) where V\u2081 are the nodes or firms and E\u2081 is the set of edges, often represented by an adjacency matrix A of dimension |V\u2081| \u00d7 |V\u2081|.\nThe aforementioned network studies consistently find that allocating capital to firms in the peripheries of the networks produces better returns, due to low correlations with other parts of the network. Similarly, to produce a diversified portfolio, mean-variance models also tend to prefer firms in the peripheries of the network (Onnela et al., 2003). However, studies using the former methods have only been deployed to small portfolios or were limited to a specific sector. Here, we look to extend these topological analyses to the whole market of US mid-cap companies, a much more challenging and realistic problem setting.\nMid-cap firms (in short 'mid-caps') are defined as firms with a 1 to 10 billion USD market capitalisation and are likely constituents of the Dow Jones Wilshire Mid-cap or S&P 400 Mid-cap indexes. Modelling the performance of these firms is complicated by the illiquid and sporadic nature of trading, which makes their return distributions non-normal (Castellano and Cerqueti, 2014). They are also far more numerous compared to large-cap firms, which makes mid-caps unsuitable for analysts to cover. However, as they behave as a separate autonomous asset class, they can further improve the diversification aspect of a portfolio if included (Petrella, 2005). Over the long term, mid-caps also provide a premium in return for the same risk, which is desirable for any portfolio seeking financial returns (Ge, 2018). The indices of such firms contain several constituents, making simple index creation strategies, such as the popular market-weighted methodology of the Russell 2000 Index (or S&P 500), suboptimal for investors. As investors have different horizons and risk tolerance combined with churn in index constituents leads to under performing the market(Cai and Houge, 2008; Cremers et al., 2020). A comprehensive and intensive approach is needed to generate portfolio weights for such firms that yield a better risk-adjusted return. Another area of practical interest that our study is applicable to, would be in devising new automated ETF strategies for such companies. Large-scale portfolio optimisation with the strategy constraints of the particular ETF needs to be modelled.\nStudying the correlation of firms is an integral part of any portfolio optimisation procedure. Pearson correlation approaches, generally used by mean-variance models, can only capture linear dependencies and pairwise correlations. In this work, we deploy the distance correlation measure (Sz\u00e9kely et al., 2007). This can capture non-linear relationships between pairs of firms.Sun et al. (2019) compared distance correlation with Pearson correlation for portfolio optimisation and found that the distance correlation strategy performs well. The portfolios in most studies, however, do not have a natural churn of firms which, we believe, makes distance correlation even more useful and deploy this for our data. Also, we optimise the metric further for a large-scale portfolio. As mid-cap companies are illiquid when compared to large-caps, the available trading history may be shorter, less uniform or missing for some period of time. The distance correlation measure can handle such time series features and still produce a quantitative measure of relationship between firms. This allows us not to drop any firms, which mitigates any selection bias. We use the covariance matrix generated by distance correlations to generate a fully connected network of firms. Furthermore, we deploy the Triangulated Maximally Filtered Graph (TMFG) method introduced by Massara et al. (2017) to filter the dense correlation matrices generated by the distance correlation measure. This process generates a filtered network with fewer edges without loss of information and represents the state of relationships between firms. Deep learning techniques further build on these representations and using all the historical data can create more complex higher-order representations.\nOnce data from stock prices and networks is included, traditional methods fall short on handling the complex resulting data structure. Deep learning techniques however excel in this area. They have produced state-of-the-art results in several domains, such as speech recognition, natural language processing, object detection, drug discovery, and genomics (LeCun et al., 2015). As they do not impose restrictions on data distribution and handle exotic data types by design, they are applicable in a wide range of situations. This suggests that deep learning methods may be well-suited to mid-cap with high default rates and reclassifications into large-caps, their joint distribution of returns, or other temporal features that may not be suitable for classical models. Deep learning models also scale well to big datasets with numerous features. They can also find optima with constraints for different problems, which makes them again suitable for large-scale portfolio optimisation which involves similar objectives with constraints, as a recent study on midcap default prediction has shown (Korangi et al., 2022). In this paper, we propose a particular class of deep learning networks, Graph Neural Networks(GNN), which can find higher-order representations taking the relationships between various firms into account. This is an important and valuable feature for portfolio optimisation as the relationships between firms change over time and also behave differently in different macroeconomic environments. The optimising procedures of deep learning techniques are particularly suitable in a higher-dimensional space, which also supports its use in this problem real. They can also be trained to any complex objective function and are flexible in their output, which we exploit to design a model that delivers optimal weights for a large set of mid-cap stocks. The distance correlation and TMFG filtering provide an important step for the GNN models where a suitable initial relationship between firms is provided, which the deep learning models build on to find higher-order relationships.\nPortfolio optimisation often involves estimating the expected returns and volatility of the set of stocks, or its corresponding covariance matrix, and then using a constrained optimisation method to find the asset allocation weights that maximise the portfolio objective. Topological studies have shown that network structure plays a vital role in this process. In this study, instead of relying on just a few measures, as done in previous studies, we allow the graph neural network to distil information from the relationships to produce the portfolio weights. We use the widely applied Sharpe ratio, i.e. the ratio of returns over volatility, to measure the portfolio's performance (Sharpe, 1966). The models are tuned to predict the Sharpe ratio directly without predicting returns separately as (Zhang et al., 2020) did. Training the networks is also based on the same measure, using a custom loss function derived from the Sharpe ratio. We are the first ones to deploy Graph Attention networks (GATs) for portfolio optimisation in a computationally efficient manner, allowing different aspects of the network structure to be learnt at scale (Veli\u010dkovi\u0107 et al., 2018). One of the important aspects of this study is the dynamic nature of the problem, the set of active mid-cap firms varies over time and so the resultant graphs are dynamic. Hence, the output, i.e. the weights allocated to each firm, varies with each graph. This further complicates the problem compared to most portfolio studies, which often assume a constant number of firms. This requires additional data preprocessing and a suitable definition of the problem for training.\nWe use a different graph for each forecast period and different returns data for training, validation, and testing. Using a rolling window approach, we move the forecast period; this changes both the graph inputs and returns data.\nTherefore, the three key research questions addressed in the paper are the following:\n1. Can an effective network topology be constructed from sparse historical data on a large collection of firms?\n2. Are graph attention networks able to generate higher-order representations of this network to construct an optimal portfolio for mid-caps?\n3. How does the model perform under different market conditions, and can we infer useful strategies from the model results?\nIn so doing, the paper makes three main contributions. First, we develop topological portfolio optimisation models, extending the literature that hitherto focused chiefly on stochastic models for the segment, applying our result to the challenging set of mid-cap firms. Second, we optimise distance correlation measures, use TMFG to generate large-scale networks, and describe the complex interactions among these firms. Finally, we use these networks as inputs to graph-based deep learning models that are capable of producing portfolio weight for the large number of firms with which we are faced.\nThe remainder of the paper is organised as follows. Section 2 reviews the relevant literature, focusing on large-scale portfolio optimisation, graphs and graph-based deep learning models. Section 3 describes the data and the process by which this data is converted into graphs and relevant measures used in the study. The proposed models, and the baseline models against which they are compared, are described in Section 4. Section 5 discusses how we set up our experiments, including the data flow from input to output, the loss function used, and other key experimental design choices. Section 6 then presents the results of the various models applied to different data types. Finally, Section 7 summarises the contributions and suggests future steps."}, {"title": "2 Related literature", "content": "In this section, we present the relevant literature for the study, focusing on large-scale portfolio optimisation studies and GNNs."}, {"title": "2.1 Correlation Networks and Portfolio models", "content": "Portfolio optimisation has been the subject of a large body of research, and novel methods, with various constraints and objectives, continue to be developed (DeMiguel et al., 2009a; Branch et al., 2019). Of particular interest to our work are studies that focus on optimising large-scale portfolios effectively. Perold (1984) was the first to do so, by considering the specific nature of dense covariance matrices, and recommending strategies to make them sparse so they become computationally feasible. The first branch of large-scale portfolio optimisation studies folloow a similar approach in devising algorithms to efficiently use computational resources, such as computing time and memory.\nLater studies on large-scale portfolio optimisation focus on generating weights for the firms and measuring the model portfolio performance against metrics such as the Sharpe ratio. Bonami and Lejeune (906) proposed an exact solution approach for a portfolio of up to 200 firms by fixing the expected returns with a probabilistic confidence level and imposed several trading constraints to extend the mean-variance framework. On a portfolio of 100 firms within the S&P 500, Ao et al. (2019) converted mean-variance portfolio optimisation into a sparse linear regression, and combined them with Fama-French factors to study portfolio performance. Liu (2017) used a combination of efficient computing resources with evolutionary algorithms for portfolio optimisation. Bian et al. (2020); Dong et al. (2020) applied regularisation methods for portfolio optimisation as, without such methods, a large portfolio would lead to overly small allocations; they also estimated these covariance matrices using factor models depending on size, book-to-market values and industry sector, and found regularization helped to arrive at better optima. Performance-based regularization where the sample variance is restricted was also found to perform better on portfolios from Fama-French data sets (Ban et al., 2018). Recently, Bertsimas and Cory-Wright (2022) studied the size of portfolios of previous large-scale portfolio optimisation studies and proposed a ridge regression-based regularisation algorithm which could theoretically scale to a portfolio of over 3000 firms by converging to a solution. However, we argue that the portfolio scale considered thus far is still not large enough when using real data or otherwise studies use simulated data to measure the efficacy of portfolio optimisation algorithms. Additionally, in all these studies, the universe of firms or assets to select from is always constant, thus eliminating firms that default, get acquired or liquidate, eliminating a large driver of idiosyncratic portfolio risk. Instead, we take a dynamic approach and allow the portfolio to vary over time, allowing us to use real data and use them on smaller firms instead of focusing on large firms with readily available data. Our approach mitigates some of the selection bias and is closer to a real-world scenario where investors want to invest in a certain sector or asset class of the market, and would like to make an optimal decision considering all the firms available within that asset class.\nIn order to create a sparse covariance matrix, another alternative explored in the literature is information filtering using graphs or network data. Information filtering in large data sets by building sparse networks is an active area of study in various domains such as internet search (Xie et al., 2018), social networks (Berkhout and Heidergott, 2019) and, same as our paper, finance (Fan et al., 2013). Specifically, to optimise the portfolio, Onnela et al. (2003); Cho and Song (2023) showed that investing in the peripheries identified by the graph provided benefits for portfolio diversification. They used a static slice of S&P 500 companies, starting at a larger base than previous studies, but the firms are unchanged over the long time frame of 20 years they studied. In the area of information filtering of large-scale networks there continue to be major improvements, such as social network analysis for link prediction (Zhang and Chen, 2018), recommender systems (Fan et al., 2019), and the study of object interactions in complex systems (Battaglia et al., 2016). The goal of these studies is to maintain the most relevant information by constraining the topology of the graphs. The Minimum-spanning tree is a filtering mechanism for dense graphs that keeps the edges with the highest weights and allows no cycles or loops in the graph (Mantegna, 1999). Planar Maximally Filtered Graph (PMFG) uses a different constraint on the graph's topology, requiring it to be planar; i.e., there should be no edge crossing on a plane (Tumminello et al., 2005). They are found to be robust for financial market networks as market conditions change without losing much information content (Yan et al., 2015). Triangulated Maximally Filtered Graph (TMFG) is a more computationally efficient algorithm since it can be parallelized compared to PMFG (Massara et al., 2017). In this work, we adapt this methodology for correlation networks of stocks, making them suitable for the large datasets we work with.\nAs we mentioned earlier, we use the distance correlation measure to account for the strength of the relationship between firms. Diebold and Yilmaz (2012, 2014) developed the connectedness metric, using VAR (Vector Auto Regression) decomposition methods, to quantify the systemic relationship between firms. They can identify clusters and central firms crucial to networks and quantify the direction of risk spillovers. However, for large networks such as our mid-cap firms, these methods have limitations due to the amount of historical data they need. Given these limitations, we instead used distance correlation measures for pairs of firms and TMFG for filtering, to provide the sparse network which serves as the input to our deep learning-based models."}, {"title": "2.2 Graph Neural Networks", "content": "GNNs were first proposed by Scarselli et al. (2009) for node classification tasks. Similar to recurrent neural networks (RNNs), they employ recursion, which is used to learn higher-order representations for a node from its neighbours. As the deep learning field for Euclidean data evolved with RNNs for sequential data, Convolutional Neural Networks (CNNs) for primarily image processing, and Attention-based models Vaswani et al. (2017) for spatial analysis of unstructured data, non-euclidean data models based on GNNs also developed in parallel. Graph Convolutional Networks (GCNS) borrow concepts from CNNs, such as kernel filter size and stride, to generate representations for graphs (Kipf and Welling, 2017). They produced state-of-the-art results on popular graph datasets such as citation networks and knowledge graphs over semi-supervised or skip-gram-based graph embeddings, label propagation and regularization approaches. Graph Attention Networks (GATs) further improved on these results by introducing variable aggregation of neighbours, and they also proved successful in transductive learning tasks where the data is not fully labelled (Veli\u010dkovi\u0107 et al., 2018). We refer the reader to a comprehensive survey by Wu et al. (2021) for a general introduction to GNNs and their various flavours. Thus far, GNNs have been applied in some application areas related to finance or financial markets. For example, in consumer finance, they were used for fraud detection (Xu et al., 2021) and credit risk prediction (\u00d3skarsd\u00f3ttir and Bravo, 2021) in consumer finance. Avramov et al. (2023) applied other deep learning techniques for listed NYSE firms and found it can produce complex signals that are profitable but become costly with high turnover. Feng et al. (2022) used a variant of GNNs and applied self-attention separately for the stock recommendation out of 738 stocks which is to predict top-3 stocks for the next period. We could have given the fully connected graph as an input to the GAT model. However with such high-dimensionality with an average 6000 companies to look at every quarter, the training would have been lot complicated but by taking advantage of econometric-based preprocessing of data to extract initial relationships we initialize the model with better input. Overall,we hypothesise the deep learning methods are better deployed at large scale with illiquid, risky firms than on a small set of established firms, as they are better at extracting complex relationships and with a lower churn the portfolio which reduces transaction costs, but need large data to arrive at a stable"}, {"title": "3 Methods", "content": "In this section, we describe the data used for this study, the distance correlation measure and the graph filtering algorithm, which all together create the inputs for the GNN and other benchmark models."}, {"title": "3.1 Data", "content": "We collected the daily closing prices of all mid-cap companies listed in the US over 30 years, from 1990 to 2021. There are approximately 20,000 firms active for at least part of this period. For portfolio selection, we use a three-year rolling window, so the portfolio allocation problem is limited to around 5,000 firms at any given time point. This number changes substantially over time, however, due to firms entering and leaving the market. We convert the prices to daily returns for each firm, defined as r_{ut} for a firm u at time t. These return series are sequentially divided into a training (50%), validation(25%), and test set(25%), and serve as the main input to our models.\nTo create the relationships between firms, we also calculate the return-volatility series using the standard deviation of these returns and a 30-day look back period. For a firm u at time t it is defined as c_{ut} = \u03c3(r_{ut}, r_{ut\u22121}, ..., r_{ut-30}).\nThe return-volatility series is input to calculate the covariance matrix between firms using distance correlation. We used a three-year look back period between a pair of firms to derive the covariance matrix. This look back period allows for a relatively new firm to also be part of the portfolio, as mid-cap firms generally have some previous trading history as small-cap firms. The return-volatility series also has interesting features compared to return series, such as clustering where volatilities are asymmetric with short periods of co-movements particularly during risk-off situations in the market, while during benign times markets show more independence in relationships. This series also has strong serial correlation, and also it tracks market investors sentiment (e.g.: the volatility index, VIX), which are essential for identifying crises (Huang et al., 2019). These are the situations where the performance of different portfolio allocation strategies can diverge substantially.\nWe define the universe of firms as V. We sample them over N periods where V\u2081 \u2286 V denotes"}, {"title": "3.2 Distance correlation", "content": "We quantify the strength of relationships between two firms using the distance correlation measure. In this section, we describe this measure. Distance correlation is a generalised measure of dependence, which captures non-linear dependencies and performs well in domains such as signal processing (Brankovic et al., 9 12) and computational biology (Mendes and Beims, 2018). The distance correlation using the estimation procedure is defined below, dcor(u, v) between two firms using the volatility series (l\u1d64, l\u1d65) defined earlier in (2). For each firm, we get the absolute change in volatility over all periods and scale these changes by removing the averages to get firm-level scaled change matrices, which are compared with other firm-level matrices to derive the distance correlation measure. First, we define two matrices A and B for a pair of firms u and v.\n\nA_{i,j} = ||l_{ui} \u2013 l_{uj}||\nb_{i,j} = ||l_{vi} - l_{vj}||\nwhere i, j = 1, 2, ..., T, and || \u00b7 || is the Euclidean distance. This captures times when the firm has"}, {"title": "3.3 Graph Filtering", "content": "The filtering technique we chose is the Triangulated Maximum Filtered Graph (TMFG) method proposed by Massara et al. (2017) which, like PMFG, imposes a planarity constraint on the graph but is more scalable to larger datasets such as ours. Using the topological features of the graph as a constraint, a planar graph retains most of the information with fewer edges. A graph is planar if no edges cross between the nodes on a sphere. Such graphs have attractive features, making, for example, cluster or community detection easier, which reduces the information need and thus making large graphs tractable for analysis.\nWe define the dense graph before filtering as K = (V\u2081, F\u2081), where F\u2081 is the adjacency matrix. We let an element f_{uv} \u2208 F\u2081 take the value\n\nf_{uv} = \\begin{cases}\n1, & \\text{if } d_{uv} > 0 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\nTMFG along with the adjacency matrix F\u2081 uses the distance correlation values from Equation (4) to filter the graph K into a sparse planar subgraph G = (V\u2081, E\u2081) that is maximal, i.e. the sum of edge weights is maximum. Planarity constraints reduce the edges from |V\u2081|(|V\u2081| \u2212 1)/2 in K to (3(|V| - 2)) in graph G.\nThe TMFG algorithm first identifies a clique of four firms with the highest distance correlation measure. A clique is when all the distinct vertices in the sub-graph have an edge between them, i.e, all members of the clique are connected. In our volatility networks, this happens in most"}, {"title": "3.4 Network measures", "content": "The graphs created after TMFG filtering could be used directly for portfolio optimisation, as some studies (Li et al., 2019; Pozzi et al., 2013) have done. We calculate a score for each node in the graph G = (V\u2081, E\u2081). This score is transformed as the weight assigned to the firms in the portfolio through the method that follows.\nWe define the score for a firm u \u2208 V\u2081 as p\u1d64, an average of the three measures of the centrality of the networks, the degree centrality, betweenness centrality, and closeness centrality. These measures have been studied since the 1970s (Freeman, 1977) and are still popular in the network literature (\u00d3skarsd\u00f3ttir and Bravo, 2021). The first measure, degree centrality, is the fraction of nodes to which the node u is directly connected to. We count all the nodes adjacent to node \u0438 and normalise it by the total number of nodes |V\u2081| \u2013 1. These set of nodes are also the neighbours, N\u1d64, for node u. Formally, we define neighbours as\n\nN\u1d64 = {v|(u, v) \u2208 E\u2081}\n\nThat is, nodes v that have an edge connecting them to u. This neighbourhood definition is also useful for the description of GNN models, as we will see in Section 4.1 where we define the GAT model.\nThe degree centrality dc\u1d62 for firm i is defined as\n\ndc\u1d62 = |N\u1d64|/(|V\u2081| \u2212 1)\n\nor the total number of neighbours divided by the total number of nodes minus one.\nThe betweenness centrality measure specifies the activity that passes through the graph node when any changes occur in the network. To measure this activity level, we first compute the shortest paths between all pairs of nodes. Betweenness centrality is the fraction of the shortest paths that pass through a node that does not include the given node. For a firm i, this is defined as\n\nbc\u1d62 = \u03a3_{u,v\u2208V\u2081} \\frac{s(u,v/i)}{s(u, v)}\n\nwhere s(u, v) is the number of shortest paths between (u, v) and s(u, v\u013ci) is the number of shortest paths that pass through i. s(u, v|i) = 0 if i = u or i = v and s(u, v) = 1 if u = v.\nThe final centrality measure we use is closeness centrality cc\u1d62. It is the reciprocal of the average shortest paths to all the nodes that are reachable from a given node. Higher scores meaning higher centrality.\n\ncc\u1d62 = \\frac{(n-1)}{(V\u2081 - 1) \u03a3_{u=1}^{n-1} d(u, i)}"}, {"title": "3.5 Model performance and loss metric", "content": "We adopt the Sharpe ratio as the final performance measure for the models (Sharpe, 1966). This is a well-studied metric to measure portfolio performance. There are several studies in the portfolio performance literature on further refinements or limitations of the Sharpe ratio (Lo, 2002; Farinelli et al., 2008).\nHere, we use the widely accepted form of the metric. From equation (1) for an individual firm u, the Sharpe ratio is estimated from the sample mean and variance of the returns. This same method applies to portfolio returns as well.\n\n\u03bc\u1d64 = \\frac{1}{T} \u03a3_{i=1}^{T} r_{ui}\n\n\u03c3\u1d64\u00b2 = \\frac{1}{T} \u03a3_{i=1}^{T}(r_{ui} \u2013 \u03bc)\u00b2\n\nFrom these quantities, the Sharpe ratio SR\u1d64 can be easily estimated\n\nSR = \\frac{\u03bc - R_{f}}{\u03c3\u1d64}\n\nwhere R_{f} is the risk-free interest rate. For ease of calculation,we normalize it to zero as it is a constant baseline for all. So, the Sharpe ratio produced cannot be compared to external studies on the Sharpe ratio, but the results are comparable between the models in this study. We use this form of Sharpe ratio after training to test the performance of models and report this measure over time for all models. For training, we take a modified approach outlined below.\nMost supervised deep learning models have a prediction target, but in our problem, we do not aim to predict but to provide a score for each firm, the normalised portfolio allocation. This has implications for training and measuring portfolio performance. We use the Sharpe ratio as a loss metric during training, which the deep learning models look to minimise. A similar approach was used by Zhang et al. (2020) where they used gradient ascent and a differentiable Sharpe ratio. The Sharpe ratio could be expressed as a constrained optimisation problem where the expected returns are to be maximised while minimising the volatility. We use the negative logarithmic Sharpe ratio (-In SR) as the loss metric. The loss function is thus expressed as\n\nLF = - ln \u03bc + 2 * ln(\u03c3\u1d64)"}, {"title": "4 Models", "content": null}, {"title": "4.1 Graph Attention Networks", "content": "GNNs are applied directly to graph data and successfully exploit the structure within the data for various tasks, such as node classification and edge prediction. In domains with natural graph data, such as protein structures in chemistry or biology, citation networks, or recommendation systems, they produce state-of-the-art results even when compared to other large-scale deep learning models (Ying et al., 2018; Gilmer et al., 2017). Graph convolutional networks were developed along with convolutional neural networks used for image processing. Unlike images with strict structure, graph data are dynamic and have much more complexity and meaning between connections. Graph convolutional networks generate a higher-order representation of input features and neighbours by using the neighbourhood degree centrality to weigh the neighbour's features. As degree centrality is a fixed measure, more complex weighing mechanisms are impossible. Graph Attention networks solve this problem by using the self-attention mechanism, creating learnable parameters to generate the weights for neighbours, making them more dynamic in learning from the neighbourhood features (Veli\u010dkovi\u0107 et al., 2018). Large graphs such as the mid-cap graph we create in this paper are dynamic and show significant evolution as time passes. Also, the market conditions vary for each time period, meaning fixed weighing of neighbours may not perform well with these changes. Added to this, the Attention operations are more efficient than alternative approaches, since they can be parallelizable across node neighbour pairs.\nHere, we formally define the GAT specific to our problem. Given a graph G = (V\u2081, E\u2081) as defined in section 3.3, and the input features defined in (3), GAT transforms the input features X\u209c into a higher-order representation H\u209c given by\n\nH\u209c = [h\u2081, h\u2082, .., h\u1d64, h\u1d65, .., h_{|V\u209c|}]\u1d40\n\nwhere h\u1d64 \u2208 \u211d\u1d40'\nSpecifically, for a given firm u, the transformation function from x\u1d64 to h\u1d64 is where each variant of GNN differs.\n\nh\u1d64 =||_{k=1}^{K} F(x\u1d64, \u03a3_{v\u2208N\u1d64}a(u, v)(Wx\u1d65))\n\nN\u1d64 defined in equation (6) are the neighbours of firm u. W\u2208 \u211d\u207d\u1d40'\u00d7\u1d40\u207e is a weight parameter matrix of the model that is learnt during training. a(u, v) is the weighted importance score of firm v on firm u. A softmax layer does the weighing after a feed-forward network over each neighbour firm v with the present firm u. The function a(u, v) is the one that causes the differences, where it is a convolutional function for GCNs and attention for GATS.\n\na(u,v) = softmax (\u03c3(a [Wx\u1d64||Wx\u1d65]))\n\nwhere \u03c3 is a non-linear function, LeakyRelu in this case. a \u2208 \u211d\u00b2\u1d40' and || is the concatenation operator.\nF(\u00b7) also applies the non-linearity function (usually a Regularized Linear Unit, ReLU) after aggregating all weighted outputs. GAT also allows for multi-head attention, where each head learns a different aspect of the input. For a K-multi-head attention, we concatenate each head's outputs to construct the final representation. We used T' = 24 and K = 8 in our experiment.\nThe H\u209c function is a higher-order representation. We scaled these representations to lower dimensions to make them suitable for predicting weights for the portfolio. We apply batch normalisation, which normalises the feature inputs for the next layer, and use two feedforward networks with dropout to scale the representation as we want to generate a single number for each firm finally. The dropout process helps to improve the stability of the training while reducing complexity. The final feedforward network also uses L1, or LASSO, regularisation to further shrink"}, {"title": "4.2 Network index model", "content": "As studies have shown, portfolios invested in peripheral assets outperform portfolios containing more central firms (Pozzi et al., 2013; Giudici et al., 2020). This is possible due to the diversification benefits of peripheral firms, as they are less correlated with market moves. Our model uses the peripherality measure defined in Section 3.4 and allocates capital to the portfolio as the inverse of the network index score. We rescale these weights so that they sum up to one. For each node or asset i in the network,\n\nw\u1d62 = 1/p\u1d62\n\nw\u1d62"}]}