{"title": "PROPNEAT - EFFICIENT GPU-COMPATIBLE BACKPROPAGATION OVER NEUROEVOLUTIONARY AUGMENTING TOPOLOGY NETWORKS", "authors": ["Michael Merry", "Patricia Riddle", "Jim Warren"], "abstract": "We introduce PropNEAT, a fast backpropagation implementation of NEAT that uses a bidirectional mapping of the genome graph to a layer-based architecture that preserves the NEAT genomes whilst enabling efficient GPU backpropagation. We test PropNEAT on 58 binary classification datasets from the Penn Machine Learning Benchmarks database, comparing the performance against logistic regression, dense neural networks and random forests, as well as a densely retrained variant of the final PropNEAT model. PropNEAT had the second best overall performance, behind Random Forest, though the difference between the models was not statistically significant apart from between Random Forest in comparison with logistic regression and the PropNEAT retrain models. PropNEAT was substantially faster than a naive backpropagation method, and both were substantially faster and had better performance than the original NEAT implementation. We demonstrate that the per-epoch training time for PropNEAT scales linearly with network depth, and is efficient on GPU implementations for backpropagation. This implementation could be extended to support reinforcement learning or convolutional networks, and is able to find sparser and smaller networks with potential for applications in low-power contexts.", "sections": [{"title": "1 Introduction", "content": "The NeuroEvolution of Augmenting Topologies (NEAT) algorithm [1] is a genetic algorithm for training sparse neural networks that has been used for a range of purposes, especially as a competitor to reinforcement learning in control systems [2, 3]. NEAT evolves complex networks by incrementally adding nodes and edges, optimizing weights through genetic techniques. Its relative simplicity and lack of need for advanced hardware or GPUs have driven a high level of interest, including demonstrations of AI on YouTube channels when applied to computer games [4]. However, NEAT has had limited application in tabular data due to slow convergence and subpar performance compared to alternatives such as dense neural networks and decision trees. The limitation derives from the use of genetic-based weight optimisation. Gradient-descent based methods have been used but have also been limited due to the inherently sequential implementation of the activation of nodes in NEAT which does not permit the efficiencies of GPU-based backpropagation to be applied.\nHere, we present the PropNEAT algorithm, first covering a naive implementation of backpropagation before presenting the PropNEAT algorithm itself. We cover the challenges that arise from the genetic algorithm, the details of the construction and mapping of the layer-based representation that allows for efficient linear-algebra operations, and the details of other secondary changes to NEAT including topological change rates that are subsequently required when"}, {"title": "2 Background", "content": "NEAT [1] creates sparse neural networks by adding nodes and connections starting from a minimally connected network where the inputs are connected to the outputs. After initiation, the algorithm follows these steps:\n1. Evaluate the performance of all models,\n2. Speciate based on structural similarities using innovation numbers (a unique identifier for each gene),\n3. Eliminate the weakest individuals in each species\n4. Reproduce with crossover and mutation of weights and topology to replace the eliminated individuals.\nNEAT's key innovation is the use of innovation numbers which are incremental numbers as identifiers of genes to track network topology. This simplifies the alignment of the network graphs for crossover without requiring subgraph analysis. This mechanism enables evolution of both weights and topology using crossover, mutation and speciation. Competition is done within species based on structural and weight differences, configurable with hyperparameters.\nThe resulting graph model from NEAT networks does not have the same layer-based structure used in other neural networks. Although the per-node calculations are equivalent, they are done sequentially through the graph on a per-node basis, rather than using matrix or tensor operations through layers.\nDense neural networks have had multiple speed improvements since their invention. Raina et al. in 2009 [5] demonstrated a 70-times faster implementation of backpropagation over artificial neural networks (ANNs) by using GPUs. They demonstrated techniques for parallelisation for deep networks that allowed the memory advantages and parallelisation of GPUs to be applied to the backpropagation of neural networks. This allows linear algebra optimisations such as from Volkov and Demmel [6] over NVidia GPUs to improve the performance of neural network training. Le et al. [7] demonstrated how this work, when applied to stochastic gradient descent (SGD), was able to achieve state-of-the-art results over MNIST.\nA limitation of the original NEAT implementation has been its use of the genetic algorithm to update weights, with this being either slow or not very effective in converging to an optimal solution. Several researchers have added backpropagation to resolve this problem. Chen and Alahakoon added backpropagation for classification tasks [8]. They note the combined benefits of the breadth of the exploration space from speciation and the genetic algorithm, with the weight refinement of the backpropagation. They used a limited amount of backpropagation in their search due to the computational complexity of the process. They only added in backpropagation in a subset of generations, and only over a subset of data. They saw a substantial improvement over NEAT.\nWhiteson and Stone added a backpropagation implementation to NEAT to support reinforcement learning [2]. They do not provide details of their backpropagation implementation. They note the improvement in performance over NEAT, and the improved speed in convergence due to the improved weight optimisation process.\nDesell added backpropagation to NEAT to allow convolutional neural networks (CNNs) to be created using NEAT [9]. In this work they showed predictive performance comparable with human-designed CNNs. In this work, they describe their backpropagation methodology in section VII.b. The implementation described is a simple approach which first computes the order of nodes in the feed-forward direction and backpropagates the the error in the reverse order. This is what we would describe as a naive implementation. They note the substantial computation requirements for backpropagation over CNNs, and tackled this primarily by making adjustments to allow parallel and asynchronous computation of models on multiple machines, using 5,500 in their experiment. This parallel computation was done at a per-genome level, and did not optimise the training at a weights level.\nPublic and open-source NEAT implementations such as neat-python [10] have identified and implemented parallelism of the node computations. However, this has been used to ensure the correct order of activation of nodes and potential CPU threading optimisations. It has not been used to implement tensor-based computations of the nodes. To the best of our knowledge, there is no public implementation that brings tensor-based GPU improvements to NEAT."}, {"title": "3 PropNEAT Algorithm", "content": "The PropNEAT algorithm efficiently applies backpropagation on GPUs to train the weights of NEAT-created neural networks. It maps NEAT's graph topology to a layer-based structure allowing a bi-directional mapping of nodes and weights to tensors. This mapping handles skip layers and unreachable nodes, which ultimately permits backpropagation using standard tensor libraries with their GPU optimisations. We will describe the naive approach to backpropagation over NEAT, its limitations, and the PropNEAT algorithm."}, {"title": "3.1 Naive approach", "content": "Typical implementations of NEAT implement a per-node approach to calculate the forward pass of the network. The order of operations can be optimised by adding a graph-analysis step first to ensure all node-level dependencies are met, ensuring that all inputs of a node have been calculated before calculating the result at that node, though other approaches are also possible. This approach can be directly replicated in PyTorch or similar, with the computation simplified as\n$$out[node] = activation(bias + \\sum(inputs))$$\nfor each node in order. This is the implementation by Desell et al. [9]\nThis approach takes no advantage of any GPU linear-algebra optimisations, and instead requires the backpropagation algorithms to differentiate and propagate errors through every single node, rather than through each layer. This does not have any impact on the final result - the computations will be equivalent subject to rounding errors. While it is a sizeable improvement over a genetic search over the weights, and it is relatively straightforward to implement, it takes no advantage of any of the advantages that GPUs provide, which allows effectively for signficantly scaled parallel computation using tensor algebra."}, {"title": "3.2 PropNEAT", "content": ""}, {"title": "3.2.1 Genome-Tensor Mapping", "content": "PropNEAT creates a mapping to a tensor-based structure that is nearly-bijective and that is compatible with tensor-based computations for GPU compatibility. By nearly-bijective, we mean that there exists a bijection to the tensor-based representation, which is augmented by a null-space of zeros within the tensors that have no computational impact and are not affected by the rest of the training process. This bijection allows both a genome to be mapped to tensors, and the tensor to be mapped to a genome. The only limitations is that gene tracing (via gene IDs) are lost if the metadata is not maintained.\nThere are two main challenges in creating such a mapping:\n1. NEAT creates nodes and connections that can be unreachable in the forward or backward direction but may result in advantageous mutations; and\n2. NEAT creates skip connections where the outputs of a single node might be at different depths.\nThese challenges are demonstrated with an example in Fig 1.\nIn addition, we would want the mapping to maintain the bijective properties, map efficiently onto the minimal tensor representation possible, and be relatively easy to implement.\nWe solve this by taking several graph traversal and analysis steps before constructing the mapping:\n1. Traverse the nodes breadth-first in a forward (input-to-output) direction to compute depth, reachability, and full connectivity\n2. Traverse the nodes breadth-first in a backward (output-to-input) direction to compute reachability for back-propagation\n3. Compute the layer structures and connectivity, e.g., skip layers\n4. Map the edges to weights, nodes to their biases, formalising the map of genes to tensor indices\nThe resulting structure is demonstrated in Fig 2."}, {"title": "3.2.2 Definitions", "content": "Depth: the length of the longest path that connects an input node to the node. Inputs have $depth = 0$.\nReachable: In the forward direction, there exists a path from at least one input to this node. In the backwards direction, from at least one output to the node. In this paper, we are only treating single-output models, but this is generalisable to tasks with multiple outputs. This path must be in a consistent direction aligned with either the feed-forward or backpropagation calculations.\nUnreachable nodes: nodes that are not reachable in at least one direction. This implies that they either are unable to be affected by the inputs (unreachable forward), unable to affect the outputs (unreachable backward), or unable to affect either.\nSkip layer: a layer that has two input layers of different depths.\nDense: a model in which all connections between layers have non-zero weights.\nSparse: a model in which not all connections between layers have non-zero weights, or could be represented by such a model, such as those produced by NEAT.\nDensity: The proportion of connections between layers which are non-zero."}, {"title": "3.2.3 Forward pass", "content": "The forward pass does a breadth-first traversal of nodes starting from the inputs. During this step, it records the following for each node:\n1. Depth of the node\n2. List of gene ids of inbound and outbound connections\n3. List of depths of inbound and outbound connections\n4. Is the node an input/output node\n5. Is the node reachable in the forward direction\n(1) is used to map this to the correct layer. (2, 3) are used to compute the layer connectivity. This can also be done during the backward pass, or partially in both depending on the implementation of edges. (4) makes input and output edge-case handling easier. (5) allows unreachable nodes to be discarded for the GPU."}, {"title": "3.2.4 Backward pass", "content": "The backward pass does a breadth-first traversal of nodes starting from the outputs. During this step, record for each node if it is reachable in the backward direction. This allows the handling of unreachable nodes explicitly. You can also build or verify depth and connectivity of nodes at this stage as part of (2,3) of the forward pass."}, {"title": "3.2.5 Layer mapping", "content": "Create 'k+1' layers, where 'k' is the depth of the deepest output node. The depth of each node defines the layer in which it exists.\nFor each layer, the following properties are calculated:\n\u2022 The list of nodes in the layer\n\u2022 The order of the nodes, ordered by gene id\n\u2022 Layer type from [input, output, connected]\n\u2022 The output dimension, which is equal to the count of nodes in the layer\n\u2022 If this layer has skip inputs, calculated by whether any input nodes have $d_{input} < d_{layer} - 1$\n\u2022 If this layer has skip outputs, calculated by whether any output nodes have $d_{output} > d_{layer} +1$\n\u2022 The layer index of any input or output skip layers\nAfter this has been computed for all layers, then the input dimension can be computed for all layers, defined as the sum of output dimensions of all input layers (including skip connections)."}, {"title": "3.2.6 Instantiating the model", "content": "Instantiating the model is done using the standard approach within a chosen library but specific handling must be in place for skip connections. Skip connections are handled using concatenation as per DenseNet [11] which most accurately represents the calculations in NEAT. In order to maintain the bijection, it is important to have a deterministic method to determine which nodes map to which index within a tensor. In order to accurately index the node weight within a tensor, we use the following conventions to manage the concatenation of skip layers:\n\u2022 The node index is the position of the node, ordered by gene id, within the layer\n\u2022 Layers are concatenated in order of layer depth for skip layers.\n\u2022 Layer offset is the sum of output dimensions of the input layers with lower layer depth\n\u2022 The resulting index of a node within a tensor is therefore the layer offset + the node index\nWe also rely on the property that a node that has input weights that are all zero and a bias of zero will have an output of zero under most activation functions. We will limit our use of activation functions to just those that have this property, especially focused on ReLU and Sigmoid. In this case, no error gradient will be associated with this node and all associated weights will be zero. This is equivalent to this node not existing, or existing but with no connections. It is expected that the vast majority of weights will be zeros due to the sparse networks created by NEAT.\n1. The input layer is instantiated in the standard way\n2. For each connected layer in order of depth\n(a) The weight tensor is instantiated according to the input/output dimensions with zero weights. This is a starting point of no connections between the layers.\n(b) The weight is overwritten by the weight of each edge, with tensor indexing being determined by the conventions above. This is the process of setting the connections between the nodes.\n(c) The input tensor is calculated as the concatenation of the output. tensors, in increasing order of depth\n(d) The bias tensor is instantiated in node index order.\n(e) The output tensor is computed in the standard way.\n3. Any final step with the output layer, with different activation functions etc., are implemented.\nThis method results in a model creation that will have standard forward and back functions that is able to be mapped to GPUs, using standard tensor libraries such as PyTorch."}, {"title": "3.2.7 Updating genome weights", "content": "After training the model, the genome weights can be updated by reading the genome weights from the model according to the conventions described above."}, {"title": "3.3 Other changes to NEAT", "content": "Replacing the weight optimisation of NEAT with this approach requires a few changes to the NEAT algorithm.\nThe weight training happens after genetic cross over, and before evaluation. This ensures that any topological change is associated with a fine-tuning of the model before evaluation, and any improvement can be immediately seen.\nBy updating the weights via backpropagation, all weight-changing mutations should be disabled. An exception could be made for reinstantiating all weights, which can be used to effectively reseed the weights as part of the training but this was not implemented in this experiment. Otherwise, the weights in the genome should be continued through generations according to the results of backpropagation.\nSimilarly, as topological changes do not need as many generations for weights to converge, the rate of topological mutations can be increased compared to the NEAT parameters. This is because backpropagation is equivalent to multiple generations of genetic improvements of weights. The extent to which the rate of topological mutations can be increased warrants further research.\nThe model can be trained for a fixed number of epochs per generation. On the basis of tests during experiments, and a first-principles basis that a model can continue to converge during subsequent generations should it not have occuured, we expect a small number of epochs (e.g., 25) per generation to be chosen. As weights persist through generations, early stopping methods and similar are managed with the per-generation evaluation and selection methods. The improvements of a generation will continue if it results in an overall improvement according to the GA. The optimisation of this hyperparameter was not explored in this series of experiments.\nThe balance of topological mutations versus backpropagation time can now be considered an exploration vs exploitation balance. Increasing topological mutation rates increases the topological exploration. Increasing the amount of backpropagation per topological change increases the exploitation within a topology.\nThe other mechanisms of NEAT work as per the original implementation [1], including speciation, penalty functions and crossover. Adjustments of hyperparameters such as weight vs topology penalties for the similarity function for species may need to be adjusted for optimal performance."}, {"title": "3.4 Retraining the minimal-covering network", "content": "As NEAT creates a predominantly sparse network, most implied connections in the resulting layer-based model are effectively disabled and the weights matrices will be predominantly zeros as there are a large number of potential connections that have not been made (e.g., between nodes 1 and 5 in Fig 1). If one disregards the requirement to map back to the creating genome, then one can reseed the weights using any standard strategy (including reseeding just the zeros and maintaining the weights), and train the fully-connected network represented on the GPU. As weights that were forced to be kept as zeros will now become non-zero, the implied topology will no longer match the source genome and so the bijection will be lost\nThe methodology described above will create a set of tensors (network) of minimum size (minimal) that can represent the topology of the genome (covering), creating the minimal covering network. This is the smallest dense network for which the topology of the genome is a subgraph, and the majority of the weights are zero as there are no genes coding the connection between those nodes.\nIt is worth noting the similarity to dropout. Dropout takes the tensors of a dense network with all possible connections and masks a random subset to be disconnected, representing them with zeros. This results in a tensor that is a combination of the weights of the activated connections, and zeros for the dropped connections. In PropNEAT, we start with a tensor with no connections, and mask it with the connections that are present in the genome."}, {"title": "3.5 Recurrence, convolutions, and other variants", "content": "This implementation has been targeted at tabular data, with a single vector of inputs, and a single output. As a first implementation of this technique, the simplest task was chosen for evaluation. As such, this implementation enforces a directed, acyclic graph (DAG).\nThe methods described here are also applicable to models with recurrence, convolutions, long short-term memory modules, and other node activation functions. These will require further implementation and experimentation."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Setup", "content": "We benchmarked the PropNEAT algorithm against existing models on open datasets to characterize it according to:\n\u2022 Predictive performance\n\u2022 Time to train\n\u2022 Model size and complexity\nThe primary hypothesis is that PropNEAT has equivalent predictive performance to other models on binary classification problems over tabular datasets. This was chosen as a good first demonstration of the algorithm. Rich datasets, including images, video and time series data were excluded on the basis that further changes to NEAT would be required to allow for either recurrence, long-short-term memory, convolutions, and similar which would be necessary for appropriate benchmarking.\nThe secondary hypotheses are that:\n1. For the same compute time, PropNEAT has better performance than NEAT\n2. For the same bounding model size, PropNEAT has equivalent performance to a fully connected dense neural network\nPropNEAT is compared against:\n\u2022 logistic regression (LR),\n\u2022 random forests (RF), and\n\u2022 dense neural networks (NN).\nThe datasets where chosen from the Penn Machine Learning Benchmarking datast (PMLB) [12]. The criteria for selection were tabular datasets with a binary-classification target. Image datasets, time series, and other rich datasets were excluded, as were multi-class classification datasets. Each model will be trained and evaluated on the following datasets:\n1. adult\n2. appendicitis\n3. australian\n4. backache\n5. biomed\n6. breast\n7. breast_cancer\n8. breast_cancer_wisconsin\n9. breast_w\n10. buggyCrx\n11. bupa\n12. chess\n13. churn\n14. clean1\n15. clean2"}, {"title": "4.2 Model training", "content": "Each model was trained in a separate script, with standard methods to load the train dataset and record predictions. For the stochastic models, the model was run multiple times. For models requiring hyperparameter selection, this was done with a dedicated configuration for hyperparameter search ranges and selection was done using sklearn RandomizedSearchCV. The test data split was not accessed within these scripts, as all performance evaluation is done in the analysis process."}, {"title": "4.2.1 PropNEAT", "content": "PropNEAT was implemented according to the above process in Python 3.7. This implementation expands from the NEAT Python library [10] and uses PyTorch [14] for the backpropagation and linear algebra.\nTraining was done for 500 generations with a population of 500 with 25 epochs per generation. Activation is ReLU for all hidden layers and sigmoid on the output layer. Optimisation is done using Adadelta.\nNEAT parameters are 0 mean 1 standard deviation (s.d.) for initialisation weights with the following mutation probabilities:\n\u2022 P(add connection) = 0.6\n\u2022 P(remove connection) = 0.6\n\u2022 P(add node) = 0.5\n\u2022 P(remove node) = 0.5\n\u2022 P(reinitialise weights) = 0.0\nThe probabilities were static across all generations and default values from the pyneat library [15] were used for all other methods."}, {"title": "4.2.2 PropNEAT - Retrain", "content": "The PropNEAT - Retrain evaluates the minimal-bounding NN architecture of the best PropNEAT model in each run as described in Section 3.5. The highest performing individual from a PropNEAT run was chosen, and all the weights in the tensors were reinitialised using Kaiming Normal initialisation [16]. The training then continued using the PropNEAT implementation. All backpropagation parameters were the same as the PropNEAT implementation. Early stopping was implemented using the train/validate split from the parent PropNEAT training method.\nThe reuse of the PropNEAT implementation which does not require dropout meant the retrain model did not have dropout implemented either. This is a limitation which means the model will likely not result in the highest performance theoretically possible for the given architecture."}, {"title": "4.2.3 Dense neural networks", "content": "The dense neural networks used a standard fully-connected model using dropout (p=0.25) and early stopping. The hidden layer sizes selected were [256, 512, 512, 256] with input and outputs determined by the datasets. Hidden activation was ReLU and final output activation was sigmoid."}, {"title": "4.2.4 Regression", "content": "Regression models were trained using the LogisticRegression models from sklearn"}, {"title": "4.2.5 Random forests", "content": "Random forests were trained using the RandomForestRegressor from sklearn with hyperparameters chosen using the RandomisedSearchCV module."}, {"title": "4.3 Ablation", "content": "We also compared the difference between PropNEAT, the naive back-propagation implementation of NEAT, and the original NEAT implementation through an ablation experiment over the Adult dataset. The three algorithms were compared with performance over time to test the following hypotheses:\n1. Backpropagation achieves better results than the genetic-algorithm based weight optimisation\n2. PropNEAT runs faster than a naive implementation of backpropagation\nTo test these, we ran each of the three algorithms across the Adult data set, measuring time to run and performance. For PropNEAT and the naive implementation, they had the same configuration as above. For the original NEAT implementation, we gave a number of generations equal to the number of epochs times the number of generations in the other implementation.\nThe experiment was only run over the Adult data set due to the extended computation time for the original implementation."}, {"title": "4.4 Performance Characterisation", "content": "We characterised the performance of PropNEAT using big-O notation by measuring the training time per individual per epoch, the size, depth and width of the genome, and then took the average of the population per generation. On a training run across all the above datasets, the training time was measured per individual per generation, and then the average time per epoch was calculated. At each generation, this was measured with both the size, depth and width of the genome, with size being the total number of genes, the depth being the total number of layers including input and output, and width being the max width of the layers of the model. This was measured for all iterations of the training of the model."}, {"title": "4.5 Analysis", "content": "The evaluation of all models was with the AUC over the test set, followed by the rank score of each model. The analyses were done in R unless otherwise stated."}, {"title": "6 Discussion", "content": "PropNEAT was shown to have predictive performance between Logistic Regression and Random Forests, and comparable to Neural Networks, on these datasets. This is to be expected, as the NEAT models are a subset of all possible Neural Networks, which are generally expected to be better than logistic regression without substantial additional feature generation. It is likely that increased performance is possible for most of the models, especially for PropNEAT.\nThe PropNEAT models were trained using a fixed number of generations and epochs due to challenges in specifying other appropriate stopping criteria. It is likely that the models have not \"fully converged\", though this term is perhaps not entirely appropriate for describing the genetic algorithm finding the optimal topology. It is possible that further performance can be attained with better stopping criteria, such as number of generations without a topological innovation driving improvements. This experiment shows the relative performance with as little human intervention as possible; it is expected that all models could be improved with additional effort from the modellers.\nDifferent datasets had different spreads of results between models. There are some datasets where the logistic regression was not well differentiated from other models, such as twonorm, or breast_cancer_wisconsin. There are other datasets where there is a significant different between the results, such as coil2000 or magic. Although there was no significant difference between the results of the models, there are clearly some datasets where there is signal in the data that cannot be modelled with logistic regression but where the machine-learning models are able to do so.\nThat there was no particular correlation between the AUC and topology of the models is to be expected. As there is no particular reason to expect that a dataset with a maximum possible AUC for models has a specific complexity or topology (rather, it is a function of the total explainable variance of the target variable from the data provided). This means that the model is likely explaining as much of the variance as is possible, without excess. This is further evidence for the parsimony of the PropNEAT models.\nThe combination of topological mutation through the genetic algorithm and the optimisation with the backpropogation has a clear parallel to exploration and exploitation tradeoffs. Hyperparameter tuning can likely substantially improve performance by balancing the mutations with generation length, and optimal backpropagation strategies. There is a wide range of possibilities for the backpropagation strategies beyond current methods as a topology can persist over multiple generations. Concepts such as momentum might need to be reconsidered in this context, and hyperparameters such as mutation likelihood may have possibilities for longitudinal strategies allowing for change in exploration-exploitation balances over time, decreasing mutation and increasing backpropagation over the course of training. There may also be other considerations as the overhead of transferring the models on and off the GPU may become an important factor in training speed, though code optimisation and memory management may also help mitigate this.\nThe similar performance of PropNEAT compared to the retrained version, and the Neural Networks, despite orders of magnitude fewer parameters has some potential advantages. Smaller, sparse networks may have value in low-energy or otherwise resource-constrained devices. Further, as the complexity of the networks is lower, the models are potentially more explainable. There is the possibility for graph analysis of the network topologies. The specific structure of connections and weights may yield clearer relationships within the network than is possible to identify with dense networks, but there are potentially computational challenges with graph and sub-graph analysis that would need to be tackled.\nThe ablation experiment demonstrates that there is no real difference in performance between a naive implementation and PropNEAT, but both are substantially better than the original NEAT implementation. This is to be expected - there are computation efficiencies between the naive implementation and PropNEAT, but the two methods are equivalent up to numerical methods at computation time. In comparison, backpropagation versus genetic optimisation of weights has an appreciable difference. There is also a large improvement in speed with PropNEAT, and this makes it a viable option."}, {"title": "7 Conclusion", "content": "PropNEAT demonstrates a viable method for using backpropagation on NEAT-topology neural networks whilst making use of the advantages of GPUs on tabular data. It has predictive performance similar to dense neural networks, but with smaller networks. There are large speed improvements over a naive implementation of backpropagation for NEAT-based models, with training time being determined by the depth of the networks. Analysis of these structures may be of interest for explainability of specific networks and understanding the importance of skip and sparse structures. Further work is needed to extend this to convolutional and recurrent network structures."}, {"title": "A Worked Example of Figure 2", "content": ""}, {"title": "A.1 Matrix operations", "content": "For clarity, we have repeated Figure 2\nWe define the operator to be the concatenation operation between two vectors. We will use a, b, c to be the values of the inputs. We will use $w_{pq}$ to represent the weight that connects node p to node q. We will use $h_n$ to be the output of the hidden node n. We will use $H_l$ to denote the output vector of the lth layer, and $W_{kl}$ to denote the weights between the k and Ith layers. We define Layer O as the output layer and layer I as the input layer.\nThe linear algebra operations for the network in Figure 5 can be stated as:\n$$F(H_IW_{I1}) = F\\begin{pmatrix} b_a \\\\ b_b \\\\ b_c \\end{pmatrix} \\begin{pmatrix} w_{a1} & w_{b1} & w_{c1} \\\\ w_{a2} & w_{b2} & w_{c2} \\end{pmatrix} = F \\begin{pmatrix} h_1 \\\\ h_2\\end{pmatrix} := H_1$$\n$$F(H_1W_{12}) = F\\begin{pmatrix} h_1 \\\\ h_2\\end{pmatrix} \\begin{pmatrix} w_{31} & w_{32} \\\\ w_{41} & w_{42} \\\\ w_{51} & w_{52} \\end{pmatrix} = \\begin{pmatrix} h_3 \\\\ h_4 \\\\ h_5\\end{pmatrix} := H_2$$"}, {"title": "A.2 Instantiating a matrix", "content": "We will also present the instantiation of the $W_{12}$ matrix from this example according to the PropNEAT algorithm. The steps are to instantiate with zeros, and then to replace the weights with the connection weights from the genome. We will denote $C_{ij}$ to be a connection gene between nodes i and j. The matching genome has the following connection genes: $[C_{1a}, C_{2a}, C_{1b}, C_{2b}, C_{2c}, C_{31}, C_{41}, C_{32}, C_{52}, C_{O2}, C_{O3}, C_{O4}, C_{O5}]$.\nWe first instantiate the matrix with zeros\n$$W_{12}=\\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$$\nThen, for each of our connection genes within this layer, we replace the zero with the relevant weight. This results in the following matrix\n$$W_{12}=\\begin{pmatrix} w_{31} & w_{32} \\\\ w_{41} & 0 \\\\ 0 & w_{52} \\end{pmatrix}$$"}, {"title": "A.3 Demonstration of equivalence to NEAT", "content": "We will demonstrate that the computation for the output is equivalent between PropNEAT and the NEAT implementation. As this includes inputs from a concatenated layer, a standard layer, and has a disconnect with one of the nodes, we use this as an example to demonstrate the equivalence. We acknowledge that it is not a full proof, but hope that it can help clarify the relationship between the two methods.\nWe will use f(x) to denote the activation function applied to a single output, rather than the matrix.\nThe NEAT genome has the following genes: $[C_{1a}, C_{2a}, C_{1b}, C_{2b}, C_{2c}, C_{31}, C_{41}, C_{32}, C_{52}, C_{O2}, C_{O3}, C_{O4}, C_{05}]$. By the NEAT algorithm, the output node is calculated the following way.\n$$O = f(\\sum_{k=1}^{n} w_{ok}h_k); C_{oj}$$\n$$ = f(0.h_1 + w_{o2}h_2 + w_{o3}h_3 + w_{o4}h_4 + w_{o5}h_5)$$\n$$H_2 = F \\begin{pmatrix} h_1 \\\\ h_2 \\\\ H_3\\end{pmatrix} \\begin{pmatrix} 0 & w_{o2} & w_{o3} & w_{o4} & w_{o5} \\\\ \\end{pmatrix}$$\nThis example covers the different cases of how nodes are connected, and generalises to layers with different numbers of nodes."}]}