{"title": "Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models", "authors": ["Fushuo Huo", "Wenchao Xu", "Zhong Zhang", "Haozhao Wang", "Zhicheng Chen", "Peilin Zhao"], "abstract": "While Large Vision-Language Models (LVLMs) have rapidly advanced in recent years, the prevalent issue known as the 'hallucination' problem has emerged as a significant bottleneck, hindering their real-world deployments. Existing methods mitigate this issue mainly from two perspectives: One approach leverages extra knowledge like robust instruction tuning LVLMs with curated datasets or employing auxiliary analysis networks, which inevitable incur additional costs. Another approach, known as contrastive decoding, induces hallucinations by manually disturbing the vision or instruction raw inputs and mitigates them by contrasting the outputs of the disturbed and original LVLMs. However, these approaches rely on empirical holistic input disturbances and double the inference cost. To avoid these issues, we propose a simple yet effective method named Self-Introspective Decoding (SID). Our empirical investigation reveals that pre-trained LVLMs can introspectively assess the importance of vision tokens based on preceding vision and text (both instruction and generated) tokens. We develop the Context and Text-aware Token Selection (CT2S) strategy, which preserves only unimportant vision tokens after early layers of LVLMs to adaptively amplify text-informed hallucination during the auto-regressive decoding. This approach ensures that multimodal knowledge absorbed in the early layers induces multimodal contextual rather than aimless hallucinations. Subsequently, the original token logits subtract the amplified vision-and-text association hallucinations, guiding LVLMs decoding faithfully. Extensive experiments illustrate SID generates less-hallucination and higher-quality texts across various metrics, without extra knowledge and much additional computation burdens. Codes are available at https://github.com/huofushuo/SID.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) [1-6] have demonstrated great success over the past few years, and many efforts have been tried to extend LLMs to Large Vision-Language Models (LVLMs) [7-17] and achieved impressive performance across various vision tasks [18\u201321] as well as more complex tasks like content comprehension [22] and generation [23, 24]. Despite their extraordinary versatility, LVLMs face a significant challenge known as the \"hallucination\" problem. Concretely, hallucinated textual content is fluent and semantically coherent but contains incorrect or non-exist statements about the given image, e.g., generating irrelevant or meaningless responses, identifying inaccurate colors, numbers, and locations of objects not present in the image [25]. This flaw poses a significant risk for real-world applications of LVLMs to become trustworthy AI assistants. For instance, in model-assisted computer-aided diagnosis scenarios [26], such misinterpretation of medical images can lead to misdiagnoses and serious medical accidents. Various approaches have been explored to probe and alleviate hallucinations in Vision-Language Models (VLMs). Early attempts on relatively small-scale VLMs reduce statistical bias via generating counterfactual datasets [27] or performing fine-grained modality alignment [28]. However, these methods do not generalize and scale up well to Large VLMS (LVLMs) due to the inherent nature of foundation models [29, 30]. Recent studies on alleviating hallucinations in LVLMs can be broadly categorized into two main approaches: One approach leverages external knowledge, including elaborating robust instruction tuning datasets or post-hoc utilizing auxiliary analysis networks [31-36]. Specifically, some methods [31-33] establish high-quality visual instructions generated by high-level automated annotation tools, such as GPT-4. HalluciDoctor [34] detects and revises various hallucinations from pre-trained visual instruction datasets by consistency cross-checking. Robust instruction tuning with curated datasets alleviates the LVLMs' hallucinations while inevitably inducing enormous computation burdens. Hallucination revisor network [35] is pre-trained to transform potential LVLM-generated hallucinatory descriptions into accurate ones. Woodpecker [36] detects and corrects hallucinations step by step using various auxiliary networks, including external LLMS, open-set object detection, and the VQA model. The post-hoc rectification methods rely heavily on the additional network and also entail high inference costs. Another approach is Contrastive Decoding (CD) [37-40], which does not require additional training. CD strategies adjust the logits for next-token prediction in a contrastive manner. Concretely, vision CD manipulated vision inputs with Gaussian noise"}, {"title": "2 Related Work", "content": "2.1 Large Vision-Language Models Motivated by the success of Large Language Models (LLMs) [1-6], recent studies have extended LLMs to multimodal regions and provided Large Vision-Language Models (LVLMs) [7-15, 17, 41, 42] powered by pre-trained LLMs. LVLMs understand and generate diverse content in a more comprehensive way by integrating user instruction and vision inputs. LLaVA [7] connects open-set vision encoder with LLMs (i.e., Vicuna [3]) by instruct tuning with elaborated language-image instruction-following data. Then, LLaVA-1.5 [14] develops the vision-language connector that is data-efficient and powerful for better multimodal understanding. Shikra [41] further incorporates grounding data and trains the model to understand the grounding knowledge in the given images. Blip-2, InstructBLIP, and MiniGPT-4 [8, 12, 13] introduce a learnable querying transformer to fusion multimodal features and largely reduce image tokens. Fuyu [15] proposes a vanilla decoder-only architecture without the vision encoder and adapter that makes it easier to understand, scale, and deploy. InternVL [42] three simple but effective improvements, including strong vision encoder, dynamic high-resolution, and high-quality bilingual dataset. Recently, built on SOTA open-source LLaMA 3 [6] and increasing the input vision resolution to 4x more pixels, LLaVA-NeXT [17] exhibits excellent multimodal capabilities. Despite the impressive results, all of the above LVLMs suffer from serious hallucination problems, and we mainly conduct experiments on advanced LVLMs, including InstructBLIP, Shikra, LLaVA-1.5, and LLaVA-NeXT. 2.2 Hallucination in Foundation Models Hallucination [43-45] is the generation of irrelevant, factually incorrect, or meaningless text in a given context, which is one of the bottlenecks in current foundation models. This problem can arise from overfitting specific patterns in the training data, a lack of understanding of world knowledge, or an inability to effectively contextualize"}, {"title": "2.3 Decoding Strategy in LLMs", "content": "Selecting decoding strategies in language models is crucial, as it determines how models generate text. Greedy decoding selects the highest probability next token at each step but might lead to less varied text. Beam search [55] is an accumulated-score-based decoding strategy. It maintains a set of beams to enlarge the candidate range and finally selects the best one in beams, which is more sophisticated than greedy decoding. Sampling decoding generates the next words by randomly selecting from the output distribution, where Top-k sampling [56] samples from Top-k likely tokens [56] and brings diversity but sometimes induces less coherent outputs. Top-p (Nucleus) sampling [57] improves Top-k sampling that considers the dynamic number of words that reach the probability p, achieving a balance between randomness and relevance. Recently, to alleviate the hallucination issue, DoLa [53] decoding emphasizes the knowledge of mature layers and downplays that of pre-mature layers. OPERA [25] is established on Beam search and finds the interesting phenomenon of high-probability co-occurrence between the hallucination and the knowledge aggregation patterns. OPERA penalizes 'Over-Trust Logit' in the beam score to alleviate aggregation patterns. Self-Introspective Decoding (SID) can be seamlessly integrated into different decoding strategies to mitigate hallucinations without sacrificing text generation quality, such as diversity, coherence, and relevance."}, {"title": "3 Method", "content": "In the following, we first illustrate the generation process paradigm of LVLMs to facilitate the understanding of SID. We then re-think potential defects in contrastive decoding and finally propose the Self-Introspective Decoding (SID) strategy. 3.1 Paradigm of LVLMs Generation Vision and Language Inputs. The inputs of LVLMs consist of both text (t) and image (v). Generally, the raw images are commonly fed to the visual encoder, and than the cross-model projection module maps vision information into LLMs' input space, which is denoted as vision tokens $v = \\{v_1, v_2...v_n\\}$ (n is the length of vision tokens)."}, {"title": "LVLMS Forward", "content": "The backbone networks of LVLMs are pre-trained LLMs like vicuna [3] and LLaMA 2 [5], parameterized by \u03b8. Given multimodal tokens {v,t}, LVLMs predict the next token probability (yi) at i time step in an auto-regressive manner, following LLMs' methodology, over the vocabulary set \u03bd: $p(y_i|v,t,y_{<i}) = \\text{softmax}(\\text{logit}_\\theta(y_i|v, t, y_{<i})), y_i \\in \\nu$ (1) Next Token Decoding. After obtaining the next token probability $p(y_i|v, t, y_{<i})$, different decoding strategies are proposed to predict next token. The decoded token is concatenated to the last original input token, for the next round of generation until the end of generation."}, {"title": "3.2 Rethinking Contrastive Decoding in LVLMs", "content": "Following the seminal works [52] in natural language processing, which introduced the Contrastive Decoding (CD) mechanism to enhance coherence and informativeness by considering the differences between expert and amateur models, various works have adapted this strategy to LVLMs by distorting the visual content or instruction information for contrastive purposes. As the vision and instruction contrastive process is symmetrical, we take the visual contrastive decoding as an example. The contrastive decoded probability of next-token ($p_{cd}$) can be generally formulated as follows: $p_{cd}(y_i|v, v_d, t, y_{<i}) = \\text{softmax}[(1+\\alpha)\\text{logit}_\\theta(y_i|v, t, y_{<i}) - \\alpha\\text{logit}_\\theta(y_i|v_d, t, y_{<i})]$ (2) where d and \u03b1 indicate distortion operation and hyperparameter, respectively. Generally, CD methods employ an adaptive plausibility constraint to calibrate the entire output distribution, preventing implausible outputs from the augmented distribution [37-40, 52-54]: $V_{token}(y_{<i}) = \\{y_i| y_i \\in \\nu: p_\\theta(y_i|v, t, y_{<i}) \\ge \\beta \\underset{w}{max} p_\\theta(w|v,t,y_{<i})\\},$ (3) $p_{cd}(y_i|v, v_d, t, y_{<i})) = 0, \\text{ if } y_i \\notin V_{token}(y_{<i})$ where \u03bd is the output vocabulary set and $V_{token}$ is the set of selected tokens. The hyperparameter \u03b2 controls the strength of truncation, with larger \u03b2 indicating more aggressive truncation that retains only high-probability tokens. However, we argue that manually disturbing raw inputs might not trigger the desired hallucinations, while holistic disturbances will bring uncertainty noise that compromises the normal decoding. To validate our claim, we analyze the performances of normal decoding, VCD, and ICD using the Polling-based Object Probing Evaluation (POPE) metric, under sampling and greedy decoding settings. POPE quantitatively converts the hallucination evaluation into a binary classification problem by using the question format to prompt the model: \u2018Is there a <object> in the image?'\u201d, with the expected answers with 'Yes' or 'No'. From Table 1, under the greedy decoding setting, CD methods improve performance in the adversarial setting, which are more challenging as they prioritize co-occurring confusing objects. CD methods achieve this by exacerbating and subtracting hallucinated concepts from the original distribution. However, in random settings, where objects absent from the image are chosen randomly and are easily recognized, CD methods slightly underperform compared to normal greedy decoding. This indicates that the highest logit of the correct token is compromised during contrastive decoding. In the sampling decoding setting, CD methods obviously outperform the normal sampling strategy. However, CD methods rely on the adaptive plausibility constraint (Eq. 3) to filter out low-probability tokens. Without Eq. 3, CD methods are inferior to normal decoding in both random and adversarial settings, which validates vision-and-text-agnostic input distributions induces potential uncertainty noise after Eq. 2. To address the aforementioned issues, we propose a decoding strategy named Self-Introspective Decoding (SID). SID amplifies vision-and-text association hallucinations to guide LVLMs in exploring factualness based on text and vision inputs by using a Context and Text-aware Token Selection (CT2S) strategy, which will be explained later. Details of the SID are illustrated in the Section 3.3 and Fig. 2."}, {"title": "3.3 Self-Introspective Decoding", "content": "Understanding the Self-Introspective Pre-trained LVLMs. LLMs [1-6] have been scaled up to billions of paramters and pre-trained on trillions of tokens, endowing LLMs with encyclopedic ability like in-context learning [59], zero [60]/few-shot [61] ability. LVLMs extend LLMs to multimodal understanding capabilities by visual instruction tuning. Some works [62-64] pointed out that vision information is redundant in LVLMs, and develop vision token reduction technologies to prune [65] and merge [66] tokens guided by importance metrics without further re-training. Regarding the hallucination issue, we argue that vision tokens with low attention scores in the self-attention module induce vision-and-text association hallucination. Formally, for the transformer block [67] in the auto-regressive decoder 1, text instruction (t), vision (v), and generated tokens (g) are concatenated and projected into three distinct vectors: the query vector Q, the key vector K, and the value vector V, utilizing three linear transformations Wq, Wk, and W. The self-attention (SA) mechanism computes the relevance of each item to other items as follows: $R = SA(Q, K, V) = A \u00b7 V,$ (4) $A = \\text{softmax}(\\frac{Q\u00b7K^T}{\\sqrt{d_i}} + M)$ where di represents the dimension of Q, K, V, M represents the casual mask. A \u2208 R(b,h,n,n), where b, h, and n denote batch size, number of key-value heads, and total token number, respectively. We denote the $A_i$ as the attention matrix after Layer i of LVLMs. Then we calculate vision token importance scores (Scorei(v)) as shown in Fig. 2 (Selector) based on Ai: $Score_i(v) = \\frac{1}{h} \\sum_{j=1}^{h} A_i^{(j)}[v]$ (5)"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings Models. We conduct experiments on four representative LVLMs for evaluation: InstructBLIP [13], Shikra[41], LLaVA-1.5 [14], and LLaVA-NeXT [17]. InstructBLIP employs Q-former [12] to encode 32 image tokens to feed into LVLMs, while Shikra, LLaVA-1.5, and LLaVA-NeXT directly leverage linear projection layers as vision-language connectors to align multimodal features. Shikra and LLaVA-1.5 encode 256 and 576 image tokens to LVLMs. LLaVA-NeXT increases the input vision resolution by 4x to capture more visual details, resulting in 4\u00d7 more encoded vision tokens than LLaVA-1.5. All LVLMs utilize pre-trained vision encoders like CLIP [71] vision encoder, as well as pre-trained LLMs as language decoders, such as Vicuna v1.1 [3], LLaMA 2 [5], and recently released LLaMA 3 [6]. We provide results at the 7 Billion (B) scale, and larger-scale results are in the Sec. 5."}, {"title": "Baselines", "content": "Since our method aims to propose decoding strategies in a training-free manner, we compare six decoding methods: Sampling (Top-p=1) Decoding, Greedy Decoding, Visual Contrastive Decoding (VCD) [37], Instruction Contrastive Decoding (ICD) [38], Beam Search Decoding [55], and beam-search-based OPERA [25]. For comprehensive comparisons, we apply contrastive decoding methods in both sampling (Top-p=1) and greedy decoding settings. Additionally, we equip SID with different decoding strategies, including beam search decoding and Top-p sampling, as detailed in Sec. 5."}, {"title": "Implementation Details", "content": "As analyzed in Sec. 3.3, we generally set Layer i=3 and preserve top 10% least important vision tokens for Shikra, LLaVA-1.5, and LLaVA-NeXT and i=5 and top 10% least important vision tokens for Q-former based LVLMS (InstructBLIP) to induce vision-and-text association hallucinations. The hyper-parameters in Eq. 2 and 3 follow [37, 38]. Other parameters use default settings. Due to amplified fine-grained hallucinations, SID is more robust to hyperparameters compared to other CD methods (Sec. 5). Experiments are performed on NVIDIA V100/A100 GPUs."}, {"title": "4.2 Quantitative Evaluations", "content": "In this section, we follow previous methods [25, 37, 38] to evaluate the performances of SID in alleviating hallucinations on both discrimination and open-end generative tasks. Besides manually designed metrics, we also leverage both GPT-4 and GPT4-V to evaluate hallucination alleviation and generated text quality. POPE Evaluations. The Polling-based Object Probing Evaluation (POPE) [68] was recently developed to assess hallucination problems in LVLMs. POPE queries the LVLMs with the template: Is there a <object> in the image? The ratio between queries about existing and no-existing objects is balanced (i.e., 50%-50%). This benchmark consists of three sampling settings: random, popular, and adversarial, each differing in the construction of negative samples. Specially, In the random setting, objects that are not present in the image are selected at random. The popular setting selects missing objects from the high-frequency pool, whereas in the adversarial setting, co-occurring objects that are not present in the image are prioritized. POPE consists of three different datasets, including MSCOCO [58], A-OKVQA [72], and GQA [73]. POPE involves 500 images from each dataset with 6 questions each, ultimately yielding 27,000 query-answer pairs. Accuracy and F1 score are chosen as evaluation metrics. As shown in Table 3, our method achieves the best results among random, popular, and adversarial sampling settings. Concretely, in the sampling decoding situation, SID surpasses the normal sampling decoding by a large margin in a train-free manner. SID also clearly outperforms CD methods (ICD [38] and VCD [37]) because the self-introspective decoding strategy amplifies vision-and-text association hallucinations then subtracts them, rather than coarsely disturbing raw inputs. Additionally, owing to the context and text-aware token selection strategy, SID is more computation-efficient than CD methods, as analyzed in Table 6. Note that for binary discrimination task, Beam Search and beam-search based OPERA [25] obtain marginal gains when addressing hallucinations in short answers, primarily because answering"}, {"title": "Different Token Selection Strategies", "content": "To validate the effectiveness of SID in selecting low attention scores to induce vision-and-text association hallucination, we further conduct quantitative experiments under different vision token selection strategies the same preserved vision token number and Layer i=3 as ours. Table 7 shows that vision tokens with high attention score degrades obviously, as it does not amplify contextual hallucinations rather than retain original important information. Contrastive decoding does not benefits from subtracting hallucinations amplified by the disturbed inputs rather than suffers from the potential disturbance noise. Selecting random vision tokens brings improvements in the adversarial setting, because randomly selected vision tokens amplify the over-reliance on statistical bias and language priors, similar to Vision CD [37] and Instruction CD [38]. However, token-level random disturbance also induces uncertainty noise, resulting in the inferior performance in the random setting to greedy decoding. Overall, these experiments further validate the rationality of our token selection strategy based on attention sores."}, {"title": "Hyper-parameter Sensitivity", "content": "Beyond analyzing the sensitivity of pruning layer i and ratio in Fig 6, we validate the robustness of SID concerning \u03b1 and \u03b2 of Eq. 2 and 3, compared to other contrastive decoding methods like VCD. From Fig. 9 (left), it is evident that as \u03b1 decreases, the contrastive decoding mechanism diminishes. However, SID also achieves pleasant results while VCD degrades close to Sampling when a=0.1 as the CT2S strategy induces high-intensity vision-and-text association hallucinations. When \u03b1 increases, VCD degrades to some extent because holistic input disturbance does not always trigger contextual-related hallucination and may exacerbate uncertainty noise. Regarding \u03b2, a larger \u03b2 indicates more aggressive truncation of output vocabulary. Fig. 9 (right) shows that VCD's performance heavily relies on large \u03b2 to retain only high-probability tokens. With mild or no adaptive plausibility constraint (Eq. 3), VCD performs worse than the sample decoding strategy due to output logits influenced by distorted visual inputs. Ours is robust to the \u03b2 setting because the CT2S strategy induces discriminative contrastive logits to generate plausible tokens."}, {"title": "Larger-scale Backbone", "content": "We validate the effectiveness of SID in terms of 13B scale backbones on LLaVA-1.5 and InstructBLIP architectures. We choose POPE [68] and CHAIR [45] to validate the hallucination issues in both discrimination and open-end generation tasks. Table 8 shows that SID remains effective as backbone networks scale up."}, {"title": "Adopting Other Decoding Strategies", "content": "Meanwhile, besides direct sampling and greedy decoding, we conduct experiments on LLaVA-1.5 7B using the MSCOCO dataset with various decoding strategies, including Top-p sampling (p=0.9), Top-k"}, {"title": "6 Conclusion", "content": "In this paper, we alleviate the hallucination issue in Large Vision Language Models (LVLMs). We re-think contrastive decoding in LVLMs and find that vision and text-agnostic input distributions in CD do not always amplify desired hallucinations rather than induce potential uncertainty noise. To mitigate these issues, we propose a decoding strategy named Self-Introspective Decoding (SID). SID amplifies vision-and-text association hallucinations to guide LVLMs contrastive decoding in improving faithfulness by developing Context and Text-aware Token Selection (CT2S) strategy. SID is train-free and easy to integrated into various decoding strategies and backbones. Extensive experiments validate the effectiveness and robustness of SID. Codes are available at https://github.com/huofushuo/SID."}, {"title": "7 Data Availability Statement", "content": "The data supporting the results of this study are publicly available at MSCOCO (https://cocodataset.org/) GQA (https://cs.stanford.edu/people/dorarad/gqa/about.html), A-OKVQA (https://github.com/allenai/aokvqa), LLaVA-Bench-Wild (https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild), Visual Genome (https://homes.cs.washington.edu/ ranjay/visualgenome/api.html). We release our codes at https://github.com/huofushuo/SID for reproduction."}, {"title": "8 Appendix", "content": "Case Study. We qualitatively present several case studies of SID's hallucination alleviation ability compared to Greedy decoding, VCD [37], and OPERA [25] on LLaVA-1.5 7B. Examples shown in Fig. 14, 15, and 16 are from LLaVA-Bench-in-the-Wild dataset [7]."}]}