{"title": "Riemannian Manifold Learning for Stackelberg Games with Neural Flow Representations", "authors": ["Larkin Liu", "Kashif Rasul", "Yutong Chao", "Jalal Etesami"], "abstract": "We present a novel framework for online learning in Stackelberg general-sum games, where two agents, the leader and follower, engage in sequential turn-based interactions. At the core of this approach is a learned diffeomorphism that maps the joint action space to a smooth Riemannian manifold, referred to as the Stackelberg manifold. This mapping, facilitated by neural normalizing flows, ensures the formation of tractable isoplanar subspaces, enabling efficient techniques for online learning. By assuming linearity between the agents' reward functions on the Stackelberg manifold, our construct allows the application of standard bandit algorithms. We then provide a rigorous theoretical basis for regret minimization on convex manifolds and establish finite-time bounds on simple regret for learning Stackelberg equilibria. This integration of manifold learning into game theory uncovers a previously unrecognized potential for neural normalizing flows as an effective tool for multi-agent learning. We present empirical results demonstrating the effectiveness of our approach compared to standard baselines, with applications spanning domains such as cybersecurity and economic supply chain optimization.", "sections": [{"title": "1 Introduction", "content": "A Stackelberg game consists of a sequential decision-making process involving two agents, a leader and a follower. This framework, introduced in [Sta34] models hierarchical strategic interactions where the leader moves first, anticipating the follower's best response, and then the follower reacts accordingly. These games have become central to understanding interactions in various fields, from economics to societal security, providing a formal method for analyzing situations where one party commits to a strategy before the other, affecting the subsequent decision-making process and reward outcomes. Over time, Stackelberg games have evolved to address more complex environments, incorporating factors like imperfect information and no-regret learning of system parameters. The solution to such a game typically revolves around finding a Stackelberg equilibrium, where the leader optimizes his strategy assuming or knowing the follower type, which affects how she optimizes her utility based on the leader's action. [KCP10; Kar+15].\nSeveral challenges arise in the practical applications of Stackelberg games. One key issue is the uncertainty regarding the follower's type or rationality (or sub-rationality). In many real-world scenarios, the follower might not be fully rational or the leader might have incomplete knowledge of the follower's preferences, leading to uncertainty in the leader's decision-making process. Additionally, imperfect information regarding reward outcomes adds another layer of complexity, as the leader may not have accurate knowledge of the payoffs associated with various strategies. These uncertainties have been addressed in domains such as security, where randomized strategies and robust optimization approaches are deployed to mitigate risks arising from incomplete information and unpredictable follower behaviour [Jia+13; Kar+15; Kar+17]. For instance, in deployed systems like ARMOR at LAX"}, {"title": "2 Formal Definitions", "content": "In a Stackelberg game, two players take turns executing their actions. Player A is the leader, she acts first with action a selected from her action space A. Player B is the follower, he acts second with action b \u2208 B. The follower acts in response to the leader's action, and both players earn a joint payoff as a function of their actions."}, {"title": "2.1 Repeated Stackelberg Games", "content": "In a repeated Stackelberg game, the leader chooses actions $a_t \\in A$, and the follower reacts with actions $b_t \\in B$ at each round $t = 1, 2, ..., T$. The leader's strategy $\\pi_A(\\cdot|H_t)$ is a probability distribution over the action space A which selects $a_t$ based on past joint actions up to time t, i.e., $H_t = \\{(a_\\tau, b_\\tau)|\\tau < t\\}$. Similarly, the follower's strategy $\\pi_B(\\cdot|H_t)$ is a conditional probability distribution over B which determines $b_t$ given the full history, i.e., $H_t = H_t \\cup \\{a_t\\}$.\nBest Response Strategy of the Follower: To be specific, the follower selects his best response strategy at round t by maximizing his expected reward function $\\mu_B(a, b) : A \\times B \\rightarrow \\mathbb{R}$ given that the"}, {"title": "2.2 The Stackelberg Manifold", "content": "To address the complexity of solving for Stackelberg equilibrium under uncertainty, we propose the idea of mapping actions from the ambient space onto a manifold leading to several key advantages. Simplifying the problem by mapping to a geometric structure, such as a unit sphere, allows for significantly faster numerical computation while optimizing directly on an intuitive intrinsic geometry, reducing redundancies and providing ease with respect to enforcing constraints. Additionally, smoothness on such a structure enables computational advantages through methods like Riemannian gradient descent [Bon13], which exploits differentiability for efficient optimization. The core idea, is to design a method that shifts the complexity of learning in Stackelberg games by constructing a mapping that transforms the data from its native action space to a more convenient representation. In terms of trade-offs computational efficiency, by constructing this manifold map, we trade off the complexity of classical multi-agent learning for the complexity of learning a neural representation for \u03a6.\nThis concept of mapping the data from the ambient space, in our case defined by the joint action space $A \\times B$, onto a latent space has been explored in several prior works. For a well defined manifold, typically the approach is to learn a diffeomorphism between the ambient data space, and the objective manifold, which is a subspace of the ambient data space [Rez+20] [GRM16]. Suppose the manifold is not given, or there lies flexibility in defining the structure of such a manifold, the certain manifold learning techniques could be devised [BC20]. These approaches typically define invertible, or pesudo-invertible, probability density maps between the ambient data space, the latent space, and the manifold space."}, {"title": "2.2.1 Normalizing Flows for Joint Action Space Projection", "content": "We leverage normalizing flows to map a joint action space $A \\times B \\subset \\mathbb{R}^D$ onto a manifold, $\\Phi$ embedded in $\\mathbb{R}^D$ [RM15; DSB16; Pap+21]. Normalizing flows are a class of generative models that transform a high dimensional simple distribution (i.e., isotropic Gaussian) into a complex one through a series of invertible bijective mappings using neural networks that are computationally tractable. The joint action space consists of actions taken by two agents, denoted as $a \\in A$ and $b \\in B$, modelled via"}, {"title": "2.2.2 Specifications of the Feature Map (a, b)", "content": "Feature Map $(.):$ We propose a function $\\phi$, which is a feature map [Zan+21; Mor+22; AAT19]. Let $|A|$ and $|B|$ denote the finite dimension of the action space of the leader and follower respectively, the feature map $\\phi : A \\times B \\rightarrow \\mathbb{R}^D$, which effectively maps any A by B combination of vectors to a D dimensional feature representation.\nFurther, we introduce a concept known as the Stackelberg embedding, denoted by $\\Phi$, which is defined as the image of $\\phi$ over the joint action space domain $A \\times B$,\n$\\Phi = Im(\\phi) = \\{(\\phi(a, b)| a \\in A, b \\in B\\}.$  (2.6)\nThe construction of $\\phi : A \\times B \\rightarrow \\mathbb{R}^D$ can be via any means, in our case a normalizing neural flow network (but possibly any other architecture), but should abide by the imposed assumptions in Table 1. To be precise, $\\hat{\\phi}$ should denote our best learned representation of the ideal map $\\phi$. Provided that we only have access to $\\hat{\\phi}$, purely for notational convenience, we will use $\\phi$ to represent $\\hat{\\phi}$ moving forward.\nDefinition 2.1. Bipartite Spherical Map $\\phi(a, b):$ Let $a \\in A$ and $b \\in B$, and define a mapping $\\phi : A \\times B \\rightarrow \\mathbb{S}^{(D-1)}$ from Cartesian coordinates to spherical coordinates on the D-dimensional unit sphere $\\mathbb{S}^{(D-1)}$. The spherical coordinates are partitioned such that, $v_a(a)$ parametrizes a subset of the spherical coordinates, and $v_b(b)$ parametrizes the remaining coordinates. Also, $v_a \\cap v_b = \\emptyset$, meaning the partitions are disjoint. Thus, the full mapping is given by:\n$\\phi(a, b) = (v_a(a), v_b(b)) \\in \\mathbb{S}^{(D-1)},$\nwhere $v_a$ and $v_b$ represent distinct angular components of the spherical coordinates.\nMapping to a Spherical Manifold: The transformation from spherical coordinates to Cartesian coordinates is used to map input features onto an D-dimensional spherical manifold. Therefore, in addition to the properties of our feature map $\\phi$ from Table 1, we also enforce $\\phi$ as a bipartite spherical map from Def. 2.1. This bipartite spherical map which constructs a disjoint spherical mapping to parameterize two subspaces in $\\Phi$. To accomplish this, we define two heads in the neural network input, the head from A specifically controls the azimuthal spherical coordinate and the head from B specifically controls other coordinates. The justification for this mapping involves a fundamental trade-off between learning an optimal embedding and reducing the complexity inherent in the native multi-agent learning problem. When specific multi-agent optimization problems are too complex to solve in their native forms (see Section 5 for examples), we leverage normalizing flows as an enabling link [BC20; Dur+20] to transform the problem into a simpler representation. (A visualization of the empirical mapping results, showcasing the learned bipartite mapping to $\\Phi$ as a 3D spherical surface, is provided in Appendix E.1 and E.2. This visualization is generated by varying a or b to create longitudinal or latitudinal subspaces.)"}, {"title": "2.3 Reward Function", "content": "Reward Mechanisms: A Stackelberg game provides two reward functions $\\mu_A(a, b)$ and $\\mu_B(a, b)$. Both of which are linearizable with sub-Gaussian noises, $\\epsilon_A$ and $\\epsilon_B$, i.e.,\n$\\mu_A(a, b) = (\\theta, \\phi(a, b)) + \\epsilon_A,$\n  (2.15)\n$\\mu_B(a, b) = (\\theta, \\phi(a, b)) + \\epsilon_B.$\n  (2.16)\nWe assume zero-mean sub-Gaussian distribution for both $\\epsilon_A$ and $\\epsilon_B$ but they do not necessarily need to be identical. The objective is to learn the parameters $\\theta \\in \\mathbb{R}^D$, and possibly as an extension problem $\\theta$. The feature map $\\phi(\\cdot)$ maps the joint action space $A \\times B$, to a subspace in $\\mathbb{R}^D$. The parameters of the model, can be estimated via parameterized regression,\n$\\Theta_t = (\\Phi_{1:t}\\Phi_{1:t}^T + \\Lambda_{reg}I)^{-1}\\Phi_{1:t}\\mu_{1:t},$\n  (2.17)\nfor A and B, respectively.\nWhere $\\Phi_{1:t}$ represents the sequence of $\\phi(\\cdot)$ values via the feature map given the action sequences $a_{1:t}$ and $b_{1:t}$, $\\Lambda_{reg}$ serves as a regularization parameter, $I$ is the identity matrix, and $\\mu_{1:t}$ are the historical rewards of players A or B (depending on the subscript). Here, we extend the reward structure of classical linear bandits in [APS11; Chu+11] to a setting where two players jointly decide on the action sequence. We stipulate assumptions to ensure that the covariance matrix $\\Sigma_t^{-1}$ is well-conditioned and positive semi-definite (PSD), with a regularization parameter $\\Lambda_{reg}$ balancing bias and variance, while the norm $|\\phi(a_t, b_t)||_2^1$ must remain small to facilitate efficient uncertainty reduction. (These assumptions are outlined in detail in Appendix A.2.)\nLinearity by Design: This formulation in Eq. (2.15) and Eq. (2.16) not only simplifies its analysis but also enables the derivation of theoretical guarantees for online learning, particularly with respect to convergence to the optimal solution. The problem is framed as a linear bandit construct [Chu+11; CL06; LS20], a well-established setting with strong theoretical foundations. Additionally, the multi-agent aspect aligns with game-theoretic online learning frameworks explored in [LR24; Zha+23; Hag+22; Bai+21]. The methodology involves mapping the action space to a structured space $(.)$. This linearity is analogous to the role of the final layer in a neural network, where the relationship between the ambient space (or joint action space) and the embedding space is learned. Subsequently, a linear set of weights maps the embedding space to the reward space. The existence of such a mapping, $(.)$, with valid linear parameters stems from the bijectiveness, smoothness, and unrestricted complexity of $\\phi(\\cdot)$. We provide further discussion of this in Appendix C.2."}, {"title": "3 Optimization of Stackelberg Games", "content": "Optimization under Perfect Information: We see that regardless of the convexity of A or B, so long as we are dealing with compact spaces, under perfect information, we can solve the Stackelberg equilibrium by solving a bilevel optimization problem expressed as,\n$\\pi_A^* = \\arg \\max_{\\pi_A \\in \\Pi_A} \\mathbb{E} [(\\theta^A, \\phi(\\pi_A, \\pi_B(\\pi_A)))],$  (3.1)\n$\\pi_B(\\pi_A) = \\arg \\max_{\\pi_B \\in \\Pi_B} \\mathbb{E} [(\\theta^B, \\phi(\\pi_A, \\pi_B))],$  (3.2)"}, {"title": "4 Online Learning on the Stackelberg Manifold", "content": "To enable efficient multi-agent online learning on the Stackelberg manifold, $\\Phi$, we enforce $\\Phi$ to be a convex manifold. The convex manifold is a manifold where the geodesic between any two points on the manifold is contained within or forms a geodesically convex set (Def. C.1). Essentially, in a convex manifold, every geodesic between two points is contained within the manifold, adhering to the geodesic convexity property. The formal definitions can be found in Appendix C.1."}, {"title": "4.1 Stackelberg Optimization under Perfect Information", "content": "Provided that we can transform data from the joint action space (or ambient data space) onto a spherical manifold, we can leverage the properties of the D-sphere to determine the best response solution for the Stackelberg follower and optimize the corresponding Stackelberg regret. Consider the reward function structure outlined in Section 2.3. In general, for each agent, $\\mu = (\\theta, \\phi)$. Here, $\\theta$ represents a D-dimensional vector in the manifold space, and we must find the element in $\\Phi$ that maximizes this inner product. In the Stackelberg game, since the leader moves first, they define a restricted subspace on the $\\Phi$. The follower must then optimize within this subspace. Moving forward, $\\theta_A$ and $\\theta_B$ will be referred to as objective vectors.\nWe define the divergence angle, $\\alpha_{Div}$ as the angle between the two objective vectors. Further, we define the geodesic distance between two vectors, denoted as $G(\\theta_A, \\theta_B)$, for a unit-spherical manifold, as follows,\n$\\cos(\\alpha_{Div}) = \\frac{(\\theta_A, \\theta_B)}{||\\theta_A|| ||\\theta_B||},$    (4.1)"}, {"title": "4.2 Regret Definitions", "content": "Definition 4.1. Stackelberg Regret: We define Stackelberg regret, denoted as $R_t^A$ for the leader, measuring the difference in cumulative rewards between a best responding follower and an optimal leader in a perfect information setting, against best responding follower and leader exhibiting bounded rationality. The leader policy stipulates that the she acts rationally given the estimates of the expected reward function from the data gathered, as in Eq. (3.7) and Eq. (3.6),\n$R_t^A = \\sum_{t=1}^T \\mathbb{E} [ \\max_{a \\in A} \\mu_A(a, B(a)) - \\mu_A(a^t, B(a^t))] \\leq \\sum_{t=1}^T (H(\\theta^*, t) - H(\\theta_t, t)).$\n  (4.2)\nThe leader selects $a^t$ from policy $\\pi_A$ according to their best estimate of $\\theta_A$ and $\\theta_B$, following the maximization equations in Eq. (3.4) and Eq. (3.5) respectively.\nThe leader commits to a strategy $\\pi_A$ aimed at maximizing her reward while accounting for the uncertainty in the follower's response. The leader is free to estimate the follower's response rationally, and within the confidence interval. Our algorithm minimizes the Stackelberg regret, providing a no-regret learning process for the leader. To compute the Stackelberg regret of the algorithm, which is defined from the leader's perspective, we must derive a closed form expression for the gap over time between the expected reward under the optimal policy and the expected reward under any algorithm.\nDefinition 4.2. Simple Regret: We define the simple regret, where with probability $1-\\delta$ at time t,\n$\\text{reg}(t) =  (\\theta, \\phi(a^*, B(a^*))) - (\\theta_t, \\phi(a^*, B(a^*))) \\leq H(\\theta^*, t) - H(\\theta_t, t)$ (4.3)\nThis assumes that the leader is acting under the bounded rationality assumption."}, {"title": "4.3 Quantifying Uncertainty on the Stackelberg Manifold", "content": "We now revisit the parameter uncertainty constraints introduced in Sec. 2.3, which dictate the uncertainty of a given learning algorithm, characterized by an uncertainty radius $C_0(t)$. Given the feature map $\\phi(\\cdot)$, which adheres to the linear reward assumptions, particularly with respect to the covariance matrix of the regression (as outlined in Sec. 2.3), the learning leader can apply any bandit learning algorithm that imposes a high-probability bound on the parameter estimate. This constraint is formalized in Eq. (3.3) by the uncertainty region $C_0(t)$. Let us define $\\mathbb{T}_a$ and $\\mathbb{T}_b$ as two subspaces, which we will use to analyze the leader's actions under these uncertainty constraints.\n$\\mathbb{T}_a = \\{(\\phi(a, b')|b' \\in B\\},$\n (4.4)\n$\\mathbb{T}_b = \\{(\\phi(a', b)| a' \\in A\\},$\n (4.5)\nwhere $\\mathbb{T}_a$ and $\\mathbb{T}_b$ are the sub-spaces formed when we fix one of the leader or follower's action, and let the other action vary freely.\nLemma 4.3. Intersection of $\\mathbb{T}_a$ and $\\mathbb{T}_b:$ Given a bipartite spherical map $\\phi(\\cdot)$ from Definition 2.1, with a parameterizing the azimuthal (latitudinal) coordinates, the cardinality of the intersect between $\\mathbb{T}_a$ and $\\mathbb{T}_b$ will be non-empty. That is, $|\\mathbb{T}_a \\cap \\mathbb{T}_b| > 0$.\nThe purpose of Lemma 4.3 is to highlight that, given the bipartite map in Def. 2.1, subspaces are guaranteed to intersect on the manifold. This is easy to visualize on a spherical manifold in the 2-sphere setting (e.g., longitudinal and latitudinal lines) but becomes challenging to intuit in higher dimensions. We rigorously argue that, just as in the 2-sphere case, the same principle holds in a D-sphere setting. The derivation of Lemma 4.3 first comes by isolating the subspaces in terms of angular coordinates. Next, due to the Poincare-Hopf theorem [Poi85; Hop27], the compactness of the smooth Riemmanian manifold imposes strong geometric constraints such that the two subspaces cannot avoid each other.\nLemma 4.4. Orthogonality of Subspaces $\\mathbb{T}_a$ and $\\mathbb{T}_b:$ The two submanifolds $\\mathbb{T}_a$ and $\\mathbb{T}_b,$ are orthogonal to each other within $\\Phi$.\nLemma 4.4 is proven by isolating and taking the partial derivatives of the cartesian coordinates with respect to their spherical coordinates to obtain tangent vectors. Afterwards, by computing the dot product between these two tangents and demonstrating that it equates to 0, we establish their orthogonality.\nGeodesic Isoplanar Subspace Alignment (GISA): The general methodology in which we can compute the optimal leader strategy for a Stackelberg game, for manifold that forms a convex boundary, is that the leader can anticipate the follower strategy based on knowledge of follower's reward parameters $\\theta'_B$ and the isoplane $\\mathbb{T}_a$. We denote this homeomorphism as $f_1(\\mathbb{T}_a, \\theta_B) : \\mathbb{T}_a \\rightarrow \\mathbb{T}_b$.\nThereafter, we compute the geodesic distance minimizing distance from $\\mathbb{T}_b$ to $\\theta'_b$ via injective map $f_2(\\mathbb{T}_b) : \\mathbb{T}_b \\rightarrow \\mathbb{R}$. Leader's objective is to find $a \\in A$ such that it minimizes the composition of $f_1 \\circ f_2$, giving us the geodesic distance. This composition is abstractly defined as,\n$G(a, b) \\in \\mathbb{R}, \\text{ where, } \\theta' = \\frac{\\theta}{|\\theta||},$ for A and B.  (4.6)\nTheorem 1. Isoplane Stackelberg Regret: For D-dimensional spherical manifolds embedded in $\\mathbb{R}^D$ space, where $\\phi(a, \\cdot)$ generates an isoplanes $\\mathbb{T}_a$, and the linear relationship to the reward function in Eq. (2.15) and Eq. (2.16) and Eq. (2.15) and Eq. (2.16), the simple regret, defined in Eq. (4.3), of any learning algorithm with uncertainty parameter uncertainty $C_0(t)$, refer to in Eq. (3.3), is bounded by $O(\\arccos(1 - C_0(t)^2/2))$.\nThe proof of Theorem 1 focuses on analyzing the geodesic distances on $\\Phi$ due to uncertainty. First, we argue that any norm-like confidence ball in Cartesian coordinates, Ball(.), can be transformed into"}, {"title": "5 Empirical Experiments", "content": "We provide three practical instances of Stackelberg games in practice. We benchmark the GISA from Algorithm 1 against a dual-UCB algorithm, where both agents are running a UCB algorithm. Although a simplistic, benchmark, the dual-UCB algorithm does constitute a no-regret learning algorithm [BM07]. The key concept is to abstract away the need for knowledge regard exact reward structure of the problem by leveraging our transform, allowing for the learning of well-behaved representations suitable for online learning that can adapt to new problem settings without requiring exact specifications of the problem structure. Our work addresses the limitations of previous solution algorithms, which are often problem-specific, by overcoming the challenges associated with Stackelberg game learning methods that lack closed-form solutions or are computationally intractable.\nR\u00b9 Stackelberg Game: In this Stackelberg game, the leader selects an action while anticipating the follower's best response. The action spaces of both the leader and the follower are one-dimensional,"}, {"title": "6 Conclusion", "content": "This work establishes a foundational connection between Stackelberg games and normalizing neural flows, marking a significant advancement in the study of equilibrium learning and manifold learning. By utilizing normalizing flows to map joint action spaces onto Riemannian manifolds, particularly spherical ones, we offer a novel, theoretically grounded framework with formal guarantees on simple regret. This approach represents the first application of normalizing flows in game-theoretic settings, specifically Stackelberg games, thereby opening new avenues for learning on convex manifolds. Our empirical results, grounded in realistic simulation scenarios, highlight promising improvements in both computational efficiency and regret minimization, underscoring the broad potential of this methodology across multiple domains in economics and engineering. Despite potential challenges related to numerical accuracy for the neural flow network, this integration of manifold learning into game theory nevertheless exhibits strong implications for online learning, positioning neural flows as a promising tool for both machine learning and strategic decision-making."}, {"title": "A Key Assumptions and Definitions", "content": null}, {"title": "A.1 Compact and Closed Sets", "content": "In this formal definition, $\\Phi$ is both compact and closed in the product space A\u00d7B. A set is compact if for every open cover $\\{U_i\\}_{i\\in I}$ of $\\Phi$, there exists a finite subcover such that $\\Phi \\subseteq \\bigcup_{k=1}^{K} U_{i_k}$, where $U_{i_k}$ are open sets in A \u00d7 B. This ensures that $\\Phi$ is \"contained\" in a finite manner within the space, even if A \u00d7 B is infinite. Furthermore, $\\Phi$ is closed if its complement, $\\Phi^c = (A \\times B) \\setminus \\Phi$, is open. This implies that $\\Phi$ contains all its limit points, making it a complete set within the topological space. Thus, $\\Phi$ is a compact and closed subset of A \u00d7 B, meaning that it is both bounded and contains its boundary, providing useful properties for convergence and stability in this space.\n$\\forall \\{U_i\\}_{i\\in I}, \\Phi \\subseteq \\bigcup_{i \\in I} U_i  \\exists \\{U_1, U_2,..., U_{i_n}\\} \\text{ such that } \\Phi \\sube \\bigcup_{k=1}^{n} U_{i_k} (A \\times B) \\setminus \\Phi \\text{ is open.} $ (A.1)"}, {"title": "A.2 Assumptions on Linear Reward Function", "content": "1. Covariance Matrix:\n$\\Sigma_t = \\sum_{t=1}^T \\phi(a^t, b^t)\\phi(a^t, b^t)^T + \\Lambda_{reg}I$  (A.2)\n$\\phi(a^t, b^t)$ must ensure that the covariance matrix $\\Sigma_t^{-1}$ (a.k.a. the inverse of the covariance matrix) is sufficiently large for effective learning.\n2. Norm Bounds:\n$||\\phi(a^t, b^t)||_{\\Sigma_t^{-1}} = \\sqrt{\\phi(a^t, b^t)^T \\Sigma_t^{-1} \\phi(a^t, b^t)}$  (A.3)\n$||\\phi(a^*, b^t)||_{\\Sigma_t^{-1}}^2$ must be small to ensure efficient uncertainty reduction.\n3. Regularization Effect: Regularization parameter $\\Lambda_{reg}$ balances bias and variance, affecting sample complexity.\n4. Positive Semi-Definiteness: $\\Sigma_t^{-1}$ is positive semi-definite (PSD)."}, {"title": "A.3 Discrete Measure Interpretation", "content": "Let $\\{x_1, x_2, ..., x_n\\}$ be a set of discrete points in $\\mathbb{R}^n$. We define the measure $\\alpha$ on these points as,\n$\\alpha = \\sum_{i=1}^{n} \\alpha(\\{x_i\\}) \\delta_{x_i}$  (A.4)\nwhere $\\delta_{x_i}$ is the Dirac measure centered at $x_i$. The integral of a function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ with respect to the measure $\\alpha$ is given by,\n$\\int_{\\mathbb{R}^n} f(x) d\\alpha(x) = \\sum_{i=1}^{k} \\alpha(\\{x\\}) f(x_i)$ (A.5)"}, {"title": "A.4 Definition of Riemann Manifold", "content": "A Riemannian manifold, expressed as $\\Phi$, consists of a smooth manifold equipped with a smoothly varying collection of inner products $w_p$ on each tangent space $T_p \\Phi$ at every point $p \\in \\Phi$. This assignment $w_p : T_p\\Phi \\times T_p\\Phi \\rightarrow \\mathbb{R}$ is positive-definite, meaning it measures angles and lengths in a consistent and non-degenerate manner. Consequently, each vector $v \\in T\\Phi$ inherits a smoothly defined norm $||V||_p = \\sqrt{w_p(v,v)}$. This structure allows $\\Phi$ to possess a locally varying yet smoothly coherent geometric framework."}, {"title": "A.5 Stochastic Perturbation Function", "content": "To model uncertainty in the joint action space, we introduce a stochastic perturbation over the leader and follower actions. Specifically, we define a small, one-step random perturbation function J(a, b), where $a \\in \\mathbb{R}^m$ and $b \\in \\mathbb{R}^n$ are the actions of the leader and follower, respectively. The perturbed joint action is given by:\n$J(a, b) = (a', b') = (a + \\epsilon_a, b + \\epsilon_b)$ (A.6)\nwhere $\\epsilon_a \\in \\mathbb{R}^m$ and $\\epsilon_b \\in \\mathbb{R}^n$ are independent Gaussian perturbations with zero mean and variance $\\sigma_a^2$ and $\\sigma_b^2$, respectively:\n$\\epsilon_a \\sim \\mathcal{N}(0, \\sigma_a^2 I_m), \\epsilon_b \\sim \\mathcal{N}(0, \\sigma_b^2 I_n)$  (A.7)\nHere, $\\sigma_a$ and $\\sigma_b$ are scalar diffusion parameters controlling the magnitude of the perturbation, and $I_m$ and $I_n$ are identity matrices of size m \u00d7m and n\u00d7 n, ensuring isotropic perturbations in each component of a and b.\nIn component form, this perturbation can be written as:\n$a'_i = a_i + \\epsilon_{a_i}, \\quad \\epsilon_{a_i} \\sim \\mathcal{N}(0, \\sigma_a^2)$ (A.8)\n$b'_j = b_j + \\epsilon_{b_j}, \\quad \\epsilon_{b_j} \\sim \\mathcal{N}(0, \\sigma_b^2)$ (A.9)\nThis formulation introduces small, independent, and isotropic random deviations from the original actions, modeling the stochastic uncertainty in the decision-making process."}, {"title": "A.6 Geodesic Repulsion Loss", "content": "To encourage an even distribution of points on the spherical manifold, we employ the Geodesic repulsion loss, which penalizes pairs of points that are too close in geodesic distance. This loss function facilitates the spreading out of points uniformly over the sphere, preventing clustering.\nGeodesic Distance: Let $y_i, y_j \\in \\mathbb{R}^D$ be points on the surface of a Riemmanian manifold denoted as $G(y_i, y_j)$ in the abstract sense. For a unit sphere it would hold that $||y_i|| = ||y_j|| = 1)$. The geodesic distance between two points on the sphere is the angle between them, which can be computed from their dot product,\n$G(y_i, y_j) = \\arccos \\left( \\frac{y_i^T y_j}{||y_i|| ||y_j||} \\right),$ (A.10)\nwhere $y_i^T y_j$ is the dot product of $y_i$ and $y_j$.\nRepulsion Term: To penalize pairs of points that are close in geodesic distance, we use an exponential decay function, which strongly penalizes small distances:\n$\\exp \\left( \\frac{G(y_i, y_j)}{\\gamma} \\right),$  (A.11)\nwhere $\\gamma > 0$ is a sensitivity parameter controlling how strongly the loss reacts to small distances. A smaller $\\gamma$ enforces stronger repulsion between nearby points."}, {"title": "A.7 Negative Log-Likelihood Loss for Normalizing Flows", "content": "Let $x \\in \\mathbb{R"}, "d$ be an input data point, and let $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ be an invertible transformation defined by the normalizing flow. The transformation $f$ maps the input data $x$ to a latent variable $z = f(x)$ that follows a simple base distribution $p_z(z)$. Assume that the base distribution is a standard normal distribution, $Z \\sim \\mathcal{N}(0, I_d)$, with the probability density function (PDF) given by,\n$p_z(z) = \\frac{1}{(2\\pi)^{d/2}} \\exp \\left( -\\frac{1}{2} ||z||^2 \\right).$ (A.14)\nThe log probability under this distribution is,\n$\\log p_z(z) = -\\frac{1}{2} ||z||^2 - \\frac{d}{2} \\log(2\\pi).$ (A.15)\nUsing the change of variables formula, the probability density of $x$ under the model is related to the base distribution via the transformation $f$ as follows,\n$p_x(x) = p_z(f(x)) \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|$ (A.16)\nWhere $\\frac{\\partial f(x)}{\\partial x}$ is the Jacobian matrix of $f$ with respect to $x$, and $\\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|$ is the absolute value of the determinant of the Jacobian.\nNLL Loss: The negative log-likelihood (NLL) loss for a single data point $x$ is defined as,\n$\\mathcal{L}^x(x) = - \\log p_x(x) = - \\left[ \\log p_z(f(x)) + \\log \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right| \\right"], "distribution": "n$\\mathcal{L}^x(x) = \\frac{1}{2} ||f(x)||^2 + \\frac{d}{2} \\log(2\\pi) - \\log \\left| \\"}