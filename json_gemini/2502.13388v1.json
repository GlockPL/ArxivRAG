{"title": "Reflection of Episodes: Learning to Play Game from Expert and Self Experiences", "authors": ["Xiaojie Xu", "Zongyuan Li", "Chang Lu", "Runnan Qi", "Yanan Ni", "Lumin Jiang", "Xiangbei Liu", "Xuebo Zhang", "Yongchun Fang", "Kuihua Huang", "Xian Guo", "Zhanghua Wu", "Zhenya Li"], "abstract": "StarCraft II is a complex and dynamic real-time strategy (RTS) game environment, which is very suitable for artificial intelligence and reinforcement learning research. To address the problem of Large Language Model (LLM) learning in complex environments through self-reflection, we propose a Reflection of Episodes (ROE) framework based on expert experience and self-experience. This framework first obtains key information in the game through a keyframe selection method, then makes decisions based on expert experience and self-experience. After a game is completed, it reflects on the previous experience to obtain new self-experience. Finally, in the experiment, our method beat the robot under the Very Hard difficulty in TextStarCraft II. We analyze the data of the LLM in the process of the game in detail, verified its effectiveness.", "sections": [{"title": "1 Introduction", "content": "StarCraft II is a complex and dynamic real-time strategy (RTS) game environ-ment, which is very suitable for artificial intelligence and reinforcement learning research. In \"StarCraft II,\" players choose a race in the game and defeat op-ponents through resource management, base construction, technology upgrades, and combat. The game has a high level of strategic and tactical depth, requiring players to excel in economic management, army control, and decision-making. In 2017, DeepMind and Blizzard Entertainment collaborated to release SC2LE[1], a standardized reinforcement learning environment aimed at promoting AI re-search. SC2LE provides an API that allows researchers to control units and"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 StarCraft-II Environment", "content": "Prior to the application of AI to StarCraft II, AI research in RTS games had been going on for years. Early attempts focused on using rules and finite state machines to create simple game agents. These agents are often inflexible and adaptable, unable to handle the complex strategies of human players.\nIn 2017, DeepMind and Blizzard Entertainment collaborated to release SC2L E[1]. It is a standardized reinforcement learning environment designed to facil-itate AI research. SC2LE provides apis that allow researchers to control units and buildings in the game and access game state information. SC2LE provides a unified test bed for researchers, greatly advancing StarCraft II AI research.\nIn 2019, DeepMind released AlphaStar, a landmark AI system that uses Multi-Agent Reinforcement Learning to compete against top human players in StarCraft II. In multi-agent reinforcement learning, there are also many meth-ods used in StarCraft II environments, such as VDN[6], QMIX[7], WQMIX[8], \u039c\u0391\u03a1\u03a1\u039f[9], \u039cADDPG[10]. In recent years, there have been other studies of the StarCraft II environment through other methods, such as RLforSC[11].\nHowever, existing methods cannot improve the interpretability of AI decision-making processes or help researchers understand and improve AI strategies."}, {"title": "2.2 Large Language Model", "content": "LLMS typically have complex architectures with billions or even hundreds of billions of parameters that are trained with vast amounts of data and compu-tational resources. In June 2018, GPT-1 was the first generative pre-trained converter model released by OpenAI. It uses the converter architecture for un-supervised pre-training on large amounts of text data and then optimization on specific tasks through supervised fine-tuning. In November 2022, ChatGPT was released as a chat application.\nIn recent years, there have also been a variety of large models in different directions, such as Claude-2[12], LLAMA-3[13], PaLM-2[14] and so on. Claude is a large-scale language model introduced by Anthropic that focuses on security"}, {"title": "2.3 LLM and reflection", "content": "In order to make the large model continuously improve itself in the process of communication, the method of combining the large model and reflection is con-stantly proposed. A new framework, reflexition[16], was proposed in 2023, which instead of updating weights, strengthens language agents through language feed-back, allowing large models to learn quickly and efficiently from trial and error.In this approach, the focus is on action-level reflection, allowing the large model to store buffers in each chat, thereby inducing better decisions in subsequent exchanges. In addition to action-level reflection, Agent-Pro[17] methods are pro-posed to form strategy-level reflection and optimization, fine-tuning its overall strategy by reflecting on past trajectives and decisions.\nHowever, these reflection methods do not take into account the adaptability of frameworks in complex environments such as StarCraft II. Therefore, we propose Reflection of Episodes to realize strategy-level reflection in complex environment."}, {"title": "3 Methods", "content": "We propose a framework for reflection and strategy iteration as Figure 2. The ROE framework mainly includes two key improvements: key frame selection based on game phase division; reflection generation and strategy iteration based on reflection of episodes. All the prompts used are shown in Appendix A."}, {"title": "3.1 Key Frame Selection", "content": "In an entire StarCraft II match, the environment will generate 7000 frames of data for about 20 minutes, and the large model will interact with the environ-ment more than 700 times. In the process of reflection generation, in order to summarize the whole game situation with as little data as possible, we proposed a key frame selection method based on game phase division.\nGame Phase Division During the course of StarCraft II, resources, buildings, units, etc. change over battlefield situation, so game stages can be divided by these data. We came up with a standard for dividing StarCraft II game stages and included it in the prompt, so that the LLM includes an analysis of the current phase when it generates the L2 Summary. In the subsequent key frame selection process, the most critical part of the game is selected according to these stage analyses. Detailed prompt is shown in Appendix A."}, {"title": "3.2 Reflection Generation and Strategy Iteration", "content": "After the completion of the generation of key frames, a new self-reflection is generated according to the previous experience and the key frame of the game, and then the self-reflection is added to the system prompt to achieve strategy iteration. As shown in Figure 3, in three consecutive games, the strategy based on expert experience and first self-reflection failed against the Very Hard robot.\nThe second self-reflection based strategy, obtained after two iterations, wins the game. Below we will cover in detail the methods of reflection generation and strategy iteration.\nReplay analysis method To effectively analyze StarCraft replays and learn from them, we propose a replay analysis method, which is divided into the follow-ing three stages. First, When watch the Replay, focus on key aspects as \"Opening Build Order\" (Timing of first buildings), \"Economy Management\"(Worker pro-duction and expansion time), \"Scouting\" (send probe to gather information) and"}, {"title": "4 Experiments", "content": "All the experiments run on the TextStarCraft-II environment. The baseline is the COS[5] method in TextStarCraft-II environment. To compare with the baseline results, we used exactly the same settings. In the experiment, our method takes 5 consecutive rounds as one test, and winning within the limit of 5 is regarded as Victory."}, {"title": "4.1 Experiment Settings", "content": "In this section we will introduce some of the necessary setup parts of the exper-iment, including some important parameters such as player race, opposite race, difficulty and so on. Detailed data are shown in Table 1."}, {"title": "4.2 Experiment Results", "content": "In this section, we compared the winning rates of our method with the baseline method. In our experiments, agents with GPT-3.5 played Protoss in TextStarCraft-II against Zerg of varying difficulties. As shown in Table 2, our method increased the winning rate on Hard and Harder difficulties and managed to defeat Very-Hard opponents with a 20% winning rate."}, {"title": "4.3 Ablation experiment", "content": "In order to verify the role of keyframe selection, self-reflection and strategy it-eration frameworks in the experiment, we conducted an ablation experiment to verify the experimental results and showed their differences by comparing the key data."}, {"title": "5 Conclusion", "content": "In this paper, we propose a ROE framework based on expert experience and self-experience. Through the framework, LLM makes decisions based on expert experience and self-experience. After the game is completed, it obtains key in-formation in the game through a keyframe selection method, then reflects on the previous experience to obtain new self-experience. This framework has been ex-perimentally proven to effectively enhance LLM's strategies through reflections of episodes based on expert experience and self-experience. The reflections ob-tained also exhibit partial strategy interpretability, which can be regarded as a strategy generation approach using LLMs. Further exploration will be conducted in our subsequent research."}]}