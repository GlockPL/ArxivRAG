{"title": "HDFLOW: ENHANCING LLM COMPLEX PROBLEM-SOLVING WITH HYBRID THINKING AND DYNAMIC WORKFLOWS", "authors": ["Wenlin Yao", "Haitao Mi", "Dong Yu"], "abstract": "Despite recent advancements in large language models (LLMs), their performance on complex reasoning problems requiring multi-step thinking and combining various skills is still limited. To address this, we propose a novel framework HDFlow for complex reasoning with LLMs that combines fast and slow thinking modes in an adaptive manner. Our approach consists of two key components: 1) a new approach for slow, deliberate reasoning called Dynamic Workflow, which automatically decomposes complex problems into more manageable sub-tasks and dynamically designs a workflow to assemble specialized LLM or symbolic reasoning tools to solve sub-tasks; 2) Hybrid Thinking, a general framework that dynamically combines fast and slow thinking based on problem complexity. Finally, we propose an easy-to-scale method for automatically synthesizing a large-scale dataset of 27K challenging reasoning problems for complex reasoning and a hybrid thinking tuning method that trains smaller LLMs on this dataset to internalize the fast/slow hybrid reasoning strategies. Experiments on four reasoning benchmark datasets demonstrate that our slow thinking with dynamic workflows significantly outperforms Chain-of-Thought, and hybrid thinking achieves the highest accuracy while providing an effective balance between computational efficiency and performance. Fine-tuning using our hybrid thinking approach also significantly boosts the complex reasoning capabilities of open-source language models. The results showcase the promise of slow thinking, dynamic workflows, and hybrid thinking in expanding the frontier of complex problem-solving with LLMs\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, from code generation and mathematical reasoning to natural language understanding and generation. However, their performance on complex reasoning problems that require multi-step thinking and various skills is still limited. Recent advancements in symbolic reasoning and tool usage, have shown significant improvements in specific domains by integrating LLMs with specialized procedures and symbolic reasoning engines. Various prompting strategies, have been developed to enable different reasoning topologies to enhance LLM problem-solving capabilities. Despite these advancements, enhancing the reasoning abilities of LLMs to solve challenging problems across diverse domains in a unified framework remains crucial for expanding their real-world applicability.\nExisting methods for complex reasoning with LLMs have several limitations. First, complex problem-solving often requires combining various knowledge domains, skills, and tool usage. While previous approaches have demonstrated the potential of combining language models and symbolic reasoning to"}, {"title": "2 RELATED WORK", "content": "Symbolic Reasoning and Tool Usage. Bridging LLMs with symbolic reasoning and tool usage has demonstrated significant improvements across various domains. Chain of Code encourages LLMs to write pseudocode for challenging sub-problems, which is then executed by the LM itself when it cannot be handled by a standard interpreter. These approaches rely on carefully designing when and how to integrate symbolic reasoning for each task domain.\nPrompting Strategies. Various prompting strategies have been developed to enable different reasoning topologies for enhancing LLM problem-solving capabilities. Our hybrid thinking approach allows for the efficient resolution of tasks within the LLM's core capabilities through direct reasoning, while adaptively engaging in deeper, multi-step workflows for more complex problems.\nMulti-Agent Frameworks for Task-Solving. Recent advancements also led to the development of various frameworks for complex task-solving and multi-agent collaboration. In contrast to these existing works, our approach uniquely integrates hybrid thinking, combining fast and slow thinking modes with automatic workflows, to enhance LLMs' ability to tackle complex reasoning problems more effectively and with greater adaptability."}, {"title": "3 OVERVIEW OF THE HYBRID THINKING APPROACH", "content": "Our hybrid thinking approach (Figure 1) combines the strengths of fast and slow thinking modes to enable LLMs to more effectively solve complex reasoning problems. It consists of the following three key components. 1) Fast Thinking with Direct CoT. In the fast thinking mode, the LLM uses a direct chain of thought (CoT) approach to quickly solve the task query if possible. This leverages the LLM's core abilities to perform certain types of reasoning efficiently by directly generating the rationale and the final answer. 2) Adaptive Combination of Fast and Slow Thinking. Next, we employ a self-verification mechanism where the LLM examines each step of the fast-thinking CoT"}, {"title": "4 SLOW THINKING WITH DYNAMIC WORKFLOW", "content": "In contrast to the rapid responses of fast thinking (e.g., CoT), our new slow-thinking mechanism applies dynamic workflow to enable a more deliberate, analytical approach to complex problem-solving (see Figure 2). It allows an LLM to dynamically transition between reasoning in the text space (natural language reasoning) and the symbolic space (symbolic reasoning). The high-level idea is we first let the LLM decompose the original reasoning problem into several more manageable sub-tasks and solve each sub-task to form the final solution. When necessary, the LLM Engine will translate the sub-problem from the text space into the symbolic space, enabling the symbolic engine\u00b3"}, {"title": "4.1 BREAKING DOWN COMPLEXITY: PROBLEM ANALYSIS AND DECOMPOSITION (STAGE 1)", "content": "The first step in our slow thinking is problem analysis and planning. We aim to break down the original problem statement into more manageable sub-tasks. Specifically, the LLM is asked to analyze the key elements of the query, such as available information, constraints, and the desired output. It then identifies logical sub-goals needed to progress from the initial state to the solution. This decomposition allows the LLM to approach the problem in a structured manner, focusing on one part at a time. Therefore, the LLM can catch gaps in reasoning and handle complex problems that the fast thinking of CoT alone would struggle with.\nProblem Reflection. The first step in tackling complex problems is conducting a thorough problem reflection. This involves the LLM analyzing the original problem and restating it in its own words to demonstrate understanding. Our problem reflection includes two parts: 1) Identifying the core objective or question posed by the problem. 2) Recognizing any constraints, assumptions, or special conditions mentioned. By internalizing the problem through reflection, the LLM can gain a solid understanding of what needs to be accomplished before proceeding to decomposition.\nSubtask Decomposition. Once the problem is well understood, the LLM is instructed to perform a multi-level decomposition to break it down into some tractable sub-problems. The LLM is asked to follow four principles to achieve an optimal decomposition. Sequential dependency. The sub-problems are organized in a logical sequence, such that the outputs of earlier steps feed into subsequent ones, creating a structured workflow from start to finish. Non-overlapping. Each sub-problem represents a distinct portion of the original problem, with no duplication of work between sub-problems. This keeps the overall solution efficient. Proper Decomposition. The sub-problems are decomposed to the optimal level of granularity - not so small that there are too many to track and coordinate, but not so large that they are still struggling to solve. Modular. Where appropriate, sub-problems are defined in a generalizable, modular way, such that the logic and code used to solve them can potentially be reused to solve similar problems in other contexts.\nIntegrating Symbolic Reasoning. Another key aspect of our approach is leveraging the symbolic engines to modularize the solution and handle well-defined sub-tasks more accurately. For example, some sub-tasks in the decomposition can often be addressed by writing code functions. Therefore, we explicitly instruct the LLM to consider sub-tasks that can be well handled by writing and executing modular code in subtask decomposition."}, {"title": "4.2 ORCHESTRATING EXPERTISE: WORKFLOW DESIGN (STAGE 2)", "content": "With the problem decomposed into sub-tasks, our approach next proposes a team of specialized experts, each contributing unique skills and tools, arranged in a dynamic workflow. The central component is a Meta-Expert, initialized from the foundation LLM, designs the expert team, and coordinates their efforts. The orchestration process consists of four steps.\n1. Design of Experts. Based on the identified sub-tasks, the Meta-Expert designs a team of specialized experts with one expert solving one sub-task. Each expert is assigned a unique name and a clear description of their specific skills, knowledge, and responsibilities\u2074. The dynamic workflow leverages two types of experts to handle each sub-task, enabling a seamless integration of verbal and symbolic reasoning. The first type are specialized experts initiated from LLMs, such as linguists, mathematicians, and data scientists. These experts bring domain-specific knowledge and skills to the workflow, allowing for sophisticated verbal reasoning and analysis within their fields. The second type of expert focuses on"}, {"title": "4.3 FLOW EXECUTION: CONSTRUCTING AND RUNNING WORKFLOWS (STAGE 3)", "content": "With the workflow graph generated, our approach finally proceeds to execute the graph to get the final result. The execution follows the dependency order, ensuring the correct flow of data between experts. To ensure robust execution, if any of the generated code encounters errors, the corresponding symbolic reasoning experts will trace the issue, use the error message to repair the code, and rerun it. As the workflow progresses, the downstream experts continually update their memory with the intermediate results and insights generated by previous experts. Upon completion of the workflow execution, the last LLM expert analyzes the results, identifies key findings, and summarizes them into a final answer to the original problem. The workflow execution is not a one-time process. The LLM continually assesses the quality and correctness of the final generated solutions and identifies potential errors. It engages in iterative rerun by applying a different problem decomposition, expert assignments, or adjusting the workflow structure."}, {"title": "5 MODEL TUNING OF HYBRID THINKING", "content": "In our experiments, we observed that open-source language models (typically those with around 7B parameters) often struggle with advanced meta-planning and problem-solving skills required for solving difficult reasoning tasks. To address this limitation and develop local smaller models with hybrid thinking abilities comparable to the large models, we construct a comprehensive training dataset and propose hybrid thinking tuning to improve the complex reasoning abilities of local models. We define \u201clocal\" models as models that can be trained and deployed on local hardware with limited computational resources, such as the Llama-3 model (Meta, 2024). Our goal is to improve the complex reasoning abilities of these local models through our proposed approach.\nThe primary challenge lies in constructing a large-scale dataset of reasoning problems that are sufficiently diverse, high-quality, and difficult. Such a dataset is crucial for teaching smaller local models to perform complex reasoning tasks. However, manually curating such a dataset presents significant difficulties in ensuring a wide range of problem domains and maintaining high standards in problem formulation. As a result, it is extremely time-consuming and expensive to ask human experts to"}, {"title": "5.1 REASONING PROBLEMS SYNTHESIS", "content": "To enhance reasoning task diversity and coverage, our data synthesis pipeline consists of three steps (Figure 3). In the first step, we strategically leverage human-authored seed tasks to inspire the creation of new reasoning problems (similar to Self-Instruct (Wang et al., 2023)) or let the LLM brainstorm reasoning puzzles that cover a variety of task formats, difficulty levels, and problem domains. This step only focuses on generating high-level task descriptions to encourage diversity. In the second step, we apply deduplication to remove near-identical tasks. Finally, we apply LLMs again to write three specific problems based on the task descriptions and validate those problems.\nTask Generation Inspired by Seed Tasks. The first step of our reasoning data synthesis pipeline is generating an expanded set of reasoning tasks. We augment the few-shot prompts with 10 high-level task descriptions randomly sampled from the 214 BigBench tasks (Srivastava et al., 2022). Next,"}, {"title": "5.2 FINETUNING OPEN-SOURCE MODELS ON SYNTHESIZED DATA", "content": "To prepare the training data for enhancing the open-source models' complex problem-solving abilities, we utilize the GPT-4-turbo model to collect reasoning trajectories on the dataset of synthesized and mathematical problems. For each problem, GPT-4-turbo generates one or several fast/slow reasoning trajectories using the hybrid thinking approach. Each reasoning trajectory consists of a sequence of (query, answer) pairs representing the model's step-wise hybrid thinking process. Therefore, we use all (query, answer) pairs from the reasoning trajectories to construct the training data, capturing the complete problem-solving process. When multiple reasoning trajectories are produced (iterative retry), only the solution trajectory that passes the verification process is retained in the training set to optimize the model's problem-solving capabilities, while the verification results for all trajectories are kept to enhance the model's self-verification abilities.\nThe Llama-3 models have demonstrated superior performance compared to other models of similar size due to significant enhancements in both pretraining and post-training (Meta, 2024). Therefore, we choose the Llama-3-8B-Instruct model as the foundation model for our hybrid thinking tuning experiments. Specifically, The Llama-3-8B-Instruct model was fine-tuned using 8 A100 GPUs with bf16 precision. The training utilized a global batch size of 128, spanning 4 epochs. The model employed the AdamW optimizer of a learning rate of 2.0e-5, with a maximum sequence length of 4096 tokens and a maximum of 2048 new tokens generated."}, {"title": "6 EXPERIMENT", "content": "6.1 REASONING BENCHMARK DATASETS\nBIG-Bench Hard (BBH): A subset of 27 challenging tasks from the BIG-Bench benchmark (Srivastava et al., 2022), which aims to measure the capabilities and limitations of language models across diverse text-based tasks. MATH: A dataset consisting of 5,000 test problems from mathematics competitions. These problems assess the mathematical problem-solving ability and often require the application of problem-solving techniques"}, {"title": "6.2 RESULTS BASED ON PROMPTING", "content": "We first conduct experiments by prompting GPT-4-Turbo-0125\u2078 to achieve three reasoning modes: Chain of Thought (CoT), Slow Thinking with Dynamic Workflow, and Hybrid Thinking across four benchmark datasets. Table 1 shows that slow thinking with dynamic workflow significantly outperforms CoT by 22.4% on average across four benchmarks. It also reveals that Hybrid Thinking achieves the best accuracy on three datasets BBH, MATH and GameOf24. Notably, both Slow Thinking and Hybrid Thinking consistently outperform CoT across all datasets, with the most dramatic improvements seen in GameOf24, where gains are 61.0% and 62.7% respectively.\nTable 2 illustrates the average number of inference tokens used by each method. CoT consistently used the fewest tokens (average 577.8), while Slow Thinking required the most (4432.0 on average). Hybrid Thinking struck a balance with an average of 3105.5 tokens. A clear trade-off emerged between computational efficiency and performance, with CoT using the fewest tokens but achieving the lowest accuracy. Hybrid Thinking demonstrated a good balance, achieving high accuracy with moderate token usage. These findings suggest that incorporating dynamic workflows and combining fast and slow thinking processes can enhance the reasoning capabilities of LLMs, with Hybrid Thinking emerging as a particularly promising approach."}, {"title": "6.3 RESULTS OF HYBRID THINKING TUNING", "content": "We next compare the performance of the original Llama-3-8B-Instruct model and the model after our hybrid thinking tuning. As shown in Table 3, the Llama-3-8B-Instruct model after hybrid thinking tuning significantly outperforms the baseline model on all datasets. Examining the different thinking modes, hybrid thinking consistently provided the best tradeoff between performance and efficiency. Compared to the CoT baseline, hybrid thinking improved accuracy by 10.6%, 10.2%, 23.1% and"}, {"title": "6.4 FAST/SLOW ROUTING ANALYSIS", "content": "Figure 5 illustrates the proportion of fast thinking and slow thinking (orange) approaches applied by both models when solving complex problems across the datasets. The GPT-4-Turbo model demonstrates a higher reliance on fast thinking for BBH, DeepMind MATH, and Game of 24 tasks compared with Llama-3-8B-Instruct model. This observation can be attributed to the fact that GPT-4-Turbo's fast thinking (in the form of CoT) is more reliable and effective compared to Llama-3-8B-Instruct. As a result, hybrid thinking in GPT-4-Turbo tends to apply more fast thinking since it is sufficient to achieve a correct solution in many cases. In contrast, Llama-3-8B-Instruct after tuning exhibits a greater reliance on slow thinking strategies, particularly in complex tasks, where fast thinking alone may not yield the desired results. This highlights the importance of hybrid thinking to improve problem-solving efficiency, suggesting that our method can dynamically adjust the optimal balance between fast and slow thinking based on the model's downstream reasoning capabilities.\nIn summary, the dynamic combination of fast and slow thinking modes greatly enhanced the model's problem-solving capabilities. Our results showcase the potential of hybrid thinking approaches to expand the frontier of what LLMs can achieve on challenging tasks."}, {"title": "7 DISCUSSION AND FUTURE WORK", "content": "Limitations and Potential Improvements. One promising direction is to incorporate a value network that scores the successfulness or quality of completing each sub-task within the dynamic workflow. By integrating such a value network, we can formulate the problem-solving process as a reinforcement learning task, enabling the optimization and search for the best solution trajectory. This enhancement could lead to more efficient and effective problem-solving strategies, as the model learns to prioritize and select the most promising decompositions and workflows based on predicted values.\nGeneralization to Other Reasoning Tasks. Constructing high-quality and sufficiently challenging reasoning problems for training still remains a significant challenge. While our data synthesis approach offers a scalable solution, ensuring the validity and difficulty of each generated reasoning problem is crucial for effective model development. One potential improvement is to involve human experts in the data synthesis process, allowing them to verify, modify, and curate the generated problems.\nIntegration with Symbolic Reasoning Systems. Our dynamic workflow approach seamlessly integrates specialized language models and symbolic reasoning tools, enabling LLMs to tackle complex problems more effectively. However, there is significant potential to extend this integration to more advanced symbolic reasoning systems, such as Lean for mathematical theorem proving or other domain-specific tools. Moreover, integrating our approach with tools such as search engines and web browsers could enable LLMs to access and utilize external resources, further amplifying their problem-solving abilities to broader applications. By incorporating more powerful tools into the dynamic workflow, we can expand the range of problems that LLMs can solve."}, {"title": "8 CONCLUSION", "content": "This paper introduces a novel framework HDFlow for enhancing the complex problem-solving capabilities of LLMs through hybrid thinking and dynamic workflows. The dynamic workflow mechanism enables LLMs to decompose complex problems into manageable sub-tasks and integrate specialized language models and symbolic reasoning tools, while hybrid thinking strategically engages deeper, multi-step reasoning for challenging problems that exceed the capabilities of fast thinking alone. Extensive experiments demonstrate the significant advantages of our approach, with slow thinking with dynamic workflow greatly outperforming CoT and hybrid thinking achieving the highest overall accuracy by balancing efficiency and performance."}, {"title": "A APPENDIX", "content": "B DYNAMIC WORKFLOW EXAMPLE SOLUTION\nIn this section, we demonstrate how our dynamic workflow approach works by presenting an example solution to a reasoning problem. We also present the solution provided by the OpenAI 01-preview model to compare with our approach. We can see the solution produced by our approach contains many sub-goals (sub-tasks), which is very similar to the 01-preview solution. Our approach also leverages the symbolic tool - Python to enhance the accuracy of the final answer. The reasoning problem is generated using our data synthesis method."}, {"title": "B.1 REASONING PROBLEM:", "content": "An encrypted Morse code message contains a list of three items that were\ncrucial to a historical event. The Morse code is: .-/ -.-.\n.-/ .-'\\. Decode this Morse code message\ninto English text, keeping in mind the standard Morse code\nconventions where each sequence of dots (.) and dashes (-)\ncorresponds to a letter, and spaces separate words."}, {"title": "B.2 SOLUTION USING DYNAMIC WORKFLOW:", "content": "Stage 1. Problem Reflection\n### Problem Reflection start ###\n**Task Description**: Decode a given Morse code message into English text\n**Elements of the Task**:\n1. The message is encoded in Morse code, which uses a series of dots (.)\nand dashes (-) to represent letters and spaces to separate words.\n2. Decoding involves translating each Morse code sequence into its\ncorresponding English letter or symbol.\n3. The Morse code provided is a list of three items related to a\nhistorical event, which implies the output should be meaningful and\ncorrectly formatted.\n**Sub-tasks Identification**:\n1. **Morse Code Dictionary Creation**: Establish a dictionary in Python\nwhere keys are Morse code sequences (dots and dashes) and values are\ncorresponding English letters. This will facilitate the translation\nprocess.\n2. **Parsing the Morse Code**: Split the input Morse code string into\nindividual Morse code sequences for letters and words. This involves\nhandling spaces correctly to differentiate between letters and word\nboundaries.\n3. **Translation Process**: For each Morse code sequence obtained from\nthe parsing step, use the Morse code dictionary to translate it into\nthe corresponding English letter.\n4. **Reconstruction of Message**: Combine the translated letters into\nwords and then into the complete message. This step ensures that the\noutput is a readable English sentence.\n5. **Output Formatting**: Format the final output to ensure it is clear\nand presentable, adhering to proper English syntax and spacing.\nStage 2. Workflow Design"}, {"title": "Stage 3. Graph Construction and Execution", "content": "To solve the assigned sub-task of creating a comprehensive Morse code\ndictionary, I will map each Morse code symbol (comprising dots '.'\nand dashes `-`) to its corresponding English letter. This dictionary\nis essential for the subsequent decoding process, as it allows for\nthe translation of Morse code sequences into readable English text.\n**Steps to Create the Morse Code Dictionary**:\n1. **Identify Morse Code Symbols**: List all Morse code symbols for\nEnglish alphabet letters (A-Z) and numerals (0-9) if necessary.\n2. **Map to English Letters**: Each Morse code symbol will be associated\nwith its corresponding English letter. This mapping is based on the\nstandard Morse code conventions."}, {"title": "B.3 SOLUTION USING OPENAI 01-PREVIEW", "content": "Hidden Thought Process (thought for 12 seconds)\n===== Parsing Morse code =====\nI'm separating the Morse code message into three words. Each word is then\nfurther divided into its constituent letters for decoding.\n===== Parsing Morse code =====\nIn this chunk, the task involves parsing Morse code by handling letter\nconcatenation within words and word separation through slashes. Morse\ncode typically separates letters with spaces and words with slashes.\n===== Breaking down Morse code =====\nI'm looking at Morse code to decode - .-, mapping it to letters. This\nmakes me think about interpreting symbols as letters.\n==== Considering Morse code decoding =====\nI'm thinking through the method of reversing Morse code, focusing on\nidentifying letter partitions and parsing two-letter words to\ndetermine valid Morse code sequences, examining all possible\npartitions of the code.\nIdentifying letter patterns =====\nI'm working through different Morse code partitions to isolate letter\npatterns like T E A and T U. This involves analyzing various\ncombinations of Morse code symbols.\nDecoding Morse code =====\nIt's interesting to see that the Morse code was parsed as . .-, .-. and .--\nfor 'COFFIE' and 'T E A' as the most plausible word. This\nsuggests a detailed analysis of each symbol is crucial for accurate\ntranslation.\n=== Decoding Morse code =====\nI'm working through the Morse code sequence, breaking down each symbol to\nreveal letters like C, O, F, F, E, E, S, U, G, and ?.\nFinal solution\nTo decode the Morse code message .-/-.-./ .-/'\\, we'll break it down word by word and letter by\nletter.\n### Morse Code Reference Table:\n| Letter | Morse Code |\n### Decoding Process:\n####Final Decoded Message:\n####Answer:\nTEA COFFEE SUGAR"}, {"title": "C PROMPTS USED IN OUR DYNAMIC WORKFLOW APPROACH", "content": "Note that placeholders enclosed in {} are used within the prompts to represent variables that will be\nsubstituted with actual values at LLM query time."}, {"title": "Problem Reflection Prompt", "content": "You are an exceptionally capable Meta-Expert, possessing a unique capability for conducting problem\nreflection. Your primary function involves receiving the above problem query, which you must me-\nthodically decompose into smaller, more manageable sub-tasks (including sub-tasks that can solved\nby implementing Python functions). When designing the solution, you should think about its general-\nizability. A robust solution can tackle a similar range of problems effectively with minor adaptations.\nThis decomposition will later facilitate the creation of a team of specialized experts, enabling efficient\ncollaboration of experts to address and solve the above problem. When breaking down into sub-tasks,\nit is crucial to:\n1. Ensure Sequential Logic: Arrange the sub-tasks in a logical, sequential order that facilitates a\nsmooth workflow from start to finish.\n2. Avoid Overlap: Each sub-task must be distinct, with no duplication of efforts across the tasks, en-\nsuring efficient allocation of expertise.\n3. Pursue Optimal Decomposition: Ensure sub-tasks are sufficiently defined to be tackled effectively.\nMaintain a manageable number of specific sub-tasks, facilitating easier coordination and management.\nIn particular, please conduct the Problem Reflection for the given problem: Reflect on the problem,\nand describe it in your own words, in bullet points. Analyze how you can decompose the problem into\nsmaller, more manageable sub-tasks. Note that you can integrate Python-driven sub-tasks by imple-\nmenting and running modular Python code if necessary. Pay attention to small details, nuances, notes\nand examples in the problem description."}, {"title": "Experts Design Prompt", "content": "You are an extremely powerful Meta-Expert with the unique ability to design a team of specialized\nexperts and arrange those experts through a workflow to tackle and solve the above problem. Based on\nthe above problem statement and its reflection analysis, please design a team of experts and orchestrate\nthose experts to effectively address and solve the above problem.\nIn particular, you are to do Specialized Experts Design:\n- Design a list of subject-matter experts (SMEs) including, but not limited to, Essayist Expert, Python\nExpert, Linguistic Analyst, Mathematician, Data Scientist, and various other Analysts. Each expert is\nonly to perform one specific sub-task, such as processing data, making decisions, or utilizing Python\ntools.\n- Arrange the experts to operate in a sequential workflow, meaning each expert's output becomes the\ninput for the next, progressively moving towards the final answer. Avoid redundancy of functions\nacross experts.\n- Assign unique names to each expert and provide an clear description of their specific skills, knowl-\nedge, and the sub-tasks they are going to perform. Ensure the expert description is comprehensive\nand self-contained that encapsulates all important information and details from **Sub-tasks Identifi-\ncation**.\n- For sub-tasks involving logical reasoning, mathematical operations, data structure manipulation, or\nprogramming-related challenges, you can outline strategic approaches and delegate the specifics of im-\nplementation to the Python expert (Tool). The Python expert will translate the instructions into code,\nexecute it, and return the results. You can include multiple Python experts if needed. Please provide\nexplicit implementation instructions to the Python expert(s).\n- Conclude each expert's description with a name card in JSON format, summarizing key attributes.\nSpecify the type of each expert as either 'LLM' for those based on Large Language Model or 'Tool'\nfor those utilizing Python tools.\n- The final expert should be responsible for reviewing the findings of previous experts and then gener-\nating the final answer to the problem."}, {"title": "Execution Prompt of Experts Initiated from LLM", "content": "Please act as {name}. Your role: {role} You are part of a specialized expert team. You are designed to\naccomplish a sub-task and collaborate with other experts through a workflow graph to solve the above\nproblem.\nThe expert team operates based on the following design:"}, {"title": "Execution Prompt of Experts initiated from Symbolic Engine", "content": "Please act as {name}. Your role: {role} You are a specialized Python expert among a team of experts.\nYou are designed to write Python code to accomplish a sub-task and collaborate with other experts\nthrough a workflow graph to solve the above problem.\nThe expert team operates based on the following design:\nPlease write the Python code that takes input in {input_type} and return output in {output_type}.\nGuidelines: - Make sure the code includes all the necessary module imports, properly initialize the\nvariables, and address the problem requirements. - The code needs to be self-contained, and executable\nas-is. Output only code, without any explanations or comments.\nThe code output must follow this structure:"}, {"title": "Verification Prompt", "content": "Experts Design:** - Based on the problem reflection, a team of experts has been designed and\norganized through a workflow to tackle and solve the problem described above. - Experts are designed\nto operate in a sequential workflow, meaning each expert's output becomes the input for the next,\nprogressively moving towards the final answer. - The final expert is responsible for reviewing the\nfindings of previous experts and then generating the final answer to the problem.\nHere is a description of the experts' roles and the workflow structure:\nGiven the described workflow design and the results produced by the experts, your task is to eval-\nuate whether the final output of the {final_expert} successfully and correctly solves the problem\npresented.\nPlease provide your analysis and then conclude your evaluation by stating 'FINAL EVALUATION:\nYES' or 'FINAL EVALUATION: NO'."}, {"title": "D DATA SYNTHESIS OF REASONING PROBLEMS", "content": "Data Synthesis Prompt 1\nPlease develop 10 new and diverse reasoning tasks, one per line, inspired by but distinct from the\nfollowing 10 example reasoning tasks:\nGuidelines for task creation:\nEnsure each new task is distinctly different from the example tasks provided; avoid mere variations.\nClearly and accurately define each task, making its objective and scope explicit.\nDesign tasks that yield deterministic answers, facilitating the creation of single, definitive standard\nanswers for subsequent problems derived from these tasks. This helps straightforward evaluation of\ncorrectness.\nTarget a moderate to hard difficulty level for each task, requiring thorough analysis and in-depth\nreasoning to solve.\nData Synthesis Prompt 2\nPlease develop 10 new and diverse puzzle tasks, one per line, to test various reasoning abilities.\nGuidance:\nEach new puzzle task should clearly and accurately describe what the task is.\nDesign puzzle tasks that yield deterministic answers, facilitating the creation of single, definitive\nstandard answers for subsequent problems derived from these tasks. This helps straightforward evalu-\nation of correctness.\nPuzzle tasks should have a moderate to hard difficulty level - they should require thorough analysis\nand in-depth reasoning to work through."}, {"title": "Problem Validation Prompt", "content": "Your task is to verify whether the above problem is a valid reasoning problem or not.\nValid Criteria:\nIt is clear and unambiguous (NO multiple interpretations).\nIt provides all necessary information required to solve the problem.\nThe problem is logically structured so that it can be approached through reasoning skills. It does not\ndepend on subjective judgments or opinions.\nThe problem is solvable and has one single, definitive correct answer that can be derived through\nreasoning.\nThere are no internal contradictions or conflicts in the problem.\nPlease provide a concise analysis and then output '#"}]}