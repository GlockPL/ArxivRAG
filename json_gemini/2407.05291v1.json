{"title": "WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks", "authors": ["L\u00e9o Boisvert", "Megh Thakkar", "Maxime Gasse", "Massimo Caccia", "Thibault Le Sellier De Chezelles", "Quentin Cappart", "Nicolas Chapados", "Alexandre Lacoste", "Alexandre Drouin"], "abstract": "The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress toward capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena/tree/workarena-plus-plus.", "sections": [{"title": "1 Introduction", "content": "In 2024, the average adult spends about 400 minutes every day interacting with computer software and the internet [DataReportal, 2024]. While some of this time is devoted to productive work, entertainment or creative endeavors, a significant portion is consumed by monotonous, low-value tasks, particularly in enterprise settings. Employees often engage in tedious tasks such as searching for information hidden deeply in knowledge bases, coordinating group discussions to schedule meetings, or filling expense reports in counter-intuitive user interfaces designed for functionality rather than user-friendliness [Bailey and Konstan, 2006, Norman, 2013]. One easily envisions a future where autonomous assistants\u2014AI agents-handle these tasks, allowing humans to focus on more complex, skill-intensive work that generates greater value. Recent advances in the reasoning and planning capacities of large language models (LLMs), with developments such as Chain-of-Thought [Wei et al., 2022a], ReAct [Yao et al., 2023], and Tree-of-Thought [Yao et al., 2024] (see [Huang et al., 2024, Wang et al., 2024b] for a review) suggest that this future might be within reach.\nWhile the development of autonomous agents has long been a major research topic both in academia and industry, the recent advances in LLM capabilities have led to a massive surge of interest for"}, {"title": "2 Background", "content": "Before diving into our main contribution WorkArena++, we summarize BrowserGym and WorkArena, which respectively provide the environment in which agents interact with the benchmark, and the atomic tasks upon which WorkArena++ is built."}, {"title": "2.1 BrowserGym \u2013 A Gym Environment for Web Agents", "content": "WorkArena++ is integrated into BrowserGym [Drouin et al., 2024] (Fig. 2b), a gym environment that facilitates the design and evaluation of web agents and includes many common benchmarks, such as MiniWob [Shi et al., 2017a, Liu et al., 2018] WebArena [Zhou et al., 2023] and WorkArena [Drouin et al., 2024]. The salient features of BrowserGym include: i) chat-based agent-human interactions, ii) enriched multimodal observations (HTML, accessibility tree [Zhou et al., 2023], screenshot, set-of-marks [He et al., 2024], element coordinates, etc.), and iii) a standardized and flexible action space. In the rest of the paper, all agents interact with WorkArena++ using BrowserGym."}, {"title": "2.2 WorkArena", "content": "The starting point for our work is WorkArena, the first benchmark to measure the performance of web agents at solving work-related tasks in enterprise settings [Drouin et al., 2024]. Below, we outline some of its key properties.\nTask complexity While challenging, the tasks included in WorkArena do not require complex problem-solving skills. They rather measure the ability of agents to perform basic interactions with the ServiceNow platform using the main UI components of its user interface, outlined in Fig. 2a. For example, one of the tasks consists in filling out a form after receiving the explicit list of desired values for each field. While solving these tasks is a first step toward achieving anything useful in the workplace, they remain extremely simplistic and trivial for humans.\nCertifiability An interesting property of WorkArena is that the successful completion of all tasks is certifiable. Each task comes with an oracle and a validator. The oracle is a human-coded solution that uses browser automation (through Playwright [Microsoft, 2023]) to solve the task. The validator is a function that verifies if the task was solved correctly (e.g., by inspecting the database and the current page) and returns a success reward (0 or 1). Our newly proposed WorkArena++ benchmark builds on the same mechanisms and extends them further to handle more complex tasks.\nArchitecture and availability WorkArena requires agents to interact with remote-hosted clones of the ServiceNow platform called Personal Developer Instances. These can be requested for free"}, {"title": "3 WorkArena++: Taking WorkArena to the Next Levels", "content": "In WorkArena++, each task consists of a logical combination of simpler atomic tasks, chained together to form a realistic workflow. For example, consider the task of \"onboarding a new employee\" from the perspective of an IT agent. The process would require the agent to: i) navigate to the appropriate page to create a new user, ii) create a new user account by filling out a form, iii) access a service catalog, iv) order a new laptop, and v) complete a form to assign the laptop to the user in the system. Each of these steps corresponds to a WorkArena L1 task4. Next, we provide details on how the tasks in WorkArena++ are categorized across two new levels of difficulty: L2 and L3 (\u00a73.1), and five skill categories (\u00a73.2)."}, {"title": "3.1 Difficulty Levels L2 and L3", "content": "WorkArena++ introduces two new levels of difficulty, L2 and L3, which each cover the same set of 341 workflows presented to the agent in different ways, either as explicit or implicit goals using ServiceNow's ticket mechanism. This makes for 2\u00d7341=682 tasks in total. While the workflows to execute in L2 and L3 tasks remain the same, the difficulty is increased in L3 due to the less explicit and more realistic way instructions are provided. Examples of L2 and L3 tasks are showcased in \u00a7 C.\nL2-Explicit The goal is provided to the agent as detailed instructions sent as a user message in the chat. This message contains the precise steps required to complete the task (e.g., start by navigating to the \"users\" list, then create a new entry with the following values [...] for an 'onboard user' task.). In addition to following these steps, succeeding at L2 tasks requires thinking, reasoning and contextual understanding.\nL3 - Via ticket + knowledge base The goal is provided to the agent in a manner that mimics how human agents receive work assignments as knowledge workers - through a ticket assigned to them (Fig. 1). The ticket includes key details necessary to complete the task (e.g., the name of the new employee for an 'onboard user' task) but does not specify the steps required to solve it. Instead, the agent solving the task is informed that additional instructions can be found in the company's knowledge"}, {"title": "3.2 Skills and Abilities Evaluated in WorkArena++", "content": "Further, all 682 tasks in WorkArena++ fall into one of 5 categories of skills, based on the abilities they require for being solved. The distribution of tasks across categories is displayed in Fig. 4a, and further detailed in \u00a7 C. We provide a description of each skill category below, along with their number of tasks.\nPlanning and problem solving (66 \u00d7 2 tasks) We evaluate these abilities through tasks that require decision-making under constraints to achieve an expected outcome. One notable example consists of scheduling a series of work items within a given time frame while satisfying various constraints based on criticality, duration, and overlap with other items. Other examples include tasks commonly performed by managers, such as redistributing work among employees based on occupancy, and dispatching work to employees based on expertise.\nInformation retrieval (39 \u00d7 2 tasks) We formulate a series of tasks that require retrieving information from either dashboards or lists (see Fig. 2a) before performing follow-up tasks based on the retrieved values. For example, one task consists of reading a dashboard to find which item is the lowest in stock and restocking by ordering additional items.\nData-driven decision making and reasoning (156 \u00d7 2 tasks) This skill, essential to several knowledge work roles, is evaluated through tasks that require interpreting data, performing logical or mathematical reasoning, and taking subsequent actions. One notable example is a task where the agent must select investments to maximize expected return within a limited budget, effectively solving a small instance of a knapsack problem.\nSophisticated memorization (28 \u00d7 2 tasks) An essential skill for web agents is the ability to gather information by navigating through a series of pages, memorizing key details along the way, and finally using it to achieve a specific goal. For instance, in the \u201conboard new employee\u201d task described earlier, the agent receives all the information about the employee, including hardware requirements, and must navigate through multiple pages, filling relevant information at each step to complete the task.\nContextual understanding through infeasible tasks (52 \u00d7 2 tasks) Finally, we introduce a set of infeasible tasks to verify if the agent can identify them. We consider two kinds: one where the agent is required to simply declare the task infeasible and another where it must additionally justify its decision. For example, if a task were infeasible due to requesting to fill an inexistent form field, the agent would be evaluated based on its ability to name that field in the justification."}, {"title": "3.3 Salient Features of WorkArena++", "content": "WorkArena++ does not only augment WorkArena with new tasks, but also includes a number of technical improvements over it which we outline below. For more detail on this, refer to \u00a7 E.\nIncreased visual diversity and realism WorkArena lacked visual diversity, as all the pages presented to the agent had a similar style. To better assess agents' ability to generalize across different enterprise settings, we introduce 10 fictitious brands, each with distinct styles of the ServiceNow interface. For instance, the \"Charlie's Cookies\u201d brand is shown in Fig. 1. In WorkArena++, a company brand is randomly sampled at the start of each task, enhancing realism and visual diversity.\nStandardization and task isolation For a benchmark to be robust, it must ensure a standardized level of difficulty, regardless of variables like parallel inference or the hardware used for experiments. Achieving this on a remote-hosted instance without access to proprietary code presents a challenge. In WorkArena++, we enhance robustness through several measures. First, we provide an installer that configures the instance with standardized system parameters, UI themes, knowledge bases, and layouts for components like lists and forms. Second, we implement sandboxing by running each task in a new user account, created at its start and automatically deleted at its end. This allows agents to interact freely with the system (e.g., changing visible columns in a list) without affecting subsequent tasks. Together, these improvements result in a more robust benchmark."}, {"title": "4 Experiments", "content": "We now present a series of experimental results on WorkArena++. As will be shown, our proposed benchmark poses a significant challenge for state-of-the-art web agents while being relatively simple for humans to solve. This contrast underscores the benchmark's potential to drive advancements in the field, providing the community with a valuable tool for evaluating and improving web agents."}, {"title": "4.1 Evaluation Curriculum: Standardizing WorkArena++ as a Benchmark", "content": "Each WorkArena++ task can be instantiated with variability from thousands of valid configurations per task. Hence, the benchmark can be viewed as a rich distribution over task instances. In order to make the cost of evaluation accessible, improve reproducibility, and have a uniform test of various skills required for solving the tasks, we propose a standardized way to sample a collection of task instances, which we refer to as a curriculum. Concretely, we provide a mechanism that takes a numerical seed as input and can produce an arbitrary number of task instances, sampled uniformly across skills in a reproducible way. We reserve seeds 0-9 for evaluation and the remaining seeds may be used for agent tuning. Additional details in \u00a7 D."}, {"title": "4.2 Agent Design", "content": "We mostly follow the same agent design as Drouin et al. [2024], which consists in using an LLM augmented with chain-of-though prompting [Wei et al., 2022b] to produce the next best action for solving the task, based on the current observation of the web browser.\nObservation space The main elements presented to our web agents are the current goal, the current page's accessibility tree (AXTree) [Zhou et al., 2023] which can be seen as a compressed representation of the HTML, and the error message (if any) that resulted from the execution of the previous action. Additionally, our VLM-based agent is given a screenshot of the current page augmented with set-of-marks [He et al., 2024]. As most tasks in WorkArena++ require long context understanding, we also add to the prompt the history of their previous actions and thoughts (from chain of thoughts) since the start of the episode. This simple mechanism provides a crude memorization mechanism to otherwise memory-less agents, giving them more chances of solving L2 and L3 tasks.\nAction space All our agents use the high-level action space from BrowserGym, restricted to the chat, infeas and bid action sets, which allow sending messages to the chat, declaring a task infeasible, and interacting with the current webpage via primitives using element identifiers (bid attribute), e.g., clicking on an element, filling a text box, etc. Our agents are restricted to producing only one action at a time (as in [Drouin et al., 2024]), and implement a retry mechanism that can re-prompt the agent up to 4 times in case of parsing errors in their output.\nLanguage models We evaluate closed-source models GPT-3.5 and GPT-4 (OpenAI, 2023] and study the impact of providing screenshots of the pages with GPT-4 vision. On the open-source side, we evaluate Llama3-70b [Meta, 2024] and Mixtral-8x22b [Jiang et al., 2024], deployed using Hugging Face's Text Generation Inference (TGI) library on 4 A100 GPUs.. We use a maximum context length of 40K tokens for GPT-4, 15K for GPT-3.5, 8K for Llama3 and 32K for Mixtral. To ensure that prompts do not exceed those limits, we progressively truncate the accessibility tree from the end until it fits in the context. For more information on agent design and the setup, please refer to \u00a7 B."}, {"title": "4.3 Agent Results", "content": "We evaluate all baseline agents on the standardized curriculum introduced in \u00a74.1 and report the results in Tab. 1. A notable takeaway is that all agents, whether closed-source or open-source, and regardless of being LLM or VLM-based, generally fail to achieve any reasonable success on WorkArena++, despite performing reasonably well on existing benchmarks. Only GPT-4 and GPT-4-v succeed at some tasks, particularly memorization tasks within the L2 set, with no successes observed in the L3 set. Interestingly, we observe that, in contrast to its unimodal counterpart GPT-4, the GPT-4-v agent succeeds at solving a few retrieval tasks involving reading values off charts, suggesting that the vision modality can be beneficial in WorkArena++. Additionally, as expected, the GPT-4-based agent significantly outperforms its GPT-3.5 counterpart.\nThese results raise important questions: i) Are these tasks actually solvable? and ii) Why do the agents fail? In what follows, we address each of these questions through human evaluation (\u00a74.4) and error analysis (\u00a74.5), respectively."}, {"title": "4.4 Human Evaluation", "content": "To assess the feasibility of the benchmark and measure the gap between humans and agents, we conducted a study with 15 human subjects tasked with solving WorkArena++ tasks. Given the limited number and availability of subjects, we devised a shorter curriculum comprising 98 task instances, sampled uniformly across skills and the L2/L3 sets. Each participant solved a subset of the tasks using a custom-made evaluation tool that exactly matched the interface available to agents. We report these results in Tab. 1 under the Human Curriculum column, along with the corresponding performance of our GPT-4 agent on the same subset of tasks. The numbers are striking, with an overall 93.9% success rate for humans and 2.1% for GPT-4. These establish WorkArena++ as a valuable benchmark that is both solvable and relatively straightforward for humans, while being challenging for state-of-the-art LLMs, emphasizing its value as a new milestone for the community.\nWe note that all subjects consented to participate in the study without compensation. Most had little to no familiarity with ServiceNow products and underwent only a brief training session, consisting of a 15-minute video outlining the components in Fig. 2a, followed by 15 minutes of self-guided exploration on the platform. Details on the task curriculum, training received, demographics, and the evaluation platform are included in \u00a7 A."}, {"title": "4.5 Error Analysis", "content": "To understand the poor performance of agents on WorkArena++, we conducted an in-depth study of their execution traces. We focused on the best-performing open- and closed-source models, Llama3 and GPT-4. This analysis identified salient types of errors, which sheds light on current model limitations and highlight areas for improvement.\nInformation Retrieval The models tend to successfully navigate to the correct information sources (e.g., dashboards). However, they occasionally fail to accurately retrieve the relevant information from the observations. Furthermore, agents sometimes fail to identify relevant elements on the page and attempt to act on incorrect ones.\nExploration Some tasks require to explore the page for hidden information, such as opening different tabs in a form or expanding a section of foldable elements. The agents often struggle due to a lack of curiosity, leaving them stuck in their location.\nHallucination Made-Up Actions: The models sometimes hallucinate actions that would be convenient for the task at hand, but that are not available in BrowserGym. Imaginary Buttons: Similarly, we observed cases of interaction with made-up buttons that would solve tasks in one click, such as buttons that create the exact filter required. Asking for Help: When confused about the next steps, models sometimes ask for help via chat, indicating a lack of confidence or capacity in planning the next steps.\nGoal Understanding Thought/Action Consistency: There are instances where the models' thought process correctly identifies the next action, but the action produced is different and incorrect. This inconsistency undermines performance. Low-Level Understanding in L3 Tasks: In more complex L3 tasks, the models fail to comprehend the necessary subtasks fully. For example, they might start by attempting to modify ticket values that are locked, showing a misunderstanding of the task requirements.\nAction Consequences Assessment Hallucinated Consequences: The models often hallucinate the consequences of their actions, believing they have made progress on the task when no actual progress has occurred. Repeated Actions: When an action does not change the state of the webpage, models tend to retry the same action repeatedly instead of trying a different approach.\nThese errors illustrate the current limitations of state-of-the-art web agents in handling complex enterprise tasks. Addressing these issues is crucial for developing more reliable and effective autonomous agents capable of performing real-world knowledge work. Detailed examples are included in \u00a7 F."}, {"title": "5 Related Work", "content": "Early benchmarks for web agents primarily utilized synthetic web environments where agents executed low-level keyboard and mouse actions [Shi et al., 2017b,a, Liu et al., 2018]. More recently, Zhou et al. [2023] introduced WebArena, comprising 190 tasks based on realistic websites that emulate real-world domains such as e-commerce, social forums, and content management. OSworld [Xie et al., 2024] introduces a scalable, real computer environment for benchmarking multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across operating systems.\nIn terms of datasets, Deng et al. [2023] proposed Mind2Web, a large-scale collection of 2,000 web interactions from 137 websites curated by human annotators. Similarly, L\u00f9 et al. [2024] introduced WebLINX, a curated dataset of web interactions with 2337 expert demonstrations from 155 different real-world websites. He et al. [2024] proposed 300 information-retrieval tasks from 15 real-world consumer websites, evaluating WebVoyager, a vision-based web agent's capabilities. WorkArena [Drouin et al., 2024] focuses on real-world enterprise software applications by including 33 interactions tasks representative of realistic workflows typically performed by knowledge workers.\nBuilding on this foundation, WorkArena++ introduces tasks requiring advanced skills like problem-solving and data-driven decision-making. Unlike previous benchmarks, WorkArena++ evaluates agents on their ability to perform complex, multi-step tasks that closely mimic real-world enterprise scenarios. Additionally, WorkArena++ emphasizes task isolation, uniform setup, and robust evaluation guidelines, enabling fair comparisons and reproducibility. This approach makes WorkArena++ unique in evaluating the capabilities of LLM and VLM-based web agents."}, {"title": "6 Conclusion and Future Work", "content": "We propose WorkArena++, a novel benchmark consisting of 682 tasks to evaluate web agents by mimicking realistic workflows performed routinely by knowledge workers using enterprise software. WorkArena++tests various complex skills of LLM and VLM-based agents including planning, decision-making, retrieval, logical and arithmetic reasoning, as well as the ability to identify infeasible tasks. Our benchmark promotes standardized evaluation, realistic visual diversity, and provides a method for generating large amounts of fine-tuning data in the form of web-interaction traces. Empirical evaluations reveal that state-of-the-art LLMs and VLMs struggle with our benchmark, while humans achieve extremely high performance. Through our error analyses and qualitative studies, we hope WorkArena++ will be a significant step towards developing more capable autonomous web agents.\nIn future work, we aim to continue developing new sets of tasks on the ServiceNow platform. Of particular importance would be tasks for evaluating safety and cybersecurity around agents as well as a hidden test set for hosting competitions. Furthermore, our framework offers the ability to generate a vast amount of fine-tuning data through web interaction traces to train more robust LLM and VLM-based web agents. Our ultimate goal is to close the significant gap between autonomous agents and humans on WorkArena++ and other benchmarks."}, {"title": "7 Limitations and Potential Societal Impacts", "content": "Limitations While WorkArena++ includes diverse and realistic workflows, it does not exhaustively cover all possible knowledge-work tasks and personas. Achieving comprehensive coverage would require thousands of additional tasks. However, our task set is designed to be easily extendable by the community, and we welcome such contributions. Additionally, while this work evaluates the reasoning abilities of LLM-based web agents, it does not assess their safety and robustness against various malicious behaviors, which remains an important barrier to their adoption in real-world settings. Moreover, the benchmark does not include tasks that require interaction with software external to the ServiceNow platform, which would improve diversity and realism. We leave such assessments to OS-level benchmarks like that of Xie et al. [2024]. Finally, we note that additional open and closed-source LLMs and VLMs could have been included in the experiments, particularly those with extremely long context lengths, such as Gemini 1.5 pro with 1 million tokens [Reid et al., 2024].\nSocietal Impacts This work is likely to inspire the development of agents as valuable workplace assistants, positively impacting society in several ways. It can increase productivity, enabling agents to handle more complex and value-creating tasks. Additionally, it can improve accessibility for impaired users, potentially opening new job opportunities. However, there are potential negative impacts. Advanced agents may lead to job displacement as such systems take over human tasks. Reliance on these agents raises data privacy and security concerns and could erode human skills and decision-making abilities over time. Moreover, the significant computational resources required to support these agents lead to substantial energy use, contributing to negative environmental impacts."}, {"title": "A Human Evaluation \u2013 Additional Details", "content": "This section provides additional details on our human evaluation study, where humans were tasked with solving WorkArena++ tasks.\nParticipants We recruited a cohort of 15 volunteers with varying levels of familiarity with ServiceNow products. This group included core members of ServiceNow's research team and students from local institutions, namely the University of Montreal and the \u00c9cole de technologie sup\u00e9rieure. All participants provided informed consent as outlined in the consent form (see Fig. 5). The demographic profile of the participants is depicted in Fig. 6.\nProtocol An in-person event was conducted at the ServiceNow Montreal office. The session began with a brief training, during which participants watched a 15-minute recorded talk (slides shown in Fig. 7). After the talk, participants engaged in 15 minutes of hands-on self-exploration of ServiceNow products using the same free Personal Developer Instances employed in the main study. Following this self-exploration period, each participant was asked to solve up to seven tasks individually on the evaluation platform described below. The curriculum from which these tasks were sampled is also described below. Participants were instructed not to discuss the tasks among themselves. They were informed that both their time to resolution and success rate would be recorded. The exact web page that participants used for guidelines on the day of the event is available in the benchmark's GitHub Wiki.\nTask curriculum Due to the limited time and availability of human evaluators, we had to limit the size of the curriculum used in the study. Hence, we evaluated humans based on a curriculum of 98 task instances, sampled uniformly at random across skills (\u00a7 3.2), and considered both their L2/L3 variants (49 task instances x 2 levels). The exact curriculum used can be retrieved from the benchmark's codebase.\nEvaluation platform The human evaluators were asked to solve tasks in a UI that identically matches that available to the agents (i.e., BrowserGym [Drouin et al., 2024]), except for the addition of a \u201cHuman Evaluation Console\u201d overlay (see Fig. 8). This console provided functionalities such as a \"Validate\" button that allowed them to check if they had completed the task successfully and a \"Give up\" button that allowed them to abandon the task (counting as a failure). Other noteworthy features include: i) an auto-validation mechanism that constantly checked the completion status of the page in real time, complementing the validation button, and ii) the console's movability to prevent blocking the evaluator's view of the page. Finally, note that, humans did not receive feedback while completing the tasks; they could only see if the task was currently incomplete or solved. The code for the human evaluation platform is included in our main codebase, along with a command-line program to launch it.\nLimitations We disclose two limitations of this study. First, we noticed that participants became increasingly efficient at solving tasks as they progressed through their assigned curriculum, primarily due to gaining familiarity with the product. While our protocol does not directly account for this learning effect, tasks were presented to participants in a random order, which helps avoid biasing the results for any given task and mitigates this issue. Second, during the study, multiple participants encountered the same issue due to the user interface being counterintuitive, which led to a general announcement. The announcement clarified that a ticket would not be marked as \u201cClosed - Complete\" until the form was saved via the \u201cUpdate\u201d button. This is important for L3 tasks, as such tasks are only considered complete when the agent marks the ticket as \u201cClosed - Complete.\" Although most"}, {"title": "B Agent Design", "content": "Below are the general design choices of our LLM/VLM-based web agent with chain-of-thought prompting [Wei et al., 2022b].\nLanguage models: Our study distinguishes between closed- and open-source LLMs. For the closed-source segment, we evaluate GPT-3.5 (gpt-3.5-turbo-1106, 16K context) and GPT-4 (OpenAI, 2023] (gpt-4-2024-05-13, 128K context), through OpenAI's API. We also explore the effect of providing the screenshot of the page using GPT-4 vision and Set-of-Mark [Yang et al., 2023] as proposed in WebVoyager [He et al., 2024]. In the realm of open-source LLMs, we evaluate both Llama3-70b [Meta, 2024] (meta-llama-3-70B-instruct, 8K context) and Mixtral 8x22B [Jiang et al., 2024] (open-mixtral-8x22b, 64K context). These model are deployed using Hugging Face's Text Generation Inference (TGI) library on 4 A100 GPUs.\nObservation space: Our observation space is composed of the goal, the current page's HTML and/or AXTree, the currently focused element, and the error from the previous action if any. We also augment each element with two extra boolean properties provided by BrowserGym, clickable and visible. Finally, when using GPT-4 vision we also give the model a screenshot of the current page, augmented with set-of-marks [He et al., 2024] using the element identifiers (bid) provided by BrowserGym.\nAction space: We use BrowserGym's high-level action space with chat, infeas and bid primitives [Drouin et al., 2024] which respectively allow the agent to send messages to the chat ('send_msg_to_user(text)', necessary for information retrieval tasks), to declare the task infeasible (report_infeasible(reason), necessary for infeasible tasks), and to interact with the page's HTML elements using their unique identifiers (e.g., click(bid), type(bid, text) etc.). The bid primitives rely on the unique bid attribute given by BrowserGym to each HTML element, which is made available textually in the HTML and AXTree as well as visually through set-of-marks in the screenshots. The full action space is described to the agent in the prompt, with individual examples of valid function calls for each primitive. For an example prompt and the corresponding screenshot with set-of-marks, see Fig. 9 and Fig. 10.\nHistory: To extend the horizon window of our agent, at each time step we re-inject into the agent's prompt the history of all previous actions and thoughts (from chain-of-thought) since the start of the episode. This gives our agent a chance to recall its previous thoughts, thereby providing a crude memorization mechanism to otherwise memory-less agents, giving them more chances (theoretically) of solving memory-intensive L2 and L3 tasks.\nZero-shot examples: In the prompt, we provide a single generic example of how the chain-of-thought and action outputs should be formatted. This contrasts with other methods [Kim et al., 2023] where task-specific few-shot examples are provided, yet aligns with our objective of developing zero-shot agents able to solve a large range of new tasks.\nParse and retry: Once the LLM provides an answer, we have a parsing loop that can re-prompt the agent up to 4 times to make it aware of a parsing mistake. This can save the agent from making basic mistakes and is mainly useful for less capable LLMs. Once parsed, the action is executed via BrowserGym, which moves to the next step.\nPrompt truncation: We use a maximum prompt length of 40K tokens for GPT-4, 15K for GPT-3.5, 8K for Llama3 and 32K for Mixtral. When the prompt is too large, we progressively truncate the HTML and AXTree from the end until it fits the maximum allowed number of tokens."}, {"title": "C Task details", "content": "We show examples of goals given to the agent in the chat window under the \"L2\" difficulty and the task description seen by the agent under the \"L3\" difficulty for all the tasks within each skill category in WorkArena++. We also follow up (\u00a7 C.6) by showing all the 'protocols' the agent can refer to when solving an \"L3\" difficulty task mimicing a real-world scenario faced by knowledge workers."}, {"title": "C.1 Planning and Problem Solving (66 \u00d7 2 tasks)", "content": "We devise tasks that require the agent to solve common enterprise problems that require decision making while respecting constraints. The tasks require the agent to follow a global plan that dictates the problems, constraints, and the desired outcome."}, {"title": "C.1.1 Workload balancing", "content": "Redistribute non-uniformly assigned work across agents to make the workload balanced among them. We introduce more complexities based on the number of problems that need to be reassigned. In total, we have 3 workload balancing tasks across each L2 and L3 difficulty levels. We show the L2 goal and L3 task description in Fig. 11. The protocol for the task is in Fig. 23."}, {"title": "C.1.2 Work assignment", "content": "Assign work (problems) across different categories such as software and hardware to relevant expert agents. We increase complexity by requiring the agent to assign critical problems based on their priority to more experienced agents under the different categories. In total, we have 6 work assignment tasks across each L2 and L3 difficulty levels. We show the L2 goal and L3 task description in Fig. 12 for a work assignment task requiring the agent to assign problems based on the expertise of the category experts. The protocol for the task is in Fig. 24."}, {"title": "C.1.3 Scheduling requests", "content": "Schedule multiple problem requests based on constraints such as allowed time frame, overlap with other requests, impact and criticality of the problem, and allocated time to each request based on their risk. In total, we have 48 request scheduling tasks across each L2 and L3 difficulty levels. We show the L2 goal and L3 task description in Fig. 13 for a scenario where the requests are to be scheduled in an order based on their impact. The protocol for the task is in Fig. 25."}, {"title": "C.1.4 Deduplicate problems", "content": "Mark problems that share the same problem statement as duplicates of each other. We introduce further complexity here for cases where the agent has to take into account the 'priority' of the problem while marking duplicates. In total, we have 9 problem deduplication tasks across each L2 and L3 difficulty levels. We show the L2 goal and L3 task description in Fig. 14. The protocol for the task is in Fig. 26."}, {"title": "C.2 Information Retrieval (39 \u00d7 2 tasks)", "content": "To evaluate the retrieval ability of web agents, we"}]}