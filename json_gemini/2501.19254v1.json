{"title": "Linear Q-Learning Does Not Diverge: Convergence Rates to a Bounded Set", "authors": ["Xinyu Liu", "Zixuan Xie", "Shangtong Zhang"], "abstract": "Q-learning is one of the most fundamental reinforcement learning algorithms. Previously, it is widely believed that Q-learning with linear function approximation (i.e., linear Q-learning) suffers from possible divergence. This paper instead establishes the first $L^2$ convergence rate of linear Q-learning to a bounded set. Notably, we do not make any modification to the original linear Q-learning algorithm, do not make any Bellman completeness assumption, and do not make any near-optimality assumption on the behavior policy. All we need is an $\\epsilon$-softmax behavior policy with an adaptive temperature. The key to our analysis is the general result of stochastic approximations under Markovian noise with fast-changing transition functions. As a side product, we also use this general result to establish the $L^2$ convergence rate of tabular Q-learning with an $\\epsilon$-softmax behavior policy, for which we rely on a novel pseudo-contraction property of the weighted Bellman optimality operator.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL, Sutton & Barto (2018)) emerges as a powerful paradigm for training agents to make decisions sequentially in complex environments. Among various RL algorithms, Q-learning (Watkins, 1989; Watkins & Dayan, 1992) stands out as one of the most celebrated (Mnih et al., 2015). The original Q-learning in Watkins (1989) uses a look-up table for representing the action-value function. To improve generalization and work with large or even infinite state spaces, linear function approximation is used to approximate the action value function, yielding linear Q-learning.\nLinear Q-learning is, however, widely believed to suffer from possible divergence (Baird, 1995; Sutton & Barto, 2018). In other words, the weights of linear Q-learning can possibly diverge to infinity as learning progresses. However, Meyn (2024) recently proves that when an $\\epsilon$-softmax behavior policy with an adaptive temperature is used, linear Q-learning does not diverge. Instead, the weights converge to a bounded set almost surely. Built on this success, we further provide a nonasymptotic analysis of linear Q-learning. Specifically, we establish the first $L^2$ convergence rate of linear Q-learning to a bounded set. Notably, this work differs from many previous analyses of linear Q-learning in that, except for the aforementioned behavior policy, we do not make any modification to the original linear Q-learning algorithm (e.g., no target network, no weight projection, no experience replay, no i.i.d. data, no regularization). We also use the weakest possible assumptions (e.g., no Bellman completeness assumption, no near-optimality assumption on the behavior policy). Table 1 summarizes the improvements.\nOur $L^2$ convergence rate is made possible by a novel result concerning the convergence of stochastic approximations under fast-changing time-inhomogeneous Markovian noise, where the transition function of the Markovian noise evolves as fast as the stochastic approximation weights, i.e., there is only a single timescale. By contrast, Zhang et al. (2022) obtain similar results only with a two-timescale framework, where the transition function of the Markovian noise evolves much slower than the stochastic approximation weights.\nAs a side product, we also use this general stochastic approximation result to establish the $L^2$ convergence rate of tabular Q-learning with an $\\epsilon$-softmax behavior policy. To our knowledge, this is the first time that such convergence rate is established. The key to this success is the identification of a novel pseudo-contraction property of the weighted Bellman optimality operator. Table 2 summarizes the improvements over previous works."}, {"title": "2. Background", "content": "Notations. We use $\\langle x, y \\rangle = x^T y$ to denote the standard inner product in Euclidean spaces. A function $f$ is said to be $L$-smooth (w.r.t. some norm $\\| \\cdot \\|_{\\triangledown}$) if $\\forall w, w'$,\n$$f(w') \\leq f(w) + \\langle \\nabla f(w), w' - w \\rangle + \\frac{L}{2} ||w' - w||^2.$$ \nSince all norms in finite dimensional spaces are equivalent, when we say a function is smooth in this paper, it means it is smooth w.r.t. any norm (with $L$ depending on the choice of the norm). In particular, to simplify notations, we in this paper use $\\| \\cdot \\|$ to denote an arbitrary vector norm such that its square $\\| \\cdot \\|^2$ is smooth. We abuse $\\| \\cdot \\|$ to also denote the corresponding induced matrix norm. We use $\\| \\cdot \\|_{*}$ to denote the dual norm of $\\| \\cdot \\|$. We use $\\| \\cdot \\|_2$ and $\\| \\cdot \\|_\\infty$ to denote the $l_2$ and infinity norm. We use functions and vectors exchangeably when it does not confuse. For example, $f$ can denote both a function $S \\rightarrow \\mathbb{R}$ and a vector in $\\mathbb{R}^{|S|}$.\nWe consider an infinite horizon Markov Decision Process (MDP, Bellman (1957)) defined by a tuple $(\\mathcal{S}, \\mathcal{A}, p, r, \\gamma, p_0)$, where $\\mathcal{S}$ is a finite set of states, $\\mathcal{A}$ is a finite set of actions, $p : \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1]$ is the transition probability function, $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function, $\\gamma \\in [0,1)$ is the discount factor, and $p_0 : \\mathcal{S} \\rightarrow [0,1]$ denotes the initial distribution. At the time step 0, an initial state $S_0$ is sampled from $p_0$. At the time step $t$, an action $A_t$ is sampled according to some policy $\\pi : \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0,1]$, i.e., $A_t \\sim \\pi(\\cdot|S_t)$. A reward $R_{t+1} = r(S_t, A_t)$ is then emitted and a successor state $S_{t+1}$ is sampled from $p(\\cdot|S_t, A_t)$. We use $P_\\pi$ to denote the transition matrix between state action pairs for an arbitrary policy $\\pi$, i.e., $P_\\pi[(s, a), (s', a')] = p(s'|s, a)\\pi(a'|s')$. We use $q_\\pi : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ to denote the action value function of a policy $\\pi$, which is defined as $q_\\pi(s, a) = \\mathbb{E}_\\pi [\\sum_{i=0}^\\infty \\gamma^i R_{t+i+1}|S_t = s, A_t = a]$. One fundamental task in RL is control, where the goal is to find the optimal action value function, denoted as $q_*$, satisfying $q_*(s, a) \\geq q_\\pi(s, a) \\forall \\pi, s, a$. It is well-known that the $q_*$ is the unique fixed point of the Bellman optimality operator $T : \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|} \\rightarrow \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}$ defined as $(Tq)(s, a) = \\sum_{s'} p(s'|s, a) [r(s, a) + \\gamma \\max_{a'} q(s', a')]$.\nTabluar Q-Learning. Q-learning is the most celebrated method to estimate $q_*$. In its simplest form, it uses a lookup table $q \\in \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}$ to store the estimate of $q_*$ and generates the iterates ${q_t}$ as\n$$A_t \\sim \\mu_{q_t}(S_t), \\quad \\text{(tabular Q-learning)}$$\n$$\\delta_t = R_{t+1} + \\gamma \\max_{a'} q_t(S_{t+1}, a') - q_t(S_t, A_t),$$\n$$q_{t+1}(S_t, A_t) = q_t(S_t, A_t) + \\alpha_t\\delta_t.$$\nHere, {$\\alpha_t$} are learning rates and $\\mu_q$ is the behavior policy. In this paper, we consider an $\\epsilon$-softmax behavior policy defined as\n$$\\mu_q(a|s) = \\frac{\\epsilon}{|\\mathcal{A}|} + (1 - \\epsilon) \\frac{\\exp(q(s,a))}{\\sum_{b} \\exp(q(s,b))},$$ \nwhere $\\epsilon \\in (0,1]$ controls the degree of exploration.\nLinear Q-Learning. To promote generalization or work with large state spaces, Q-learning can be equipped with linear function approximation, where we approximate $q_*(s,a)$ with $x(s,a)^T w$. Here $x:\\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}^d$ is the feature funciton that maps a state action pair to a $d$-dimensional feature and $w \\in \\mathbb{R}^d$ is the learnable weight. Linear Q-learning generates the iterates ${w_t}$ as\n$$A_t \\sim \\mu_{w_t}(S_t), \\quad \\text{(linear Q-learning)}$$\n$$\\delta_t = R_{t+1} + \\gamma \\max_{a'} x(S_{t+1}, a')^T w_t - x(S_t, A_t)^T w_t,$$\n$$w_{t+1} = w_t + \\alpha_t\\delta_t x(S_t, A_t).$$\nHere we have abused $\\mu_w$ to also denote the behavior policy used in (linear Q-learning), which is defined as\n$$\\mu_w(a|s) = \\frac{\\frac{\\epsilon}{|\\mathcal{A}|} + (1 - \\epsilon) \\frac{\\exp(\\kappa_w x(s,a)^Tw)}{\\sum_{b} \\exp(\\kappa_w x(s,b)^Tw)},$$ \nwhere $\\epsilon \\in (0,1]$ and the temperature parameter $\\kappa_w$ is defined as\n$$\\kappa_w = \\begin{cases} \\frac{\\kappa_0}{\\|w\\|_2} & \\|w\\|_2 \\geq 1\\\\ \\kappa_0 & \\text{otherwise} \\end{cases},$$"}, {"title": "3. Main Results", "content": "Assumption 3.1. The Markov chain {$S_t$} induced by a uniformly random behavior policy is irreducible and aperiodic.\nNotably, since $\\epsilon$ is required to be strictly greater than 0, Assumption 3.1 ensures that for any $w$ and any $q$, the Markov chain induced by $\\mu_w$ and $\\mu_q$ is also irreducible and aperiodic. This is a common assumption to analyze time-inhomogeneous Markovian noise (Zhang et al., 2022).\nAssumption LR. The learning rate is $\\alpha_t = \\frac{a}{(t+t_0)^{e_{\\alpha}}}$, where $\\epsilon_{\\alpha} \\in (0.5, 1], a > 0, t_0 > 0$ are constants.\nTheorem 1 ($L^2$ Convergence Rate of Linear Q-Learning). Let Assumptions 3.1 and LR hold. Then for sufficiently small $\\epsilon$ in (3), sufficiently large $\\kappa_0$ in (4), and sufficiently large $t_0$ in $\\alpha_t$, there exist some constant $\\bar t$ such that the iterates {$w_t$} generated by (linear Q-learning) satisfy the following statements $\\forall t > \\bar t$.\n(1) When $\\epsilon_{\\alpha} = 1$, there exist some constants $B_{1,1}, B_{1,2}$, and $B_{1,3}$ such that\n$$\\mathbb{E}[\\| w_t \\|_2^2] \\leq \\frac{B_{1,1}}{(t+t_0)^{B_{1,2}}} \\|w_0\\|_2^2 + B_{1,3}.$$\n(2) When $\\epsilon_{\\alpha} \\in (0.5, 1)$, there exist some constants $B_{1,4}, B_{1,5}$ and $B_{1,6}$ such that\n$$\\mathbb{E} \\bigg[ \\frac{\\| w_t \\|_2^2}{\\exp \\big[ \\frac{B_{1,5}}{1 - \\epsilon_{\\alpha}} (t+t_0)^{1-\\epsilon_{\\alpha}} \\big] } \\bigg] \\leq B_{1,4} \\exp \\bigg[ \\frac{B_{1,5}}{1 - \\epsilon_{\\alpha}} (t+t_0)^{1-\\epsilon_{\\alpha}} \\bigg] \\|w_0\\|_2^2 + B_{1,6}.$$\nThe proof of Theorem 1 is in Section 5.2. The precise constraints on $\\epsilon_0$ and $\\epsilon_6$ are specified in Lemma 16 in Appendix. The constants $B_{1,3}$ and $B_{1,6}$ depend on $\\gamma, |\\mathcal{S}|, |\\mathcal{A}|, \\max_{s,a} r(s, a)$, and $\\max_{s,a} \\|x(s, a)\\|_2$, with the detailed dependency specified in Section C.4. Theorem 1 confirms the main claim of the work, with $B_{1,3}$ and $B_{1,6}$ corresponding to the bounded set.\nTheorem 2 ($L^2$ Convergence Rate of Tabular Q-Learning). Let Assumptions 3.1 and LR hold. Then for sufficiently large $t_0$ in $\\alpha_t$, there exist some constant $\\bar t$ such that the iterates {$w_t$} generated by (tabular Q-learning) satisfy the following statements $\\forall t > \\bar t$.\n(1) When $\\epsilon_{\\alpha} = 1$, there exist some constants $B_{2,1}, B_{2,2}$, and $B_{2,3}$ such that\n$$\\mathbb{E}[\\|q_t-q_*\\|_2^2]"}, {"title": "5. Proofs of the Main Results", "content": "5.1. Proof of Theorem 3\nProof. We start by recalling that we use $\\| \\cdot \\|$ to denote an arbitrary norm where $\\| \\cdot \\|^2$ is smooth. In particular, $\\| \\cdot \\|_2^2$ is smooth w.r.t. $\\| \\cdot \\|$ for some $L$. In (1), identifying $f(w)$ as $L(w)$, $w'$ as $w_{t+1}$, and $w$ as $w_t$ yields\n$$L(w_{t+1}) \\leq L(w_t) + \\alpha_t \\langle \\nabla L(w_t), H(w_t, Y_{t+1}) \\rangle + \\frac{L\\alpha_t^2}{2} ||H(w_t, Y_{t+1})||^2$$\n$$= L(w_t) + \\alpha_t \\langle \\nabla L(w_t), h(w_t) \\rangle + \\alpha_t \\langle \\nabla L(w_t), H(w_t, Y_{t+1}) - h(w_t) \\rangle + \\frac{L\\alpha_t^2}{2} ||H(w_t, Y_{t+1})||^2.$$\nWe now proceed to bounding the noise term including $H(w_t, Y_{t+1})-h(w_t)$. To this end, we notice that according to Lemma 1 of Zhang et al. (2022), Assumption A1 implies that the Markov chains in $\\mathcal{A}_P$ mix both geometrically and uniformly. In other words, there exist constants $C_0 > 0$ and $\\tau \\in (0,1)$, such that\n$$\\sup_{w,y} \\sum_{y'} |P_w (y, y') - d_{y,w}(y')| \\leq C_0 \\tau^n.$$\nWe then define\n$$\\tau_a = \\min {n \\geq 0 \\mid C_0 \\tau^n \\leq \\alpha_t }$$\nto denote the number of steps that the Markov chain needs to mix to an accuracy $\\alpha_t$. This allows us to decompose the noise term as\n$$\\langle \\nabla L(w_t), H(w_t, Y_{t+1}) - h(w_t) \\rangle = \\langle \\nabla L(w_t) - \\nabla L(w_{t-\\tau_a}), H(w_t, Y_{t+1}) - h(w_t) \\rangle$$\n$$+\\langle \\nabla L(w_{t-\\tau_a}), H(w_t, Y_{t+1}) - H(w_{t-\\tau_a}, Y_{t+1}) + h(w_{t-\\tau_a}) - h(w_t) \\rangle$$\n$$+\\langle \\nabla L(w_{t-\\tau_a}), H(w_{t-\\tau_a}, Y_{t+1}) - h(w_{t-\\tau_a}) \\rangle.$$\nBoth $T_1$ and $T_2$ can be bounded with Lipschitz continuity. To bound $T_3$, we introduce an auxiliary Markov chain {$\\tilde Y_t$} akin to Zou et al. (2019). The chain {$\\tilde Y_t$} is constructed to be identical to {$Y_t$} up to time step $t - \\tau_a$, after which it evolves independently according to the fixed transition matrix $P_{w_{t-\\tau_a}, \\cdot}$. By contrast, {$Y_t$} continues to evolve according to the changing transition matrix $P_{w_{t-\\tau_a}, \\cdot}, P_{w_{t-\\tau_a+1}, \\cdot},...$. This choice of $\\tau_a$ ensures that the discrepancy between the two chains is sufficiently small. We now further decompose $T_3$ with this auxiliary chain as\n$$\\langle \\nabla L(w_{t-\\tau_a}), H(w_{t-\\tau_a}, Y_{t+1}) - h(w_{t-\\tau_a}) \\rangle = \\langle \\nabla L(w_{t-\\tau_a}), H(w_{t-\\tau_a}, \\tilde Y_{t+1}) - h(w_{t-\\tau_a}) \\rangle$$\n$$+\\langle \\nabla L(w_{t-\\tau_a}), H(w_{t-\\tau_a}, Y_{t+1}) - H(w_{t-\\tau_a}, \\tilde Y_{t+1}) \\rangle.$$\nWe now bound the terms one by one.\n\nLemma 1. There exists a constant $C_1$ such that\n$$T_1 \\leq C_1 \\alpha_{t-\\tau_a,t} \\alpha_{t-\\tau_a}t^{-1}(L(w_t) + 1).$$\nThe proof is in Section B.1.\nLemma 2. There exists a constant $C_2$ such that\n$$T_2 \\leq C_2 \\alpha_{t-\\tau_a} \\alpha_{t-\\tau_a}t^{-1}(L(w_t) + 1).$$\nThe proof is in Section B.2.\nLemma 3. There exists a constant $C_3$ such that\n$$\\mathbb{E}[T_{31}] \\leq C_3 \\alpha_t (\\mathbb{E}[L(w_t)] + 1).$$\nThe proof is in Section B.3.\nLemma 4. There exists a constant $C_4$ such that\n$$\\mathbb{E}[T_{32}] \\leq C_4 \\alpha_{t-\\tau_a} \\alpha_{t-\\tau_a}t^{-1} \\ln(t + t_0 + 1)(\\mathbb{E}[L(w_t)] + 1).$$\nThe proof is in Section B.4.\n\nRemark 1. We prove Lemma 4 considering Assumption A3 or A3' in two cases. Under Assumption A3', our proof is similar to those in Zou et al. (2019); Zhang et al. (2022). The technical novelty here lies in the proof under Assumption A3. The proof under Assumption A3 involves bounding the term $||P_{w_t} - P_{w_{t-\\tau_a}}||$. In Zou et al. (2019), a similar term is bounded by $||w_t - w_{t-\\tau_a}||$ via the standard Lipschitz continuity of $P_w$ (cf. Assumption A3'). Zou et al. (2019) further rely on a projection operator to bound $||w_t - w_{t-\\tau_a}||$ directly. However, (SA) does not have such a projection operator. In Zhang et al. (2022), a similar term is bounded under the assumption that the transition matrix is controlled by another set of weights {$\\theta_t$}, which involve much slower than {$w_t$}. Their bound is made possible essentially because of this two-timescale setup. However, (SA) only has a single timescale where the transition matrix evolves as fast as {$w_t$}. We instead use a stronger form of Lipschitz continuity in Assumption A3.\nAssembling the bounds in the above lemmas to (9) and further to (6), we complete the proof of Theorem 3. The detailed steps are presented in Section B.5."}, {"title": "5.2. Proof of Theorem 1", "content": "Proof. We first rewrite (linear Q-learning) in the form of (SA). To this end, we define $Y_{t+1} = (S_t, A_t, S_{t+1})$, which evolves in a finite space\n$$\\mathcal{Y} = \\{(s \\in \\mathcal{S}, a \\in \\mathcal{A}, s' \\in \\mathcal{S}) \\mid p(s'|s, a) > 0 \\}.$$ We further define\n$$H(w, y)$$\n$$=(r(s, a) + \\gamma \\max_{a'} x(s', a')^Tw - x(s,a)^Tw)x(s,a).$$\nwhere we have used $y = (s,a, s')$. We now proceed to verify the assumptions of Theorem 3. We identify the norm used in Theorem 3 as the $l_2$ norm $\\| \\cdot \\|_2$. For Assumption A1, we define $P_w$ as\n$$P_w [(s_0, a_0, s'_0), (s_1, a_1, s'_1)] = \\mu_w(a_1|s_1)p(s_1|s_1,a_1)I_{s'_0 = s_1},$$\nwhere $I$ is the indicator funciton. Then it is easy to see that\n$$Pr(Y_{t+1}|Y_t) = \\mu_{w_t}(A_t|S_t)p(S_{t+1}| S_t, A_t)$$\n$$=P_{w_t}[(S_{t-1}, A_{t-1}, S_t), (S_t, A_t, S_{t+1})]$$\n$$=P_{w_t} [Y_t, Y_{t+1}].$$\nThanks to Assumption 3.1 and the fact $\\epsilon > 0$ in (3), it is easy to see that for any $P \\in \\mathcal{A}_P$, $P$ induces an irreducible and aperiodic chain in $\\mathcal{Y}$. Assumption A1 is then verified. Assumption A2 trivially holds according to the definition of $H(w, y)$ in (11) since max is Lipschitz and $\\mathcal{Y}$ is finite. For Assumption A3, the first half is trivially implied by the following lemma.\nLemma 5. There exists a constant $C_5$ such that\n$$\\frac{\\mu_{w_1}(a|s)}{\\mu_{w_2}(a|s)} \\leq \\frac{C_5 \\|w_1-w_2\\|_2}{1 + \\|w_1\\|_2^2 + \\|w_2\\|_2^2} \\forall w_1, w_2, s, a.$$\nThe proof is in Section C.1, where the adaptive temperature $\\kappa_w$ plays a key role. For the second half of Assumption A3, we use $d_{\\mu_w} \\in \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}$ to denote the stationary state-action distribution induced by the policy $\\mu_w$. Then it is easy to see that $d_{y,w}(s, a,s') = d_{\\mu_w}(s,a)p(s'|s, a)$. Then it can be computed that\n$$h(w) = \\mathbb{E}_{y \\sim d_{y, w}} [H(w, y)] = A(w)w + b(w),$$\nwhere\n$$A(w) = X^T D_{\\mu_w} (\\gamma P_{\\pi_w} - I)X,$$\n$$b(w) = X^T D_{\\mu_w}r.$$\nHere, we use $X \\in \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|\\times d}$ to denote the feature matrix, the $(s, a)$-indexed row of which is $x(s, a)^T$. We use $D_{\\mu_w} \\in \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|\\times|\\mathcal{S}||\\mathcal{A}|}$ to denote a diagonal matrix whose diagonal is $d_{\\mu_w}$. We use $\\pi_w$ to denote the greedy policy, i.e.,\n$$\\pi_w(a|s) = \\begin{cases} 1 & \\text{if } a = \\arg \\max_b x(s, b)^T w \\\\ 0 & \\text{otherwise} \\end{cases},$$\nwhere tie-breaking in arg max can be done through any fixed and deterministic prodecure. We recall that $P_{\\pi_w} \\in \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|\\times|\\mathcal{S}||\\mathcal{A}|}$ denotes the state-action transition matrix of the policy $\\pi_w$. We then have\nLemma 6. There exists a constant $C_6$ such that\n$$\\|h(w_1) - h(w_2)\\|_2 \\leq C_6 \\|w_1 - w_2\\|_2 \\forall w_1, w_2.$$\nThe proof is Section C.2. Assumption A3 is then verified.\n\nRemark 2. To prove the above lemma, we need to bound a term involving $|| D_{\\mu_{w_1}} - D_{\\mu_{w_2}} ||_2 \\|w_1\\|_2$, for which we rely on the factor $1/(\\|w_1\\|_2^2 + \\|w_2\\|_2^2+1)$ in Lemma 5 to cancel the multiplicative term $\\|w_1\\|_2$. Furthermore, despite that $\\pi_w$ is not continuous, $P_{\\pi_w}Xw$ is Lipschitz continuous.\nWe now identify $w_{ref} = 0$ and thus have $L(w) = \\frac{1}{2}\\|w\\|^2$. Invoking Theorem 3 then yields\n$$\\mathbb{E}[L(w_{t+1})] \\leq (1+ f(t))\\mathbb{E}[L(w_t)] + \\alpha_t\\mathbb{E}[\\langle \\nabla L(w_t), h(w_t) \\rangle] + f(t).$$\nSince $\\nabla L(w_t) = w_t$, we now bound the inner product term of the RHS with the following lemma.\nLemma 7. There exist constants $\\beta > 0$ and $C_7 > 0$ such that\n$$\\langle w,h(w) \\rangle \\leq -\\beta\\|w\\|^2 + C_7\\|w\\|_2 \\forall w.$$\nThe proof is in Section C.3, which is an extension of Lemma A.9 of Meyn (2024). Notably, this lemma is the place where we require sufficiently large $\\kappa_0$ and sufficiently small $\\epsilon$ in (3). Plugging the bound in Lemma 7 back to (15) yields\n$$\\mathbb{E}[L(w_{t+1})] \\leq (1 - \\beta\\alpha_t + f(t))\\mathbb{E}[L(w_t)] + O(\\alpha_t).$$\nSince $f(t)$ is dominated by $\\alpha_t$, it can be seen that $\\mathbb{E}[L(w_t)]$ remains bounded as $t\\rightarrow \\infty$. By telescoping the above inequality, we can also obtain an explicit convergence rate of $\\mathbb{E}[L(w_t)]$ to a bounded set. All those details are in Section C.4, which completes the proof."}, {"title": "5.3. Proof of Theorem 2", "content": "Proof. It is obvious that (tabular Q-learning) is a special case of (linear Q-learning) with $X$ identified as the identity matrix $I$. Most of the proofs here are similar to Section 5.3. We, therefore, focus on the different part.\nAssumption A1 can be similarly verified with\n$$P_q[(s_0, a_0, s'_0), (s_1, a_1, s'_1)"}]}