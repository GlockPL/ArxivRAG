{"title": "RCTrans: Radar-Camera Transformer\nvia Radar Densifier and Sequential Decoder for 3D Object Detection", "authors": ["Yiheng Li", "Yang Yang", "Zhen Lei"], "abstract": "In radar-camera 3D object detection, the radar point clouds\nare sparse and noisy, which causes difficulties in fusing cam-\nera and radar modalities. To solve this, we introduce a novel\nquery-based detection method named Radar-Camera Trans-\nformer (RCTrans). Specifically, we first design a Radar Dense\nEncoder to enrich the sparse valid radar tokens, and then\nconcatenate them with the image tokens. By doing this, we\ncan fully explore the 3D information of each interest region\nand reduce the interference of empty tokens during the fus-\ning stage. We then design a Pruning Sequential Decoder to\npredict 3D boxes based on the obtained tokens and random\ninitialized queries. To alleviate the effect of elevation am-\nbiguity in radar point clouds, we gradually locate the posi-\ntion of the object via a sequential fusion structure. It helps\nto get more precise and flexible correspondences between to-\nkens and queries. A pruning training strategy is adopted in\nthe decoder, which can save much time during inference and\ninhibit queries from losing their distinctiveness. Extensive ex-\nperiments on the large-scale nuScenes dataset prove the supe-\riority of our method, and we also achieve new state-of-the-\nart radar-camera 3D detection results. Our implementation is\navailable at https://github.com/liyih/RCTrans.", "sections": [{"title": "Introduction", "content": "3D object detection is an important perceptual task that can\nbe widely applied in many fields. Existing high-precision de-\ntection algorithms often rely on the input of LiDAR(Yan,\nMao, and Li 2018; Yin, Zhou, and Krahenbuhl 2021). How-\never, LiDAR is expensive and easily damaged (Lin et al.\n2024), which is not conducive to the commercialization of\nthe algorithm. To reduce the practical cost, recent works be-\ngin to focus on using low-cost radars and multi-view cam-\neras to improve detection performance. Camera sensors can\nprovide rich texture information, while radar sensors can\nprovide three-dimensional information that is not affected\nby different weather conditions (Kim et al. 2023a).\nHowever, the radar sensor has two inherent drawbacks,\ni.e., sparsity and noise. Firstly, a radar typically releases\nmmWave into the physical space, which hits the object and\nthen is reflected to the receiver. Due to the lack of diffused\nreflection, only a small part of objects can be reflected to the\nradar receiver (Singh et al. 2023). The number of non-empty\nradar pillars is only approximately 10% of average numbers\nin LiDAR pillars (Bang et al. 2024). Secondly, the radar used\nin autonomous driving (such as in nuScenes (Caesar et al.\n2020)) cannot obtain an accurate height of objects, which\ncan bring the noise in azimuth (x) and depth (z) components\n(Singh et al. 2023). In addition, some noise may also be led\nto the axes when the beam-width of the mmWave radar is too\nlarge (Singh et al. 2023). The sparsity and noise of radar data\nwill bring challenges to the cross-modality fusion. Specifi-\ncally, sparsity causes too much invalid radar representation,\nwhile noise makes wrong or inaccurate alignments between\ncamera and radar representation.\nAs shown in Fig. 1, we compare RCTrans with exist-\ning models on nuScenes val set. Previous methods may\noverlook the sparsity of the radar (Kim et al. 2023b,a), or\nneed the radar cross-section (RCS) as the prior (Lin et al.\n2024). They cannot adaptively fill every empty BEV grid.\nMoreover, the SOTA radar-camera solutions (Kim et al.\n2023b; Lin et al. 2024) adopt bird's eye view (BEV) (Huang"}, {"title": "Related Work", "content": "et al. 2021) as the unified space to fuse features. Such kind\nof methods require strict correspondences between multi-\nmodality features in BEV space, but it is difficult to establish\nprecise correspondences for radar with positional offset. In-\nspired by the success of query-based 3D detection (Liu et al.\n2022; Yan et al. 2023), we use the query-based paradigm\nto establish interaction between object queries and multi-\nmodality tokens. A positional embedding is used to con-\nstruct implicit associations, allowing the model to adaptively\nlearn more flexible correspondences.\nIn this paper, we propose a novel Radar-Camera Trans-\nformer (RCTrans) framework via radar densifier and sequen-\ntial decoder to overcome the sparsity and noise of radar\ninputs. To be specific, we design a Radar Dense Encoder\n(RDE) to avoid blurring some detailed information while\ndensifying every empty BEV grid. It uses a downsample-\nthen-upsample structure and obtains multi-scale information\nthrough a skip connection. Self-attention is adopted at the\nsmallest BEV scale to adaptively fill each grid. Consider-\ning that information from different modalities can establish\nmore flexible interaction with object queries. we design the\nPruning Sequential Decoder, which gradually fuses multi-\nmodality features and locates the position of each object in\na step-by-step way. In each layer, two separate transformers\nwill be used to fuse the radar and image information, re-\nspectively. This approach allows the model to establish the\nconnections between different modalities and object queries\nbased on their distinct traits. Thereafter, we predict the new\nposition of queries and recompute position embedding after\neach decoder layer. In the next layer, we can use the up-\ndated position embedding to calculate more precise corre-\nspondences and obtain more accurate fusion results. A prun-\ning training strategy is used to speed up the inference and\nprevent object queries from losing their distinctiveness.\nWe conduct extensive experiments on the large-scale au-\ntonomous driving dataset nuScenes (Caesar et al. 2020),\nwhich demonstrates the effectiveness of our proposed RC-\nTrans. The ablation study shows that our designed Radar\nDense Encoder and Pruning Sequential Decoder both con-\ntribute to improving the performance. Our contributions are\nsummarized as:"}, {"title": "Related Work", "content": "Bird's-eye-view (BEV) is a dense 2D representation, where\neach grid corresponds to a certain region in the scene (Mao\net al. 2023). LSS (Philion and Fidler 2020) innovatively\nsolves the transformation problem from multi-view cameras\nto BEV features. BEVDet (Huang et al. 2021) is a pioneer\nin applying BEV to 3D object detection. The major diffi-\nculty of building BEV lies in inaccurate depth estimation,\nand BEVDepth (Li et al. 2023b) solves this bottleneck by\nusing additional depth supervision. In order to predict more\naccurate speed and reduce the missed detection of objects,\nmore and more works try to incorporate temporal informa-\ntion. BEVFormer (Li et al. 2022) is the first to introduce\nsequential temporal modeling in multi-view 3D object de-\ntection. SOLOFusion (Park et al. 2022) proposes a stream\nvideo paradigm to make long-term temporal modeling. BEV\ncan also serve as a unified representation space for the fusion\nof multi-modality features. BEVFusion (Liu et al. 2023c;\nLiang et al. 2022) is a pioneer in choosing BEV as a unified\nrepresentation space for fusion. BEV can provide explicit\ncorrespondence for different modalities, which is extremely\nconvenient for fusion. However, alignment errors between\ndifferent modal BEVs can affect the fusion effect. Some ap-\nproaches (Song et al. 2024; Lin et al. 2024) try to solve this\nissue via a deformable module."}, {"title": "Query-Based 3D Object Detection", "content": "The query-based method is another important paradigm in\n3D object detection. In these methods, object queries typ-\nically interact with the tokens of input data. Inspired by\nthe successful query-based methods in 2D detection (Car-\nion et al. 2020), DETR3D (Wang et al. 2022a) introduces a\nset of points to be the references, each of them represents\nan object query. Later, researchers begin to focus on how\nto effectively extract features from multi-view images to the\nobject queries. PETR (Liu et al. 2022) proposes 3D position-\naware position embedding for image features in the LiDAR\ncoordinate. CAPE (Xiong et al. 2023) constructs the posi-\ntion embedding in camera coordinates to reduce the influ-\nence of variant camera extrinsic. StreamPETR (Wang et al.\n2023) proposes a query-based long-temporal fusion algo-\nrithm that can transfer historical information from objects to\nthe current frame through object queries. PETRv2 (Liu et al.\n2023b) aggregates short-temporal information by transform-\ning the previous 3D coordinates to the current coordinate\nsystem. Some methods also explore the query-based multi-\nmodality fusion paradigm. Futr3d (Chen et al. 2023) intro-\nduces the first unified end-to-end sensor fusion framework in\n3D detection. CMT (Yan et al. 2023) proposes a 3D position\nembedding to fuse multi-modality tokens."}, {"title": "Radar-Camera 3D Object Detection", "content": "To explore low-cost 3D perception solutions, more and\nmore methods have begun to focus on using radar sensors.\nRadarNet (Yang et al. 2020) utilizes radar information to\ncomplement other sensors in the form of Doppler velocity.\nBased on the frustum-based association method, CenterFu-\nsion (Nabati and Qi 2021) accurately associates radar de-\ntections to objects in the image. What's more, it adopts a\nmiddle-fusion approach to complement the image features\nvia radar feature maps. CRAFT (Kim et al. 2023a) intro-\nduces the Spatio-Contextual Fusion Transformer to conduct"}, {"title": "Method", "content": "The pipeline of our proposed Radar-Camera Transformer\n(RCTrans) is shown in Fig. 2. Firstly, two parallel branches\nare used to extract multi-modality tokens. In the radar\nbranch, we will use the Radar Dense Encoder to densify\nthe valid radar features. Then, position embeddings of multi-\nmodality tokens are generated and added to the tokens. Fi-\nnally, the randomly initialized queries will be sent to the\nPruning Sequential Decoder along with the tokens to pre-\ndict 3D boxes. The entire training is an end-to-end process\nand does not require freezing any parameters."}, {"title": "Token Generator", "content": "For the camera branch, given the images $I \\in \\mathbb{R}^{N_1\\times H\\times W\\times 3}$\nfrom $N_1$ views where $H$ and $W$ are the size of the image,\nthe image encoder is used to encode the images into feature\nmaps $F\\in \\mathbb{R}^{N_1\\times \\frac{H}{s}\\times \\frac{W}{s}\\times C}$. Then, the $F$ is flattened into the\nimage tokens. For the radar branch, we follow Futr3d (Chen\net al. 2023) to use the 3D coordinate, compensated veloc-\nities, and the time offset as the input channel of the radar\npoints $P \\in \\mathbb{R}^{N_r\\times 5}$, where $N_r$ is the number of the points.\nAfter that, we use MLP to obtain the features of each pil-\nlar and scatter them to the corresponding BEV grids. Due to\nthe sparsity of radar points, more than 90% BEV grids are\nempty, which may hurt the following detection results. To\nsolve it, we introduce a Radar Dense Encoder to adaptively\nfill every grid while also keeping multi-scale information.\nRadar Dense Encoder. One naive solution is to directly\nincrease the number of the BEV encoder layers. However,\nthis may smooth the small objects in the background. More-\nover, commonly used BEV encoders, such as SECOND\n(Yan, Mao, and Li 2018), are mainly designed for LiDAR\nand not entirely suitable for radar inputs. RCBEVDet (Lin\net al. 2024) proposes to use the radar cross-section (RCS) to\ndetermine the scattering radius of radar points. However, the\nscattered results rely on the quality of RCS and the number\nof scattered features in each BEV grid is different. We aim\nto find a network with a simple structure, that can adaptively\nfill each BEV gird and aggregate multi-scale information to\nfacilitate detection of objects with different sizes.\nThe simplest method to adaptively fill each BEV grid is\nto use the global self-attention mechanism. However, due\nto the large number of BEV grids, this method consumes a\nlong time. To solve it, we first downsample the original BEV\nfeatures and interact with them in a minimal resolution. At\nthe same time, the downsampling process can greatly reduce\nthe number of invalid grids, thereby improving the quality of\nfeatures obtained after interaction. Inspired by U-net (Ron-\nneberger, Fischer, and Brox 2015), we connect features of\ndifferent receptive field sizes at the same BEV resolution,\nwhich effectively preserves the features of objects of dif-\nferent sizes. The architecture of the Radar Dense Encoder\n(RDE) is shown in Fig. 3. Specifically, given the radar BEV\nfeature $B\\in \\mathbb{R}^{H\\times W\\times C_r}$, where $H$, and $W$, are the size\nof the BEV map, we first extract multi-level downsampled\nfeatures $B_d = \\{B_i \\in \\mathbb{R}^{\\frac{H}{2^i}\\times\\frac{W}{2^i}\\times C_r}, i = 1, 2, 3\\}$. Then,\nwe conduct self-attention in $B_3$ and obtain adaptively filled\nBEV map $B_f$. During this process, each grid is added by the\n2D position embedding. After that, we upsample the $B_f$ to\nthe same size as the original $B$. Skip connection is used to"}, {"title": "Pruning Sequential Decoder", "content": "fuse the upsampled BEV map and the same size features of\n$B_d$ in the upsampling process.\nPosition Embedding. For image position embedding, we\nuse the 3D position embedding proposed in PETR (Liu et al.\n2022). Given a image token $T_i$, a series points $p_{(u,v)} =$\n$\\pi(u,v) = \\{p_i(u,v) = (u \\times d_i, v \\times d_i, d_i, 1), i = 1, 2, ..., d\\}$ is pre-\ndefined in the camera frustum space. Here, $u$ and $v$ are the\nindices of the token in image space, and $d$ is the number of\npoints along the depth axis. After that, the image position\nembedding is calculated via Eq. 1.\n$\\textrm{PE}_{im} = \\Phi_{im}(K\\pi(u, v)),$ (1)\nwhere $K$ is the transformation matrix that transforms cam-\nera frustum to 3D world space. $\\Phi_{im}(.)$ is the MLP function.\nAs the radar can not obtain precise height information of ob-\njects, for radar position embedding, we use the 2D BEV em-\nbedding, ignoring the height information of the BEV grid.\nThe radar position embedding is calculated via Eq. 2.\n$\\textrm{PE}_{ra} = \\Phi_{ra}(\\Psi((h, w))),$ (2)\nwhere $(h, w)$ is the 2D coordinate of BEV grid, $\\Phi_{ra}(.)$ is the\nMLP function, and $\\Psi(.)$ is the sine-cosine function. Through\npositional embedding, information from different modalities\ncan be implicitly aligned with object queries in 3D space.\nThe Pruning Sequential Decoder consists of multiple lay-\ners, in each layer, we fuse the image tokens with object\nqueries through $\\textrm{PE}_{im}$ and $\\textrm{PE}_{3d}$, and fuse the radar to-\nkens with object queries through $\\textrm{PE}_{ra}$ and $\\textrm{PE}_{2d}$ due to\nthe inaccurate height information of radars. Following pre-\nvious query-based methods (Liu et al. 2022; Wang et al.\n2022b), we first initialize n learnable references $R = \\{r_i =$\n$(r_{xi}, r_{yi},r_{zi}), i = 1, ..., n\\}$ in 3D scene to be the position\nof the object queries, which uniformly distribute between\n$[0, 1]$. The query feature $F_q$ corresponding to each reference\nis initialized as a vector of all zeros.\nQuery Feature Update. The query positions $R$ are first\nprojected into radar and image space. The projected query\npositions $R_{ra}$ in radar space can be obtained by Eq. 3.\n$\\begin{aligned}\nr_{xi} &= r_{xi} \\times (X_{max} - X_{min}) + X_{min} \\\\\nr_{yi} &= r_{yi} \\times (Y_{max} - Y_{min}) + Y_{min} \\\\\nr_{zi} &= r_{zi} \\times (Z_{max} - Z_{min}) + Z_{min},\n\\end{aligned}$ (3)\nwhere $\\{[X_{max}, X_{min}], [Y_{max}, Y_{min}], [Z_{max}, Z_{min}]\\}$ is the\nrange of valid 3D world space. The projected query posi-\ntions $R_{im}$ in image space can be obtained by Eq. 4.\n$R_{im} = K^{-1} R_{ra},$ (4)\nwhere $K$ establishes the transformation from camera frus-\ntum to 3D world space. Following CMT (Yan et al. 2023),\nwe adopt the shared encoder used in image/radar position\nembedding generation to encode 3D/2D position embedding\nof queries. The $\\textrm{PE}_{3d}$ and $\\textrm{PE}_{2d}$ are obtained by Eq. 5.\n$\\textrm{PE}_{3d} = \\Phi_{im}(R_{im}), \\textrm{PE}_{2d} = \\Phi_{ra}(\\Psi(R_{ra}))$ (5)\nDifferent from the common decoder used for multi-modality\ntokens which concatenates them and uses a large layer to\nfuse them together, we propose to split this large layer into\ntwo small ones and fuse the multi-modality tokens sepa-\nrately. We name it sequential structure. The updated queries\nare calculated by Eq. 6.\n$F^{n+1} = f_2(f_1(F^{n} + \\textrm{PE}_{2d},T_r) + \\textrm{PE}_{3d}, T_i),$ (6)\nwhere $T_r$ and $T_i$ represent radar and image tokens. $f_1 (.,.)$\nand $f_2 (...) represent the transformer layer.\nQuery Position Update. At the end of each decoder layer,\nwe will predict the positions of the queries. In the next layer,\nnew position embedding will be generated based on the up-\ndated positions. Given the updated query $F^{n+1}$, we predict\nthe offset $AR$ of the query positions, and the updated posi-\ntion $R^{n+1}$ can be calculated by $R^{n} + AR$.\nPruning Training Strategy. In each layer, we use a se-\nquential structure to fuse multi-modality information, which\nresults in using twice as many transformer layers as tradi-\ntional decoders. This can cause additional inference time.\nWhat's more, as we update the positions of queries after\neach decoder layer, some object queries may gradually be\nlocated in the same region (please refer to visualization in\nappendix) and lose the feature distinctiveness. This causes\ninformation from some regions to be overlooked, and the at-\ntention mechanism may fail to learn effective representation\nlearning concepts which prevents the model from achieving\nthe expected performance improvement (Zhou et al. 2021).\nTo this end, we propose a pruning training strategy, which\nuses 6 layers of decoders during training and only 3 layers\nof decoders during inference."}, {"title": "Training Head and Task Loss", "content": "For the 3D object prediction head, we follow PETR (Liu\net al. 2022) to use the FFN to regress each attribute of"}, {"title": "Experiments", "content": "Following previous approaches (Kim et al. 2023a,b; Lin\net al. 2024), we conduct comprehensive experiments on a\nlarge-scale autonomous driving dataset for 3D radar-camera\nobject detection, nuScenes (Caesar et al. 2020). This dataset\nhas a total of 1000 scenes and is officially divided into\n700/150/150 for training/validation/testing. For each frame,\nnuScenes has 6 images and 5 radar point clouds covering\n360\u00b0. There are around 1.4 million annotated 3D bound-\ning boxes for ten classes. We select nuScenes detection\nscore (NDS) and mean average precision (mAP) as metrics\nto evaluate the 3D detection. For NDS, it is the weighted\nsum of mAP and other official predefined metrics, includ-\ning Average Translation Error (ATE), Average Scale Er-\nror (ASE), Average Orientation Error (AOE), Average Ve-\nlocity Error (AVE), Average Attribute Error (AAE). For\n3D tracking, we use the official multi-object tracking ac-\ncuracy (AMOTA), average multi-object tracking precision\n(AMOTP), false positive (FP), false negative (FN), and ID\nswitches (IDS) as the metrics."}, {"title": "Implementation Details", "content": "We implement RCTrans based on StreamPETR (Wang et al.\n2023) and MMDetection3D (Contributors 2020) codebases.\nFollowing CRN (Kim et al. 2023b), we accumulate the infor-\nmation of 4 previous frames to the current frames. We use\nthe object-centric temporal modeling proposed in Stream-\nPETR to conduct temporal fusion. The number of decoder\nlayers is set to 6 during training and 3 during inference.\nThe output of the last layer after pruning is inserted into\nthe memory queue in temporal fusion. We set the number of\nqueries, memory queue, and propagated queries to 900, 512,\nand 128, respectively. For radar, we accumulate 6 previous\nradar sweeps following CRAFT (Kim et al. 2023a) and set\nthe maximum number of radar points to 2048. The size of\nradar BEV is set to 128\u00d7128. We train our network for 90\nepochs with a batch size of 32 on 8 NVIDIA A100 GPUs.\nThe speed is evaluated on a single NVIDIA RTX3090 GPU.\nWe conduct optimization based on AdamW with weight de-\ncay value $10^{-2}$, and the learning rate is adjusted by cycle\npolicy with the initial value $4 \\times 10^{-4}$."}, {"title": "Performance Comparison", "content": "3D Object Detection. We first compare our method with the\nstate-of-the-art on nuScenes val set under different images\nbackbones, including ResNet18 (He et al. 2016), ResNet50,\nand Swin-T (Liu et al. 2021). As shown in Table 1, our\nmethod achieves the best performance under different back-\nbones with a slight increase in time consumption. For exam-\nple, when using Swin-T as the backbone and setting the im-\nage size to 256\u00d7704, compared to the SOTA radar-camera\nsolution RCBEVDet, RCTrans improves NDS by 3.2% and\nmAP by 2.4%, while the latency increases around 5 ms. The\ngreat experimental results of multiple backbones demon-\nstrate that RCTrans has excellent and strong adaptability,\nwhich is very beneficial for model deployment and migra-\ntion in practical applications. What's more, RCTrans beats\nall the camera-only methods, including our camera stream\nbaseline StreamPETR, which proves our method can effec-\ntively use radar information to supplement the detection re-"}, {"title": "Ablation Study", "content": "We conduct the ablation study on nuScenes val set to\ndemonstrate the effectiveness of each component. All of\nour experiments are conducted under the condition of us-\ning ResNet50 as the backbone and setting the image size\nto 256\u00d7704. As shown in Table 4, each component can\nconsistently improve performance. Compared to the single-\nmodality stream, using multi-modality inputs can signifi-\ncantly achieve improvement. Compared to commonly used\nBEV encoders, such as SECOND (Yan, Mao, and Li 2018),\nusing our proposed Radar Dense Encoder (RDE) can obtain\n1.3% NDS and 1.6% mAP improvement. What's more, the\nPruning Sequential Decoder (PSD) increases NDS by 2.2%\nand mAP by 2.7%. We also conduct experiments on the"}, {"title": "Robustness Analysis", "content": "To simulate the sensor malfunction in real autonomous driv-\ning scenarios, we randomly drop images or radar inputs and\nevaluate the results. For a fair comparison, following CRN"}, {"title": "Conclusion", "content": "In this paper, we propose a Radar-Camera Transformer (RC-\nTrans), a query-based method that can effectively solve\nthe fusion difficulties caused by the sparsity and noise of\nradar inputs. To solve the problem of sparsity, we introduce\nthe Radar Dense encoder which uses a downsample-then-\nupsample architecture. It can adaptively fill the invalid BEV\ngrids while maintaining the information of small objects.\nTo solve the problem of noise, we introduce the Pruning\nSequential Decoder which uses a step-by-step strategy to\ngradually regress the objects. We also introduce the prun-\ning training strategy for the decoder, which saves much time\nduring inference while also preventing attention collapse.\nExtensive experiments demonstrate the effectiveness and ro-\nbustness of RCTrans. We believe RCTrans can be an effec-\ntive baseline to inspire future research."}]}