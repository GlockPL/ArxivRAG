{"title": "DEEP LEARNING PIPELINE FOR FULLY AUTOMATED\nMYOCARDIAL INFARCT SEGMENTATION FROM CLINICAL\nCARDIAC MR SCANS", "authors": ["Matthias Schwab", "Mathias Pamminger", "Christian Kremser", "Agnes Mayr", "Markus Haltmeier"], "abstract": "Purpose: To develop and evaluate a deep learning-based method that allows to perform myocardial\ninfarct segmentation in a fully-automated way.\nMaterials and Methods: For this retrospective study, a cascaded framework of two and three-\ndimensional convolutional neural networks (CNNs), specialized on identifying ischemic myocardial\nscars on late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) images, was\ntrained on an in-house training dataset consisting of 144 examinations. On a separate test dataset\nfrom the same institution, including images from 152 examinations obtained between 2021 and\n2023, a quantitative comparison between artificial intelligence (AI)-based segmentations and manual\nsegmentations was performed. Further, qualitative assessment of segmentation accuracy was evaluated\nfor both human and AI-generated contours by two CMR experts in a blinded experiment.\nResults: Excellent agreement could be found between manually and automatically calculated infarct\nvolumes (pc = 0.9). The qualitative evaluation showed that compared to human-based measurements,\nthe experts rated the AI-based segmentations to better represent the actual extent of infarction\nsignificantly (p < 0.001) more often (33.4% AI, 25.1% human, 41.5% equal). On the contrary, for\nsegmentation of microvascular obstruction (MVO), manual measurements were still preferred (11.3%\nAI, 55.6% human, 33.1% equal).\nConclusion: This fully-automated segmentation pipeline enables CMR infarct size to be calculated\nin a very short time and without requiring any pre-processing of the input images while matching\nthe segmentation quality of trained human observers. In a blinded experiment, experts preferred\nautomated infarct segmentations more often than manual segmentations, paving the way for a potential\nclinical application.", "sections": [{"title": "1 Introduction", "content": "Ischemic heart disease remains a leading cause of global mortality, responsible for approximately 9.1 million deaths\nworldwide in 2019 [1, 2]. It has been shown that following ST-segment elevation myocardial infarction, accurately\nassessing infarct size and microvascular obstruction (MVO) are crucial for clinical decision-making and for prediction\nof major adverse cardiovascular events [3, 4, 5, 6]. However, obtaining these important predictors requires segmentation\nof late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) images.\nAs manual LGE segmentation by expert readers is time-consuming and additionally yields limited reproducibility [7],\nrecently a lot of work has been done developing deep learning-based algorithms for automatic infarct segmentation\n[8, 9, 10, 11, 12, 13, 14, 15]. The topic received even more attention when two challenges focusing on myocardial infarct\nsegmentation were held in the course of the 2020 MICCAI conference [16, 17]. However, a lot of these frameworks still\nhave major drawbacks, such as ignoring the extensive image preprocessing steps that would be necessary when applied\nin clinical practice. Furthermore, the segmentation performance was only measured quantitatively by comparing with\nhuman-created ground truth measurements. Recent findings call into question if metrics such as the Dice coefficient can\nbe accepted as the de facto gold standard for measuring segmentation quality beyond expert opinion [18]. Therefore, to\nbe able to develop clinically helpful segmentation models, a better understanding of the subjective quality perception\nof clinical experts is required [19]. Although there are some methods in the literature for qualitatively assessing\nsegmentation accuracy [20, 21], these often do not provide information about the specific types of segmentation errors\nand their potential effects in a clinical setting. This means that in medical image segmentation, subjective performance\nevaluation heavily depends on the underlying medical application and includes diverse approaches. These involve\nmeasuring time that experts need to manually correct automatically generated segmentations [22, 23], rating the\nsegmentation quality [24, 25], or blindly comparing manual ground truth and automatic segmentations [25]. However,\nto the best knowledge of the authors, no qualitative assessment for artificial intelligence (AI)-generated myocardial\ninfarct segmentation has yet been published.\nThe purpose of this study was to develop and evaluate a deep learning-based algorithm that enables accurate and fast\nsegmentation of myocardial infarction and MVO on clinical LGE CMR images. The developed pipeline allows to\nquantify the extent of myocardial infarction on clinical LGE CMR images in a fully automated way. This is done without\nany human intervention, i.e. the preprocessing steps required for accurate CNN segmentation of the clinical data are also\nfully automated. To validate the segmentation performance of the developed framework, not only the usual quantitative\nmetric between human-created ground truth measurements were calculated. Additionally, a comprehensive qualitative\nevaluation study incorporating the experience and knowledge of two CMR-specialized and certified radiologists was\ncarried out."}, {"title": "2 Materials and Methods", "content": "This study is concerned with the retrospective analysis of quantitative and qualitative performance of a deep learning\nsegmentation algorithm for myocardial infarct quantification on clinical data. All the LGE CMR images that were used\nin our study were originally acquired prospectively as part of the MARINA-STEMI (Magnetic Resonance Imaging In\nAcute ST-Elevation Myocardial Infarction) study (NCT04113356), which was approved by the local ethics committee,\nwith all patients providing written informed consent prior to inclusion. For both development and testing of the\nsegmentation algorithm, a total of 329 examinations were randomly selected from the MARINA-STEMI cohort and\nassigned to either the training, evaluation, or test datasets. While several articles have been published in the last decade\n[26, 27, 28] using patients from this cohort to address clinical questions, this paper is the first to take a machine learning\napproach to these data."}, {"title": "2.1 Training Dataset", "content": "For training of the algorithm, an in-house training dataset consisting of 144 LGE CMR examinations from 142 unique\npatients (baseline: n = 54; 4 months follow-up: n = 24; 12 months follow-up: n = 66) was created from data collected\nat the Department of Radiology. During training, segmentation performance was evaluated after each epoch on a\nhold-out evaluation dataset consisting of 33 LGE examinations (Table 1). Manual segmentations of the left ventricle\n(LV) were done by medical experts using the local routine diagnostic interpretation and reporting software (DeepUnity\nDiagnost, Dedalus Healthcare Systems Group, Germany) according to the guidelines explained in Appendix S2 and Fig\nS1. In each of the short axis slices of the CMR, the following four tissue regions were segmented if present: remote\nmyocardium, LGE-enhanced myocardium, MVO, and blood pool. Binary segmentation masks were created from\nthese manually defined regions for training of the deep learning models. Since we were interested in a segmentation\nframework that is able to handle unprocessed MR images as they occur in clinical practice, we presented the complete\nLGE image stack, including LV outflow tract, apex, and slices without LV myocardium, to the CNN. In addition, close\nattention was paid to marking blood within the LV outflow tract."}, {"title": "2.2 Test Dataset", "content": "The study analyzes the algorithm's performance on a CMR LGE test dataset consisting of images obtained between 2021\nand 2023 at the same institution as the training dataset. In total, images from 152 LGE CMR measurements, including\ndata from 121 unique ST-segment elevation myocardial infarction patients after successful primary percutaneous\ncoronary interventions (p-PCI), are analyzed. This includes images obtained within one week after p-PCI (baseline), 4\nmonths, and 12 months follow-up examinations, respectively. The details of the dataset demographics are displayed\nin Table 1. LGE CMR images were acquired on a 1.5 Tesla MR scanner (Magnetom AvantoFit, Siemens, Erlangen,\nGermany) 10 to 20 minutes after an intravenous gadolinium bolus injection of 0.2 mmol/kg body mass (Gadobutrol,\nGadovist, Bayer AG, Germany) using an ECG-triggered phase-sensitive inversion recovery sequence. Exact details on"}, {"title": "2.3 AI Framework Development", "content": "2.3.1 Deep Learning Pipeline\nIn summary, our deep learning pipeline consists of two main steps:\n1. Extracting a stack with smaller image sizes out of the original data that still contains the entire LV.\n2. Performing multiclass segmentation with special focus on the myocardial scar on the extracted volumes.\nThe overall configuration of the proposed deep learning framework is illustrated in Figure 1. Since myocardial scar and\nMVO are potentially very small areas, it is a very hard task to segment them from the original CMR images as they\nare obtained in clinical practice. Therefore, in a preprocessing step, our framework is extracting a range of interest\nstack with smaller image sizes out of the original data, which still contains the entire LV. To this end, a 3D U-net was\ntrained to segment the LV in the original image stack. The detailed architecture of the network and a description of"}, {"title": "2.3.2 Evaluation of Segmentation Performance", "content": "Our evaluation of segmentation performance analysis consists of:\n1. A quantitative assessment of segmentation accuracy comparing AI-segmentations to manual markings.\n2. A qualitative assessment of segmentation accuracy done by CMR experts.\nFor the qualitative assessment of segmentation accuracy both, manual and automatic segmentation masks were evaluated\nby two CMR experts in a blinded experiment. For each patient, we distributed manual and automatic segmentations\nrandomly into segmentation A and segmentation B. Not knowing which mask was created by humans and which by AI,\nthe medical experts had to subjectively assess the segmentation quality of LGE and MVO segmentations. On a per-slice\nlevel, they had to decide for each segmentation between different ratings (Fig S4):\n\u2022 optimal: The segmentation was done to their full satisfaction.\n\u2022 too big: An infarct/MVO was correctly identified. However, the area marked was too big.\n\u2022 too small: An infarct/MVO was correctly detected. However, too small an area was marked.\n\u2022\nwrong organ: Areas outside the heart were marked as infarct/MVO.\n\u2022 false negative: An infarct/MVO was completely overlooked in this slice.\n\u2022 false positive: An area in the myocardium was falsely marked as infarct/MVO in a slice where no infarct/MVO\nis present.\n\u2022 true negative: Rightfully nothing was marked in a slice where no infarct/MVO is present.\nAfter evaluating the segmentation individually, the experts additionally had to look at the two methods side by side\nand decide with which of the two segmentations they agreed more. For this task, they were able to choose between\nsegmentation A, segmentation B, or equally good (Fig S5).\nThe data was distributed between the two raters the following way: For a randomly chosen subset of 20 patients, both\nexperts gave their ratings independently of each other. Then the agreement between their answers was evaluated, and\ncases where the experts disagreed were discussed in more detail. The remaining dataset was then split between the two\nexperts so that each evaluated half of the remaining patients. For images in which the qualitative assessment was not\nentirely clear, the experts reached a consensual decision. Further details about the design of the qualitative experiments\ncan be found in the supplemental material (Appendix S5)."}, {"title": "2.4 Statistical Analysis", "content": "To quantify the segmentation accuracy of our method, we calculated different metrics between AI and human-generated\nmeasurements. These incorporate clinical as well as geometrical metrics. For assessing the geometrical agreement\nbetween the methods, Dice similarity coefficients (DICE) were calculated. Further, absolute volume difference (AVD)\nin ml as well as absolute volume difference rate (AVDR) with respect to the volume of the myocardium (VMYO) were\ncalculated. The performance metrics between AI segmentations P and manual segmentations G were obtained as"}, {"title": "3 Results", "content": "3.1 Quantitative Segmentation Accuracy\nThe deep learning method reached mean Dice coefficients of 64.11% for infarct segmentation and 82.20% for MVO\nsegmentation. The mean AVD between manually and automatically calculated infarct volumes was 4.97ml, and the\nmean AVDR was 4.04%. For MVO, only very small volume differences were found, with a mean AVD of 0.59ml and\na mean AVDR of 0.43%. However, only in 15% of all the patients in the test dataset MVO was present As the Dice\ncoefficient is undefined when both the ground truth and the predicted segmentation masks are empty, it was set to 1 for\nsuch cases. This resulted in optimal metric values for all the patients where the method correctly detected no MVO.\nWhen only taking into account patients where MVO was present, the mean Dice score reduced significantly to 25.04%,, and also for AVD and AVDR, the accuracy decreased considerably (see Table 2).\nScatter plot and Bland-Altman analysis showed good agreement between manually and automatically calculated infarct\nsizes, expressed as a percentage of the total myocardial volume (Figure 2). Concordance correlation was very high\n(\u03c1c = 0.903, 95% CI [0.871, 0.923]), and Bland-Altman analysis showed little average difference of -1.26% between\nmanual and CNN volume calculations. However, the bias of the neural network to mark slightly bigger scars is\nstatistically significant (p < 0.01). The limits of agreement in Bland-Altman analysis ranged from -11.57% to 9.05%."}, {"title": "3.2 Qualitative Segmentation Performance", "content": "Subjective evaluation of the segmentation performance was done for both LGE segmentation and MVO segmentation\non a per-slice level on the 152 patients in the test dataset. This all together resulted in ratings for LGE and MVO\nsegmentation on 1619 pairs of CMR slices. Examples of automatically and manually created segmentation masks\nwith corresponding expert ratings are displayed in Figure 3. For the evaluation of the direct comparison between AI\nand human-created segmentation masks MRI slices in which both methods correctly showed no scar/no MVO were\nexcluded, as the segmentations in these slices could only be evaluated as equally good.\nBased on the experts validation of the segmentations, we investigated the diagnostic performances of human-based and\nAI-based predictions. For myocardial scars, diagnostic performance was very high, as the AI framework only missed\ntwo scars in the whole dataset. However, these two scars were tiny and could only be clearly confirmed by the CMR\nexperts after an additional review of the functional images and previous examinations (Fig S6). For MVO detection,\nthough, AI-based predictions showed a considerably lower sensitivity (65%) compared to humans (91%). Contingency\ntables and corresponding sensitivity and specificity values are shown in Table 3.\nFor LGE segmentation, raters overall preferred the automatic measurements, see Figure 4. In 33.5% of the cases,\nthey decided to agree more with the segmentation done by the neural net, whereas in only 25.1% of the cases manual\nsegmentation was preferred. When excluding all cases that were rated as equal, a one-way chi-square test revealed\nthat AI segmentations were preferred significantly more often (p < 0.001). On a per-slice level, the experts were fully\nsatisfied (optimal segmentation or true negative) with the network's performance in 82.2% of all the evaluated cases.\nThis is slightly higher than for the human-based measurements, where experts expressed full agreement in 80.2% of\ncases. The main difference in performance was that fewer scars were overlooked (false negative) by the CNN (2.6%)\ncompared to humans (4.3%). However, the fraction of wrongly marked infarct scars (false positive) was bigger for\nthe CNN-generated segmentations (1.8%) compared to the human-created contours (0.8%). Total failure due to the\nmarking of a myocardial scar in a wrong organ has hardly ever been observed with either method (<1%)."}, {"title": "3.2.1 Rater Agreement", "content": "In addition, to confirm the informative value of the subjective ratings of the two experts, we evaluated their agreement\non a subset consisting of 20 patients (Figure 6). Calculating Cohen's kappa coefficients (\u03ba) revealed that the strength of\nagreement between the raters was very good for both LGE (\u043a = 0.82) and MVO (\u043a = 0.88) ratings. In rating LGE\nsegmentation, the biggest difference between the experts was that in 26 out of the total 212 slices, rater 1 decided for\nthe LGE segmentation to be too big, while rater 2 considered it as optimal. Similar to that, for MVO assessments in 4\ncases each, rater 1 was of the opinion that the markings were too large or too small, while rater 2 opted for optimal. In"}, {"title": "4 Discussion", "content": "In this work, we developed and evaluated a deep learning-based pipeline that allows to quantify infarct scars and MVO\nfrom LGE CMR images in a fully-automated way. Compared to existing methods, our framework allows to perform\nthe segmentation of infarcted areas without any manual preprocessing steps. Although there are public datasets for\nLGE CMR images available [31, 32], they do not reflect clinical reality. For instance, these datasets consist of image\nstacks that only include slices where the myocardium of the LV is visible. Also, often images were preprocessed such\nthat the LV is located in the center of each image. As this cannot be expected when dealing with data in a clinical\nsetting, the methods developed on these public datasets would rely on suitable preparation of the raw data before they\ncan be applied. In contrast to that, in our framework, these preprocessing steps are also automated, making them easily\napplicable in daily practice. This complete automation from raw MR data to the final clinical markers is a huge time\nsaver. On a conventional clinical computer without any GPU assistance (Intel(R) Core(TM) i7-10700 CPU @ 2.90GHz),\nour framework required between 3 and 5 seconds per patient (3.9 \u00b10.4). Compared to the manual measurements,\nwhich were reported to take around 20 minutes per patient, this is a huge improvement. In contrast to prior work\non automated infarct quantification, we evaluate the performance of our framework on a significantly larger dataset."}]}