{"title": "Pandora3D: A Comprehensive Framework for High-Quality 3D Shape and Texture Generation", "authors": ["Jiayu Yang", "Taizhang Shang", "Weixuan Sun", "Xibin Song", "Ziang Chen", "Senbo Wang", "Shenzhou Chen", "Weizhe Liu", "Hongdong Li", "Pan Ji"], "abstract": "This report presents a comprehensive framework for generating high-quality 3D shapes and textures from diverse input prompts, including single images, multi-view images, and text descriptions. The framework consists of 3D shape generation and texture generation. (1). The 3D shape generation pipeline employs a Variational Autoencoder (VAE) to encode implicit 3D geometries into a latent space and a diffusion network to generate latents conditioned on input prompts, with modifications to enhance model capacity. An alternative Artist-Created Mesh (AM) generation approach is also explored, yielding promising results for simpler geometries. (2). Texture generation involves a multi-stage process starting with frontal images generation followed by multi-view images generation, RGB-to-PBR texture conversion, and high-resolution multi-view texture refinement. A consistency scheduler is plugged into every stage, to enforce pixel-wise consistency among multi-view textures during inference, ensuring seamless integration.\nThe pipeline demonstrates effective handling of diverse input formats, leveraging advanced neural architectures and novel methodologies to produce high-quality 3D content. This report details the system architecture, experimental results, and potential future directions to improve and expand the framework.", "sections": [{"title": "1. Introduction", "content": "Automated generation of high-quality digital 3D assets has drawn more and more attention in recent years. Digital 3D assets have become deeply ingrained in modern life and production. These assets vividly express the imaginations of creators across various fields, including gaming and film, bringing joy and creating immersive experiences for both players and audiences alike. Meanwhile, 3D assets also serve as essential building blocks in the domains of physical simulation and embodied AI, enabling machines and robots to understand the elements in the real world. However, the creation of 3D assets is far from simple; it is often a complex, time-consuming, and expensive process. Taking text prompts or an image as input, the digital 3D asset production pipeline commonly involves stages of 3D shape generation and texture generation, each requiring a high level of expertise and proficiency in digital content creation software.\nIn this report, we present Pandora3D, a framework designed for high-quality 3D shape and texture generation. The framework consists of two main components: 3D shape generation and texture generation.\n\u2022 3D Shape Generation: The 3D shape generation pipeline utilizes a Variational Autoencoder (VAE) to encode implicit 3D geometries into a latent space. A diffusion network is then used to generate latents conditioned on input prompts, with modifications aimed at enhancing the model's capacity. We also explore an alternative Artist-Created Mesh (AM) generation approach, which shows promising results for simpler geometries.\n\u2022 Texture Generation: The texture generation process is multi-staged, starting with the generation of frontal images, followed by multi-view images generation, RGB-to-PBR texture conversion, and high-resolution multi-view texture refinement. A novel consistency scheduler is integrated into every stage of this process to ensure pixel-wise consistency among multi-view textures during inference, leading to seamless integration."}, {"title": "2. 3D Shape Generation", "content": ""}, {"title": "2.1. 3D Latent Space Diffusion", "content": "The process begins by generating a 3D shape from a single image, multiple images, or a text prompt. This involves the following steps:\n\u2022 Variational Autoencoder (VAE): Compresses 3D geometries into a latent space, enabling efficient representation and processing.\n\u2022 Diffusion Network: Generates latent representations conditioned on the input prompts. This network is adapted from CLAY [43] / Craftsman [16] / LAM3D [4], with modifications to improve the capacity and performance of the model."}, {"title": "2.1.1 Efficient 3D Geometry Autoencoder", "content": "For 3D geometry compression model, we build upon CraftsMan [16], which adopts structures introduced in 3DShape2VecSet [40] and Michelangelo [44]. Furthermore, we leverage the multi-resolution training strategy proposed in CLAY [43]. This approach encodes 3D geometry into latent space by progressively sampling additional points from a 3D point cloud, which incrementally extends and refines the latent representation of the shape. Progressive sampling allows the model to focus on areas of higher geometric complexity, capturing both global structure and intricate details. The primary goal of our VAE is to generate expressive latent embeddings that effectively guide the diffusion process in subsequent stages. To enhance the efficiency of this process, we propose a more advanced point sampling strategy. This method is designed to maximize the utility of the 3D point-cloud data by prioritizing points that contribute the most to capturing fine-grained features and spatial relationships. This enhancement not only increases the model's capacity to handle large-scale data for improved scalability but also preserves the fine-grained details of 3D geometry.\nThe design options of our VAE are illustrated in Fig. 1. We employ the model structure introduced in 3DShape2VecSet as our base model. This approach involves embedding the input point cloud, augmented with normal information $X \\in R^{N\\times6}$, which is sampled from a mesh M, into a latent code using a learnable embedding function and a cross-attention encoding module:\n$Z = \\varepsilon(X) = CrossAttn(q, PosEmb(X)),$                                                                                                (1)\nwhere $q \\in R^{m\\times d}$ represents a set of learnable queries that compress the sampled points into a latent embedding. The cross-attention mechanism ensures effective integration of geometric and positional features into the latent space. The VAE's decoder is composed of successive self-attention layers followed by a cross-attention layer. The cross-attention layer maps the latent embeddings back into 3D geometry, enabling reconstruction:\n$D(Z,p) = CrossAttn(PosEmb(p), Self Attn(Z)),$                                                                                  (2)\nwhere p denotes random query points in 3D space, these points query with the latent and output occupancy logits. This base VAE implementation is illustrated in Fig. 1 (A).\nFollowing the approach outlined in CLAY [43], we adopt a multi-resolution training strategy to progressively upscale the model's capacity. Specifically, we incrementally increase the number of sampled points from 4096 to 32768 while simultaneously extending the latent embedding dimensionality from 256 to 2048. This progressive training scheme gradually introduces more detailed input information to the model, enabling it to capture finer geometric details. At the same time, the expanded latent embedding length increases the model's capacity to represent complex features. Together, these enhance- ments enrich the latent space, thereby providing a more robust foundation for the subsequent diffusion model training. This multi-resolution approach ensures an efficient and scalable training process, optimizing both the model's performance and its ability to generalize across diverse 3D geometries.\nRecall that the primary objective of our VAE is to generate expressive latent embeddings. While the previously men- tioned approach progressively increases the number of sampling points, each object contains a total of 500k points, leaving many points unsampled. This results in inevitable information loss, as not all geometric details are captured in the latent"}, {"title": "2.1.2 Diffusion Pipeline", "content": "The diffusion pipeline is illustrated in Fig. 2. We employ Multimodel Diffusion Transformer(MMDiT) [9] as our diffusion backbone, utilizing two pretrained models, specifically, CLIP-ViT-L/14 [27] as the global image feature extractor, and Dino- V2-Large [20] for local image feature extraction. Instead of employing DDPM, we utilize the flow matching schedule. Following CLAY [43] methodology, the diffusion model is trained in coarse-to-fine manner.\nTo enhance the image control effect, we use both global and local image feature as condition features of diffusion model. Global condition feature $z_{global} \\in R^{L}$ is extracted with ClIP vision encoder, meanwhile, local detail condition feature $Z_{local} \\in R^{L\\times1024}$ is extracted with Dino vision encoder. The global condition and local condition are integrated into the diffusion model through MMDiT Block following Stable Diffusion 3 [9]. The diffusion model we use has 2.3B parameters and 28 layers MMDiT block.\nTo enhance practical utility in 3D design workflows, we have extended our geometry generation framework to accept multi-view conditional inputs. This architectural advancement enables finer-grained geometric control through multi-view visual guidance. The system accommodates variable numbers of reference images (stochastically sampled from 1 to 4 views per instance) within a unified architecture, eliminating requirement for fixed-size input configurations. All synthesized ge- ometries maintain spatial alignment with the primary view's coordinate system (defined by the first input image). Input images must be arranged in ascending azimuth order {$\\theta_1, \\theta_2,..., \\theta_n$} where $\\theta_1 \\in [0^\\circ, 360^\\circ)$. Multiview feature repre- sentations are aggregated through ordered concatenation along the sequence dimension, preserving relative spatial-semantic correspondence across views. Accelerated convergence is achieved via progressive transfer learning, where parameters ini- tialized from our single-view conditioned model undergo fine-tuning using multiview datasets while maintaining pretrained backbone weights during initial phases."}, {"title": "2.2. Meshing and UV Unwrapping", "content": "Once the 3D geometry is generated, it undergoes isosurface extraction, remeshing and UV unwrapping so that a texture- ready triangle mesh is produced."}, {"title": "2.2.1 Isosurface extraction", "content": "We perform a modified version of marching cubes algorithm [17, 19] to efficiently extract a watertight mesh from geometry tokens.\nMarching cubes traditionally require a dense occupancy grid of D \u00d7 D \u00d7 D occupancy values. Directly computing such a dense grid with Eq. (2) incurs a O($D^3$) time complexity that is prohibitively expensive at high resolutions D. To improve efficiency, we adopt a coarse to fine strategy: starting from a coarse grid resolution $d_0 \\ll D$, we iteratively build a sparse finer grid of resolution $d_{i+1} = 2d_i$ whose cells are subdivided from active cells in the coarser grid of resolution $d_i$ that are close to the isosurface. This strategy ensures that most occupancy queries of Eq. (2) are confined within a small margin around the isosurface and significantly reduces the number of queries required for isosurface extraction, achieving two to three orders faster mesh extraction.\nTo guarantee a watertight mesh, at the highest level $d_n \\approx D$, we expand the sparse active cells along the isosurface to eliminate holes and perform Lewiner's topology check [14] to ensure manifoldness. We implement the sparse marching"}, {"title": "2.3. Remesh and UV unwrap", "content": "The triangle meshes extracted from Marching cubes may contain poorly constructed elements such as collapsed faces or slivers. Furthermore, they often exhibit a high face count that could create problems for downstream applications. We over- come these issues with an optional remeshing step using either an off-the-shelf quad-remesher\u00b9 or isotropic remeshing [21] followed by QEM triangle decimation [10].\nIn addition, we use the open source project UV-Atlas [45] for UV charting and packing. At this point, we obtain a polygon mesh that is ready for texture generation."}, {"title": "2.4. Alternative Approach: Artist-Created Meshes Generation", "content": "An alternative approach we explored involves directly generating the mesh, bypassing the initial generation of geometry as an implicit function followed by mesh extraction. This method effectively produces meshes with reasonable topology, akin to those crafted by artists for simple shapes. However, it encounters difficulties when applied to complex geometries, where maintaining structural integrity and topological accuracy becomes challenging."}, {"title": "2.4.1 Mesh Compression", "content": "Direct regression of vertex coordinates results in substantial memory consumption, which consequently limits the number of faces the model can handle. To mitigate this issue, we adopt the methodology proposed by BPT [38], which involves compressing the original vertex coordinates using block index compression and patchified aggregation. Specifically, for a vertex $v_i = (x_i, y_i, z_i)$, the block-wise indexing $(b_i, o_i)$ is formulated as follows:\n$b_i = (x_i|O) \\cdot B^2 + (y_i|O) \\cdot B + z_i|O,$\n$o_i = (x_i\\%O) \\cdot O^2 + (y_i\\%O) \\cdot O + z_i\\%O .$                                                                                               (3)\nIn this formulation, the symbols | and % represent division without remainder and the modulo operation, respectively. This approach segments the coordinates along each axis into B blocks, each of length O. To further enhance the compression ratio, we employ the patchified aggregation technique as described in [38]. This technique aggregates the faces connected to the same vertex into a non-overlapping patch and utilizes dual-block indices to denote the starting point of a patch. Consequently, the offset vocabulary is shared between the center vertex and the surrounding vertices. The center patch is formulated as follows:\n$P_c = (b_c, o_c, b_1, o_1, b_2, o_2, ..., b_n, o_n).$                                                                                                             (4)\nIn this context, $b'$ and $o_c$ denote the blocking index and the offset index of the center patch, respectively. These indices are critical for accurately referencing the spatial configuration of the patch within the compressed data structure.\nTo achieve this, we initially convert vertex coordinates into discrete values with a resolution of R. Subsequently, we encode the mesh information, including vertices and faces, into a discrete token sequence. This sequence can be decoded back into a mesh using the same technique. It is important to note that this encoding and decoding process is governed by predefined rules and does not involve any learnable parameters."}, {"title": "2.4.2 Autoregressive Model for Mesh Generation", "content": "In this section, we describe the methodology for generating novel shapes from various modalities using the compression technique outlined in the preceding sections. Fig. 3 illustrates the pipeline of our approach. Initially, a mesh is encoded into discrete token sequences utilizing the method detailed previously. Subsequently, a decoder-only autoregressive model is employed to predict subsequent tokens based on preceding ones. To facilitate multi-modality condition control, a pre-trained condition encoder network is utilized to encode condition information, such as images, text, and point clouds, into latent features. These features serve as the contextual input for the decoder-only model. The resulting token sequence can then be decoded back into the final mesh using a mesh decoder. It is important to note that both the mesh encoder and mesh decoder are purely rule-based, as previously explained, and do not involve any learnable parameters."}, {"title": "3. Texture Generation", "content": "The proposed texture generation pipeline consists of several stages, each contributing to the generation of consistent and high-quality textures. Fig. 5 illustrates the texture generation pipeline. The pipeline begins with a 3D mesh without texture. Below we introduce each stage in detail."}, {"title": "3.1. Frontal Image Generation", "content": "If the input prompt is text, a frontal image is initially generated conditioned on a depth map derived from the 3D geometry. This process involves rendering the 3D mesh into a depth map and utilizing depth-conditioned diffusion models [42] to produce the frontal image. Alternatively, if the input is an image, we integrate the IP-Adapter [39] and ControlNet [42] to generate the frontal image. As illustrated in Fig. 6, both text and image prompts are converted into a geometry-aligned frontal image, which serves as the input for subsequent texture generation."}, {"title": "3.2. Multi-view RGB Image Generation", "content": "A single-view to multi-view image generator creates multi-view RGB images conditioned on the multi-view position maps and the frontal image. Please note that normal and depth maps can also be used here. We first train a multi-view image generator with a network structure similar with Zero123++ [30], then, we combine the ControlNet [42] with Zero123++ [30] conditioned on the position (XYZ coordinate) maps, enabling the generation of position-aligned multi-view images. Whether the frontal image originates from text and depth or is provided as input, the multi-view image generator generates six multi- view images (each 512 \u00d7 512) starting from random Gaussian noise, with geometric conditions and photometric modules."}, {"title": "3.3. Multi-view PBR Image Generation", "content": "After obtaining multi-view rgb image conditioned on multi-view position maps, we generate the corresponding multi-view PBR (Physically Based Rendering) image via a image-2-image diffusion model [28]. Specially, taking multi-view rgb image as input, we train image-2-image diffusion models to generate corresponding multi-view PBR image. This stage includes generating multi-view albedo, metallic, and roughness maps (each sub image with resolution of 512\u00d7512). Please kindly note that we train two models in multi-view PBR image generation process, where one model estimate multi-view albedo"}, {"title": "3.4. High-Resolution Refinement", "content": "To further enhance visual quality, we upscale the albedo multi-view images by several iterative upscaling steps. Firstly, we apply two steps of image repainting, utilizing a pre-trained SDXL [22] model conditioned on albedo, depth, XYZ coordinate maps, and the frontal image to upscale the albedo multi-view images to resolutions of 768 \u00d7 768 and 1024 \u00d7 1024, introducing finer details. We then use Real-ESRGAN [36] to further enhances the multi-view textures to generate detailed high-resolution albedo maps (3072 \u00d7 3072), see Fig. 8."}, {"title": "3.5. Pixel-Wise Consistency Enforcement", "content": "The multi-view generation stages may produce inconsistencies at the pixel level across different views. To address this, we implement a consistent scheduler similar to TexPainter [41], which enforces pixel-wise consistency. Specifically, the multi-view textures are firstly baked onto the 3D mesh, and a Poisson system is solved to produce seamless textures. Then, multi-view images are re-rendered and resampled into each view, ensuring consistency across different views and lighting conditions. The resampled views are used as the updated $x_0$, which is plugged into the noise scheduler of the diffusion model similar to [41]."}, {"title": "4. 3D Model Data Processing", "content": "We collect assets (mostly 3D models) from multiple sources and preprocess them to be training compatible, including converting mesh geometry to discrete sampling of implicit functions, multi-view image generation, and PBR rendering. Our"}, {"title": "4.1. Data Origins", "content": "The main data sources are:\n\u2022 Objaverse [7], which is a large open-source 3D object dataset with more than 700k objects, we use roughly 600k of it;\n\u2022 OXL i.e. Objaverse-xl [6], which is an extension of the previous dataset Objaverse [7], we use roughly 200k of it;\n\u2022 ABO [3], we only use the 8k 3D objects in this dataset;\n\u2022 BuildingNet [29], which contains 2k building models;\n\u2022 HSSD [13], which contains roughly 13k object models;\n\u2022 Toy4k [31], which contains roughly 4k object models.\n\u2022 Some models downloaded from the Internet, for instance, polygone dataset\u00b2.\n\u2022 10k private data provided by our partners."}, {"title": "4.2. Filter Mesh", "content": "We first filter mesh using the following mesh proprieties, which are modified from the standards used in MeshXL [2]:\n\u2022 Mesh face number, we only use meshes which face number is between 500 and 80k;\n\u2022 Material number, we ignore all meshes with more than 100 materials; material number is defined by total material number in blender\u00b3.\n\u2022 Annotations of the dataset. We use annotations of Objaverse to remove scanned objects. Exampled images of scanned objects can be found in Fig. 10. Scanned objects are harmful to the overall 3D generative model training process in various ways:\nFrom (B) and (D) of Fig. 10 we notice that most scanned objects possess a large number of fragmented faces; meshes containing such a large number of faces can only use the automatic method of unwrapping the mesh, which requires extremely large texture images to maintain reasonable mesh appearance. Hence, it's very slow in rendering.\nSome scanned objects do not possess an actual \"body\", like (C) in Fig. 10. It's only a thin layer and not suitable for training multiple view diffusion methods like what we discuss in Section 3.\n\u2022 Objects cannot be of pure color i.e. pure red or pure blue. This can be checked by checking object's material graph in Blender."}, {"title": "4.3. Format Conversion", "content": "For convenience of usage, we convert all 3D mesh formats to OBJ\u2074. This is because most processing pipelines that are not part of DCC software i.e. Digital Content Creation software, cannot support read full information of complex 3D formats like GLB\u2075 and FBX\u2076. If we wish to scale-up for future learning-based algorithms that need to directly read information from meshes, we have to convert 3D format to OBJ. However, directly converting 3D models to OBJ format often fails, mostly because default format conversion function in Blender cannot correctly deal with file path of texture images. We thus need some extra care of some certain file types.\n\u2022 MAX to OBJ cannot be done directly, as MAX file is only supported by 3DSMax\u2077 and OBJ exporting function in that software cannot correctly handle objects with multiple complex materials. This is because we use an extension of OBJ that supports PBR formats developed by Carla [8], which is not properly supported in 3DSMax. We thus first convert MAX files to FBX formats using 3DSMax, and convert FBX to OBJ using blender.\n\u2022 GLB to OBJ can be done in blender, but to get correct textures extra care is needed in rebuilding material graph structure. This is because GLB specification has embedded texture images within mesh files. We first convert all GLB files to GLTF formats which extract texture files to disk; then we go through the material graph of GLTF format and rebuild connections in new mesh.\n\u2022 PMX to OBJ can be done in blender using codes derived from Cats plugin\u2079. PMX is often used by some creators of the anime industry."}, {"title": "4.4. Classify Mesh", "content": "The propose of this step is to eliminate low-quality mesh as thoroughly as possible, and to mark object of distinctive types. This provides the following advantages:\n\u2022 Low-quality mesh can disturb the overall training process.\n\u2022 Labeling mesh with its class allows us to fine-tune diffusion model on data from certain domains.\nThe filtering process descirbed in this section is modified from the process used in MeshXL [2], as shown in Fig. 11. After this process, we render 9 images surrounding the mesh, and use the HunYuan vision model\u00b9\u2070 to filter the mesh by these rendered images. Note that you can substitute this vision model with any other vLLM models, such as GPT-4V\u00b9\u00b9."}, {"title": "4.5. Generate watertight mesh", "content": "Generating watertight mesh, which is essential for 3D-DiT training, is generating an envelope tightly conforming to the model's exterior geometry, rather than using both interior and exterior geometry. The latter, like Manifold [11, 12] and other works [23] that can provide similar results, is not suitable for 3D-DiT training.\nWe use similar watertight mesh generation method from 3DShape2VecSet [40], which is adopted from Stutz's work [32] and used in occupancy networks [18]\u00b9\u00b2. The method is based on TSDF fusion of a group of depth map rendering around the object. However, this method will slightly vary the exterior shape of the mesh due to the following reasons:\n\u2022 The method calculates closing of each depth map, which fills small gaps on depth image but remove some details. Distortion caused by such reason can be reduced by increasing the definition of depth image, as closing of the image is often calculated using fixed window size.\n\u2022 The method applies a bias that is half voxel size. Distortion caused by such reason can be reduced by high-definition SDF volume.\nHowever, increasing the definition of the SDF volume will lead to a mesh with particularly large face number, which requires the pipeline to provide a decimated mesh. It's worth noticing that most decimating methods in DCC software uses QEM triangle decimation [10], which will sometimes provide ill-formed faces i.e. the shape of the face may not be suitable for further developments. We therefore also recommend using ACVD [1, 35, 34] in the decimating step. We have also developed a baking tool based on baking function in Blender to provide texture for watertight mesh."}, {"title": "4.6. Rendering and Sampling", "content": "All meshes are normalized to a tightly coupled (radius is 1) bounding sphere using Welzl's algorithm [37]. We prefer not to use bounding boxes because arbitrarily rotating objects within them may cause the objects to extend beyond the confines of the box. Rendering is done in Blender using EEVEE\u00b9\u00b3 renderer, while sampling is done using trimesh\u00b9\u2074. We sample three groups of points, each with 500k points, which is similar to sampling approach used by For 3D geometry compression model, we build upon CraftsMan [16], who adopts structures introduced in 3DShape2VecSet [40]:\n\u2022 We perform uniformly random sampling within the circumscribed cube of the bounding sphere, yielding SPACE points.\n\u2022 We sample a group of points just on surface of the watertight mesh, yielding SURFACE points. We compute Gaussian curvature of each vertex on the mesh and use this curvature as importance of each area on the mesh: more points will be sampled on areas with higher curvature."}, {"title": "5. Experiments", "content": "Fig. 12 and Fig. 13 illustrate the results of 3D generation with the prompt and the image as input. As shown in these figures, which shows that our Pandora3D system could faithfully recover the shape and texture with fine-grained details and produce neat space without any floaters."}, {"title": "6. Conclusion", "content": "In this report, we present Pandora3D, a framework designed for high-quality 3D shape and texture generation. 3D shape generation and texture generation are proposed in Pandora3D. In specific, the 3D shape generation utilizes a Variational Au- toencoder (VAE) to encode implicit 3D geometries into a latent space; then, a diffusion network is used to generate latents conditioned on input prompts, with modifications aimed at enhancing the model's capacity. Meanwhile, we also explore an alternative Artist-Created Mesh (AM) generation approach, which shows promising results for simpler geometries. The texture generation process is multi-staged, starting with the generation of frontal images, followed by multi-view images gen- eration, RGB-to-PBR texture conversion, and high-resolution multi-view texture refinement. A novel consistency scheduler is integrated into every stage of this process to ensure pixel-wise consistency among multi-view textures during inference, leading to seamless integration. Experiment results demonstrate the effectiveness of Pandora3D handling of diverse input formats to produce high-quality 3D content."}]}