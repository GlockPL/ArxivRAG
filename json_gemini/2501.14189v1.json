{"title": "Distributed Multi-Agent Coordination Using Multi-Modal Foundation Models", "authors": ["Saaduddin Mahmud", "Dorian Benhamou Goldfajn", "Shlomo Zilberstein"], "abstract": "Distributed Constraint Optimization Problems (DCOPs) offer a powerful framework for multi-agent coordination but often rely on labor-intensive, manual problem construction. To address this, we introduce VL-DCOPs, a framework that takes advantage of large multimodal foundation models (LFMs) to automatically generate constraints from both visual and linguistic instructions. We then introduce a spectrum of agent archetypes for solving VL-DCOPs: from a neuro-symbolic agent that delegates some of the algorithmic decisions to an LFM, to a fully neural agent that depends entirely on an LFM for coordination. We evaluate these agent archetypes using state-of-the-art LLMs (large language models) and VLMs (vision language models) on three novel VL-DCOP tasks and compare their respective advantages and drawbacks. Lastly, we discuss how this work extends to broader frontier challenges in the DCOP literature.", "sections": [{"title": "1 Introduction", "content": "Distributed Constraint Optimization Problems (DCOPs) [Yokoo et al., 1998] are a widely studied framework that has previously been shown to be effective in various applications, including smart homes, robotic and drone coordination, surveillance, economic dispatch, network coordination, and disaster response [Maheswaran et al., 2004b; Farinelli et al., 2014; Zivan et al., 2015; Fioretto et al., 2017; Mahmud et al., 2020b]. However, in many of these cases, the problems are static and manually modeled, reducing their applicability in changing scenarios. Although some previous works have considered a dynamic formulation of DCOPs [Petcu and Faltings, 2005b], these approaches still require manual defining of a dynamics model, which limits their adaptability.\nAnother approach to mitigating this issue is to use preference elicitation to handle partially defined constraints, allowing agents to query humans to reduce uncertainty over time [Tabakhi et al., 2017; Tabakhi et al., 2019; Le et al., 2018; Tsouros et al., 2024; Zivan et al., 2024]. Even then, initial constraints must be established beforehand, queries often use language templates, and answers to queries take the form of numeric constraint cost values, which may not be a natural or convenient way for humans to communicate their preferences [Tversky et al., 1982].\nTo address these challenges, we introduce visual-linguistic instruction-based DCOPS, VL-DCOPs, an extended DCOP framework that models the interaction between humans and agents to specify constraints to solve a coordination task (Fig. 1). As a motivating example, consider a group of smartphone-based LFM-driven agents tasked with scheduling meetings for their owners. Initially, the agents use the instructions and contact lists provided to establish constraints with other agents. The LFM agents then coordinate by asking questions about schedules, reducing any uncertainty about the underlying cost structure. Since these agents rely on natural language, human responses can go beyond purely numerical feedback, incorporating explanations and other contextual details. Once constraints are established, the agents initiate a suitable optimization protocol until a mutually agreeable schedule is found.\nTo implement the VL-DCOPs we design a spectrum of autonomous agent archetypes (Table 1) each utilizing a large multi-modal foundation model [Bhambri et al., 2024] to a varying degree. At one end of the spectrum, we have A1 agents, a form of neuro-symbolic agent [Sheth and Roy, 2023] that uses these LFMs to parse the human instruction to make key algorithmic decisions such as action selection. However, the agent's workflow is still dictated by the classical algorithm. In this paper, we propose Foundation Model Centered DSA, FMC-DSA, a variant of the popular DSA [Fitzpatrick and Meertens, 2003] algorithm for this archetype.\nNext, we present a more involved neuro-symbolic agent, A2, to address issues related to uncertainty about constraint cost due to natural ambiguity in linguistic communication and conflicts that may arise due to varying preferences among humans. To that end, we propose Cooperative Preference Aggregation (CoPA), an algorithm that uses LFMs to learn a good representation of the constraint function, and then uses a classical algorithm to solve the problem. Compared to A1, A2 provides a more interactive solution through conversation with humans and other agents.\nFinally, we present A3, a general agent that in theory can simulate any algorithm depending on the instruction provided in the prompt. We model A3 as a sequential decision-making agent that uses an LFM-based (in-context) policy to solve the coordination problem. Unlike the previous two agents, for A3, we do not hard code any particular optimization procedure, making it a fully neural agent. The key motivation for this agent archetype is to provide flexibility to handle exceptional situations that may arise during optimization.\nTo evaluate these agents, we propose three new benchmarks. The first two are based on classical distributed weighted graph coloring problems [Fioretto et al., 2016]. We design a linguistic (LDGC) and a visual (VDGC) variant of this problem to evaluate these agents. The third benchmark is based on more realistic meeting scheduling problems [Fioretto et al., 2016]. To build these agents we used pre-trained LLMs and VLMs such as LLAMA [Dubey et al., 2024], Qwen [Team, 2024], and GPT-40[Hurst et al., 2024]. Our analysis provides insight into the capability of current LFMs in distributed coordination and the advantages and disadvantages of each archetype. To our knowledge, our work is the first of its kind study in the DCOP literature and opens the door to many future research directions with interesting potential in explainability [Roie Zivan, 2022], privacy [Tassa et al., 2019], and exception handling such as message delay [Rachmut et al., 2024; Matsui, 2024]. Further, with the rapid progress of GPU-based edge devices (such as Nvidia Jetson Orin Nano Super [Su, 2024]), and the ever-expanding capability of the frontier LFMs [Meta, 2024], we expect a practical deployment of A3-like agents to be operational in the near future."}, {"title": "2 Related Work", "content": "DCOPs Framework As DCOPs must be designed manually, they face flexibility challenges in dynamic environments and scenarios where critical information is missing or difficult to obtain. Several extensions have been proposed to address this problem, including D-DCOP and U-DCOP [Petcu and Faltings, 2005b; L\u00e9aut\u00e9 and Faltings, 2011]. Dynamic-DCOPS (D-DCOPs) account for DCOPs that evolve over time, although these adaptations remain manually specified. U-DCOP offers a different perspective by employing probability distributions over constraint costs\u2014manually learned from data-to enhance the framework's adaptability. In contrast, the proposed VL-DCOP can dynamically incorporate human instructions to construct new DCOP instances.\nIn parallel, a substantial body of literature on preference elicitation has investigated user queries as a means to reduce uncertainty in cost functions, thereby introducing additional adaptability [Tabakhi et al., 2019; Tabakhi et al., 2017; Tsouros et al., 2024; Zivan et al., 2024; Le et al., 2018]. However, many such methods employ queries in a manually defined template and constrain the response options, often to a single numeric cost value. This may not be the most effective approach to communicating user preferences [Tversky et al., 1982]. Unlike these works, our research explores natural linguistic and visual mechanisms to express preferences. Moreover, the proposed CoPA algorithm further investigates agent-to-agent communication in natural language.\nDCOPs Algorithm Over the last two decades, several algorithms have been proposed to solve DCOPs including, SyncBB [Hirayama and Yokoo, 1997], DPOP [Petcu and Faltings, 2005a], DSA [Fitzpatrick and Meertens, 2003], MGM [Maheswaran et al., 2004a], ACO-DCOP [Chen et al., 2018], DPSA [Mahmud et al., 2020a], Max-Sum [Farinelli et al., 2008], CoCoA [van Leeuwen and Pawe\u0142czak, 2017]. In"}, {"title": "3 Background", "content": "Formally, a DCOP is defined by a tuple (A, X, D, F, \u03b4) [Modi et al., 2005]. Here, A is a set of agents {a1, a2, ..., an }, and X is a set of discrete variables that are controlled by this set of agents. D is a set of discrete domains and Di \u2208 D is a set that contains values that may be assigned to its associated variable xi. Fis a set of constraints where fi \u2208 Fis a function of a subset of variables xi \u2286 X. Thus, the function fi: \u220fxj\u2208xi Dj \u2192 R denotes the cost for each possible assignment of the variables in xi. \u03b4 : X \u2192 A is a variable-to-agent mapping function that assigns the control of each variable xi \u2208 X to an agent of A. Each variable is controlled by a single agent. However, each agent can hold several variables. Within the framework, the objective of a DCOP algorithm is to produce X*, a complete assignment that minimizes the aggregated cost of the constraints, defined below:\n$$X^* = \\arg\\min_X \\sum_{i=1}^n f_i(x^i)$$\n(1)\nThe Distributed Stochastic Algorithm (DSA) is a widely used approach for solving Distributed Constraint Optimization Problems (DCOPs). Each agent begins with a random assignment and engages in an iterative optimization process. During each iteration, the agent communicates its assignment to neighbors, updates its local context based on their messages, and computes the locally optimal assignment. This assignment is adopted probabilistically, following a stochasticity parameter \u03f5. This \u03f5-greedy strategy helps DSA agents to escape local minima. The process repeats until convergence, providing an efficient framework for distributed optimization."}, {"title": "4 Details of VL-DCOPS", "content": "We formally define VL-DCOPs as a tuple (AC, AI, \u0393, E, Tt) where:\n\u2022 AC is a set of agents, analogous to agents in DCOPs, that will take part in coordination tasks.\n\u2022 AI is a set of instructing agents that will provide task descriptions to agents in AC. For example, it can be a set of human providing instruction to their virtual assistants (AC) or a set of LFMs that are providing instruction to another set of coordinating LFMs (AC). For some problems, the same LMF can play different roles.\n\u2022 \u0393 is a mapping function that maps agent in AC to agents in AI to define flow of instruction. In practice, this may be implicitly defined; for example, by default, the owner of a mobile device has control over the virtual agent on the device.\n\u2022 E defines the network structure\u00b9 of the agents. This is not directly analogous to F in DCOP as constraints are not established a priori. This only defines which agent is allowed to communicate with other agents."}, {"title": "5 Agents for VL-DCOPs", "content": "In this section, we provide details of the three agent archetypes that we briefly introduced to implement VL-DCOPS."}, {"title": "5.1 A1: Instruction Following DCOP Agent", "content": "We begin by describing an agent designed to solve a simple instantiation of VL-DCOPs. Specifically, for each element E, there is a corresponding F, and the variables X and the corresponding domains D are fixed. Al is a basic LFM-centered agent that processes visual-linguistic instruction Z\u2081 as input and translates it into a decision while coordinating with other agents. To achieve this, we propose FMC-DSA, a straightforward extension of DSA tailored for this purpose.\nThe FMC-DSA agent begins by randomly initializing the local variable assignment C. It then generates natural language constraint descriptions with the help of the LFM M for each of its neighbors and sends these as constraint initialization messages, denoted mi,j. Similarly, the agent receives messages from all neighbors and stores them, along with their original instruction, in the set O (Algorithm 2, lines 2 \u2013 7).\nThe agent iteratively performs two steps until the termination criterion (e.g., the maximum number of iterations) is met:\n1. It sends and receives the current assignment from neighbors, updating the local context C accordingly (Algorithm 2, lines 9 \u2013 10).\n2. The agent prompts the LFM M to generate the optimal assignment based on the current assignment context C and the set O. The agent then updates its assignment to the best option with probability 1 \u2013 \u03f5 or randomly with probability \u03f5 (Algorithm 2, lines 11 \u2013 12).\nThis can be viewed as a local \u03f5-greedy strategy that facilitates coordination among the agents using LFMs. However, it is important to note that FMC-DSA introduces preference asymmetry, which can impact the coordination process. Specifically, although agents locally exchange information about constraints, they do not reach a consensus on the cost of each assignment. As a result, two coordinating agents may interpret the cost of the same assignment differently. We will address this issue in the next sub-section."}, {"title": "5.2 A2: Conversational DCOP Agent", "content": "Consider the LGCP problem scenario where agent X receives the instruction to avoid the color of agent Y, and the general color preference is A > B > C. Agent Y is instructed to avoid the color of agent X, and their preference is B > A > C. Therefore, according to agent X, the cost order is (C, C) > (A,A) > (B, B), while according to agent Y, the cost order is (C, C') > (B,B) > (A,A). In FMC-DSA, while the agent recognizes this difference, there is no coordination between the agents to reach a consensus about the assignment preference order. To address this, we now introduce an instance of A2, where agents go through a discussion process to achieve a consensus, called Cooperative Preference Aggregation (CoPA).\nCOPA is an iterative discussion process used to generate a cost table. The procedure starts after constructing O, similar to FMC-DSA (which is omitted here). Initially, both agents construct constraints. They then iteratively propose the cost table to their neighbor. Based on the two cost tables, they redesign the cost structure and propose the updated structure for K rounds. In the end, if a consensus is reached, they can use the agreed-upon cost function; otherwise, the conflict is resolved based on a heuristic, such as taking the average or maximum."}, {"title": "5.3 A3: Algorithm Simulating DCOP Agent", "content": "The A3 agent models the iterative coordination process as a sequential decision-making problem, formulated as a Markov Decision Process (MDP). This framework allows the agent to simulate classical coordination algorithms while dynamically adapting them online to handle exceptional cases.\nMDP Definition: An MDP for simulating algorithms can be defined as (S, A, T, R):\n\u2022 S: The state space, represented by the history of algorithm execution log.\n\u2022 A: The action space, encompassing decisions like action selection, message passing, and other function calls.\n\u2022 T: The transition function, which updates the algorithmic log based on actions and interactions with neighbors (assumed model-free).\n\u2022 R: The reward function, representing local or global costs (if anytime mechanisms exist).\nInstead of explicitly training a policy from scratch, we leverage Large Foundation Models (LFMs) as generalized policies that rely on in-context learning. The LFM interprets the algorithmic log and determines the next action. Prompts for the LFM may include descriptions of various DCOP algorithms, but our observations suggest that many LFMs, due to their pre-training data, already possess knowledge of common DCOP methods. Algorithm 4 provides a general outline of this approach.\nIn this setup, ENV represents the interaction interface through which the agent communicates with other agents and its own systems. The interface continuously monitors the environment and observes task initialization Tt, incoming messages, and internal system states.\nSimulating FMC-DSA with A3 We now provide an example of the A3 agent simulating FMC-DSA by iteratively updating its algorithmic log:\n\u2022 Initialization: The initial task instruction is observed through E as of at time t. Before this, the agent may be engaged in other tasks. The agent starts with an initialization action that generates random assignments and constraints, which are then added to the log L.\n\u2022 Message Passing: The agent performs a series of message-passing actions, receiving preferences (e.g., mi,j) from neighbors, and updates the context and log accordingly.\n\u2022 Assignment Updates: The agent alternates between assignment changes and message-passing actions, iteratively refining its decisions.\n\u2022 Interruption: If an error or interruption occurs during optimization, the A3 agents will adaptively respond based on their policy.\nA key advantage of this approach is its ability to adapt to exceptional scenarios in real-time. Some examples include: If a message fails to arrive, the agent can continue optimization with partial information. The agent can also make meta-algorithmic decisions such as when to commit to an action and hyper-parameter selection, adjust constraints mid-optimization, or incorporate dynamic changes in the task environment. This flexibility makes A3 robust in environments with uncertainties or dynamic requirements, while still adhering to the principles of classical DCOP coordination algorithms."}, {"title": "6 Analysis", "content": "This section will analyze the advantages and disadvantages of different agent archetypes.\nQuery Complexity: The primary computational cost in VL-DCOPs algorithms arises from the number of LLM queries required. A1 agents make one LLM query per iteration, resulting in O(n) queries where n is the number of iterations. In contrast, A2 agents make O(k) queries per neighbor, where k is the number of CoPA rounds, requiring a total of O(k \u00b7 Deg) queries, where Deg represents the average degree of the graph. Since k < n is in practice, A2 agents generally perform fewer queries in sparse graphs. On the other hand, A3 agents require O(c\u22c5n) LLM queries, where c represents the average number of algorithmic decisions made per iteration. In practice, A3 agents are the most computationally expensive VL-DCOP agents.\nBenchmarks: We present three different benchmarks to evaluate VL-DCOP. The first is LDGC, a language-based weighted graph coloring problem. Here, each agent is instructed in natural language about which agents they should match colors with and which agents they should avoid matching colors with. In addition, instructing agents (A\u00b9) describe their color preference order using natural language. The second benchmark is VLDGC, which incorporates both visual and language inputs. Specifically, the color preferences are presented using various types of plots, such as bar charts, line charts, and histograms, while the color-matching constraints are described in natural language. Finally, LDMS addresses the classical meeting scheduling problem. However, in this"}, {"title": "6.1 Agent Archetype Analysis", "content": "In this subsection, we analyze the agent archetypes where A1 is represented by FMC-DSA, A2 is represented by CoPA+DSA (k = 2), and A3 is represented by NSA. For each benchmark, we generated 10 different problem instances, each with 10 agents and 23 edges, with domain size 4. Each agent was simulated for 50 iterations. Additionally, we present results for DSA and random baselines simulated using a handcrafted cost function based on the instructions for 100 iterations to provide a comparison with ground-truth costs. We set \u03f5 = 0.1 for all cases except for A3 which was set to 0.03 as simulation inaccuracy introduced a second source of stochasticity. The cost represents the total mean cost for the last 50% of the iterations. The anytime cost is the best solution found across all iterations. Sat is the number of constraints satisfied without considering the preference cost. The costs of the assignments are calculated using a handcrafted cost table.\nWe observe the trend that Al agents achieve better anytime costs than A2 agents. However, it is important to note that A1 agent networks, in practice, cannot use the anytime mechanism [Zivan, 2008] as they cannot calculate the global cost. However, in theory, Al agents can find better solutions than A2 agents. This indicates for LFMs the task of inferring cost tables is more difficult than the assignment selection task.\nA2 agents generally perform slightly worse than Al agents. Importantly, for A2 agents, it is possible to calculate the anytime cost. In such cases, A2 agents outperform Al's average cost across all tasks. This highlights the advantage of A2's ability to compute and utilize anytime costs.\nFinally, A3 agents perform similarly or slightly worse than Al agents across all tasks, as they simulate Al agents. The slight performance degradation arises from imperfect simulations. Most models struggled with running the simulations, with GPT-40-mini being the only model capable of near-perfect simulations. We observed that the first 10 iterations were perfectly simulated, but the simulation quality later degraded, achieving only about 65% correct algorithmic decisions. This suggests that A3 agents might be better suited for simulating non-iterative algorithms like CoCoA[van Leeuwen and Pawe\u0142czak, 2017]. These results also demonstrate the potential to design adaptive LFM-centered algorithms.\nGPT-40-mini outperformed other models, including larger GPT-40, for A1 and A3 agents. However, for A2 agents, both GPT models underperformed compared to LLAMA models in VLDGC and LDMS. LLAMA 3.3 70B achieved the best performance for A2, and as A2's anytime cost is represen-"}, {"title": "6.2 Scaling Up the Benchmark", "content": "In this subsection, we scale up the benchmark and evaluate performance using 10 instances of the LDGC problem, each consisting of 50 agents, 120 edges, and a domain size of 4. We consider two types of network architectures: random and scale-free [Barab\u00e1si and Albert, 1999].\nWe use the GPT-40-mini model, which demonstrates excellent performance and cost-effectiveness. However, the computational requirements and cost of other models make them prohibitively expensive to utilize at this scale. To address this, we consider the ModernBART Large model, a compact and efficient alternative model capable of running on CPUs and performing significantly faster than OpenAI API calls when deployed on GPUs. This efficiency makes it suitable for practical applications and deployment on edge devices, such as Raspberry Pi.\nThe main limitation of ModernBART and similar small models is their lack of general intelligence and inability to handle general decision-making tasks. As a result, these models cannot be directly used to implement the proposed agent without modifications. To overcome this limitation, we perform task-specific training on ModernBART. Specifically, we use ground truth decisions generated by the DSA algorithm along with prompts from the GPT-40-mini model as the training dataset. For this purpose, we collect 8,000 decision pairs for each type of agent. Overall, this training process requires less than 2 hours to complete.\nPerformance: ModernBERT achieves perfect task performance, surpassing even the GPT models in accuracy when evaluated against DSA. The key advantages are as follows: VL-DCOP can be deployed on edge devices, enabling realtime operations in distributed networks. The model demonstrates high scalability, with simulations of 50 A1 agents over 50 iterations completed in less than 3 minutes on a single GPU system. We also found that A2 agents were more query-efficient and quicker to simulate on these sparse graphs. However, there are notable disadvantages: ModernBERT is not a general-purpose VL-DCOP agent and is limited to solving specific tasks for which it was trained. The trained models perform well only when input prompts are similar in length and structure to the original training dataset. Performance degrades significantly when there is considerable dissimilarity. Overall, the findings indicate that VL-DCOP can be highly scalable and can be implemented on the current hardware, provided specific optimizations are implemented."}, {"title": "7 Conclusion and Future Directions", "content": "We introduce VL-DCOPs, a novel framework that integrates visual and linguistic instructions with DCOPs, making the system more dynamic and natural to use. We propose several LFM-centered agent archetypes to solve VL-DCOP tasks. Our experiments demonstrate that LFMs are capable of addressing these problems off-the-shelf and can be further fine-tuned to achieve performance comparable to symbolic systems. Our work opens up several promising avenues for future research:\nAdaptive Algorithms: By demonstrating the feasibility of using general algorithm-simulating agents in VL-DCOPs, this work paves the way for adaptive algorithms that can handle exceptions such as network delays and other types of interruptions. Investigating these adaptive capabilities is an important direction for future research.\nExplainability and Interpretability: Incorporating visual and linguistic understanding can make agents naturally more interpretable. Further work on improving and evaluating the explainability and interpretability of VL-DCOP agents would be highly valuable.\nPrivacy and Security Concerns: There are several privacy and security issues inherent to VL-DCOP networks. For instance, malicious agents with general intelligence could influence other agents' decisions to favor their preferences, extract sensitive information about the network structure, and so on. Addressing these issues and exploring mechanisms to prevent such gaming scenarios is a crucial direction for future work.\nOverall, our work highlights the potential of VL-DCOPs and the need for continued research into adaptive algorithms, interpretability, and privacy to fully realize their capabilities in dynamic, real-world scenarios."}]}