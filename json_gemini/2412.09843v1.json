{"title": "Learning Structural Causal Models from Ordering: Identifiable Flow Models", "authors": ["Minh Khoa Le", "Kien Do", "Truyen Tran"], "abstract": "In this study, we address causal inference when only observational data and a valid causal ordering from the causal graph are available. We introduce a set of flow models that can recover component-wise, invertible transformation of exogenous variables. Our flow-based methods offer flexible model design while maintaining causal consistency regardless of the number of discretization steps. We propose design improvements that enable simultaneous learning of all causal mechanisms and reduce abduction and prediction complexity to linear O(n) relative to the number of layers, independent of the number of causal variables. Empirically, we demonstrate that our method outperforms previous state-of-the-art approaches and delivers consistent performance across a wide range of structural causal models in answering observational, interventional, and counterfactual questions. Additionally, our method achieves a significant reduction in computational time compared to existing diffusion-based techniques, making it practical for large structural causal models.", "sections": [{"title": "Introduction", "content": "Deep neural networks are highly expressive and learnable, but are inherently associative, making it difficult for them to capture causal relationships. This limitation can lead to inaccurate predictions in fields where causality is crucial (LeRoy 2004; Russo and Williamson 2007; Nguyen et al. 2023). Among the efforts to alleviate this problem, a promising direction dubbed causal representation learning (Sch\u00f6lkopf et al. 2021) is to integrate neural networks within the framework of Structural Causal Models (SCMs) (Pearl 2009). SCMs are principled way to answer observational, interventional, and counterfactual questions, but learning them from data remains challenging. This paper focuses on efficient learning of SCMs from only observational data and causal ordering, leveraging deep neural networks to model causal relationships in complex systems.\nSome previous methods such as (S\u00e1nchez-Martin, Rateike, and Valera 2022) require a fully observed causal graph, which can be infeasible in real-world settings. Others (Javaloy, Martin, and Valera 2023; Khemakhem et al. 2021) use Autoregressive Normalizing Flows (Papamakarios, Pavlakou, and Murray 2017; Durkan et al. 2019), which restrict model design to be monotonic and require additional regularization to scale to multiple layers. Sanchez and Tsaftaris (2022) propose a counterfactual estimation method using diffusion models and classifier guidance, but it only considers bivariate causal graphs and lacks theoretical analysis. (Chao et al. 2024) generalize previous diffusion-based methods but require a fully observed causal graph and a separate deep neural network for each observed variable, resulting in slow sequential inference and a large number of parameters.\nWe propose a identifiable flow models that requires only observational data and a valid causal ordering. Our approach is designed to represent SCMs and ensure causal consistency through its structure. A parallel sped-up design can answer observational, interventional, and counterfactual questions with computational complexity scaling linearly with the number of model layers, independent of the causal graph's node count. This scalability enables efficient handling of large causal models, making our approach practical and effective for diverse applications.\nWe empirically demonstrate that our method outperforms competing approaches across a wide range of synthetic and real datasets, excelling in estimating both the mean and overall shape of interventional and counterfactual distributions. The experiments confirm that our parallel architecture is not only scalable but also maintains high performance as the complexity and size of the datasets increase.\nOur key contributions are:\n1. We prove the identifiability of flow models for learning SCMs from observational data and causal ordering.\n2. We introduce novel model designs enabling parallel abduction and approximated prediction, removing autoregressive constraints and significantly reducing computational and memory requirements.\n3. We validate our methods' effectiveness and performance on diverse synthetic and real-world datasets."}, {"title": "Related Work", "content": "Recent advances in deep generative models (DGMs) have found their way into learning Structural Causal Models (SCMs). Karimi et al. (2020) propose a conditional variational autoencoder (VAE) for each Markov factorization implied by the causal graph. VAEs on graphs are also studied assuming certain design constraints but have yet to achieve empirical success (Zecevic et al. 2021; S\u00e1nchez-Martin, Rateike, and Valera 2022). (Kocaoglu et al. 2017) use generative adversarial network for learning a causal implicit generative model for a given causal graph. (Geffner et al. 2022) propose an autoregressive-flow based non-linear additive noise end-to-end framework for causal discovery and inference. (Pawlowski, Coelho de Castro, and Glocker 2020) introduce several generative models to learn SCMs, but these are not guaranteed to learn the true causal mechanism, as multiple models can produce the same observational distribution. Another DGM class, autoregressive normalizing flows, has also been suggested (Khemakhem et al. 2021). (Javaloy, Martin, and Valera 2023) generalize this by considering a class of triangular monotonically increasing maps that are identifiable up to invertible, component-wise transformations. However, this model design is restricted to monotonic functions and requires additional regularization to scale to multiple layers. (Scetbon et al. 2024) view SCMs as a fixed-point problem over causally ordered variables, infer causal ordering from data and use it to develop a fixed-point SCM via an attention-based autoencoder.\nDiffusion models represent another competitive class of DGMs. Sanchez and Tsaftaris (2022) use diffusion models for counterfactual estimation in bivariate graphs where an image class is the parent of an image. However, this approach requires training a separate classifier (Dhariwal and Nichol 2021) for do-interventions, lacks theoretical guarantees, and shows poor performance for more complex images. (Chao et al. 2024) offer both interventional and counterfactual inferences, but only guarantee identifiability with a fully observed causal graph. Their method requires a separate neural network for each causal node and sequential inference, making it computationally expensive and memory-intensive for large causal graphs. In contrast, our proposed flow models are flexible in design while maintaining causal consistency. Our approach reduces computational and memory complexity, enhances scalability by avoiding separate neural networks for each causal node, and eliminates sequential inference, making it more practical for large causal graphs.\nAlternative to DGMs, Ordinary Differential Equations (ODEs) have been used to describe deterministic SCMs, which is often unrealistic (Mooij, Janzing, and Sch\u00f6lkopf 2013). (Peters, Bauer, and Pfister 2022) introduce the Causal Kinetic Model, a collection of ODEs requiring parent values at each time step. (Hansen and Sokol 2014) illustrate a causal interpretation of SDE, show how to apply interventions to a SDE. (Wang, Jennings, and Gong 2023) combine neural SDE with variational inference to model causal structure in continuous-time time-series data.."}, {"title": "Preliminaries", "content": "Structural Causal Models (SCMs)\nGiven a directed acyclic graph (DAG) graph G = (V,E) representing the causal relationships between d endogenous variables x = {x1,...,xd}, a SCM (Pearl 2009) M associated with G is a set of structural equations xi = fi(xpai, ui) for all i \u2208 {1, ..., d} that characterize how each node xi in V is generated from its parent nodes xpa := {xj | the directed edge (j, i) \u2208 E} and the corresponding exogenous variable ui via a deterministic function fi. Usually u = {u1, ..., ud} are assumed to be jointly independent, i.e., p (u1, ..., ud) = \u220fdi=1 p (ui). This makes the SCM M Markovian, leading to the factorization p (x1, ..., xd) = \u220fdi=1 p (xi|xpa).\nSince G is acyclic, we can specify a causal ordering of all nodes such that if node xj is a parent of node xi then \u03c0 (j) < \u03c0 (i).\nFor deep neural networks to answer causal questions, we can treat u as latent exogenous variables, encode exogenous variables to latent spaces z = fencode (x, xpa) as the abduction step, and decode back x = fdecode (z, xpa) as the prediction step. This structure enables VAES, GANs and Normalizing Flows to learn deep SCMs.\nCausal consistency A mapping T between variables u and x is deemed causally consistent with structural causal model M if it shares the same causal dependencies with the M. It means their Jacobian matrices have zero values in the same positions, i.e., \u2207uT (u) = I + diam(A) diam AT and \u2207xT\u22121 (x) = I \u2013 A, where I is the identity matrix and A is the adjacency matrix of the causal graph."}, {"title": "Triangular Monotonically Increasing (TMI) Maps for Identifiable SCMs", "content": "A function f: Rd \u2192 Rd is a monotone increasing triangular map if:\n\n\nf (x) =\n\nf1 (x1)\nf2 (x1, x2)\n:\nfd (x1,...,xd)\n\n\n,  (1)\nwhere each fi: R\u2192 R is monotone increasing (or decreasing) with respect to xi for any x1:i\u22121. In case \u03bc = \u03bd\u22c5f where \u03bc, \u03bd are strictly positive density and f is a TMI map, then f is equivalent to the Knothe-Rosenblatt (KR) transport almost everywhere (Jaini, Selby, and Yu 2019).\nIdentifiability refers to recovering ground truth latent factors. (Xi and Bloem-Reddy 2023) show that nonlinear independent component analysis (ICA) models with generator functions that are TMI maps, and fully supported latent distributions with independent components, are identifiable up to invertible, component-wise transformations. This means we can recover the model up to an invertible, component-wise transformation of the true latent factors. In causal representation learning, on top of identifying the latent representation, the causal graph encoding their relations must also be identifiable. (Javaloy, Martin, and Valera 2023) further show that not only can the model isolate the exogenous variables, it also shares the functional dependencies with true structural equations. However, due to the TMI assumption, their model design must fulfill the monotonic requirements."}, {"title": "Diffusion and Flow Models", "content": "In continuous-time diffusion models, the stochastic process of generating data from a Gaussian prior can be represented via a backward SDE (Song et al. 2021):\ndx = (f (x, t) \u2013 g2 (t) \u2207x log pt (x)) dt + g (t) dw (2)\nwhere w denotes the reverse-time Wiener process. The probability flow ODE (PF ODE) which shares the same marginal probability densities pt (xt) as the SDE above is expressed as follow:\ndx = (f (x, t) \u2212 1/2 g2 (t) \u2207xlogpt (x)) dt,  (3)\nThe PF ODE enables efficient mappings in both directions, from the data to the prior and vice versa. The PF ODE can be modeled via the score matching (Hyv\u00e4rinen and Dayan 2005; Song and Ermon 2019) or flow matching framework (Lipman et al. 2023; Liu, Gong, and Liu 2023). In the latter case, the velocity vo (x, t) of the PF ODE is learned by minimizing the mean square error w.r.t. the target velocity v (x0, x1, t) over time t sampled uniformly in [0, 1]:\n\nLFM = Ex\u223c\u03c0 E[||vo (xt, t) \u2212 v (x0, x1, t)||22] (4)\n\nTo enable fast sampling, the data and prior distributions can be connected through a \u201cstochastic interpolant\u201d Xt = (1 \u2212 t) Xo + tX1 (Albergo, Boffi, and Vanden-Eijnden 2023; Liu, Gong, and Liu 2023). In this case, v (x0, xt, t) = x1 \u2212 xo and the PF ODE is called the Rectified Flow."}, {"title": "Method", "content": "In this section, we first prove that set of flow models are an identifiable and flexible choice in learning SCMs given only observational data and causal ordering. For ensuring identifiablity we assume that SCMs are Markovian, acyclic with diffeomorphic structural equations. Then, we propose a compact model design that can do fast inference (prediction and abduction) in parallel."}, {"title": "Identifiable Causal Flow Models for Learning of SCMs with Ordering", "content": "We introduce a flow-based model for learning SCMs with ordering that is identifiable. Our approach involves constructing a set of flows, one for each node, so that i) the distribution of exogenous variables is a factorized distribution with support over the entire space Rd, and ii) the mapping from endogenous variables to exogenous variables is a TMI map. This allows us to leverage a theoretical result from (Xi and Bloem-Reddy 2023) (Proposition 5.2) to prove for identifiability.\nLet p (u) represent a jointly factorized distribution of all d exogenous variables u = {u1,..., ud}, i.e., p (u) = \u220fdi=1 p (ui) where p (ui) is a Gaussian distribution in R. We assume that while the structural causal model (SCM) M is unknown, the causal ordering \u03c0 among the nodes is known. We represent each node xi \u2208 R as the value at time t = 1 of the initial value problem (IVP) below:\n\ndzi = vi (zi, u<\u03c0i, t) dt, z0i = ui  (5)\n\nwhere zi denotes the state at time t, u<\u03c0i := {uj|\u03c0j < \u03c0i} is the set of nodes with lower orders than ui according to \u03c0. The solution at time t of the above IVP can be expressed as zti = z0i + \u222b0tvi (zi\u03c4, u<\u03c0i, \u03c4) d\u03c4, which means zi can be regarded as a function of the initial value z0i. Let fi (ui, u<\u03c0i,t) = fi (z0i, u<\u03c0i,t) := zti denote the representation of node i at time t, and let f (u, t) := (f1 (u1, u<\u03c01, t), ..., fd (ud, u<\u03c0d,t)) represent the state of all nodes at time t. Below, we show that f (u, t) is a TMI map of u satisfying two key properties: monotonicity and triangularity, as stated in Theorem 1.\nTheorem 1. Let f (u,t) be the solution of the set of initial value problems (IVPs) for all nodes at time t, with the IVP for node i is described in Eq 5. If the velocity function vi (zi, u<\u03c0i, t) is continuous w.r.t. t and Lipschitz continuous w.r.t. zi for all t \u2208 (0,1), u<\u03c0\u03af \u2208 R\u03c0i\u22121, and i \u2208 {1, ..., d}, then f is a triangular monotonically increasing (TMI) map of u.\nProof. First, we prove that fi (ui, u<\u03c0i,t) is a monotonically increasing function of ui. Since u<i is constant w.r.t. ui and t, we can simplify the notation by denoting fi (ui, t) := fi (ui, u<\u03c0i,t) and consider fi as a function of ui and t only. Suppose there exists two initial values a, b of ui such that a > b (or equivalently, fi (a,0) > f (b, 0)) but"}, {"title": "", "content": "fi (a,t) \u2264 fi (b,t). Since fi (ui, t) is continuous w.r.t. t, we can find \u03b3 \u2208 (0,t] such that fi (a, \u03b3) = fi (b, \u03b3) = \u03b6. This leads to two distinct solutions of the IVP starting at z0i = \u03da, which contradicts the Picard-Lindelof theorem.\nfi (a,t) > fi (b,t). Since fi (a, t) and fi (b, t) are continuous for all t \u2208 (0,1) (due to the continuity of vi w.r.t. t and zi), there must exists \u03b3 \u2208 (0,t] such that fi (a, \u03b3) = fi (b, \u03b3) = \u03da (as illustrated in Fig. 2). This implies that fi (a,t), fi (b, t) are two distinct solutions of the IVP:\n\ndzi = vi (zi, u<i, t) dt, z\u03b3i = \u03b6. (6)\n\nwith t \u2208 (\u03b3, 1).This contradicts the Picard-Lindelof theorem, which states the IVP in Eq. 6 has a unique solution if vi (zi, u<\u03c0i, t) is continuous w.r.t. t and Lipschitz continuous w.r.t. zi (Simmons 2016; Chen et al. 2018). This means fi (a,t) > fi (b,t) if a > b, meaning fi (ui, t) is a monotonically increasing function of ui. Proving that f is a triangular map is straightforward since, by design, fi is a function of nodes with orders lower than or equal to \u03c0i. Consequently, f is a TMI map of u. \u25a1\nLet vo be a parameterized model of vi. To ensure that vo (zi, u<i, t) is continuous w.r.t. t and Lipschitz continuous w.r.t. zi, we model it using a feed-forward neural network with finite weights and 1-Lipschitz continuous activation functions (e.g., sigmoid, tanh, ReLU, ELU). A detailed explanation for this can be found in Appendix. The input to this network is the concatenation of zi, u<\u03c0i,and t.\nSince ui is typically unknown and often varies during training vo (zi, u<i,t), we instead model vo (zi, x<i,t) in practice and consider this velocity in subsequent discussions. This velocity can be viewed as a reparameterization of vo (zi, u<i, t) because x<\u03c0i = f (u<\u03c0i, 1).\nWe note that our method can be applied when the causal graph G is available. In this case, we simply replace x<\u03c0i in vo (zi, x<i, t) with xpai, ensuring that our method remains consistent with the SCM M.\nSince v represents the velocity of an ODE that maps p (ui) = N (0, I) to p (xi), we learn vo by first constructing a reference diffusion process between p (xi) and p (ui), and then aligning v with the velocity of the PF ODE corresponding to this diffusion process. In this work, we select Zt = (1 \u2212 t) Ui + tXi as the reference process due to the sampling efficiency of its PF ODE. This choice results in the following training objective for all vi (i \u2208 {1, ..., d}):\n\nL (\u03b8) = \u2211di=1 Et Ezi,ui [||vo (zti, x<\u03c0i, t) \u2013 (xi \u2013 ui)||22]+ \u03bb ||\u03b8||2 (7)\n\nIn Eq. 7, we include a weight regularization term to ensure that \u03b8 is finite. We refer to our method as a Causal Flow Model (CFM). After training vo (zi, x<i, t) for all i \u2208 {1,...,d}, we can compute xi from ui and x<\u03c0i as xi = fi (ui,x<\u03c0i, 1) by solving the ODE in Eq. 5 forward in time. Similarly, we can compute ui from xi and x<\u03c0i as ui = (fi)\u22121 (xi, x<i, 0) by solving this backward in time. These two steps correspond to the prediction and abduction steps."}, {"title": "Efficient Causal Flow Models", "content": "Efficient Abduction with Masked Autoregressive Velocity neural Network (MAVEN) To approximate the velocity v for all nodes while reducing time and memory complexity, we propose the MAVEN architecture. It comprises two key components: a Masked Autoregressive Neural Network (MADE \u03b8) with a fixed ordering (Germain et al. 2015) and a simple feed-forward neural network heo. MADE \u03b8 encodes x into ci \u2208 Rdxk, where each vector ci contains node-specific encoded information dependent only on preceding variables. The feed-forward network then predicts node velocity by taking a concatenated vector of [zi, ci, t, i].\n\ncj = MADE \u03b8(x<\u03c0i)  (8)\nvi = h\u03b8([zi, ci, t]) (9)\n\nvo (zt, x, t) = [ho ([zt, c1, t]), ..., ho ([zt, cd,t])] (10)\nWe enforce the flow of each node to follows a linear path from the endogenous variable xi to exogenous variable ui. This is achieved through the prior ODE defined as dx = (ui \u2013 xi) dt. vo can be trained using gradient descent by solving the regression problem:\n\n\u03b8\u2217 = argmin\u03b8 E[ \u222b01 ||vo (zt, x, t) \u2013 (u \u2212 x) ||2 dt.] (11)\n\nWe call the proposed method as Sequential-Causal Flow Model (S-CFM). It optimizes the abduction process by reducing computational complexity from O(nd) to O(n), enabling more efficient processing of large-scale causal networks. See Abduction in Fig. 1 for an example of 3-node causal graph.\nNext, we propose a method that reduce complexity of prediction step from O (nd) to O (n), enabling parallel prediction.\nEndogenous predictor During prediction, we encounter challenges due to limited information about endogenous variables, necessitating a sequential, node-by-node prediction approach. At each time step t, the i-th endogenous variable xi is deterministically derived from current and preceding exogenous variables u\u2264\u03c0i := {\u03c5|\u03c0j \u2264 \u03c0i}. By"}, {"title": "", "content": "leveraging a neural network, specifically a MADE, denoted as EPo (zt, u, t), we can accurately predict these variables. The predicted endogenous variables then serve as input for MAVEN to estimate velocity at each time step, effectively acting as a distillation mechanism for the flow model.\nThe endogenous predictor is trained by solving the following optimization problem:\n\n\u03b8\u2217 = argmin\u03b8 E[ \u222b01 ||EP\u03b8 (zt, u, t) \u2013 x||2] dt. (12)\n\nWe introduce the Parallel-Causal Flow Model (P-CFM) by integrating S-CFM with an endogenous predictor. The training of the predictor leverages the abduction process, sampling from x with linear complexity. This approach ensures efficient training, enabling scalability and effective handling of larger datasets with minimal computational overhead.\nSee Fig. 1 for an example of P-CFM on a 3-node SCM. The training of P-CFM is summarized in Alg. 1, the abduction step in Alg. 3 and the prediction step in Alg. 2."}, {"title": "Do operator", "content": "To enable intervention and counterfactual calculations, we modify Pearl's do-operator do (xi = \u03b1) differently for S-CFM and P-CFM. In S-CFM, we replace the i-th causal mechanism fi with a constant function, fi = \u03b1. For P-CFM, we adopt the backtracking counterfactual approach (Von K\u00fcgelgen, Mohamed, and Beckers 2023), which modifies the exogenous distribution p (u) while maintaining causal mechanisms f. An intervention do(xi = \u03b1) updates p(u) by restricting plausible u values to those yielding the intervened value \u03b1. The intervened SCM is defined as Mi = (f,p1(u)), with p1(u) density determined by:\n\np1(u) = \u03b4 ({ui : fi (xPai, ui) = \u03b1})\u22c5\u220fj\u2260i p(uj). (13)"}, {"title": "Settings", "content": "Implementation For MAVEN and endogenous predictor EP\u03b8, we use a MADE with three hidden layers [256, 256, 256] and ELU activation, and a fully connected neural network with the same layers and activation. We use the Adam optimizer with a learning rate of 0.001 and apply a decay factor of 0.95 to the learning rate if it remains at a plateau for more than 10 epochs, batch size of 2048, and train for 900 epochs. Inference process is done in 50 steps.\nSynthetic datasets We consider synthetic SCMs various in the number of nodes and edges as they allow us to have direct access to the true observational, interventional and counterfactual distributions to evaluate methods. We study two main classes of structural equations:\n>Non-linear additive noise (NLIN): Simpson, a 4-node SCM simulating a Simpson's paradox that is difficult to approximate. Diamond, a 4-node SCM with 4 egdes arranged in a diamond shape. Large Backdoor (LargeBD), a 9-node non-Gaussian noise with sparse causal graph.\n>Non-additive noise (NADD): Triangle, a 3-node SCM with confounding graph in which x1 is the parent of x2, x3 and x2 causes x3. Y, a 4-node Y-shaped SCM.\nWe randomly sample 50,000/5,000/5,000 samples for training, evaluation and testing, respectively. See appendix for more details on the structural equations and all results.\nBaselines For the synthetic experiments, we compare our methods with two recently state-of-the-art methods Causal Normalizing Flows (CausalNF) (Javaloy, Martin, and Valera 2023) and VACA (S\u00e1nchez-Martin, Rateike, and Valera 2022). For CausalNF, we use two types of flow architectures: Masked Autoregressive Flow (MAF) (Papamakarios, Pavlakou, and Murray 2017) and Neural Spline Flow (NSF) (Durkan et al. 2019). As a requirements of VACA, we use fully observed causal graph for training in testing VACA. Due to the differences in settings, we do not show comparison between our methods and Diffusion-based Causal Models (DCM) (Chao et al. 2024), which requires fully observed causal graph. When only the causal ordering is observed, training the DCM requires either multiple random selections of parents for each node or full causal graph discovery, which are both expensive. We re-implement these methods based on provided code. For a fair comparison, each model uses the same budget for optimization. For the real datasets, we compare with CausalNF-MAF, CausalNF-NSF, CAREFL (Khemakhem et al. 2021), regression function from an additive noise model (ANM) (Hoyer et al. 2008) and standard ridge regression model (Linear SCM).\nMetrics To evaluate the methods on the three-layer causal hierarchy, we report the Maximum Mean Discrepancy (MMD) (Gretton et al. 2012) between the true and the estimated observational and interventional distributions. We do not evaluate the Average Treatment Effect (ATE) as in (Javaloy, Martin, and Valera 2023), as the metric does not provide insights into the distribution of treatment effects within the population. For counterfactual estimation, we report the mean squared error (MSE) between the actual and estimated counterfactual values."}, {"title": "Results on Synthetic Datasets", "content": "Table 1 reports the experimental results on five synthetic SCMs for observational, interventional and counterfactual questions. Each model's performance is averaged over five random initializations of parameters and training datasets. VACA demonstrates poor performance across all datasets, which can be attributed to the limitations of the encoder-decoder architectures and the constraints of the graph neural network. CausalNF-NSF and CausalNF-MAF show competitive results compared to ours. However, due to the affine constraints of the Masked Autoregressive Flow, CausalNF-MAF only learns an affine transformation between exogenous and endogenous distributions, performing well in cases with additive structural equations but failing in non-additive cases where the endogenous distribution is not Gaussian. CausalNF-NSF closely approximates observational and interventional distributions in both additive and non-additive cases but struggles with answering counterfactual questions. Our methods exhibit a significantly higher and more consistent performance compared to existing methods across the majority of queries, over both additive and non-additive relationships. Specifically, P-CFM, despite approximating endogenous values x, demonstrates a superior performance relative to previous methods.\nScalability Fig. 3 demonstrates the scalability of P-CFM against previous Diffusion-based methods such as DCM, and our own S-CFM, all using the same number of parameters. DCM's inference time grows linearly with the number of nodes d. S-CFM significantly reduces counterfactual generation time through parallel abduction. In contrast, P-CFM maintains a nearly constant inference time regardless of d. This result highlights that P-CFM is not only accurate but also efficient, making it suitable for larger problems."}, {"title": "Results on Real Dataset", "content": "We evaluate S-CFM on real-world setting on the electrical stimulation interventional fMRI data (Thompson et al. 2020), experimental setup is obtained from (Khemakhem et al. 2021). We do not evaluate P-CFM since it does not show much advantages in case of only two nodes. The fMRI data includes time series from the Cingulate Gyrus (CG) and Heschl's Gyrus (HG) for 14 patients with medically refractory epilepsy. The underlying simplified causal structure is a bivariate graph with CG \u2192 HG. Our task is to predict value of HG given intervened CG.\nTable 2 reports the interventional results on fMRI datasets. S-CFM demonstrates slightly better performance compared to the alternatives. This is because the evaluation is based on the absolute error of a single intervention, which does not account for distributional information. Additionally, the causal ordering is consistent with the causal structure in the bivariate case. Nonetheless, the experiment provides a benchmark for comparing various causal inference algorithms using real datasets."}, {"title": "Conclusion", "content": "We have introduced a new scalable method of causal learning and inference using flow models: We proved that a flow-based models can learn identifiable SCMs from only observational data and causal ordering. To make the models efficient, we designed a masked autoregressive velocity network and an endogenous predictor that enables parallel inference. We empirically demonstrated that our method outperforms SOTA rivals on a wide range of SCMs over non-additive and non-linear relationships. We showed that our parallel causal flow model is scalable, maintaining a nearly constant inference time regardless of the number of variables. We validated our method on real-world datasets, including an fMRI study, showcasing its practical applicability."}, {"title": "Identifiability of TMI map", "content": "Below is the proposition for the identifiability of TMI maps from (Xi and Bloem-Reddy 2023), included herein for the purpose of comprehensiveness.\nProposition 1. Let Z = X = Rd. The nonlinear ICA model where F are TMI maps and Pz are fully supported distributions with independent components is identifiable up to invertible, component-wise transformations."}, {"title": "Picard-Lindelof theorem", "content": "The Picard-Lindelof theorem, as discussed by (Simmons 2016) states the specific conditions under which an initial value problem guarantees a unique solution. We include it here for the completeness of the theoretical foundations.\nTheorem 2. Let f (x, y) be a continuous function that satisfies a Lipschitz condition\n\n|f (x, y1) - f (x, y2)| \u2264 K|y1 - y2|\n\non a strip defined by a \u2264 x \u2264 b and \u2212\u221e < y < \u221e. If (x0, y0) is any point of the strip, then the initial value problem\n\ny' = f (x,y),\ny(x0) = y0\n\nhas one and only one solution y = y (x) on the interval a < x < b."}, {"title": "Lipschitz continuity of the velocity network", "content": "Recalling that the velocity for node i, vo (zi, x<\u03c0i,t), is modeled as follows:\n\ncj = MADE\u03b8(x<\u03c0i)\nvi = MLP([zi, c<i,t])\n\nwhere [a, b] denotes the concatenation of a and b. The MLP is a feed-forward neural network with L layers, where each layer l (1 \u2264 l < L) has finite weights Wl, which can be enforced via weight regularization during training, and uses a 1-Lipschitz continuous activation function \u03c3l (e.g., sigmoid, tanh, ReLU, ELU). According to the theoretical result in (Kim, Papamakarios, and Mnih 2021) (Corollary 2.1), the Lipschitz constant of the MLP is given by:\n\nLip (MLP) = Lip (WL \u25e6\u03c3L\u22121\u25e6 WL\u22121 \u25e6 ... \u25e6 \u03c31\u25e6 W1)\n\n\u2264 \u220fLl=1 Lip (Wl)\nwhere Lip (f) denotes the Lipschitz constant of the function f. In the case f is a matrix W \u2208 Rm\u00d7n, Lip (W) = ||W||2 := sup||a||2=1 ||Wa||2. Since the matrices W1, W2,..., WL have finite values, their Lipschitz constants are finite. This implies that the Lipschitz constant of the MLP is finite. In other words, vi is a Lipschitz continuous function of zi."}, {"title": "Algorithms", "content": "Observational/Interventional generation To generate observational/interventional samples, we first sample from the latent distribution u ~ p(u). In case of doing intervention p (x|do (xi = \u03b1)), we change the value of latent by doing xi = \u03b1, ui = Abduct (x)i. However, if we have access to observational data, we can skip the prediction part of intervention. Then, we predict the value of exogenous variables based on generated latent xint = Predict(u). See Alg. 4 for observational sampling algorithm, Alg. 5 for interventional sampling algorithm.\nCounterfactual generation The counterfactual generation process is the same with interventional generation except that instead of sample the latent from latent distribution p (u), we acquire latent from factual endogenous values u = Abduct (xf). See Alg. 6 for counterfactual sampling algorithm."}, {"title": "Data denoising", "content": "We consider another training strategy in which we try to predict x = 20 from zt and xii via objective function:\n\nL (\u03b8) = EtEx,u ||Vo (zt, x, t) \u2013 x||2 + \u03bb ||\u03b8||2 .\n\nVelocity of the ODE is approximated by the equation:\n\nvo =\n\nzt \u2212Vo (zt, x, t)\nt\n.\n\nHowever, this approximation suffers from the denominator, causing high variance where t is near 0. Empirically, we cap t between [5e-2,1]. The performance of the method nearly as good as velocity matching in observational and interventional metrics, but worse in counterfactual metric."}, {"title": "Causal inference experiments", "content": "In this section, we provide complete experimental setups and provide additional results that can be shown in the main paper"}]}