{"title": "Fine-tuning Language Models for Recipe Generation: A Comparative Analysis and Benchmark Study", "authors": ["Anneketh Vij", "Changhao Liu", "Rahul Anil Nair", "Theodore Eugene Ho", "Edward Shi", "Ayan Bhowmick"], "abstract": "This research presents an exploration and study of the recipe generation task by fine-tuning various very small language models, with a focus on developing robust evaluation metrics and comparing across different language models the open-ended task of recipe generation. This study presents extensive experiments with multiple model architectures, ranging from T5-small (Raffel et al., 2023) and SmolLM-135M (Allal et al., 2024) to Phi-2 (Research, 2023), implementing both traditional NLP metrics and custom domain-specific evaluation metrics. Our novel evaluation framework incorporates recipe-specific metrics for assessing content quality and introduces approaches to allergen substitution. The results indicate that, while larger models generally perform better on standard metrics, the relationship between model size and recipe quality is more nuanced when considering domain-specific metrics. SmolLM-360M and SmolLM-1.7B demonstrate comparable performance despite their size difference before and after fine-tuning, while fine-tuning Phi-2 shows notable limitations in recipe generation despite its larger parameter count. The comprehensive evaluation framework and allergen substitution systems provide valuable insights for future work in recipe generation and broader NLG tasks that require domain expertise and safety considerations.", "sections": [{"title": "1 Introduction", "content": "The generation of safe and high-quality recipes presents unique challenges in natural language generation. Beyond generating coherent and creative recipes, recipe generation requires high-level knowledge of culinary techniques, nutritional principles, and awareness of dietary restrictions to ensure user safety. This necessitates approaches that balance linguistic fluency with domain-specific expertise, particularly in the domain of allergen substitution.\nThis study focuses on addressing these challenges by experimenting with different model architectures for recipe generation and allergen substitution through controlled fine-tuning and comprehensive evaluation metrics. There are three primary research questions:\n1. Given the scope of this study, which models will achieve the best results after fine-tuning for recipe generation?\n2. How should the generated recipes be evaluated to ensure that they are coherent and safe for users with dietary restrictions?\n3. How should allergen substitution be implemented into large-scale models to achieve high-quality allergen-free recipes?\nTo answer these questions, this study makes the following contributions:\n\u2022 Comprehensive comparison of model architectures across scales including smaller models like GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2020), and larger models like Phi-2 (Research, 2023) and SmolLM-1.7(Allal et al., 2024)\n\u2022 Multi-dimensional evaluation framework combining novel recipe-specific evaluation metrics, traditional metrics, and LLM-based assessment.\n\u2022 Development of RAG-assisted and prompt-based approaches for allergen substitution\nOur work represents a step forward in adapting NLG systems for practical applications in the culinary domain, emphasizing safety, personalization, and quality."}, {"title": "2 Related Work", "content": "Previous works have explored various language model architectures for recipe generation. Lam et al. (2024) explored the performance of BART-based (Lewis et al., 2020) and BRIO-based (Liu et al., 2022b) across different recipe datasets, mostly in English and Vietnamese, which were then evaluated using ROUGE scores. Our work extends these previous ones by systematically comparing models across different sizes and architectures, from smaller models like SmolLM-135M to larger ones like Phi-2. This paper provides a detailed analysis on how model size impacts different aspects of recipe quality."}, {"title": "2.2 Recipe Generation Models", "content": "This paper builds on several recent advances in recipe generation and personalization. Majumder et al. (2019) proposed a personalized recipe generation model using attention mechanisms to focus on recipes previously consumed by the user. Their approach showed promising results in generating recipes that aligned with user preferences. The dataset from this paper has been used in this research with their encoder-decoder model being used as a baseline.\nChen et al. (2021) implemented a framework using constrained question answering over a large-scale knowledge graph to recommend food recipes considering users' explicit requirements and health factors. This helped recommend healthy alternatives to users, which aligned with the study's goal of providing allergen-free options and gave us the inspiration for a RAG-based system for allergen substitution."}, {"title": "2.3 Multi-modal Approaches", "content": "The FIRE system, by Chhikara et al. (2024) and Nutrify AI by Han and Chen (2024) both use a multi-modal approach, generating recipes from food images and ingredients. While it differs from this work due to us not using images, these studies also incorporate different types of input in the process of recipe generation.\nLLava-Chef, by Mohbat and Zaki (2024) and is another multi-modal approach to recipe generation, which was fine-tuned on both the cross-entropy loss and a novel loss function computed using BLEU and ROUGE scores to ensure that the model generated recipes that were closer to the ground truth.\nThis paper adopted these evaluation metrics and the idea of creating custom ones from this paper, as well as what inputs to include for recipe generation. However, this work doesn't use the novel loss function to fine-tune the language models, since penalizing generations for not being closer to the ground truth might hinder the personalization of the generated recipes, which is an important part of allergen substitution.\nOther multi-modal recipe generation approaches include ChefFusion by Li et al. (2024) and Inverse Cooking by Salvador et al. (2019). ChefFusion provides complete multimodality by developing a framework for both recipe generation using images and image generation using recipes. This paper, along with LLava-Chef, uses metrics like Sacre-BLEU(Post, 2018) and ROUGE, which wasn't preferred due to the limitations of these metrics for creative generation. Inverse Cooking, which uses encoder-decoder transformer (Vaswani, 2017) blocks to generate recipes from images, provided the inspiration to use Ingredient Coverage as an evaluation metric, which is similar to how the paper evaluates the ingredients extracted from the image and the ground truth."}, {"title": "2.4 Evaluation", "content": "Many of these studies, such as LLava-Chef or Retrieval Augmented Recipe Generation by (Liu et al., 2024), use conventional metrics such as BLEU, ROUGE, and F1-score for ingredient matching to assess recipe quality. This paper distinguishes itself by employing both general and domain-specific metrics such as ingredient coverage, which was used by both Liu et al. (2022a) and Salvador et al. (2019) to attain a more profound understanding of the quality implications across many aspects of the generated recipe since traditional metrics focus more on overlap and thus hinder creativity in generation."}, {"title": "3 Approach", "content": null}, {"title": "3.1 Food.com Dataset", "content": "The Food.com dataset (Majumder et al., 2019) contains more than 180,000 recipes and 700,000 recipe reviews across 18 years. Each entry includes the recipe name, the list of ingredients, the cooking instructions, nutritional information, and user ratings and reviews. This study used the RAW_recipes dataset from the Food.com dataset for research. The data preprocessing pipeline consisted of the following steps:\n\u2022 Extraction of recipe names, ingredients lists, and cooking instructions\n\u2022 Standardization of ingredient formats and measurements\n\u2022 Tokenization and formatting of recipe names, standardized ingredients and instructions\n\u2022 Creation of input-output pairs for model training\nThe format of the input is as follows:\n<|startoftext|>[Recipe Name]\nIngredients: [Ingredients List]\nThe cooking instructions were used as the target output for the models."}, {"title": "3.2 Exploratory Data Analysis", "content": "A statistical analysis of the entire dataset was conducted to gain insights into the distribution of ingredients and recipe length.\nThe distribution of ingredient occurrences is dominated by a few common ingredients such as salt, butter, sugar, etc. When considering the set of unique ingredients, 9.66% were included in 90% of the recipes, while the remaining 91.44% were only included in 10% of the recipes.\nThe tokenized length of recipes was also measured. 99. 4% of the recipes had a tokenized length of less than 512 tokens and 90.4% had less than 256. These statistics were used to determine the size of the context when training the models. Additional analysis can be found in Appendix A."}, {"title": "3.3 Fine-Tuning Small Scale Models", "content": "From our dataset, we randomly sampled 100,000 recipes. This was then split into training (80%), validation (10%), and test (10%) sets. When evaluating the generated recipes, the first 500 samples from the test set are used to ensure consistency across different model evaluations. We initially implemented a custom encoder-decoder model with attention, inspired by the architecture described in Bahdanau et al. (2016). The model consisted of an embedding layer, a bidirectional GRU encoder, a GRU decoder with attention mechanism, and a final linear layer for output generation. However, this model produced near-zero scores on our evaluation metrics, indicating significant challenges in learning the complex patterns required for recipe generation. Following the challenges with the custom model, we turned to pre-trained language models, such as SmolLM (Allal et al., 2024) (135 M), GPT-2(small and medium variants)(Radford et al., 2019), and encoder-decoder language models like T5-small (Raffel et al., 2023) to explore the impact of model size and architecture on recipe generation. These models were fine-tuned on the recipe dataset, using the following approach:\n\u2022 Input: Combined recipe name and ingredients\n\u2022 Output: Cooking instructions\nTraining configurations for the small-scale models are listed in Appendix J and a sample output for these small-scale models is given in Appendix B."}, {"title": "3.4 Fine-Tuning Larger Models", "content": "From the evaluation metrics of the small-scale model generations, as seen in Table 1, we decided to scale up the size of our dataset to now include the entire dataset and turned towards large-scale models such as SmolLM-360M(Allal et al., 2024), SmolLM-1.7B and Phi-2 instead. We achieved this with our limited computational resources by using the QLORA approach and setting the rank to 8. The entire data set, consisting of 231637 recipes, was split into training (80%), validation (10%), and test (10%) sets. As before, the first 500 samples of the test set were used for evaluation to ensure consistency between the evaluation results for the different model generations. Also, generation evaluation was now performed for both baseline and fine-tuned versions to better understand the impact fine-tuning had on the generated recipes. The training configurations of these large-scale models are listed in Appendix K, and all models were trained on 1 epoch on these configurations for 8 hours on 2 NVIDIA A100 GPUs."}, {"title": "3.5 Allergen Substitution", "content": "Allergen substitution was performed when generating recipes using the following two approaches:"}, {"title": "3.5.1 Prompt based Allergen Substitution", "content": "Since we had fine-tuned three large-scale models on the entire data set, we hypothesized that these models should be powerful enough to substitute the allergens present in the generated recipe just by prompting the model. This was done by adding a list of allergens to avoid in the prompt along with the recipe name and the ingredient list. In order to test this approach, some common allergens, such as milk, eggs, and fish, are added to a list of allergens to avoid in the prompt. The prompt is given as follows:\n\"You are an expert chef and recipe writer with a deep understanding of culinary techniques and food allergies. Your goal is to create a detailed and high quality recipe that uses the provided list of ingredients, while making substitutions for any allergens to ensure the recipe is safe for individuals with those allergies. Please follow these instructions:\n1. Create a Recipe: Write a full, detailed recipe based on the name and ingredients provided.\n2. Substitute Allergens: Some people are allergic to certain ingredients. You must avoid these allergens in the recipe and suggest substitutions from the list of safe ingredients. If the allergen is an essential part of the recipe, ensure the substitute maintains the flavor and texture as much as possible.\n3. Ensure Clarity and Detail: Provide precise instructions, including cooking methods, preparation steps, and any necessary tips. The recipe should be easy to follow for someone with basic cooking knowledge.\nCreate a recipe for: name\nUsing these ingredients: ingredients\nSubstitute these allergens for other ingredients: allergens\nRecipe:\"\nA sample output for these models with and without allergen substitution is given in Appendix D. The hyperparameters for the prompt-based model is given in Appendix L."}, {"title": "3.5.2 RAG-assisted Allergen Substitution System", "content": "The second approach was to develop an experimental RAG-assisted allergen substitution system (Lewis et al., 2021) to replace allergens in the generated recipes with similar ingredients as mentioned in an allergen database that we built. Key components include:\n\u2022 FAISS vector store for efficient similarity search\n\u2022 HuggingFace embeddings (sentence-transformers/all-MiniLM-L6-v2)\n\u2022 Custom allergen database with substitution rules\n\u2022 Ingredient parsing and validation system\nImplementation details:\n\u2022 Chunk size: 1000 tokens\n\u2022 Chunk overlap: 200 tokens\n\u2022 Top-k retrieval: k=1 for substitution matches\nA workflow for the RAG-assisted system can be found in Appendix C, and the hyperparamters can be found in Appendix M. The system finds the ingredients present in the generated recipe and, if they are present in the allergen database, substitutes them with an appropriate ingredient from the database. This allergen ingredient database can be seen in Appendix F. A sample output for these models with and without allergen substitution is given in Appendix G."}, {"title": "4 Evaluation Metrics", "content": "A comprehensive evaluation framework has been implemented to evaluate the recipes generated by these models. The metrics can be divided into three parts."}, {"title": "4.1 Traditional NLP Metrics", "content": "This work uses traditional NLP metrics to evaluate the quality of the generated recipes.\n1. BLEU (Bilingual Evaluation Understudy) (Papineni et al., 2002) is a metric that evaluates the generated text by comparing it with the ground truth. It compares the n-grams between the generated recipe and the ground truth recipe, assigning a score between 0 and 1.\n2. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin, 2004), is a metric that evaluates the generated recipe by comparing the overlap between the generated recipe and the ground truth. This study will use ROUGE 1, ROUGE 2 and ROUGE L for evaluation.\n3. Perplexity is another traditional NLP metric that is used to measure the quality of the generated text. It is calculated as the exponentiated average negative log-likelihood of a sequence."}, {"title": "4.2 Recipe Specific Auto Evaluation Metrics", "content": "The traditional metrics above are good for measuring overlap with the ground truth. However, they do not work well for evaluating a creative task such as generating recipes. A high quality generated recipe could be given a low score because it does not have much overlap with the ground truth. Therefore, we have implemented custom auto-evaluation metrics which are tailored to evaluate the generated recipes in various subdomains.\n1. Ingredient Coverage Tracking: Measures how effectively the generated recipe utilizes the input ingredients. It tokenizes the ingredient list, matches the ingredients in the generated instructions, and then calculates the coverage ratio, which is the number of present ingredients divided by the total number of ingredients. The metric can handle several variations and forms.\n2. Step Complexity: Evaluates instruction completeness and detail. This is done by counting the distinct operations, analyzing the step length and detail, evaluating the parameter specifications, and then calculating the complexity score.\n3. Recipe Coherence: Assesses the logical flow and structure of the recipe. This is done by building a step dependency graph, verifying the logical ordering, checking the temporal consistency, and finally calculating the coherence score.\n4. Temperature/Time Specification Checks:- Verifies critical cooking parameters by extracting the numerical values of temperature and time in the generated recipe, validating the ranges per method, checking the completeness, and then calculating the final score.\nAll of these metrics result in scores between 0 and 1, where the higher the score, the better. A more detailed explanation of these metrics can be found in Appendix E."}, {"title": "4.3 LLM-As-A-Judge", "content": "This work also uses the LLM-as-a-judge method to evaluate the recipes generated by the baseline and fine-tuned versions of the models. Initially, we used Qwen2.5-1.5B Instruct (Yang et al., 2024) (Team, 2024), but shifted to a much larger model in Qwen2.5-7B (Team, 2024) for more accurate scores when judging the quality of the generated recipes. The recipes are evaluated using six Likert scale categories and are scored on a scale of 1-5. These categories are as follows:\n1. Clarity: Instruction comprehensibility\n2. Completeness: Coverage of necessary steps\n3. Consistency: Logical flow and coherence\n4. Practicality: Feasibility of execution\n5. Relevance: Alignment with recipe goals\n6. Allergen Safety: Checks if allergen is substituted correctly"}, {"title": "5 Results", "content": null}, {"title": "5.1 Initial Results with Small-Scale Models", "content": "Table 1 presents the initial results, comparing the recipes generated with small-scale models, using BLEU and ROUGE metrics."}, {"title": "5.2 Results with Large-Scale Models", "content": "Table 2 and Table 3 contain the evaluation scores of the baseline and fine-tuned versions of the large-scale models for both traditional NLP metrics and domain-specific auto-evaluation metrics. As mentioned above, the models have low BLEU and ROUGE scores due to there not being much overlap with the ground truth, hence the use of the domain-specific evaluation metrics."}, {"title": "5.3 Results of Prompt-based Allergy Substitution", "content": "Table 4 contains the domain-specific auto-evaluation metrics of the baseline and fine-tuned versions of the large-scale models using prompt-based allergy substitution. Table 5 shows the results of the evaluation using Qwen2.5-7B as a judge for the allergen-substituted recipes generated by the baseline and fine-tuned versions of the models. Evaluation is performed on the first 500 samples of the test set. The radar charts of these results are given in Appendix H."}, {"title": "5.4 Results of RAG-Assisted Allergy Substitution", "content": "Table 6 contains the domain-specific auto-evaluation metrics of the baseline and fine-tuned versions of the large-scale models with RAG-assisted allergy substitution. Table 7 contains the results of the evaluation conducted by Qwen2.5-7B as a judge. As before, evaluation for the LLM-as-a-judge is performed on the first 500 samples of the test set. The radar charts of these Qwen2.5-7B results are given in Appendix I."}, {"title": "6 Discussion", "content": "Our comprehensive evaluation across model architectures and scales reveals several profound insights about the intersection of recipe generation and allergen awareness, challenging conventional assumptions about model scaling and domain adaptation."}, {"title": "1. Comparison between Recipe Generation and Allergen Substitution Generation:", "content": "In the domain-specific metrics, the recipes generated by the large-scale models had higher step complexity and ingredient coverage compared to the recipes generated by the prompt-based and RAG-assisted methods. There were improvements, albeit marginal ones, in overall recipe coherence in the allergen-substituted generations versus the normal generations, signifying a comparatively smaller trade-off between step complexity and other metrics. This decrease in performance for the prompt-based method is most likely due to changes in the prompt, i.e., asking the model to substitute allergens, overwhelming the model and preventing it from generating high-quality recipes. For the RAG-assisted method, the change in hyperparameters, where the top p-value was lowered to allow the substitution of ingredients, inadvertently resulted in lower-quality recipes."}, {"title": "2. Fine-tuning Dynamics:", "content": "The most interesting findings come from the fine-tuned models. For instance, despite its sophisticated architecture, Phi-2 exhibited unexpected behavior post-fine-tuning. While the baseline model achieved high scores in ingredient coverage (0.59) and temperature specification (0.329), the fine-tuned version showed significant degradation across multiple metrics. Although the fine-tuned version showed a remarkable improvement in step complexity (0.82 to 0.99), Phi-2 noticeably showed degradation in the other three metrics, suggesting that its improvement in generating recipes in a complete step-by-step manner is done by trading off semantic relations within the instructions. A similar trend was also observed in the other models to a lesser extent. This shows that conventional fine-tuning approaches may need to be revised for larger models in specialized domains."}, {"title": "3. Allergen Substitution and Evaluation Framework:", "content": "The prompt-based substitution system revealed trade-offs between safety and culinary creativity. The fine-tuned SmolLM models, both 360M and 1.7B, demonstrated promising results in allergen safety (scores of 2.57 and 2.54), although these improvements came at the cost of recipe coherence, similar to the Phi-2 models. The multi-dimensional evaluation approach revealed significant discrepancies between traditional metrics and practical applicability, as seen by Phi-2's metrics in both prompt-based and RAG-assisted allergen substitution."}, {"title": "4. Comparison between Prompt-based and RAG-assisted Allergen Substitution Systems:", "content": "For domain-specific metrics, the RAG-assisted method had higher scores in step complexity and temperature and time specification in all three models compared to the prompt-based method, with similar scores in recipe coherence and lower scores in ingredient coverage. The lower scores are most likely due to the RAG-assisted method having more ingredients to substitute. The increased scores in step complexity and temperature and time specification are most likely due to the prompt-based method struggling to generate a step-by-step recipe when allergens are present in the recipe, whereas the RAG-assisted approach only needs to substitute allergens in the generated recipe. We also find that for the LLM-as-a-judge metric, the prompt-based method outperforms the RAG-assisted method across all models and metrics. This shows that allergen substitutions alone will not produce high-quality recipes, hence the lower scores."}, {"title": "7 Future Work", "content": "Based on the findings in this paper, we identify several promising directions to advance recipe generation with allergen awareness.\n1. The performance degradation observed in larger models during fine-tuning calls for more sophisticated adaptation approaches. Future work should explore constitutional fine-tuning techniques that better preserve model capabilities while adapting to the culinary domain, complemented by specialized pre-training objectives incorporating culinary domain knowledge. We envision a multi-task learning framework that simultaneously optimizes for recipe quality and allergen safety.\n2. Future work should explore other datasets and consider using multiple datasets for fine-tuning, as well as focus on better evaluation metrics for evaluation of the generated recipes.\n3. The RAG-assisted allergen substitution system shows promise, but requires further development. Future research should focus on integrating comprehensive domain-specific knowledge bases for more accurate substitutions, with real-time validation mechanisms ensuring substitution safety while maintaining recipe coherence."}, {"title": "8 Conclusion", "content": "This work presents a comprehensive exploration of recipe generation and allergen substitution, demonstrating both the possibilities and challenges in developing practical AI systems for culinary applications. Our systematic evaluation across multiple model scales and architectures provides valuable insights into the relationship between model capacity and domain-specific performance. Our results highlight three key findings."}, {"title": "1. There was a comparatively lower trade-off between step complexity and other metrics", "content": "observed in the recipes generated by the allergen substitution systems compared to the normal generations by the large-scale models. The lower performance of the allergen substitution systems in the domain-specific metrics can be attributed to the allergen substitution prompt demanding too much from the model and the tweaking of hyperparameters allowing lower-quality recipes to be generated."}, {"title": "2. The challenge of maintaining recipe quality", "content": "while implementing allergen substitutions requires careful balancing, as shown by the prompt-based substitution results and validated through an LLM-based evaluation. Merely substituting the allergen for a different ingredient, as shown in the RAG-assisted method, is not sufficient to solve this problem."}, {"title": "3. The multi-dimensional evaluation framework", "content": "reveals that traditional NLP metrics alone are insufficient for assessing recipe generation quality, emphasizing the need for domain-specific metrics. The performance degradation, particularly in fine-tuning larger models and implementing reliable allergen substitutions, should be the main focus for future developments in recipe generation systems.\nUltimately, this work contributes to the broader field of natural language generation by demonstrating that successful recipe generation systems must balance multiple objectives: linguistic coherence, culinary accuracy, and safety considerations. These insights extend beyond recipe generation to inform the development of other domain-specific LM's where safety and expertise are paramount."}, {"title": "9 Limitations", "content": "1. Computation resources: This study examines comparatively smaller models trained for 1-2 epochs. Future research can extend this work by exploring larger models and training for more epochs to enhance performance and robustness.\n2. Using LLM-As-A-Judge for Evaluation: LLM-as-a-judge is quite stochastic and computationally expensive in terms of generating scores for each recipe. Future research should focus on improving the trustworthiness of LLM-based evaluation and the efficient calculation of scores.\n3. Evaluation on only a part of test set: Evaluation was only performed on 500 of the generated recipes from the test set. Future research could expand the sample size to improve statistical significance and generalizability.\n4. Language of the dataset used: The dataset used for this research is predominantly in English, as are the generated recipes. Future research can focus on expanding to incorporate datasets in multiple languages for recipe generation."}]}