{"title": "Skeleton-Guided-Translation: A Benchmarking Framework for Code Repository Translation with Fine-Grained Quality Evaluation", "authors": ["Xing Zhang", "Jiaheng Wen", "Fangkai Yang", "Pu Zhao", "Yu Kang", "Junhao Wang", "Maoquan Wang", "Yufan Huang", "Elsie Nallipogu", "Qingwei Lin", "Yingnong Dang", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "abstract": "The advancement of large language models has intensified the need to modernize enterprise applications and migrate legacy systems to secure, versatile languages. However, existing code translation benchmarks primarily focus on individual functions, overlooking the complexities involved in translating entire repositories, such as maintaining inter-module coherence and managing dependencies. While some recent repository-level translation benchmarks attempt to address these challenges, they still face limitations, including poor maintainability and overly coarse evaluation granularity, which make them less developer-friendly.\n\nWe introduce Skeleton-Guided-Translation, a framework for repository-level Java to C# code translation with fine-grained quality evaluation. It uses a two-step process: first translating the repository's structural \u201cskeletons,\u201d then translating the full repository guided by these skeletons. Building on this, we present TRANSREPO-BENCH, a benchmark of high-quality open-source Java repositories and their corresponding C# skeletons, including matching unit tests and build configurations. Our unit tests are fixed and can be applied across multiple or incremental translations without manual adjustments, enhancing automation and scalability in evaluations. Additionally, we develop fine-grained evaluation metrics that assess translation quality at the individual test case level, addressing traditional binary metrics' inability to distinguish when build failures cause all tests to fail. Evaluations using TRANSREPO-BENCH highlight key challenges and advance more accurate repository-level code translation.", "sections": [{"title": "1 Introduction", "content": "Large language models are transforming software development, driving the need for enterprises to modernize systems and migrate legacy code to cloud-friendly languages. For example, migrating from C to Rust offers enhanced safety benefits (Matsakis and Klock, 2014), and libraries like TensorFlow require synchronized updates across languages. However, existing code translation benchmarks fall short in addressing real-world complexities. Most focus on function-level tasks or competition-style problems (Yan et al., 2023; Lu et al., 2021; Khan et al., 2024), which, while foundational, fail to capture the challenges of translating entire repositories. Repository-level translation is critical for managing interconnected components, dependencies, and structural integrity (Jiao et al., 2023), making reliable benchmarks essential to evaluate model performance in these scenarios.\n\nA key challenge in building a repository-level code translation benchmark is the absence of a systematic framework. For example, when updating part of a large Java-based SDK with new features, developers often cannot incrementally apply those changes to a corresponding C++ codebase without re-translating major portions. This lack of fine-grained control hinders maintainability, as small updates become disproportionately costly. A robust framework is required to accommodate partial updates and reduce overhead, ensuring that evolving codebases can be maintained efficiently across multiple languages without constant full-scale re-translation."}, {"title": "2 Motivating Example", "content": "In this section, we use an example to illustrate the challenges involved in building a repository-level code translation benchmark and explain our solutions more effectively."}, {"title": "2.1 Challenges in Repository Translation and Quality Evaluation", "content": "Lack of a Systematic Translation Framework. The partial Java-to-C# translation shown in Figure 2 highlights the challenges posed by incremental updates when using LLMs for translation and underscores the necessity of a systematic framework. In the Java snippet in Figure 2(a), the FrameBuffer class correctly calculates indices and renders pixels. However, if a developer adds a new method to the FrameBuffer class, the absence of a structured translation framework would require re-translating significant portions of the code whenever the original Java module changes-drastically reducing maintainability.\n\nLack of Parallel Corpora. Repository-level translations face challenges due to misaligned source and target files, making it hard to verify correctness across languages. For example, translating Java code (Figure 2(a)) to C# is difficult without corresponding C# tests, as there's no direct way to compare behaviors or outcomes, especially for complex logic or edge cases. One solution is translating high-coverage Java tests into equivalent C# tests, but this raises another issue: how to efficiently verify that the translated tests maintain the original intent, coverage, and reliability. Without this, testing inconsistencies could undermine confidence in the translation.\n\nLack of a Fine-Grained Evaluation Metric. An overreliance on coarse metrics (e.g., whether a repository \"builds\u201d at all) limits developers' ability to identify issues in translated code. For example, if the Draw method is translated incorrectly by calling getIndex instead of GetIndex, the code fails to compile, making it impossible to evaluate the translation quality of other functions. However, methods like GetPixels and GetIndex might have been translated correctly. This binary pass/-fail approach obscures partial successes and forces developers to manually search for issues. More granular metrics\u2014such as module-level correctness or individual function fidelity\u2014would help developers locate problematic sections more efficiently, streamlining debugging and refinement."}, {"title": "2.2 Solution: Standardizing Code Repository Translation with Partial Evaluation", "content": "Figure 3 illustrates our solution overcoming the above challenges. To ensure consistency between the translation process and the testing of the target repository, as well as to achieve fine-grained evaluation of individual unit tests, we propose introducing a \"skeleton\" of the target repository during the translation process. This skeleton serves to guide large language models (LLMs) in focusing on the accuracy of dependencies and interface translations within the code. Additionally, the skeleton can be incrementally filled with partial translation results, enabling fine-grained execution-based evaluation of translation quality.\n\nFacilitating Maintainability. Figure 2(b) demonstrates our framework, Skeleton-Guided-Translation, where the C# code serves as a \"target repository skeleton\". Unlike the fully translated Java code in Figure 2(a), this skeleton establishes consistent interfaces while leaving method bodies mostly empty or trivial. This systematic methodology underpins maintainability: whenever the Java repository evolves, only the corresponding skeleton sections in C# require incremental updates, avoiding the need to overhaul the entire translation."}, {"title": "3 TRANSREPO-BENCH Benchmark", "content": "In our benchmark, the source repository and target repository skeleton are provided for users, aiming for LLMs to generate a complete target repository. The correctness of the generated repository is verified using the target repository's unit tests within the designated testing environment. In this section, we begin by presenting the benchmark content, followed by detailing the construction process of TRANSREPO-BENCH, and conclude with an introduction to our fine-grained evaluation design."}, {"title": "3.1 Benchmark Overview", "content": "Each code translation task in TRANSREPO-BENCH consists of a source repository and its evaluation setup, structured as <source repository, target repository skeleton, target repository unit tests, testing environment>. Currently, we use Java to C# translation as an example, with plans to extend the benchmark to include additional language pairs in the future.\n\nAs illustrated in Figure 2, the input for the translation task comprises source repositories, written in Java, serving as the code bases to be translated, and a target repository skeleton, which acts as a crucial contract for effective evaluation, supporting both the translation and evaluation phases. It is a highly simplified version of the target repository, where all functions are replaced with trivial implementations (e.g., a single return statement). This skeleton preserves the original repository's file structure, dependencies, and static values while ensuring it compiles successfully. The evaluation setup comprises the unit tests for the target repository and the necessary testing configuration files to execute those tests.\n\nTRANSREPO-BENCH comprises 13 tasks for translating code repositories. Detailed information is provided in Figure 4. It outlines key features for each repository, such as the number of classes,"}, {"title": "3.2 Benchmark Construction", "content": "This section provides a detailed explanation of the benchmark construction process, as summarized in Fig. 5. We begin by describing the collection of source repository datasets (\u00a73.2.1). Next, we outline the method for extracting the skeletons of source repositories and translating them into the target language (\u00a73.2.2). We then detail the process of obtaining unit tests for the target repositories (\u00a73.2.3). Finally, we describe the preparation of testing environments for each repository (\u00a73.2.4)."}, {"title": "3.2.1 Source Repository Collection", "content": "The dataset for the source repository is curated from open-source GitHub projects, following these selection criteria: (1) repositories must have over 100 stars; (2) they must include a testing workflow; and (3) their testing workflow must execute successfully and pass when run locally.\n\nGiven that previous attempts (Pan et al., 2024) to translate entire repositories often failed to even compile, we decided to start with common and mature repositories. We selected \u201cjava-design-patterns,\" a Java library that provides a comprehensive collection of design pattern implementations. This choice was made because libraries of this nature typically feature higher code quality, comprehensive testing, and successful test execution."}, {"title": "3.2.2 Skeleton Extraction and Translation", "content": "Repository skeletons are simplified versions of repositories where all function implementations in files (excluding testing files) written in the source language are replaced with trivial return statements. Files written in other languages are retained, ensuring that the skeleton can still compile and execute successfully. These skeletons preserve the original file structure, dependencies, interfaces, and static values of the source repository.\n\nSpecifically, function bodies are replaced with return statements that provide trivial values corresponding to the output types of the functions, enabling successful compilation by satisfying type-checking requirements. For example, if a function returns an int, we replace its body with \"return 0;\"; for functions returning objects, we use \"return null;\". For class constructors that have no return type, the function body is left empty. Similarly, for static blocks, only the assignments are retained, while the rest of the block is removed.\n\nAfter extracting the skeletons, we translate them into the target language using a large language model (GPT-40). However, most translated skeletons fail to compile successfully, necessitating significant manual effort to correct translation errors and restore proper functionality. As illustrated in Figure 4, the column labeled \"Skeleton Fix Time\" captures the time spent on fixing the translated target repository skeletons. This metric reflects the approximate manual effort required to produce a target repository skeleton, serving as the foundation step in the entire repository translation process."}, {"title": "3.2.3 Unit Test Translation", "content": "We translate the unit tests within source repositories into the target language using a large language model (GPT-40) and the NUnit testing framework. However, the translated unit tests often fail to compile successfully, necessitating significant manual effort to resolve translation errors and ensure that the tests correctly interact with the source code they are intended to validate."}, {"title": "3.2.4 Testing Environment Construction", "content": "Building a testing environment involves creating a setup where unit tests can be executed, typically by specifying a Docker image, installing the necessary dependencies, and running the tests. For our process, we prepare a build configuration file in YAML format for the translated C# project, based on the existing build file from the original Java project.\n\nThis step is primarily done manually, referencing the translated C# skeleton to configure the build file. Additionally, we leverage a large language model (such as GPT-40) to assist in translating the Java build file directly into a corresponding C# build file. The translated build file is then refined and corrected to ensure it functions as intended for the C# project.\n\nTo reduce the manual effort and facilitate the broader use of our framework, we have released several supporting resources: static repair scripts for skeletons and unit tests and automated configuration scripts for C# projects. These tools significantly lower the barrier to adoption and improve efficiency for researchers and developers. The capabilities of the automatic repair scripts are currently limited, which required us to invest significant manual effort during the process."}, {"title": "3.3 Fine-Grained Evaluation Metrics Design", "content": "To provide a more fine-grained evaluation of user-translated code, we leverage unit tests to score the translated output. Previous attempts to translate entire repositories often ended in compilation failures, preventing even the execution of unit tests. Pan et al. (Pan et al., 2024) claim that 77.8% of the failures in large-model translations are due to compilation errors. In their experiments on two translated repositories, tasks failed solely because of these errors, making it difficult to obtain a more detailed error analysis. This limitation potentially obscures translations that might be correct or valuable, hindering a comprehensive evaluation of the model's translation capabilities.\n\nTo mitigate the binary impact of \u201ccompile success vs. failure\u201d on our evaluation, we extract the source code relevant to individual unit tests and execute them within a guaranteed-compilable skeleton. During evaluation, the translated functions related to a specific unit test are copied into this skeleton, and the dotnet build and dotnet test commands are executed. This method ensures fine-grained scoring, free from the broader impact of translation errors in unrelated parts of the repository. We provide dynamic instrumentation scripts for extracting relevant code from unit tests within the evaluation system.\n\nOur evaluation employs two primary metrics: build success rate and unit test success rate. The build success rate is calculated as the proportion of unit tests that compile successfully out of all unit tests for each library. Similarly, the unit test success rate measures the proportion of unit tests that pass successfully out of those that compile. For an overall assessment of a model's performance, we compute the average of these scores across all evaluated libraries. This approach allows unit tests to execute successfully while isolating evaluation from compilation errors in unrelated parts of the translated repository. The key challenge lies in extracting the source code relevant to each unit test. To address this, we instrument the Java source code at the function level to identify the code invoked by each unit test during execution. Using the structural mapping between the source Java and target C# repositories, which share the same file and class structure, we then locate the corresponding C# source code required to execute the unit tests."}, {"title": "4 Evaluation", "content": "We first analyze how LLMs perform on our benchmark. Then, we highlight the effectiveness of our novel framework, which incorporates target repository skeletons for translation and fine-grained evaluation."}, {"title": "4.1 Model Performance on TRANSREPO-BENCH", "content": "We evaluate the performance of state-of-the-art LLMs on the task of translating code repositories from Java to C#. Next, we conduct a failure analysis based on the experimental results."}, {"title": "4.1.1 Model Selection", "content": "To evaluate the performance of state-of-the-art large language models (LLMs) on the code repository translation task, we selected six representative models: GPT-40, GPT-40-mini, GPT-4-turbo, Qwen-plus-1220, Claude-3.5-sonnet-20240620, and Deepseek-v3. GPT-40, GPT-40-mini, and GPT-4-turbo represent versatile general-purpose LLMs with strong capabilities in reasoning and language understanding, optimized for different levels of computational efficiency and application contexts. Qwen-plus-1220 and Claude-3.5-sonnet-20240620 are advanced models that bridge general-purpose tasks and specialized reasoning, offering nuanced language comprehension for complex scenarios. Deepseek-v3, on the other hand, is fine-tuned specifically for code-related tasks, focusing on programming language understanding and transformation."}, {"title": "4.1.2 LLMs Performance", "content": "Figure 6 presents the overall performance evaluation of various large language models (LLMs) across three iterations, focusing on two metrics: Build Rate and Unit Test Pass Rate. Notably, DeepSeek-v3 demonstrates a consistent improvement across iterations, achieving the highest Build Rate (71.14%) and a competitive Unit Test Pass Rate (17.56%) by the third iteration, showcasing its robustness and optimization. GPT-4-turbo starts strong with a Build Rate of 60.54% in Iteration 1 but drops significantly to 50.00% by Iteration 3, with its Unit Test Pass Rate also decreasing to 11.25%. GPT-40 maintains a steady performance, with its Build Rate stabilizing at 57.34% and a slight fluctuation in Unit Test Pass Rate, ending at 16.03%. Models like GPT-40-mini and Claude-3.5 exhibit weaker performance, with declining Build Rates and inconsistent trends in Unit Test Pass Rates. Overall, DeepSeek-v3 stands out as the most effective model, while others face challenges in sustaining performance.\n\nThe table results demonstrate that iterative refinement does not always lead to improved performance. One possible reason is the error propagation effect. In these scenarios, the outputs from previous iterations serve as the inputs for subsequent ones. If errors or inefficiencies are introduced in earlier stages, they may accumulate or intensify instead of being corrected. This is especially problematic if the models struggle to differentiate between constructive feedback and irrelevant noise during refinement.\n\nBuild Rates. From the build success rates in Figure 7, DeepSeek-v3 exhibits the highest overall performance at 71.14%, followed by GPT-4-turbo at 66.31% and Qwen-plus at 65.70%. Meanwhile, GPT-40, GPT-40-mini, and Claude-3.5 show slightly lower aggregate rates, at 65.03%, 57.00%, and 63.23% respectively. Despite these average trends, there are notable variations across individual repositories. For instance, decorator and producer-consumer pose challenges for most models (with many yielding a 0% build rate), whereas repositories such as converter, partial-response, and unit-of-work reach 100% build success for multiple models. These discrepancies suggest that certain repository structures and coding patterns can significantly influence the success or failure of automatic translation.\n\nUnit Test Pass Rates. Figure 7 highlights the unit test pass rates (%) across various repositories for different large language models. In terms of unit test pass rates, DeepSeek-v3 again leads with an average of 22.32%, slightly outperforming the next-best models: GPT-40 at 21.50% and Claude-"}, {"title": "4.1.3 Failure Analysis", "content": "Overview. Figure 8 illustrates the distribution and reduction of error types across three iterations, demonstrating the effectiveness of the iterative refinement process. The most frequent error category, Runtime Errors, decreased significantly from 439 in Iteration 1 to 428 in Iteration 3, highlighting its prominence and the consistent efforts to address it. Other common error types include CS0246 (missing type or namespace), CS1061 (missing member in an object), and CS0103 (undefined variable or name), all of which exhibit similar trends of gradual reduction, indicating that the model effectively identifies and rectifies these errors over iterations. For instance, CS0106 errors dropped from 23 to 16, while CS1061 reduced from 23 to 17, respectively. The inconsistent decrease in CS0103 and CS0246 errors may result from the iterative process introducing new variables or dependencies without properly defining or importing them.\n\nAdditionally, less frequent error categories such as CS0106 (modifier errors in method declarations) and \"Others\" demonstrate modest but steady reductions, signifying the model's capability to address a broader range of issues. The overall decline in total errors from 747 in Iteration 1 to 619 in Iteration 3 suggests that the model particularly excels at resolving syntactical and common logical errors, which tend to follow recognizable patterns. This result supports the claim that iterations are effective in error mitigation and highlights the strengths of large models in correcting repetitive, rule-based error types.\n\nCommon Failure Patterns. We explore the most common failure patterns encountered during large model-based code translation, focusing on their underlying causes, how they manifest in practice, and the strategies needed to address them. By analyzing these recurring issues, we aim to provide actionable insights for improving the accuracy and reliability of cross-language code conversion processes.\n\nStatic Variable Misalignment. A frequent issue encountered during translation is the inconsistent handling of static variable naming conventions. For example:\n\npublic void Stop ()\n{\n}\nstatus = GameStatus.Stopped;\n\nThe corresponding C# code raised an error (CS0117) because the enum member Stopped was incorrectly translated. In C#, enum members are often defined using uppercase conventions, such as STOPPED. This discrepancy arises because Java typically uses mixed-case identifiers, leading to capitalization errors during translation. To avoid such issues, translators should implement specific mappings for capitalization-sensitive identifiers between languages.\n\nNamespace and Duplicate Definitions. Another common error (CS0101) occurs when namespaces contain duplicate definitions due to repetitive code generation. Consider the following Java snippet:\n\npublic class Candy\n{\n}\npublic Candy (string flavor) { }\n\nIf the translator generates multiple constructors with identical signatures for this class in C#, the compiler will flag a conflict, as C# enforces unique member definitions within a namespace or class. The solution involves ensuring that constructors or methods with overlapping signatures are merged or disambiguated during translation.\n\nUnresolved Names and Contextual Misinterpretations. Translation errors often stem from missing imports or incorrect mappings of contextual elements, leading to errors like CS0103 (\u201cThe name"}, {"title": "4.2 TRANSREPO-BENCH Effectiveness", "content": "To demonstrate the effectiveness of our method, we conducted two comparative experiments on GPT-40. These experiments aim to validate (1)"}, {"title": "4.2.1 Validating the Fineness of Our Evaluation Mechanism", "content": "The first experiment demonstrates that our evaluation mechanism provides a more fine-grained and comprehensive evaluation of a library's translation quality. Unlike RepoTransBench (Wang et al., 2024), which evaluates the entire translated project by directly building and testing it without skeletons, our method scores individual components. This avoids the issue of a single error overshadowing other correct translations.\n\nAs shown in Figure 9, RepoTransBench (Wang et al., 2024) achieves a score of 0 for most tasks, with only two out of thirteen tasks being successfully evaluated. In contrast, our method can assign scores to each segment of a project, even in cases where overall compilation fails. For instance, our approach successfully compiles and scores all unit test-related code segments, achieving a 100% success rate for these cases. This highlights the advantage of our fine-grained evaluation, which ensures partial successes are recognized rather than completely dismissing the translation due to isolated failures."}, {"title": "4.2.2 Proving the Necessity of Skeletons in Translation", "content": "The second experiment aims to validate the necessity of providing skeletons of the target repository during the translation process. As shown in Figure 10, omitting these skeletons substantially degrades both the build success rate and the unit test"}, {"title": "Figure 11: Experiments without Skeletons", "content": "pass rate across three iterations of our translation pipeline.\n\nThe reason for the degradation of the score is caused by the lack of the target repository skeleton in the translation process for the inter-file dependencies and interfaces set in advance, resulting in the failure to find the function under test during the test. As illustrated in Figure 11, omitting the skeletons results in many dependencies being unresolved during the translation process. This failure renders our scoring mechanism ineffective because unresolved dependencies cause all build and test scores to drop to zero.\n\nFor example, the absence of skeletons leads to dependencies being completely unresolvable for certain libraries, as indicated by the high proportion of tests that cannot locate dependencies in the bar chart. This demonstrates that skeletons are essential in the translation process, ensuring that dependencies are correctly identified and enabling the successful evaluation of the translated code.\n\nSummary. These experiments collectively establish that our method is superior in two key aspects:\n\u2022 Our evaluation mechanism is more granular and comprehensive, capturing the quality of translation even when partial failures occur.\n\u2022 Providing skeletons during translation is crucial to ensure dependency resolution and enable accurate evaluation."}, {"title": "5 Related Work", "content": ""}, {"title": "5.1 Code Translation", "content": "Code translation (source-to-source translation) converts code from one language to another while preserving semantics. Traditional rule-based compilers and intermediate representations (e.g., Babel, Roslyn) work well for constrained cases but falter with complex constructs. Recent AI-driven approaches use neural networks, including sequence-to-sequence models (Luong et al., 2016), transformers (Vaswani et al., 2017), and pre-trained models like CodeBERT (Feng et al., 2020), CodeT5 (Wang et al., 2021), to improve translation by capturing structural nuances.\n\nBhattarai et al. (Bhattarai et al., 2024) introduced a few-shot, retrieval-based technique for guiding LLMs in code translation, while Tao et al. (Tao et al., 2024) utilized an intermediary language (Go) to facilitate translations. Unsupervised cross-lingual code representations have also emerged, exemplified by TransCoder (Roziere et al., 2020), which handles translations without parallel datasets.\n\nAlphaTrans (Ibrahimzada et al., 2024) is a neuro-symbolic framework for repository-level code translation, employing program analysis and dynamic testing for validation. Shiraishi et al. (Shiraishi and Shinagawa, 2024) proposed a context-aware C-to-Rust translator, enhancing large-scale compilation success via code segmentation and context-supplementing prompts. Oxidizer (Zhang et al., 2024) likewise addresses repository-level translation with feature mapping, type-compatibility checks, and a semantics-driven phase using unit tests to preserve functionality.\n\nDespite these contributions, AlphaTrans has notable scalability gaps: (1) it handles build errors poorly and lacks comprehensive testing; (2) it overlooks semantic alignment in test case translation, risking untranslated assertions or artificially aligned classes; (3) its fixed-rule approach struggles with special syntax (e.g., method annotations in Java). Our method mitigates these issues by focusing on build error ratios, test case alignment, and more flexible handling of complex syntax."}, {"title": "5.2 Code Translation Benchmarks", "content": "Benchmarks are crucial for evaluating the performance of code translation systems. Early benchmarks consisted of manually curated small-scale function pairs, which were limited in scale and diversity. Modern benchmarks have expanded to include large-scale datasets with a variety of programming languages, encompassing both open-source projects and synthetic code samples.\n\nAdvBench (Robey et al., 2021) is a benchmark designed for evaluating TransCoder, including programs written in Java, C++, and Python. It assesses translation fidelity using metrics such as BLEU, Exact Match (EM), and Code Execution Accuracy, focusing on real-world applicability. Similarly, CodeNet (Puri et al., 2021) is a vast dataset containing 14 million code samples across 50 programming languages, providing a comprehensive foundation for training and evaluating code translation models.\n\nBeyond general-purpose benchmarks, task-specific benchmarks address specialized domain challenges. For instance, the CodeXGLUE benchmark (Lu et al., 2021) evaluates various programming tasks, including code translation, by incorporating execution-based metrics to ensure functional correctness. However, these benchmarks often lack coverage for niche languages, complex system-level code, or real-world constraints like incomplete libraries and ambiguous syntax.\n\nRustRepoTrans (Ou et al., 2024) is the first benchmark to include repository-level dependencies for Rust code translation, addressing the limitations of function-level datasets. Evaluations using RustRepoTrans revealed a significant performance drop (41.5%-56.2%) when handling repository-level tasks, highlighting the difficulties in managing dependencies and cross-file interactions in real-world scenarios. Similarly, RepoTransBench (Wang et al., 2024) is a benchmark for repository-level code translation that features 100 repositories with automated test suites to evaluate translation quality. It tackles challenges such as complex configurations, resource file handling, and test case migration.\n\nHowever, RepoTransBench has certain limitations that our approach overcomes: (1) Lack of Skeleton Framework: RepoTransBench does not utilize skeletons, making it difficult to constrain the interfaces generated by large models. This often leads to interface misalignments during testing and restricts their use in incremental translation scenarios. In contrast, our skeleton-based approach ensures tighter control and better adaptability. (2) Absence of Test Checking: RepoTransBench lacks a robust mechanism for verifying test results. Our method ensures alignment by running unit tests on both source and target language skeletons, providing a more reliable evaluation process. (3) Coarse-Grained Evaluation: RepoTransBench executes unit tests directly without isolating dependencies, which can result in compounded translation errors affecting test outcomes. Our approach isolates relevant dependencies within the skeleton, allowing for finer-grained evaluation of translation quality and minimizing the impact of such errors."}, {"title": "6 Conclusions", "content": "This paper presents TRANSREPO-BENCH, a novel benchmark and framework addressing critical challenges in repository-level code translation, including inter-module coherence, dependency management, and fine-grained evaluation. By leveraging a two-step translation approach centered on repository skeletons, we ensure structural consistency while enabling precise and incremental translation. The proposed fine-grained evaluation mechanism, which scores translation quality at the unit test level, offers detailed feedback beyond traditional binary metrics.\n\nWe validate the effectiveness of skeleton-based translation and fine-grained evaluation, demonstrating that incorporating repository skeletons significantly improves translation accuracy by resolving inter-file dependencies and enabling partial validation. Comprehensive experiments on state-of-the-art large language models reveal key challenges in repository-level translation, such as error propagation and runtime failures. Our failure analysis highlights common pitfalls, including static variable misalignment, unresolved dependencies, and namespace conflicts, offering actionable insights for enhancing cross-language translation systems."}, {"title": "public void Stop () {\n}\nstatus = GameStatus.Stopped;", "content": ""}, {"title": "public class Candy {\n}\npublic Candy (string flavor) { }", "content": ""}, {"title": "private int RandomInt(int min, int max) {\n}\nreturn ThreadLocalRandom. Current.\nNext(min, max + 1);", "content": ""}, {"title": "_wizards [wizard]. Set Wisdom (amount);", "content": ""}, {"title": "private void Register (Weapon weapon,\nstring operation) {\n}\nif (!_context. TryGetValue(operation,\nout var weapons To Operate))\n{\n}\nweapons To Operate = new List<\nWeapon >();\nweapons To Operate.Add(weapon);\n_context[operation] =\nweapons ToOperate;", "content": ""}]}