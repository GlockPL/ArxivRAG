{"title": "AmCLR: Unified Augmented Learning for Cross-Modal Representations", "authors": ["Ajay Jagannath", "Aayush Upadhyay", "Anant Mehta"], "abstract": "Contrastive learning has emerged as a pivotal framework for representation learning, underpinning advances in both unimodal and bimodal applications like SimCLR and CLIP. To address fundamental limitations like large batch size dependency and bimodality, methods such as SogCLR leverage stochastic optimization for the global contrastive objective. Inspired by SogCLR's efficiency and adaptability, we introduce AmCLR and xAmCLR-objectives tailored for bimodal vision-language models to further enhance the robustness of contrastive learning. AmCLR integrates diverse augmentations, including text paraphrasing and image transformations, to reinforce the alignment of contrastive representations, keeping batch size limited to a few hundred samples unlike CLIP which needs batch size of 32,768 to produce reasonable results. xAmCLR further extends this paradigm by incorporating intra-modal alignments between original and augmented modalities for richer feature learning. These advancements yield a more resilient and generalizable contrastive learning process, aimed at overcoming bottlenecks in scaling and augmentative diversity. Since we have built our framework on the existing SogCLR, we are able to demonstrate improved representation quality with fewer computational resources, establishing a foundation for scalable and robust multi-modal learning. For more details, please visit: https://github.com/AmCLR.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) has revolutionized representation learning by eliminating the need for labeled data and leveraging intrinsic data structures for pretraining deep neural networks. While SSL has shown immense success in unimodal settings [1] such as computer vision and natural language processing, its application in bimodal settings, where alignment between two distinct modalities (e.g., vision and language) is critical, has gained significant attention in recent years. The bimodal SSL paradigm is particularly impactful in applications like image-caption retrieval, visual question answering, and cross-modal content understanding.\nOne of the most prominent frameworks for bimodal SSL is CLIP [2], which uses a contrastive learning objective to align image and text representations. CLIP trains vision and language encoders simultaneously by maximizing the similarity between paired image-text representations (positive pairs) and minimizing the similarity with unpaired combinations (negative pairs). This simple yet effective approach allows CLIP to achieve state-of-the-art performance in zero-shot learning and retrieval tasks on popular benchmarks like ImageNet [3] and MS COCO [4]. However, despite its success, CLIP's reliance on extremely large batch sizes (e.g., 32,768) poses practical challenges, including high memory requirements and computational overhead."}, {"title": "2 Related Works", "content": "Self-supervised learning (SSL) has become a cornerstone for representation learning in bimodal settings, particularly in vision-and-language pretraining (VLP) [13] . These approaches utilize paired image-text data to train models that effectively align the modalities, enabling applications such as image-caption retrieval and zero-shot classification. At the heart of these methods is contrastive"}, {"title": "3 Proposed Framework for Optimizing Global Bimodal Contrastive Loss", "content": "In this work, we introduce two algorithms built on the SogCLR framework [5]. Before discussing our proposed algorithms, it is essential to first address the foundational SogCLR loss function. SogCLR addresses the challenges of optimizing the global contrastive loss by offering a memory-efficient stochastic optimization algorithm. This method enables efficient handling of large datasets without relying on large mini-batches. We chose to build upon the SogCLR loss because of these advantages. Understanding the SogCLR loss is critical, as it forms the basis for our proposed models, which aim to improve upon SogCLR's performance while maintaining its efficiency.\nThe SogCLR loss function is formulated to optimize the global contrastive loss over the entire dataset, avoiding the need for large mini-batches. The objective function can be defined as:\n$\\mathcal{F}(\\omega) = \\sum_{i=1}^{n} \\log \\frac{\\exp \\left( \\frac{F_{l}(x_{i})^{T} F_{T}(t_{i})}{\\tau} \\right)}{\\sum_{t \\in D} \\exp \\left( \\frac{F_{l}(x_{i})^{T} F_{T}(t)}{\\tau} \\right)} - \\sum_{i=1}^{n} \\log \\frac{\\exp \\left( \\frac{F_{l}(x_{i})^{T} F_{T}(t_{i})}{\\tau} \\right)}{\\sum_{x \\in D} \\exp \\left( \\frac{F_{l}(x)^{T} F_{T}(t_{i})}{\\tau} \\right)},$\nwhere $F_{l}(x_{i})$ and $F_{T}(t_{i})$ are the image and text embeddings for the sample $(x_{i}, t_{i})$, and $D$ is the entire dataset. $\\tau$ is temperature. Here, $n$ refers to the size of full dataset.\nGiven the large size of the dataset $D$, the challenge lies in computing the terms:\n$g(\\omega; x_{i}) = E_{t \\sim D} \\left[ \\exp \\left( \\frac{F_{l}(x_{i})^{T} F_{T}(t)}{\\tau} \\right) \\right]$\nand\n$g(\\omega; t_{i}) = E_{x \\sim D} \\left[ \\exp \\left( \\frac{F_{l}(x)^{T} F_{T}(t_{i})}{\\tau} \\right) \\right].$\nTo address this, we propose a stochastic gradient estimator to handle the large dataset efficiently. The estimator is given by:\n$m_{t} = \\frac{1}{B} \\sum_{i \\in B} F_{l}(x) F_{T}(t_{i}) + \\frac{\\tau}{U_{I_{t}}} \\sum_{i \\in B} \\nabla g(\\omega_{t}; x_{i}, B) + \\frac{\\tau}{U_{T_{t}}} \\nabla g(\\omega_{t}; t_{i}, B),$ \nwhere $g(\\omega; x_{i}, B)$ and $g(\\omega; t_{i}, B)$ are the mini-batch estimators of $g(\\omega; x_{i})$ and $g(\\omega; t_{i})$, respectively. The scalars $U_{I_{i, t}}$ and $U_{T_{i, t}}$ are updated for the sampled data according to:\n$U_{I_{i, t+1}} = (1 - \\gamma) U_{I_{i, t}} + \\gamma g(\\omega_{t}; x_{i}, B),$\n$U_{T_{i, t+1}} = (1 - \\gamma) U_{T_{i, t}} + \\gamma g(\\omega_{t}; t_{i}, B),$\nwhere $\\gamma$ is a hyperparameter controlling the update rate. This formulation enables the use of the full dataset for analysis, rather than relying on mini-batches, improving both the efficiency and robustness of the training process.\nFinally, the model parameters $\\omega$ are updated using an Adam-style or momentum-style update.\nThis formulation of SogCLR forms the foundation of our proposed algorithms. By building on SogCLR, we aim to further improve its performance and generalizability, particularly in the context of large-scale datasets with long-tailed distributions, which is a key challenge in vision-and-language pretraining. Our proposed enhancements retain the memory efficiency and scalability of SogCLR while offering improved performance in handling multimodal data."}, {"title": "3.1 AmCLR", "content": "In AmCLR, we introduce augmentations by randomly selecting $w$ image augmentations from a set of $P_{1}$ augmentations for each image. Let us consider the space of all possible text paraphrases for a given text as $P_{2}$. So, now $w$ augmented texts or paraphrasings are sampled from a set of $P_{2}$ augmentations for each text. From these, we create combinations of image and text pairs for various contrastive losses. The intuition behind this approach is that by generating additional augmented versions of each image-text pair in a batch, the model can learn more robust and generalized representations."}, {"title": "3.2 xAmCLR: Extension of AmCLR", "content": "In xAmCLR, in addition to AmCLR, we add extra terms for intra-modality learning, where we contrast augmented images with other images and augmented text with other texts. This approach is inspired by the unimodal SogCLR framework, and results in the following $\\kappa$ combinations for contrastive loss for each batch sampled from the dataset. The motivation behind this strategy is to enhance representation learning by incorporating unimodal losses alongside the existing multimodal losses. By considering unimodal losses between images and their augmented versions, as well as between text and their paraphrased versions, the model aims to achieve a more generalized representation learning by capturing the nuances within each modality independently."}, {"title": "4 Experiments", "content": "In this section, we compare our proposed losses AmCLR and xAmCLR to SogCLR and iSogCLR losses. To ensure fairness, we adopt the same settings for all. We evaluate the performance of different loss functions combined with various optimizers on three tasks: Retrieval (Text), Retrieval (Image), and Zero-shot classification. For each task, we report standard metrics including Top-1, Top-5, and Top-10 accuracy to provide a comprehensive comparison."}, {"title": "4.1 Setup", "content": "We utilized the NVIDIA RTX 6000 GPU nodes for distributed parallel training and evaluation of our models. The batch size was fixed at 128, with models trained over 30 epochs to ensure convergence. We chose a small batch size to leverage the improvements introduced by SogCLR, which addresses the limitations of traditional contrastive learning models like CLIP that require large batch sizes for effective training. By approximating the global contrastive loss, SogCLR allows us to maintain high accuracy even with smaller batches, thus reducing computational demands and enhancing training efficiency. Our approach aims to further improve upon SogCLR by optimizing performance with this efficient batch size. We experimented with various optimizers including AdamW, AdamP, RAdam, SGDP, NAdam, and NvNovograd to assess their impact on model performance."}, {"title": "4.2 Datasets", "content": "Training data: We used a 100k subset of the Conceptual Captions 3M dataset for training. This dataset offers a diverse collection of image-text pairs that facilitate effective learning of diverse and discriminative features, making it suitable for contrastive learning.\nValidation data: We employed a subset of the MSCOCO dataset to evaluate retrieval performance. MSCOCO's detailed annotations makes it suitable for assessing models' retrieval capabilities. We then employed a subset of the ImageNet dataset to evaluate zero-shot classification performance. ImageNet's extensive collection of annotated images across numerous categories provides a robust benchmark for testing models' ability to classify unseen categories."}, {"title": "4.3 Model Architecture", "content": "Image Encoder: We utilized a ResNet-50 model pretrained on ImageNet. ResNet-50, comprising of 50 layers and 25.6 million parameters is particularly well-suited for tasks involving complex visual feature extraction, allowing it to classify images into 1,000 different categories, providing a strong foundation for downstream vision-language tasks.\nText Encoder: We utilized DistilBERT, a distilled version of BERT, pretrained on BookCorpus (800M words) and English Wikipedia (2.5B words). DistilBERT, with 66 million parameters, is 40% smaller and 60% faster than BERT while retaining 97% of its language understanding capabilities."}, {"title": "5 Results", "content": "In our preliminary experiments, we observed that optimizers such as RAdam, NAdam, NvNovograd and SGDP performed poorly when used with existing loss functions like SogCLR and iSogCLR. This suboptimal performance can be attributed to their inability to effectively handle the stochasticity of smaller batch sizes. Consequently, we chose not to experiment with these optimizers for our new loss functions, AmCLR and xAmCLR, focusing instead on AdamW and AdamP, which showed more promising results in initial tests. We have used two versions of iSogCLR loss in our experiments, one which uses a temperature generator to dynamically adjust the temperatures for image and text features and the other which introduces a regularization term to control the loss from positive pairs. Figure's 1 and 2 show the comparison."}, {"title": "5.1 Retrieval (Text) Tasks", "content": "Table 1 presents a detailed comparison of performance metrics for text retrieval tasks. Our proposed AmCLR with AdamW achieved a Top-1 accuracy of 14.64%, significantly outperforming SogCLR with the same optimizer, which achieved 13.1%. Similarly, AmCLR with AdamP reached a Top-1 accuracy of 14.54%, surpassing iSogCLR_New's best performance of 13.14% with AdamP. In terms of broader retrieval metrics, AmCLR consistently led in Top-5 and Top-10 categories, confirming its superior generalization capabilities.\nxAmCLR also demonstrated strong performance with both optimizers, achieving a Top-1 accuracy of 14.14% with AdamW and 13.62% with AdamP, maintaining a competitive edge over other losses."}, {"title": "5.2 Retrieval (Image) Tasks", "content": "As shown in Table 2, the image retrieval tasks further highlight the advantages of our solutions. AmCLR with AdamP achieved the highest Top-1 accuracy at 11.46%, compared to iSogCLR_New's 10.13% with AdamP. This trend continued across Top-5 and Top-10 metrics, where AmCLR consistently outperformed other losses by substantial margins. Additionally, we observed that AdamP performed slightly better than AdamW across image retrieval tasks.\nxAmCLR closely followed AmCLR, achieving Top-1 accuracy at 11.14% with AdamW. Similar trends were observed for Top-5 and Top-10 metrics, with xAmCLR results within 0.3\u20130.7 percentage points of AmCLR."}, {"title": "5.3 Zero-shot Tasks", "content": "The zero-shot classification tasks presented in Table 3 reveal that AmCLR with AdamW reached a Top-1 accuracy of 25.87%, outperforming all other configurations including SogCLR's 24.28% with AdamW. The results were consistent across Top-5 and Top-10 metrics, where AmCLR demonstrated superior adaptability to unseen data distributions.\nxAmCLR also performed exceptionally well in zero-shot tasks, with results nearly matching those of AmCLR, highlighting its effectiveness in scenarios requiring high generalization.\nOverall, our proposed loss functions, AmCLR and xAmCLR, consistently outperformed existing methods across all tasks and metrics when paired with both AdamW and AdamP optimizers. This underscores their potential as robust solutions for diverse retrieval and classification challenges."}, {"title": "6 Conclusion", "content": "In this work, we analyzed the performance of various optimizers combined with existing and proposed loss functions for Retrieval (Text), Retrieval (Image), and Zero-shot Classification tasks. Our experiments revealed that optimizers such as RAdam, NAdam, NvNovograd, and SGDP exhibited suboptimal performance when paired with existing loss functions like SogCLR and iSogCLR. Based on these findings, we limited our experiments with the proposed loss functions, AmCLR and xAmCLR, to AdamW and AdamP optimizers.\nBoth AmCLR and xAmCLR consistently outperformed the existing loss functions across all evaluation metrics, in all tasks. While AdamW showed marginally better performance in Zero-shot Classification tasks, AdamP had a slight advantage in Retrieval (Text) and Retrieval (Image) tasks.\nAmong the proposed loss functions, AmCLR generally achieved higher accuracies compared to xAm-CLR. However, the performance gap between the two was notably smaller in Zero-shot Classification tasks, suggesting that xAmCLR excels at generalization. These results highlight the potential of AmCLR and xAmCLR as robust loss functions for bimodal contrastive learning."}, {"title": "7 Future Research", "content": "Our initial experiments with AmCLR and xAmCLR demonstrate promising results on a 100K subset of the Conceptual Captions 3M dataset. The significant performance improvements achieved with relatively constrained computational resources - a batch size of 128 and 30 epochs - strongly suggest that scaling up these approaches could yield substantially better results. We plan to extend our research in several key directions.\nFirst, we anticipate that training on the complete CC3M dataset of 3 million image-text pairs will enable our models to learn more nuanced and robust representations. The strong performance on the current subset indicates that our architectural choices and loss functions can effectively capture cross-modal relationships. With access to the full dataset, we expect to achieve more comprehensive coverage of semantic relationships and improved generalization capabilities. This expansion will require careful hyper-parameter optimization, particularly for batch sizes and epoch counts, to fully leverage the increased data volume while maintaining computational efficiency.\nA critical focus of our future work will be the development of a robust distributed training infrastructure. We envision implementing a hierarchical distributed training architecture with primary and secondary coordinator nodes managing multiple worker clusters. This system will incorporate sophisticated fault tolerance mechanisms, including checkpoint synchronization protocols and automated worker node recovery. We plan to implement a distributed logging system that aggregates training metrics, system health indicators, and resource utilization data in real-time. To ensure training stability, we will deploy an automated monitoring system that tracks gradient norms, loss convergence patterns, and cross-node consistency metrics. The infrastructure will include fallback mechanisms such as gradient accumulation buffers and dynamic batch size adjustment to handle temporary node failures without compromising training integrity. Additionally, we will implement distributed data loading pipelines with pre-fetching and caching mechanisms to optimize I/O operations across the training cluster.\nOur current implementation uses a conservative augmentation strategy with $w = 1$. Future research will explore the impact of increasing the number and diversity of augmentations. For images, we plan to investigate more sophisticated transformation pipelines that preserve semantic content while creating challenging positive pairs. This includes implementing adaptive augmentation strategies that adjust transformation intensity based on training dynamics. For text modality, we aim to use more nuanced paraphrasing techniques that generate semantically equivalent but syntactically diverse expressions. These enhanced augmentation strategies should create more challenging negative samples while maintaining semantic consistency, thereby improving the model's ability to learn robust cross-modal representations.\nFinally, we recognize the potential synergy between our augmentation-based approaches and the distributionally robust optimization (DRO) framework employed in iSogCLR. We"}]}