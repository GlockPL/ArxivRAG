{"title": "ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors", "authors": ["Yuguo Yin", "Yuxin Xie", "Wenyuan Yang", "Dongchao Yang", "Jinghan Ru", "Xianwei Zhuang", "Liming Liang", "Yuexian Zou"], "abstract": "Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to retrieve audio clips or multilingual texts from databases. However, existing ML-ATR schemes suffer from inconsistencies for instance similarity matching across languages. We theoretically analyze the inconsistency in terms of both multilingual modal alignment direction error and weight error, and propose the theoretical weight error upper bound for quantifying the inconsistency. Based on the analysis of the weight error upper bound, we find that the inconsistency problem stems from the data distribution error caused by random sampling of languages. We propose a consistent ML-ATR scheme using 1-to-k contrastive learning and audio-English co-anchor contrastive learning, aiming to mitigate the negative impact of data distribution error on recall and consistency in ML-ATR. Experimental results on the translated AudioCaps and Clotho datasets show that our scheme achieves state-of-the-art performance on recall and consistency metrics for eight mainstream languages, including English. Our code will be available at https://github.com/ATRI-ACL/ATRI-ACL.", "sections": [{"title": "1 Introduction", "content": "In an audio-text retrieval (ATR) task, the system searches for matching audio clips or text captions in a database based on cross-modality queries (Zhu et al., 2024; Zhuang et al., 2024). With the convergence of audio and text, ATR techniques have seen significant advancements in recent years and are widely applied in content retrieval and multimedia information retrieval. However, most existing ATR systems are designed for monolingual retrieval, and research on multilingual audio-text retrieval (ML-ATR) remains limited (Yan et al., 2024). The shift to ML-ATR brings new challenges, particularly in dealing with high multilingual recall and ensuring the consistency (Nie et al., 2024) of multilingual retrieval results.\nTo the best of our knowledge, the existing mainstream ML-ATR scheme has a model training process as shown in Fig. 1, which pairs audio with randomly selected linguistic text in each epoch. This may not allow the model to learn the embedding space of audio and multilingual texts very well, which not only reduces the recall of retrieval, but also makes it difficult to obtain the same retrieval results for audio and multilingual text instances in different languages.\nIn this paper, we theoretically analyzes the causes of the inconsistency problem in ML-ATR. We first visualize the inconsistency problem in terms of the modal alignment direction error. The alignment direction error leads to the gradient error, which in turn invites the model weights to fail to converge to the optimal weights for multilingual modal alignment during the training process. We further heuristically derive theoretical upper bounds on the weight errors to quantify the adverse effects of inconsistency on the model weights. We analyze the composition of the weight error upper bound and conclude that the root cause of the error inconsistency is the data distribution error in training.\nBased on the theoretical analysis, we propose a scheme to mitigate the inconsistency of ML-ATR, called ATRI. ATRI consists of two training strategies: 1-to-K Contrastive Learning (KCL) for the retrieval-performance-first scenario, and Audio-English Co-Anchor Contrastive Learning (CACL) for the overhead-first scenario. KCL theoretically eliminates the data distribution errors in each training epoch, thus achieving state-of-the-art performance in recall and consistency metrics. CACL aligns the other languages with audio and English text to correct the modal alignment direction and reduce the data distribution error. Compared to existing ML-ATR schemes, CACL improves retrieval recall and consistency while offering advantages in training time and GPU memory overhead over KCL.\nOur contributions are shown below:\n\u2022 We analyze the inconsistency in terms of analyzing the modal alignment direction error and weighting error, and demonstrate an upper bound on the weighting error. We further conclude that the root cause of the inconsistency of existing ML-CLAP schemes lies in the distribution error of the training data.\n\u2022 We propose ATRI, which solves the inconsistency problem in multilingual audio text retrieval by reducing the data distribution error and correcting the modality alignment direction. ATRI contains the CACL and KCL training strategies for overhead-first and performance-first requirements, respectively.\n\u2022 We evaluate the proposed scheme using the AudioCaps and Clotho datasets translated by Deepseek. The results show that ATRI effectively improves recall and consistency in both monolingual English ATR and ML-ATR tasks, achieving state-of-the-art performance."}, {"title": "2 Related Work", "content": "Audio-text retrieval (ATR) (Lou et al., 2022; Xie et al., 2024; Xin et al., 2024) is a task that matches audio with text, which has seen significant advancements and widespread applications in recent years. The prevailing approach involves constructing a shared embedding space for audio and text, enabling seamless feature alignment and retrieving results based on similarity rankings. Widely adopted methods include CLIP-inspired (Yu et al., 2022; Li et al., 2022) comparative audio-text pretraining (Elizalde et al., 2023; Wu et al., 2022; Guzhov et al., 2022) and the triplet-loss method (Mei et al., 2022), both of which have achieved success in learning audio-text joint embedding spaces.\nExisting ATR methods predominantly focus on English-centric monolingual tasks, with few solutions for multilingual scenarios (Yan et al., 2024). The scarcity of large-scale, accurately annotated non-English audio-caption datasets has led current ML-ATR methods to rely heavily on machine translation (Tiedemann and Thottingal, 2020; Team et al., 2022) to convert English datasets into multilingual versions. This translation-based strategy (Cousin et al., 2023; Yan et al., 2024) has demonstrated its effectiveness in enhancing datasets for multilingual use, significantly improving the recall performance of ATR systems.\nHowever, existing ML-ATR scheme (Yan et al., 2024) use audio-text pairs with randomly selected languages for training. As analyzed in Sect. 3, the training method employed presents significant challenges in achieving convergence to the optimal weights. This difficulty not only exacerbates issues related to inconsistent cross-language retrieval, but also leads to a degradation in the retrieval performance, particularly in terms of both recall and accuracy."}, {"title": "3 Definition and Inconsistency Analysis", "content": "Audio-text retrieval is the task of learning cross-modality alignment between audio and multilingual text captions. Contrastive learning (Ru et al., 2023; Zhuang et al., 2025) has become the most effective method for learning expressive cross-modality embedding spaces.\nDenote a dataset $\\mathcal{D} = \\{(a_i, t_{i1},...,t_{ik})\\}_{i=1}^N$ as a multilingual audio text retrieval dataset, where $N$ denotes the size of dataset, $K$ refers the total language number in the dataset, $a_i$ denotes the audio in i-th data, $t_{ik}$ denotes the k-th language in i-th data. Given an audio encoder $f_{\\theta}(\\cdot)$ and a multilingual text encoder $g_{\\phi}(\\cdot)$, we denote the joint probability distribution as:\n$$p(a_i, t_{ik}) = \\frac{\\exp (s(f_{\\theta}(a_i), g_{\\phi}(t_{ik}))/T)}{\\sum_{j=1}^N \\sum_{l=1}^K \\exp (s(f_{\\theta}(a_j), g_{\\phi}(t_{jl}))/T)},$$\n$s(\\cdot)$ denotes the cosine similarity between audio and text embedding. The ideal optimization function of learning the embedding space is\n$$\\max_{\\theta,\\phi} \\sum_{i=1}^N \\sum_{k=1}^K \\mathbb{E}_{(a_i,t_{ik})} [\\log p(a_i, t_{ik})].$$\nHowever, instead of training all the languages of a piece of data in an epoch, the existing ML-ATR scheme randomly selects the text of a language to do the training. For each epoch $e$, a set of random numbers $Q = \\{q_1,....q_N\\},q_i\\pounds \\{1,...K\\}$. The optimization function they used is formalized as:\n$$p_e(a_i, t_{iq_i}) = \\frac{\\exp (s(f_{\\theta}(a_i), g_{\\phi}(t_{iq_i}))/T)}{\\sum_{j=1}^N \\exp (s(f_{\\theta}(a_j), g_{\\phi}(t_{jq_j}))/T)},$$\n$$\\max_{\\theta,\\phi} \\sum_{i=1}^N \\mathbb{E}_{(a_i,t_{iq_i})} [\\log p_e(a_i, t_{iq_i})].$$\nThe probability distribution $p_e(a_i, t_{iq_i})$ of their scheme is not the same as the original probability distribution $p(a_i, t_{ik})$. This results in a model that does not fit the training data perfectly, making modality alignment ineffective, which in turn results in reduced recall and inconsistency problems."}, {"title": "3.2 Analysis of the Inconsistency Issue", "content": "We first analyze the issue of inconsistency from the perspective of modality alignment directional errors. As shown in Fig. 2, an intuitive example of modality alignment error is illustrated. Consider a simple case of bilingual audio-text retrieval, let the embedding of an audio sample be a, and the embeddings of the corresponding texts in two languages be $t_1$ and $t_2$. Ideally, the audio embedding a should be aligned with the combined representation of both text embeddings ($t_1 + t_2$) (indicated by the green arrow). However, in existing ML-ATR schemes, the audio embedding is only aligned with the text embedding of a randomly selected language within each epoch. For instance, if the selected language is $t_2$, the audio embedding a will be aligned solely towards $t_2$ (indicated by the red arrow). The angle between the red and green arrows is the modality alignment direction error, which makes the audio and multilingual text modes not well aligned.\nIt's obvious that incorrect alignment introduces noise to the gradient, leading to errors between the model weights and their optimal values, making the model's retrieval recall and consistency metrics degrade. We give a theoretical weight error upper bound and analyze its composition to mitigate the inconsistency problem and improve retrieval recall. The detailed proof can be found in Appendix C.\nWe assume that the optimization algorithm is stochastic gradient descent (SGD) (Ru et al., 2025) to heuristically analyse the upper bound of the weight error. Given that the number of training steps per epoch T, the data distribution obtained by randomly sampling the language according to the existing ATR scheme is denoted as $p'_e$, and the original data distribution is denoted as $p$. $w_{eT}$ denotes the model weight in the T-th step under the e-th epoch trained with the data distribution $p'_e$, whereas $w'_{eT}$ denotes the weight that is trained with the data distribution $p$. If the gradient $\\nabla_w E(a,t) [\\log p(a, t)]$ is $\\Lambda(x,y)$-Lipschitz (B\u00e9thune et al., 2023), then we have the following inequality for weight error upper bound:\n$$\\lVert w_{eT} - w'_{eT} \\rVert \\\n< \\alpha \\lVert w_{(e-1)T} - w'_{(e-1)T} \\rVert +$$\n$$\\eta \\sum_{(a,t)} \\lVert p(a, t) \u2013 p'_e(a, t) \\rVert \\sum_{j=1}^{T-1} (\\alpha^2 g_{max} (w_{eT-1-j})),$$\n$$g_{max}(w) = \\max_{(a,t)} \\lVert \\nabla_w E(a,t) [\\log p(a, t)] \\rVert,$$\n$$\\alpha = 1 + \\eta \\sum_{(a,t)} p_e(a, t) \\Lambda(x,y).$$\nNote: The weight $w$ consists of the parameter $\\theta$ for the audio encoder $f_{\\theta}$ and the parameter $\\phi$ for the multilingual text encoder $g_{\\phi}$ in ML-ATR. The data distributions $p$ and $p'_e$ correspond to the Eq. (2) and (4), respectively. For simplicity, we denote (a, t) as all audio-text pairs in the batch of the T-th step, where the text t can be in any one of the languages. $\\sum_{(a,t)} \\lVert p(a, t) \u2013 p'(a, t) \\rVert$ denotes the data distribution error in the batch at step T.\nDetailed proof of Eq (6) can be found in Appendix C. Based on Eq. (6), we have the following results:\n\u2022 Intuitively, the weight error $\\lVert w_{eT} - w'_{eT} \\rVert$ comes from two main sources. One is the weight error after the (e \u2013 1)-th epoch, i.e. $\\lVert w_{(e-1)T} - w'_{(e-1)T} \\rVert$. The other is caused by the probabilistic distances of the data distributions, i.e. $\\sum_{(a,t)} \\lVert p'_e(a, t) -p(a, t) \\rVert$. Since \u03b1 \u2265 1, the error from both sources increases with epoch and step. In addition, the weight error is also affected by the learning rate \u03b7, the number of training steps T and the maximum gradient $g_{max} (w_{eT-1-j})$.\n\u2022 Further expansion of Eq. (6) shows that the weighting error arises from the data distribution error of each epoch. Expanding $\\lVert w_{(e-1)T} - w'_{(e-1)T} \\rVert$ in Eq. (6), we find it consist of $\\lVert w_{(e-2)T}-w'_{(e-2)T} \\rVert$ and $\\lVert p(a, t) - p'_{e-1}(a, t) \\rVert$. Further expanding Eq. (6) to the weight error in 1-th epoch, it can be concluded that the weight error of the existing ML-ATR scheme comes from the data distribution error $\\sum_{i=1}^{e} \\sum_{(a,t)} \\lVert p(a, t) - p_i(a,t) \\rVert$ due to the randomly selected languages in each epoch. We can mitigate the inconsistency problem and improve the recall by reducing the weight error upper bound by reducing the data distribution error for each epoch."}, {"title": "4 Proposed ML-ATR Scheme", "content": "We propose two methods to reduce the data distribution error during training. One is 1-to-K contrastive learning, which has a higher memory overhead. The other is audio-English co-anchor contrastive learning, which achieves performance close to 1-to-K Contrastive Learning while approximating the memory overhead to the existing ML-ATR scheme. Here are the details of the two methods."}, {"title": "4.1 1-to-K Contrastive Learning", "content": "Building on our theoretical analyses, we conclude that reducing data distribution error is critical for addressing the inconsistency problem in multilingual audio-text retrieval. To achieve this, we propose 1-to-K Contrastive Learning (KCL), a training strategy that replaces random language sampling with the simultaneous use of all K linguistic texts corresponding to each audio instance. This approach theoretically eliminates data distribution error, corrects modal alignment direction, and significantly enhances both the recall and consistency of retrieval performance. The loss function $L_{kcl}^{t2a}$ for the proposed 1-to-K Contrastive Learning in ML-ATR is defined as follows:\n$$L_{kcl} = \\frac{1}{2NK}(L_{at}^{t2a} + L_{ta}^{t2a}).$$\nThe loss function $L_{kcl}^{t2a}$ consists of two parts, $L_{at}^{kcl}$ and $L_{ta}^{kcl}$, and they are calculated as follows:\n$$L_{at}^{kcl} = -\\sum_{k=1}^K \\sum_{i=1}^N \\log \\frac{\\exp(s(f_{\\theta}(a_i), g_{\\phi}(t_{ik}))/T)}{\\sum_{j=1}^K \\exp(s(f_{\\theta}(a_i), g_{\\phi}(t_{jk}))/T)},$$\n$L_{at}^{kcl}$ denotes the contrastive learning loss function from audio to multilingual text.\n$$L_{ta}^{kcl} = -\\sum_{k=1}^K \\sum_{i=1}^N \\log \\frac{\\exp(s(g_{\\phi}(t_{ik}), f_{\\theta}(a_i))/T)}{\\sum_{j=1}^N \\exp(s(g_{\\phi}(t_{ik}), f_{\\theta}(a_j))/T)},$$\n$L_{ta}^{kcl}$ denotes the contrastive learning loss function from multilingual text to audio.\nK is the number of languages and N is the number of data instances. As shown in Tab. 4, including multiple multilingual texts in 1-to-K contrastive learning increases GPU memory usage and training time. In practical ML-ATR applications, supporting more languages amplifies these overheads compared to existing schemes.\nTo address this, we further propose CACL, which improves retrieval consistency and recall without significantly increasing overhead."}, {"title": "4.2 Audio-English Co-Anchor Contrastive Learning", "content": "To reduce the weighting error with as little increase in training time and GPU memory consumption as possible, we propose audio-English co-anchor contrastive learning (CACL). During the training process, each data takes its audio, English text, and text in other random languages and does contrastive learning with each other.\nFor each epoch, given a set of random numbers $Q = \\{q_1,...q_N\\},q_i \\pounds \\{2,...K\\}$, get the triplet of the training data $(a_i, t_{i1}, t_{iq_i})$, where $a_i$ denotes i-th audio, $t_{i1}$ denotes the English text, and $t_{iq_i}$ denotes the text of $q_i$-th language. We have the training loss $L_{cacl}$ shown below:\n$$L_{cacl} = \\frac{1}{6N}(L_{a-e}^{cacl} + L_{a-o}^{cacl} + L_{e-o}^{cacl}).$$\nThe loss function $L_{cacl}$ consists of three components $L_{a-e}^{cacl}$, $L_{a-o}^{cacl}$, $L_{e-o}^{cacl}$. All three components are based on the following general contrastive learning loss formulation:\n$$L_{u-v} = -\\sum_{i=1}^N \\log \\frac{\\exp(s(u_i, v_i)/T)}{\\sum_{j=1}^N \\exp(s(u_i, v_j)/T)}$$\n$$- \\sum_{i=1}^N \\log \\frac{\\exp(s(v_i, u_i)/T)}{\\sum_{j=1}^N \\exp(s(v_i, u_j)/T)},$$\nwhere $u_i$ and $v_i$ represent input embeddings from different modalities or languages. The three components are defined as follows:\n\u2022 Audio-English Alignment ($L_{a-e}^{cacl}$):\n$u_i = f_{\\theta}(a_i)$ represents audio embeddings, and $v_i = g_{\\phi}(t_{i1})$ represents English text embeddings.\n\u2022 Audio-Multilingual Alignment ($L_{a-o}^{cacl}$):\n$u_i = f_{\\theta}(a_i)$ represents audio embeddings, and $v_i = g_{\\phi}(t_{iq_i})$ represents text embeddings in a randomly selected language.\n\u2022 English-Multilingual Alignment ($L_{e-o}^{cacl}$):\n$u_i = g_{\\phi}(t_{i1})$ represents English text embeddings, and $v_i = g_{\\phi}(t_{iq_i})$ represents text embeddings in a randomly selected language.\nThe effectiveness of audio-English CACL can be explained from two perspectives:\n\u2022 From the perspective of modality alignment (Fig. 2), the loss function $L_{e-o}^{cacl}$ in CACL brings embeddings of English and other languages closer, reducing the distance between the text embedding $t_1$, $t_2$ and the mean $(t_1 + t_2)$ and minimizing the deviation in the modality alignment direction of audio and text.\n\u2022 From the perspective of data distribution error $\\sum_{(a,t)} \\lVert p(a, t)-p_e(a, t) \\rVert$ in Eq. (6), CACL's loss functions $L_{a-o}^{cacl}$, $L_{e-o}^{cacl}$ ensures that the model learns more pairs of audio texts in an epoch. The text in them also contains a large percentage of high-quality English text. It makes the data distribution in CACL closer to the original one, and reduces the weight error of the model.\nNote that in CACL, the number of texts used for training in each epoch does not increase with the number of languages, which effectively reduces both GPU memory and time overhead in ML-ATR scenarios with a large number of languages. Our experimental results illustrate that CACL approximates the training time and explicit memory overhead of existing ML-ATR schemes, yet achieves recall and consistency metrics close to those of 1-to-K comparative learning."}, {"title": "5 Experiments", "content": "We employ the AudioCaps (Kim et al., 2019), and Clotho (Drossos et al., 2020) for our experiments. AudioCaps includes around 49,000 audio samples, each lasting about 10 seconds. Each audio is paired with a single sentence in the training set, while in both the validation and test sets, each audio has five associated sentences. The Clotho dataset consists of 6,974 audio samples, each ranging from 15 to 30 seconds long and annotated with five sentences. It is split into 3,839 training samples, 1,045 validation samples, and 1,045 test samples.\nAdditionally, to assess our scheme's performance in the ML-ATR task, we use the Deepseek (Bi et al., 2024) API to translate the text from AudioCaps and Clotho into seven widely spoken languages, including French (fra), German (deu), Spanish (spa), Dutch (nld), Catalan (cat), Japanese (jpn), and Chinese (zho)."}, {"title": "5.2 Models", "content": "Audio Encoder: We utilize the recently proposed CED-Base model (Dinkel et al., 2024), a vision transformer with 86 million parameters for the Audio Encoder. Trained on Audioset through knowledge distillation from a large teacher ensemble, the model processes 64-dimensional Mel-spectrograms derived from a 16 kHz signal. It then extracts non-overlapping 16 \u00d7 16 patches from the spectrogram, resulting in 248 patches over a 10-second input (4 \u00d7 62).\nText Encoder: The key to multilingual audio-text retrieval is the text encoder's ability to handle texts in multiple languages. In this work, we focus solely on the SONAR-TE model (Duquenne et al., 2023)."}, {"title": "5.3 Setup", "content": "We use ML-CLAP (Yan et al., 2024) as the baseline, which is the state-of-the-art for ML-ATR tasks. To have a fair comparison, the model is initialized using the pre-trained weights of ML-CLAP and is further fine-tuned on our multilingual Audiocaps and Clotho datasets using three training methods: ML-CLAP, proposed CACL, and proposed KCL.\nAll models were fine-tuned for 10 epochs on a single A100 80GB PCIe GPU with a batch size of 24, a learning rate of 5 \u00d7 10\u20136, using the Adam optimizer. The temperature hyperparameter t was set to 0.07 for all configurations. The audio was sampled at 1.6 \u00d7 104. We selected the model with the best recall performance during the fine-tuning period for each scheme to perform the experiments."}, {"title": "5.4 Evaluation Metric", "content": "We use the recall of rank k (R@k) and the average precision of rank 10 (mAP10) as the metrics for the retrieval performance of the model to show that reducing data distribution errors improves the retrieval performance in each language. R@k refers to the fact that for a query, R@kis 1 if the target-value item occurs in the first k retrieved items, and 0 otherwise. mAP10 calculates the average precision of all the queries among the first 10 retrieved results. With these two metrics, we can comprehensively evaluate the retrieval performance of the model on multilingual datasets.\nTo assess the consistency of the embedding space across languages, we use three metrics: embedding space gap $\\triangle_{gap,k}$ (Liang et al., 2022), average embedding distance $\\triangle_{dis,k}$, mean rank variance (MRV). The computation of $\\triangle_{gap,k}$, $\\triangle_{dis,k}$ and MRV is shown below:\n$$\\triangle_{gap,k} = \\frac{1}{N} \\sum_{i=1}^N \\lVert g_{\\phi}(t_{i1}) - \\frac{1}{K} \\sum_{k=1}^K g_{\\phi}(t_{ik}) \\rVert,$$\n$$\\triangle_{disk} = \\frac{1}{N} \\sum_{i=1}^N ||g_{\\phi}(t_{i1}) \u2013 g_{\\phi}(t_{ik})||,$$\n$$MRV = \\frac{1}{NK} \\sum_{i=1}^N \\sum_{k=1}^K |Rank_{ik} \u2013 Rank_i|^2.$$\n$\\triangle_{gap,k}$ and $\\triangle_{dis,k}$ denotes the embedding space gap and average embedding distance between English and k-th language respectively. $Rank_{ik}$ denotes the similarity ranking of the k-th language under the i-th data, and $Rank_i$ denotes the average similarity ranking under the i-th data."}, {"title": "5.5 Evaluation Result of Recall and Precision", "content": "We present a detailed numerical comparison analysis of the experiment results in Tab 1, focusing on the performance improvements of our proposed methods, CACL and KCL, over the baseline ML-CLAP across various languages and datasets."}, {"title": "5.5.1 Analysis of Evaluation Results", "content": "Overall, the proposed CACL and KCL consistently outperform ML-CLAP across the majority of languages and datasets in terms of recall at 1 (R@1), recall at 5 (R@5), and mean average precision at the top 10 results (mAP10) for both Text-to-Audio (T2A) and Audio-to-Text (A2T) tasks. Notably, our proposed KCL achieves state-of-the-art performance, delivering a 5% improvement in R@1 for the English-oriented monolingual ATR task and a 4.3% improvement in R@1 for the multilingual ATR task compared to ML-CLAP. This experimental result corroborates our theoretical analysis of the weighting error in Sect. 3. Here is the detailed analysis:\nCACL's average metrics across languages are higher than ML-CLAP, while KCL's average metrics across languages have further improvement compared to CACL. Our theoretical analyses in Sect can explain this phenomenon.\n\u2022 CACL uses audio and text together as the anchor point for modality alignment in other languages, which can effectively reduce the data distribution error and modality alignment error, thus achieving better modality alignment results and improved metrics compared to ML-CLAP.\n\u2022 Compared to CACL, which mitigates data distribution errors, KCL theoretically eliminates these errors. As a result, KCL achieves superior modality alignment compared to CACL, leading to further improvements in both recall and precision."}, {"title": "5.5.2 Analysis of Special Situations", "content": "Occasional Metric Anomalies: We observed occasional anomalies where a small proportion of KCL metrics were lower than CACL metrics, and some CACL metrics were lower than ML-CLAP metrics. We attribute these discrepancies to noise in the dataset. Specifically, the weight error in Eq. (6) represents the difference between the current and optimal model weights for fitting the training data. If the dataset is too noisy, the optimal weights may not improve the test set's performance. As a result, KCL and CACL, which have lower weight errors, may still underperform ML-CLAP on certain metrics. The higher frequency of such anomalies in the noisier Clotho dataset, compared to Audiocaps, supports this explanation. Given that these anomalies are rare among the 108 evaluated metrics, we consider them acceptable and conclude that they do not impact the overall performance advantage of CACL and KCL in the ML-ATR task.\nPerformance Gaps Across Languages: The lower metrics for Japanese and Chinese in Tab. 1 are mainly due to their significant syntactic differences from other languages, making them harder for the model to learn. Expanding the dataset for these languages could improve the model's performance by providing more representative data.\nBetter Replicated Performance: Compared to the original paper, our replicated ML-CLAP model achieves significant improvements across all metrics, mainly due to differences in data quality. Compared to the SONAR-translated text used by baseline, the multilingual text we translated with LLM is of higher quality, which in turn can improve the retrieval performance of the model."}, {"title": "5.6 Evaluation Result of Consistency", "content": ""}, {"title": "5.6.1 Analysis of Embedding Space Consistency", "content": "The results of the consistency metrics embedding space gap $\\triangle_{gap,k}$ and average embedding distance $\\triangle_{dis,k}$ are shown in Tab. 2. In addition, we give a visualization of the embedding space in Appendix A and case analysis in Appendix B to further illustrate the effectiveness of ATRI in solving the consistency problem.\nSmaller values of $\\triangle_{gap,k}$ and $\\triangle_{dis,k}$ indicate better alignment of a language's embedding space with English, leading to more consistent retrieval in the ML-ATR task. Compared to the baseline ML-CLAP, CACL achieves an average reduction of 12.9% in Gap and 4.4% in Dis, while KCL reduces Gap by 19.1% and Dis by 14.3%, demonstrating improved cross-language retrieval consistency."}, {"title": "5.6.2 Analysis of Rank Consistency", "content": "MRV quantifies the consistency of search rankings across languages, with lower values indicating more consistent results across languages. Unlike metrics based on embedding space, MRV offers a more direct assessment of model consistency in the ML-ATR task. As shown in Tab. 3, KCL achieves the lowest MRV, representing a 25.9% reduction compared to ML-CLAP, while CACL achieves a 22.3% reduction. This effectively shows that the inconsistency issue can be effectively mitigated by reducing the data distribution error.\nWe note that the MRV metrics under the Audiocaps dataset are significantly lower than Clotho's. This is due to the fact that the Clotho dataset is much noisier and more difficult to get consistent retrieval results across languages."}, {"title": "5.7 Evaluation Results about Training Overhead", "content": "Tab.4 summarises the GPU memory overhead (GMO) and time overhead (TO) during training for three scenarios: ML-CLAP, CACL, and KCL.\nKCL training requires simultaneous input of text in eight languages, which significantly increases overhead, resulting in a higher GMO of about 2.8 times and a 27% increase in TO compared to ML-CLAP. In contrast, CACL inputs just twice as much text as ML-CLAP, resulting in a modest increase of about 10% in both overheads. This makes CACL more suitable for scenarios that prioritize lower training overheads, while KCL is more suitable for applications that emphasize retrieval performance."}, {"title": "6 Conlusion", "content": "In this paper, we address the inconsistencies in ranking results observed in existing ML-ATR schemes. Through an analysis of modality alignment errors and weighting errors, we identify data distribution errors during training as a key factor impacting cross-lingual modality alignment, ultimately leading to retrieval inconsistencies. To address this, we propose two training strategies: KCL and CACL, designed for scenarios prioritizing retrieval performance and training overhead, respectively. Experimental results demonstrate that both CACL and KCL effectively enhance retrieval performance and consistency in ML-ATR tasks. Notably, KCL achieves state-of-the-art results in both English-oriented monolingual ATR and ML-ATR tasks. Furthermore, the proposed approach of mitigating data distribution errors to reduce inconsistencies holds potential for broader applications, including multilingual modality alignment in image and video modalities."}, {"title": "Limitation", "content": "We acknowledge that the upper bound on the weighting error in Eq. (6) is heuristically proven for the SGD optimizer. For more complex optimizers such as Adam, giving a direct upper bound on the weighting error is difficult. But we provide proof of momentum error upper bound for Adam in the Appendix C.1, and show that our idea of reducing the data distribution error is still feasible under the Adam optimizer by showing that momentum error leads to weight error."}, {"title": "C Proof of Weight Error Upper Bound", "content": "We analyze the upper bound on the weighting error heuristically based on the stochastic gradient descent (SGD) optimization algorithm. The following is a detailed theoretical proof of the upper bound on the weighting error in Eq. (6).\nProof. Based on the definition of the SGD optimization algorithm", "have": "n$$w_{eT} = w_{eT-1} -\\eta\\sum_{(a,t)} p(a,t)\\nabla_{w_{eT-1}}E_{(a,t)} [\\log p(a, t)", "t)": "."}, {"t)": "n$$- w'_{eT-1} + \\eta \\sum_{(a, t)}p_e(a, t) \\nabla_{w'_{eT-1}}E_{(a,t)} [\\log p(a, t)"}]}