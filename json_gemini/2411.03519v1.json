{"title": "AI METROPOLIS: SCALING LARGE LANGUAGE MODEL-BASED\nMULTI-AGENT SIMULATION WITH OUT-OF-ORDER EXECUTION", "authors": ["Zhiqiang Xie", "Hao Kang", "Ying Sheng", "Tushar Krishna", "Kayvon Fatahalian", "Christos Kozyrakis"], "abstract": "With more advanced natural language understanding and reasoning capabilities, large language model (LLM)-\npowered agents are increasingly developed in simulated environments to perform complex tasks, interact with other\nagents, and exhibit emergent behaviors relevant to social science research and innovative gameplay development.\nHowever, current multi-agent simulations frequently suffer from inefficiencies due to the limited parallelism\ncaused by false dependencies, resulting in performance bottleneck. In this paper, we introduce AI Metropolis, a\nsimulation engine that improves the efficiency of LLM agent simulations by incorporating out-of-order execution\nscheduling. By dynamically tracking real dependencies between agents, AI Metropolis minimizes false depen-\ndencies, enhancing parallelism and enabling efficient hardware utilization. Our evaluations demonstrate that AI\nMetropolis achieves speedups from 1.3\u00d7 to 4.15\u00d7 over standard parallel simulation with global synchronization,\napproaching optimal performance as the number of agents increases.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) are advanced machine\nlearning models trained on vast amounts of data, excelling in\nunderstanding and generating natural language. They have\ntransformed natural language processing, enabling high-\naccuracy applications like text completion (Merity et al.,\n2016), summarization (Narayan et al., 2018), and reason-\ning (Cobbe et al., 2021). Beyond simple queries and chat-\nbot interactions (OpenAI, 2024a), there is growing interest\nin using LLMs to create self-planning, decision-making,\nproblem-solving, and reasoning engines (Wang et al., 2024).\nThese advancements aim to develop human-like agents (Xi\net al., 2023) capable of performing complex tasks, inter-\nacting with environments and other agents, and making\ninformed decisions based on context.\nThis interest is particularly pronounced in developing LLM-\npowered agents within simulated environments, where two\nunique opportunities arise. First, simulation environments\nprovide an efficient platform for testing and tuning LLM\nagents (Dubois et al., 2024; Liu et al., 2023; Wang et al.,\n2023b), with potential applications extending to real-world\nsettings or virtual environments like gaming. Second, the\nenhanced natural language understanding and reasoning ca-\npabilities of LLMs have sparked a trend of examining emer-\ngent social behaviors of these agents in game-like simula-\ntions (Park et al., 2023; Altera. AL et al., 2024). Such studies\ncan serve as predictive models, forecasting real-world hu-\nman behaviors, which is highly valuable for social science\nresearch (Ziems et al., 2023; Grossmann et al., 2023).\nDespite the significance of simulation environments for\nLLM agents, the efficiency of managing simulation states\nand scheduling LLM requests in simulations are often over-\nlooked, leading to slow and inefficient simulation processes.\nRecent research work commonly implements their LLM\nagents simulation (Park et al., 2023; Gong et al., 2023)\ndirectly adhering to a paradigm borrowed from reinforce-\nment learning agent training and traditional multi-agent\nsimulation (Emau et al., 2011), where simulation time is\ndiscretized into time steps and a step (or similar) function\nis invoked to apply agents' actions, synchronize the en-\nvironment, and coordinate agents at each interval. This\npattern, illustrated in Algorithm 1, is prevalent in promi-\nnent reinforcement learning frameworks such as OpenAI\nGym (Brockman et al., 2016), Meta Pearl (Zhu et al., 2024),\nand TensorFlow Agents (Guadarrama et al., 2018). The\nrationale behind this design is that global synchronization,\nenforced through the step function, easily maintains tempo-\nral causality within the simulation by serializing tasks along\nthe simulation time axis.\nWhile this design suits the needs of reinforcement learning\nand traditional multi-agent simulations, we found it ineffi-\ncient for LLM agents due to their unique performance char-\nacteristics, necessitating a new scheduling approach. Sim-\nulations involving LLM agents, like other LLM-powered\napplications, are heavily dominated by inference time. Tak-\ning the pioneering work on generative agents (Park, 2024)"}, {"title": "2 SIMULATION OF LLM AGENT\nINTERACTION", "content": "While simulation environments can be diverse with complex\naction spaces and interactions, the high-level procedure of a\nsimulation generally follows a common pattern as outlined\nin Algorithm 1. In this pattern, agents determine future\nactions based on the current world states as well as their\ninternal states, and these actions affect the world as the sim-\nulation progresses. Note the agent.proceed and world.step\nfunctions are defined by the agent developers and environ-\nment respectively, and can be implemented by manual rules\nor via invoking LLMs (or both) for decision making.\nTo illustrate the challenge of achieving efficient simulation,\nwe use GenAgent as a concrete example within the broad\nfamily of simulations for LLM agent interaction. GenAgent\nproposes a comprehensive agent architecture and interac-\ntion mechanisms that have inspired many subsequent works.\nTheir concepts and workflows are widely adopted in the\ncommunity. In GenAgent, 25 agents inhabit a world called\nSmallVille, akin to a grid-based game. Each agent possesses\nits own personality, social relationships, and daily routines."}, {"title": "2.2 Motivation and Challenges", "content": "Imbalanced workload reduces available parallelism.\nAlthough line 5 in Algorithm 1 suggests that parallelism can\nbe achieved up to the number of agents, the effective paral-\nlelism is often much less primarily due to the imbalanced\nworkload across agents. As illustrated in Figure 1, there\nare moments when many agents send out LLM requests\nuniformly. However, for the majority of the execution time,\na few agents dominate each step, resulting in prolonged idle\nperiods for many other agents who issue no LLM queries.\nThis sparsity is an inherent feature of simulation, as differ-\nent agents have their own schedules and encountered events,\nmaking even distributions unlikely. Additionally, as shown\nin Figure 1, even for the same type of LLM calls, comple-\ntions can vary significantly depending on inputs provided\nby different agents, further introducing imbalance. Our\nmeasurements indicate that for a full-day simulation of 25\nagents, there are, on average, only 1.94 concurrent LLM\nqueries throughout the simulation.\nFalse Dependency. While the workload imbalance across\nagents might be inevitable, low parallelism is not. We found\nthat the primary cause of idleness is the overly strict en-\nforcement of time causality. Requiring all events in one\ntime step to complete before advancing to the next step\nintroduces unnecessary dependencies. As shown in Fig-\nure 2, this approach creates an implicit all-to-all dependency\nacross agents in consecutive steps. However, some agents,\nsuch as agent A, may be sufficiently isolated and unable\nto interact with others, thus not creating dependencies on\nagents like B or C. Our trace analysis for a whole day sim-\nulation of GenAgent indicates that, on average, each agent\nis dependent on only 1.85 agents (including itself) from the\nprior step, far less than the default 25. The issue of false\ndependencies worsens as the agent count grows, as more\nfalse dependencies are enforced, diminishing the benefits\nof increased parallelism. Although agents' behaviors are\ndriven by responses from LLMs, which limits a scheduler's\nability to optimally manage dependencies without foresight,\nagent behavior is still somewhat predictable. Agents are con-\nstrained by their movement speed and limited action space,\nproviding an opportunity to reduce most false dependencies\nthrough analysis of agents' temporal-spatial relationships.\nRequests of Different Priorities. The dependency lens\nalso reveals something unique about simulation compared to\ncommon LLM services like chatbots: there are long critical\npaths in the task of simulation, consisting of a chain of"}, {"title": "3 DESIGN OF AI METROPOLIS", "content": "Motivated by observations described in \u00a72.2, we design\nAI Metropolis, an optimized simulation engine that serves\nas middleware between the developer-defined world and\nagents and the LLM serving engine, efficiently managing\nstate updates and scheduling LLM queries. By allowing\nagents to progress at varying speeds based on their LLM\ncall loads, AI Metropolis eliminates the need for frequent\nglobal synchronization, reducing false dependencies and\nmaximizing parallelism. Algorithm 3 provides an overview\nof the new scheduling workflow adopted by AI Metropolis,\ncontrasting it with the traditional time step synchronized\nscheduling shown in Algorithm 1."}, {"title": "3.1 Overview", "content": "Below we explain the essential terms in AI Metropolis:\n\u2022 Blocked: An agent A becomes blocked if it has to wait\nfor another agent B to finish the current step before it\ncan proceed, ensuring temporal causality and simula-\ntion correctness. This is formally defined in \u00a73.2.\n\u2022 Coupled: Agents A and B, become coupled if they\nare sufficiently close, interact with each other, and thus\nmust proceed together. This is formally defined in \u00a73.2.\n\u2022 Cluster: A cluster is a group of coupled agents at the\nsame step. Each agent can independently issue requests\nto LLMs within its proceed function. However, the\nentire group needs to synchronize at the end of the step\nto resolve potential conflicts and avoid dependency\nviolations as described in \u00a73.2.\n\u2022 Worker: A worker is a process that handles one clus-\nter, to proceed one step at a time. Within the worker\nprocess, each agent in the cluster operates in its own\nthread to communicate with the LLM serving engine\nand process its tasks. Workers are independent pro-\ncesses without synchronization between them, and the\nnumber of workers can be adjusted based on available\nCPU resources.\n\u2022 Controller: The controller is the main process of\nthe simulation engine. After initializing the world,\nit periodically communicates with workers through\ntwo queues: it prepares tasks for workers via the\nready_queue and confirms the completion of clusters\nfrom workers through the ack_queue.\nDuring a simulation, workers continuously pull clusters\nfrom the ready_queue. After finishing the step for a cluster,\na worker updates the dependency graph stored in a database\nfor all agents in the cluster and then places the completed\ncluster in the ack_queue as a completion confirmation. Si-\nmultaneously, the controller continuously pulls notifications\nfrom the ack_queue. Each time it processes a confirma-\ntion, it queries the dependency graph to filter out the agents\nthat are not blocked, creating new ready clusters out of\nthe ready agents and placing them into the ready_queue\npromptly. This workflow allows agents to advance steps\nahead of others as long as dependency permits. Notably,\nboth the ready_queue and ack_queue are priority queues that\nautomatically sort tasks based on their associated steps.\nWe discuss the the dependency tracking mechanism in \u00a73.2,\nthe spatiotemporal dependency graph realizing the mecha-\nnism in \u00a73.3, agent clustering in \u00a73.4, priority scheduling in\n\u00a73.5, and key implementation details in \u00a73.6."}, {"title": "3.2 Dependency Tracking", "content": "As described in \u00a72.2 and shown in Figure 2, temporal causal-\nity introduces the dependence of tasks in a simulation. Using\nthe language of computer systems, temporal causality cre-\nates an order of a set of reads and writes on a shared memory.\nAt the beginning of each step, agents read different parts"}, {"title": "3.3 Spatiotemporal Dependency Graph", "content": "The rules defined in \u00a73.2 define a specialized dependency\ngraph that tracks dependencies between agents. Each node\nin the graph represents an agent, containing its temporal\n(time step) and spatial (coordinates on the map) information.\nAn edge A \u2192 B indicates that agent B is currently blocked\nby agent A, while A \u2192 B signifies that agents A and B are\ncoupled as shown in Figure 3.\nOur system maintains this graph in an in-memory database,\nand whenever a worker advances a cluster to the next step,\nit applies the rules to re-examine the relationships of each\nagent in the cluster with any other relevant agents. Any\nchanges are then written to the database. The examination\nand graph updates are encapsulated within a transaction\nto ensure data consistency. Afterward, the worker process\nplaces a completion confirmation into the ack_queue.\nThe dependency graph will be utilized by the controller\nprocess for efficiently identifying the agents that are not\nblocked, allowing it to release maximum parallelism."}, {"title": "3.4 Agent Clustering", "content": "When agents are close enough to each other, they perceive\neach other's actions committed in the last step. In other\nwords, they collectively read what they wrote in the last\nstep. They might also encounter write conflicts that must\nbe resolved by developer-specified rules; for example, two\nagents might both want to use the bathroom, but only one\ncan step in. These potential interactions couple them into a\ncluster that must proceed together. Whenever there are new\nready agents who are not blocked, the controller process\nruns geo_clustering to group coupled agents into clusters\nbased on the rule described in \u00a73.2. If none of the members\nof a cluster are blocked, the cluster is considered ready and\nwill be placed into the ready_queue. Using clusters as the\nminimal synchronized units, as opposed to synchronizing\nall agents as described in Algorithm 1, effectively reduces\nfalse dependencies and scheduling overhead."}, {"title": "3.5 Priority Scheduling", "content": "As motivated in \u00a72.2, allowing agents to process tasks as-\nsociated with different time steps simultaneously creates\nrequests of varying priorities. We found that the time step\nassociated with a request serves as a good measure of its\npriority. A write operation in a prior step can block many\nreads in subsequent steps; intuitively, the smaller the time\nstep, the more future actions it can potentially block. To\nenhance parallelism, we maintain both the ready_queue and\nack_queue as priority queues, prioritizing the execution of\ntasks from earlier steps. No preemption during LLM infer-\nence is applied as that might cause extra overhead (Sheng\net al., 2023b). We demonstrate the effectiveness of this\npriority scheduling in \u00a74.4."}, {"title": "3.6 System Implementation", "content": "In addition to the design choices around dependency man-\nagement, clustering, and priority scheduling that enable AI\nMetropolis to expose more parallelism from the simulation,\nit is also worth highlighting some of the design choices that\nmake AI Metropolis scalable:\nProper Mapping of Parallelism. Choosing the right pro-\ngramming abstraction for different tasks is critical to scala-\nbility. Al Metropolis employs threads for agents, as the need\nfor synchronization within clusters requires low-overhead\ncommunication. Processes are used for the controller and\nworkers to avoid the Python GIL and to enable scaling be-\nyond a single machine.\nLight and Fast Critical Path on the Controller Process.\nAll tasks on the critical path are implemented in C++ to\nreduce overhead and bypass the limitations of the Python\nGIL. Furthermore, heavy lifting, including complex agent\nprocessing logic and dependency graph updates, is offloaded\nto concurrent workers. This approach lightens the critical\npath for the controller process, minimizing workers' waiting\ntime for task allocation.\nScalable I/O. Except for ready_queue and ack_queue, all\ninter-process synchronization are handled through an in-\nmemory (Redis) database. This database also handles trans-\nactional updates for all simulation states and stores instru-\nmentation data, supporting automatic scalability beyond a\nsingle node.\nDecoupling Simulation Engine from LLM Serving En-\ngine. In AI Metropolis, only workers communicate with\nthe LLM serving engine through a thin shim layer, providing\neasy observability and scalability.\nWe implemented the core of AI Metropolis in about 1k\nlines of C++ and 2k lines of Python code. An additional 3k\nlines of Python code (excluding assets and prompts) were\nwritten to port the GenAgent simulation using our interfaces.\nThis is about 50% of the code compared to the original\nimplementation, achieving up to an order of magnitude\nspeed-up and promising scalability, as demonstrated in \u00a74."}, {"title": "4 EVALUATION", "content": "In the evaluation, we aim to answer the following questions:\n\u2022 Does AI Metropolis effectively enhance parallelism by\ntracking real dependencies, and how does this translate\nto shorter completion times?\n\u2022 Does AI Metropolis scale as the size of the simulated\nworld increases and the number of agents grows?\n\u2022 Given that AI Metropolis does not eliminate all false\ndependencies as illustrated in \u00a73.2, how well does it\nperform compared to the optimal solution?\nWe describe the experimental setup in \u00a74.1 and discuss the\nperformance results of full-day simulations at a small scale\nin \u00a74.2, which uses the same simulation settings reported\nin the GenAgent paper. We then examine the performance\ncomparisons as the size of the world increases and the num-\nber of agents grows to a thousand, assessing scalability in\n\u00a74.3. Finally, we conduct a performance breakdown in \u00a74.4\nto demonstrate the effectiveness of priority scheduling."}, {"title": "4.1 Methodology", "content": "Serving Engine. We use SGLang (Zheng et al., 2024)\n(v0.1.17) as the LLM serving engine, as it is not only\none of the state-of-the-art LLM serving engines but also\nlightweight and easy to instrument and modify. For consis-\ntent and stable performance benchmark results, we turned\noff its automatic common prefix caching feature; however,\nenabling the cache generally provides about a 20% through-\nput gain across all settings.\nModel and Hardware Platform. We benchmarked AI\nMetropolis with various models and GPUs to assess its effec-\ntiveness across different sizes and complexities. We chose\nstate-of-the-art open-source LLMs from the Meta Llama-3\ninstruct series (Meta, 2024). Community benchmarks (Chi-\nang et al., 2024) indicate that the smallest 8B model already\nsurpasses the ChatGPT-3.5 model used in the original GenA-\ngent paper, making it ideal for performance evaluation. We"}, {"title": "4.2 Full Day Simulation in SmallVille", "content": "We benchmark AI Metropolis using the same setup de-\nscribed in the GenAgent paper, which involves 25 agents\nwithin a world named SmallVille, a 100x140 grid, running\nfor a full simulation day.\nThe following experiment settings are used in benchmark:\n\u2022 single-thread employs a single thread to handle simu-\nlation states and issue LLM requests, as per the design\nadopted by the original implementation to simplify\nsimulator implementation. No parallelism is exposed\nfor LLM requests from different agents.\n\u2022 parallel-sync is a stronger baseline where all agents\noperate within the same time step and issue LLM re-\nquests independently, though global synchronization\nlimits achievable parallelism as discussed in \u00a7 2.2. We\nimplemented this baseline as a mode of AI Metropolis.\n\u2022 oracle represents the optimal dependency management\nsolution for comparison. This setting constructs an\noptimal dependency graph by analyzing the full trace\nand mining all necessary dependencies based on agent\ninteractions. For example, if two agents appear in each\nother's observation space, they synchronize before and\nafter the step to ensure temporal causality. This setting\nis unattainable in real systems and serves to illustrate\nthe potential improvement of dependency management.\nBy having an optimal dependency graph, the most\navailable parallelism will be released.\n\u2022 critical refers to the critical path of the simulation, ex-\ntracted from the optimal dependencies used in the or-\nacle setting. It identifies the path containing the most\nLLM input and output tokens, setting an lower bound\nof completion time regardless of available resources.\nFirst, AI Metropolis outperforms the single-thread and\nparallel-sync baselines by 2.38\u00d7 and 1.44\u00d7 on a single\nL4 GPU. As more GPUs are added, requiring greater par-\nallelism, the speedup increases to 3.25\u00d7 and 1.67\u00d7 re-\nspectively on 8 GPUs. We also measured the achieved"}, {"title": "4.3 Scaling up to a Thousand Agents", "content": "Given the limited research on accommodating hundreds of\nagents, we simulate a larger environment by concatenating\nmultiple SmallVilles into a single, large ville for benchmark-\ning. Agents in each segment replay different traces that we\ncollected independently, but they operate within the same\ntime and space. Since the concatenation approach intro-\nduces straightforward parallelism, rather than focusing on\nthe critical path, which is artificially shortened due to the\nlack of interaction between different parts of the large ville,\nwe introduce no-dependency as a more suitable lower bound\nfor completion time when scaling agents. In this setting, all\nLLM calls can be issued simultaneously, maximizing hard-\nware utilization. Moreover, for benchmark with a larger number of\nagents, we opted to focus on two specific intervals from an\nentire day's simulation: the busy\nhour and the\nquiet hour This setup shortens experiment time and highlights scaling\neffects across different workloads, where busy hours fea-\nture long conversations, and quiet hours are mainly routine\nactivities with less LLM queries as agents just wake up.\nThe benefits of AI Metropolis increase with increasing num-\nbers of agents. Figure 5 shows that AI Metropolis achieves\ncloser performance to oracle as the number of agents in-\ncrease: it achieves 90% of oracle on one GPU with 100\nagents, reaching parity with oracle at 500 agents. On 8\nGPUs, AI Metropolis improves from 53.1% to 97.0% of\noracle across settings. Speedups over single-thread and\nparallel-sync also scale with agent count, increasing from\n3.37x and 1.88\u00d7 at 25 agents to 19.5\u00d7 and 4.15\u00d7 at 500\nagents. Unlike single-thread, which cannot leverage paral-\nlelism, and parallel-sync, which suffers from costly synchro-\nnization, AI Metropolis utilizes parallelism more effectively,\nmaximizing speedup as agent count grows.\nAfter reaching peak speedup over parallel-sync at 500\nagents, the speedup plateaus, slightly decreasing to 3.94\u00d7 at\n1000 agents. This is because, as agent count grows relative\nto available computational resources, even less efficient de-\npendency management achieves adequate hardware utiliza-\ntion. Meanwhile, AI Metropolis reaches 97% of oracle per-\nformance, indicating that additional parallelism is less effec-\ntive. This trend appears earlier on a single L4 GPU, where\ncomputational resources are more limited. AI Metropolis\nachieves a maximum speedup of 1.87\u00d7 over parallel-sync\nat 100 agents, tapering to 1.60\u00d7 as AI Metropolis's perfor-\nmance approaches oracle\u2014from 90.9% at 100 agents to\n100% at 500 agents.\nSimilar trends appear in the quiet hour benchmark, as shown\nin Figure 5, with some variation: the lighter and less fre-\nquent LLM calls in the quiet hour benchmark reduce the syn-\nchronization overhead for parallel-sync, allowing more par-\nallelism. As a result, AI Metropolis shows a smaller speedup\nover parallel-sync with the same agents and GPUs-for\ninstance, 1.28\u00d7 in the 25-agent, 8-GPU setting, where\nachieved parallelism is 2.25 for parallel-sync and 2.80 for\nAI Metropolis. By comparison, the busy hour benchmark\nachieves parallelism values of 1.89 and 3.74 on the same\nsetting, respectively. Nevertheless, as the number of agents\nincreases, the speedup for AI Metropolis rises from 1.28\u00d7\nto 2.79x at 500 agents on 8 GPUs."}, {"title": "4.4 Priority Scheduling Breakdown", "content": "All the experiments discussed so far have priority scheduling\nenabled, where every request includes a step count, and\nrequests with smaller counts have higher execution priority.\nThis applies to the oracle baseline as well. We repeated the\nexperiment of busy hours with 500 agents on 4 and 8 L4\nGPUs for AI Metropolis and the oracle, but with priority\nscheduling turned off.\nAs shown in Table 1, priority scheduling does not signif-\nicantly impact performance of oracle because it already\nachieves sufficient parallelism, and its dependency graph\nis sparse as discussed in \u00a72.2, making priority less criti-\ncal for unlocking additional parallelism. In contrast, we\nobserved up to a 15.7% speedup for AI Metropolis with\npriority scheduling. This is because the conservative rules\ndefined in \u00a73.2 make agents falling behind to block others\nmore frequently. Priority scheduling reduces this block-\ning, allowing AI Metropolis to perform closer to the oracle.\nWith priority enabled, the average achieved parallelism in\nthe 500-agent, 8-GPU benchmark increases from 41.9 to\n50.9 for AI Metropolis, compared to a minor increase from\n69.4 to 69.9 for oracle."}, {"title": "5 RELATED WORK", "content": "LLM Agent Society Simulation and Multi-agents Collab-\noration. With increasing interest in LLM agents, several\nrecent frameworks, such as Camel (Li et al., 2023), Auto-\nGPT (Significant Gravitas, 2023), and OpenAI Swarm (Ope-\nnAI, 2024d), have emerged to simplify the development of\nmulti-LLM agent interactions. However, these frameworks\nprimarily provide interfaces for connecting LLM agents but\nlack a shared environment or state synchronization, making\nmulti-agent interactions more akin to microservices con-\nnected via remote procedure calls. In contrast, GenAgent\nand similar explorations (Wang et al., 2023a; Gong et al.,\n2023; Altera.AL et al., 2024) represent a different approach,\nwhich we call Al society. In these systems, agents exist\nwithin a virtual world, interacting with both each other and\nthe environment. However, this line of research typically\nemphasizes agent architectures and prompt engineering, of-\nten resulting in a lock-step simulation process for simplic-\nity, which can lead to slower simulations. AI Town (a16z\ninfra, 2023), although inspired by generative agents and\nresembling AI society frameworks, offers an open-source\nplatform where the environment is only minimally inter-\nactive for agents. Agent interactions are limited to simple\nrule-based operations, which constrains its versatility. AI\nMetropolis tackles the performance challenges inherent to\nAl society simulation, offering a solution that enhances\nefficiency without compromising the functionality.\nGeneral Multi-agent Simulation. While LLM agent sim-\nulation may appear superficially similar to general multi-\nagent simulations, such as those used in reinforcement learn-\ning and multi-agent processing (Emau et al., 2011), they\npresent fundamentally different scheduling challenges due\nto the high per-agent computational demands and signifi-\ncant workload imbalances inherent in LLM execution, as\ndiscussed in \u00a72.2. These unique demands necessitate spe-\ncialized scheduling strategies. AI Metropolis is designed to\noffer a user experience comparable to that provided by estab-\nlished reinforcement learning frameworks, while seamlessly\nmanaging the additional complexities behind the scenes.\nLLM Serving Optimizations. An active line of re-\nsearch  in LLM inference optimization has been making con-\ntinuous advancements in increasing throughput and reduc-\ning latency from various angles. These engines typically\nachieve optimal performance when there are ample requests\navailable for scheduling but generally do not address request\ndependencies. AI Metropolis complements these efficient\nengines by increasing parallelism within the simulation,\nthereby enhancing overall throughput. Since AI Metropolis\ndecouples the simulation engine from the serving engine,\nimprovements in serving engines directly boost simulation\nthroughput. While a substantial body of research exists on\ngeneral inference and serving, it remains largely orthogonal\nto our work. Some studies address dependencies among\nLLM requests, such as  for function call-\ning and  for LLM programs to achieve\nhigher parallelization. However, these approaches assume\nrigid dependencies, unlike AI Metropolis, which focuses on\nreducing false dependencies to further enhance efficiency."}, {"title": "6 DISCUSSION AND FUTURE WORK", "content": "Conservative or Speculative Execution. As discussed in\n\u00a73.2, AI Metropolis applies conservative rules to prevent\ncausality violation, which can extend the critical path and\nreduce overall parallelism. Nonetheless, as demonstrated\nin \u00a74, AI Metropolis achieves performance close to the\noracle setting in most cases, primarily due to effective prior-\nity scheduling. Although scenarios exist where additional\nparallelism could enhance performance, the performance\ngap between AI Metropolis and the oracle remains minor,\nas evidenced by the quiet hour benchmark in \u00a74.3. This\ngap, however, highlights opportunities to further improve\nAI Metropolis. Introducing speculative execution with race\ndetection could potentially bridge this gap, although do-\ning so may challenge the system's scalability principles-a\ndirection we leave open for future work.\nOffline and Interactive. While AI Metropolis currently\nfocuses on throughput for offline simulation, its core prin-\nciples including fine-grained dependency management and\npriority scheduling are also applicable to interactive environ-\nment like video games. The key difference between a real\ngame and a simulation lies in interactivity, which imposes\nlatency requirements. We view games like The Sims (Elec-\ntronic Arts, 2000) as hybrids of interactive and offline simu-\nlation components. The part with which the player interacts\nmust have low latency for real-time responsiveness, while\nbackground agents should operate in a simulation manner\nto exhibit realistic social behaviors. An important future\nwork of AI Metropolis is to support such hybrid deployment,\nbalancing request prioritization to reduce latency in inter-\nactive components and optimize throughput in background\nsimulations.\nApplications of AI Metropolis. Although this paper high-\nlights GenAgent as the primary use case, AI Metropolis\noffers broad applicability across various domains: 1) As a\npioneering framework for simulating social behaviors with\nLLMs, GenAgent has influenced many studies, expanding\nits impact. An efficient engine for GenAgent reaches a wide\naudience, as the observed imbalanced load and sparse depen-\ndency patterns are common across human-like simulations.\nAdditionally, the dependency rules in AI Metropolis can be\nadapted to other environments by adjusting parameters like\nperception radius and movement speed to suit different dy-\nnamics. 2) While our derivations focus on temporal-spatial\nrelationships in Euclidean space, they can extend to non-\nEuclidean spaces, such as social networks, highlighting AI\nMetropolis's flexibility for diverse simulations needing de-\npendency management.\nOnline APIs and Local Models. While proprietary APIs\nlike GPT-4 and Claude 3 lead\nin performance, open-source models are on the rise.\nAI Metropolis supports local model serving with optimized\ndependency management for faster, cost-effective simula-\ntions, yet remains compatible with online APIs, enhancing\nparallelism and simplifying state management for users."}, {"title": "A RULES DERIVATION", "content": "At any point during the out-of-order simulation", "state": "n$ \\forall$ agents A", "need": "n$dist(A", "if": "n$dist(A"}, {"need": "n$dist(A, B) \u2013 max\\_vel$\n$> radius_p + (Step_A - Step_B) \\times max\\_vel$\nTherefore, on the other side, A got blocked by B if\n$dist(A, B) < (Step_A - Step_B + 1) \\times max\\_vel$\n$+ radius_p$\n\u2022 Assume steps $Step_A < Step_B$. There should be\n$dist(A', B) > (Step_B \u2013 Step_A \u2212 1 \u2212 1) \\times max\\_vel$\n$+ radius_p$\nSince"}]}