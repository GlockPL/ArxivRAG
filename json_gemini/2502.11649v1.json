{"title": "Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation", "authors": ["Amin Qasmi", "Usman Naseem", "Mehwish Nasim"], "abstract": "We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychology such as confirmation bias, resource constraints, and influence penalties. Our simulation features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating messages that propagate or counter misinformation. This framework integrates resource optimisation into the agents' decision-making process. Our findings demonstrate that while higher confirmation bias strengthens opinion alignment within groups, it also exacerbates overall polarisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs. Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and diminished long-term influence.", "sections": [{"title": "Introduction and Background", "content": "The study of opinion dynamics, originating from efforts to understand how individuals modify their views under social influence (Kelman, 1958, 1961), has broad applications in areas such as public health campaigns, conflict resolution, and combating misinformation. Within social networks, opinions spread and evolve, influenced by various factors including peer interactions (Kandel, 1986), media exposure (Zucker, 1978), and group dynamics (Friedkin and Johnsen, 2011). Developing accurate models of these processes is essential not only for predicting trends like opinion polarisation (Tan et al., 2024) or consensus formation but also for crafting targeted interventions to mitigate harmful effects, such as the spread of misinformation or societal fragmentation (Hegselmann and Krause, 2015). Agent-based models (ABMs), simulating interactions among individual agents as proxies for humans, serve as valuable tools for examining the emergent properties of opinion dynamics. These models offer robust frameworks for analysing complex scenarios (Deffuant et al., 2002; Mathias et al., 2016), evaluating strategies to reduce negative consequences, and potentially fostering constructive social influence by integrating explicit cognitive mechanisms into opinion-updating processes.\nThis work investigates how Large Language Models (LLMs) can model human-like opinion dynamics and influence propagation within social networks. Traditional ABMs often employ simplified rules that fail to capture the complexity of human communicative strategies. To address this limitation, we introduce a novel non-cooperative game framework where adversarial LLMs, one spreading misinformation and the other countering it, interact. This work introduces a non-cooperative game where LLM agents engage in adversarial interactions to model misinformation spread and countering. Unlike prior studies (Wang et al., 2025; Chuang et al., 2024) on passive opinion evolution and nudging, it focuses on resource-constrained influence operations and debunking effectiveness in competitive environments.\nWe pose the following research questions:\nRQ1 What are the emergent behaviors in networks of agents influenced by competing LLMs?\nRQ2 How does the competition between LLM agents shape the evolution of opinion clusters over time, also known as echo-chambers?"}, {"title": "Methodology", "content": "We use LLMs to simulate the propagation and debunking of misinformation on social media within a non-cooperative game framework.\nScenario: Our scenario is strategically designed to reflect the asymmetric nature of contested information environments, specifically highlighting the challenges faced by the \"Blue team\" (i.e., those countering misinformation). This framework mirrors adversarial dynamics commonly modelled in serious games or wargames, particularly in the context of cybersecurity. The \"Red Team\" and \"Blue Team\" construct (Paul, Christopher and Connable, Ben and Welch Jonathan and Rosenblatt, Nate and McNeive, Jim, 2021), familiar in cybersecurity practices (as detailed in NIST's Glossary (National Institute of Standards and Technology (NIST), 2015)), is adapted to our simulation. The system comprises two LLM-based agents with opposing objectives: the Red Agent (adversarial agent) disseminates misinformation, while the Blue Agent (debunking agent) counters it and aims to restore trust. These agents operate within a directed network of neutral agents, termed Green Nodes, representing individuals in a population. Figure 1 shows the structure of our non-cooperative game.\nOperationalisation We have the following agents in the simulation:\nAgent Roles and Mechanics: The simulation incorporates the following agent roles:\nRed Agent: The Red Agent aims to amplify doubt and confusion by generating misinformation messages of varying potency. Messages of higher potency incur penalties through rejection, reflecting real-world scenarios where informed populations are sceptical of, and less susceptible to, high-strength misinformation.\nBlue Agent: The Blue Agent counters the misinformation spread by the Red Agent, but operates under a resource constraint. High-potency counter-messages incur a greater resource cost. Therefore, the Blue Agent must strategically manage its available resources.\nJudge Agent: To prevent agents from assigning arbitrarily high potencies to their messages, a Judge Agent assigns potencies based on specified criteria:\n\u2022 Clarity:Is the message clear and well-articulated?\n\u2022 Evidence: Does the message provide credible evidence or logical reasoning?\n\u2022 Relevance: Does the message effectively address the misinformation?\n\u2022 Impact: Does the message effectively persuade or influence the target audience?\nSimulation settings: The simulation begins with n nodes, of which x are initially aligned with the Red Agent's misinformation (pro-conspiracy), y < x are aligned against it (anti-conspiracy), and the remaining z = n - x - y are neutral. Both Red and Blue Agents generate messages, the potencies of which are determined by the Judge Agent.\nOpinion Modeling: We use the Bounded Confidence Model (BCM) (Mathias et al., 2016) to simulate opinion dynamics. In the BCM, a node updates its opinion if the difference between its opinion and that of a neighbouring node is less than a threshold (the confirmation bias value, \u03bc). The opinion update condition and formula are summarised below:\nfor all $n \\in N$ do\nfor all $m \\in Neighbours(n)$ do\nif $|O_m - O_n| < \\mu$ then\n$O_m \\leftarrow O_m + \\mu(O_m \u2013 O_n)$\nend if\nend for\nend for\nwhere N is the set of all nodes, $O_n$ is the opinion of node n, $O_m$ is the opinion of neighbour m, and Neighbours(n) returns the neighbours of n. Opinions range between [-1, 1]. Nodes with opinions less than -0.5 are considered aligned with the Blue Agent, those greater than 0.5 with the Red Agent, and those in between are neutral.\nTopics Classification: Topics for misinformation include serious debates and popular conspiracy theories (e.g., \"The Earth is Flat\") as well as more frivolous claims (e.g., \"The Moon is made of cake\").\nModels: Our study employs GPT-4O and 40-MINI as judges (Hurst et al., 2024; OpenAI, 2024). Experiment A compares Mixtral-8x7B-Instruct with Gemma-2-9b (Jiang et al., 2024; Team et al., 2024b), while experiment B evaluates"}, {"title": "Simulations and Evaluation", "content": "We conducted simulations of 100 rounds for each experiment. The network configuration remained constant across all simulations: a directed network of 50 nodes, with 40% (20 nodes) initially aligned with the Blue Agent (anti-conspiracy) and 20% (10 nodes) aligned with the Red Agent (pro-conspiracy). This initial distribution reflects the observation that conspiracy theorists typically represent a minority within social media populations. The Blue Agent was initialised with a resource value of 100 and an influence factor (base potency) of 0.6, while the Red Agent's influence factor was set to 0.5. We explored three Bounded Confidence Model (BCM) thresholds (\u03bc): 0.3, 0.7, and 0.9.\nTo investigate the impact of increased resource investment in debunking, we conducted a further set of experiments (using \u03bc = 0.9) where the Blue Agent generated highly potent messages during the first 20 rounds (10 messages per agent per round). During these initial rounds, the Blue Agent's messages had their base potency scaled by a factor of 1.2, with the final assigned potency capped at 1.0 (i.e., min(potency \u00d7 1.2, 1.0)). This simulated a \"high-resource\" debunking strategy.\nAll simulations were run on a local machine having an Intel(R) Core(TM) i7-1355U 13th Gen CPU and 32 Gigabytes of RAM. While the machine was equipped with an integrated GPU (Intel Iris Xe Graphics (shared 15.8GB memory)), no dedicated GPU acceleration was used in this project. The LLM models were used with the following parameters: For all models, temperature was kept at 0.5, top_p at 1.0, and max_tokens were set to 100. The results of these simulations are presented in the following section.\nMetrics: We employ several metrics to analyse the results of our simulations.\nPolarisation refers to the division of individuals or groups into opposing factions, leading to the reinforcement of extreme views within social networks. Network polarisation (P) is calculated (Chitra and Musco, 2020) as:\n$P = \\frac{1}{N} \\sum_{n \\in V}(O_n - \\overline{O})^2$ (1)\nwhere:\n\u2022 N is the total number of nodes in the network.\n\u2022 V is the set of all nodes.\n\u2022 $O_n$ is the opinion of node n.\n\u2022 $\\overline{O}$ is the mean opinion across all nodes.\nJudge's Agreement: To assess the consistency of the potencies assigned by Judge Agent, we used two Judge Agents in each round to independently generate potency values for the same messages. All generated potency values were recorded, and inter-rater agreement was measured using two metrics: Intraclass Correlation Coefficient (ICC) and Krippendorff's Alpha. ICC values range from 0 to 1, where values closer to 1 indicate strong agreement and values closer to 0 indicate poor agreement. Krippendorff's Alpha ranges from -1 to 1, with higher values indicating stronger agreement."}, {"title": "Results and Discussion", "content": "RQ1 explores how adversarial interactions between competing LLM-driven agents (representing misinformation and counter-misinformation) influence collective opinion dynamics. Specifically, we examine the role of cognitive biases (represented by the BCM threshold) in shaping the stability and evolution of opinion alignment. Figure 2 summarises our findings.\nFigure 2(a) illustrates the evolution of agent alignment over time for different BCM thresholds. At a low threshold (\u00b5 = 0.3), the opinion landscape becomes highly fragmented, with the Blue Agent's alignment stagnating at its initial 40%. In contrast, at moderate and high thresholds (\u03bc = 0.7 and \u03bc = 0.9), the Blue Agent's alignment increases to 45% and 50%, respectively. The Red Agent's alignment also increases with higher thresholds, rising from 20% to 25%, 35%, and 38% at thresholds of 0.3, 0.7, and 0.9, respectively. This demonstrates that, without resource constraints, accumulating support is more feasible. Furthermore, the early rounds of interaction appear crucial in shaping long-term opinion trajectories, highlighting the strategic importance of early influence.\nFigure 2(c) shows the corresponding polarisation trends. A low threshold (\u03bc = 0.3) results in only a marginal increase in polarisation (~40%), while higher thresholds (\u03bc = 0.7 and \u00b5 = 0.9) lead to substantially higher polarisation levels (~70% and ~80%, respectively). These results align with the BCM update algorithm (Section 2) and the polarisation calculation (equa 1). With a small BCM threshold, agents only update their opinions if they are already closely aligned, resulting in multiple localised opinion clusters rather than a single consensus. Consequently, polarisation remains moderate as divergence occurs within sub-clusters. However, at high BCM thresholds, interactions occur more frequently across a broader range of opinions, amplifying extreme positions. As observed in Figure 2(a) (for \u03bc = 0.9), nearly 90% of agents become strongly aligned with either the Red or Blue Agent, reflecting this sharp increase in polarisation.\nRQ2 investigates optimal strategies for the Blue Agent to effectively counter misinformation under resource constraints. Specifically, we analyse the impact of an aggressive early-game approach, where high-potency debunking messages are deployed at a substantial resource cost. Due to the rapid resource depletion associated with this strategy, these simulations were limited to 50 rounds.\nFigure 2(b) shows that this aggressive strategy enabled the Blue Agent to surpass the 50% alignment threshold, reaching a peak of 52%. Importantly, all three experimental conditions (A, B, and C) exhibited higher maximum alignment compared to the previous strategies, suggesting that an initial surge of high-potency messages leads to a greater overall shift towards the Blue Agent's perspective.\nFigure 2(d) shows a sharp increase in polarisation during the first 20 rounds (corresponding to the high-resource debunking period), followed by a gradual convergence. This indicates that while an aggressive approach initially amplifies divisions, it eventually stabilises as the influence of misinformation diminishes. These findings highlight the trade-off between immediate impact and long-term sustainability in misinformation counter-strategies, emphasizing the importance of energy management in prolonged engagements."}, {"title": "Conclusions and Future Work", "content": "This study examines opinion polarization in a non-cooperative game with adversarial LLMs spreading and countering misinformation. Higher BCM thresholds enhance faction alignment but intensify societal polarization. We identify a trade-off between immediate impact and sustainability: high-impact interventions deplete resources quickly, while frequent interactions may deepen polarization. These findings inform LLM-driven influence operations and suggest future research on adaptive agents and real-world network integration."}, {"title": "Limitations", "content": "The study is based on simulated interactions rather than real-world datasets from social media or online discourse. Validating findings with empirical data would enhance their applicability. The study relies on the BCM, which, while effective, does not capture more complex psychological and social dynamics influencing opinion formation, such as emotional contagion, identity-based biases, or network homophily."}, {"title": "Ethical Statement", "content": "This study, involving the simulated generation of misinformation and counter-misinformation, necessitates careful ethical considerations. To prevent potential misuse, the specific prompts used to generate misinformation via LLMs cannot be disclosed. Disclosure could inadvertently facilitate real-world misinformation spread. Mitigation strategies included containing all generated content within a closed experimental environment, focusing the research objective on analysis and countermeasure development (not propagation), and ensuring that any released findings emphasise generalisable insights rather than specific prompt engineering techniques. We underscore that responsible misinformation modelling research is paramount, ensuring that the development of countermeasures does not contribute to the problem itself."}]}