{"title": "ACE: ALL-ROUND CREATOR AND EDITOR FOLLOWING INSTRUCTIONS VIA DIFFUSION TRANSFORMER", "authors": ["Zhen Han", "Zeyinzi Jiang", "Yulin Pan", "Jingfeng Zhang", "Chaojie Mao", "Chenwei Xie", "Yu Liu", "Jingren Zhou"], "abstract": "Diffusion models have emerged as a powerful generative technology and have been found to be applicable in various scenarios. Most existing foundational diffusion models are primarily designed for text-guided visual generation and do not support multi-modal conditions, which are essential for many visual editing tasks. This limitation prevents these foundational diffusion models from serving as a unified model in the field of visual generation, like GPT-4 in the natural language processing field. In this work, we propose ACE, an All-round Creator and Editor, which achieves comparable performance compared to those expert models in a wide range of visual generation tasks. To achieve this goal, we first introduce a unified condition format termed Long-context Condition Unit (LCU), and propose a novel Transformer-based diffusion model that uses LCU as input, aiming for joint training across various generation and editing tasks. Furthermore, we propose an efficient data collection approach to address the issue of the absence of available training data. It involves acquiring pairwise images with synthesis-based or clustering-based pipelines and supplying these pairs with accurate textual instructions by leveraging a fine-tuned multi-modal large language model. To comprehensively evaluate the performance of our model, we establish a benchmark of manually annotated pairs data across a variety of visual generation tasks. The extensive experimental results demonstrate the superiority of our model in visual generation fields. Thanks to the all-in-one capabilities of our model, we can easily build a multi-modal chat system that responds to any interactive request for image creation using a single model to serve as the backend, avoiding the cumbersome pipeline typically employed in visual agents.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, foundational generative models have made groundbreaking progress in natural language processing (NLP) (Anil et al., 2023; Anthropic, 2023a;b; Ouyang et al., 2022). Conversational language models like ChatGPT (Brown et al., 2020; OpenAI, 2023b) offer a unified framework for addressing various NLP tasks through a prompt-guided approach. By employing a unified input-output structure, these models can achieve dynamic multi-turn interactions with users. Furthermore, by harnessing the knowledge of historical dialogues (Anthropic, 2024; OpenAI, 2024), they possess the capacity to comprehend intricate queries with greater nuance and depth. However, such unified architecture has not been fully explored in visual generation field. Existing foundational models of visual generation typically create images or videos from pure text, which is not compatible with most visual generation tasks, such as controllable image generation (Zhang et al., 2023b; Jiang et al., 2024) or image editing (Brooks et al., 2023). Thereby, specific visual generation tasks still require tailored tuning based on these foundational models, which is inflexible and inefficient. For this reason, the visual generative model has not yet become a powerful and unified productivity tool in various application scenarios like large language models (LLMs) (Abdin et al., 2024; Dubey et al., 2024; Bai et al., 2023; Yang et al., 2024).\nOne major challenge of building an all-in-one visual generation model lies in the diversity of multi-modal input formats and the variety of supported generation tasks. To address this, we design a unified framework using a Diffusion Transformer generation model that accommodates a wide range of inputs and tasks, empowering it to serve as an All-round Creator and Editor, which we refer to as ACE. First, we analyze the condition inputs of most visual generation tasks, and define Condition Unit (CU), which establishes a unified input paradigm consisting of core elements such as image, mask, and textual instruction. Second, for those CUs containing multiple images, we introduce Image Indicator Embedding to ensure the order of the images mentioned in instruction matches image sequence within the CUs. Besides, we imply 3d position embedding instead of 2d spatial-level position embedding on the image sequence, allowing for better exploring the relationships among conditional images. Third, we concatenate the current CU with historical information from previous generation rounds to construct the Long-context Condition Unit (LCU). By leveraging this chain of generation information, we expect the model to better understand the user's request and create the desired image. As depicted in Fig. 1, ACE supports a range of generating and editing capabilities, allowing it to accomplish complex and precise generation tasks through multi-turn instructions.\nTo address the issue of the absence of available training data for various visual generation tasks, we establish a meticulous data collection and processing workflow to collect high-quality structured CU data at a scale of 0.7 billion. For visual conditions, we collect image pairs by synthesizing images from source images or by pairing images from large-scale databases. The former utilizes powerful open-source models to edit images to meet specific requirements, such as changing styles (Han et al., 2024) or adding objects (Pan et al., 2024), while the latter involves clustering and grouping images from extensive databases to provide sufficient real data, thereby minimizing the risk of overfitting to the synthesized data distribution. For textual instructions, we first manually construct instructions for diverse tasks by building templates or requesting LLMs, then optimize the instruction construction process by training an end-to-end instruction-labeling multi-modal large language model (MLLM) (Chen et al., 2024), thereby enriching the diversity of the text instructions.\nOur ACE provides more comprehensive coverage of tasks on a single model compared to previous approaches. Therefore, to thoroughly evaluate the performance of our generation model, we construct an evaluation benchmark that encompasses the main tasks. This benchmark incorporates inputs sourced from both the real world and model-generated data, supporting global and local editing tasks. It is larger in scale and broader in scope compared to previous benchmarks (Sheynin et al., 2024; Zhang et al., 2023a). We conduct a user study to subjectively assess the quality of images generated by our method and the adherence to instructions, revealing that our approach generally aligns more closely with human perception across the majority of tasks. We summarize our main contributions as follows:\n\u2022 We propose ACE, a unified foundational model framework that supports a wide range of visual generation tasks. To our knowledge, this is the most comprehensive diffusion generation model to date in terms of task coverage.\n\u2022 By defining the CU for unifying multi-modal inputs across different tasks and incorporating long-context CU, we introduce historical contextual information into visual generation tasks, paving the way for ChatGPT-like dialog systems in visual generation.\n\u2022 We design specific data construction pipelines for various tasks to enhance the quality and efficiency of data collection, and we ensure the richness of multi-modal data through MLLM fine-tuning for automated instruction labeling.\n\u2022 We establish a more comprehensive evaluation benchmark compared to previous ones, covering the most known visual generation tasks. Evaluation results indicate that ACE demonstrates notable competitiveness in specialized models while also exhibiting strong generalization capabilities across a broader range of open tasks."}, {"title": "2 ALL-ROUND CREATOR AND EDITOR", "content": "ACE is an image creation and editing model based on the Diffusion Transformer that follows textual instructions. It establishes a unified framework that covers a wide range of tasks through the definition of standard input paradigm and strategy for aligning multi-modal information. With this exquisite design, the model is capable of handling various single tasks, multi-turn tasks, and long-context tasks with historical information."}, {"title": "2.1 PROBLEM DEFINITION", "content": ""}, {"title": "2.1.1 TASKS", "content": "When it comes to generation and editing, the input condition information varies significantly depending on the specific task types. This encompasses a diverse range of forms, including textual instructions, conditioning images in controllable generation, masks used in region editing, and images in guided generation, among others. We analyze and categorize these conditions from textual and visual modalities respectively: (i) Textual modality: we refer to all types of textual conditions as instructions and categorize them into Generating-based Instructions and Editing-based Instructions, depending on whether they describe the content of the generated image directly or the difference from the input visual cues; (ii) Visual modality: we categorize all generation tasks into 8 basic types, as shown in Fig. 2.\n\u2022 Text-guided Generation. It only uses generating-based text prompt as a condition to create images, and none of the visual cues are adopted.\n\u2022 Low-level Visual Analysis. It extracts low-level visual features from input images, such as edge maps or segmentation maps. One source image and editing-based instruction are required in the task to accomplish creation.\n\u2022 Controllable Generation. It is the inverse task of Low-level Visual Analysis, which creates vivid images based on given conditions, e.g., edge map, contour image, doodle image, scribble image, depth map, segmentation map, low-resolution image, etc.\n\u2022 Semantic Editing. It aims to modify some semantic attributes of an input image by providing editing instructions, such as altering the style of an image or modifying the facial attributes of a character.\n\u2022 Element Editing. It focuses on adding, deleting, or replacing a specific subject in the image while keeping other elements unchanged.\n\u2022 Repainting. It erases and repaints partial image content of input image indicated by given mask and instruction.\n\u2022 Layer Editing. It decomposes an input image into different layers, each of which contains a subject or background, or reversely fuses different layers.\n\u2022 Reference Generation. It generates an image based on one or more reference images, analyzing the common elements among them and presenting these elements in the generated image."}, {"title": "2.1.2 INPUT PARADIGM", "content": "A significant obstacle to implementing different types of generation and editing task requests within one framework lies in the diverse input condition formats of tasks. To address this issue, we design a unified input paradigm defined as Conditional Unit (CU) that fits as many tasks as possible. The CUs composed of a textual instruction T that describes the generation requirements, along with visual information V, where V consists of a set of images I that can be defined as I = () (if there are no source image) or I = {I1, I2, . . . , IN } (if there are source images) and corresponding masks M = {M1, M2, . . ., MN }. When there is no specific mask, M is set to a blank image. The overall formulation of the CU is as follows:\n\\(CU = \\{T, V\\}, V = \\{[I^1; M^1], [I^2; M^2], . . . , [I^N; M^N]\\}\\),\nwhere a channel-wise connection operation is performed between corresponding I and M, N represents the total number of visual information inputs for this task.\nFurthermore, to better address the demands of complex long-context generation and editing, historical information can be optionally integrated into CU, which is formulated as:\n\\(LCU_i = \\{\\{T_{i-m}, T_{i-m+1},..., T_i\\}, \\{V_{i-m}, V_{i-m+1},..., V_i\\}\\}\\)\nwhere m denotes the maximum number of rounds of historical knowledge introduced in the current request. LCU is a Long-context Condition Unit used to generate desired content for the i-th request."}, {"title": "2.2 ARCHITECTURE", "content": "In this section, we introduce a unified visual generation framework that can perform all visual generation tasks within a single model, and incorporate long-context conditions to enhance comprehension. As illustrated in Fig. 3a, the overall framework is built based on a Diffusion Transformer model (Vaswani et al., 2017; Peebles & Xie, 2023), and integrated with three novel components to achieve unified generation: Condition Tokenizing, Image Indicator Embedding, and Long-context Attention Block. We will provide a detailed description of them below."}, {"title": "Condition Tokenizing.", "content": "Considering an LCU that comprises M CUs, the model involves three entry points for each CU: a language model (T5) (Raffel et al., 2020) to encode textual instructions, a Variational Autoencoder (VAE) (Kingma & Welling, 2014) to compress reference image to latent representation, and a down-sampling module to resize mask to the shape of corresponding latent image. The latent image and its mask (an all-one mask if no mask is provided) are concatenated along the channel dimension. These image-mask pairs are then patchified into 1-dimensional visual token sequences Um,n,p, where m, n are indexes for CUs and visual information Vs in each CU, while p denotes the spatial index in patchified latent images. Similarly, textual instructions are encoded into 1-dimensional token sequences ym. After processing within each CU, we separately concatenate all visual token sequences and all textual token sequences to form a long-context sequence."}, {"title": "Image Indicator Embedding.", "content": "As illustrated as Fig. 3b, to indicate the image order in textual instructions and distinguish various input images, we encode some pre-defined textual tokens \"{image}, {image1}, ..., {imageN}\" into T5 embeddings as Image Indicator Embeddings (I-Emb). These indicator embeddings are added to the corresponding image embedding sequence and text embedding sequence, which is formulated as:\n\\(y_{m,n} = y_m + I\\text{-}Embm,n,\\)\n\\(u_{m,n,p} = u_{m,n,p} + I\\text{-}Embm,n.\\)\nIn this way, image indicator tokens in textual instructions and the corresponding images are implicitly associated."}, {"title": "Long-context Attention Block.", "content": "Given the long-context visual sequence, we first modulate it with the time step embedding (T-Emb), then incorporate a 3D Rotational Positional Encodings (ROPE) (Su et al., 2023) to differentiate between different spatial- and frame-level image embeddings. During the Long Context Self-Attention, all image embeddings of each CU at each spatial location, are equivalently and comprehensively interact with each other by \u00b5 = Attn(u', u'). Next, unlike the cross-attention layer of the conventional Diffusion Transformer model, where each visual token attends to all of the textual tokens, we implement cross-attention operation with each condition unit. That means image tokens in m-th CU will only attend to the textual tokens from the same CU. This can be formulated as:\n\\(\\hat{u}_{m,n} = Attn(\\mu_{m,n}, y_{m,n}).\\)\nThis ensures that, within the cross-attention layer, the text embeddings and image embeddings align on a frame-by-frame basis."}, {"title": "3 DATASETS", "content": ""}, {"title": "3.1 PAIR DATA COLLECTION", "content": "A critical challenge of training foundational visual generation model lies in how to acquire pairwise images for various tasks. In this section, we introduce two ways to efficiently build high-quality datasets for most of the generation and editing tasks: (i) Synthesizing from source image: thanks to the rapid development in the field of visual generation, there have been many of powerful open-source models designed to solve one specific problem. Leveraging these powerful single-point technologies, we could synthesis plenty of image pairs for lots of generation and editing tasks, such as controllable generation, style editing, object editing, and so on. (ii) Pairing from massive databases: though the synthesis-based method is efficient and straightforward in acquiring pairwise data. However, It still possesses two drawbacks. First, some editing problems have not been fully explored, and there are no powerful open-source models available for these tasks. Second, using only synthetic data can easily cause over-fitting and reduce the quality of generated images. Therefore, it is essential to provide sufficient real data to address the aforementioned drawbacks. We propose a hierarchically aggregating pipeline for pairing content-related images from massive databases to build pairs of data for training. We first extract semantic features using SigLIP (Zhai et al., 2023) from large-scale datasets (e.g., LAION-5B (Schuhmann et al., 2022), OpenImages (OpenImage, 2023), and our private datasets). Then leveraging K-means clustering technology, coarse-grained clustering is implemented to divide all images into tens of thousands of clusters. Within each cluster, we implement a two-turn union-find algorithm to achieve fine-grained image aggregation. The first turn is based on the SigLIP feature and the second turn uses a similarity score tailored for specific tasks. For instance, we calculate the face similarity score for the facial editing task and the object consistency score for the general editing task. Finally, we collect all possible pairs from each disjoint set and implement cleaning strategies to filter high-quality pairs. Benefiting from these two automatic pipelines, we construct a large-scale training dataset that consists of nearly 0.7 billion image pairs, covering 8 basic types of tasks, multi-turn and long-context generation. We depict its distribution in Fig. 6 and provide a detailed description of the specific data construction methods for each task, please refer to appendix B."}, {"title": "3.2 INSTRUCTIONS", "content": "In addition to collecting image pairs, it is essential to label clear natural language instructions that indicate how to transform one image into another. Compared to the caption generation commonly used in text-to-image task, instruction labeling is generally more challenging, as it requires analyzing not only the semantics of individual images, but also the discrepancies across multiple images. We employ both Template-based and MLLM-based methods to tackle this challenge. Template-based method constructs instruction templates for specific vision tasks by leveraging human knowledge priors. However, the instructions generated by this method lack diversity, which can lead to significant overfitting problems. MLLM-based method generates unique instructions for each given editing pair, leveraging off-the-shelf MLLMs. Nonetheless, current MLLMs exhibit limitations in producing precise instructions for editing tasks involving non-natural images, such as depth-controlled image generation and image segmentation. Thus, we combine these two methods and design an effective strategy to mitigate the aforementioned drawbacks. For tasks that contain non-natural images, we utilize a template-based method to generate instruction templates. These templates are then combined with the generated captions to produce the final instructions. To address the issue of insufficient diversity, we employ LLMs to reformulate instructions multiple times, and tune prompts to ensure that each rewritten version is distinct from all preceding instructions. For tasks that contain natural images, we employ an MLLM to predict the differences and commonalities between the images in the input pair. Then an LLM is used to generate instructions focusing on semantic distinctions according to the analysis of the differences and commonalities. Further, the collected instructions generated by these two methods undergo human annotation and correction. The revised instructions are used for fine-tuning an open-source MLLM, enabling it to predict instructions for any given image pair. Specifically, we collect a dataset of approximately 800,000 curated instructions and train an Instruction Captioner by fine-tuning the InternVL2-26B (Chen et al., 2024)."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 BENCHMARKS AND METRICS", "content": "Existing Benchmarks. We first evaluate on the commonly used benchmark MagicBrush (Zhang et al., 2023a). It contains an overall 1,053 edit turns and 535 edit sessions for single-turn and multi-turn image editing respectively. It compares the output images with groundtruth images and the provided target text descriptions. Following the setting proposed in the MagicBrush benchmark, we calculate the L1 distance, L2 distance, CLIP (Radford et al., 2021) similarity, DINO (Liu et al., 2023a) similarity between the generated image and groundtruth image, and CLIP similarity between the generated image and textual prompt. We also evaluate the Emu Edit benchmark (Sheynin et al., 2024), please see appendix E for details.\nACE Benchmark. To thoroughly evaluate the performance of various visual generation tasks, we build a benchmark dataset that covers all types of tasks the aforementioned. ACE benchmark consists of both real and generated images. The real images are primarily sourced from the MS-COCO (Lin et al., 2014) dataset and the generated images are created by Midjourney (Midjourney, 2023), using prompts obtained from JourneyDB (Sun et al., 2023a). For each task type, we manually craft instructions and masks to closely resemble actual user input patterns, reaching a total of 12,000 entries. The detailed statistics of ACE benchmark can be found in Fig. 24. We evaluate image quality and prompt following scores through a user study. The image quality score assesses the aesthetic quality of the generated images, while the prompt following score measures how well the images align with the provided textual instructions."}, {"title": "4.2 QUALITATIVE EVALUATION", "content": "In our qualitative evaluation, we present a comparison of our method with SOTA approaches across various tasks, including ControlNet (Zhang et al., 2023b), InstructPix2Pix (Brooks et al., 2023), MagicBrush (Zhang et al., 2023a), CosXL (StabilityAI, 2024), SEED-X Edit (Ge et al., 2024a), UltraEdit (Zhao et al., 2024), StyleBooth (Han et al., 2024), SDEdit (Meng et al., 2021), LORA (Hu et al., 2022), SD-Inpaint (AI, 2022b), LaMa (Suvorov et al., 2022), IP-Adapter (Ye et al., 2023), InstantID (Wang et al., 2024b), FaceChain (Liu et al., 2023b), AnyText (Tuo et al., 2023), UDiff-Text (Zhao & Lian, 2024). In Fig. 5, we present qualitative comparisons between our single ACE model and 16 other methods across 12 subtasks. Overall, our method not only addresses a diverse range of tasks but also performs superior compared to task-specific methods. Additionally, we also show some extra tasks that the comparison methods do not perform well in the last three lines. Please see appendix G, for more examples of qualitative evaluation."}, {"title": "4.3 QUANTITATIVE EVALUATION", "content": "Evaluation on Existing Benchmarks. We first compare our method with baselines on the MagicBrush benchmark. Results are present on Tab. 1. For single-turn image editing, ACE significantly outperforms other methods under an instruction-guided setting while demonstrating comparable performance under a description-guided setting. For each setting of multi-turn image editing, we first employ the same inference way as MagicBrush, performing independent and continuous edits on a single image. The results show that our approach has significant advantages. Furthermore, we construct a long sequence using the historical information from each editing round, achieving a certain improvement in performance compared to not using it. This also demonstrates the effectiveness of LCU and architecture design.\nEvaluation on ACE Benchmark. We conduct a comprehensive human evaluation using our benchmark to assess the performance of generated images, employing image scoring as the evaluation metric. Specifically, we score each image considering two aspects: prompt following and image quality. The prompt following metric measures the image compliance with text instructions or text descriptions, and is categorized into five levels. The image quality metric encompasses various aspects such as generated color, details, layout, and visual appeal, and is scored on a scale from 1 to 5. Considering the broad capabilities of our method, we compare it with several common approaches and some experts designed for specific tasks. We engaged 5 professional designers as evaluators to carry out these assessments. For each task, the data is evenly distributed among the evaluators in an anonymous manner, and scores are aggregated for analysis.\nAs shown in Tab. 2, we compare our approach across multiple global editing tasks and local editing tasks. The prompt following score and image quality score are presented together, separated by a \u201c/\u201d pattern. The bold numbers represent the best, and the underlined numbers indicate the second best. Our method achieves the highest prompt following scores in 7 of 12 global editing tasks and 8 of 10 local editing tasks, which demonstrates that ACE fully understands the intention of the instruction and is able to correctly generate an image that meets the instruction. Furthermore, ACE achieves the best image quality scores in 5 of 10 global editing tasks and 7 of 10 local editing tasks. These results indicate that ACE excels at generating high aesthetic images across various image editing tasks. Nonetheless, our method performs unsatisfactorily in certain tasks, such as general editing and style editing. One possible reason is that images generated by methods using larger models, such as those producing 1024-resolution images based on the SDXL model, are more preferred by evaluators compared to those produced by our model, which has a size of 0.6B parameters and an output resolution of around 512."}, {"title": "5 CONCLUSION", "content": "We propose ACE, a versatile foundational generative model that excels at creating images, and following instructions across a wide range of generative tasks. Users can specify their generation intentions through customized text prompts and image inputs. Furthermore, we advance the exploration of capabilities within interactive dialogue scenarios, marking a significant step forward in the processing of long contextual historical information in the field of visual generation. Our work aims to provide a comprehensive generative model for the public and professional designers, serving as a productivity enhancement tool to foster innovation and creativity."}, {"title": "A RELATED WORK", "content": "Visual generation, which takes multi-modal conditions (e.g., textual instruction and reference image) as input to generate creative image, has emerged as a popular research trend in recent years. As the basic task, text-guided image generation has undergone a significant development, marked by remarkable advancements in recent years. Many approaches (Nichol et al., 2022; Saharia et al., 2022; OpenAI, 2022; Rombach et al., 2022; StabilityAI, 2022; OpenAI, 2023a; Midjourney, 2023; Cloud, 2023; Zhang et al., 2021; Chen et al., 2023a; Esser et al., 2024; KOLORS, 2024; Li et al., 2024; FLUX, 2024) have been proposed and achieved impressive results in terms of both image quality and semantic fidelity. By incorporating low-level visual features as input, Huang et al. (2023) and Zhang et al. (2023b) pave the way for the initial forms of multi-modal controllable generation. Recently, some approaches (Mou et al., 2023; Zhao et al., 2023; Qin et al., 2023) have tried to use multiple visual features as conditions, facilitating the multi-modal controllable generation. By integrating fine-tuning technologies such as Ruiz et al. (2023); Hu et al. (2022), these approaches have further enabled the customization of diverse controllable generation applications. Another popular trend is image editing technology (Ye et al., 2023; Han et al., 2024; Wang et al., 2024b; Huang et al., 2024a; Wang et al., 2024a; Liu et al., 2023b; Tuo et al., 2023; Chen et al., 2023b; Pan et al., 2024; Wang et al., 2023; Xie et al., 2023; Sun et al., 2023b; Huang et al., 2024b; Bodur et al., 2024; Shi et al., 2024; Li et al., 2023; Meng et al., 2021), which focus on editing input images according to text prompts and preserving some identity such as person, scene, subject, or style. While the above models excel at generating image in one specific task or scenario, they have difficulty in extending to unseen tasks. To address the aforementioned challenges, some methods have been introduced to edit input images by following natural language instructions (Brooks et al., 2023; StabilityAI, 2024; Geng et al., 2024; Sheynin et al., 2024; Zhao et al., 2024; Ge et al., 2024b) which is more flexible to implement various tasks within a single model. However, a key bottleneck for these methods lies in the construction of high-quality instruction-paired datasets with annotated edits, which cause limited generalizability and suboptimal performance. In this paper, we focus on establishing a unified definition for multi-modal generation problems. Based on this definition, we aim to construct higher-quality, annotated data and instruction sets further to develop a unified foundational model for multimodal generation."}, {"title": "B DATASETS DETAIL", "content": "We use an internal dataset of 0.7 billion data pairs to train a foundational model for generation and editing. The supported tasks include 8 basic types consisting of 37 subtasks, as well as a multi-turn and long-context generation task. These tasks use textual instructions along with zero or more reference images for generating or editing image. The data distribution is depicted in Fig. 6a, and the absolute data scale is illustrated in Fig. 6b. In this section, we provide a detailed introduction to the data construction methods for various tasks."}, {"title": "B.1 TEXT-GUIDED GENERATION", "content": "We collect approximately 117 million images and use MLLM model to supplement captions for images, creating pair data for text-to-image tasks. Additionally, this portion of the data serves as an intermediary bridge in various generation and editing tasks, allowing the combination of different task instructions to obtain pairs from original images to target images."}, {"title": "B.2 LOW-LEVEL VISUAL ANALYSIS", "content": "Low-level Visual Analysis tasks involve analyzing and extracting various low-level visual features from a given image, like an edge map or segmentation map. These low-level visual features are typically employed as control signals in the controllable generation. We select 10 commonly used low-level features in the controllable generation, including segmentation map, depth map, human pose, mosaic image, blurry image, gray image, edge map, doodle image, contour image, and scribble image. The visual features extracted at global and local levels are illustrated in Fig. 7 and Fig. 8, respectively.\n\u2022 Image Segmentation involves extracting image spatial region information for different targets within an image. This is achieved by selecting and modifying specific areas for operations and editing in downstream tasks. We employ the Efficient SAM (Xiong et al., 2023) tool for marking different target areas within an image.\n\u2022 Depth Estimation indicates the relative distance information of different targets within an image. We use the Midas (Ranftl et al., 2022) algorithm to extract depth information.\n\u2022 Human-pose Estimation is employed for modeling the human body to obtain structured information about body posture. We make use of the RTMPose (Jiang et al., 2023) algorithm to extract information from images containing human figures, and posture information visualization is done using OpenPose's 17-point (Cao et al., 2021) modeling method.\n\u2022 Image Mosaic pixelates specific areas or the entire image to protect sensitive information.\n\u2022 Image Degradation is used to degrade the quality of an image to simulate the phenomenon of image distortion found in the real world. Following the practice of super-resolution algorithms (Wang et al., 2021), we add random noise to the input images.\n\u2022 Image Grayscale is typically done to facilitate the editing of an image's original colors downstream. We do this conversion directly using OpenCV's Grayscale function.\n\u2022 Edge Detection detects the edge information from the original image. We utilize the edge detection method named Canny (Canny, 1986) implemented by OpenCV.\n\u2022 Doodle Extraction is usually used to simulate relatively rough hand-drawn sketches by extracting the outline of objects and ignoring their details. We use the PIDNet (Xu et al., 2023) and SketchNet (Zhang et al., 2016a) to extract this information.\n\u2022 Contour Extraction is about delineating the outline of targets within an image, which simulates the drawing process of the image and is often used for secondary processing of images. We use the contour module from the informative drawing (Chan et al., 2022) for this information extraction.\n\u2022 Scribble Extraction involves retrieving the original line art information to capture the sketch-like form of the image. We utilize the anime-style module from informative drawings (Chan et al., 2022) to extract the relevant information."}, {"title": "B.3 CONTROLLABLE GENERATION", "content": "In the realm of vision-based generative foundation models, the ability to generate corresponding content using any provided prompts is commonly present. To further control aspects such as spatial layout, structure, or color in the generated images, additional conditional information is often incorporated as inputs to the model. We integrate various controllable condition-to-image tasks within a unified framework to accommodate different visual conditions. The control conditions include the visual features mentioned in the low-level visual analysis section. For training data, we employ pairs constituted by the aforementioned control conditions in Fig. 7 and regional control conditions in Fig. 8 obtained through low-level visual analysis, using the conditional part as inputs to the model to achieve pixel-precise image generation. For text guidance, we construct the instructions based on image captions with our proposed Instruction Captioner."}, {"title": "B.4 SEMANTIC EDITING", "content": "Semantic Editing aims to modify specific semantic attributes of an input image by providing detailed instructions. It involves facial editing, which aims to modify partial attributes of characters while preserving the overall identity, and style transforming, which aims to transform the image style to a specific artist theme guided by instruction while keeping content unchanged. Additionally, any other semantic editing requests that do not fall into these two categories are classified as general editing, e.g., changing the background scene of an image, adjusting a subject's posture, and modifying the camera view. We discuss the specifics according to the particular tasks."}, {"title": "B.4.1 FACIAL EDITING", "content": "Facial Editing encompasses both the transformation and preservation of facial attributes. Specifically", "parts": "aligned and misaligned facial data as shown in Fig. 10a. There are two novel processing workflows as shown in Fig. 9. (i) Aligned facial data. We generate pixel-aligned face data using generative models such as InstantID (Wang et al., 2024b) and combine it with GPT models to produce diverse prompts. Subsequently, we train multiple lightweight binary classification models to clean the generated data based on image quality, PPI score, aesthetic scores, and other metrics. Additionally, we extract facial features using ArcFace (Deng et al., 2019a) for similarity calculations, selecting high-matching data pairs with a similarity score exceeding 0.65. Once our model demonstrates the ability to maintain facial integrity, we initiate a self-iterative training process to generate higher quality data, as illustrated in Fig. 9a. (ii) Misaligned facial data. We first employ a face detection algorithm (Zhang et al., 2016b) to filter images containing only one face. Subsequently, we utilized facial features to perform K-means clustering, resulting in 10,000 clusters. Within each cluster, we conducted a second clustering using the union-find algorithm. Faces with a similarity score greater than 0.8 and less than 0.9 were linked to avoid grouping perfectly identical images. Finally, manual annotation and deduplication were performed"}]}