{"title": "LeKUBE: A Legal Knowledge Update BEnchmark", "authors": ["Changyue Wang", "Weihang Su", "Yiran Hu", "Qingyao Ai", "Yueyue Wu", "Cheng Luo", "Yiqun Liu", "Min Zhang", "Shaoping Ma"], "abstract": "Recent advances in Large Language Models (LLMs) have significantly shaped the applications of AI in multiple fields, including the studies of legal intelligence. Trained on extensive legal texts, including statutes and legal documents, the legal LLMs can capture important legal knowledge/concepts effectively and provide important support for downstream legal applications such as legal consultancy. Yet, the dynamic nature of legal statutes and interpretations also poses new challenges to the use of LLMs in legal applications. Particularly, how to update the legal knowledge of LLMs effectively and efficiently has become an important research problem in practice. Existing benchmarks for evaluating knowledge update methods are mostly designed for the open domain and cannot address the specific challenges of the legal domain, such as the nuanced application of new legal knowledge, the complexity and lengthiness of legal regulations, and the intricate nature of legal reasoning. To address this gap, we introduce the Legal Knowledge Update BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for legal LLMs across five dimensions. Specifically, we categorize the needs of knowledge updates in the legal domain with the help of legal professionals, and then hire annotators from law schools to create synthetic updates to the Chinese Criminal and Civil Code as well as sets of questions of which the answers would change after the updates. Through a comprehensive evaluation of state-of-the-art knowledge update methods, we reveal a notable gap between existing knowledge update methods and the unique needs of the legal domain, emphasizing the need for further research and development of knowledge update mechanisms tailored for legal LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in a wide range of natural language processing (NLP) tasks. Particularly in the legal field, a considerable number of legal LLMs have been developed and used in legal practice today. Trained on a comprehensive legal corpus encompassing statutes, legal textbooks, and legal documents, these models possess an in-depth understanding of legal knowledge, which enables them to solve many legal problems in practice. Yet, despite these advancements, legal LLMs face significant challenges due to the dynamic nature of legal statutes and administrative documents. This necessitates efficient and reliable methods for updating the internal knowledge of legal LLMs to reflect the latest changes in the legal field.\nKnowledge updating has been widely considered to be an important research question for the construction of LLMs in open domains. There have already been many datasets built focusing on benchmarking the performance of LLM knowledge editing methods in open domains. For example, CounterFact[1], MQUAKE-CF[2],"}, {"title": "2 RELATED WORK", "content": "Existing knowledge update methods can be divided into two categories, parametric and non-parametric. Parametric strategies involve the direct modification of model parameters to update knowledge. Two principal techniques exist within this category: model fine-tuning and model editing. Fine-tuning is a process where the model parameters are adjusted based on a specific task or dataset. The objective is to minimize the loss between the model's predictions and the fine-tuning dataset. Fine-tuning can be further classified into full-parameter fine-tuning, which adjusts all parameters of the model, and Lora fine-tuning [6], a more resource-efficient method that introduces a low-rank structure in the model's weight matrix for adjustment. On the other hand, model editing techniques aim to precisely modify the model parameters that influence the integration of new knowledge. This method effectively incorporates new knowledge while preserving unrelated knowledge within the model. Techniques in this category include Knowledge Neurons (KN) [7], which identifies and edits \"knowledge neurons\" in the pre-trained model, Rank-One Model Editing (ROME) [1], which uses causal tracing to directly write new key-value pairs into the earlier feed-forward network (FFN) layers of the model, and Self-Edit [3], which defines editing boundaries using event context and improves editing consistency while maintaining the naturalness of the generated text.\nNon-parametric strategies allow for flexible knowledge updates without the need for retraining the entire model. A notable example is Retrieval-Augmented Generation (RAG) [8-10], which integrates results retrieved from a knowledge base to assist the generation process of the language model. RAG has been demonstrated to enhance the performance of LLMs and alleviate hallucinations [11-13]. With this approach, knowledge updates only require modifications to the knowledge base, not the internal parameters of the model. While these strategies offer promising results, they also present challenges, such as handling noise in retrieval results [14], managing interactions between the retriever and generator [15], and addressing context limitations in long text generation [16]."}, {"title": "2.2 Evaluation of Knowledge Updating", "content": "Analyzing the efficiency of knowledge update methods for LLMs is crucial. As Wang et al.[17] highlighted, these methods can be evaluated across five dimensions: accuracy, locality, generality, Retainability, and Scalability. Accuracy [1, 2, 7, 18] gauges the ratio of successful updates in the dataset. Dong et al.[18] introduced the CKA framework to assess accuracy by comparing the scores of correct and incorrect facts. Locality [2, 19, 20] assess how a method retains unrelated knowledge during specific updates. The Drawdown metric by Sinitsin et al. [20] measures locality. And SERAC[19] further refined this evaluation method using the similarity of embedding vectors. Generality[1-3] tests if updated knowledge can be generalized to other relevant inputs. EVEDIT[3] and MQUAKE[2] test generality through inferential questions and multi-hop questions, respectively. Retainability[21] aims to check if"}, {"title": "2.3 Existing Benchmarks", "content": "A variety of datasets have been developed to thoroughly evaluate the performance of knowledge update techniques in general domains. These datasets span different fields and task types and can be categorized into three main groups based on their construction methods and the types of knowledge updates they focus on: evaluating counterfactual update capacity, assessing time awareness, and examining the ability to acquire the latest information. For evaluating counterfactual update capacity, datasets such as CounterFact[1], MQUAKE-CF[2], and EVEDIT[3] have been developed. These datasets focus on testing the ability of LLMs to handle information that contradicts their prior knowledge, thereby enhancing the robustness and adaptability of knowledge update techniques. Datasets like MQUAKE-T[2] and TimeSensitive-QA[4] are designed to assess temporal evolution awareness. They focus on how well LLMs can understand and update outdated information, reflecting the model's awareness of the temporal evolution of real-world information. Finally, for examining the ability to acquire the latest information, datasets like FreshQA[5] have been created. These datasets focus on how effectively knowledge update techniques can inject the latest knowledge into LLMs in a timely and accurate manner."}, {"title": "3 CHALLENGES OF LEGAL KNOWLEDGE UPDATE", "content": "Updating knowledge in the legal domain presents unique challenges that are not fully addressed by current benchmarks for the general one. These challenges arise from the specific characteristics of legal knowledge and the way it is applied and updated.\n\u2022 Application of New Legal Knowledge: One primary challenge in the legal domain is the correct application of new legal knowledge. Unlike general LLMs that respond to temporal changes by emphasizing the application of the latest information, the legal domain requires careful consideration of specific case circumstances. Factors such as the time span, differences between new and old laws, and the severity of penalties must be taken into account to determine whether to apply new or old statutes. This requirement significantly complicates the task of knowledge updating. Furthermore, beyond merely updating knowledge, legal LLMs should also learn the patterns and evolution of legal changes. This deeper understanding is essential for accurately navigating the complexities of legal reasoning and judgments over time.\n\u2022 Complexity of Legal Regulations: Legal regulations are typically more detailed and complex than the entity names in the general domain. Existing benchmarks for evaluating the accuracy of knowledge update often require models to paraphrase short entity names. However, when the legal domain requires the LLM to paraphrase lengthy legal regulations, it poses a significant challenge to existing knowledge update methods. For example,"}, {"title": "4 EVALUATION DIMENSIONS OF LEKUBE", "content": "This section introduces the evaluation dimensions in LeKUBE, designed to assess the effectiveness of knowledge update methods in legal LLMs, modeled after those in the general domain [17]."}, {"title": "4.1 Accuracy", "content": "In the general domain of knowledge update, accuracy measures the proportion of data that is successfully updated, i.e., the proportion of the updated entity that the LLM can successfully paraphrase when given the prompt. In the legal domain, the challenge for updating the LLM is the complete memorization of the updated legal knowledge. In LeKUBE, the main updated knowledge is the legal statutes, and this part involves two tasks:\n\u2022 Recitation of Statutes: Given the updated statute, the LLM needs to accurately recite the specific content of the statute.\n\u2022 Recall of Statute Items: The dual task of recitation of statutes. Given the specific content of the updated statute, the LLM needs to answer which clause of which law the statute comes from."}, {"title": "4.2 Generality", "content": "Generality requires that the updated LLM not only perform well on the training dataset but also generalize the relevant knowledge to other inputs. In the general domain, the examinations generally fall into two dimensions: examining the LLM's sense of time, or examining the LLM's inference of updated knowledge. In LeKUBE, we require the LLM to complete the following two tasks:\n\u2022 True-or-False Questions of Change in Statute: For the updated legal statute, the LLM needs to judge whether a certain detail of the statute has changed compared to before. This task requires the LLM to have a good grasp of the revision process and details of legal texts. This is the basis for the LLM to understand the application of the new statutes.\n\u2022 Multiple-Choice Questions (MCQ) of the Legal Scenario: For each updated statute, we designed a choice question that incorporates legal scenarios or virtual cases. The answer to this"}, {"title": "4.3 Locality", "content": "Locality requires that the updated LLM still maintain a good grasp of other unchanged knowledge, that is, the knowledge update method should not only perform well on tasks related to updated knowledge but also not affect the LLM's original performance of unrelated knowledge. The legal domain requires the updated LLM to maintain the memory of the unchanged statutes, so in this part, the tasks we designed are the Recitation of Statutes, the Recall of Statute Items, and the True-or-False Questions of Change in Statute, related to statutes that have not been updated."}, {"title": "4.4 Scalability", "content": "Scalability evaluates the relationship between the effect of knowledge update and the amount of updated knowledge. If, When the amount of updated knowledge increases, the effect of knowledge update does not significantly decrease, then we believe that the knowledge update method has good scalability. In LeKUBE, we divide the data into chapters (such as \"Chapter III of Part II in Criminal Law\", \"Book Six of the Civil Code\", etc.). For a certain knowledge update method, we let the LLM update the knowledge by chapters (i.e., assuming that the statutes are only updated within a single chapter), and then complete the Multiple-Choice Questions of the Legal Scenario of this chapter. We compare this part of the performance with the performance of the LLM updating all the chapters that need to be updated, and the difference in performance reflects the scalability of the knowledge update method."}, {"title": "4.5 Retainability", "content": "Retainability evaluates the degree to which the LLM retains the effect of early updates after multiple updates. Similarly, we use chapters as units and update the knowledge in a predetermined order. Then we choose the task of Multiple-Choice Questions of the Legal Scenario and evaluate the performance of the LLM after updating some data and updating all the data. The difference in performance reflects the retainability of the knowledge update method."}, {"title": "5 DATASET CONSTRUCTION", "content": "In this paper, we focus on the Chinese legal system, specifically targeting two principal statutes: the Criminal Law and the Civil Code of the People's Republic of China, referred to as the Criminal Law and the Civil Code, respectively. The statutes incorporated into our dataset are derived from the most recent versions of these laws. Our primary data source is the corpus of the STARD dataset [23], which encompasses a comprehensive collection of all national-level laws, regulations, and judicial interpretations in China, sourced directly from official government platforms."}, {"title": "5.2 Annotation Recruitment and Payment", "content": "We recruit expert annotators from top law schools for dataset annotation tasks. Annotators are paid based on the number of complete annotations, i.e., pairs of annotations involving a statute modification and a corresponding legal multiple-choice question. The average payment per annotation is 10 CNY. Typically, an annotator can complete 4-5 annotations per hour, resulting in an average hourly wage of 40-50 CNY, which is over 80% higher than Beijing's minimum wage. We also employ other annotators for quality evaluation of the annotations and questions generated by the LLMs, at an average payment of 3 CNY per evaluation. An evaluator can"}, {"title": "5.3 Annotation Process", "content": "To ensure diverse and non-conflicting legal modifications, chapters from Criminal Law and Civil Code are assigned to different annotators who are tasked to modify parts of the statutes. They then proposed a multiple-choice question related to the modification.\nFor simplicity, we only allow three types of modifications in our annotation process, i.e., changing legal consequences, changing constituent elements, and changing behavior patterns. Annotators are required to select one method before modifying a statute and provide a brief reason for their modification. Also, we require that the modified statutes should be reasonable, legally accurate, and diverse. The modification of related statutes should be consistent to avoid contradictions. And the multiple-choice questions should involve a legal scenario or a fictional case with an answer that changes based on the statute modification. Each data point is added to the final LeKUBE dataset only after another annotator has checked and confirmed their quality.\nTrue-or-false questions are generated for each modified statute to assess the details of the statute change. Unmodified statutes are also included to get questions for evaluation of locality.\nWe concatenate both the pre- and post-modification versions of each legal statute, including the modification method, into a single prompt for GPT-4-0125-preview [24]. Then we instruct the model to analyze the differences between these two versions and subsequently generate a series of true-or-false questions. Human annotators then filter the data, retaining only clear, reasonable, and correct questions and answers. The data is added to the final LeKUBE dataset only after another annotator approves of its quality."}, {"title": "6 DATASET STATISTICS AND ANALYSIS", "content": "Table 1 and Table 2 provide a basic statistical overview of the LeKUBE dataset. LeKUBE contains 180 updated statutes, with the average length of each statute (calculated as Chinese characters) approximately 114 pre-update and around 124 post-update. Notably, some statutes in the dataset have been shortened after the update, such as those with certain clauses removed. Additionally, 60 non-updated statutes are randomly selected to evaluate the locality. The average length of these statutes is approximately 119, and other statistical indicators are presented in Table 2."}, {"title": "7 EXPERIMENTAL SETUP", "content": "We now provide a detailed description of the evaluation of different knowledge update methods on LeKUBE. Let the set of all statutes that need to be updated in LeKUBE be $D_1$, and the set of statutes that haven't been updated in LeKUBE be $D_2$.\nIn the three types of evaluation, which evaluate the accuracy, generality, and locality respectively, the set of statutes to update is $D_1$. Then tasks for testing accuracy and generality are related to $D_1$, and tasks for testing locality are related to $D_2$.\nThen, for scalability and retainability, we divide $D_1$ into $D_1 = D_1 \\cup D_2 \\cup ... \\cup D_9$ by chapters. The order is as follows: Chapter III, VI, VIII, and IX of Part II in the Criminal Law, and Book 2, 3, 5, 6, and 7 in the Civil Code. For tasks corresponding to scalability, we update the LLM on $D_i$, $i = 1, ..., 9$ respectively, and evaluate the performance of the updated model $M_i$ on tasks corresponding to $D_i$, then comparing it with the LLM updating the whole $D_1$ with the corresponding method. For tasks related to retainability, we sequentially update the model on $D_i$, $i = 1, ..., 9$, and evaluate the performance difference of the model on tasks corresponding to $D_i$, $i = 1, 2, 3, 4$ after updating to $i = 4$ (since there are four chapters of criminal law to update in LeKUBE, this corresponds to the situation where the criminal law is updated while the civil code is not).\nDepending on the task formats, the evaluation metrics we used include:"}, {"title": "7.1 Evaluation Procedure and Tasks", "content": "We now provide a detailed description of the evaluation of different knowledge update methods on LeKUBE. Let the set of all statutes that need to be updated in LeKUBE be $D_1$, and the set of statutes that haven't been updated in LeKUBE be $D_2$.\nIn the three types of evaluation, which evaluate the accuracy, generality, and locality respectively, the set of statutes to update is $D_1$. Then tasks for testing accuracy and generality are related to $D_1$, and tasks for testing locality are related to $D_2$.\nThen, for scalability and retainability, we divide $D_1$ into $D_1 = D_1 \\cup D_2 \\cup ... \\cup D_9$ by chapters. The order is as follows: Chapter III, VI, VIII, and IX of Part II in the Criminal Law, and Book 2, 3, 5, 6, and 7 in the Civil Code. For tasks corresponding to scalability, we update the LLM on $D_i$, $i = 1, ..., 9$ respectively, and evaluate the performance of the updated model $M_i$ on tasks corresponding to $D_i$, then comparing it with the LLM updating the whole $D_1$ with the corresponding method. For tasks related to retainability, we sequentially update the model on $D_i$, $i = 1, ..., 9$, and evaluate the performance difference of the model on tasks corresponding to $D_i$, $i = 1, 2, 3, 4$ after updating to $i = 4$ (since there are four chapters of criminal law to update in LeKUBE, this corresponds to the situation where the criminal law is updated while the civil code is not).\nDepending on the task formats, the evaluation metrics we used include:"}, {"title": "7.2 Large Language Models", "content": "We choose BaiChuan2-13B-Chat[25], ChatGLM3-6B[26], ChatLaw-13B[27], and LegalAID-7B [28] for our experiments. The first two models are open-domain LLMs, while the latter two are Chinese legal LLMs.\nTo ensure that knowledge updating is performed when LLMs already possess outdated legal knowledge, we further train BaiChuan2-13B-Chat and ChatGLM3-6B on the STARD dataset's corpus, which encompasses a comprehensive collection of all national-level laws, regulations, and judicial interpretations in China."}, {"title": "7.3 Knowledge Update Baselines", "content": "In this paper, the knowledge update baselines evaluated are categorized into two main types based on whether they alter or introduce new model parameters."}, {"title": "7.3.1 Non-parametric Update Strategies.", "content": "The most popular non-parametric update strategy for LLMs is Retrieval Augmented Generation (RAG), which concatenates the retrieved legal text directly with the question as a prompt, injecting knowledge into the model in the form of context. Existing mainstream retrieval methods include vocabulary-based lexical-matching approaches [29-31] and dense retrieval [32-39]. Due to the lack of domain-specific knowledge, dense retrieval models in open domains do not yield optimal results in legal search tasks. Consequently, considerable research has focused on better adapting dense retrieval methodologies to the legal domain [40-44]. In this paper, we selected both vocabulary-based lexical-matching method and dense retrieval method as our retrieval method:"}, {"title": "7.3.2 Parametric Update Strategies.", "content": "We implemented and tested two types of knowledge updating methods that involve the training of LLMs, namely model fine-tuning and model editing."}, {"title": "Model Fine-tuning", "content": "We evaluate full fine-tuning (FT) and Lora fine-tuning [6]. We construct fine-tuning datasets for each model in the form of instructions, with the basic form as follows:"}, {"title": "Model Editing", "content": "We evaluate three model editing methods: KN[7], ROME[1], and Self-Edit[3]. The first two belong to triplet-level knowledge updates, while the last hopes to define editing scope through event reasoning anchor points to achieve event-level knowledge updates. For the first two knowledge editing methods, we use EasyEdit library [48] to edit the model."}, {"title": "8 EXPERIMENT RESULTS", "content": "The evaluation of accuracy encompasses two tasks: the recitation of statutes and the recall of statute items. However, the performance of the same knowledge update method varies across these tasks."}, {"title": "8.1 Evaluation of Accuracy", "content": "The evaluation of accuracy encompasses two tasks: the recitation of statutes and the recall of statute items. However, the performance of the same knowledge update method varies across these tasks."}, {"title": "8.1.1 Recitation of Statutes.", "content": "Table 3 presents the experimental results for evaluation of the accuracy and generality. From the first major column in Table 3, non-parametric strategies perform poorly on this task. According to statistics, the recall rate (i.e. the proportion of the corresponding statute included in the top 3 retrieved results) of Lawformer in this task is 0, but BM25, which is based on lexical matching, is 0.5167. Clearly, errors in the retrieval results of Lawformer mislead the model. The RAG-BM25 model also underperforms. Combined with the poor performance of ICL-Golden, it shows that it is difficult for the model to accurately recite statutes by updating in the context. The best performers in this task are full fine-tuning and the Self-Edit method in model editing, both"}, {"title": "8.1.2 Recall of Statute Items.", "content": "The second major column in Table 3 presents the results for the recall of statute items. Interestingly, as the dual task of statute recitation, the performance of the knowledge update method in this task is inconsistent with that of statute recitation. The best performance in this task is achieved by the baseline ICL-Golden, indicating that in this task, updating knowledge through additional context in the input prompts maybe enough. Since the retrieval methods of BM25 and Lawformer can easily retrieve the accurate statutes, leading to the excellent performance of the RAG method. The five methods of parametric strategies do not show a performance that is significantly better than the models without knowledge updates, indicating that these methods might only fit the one-way connection from the statute item name to the statute content. Furthermore, we delve into the reasons for the poor performance of Model Editing. We count the proportion of model answers containing statute item names (regardless of correctness), then show the proportion less than 0.98 in Table 4. We find that the Self-Edit and ROME methods severely impair the model's ability to follow instructions, leading to a decline in response accuracy."}, {"title": "8.2 Evaluation of Generality", "content": "The task assesses the capability of the updated LLMs to identify differences between historical and updated statutes. From the third major column in Table 3, we can see that the best performance is achieved by ICL-Golden. In the parametric strategies, only the Full Fine-tuning (FT) shows superior performance to Raw. Other methods do not exhibit a clear improvement over the models which have not been updated. This suggests that the ability to compare differences in new and old knowledge remains a significant challenge for the commonly used parametric knowledge update methods."}, {"title": "8.2.1 True-or-False Questions of Change in Statute.", "content": "The task assesses the capability of the updated LLMs to identify differences between historical and updated statutes. From the third major column in Table 3, we can see that the best performance is achieved by ICL-Golden. In the parametric strategies, only the Full Fine-tuning (FT) shows superior performance to Raw. Other methods do not exhibit a clear improvement over the models which have not been updated. This suggests that the ability to compare differences in new and old knowledge remains a significant challenge for the commonly used parametric knowledge update methods."}, {"title": "8.2.2 Multiple-Choice Questions of the Legal Scenario.", "content": "This task examines the reasoning and application of updated knowledge. As can be seen, from the last major column in Table 3, similar to the previous task, ICL-Golden performs the best. However, the low accuracy rates of all tested methods suggest that the task poses a significant challenge to some models' legal reasoning capabilities. The performance of the two non-parametric strategies is not significantly better than the models that have not been updated. Among the parametric strategies, only Self-Edit performs better than the models that have not been updated. This suggests that simple fine-tuning and editing are insufficient for generalizing knowledge. It is difficult for models to acquire the reasoning capability of the knowledge with fitting of the knowledge. The slightly better performance of Self-Edit is because it includes the process of self-questioning and answering with new knowledge as the context in the training data, which clearly defines the reasoning process and the scope of reasoning influence. [3]"}, {"title": "8.3 Evaluation of Locality", "content": "Figure 3 summarizes the evaluation results of three tasks, namely recitation of statutes, recall of statute items, and true-or-false questions of change in statutes. It deserves to be noted that the statutes we used to test locality here are non-updated. The statistic on the horizontal axis is calculated as follows: we set the score of Raw as 0, and the initial scores for other methods are 0. For a certain task, if the performance of a certain knowledge update method applied on a certain LLM is not worse than that of the original LLM (i.e. Raw), the score is increased by one; otherwise, the score is decreased by one. Thus, the higher the score, the better the locality of the knowledge update method.\nFrom Figure 3, it can be intuitively seen that the locality of the non-parametric strategy is the best. One important reason is that the knowledge update of RAG only involves an update of the retrieval corpus and index, which has little impact on non-updated knowledge in both the retrieval corpus and the target LLM. Among the parametric strategies, the locality of the KN method is the best. Compared to full fine-tuning, Lora performs better since it only adds some low-rank structures to some layers of the LLM without modifying its original parameters. Taking ChatGLM3 as an example, in this experimental setting, the number of parameters trained only accounts for 0.1248% of the total parameters of the model. This allows Lora to better maintain the model's memory of the original other knowledge. The locality of Self-Edit and ROME is very poor, indicating that the editing process has severely damaged the model's grasp of other knowledge."}, {"title": "8.4 Evaluation of Scalability", "content": "In this subsection, we compare two different experimental setups: the first involves processing each chapter by a separate LLM which only updates the knowledge of that particular chapter, with the final results being the average accuracy of all questions. The second setup involves processing all chapters by the same LLM which updates the knowledge of all chapters, and similarly calculates the average accuracy of all questions.4. Table 5 shows the results, and the first value in each blank cell represents the result of the first setup, while the second one represents the difference between the first setup and the second setup. We focus on the second value. If this value is significantly greater than 0, it indicates that the knowledge update method has poor scalability, because its update performance on large datasets is significantly worse than that on small datasets.\nThe experimental results show that non-parametric strategies demonstrate good scalability, which is consistent with their good locality, as they only involve updates to the knowledge base. The performance of parametric strategies on large datasets declines to varying degrees compared to small datasets. Among them, the ROME method shows the poorest generality, which is most evident"}, {"title": "8.5 Evaluation of Retainability", "content": "Table 6 presents the results of the retainability evaluation. In the table, Acc4 represents the model accuracy after updating to the fourth subset (in the experimental setup, this point corresponds to the completion of the update of the criminal law), while Acc9 represents the model accuracy after updating to the ninth subset (in the experimental setup, this point corresponds to the completion of the update of both the criminal and civil codes). The Diff. column is the difference between Acc4 and Acc9. We focus on the Diff. value. If this value is significantly greater than 0, it indicates that the knowledge updating method has poor retainability, and its performance after multiple updates is far worse than its performance after early updates.\nFrom the results, both non-parametric strategies and fine-tuning demonstrate good retainability, especially the non-parametric strategies. For model editing, compared to KN, ROME and Self-Edit show much worse performance in terms of retainability, which indicates that these types of knowledge edit methods couldn't edit LLM parameters precisely and new updates may overtake previous updates."}, {"title": "8.6 Analysis of Time Cost", "content": "Table 7 provides a comparison of the time cost for various knowledge update methods when updating Baichuan2Chat and conducting multiple-choice tasks. For the non-parametric strategy, we did not include the time required to build the index; for the parametric strategy training process, the GPU resources used for model fine-tuning and Self-Edit are dual Nvidia A100(40G), while the GPU resources used for KN and ROME methods are quad Nvidia A100(40G). Inference was conducted on a single Nvidia A100(40G).\nFrom the table, we can see that from the perspective of training, Self-Edit takes the longest time, mainly for two reasons:\n\u2022 Self-Edit requires the model to first generate questions and answers through context learning, and multiple different questions need to be sampled, which already takes 1603s;\n\u2022 During training, the data involved in Self-Edit is much more than other parametric methods, which will take longer time under the same training epoch, consuming 8843s.\nIn addition, the other two model editing methods are not fast either, taking much longer than the time for model fine-tuning, which contradicts the claim of \"efficient editing\" of model editing. We speculate that the efficiency of model editing is reflected in a single or a small number of updates, and for a large range of knowledge updates (e.g., LeKUBE's update involves 180 statutes), model editing does not have an advantage. Moreover, many frameworks for model fine-tuning reduce resource consumption or accelerate training, such as Deepspeed[49], while the tools for model editing are still mainly considering integrating various editing methods and ensuring that editing is applicable to more models.\nIn model fine-tuning, the advantages of Lora fine-tuning in low resource consumption and fast speed are obvious in our experiments. Even if the number of training epochs is twice that of full-parameter fine-tuning, the time consumed is only half of that of full-parameter fine-tuning."}, {"title": "9 CONCLUSIONS AND FUTURE WORKS", "content": "In this paper, we conduct an in-depth analysis of knowledge update methods applied in legal LLMs and propose a novel legal knowledge update benchmark, LeKUBE, to evaluate the performance of these methods. Our study highlights potential areas for further investigation. Firstly, LeKUBE only evaluates the most basic capability of \"comparing new and old knowledge differences\" for the legal applicability issue. Future tasks could consider more complex challenges related to legal applicability, which would require the participation of more legal experts. Secondly, existing parametric strategies could be improved to better meet the demands of the legal domain. The exploration of a combination of parametric and non-parametric strategies might lead to the development of update techniques that are more suitable for legal knowledge updating."}]}