{"title": "Reasoning with Natural Language Explanations", "authors": ["Marco Valentino", "Andr\u00e9 Freitas"], "abstract": "Explanation constitutes an archetypal feature of human rationality, underpinning learning and generalisation, and representing one of the media supporting scientific discovery and communication. Due to the importance of explanations in human reasoning, an increasing amount of research in Natural Language Inference (NLI) has started reconsidering the role that explanations play in learning and inference, attempting to build explanation-based NLI models that can effectively encode and use natural language explanations on downstream tasks. Research in explanation-based NLI, however, presents specific challenges and opportunities, as explanatory reasoning reflects aspects of both material and formal inference, making it a particularly rich setting to model and deliver complex reasoning. In this tutorial, we provide a comprehensive introduction to the field of explanation-based NLI, grounding this discussion on the epistemological-linguistic foundations of explanations, systematically describing the main architectural trends and evaluation methodologies that can be used to build systems capable of explanatory reasoning.", "sections": [{"title": "1 Introduction", "content": "Building systems that can understand and explain the world is a long-standing goal for Artificial Intelligence (AI) (Miller, 2019; Mitchell et al., 1986; Thagard and Litt, 2008). The ability to explain, in fact, constitutes an archetypal feature of human rationality, underpinning communication, learning, and generalisation, as well as one of the mediums enabling scientific discovery and progress through the formulation of explanatory theories (Lombrozo, 2012; Salmon, 2006; Kitcher, 1989; Deutsch, 2011).\nDue to the importance of explanation in human reasoning, an increasing amount of work has started reconsidering the role that explanation plays in learning and inference with natural language (Camburu et al., 2018; Yang et al., 2018; Rajani et al., 2019; Jansen et al., 2018). In contrast to the existing end-to-end paradigm based on Deep Learning, explanation-based NLI focuses on developing and evaluating models that can address downstream tasks through the explicit construction of a natural language explanation (Dalvi et al., 2021; Jansen et al., 2016; Wiegreffe and Marasovi\u0107, 2021; Stacey et al., 2022). In this context, explanation is seen as a potential solution to mitigate some of the well-known limitations in neural-based NLI architectures (Thayaparan et al., 2020), including the susceptibility to learning via shortcuts, the inability to generalise out-of-distribution, and the lack of interpretability (Guidotti et al., 2018; Biran and Cotton, 2017; Geirhos et al., 2020; Lewis et al., 2021; Sinha et al., 2021; Schlegel et al., 2020).\nResearch in explanation-based NLI, however, presents several fundamental challenges (Valentino and Freitas, 2024). First, the applied methodologies are still poorly informed by theories and accounts of explanations (Salmon, 2006; Woodward and Ross, 2021). This gap between theory and practice poses the risk of slowing down progress, missing the opportunity to formulate clearer hypotheses on the inferential properties of natural language explanations and define systematic evaluation methodologies (Camburu et al., 2020; Jansen et al., 2021; Atanasova, 2024). Second, explanation-based NLI models still lack robustness, control, and scalability for real-world applications. In particular, existing approaches suffer from several limitations when composing explanatory reasoning chains and performing abstraction for NLI in complex domains (Khashabi et al., 2019; Valentino et al., 2022a).\nIn this tutorial, we will provide a comprehensive introduction to explanatory reasoning in the context of NLI, by systematically categorising and surveying explanation-supporting benchmarks, architectures, and research trends. Specifically, we will present how the understanding of explanatory inference have evolved in recent years, together with the emerging methodological and modelling strategies. In parallel, we will attempt to provide an epistemological-linguistic characterisation of natural language explanations reviewing the main theoretical accounts (Valentino and Freitas, 2024; Salmon, 2006) to derive a fresh perspective for future work in the field."}, {"title": "2 Description", "content": "This section outlines the content of the tutorial."}, {"title": "2.1 Epistemological-Linguistic Foundations", "content": "One of the main objectives of the tutorial is to provide a theoretically grounded foundation for explanation-based NLI, investigating the notion of explanation as a language and inference scientific object of interest, from both an epistemological and linguistic perspectives (Valentino and Freitas, 2024; Salmon, 2006; Jansen et al., 2016).\nTo this end, we will present a systematic survey of the contemporary discussion in Philosophy of Science around the notion of a scientific explanation, attempting to shed light on the nature and function of explanatory arguments and their constituting elements. Here, we will critically review the main accounts of explanations, including the deductive-nomological and inductive-statistical account (Hempel and Oppenheim, 1948), the notion of statistical relevance and the causal-mechanical model (Salmon, 1984), and the unificationist account (Kitcher, 1989), aiming to elicit what it means to perform explanatory reasoning. Following the survey, we will focus on grounding the theoretical accounts for explanation-based NLI, attempting to identify the main feature of explanatory arguments in existing corpora of natural language explanations (Jansen et al., 2016; Xie et al., 2020; Jansen et al., 2018)."}, {"title": "2.2 Resources & Evaluation Methods for Explanation-Based NLI", "content": "In order to build NLI models that can reason through the generation of natural language explanations it is necessary to develop systematic evaluation methodologies. To this end, The tutorial will review the main resources, benchmarks and metrics in the field (Wiegreffe and Marasovic).\nDepending on the nature of the NLI problem, an explanation can include pieces of evidence at different levels of abstraction (Thayaparan et al., 2020). Traditionally, the field has been divided into extractive and abstractive tasks. In extractive NLI, the reasoning required for the explanations is derivable from the original problem formulation, where the correct decomposition of the problem contains all the necessary inference steps for the answer (Yang et al., 2018). On the other hand, abstractive NLI tasks require going beyond the surface form of the problem, where an explanation needs to account for and cohere definitions, abstract relations, which are not immediately available from the original context (Jansen et al., 2021; Thayaparan et al., 2021b).\nIn addition, the tutorial will review the main evaluation metrics adopted to assess the quality of natural language explanations. Evaluating the quality of explanations, in fact, is a challenging problem as it requires accounting for multiple concurrent properties. Different metrics have been proposed in the field, ranging from reference-based metrics designed to assess the alignment between automatically generated explanations and human-annotated explanations (Camburu et al., 2018; Jansen et al., 2021), and reference-free metrics designed to evaluate additional dimensions such as faithfulness (Parcalabescu and Frank, 2024; Atanasova et al., 2023), robustness (Camburu et al., 2020), logical validity (Quan et al., 2024b; Valentino et al., 2021a), and plausibility (Dalal et al., 2024)."}, {"title": "2.3 Explanation-Based Learning & Inference", "content": "We review the key architectural patterns and modelling strategies for reasoning and learning over natural language explanations. In particular, we focus on the following paradigms:\nMulti-Hop Reasoning & Retrieval-Based Models. The construction of explanations typically requires multi-hop reasoning \u2013 i.e., the ability to compose multiple pieces of evidence to support the final answer (Dalvi et al., 2021; Xie et al., 2020). Multi-hop reasoning has been largely studied in a retrieval settings, where, given an external knowledge base, the model is required to select, collect and link the relevant knowledge required to arrive at a final answer (Valentino et al., 2022a, 2021b, 2022b). Here, we will review the main retrieval-based architectures for multi-hop reasoning and explanation, highlighting some of the inherent limitations of such paradigm, including the tension between semantic drift and efficiency (Khashabi et al., 2019).\nNatural Language Explanation Generation. In parallel with retrieval approaches, NLI using generative models have been used for supporting explanatory inference (Camburu et al., 2018; Rajani et al., 2019). In this setting, early approaches leverage human-annotated natural language explanations for training generative models (Dalvi et al., 2021). Subsequently, the advent of Large Language Models (LLMs) has made it possible to elicit explanatory reasoning via specific prompting techniques and in-context learning (Wei et al., 2022; Yao et al., 2024; Zheng et al., 2023; He et al., 2024). Here, we review the main trends in the LLM-based generative paradigms, highlighting persisting limitations such as hallucinations and faithfulness (Turpin et al., 2024)."}, {"title": "2.4 Semantic Control for Explanatory Reasoning", "content": "Controlling the explanation generation process in neural-based models is particularly critical while modelling complex reasoning tasks. In this tutorial, we will review emerging trends which combine neural and symbolic approaches to improve semantic control in the explanatory reasoning process, which can provide formal guarantees on the quality of the explanations. These methods aim to integrate the content flexibility of language models (instrumental for supporting material inferences) and a formal inference properties.\nIn particular, we focus on the following key methods:\nLeveraging Explanatory Inference Patterns for Explanation-Based NLI. Inference patterns in explanation corpora can be leveraged to improve the efficiency and robustness of neural representations (Valentino and Freitas, 2024; Zhang et al., 2023). In particular, we will review approaches that attempt to leverage the notion of unification power in corpora of natural language explanations to improve multi-hop reasoning in a retrieval setting and alleviate semantic drift (Valentino et al., 2022a, 2021b, 2022b).\nConstraint-Based Optimisation for Explanation-Based NLI. We will focus on describing neuro-symbolic methods which target encoding explicit assumptions about the structure of natural language explanations (Thayaparan et al., 2021a). Here, we will review methods performing multi-hop inference via constrained optimisation, integrating neural representations with explicit constraints via end-to-end differentiable optimisation approaches (Thayaparan et al., 2022, 2024).\nFormal-Geometric Inference Controls over Latent Spaces. Covers emerging methodologies which focus on learning latent spaces with better representational properties for explanatory NLI, using language Variational Autoencoders (VAEs) for delivering better disentanglement and separability of language and inference properties (Zhang et al., 2024a,c,b,a) which support better inference control. These methods deliver an additional geometrical structure to latent spaces, aiming to deliver the vision of 'inference as latent geometry'.\nLLM-Symbolic Architectures Finally, we will focus on hybrid neuro-symbolic architectures that attempt to leverage the material/content-based inference properties of LLMs for explanation generation with external symbolic approaches, which accounts for formal/logical validity refinement properties. In particular, we will review approaches that perform explanation refinement via the integration of LLMs and Theorem Provers to verify logical validity (Quan et al., 2024b,a) and additional external tools to evaluate explanation properties such as uncertainty, plausibility and coherence (Dalal et al., 2024)."}, {"title": "3 Schedule", "content": "The tutorial will be organised according to the following timeline:\n1. Introduction & Motivation (20 min.)\n2. Epistemological-Linguistic Foundations (20 min.)\n3. Resources & Evaluation for Explanation-Based NLI (40 min.)\n4. Explanation-Based Learning & Inference (40 min.)\n5. Semantic Control for Explanatory Reasoning (40 min.)\n6. Synthesis, Discussion, and Q&A (20 min.)"}, {"title": "4 Breadth & Diversity", "content": "The tutorial will cover a wide spectrum of topics in different fields, ranging from Philosophy, Machine Learning, Natural Language Processing, Knowledge Representation and Automated Reasoning. This diversity of topics will help create a rich environment in which academics from different backgrounds and cultural contexts can integrate different perspectives. The tutorial plan includes integrated open Q&A sessions and practical demonstrations."}, {"title": "5 Prerequisites", "content": "We do not expect attendees to be familiar with previous research on NLI and Explanatory inference. On the opposite, we intent this tutorial to be an efficient and deep onboarding into the state-of-the-art in those areas. Participants should have a general background knowledge in deep learning, including recent trends and architectures such as Large Language Models. Participants are expected to be familiar with some of the broader NLI tasks, such as Textual Entailment and Question Answering."}, {"title": "6 Reading List", "content": "Epistemological-Linguistic Foundations\nValentino and Freitas (2024) On the Nature of Explanation: An Epistemological-Linguistic Perspective for Explanation-Based Natural Language Inference.\nSalmon (2006) Four Decades of Scientific Explanation.\nJansen et al. (2016) What's in an Explanation? Characterizing Knowledge and Inference Requirements for Elementary Science Exam.\nResources, Models and Evaluation\nWiegreffe and Marasovi\u0107 (2021) Teach me to Explain: A Review of Datasets for Explainable Natural Language Processing.\nThayaparan et al. (2020) A Survey on Explainability in Machine Reading Comprehension.\nZhao et al. (2024) Explainability for Large Language Models: A Survey.\nRelated Tutorials\nZhu et al. (2024) Explanation in the Era of Large Language Models.\nCamburu and Akata (2021) Natural-XAI: Explainable AI with Natural Language Explanation."}, {"title": "7 Instructor information", "content": "Marco Valentino, Idiap Research Institute.2 Marco is a postdoctoral researcher at the Idiap Research Institute, Switzerland. His research is carried out at the intersection of Natural Language Inference and Neuro-Symbolic models focusing on building systems that can reason through natural language explanations in complex domains (e.g., mathematics, science, biomedical and clinical applications, ethical reasoning). He has published papers in major AI and NLP conferences including AAAI, ACL, EMNLP, NAACL and EACL. Marco was involved in the organisation of workshops including MathNLP (EMNLP 2022 and LREC-COLING 2024), and TextGraphs (COLING 2022 and ACL 2024).\nAndr\u00e9 Freitas, University of Manchester & Idiap Research Institute.3 Andr\u00e9 Freitas leads the Neuro-symbolic AI Lab at the University of Manchester and IDIAP Research Institute. His main research interests are on enabling the development of AI methods to support abstract, flexible and controlled reasoning in order to support AI-augmented scientific discovery. In particular, he investigates how the combination of neural and symbolic data representation paradigms can deliver better models of inference. He is an active contributor to the main conferences and journals in the AI/Natural Language Processing (NLP) interface (AAAI, NeurIPs, ACL, EMNLP, EACL, COLING, TACL, Computational Linguistics), with over 100 peer-reviewed publications. He contributed to the organisation of MathNLP at EMNLP 2022 and LREC-COLING 2024. Andr\u00e9 participated in 7 tutorials, and co-organised 1 conference and 6 workshops."}]}