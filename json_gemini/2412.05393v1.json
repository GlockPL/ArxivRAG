{"title": "HiVeGen \u2013 Hierarchical LLM-based Verilog Generation for Scalable Chip Design", "authors": ["Jinwei Tang", "Jiayin Qin", "Kiran Thorat", "Chen Zhu-Tian", "Yu Cao", "Yang (Katie) Zhao", "Caiwen Ding"], "abstract": "With Large Language Models (LLMs) recently demonstrating impressive proficiency in code generation, it is promising to extend their abilities to Hardware Description Language (HDL). However, LLMs tend to generate single HDL code blocks rather than hierarchical structures for hardware designs, leading to hallucinations, particularly in complex designs like Domain-Specific Accelerators (DSAs). To address this, we propose HiVeGen, a hierarchical LLM-based Verilog generation framework that decomposes generation tasks into LLM-manageable hierarchical submodules. HiVeGen further harnesses the advantages of such hierarchical structures by integrating automatic Design Space Exploration (DSE) into hierarchy-aware prompt generation, introducing weight-based retrieval to enhance code reuse, and enabling real-time human-computer interaction to lower error-correction cost, significantly improving the quality of generated designs.", "sections": [{"title": "I. INTRODUCTION", "content": "With the slowing pace of technology scaling and the explo-sive growth in artificial intelligence (AI)-driven applications,there is a growing demand for scalable chip designs that caneffectively meet evolving application requirements while un-locking new capabilities. Domain-Specific Accelerator (DSA)chips exemplify this approach, as they are specifically cus-tomized to optimize particular applications, enabling signif-icant improvements in power, performance, and area (PPA).Typically, DSAs, like other complex chip designs, employhierarchical structures that decompose complex designs intomanageable submodules. These hierarchical structures can bereprogrammed or tailored to meet various user/applicationrequirements by adjusting their design configurations. How-ever, designing DSAs remains an arduous and time-consumingventure. First, manually writing, debugging, and modifyinghierarchical hardware description language (HDL) codes re-quires substantial design time and hardware design expertise,especially for complex designs. Furthermore, as design com-plexity grows, the design space expands, making design spaceexploration (DSE) a significant bottleneck that hinders bothdesign efficiency and quality.\nRecent advancements in Large Language Models (LLMs)for natural language understanding and generation [16] haveinspired efforts to extend their ability to facilitate hardwarechip designs. Prior works have demonstrated LLMs' potentialin generating HDL code from natural language descriptionsor high-level specifications [1], [3], [8], [19], [21], [29].Some studies [6], [22] have further explored the design spacewith the assistance of LLMs, through their ability to learnby imitation. However, the performance of LLM-generatedhardware designs still needs improvements, especially forcomplex designs like DSAs. Through analyzing their outputs,we observe that LLMs tend to generate all code within onesingle block. This results in excessively lengthy code, therebyleading to three critical issues: (1) Token length limit: thelength of code generation is constrained by the token limit ofLLMs, making it challenging to successfully produce complexDSAs, let alone perform effective DSE. For example, LLMsmay omit essential modules by replacing their implementa-tions with comments to save tokens. (2) Code redundancy:generating single code blocks disregards module reuse, leadingto code redundancy and poor readability, making the designdifficult to maintain and expand. (3) High error-correctioncost: as the single code block is generated at one time, usersare unable to identify structural errors in real-time. However,continuously sending the user's prompt to LLMs for feedback- the most straightforward approach - is impractical due tocomputational demands and the unpredictable nature of LLMoutputs [20]. These limitations result in significant overheadwhen generating lengthy code for complex designs since anyerrors encountered require substantial effort to correct.\nBased on these observations, we propose HiVeGen, a hierar-chical LLM-based Verilog generation framework that decom-poses chip generation tasks into LLM-manageable hierarchicalsubmodules and further reaps the hierarchical structures forDSE, code reuse, and low-cost error correction. The contribu-tions of our proposed HiVeGen can be summarized as follows:\n\u2022 To enable hierarchical structures, we propose aHierarchy-Aware Prompt Generation Engine equippedwith a Design Space Explorer to perform top-downhierarchical decomposition of designs. It iteratively opti-mizes the hierarchical configurations with PPA feedbackfrom previous configuration rounds while maintainingalignment with application-specific constraints.\n\u2022 Based on the hierarchical configuration, our framework"}, {"title": "III. FRAMEWORK", "content": "A. Overview\nIn our HiVeGen framework, as shown in Figure 1, weleverage a text-based natural language description as userinputs. The framework supports two modes of design. Wedirectly generate the hierarchy-aware prompts with DesignSpace Explorer for simple designs without templates. For"}, {"title": "B. Hierarchy-aware Prompt Generation Engine", "content": "Utilizing the insights that the initial input provided by usersmay lack the necessary hierarchy details for LLM to generatecorresponding designs, we develop a Hierarchy-Aware PromptGeneration Engine in our framework, which involves an LLM-based Design Space Explorer, a Prompt Enhancer, anda Configuration Evaluator.\nFigure 1(a) illustrates the flow of the Hierarchy-AwarePrompt Generation Engine that supports two design modes.For simple designs that do not have pre-defined templates, theDesign Space Explorer takes in the natural language descrip-tion and directly identifies the optimized prompt with hierar-chical structure and augmented details utilizing LLM agentsbased on previous PPA feedback. However, for complicateddesigns, including highly-specialized DSAs, it is challengingfor LLMs to independently explore all hierarchy-based details.To address this, the Design Space Explorer will first performan optimal design search based on hierarchical templates withconfigurable parameters, which narrows the design space. ThePrompt Enhancer will then proceed with the generated JSONconfiguration to produce the enhanced prompt. For the twomodes of designs, i.e., simple and complicated designs, weprovide two distinct PPA-aware system prompts to enable thegeneration of a direct prompt and a configuration JSON filewith explorable parameters, respectively.\nObserving that parameter coupling and interdependencyexist in the templated-based design, we developed a Con-figuration Evaluator to check the design rule conflicts in thegenerated configuration. If the prompt fails to meet the designrules, it will be fed back to the Design Space Explorer toproduce new configurations; otherwise, it will be fed back tothe next-stage parser.\nThrough this prompt generation flow, we have implementedPPA-aware design space exploration with hierarchy decompo-sition, which empowers LLMs with hierarchy-aware explo-"}, {"title": "C. On-the-fly Parsing Engine", "content": "The On-the-fly Parsing Engine is composed of two com-ponents: a Task Manager and a Runtime Parser, as shown inFigure 1(b).\nWe first introduce a LLM-based Task Manager that pri-marily extracts the required modules based on input prompthierarchy. In other words, it retrieves the module names fromthe prompt, deduplicates and sorts the generation order ofmodules. The task manager generates a task list composed ofmodule names, which serves as the reference for subsequentmodification and retrieval.\nIn traditional automation, there is often a lack of interfacefor users to view or modify the code structures in realtime. Consequently, errors such as incorrect port definitionsand mismatched module names can only be corrected aftergenerating the entire design, which is costly. By incorporatingthe proposed Runtime Parser, users can receive immediatefeedback on HDL code formulations without accessing outputs"}, {"title": "D. Weight-based Retrieving Engine", "content": "To save generation time and effort by utilizing modulereuses, we present a Weight-Based Retrieving Engine com-posed of a Code Retriever, a Module Generator, and a PromptAssembler. The Code Library from which codes are retrievedfurther integrates two essential mechanisms, that are weightmanagement and garbage collection, as illustrated in Figure1(c).\nEach code generation will be on its own thread, thusimproving the performance of the code generation. Within thethread, the code retriever looks in the code database based oncosine similarity and weight based on Eq. 1:\n$\\text{Code}(T) = \\text{Code}(\\arg \\max_{Tdb \\in DB} (\\text{cos}(T, Tdb) \\cdot W(Tdb)))$\nwhere T is the token used to retrieve code, and Tdb is tokenin the Code Library.\nIf no such code block reaches the controlling threshold, theLLMs will be called through API to generate a new block withspecifications. To further increase the stability and reliabilityof the Code Library, each code will be automatically verifiedby the LLM-generated testbench before they enter it. After allcode blocks are retrieved and filled in the code sketch, themain function will be generated with the input and outputpins of the sub-modules as part of the prompt. The resultwill subsequently be sent to the Code Validator and PPAChecker for evaluation, and the Code Library will maintainitself through weight management and garbage collection,depending on whether the evaluation succeeds."}, {"title": "IV. EVALUATION", "content": "A. Experimental Setup\nDataset. To ensure fair comparisons with existing work, ourexperiment adopts two datasets. The first dataset is composedof simple designs to be generated with Path 2 in Figure 1,including the Multiplexer, the Decoder, the Barrel Shifter, andthe UART. These designs are the same as the first datasetin ROME [15]. Given the focus of this study on DSAs, thesecond dataset selects and evaluates three hierarchical DSAdesigns for different reasons, which involves a commonly-usedSystolic Array with buffers, a renowned Convolutional NeuralNetwork accelerator ShiDianNao [5], and Coarse-grained Re-configurable Architecture (CGRA) [17] with a relatively largerdesign space, as shown in Figure 4. Note that our generationof CGRA only involves the GPE and GIB array, and the"}, {"title": "B. Generation Accuracy", "content": "Manual-written hierarchical prompt. To enable a faircomparison of generation accuracy and speed, our experimentinitially employs a manual-written hierarchical prompt directlyusing Engine (b) and (c) in Figure 1 with no user input"}, {"title": "C. PPA Optimization", "content": "For Domain-Specific Accelerators shown in Figure 4, weutilize Path in Figure 1 to generate our design. We allowa total of 20 generations for each design, with four correctionattempts within each of the 5 generation iterations. Ourgeneration pipeline is application-oriented, which means thatLLM will automatically explore the best configuration for thespecified application. We employ user input \u201cDefine a CGRAfor application FFT\" as an illustration. The kernel extractorwill first analyze the kernel FFT and generate the DFG withoperator information, which will then be fed to the LLM-basedDesign Space Explorer to define the configuration of CGRA.For example, LLM agent defines the scale of CGRA thatsupports the FFT kernel to be 2x2, and the ALU supportedoperations in GPE to be \u201cPASS, ADD, SUB\u201d.\nWe divide HiVeGen-based experiments into two groups.The first group generates without utilizing a PPA-aware in-context learning (ICL) template, which means that it willgenerate the configuration with no shot and only a blanktemplate, whereas the second group incorporates the PPA-emphasized one-shot JSON template into prompt generation,with a concise text file involving parameter explanation, PPAfeedback, objectives, and suggested optimization strategies.Here, we position \u201cpipelining\" as the suggested strategy and"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose HiVeGen, a hierarchical LLM-based automatic Verilog generation framework. HiViGen ex-hibits its potential to be an effective solution to token limits,"}]}