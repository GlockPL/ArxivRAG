{"title": "DapPep: Domain Adaptive Peptide-agnostic Learning for Universal T-cell Receptor-antigen Binding Affinity Prediction", "authors": ["Jiangbin Zheng", "Qianhui Xu", "Ruichen Xia", "Stan Z. Li"], "abstract": "Identifying T-cell receptors (TCRs) that interact with antigenic peptides provides the technical basis for developing vaccines and immunotherapies. The emergent deep learning methods excel at learning antigen binding patterns from known TCRs but struggle with novel or sparsely represented antigens. However, binding specificity for unseen antigens or exogenous peptides is critical. We introduce a domain-adaptive peptide-agnostic learning framework DapPep for universal TCR-antigen binding affinity prediction to address this challenge. The lightweight self-attention architecture combines a pre-trained protein language model with an inner-loop self-supervised regime to enable robust TCR-peptide representations. Extensive experiments on various benchmarks demonstrate that DapPep consistently outperforms existing tools, showcasing robust generalization capability, especially for data-scarce settings and unseen peptides. Moreover, DapPep proves effective in challenging clinical tasks such as sorting reactive T cells in tumor neoantigen therapy and identifying key positions in 3D structures.", "sections": [{"title": "I. INTRODUCTION", "content": "Computational recognition of T-cell receptor (TCR)-peptide complexes is crucial for understanding tumors, autoimmune, and viral infectious diseases [17], [20], [21], where antigenic and viral peptides are presented by MHC-I. TCRs on T cells recognize these complexes, triggering an immune response. As key molecules in acquired immunity, TCRs exhibit complex diversity due to genetic recombination and evolutionary screening. Predicting TCR-peptide binding is a fundamental challenge in computational immunology, offering opportunities for vaccine development and immunotherapy. While traditional experimental methods [1], [15], [34] exist to detect TCR-MHC interactions, they are often time-consuming and technically difficult.\nWith the rise of deep learning, many approaches have emerged for analyzing TCR patterns and predicting TCR-peptide binding affinity, categorized as follows: 1) Quantitative similarity measurement methods [2], [3], [6], [9], [22], [32], [33], which cluster TCRs and decipher antigen-specific binding patterns, but are not directly applicable for TCR-peptide binding prediction. 2) Peptide-specific TCR binding prediction models [5], [12], [19], which are limited to specific peptides, restricting their utility. 3) Peptide-agnostic TCR-peptide binding prediction models, such as pMTnet [18], DLpTCR [31], ERGO2 [23] and TITAN [27], which can handle a broader range of TCR-peptide pairs but struggle with generalizing to unseen peptides or those with limited TCR interactions, posing challenges for immunotherapy. 4) PanPep [4], a recent study, improves generalization through various settings (majority, few-shot, and zero-shot). However, it relies on fine-tuning in few-shot cases and struggles with unseen peptides in zero-shot settings. PanPep is a pseudo-peptide-agnostic method combining peptide-agnostic and peptide-specific models, and its effectiveness depends on experience and further fine-tuning.\nIn addition to the limitations of tool design, the diversity and complexity of the data remain the main reasons for the accurate identification. On the one hand, TCRs exhibit a high degree of diversity, making it difficult to generalize existing computational tools. On the other hand, known peptide-TCR pairing data obey a long-tailed distribution [4], resulting in a severely uneven distribution, in which a few peptides combine lots of known TCR binding data, but most peptides record only a few known TCR binding information. In this case, the conventional supervised paradigm leads to learning binding patterns in the majority setting and is difficult to generalize to the few/zero-shot settings.\nTo tackle these challenges, we propose a novel domain-adaptive peptide learning framework, DapPep, for TCR-peptide binding affinity prediction, as illustrated in Figure 1. DapPep is a universal peptide-agnostic model capable of adapting to various data settings (e.g., majority, few-shot, and zero-shot) without bias. Experimental results across multiple benchmarks demonstrate DapPep's strong generalization capabilities, significantly outperforming existing methods, particularly for unseen peptides like exogenous or neoantigens. DapPep has also been successfully applied to complex clinical tasks, yielding promising results in line with expectations for challenging treatments.\nThe main contributions are as follows:\n\u2022 DapPep is a novel tool for peptide-agnostic TCR-peptide"}, {"title": "II. METHODS", "content": "Peptide-agnostic DapPep consists of three key modules: TCR representations (TRepr) module in Figure 1 (A), peptide representations (PRepr) module in (B), and TCR-peptide representations (TPRepr) module in (D). Following the TPRepr module is a linear decoder in Figure 1 (E) for scoring binding affinity. For simplicity and efficiency, all modules are powered by multi-head self-attention layers. The training is divided into two stages: Stage 1 is the initialization of the Trepr module and the pre-training of the TPRepr. Since TCRs are special proteins, the Trepr module can be initialized with parameters derived directly from off-the-shelf powerful protein language models. The pre-trained TPRepr module can be viewed as a sequence-to-sequence machine-translation task based on an asymmetric auto-encoder (asyAE) network. Stage 2 is to transfer the pre-trained TRepr and TPRepr modules to the binding affinity learning pipeline.\nTCR Representation Module. The Trepr focuses on TCR sequence representations. To leverage the potential rich information contained in TCRs, we initialize the TRepr module with the existing pre-trained protein language model ESM-2 [16]. Since ESM-2 is trained on large-scale protein data, transfer learning enhances effectively the capability of the Trepr to capture the diverse and information-dense nature of TCRs.\nPeptide Representation Module. The PRepr module is responsible for encoding the features of peptide sequences. Compared to TCRs, most peptides are short in length (typi-"}, {"title": "A. Inner-loop TCR-Peptide Module Pre-training", "content": "Instead of transfer learning, the TPRepr module embraces the challenge of reconstructing peptide sequences through an asyAE framework. On the one hand, there is no specialized large-scale pre-trained language model for peptides, possibly due to the limited number of trainable peptides and the instability of the peptide structure. On the other hand, it is unnecessary to use peptides extensively because of their short and structurally simple sequences. By training the module to reconstruct peptides, it learns to capture the essential characteristics and unique patterns inherent in the peptide sequences. This unsupervised learning approach enhances the module's ability to represent peptides accurately, setting the stage for accurate binding affinity predictions. Additionally, the pre-training process simulates the interaction between sequences to initialize the parameters of the cross-attention layers."}, {"title": "Inner-loop asyAE Architecture.", "content": "As depicted in Figure 1(C), both the input and output being peptides, the multi-head cross-attention network in the TPRepr module degrades to self-attention layers. Consequently, the TPRepr module only functions as the encoder for peptides (different from the PRepr module in the binding affinity prediction pipeline). Hence, the TPRepr module receives the peptide sequence features (Word2Vec) as key, query, and value. This is also different from the input of the TPRepr in the binding affinity prediction pipeline. Acting as an encoder, it leverages the learned representations from the TCR and peptide modules to capture the intricate interplay between TCR and peptide residues. The peptide decoder, followed by the cross-attention encoder, essentially a non-autoregressive linear layer, aims to generate peptide sequences. The encoder-decoder architecture forms an asyAE model, as the model's objective is to reconstruct the peptide sequences. Shared amino acid embeddings and position encodings between the encoder and decoder, the asyAE architecture learns to preserve important contextual and positional features during the reconstruction process. To further enhance the learning process, the peptide decoder utilizes a lower triangular mask mechanism during training, which ensures that asyAE model is capable of effectively reconstructing the native sequences, leading to a more comprehensive understanding of the underlying sequence semantics."}, {"title": "Pre-training Objective.", "content": "Using only the peptide sequences from the training set of TCR-peptide pairs, we pre-train the proposed asyAE framework [7], [8], [10], [11], [13], [24], [28]-[30], [35]-[43], as shown in Figure 1(C). This enables the asyAE to learn contextual semantic knowledge, while the cross-entropy loss is used to fit the probability distribution of the generated sequences. Formally, we assume that an input peptide sequence is denoted as $S_{in} = {s_1, s_2,\u2026\u2026, s_n}$ with n amino acids, and the recovered TCR sequence of the decoder is denoted as $S_{out} = {\\hat{s_1},\\hat{s_2},\u2026\u2026,\\hat{s_n}}$. The corresponding probability distribution is denoted as $S_{logits}$. The reference native sequence is $S_{native} = S_{in} = {s_1, s_2,\u2026\u2026, s_n}$. The asyAE pre-training as:\n$S_{logits} = PepDecoder(TPRepr(Residue2Vec(S_{in}))), (1)$\nwhere Residue2Vec represents the Word2Vec for amino acids, and PepDecoder represents the peptide decoder.\nThe sequence recovery cross-entropy loss is defined as:\n$\\mathcal{L}_{seqCE} = CE(Softmax(S_{logits}), S_{native})). (2)$"}, {"title": "B. Binding Affinity Prediction Pipeline", "content": "As shown in Figure 1(F), the binding affinity learning stage aims to predict the binding scores for TCR-peptide pairs. The output features of cross-attention TPRepr module are fed into the binding decoder module, which consists of mean pooling layers and simple linear layers. The binding decoder then passes the encoded features through a sigmoid function to produce a continuous binding affinity score ranging from 0 to 1, which reflects the probability of binding between a TCR-peptide pair. Higher affinity scores indicate a stronger likelihood of clone expansion for the TCR in response to the peptide. The binding affinity training target guides the model to learn and capture the complex binding patterns between TCRs and peptides, enabling it to accurately predict the affinity between unknown TCR-peptide pairs. During the training phase, a mean squared error (MSE), i.e., squared $L_2$ norm, is employed to calculate the loss criterion.\nFormally, given a TCR-peptide pair, let $T = {X_1,X_2,,X_n}$ denote the input TCR sequence and $P = {X_1,X_2,\u2026\u2026,X_m}$ denote the input Peptide sequence for DapPep, where n and m is the length of the sequences. The corresponding reference binding affinity score is represented as $Score_{target} \\in [0,1]$. The TCR sequence $T$ is first transformed into its dense features $feat_{ter}$ via Trepr module, and the peptide sequence $P$ is simultaneously transformed into its dense features $feat_{pep}$ via PRepr module. These features are then used as inputs for the TCR-Pep cross-attention module, which generates a combined representation features $feat_{pep}$ capturing the interdependencies among TCR-peptide residues. Finally, the binding decoder BindDecoder takes the $feat_{pep}$ as input and predicts the binding affinity score $Score_{pred}$. The entire pipeline can be expressed as:\n$Score_{pred} = BindDecoder(feat_{ter}, feat_{pep}) \\in [0, 1]. (3)$\nThe final MSE loss is calculated as follows:\n$\\mathcal{L}_{DapPep} = MSE(Score_{pred}, Score_{target})). (4)$"}, {"title": "III. EXPERIMENTS", "content": "Datasets. Following PanPep [4], we adopt the majority dataset(MajorSet), zero-shot dataset(ZeroSet), and few-shot dataset(FewSet) for primary evaluation. All the binding TCRS are balanced by a controlTCRset, where the controlTCRset contains 60,333,379 non-binding TCRs (negative samples).\nEvaluation Metrics. The evaluation of the proposed models is performed using two important metrics: PR-AUC (Precision-Recall Area Under the Curve) and AU-ROC (Area Under the Receiver Operating Characteristic curve)."}, {"title": "B. DapPep Outperforms Baselines Across Different Settings", "content": "We investigate the generalization ability to predict binding affinity under three different data settings, i.e., majority setting, few-shot setting, and zero-setting, compared with the SOTA model PanPep, as shown in Figure 2. Overall, DapPep has excellent and balanced performances. For a fair comparison, we use the same evaluation datasets as PanPep.\nFew-shot Setting. We first analyze the comparison in the few-shot setting. In this setting, the baseline PanPep undergoes a meta-learning approach where the support set for each peptide-specific task in the meta-test dataset is used to finetune the model. In contrast, our trained DapPep does not undergo any continued training or finetuning and is directly validated for generalizability. As a result, our DapPep achieves an average of 0.784 ROC-AUC and 0.814 PR-AUC in the few settings, while PanPep only achieves an average of 0.734"}, {"title": "C. DapPep Specializes in Unseen Peptides", "content": "Evaluation of the zero-setting is our primary goal. In this test, we compare DapPep with existing tools (PanPep, pMTnet, ERGO2, and DLpTCR) that can predict unseen peptides bound with TCRs. The curated ZeroSet is used as the evaluation set, and the peptides in this dataset are not available in the training set of DapPep and other baseline tools."}, {"title": "D. Validation for Potential Clinical Applications", "content": "Adoptive cell transfer is a promising approach for cancer immunotherapy, but its efficiency essentially depends on the enrichment of tumor-specific T cells in the graft [2], [14]. We reproduce the results of baselines based on the gastrointestinal cancer dataset [25], where the dataset study combines next-generation sequencing technology with high-throughput immunological screening to identify tumor-infiltrating lymphocytes from patients with metastatic gastrointestinal cancer, collected from 10 different of neoantigens, and experimentally validated the specific TCRs they bind."}, {"title": "IV. CONCLUSIONS AND LIMITATIONS", "content": "With powerful pre-trained representations, DapPep accomplishes excellent performances on various benchmarks, especially highlighting the ability to generalize to unseen and few peptides, which is meaningful for exogenous or neonatal antigen recognition in computational immunology. Nevertheless, there are potential areas for improvements: 1) Incorporation of additional features or modalities, such as structural information or gene expression data, to enhance the predictive power. 2) Collaboration with experimental biologists and clinicians to validate the predictions in real-world settings and assess their potential for guiding immunotherapeutic interventions."}]}