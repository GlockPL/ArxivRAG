{"title": "Diffusion Model-Based Data Synthesis Aided Federated Semi-Supervised Learning", "authors": ["Zhongwei Wang", "Tong Wu", "Zhiyong Chen", "Liang Qian", "Yin Xu", "Meixia Tao"], "abstract": "Federated semi-supervised learning (FSSL) is primarily challenged by two factors: the scarcity of labeled data across clients and the non-independent and identically distribution (non-IID) nature of data among clients. In this paper, we propose a novel approach, diffusion model-based data synthesis aided FSSL (DDSA-FSSL), which utilizes a diffusion model (DM) to generate synthetic data, bridging the gap between heterogeneous local data distributions and the global data distribution. In DDSA-FSSL, clients address the challenge of the scarcity of labeled data by employing a federated learning-trained classifier to perform pseudo labeling for unlabeled data. The DM is then collaboratively trained using both labeled and precision-optimized pseudo-labeled data, enabling clients to generate synthetic samples for classes that are absent in their labeled datasets. This process allows clients to generate more comprehensive synthetic datasets aligned with the global distribution. Extensive experiments conducted on multiple datasets and varying non-IID distributions demonstrate the effectiveness of DDSA-FSSL, e.g., it improves accuracy from 38.46% to 52.14% on CIFAR-10 datasets with 10% labeled data.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated Learning (FL) allows multiple clients to collaboratively train machine learning models without the need to share raw data directly. It not only protects data privacy but also enhances the utilization of diverse and distributed data resources across different locations, thereby emerging as a pivotal technology for edge artificial intelligence applications [1]. The main challenge in FL is the substantial heterogeneity in data distribution across clients, commonly referred to as non-independent and identically distributed (non-IID) data [2]. This disparity can cause divergence between the global and local models, known as client drift [3]. Additionally, the scarcity of labeled data presents another obstacle, as obtaining large amounts of labeled data is often time-consuming and expensive in many domains, whereas unlabeled data is typically more readily available [4].\nTo address these challenges, researchers have turned to Federated Semi-Supervised Learning (FSSL). FSSL combines the advantages of semi-supervised learning, which can leverage unlabeled data, with the collaborative framework of FL. SemiFed [4] incorporates consistency regularization and pseudo-labeling techniques, utilizing consensus among multiple client models to generate high-quality pseudo-labels, showing effectiveness in heterogeneous data distribution. FedMatch [5] explores two scenarios: labels-at-client and labels-at-server, by introducing inter-client consistency loss and parameter decomposition, outperforming simple combinations of FL and semi-supervised learning in both cases. FedDure [6] presents an FSSL framework with dual regulators to manage non-IID data across and within clients.\nAnother approach is to use deep learning-based data augmentation techniques to aided FL. The synthetic data aided FL (SDA-FL) [7] framework shares synthetic data generated by locally pre-trained generative adversarial networks (GANs), utilizing an iterative pseudo labeling mechanism to improve consistency across local updates and enhance global aggregation performance in both supervised and semi-supervised cases. In [8], a global generator is collaboratively trained within the FL framework to produce synthetic data. FedDISC [9] introduces pre-trained diffusion models (DMs) [10] into FL, utilizing prototypes and domain-specific representations to generate high-quality synthetic datasets.\nFSSL and SDA-FL offer distinct approaches to addressing the challenges of heterogeneous data distribution and limited labeled data, providing valuable insights from two different perspectives. However, directly combining FSSL with SDA-FL poses new challenges. Existing SDA-FL methods often rely on pre-trained large generative models [9], which may not be suitable due to potential domain mismatches between the pre-trained models and the specific tasks. Additionally, training generative models capable of producing high-quality data [8] in FSSL is particularly difficult because of the scarcity of labeled data for model training and validation.\nMotivated by the observation, we propose a novel approach called Diffusion Model-based Data Synthesis Aided Federated Semi-Supervised Learning (DDSA-FSSL) to address the challenges in FL. Specifically, to overcome the challenge of data scarcity, a global classifier is employed for pseudo-labeling amounts of unlabeled data, and a precision-driven optimization process is applied to refine these pseudo-labeled samples, enhancing their quality and reliability. Instead of using pre-trained generative models, a global DM is collaboratively trained using both the labeled data and optimized pseudo-labeled data, thus avoiding domain mismatch issues. The DM enables clients to generate synthetic data for absent classes in their local datasets, effectively addressing the challenge of heterogeneous data distribution. Experimental results show that DDSA-FSSL significantly enhances classification accuracy compared to existing methods. For example, with 10% labeled CIFAR-10 data under dual heterogeneity, it raises accuracy from 38.46% to 47.72% using 10% synthetic data, and to 53.01% with 90% synthetic data."}, {"title": "II. SYSTEM MODEL", "content": "We consider a FSSL scenario with heterogeneous data distributions, specifically focusing on the labels-at-clients setting [5]. As shown in Fig. 1, the system consists of K clients, each with their own local data, and one base station (BS) with a central server that coordinates the FL process without having direct access to the client's data. FL seeks to train a global classifier by coordinating these clients, each of which trains a local model using its own data. The central server then applies a federated aggregation algorithm to obtain the global parameters $\\theta_g$.\nFor FSSL, each client's dataset consists of a labeled subset $D_l^k = \\{x_{k,i}, y_{k,i}\\} = \\cup_{c=1}^C \\{D_l^{k,c}\\}$ and an unlabeled subset $D_u^k = \\{x_{k,i}\\} = \\cup_{c=1}^C \\{D_u^{k,c}\\}$, where $x_{k,i}$ represents a labeled sample from client $k$, $y_{k,i}$ is its corresponding label, and $C$ denotes the total number of classes. In real-world applications, the assumption that all data is fully annotated is unrealistic due to the significant effort and cost associated with data labeling [5]. Consequently, the ratio of labeled data, denoted as $\\lambda = \\frac{|D_l^k|}{(|D_l^k| + |D_u^k|)}$, is typically small. In heterogeneous data distribution scenarios, both the labeled and unlabeled datasets on each client may only contain samples from a subset of classes, with few or no samples from other classes. Moreover, the distributions of labeled and unlabeled data across different clients are also heterogeneous [6]. This scarcity of labels and the heterogeneity of data distributions [6], [11] can result in significant performance degradation in FL.\nTo address this challenge, we consider that each client is equipped with a local conditional latent diffusion model (c-LDM). Unlike existing methods that rely on unsupervised GANs [7], which cannot generate samples for specific classes, c-LDM not only allows precise control over the synthesis of data for targeted classes, but also provides more stable training and generates higher-quality, more diverse synthetic data. While some approaches utilize pre-trained DMs like Stable Diffusion [12], these approaches have limitations in FSSL scenarios. Pre-trained models, trained on large-scale general datasets, often struggle to adapt to the client-specific data distributions and impose high computational demands, making them impractical for resource-constrained devices in FL environments."}, {"title": "III. DIFFUSION MODEL-BASED DATA SYNTHESIS AIDED FSSL", "content": "In this section, we introduce DDSA-FSSL framework that leverages DMs to generate synthetic data for classes. As show in Fig. 1, the proposed DDSA-FSSL is divided into five steps, which are detailed in the following subsections."}, {"title": "A. Enhancing Dataset with Pseudo-Labels", "content": "At the start of DDSA-FSSL, due to the scarcity of labeled data and the inherent complexity of high-dimensional parameter spaces in c-LDM, it is challenging to directly train a c-LDM capable of producing high-quality and diverse synthetic data. To leverage the unlabeled data $\\{D_u^k\\}_{k=1}^K$, clients first collaboratively train a global classifier using the FedAvg algorithm [13] based on the labeled data $\\{D_l^k\\}_{k=1}^K$.\nIn FedAvg, the local objective function for client $k$ at each communication round $r = 1, ..., R$ is defined as follows:\n$F_k(\\theta_k^r) \\triangleq E_{(x,y)\\sim D_l^k}L_c(\\theta_k^{r,e}; x, y),$\nwhere $L_c$ is the cross-entropy and $\\theta_k^{r,e}$ denotes the parameters of client $k$ after $e$ local updates in communication round $r$. During each local training epoch $e = 0, 1, ..., E$, each client updates its local parameters:\n$\\theta_k^{r,e+1} \\leftarrow \\theta_k^{r,e} - \\eta_r \\nabla F_k(\\theta_k^{r,e}), \\quad \\theta_k^{r,0} = \\theta_g^{r-1},$\nwhere $\\eta_r$ is the learning rate of round $r$.\nThe updated local parameters $\\theta_k^r$ for the $k$-th client are then sent back to the central server for parameters aggregation. The FedAvg algorithm aims to aggregate the local parameters of clients based on the amount of training data each client contributes:\n$\\theta_g^r \\triangleq \\sum_{k=1}^K p_k \\theta_k^r, \\quad p_k = \\frac{|D_l^k|}{\\sum_{k=1}^K |D_l^k|},$\nwhere $p_k$ is the aggregation weight for client $k$.\nWhen $e = 0, \\theta_k^{r,0} = \\theta_g^{r-1}$, which indicates the beginning of each local training round where clients download the global parameters from the central server.\nThrough the iterative process of local training and global aggregation, the central server is expected to converge to a global model with the global parameters $\\theta_g$:\n$\\theta_g = FedAvg(K, R, E, \\{D_l^k\\}_{k=1}^K, L_c).$\nThe global classifier is used to perform pseudo labeling on the unlabeled data. The resulting pseudo-labeled datasets are denoted as $\\{D_{pl}^k\\}_{k=1}^K = \\{\\{x_{k,i},\\hat{y}_{k,i}\\}\\}$, where $\\hat{y}_{k,i}$ denotes the corresponding pseudo-label of the unlabeled sample $x_{k,i}$. The data utilized for training the c-LDM comprises two components: labeled data $\\{D_l^k\\}_{k=1}^K$ and pseudo-labeled data $\\{D_{pl}^k\\}_{k=1}^K$. Ideally, the c-LDM should accurately learn the distribution of the training data, enabling it to generate synthetic data that similar to the true distribution. However, the presence of mislabeled samples within the pseudo-labeled data leads to the generation of synthetic data that deviates from the true distribution, thus compromising the quality and reliability of the synthetic data. To mitigate this issue, we propose a data selection method based on precision optimization, which aims to filter pseudo-labeled data and enhance the quality and reliability of the data participating in the c-LDM training process."}, {"title": "B. Precision-Optimized Data Selection", "content": "The confusion matrix reflects the characteristics of the samples and their corresponding labels, therefore, we use the confusion matrix to measure the accuracy of pseudo-labels for samples predicted as specific classes. For $D_l^k$, the confusion matrix $M_l^k$ is inherently a diagonal matrix, indicating perfect alignment between true and predicted labels. However, for $D_{pl}^k$, the confusion matrix $M_{pl}^k$ needs to be estimated, as the true labels are not known with certainty. Specifically, each client generates the confusion matrix $M_t^k$ by applying the global classifier to their local test set. These matrices are then uploaded to the central server, which aggregates them to construct a global confusion matrix $M_g = \\sum_{k=1}^K M_t^k$. Each client subsequently downloads $M_g$ to estimate the $M_{pl}^k$ based on the principle that each column in $M_g$ represents the distribution of true classes among samples predicted as a particular class. For each class $j$:\n$M_g[:, j] = \\frac{M_g[:, j]}{\\sum_{i=1}^C M_g[i, j]} = \\frac{\\sum_{k=1}^K M_t^k[i, j]}{\\sum_{i=1}^C \\sum_{k=1}^K M_t^k[i, j]} n_{k,j},$\nwhere $n_{k,j}$ is the number of samples pseudo-labeled as class $j$ in the $D_{pl}^k$, and $M_g[:, j]$ denotes the $j$-th column of $M_g$.\nLet $p_k = (p_{k,1}, \\cdots, p_{k,C})$ represents the proportion of each class in the $D_{pl}^k$ selected by client $k$, the confusion matrix of the training data can be written as $M(p_k) = M_l^k + M_{pl}^k \\cdot diag(p_k)$. Each client seeks to find the optimal selection of data that maximizes the average label precision:\n$P_k(p_k) = \\frac{1}{|T_k|} \\sum_{j\\in T_k} \\frac{M(p_k)[j, j]}{\\sum_{i=1}^C M(p_k)[i, j]},$\nwhere $T_k = \\{j: \\sum_{i=1}^C M(p_k)[i, j] \\neq 0\\}$.\nThe optimization problem $P_1$ can be formulated as:\n$\\max_{p_k} P_k(p_k) - w_{L_1} \\sum_{c=1}^C p_{k,c} - w_p \\frac{1}{C} \\sum_{c=1}^C (p_{k,c} - \\tau)^2$\ns.t. $0 \\leq p_{k,c} \\leq 1, \\quad c = 1, ..., C,$\nwhere $w_{L_1}$ denotes the weight for the $L_1$ regularization term, $w_p$ denotes the weight for the penalty term and $\\tau$ denotes the target proportion used to control the average selection proportion across all classes.\nThe $L_1$ regularization promotes sparsity in the optimal solution and the penalty function facilitates a trade-off between the quantity of data employed and the precision of labels. $P_1$ can be solved using sequential least squares programming (SLSQP). When the local optima proportion $p_k$ is obtained, each client randomly removes a corresponding proportion of data from each class in $D_{pl}^k$, resulting in the optimized pseudo-labeled datasets $\\{D_{pl}^{k*}\\}_{k=1}^K$."}, {"title": "C. Federated Diffusion", "content": "The c-LDM used in DDSA-FSSL consists of three components: an encoder $En(\\cdot)$, a decoder $De(\\cdot)$, and a conditional diffusion model (CDM) $D(\\cdot)$ operating in the latent space. The encoder and decoder are implemented using a"}, {"title": "Algorithm 1: Generate Class-conditional Synthetic Data", "content": "Input: Local labeled data distribution $|D_l^k|$, global data distribution $|D^g|$, augmentation strength $\\alpha$, timestep $T$ and global VAE's decoder $D_e(.)$\nOutput: Synthetic dataset $D_{syn}^k$\n// Calculate number of synthetic samples to generate\nfor $c= 1$ to $C$ do\n$|D_{syn}^{k,c}| = max(0, \\alpha(|D_l^k|+ \\frac{T_D^g}{TDT})|D_l^k|-\\frac{|D_l^k|}{TDT})$\nend\n$D_{syn}^k = \\{\\}$\nfor $c = 1$ to $C$ do\nfor $i = 1$ to $|D_{syn}^{k,c}|$ do\n$z_T \\sim N(0, I)$\n//Denote $\\bar{\\alpha}_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\Pi_{s=1}^t \\alpha_s$\nfor $t = T$ to $1$ do\n$\\epsilon \\sim N(0, I)$ if $t > 1$, else $\\epsilon = 0$\n$z_{t-1} = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\left(z_t - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta}(z_t, t, c)\\right) + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$\nend\n$x_{syn} = D_e(z_0)$\n$D_{syn}^k = D_{syn}^k \\cup \\{(x_{syn}, c)\\}$\nend\nend\nreturn $D_{syn}^k$\nVariational Autoencoder (VAE) [14]. The encoder encodes the input data $x$ into a latent representation $z = E_n(x)$, while the decoder reconstructs the data from the latent space, $x = D_e(z) = D_e (E_n(x))$.\nThe CDM follows a two-phase process: a forward process and a reverse process. The forward process consists of a series of $T$ timesteps, during which Gaussian noise is gradually added to a clean latent representation $z_0$ according to a variance schedule $\\beta_{1:T}$. The reverse process is then trained to reconstruct the original $z_0$ by progressively removing the noise from $z_t$. $z_t$ can be sampled in a single step given the $z_0$ and fixed variances:\n$z_t = \\sqrt{\\bar{\\alpha}_t}z_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t, \\quad \\epsilon_t \\sim N(0,1),$\nwhere $t \\in [1, T]$ and $0 < \\beta_{1:T} < 1$. The reverse process involves training a neural network, typically U-Net, denoted as $\\epsilon_{\\theta}$ to serve as the noise predictor by estimating the noise $\\epsilon_t$ at each timestep $t$. To enable each client to generate images of specific classes, we adopt a solution [15] that incorporates a cross-attention mechanism into the intermediate layers of the U-Net network. The loss function can be written as:\n$L_{CDM} = E_{z_t, y, \\epsilon \\sim N(0,1), t} [||\\epsilon_t - \\epsilon_{\\theta} (z_t, t, y) ||^2],$\nwhere $y$ represents the label associated with $z_t$.\nHere, all components of the c-LDM are collaboratively trained through FL. Local training of the CDM alone would typically result in models that are unable to generate samples for missing classes. While [7] attempted to address this issue by uploading locally generated synthetic data to form a global synthetic dataset, their method requires substantial communication resources, as it involves transmitting entire"}, {"title": "D. Synthetic Data Augmentation", "content": "To generate specific synthetic data that aligns local data distribution with the global data distribution, each client $k$ uploads its local data distribution $|D_l^k| = \\sum_{c=1}^C D_{k,c}$ to the central server. The server aggregates these distributions to construct the global data distribution $|D^g| = \\sum_{k=1}^K D_l^{k,c}$, which is then downloaded by each client. To measure the degree of synthetic data augmentation, we introduce a variable, denoted as augmentation strength $\\alpha$:\n$\\alpha = \\frac{|D_l^k|+|D_{syn}^k|}{|D_l^k|},$\nwhere $D_{syn}^k$ denotes the synthetic data generated by client $k$.\nThe augmentation strength $\\alpha$ determines the quantity of data in the synthetic dataset $D_{syn}^k$. Given the constraint that the distribution of the augmented local dataset $(D_{syn}^k \\cup D_l^k)$ matches the global data distribution $D^g$, $D_{syn}^{k^*}$ can be determined by (13). It is important to note that in cases of extreme data imbalance, such as when a client's dataset contains only one or two classes and lacks data from the remaining classes, (13) and $(D_{syn}^k \\cup D_l^k) \\sim D^g$ cannot be simultaneously satisfied. To address these scenarios, we implement a two-phase strategy. First, each client computes the total size of"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In this section, we present experimental results to verify the performance gain of the proposed DDSA-FSSL.\nA. Experimental Settings\nWe conduct extensive experimental analyses on two distinct datasets: CIFAR-10 and Fashion-MNIST. To simulate the complex data distribution imbalances commonly encountered in real-world scenarios, the simulation incorporates two types of non-IID imbalances [6]. 1) External imbalance: the labeled data distributions across different clients are heterogeneous. 2) Internal imbalance: within each client, the labeled and unlabeled data typically exhibit distinct distributions. The experiments can be divided into three scenarios based on the distribution of labeled and unlabeled data (\\{D_l^k\\}_{k=1}^K, \\{D_u^k\\}_{k=1}^K): (IID, IID), (IID, DIR), and (DIR, DIR). Here, DIR represents a non-IID scenario where data allocation follows a Dirichlet distribution Dir($\\gamma$). We set the concentration parameter $\\gamma$ to 0.1 across all datasets, determining the degree of data heterogeneity among the $K$ clients.\nFor the training of classifier, we use the ResNet-18 architecture as the default backbone. The Stochastic Gradient Descent (SGD) optimizer is employed with a momentum of 0.9, weight decay of 10-4, and a learning rate of $\\eta_c = 10^{-4}$. For the VAE, we adopt the architecture outlined in [17], with the encoder downsampling factor set to $f = H/h = W/w = 2 [15]. The VAE is optimized using the Adam optimizer with a learning rate of $\\eta_m = 10^{-4}$. The exponential decay rates for the first and second moment estimates are set to 0.5 and 0.9, respectively. For the CDM, we employ a U-net convolutional neural network to approximate the predicted noise $\\epsilon_{\\theta}$. We use the diffusion parameters from [18] with $T = 1000$ timesteps and a linear noise schedule with $\\beta_1 = 10^{-4}$ and $\\beta_{1000} = 2 \\times 10^{-2}$. Additionally, the Adam with weight decay (AdamW) optimizer is used with a learning rate of $\\eta_a = 2 \\times 10^{-4}$. In all FL scenarios, we adopt the FedAvg [13] algorithm, as it serves as a general approach. This choice is motivated by our focus on evaluating the impact of generated synthetic data on the system. It is worth noting that the parameter aggregation algorithm can be replaced with alternative algorithms tailored for FSSL.\nB. Results Analysis"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel FSSL framework, DDSA-FSSL, based on DMs to tackle the challenges of data scarcity and non-IID distributions in FL. DDSA-FSSL utilizes collaboratively trained DMs to enable clients to generate synthetic data for missing classes of specific tasks. Additionally, ablation studies show that our proposed precision-optimized data selection method can improve the quality of the generated synthetic data, thereby leading to additional performance gains."}]}