{"title": "Choice between Partial Trajectories", "authors": ["Henrik Marklund", "Benjamin Van Roy"], "abstract": "As AI agents generate increasingly sophisticated behaviors, manually encoding human preferences\nto guide these agents becomes ever more challenging. To address this, it has been suggested that\nagents instead learn preferences from human choice data. This approach requires a model of choice\nbehavior that the agent can use to interpret the data. For choices between partial trajectories of\nstates and actions, previous models assume choice probabilities to be determined by the partial\nreturn or the cumulative advantage.\nWe consider an alternative model based instead on the bootstrapped return, which adds to the\npartial return an estimate of the future return. Benefits of bootstrapped return over partial return\nand cumulative advantage models stem from their treatment of human beliefs. Unlike partial return,\nchoices based on bootstrapped return reflect human beliefs about the environment. Further, while\nrecovering the reward function from choices based on cumulative advantage requires that those\nbeliefs are correct, doing so from choices based on bootstrapped return does not. In this sense, the\nbootstrapped return model disentangles goals from beliefs.\nTo motivate the bootstrapped return model, we formulate axioms and prove an Alignment The-\norem. This result formalizes how, for a general class of human preferences, such models are able to\ndisentangle goals from beliefs. This ensures recovery of an aligned reward function when learning\nfrom choices based on bootstrapped return.\nThe bootstrapped return model also affords greater robustness to choice behavior. Even if human\nchoices are based on partial return, learning via a bootstrapped return model recovers an aligned\nreward function. The same holds with choices based on the cumulative advantage if the human and\nthe agent both adhere to correct and consistent beliefs about the environment. On the other hand,\nif choices are based on bootstrapped return, learning via partial return or cumulative advantage\nmodels does not generally produce an aligned reward function.", "sections": [{"title": "1 Introduction", "content": "To align an AI agent with human goals, it is common to design the agent to accumulate rewards that\nexpress human preferences. Manual specification of an effective reward function, however, is notoriously\ndifficult [Akrour et al., 2012, Hadfield-Menell et al., 2016]. And mispecification engenders reward hacking.\nAlignment can be improved via a reward function learned from human choice data [Akrour et al., 2012,\nSadigh et al., 2017, Christiano et al., 2017, Ibarz et al., 2018, Brown and Niekum, 2019].\nTo generate choice data, it is common to present a human with pairs of action-observation trajectories\nand ask which they prefer. Learning from this data requires a model of human choice behavior. In\nexisting models, the probability of choosing one trajectory over another is determined by scores assigned\nto the two, with each score calculated using a reward function. Christiano et al. [2017] popularized the\nuse of partial return, expressed in the first row of Table 1. We use the term partial to indicate that this\nis the return over a partial trajectory, which is of finite length, as opposed to an infinite trajectory."}, {"title": "2 Axiomatic Motivation", "content": "We will establish that, under two axioms, a human's preferences between infinite trajectory lotteries are\ndetermined by their preferences between partial trajectories. We begin by formally defining these terms\nand then state axioms and results."}, {"title": "2.1 Trajectories and Lotteries", "content": "Let S and A be finite state and action spaces. We refer to any sequence of state-action pairs as a\ntrajectory. An infinite trajectory takes the form (so, ao, 81,91,...). Let Ho be the set of all infinite\ntrajectories. A partial trajectory takes the form (so, ao,..., \u0430\u0442\u22121, st), beginning and ending at states\nand lasting over any finite duration T. Let H be the set of all partial trajectories.\nA policy \\( \\pi \\) assigns a probability \\( \\pi(a|s) \\) to each action given a current state s. A transition matrix\n\\( P\\in\\mathbb{R}^{A \\times S|\\times|S} \\) assigns a probability \\( P_{ass'} \\) over next state s' given a current state-action pair (s, a).\nEach state s, policy \\( \\pi \\), and transition matrix P, induce a distribution \\( P_{\\pi,P}(\\cdot|s) \\) over infinite trajec-\ntories. For any partial trajectory h = (so,ao,...,sr), let \\( P_{\\pi,P}(h) \\) be the distribution over infinite\ntrajectories that begin with h. In particular, if H\u221e = (So, A0, S1, A1, . . .) is sampled from \\( P_{\\pi,P}(\\cdot|h) \\) then\n(So, A0,..., ST) = h, and (ST, AT, ST+1, ...) is distributed according to \\( \u03a1_{\u03c0,\u03a1}(ST) \\).\nA lottery over infinite trajectories is a probability distribution. For example, for any policy \\( \\pi \\), transition\nmatrix P, and partial trajectory h, the probability distribution \\( P_{\\pi,P}(\\cdot|h) \\) is a lottery. Given a lottery l and\nfunction f: H\u221e \u2192 R, we denote by \\( E_e[f(H_{\\infty})] \\) the expectation of f(H\u221e) with H\u221e = (So, A0, S1, A1, ...)\nsampled from l. Similarly, we denote by \\( E_{\\pi,\u03a1}[f(H_{\\infty})|h] \\) the expectation with respect to the lottery\n\\( \u03a1_{\u03c0,\u03c1}(h) \\).\nTo simplify analysis we will assume that a notion of relative state frequency is defined for each lottery.\nIn particular, we will restrict attention to lotteries l for which the limit \\( \\lim_{y\\rightarrow\\infty} E_e[\\frac{1}{T} \\sum_{t=0}^{T-1} 1(S_t = s)] \\)\nexists for each s\u2208 S. Let Loo be the set of such lotteries."}, {"title": "2.2 Preferences between Lotteries", "content": "Let \\( \\succeq \\) be a binary relation on Lo. We interpret this relation as indicating the human's preferences\nbetween infinite trajectory lotteries. For each infinite trajectory lottery l\u2208 H\u221e, discount factor \u03b3 \u0395\n(0, 1), and function r : S \u00d7 A \u2192 R, let\n\\[\nVe,r,y = \u0395 \u03a3 (St, At)\nLt=0\n\\]\nWe interpret this as the expected discounted return of the lottery. A reward function expresses preferences\nbetween infinite trajectory lotteries in the following sense."}, {"title": "Definition 1. (expressing \u2265)", "content": "A function r : S \u00d7 A \u2192 R is said to express \\( \\succeq \\) if, for all l, l' \u2208 L\u221e,\n\\[\nl \\succeq l'  \\quad \\text{if and only if} \\quad \\lim_{\\uparrow 1}(ve,r,y - Ve',r,y) \\geq 0.\n\\]\nLoosely speaking, when r expresses \\( \\succeq \\), lotteries with higher expected cumulative reward are preferred."}, {"title": "2.3 Preferences between Partial Trajectories", "content": "In practice, we cannot elicit preferences between infinite trajectory lotteries. This is because infinite\ntrajectories - let alone distributions over infinite trajectories - comprise too much data for a human to\nprocess. Instead, we can elicit preferences between partial trajectories. This can be done, for example,\nby presenting pairs of partial trajectories to a human and observing choices.\nTo model what drives these choices, we consider a binary relation \\( \\succeq_\\partial \\) between partial trajectories H. We\ninterpreta \\( \\succeq_\\partial \\) as indicating the human's preferences between partial trajectories.\nWe now define what it means for a pair of functions (r, V) to express preferences between partial trajec-\ntories."}, {"title": "Definition 2. (expressing \u2265\u0259)", "content": "A pair (r, V), comprised of functions r : S\u00d7A \u2192 R and V : S \u2192 R,\nis said to express \\( \\succeq_\\partial \\) if, for all h,h' \u2208 H,\n\\[\nh \\succeq_\\partial h'  \\quad \\text{if and only if} \\quad \\sum_{t=0}^{T-1}r(S_t, a_t) + V(S_T) \\ge \\sum_{t=0}^{T'-1}r(s'_t, a'_t) + V(s'_T),\n\\]\nwhere (so, ao,..., sr) = h and (s\u00f3, a, ..., s'\u2081\u2081) = h'.\nWe refer to the quantity \\( \\sum_{t=0}^{T-1}r(s_t, a_t) + V(s_T) \\) as a boostrapped return. It is bootstrapped in the sense\nthat it relies on a value V(sr) that can be interpreted as approximating subsequent return. In the\ndefinition, (r, V) expresses preferences via comparing bootstrapped returns."}, {"title": "2.4 Alignment", "content": "In this section, we relate preferences over partial trajectories to preferences over infinite trajectory\nlotteries. We do so through proving a result that relies on two axioms: one constrains \\( \\succeq \\) and the other\nconstraints how \\( \\succeq_\\partial \\) relates to \\( \\succeq \\).\nOur first axiom requires that \\( \\succeq \\) can be expressed by a reward function. One could alternatively assert\nmore primitive axioms, along the lines considered in rational choice theory [Von Neumann and Morgen-\nstern, 1947, Koopmans, 1960, 1972, Bastianello and Faro, 2019], that imply reward representation. We\nforgo that and simply assert reward representation itself as an axiom in order to focus attention on what\nis required beyond that to form our theory of alignment."}, {"title": "Axiom 1. (reward representation)", "content": "There exists a function r : S \u00d7 A \u2192 R that expresses \\( \\succeq \\)"}, {"title": "Axiom 2. (alignment)", "content": "There exists (\u03c0, \u03a1) \u2208 P such that, for all partial trajectories h and h',\n\\[\nh \\succeq_\\partial h'  \\quad \\text{if and only if} \\quad \\mathbb{\u03a1}_{\u03c0,\u03a1}(\\cdot|h) \\succeq \\mathbb{\u03a1}_{\u03c0,\u03a1}(\\cdot|h').\n\\]\nTo interpret this axiom, think of (\u03c0, P) as how the human imagines the future to be generated. The\naxiom says that the human will prefer a partial trajectory h over h' if and only if the infinite trajectory\nlottery P\u3160,P(h) is preferred over P\u3160,P(h'). The requirement that (\u03c0, \u03a1) \u2208 P can be interpreted as\nassuming the human believes that each partial trajectory bears transient impact on the future state\nevolution."}, {"title": "Theorem 1. (alignment)", "content": "Suppose that (\\( \\succeq \\),\\( \\succeq_\\partial \\)) satisfies Axioms 1 and 2.\n1. There exists a pair (r, V) that expresses \\( \\succeq_\\partial \\).\n2. If (r, V) expresses \\( \\succeq_\\partial \\) then r expresses \\( \\succeq \\).\nThe first part of this theorem indicates that preferences between partial trajectories are determined by\nbootstrapped returns, calculated via some reward function and value function. The second part indicates\nthat this reward function, in turn, determines preferences between lotteries."}, {"title": "2.5 Fictitious States and Actions", "content": "A pair of preferences (\\( \\succeq \\), \\( \\succeq_\\partial \\)) can violate Axiom 2 though they ought to satisfy the conclusions of Theorem\n2, which ensure that \\( \\succeq_\\partial \\) can be expressed by some (r, V) and that the reward function r is aligned with\n\\( \\succeq \\). At a high level, the issue is that our current axiom demands a kind of consistency between V and r.\nFor instance, if V = 0 everywhere, that implies r must be 0 everywhere also. This is an unnecessarily\nstrong restriction and could motivate weakening the axiom. However, the issue can alternatively be\naddressed by expanding the state and action spaces. We now explain the nature of such preferences and\nhow to address them."}, {"title": "Corollary 1. (alignment on a subset)", "content": "Suppose that (\\( \\succeq \\),\\( \\succeq_\\partial \\)) satisfies Axioms 1 and 2. For any\nS'S and A' \u2286 A:\n1. There exists a pair (r, V) that expresses \\( \\succeq_\\partial \\) on (S', A').\n2. If (r, V) expresses \\( \\succeq_\\partial \\) on (S', A') then r expresses \\( \\succeq \\) on (S', A').\nThis corollary follows almost immediately from Theorem 1. To illustrate its use, let us revisit our\nexample. Let S' and A' be the original state and action spaces, before enlargement to S and A. Our\noriginal example began with relations > and \\( \\succeq_\\partial \\) on (S', A'), with the latter expressed by some (r, V).\nThe corollary ensures that r is aligned with \\( \\succeq \\) on (S', A'), which is the same conclusion we drew from\narguments specific to this example made before stating this general corollary.\nThis corollary justifies our treatment later in the paper of reward learning from choices determined by\npreferences that do not satisfy Axiom 2. For example, consider what happens when a human chooses\nwhichever partial trajectory maximizes partial return with respect to some reward function r, which also\nexpresses \\( \\succeq \\). This is equivalent to bootstrapped return with a value function V that assigns zero value\nto every state. The preference relations violate Axiom 2, but this can be addressed by expanding the\nstate and action spaces and conjuring Corollary 1 to ensure that a reward function learned from choices\naligns with \\( \\succeq \\)."}, {"title": "3 Reward Learning", "content": "In this section, we will discuss an approach to learning a reward function from choice data. The point\nis for this reward function to express a human's preferences between infinite trajectory lotteries. Such\na reward function enables alignment of agent behavior with the human's preferences. In particular, by\nincreasing expected cumulative reward, the agent better serves the human's interests."}, {"title": "3.1 Choice Data", "content": "Our approach learns from choice data. By this we mean a set D of triples (h, h', y) \u2208 D, each comprising\ntwo partial trajectories h and h' and a choice y \u2208 {0,1}. A choice of h is indicated by y = 1, while a\nchoice of h' is indicated by y 0.\nWe interpret this dataset as a set of queries, each consisting of a pair of partial trajectories, presented\nto a human, who then chooses between the two. A reward learning algorithm takes this data as input\nand produces a reward function as output."}, {"title": "3.2 Deterministic Choice", "content": "In order to learn from choice data, we need to interpret how the human makes choices. In the simplest,\nperhaps idealized, case, we could assume that each choice y perfectly reflects the humans preference\nbetween partial trajectories. In particular, y = 1 if h \\( \\succeq_\\partial \\) h' and y = 0 if h' \\( \\succeq_\\partial \\) h. Otherwise, if the\nhuman is indifferent, the choice y is randomly sampled, taking value 0 or 1 with equal probability.\nThe set H \u00d7 H of partial trajectory pairs is countably infinite. Suppose that, as D is made up of random\npartial trajectory pairs and choices made in response, sampled such that each pair eventually appears"}, {"title": "3.3 Stochastic Choice", "content": "In practice, it is unrealistic to expect a human to consistently make choices that perfectly match their\npreferences. For instance, if the human is almost indifferent between two trajectories, their choices may\nvary if presented with the same two trajectories at different points in time.\nAssuming choices to be stochastic is more realistic. To model stochastic choice over partial trajectories,\nwe use a choice probability function p : H \u00d7 H \u2192 [0,1]. We interpret p(h, h') as the probability with\nwhich the human chooses h over h' when presented with these two partial trajectories. Each choice is\nsampled independently according to this probability. Note that p(h, h') + p(h', h) = 1.\nFor choices to convey preferences, they need to reflect those preferences more often than not. Perhaps\nthe weakest assumption of this sort that enables learning preferences from choices is that, for all partial\ntrajectories h, h' \u2208 H,\n\\[\np(h, h') \\geq 0.5  \\quad \\text{if and only if} \\quad h \\succeq_\\partial h'.\n\\]\nDeterministic choice is a special case in which p(h,h') = 1 if h\\( \\succeq_\\partial \\) h'. Under this assumption, the\npreference relation \\( \\succeq_\\partial \\) can again be learned from data generated by any deterministic or stochastic choice\nmodel. In particular, suppose that, as D grows, choices between any pair h and h' can be averaged to\nidentify p(h, h') and thus the human's preference between h and h'. By identifying preferences between\nall pairs, we can again obtain a reward function that identifies \\( \\succeq \\) and thus enables alignment between\nagent and human.\nOne well versed in choice theory may wonder whether stronger assumptions on the stochastic relation\nbetween preferences and choice ought to be required to identify \\( \\succeq \\). For example, it is common to require\nthat choices not only between outcomes but also between lotteries be observed. This is not needed\nin our context because the random mixing induced by lotteries can instead be induced by temporal\nmixing across an infinite trajectory. That is, an infinite trajectory effectively serves as a lottery in which\nprobabilities are determined by relative state frequencies. While we do not consider requiring the human\nto compare infinite trajectories, long partial trajectories can approximate the effect arbitrarily well.\nStronger assumptions about the stochastic choice model can ensure identifiability even with choices\nbetween partial trajectories that are short. For example, this is the case with the logit model considered\nin the next section."}, {"title": "3.4 The Logit Choice Model", "content": "To reduce the amount of data and computation required for reward learning, it is common to assume\nthat \\( \\succeq_\\partial \\) presides in a particular function class. We refer to such a function class as a stochastic choice\nmodel. The most common is the logit choice model. In the logit choice model, each partial trajectory in\nHis assigned a numerical score. In our case, we will take this score to be the bootstrapped return for\nsome reward function r and value function V. Then, the choice probability for partial trajectories h and\nh' is given by\n\\[\np(h,h') = \\sigma\\Bigg(\\frac{ \\sum_{t=0}^{T-1}r(s_t, a_t) + V(s_T) - \\sum_{t=0}^{T'-1}r(s'_t, a'_t) - V(s'_{T'})}{\\sigma}\\Bigg)\n\\]\nwhere (so, ao,..., sr) = h, (s\u00f3, a, ..., s',) = h', and \u03c3 : R \u2192 (0,1) is the standard logistic function.\nThis is sometimes referred to as the Bradley-Terry model for binary choice with unit temperature.\nData and computation requirements can be reduced further by constraining the the functions r and V.\nIn particular, these we can take these functions to be parameterized by a vector \u03b8\u2208 RK. For example,\nre(s, a) and Ve(s) could be outputs of a neural network architecture with tunable parameters 0 that takes\ns and a as inputs. In this case, the choice probability model can itself be viewed as a neural network"}, {"title": "3.5 Computational Example", "content": "We will now study reward learning via a logit choice model based on bootstrapped return with a tabular\nrepresentation of reward and value functions. Our aim is to generate insights that extend beyond tabular\nrepresentations. In particular, our study demonstrates that:\n1. A reward function that aligns with human preferences can be recovered from choice data even if\nthe human makes choices based on erroneous beliefs about the environment. In particular, we\nrecover the same reward function from two humans who share goals but make different choices due\nto differing beliefs.\n2. When using the logit choice model, practical learning algorithms infer the reward function, with\naccuracy diminishing quickly as choice data accumulates. This is true even if partial trajectories\nin each pair are short. And even if partial trajectories in each pair terminate at different states.\nBut learning can be accelerated by selecting each pair to share a common terminating state.\nWe consider two humans who are trying to communicate their goals to a robot. The two humans have\na common goal, but have very different beliefs about how the robot can best achieve this goal. Because\nthe two humans have different beliefs, they will make different choices when asked to compare partial\ntrajectories.\nThe robot acts on a two-dimensional grid with coordinates ranging from (0,0) to (4, 4), as illustrated in\nFigure 2. Both humans wish for the robot to go the top right corner, which we call the goal state, and\nthen stay there. Specifically, each human's preference relation over lotteries is expressed by a reward\nfunction r: S \u00d7 A \u2192 R, which assigns reward 1 to the goal state and 0 everywhere else. For simplicity,\nwe assume that the human's reward function does not depend on the action, but only on state. This is\nequivalent to assuming that |A| = 1.\nDiffering beliefs. The two humans share the following beliefs about the dynamics of the environment.\nBoth humans know that the robot can only move in one of the four directions, or stay put. The humans\nalso agree that at each timestep the robot progresses towards the goal, in the sense of reducing the\nminimal number of steps to the goal, with 90% probability. With 10% probability, the agent stays put.\nIf there are multiple next states that progress toward the goal, these are assigned equal probability. Once\nthe robot reaches the goal, it stays there forever."}, {"title": "4 Robustness of Bootstrapped Return", "content": "In this section, we discuss benefits to robustness of learning via the bootstrapped return rather than\nthe partial return and cumulative advantage models. These benefits are summarized in Table 2. The\nfollowing sections will elaborate on and justify claims made in this table."}, {"title": "4.1 Choice Based on Partial Return", "content": "We will now assume that choices are based on partial return. This corresponds to the first row in Table 2.\nWe first note that if the agent knows that choices are made based on partial return, then under suitable\nconditions, the agent can clearly recover \\( \\succeq \\).\nWe now ask: if the agent assumes that choices are based upon bootstrapped return, will it still recover\na reward function that expresses \\( \\succeq \\)?\nFortunately, the answer is yes. To see this, note that the bootstrapped return \\( \\sum_{t=0}^{T-1}(s_t, a_{t+1}) +V(s_T) \\)\nsimplifies to the partial return \\( \\sum_{t=0}^{T-1} r(s_t, a_{t+1}) \\) if we set V(s) = 0 for all s \u2208 S. Thus, the bootstrapped"}, {"title": "4.2 Choice Based on Cumulative Advantage", "content": "We now turn attention to choices based on cumulative advantage. This corresponds to the second row\nof Table 2. We separately treat cases where the human computes advantages using correct and incorrect\ntransition probabilities. In all cases, we assume that agent knows correct transition probabilities."}, {"title": "4.2.1 Preliminaries", "content": "We first introduce some notation and basic mathematical results that will facilitate our discussion. For\nany state s, action a, policy \u03c0, transition matrix P, reward function r, and discount factor \u03b3\u2208 (0,1),\nvalue functions are defined by\n\\[\nV_{\\pi,P,r,\\gamma} (s) = \u0395_{\\pi,\u03a1} \\Bigg[\\sum_{t=0}^{\\infty}\\gamma^t r(S_t, A_t) \\Bigg| s\\Bigg]  \\quad \\text{and} \\quad Q_{\\pi,P,r,\\gamma}(s,a) = r(s, a) + \\sum_{s'\\in S} P_{ass'} V_{\\pi,P,r,\\gamma} (s'),\n\\]\nand the corresponding advantage function is \\( \u0394_{\\pi,P,r,\\gamma}(s) = Q_{\\pi,P,r,\\gamma}(s, a) \u2013 V_{\\pi,P,r,\\gamma}(s) \\). As the discount\nfactor approaches one, there is a well-defined limit, which we can then maximize over policies:\n\\[\n\u0394_{\\pi,\u03a1,r}(s) = \\lim_{\\uparrow 1} \u0394_{\\pi,\u03a1,r,\\gamma} (s)  \\quad \\text{and} \\quad \u0394^*_{,P,r}(s) = \\max_{\\pi} \u0394_{\\pi,\u03a1,r}(s).\n\\]\nIn Section 1, we denoted the optimal advantage function by A. The additional subscripts we introduce\nfor this section make explicit the dependence on P and r.\nWe define a sense in which a reward function r expresses preferences between policies for a fixed transition\nmatrix P."}, {"title": "Definition 3. (expressing \u2265 for P)", "content": "A function r : S \u00d7 A \u2192 R is said to express \\( \\succeq \\) on P if, for all \u03c0\nand \u03c0' such that (\u03c0, \u03a1) \u2208 P and (\u03c0', P) \u2208 P,\n\\[\nl \\succeq l'  \\quad \\text{if and only if} \\quad \\lim_{\\uparrow 1}(V_{\\pi,r,y}(s) \u2013 V_{\\pi',r,y}(s)) \\geq 0,\n\\]\nfor all s \u2208 S, where l,l' \u2208 L\u221e are lotteries induced by (s,\u03c0, P) and (s, \u03c0', P), respectively.\nIf a learning algorithm recovers a reward function that expresses \\( \\succeq \\) on P, where P is the correct\ntransition matrix, then its use in comparing policies will be consistent with ~. Note that the requirement\nthat (\u03c0, P) and (\u03c0', P) are in P ensures that the preference between policies is consistent across all states\nS."}, {"title": "Theorem 2. (advantages as rewards)", "content": "For all r, \u03c0, \u03c0, and \u03b3\u2208 (0,1),\n\\[\nV_{\\pi,P,r,\\gamma}(s) \u2013 V_{\\pi,P,\\\u0159,\\gamma}(s) = V_{\\bar{\\pi},P,r,\\gamma}(s),\n\\]\nwhere \\( \\check{r} = \u0394_{\\pi,P,r,\\gamma} \\).\nThe V\u5143,P,r,y(s) on the right hand side does not change with \u03c0. Hence, the difference V\u03c0,P,r,y(s) \u2013\nV\u03c0,P,\u0159, (s) is constant across \u03c0. In other words, discounted values assessed byr and are consistent. In\nparticular, for each starting state, the ordering of policies by value is the same.. The following corollary\nfollows almost immediately."}, {"title": "Corollary 2. (advantages express > on P)", "content": "If r expresses \\( \\succeq \\) then, for all \u03c0 and P, \\( \u0394_{\\pi,\u03a1,r} \\) expresses\n\\( \\succeq \\) on P."}, {"title": "Corollary 3. (imputed rewards express > on P)", "content": "For all P, \u3160, and r that expresses \\( \\succeq \\), any \u00ef for\nwhich \\( \u2206_{\\pi,P,r} = \u2206_{\\pi, P,\\check{r}} \\) expresses \\( \\succeq \\) on P."}, {"title": "4.2.2 Cumulative Advantage Based on Correct Transition Probabilities", "content": "Let P denote correct transition probabilities. Recall that, with the cumulative advantage model, choice\nprobabilities are determined by scores of the form \\( \\sum_{t=0}^{T-1} \u0394^*_{,P,r}(S_t, a_{t+1}) \\). In this section, we will make a\nweaker assumption that the human uses scores of the form \\( \\sum_{t=0}^{T-1} \u0394_{\\pi,P,r} (S_t, a_{t+1}) \\) for some policy that\nis not necessarily optimal. We will establish that if the human knows (\u3160, P) then an agent that knows\n(\u3160, P) and learns via any of the three models we consider will recover a reward function that expresses\n\\( \\succeq \\) on P. The results of this section extend those of Knox et al. [2024b] and Knox et al. [2024a].\nFirst consider learning via the cumulative advantage model itself. If the agent can identify choice\nprobabilities for every pair of trajectories, which is the case given infinite data of the right sort, then the\nagent can identify \\( \u0394_{\\pi,P,r} \\). And since the agent knows and P, the agent can impute a reward function\n7 such that \\( \u2206_{\\pi,P,\\check{r}} = \u2206_{\\pi,P,r} \\). It follows from Corollary 3 that expresses \\( \\succeq \\) on P.\nWe next consider learning via the partial and bootstrapped return models. Choices determined by\nthe cumulative advantage \\( \\sum_{t=0}^{T-1} \u0394_{\\pi,\u03a1,r} (S_t, A_{t+1}) \\) are identical to ones based on the bootstrapped return\n\\( \\sum_{t=0}^{T-1}(S_t, a_{t+1}) + V(s_T) \\) with with reward function\u0159 = \\( \u0394_{\\pi,\u03a1,r} \\) and value function V = 0. Thes choices\nare also identical to ones based on partial return with reward function 7 = \\( \u0394_{\\pi,\u03a1,r} \\). Consequently, learning\nvia a bootstrapped return model will recover (\u0159, V) and learning via a partial return model will recover\n\u0159. In either case, we obtain a reward function r = \\( \u0394_{\\pi,P,r} \\) that, by Corollary 2, expresses \\( \\succeq \\) on P."}, {"title": "4.2.3 Cumulative Advantage Based on Incorrect Transition Probabilities", "content": "We now consider choices made based on a transition matrix P \u2260 P, which expresses human be-\nliefs about the environment. In this case, choice probabilities are determined by scores of the form\n\\( \\sum_{t=0}^{T-1} \u0394^*_{,P,r}(S_t, a_{t+1}) \\).\nWhen learning via a cumulative advantage model, the agent recovers Apr since these are determined\nby the choice probabilities. From this, the agent infers a reward function that results in the same"}, {"title": "4.3 Choice Based on Bootstrapped Return", "content": "We now consider the case when choices are based on bootstrapped return. This corresponds to the third\nrow of Table 2. We first note that if the agent knows that choices are made based on bootsrapped return,\nthen under suitable conditions, the agent can clearly recover \\( \\succeq \\), as established in previous sections.\nLet us consider the case when the agent assumes that choices are made by partial return. We first note\nthat the agent is then misspecified in the sense that there is no way for the agent to perfectly estimate\nchoice probabilities unless V = 0. A corollary is that what reward function that is recovered depends\non the distribution over trajectory pairs shown to the human. We will leverage this fact. Consider some\narbitrary MDP and consider two state-action pairs (s, a) and (s, a') with a shared state s. Assume that\nr(s, a) =0 r(s, a') = 0.5.\nFurther, consider two states s' and s\" that will serve as terminal states in the trajectories that we will\nconstruct. Assume that\n\\[\nV(s') = \u2212 1 \\quad V(s\") = 1.\n\\"}]}