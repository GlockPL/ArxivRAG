{"title": "TIMEKAN: KAN-BASED FREQUENCY DECOMPOSITION LEARNING ARCHITECTURE FOR LONG-TERM TIME SERIES FORECASTING", "authors": ["Songtao Huang", "Zhen Zhao", "Can Li", "Lei Bai"], "abstract": "Real-world time series often have multiple frequency components that are intertwined with each other, making accurate time series forecasting challenging. Decomposing the mixed frequency components into multiple single frequency components is a natural choice. However, the information density of patterns varies across different frequencies, and employing a uniform modeling approach for different frequency components can lead to inaccurate characterization. To address this challenges, inspired by the flexibility of the recent Kolmogorov-Arnold Network (KAN), we propose a KAN-based Frequency Decomposition Learning architecture (TimeKAN) to address the complex forecasting challenges caused by multiple frequency mixtures. Specifically, TimeKAN mainly consists of three components: Cascaded Frequency Decomposition (CFD) blocks, Multi-order KAN Representation Learning (M-KAN) blocks and Frequency Mixing blocks. CFD blocks adopt a bottom-up cascading approach to obtain series representations for each frequency band. Benefiting from the high flexibility of KAN, we design a novel M-KAN block to learn and represent specific temporal patterns within each frequency band. Finally, Frequency Mixing blocks is used to recombine the frequency bands into the original format. Extensive experimental results across multiple real-world time series datasets demonstrate that TimeKAN achieves state-of-the-art performance as an extremely lightweight architecture. Code is available at https://github.com/huangst21/TimeKAN.", "sections": [{"title": "INTRODUCTION", "content": "Time series forecasting (TSF) has garnered significant interest due to its wide range of applications, including finance (Huang et al., 2024), energy management (Yin et al., 2023), traffic flow planning (Jiang & Luo, 2022), and weather forecasting (Lam et al., 2023). Recently, deep learning has led to substantial advancements in TSF, with the most state-of-the-art performances achieved by CNN-based methods (Wang et al., 2023; donghao & wang xue, 2024), Transformer-based methods(Nie et al., 2023; Liu et al., 2024b) and MLP-based methods (Zeng et al., 2023; Wang et al., 2024a).\nDue to the complex nature of the real world, observed multivariate time series are often non-stationary and exhibit diverse patterns. These intertwined patterns complicate the internal relationships within the time series, making it challenging to capture and establish connections between historical observations and future targets. To address the complex temporal patterns in time series, an increasing number of studies focus on leveraging prior knowledge to decompose time series into simpler components that provide a basis for forecasting. For instance, Autoformer (Wu et al., 2021) decomposes time series into seasonal and trend components. This idea is also adopted by DLinear (Zeng et al., 2023) and FEDFormer (Zhou et al., 2022b). Building on this foundation, TimeMixer (Wang et al., 2024a) further introduces multi-scale seasonal-trend decomposition and highlights the importance of interactions between different scales. Recent models like TimesNet (Wu et al.,"}, {"title": "RELATED WORK", "content": null}, {"title": "KOLMOGOROV-ARNOLD NETWORK", "content": "Kolmogorov-Arnold representation theorem states that any multivariate continuous function can be expressed as a combination of univariate functions and addition operations. Kolmogorov-Arnold Network (KAN) (Liu et al., 2024c) leverages this theorem to propose an innovative alternative to traditional MLP. Unlike MLP, which use fixed activation functions at the nodes, KAN introduces"}, {"title": "TIME SERIES FORECASTING", "content": "Traditional time series forecasting (TSF) methods, such as ARIMA (Zhang, 2003), can provide sufficient interpretability for the forecasting results but often fail to achieve satisfactory accuracy. In recent years, deep learning methods have dominated the field of TSF, mainly including CNN-based, Transformer-based, and MLP-based approaches. CNN-based models primarily apply convolution operations along the temporal dimension to extract temporal patterns. For example, MICN (Wang et al., 2023) and TimesNet (Wu et al., 2023) enhance the precision of sequence modeling by adjusting the receptive field to capture both short-term and long-term views within the sequences. ModernTCN (donghao & wang xue, 2024) advocates using large convolution kernels along the temporal dimension and capture both cross-time and cross-variable dependencies. Compared to CNN-based methods, which have limited receptive field, Transformer-based methods offer global modeling capabilities, making them more suitable for handling long and complex sequence data. They have become the cornerstone of modern time series forecasting. Informer (Zhou et al., 2021) is one of the early implementations of Transformer models in TSF, making efficient forecasting possible by carefully modifying the internal Transformer architecture. PatchTST (Nie et al., 2023) divides the sequence into multiple patches along the temporal dimension, which are then fed into the Transformer, establishing it as an important benchmark in the time series domain. In contrast, iTransformer (Liu et al., 2024b) treats each variable as an independent token to capture cross-variable dependencies in multivariate time series. However, Transformer-based methods face challenges due to the large number of parameters and high memory consumption. Recent research on MLP-based methods has shown that with appropriately designed architectures leveraging prior knowledge, simple MLPs can outperform complex Transformer-based methods. DLinear (Zeng et al., 2023), for instance, preprocesses sequences using a trend-season decomposition strategy. FITS (Xu et al., 2024b) performs linear transformations in the frequency domain, while TimeMixer (Wang et al., 2024a) uses MLP to facilitate information interaction at different scales. These MLP-based methods have demonstrated strong performance regarding both forecasting accuracy and efficiency. Unlike the aforementioned methods, this paper introduces the novel KAN to TSF to represent time series data more accurately. It also proposes a well-designed Decomposition-Learning-Mixing architecture to fully unlock the potential of KAN for time series forecasting."}, {"title": "TIME SERIES DECOMPOSITION", "content": "Real-world time series often consist of various underlying patterns. To leverage the characteristics of different patterns, recent approaches tend to decompose the series into multiple subcomponents, including trend-seasonal decomposition, multi-scale decomposition, and multi-period decomposition. DLinear (Zeng et al., 2023) employs moving averages to decouple the seasonal and trend components. SCINet (Liu et al., 2022) uses a hierarchical downsampling tree to iteratively extract and exchange information at multiple temporal resolutions. TimeMixer (Wang et al., 2024a) follows a fine-to-coarse principle to decompose the sequence into multiple scales across different"}, {"title": "\u03a4\u0399\u039c\u0395\u039a\u0391\u039d", "content": null}, {"title": "OVERALL ARCHITECTURE", "content": "Given a historical multivariate time series input $X \\in \\mathbb{R}^{N \\times T}$, the aim of time series forecasting is to predict the future output series $X_o \\in \\mathbb{R}^{N \\times F}$, where T, F is the look-back window length and the future window length, and N represents the number of variates. In this paper, we propose TimeKAN to tackle the challenges arising from the complex mixture of multi-frequency components in time series. The overall architecture of TimeKAN is shown in Figure 1. We adopt variate-independent manner (Nie et al., 2023) to predict each univariate series independently. Each univariate input time series is denoted as $X \\in \\mathbb{R}^T$ and we consider univariate time series as the instance in the following calculation. In our TimeKAN, the first step is to progressively remove the relatively high-frequency components using moving averages and generate multi-level sequences followed by projecting each sequence into a high-dimensional space. Next, adhering to the Decomposition-Learning-Mixing architecture design principle, we first design Cascaded Frequency Decomposition (CFD) blocks to obtain sequence representations for each frequency band, adopting a bottom-up cascading approach. Then, we propose Multi-order KAN Representation Learning (M-KAN) blocks to learn and represent specific temporal patterns within each frequency band. Finally, Frequency Mixing blocks recombine the frequency bands into the original format, ensuring that the Decomposition-Learning-Mixing process is repeatable. More details about our TimeKAN are described as follow."}, {"title": "HIERARCHICAL SEQUENCE PREPROCESSING", "content": "Assume that we divide the frequency range of raw time series X into predefined k frequency bands. We first use moving average to progressively remove the relatively high-frequency components and generate multi-level sequences {$x_1,\u2026\u2026,x_k$}, where $x_i \\in \\mathbb{R}^{a_i}$ $(i \\in \\{1,\u2026, k\\})$. $x_1$ is equal to the input series X and d denotes the length of moving average window. The process of producing multi-level sequences is as follows:\n$x_i = AvgPool(Padding(x_{i-1}))$\nAfter obtaining the multi-level sequences, each sequence is independently embedded into a higher dimension through a Linear layer:\n$x_i = Linear(x_i)$ (2)"}, {"title": "CASCADED FREQUENCY DECOMPOSITION", "content": "Real-world time series are often composed of multiple frequency components, with the low-frequency component representing long-term changes in the time series and the high-frequency component representing short-term fluctuations or unexpected events. These different frequency components complement each other and provide a comprehensive perspective for accurately modeling time series. Therefore, we design the Cascaded Frequency Decomposition (CFD) block to accurately decompose each frequency component in a cascade way, thus laying the foundation for accurately modeling different frequency components.\nThe aim of CFD block is to obtain the representation of each frequency component. Here, we take obtaining the representation of the i-th frequency band as an example. To achieve it, we first employ the Fast Fourier Transform (FFT) to obtain the representation of $X_{i+1}$ in the frequency domain. Then, Zero-Padding is used to extend the length of the frequency domain sequence, so that it can have the same length as the upper sequence $x_i$ after transforming back to the time domain. Next, we use Inverse Fast Fourier Transform (IFFT) to transform it back into the time domain. We refer to this upsampling process as Frequency Upsampling, which ensures that the frequency information remains unchanged before and after the upsampling. The process of Frequency Upsampling can be described as:\n$\\hat{x_i} = IFFT(Padding(FFT(x_{i+1}))) $ (3)\nHere, $\\hat{x_i}$ and $x_i$ have the same sequence length. Notably, compared to $x_i$, $\\hat{x_i}$ lacks the i-th frequency component. The reason is that $x_{i+1}$ is originally formed by removing i-th frequency component from $x_i$ in the hierarchical sequence preprocessing and $x_{i+1}$ is now transformed into $\\hat{x_i}$ through a lossless frequency conversion process, thereby aligning length with $x_i$ in the time domain. Therefore, to get the series representation of the i-th frequency component $f_i$ in time domain, we only need to get the residuals between $x_i$ and $\\hat{x_i}$:\n$f_i = x_i - \\hat{x_i}$ (4)"}, {"title": "MULTI-ORDER KAN REPRESENTATION LEARNING", "content": "Given the multi-level frequency component representation {$f_1,\u2026\u2026, f_k$} generated by the CFD block, we propose Multi-order KAN Representation Learning (M-KAN) blocks to learn specific representations and temporal dependencies at each frequency. M-KAN adopts a dual-branch parallel architecture to separately model temporal representation learning and temporal dependency learning in a frequency-specific way, using Multi-order KANs to learn the representation of each frequency component and employing Depthwise Convolution to capture the temporal dependency. The details of Depthwise Convolution and Multi-order KAN will be given as follows.\nDepthwise Convolution To separate the modeling of temporal dependency from learning sequence representation, we adopt a specific type of group convolution known as Depthwise Convolution, in which the number of groups matches the embedding dimension. Depthwise Convolution employs D groups of convolution kernels to perform independent convolution operations on the series of each channel. This allows the model to focus on capturing temporal patterns without interference from inter channel relationships. The process of Depthwise Convolution is:\n$f_{i,1} = CONV_{D\\rightarrow D}(f_i, group = D)$ (5)\nMulti-order KANS Compared with traditional MLP, KAN replaces linear weights with learnable univariate functions, allowing complex nonlinear relationships to be modeled with fewer parameters and greater interpretability. (Xu et al., 2024a). Assume that KAN is composed of L + 1 layer neurons and the number of neurons in layer l is $n_l$. The transmission relationship between the j-th neuron in layer l + 1 and all neurons in layer l can be expressed as $z_{l+1,j} = \\Sigma \\Phi_{l,j,i}(z_{l,i})$, where $z_{l+1,j}$ is the j-th neuron at layer l + 1 and $z_{l,i}$ is the i-th neuron at layer l. We can simply understand"}, {"title": "FREQUENCY MIXING", "content": "After specifically learning the representation of each frequency component, we need to re-transform the frequency representations into the form of multi-level sequences before entering next CFD block, ensuring that the Decomposition-Learning-Mixing process is repeatable. Therefore, we designed Frequency Mixing blocks to convert the frequency component at i-th level $f_i$ into multi-level sequences $x_i$, enabling it to serve as input for the next CFD block. To transform the frequency component at i-th level $f_i$ into multi-level sequences $x_i$, we simply need to to supplement the frequency information from levels i + 1 to k back into the i-th level. Thus, we employ Frequency Upsampling again to incrementally reintegrate the information into the higher frequency components:\n$x_i = IFFT(Padding(FFT(x_{i+1}))) + f_i$ (11)\nFor the last Frequency Mixing block, we extract the highest-level sequence $x_1$ and use a simple linear layer to produce the forecasting results $X_o$.\n$X_o = Linear(x_1)$ (12)\nDue to the use of a variate-independent strategy, we also need to stack the predicted results of all variables together to obtain the final multivariate prediction $X_o$."}, {"title": "EXPERIMENTS", "content": "Datasets We conduct extensive experiments on six real-world time series datasets, including Weather, ETTh1, ETTh2, ETTm1, ETTm2 and Electricity for long-term forecasting. Following previous work (Wu et al., 2021), we split the ETT series dataset into training, validation, and test sets in a ratio of 6:2:2. For the remaining datasets, we adopt a split ratio of 7:1:2.\nBaseline We carefully select eleven well-acknowledged methods in the field of long-term time series forecasting as our baselines, including (1) Transformer-based methods: Autoformer (2021), FEDformer (2022b), PatchTST (2023), iTransformer (2024b); (2) MLP-based methods: DLinear (2023) and TimeMixer (2024a) (3) CNN-based method: MICN (2023), TimesNet (2023); (4) Frequency-based methods: FreTS (2024) and FiLM (2022a). And a time series foundation model Time-FFM (2024a).\nExperimental Settings To ensure fair comparisons, we adopt the same look-back window length T = 96 and the same prediction length F = {96, 192, 336, 720}. We utilize the L2 loss for model training and use Mean Square Error (MSE) and Mean Absolute Error (MAE) metrics to evaluate the performance of each method."}, {"title": "MAIN RESULTS", "content": "The comprehensive forecasting results are presented in Table 1, where the best results are highlighted in bold red and the second-best are underlined in blue. A lower MSE/MAE indicates a more accurate prediction result. We observe that TimeKAN demonstrates superior predictive performance across all datasets, except for the Electricity dataset, where iTransformer achieves the best result. This is due to iTransformer's use of channel-wise self-attention mechanisms to model inter-variable dependencies, which is particularly effective for high-dimensional datasets like Electricity. Additionally, both TimeKAN and TimeMixer perform consistently well in long-term forecasting tasks, showcasing the generalizability of well-designed time-series decomposition architectures for accurate predictions. Compared with other state-of-the-art methods, TimeKAN introduces a novel"}, {"title": "ABLATION STUDY", "content": "In this section, we investigate several key components of TimeKAN, including Frequency Upsampling, Depthwise Convolution and Multi-order KANS.\nFrequency Upsampling To investigate the effectiveness of Frequency Upsampling, we compared it with three alternative upsampling methods that may not preserve frequency information before and after transformation: (1) Linear Mapping; (2) Linear Interpolation; and (3) Transposed Convolution. As shown in Table 2, replacing Frequency Upsampling with any of these three methods resulted in a decline in performance. This indicates that these upsampling techniques fail to maintain the integrity of frequency information after transforming, leading to the Decomposition-Learning-Mixing framework ineffective. This strongly demonstrates that the chosen Frequency Upsampling, as a non-parametric method, is an irreplaceable component of the TimeKAN framework.\nMulti-order KANS We designed the following modules to investigate the effectiveness of Multi-order KANs: (1) MLPs, which means using MLP to replace each KAN; (2) Fixed Low-order KANs, which means using a KAN of order 2 at each frequency level; and (3) Fixed High-order KANs, which means using a KAN of order 5 at each frequency level. The comparison results are shown in Table 3. Overall, Multi-order KANs achieved the best performance. Compared to MLPs, Multi-order KANs perform significantly better, demonstrating that well-designed KANs possess stronger representation capabilities than MLPs and are a compelling alternative. Both Low-order KANS and High-order KANs performed worse than Multi-order KANs, indicating the validity of our design choice to incrementally increase the order of KANs to adapt to the representation of different frequency components. Thus, the learnable functions of KANs are indeed a double-edged sword; achieving satisfactory results requires selecting the appropriate level of function complexity for specific tasks.\nDepthwise Convolution To assess the effectiveness of Depthwise Convolution, we replace it with the following choice: (1) w/o Depthwise Convolution; (2) Standard Convolution; (3) Multi-head Self-Attention. The results are shown in Table 4. Overall, Depthwise Convolution is the best choice. We clearly observe that removing Depthwise Convolution or replacing it with Multi-head Self-Attention leads to a significant drop in performance, highlighting the effectiveness of using convolution to learn temporal dependencies. When Depthwise Convolution is replaced with Standard"}, {"title": "MODEL EFFICIENCY", "content": "We compare TimeKAN with MLP-based method TimeMier and Transformer-based methods iTransformer and PatchTST, in terms of model parameters and Multiply-Accumulate Operations (MACs), to validate that TimeKAN is a lightweight and efficient architecture. To ensure a fair comparison, we fix the prediction length F = 96 and input length T = 96, and set the input batch size to 32. The comparison results are summarized in Table 5. It is clear that our TimeKAN demonstrates significant advantages in both model parameter size and MACs, particularly when compared to Transformer-based models. For instance, on the Electricity dataset, the parameter count of PatchTST is nearly 295 times that of TimeKAN, and its MACs are almost 118 times greater. Even when compared to the relatively lightweight MLP-based method TimeMixer, TimeKAN shows superior efficiency. On the Weather dataset, TimeKAN requires only 20.05% of the parameters needed by TimeMixer and only 36.14% of the MACs. This remarkable efficiency advantage is primarily attributed to the lightweight architectural design. The main computations of the TimeKAN model are concentrated"}, {"title": "CONCLUSION", "content": "We proposed an efficient KAN-based Frequency Decomposition Learning architecture (TimeKAN) for long-term time series forecasting. Based on Decomposition-Learning-Mixing architecture, TimeKAN obtains series representations for each frequency band using a Cascaded Frequency Decomposition blocks. Additionally. a Multi-order KAN Representation Learning blocks further leverage the high flexibility of KAN to learn and represent specific temporal patterns within each frequency band. Finally, Frequency Mixing blocks recombine the frequency bands into the original format. Extensive experiments on real-world datasets demonstrate that TimeKAN achieves the state of the art forecasting performance and extremely lightweight computational consumption."}, {"title": "ADDITIONAL MODEL ANALYSIS", "content": null}, {"title": "COMPUTATIONAL COMPLEXITY ANALYSIS", "content": "In our TimeKAN, the main computational complexity lies in Fast Fourier Transform (FFT), Depth-wise Convolution block and Multi-order KAN block. Consider a time series with length L and the hidden state of each time point is D. For FFT, the computation complexity is $(L logL)$. For Depthwise Convolution block, if we set the convolutional kernel to M and stride to 1, the complexity is $(LDM)$. Finally, assuming that the highest order of Chebyshev polynomials is K, the complexity of Multi-order KAN block is $O(LD^2K)$. Since M, D, K are constants that are independent of the input length L, the computational complexity of both the Depthwise Convolution block and the Multi-order KAN block can be reduced to $O(L)$, which is linear about the sequence length. In summary, the overall computational complexity is $max((L logL), O(L) = O(L logL)$. When the input is a multivariate sequence with M variables, the computational complexity will expand to $O(ML logL)$ due to our variable-independent strategy."}, {"title": "MODEL EFFICIENCY", "content": "Here, we provide the complete results of model efficiency in terms of parameters and MACs in Table 6. As can be seen, except for DLinear, our TimeKAN consistently demonstrates a significant advantage in both parameter count and MACs compared to any other model. DLinear is a model consisting of only a single linear layer, which makes it the most lightweight in terms of parameters and MACs. However, the performance of DLinear already shows a significant gap when compared to state-of-the-art methods. Therefore, our TimeKAN actually achieves superior performance in both forecasting accuracy and efficiency."}, {"title": "ERROR BARS", "content": "To evaluate the robustness of TimeKAN, we repeated the experiments on three randomly selected seeds and compared it with the second-best model (TimeMixer). We report the mean and standard deviation of the results across the three experiments, as well as the confidence level of TimeKAN's superiority over TimeMixer. The results are averaged over four prediction horizons (96, 192, 336, and 720). As shown in the Table 7, in most cases, we have over 90% confidence that TimeKAN outperforms the second-best model and demonstrates good robustne of TimeKAN."}, {"title": "FREQUENCY LEARNING WITH LONGER WINDOW", "content": "In Table 1, TimeKAN performs relatively poorly on the Electricity dataset. We infer that its poor performance on the electricity dataset is due to the overly short look-back window (T = 96), which cannot provide sufficient frequency information. To verify this, we compare the average number of effective frequency components under a specific look-back window. Specifically, we randomly select a sequence of length T from the electricity dataset and transform it into the frequency domain using FFT. We define effective frequencies as those with amplitudes greater than 0.1 times the maximum amplitude. Then, we take the average number of effective frequencies obtained across all variables to reflect the amount of effective frequency information provided by the sequence. When T = 96 (the setting in this paper), the average number of effective frequencies is 10.69. When we extend the sequence length to 512, the average number of effective frequencies becomes 19.74. Therefore, the effective frequency information provided by 512 time steps is nearly twice that of 96 time steps. This indicates that T = 96 loses a substantial amount of effective information.\nTo validate whether using T = 512 allows us to leverage more frequency information, we extend the look-back window of TimeKAN to 512 on the electricity dataset and compare it with the state-of-the-art methods TimeMixer and time series foundatiom model MOMENT (Goswami et al., 2024). The results are shown in Table 8. Although TimeKAN performs significantly worse than TimeMixer when T = 96, it achieves the best performance on the electricity dataset when the look-back window is extended to 512. This also demonstrates that TimeKAN can benefit significantly from richer frequency information."}, {"title": "IMPACT OF NUMBER OF FREQUENCY BANDS", "content": "To explore the impact of the number of frequency bands on performance, we set the number of frequency bands to 2, 3, 4, and 5. The effects of different frequency band divisions on performance are shown in the Table 9. As we can see, in most cases, dividing the frequency bands into 3 or 4 layers yields the best performance. This aligns with our prior intuition: dividing into two bands results in excessive frequency overlap, while dividing into five bands leads to too little information within each band, making it difficult to accurately model the information within that frequency range."}, {"title": "MATHEMATICAL DETAILS", "content": null}, {"title": "KOLMOGOROV-ARNOLD NETWORK", "content": "Kolmogorov-Arnold representation theorem states that any multivariate continuous function can be expressed as a combination of univariate functions and addition operations. More specifically, a multivariate continuous function g : $[0, 1]^n \\Rightarrow \\mathbb{R}$ can be defined as:\n$g(x) = g(x_1,\\dots, x_n) = \\sum_{i=1}^{2n+1} \\Phi_i(\\sum_{j=1}^n\\phi_{i,j}(x_j))$ (13)\nwhere $\\phi_{i,j}$ and $\\Phi_i$ are univariate functions. Following the pattern of MLP, Kolmogorov-Arnold Network (KAN) (Liu et al., 2024c) extends the Kolmogorov-Arnoldtheorem to deep representations, i.e., stacked multilayer Kolmogorov-Arnold representations. Assume that KAN is composed of L+1 layer neurons and the number of neurons in layer l is $n_l$. The transmission relationship between the j-th neuron in layer 1 + 1 and all neurons in layer I can be expressed as:\n$x_{l+1,j} = \\sum_{i=1}^{n_l} \\Phi_{l,j,i}(x_{l,i})$ (14)\nWe can simply understand that each neuron is connected to other neurons in the previous layer through a univariate function $\\Phi$. Similar to MLP, the computation of all neurons at layer l can be reorganized as a function matrix multiplication $\\Phi_{l-1}$. Therefore, given a input vector x \u2208 $\\mathbb{R}^{n_0}$, the final output of KAN network is:\nKAN(x) = ($\\Phi_{L-1} \\circ \\dots \\circ \\Phi_1 \\circ \\Phi_0$)x (15)\nIn vanilla KAN (Liu et al., 2024c), the univariate function $\\Phi_{l,j,i}$ is parametrized using B-splines, which is a class of smooth curves constructed via segmented polynomial basis functions. To ensure the stability and enhance the representational capacity, KAN overlays the spline function on a fixed basis function b, which is typically the SiLU function:\n$\\phi(x) = w_1b(x) + w_s spline(x)$ (16)\nspline(x) = $\\sum c_iB_i(x)$ (17)\nwhere $w_1$ and $w_s$ are learnable weights and spline(x) is the spline function constructed from the linear combination of B-spline basis functions $B_i$. However, the complex recursive computation process of high-order B-spline functions hinders the efficiency of KAN. Therefore, in this work, we adopt the simpler Chebyshev polynomial as the univariate function to replace the B-spline function (SS, 2024). The univariate function defined by the Chebyshev polynomial is given as follows:\n$T_k (x) = cos(k arccos(x))$ (18)\nHere, k represents the order of the polynomial. Then, we consider the univariate function as a linear combination of Chebyshev polynomials with different orders:\n$x_{l+1,j} = \\sum_{i=1}^{n_l} \\Phi_{l,j,i}(x_{l,i}) = \\sum_{i=1}^{n_l} \\sum_{k=0}^K O_{i,k}T_k (tanh(x_{l,i}))$ (19)\nWhere $O_{i,k}$ is the coefficients of k-th order Chebyshev polynomials acting on the $x_{l,i}$ and tanh is the tanh activation function used to normalize the inputs to between -1 and 1. By adjusting the highest order of the Chebyshev polynomial K, we can control the fitting capability of KAN. This also inspires tour design of the Multi-order KAN to dynamically represent different frequencies."}, {"title": "FOURIER TRANSFORM", "content": "Time series are often composed of multiple frequency components superimposed on each other, and it is difficult to observe these individual frequency components directly in the time domain. Therefore, transforming a time series from the time domain to the frequency domain for analysis is often necessary. The Discrete Fourier Transform (DFT) is a commonly used domain transformation algorithm that converts a discrete-time signal from the time domain to the complex frequency domain. Mathematically, given a sequence of real numbers x[n] in time domain, where n = 0, 1, . . ., N \u22121 the DFT process can be described as:\n$X[k] = \\sum_{n=0}^{N-1} x[n].e^{-j \\frac{2 \\pi}{N} kn} = \\sum_{n=0}^{N-1} x[n] (cos(\\frac{2 \\pi}{N} kn) - i sin(\\frac{2 \\pi}{N} kn)), \\quad k = 0,1,..., N-1$ (20)\nwhere X [k] is the k-th frequency component of frequency domain signal and i is the imaginary unit. Similarly, we can use Inverse DFT (iDFT) to convert a frequency domain signal back to the time domain.\n$x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} X[k].e^{j \\frac{2 \\pi}{N} kn} = \\frac{1}{N} \\sum_{k=0}^{N-1} X[k] (cos(\\frac{2 \\pi}{N} kn) + i sin(\\frac{2 \\pi}{N} kn))$ (21)\nThe computational complexity of the DFT is typically $O(N^2)$(Zhou et al., 2022b). In practice, we use the Fast Fourier Transform (FFT) to efficiently compute the Discrete Fourier Transform (DFT) of complex sequences, which reduces the computational complexity to $O(N logN)$. Additionally, by employing the Real FFT (rFFT), we can compress an input sequence of N real numbers into a signal sequence in the complex frequency domain containing N/2 + 1 frequency components."}]}