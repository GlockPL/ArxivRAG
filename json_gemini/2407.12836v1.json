{"title": "OSPC: Artificial VLM Features for Hateful Meme Detection", "authors": ["Peter Gr\u00f6nquist"], "abstract": "The digital revolution and the advent of the world wide web have transformed human communication, notably through the emer-gence of memes. While memes are a popular and straightforward form of expression, they can also be used to spread misinformation and hate due to their anonymity and ease of use. In response to these challenges, this paper introduces a solution developed by team 'Baseline' for the AI Singapore Online Safety Prize Challenge. Focusing on computational efficiency and feature engineering, the solution achieved an AUROC of 0.76 and an accuracy of 0.69 on the test dataset. As key features, the solution leverages the inherent probabilistic capabilities of large Vision-Language Models (VLMs) to generate task-adapted feature encodings from text, and applies a distilled quantization tailored to the specific cultural nuances present in Singapore. This type of processing and fine-tuning can be adapted to various visual and textual understanding and classifi-cation tasks, and even applied on private VLMs such as OpenAI's GPT. Finally it can eliminate the need for extensive model training on large GPUs for resource constrained applications, also offering a solution when little or no data is available.", "sections": [{"title": "1 INTRODUCTION", "content": "Meme is a term originally coined by Richard Dawkins in his book The Selfish Gene [1]. He described it as mimeme: mim, as in mimic and -eme, as a structure of language unit, which was then further shortened to meme, having it sound closer to the word gene. The purpose was to discuss the evolutionary nature, similar to genes, in the spread of ideas and concepts, going as far as to say that culture is what mostly distinguishes us as unique. He noted that beyond inheriting our genes, we also inherit many cultural concepts, and memes (originally described with the examples of tunes, catch-phrases and ideas), are their primary replicator, similar to genes with our biology. However, as technology evolved, so did human communication, and memes evolved from simple text-books, songs and word-of-mouth sharing, to find an exceptional breeding ground in the digital and online domain.\nSocial media has taken the world by storm, according to recent research [13], in Singapore alone there are 4.6 Million active users as of 2020, that is 79% of the total population, with 96% using it daily. It has undeniably become part of our everyday life. Therefore it is all the more alarming that these highly efficient idea repli-cators, we mostly know from our daily lives as images with text, could be used for nefarious means. The internet already provided a layer of abstraction and ease of use that had already reduced the accountability from entities spreading harmful content, but with rise of Artificial Intelligence (AI), the World Economic Forum has rung the alarm bells, listing misinformation and disinformation as the number one threat humanity is facing right now\u00b9. Memes and social media being one of the main spreaders of such ideas. Therefore it comes as no surprise we would want to also use AI to combat such harmful memes.\nThe solution presented in this short paper was submitted as a technical report to the AI Singapore (AISG) Online Safety Prize Challenge (OSPC)\u00b2 [5], which had as aim to identify such harmful online content.\nTo successfully tackle this challenge required a precise definition of scope, context, and technical criteria, which this paper outlines sequentially in the Methodology across four key areas:\n\u2022 Harmfulness Classification: Defining and identifying harm-ful content within the broad and contextually nuanced scope of the challenge.\n\u2022 Natural Language Understanding: Processing and inter-preting text in any of the four Singaporean national lan-guages.\n\u2022 Visual Understanding: Analyzing and understanding the imagery within memes.\n\u2022 Efficient Processing: Most importantly, ensuring the solu-tion is operable on a relatively small processing unit, which was crucial in the context of this challenge, but also impor-tant for scalability and practical application.\nSignificant efforts were invested into optimizing the solution to meet the strict challenge requirements, for which we refer the reader to the specifications in the code submission. Below we present the"}, {"title": "2 METHODOLOGY", "content": "In this section, the overall methodology is described, with the ratio-nale and motivation explained in the individual sections, followed by the respective innovative aspects and their evaluation. It is im-portant to note, that whilst each model was locally tested on a larger dataset (MultiOFF [10]), and heuristically tested on an extremely small dataset aimed at sanity-checking solutions, evaluation met-rics that are obtained will mostly be reported on the actual OSPC test-dataset, in attempt to achieve easier comparability between solutions of the diverse teams."}, {"title": "2.1 Harmfulness Classification", "content": "Hatefulness in memes can be classified into several categories. Ac-cording to a definition from the challenge website, these include: Hate, Offensive, Propaganda, Harassment/Cyberbullying, Violence, Self-inflicted harm, and Exploitation [9]. Each category requires a nuanced understanding and specific detection strategies. How-ever, defining a meme as hateful is complicated by the inherent subjectivity and cultural sensitivity of each category, making the establishment of a universal threshold for meme classification chal-lenging.\nIn the context of Singaporean meme analysis, the TotalDefMeme dataset [8], another resource cited on the challenge website, illus-trates the complexity of this task. The dataset was annotated by three annotators per meme, revealing significant discrepancies in their interpretations, despite all annotators being from the same cultural background.\nGiven these challenges, our approach shifts focus from trying to replicate the exact sensitivity levels defined by the OSPC. Instead, we utilize foundational models trained on a vast scale to encompass a broad range of contextual and cultural nuances. These models are better suited to capturing a general consensus on harmfulness than any single annotator, particularly in a zero-shot classification scenario where no specific dataset is provided for training."}, {"title": "2.2 Natural Language Understanding and Visual Understanding", "content": "Beyond merely processing text, the model must comprehend its meaning and the intentions behind it. The phrase \"ya ya papaya\" might sound like a reference to a delicious fruit to an average reader, but in Singapore, it connotes an arrogant or proud person. This highlights a critical challenge: the model must grasp the cultural context of multiple languages. To achieve comprehensive under-standing, a model trained across all four national languages of Singapore is essential, as merely relying on a selectively curated dataset would be insufficient.\nWhilst such LLMs like AISG's SEA-LION [7] exist, these models lack integration with visual understanding capabilities. For instance, the phrase \"I love Singapore\" can convey drastically different mean-ings depending on the associated imagery a burning trashcan versus a plate of delicious chili-crab.\nHence, it is crucial to employ a Vision-Language Model (VLM) trained on a broad spectrum of Singaporean national languages and equipped with visual adaptors. The best candidate for our requirements was the NousHermes2Yi model from NousResearch, which boasts 34 billion parameters\u00b3, and has been recently adapted for visual tasks in Llava-NeXT [6]. Through heuristic testing, this model demonstrated robust understanding capabilities in English, decent performance in Chinese, and basic proficiency in Tamil and Malay. It also excelled in identifying cultural nuances and harmfulness, proving its efficiency.\nHowever, to fully harness its capabilities, support for under-standing Tamil and Malay was necessary. A baseline model without translation achieved an AUROC of 0.6127 on the OSPC test dataset; with Tamil translation, this score increased to 0.6522 AUROC. In-terestingly, adding languages the model was already proficient in, like English or Chinese, resulted in a decreased AUROC to 0.6292, when captured via Optical Character Recognition (OCR) using Pad-dleOCR [3], which demonstrated excellent Chinese and Tamil text recognition capabilities. In fact to further speed-up processing, we make use of an OCR trick, that is to only consider characters which contain Chinese or Tamil symbols. This very efficiently filters out any English and allows the VLM model to refer to the relevant pas-sages, without needing a language-based dictionary. In contrast, the inclusion of Malaysian text, which uses the Latin alphabet, proved problematic as the OCR would inadvertently capture English text as well, leading to poorer performance.\nAfter processing the text, we turn to visual processing, which is integral to our understanding of imagery, as supported by recent research [6]. Llava's innovation to increase the input dimensions of images and allocate more tokens for encoding visual information significantly enhanced model performance. Using OpenAI's4 CLIP technology, they managed to encode images up to 672x672 pixels from an original 336x336, marking a substantial improvement. Local testing on the MultiOFF dataset [10] with the adjusted resolution, resulted in a 6% improvement in AUROC, showcasing a direct correlation between enhanced image resolution and better meme understanding."}, {"title": "2.3 Efficient Processing", "content": "One of the most critical aspects of the challenge was to engineer a solution that is efficiently deployable within limited performance constraints, notably a maximum evaluation time of five seconds"}, {"title": "3 DISCUSSION AND LIMITATIONS", "content": "Initial experiments with the Llava 1.6 7b mistral model revealed that models perform better when they first describe a meme and then classify its offensiveness, utilizing a method akin to Chain-of-Thoughts (CoT) [12]. Despite the initial success, the superior cultural context understanding of a larger model approach yielded better performance (0.65 vs. 0.69 AUROC on the OSPC test set), even without CoT, as it did not fit within the computational budget of the competition.\nSimple practical improvements like increasing OCR or VLM res-olution, and enlarging the VLM context also enhanced performance locally but could not be submitted due to constraints on the OSPC V100 server. The largest limitation in this challenge was by far the adaptation of a solution to the challenge server and handling"}, {"title": "4 FUTURE DIRECTIONS", "content": "Future work could explore extending research on artificial feature layers, applying it not only to smaller quantized models but also to large-scale commercial models with hidden weights. This approach could allow for resource-efficient fine-tuning, using only text to manipulate foundational models and extract an artificial feature layer, which in turn could be used as input for any classical ML model. Beyond artificial feature layers, such strategies could also provide a direct benefit for any type of numerical token prediction within an LLM."}]}