{"title": "A Comprehensive Approach to Misspelling Correction with BERT and Levenshtein Distance", "authors": ["Amirreza Naziri", "Hossein Zeinali"], "abstract": "Writing, as an omnipresent form of human communication, permeates nearly every aspect of contemporary life. Consequently, inaccuracies or errors in written communication can lead to profound consequences, ranging from financial losses to potentially life-threatening situations. Spelling mistakes, among the most prevalent writing errors, are frequently encountered due to various factors. This research aims to identify and rectify diverse spelling errors in text using neural networks, specifically leveraging the Bidirectional Encoder Representations from Transformers (BERT) masked language model. To achieve this goal, we compiled a comprehensive dataset encompassing both non-real-word and real-word errors after categorizing different types of spelling mistakes. Subsequently, multiple pre-trained BERT models were employed.\nTo ensure optimal performance in correcting misspelling errors, we propose a combined approach utilizing the BERT masked language model and Levenshtein distance. The results from our evaluation data demonstrate that the system presented herein exhibits remarkable capabilities in identifying and rectifying spelling mistakes, often surpassing existing systems tailored for the Persian language. Notably, the proposed method achieved an increase in the relative F1-score by more than 28%.", "sections": [{"title": "1. Introduction", "content": "Throughout history, humans have tirelessly endeavored to discover effective means of communication and message conveyance. Over time, they have successfully developed diverse methods of expression, including spoken language, visual artistry through drawing, and the written word. However, it did not take long for writing to emerge as one of the most paramount forms of communication. This is primarily due to the advent of books and other written documents, which allowed for an extensive range of ideas and concepts to be recorded.\nThe significance of writing cannot be overstated, as it serves as the foundation for the existence and preservation of crucial elements within society. News reports, legal statutes, and educational resources in schools and universities - all owe their existence to the mastery of writing skills. Countless other facets of life are also indebted to this profound ability.\nIn today's digital age, particularly with the rise of the Internet and social media platforms, virtually anyone can effortlessly produce content. Consequently, it is only natural that the likelihood of encountering writing errors in texts has significantly increased. To illustrate this point further, let us consider the\nPersian language: two words - \"\u062d\u064a\u0627\u062a meaning \"life\") and \u201c\u062d\u064a\u0627\u0637\" )meaning \"yard\") - bear striking similarities in terms of both spelling and phonetics. Consequently, individuals may inadvertently utilize one word instead of the intended counterpart.\nAdditionally, with keyboards and typewriters becoming commonplace tools for written communication, a new set of challenges has arisen. The adjacency between different letter combinations on these devices has amplified the potential for spelling errors compared to traditional handwritten forms. Thus, we find ourselves faced with an ever-present possibility of encountering such typos.\nFurthermore, given that writing permeates various critical contexts in our lives, its influence becomes even more pronounced. The transmission of messages can be distorted or manipulated due to errors or intentional misrepresentation within written works. This can lead to false promotion or misinterpretation within scientific realms and other domains.\nOn the other hand, the presence of incorrect words within a sentence can significantly compromise the quality of results produced by different language models, such as Name Entity Recognition (NER) [1], Semantic Role Labeling (SRL) [2], Next Word Prediction [3], Language Translation models [4], and others. Therefore, it is primarily due to the fact that all these aforementioned models heavily depend on word probabilities in order to effectively carry out their intended tasks. Consequently, it becomes imperative to develop a viable solution that can effectively mitigate such errors."}, {"title": "2. Related Works", "content": "In this paper, our primary objective revolves around rectify- ing different types of spelling mistakes through the utilization of machine learning techniques. Given that spelling mistakes can be categorized into two distinct groups, real and non-real- words. Non-real-words refer to those terms that are not typ- ically included within a language's vocabulary. For instance, consider the word \u062b\u0628\u0627\u062a )meaning \u201csteadiness\") which may be erroneously written as \"\u0635\u0628\u0627\u062a\" due to various reasons, including author negligence. Since this particular word does not exist within the Persian language's lexicon, it becomes rel- atively straightforward to identify its incorrectness within a given text. However, suggesting alternative words poses a sig- nificant challenge due to factors such as an extensive set of can- didate words.\nConversely, the second category consists of real-words. However, unlike their non-real counterparts, these words do ex- ist within a language's vocabulary; however, they should not be used in a sentence based on their semantic meaning. For instance, in the phrase \"\u0635\u0648\u062a \u0648 \u062a\u0635\u0648\u06cc\u0631\" )meaning \u201csound and vision\u201d) substituting \u201c\u0633\u0648\u062a\u201d )meaning \u201cwhistle\u201d) for \"\u0635\u0648\u062a (meaning \u201csound\") would be considered incorrect. Recognizing this category of words becomes more arduous than identify- ing non-real-words; nevertheless, owing to the limited number of potential replacement words available for consideration, sug- gesting an appropriate substitute becomes relatively easier.\nTo achieve this objective, both the minimum edit distance algorithm [5] and various versions of BERT masked language models [6] were applied. These methodologies are employed in order to effectively address the identified problem and ulti- mately enhance the accuracy of spelling correction within tex- tual data. The BERT language model is a prominent natural language processing (NLP) model that is used for addressing a wide range of NLP challenges, including but not limited to emotion recognition, question answering, summarization, and translation [7].\nIn our study, we tried to improve the effectiveness and efficiency of misspelling correction using BERT. Pre-trained BERT masked language models was used to suggest potential words for misspelled words and the minimum edit distance was utilized to identify related words. In addition, the algorithm was refined by incorporating additional heuristic techniques to han- dle non-real-word and real-word errors more accurately. More- over, a novel biasing mechanism was introduced to increase the precision of our error correction model.\nThis paper adopts a structured approach to present its findings. It begins with a discussion of related works in Section 2, where existing literature and research relevant to the topic are examined. Section 3 focuses on data preparation, outlining the procedures employed to collect, clean, and preprocess the data utilized in the study. The proposed method is introduced in Section 4, presenting novel approaches developed by us to address misspelling correction. Subsequently, Section 5 presents the experimental setup, encompassing variables, parameters, and the results obtained from applying the proposed method to the prepared data. The study then delves into error interpretation, analysis, and limitations in Section 6. Finally, the Conclusions section provides a summary of all the findings.\""}, {"title": "2.1. Traditional Methods", "content": "Traditional methods for spelling error detection and correc- tion often rely on n-gram [8, 9, 10] and Bayesian approaches [11, 12, 13]. One such system, presented in [14], focuses on En- glish and offers an automated approach to identify and rectify spelling mistakes. This system utilizes the Bayesian model with trigram model. The experimental results demonstrate that this approach attains an accuracy ranging from 86.16% to 89.83%.\nSimilarly, Mays et al. proposed a comparable model in [15] that utilizes tri-grams to detect approximately 76% of simple spelling errors while successfully correcting 73% of them.\nFurthermore, CloniZER [16] aims to develop a language- independent error correction system capable of adapting to any given language for the purpose of rectifying non-real-words. This system employs a triple search tree data structure and leverages non-deterministic traversal techniques to determine the appropriate form for incorrectly spelled words. After con- ducting 21 scans, CloniZER obtains an accuracy rate exceeding 80%.\nChurch and Gale [17] proposed a context-based system that incorporates a noisy channel model utilizing a basic word bi- gram model along with frequency estimation techniques.\nIn another work [18], Carlson et al. demonstrated the effec- tiveness of n-gram models in correcting spelling errors, achiev- ing an impressive accuracy rate of 92.4% for detecting insertion errors using 5-grams.\nFurthermore, a project focusing exclusively on the English language [19] addresses the recognition and correction of real-words exclusively. This method applies an enhanced version of the longest common sequence algorithm, resulting in a recall value of 89% for error detection and 76% for error correction.\nA misspelling detection and correction system called Farsi- Spell [20] has been developed specifically for the Persian lan- guage, utilizing a large monolingual dataset. The system em- ploys the Mean reciprocal rank (MRR) metric method to evalu- ate, sort, and assign value to suggested words. Remarkably, this method has achieved an impressive accuracy rate of 94.3% in detecting erroneous words and 67.4% in suggesting the correct alternatives. However, it is important to note that Farsi-Spell is limited to identifying and correcting non-real-word errors.\nIn another related study by Faili and Azadnia [21], they pro- posed a context-sensitive spelling checker that attains an F1 score of 80%. This approach incorporates the Bayesian method"}, {"title": "2.2. Neural Network Based Methods", "content": "The detection and correction of spelling errors can be effec- tively achieved by using sequential neural networks [22, 23]. Lee and Kim [24] also achieved remarkable correction perfor- mance on English by utilizing an auto encoding (AE) language model-based approach.\nIn the context of the Arabic language, Alkhatib et al. [25] introduced a novel approach that outperformed Microsoft Word 2013 and Open Office Ayaspell 3.4. Their method relied on a bidirectional long short-term memory mechanism combined with word embedding techniques. Additionally, AraSpell [26] is another noteworthy Arabic spelling correction system that employs various seq2seq model architectures such as Recurrent Neural Network (RNN) [27, 28] and Transformers [29].\nFurthermore, similar research has been conducted on the Malaysian language [30], where a long short-term memory (LSTM) network [31] was employed. Moreover, HINDIA [32], a deep learning-based model for spell-checking in Hindi lan- guage, exhibited superior performance compared to existing Malaysian spell checkers.\nIn another study, Etoori et al. [33] proposed SCMIL, an ar- chitecture based on neural networks, which demonstrated an impressive accuracy of 85.4% for Hindu and 89.3% for Telugu languages.\nKuznetsov and Urdiales [34] proposed a spelling correction method for different languages using a denoising transformer specifically designed for short strings like queries. This ap- proach proved to be effective in achieving accurate corrections.\nFor the Azerbaijani language, Ahmadzade and Malekzadeh [35] introduced a sequence-to-sequence model incorporating an attention mechanism. Their model achieved impressive F1 scores of 75% for edit distance 0, 90% for edit distance 1, and 96% for edit distance 2.\nAnother notable example is the work conducted by Jing et al. [36], who introduced an innovative approach that em- ploys BERT for spelling correction on the English language. This particular research served as the foundation for our own project. The BERT model is used to accurately predict masked words. In order to achieve this, a dataset that encompasses both Cambridge and First English Certificate datasets is uti- lized. The working methodology involves sequentially masking words within each sentence and subsequently utilizing BERT's language model to estimate the masked words.\nDuring this stage, it explored two distinct approaches. The first approach entails employing the Levenshtein distance [37] to evaluate the words suggested by BERT after its usage. Conversely, the second approach involves utilizing the Levenshtein distance to generate candidate words prior to leveraging BERT. The experimental results demonstrate that the introduced model achieves an impressive accuracy rate of 78.84% when imple- menting the first approach. However, in the second scenario, if all incorrectly spelled words possess a Levenshtein distance of at least 2 and are present within the dataset, the model attains an even higher accuracy rate of 84.91%.\nOverall, these studies highlight the effectiveness of utiliz- ing neural networks in detecting and correcting spelling errors across various languages. The advancements made in each spe- cific language demonstrate the potential of these models in im- proving spell-checking accuracy and efficiency."}, {"title": "3. Dataset Preparation", "content": "The dataset used in this research paper is a subset extracted from the digital books available in Taaghche\u00b9 database as well as Persian Wikipedia\u00b2 articles. The dataset consists of entirely accurate and unprocessed data, without any labels or metadata. Consequently, in order to employ this dataset for both training and evaluating the model proposed in this paper, it is necessary to perform pruning and error generation steps. Prior to that, it is advisable to study the characteristics of misspelled words and the methods employed for generating them in Persian language."}, {"title": "3.1. Different Categories of Misspelling Errors in General", "content": "In general, as discussed in the introduction, there are two main types of spelling errors found in all languages. The first type consists of real-words that exist within the language's gen- eral vocabulary; however, these words do not convey the in- tended meaning, rendering the sentence incomplete. On the other hand, the second category consists of non-real-words that are not listed in any dictionary. Consequently, these words are fundamentally incorrect.\nFor the purpose of this paper, it was imperative to detect both types of errors. Throughout the project, a comprehensive Per- sian dictionary containing more than 264K words was used. This dictionary had been meticulously arranged alphabetically and prepared well in advance. Furthermore, each error type was further subdivided into three distinct categories which will be thoroughly examined in the subsequent section."}, {"title": "3.2. Different Types of Misspelling Errors in Each Category", "content": "Keyboard Error: This particular type of error typically occurs as a result of carelessness and inaccuracy when using the key- board. For instance, the user might have made an error while typing the word \u201c\u062a\u0646\u0647\u0627\u201d )meaning \u201calone\u201d) and mistakenly wrote it as \"\u0645\u0646\u0647\u0627\" or \"\u062f\u0646\u0647\u0627\" )meaning \"minus", "\u062f\" or \"\u0645\" button instead of pressing the correct \\\"\u062a\\\" key.\nTo generate such erroneous words, it is first necessary to es- tablish a one-to-many data structure for storing neighboring let- ters based on the Persian standard keyboard layout. For exam-\nple, the letter \\\"\u06cc\\\" corresponds to a list of letters including \u201c\n\u0628\u060c \u0642\u060c \u062b\u060c \u0635 \u060c \u0633 \u060c \u0637\u060c \u0632\u060c \u0631": "Then, one letter from the chosen word, which was in the data structure's key list, was randomly selected and then replaced with one of its corresponding letters from the data structure. Finally, if the generated word exists within the language's vocabulary, it is considered a real-word error; otherwise, it is deemed a non-real-word error.\nSubstitution Error: Similar to the previous type, this kind of error also arises due to carelessness and inaccuracy when us- ing either a keyboard or traditional handwriting methods. For instance, while writing the word \"\u0628\u0633\u062a\u0647\" )meaning \"package", "\u0633\u0628\u062a\u0647\". In this case, the user inadvertently interchanged the letters \"\u0633\" and \"\u0628\".\nTo generate words with substitution errors like these, it is sufficient to randomly select two adjacent letters within a word and swap them. Similarly, in this case, the generated words will be added to the list of real and non-real-word errors.\nHomophone Error: These errors typically arise from a lack of writing proficiency or the user's imprecision. For instance, while attempting to write the word \"\u0633\u0627\u0639\u062f\" )meaning \u201cfore- arm": "the user might mistakenly write it as \"\u0635\u0627\u0639\u062f\" )meaning \"ascending"}, {"title": "3.3. Pruning Dataset", "content": "The pruning step was carried out line by line. First, all numeric values, punctuation marks, and Latin letters were re- moved from the data. Then, the sentences were converted into a list of tokens. If any token was not found in the main vocab- ulary or if the number of tokens was less than five or exceeded 256, the sentence was excluded."}, {"title": "3.4. Error Generation", "content": "In the subsequent phase, 50% of the lines remained unaltered while the remaining 50% underwent modifications according to the following guidelines:\nIf it was feasible to introduce a homophone error within a sentence, there was an 80% likelihood that the word would be replaced with a real-word homophone error."}, {"title": "4. Proposed Method", "content": "Generally, the method explained by Jing et al. [36] was used as the baseline in our study. Our aim was to enhance the ef- fectiveness and efficiency of the original approach while ad- dressing certain limitations. The method described as the base-"}, {"title": "4.1. Masking Misspelled Words", "content": "To use BERT masked language model for suggesting vari- ous alternatives for misspelled words, it is crucial to replace the"}, {"title": "4.2. Biasing BERT Masked Language Model", "content": "In order to enhance the quality of the best model's results, BERT masked language models were biased using the gener- ated training data. Before delving into the process and its de- scription, it is imperative to address the general procedure of learning a masked language model. Typically, every machine learning model requires a method to assess the deviation be- tween the output and the actual result. This deviation is mea- sured through the cross-entropy cost function [38].\nSince the model's output consists of a sequence of words, calculating the cost value for an entire sentence is expensive. Furthermore, research findings [6] indicate that training the model on the complete output does not yield satisfactory out- comes. Consequently, during training, it suffices to apply the cost function solely on a series of specific output words.\nInitially, 15% of the overall input data is randomly chosen, with a majority (80%) of these selected words being masked in the input. Thus, the model endeavors to predict these concealed words. However, not all of these words are masked; some are substituted with other random words in the input (10%). In such cases, the model strives to identify less significant words that do not contribute significantly to estimating the output.\nThe remaining words remain unaltered (10%). Herein lies an attempt by the model to preserve correctly written words within sentences. Nevertheless, as mentioned earlier, these three afore- mentioned groups collectively constitute only 15% of all input data. Consequently, training can be accomplished with less effort [6].\nThe initiative presented in this paper, apart from adhering to the aforementioned requirements, incorporates an additional clause into the algorithm. In this scenario, the cost function is also applied to evaluate all incorrectly spelled words within the input. Consequently, the model will prioritize correcting spelling errors.\nFurthermore, in case of erroneous input, all three introduced processes can be reapplied; however, it is crucial to emphasize that the output of these words holds significance in all instances. As a result, the model learns to initially disregard these incor- rect words and strives to suggest the most suitable candidate words (based on contextual meaning) as accurately as possible.\nIt is important to note that throughout the entire masking and modification process, it is performed on individual words rather than tokens. This distinction arises due to the fact that a single"}, {"title": "4.3. Versions of BERT Model", "content": "In order to identify the most suitable model for our problem, various versions of the BERT model were tested. Throughout this project, models implemented by Hooshvare Research Lab\u00b9 were used. These models include different configurations of the BERT model, which have been fine-tuned using Persian datasets. DistilBERT [40], ParsBERT v1.0 [41], ParsBERT v3.0 (i.e. \"bert-fa-zwnj-base\u201d), ParsBERT v2.0 (i.e. \"bert- fa-based-uncased\") were examined to determine the optimal model for our needs."}, {"title": "4.4. Levenshtein Distance", "content": "The Levenshtein distance, also known as the edit distance, is a metric used to determine the difference between two distinct strings. This metric represents the minimum number of oper- ations (deletion, substitution, and insertion) required to trans- form one string into another. In essence, the Levenshtein dis- tance shows how similar or dissimilar two strings are [5, 37].\nFor instance, the edit distance between \u2018\u062e\u0648\u0627\u0646\u201d )meaning \"tablecloth", "\u062e\u0627\u0645\u0647": "meaning \"cream", "one": "removing \u201c\u0648\u201d, substituting \u201c\u0646\u201d instead of \"\u0645\", and inserting \"\u0647\"."}, {"title": "4.5. Approach for Non-real-Word Errors", "content": "To correct non-real-word errors, a specific approach was em- ployed. It involved selecting all words from the dictionary that had an edit distance of one from the misspelled word.\nFurthermore, a heuristic approach that found all similar words with an edit distance of two from the misspelled word was implemented. The condition for inclusion is that by only replacing two adjacent letters in candidate words, they can be transformed into the misspelled word.\nThen, the masked sentence, along with a list of close words, were fed into BERT's masked language model. The final candi- date was selected based on the highest score given by BERT's masked language model."}, {"title": "4.6. Approach for Real-word Errors", "content": "To address this type of error, a data structure was created to include all words in the main dictionary. This structure stored all potential candidate words that could be real-word errors caused by shifting, replacing a letter with a neighboring letter on the keyboard, or changing the consonant of the main word.\nSubsequently, the masked sentence, along with the list of words in the data structure that contained misspelled words, was fed into BERT's masked language model. If BERT's score for all suggested words fell below a certain adjustable thresh- old (K), or if the edit distance exceeded 2, the original word was returned. Conversely, if a suggestion met these conditions and had the highest score according to BERT's model, it would be returned as the final candidate. The choice of setting 2 as the edit distance threshold came from BERT's difficulty in han- dling candidate words that were not present in its dictionary. In such cases, BERT would substitute an unknown candidate with another word, potentially leading to complete inaccuracies. By selecting 2 as the edit distance threshold, erroneous suggestions were eliminated from the final results.\nIt is important to note that all misspelled words were labeled beforehand; thus, the model only performed correction proce- dures on desired words."}, {"title": "5. Experiments and Results", "content": "This section presents the findings for both the initial and bi- ased scenarios of the proposed spelling correction solution. It is important to note that all the results presented in this section were obtained using the evaluation dataset, as detailed in Table 2."}, {"title": "5.1. Evaluation Criteria", "content": "Throughout the project, a variety of established evaluation criteria were employed to assess the proposed solution. How- ever, due to the relatively low incidence of misspelled words in the text and the predominance of correctly spelled words, the data exhibits an uneven distribution. As a result, traditional metrics such as accuracy may not always be the most appropri- ate measures in certain cases. This paper utilizes the following metrics for evaluation:\n\u2022 True Positive (TP): This metric represents the number of initially misspelled words that were subsequently cor- rected by the model.\n\u2022 True Negative (TN): It indicates the number of words that were correctly spelled and remained unchanged by the model.\n\u2022 False Negative (FN): This metric corresponds to the num- ber of words that were initially misspelled and remained uncorrected after passing through the model. It also covers cases where the model recognized the errors but suggested incorrect alternatives.\n\u2022 False Positive (FP): This metric signifies the number of words that were correctly spelled but were mistakenly al- tered by the model.\n\u2022 Accuracy:\n$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$      (1)\n\u2022 Precision:\n$Precision = \\frac{TP}{TP + FP}$      (2)\n\u2022 Recall:\n$Recall = \\frac{TP}{TP + FN}$      (3)\n\u2022 F1-Score:\n$F1\\text{-}Score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$       (4)\nIn cases involving non-real-words, precision is the primary metric that can be applied (or accuracy, as both metrics are of- ten considered synonymous in this context). Under such cir- cumstances, if a word was not found in the dataset, assuming that correct sentences were constructed in such a way that all words are part of the core vocabulary, any detection by the model would be classified as an error related to non-real-words. Consequently, the recall index has limited practical relevance in this context, as it typically approximates 100%.\nTo compute the average of the specified metrics, we em- ployed both Micro and Macro averaging techniques.\n\u2022 Micro Average: This approach computes the desired met- ric (e.g., accuracy or recall) separately for each class, tak- ing into account the number of samples within each class. The individual metrics are then aggregated and divided by the total sample count. This method proves particularly valuable when handling imbalanced data, as it accommo- dates variations in sample sizes among different classes.\n\u2022 Macro Average: In contrast, the macro average method calculates the desired metric for each class independently and subsequently computes the average. Unlike the mi- cro average, this approach disregards the sample count within each class, treating all classes with equal weight. In simpler terms, as it does not factor in sample sizes, this method is better suited for balanced data.\nUtilizing both micro and macro averaging techniques ensures a comprehensive evaluation of our metrics, while also address- ing potential data imbalances."}, {"title": "5.2. Evaluation Results", "content": "As previously highlighted, the misspelling errors can be cat- egorized into real-word and non-real-word errors, and as such, the results are presented separately for each error category.\nConcerning non-real-word errors, the detection and correc- tion of these errors remain consistent across varying thresholds (refer to Section 4.5). Figure 1 demonstrates that both Pars- BERT v1.0 and ParsBERT v2.0 exhibit slightly higher precision compared to the other models. Nevertheless, this difference in precision, while noteworthy, does not attain statistical signifi- cance, suggesting that all models perform comparably well for non-real-word errors.\nFor real-word errors, a comprehensive evaluation of all mod- els was conducted using different BERT threshold values (refer to Section 4.6). The thresholds applied in the experiment en- compassed values of 1e-1, 1e-3, 1e-5, 1e-7, and le-9.\nAs depicted in Figure 2, the results pertaining to real-word er- rors can be bifurcated into two distinct groups: one composed of the outcomes for ParsBERT v1.0 and ParsBERT v2.0, and the other consisting of the results for ParsBERT v3.0 and Distil- BERT. The former group exhibited higher recall, while the lat- ter demonstrated superior precision across the same thresholds. It is evident that models within each group yielded similar re-"}, {"title": "5.3. Comparison with the Baseline Method", "content": "As mentioned earlier, our research builds on the main idea presented by Jing et al. [36] and proposes a similar method with several notable improvements. Our goal was to enhance the effectiveness and efficiency of the original approach while addressing some of its limitations.\nAs previously stated, our research is based on the main idea introduced by Jing et al. [36], offering a method that shares some similarities while incorporating several significant en- hancements. Our goal was to enhance the effectiveness and ef- ficiency of the original approach while also mitigating some of its inherent limitations."}, {"title": "5.4. Comparison with External Systems", "content": "This section involves a comparative assessment of the mod- els' performance in relation to several established systems. To facilitate this evaluation, a set of 100 sentences, each containing an error, was carefully selected. The systems' and our models' capabilities in correcting various types of errors were evaluated in this context. These 100 sentences were individually assessed, and the efficiency of the different methods was measured by counting the number of corrected errors. The reason for lim- iting the evaluation to 100 sentences is the necessity for man- ual inspection of the sentences in the two systems introduced, which is a time-consuming operation.\nThe comparative analysis is specifically conducted against two Google systems: the Google Translate system\u00b9 and the Gmail system\u00b2. It is essential to underscore that the principal objective of these two systems is not error correction; they of- fer this functionality as an ancillary feature. More precisely, in the Google Translate system, the evaluation is based on the text translation output and the suggestions provided under the \u201cDid you mean...\" blue line. In the case of the Gmail system, its spell-checking feature is employed to assess its performance.\nFigure 5 illustrates that our models exhibit superior perfor- mance compared to the other two systems. Notably, our mod- els demonstrate a significantly greater capacity to correct real- world errors. However, it should be acknowledged that the Google Translate model outperforms our models when it comes to correcting non-real-word errors.\nIn summary, the results of the comparative analysis between our models, Gmail, and Google Translate reveal that our models excel in detecting a substantially higher number of errors.\""}, {"title": "6. Discussion", "content": "In order to evaluate the strengths and weaknesses of the mod- els, each metric was computed for distinct error types individ- ually. As depicted in Figure 6, it is evident that the models ex- hibit similar performance, with variations typically within the"}, {"title": "6.2. Zero-width Non-joiner Space in Persian", "content": "The Zero-width Non-joiner (ZWNJ) space in Persian is a type of space that is inserted between certain letters and words.\nIt appears as a small, very thin vertical line. ZWNJ space serves to enhance text readability by increasing the spacing between specific letters and words. For instance, in the phrase \u201c\u062d\u0645\u0644\u0647 \u0647\u0627 meaning \"Sentences\u201d), the words \"\u062d\u0645\u0644\u0647\" )\"Sentence\u201d) and \u201c \u0647\u0627\" )a postfix to make plural nouns) are separated by a ZWNJ space. Consequently, writing it as \"\u062d\u0645\u0644\u0647 \u0647\u0627\" or \"\u062d\u0645\u0644\u0647\u0647\u0627\" is deemed incorrect in formal writing.\nGenerally, the use of a ZWNJ space in Persian enhances text readability and comprehensibility. It is widely employed in typ- ing and writing texts. Nevertheless, it may pose challenges for model performance if the BERT model lacks training on words containing ZWNJ spaces. ParsBERT v3.0 stands out as a model capable of handling words with ZWNJ spaces, thereby address- ing the ZWNJ space issue. Consequently, despite demonstrat- ing similar results on the evaluation dataset, ParsBERT v3.0 is the preferred model for real-world applications.\nTo draw a more definitive conclusion, a scenario was sim- ulated in which ZWNJ spaces were eliminated from both the labels and the models' suggestions (by replacing all ZWNJ spaces with empty characters). The performance of both mod- els was then compared under these conditions.\nThe simulation results revealed that ParsBERT v3.0's per- formance remained unchanged, as anticipated. In contrast, ParsBERT v2.0 exhibited a noteworthy improvement in perfor- mance (refer to Figures 6 and 8). A similar trend was noted for real-world errors, with each metric for ParsBERT v2.0 showing an increase of at least 3~4%.\nIn addition, Figure 9 has been recreated (based on Figure 7) to illustrate the impact of ZWNJ spaces in keyboard real-word errors. The distribution of both Levenshtein distance and BERT scores for ParsBERT v2.0 has been altered. When comparing Figures 7 and 9, it becomes evident that, with the removal of ZWNJ spaces, a significant number of errors with a Levenshtein distance of 1 have been corrected. However, errors with a dis- tance of 0 remained unchanged, although their overall count increased (this percentage increase is attributed to the reduction in the total number of incorrect suggestions). Furthermore, a majority of errors with a Levenshtein distance of 2 were trans- formed into errors with a distance of 1.\nOverall, all these results underscore that ParsBERT v2.0 is unable to handle ZWNJ spaces and would outperform Pars- BERT v3.0 in the absence of ZWNJ spaces."}, {"title": "7. Conclusions", "content": "In this research, we presented a comprehensive algorithm for addressing spelling errors using the BERT masked language model. The evaluation results showcased the effectiveness of our models and algorithm in rectifying spelling errors.\nFurthermore, our results benefited from fine-tuning pre- trained BERT models on our dataset, leading to improved per- formance. Notably, our proposed method demonstrated greater robustness and potency when compared to the baseline ap- proach of using BERT for spelling correction.\nIn a broader context, while we endeavored to address various aspects, there is room for improvement. Regrettably, model performance can be impacted by short sentences and multiple spelling errors within a single sentence. Therefore, the develop- ment of a model capable of correcting a larger number of words within a sentence represents an important avenue for future re- search.\nTo conclude, this project addresses a specific subset of writ- ing issues, leaving more extensive concerns such as automatic paragraphing, punctuation checking, and grammar rules unex- plored. The development of a comprehensive system encom- passing all these aspects, akin to Grammarly\u00b9 for the Persian language, holds potential for practical and effective outcomes."}]}