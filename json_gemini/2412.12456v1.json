{"title": "Graph Learning in the Era of LLMs: A Survey from the Perspective of Data, Models, and Tasks", "authors": ["XUNKAI LI", "ZHENGYU WU", "JIAYI WU", "HANWEN CUI", "JISHUO JIA", "RONGHUA LI", "GUOREN WANG"], "abstract": "With the increasing prevalence of cross-domain Text-Attributed Graph (TAG) Data (e.g., citation networks, recommendation systems, social networks, and ai4science), the integration of Graph Neural Networks (GNNs) and Large Language Models (LLMs) into a unified Model architecture (e.g., LLM as enhancer, LLM as collaborators, LLM as predictor) has emerged as a promising technological paradigm. The core of this new graph learning paradigm lies in the synergistic combination of GNNs' ability to capture complex structural relationships and LLMs' proficiency in understanding informative contexts from the rich textual descriptions of graphs. Therefore, we can leverage graph description texts with rich semantic context to fundamentally enhance Data quality, thereby improving the representational capacity of model-centric approaches in line with data-centric machine learning principles. By leveraging the strengths of these distinct neural network architectures, this integrated approach addresses a wide range of TAG-based Task (e.g., graph learning, graph reasoning, and graph question answering), particularly in complex industrial scenarios (e.g., supervised, few-shot, and zero-shot settings). In other words, we can treat text as a medium to enable cross-domain generalization of graph learning Model, allowing a single graph model to effectively handle the diversity of downstream graph-based Task across different data domains. This survey offers a comprehensive review of graph learning in the era of LLMs, introducing a systematic summary built around the three pillars of machine learning: Data, Model, and Task. It examines the unique potential of LLM-GNN integration in handling heterogeneous Data sources, utilizing advanced neural Model architectures, and tackling various downstream Task in practical applications. Categorizing existing studies within this comprehensive survey, we highlight key trends, reveal current challenges, and propose directions for future research. This work serves as a foundational reference for researchers and practitioners looking to advance graph learning methodologies in the rapidly evolving landscape of LLM. We consistently maintain the related open-source materials at https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs are an effective means of capturing the structural relationships between entities by representing nodes as individual entities and edges as their interrelationships. This makes graphs an ideal representation for a wide range of fields, including social networks, recommendation systems, and molecular structures, all of which involve complex interdependencies that are challenging to model using traditional data structures. Despite their power, graph data is inherently complex and heterogeneous, posing significant challenges for conventional methods. Specifically, traditional approaches often fail to fully capture the intricate structural patterns within graph data, limiting their ability to model complex relationships and interactions effectively. These limitations are exacerbated when dealing with large-scale graphs, where relationships between nodes are not only context-dependent but also exhibit considerable variability across different domains.\nIn response to these challenges, Graph Neural Networks (GNNs) have emerged as a transformative tool. Specifically, GNNs overcome the limitations of traditional methods by employing recursive message-passing mechanisms, which enable information to propagate across the graph structure. Through this iterative process, GNNs can capture long-range dependencies and complex structural relationships, even in large and intricate graphs, allowing them to generate rich embeddings for nodes, edges, and entire graphs. These learned embeddings empower GNNs to effectively support a wide range of graph-based learning tasks, such as node classification, link prediction, and graph classification. The ability of GNNs to process complex graph data in an end-to-end, scalable manner has solidified their position as a core technique in graph machine learning research.\nRecently, the rise of Large Language Models (LLMs) has significantly advanced the field of artificial intelligence, demonstrating exceptional capabilities in natural language understanding, generation, and the modeling of complex patterns from vast datasets. Simultaneously, the concept of data-centric machine learning has gained widespread attention, prompting researchers in the graph machine learning community to focus on enhancing graph representations from a data-driven perspective. This shift has led to the proliferation of Text-Attributed Graphs (TAGs) across various domains, as the rich semantic information embedded in graph descriptions can further refine graph representations and improve data quality in many practical applications.\nBuilding on this progress, the integration of LLMs with graph learning has emerged as a transformative approach, offering new opportunities to push the boundaries of graph learning. As a general-purpose machine learning framework, LLMs provide unparalleled capabilities that can significantly enhance graph learning in ways previously unattainable. The motivation for combining LLMs with graph learning can be understood from three key perspectives: Data, Model, and Task. This integration not only improves graph data representation but also enables the development of more flexible models and strengthens reasoning capabilities for various graph-based tasks.\nFrom a data perspective, the increasing availability of high-quality, multi-domain TAGs has created more enriched, and diversified datasets. LLMs, with their powerful natural language understanding and generation capabilities, can effectively capture semantic features from the text, which GNNs can then leverage to enhance the representational power of graph models. These features, which provide a deep understanding of node and edge semantics, are crucial for improving graph models' ability to learn complex, context-sensitive relationships. Leveraging LLMs to extract high-quality semantic features marks the advancement in data-centric machine learning, and such integration pushes the boundaries of model accuracy and generalization further.\nFrom a model perspective, the collaboration between LLMs and GNNs offers a promising approach to overcoming the limitations of traditional graph learning techniques. Researchers are increasingly exploring joint training and"}, {"title": "2 WHY GNN NEEDS LLM: THE ROLE OF DATA", "content": "GNNs have made significant advancements in processing structured graph data. However, when handling with graphs enriched with textual attributes, i.e., text-attributed graphs, traditional GNNs struggle to manage excessive textual information. In applications including recommendation systems, citation networks, and social network analysis, correspondent text-attributed graphs contain a larger amount of textual information compared to conventional graph datasets. Textual information is vital for improving the performance of graph learning tasks, but existing GNN models are generally unable to process text data directly. Consequently, a key challenge is how to effectively extract features from text and integrate them into graph learning processes to enhance model performance.\nTraditional text feature extraction methods, such as TF-IDF and Word2Vec, while useful in some applications, have significant limitations when dealing with long texts and complex contextual relationships. These methods primarily rely on fixed text vector representations that do not capture the deeper hierarchical and context-dependent information inherent in the node-edge structure of the graph. As a result, they struggle to handle sophisticated linguistic or graph-based tasks. In contrast, LLMs, which are trained on vast amounts of text, excel at capturing long-range dependencies and contextual semantic relationships. LLMs can generate more nuanced and high-quality text embeddings, overcoming the limitations of traditional methods. By leveraging tasks such as text generation, question answering, and sentiment analysis, LLMs provide richer and more precise feature representations for graphs. This enables more effective integration of text and graph data, leading to advancements in graph learning. As a result, the integration of GNNs and LLMs has led to new breakthroughs in graph learning tasks.\nWe categorize existing research into five distinct groups based on data characteristics and the generalizability of methods across different graph-learning tasks. This approach is both intuitive and effective, as it addresses two key dimensions of graph-based research from the data perspective: graph-learning tasks and application domains."}, {"title": "3 OVERVIEW OF APPLIED DATASETS BY INCLUDED METHODS", "content": "This section introduces the datasets used in the evaluated methods, organizing them into five domains for improved clarity and structure. From a data-centric perspective, categorizing datasets by domain is crucial for emphasizing their unique characteristics, such as data structure, scale, and task relevance. It also helps readers understand which specific domains the methods are intended to apply to. Table 1 includes datasets that share a common characteristic of modeling interconnections between pieces of knowledge. The Citation Network captures relationships between scientific publications, typically sourced from DBLP and Arxiv, while Wikipedia and Web Page datasets represent pages of information regarding real-world entities as nodes, with edges capturing interactions between them.\nTable 2 presents datasets derived from e-commerce platforms, which typically include user-item interactions and are commonly used in recommendation system tasks. This category is well-established in the literature and represents a distinct domain with clear applications in personalized recommendations and user behavior analysis. Table 3 contains datasets derived from social network interactions, crucial for tasks such as social analysis, sentiment modeling, and community detection in the Computer Science field. These datasets focus on user interactions in online platforms, which are key for understanding social dynamics and sentiment trends. Table 4 includes datasets from domains outside the primary categories, contributing to specialized research areas, as discussed in the main text.\nIn addition to node-level and edge-level tasks, the datasets listed in Table 5 are frequently used for graph-level tasks, such as graph classification, molecule classification, and graph regression. These research directions are particularly relevant to AI4Science, with applications in fields like medical research and pharmaceuticals, where graph-based models are used to study molecular structures, drug interactions, and disease modeling."}, {"title": "3.1 Detailed Datasets Description:", "content": "For readers who intend to select the combinations of diversified datasets for research purposes, we provide detailed descriptions for the datasets. By clearly outlining where each dataset comes from and the specific context in which it is applied, we give readers a deeper understanding of how these datasets are utilized in different research settings. This transparency allows readers to assess the relevance and suitability of each dataset for their own work, ensuring they can make informed decisions when selecting data for their own studies. The details are as follows:\nMUTAG [24] is a widely used bioinformatics dataset consisting of 188 graphs, each representing a nitro compound. The nodes are labeled with one of 7 distinct node labels. The primary objective of this dataset is to classify each graph to determine whether the corresponding compound is mutagenic, specifically distinguishing between aromatic and heteroaromatic compounds.\nBZR [66] is a bioinformatics dataset used for compound activity prediction, with a primary focus on a collection of benzimidazole compounds. The dataset is designed to indicate the concentration of each compound necessary to inhibit the activity of specific biomolecules, providing valuable insights into the effectiveness of these compounds in biological processes.\nCOX2 [66] is a dataset centered on Cyclooxygenase-2, an enzyme that plays a critical role in inflammation and pain mechanisms. The dataset is utilized to classify various compounds and predict their potential inhibition potency against the COX-2 enzyme, which is a key target in drug development for anti-inflammatory therapies.\nAIDS [60] is a graph dataset comprising 2000 graphs, each representing molecular compounds derived from the AIDS Antiviral Screen Database of Active Compounds. The dataset includes a total of 4395 chemical compounds, categorized into three classes: 423 compounds belonging to class CA, 1081 to class CM, and the remaining compounds to class CI. This dataset is widely used for tasks involving molecular classification and drug discovery research.\nNCI1 [71] is a bioinformatics dataset comprising 4,110 graphs representing chemical compounds. It contains data published by the National Cancer Institute (NCI). Each node is assigned one of 37 discrete node labels. The graph classification label is determined by NCI anti-cancer screens assessing the ability to suppress or inhibit the growth of a panel of human tumor cell lines.\nENZYMES [17] is a comprehensive dataset containing 600 protein tertiary structures, meticulously curated from the BRENDA enzyme database. Within the ENZYMES dataset, researchers can explore the intricate structures of six unique enzymes, providing a rich resource for computational analysis and machine learning applications.\nDD [25] is a bioinformatics dataset composed of 1,178 graph structures representing proteins. In these graphs, nodes correspond to amino acids, and edges connect nodes that are within 6 Angstroms of each other, reflecting the spatial proximity of amino acids within the protein structure. The primary task associated with this dataset is a binary classification to differentiate between enzymes and non-enzymes, making it a valuable resource for studies in protein function prediction and structural bioinformatics.\nPROTEINS [35] is a bioinformatics dataset comprising 1,113 structured proteins. Nodes in these graph-based proteins denote secondary structure elements and are assigned discrete node labels indicating whether they represent a helix, sheet, or turn. Edges indicate adjacency along the amino-acid sequence or in space between two nodes. The objective is to predict the protein function.\nCOLLAB [44] is a scientific collaboration dataset consisting of 5,000 ego networks represented as graphs. This dataset is compiled from three public collaboration datasets. Each ego network comprises researchers from various fields and is labeled according to the corresponding field, namely High Energy Physics, Condensed Matter Physics, and Astrophysics.\nIMDB-BINARY [82] is a movie collaboration dataset comprising 1,000 graphs representing ego networks for actors and actresses. Derived from collaboration graphs within the Action and Romance genres, each graph features nodes representing actors/actresses and edges denoting their collaboration in the same movie. Graphs are labeled according to the corresponding genre, and the objective is to classify the genre for each graph.\nIMDB-MULTI [82] is the multi-class extension of the IMDB-BINARY dataset, comprising 1,500 ego-networks. It includes three additional movie genres: Comedy, Romance, and Sci-Fi, making it suitable for multi-class classification tasks. This dataset is commonly used to evaluate the performance of graph-level algorithms.\nCora, CiteSeer, and PubMed [83] are widely used citation network datasets, where nodes represent papers and edges denote citation relationships. Node features are word vectors, indicating the presence or absence of specific words in each paper. These datasets are frequently used for node classification.\nogbn-arxiv [37] is a widely used citation graph indexed by Microsoft Academic Graph (MAG) [75], especially for the large-scale graph learning. Each paper in the dataset is represented by the average of the word embeddings derived from its title and abstract. These word embeddings are generated using the skip-gram model, which captures semantic relationships between words based on their context within the text. This dataset is widely used for graph-based learning tasks, such as node classification.\nAmazon Photo and Amazon Computers [62] are subsets of the Amazon co-purchase graph, where nodes represent individual products, and edges signify that two products are frequently bought together. The node features for these datasets are derived from product reviews, represented as bag-of-words vectors, capturing the textual information associated with each item. These datasets are commonly used for graph-based downstream tasks such as node classification in graph-based recommendation systems.\nogbn-products [37] is a co-purchasing network where nodes represent products and edges indicate frequent co-purchases. The node features are derived from bag-of-words representations of product descriptions. Due to its extensive size and complex structure, this dataset is particularly well-suited for large-scale graph learning applications, making it an ideal benchmark for evaluating the scalability and performance of graph-based algorithms.\nCoauthor CS and Coauthor Physics [62] are co-authorship graphs derived from the MAG [75]. In these graphs, nodes represent individual authors, edges denote co-authorship relationships between them, and node features are constructed from the keywords of the authors' publications. The labels assigned to the nodes indicate the specific research fields in which the authors are active. These datasets are commonly used for evaluating graph-based methods, particularly in the context of node classification.\nChameleon and Squirrel [76] are two page-page networks extracted from specific topics within Wikipedia. In these datasets, nodes represent web pages, while edges signify mutual links between pages. Node features are derived from several informative nouns found on Wikipedia. They categorize the nodes into five groups based on the average monthly web page traffic.\nActor [3] is an actor co-occurrence network where nodes represent actors, and edges indicate their co-appearance on Wikipedia pages. Node features are bag-of-words vectors derived from these pages, and actors are categorized into five groups based on the terms found in their respective Wikipedia entries. This dataset is commonly used for graph-based tasks like node classification.\nMinesweeper [88] draws inspiration from the Minesweeper game and stands as the synthetic dataset. The graph is a regular 100x100 grid, where each node (cell) is linked to its eight neighboring nodes (excluding nodes at the grid's edge, which have fewer neighbors). Twenty percent of the nodes are randomly designated as mines. The objective is to predict which nodes conceal mines. Node features consist of one-hot-encoded counts of neighboring mines. However, for a randomly chosen 50% of the nodes, the features are undisclosed, indicated by a distinct binary feature.\nTolokers [88] is derived from the crowdsourcing platform [50]. Nodes correspond to workers who have engaged in at least one of the 13 selected projects. An edge connects two workers if they have collaborated on the same task. The objective is to predict which workers have been banned in one of the projects.\nRoman-empire [76] is based on the Roman Empire article from the English Wikipedia [46], each node corresponds to a non-unique word in the text, mirroring the article's length. Nodes are connected by an edge if the words either follow each other in the text or are linked in the sentence's dependency tree. Thus, the graph represents a chain graph with additional connections.\nAmazon-ratings [15] is derived from the co-purchasing network and its metadata available in the SNAP [45]. Nodes are items and edges connect items frequently bought together. The task is predicting the average rating given by reviewers, categorized into five classes. Node features are based on the FastText embeddings [30] of words in the product description. To manage graph size, only the largest connected component of the 5-core is considered.\nQuestions [88] is derived from data collected from the question-answering platform Yandex Q. In this dataset, nodes represent users, and an edge exists between two nodes if one user answers another user's question within a one-year timeframe (from September 2021 to August 2022). The objective is to predict which users remained active on the website (i.e., were not deleted or blocked) by the end of the specified period. For node features, it utilizes the average FastText embeddings for words found in the user descriptions.\nPokec [4] is a social network, where nodes are users, and edges are friendships. The task is to predict the gender.\nTape-Arxiv23 [32] is a directed graph of citation networks among computer science arXiv papers from 2023 onwards. Each node represents a paper, and the edges show citation links. The task is to predict the 40 subject areas of these papers, such as cs.AI, cs.LG, and cs.OS, based on author and moderator labels.\nEle-Computer/Photos [92] both are extracted from the Amazon-Electronics dataset. Ele-Computers consists of items with the second-level label \"Computers\", while Ele-Photo consists of items with the second-level label \"Photo\". The two datasets are extracted from the updated 2018 Amazon Computer and Amazon Photo datasets. The nodes in the dataset are electronics-related products, and the edge between the two products means that they are frequently co-purchased or co-viewed. The label of each dataset is the three-level label of the electronics products. We adopt user reviews on the item as its text attribute. Since the item has multiple reviews, we mainly adopt the review with the highest number of votes. For some items lacking highly voted reviews, we randomly adopt a user review as the text attribute. The task of the two datasets is to classify electronics products into 10 and 12 categories, respectively.\nGoodreads [51] is collected by the popular social media platform Goodreads, which allows users to track, review, and rate books. The datasets primarily include information on books, users, authors, reviews, ratings, and shelves (e.g., \"to-read\", \"currently-reading\", \"read\"). They provide rich metadata about books, such as titles, genres, descriptions, and publication details, as well as user interactions, including ratings, reviews, and social connections.\nArt, Industrial, and Music Instrument [77] are three Amazon review datasets, respectively from three broad areas, namely, arts, crafts, and sewing(Art), industrial and scientific (Industrial), and musical instruments(M.I.). The description of each product is deemed a text document, whereas the reviews of a user are combined into one document to reflect the user's preferences. If a user has reviewed a product, a link is constructed between them. The product subcategories within a broad area present the classes, which are fine-grained and may involve thousands of classes with subtle differences. The classification is only performed on product descriptions, whereas the user reviews only serve to enrich the text semantics.\nMIMIC-III [67] s a graph of diseases, patients, and visits, where nodes and relations are extracted from clinical records [26]. Diseases are classified into 19 categories according to ICD-9-CM.\nDeezer [88] is A social network of Deezer users that was collected from the public API in March 2020. Nodes are Deezer users from European countries and edges are mutual follower relationships between them. The vertex features are extracted based on the artists liked by the users. The task related to the graph is binary node classification one has to predict the gender of users. This target feature was derived from the name field for each user.\nLastFM Asia [88] is an online social network of people who use the online music streaming site LastFM and live in Asia. The links represent reciprocal follower relationships and the vertex features describe the list of musicians liked by the users. The machine learning task is the prediction of nationality for the users of the site.\nBlogCatalog [7] is a social network with bloggers and their social relationships. Node features are constructed by the keywords of user profiles, and the labels are the topic categories provided by the authors.\nReddit-Multi (5k) [8] is generated in a similar way to Reddit-Binary. The difference is that there are World News, Videos, Advice Animals, and Mildly Interesting. Graphs are labeled with their corresponding subreddits.\nReddit-Binary [8] consists of graphs corresponding to online discussions on Reddit. In each graph, nodes represent users, and there is an edge between them if at least one of them responds to the other's comment. There are four popular subreddits, namely, IAmA, AskReddit, TrollXChromosomes, and atheism. IAmA and AskReddit are two question/answer-based subreddits, and TrollXChromosomes and atheism are two discussion-based subreddits. A graph is labeled according to whether it belongs to a question/answer-based community or a discussion-based community.\nFB-15k [74] was introduced in \"Translating Embeddings for Modeling Multi-relational Data\". It is a subset of Free-base which contains about 14,951 entities with 1,345 different relations. When creating the dataset, a reverse edge with reversed relation types is created for each edge by default.\nWN18RR [74] was introduced in \"Translating Embeddings for Modeling Multi-relational Data\". It included the full 18 relations scraped from WordNet for roughly 41,000 synsets. When creating the dataset, a reverse edge with reversed relation types is created for each edge by default."}, {"title": "4 WHY GNN NEEDS LLM: THE ROLE OF MODELS", "content": "In Graph Neural Networks (GNNs), downstream tasks are often closely tied to specific data domains, leading to research that focuses on domain-specific tasks with tailored datasets. For example, a GNN trained on a citation network for node classification is optimized for task-specific knowledge, but this results in significant performance degradation when applied to other domains. Such deficits reveal GNNs' heavy reliance on the alignment between task and data domains.\nWhile maintaining consistency between data and task domains during both training and inference can ensure reliable performance, this assumption fails to meet real-world demands, where graph data and task requirements often vary widely. To address this, integrating large pre-trained models, such as Large Language Models (LLMs), as collaborative modules with GNNs offers an effective solution by improving generalizability across diverse domains."}, {"title": "5 OVERVIEW OF INCLUDED APPROACHES:", "content": "By providing succinct yet intuitive summaries of the approaches included in this survey below", "follows": "nLPNL [16", "61": "aims to decompose the heterogeneous semantic relations from existing edges on TAGs", "87": "is a self-supervised learning framework that jointly encodes the textual semantics and topological information of text nodes to generate a unified and comprehensive representation of TAGs. This design allows TAGA to effectively capture both the rich semantic context embedded in the textual descriptions and the structural relationships inherent in the graph topology. By leveraging this dual encoding approach", "57": "proposes a novel knowledge distillation-inspired TAG learning framework", "1": "aims at enhancing the alignment between GNNs and LLMs by balancing the node-based learning difficulty for TAG learning. In this paper", "2": "investigates the privacy leakage issue when using the graph prompt learning for sensitive tasks and further explores the deficit of the standard privacy method", "55": "addresses the scalability issues for effectively learning node representations on textual graphs. Specifically", "6": "emphasizes further leveraging the strengths of both LLMs and GNNs by encoding the structural insights(contextual) into the semantic analysis. To enhance its scalability", "23": "develops a topology-aware self-supervised learning framework that enjoys the scalability advantages. The core of its effectiveness builds upon the numerical node feature extraction technique terms eXtreme to fine-tune the LM output based on the graph information", "89": "improves computational efficiency and enables the scalable learning of TAGs on large datasets. By leveraging the variational Expectation-Maximization (EM) framework", "32": "offers a unique feature extraction pipeline termed LLM-to-LM interpreter that utilizes the textual modeling ability of LLMs to couple with GNNs' capability for the node classification. It first designs the prompt for LLMs to conduct the zero-shot classification and simultaneously requests the textual explanation for deliberating its decision-making process", "92": "enhances the memory usage and reduces the training complexity for TAG learning", "64": "reduces the fine-tuning cost for adopting the pre-trained models to target downstream tasks under the context of transfer learning for GNNs", "28": "improves the adaptability of graph contrastive learning in TAG by preserving the textual semantics while modifying raw text into numerical feature space. Furthermore", "70": "To break the constraints of generalizability to diverse downstream tasks for heterogeneous graph learning", "49": "proposes a new framework facilitating a more efficient transfer learning paradigm. It leverages the language model to encode both note attributes and class semantics for ensuring the consistency of feature dimensions across datasets. Additionally", "10": "offers an extensive benchmark work that has been deliberated for evaluating the link prediction performance of GCNs and the Pre-trained Language Model (PLM). Based on its investigation", "27": "is a universal prompt-based tuning method that is applicable under any pre-training strategies. Instead of relying on designing the specialized prompt function for each pre-training strategy", "14": "To address the limitations of existing graph models in capturing complex node relationships", "77": "aims to address the label scarcity issues in graph-text model learning. Specifically", "9": "aims to directly adapt LMs for TAG task. It retrieves neighboring nodes for the target node based on both structural and text similarity. Both similarities are aggregated and the resulting outcomes carrying enriched text attributes are used as input for LM predictors.\nGraphPrompt [53", "40": "To model TAG with large-scale LLMs", "81": "introduces a novel approach that enables off-the-shelf LMs to perform comparably to state-of-the-art GNNs on node classification tasks without modifying the LMs' architecture. The approach leverages two augmentation strategies: enriching Language model inputs with topological and semantic retrieval for richer context, and guiding classification through a lightweight GNN classifier to prune class candidates, resulting in LMs that outperform specialized node classifiers and demonstrating greater versatility across diverse datasets.\nDGTL [59", "76": "decouples tokenizer training from Transformer training using multi-task self-supervised learning, yielding robust and generalizable graph tokens. By utilizing residual vector quantization for hierarchical discrete tokens, GQT significantly reduces memory requirements and enhances generalization, enabling Transformer models to achieve state-of-the-art performance, including large-scale homophilic and heterophilic datasets.\nGraphAny [88", "7": "address the scalability and generalizability limitations of GNNs that are typically trained on individual datasets. GRAPHFM is a scalable multi-graph pretraining approach. It uses a Perceiver-based encoder to compress domain-specific features into a shared latent space, enabling the model to generalize across diverse graph datasets, and demonstrating improved performance across multiple node classification tasks by pretraining on a variety of real and synthetic graphs.\nGraphProp [8", "42": "enhances the generalizability of pre-training methods on Text-rich networks for various downstream tasks. PATTON leverages both textual information and network structure information within a Text-rich Network to optimize learning for tokens and documents.\nFineMolTex [47", "21": "introduce a hierarchical graph tokenizer to capture high-order hierarchical structural information to improve LLM's perception of the graph. Additionally, it adopted an enriched fine-tuning dataset with hierarchical graph information to improve the graph-language alignment.\nInstruct-GLM [84", "48": "aims to advance self-supervised learning to improve representation learning for TAG. It combines a pre-trained language model (PLM) and GNNs encoder and leverages two algorithms, which is for enhance the structure-aware representation and mutual reinforcing learning between PLM and GNNs.\nTHLM [93", "12": "aims to reduce the annotation cost in collaborative training between GNNs and LLMs for TAG. Specifically, it first annotates the representative nodes and edges, which correspondently construct the annotation graph. Then GAGA facilitates the effective alignment of structures"}]}