{"title": "MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data", "authors": ["Jianyi Zhang", "Xin GUO", "Yiran Chen", "Hao Frank Yang", "Pu Wang", "Hai Li", "Ang Li", "Haiming Wang"], "abstract": "Previous studies on federated learning (FL) often encounter performance degradation due to data heterogeneity among different clients. In light of the recent advances in multimodal large language models (MLLMs), such as GPT-4v and LLaVA, which demonstrate their exceptional proficiency in multimodal tasks, such as image captioning and multimodal question answering. We introduce a novel federated learning framework, named Multimodal Large Language Model Assisted Federated Learning (MLLM-FL), which which employs powerful MLLMs at the server end to address the heterogeneous and long-tailed challenges. Owing to the advanced cross-modality representation capabilities and the extensive open-vocabulary prior knowledge of MLLMs, our framework is adept at harnessing the extensive, yet previously underexploited, open-source data accessible from websites and powerful server-side computational resources. Hence, the MLLM-FL not only enhances the performance but also avoids increasing the risk of privacy leakage and the computational burden on local devices, distinguishing it from prior methodologies. Our framework has three key stages. Initially, prior to local training on local datasets of clients, we conduct global visual-text pretraining of the model. This pretraining is facilitated by utilizing the extensive open-source data available online, with the assistance of multimodal large language models. Subsequently, the pretrained model is distributed among various clients for local training. Finally, once the locally trained models are transmitted back to the server, a global alignment is carried out under the supervision of MLLMs to further enhance the performance. Experimental evaluations on established benchmarks, show that our framework delivers promising performance in the typical scenarios with data heterogeneity and long-tail distribution across different clients in FL.", "sections": [{"title": "1 Introduction", "content": "The surge in IoT devices has unlocked vast potential for leveraging edge-generated data in driving cooperative computing applications such as autonomous vehicles, video analytics, and recommendation systems. Traditionally, the centralized training process raises significant data privacy and security concerns due to the necessity of transferring local information. Federated learning (FL), as introduced by [31], offers a solution to these privacy challenges by enabling collaborative model training across numerous clients under the orchestration of a central server, without sharing the raw data. FL systems bring obvious advantages by involving clients downloading a global model, performing local updates using their data, and then sending these updates back to the server. The server aggregates these updates to enhance the global model, thereby preserving data privacy. Aside from the privacy considerations mentioned above, there are two fundamental acknowledgements about (cross-device) federated learning which have been widely recognized: data heterogeneity among clients and the limited and diverse computational resources on local devices [18, 30, 31, 52].\nData heterogeneity represents a significant challenge in federated learning. It largely stems from the fact that the data across participating clients are distributed independently, with each client having a different sample distribution. Due to the diversity in clients' datasets. these datasets often exhibit a long-tailed distribution, leading to client models that are biased toward the more common classes [36, 43]. This discrepancy often results in a drop in model accuracy. Although several approaches have been proposed [7, 11, 14\u201316, 21, 39, 47], the majority fail to strike an optimal balance between performance and mitigating two critical issues: 1. avoiding concerns of privacy leakage, and 2. preventing the imposition of extra computational loads on local edge devices. For instance, some contemporary methodologies necessitate the transmission of both gradients and parameters from local models to the server, which introduces substantial privacy risks. This is because attackers could potentially reverse-engineer the transmitted data to reconstruct client-specific images, as highlighted in various studies [9, 10, 54]. Alternatively, other approaches require the deployment of sizable models on local devices, which increases the memory and computational demands.\nIn light of the current popularity and exceptional proficiency of multimodal large language models (MLLMs) in tasks involving multimodalities, such as image captioning and multimodal question answering [22, 26, 33, 41, 53], we introduce a three-stage framework, named Multimodal Large Language Model sisted Federated Learning (MLLM-FL), which utilizes multimodal large language models (MLLMs) to the FL performance on heterogeneous and long-tailed data. The adaptation of MLLMs in FL is supported by two main considerations. Firstly, beyond the heterogenous and long-tailed"}, {"title": "2 Related Work", "content": "Current methodologies tackling the challenge of data heterogeneity fall into the following broad categories. Some approaches aim to simultaneously improve the models on both clients and server sides through optimization techniques. Key contributions in this area have been made by the work of [7, 14, 15, 21], who have investigated various optimization methods. Other strategies focus on enhancing the stability of local models via knowledge transfer, a technique that is model-agnostic and has been explored in the research by [3, 45, 49], aiming to mitigate data heterogeneity by spreading local training knowledge throughout the whole FL framework. Additional methods, such as those proposed by [1, 25], concentrate on improving model aggregation on the server side to address data heterogeneity. Certain strategies also regulate the scheduling of client participation to avoid biasing the FL model towards classes that are more prevalent, as explored in the studies by [46, 50]. While these approaches have advanced the handling of data heterogeneity, they often do not fully address the specific issues related to long-tailed distributions in FL. A recent approach, CReFF [38], introduces a decoupling strategy to create balanced class-distribution federated features for the server model and to retrain the classifier with these features. Nonetheless, CReFF encounters two main limitations due to its reliance on generating federated features through client-side gradient information: 1) The one-to-many relationship between gradients and samples can result in the problem becoming ill-posed; 2) The absence of semantic guidance might lead to federated features that lack discriminative ability for their respective classes. The subsequent attempt, CLIP2FL [39], seeks to overcome these drawbacks by integrating a multimodal model to direct the federated learning process. However, it still has its own drawbacks. Firstly, deploying the sizable CLIP model on devices increases memory and computational demands. Secondly, transmitting both the gradient and parameters of local models to the server, as necessitated by both CLIP4FL and CReFF, raises significant privacy concerns, as attackers could potentially reconstruct client images through reverse engineering [9, 10, 54]."}, {"title": "2.1 Multimodal large language model", "content": "The introduction of GPT-4(Vision) [33] and Gemini [41] have demonstrated remarkable abilities in Multimodal understanding and generation, sparking a research fervor on Multimodal large language model. This enthusiasm extends to a variety of tasks, including image-text comprehension [22, 26, 53]; video-text understanding [23, 29]; and audio-text understanding [5]. Among them, recent studies in image-text comprehension with large vision-language models (LVLM) [22, 26, 53] have catalyzed notable advancements in harnessing the robust capabilities of large language models to tackle multimodal tasks effectively, such as crafting narratives from images and executing intricate reasoning tasks. Prominent instances include Visual ChatGPT [44], which amalgamates diverse visual foundational models for intricate visual tasks and instructions, employing iterative feedback to synchronize visual and textual modalities. In a similar way, MM-REACT [48] merges ChatGPT with visual models for multimodal undertakings, especially in the Visual Question Answering (VQA) framework. BLIP-2 [22], notable for its Q-former model, has shown encouraging outcomes in VQA tasks, both in zero-shot and fine-tuning settings. LLaMA-Adapter [8] enhances multimodal fine-tuning efficiency by integrating adaptation prompt vectors as adjustable parameters, showcasing versatility in multimodal contexts. MiniGPT-4 [53], derived from GPT-4 and incorporating elements from BLIP-2 and Vicuna [4], specializes in caption generation and model refinement through image-text pair fine-tuning. LLaVA [26], leveraging GPT-4, focuses on a broad spectrum of instruction fine-tuning data, ranging from multi-turn QA to image descriptions, adopting a dual-stage fine-tuning approach that prioritizes language model loss while keeping the visual model static."}, {"title": "3 Methodology", "content": "In the conventional federated learning (FL) pipeline, the FL models are typically assigned to local clients for training on their heterogeneous datasets. These models are then sent back to the server for"}, {"title": "3.1 Global Multimodal Pretraining", "content": "Pretraining Dataset. As discussed in Section 1, the wealth of open-source multimodal data, such as images and their captions, remains underutilized resources for pretraining FL models. Often, these datasets are noisy, unlabeled or contain elements that are too complex, making them unsuitable for straightforward pretraining of the compact FL models. However, given the current advanced capabilities in multimodal processing of MLLMs, we now have new, convenient methods to leverage such data for pretraining purposes. Utilizing GPT-4, akin to the approach used in LLaVA, we can transform complex image data collected from the internet into three main categories:\n\n*   Conversation: This category includes dialogues between an assistant and an individual seeking specific information about a photo. The assistant's responses simulate observing the image directly, answering a variety of questions about the visual content, such as identifying objects, counting them, describing actions, pinpointing locations, and noting their spatial relations.\n*   Detailed Description: To gain a thorough understanding of an image, we formulated a series of questions designed to elicit detailed descriptions. Responses to these questions were generated using GPT-4, enriching our dataset with nuanced insights into the images.\n*   Complex Reasoning: This category focuses on more sophisticated reasoning questions based on the content of the images. Answering these questions involves a detailed logical breakdown, reflecting a deep comprehension of the images and the ability to reason through them.\n\nLeveraging the aforementioned dataset formulations, we are equipped to facilitate the pretraining of FL models with the support of Multimodal Large Language Models.\nPretraining Mechanism. Our pretraining mechanism draws inspiration from the structure of LLaVA, a highly effective and recent multimodal large language model. It consists of three key components: a visual encoder $g$, which is a frozen pretrained CLIP model; a projection layer designed to align the features of the visual model with the text domain embeddings, where the projection layer is a trainable matrix $W$; and a part comprising a large language model (LLM), typically employing promising models such as Vicuna or LLAMA-2. The workflow of LLAVA proceeds as follows: For the data formats mentioned earlier, whether it be conversation, detailed description, or complex reasoning, the input includes a text modality instruction $X_q$ (e.g., \"Could you provide a detailed description of this image?\") and an image $X_v$. The instruction $X_q$ passes through an embedding layer to obtain the text embedding $H_q$. As for the image $X_v$, it first goes through the visual encoder $g$ to acquire the"}, {"title": "3.2 Federated Finetuning", "content": "In this subsection, we delve into the Federated Finetuning phase. Upon obtaining a pretrained FL model $g_{f l}$ from the initial stage, we append classifier layers to it, tailoring the model for image classification tasks on local datasets on the client side. If we denote the set of all model parameters for the $k$-th client at the $t$-th local step as $w_k^t$, and the local data as $D_k$, then the $k$-th client updates the received model in a manner similar to FedAvg:\n\n$w_k^{t+1} \\leftarrow w_k^t - \\eta \\nabla_w L_{l o c}\\left(w_k^t ; D_k\\right)$\n\n, where the $L_{l o c}$ represents the local loss function.\nAfter local training, the model parameters $w_k$ are sent back to the server for global aggregation, where we also utilize FedAvg:\n\n$w^{t+1}=\\sum_{k=1}^{\\left|Q^t\\right|} \\frac{D_k}{\\sum_{j=1}^{\\left|Q^t\\right|} D_j} w_k^{t+1}$"}, {"title": "3.3 Global Alignment", "content": "Recent studies on the alignment of large language models, such as Reinforcement Learning from Human Feedback (RLHF), are focused on refining the model's outputs to more closely resonate with human-like understanding and reasoning. This enhancement significantly improves the model's capability in tasks that require the interpretation and execution of complex instructions. In a similar vein, companies engaged in federated learning (FL) have analogous requirements for models after global aggregation. For instance, concerning safety requirements, an FL company must ensure that models trained via federated learning do not leak user information. Additionally, there are performance-related requirements, such as adjusting the model to prevent biases caused by long-tailed distributions or training the model on new datasets to acquire new skills. Typically, this involves constructing an alignment dataset $D_{a l i g n}$ and selecting a suitable global alignment function $L_{a l i g n}$:\n\n$w_{a g g}^{n e w} \\leftarrow w_{a g g} - \\eta \\nabla_w L_{a l i g n}\\left(w_{a g g} ; D_{a l i g n}\\right)$\n\nTo address the issue of long-tailed distributions, one could design $D_{a l i g n}$ as a small, class-balanced dataset encompassing all categories, with $L_{a l i g n}$ defined as follows:\n\n$L_{a l i g n}=L_{c e}(y, p)+\\beta \\cdot K L(q \\| p)$,\n\nwhere $L_{c e}(\\cdot, \\cdot)$ is the cross-entropy loss, $y$ is the label and $p$ is the output logits vector of the FL models. Since the pretrained CLIP model in the pertaining mechanism has zero-shot image classification capability [35], $q$ denotes the output logits vector of the CLIP model. $K L$ is the Kullback-Leibler divergence and $\\beta$ is a hyperparameter balanced these two losses.\n\nThe idea of using a class-balanced dataset to alleviate the challenges of long-tailed distributions aligns with the concepts of data resampling in centralized training to handle class imbalance and client selection in federated learning. The feasibility of such an alignment dataset $D_{a l i g n}$ existing on the server side is justifiable in most cases because, in practice, for global aggregation, the server typically predefines the categories for model classification and organizes them, which is essential for subsequent federated learning processes. Otherwise, the global aggregation of classifier layers would become chaotic. Knowing the categories, companies could feasibly collect data from the internet or generate data using powerful image-generation models like Stable Diffusion or Midjourney. However, we acknowledge that in some extreme cases, data collection can be challenging, necessitating the design of more specific $L_{a l i g n}$ and $D_{a l i g n}$, which we leave for future work."}, {"title": "4 Experiment", "content": "Baselines. We compare MLLM4FL with 13 FL methods: FedAvg [31], FedAvgM [13], FedProx [24], FedDF [40], FedBE [1], CCVR"}, {"title": "4.1 Experiment Setup", "content": "Dataset&Implementation. We applied our MLLM-FL framework to three widely-used long-tailed datasets: CIFAR-10/100LT [20] and ImageNet-LT [27]. As for the first two datasets, we adopt the same sampling technique as previous studies [6] to create long-tailed distributions with various imbalance factors (IF = 100, 50, 10), and we follow CReFF [38] to use Dirichlet distribution with the key parameter $a$ to generate the heterogeneous data partition among clients, where the value of $a$ is set to 0.5 on CIFAR-10/100-LT. ImageNet-LT has 115.8 K images from 1000 classes and the number of images per class ranging from 1280 to 5, where the value of $a$ is set to 0.1. We utilized ResNet-8 as the feature extractor for CIFAR-10/100-LT and ResNet-50 for ImageNet-LT, adding an MLP layer to each to align their feature dimensions with CLIP's outputs. The number of clients is set to 20, and we select 40% at random for each training round. The client-side training batch size was uniform at 32 across Cifar-10/100 and imagenet. All the above settings are the same as the previous work in [39]. We employed the standard cross-entropy loss by default and executed 200 communication rounds. For the pertaining part, we adopt the pertaining dataset of LLaVA, CC-595K, and train the model for 4 epochs with a learning rate of 2e-3 and a batch size of 128. During the first 2 epochs, the alpha in our pretraining mechanism increase from 0 to 1 following the cosine scheduler and then the value remains 1 for the following epochs. All the experiments were conducted using PyTorch on a single Nvidia A100 80G GPU."}, {"title": "4.2 Experimental Results", "content": "Results for CIFAR-10/100-LT are presented in Table 2, where we evaluate the performance of our CLIP2FL against a range of FL approaches on both CIFAR-10-LT and CIFAR-100-LT datasets. Notably, MLLM-FL outperforms other methods in terms of classification accuracy on both datasets. Specifically, at an Imbalance Factor (IF) of 100, which presents a severe imbalance, MLLM shows an improvement of 2.12% and 1.94% in classification accuracy over CLIP2FL for CIFAR-10-LT and CIFAR-100-LT, respectively. Under the condition of IF = 50 or 10, MLLM still manages to enhance performance by around 1%. This underscores MLLM-FL's effectiveness and its outperforms over competing methods to deal with heterogeous and long-tailed distributions.\nIn the context of ImageNet-LT, Table 3 presents a comparison of the accuracy achieved by our MLLM-FL framework against various FL approaches. The evaluation is segmented into four groups based on the number of samples per class: \"Many\u201d (over 100 samples), \"Medium\" (20 to 100 samples), \"Few\" (less than 20 samples), and \"All\" (overall accuracy). While our method may not fully match the performance of CReFF in the \"Many\" categories, it excels in \"Overall\" accuracy and the \"Medium\" and \"Few\" category. These results underscore MLLM-FL's capability not just in enhancing overall model performance but also in significantly improving classification outcomes for categories with fewer samples. The ImageNet-LT results"}, {"title": "4.3 Further Analysis", "content": "We conduct some further analysis to verify the effectiveness of our framework, especially the importance of global pertaining and global alignment."}, {"title": "4.3.1 Ablation studies on our pretraining mechanism", "content": "To evaluate the effectiveness of pretraining, we conducted a comparative analysis between a pretrained model and a non-pretrained model under a constrained training dataset scenario akin to few-shot learning, aiming to mirror real-world conditions where the amount of data available on each device is limited. We generated subsets of the CIFAR-10 and CIFAR-100 datasets at varying proportions and trained both models for 30 epochs. Our findings, detailed in Table 4, outline the number of epochs required by each model to reach predetermined accuracy thresholds with different training sample sizes, along with the highest accuracy achieved by each model within the 30-epoch span.\nSpecifically, within the CIFAR-10 context, to attain a target accuracy of 25%, the pretrained model consistently outperformed the non-pretrained model, requiring fewer epochs across all sample sizes. Similarly, for the CIFAR-100 dataset, in pursuit of a target accuracy of 15%, the pretrained model proved more efficient, also necessitating fewer epochs. Figure 3 further illustrates the superiority of our pretraining approach, demonstrating that models pretrained using our methodology surpass those trained from scratch in terms of accuracy."}, {"title": "4.3.2 Ablation studies on our global alignment mechanism", "content": "In Figure 4, we present the confusion matrix for our model equipped with global alignment within the CIFAR-10-LT dataset, characterized by an imbalance factor of 100. We normalize this confusion matrix by the volume of data in each class. Additionally, we illustrate a normalized confusion matrix for a baseline model devoid of global"}, {"title": "4.3.3 Discussion", "content": "Privacy. At the forefront of federated learning challenges is the protection of user privacy. Our strategy sidesteps the conventional requirement for clients to send gradients back to the server, as seen in methods like CReFF [38] and CLIP2FL [39]. This aspect is vital because the transmission of gradients could enable the server to perform reverse engineering attacks [9, 10, 54], potentially endangering client data confidentiality. By eliminating this step, our method diminishes the likelihood of leaking sensitive client information, promoting a more secure and privacy-centric learning environment.\nComputational Efficiency. Our approach also stands out for its computational economy. Contrary to approaches like CLIP2FL, which necessitate deploying sizable multimodal models such as CLIP on client devices-demanding significant memory and potentially being unfeasible for edge devices with limited resources-our method positions the MLLM solely on the server side. At the client level, we deploy only the compact FL models. This resolution not only addresses memory constraints but also reduces the time and energy expenditure associated with federated local training. Consequently, our framework is rendered more practical and appealing for an extensive array of devices, particularly those with restricted storage capacities.\nCompatibility.: Our approach stands out for its adaptability, unlike specific methodologies like CReFF and CLIP2FL that impose unique requirements on federated local training and global aggregation. Our framework can be compatible with a wide array of existing FL algorithms. This includes, but is not limited to, client selection strategies and various techniques aimed at further enhancing client privacy protection. This flexibility ensures that our method can be seamlessly integrated into diverse FL environments and fully leverage the accumulated advancements from previous federated learning research. For a more intuitive comparison, please refer to the Table 1 highlighting the advantages of our method."}, {"title": "5 Conclusion", "content": "To overcome the challenges of federated learning in the context of heterogeneous and long-tailed data distributions, we introduced a novel framework, MLLM-FL. This framework is structured around three core stages: global pretraining, federated fine-tuning, and global alignment. This marks the inaugural integration of Multimodal Large Language Models (MLLMs) into an FL system. Leveraging the strong multimodal capacities of MLLMs, our approach taps into the vast yet previously underutilized reservoir of open-source data available online, alongside substantial server-side computational resources. Crucially, our methodology does not compromise privacy nor impose additional computational demands on client devices. Experimental evidence verifies the efficacy of our framework, paving the way for future research to explore a broader array of multimodal tasks beyond image-text interactions, thereby enhancing FL performance across diverse multimodality challenges."}]}