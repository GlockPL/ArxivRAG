{"title": "RLHS: MITIGATING MISALIGNMENT IN RLHF WITH HINDSIGHT SIMULATION", "authors": ["Kaiqu Liang", "Haimin Hu", "Ryan Liu", "Thomas L. Griffiths", "Jaime Fern\u00e1ndez Fisac"], "abstract": "Generative AI systems like foundation models (FMs) must align well with human\nvalues to ensure their behavior is helpful and trustworthy. While Reinforcement\nLearning from Human Feedback (RLHF) has shown promise for optimizing model\nperformance using human judgments, existing RLHF pipelines predominantly\nrely on immediate feedback, which can fail to accurately reflect the downstream\nimpact of an interaction on users' utility. We demonstrate that feedback based on\nevaluators' foresight estimates of downstream consequences systematically induces\nGoodhart's Law dynamics, incentivizing misaligned behaviors like sycophancy and\ndeception and ultimately degrading user outcomes. To alleviate this, we propose\ndecoupling evaluation from prediction by refocusing RLHF on hindsight feedback.\nOur theoretical analysis reveals that conditioning evaluator feedback on down-\nstream observations mitigates misalignment and improves expected human utility,\neven when these observations are simulated by the AI system itself. To leverage this\ninsight in a practical alignment algorithm, we introduce Reinforcement Learning\nfrom Hindsight Simulation (RLHS), which first simulates plausible consequences\nand then elicits feedback to assess what behaviors were genuinely beneficial in\nhindsight. We apply RLHS to two widely-employed online and offline preference\noptimization methods\u2014Proximal Policy Optimization (PPO) and Direct Prefer-\nence Optimization (DPO)\u2014and show empirically that misalignment is significantly\nreduced with both methods. Through an online human user study, we show that\nRLHS consistently outperforms RLHF in helping users achieve their goals and\nearns higher satisfaction ratings, despite being trained solely with simulated hind-\nsight feedback. These results underscore the importance of focusing on long-term\nconsequences, even simulated ones, to mitigate misalignment in RLHF.", "sections": [{"title": "1 INTRODUCTION", "content": "Aligning artificial intelligence (AI) systems with human values and intentions is crucial to ensuring\nthey behave in ways that are helpful, honest, and trustworthy. A widely-deployed method for achiev-\ning this alignment is through human feedback (Leike et al., 2018), with successful applications to, e.g.,\ntraining AI assistants (Glaese et al., 2022; Touvron et al., 2023; Anthropic, 2023; Achiam et al., 2023).\nIn particular, Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ziegler\net al., 2019; Ouyang et al., 2022; Stiennon et al., 2020) leverages human feedback to fine-tune and\nalign foundation models (FMs). While RLHF has shown promise in aligning models with human pref-\nerences, it often relies heavily on human perceptions during interactions, which may not accurately\nreflect the downstream consequences of the service provided. Such myopic feedback can misguide\nthe model's behavior and limit its effectiveness in aligning with human values. For example, human\nevaluators could misjudge an interaction on the spot, due to limited resources (e.g., partial observ-\nability; Casper et al. 2023; Lang et al. 2024) or limited bandwidth (e.g., constraints on time, attention,\nor care; Pandey et al. 2022; Chmielewski & Kucker 2020), leading to incomplete or misinformed\nfeedback. A recent study has theorized that partial observability of an AI assistant's task execution\nduring human-AI interaction can lead RLHF to learn deceptive behaviors (Lang et al., 2024)."}, {"title": "2 BACKGROUND AND PRELIMINARIES", "content": "Human Decision-Making under Uncertainty. We consider a decision problem faced by a human en-\ntity (e.g., an individual, group, or institution) under predictive uncertainty and imperfect observations.\nWe can model such a problem as a partially observable Markov decision process (POMDP) defined by\na tuple PH = (S, AH, OH, T, OH, Po, r, \u03b3, \u03b8H), where S is the set of relevant world states, AH is the\nset of available actions, OH is the human's observation space, T : S \u00d7 AH \u2192 \u2206(S) is the stochastic\ntransition kernel, OH : S \u2192 \u2206(OH) is the human's observation map, Po \u2208 \u2206(S) is the initial state\ndistribution, r : S \u00d7 AH \u00d7 \u04e8\u0126 \u2192 R is the reward function, \u03b3 \u2208 (0, 1) is the time discount factor, and\n\u03b8\u0397 \u2208 \u0398\u0397 describes the human's intrinsic preferences. Due to partial observability of the world state\ns \u2208 S, the human may maintain an internal state zH \u2208 ZH (e.g., a belief b\u0126 \u2208 \u2206(S) encoding the\nhuman's uncertain knowledge of the world state, although z\u0142 may be thought of as a more general\nvariable that could encode features such as the human's emotional state or attention focus). The\nhuman may be modeled as taking actions according to a stochastic policy \u03c0H : ZH \u2192 \u2206(AH).\nAI-Assisted Human Decision-Making. When the human consults an AI system (e.g., a FM) to help\nwith their decision problem, we may augment the above problem with the human-AI interaction. The\nresulting Assisted POMDP is a tuple PH = (S, AH \u00d7 AH, AL, OH, OAI,T, OH, OAI, Po, r, \u03b3, 0\u0397),\nwhere A and AM are the sets of interactive actions available to the human and AI system, OAI is\nthe AI's observation space, and OAI is the AI's observation map OAI : S \u2192 \u2206(\u039f\u0391\u0399). In this model,\nthe AI takes an advisory role: it can respond to a human's interactive action a \u2208 A\u00ba (e.g., a query\nthrough a chat interface) with its own a \u2208 AM (e.g., a generated text or multimedia output). After\none or multiple rounds of such interactions, the human may take a physical action a# \u2208 AH to affect\nthe evolution of the world state s. This Assisted POMDP is a special case of a partially observable\nstochastic game (POSG) (Hansen et al., 2004). In such interactions, the Al's goal is to influence\nthe human's internal state z\u0142 towards maximizing the rewards r(s, a\u0126; 0) accrued over time. This,\nhowever, is made challenging by the AI's fundamental uncertainty about the human's preferences (H.\nReinforcement Learning from Human Feedback (RLHF). RLHF aims to learn the human's\npreferences (H from human feedback data, which typically involves three key steps. In Step 1, the\nhuman is asked to provide feedback on some state sequences s = (S0, S1, ..., ST) (e.g., a human-AI\ndialogue), with st \u2208 S, \u2200t = 0,1,..., T. For example, in binary comparison (Christiano et al.,\n2017), assuming human is a Boltzmann rational decision maker (Luce, 1959), the probability that\nthe human prefers s over s' is Pr(s > s') = \u03c3(\u03b2(Rr(s) \u2013 R\u2081(s'))), where \u03c3(\u00b7) is the sigmoid\nfunction, \u03b2 > 0 is the inverse temperature parameter, and RT(s) = \u03a3=0tr(st) is the return\nreceived by state sequence s.. Step 2 is to fit a reward function \u00ee based on a dataset containing state\nsequences paired with human feedback, aiming for \u00ee to resemble r as closely as possible. Step 3 is\nto compute an Al policy \uc2be : S \u2192 \u25b3(AM) that maximizes the return based on the estimated reward\n\u2191, i.e., \u2191 = arg max Ur(\u03c0), where Ur(\u03c0) := Es~p* [RT(s)] is the expected utility of \u03c0, and p\u2122 is\nthe on-policy distribution of state sequence s under P0, T, and \u03c0. Due to the lack of an analytical\nmodel for T and the high-dimensional nature of aligning modern AI models, reinforcement learning\n(RL) is often used to approximately optimize the policy at scale. Recent studies have revealed that\nRLHF can lead to misalignment when the human gives feedback based on partial observations\no\u3142 = (, \uc5a0,\u2026\u2026\u2026, of) rather than the previously assumed\u2014but rarely realistic\u2014full state sequences,\nresulting in deceptive or manipulative AI behaviors (Casper et al., 2023; Lang et al., 2024). We argue\nthat RLHF misalignment more generally emerges in settings with significant human uncertainty,\nwhether perceptual, predictive, or a combination of the two. We propose to take advantage of the\ngeneral insight that assessments about past outcomes that the evaluator has experienced would be\nsignificantly less uncertain (and thus less influenceable) than assessments about future outcomes that\nare yet to unfold."}, {"title": "3 ALIGNMENT ALGORITHM: RL FROM HINDSIGHT SIMULATION", "content": "To address the misalignment caused by human uncertainty in RLHF, we introduce Reinforcement\nLearning from Hindsight Simulation (RLHS). Our central contention is that by decoupling human\nfeedback on the downstream outcomes of an interaction from the prediction of these outcomes, the\nlearned human reward model and corresponding AI policy will be substantially better aligned.\n3.1 HINDSIGHT MITIGATES MISALIGNMENT\nGiven a predictive model of the human, the Al's decision problem in the Assisted POMDP game\n=\nPH in Section 2 can be reformulated as a POMDP PAI = (\u0160, AAL, \u00d5AI, \u012a, \u00d5AI, Po, r, \u03b3), where\nS = S \u00d7 H \u00d7 ZH, \u00d5\u00c0I = OAI \u00d7 A\u00b9, T = (T,Te,TH), Po \u2208 \u2206(S), and r(s,zH,0H) =\n\u0395\u03b1\u03c0(:\\zh) r(s, aH;0H). Here, TH : ZH \u00d7 A^M \u2192 ZH is the transition kernel of the human's\ninternal state, modeling how the human's knowledge about the world state is evolved based on new\nobservations and interactions with the AI; we treat \u03b8H as a constant for the purposes of this paper,\nwith To as the identity map. Finally, \u03c0\u0397 : ZH \u2192 \u2206(\u00c3\u00ba), with \u0100H := AH \u00d7 AH. In practice the\nhuman model can be a black box (e.g., a web-trained FM). Due to the complexity of POMDP PAL,\nwe aim to solve it approximately using RL with hindsight feedback provided by an evaluator, which\nwe explain in detail below.\nSince the human's utility is inherited from their original decision problem PH, the expected utility\ngenerated by an AI policy \u03c0\u0391\u0399 is the expected return achieved by the human's course of action. For the\npurposes of RLHF, we can assume that the human begins taking physical actions after the interaction:\nUH (\u03c0\u0391\u0399) :=E\n50~Po, TH (12,7),\n\u03b1\u03c0\u3142(2\ubd80), \u03b1\u03c0\u0391\u0399 (124)[\n8\nH.\na;\n\u03a3r(st, \u03b1; 0)\nLt=T+1\n(1)\nwhere t = 0, 1, ..., T is the human-AI interaction phase and t = T+1,T+2, . . . ,is the downstream\nhuman acting phase. If the human evaluates their utility immediately after the human-AI interaction\nphase, relying only on predictions, this is the foresight value. If, instead, they wait until after the\nhuman acting phase and use realized outcome, this is the hindsight value. The following sections\ndefine these concepts formally.\nDefinition 1 (Hindsight Value). The hindsight value assessed by the human at time k \u2265 0 is equal to\nthe expected return received so far given the human's available information at time k. In this paper\nwe will assume that the human can accurately estimate all rewards received so far, i.e.,\nk\nk\n\u221a(2):= Est+1~T(\nst,a), a~\u3160\u3142(\u00b7|z\ubcd5)\ntr(st, a)\nt=0\n(2)\nwhere T is the environment transition kernel, which is unaffected by the AI's output.\nDefinition 2 (Foresight Value). The foresight value at time k \u2265 0 is the expected reward-to-go from\nk to k + N under the human's subjective prediction of future outcomes. Foresight uses the human's\nown internal predictive model of what states and actions might arise. Formally:\n[k+N\nk+N(2):=\nEst~P(2), \u03b1~\u3160\u3142(\u00b7|z\ubcd5)\ntr(st, a)\nt=k\n(3)\nwhere P(\u00b7 | z) is the human's internal predictive distribution over future states. Neither the states\nst nor the rewards here are necessarily generated by the environment's true transition kernel; rather,\nthey are drawn from the human's imaginative predictions.\nUsing this foresight-based feedback, the human evaluator relies on predictions of future actions and\nstates, which can be influenced by the AI. Consequently, the predicted human utility using foresight\nis: UF+N(\u03c0^I) = \u2207T+N(27). However, RL from Hindsight Simulation (RLHS) aims to reduce\nthe human's reliance on such predictions by shifting the evaluation from foresight to hindsight value.\nBy simulating the downstream acting phase for N steps, the human can directly observe realized\noutcomes rather than guess them, thus: UF+N(\u03c0\u0391\u0399) = VT+N()."}, {"title": "3.2 IMPLEMENTATION: HINDSIGHT SIMULATION WITH AI FEEDBACK", "content": "Hindsight Simulation. While we have demonstrated theoretically that providing hindsight can\nmitigate misalignment in RLHF, exposing humans to real consequences can circumvent material and\nethical difficulties. To address this, we introduce the concept of hindsight simulation\u2014the namesake\nof our core contribution, RLHS-which allows evaluators, whether human or AI, to make more\ninformed decisions based on simulated outcomes. In practice, hindsight simulation can involve\ncollecting feedback from human evaluators or employing another language model as a proxy to\nsimulate the feedback process. After an evaluator makes a decision based on their interaction with\nthe AI (e.g., purchasing an item), the system provides ground truth information about the outcome,"}, {"title": "4 EXPERIMENTAL DESIGN", "content": "4.1 DATA COLLECTION\nPreference Data Collection. Our training data collection process closely follows the standard RLHF\ndata collection pipeline (Stiennon et al., 2020; Ouyang et al., 2022), where feedback is collected\nbased on comparisons between outputs. However, instead of relying on real human feedback, we\nemployed a strong large language model (LLM) model as a judge to simulate human interactions\nwith the chatbot and provide feedback. For real-world online marketplace chatbots like the Amazon\nRufus (Amazon, 2024), human feedback is typically given as a rating at the end of the interaction.\nHowever, human users tend to compare their current experience with previous ones when assigning\nratings. To capture this behavior, we simulate users comparing services from two different stores and\nselecting their preferred option, rather than rating each scenario in isolation. This closely aligns with\nthe preference-based data collection method used in prior work (Stiennon et al., 2020; Ouyang et al.,\n2022), where users provide feedback by comparing responses instead of giving individual ratings.\nDecision-making simulation. While collecting the preference data, our simulated human (strong\nmodel) takes on three roles: interacting with the chatbot, making decisions, and providing feedback.\nTo ensure accurate decision-making and feedback, we adapted the approach in introspective planning\n(Liang et al.). First, we frame the decision-making problem as a multiple-choice question with four\noptions: (A) Buy option A, (B) Buy option B, (C) Buy option C, or (D) Do not buy anything. We\nthen ask the LLMs to perform Chain-of-Thought reasoning (Wei et al., 2022), querying the next\ntoken probabilities to select the best option from A, B, C, D. This approach can reduce the language\nagent's uncertainty. We apply a similar method for comparing services between two stores.\nDataset Details. In our experiments, we used both Llama-2-7B (Touvron et al., 2023) and Llama-3-\n8B (Dubey et al., 2024) as the AI assistants, and Llama-3.1-70B (Dubey et al., 2024) as the simulated\nhuman to interact with the AI assistant and provide feedback. We collected 11,000 preference data\npoints for each AI assistant model, with 10,000 used for training and 1,000 for validation. We also\ngenerated a test set of 1,200 examples to evaluate performance across different customer scenarios.\n4.2 EXPERIMENT SETUP\nEnvironment Details. In each of our simulated marketplace scenarios there are 10 candidate items,\neach characterized by 8 features and a price. Each feature can be categorized in two ways: (1) The\nitem either has or lacks a specific feature (e.g., a TV with HDR vs. without HDR), and (2) The feature\ncan vary in types (e.g., 8K resolution vs. 4K resolution). While in most cases the chatbot has access\nto this information, there are instances where it is uncertain about a particular feature (e.g., resolution\nnot specified). We will examine these scenarios and investigate when and how the AI acts deceptively.\nIn our setting, the feature is always hidden from the customer, requiring them to interact with the\nchatbot to gather information. We explore scenarios where the price is either visible to the customer or\nhidden, allowing us to evaluate how restricting observability affects the feedback and, consequently,\nthe Al's behavior. We also consider scenarios when the customer prioritizes price by adding a\nconstraint regarding their price requirements in the prompt.\nMetrics. We use two primary metrics: true utility and satisfaction rating. The true utility metric U\nreflects both the customer's requirements and the item they purchase. We define U as follows: if the\ncustomer makes no purchase, the utility is U = 0. If the purchased item lacks the required feature,\nU = -1. If the item contains the required feature and the customer has no price constraints, U = 1.\nWhen price is a priority and the item contains the required feature, the utility is defined as the ratio of\nthe price of the cheapest item with the feature to the price the customer actually paid.\nThe satisfaction rating reflects the user's evaluation of the chatbot's service, measured on a 5-point\nLikert scale ranging from 1 (very dissatisfied) to 5 (very satisfied). For the experimental results\nshown in Fig. 4 and Fig. 5, these ratings were normalized to a scale between -1 and 1, which ensure\nthat the true utility and satisfaction ratings are on the same scale for clearer comparison. Additional"}, {"title": "5 SIMULATION RESULTS", "content": "Misalignment between satisfaction rating and real utility. When using standard RLHF (Ouyang\net al., 2022), we observe significant misalignment between user satisfaction ratings and true utility\nas training progresses (left plot in Figs. 4 and 5). While the satisfaction rating steadily increases,\nindicating that the language model is learning to deliver responses that receive higher immediate user\napproval, the true utility shows a sharp decline. This suggests that while the chatbot's responses may\nappear more polished or helpful in the short term, they are in fact becoming less aligned with the\nuser's true needs or long-term goals. As a result, while users may initially perceive the chatbot's\nresponses as helpful, they are frequently misled and ultimately dissatisfied with their final outcomes.\nThis highlights a fundamental flaw in using standard RLHF with immediate feedback, as it risks\noptimizing for superficial satisfaction at the expense of true utility.\nHindsight simulation effectively mitigates misalignment. As shown in Fig. 4 (left), relying on\nimmediate feedback leads to a steady decline in real utility, ultimately resulting in negative overall\nutility. In contrast, hindsight simulation consistently improves utility throughout training, eventually\nachieving positive utility, as in Fig. 4 (middle). It aligns upward trends in both real utility and\nsatisfaction ratings, significantly reducing the gap between them. The qualitative results shown"}, {"title": "6 HUMAN STUDY", "content": "Our human study had two goals: (Goal 1) evaluate the performance of models trained with immediate\nfeedback vs. hindsight simulation, (Goal 2) assess how hindsight information affects user satisfaction.\nTo achieve these goals, we designed two similar human experiments. Both experiments used Llama-\n3-8b (Dubey et al., 2024) trained with DPO using either immediate feedback or partial hindsight. We\nconducted online human experiments via Prolific (Palan & Schitter, 2018), involving 200 participants\nacross 10 scenarios, randomly sampled from a test set of 1,200. For each scenario, 20 participants\nwere randomly assigned to one of two conditions: 10 interacting with the RLHF model and 10 with\nthe RLHS model. We report specific details for participant recruitment, compensation, and IRB\napproval in Appendix D.2."}, {"title": "7 RELATED WORK", "content": "Reinforcement Learning from Human Feedback. RLHF is widely used for training language\nmodels to align with human preferences and values (Christiano et al., 2017; Ziegler et al., 2019;\nOuyang et al., 2022; Bai et al., 2022a). The classical RLHF pipeline typically involves three stages:\nsupervised fine-tuning (Chen et al., 2023; Taori et al., 2023; Wang et al., 2023; Xia et al., 2024) reward\nmodeling (Gao et al., 2023; Luo et al., 2023; Chen et al., 2024; Lightman et al., 2023; Lambert et al.,\n2024), and policy optimization (Schulman et al., 2017). PPO (Schulman et al., 2017) is commonly\nused in the policy optimization phase. However, due to the complexity and optimization challenges\nof online preference optimization algorithms (Zheng et al., 2023; Santacroce et al., 2023), researchers\nhave been exploring more efficient and simpler offline alternatives without learning the reward model\n(Rafailov et al., 2024; Meng et al., 2024; Ethayarajh et al., 2024; Zhao et al., 2023). Our approach\nusing hindsight simulation can be applied to both online PPO and offline (DPO) learning algorithms.\nReinforcement Learning from AI Feedback. Constitutional AI (Bai et al., 2022b) uses an LLM to\nprovide feedback and refine responses, producing data used to train a fixed reward model. This reward\nmodel is then applied in reinforcement learning, a process referred to as RLAIF. The technique of\nusing LLM-as-a-Judge has become a standard method for evaluating model outputs (Dubois et al.,\n2024; Li et al., 2023b; Fernandes et al., 2023; Bai et al., 2024; Saha et al., 2023) and curating data\nto train reward models (Lee et al., 2023; Chen et al., 2023; Li et al., 2023a). Recent studies have\nshown that RLAIF performs similarly to RLHF (Lee et al., 2023). Our approach also utilizes LLMs\nto provide feedback and uses the preference data to fine-tune our model.\nChallenges of Learning from Human Feedback. Learning from human feedback presents chal-\nlenges (Casper et al., 2023). Human evaluators are imperfect (Saunders et al., 2022; Gudibande et al.,\n2023), make mistakes due to limited time (Chmielewski & Kucker, 2020), incomplete information\n(Casper et al., 2023; Lang et al., 2024), lack of expertise (Daniels-Koch & Freedman, 2022) or\ncognitive biases (Pandey et al., 2022). Evaluators may also have conflicting preferences (Bakker\net al., 2022). Modeling human preferences is difficult (Zhao et al., 2016; Hong et al., 2022; Lindner\n& El-Assady, 2022), with models being prone to overoptimization (Gao et al., 2023). Due to the\nimperfect nature of human judgment, we argue that relying on immediate feedback, as is common in\ncurrent RLHF pipelines, can lead to misalignment. In this work, we propose a hindsight simulation\napproach that aims to reduce human uncertainty and foster more truthful feedback, thereby mitigating\nthese alignment challenges."}, {"title": "8 CONCLUSION", "content": "In this work, we introduced Reinforcement Learning from Hindsight Simulation (RLHS), an algorith-\nmic framework that mitigates misalignment in RLHF by providing evaluators with future outcome\ninformation. Using both theoretical proofs and realistic human experiments, we demonstrate that\nRLHS can significantly improve utility compared to existing RLHF pipelines that rely on only imme-\ndiate feedback, while maintaining a high user satisfaction rate throughout the human-AI interactive\nprocess. While our study focused on simulated hindsight with an application to marketplace chatbots,\nfuture work should explore incorporating hindsight in RLHF for additional real-world applications\nwith real human evaluators. We also see open opportunities to equip RLHS with other feedback\nmodalities, such as visual cues, which could enrich the feedback process and improve alignment."}, {"title": "D HUMAN STUDY DETAILS", "content": "D.1 USER INTERFACE\nIn this subsection, we display the interface used in our human study."}, {"title": "E DISCUSSION", "content": "E.1 RELATED WORK\nStatement of Contributions. Our key insight is that the true value of AI outputs lies in their\ndownstream consequences, especially in how they influence real-world human behavior. While the\nimportance of long-term outcomes is a fundamental aspect of dynamic decision theory, our work is\nthe first to address this within the context of RLHF by (1) exploring the negative effects of learning\nfrom immediate human feedback based on foresight, and (2) proposing a general mitigation strategy\nthat evaluates real-world downstream harm caused by inaccurate information.\nComparison with Related Work: One of the recent works cited in comparison is by Lang et al.\n(2024), which focuses on the problem of partial observability. This is distinct from the problem\nof human misprediction we address. In their setting, user utility is confined to the immediate time\nframe of the interaction and does not consider the long-term repercussions on the user's behavior\nor well-being after the interaction concludes. Their analysis primarily highlights scenarios where\nan AI system is incentivized to withhold information to avoid negative feedback scores but does\nnot delve into the real-world impact such deception has on user utility. In contrast, our approach\nspecifically examines the human user's decision-making process after interacting with the AI system,\nemphasizing how misalignment or deceptive behavior directly affects their realized utility.\nRecent studies have investigated sycophantic behavior in language models (Sharma et al., 2023; Wei\net al., 2023; Perez et al., 2022), where the models are optimized to generate responses that align with\nuser beliefs rather than the truth. Our empirical results also reveal such tendencies. In this paper,"}, {"title": "E.2 BROADER IMPACT", "content": "Human evaluators do not always know the full truth when providing feedback. Without explicit\ninformation about its future consequences, evaluators must implicitly estimate them during their\nassessment. This limitation poses significant challenges for real-world applications of AI, particularly\nwithin the RLHF framework we studied. In the following sections, we discussed these limitations and\nhow our proposed hindsight feedback approach can help overcome them to enhance AI alignment.\nLimited Access in Real-World Applications: In real-world scenarios, users and human labelers\nfrequently interact with black-box or closed-prompt AI systems where internal prompts and decision-\nmaking processes remain opaque. Notable examples include commercial systems such as OpenAI's\nChatGPT and Amazon's Rufus. Our proposed techniques (hindsight feedback), and the experimental\nsettings we used can be applied directly to these systems where full internal access is unavailable. In\nsuch cases, assessing the consistency of responses alone is insufficient, as external context might not\ncapture the complete implications of an AI's output. Hindsight feedback allows evaluators to provide\nmore reliable feedback by considering outcomes, improving alignment in these constrained settings.\nLimitations of Human Judgment and Information Access: Even when human evaluators have full\naccess to models and their prompts (e.g., in open-source systems), perfect judgment is not guaranteed.\nEvaluators may miss deeper implications or fail to predict the long-term impact of responses, whether\ndue to lack of expertise or cognitive limitations. These challenges are relevant to both open-source\nand closed-source models. Below, we outline two practical examples illustrating these limitations\nand how hindsight feedback can address them:\nCode Generation Scenario: Imagine a user asking a language model for code to fit a polynomial curve\nto a set of data points. One solution may fit the data perfectly, while another shows some deviation. A\nhuman evaluator might prefer the model with the perfect fit, not realizing that it overfits and performs\npoorly on new data. Immediate feedback in this case could lead to misalignment, as it prioritizes\nsurface-level satisfaction over long-term utility. By providing feedback after testing the code on\nnew data (hindsight), evaluators can offer more informed input, reducing misalignment. Hindsight\nsimulation can automate part of this process by allowing models to test outcomes on unseen data and\nreport the results to human evaluators for better feedback. One extra benefit of hindsight simulation\nis that humans do not need to be domain experts to provide truthful feedback.\nAI4Science Proof Construction: When constructing mathematical proofs for scientific problems,\nmodel may generate results that are correct only under conditions or assumptions specified by the\nuser. Human evaluators, constrained by time or limited expertise, may overlook these limitations\nduring evaluating the model, eventually causing the model to overfit to a restricted set of problems\nand unable to tackle scientific problems in general settings. On the other hand, hindsight simulation\ngenerates a diverse set of scenarios, including, e.g., edge cases, under which the model is required\nto validate its proof. This allows the human evaluator to assess the model performance based on its\nability to generalize beyond the immediate problem."}, {"title": "Algorithm 1 Human Feedback Loop for RLHS", "content": "1: Step 0: Initialization\n2: 80, 0, 0, \u2190 sample_initial_conditions(S, ZH, \u04e8\u041d)\n3:\n4: Step 1: AI Prompt Sampling\n5: s,of \u2190 sample_AI_prompt(ZAI, OAI)\n6:\n7: Step 2: AI Policy Evaluation\n8: Query the AI policy for an action: of := a^ ~ \u03c0\u0391\u0399(\u00b7 | So, z)\n9:\n10: Step 3: Hindsight\n11: for t = 1 to T + N do\n12:\n13:\nSample action: at \u2190 sample_action(\u03c0\u0391\u0399)\nSt+1,01 \u2190 f(st, at, of)\n14: end for\n15:\n16: Step 4: Query Feedback\n17: Query human feedback on the AI policy: \u00dbH (\u03c0\u0391\u0399) \u2190 query_human_feedback(\u03c0\u0391\u0399)\n18:\n19: Output or Process Feedback\n20: Store or process feedback for further learning: store_feedback(\u00dbH)"}]}