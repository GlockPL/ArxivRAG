{"title": "Domain Generalizable Knowledge Tracing via Concept Aggregation and Relation-Based Attention", "authors": ["Yuquan Xie", "Wanqi Yang", "Jinyu Wei", "Ming Yang", "Yang Gao"], "abstract": "Knowledge Tracing (KT) is a critical task in online education systems, aiming to monitor students' knowledge states throughout a learning period. Common KT approaches involve predicting the probability of a student correctly answering the next question based on their exercise history. However, these methods often suffer from performance degradation when faced with the scarcity of student interactions in new education systems. To address this, we leverage student interactions from existing education systems to mitigate performance degradation caused by limited training data. Nevertheless, these interactions exhibit significant differences since they are derived from different education systems. To address this issue, we propose a domain generalization approach for knowledge tracing, where existing education systems are considered source domains, and new education systems with limited data are considered target domains. Additionally, we design a domain-generalizable knowledge tracing framework (DGKT) that can be applied to any KT model. Specifically, we present a concept aggregation approach designed to reduce conceptual disparities within sequences of student interactions from diverse domains. To further mitigate domain discrepancies, we introduce a novel normalization module called Sequence Instance Normalization (SeqIN). Moreover, to fully leverage exercise information, we propose a new knowledge tracing model tailored for the domain generalization KT task, named Domain-Generalizable Relation-based Knowledge Tracing (DGRKT). Extensive experiments across five benchmark datasets demonstrate that the proposed method performs well despite limited training data. Traditional KT models show significant performance degradation on target domains, while models with the DGKT framework alleviate this issue. Our proposed DGRKT model outperforms five knowledge tracing models, achieving an average AUC improvement of 4.16%, benefiting from a specially designed relation-based attention mechanism.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past decades, a large number of online education systems have emerged, which provide remote learning environments and personalized guidance for users. These advancements have revolutionized the educational landscape, enabling platforms to cater to the individual learning needs of students more effectively [1], [2], [3]. Knowledge tracing (KT) plays a crucial role in this context, as it allows education systems to monitor and evaluate the evolving knowledge states of students. Knowledge tracing (KT) aims to trace a student's knowledge states accurately which is often realized via the task of predicting students' future performance according to their historical interactions [4], [5]. This enables online education platforms to better assess students' comprehension and provide tailored assistance for learners [6], [7].\nA number of KT models have demonstrated their effectiveness [8], [9], [10]. However, these methods require a substantial amount of student interactions to adequately train a KT model for a specific subject. In the majority of knowledge tracing datasets, students' problem-solving records typically have a scale of several hundred thousand or more. For example, there are 346, 860 interactions in ASSISTment 2009 which is completely enough to train a KT model sufficiently while it is difficult for a newly developed education system to gain such a substantial amount of data in the beginning. In practice, developing a new online education system or adding a new question bank to an existing one often lacks the availability of abundant student interactions, leading to a scarcity of data for training a KT model [11], [12]. This scarcity of interactions results in the challenge of insufficient training data. As shown in Fig. 1, common KT models suffer significant AUC degradation when confronted with limited data sizes.\nCurrently, few studies have delved into the data scarcity issue in knowledge tracing. Some prior efforts have aimed to address the sparse problem that students tend to interact with only a small set of questions, using approaches like pre-trained question embeddings [13] or contrastive learning [7], [14], [15]. However, these methods assume relatively sufficient interaction records. In cases where interactions are severely limited, these approaches might not be effective in learning question embeddings and students' knowledge states, as they heavily rely on a larger amount of student interactions. Thus, the scarcity of student interactions in new educational systems presents a significant challenge.\nIn the absence of student interaction records in the new system, numerous student interaction records from diverse online education platforms offer valuable insights. These records may originate from different sources, yet they mirror the process of knowledge acquisition. Extracting overarching cognitive patterns from these varied student interaction sequences is pivotal for addressing the data scarcity issue in KT tasks. AdaptKT, as proposed by [16], delved into domain adaptation for knowledge tracing. However, it is demanding that AdaptKT requires the question texts and relatively sufficient interactions in the target domain, which is often not met in reality. Moreover interaction sequences from different education system differs a lot so it is unfeasible to directly use them as training data. Thus, the significant differences between various educational systems present another major challenge.\nWe innovatively frame the dilemma of the new education system as a domain generalization issue, as shown in Fig. 2. We aim to design a novel KT method that derives meta-knowledge from various student interaction sequences from different education systems(source domains), enabling seamless transfer to a new education system(target domain). In the context of domain generalization for knowledge tracing, different domains corresponds to the questions and interactions from different online education platforms, such as engineering statics course and Algebra course. These domains have different knowledge concepts and these concepts are related to different questions, we aim to assess student's mastery on various concepts. Sufficient student interaction records from specific systems are treated as source domains, while a limited number of interactions in the new system represent the target domain.\nTo solve the domain generalization issue, this paper introduces a novel approach, namely Domain Generalizable Knowledge Tracing (DGKT). DGKT's goal is twofold: firstly, to utilize data from multiple source domains for training a versatile KT model, and subsequently, to swiftly adapt this model to a novel target domain with a few student interactions. Notably, DGKT faces two primary challenges: 1) significant differences of concepts among the source domains, and 2) scarce student interactions within the target domain.\nFor the problem of significant differences of concepts among the source domains, since diverse source domains encompass different concepts, raw concepts are ineffective to achieve domain generalization. To overcome this challenge, we propose the concept aggregation-a clustering algorithm for concept embedding that analyzes students' interaction sequences from a unified perspective. Moreover, we propose a relation-based attention encoder that fully leverage the relation\nFor the problem of scarce student interactions within the target domain, we find it difficult to learn the target question embedding with a few interactions. Consequently, we design a unique concept representation for target domain that can effectively adapt to target domain with a few training data. Moreover, we propose a relation-based attention that fully leverage exercise information in target domain. The contributions of this paper are summarized as follows:\n\u2022 A novel solution of knowledge tracing for the data scarcity issue. The data scarcity issue is valuable but insufficiently studied. We aim to train a domain-generalizable KT model from auxiliary source domains and quickly adapt the model to new target domains with only a few interactions.\n\u2022 Innovative concept aggregation for reducing domain discrepancy. This addresses the challenge that questions and concepts from different domains have enormous differences. Through concept aggregation, similar concepts are associated with the same cluster centroid.\n\u2022 Domain-generalizable relation-based knowledge tracing. We introduce a domain-generalizable relation-based knowledge tracing approach that leverages relational information within the target domain. Our specially designed model demonstrates superior performance in domain generalization for knowledge tracing.\nAfter conducting extensive experiments on five benchmark datasets, our approach shows a performance of 4.16% improvement over five KT methods on domain generalization of knowledge tracing tasks."}, {"title": "II. RELATED WORK", "content": "In this section, we introduce the related work including congnitive diagnosis, knwoledge tracing and domain generalization."}, {"title": "A. Cognitive Diagnosis", "content": "In online education systems, cognitive diagnosis is an essential task aimed at analyzing a student's performance and providing a detailed evaluation of their proficiency across various concepts. Over the past decades, numerous cognitive diagnosis models have been developed, which can be broadly classified into two categories: traditional cognitive diagnosis models and deep learning-based cognitive diagnosis models. Traditional models include IRT [17], DINA [18] and MIRT [19], and one of the most prominent is IRT. IRT represents a student's ability as a one-dimensional scalar and calculates the probability of correctly answering an exercise using a logistic function. This model has been widely used due to its simplicity and interpretability. However, it is limited by its assumption of uni-dimensionality and its relatively simplistic modeling of student responses. In contrast, deep learning-based cognitive diagnosis models leverage the power of neural networks to capture more complex patterns in student data. A notable example is NeuralCD [20], which applies a multilayer perceptron to model the cognitive diagnosis process. By utilizing deep neural networks, NeuralCD can learn intricate relationships between student performance and underlying knowledge concepts.\nWhile cognitive diagnosis models are effective in evaluating a student's proficiency at a given point in time, they often lack the capability to continuously track the student's knowledge state over an extended learning period. This limitation hinders their ability to provide ongoing, dynamic feedback that adapts to the student's evolving learning needs."}, {"title": "B. Knowledge Tracing", "content": "Knowledge tracing task is an essential task to trace a student's knowledge state over an extended learning period. Previous methods for KT can be divided into two categories, i.e., traditional machine learning methods and deep learning methods. Among the methods based on traditional machine learning algorithms, the most representative one is BKT [21]. BKT builds a hidden Markov model for each knowledge concept to predict a student's mastery of specific concepts. Other traditional machine learning KT models include Performance Factors Analysis (PFA) [22] and item response theory (IRT) [23].\nRecently, many deep models for KT have emerged. The earliest deep model for KT is Deep Knowledge Tracing (DKT) [6]. DKT applies recurrent neural networks (RNNs) and outperforms traditional KT models. Many variants of DKT are proposed afterwards such as DKT+ [24]. Exercise-Enhanced Recurrent Neural Network (EERNN) [25] is proposed for student performance prediction by taking full advantage of both student exercising records and the text of each exercise. Another typical deep KT model is Separated Self-AttentIve Neural Knowledge Tracing (SAINT) [8] which applies Transformer to KT tasks. Attentive Knowledge Tracing (AKT) [9] is another attention-based KT model using a novel monotonic attention mechanism that relates a learner's future responses to assessment questions to their past responses. Learning Process-consistent Knowledge Tracing (LPKT) [26] monitors students' knowledge state through directly modeling their learning process. As far as we know, few works focus on the data scarcity issue in online education systems, and we innovatively attempt to train a generalizable KT model."}, {"title": "C. Domain Generalization", "content": "Domain generalization (DG) has attracted increasing interest in recent years [27]. Some DG methods improve generalization via designing novel learning strategies or augmentation [28], [29]. Mancini et al. use learnable weights for aggregating the predictions from different source-specific classifiers [30], where a domain predictor is adopted to predict the probability of a sample belonging to each domai. Learning and Removing Domain-specific features for Generalization (LRDG) [31] learns a domain-invariant model by tactically removing domain-specific features from the input images. There are several DG methods based on meta-learning. Inspired by model-agnostic meta-learning (MAML) [32], Li et al. propose Meta-Learning for DG (MLDG) [33], which is an optimization-based meat-learning strategy for domain generalization. MetaReg [34] introduce a meta regularizer in meta-learning for fine-grained regularization. Another way to solve the DG problem is representation learning. Many DG methods reduce distribution discrepancy across training domains to learn domain invariant representations via feature normalization [35] or minimize feature distribution divergence explicitly [36]. SNR [37] is proposed to simultaneously ensure both high generalization and discrimination capability of the networks. However, these methods cannot be directly borrowed for KT tasks since KT relies on comprehending a student's learning trajectory."}, {"title": "III. DOMAIN-GENERALIZABLE KNOWLEDGE TRACING", "content": "In this section, we elaborate on the proposed DGKT method. Firstly, we formulate the knowledge tracing (KT) tasks and their domain generalization setting. Subsequently, we present the model architecture and concept aggregation. Furthermore, we introduce the generalization of KT model, which empowers the model to acquire meta-knowledge from source domains and effectively apply it to the target domain."}, {"title": "A. Problem Definition", "content": "A KT task with few sequences. In a KT task, there are very few sequences of interactions for training, denoted as $I = \\{(q_1, r_1), (q_2, r_2), ..., (q_T, r_T)\\}$. Here, $T$ represents the length of the sequence, $q_t \\in \\mathbb{N}^+$ corresponds to the question ID of the $t$-th interaction ($t < T$), and $r_t \\in \\{0,1\\}$ means the correctness of the student's answer to the question $q_t$. These questions are associated with $n_c$ concepts that students need to master. The objective is to train a KT model capable of mining the knowledge state of students and predicting the probability that a student will answer the next question correctly, denoted as $P(r_{n+1} | q_{n+1}, I)$. Since data scarcity affects model training, we resort to other available sequences as auxiliary source domain data for domain generalization.\nDomain generalization setting. In the context of domain generalization for knowledge tracing, there are $N$ auxiliary source domains $\\{D^i | 1 \\leq i \\leq N\\}$ and a target domain $D^t$. Each source domain $D^i$ consists of $m_i$ sequences of student interactions, denoted as $D^i = \\{I^j | 1 \\leq j \\leq m_i\\}$ along with a set of questions $\\{q_1, q_2, ..., q_{n_{q^i}}\\}\\$ and a set of concepts $\\{C_1, C_2, ..., C_{n_{c^i}}\\}\\$, where $n_{q^i}$ represents the number of questions, $n_{c^i}$ represents the number of concepts. The target domain, on the other hand, contains only $m_t$ student interaction sequences, represented as $D^t = \\{I^j | 1 \\leq j \\leq m_t\\}$, with $m_t < m_i$. Importantly, the data $D^t$ from the target domain is unseen during the meta-training phase. The objective is to train a generalized KT model by leveraging all the source domain data $\\{D^i | 1 \\leq i \\leq N\\}$, and subsequently adapt this model to the target domain data $D^t$ for knowledge tracing."}, {"title": "B. Model Architecture", "content": "The process outlined in Fig. 3 illustrates the key steps of our DGKT approach. Initially, we employ the feature embedding module to transform the sequence into embedding sequence. These embedding sequences are then input into the knowledge state encoder, which generates the hidden knowledge state. Ultimately, this knowledge state is decoded by the knowledge state decoder, producing the anticipated probability of a student providing a correct response to the subsequent question.\nFeature embedding module. We aim to convert the sequence of questions and responses into the embedding sequence. For each knowledge tracing domain, we have a concept matrix $Q \\in \\mathbb{R}^{n_c \\times n_q}$, where $n_q$ represents the total number of question IDs and $n_c$ represents the total number of knowledge concept IDs. The element $Q_{ij}$ is set to 1 if the j-th question is related to the i-th knowledge concept. Thus, we learn the embedding $e_{q_t} \\in \\mathbb{R}^d$ of question $q_t$ ($d$ is the dimension of the embedding), which can be written by\n$\\e_{q_t} = \\frac{\\sum I(Q_{iq_t} = 1)e_i}{\\sum I(Q_{iq_t} = 1)},\\qquad (1)$,\nwhere $I()$ represents the indicator function, which equals 1 if the condition inside the parentheses is true, and 0 otherwise. $e_i \\in \\mathbb{R}^d$ is a learnable vector representing the i-th concept.\nUsing the questions and responses, we construct the question-response embedding $e_{q_t r_t} \\in \\mathbb{R}^{2d}$ as follows:\n$e_{q_t r_t} = \\begin{cases} e_{q_t} \\oplus \\mathbf{0}, & \\text{if } r_t = 1, \\\\ \\mathbf{0} \\oplus e_{q_t}, & \\text{if } r_t = 0, \\end{cases} \\qquad (2)$\nwhere $\\mathbf{0} = (0,0,..., 0)$ is an all zero vector with the same dimension $d$ as $e_{q_t}$, and $\\oplus$ is the concatenation operator.\nBy concatenating all the embeddings in a sequence, we obtain the question embedding matrix $M_q\\{1:T\\} = (e_{q_1}, e_{q_2}, ..., e_{q_T}) \\in \\mathbb{R}^{d\\times T}$ and the question-response embedding matrix $M_{qr}\\{1:T\\} = (e_{qr_1}, e_{qr_2}, ..., e_{qr_T}) \\in \\mathbb{R}^{2d \\times T}$.\nKnowledge tracing backbone. To encode question embedding and question-response embedding into knowledge state, we employ backbone of existing knowledge tracing model. Here we take DKT, SAINT and AKT as examples to illustrate how to transform embedding into knowledge state using off-the-shelf knowledge tracing models.\nFor DKT, we use LSTM as knowledge state emcoder to get student's knowledge state:\n$\\begin{aligned} f_t &= \\sigma(W_f e_{q r_t} + U_f h_{t-1} + b_f), \\\\ i_t &= \\sigma(W_i e_{q r_t} + U_i h_{t-1} + b_i), \\\\ o_t &= \\sigma(W_o e_{q r_t} + U_o h_{t-1} + b_o), \\\\ \\hat{c}_t &= \\sigma(W_c e_{q r_t} + U_c h_{t-1}+b_c), \\\\ C_t &= f_t \\cdot C_{t-1} + i_t \\cdot \\hat{c}_t, \\\\ h_t &= o_t \\cdot \\sigma(c_t), \\end{aligned} \\qquad (3)$\nwhere $e_{q r_t}$ denotes the t-th question-response feature and $h_t$ represents the student's knowledge state at timestep t.\nLikewise, in the AKT model, three attention-based modules are utilized: the question encoder, the knowledge encoder, and the knowledge retriever. The student's knowledge state can be represented as:\n$\\begin{aligned} x_t &= \\text{Encoder}_q(e_{q_1}, e_{q_2}, ..., e_{q_t}), \\\\ Y_t &= \\text{Encoder}_k(e_{qr_1}, e_{qr_2}, ..., e_{qr_t}), \\\\ h_t &= \\text{Decoder}((X_1,Y_1), (X_2,Y_2), ..., (X_{t-1}, Y_{t-1}), x_t), \\end{aligned} \\qquad (4)$\nwhere Encoderq represents the question encoder which produce modified, contextualized representations of each question based on the sequence of questions the learner has previously practiced on, Encoderk represents the knowledge encoder which produces modified, contextualized representations of the knowledge the learner acquired while responding to past questions, and Decoder represents the knowledge retriever which retrieves knowledge acquired in the past that is relevant to the current question using an attention mechanism. In AKT, the student's knowledge state h is obtained in the knowledge retriever module by first constructing question features x based on the student's question sequence and constructing question-interaction features y based on the student's question-interaction sequence. Then, h is derived through attention layers applied to x and y.\nAs for SAINT, the transformer's encoder is responsible for receiving the student's question sequence, while the decoder receives both the student's question-interaction sequence and the output from the encoder:\n$\\begin{aligned} o_t &= \\text{Encoder}(e_{q_1}, e_{q_2}, ..., e_{q_t}), \\\\ h_t &= \\text{Decoder}(e_{qt_1}, e_{qt_2}, ..., e_{qt_{t-1}}, o_1, o_2, ..., o_t), \\end{aligned} \\qquad (5)$\nwhere Encoder and Decoder represent the Transformer's encoder and decoder respectively.\nKnowledge state decoder. To decode knowledge state into the probability, we use the same knowledge encoder to predict the probability of a student answering the next question correctly:\n$\\begin{aligned} \\hat{y}_{t+1} &= \\text{Dec}(h_{t+1}, q_{t+1}) \\\\ &= \\sigma(W_2 \\cdot \\text{ReLU}(W_1\\cdot [h_{t+1},e_{q_{t+1}}] + b_1) + b_2), \\end{aligned} \\qquad (6)$\nwhere Dec represents the knowledge state decoder, $W_1, W_2$ and $b_1, b_2$ denote weights and biases, respectively. $\\sigma(\\cdot)$ is the sigmoid function."}, {"title": "C. Sequence Instance Normalization", "content": "To reduce the distribution discrepancy from different domains, utilizing normalization method is a common solution. However, existing normalization methods may not be applicable for sequence feature in knowledge tracing task since feature after timestept should be kept unseen when calculating y at timestep t. To tackle the dilemma, we design the sequence instance normalization SeqIN in our KT model to normalize the feature embeddings of sequential student interactions among domains. Since the normalization process takes into account the fact that the later interactions cannot be seen in the previous interactions, the feature embedding of the interaction each moment is normalized by only the statistics of all the previous moments.\nGiven a sequential feature matrix $M = (m_1,M_2, ..., M_n)$ where $m_t \\in \\mathbb{R}^d$ is the feature embedding at time t, the normalized feature matrix $M$ is calculated by\n$\\tilde{M} = (\\tilde{m_1}, \\tilde{m_2}, ..., \\tilde{m_n}), \\qquad (7)$\n$\\begin{aligned} \\tilde{m_t} &= \\gamma(\\frac{m_t - \\mu_t(M)}{\\sigma_t(M)}) + \\beta, \\qquad (8) \\\\ \\mu_t(M) &= \\frac{1}{t+1} (p + \\sum_{i=1}^{t} m_i), \\qquad (9) \\\\ \\sigma_t(M) &= \\sqrt{\\frac{\\sum_{i=1}^{t} (p - \\mu_t(M))^2 + \\sum_{i=1}^{t} (m_i - \\mu_t(M))^2}{t+1}}, \\qquad (10) \\end{aligned}$\nwhere $\\gamma, \\beta\\in \\mathbb{R}^d$ are the parameters learned from the data. $\\mu_t(M)$ and $\\sigma_t(M)$ denote the mean and standard deviation of the sequence $\\{m_1, m_2, ..., m_t\\}(1 \\leq t \\leq n)$. As seen, SeqIN normalizes $m_t$ by considering all previous feature embeddings up to time t while remaining later feature embeddings unseen. In consideration of the meaninglessness to compute the mean and standard deviation of $m_1$ itself, we add a padding vector $p$ in front of the original sequence, i.e., $\\{p, m_1, m_2, ..., m_n\\}$, where p is learned along with the model. Moreover, SeqIN can allow the feature embedding matrices of student interactions from different source domains to be aggregated together, which will be obviously observed in Fig. 5 in the visualization experiments.\nWe apply SeqIN to aforementioned knowledge tracing backbone networks, including DKT, AKT and SAINT. For DKT, the SeqIN is directly used on students' knowledge state h:\n$\\tilde{h} = \\text{SeqIN}(h), \\qquad (11)$\nwhere h is the students' knwoledge state. For AKT, we apply SeqIN to intermediate features x and y as follows:\n$\\begin{aligned} \\tilde{y} &= \\text{SeqIN}(y), \\\\ \\tilde{x} &= \\text{SeqIN}(x), \\end{aligned} \\qquad (12)\\qquad (13)$\nwhere x and y is respectively derived from the question encoder and the knowledge encoder of AKT.\nAs for SAINT, we apply SeqIN to intermediate features o:\n$\\tilde{o} = \\text{SeqIN}(o), \\qquad (14)$\nwhere o is the output of Transformer's encoder."}, {"title": "D. Concept Aggregation", "content": "We here illustrate the concept aggregation for KT model, which enables KT model to retrive meta-knowledge from source domains, aggregate concepts from different domains and adapt to target domain.\nIn the knowledge tracing task, each question is typically associated with a few specific knowledge concepts, which is directly utilized by conventional knowledge tracing models. However, this approach is not applicable for domain generalization knowledge tracing tasks, as knowledge concepts can vary across different domains, and they may even share the same concept ID. Additionally, the features of interaction sequences from different knowledge tracing domains show significant variations. For example, in certain knowledge tracing tasks, students may practice a question repeatedly, while in other tasks, students will move on to the next question regardless of their performance on the current question. Consequently, the question sequences from different domains exhibit different levels of granularity, which poses challenges for knowledge tracing models to effectively analyze a student's learning patterns.\nTo address the issue of granular differences, we propose concept aggregation for domain-specific knowledge concepts. This approach involves learning the domain-specific concept embedding and conducting k-means algorithm for embedding across all source domains. Subsequently, we replace concept embeddings with calculated centroid embedding and further train the model using concept embeddings.\nConcept feature learning. We first train the KT model on all source domains. Each source domain learns their own concept embedding matrix $E_i = \\{e_j^i\\}_{j=1}^{n_{c^i}}$ (referred to Eq. (1)), while the remaining parameters of knowledge state encoder and decoder, namely $\\theta_{enc}$ and $\\theta_{dec}$ in KT model respectively, are shared across all domains. During this stage, we randomly sample data from each source domain. The model is trained by minimizing classification loss on all source domains, which can be formulated as:\n$\\mathbb{E}_{D^i \\in D, \\{I^j, Y^j\\} \\sim D^i} [\\mathcal{L}_{CE}(Y^j, R)],\\qquad (15)$\nwhere $\\mathcal{L}_{CE}$ is the cross-entropy loss, $I^j$ is the j-th interaction sequence in the i-th source domain, and $R$ is the corresponding response to the next question.\nConcept clustering. After first training on source domains, we obtain the concept embeddings $\\{E^i\\}_{i=1}^N$ from various domains. In this stage, we apply the k-means algorithm to all concept embeddings, dividing numerous concepts into k clusters. This process provides KT model with a coarse-grained perspective of interaction sequences. Specifically, through the k-means procedure, we obtain a cluster assignment matrix A and the embeddings of each centroid Ec.\n$\\begin{aligned} E_{cat} &= [E_1, E_2, ..., E_N], \\qquad (16) \\\\ A &= \\text{Kmeans}(E_{cat}), \\qquad (17) \\\\ e_{c_i} &= \\frac{1}{C_i} \\sum_{j=1}^{n_{c^i}} E_{cat_{j}} A_{ij}, \\qquad (18) \\\\ E_c &= [e_{c_1}, e_{c_2}, ..., e_{c_k}], \\qquad (19) \\end{aligned}$\nwhere $E_{cat}$ concatenates all concept embeddings from N source domains, and $n_c$ represents the total of the concepts. $A \\in \\{0,1\\}^{k \\times n_c}$ is the cluster assignment matrix where $A_{ij} = 1$ means the j-th concept of $E_{cat}$ is assigned to the i-th centroid. $C_i$ denotes the number of concepts of the i-th cluster. $e_{c_i}$ denotes the embedding of i-th centroid and $E_c\\in \\mathbb{R}^{d \\times k}$ represents the matrix of centroid embeddings.\nConcept centroid refinement. Based on the results of concept clustering, we replaced the concept embeddings of each source domain with the embeddings of the cluster centroids to narrow the difference between the source domains. Accordingly, for domain x, the question embedding is rewritten as:\n$Q = A_xQ_x,\\qquad (20)$\n$e_{q_t} = \\frac{\\sum I(Q_{iq_t} = 1)e_i}{\\sum I(Q_{iq_t} = 1)},\\qquad (21)$\nwhere $A_x \\in \\{0,1\\}^{k \\times n_{c^x}}$ is the cluster assignment matrix of domain x, where $A_{xij} = 1$ means the j-th concept of $E_x$ is assigned to the i-th centroid. Q represent the relationship between questions and centorids, where $Q_{ij} = 1$ when j-th question is related to i-th centroid. $Q_x$ is the concept matrix of domain x. er is the i-th vector in $E_c$, and A are calculated in concept clustering phase. Similar to Eq. (15), we randomly sample data from source domains and update the parameters ($E_c, \\theta_{enc}$ and $\\theta_{dec}$) in KT model by minimizing the classification loss.\nThe procedure of concept aggregation can be succinctly summarized as Algorithm 1."}, {"title": "E. Generalization to Target Domain", "content": "For domain generalization of KT tasks, we delve into transferring the learned KT model to a designated target domain. Firstly, we initialize the concept embedding of the target domain and subsequently design a novel concept representation for the target domain which not only exhibits adaptability to the target domain but also maintains the centroid embeddings learned from the source domain to prevent overfitting.\nInitialization for target embedding. We begin by initializing the concept embedding for the target domain, namely target embedding. Given that several concepts in the target domain remains unseen, the optimal approach for initializing the target embedding is to utilize the learned centroid embedding. For the target embedding, the formulation is as follows:\n$\\begin{aligned} e_{t_i} &\\in \\{e_{c_1}, e_{c_2}, ..., e_{c_k} \\}, \\\\ E_t &= [e_{t_1}, t_{t_2}, ..., e_{t_{n_{c^t}}}], \\end{aligned} \\qquad (22)\\qquad (23)$\nwhere $e_{t_i}$ represents the i-th concept embedding in target domain, $1 \\leq i \\leq n_{c^t}$. $n_{c^t}$ is the number of concepts in target domain. Initially, $e_{t_i}$ is randomly selected from the set of centroid embeddings. $E_t$ represents target embedding matrix.\nConcept representation of target domain. Regarding the representation of concepts in the target domain, we aim for it to leverage the parameters learned during the concept aggregation phase effectively. Additionally, we seek its efficient adaptation to the target domain, enabling the model to swiftly transition with limited data while avoiding overfitting. For problems in target domain, we have:\n$\\begin{aligned} e_{q_t} &= \\frac{\\sum I(Q_{iq_t} = 1)e_i}{\\sum I(Q_{iq_t} = 1)}, \\qquad (24) \\\\ j &= \\underset{1<i<k}{\\text{argmin}} ||e_{q_t} - e_{c_i} ||, \\qquad (25) \\\\ e_{target} &= (1 - \\lambda)e_{c_j} + \\lambda e_{q_t}, \\qquad (26) \\end{aligned}$\nwhere $e_r$ is the concept embedding retrived from $E_t$. $e_{c_j}$ denotes the j-th centroid embedding. $\\lambda$ is a hyperparameter used to adjust the degree of freedom of the target concept embedding. $E_{target}$ is the final representation of $q_t$ in target domain that will be fed into KT model.\nTarget embedding adaptation. We fine-tune our KT model on target domain using the aforementioned concept representation. During this phase, only the target embedding $E_t$ is optimized, while other parameters including $\\theta_{enc}$ and $\\theta_{dec}$ are frozen, which is similar to Eq. (15).\nIn this way, our model will effectively adapt to target domain while fully utilizing its learned centroid embedding in concept aggregation. For clarification, the whole process of generalization is summarized in Algorithm 2."}, {"title": "IV. DOMAIN-GENERALIZABLE RELATION-BASED KNOWLEDGE TRACING", "content": "Although our proposed DGKT framework effectively enhances the performance of existing KT models in new domains, these models are not inherently designed for domain generalization in knowledge tracing. One of the biggest challenges hindering these models from transferring to a new domain is the complete discrepancy in exercise IDs across domains, which leads to an inability to fully leverage the information from exercise IDs in students' interaction sequences.\nTo address this issue, we propose a domain-generalizable relation-based knowledge tracing model(DGRKT) specifically designed for domain generalization in knowledge tracing. This model captures the relationships between exercises and concepts across different timesteps based on attention mechanism. In this section, we illustrate DGRKT in detail including DGRKT model architecture and relation-based attention encoder."}, {"title": "A. DGRKT Model Architecture", "content": "Similar to the aforementioned model architecture, our DGRKT consists of a feature embedding module, a knowledge state encoder, and a knowledge state decoder. In DGRKT, the feature embedding module and knowledge state decoder remain the same, while the knowledge state encoder is specifically designed to capture the relationships between different timesteps. As described above, we can transform a student's interaction sequence into question embedding matrix and question-response embedding matrix:\n$M_q, M_{qr} = \\text{FeatureEmbeding}((q_1, r_1), (q_2, r^2), ..., (q_t, r_t)).\\qquad (27)$\nGiven a question embedding matrix $M_q$ and a question-response embedding matrix $M_{qr}$, the outputted knowledge state can be represented as:\n$\\begin{aligned} X_t &= \\text{R-Attn}(Q = M_q, K = M_q, V = M_{qr}), \\\\ h_t &= \\text{SeqIN}(X_t), \\end{aligned} \\qquad (28)$\nwhere $M_q$ denotes the question embedding matrix and $M_{qr}$ denotes the question-response embedding matrix. R-Attn(\u00b7) is the relation-based attention mechanism, which will be elaborated in the upcoming subsection. Q, K, and V represent the query, key, and value, respectively, in the attention mechanism. SeqIN(\u00b7) is the aforementioned sequence instance normalization."}, {"title": "Likewise, the probability of a student answering the next question correctly can be calculated as:", "content": "$\\hat{y}_{t+1} = \\text{Dec}(h_{t+1}, q_{t+1}).\\qquad (29)$\nSame as before, we conduct concept aggregation to realize generalization to"}]}