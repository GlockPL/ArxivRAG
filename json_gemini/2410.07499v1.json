{"title": "Dense Optimizer : An Information Entropy-Guided Structural Search Method for Dense-like Neural Network Design", "authors": ["Tianyuan Liu", "Libin Hou", "Linyuan Wang", "Xiyu Song", "Bin Yan"], "abstract": "Abstract-Dense Convolutional Network has been continuously refined to adopt a highly efficient and compact architecture, owing to its lightweight and efficient structure. However, the current Dense-like architectures are mainly designed manually, it becomes increasingly difficult to adjust the channels and reuse level based on past experience. As such, we propose an architecture search method called Dense Optimizer that can search high-performance dense-like network automatically. In Dense Optimizer, we view the dense network as a hierarchical information system, maximize the network's information entropy while constraining the distribution of the entropy across each stage via a power-law, thereby constructing an optimization problem. We also propose a branch-and-bound optimization algorithm, tightly integrates power-law principle with search space scaling to solve the optimization problem efficiently. The superiority of Dense Optimizer has been validated on different computer vision benchmark datasets. Specifically, Dense Optimizer completes high-quality search but only costs 4 hours with one CPU. Our searched model DenseNet-OPT achieved a top-1 accuracy of 84.3% on CIFAR-100, which is 5.97% higher than the original one.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of computer vision, searching for superior and lightweight network architecture will never be an outdated task. Series of work like AlexNet [1], VGG [2], ResNet [3], and DenseNet [4] has improved the effectiveness of neural networks and our understanding of network design. Since the proposal of DenseNet, its design concept has been widely applied to the design of various advanced backbone network models. Networks based on improvements to Dense, such as Channel Cross-Linked Dense Convolutional Networks [5], lightweight DenseNet structure, dense unit, dense connection mode, and attention mechanism, have achieved significant accomplishments in tasks such as EEG emotion detection, medical analysis, etc. [6]-[9].\nHowever, with the expansion of model scale, it becomes increasingly difficult to adjust the network connection channel design and reuse level based on past experience. So far, Neural Architecture Search(NAS) have provided convenience for structural design by constructing high-performance networks using reinforcement learning or gradient-based algorithms within a given fixed search space [10] [11]. But to some extent, automatic search does not equal a good automatic design method. These previous NAS works cannot skillfully control aspects such as the convolution kernel and the number of channels. Meanwhile NAS methods require the training and evaluation of a large number of networks during the search phase, which is both time-consuming and computationally intensive.\nThe bilevel optimization of privious NAS, like differentiable architecture search(DARTS) [11], brings serious computa-tional overhead problem. Our solution is to separate the struc-tural parameter search and weight parameter training of Dense, decouple the bilevel optimization into a two-step optimization, and transform the design of structural hyperparameters (such as the size of the convolution kernel, the number of channels, etc.) into an optimization problem. Information entropy is an effective tool to describe the network representation ability [12], which can be used as the main optimization objective to search Dense structures. After the Dense structural parameters are optimized, the weight parameters will be trained to obtain the final high-performance model.\nIn practical terms, we propose Dense Optimizer, a uni-versal structural parameters optimizer for dense-like network structures. Dense Optimizer establishes an optimization model to maximize the structural information entropy of the dense backbone network by searching for the optimal configuration of network depth and width and kernel size. And it incorporate a power-law distribution as an evaluation metric, and naturally embedding it into the optimization model. Besides, we propose a branch-and-bound optimization algorithm based on search space scaling to solve the problem. Utilizing traditional dense-BC modules, the models designed by Dense Optimizer are comparable with CNN models of the same size and FLOPs.\nThe following are the key contributions of this work:\n\u2022 Dense Optimizer is proposed as a dense architecture search method. The search process is transformed into an optimization problem to construct Dense model effi-ciently.\n\u2022 Maximizing the network's information entropy under multi-scale entropy power-law distribution principle is proposed to conduct the optimization model.\n\u2022 A branch-and-bound optimization algorithm is proposed, tightly integrate power-law principle with search space scaling."}, {"title": "II. RELATED WORK", "content": "A. Design of DenseNet\nDenseNet has garnered widespread attention and research interest in the field of computer vision [13]. Owing to its excellent performance and flexibility, a multitude of manual designs in recent years have leveraged to refine and augment the DenseNet architecture [14] [15]. However, these designs still heavily rely on on human expertise and lack principles for structural design guidance [6]. When selecting hyperpa-rameters such as channel growth rate and convolution kernels, extensive experimentation and tuning are often required [16]. Dense Optimizer provides an efficient and automatic struc-tural search method for Dense-like networks and promote its performance at the same time.\nB. Neural Architecture Search\nNeural Architecture Search(NAS) have been greatly stud-ied during the past few years to automatically design more effective architectures. Popular NAS algorithm options in-clude genetic algorithms [17]\u2013[20], reinforcement learning (RL) [21], [22], differentiable search [11], [23] and many other types of optimization algorithms, e.g., Bayesian opti-mization and particle swarm optimization [24]-[26]. As a classic differentiable architecture search method, DARTS is the most widely attention-grabbing algorithms. It models the architecture design as a bi-level optimization problem, which requires training vast numbers of candidate networks to inform the search process, and often leads to high computational cost. Recent NAS work Mellor et al. [27] start to explore indica-tors that can predict a network's performance without training. Moreover, Xuan et al. [28] endeavor to decouple network weights from network architecture, focusing on the discovery of improved design principles through the exploration of structural parameters alone. These methods have represented an effective exploration in neural network architecture search that decouples structural parameters and weights, but still lack good design guidelines. Dense Optimizer conducts further in-depth research. Dense Optimizer circumventing the time-consuming issue of bi-level optimization, and uses information entropy as a design criterion to obtain a better structure by solving an optimization problems.\nC. Mathematical Architecture Design\nInformation theory is a powerful tool for studying complex systems such as deep neural networks. Recently, mathematical architecture design(MAD) is proposed [28]. Unlike the ex-isting hyperparameter optimization methods [29]\u2013[31], MAD does not require any model training during optimization, allowing for the acquisition of optimized network structures within minutes. It maximizes the network entropy with three empirical guidelines and demonstrate an advancement in de-signing network structures using mathematical programming.\nHowever, MAD unable to characterize the information flow structure of the concatenation operation and accurately es-timate the information entropy of the dense network struc-ture. And its three empirical guidelines do not have strong theoretical foundation. Dense Optimizer not only specifically addresses these issues, but also constraints the distribution of entropy at different scales with power-law and proposes a new optimization model for dense-like network architectures."}, {"title": "III. DENSE BASED ARCHITECTURE OPTIMIZER", "content": "In this section, we introduce the core architecture of Dense Optimizer. Specifically, we consider the deep neural networks as a continuous information processing system. We provide a definition of structural entropy's effectiveness and extend it from Multi-Layer Perceptron(MLP) to networks with dense connections. Then, we propose an optimization model to study the architectural design of DenseNet. To formulate the optimization problem for DenseNet, we first define the entropy that governs its expressiveness, followed by the power-law constraints that regulate the efficacy of the multi-scale entropy distribution. Ultimately, We provided a precise optimization model and proposed the corresponding branch-and-bound op-timization algorithm.\nA. Entropy of DenseBlock\nThe principle of maximum entropy is one of the most widely applied principles in information theory [32]. Some previous works have also attempted to establish a connection between entropy and the structure of neural networks. Here, we provide a re-derivation, and give the entropy upper bound of DenseBlock. Suppose that in an L-layer MLP $f(\\cdot)$, the i-th layer has $w_i$ input channels and $W_{i+1}$ output channels. The output $x_{i+1}$ and the input $x_i$ are connected by $X_{i+1} = M_ix_i$, where $M_i \\in R^{W_{i+1}\\times W_i}$ is trainable weights. Then the structural parameters define how the input $x_i$ propagates inside the network, which is capable of being depicted.\nFor a DenseBlock with L layers, we consider the in-formation reuse caused by the network's dense connections and the impact of information distribution brought about by the concatenation operation. Then we obtain the information entropy of the basic block of the dense network, defined in Proposition 1:\nProposition 1.The normalized Gaussian entropy upper bound of the DenseBlock $f(\\cdot)$ is\n$H_f = W_L\\log (w_0^* i!)$,\nwhere the $W_L$ is the width of L-th layer, and $w_0$ is the initial width of the DenseBlock. The whole derivation is given in Appendix A. The entropy measures the expressiveness of a DenseBlock. Following the Principle of Maximum Entropy [33], [34], we propose to maximize the entropy of DenseBlock under given computational budgets. When calculating the precise information entropy of a dense block, the number of the i-th dense layer, the input channels is $c_i$, the number of output channels is $c_{i+1}$, and the kernel size is $k_i$. Consequently, the \"width\" of a dense block layer is projected as $c_ik_i$ in (1).\nB. Effectiveness Defined in DenseBlock\nInspired by previous research, an infinitely deep network will become hard to train unless it meets particular structural requirements. Therefore, in Dense Optimizer, we propose to control the depth of the dense-like network to facilitate gradient flow throughout the entire architecture effectively. Typically, the depth and width of a network are relative; thus, the effectiveness of a network with L layers, where each layer possesses the same width W, can be defined as follows:\n$\\rho = L/W$.\nNormally, the width $w_i$ of each layer can be different. So the average width of an L layer network f(.) is defined by\n$\\overline{W} = (\\prod\\limits_{i=1}^{L} W_i)^{1/L} = \\exp(\\frac{1}{L} \\sum\\limits_{i=1}^{L} log(W_i))$\nBut in DenseBlock, each layer connects to all the previous layers. This results in a relatively steady increase in the number of parameters with each layer. The growth rate K is always the same, and much smaller than the number of input channels. So the average of the width can be defined as:\n$\\overline{W} = W_0 + K/2 \\approx W_0$.\nSo a DenseBlock has L-layers with $w_0$ input width and growth rate K, the effectiveness is defined by\n$\\rho = L/W_0$.\nC. Power law in entropy distribution\nIn information theory, entropy reflects the amount of in-formation or uncertainty within a system. Higher information entropy implies the system is more dispersed and diverse [36]. For multi-stage neural networks, they are akin to independent information systems. To eliminate the discrete distribution of information entropy and uncertainty across these systems, it is essential to propose a multi-scale entropy distribution under mathematical constraints. According to highly optimized tolerance(HOT) [37], when a complex system is in a HOT state, the system will satisfy power-laws, that is, a global optimization process can lead to power-law distributions: inputs with characteristic scales, after undergoing a global system's \"output\" optimization process, can produce outputs with power-law characteristics [38].\nBased on extensive experimental statistics, We find that the information entropy distribution of dense network follows a power-law distribution, see figure 1.\nTo reinforce this constraint, We propose a power-law in en-tropy distribution, and we use a two-parameter fitting function with parameters 'a' and 'b' to optimize the distribution. The\nobject is to maximize the value of 'a' and minimize the value of 'b' under the same fitting parameter settings.\nHere, we provide specific definitions. Following (2), The cumulative entropy distribution sequence at the current stage is:\n$H = [H_1, H_2, H_3...H_i].$\nThe fitting expression of this sequence under the power law function is as follows:\n$H = a * M^b$,\nwhere a, b are power index parameters, and $M_i$ represent the i th stages. So the optimization target S is:\n$S = a - b$.\nSubsequently, we establish optimization constraints to achieve the objective of maximizing a while minimizing b.\nD. Optimization model and Solutions\nWe gather everything together and present the final opti-mization model for Dense Optimizer. Suppose that we aim to design an L-layer dense-like model F with M stages. The entropy of the i-th stage is denoted as $H_i$ defined in (2). We propose to optimize ${c_i, k_i, L_i}$ via the following optimization problem:\n$\\max_{w_i, L_i} \\sum\\limits_{i=1}^{M} w_i(10^i H_i + \\beta * S)$,\n$s.t.\\ L_i/W_i \\leq \\rho, \\\\ FLOPs [f(\\cdot)] \\leq budget, \\\\ Params [f(\\cdot)] \\leq budget, \\\\ W_1 \\leq W_2 \\leq ... \\leq W_L.$\nwhere $a_i$ is the weights of entropies at different scales, $\\beta$ is a tuning coefficient. $\\rho$ controls the effectiveness of the network whose value is usually tuned in range [10, 20].\nIntegrating the complexity of above optimization problem and the particularity of the candidate solution space of net-work structure, we propose a branch-and-bound algorithm"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "In this section, we first describe the detailed settings for search optimization using Dense Optimizer. Then, the op-timized dense network is trained, and the training settings are introduced in detail in subsection 4.2, and tested on the CIFAR-10, CIFAR-100, SVHN datasets. Meanwhile, the performance of the optimized structure is compared with the classic ResNet and DenseNet, and ablation experiments are conducted in Section 4.4 to verify the effectiveness of the multi-scale information entropy power-law distribution.\nA. Search Settings\nIn Dense Optimizer, the number of search populations N is 256, and the number of iterations is 500,000. The classic densenet121 is used as the initial backbone network. Search space parameters: the number of input and output channels for each block, convolution kernel size: [3,5,7], budget lay-ers=130, maximum growth rate: [12,24,40]. Meanwhile the optimization problem is solved on CPU device.\nB. Training Settings\nFollowing previous works, SGD optimizer with momentum 0.9 is adopted to train the dense models. The weight decay is 5e-4 for CIFAR dataset. The initial learning rate is 0.1 with batch size of 32. We use cosine learning rate decay with 5 epochs of warm-up. The number of training epochs is 100 for CIFAR-100. All experiments use the following data augmentations: mix-up, label-smoothing, random erasing, random crop/resize/flip/lighting, and Auto-Augment .\nC. Result\nAs shown in table I and table II, after optimization with Dense Optimizer, the performance of the dense network on the CIFAR-100 dataset significantly surpassed the original net-work. Given diverse latency budgets, our method outperforms the compared NAS methods in terms of the accuracy of the generated/searched architectures. It is solved in CPU device with small search cost. The optimized model, with a size of 32M, achieved a TOP-1 error rate of 16.96%, while the larger network of 171M size had a TOP-1 error rate of 15.7%.\nMoreover, we compare the searched architectures on CIFAR-10 and SVHN datasets, showing in table III and table IV.\nThe results demonstrate that Dense Optimizer achieves effective results on different datasets. Within the same model size, the accuracy of the optimized models is significantly higher than the original densenet, and they can easily out-perform ResNet family.\nD. Ablation Study and Analysis\nIn this section, we conducted ablation experiments on the CIFAR-100 dataset with power-law distribution constraints and performed statistical analysis on the information entropy"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a dense architecture search method, Dense Optimizer, which can achieve automatic net-work structure design under mathematical optimization and improve network performance. Dense Optimizer decouples network weights from network architecture and maximizes network entropy while maintaining the distribution of network structure entropy under power-law constraints. We show that Dense Optimizer can design models comparable to modern CNN models using only traditional dense-BC convolutional blocks, proving the powerful capabilities of Dense Optimizer to release the potential of traditional DenseNet models. Fur-thermore, Dense Optimizer can be applied to the design of other dense-like networks and the power-law distribution characteristic of structural information entropy provides con-siderable insight for models with multi-scale features."}]}