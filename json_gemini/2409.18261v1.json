{"title": "Omni6D: Large-Vocabulary 3D Object Dataset for Category-Level 6D Object Pose Estimation", "authors": ["Mengchen Zhang", "Tong Wu", "Tai Wang", "Tengfei Wang", "Ziwei Liu", "Dahua Lin"], "abstract": "6D object pose estimation aims at determining an object's translation, rotation, and scale, typically from a single RGBD image. Recent advancements have expanded this estimation from instance-level to category-level, allowing models to generalize across unseen instances within the same category. However, this generalization is limited by the narrow range of categories covered by existing datasets, such as NOCS, which also tend to overlook common real-world challenges like occlusion. To tackle these challenges, we introduce Omni6D, a comprehensive RGBD dataset featuring a wide range of categories and varied backgrounds, elevating the task to a more realistic context. 1) The dataset comprises an extensive spectrum of 166 categories, 4688 instances adjusted to the canonical pose, and over 0.8 million captures, significantly broadening the scope for evaluation. 2) We introduce a symmetry-aware metric and conduct systematic benchmarks of existing algorithms on Omni6D, offering a thorough exploration of new challenges and insights. 3) Additionally, we propose an effective fine-tuning approach that adapts models from previous datasets to our extensive vocabulary setting. We believe this initiative will pave the way for new insights and substantial progress in both the industrial and academic fields, pushing forward the boundaries of general 6D pose estimation.", "sections": [{"title": "1 Introduction", "content": "6D pose estimation aims at predicting the position, orientation, and size of objects in a 3D space using RGB (D) images, enabling various applications such as augmented/virtual reality [26,33], robot manipulation [11,35], and scene understanding [15,28].\nEarly instance-level pose estimation approaches [32, 38, 39, 42, 43] typically involve providing instance CAD models and predicting poses of instances that were seen during training, restricting the generalization to unseen objects. In contrast, recent research has shifted towards category-level 6D object pose estimation [6\u20138, 10, 16, 17, 20, 24, 25, 29, 34, 37, 40, 44\u201347], which learns category prior from a large number of instances within a category, allowing for pose estimation of new instances within the samze category without the need for CAD models. By learning on a diverse range of categories, category-level approaches could be a more versatile solution for 6D pose estimation in real-world scenarios.\nHowever, most existing datasets [22, 40, 44] are limited to a small number of object categories, typically less than 10, as shown in Tab. 1, hindering their practical applicability to complex scenes.\nTo overcome the limitations in previous category-level 6D pose estimation datasets, such as limited category numbers, lack of instance diversity within categories, and overly simplistic scenes, this paper presents a novel category-level dataset dubbed Omni6D for 6D pose estimation. Omni6D significantly extends the number of object categories to 166, and includes 4,688 real-scanned and well-annotated instance objects with a diverse range of shapes, sizes, and textures. The constructed benchmark includes 0.8M images featuring complex scenes with various occlusions, changing lighting conditions, complex backgrounds, and varying viewpoints. For each scene, we provide the rendered image, depth map, NOCS map, and instance mask. Also, considering the widespread rotational symmetry in objects, we examine three types of rotational invariance where an object maintains its original shape under following rotations: any degrees (Sym-1), multiples of 90 degrees (Sym-2) and 180 degrees (Sym-3). Additionally, we introduce a symmetry-aware metric to specifically address rotational invariance. Every object in Omni6D is adjusted to the canonical pose and annotated with rotational symmetry around three axes."}, {"title": "2 Related Work", "content": "Existing work on category-level 6D object pose estimation can be generally divided into two types. After extracting features from images or point clouds, they compute Rotation, Translation, and Size (RTS) either through implicit point correspondence or explicit regression.\nExisting Datasets. The most commonly used dataset for category-level 6D object pose estimation is NOCS [40], comprising both the synthetic CAMERA dataset and the real-world REAL dataset. CAMERA includes 300k RGBD images of 31 indoor scenes with 1,085 object instances across 6 categories, while REAL mirrors the categories in CAMERA and includes 8k RGBD images capturing 42 instances in 18 real scenes. Wild6D [44] consists of 5,166 videos with 1.1 million images over 1,722 object instances in 5 categories. ShapeNet-SRN Cars dataset and Sim2Real Cars dataset proposed in iNerf [22] both exclusively include a single car category. The former includes 3,514 instances derived from ShapeNet cars, while the latter is extracted from videos capturing 10 distinct unseen car models. These datasets are limited by their narrow range of categories, hindering their ability to generalize broadly. Additionally, most training images are synthetic and lack realism, and their scenes are overly simplified, failing to account for common real-world challenges like occlusions.\nImplicit Methods. Implicit methods are based on point correspondence [6, 20, 24, 34, 37, 40, 44, 46]. NOCS [40], one of the pioneering works in this area, introduced the concept of Normalized Object Coordinate Space (NOCS). The final pose and size of the object are obtained by matching the predicted NOCS map with the observed depth input using the Umeyama algorithm [36] and RANSAC algorithm [12].\nSubsequent algorithms such as DualPoseNet, RBP-Net and RePoNet [20,44, 46] have continued to develop along the vein of NOCS, implicitly solving for pose after predicting the NOCS map. SPD [34] proposed a category-level shape prior, subsequently deforming this shape prior (i.e., average shape) to fit observed point cloud. SGPA, RePoNet, and CATRE [6,24,44] continue to develop along SPD's category-level shape prior approach. Algorithms like 6-PACK and SGPA [6,37] extract low-rank structure points, i.e., keypoints, from dense observed point clouds. 6-PACK [37] predicts interframe motion of target instances through keypoint matching, while SGPA [6] employs keypoints for more effective incorporation of sparse structural information during prior adaptation. These methods rely heavily on the RANSAC process to eliminate outliers, making them non-differentiable and time-consuming.\nExplicit Methods. Explicit methods are based on direct pose regression [7, 10, 20, 24, 46, 47]. DualPoseNet and RBP-Net [20,46] conduct both explicit and implicit training,, where one parallel pose decoder explicitly regresses the pose. CATRE [24], recognizes the inherent difference between estimations of rotation and translation/size, explicitly regressing their residuals and carrying out an iterative pose estimation process. FS-Net [7] designs an autoencoder with 3D Graphic Convolution for latent feature extraction and separates the predictions for rotation and translation/size into two distinct networks: one estimates translation/size through two residuals, while the other handles rotation prediction by estimating deflections on two orthogonal axes. GPV-Pose and HS-Pose [10,47] utilize the same foundational mechanism introduced by FS-Net [7]. GPV-Pose [10] proposes a decoupled confidence-driven rotation representation that facilitates geometrically-aware recovery of correlated rotation matrices and introduces a new geometry-guided point-by-point voting paradigm for robust retrieval of 3D object bounding boxes. Meanwhile, HS-Pose [47] extends 3D-GC to extract mixed-range latent features from point cloud data through a simple network structure known as the HS layer."}, {"title": "3 Omni6D Dataset", "content": "As shown in Tab. 1, Omni6D comprises 4,688 instances across an impressive span of 166 categories. Each instance is a high-resolution textured mesh, obtained using Shining 3D scanner\u00b9 and Artec Eva 3D scanner2, collected from OmniObject3D [41]. We normalize object models to fit within a ( \u2212 1 , 1 ) 3 ( m 3 ) (-1,1)^3 (m^3) three-dimensional space, and align objects within each category to a consistent canonical pose. In the latest dataset, Omni6D-xl builds upon and extends Omni6D, comprising 15,957 instances across an impressive span of 419 categories. For more details, please refer to Appendix Section C.\nRendering. We employ stratified sampling to split instances within each category, subsequently dividing them into training, validation, and test sets in a 7:2:1 ratio. In the construction of our dataset, we utilize 9 room models from the Replica dataset as backdrops. For each scenery setup, we randomly select a room model to act as the background, along with 6-8 object instance models, which are allowed to perform free-fall motion within the room model, resulting in random scattering in a specific section of the room. Each object model is scaled by a random factor ranging from 0.8 to 1.2 as part of our data augmentation strategy. Considering the attention center of the combined instance models as the origin point, the camera randomly selects ten positions within a radius of 8-9 m and an elevation angle range between 30-90\u00b0. The camera then performs rendering at these selected positions while facing towards the attention center. \nSetting. We utilize BlenderProc 2.5.0 [9] to implement the aforementioned rendering process. The intrinsic parameters of the camera are set to [577.5, 577.5, 319.5, 239.5], with an image size specified as 640 \u00d7 480. Our approach ensures the diversity and breadth of the dataset, making it suitable for rigorous testing and yielding accurate results."}, {"title": "3.2 Data Annotations", "content": "Each rendered output includes a rendered RGB image, instance mask, NOCS mapping [40], depth map, ground truth class label, as well as 6D pose and size. Fig. 1 exhibits a selection of rendered outputs. To reduce the storage size of the dataset, we encode high-precision depth maps into RGB images by multiplying depth by 10,000, rounding to nearest integer, and converting to base 256. The resulting three digits represent RGB channels.\nRotational Invariance. Rotational invariance implies that a symmetric object can retain its original shape after rotation by certain angles. Many common objects have this property. As shown in Fig. 6, we define the coordinate system as a right-handed system with the x-axis pointing outwards and the y-axis oriented upwards. We contemplate three cases of rotational invariance where an object maintains its original shape after following rotations: any degrees (Sym-1), multiples of 90 degrees (Sym-2) and 180 degrees (Sym-3). Additionally, we denote the case of no rotational invariance around the axis as Sym-0. According to these definitions, all objects in Omni6D are annotated for their rotational symmetry around the xyz-axes. It's worth noting that symmetry attributes may differ among instances within the same category, requiring instance-level rather than category-level annotations. Fig. 6 illustrates all kinds of symmetry cases using object instances and quantifies their occurrence frequency. Fig. 1 selects several examples to provide a more visual explanation of rotational invariance. These considerations are then integrated into our evaluation protocols in Sec. 4.2."}, {"title": "3.3 Dataset Statistics", "content": "Omni6D aims to overcome challenges in estimating poses for occluded object instances. Fig. 3a and Fig. 3b show the spatial distribution of point clouds and objects by projecting their centroids on the XY-plane (top) and XZ-plane (bottom) [4]. Fig. 3c depicts the relative object size distribution, defined as the square root of the object-to-image area ratio. We observe that the spatial distribution of Omni6D is similar to that of CAMERA and REAL, with a greater resemblance to CAMERA despite having a closer depth range. However, a more pronounced discrepancy between the spatial distribution of point clouds and objects is evident in Omni6D compared to CAMERA and REAL. This observation suggests a higher occurrence of occlusion scenes in Omni6D, highlighting the intricate challenges it presents to 6D object pose estimation. Nonetheless, as depicted in Fig. 4a, algorithms trained on Omni6D demonstrate their robustness in tackling these complexities.\nAngular Deviation. Omni6D enables accurate pose estimation using only the lower half or bottom appearance of objects. Fig. 3d depicts the density of angular deviations from the upward direction, i.e. y-axis. Our dataset displays a more uniform distribution of object angles relative to the upward axis and exhibits greater deviation from the canonical pose angles. Unlike NOCS, which primarily uses upright object placement, Omni6D utilizes physical simulations for free-fall object positioning [9]. As a result, it presents more challenging and diverse pose estimation scenes. Training on Omni6D enhances algorithms' robustness to object rotation angles, as evidenced by the image in Fig. 4b.\nShape Priors. We obtain the mean latent embedding and shape prior for each category from the variational autoencoder [5]. Fig. 1 showcases categorical shape priors, each displaying unique characteristics, facilitating an intuitive association between point cloud shapes and corresponding real-world entities. Meanwhile, Fig. 3e explains clustering results based on categorical latent embeddings, where we employ agglomerative clustering [27] to group categories into 20 clusters. It highlights the geometric coherence among semantically identical objects (especially man-made ones) in Omni6D dataset and further confirms that these categorical shape priors can effectively leverage the wealth of shape information from numerous similar objects to elucidate category features. These insights provide a theoretical basis for applications of category-level 6D object pose estimation using our Omni6D dataset."}, {"title": "4 Evaluation and Analysis", "content": "Our experimentation utilized two datasets, namely Omni6D and Omni6Dout. Omni6D are partitioned into training, validation, and test sets in a 7:2:1 ratio, denoted as Omni6Dtrain, Omni6Dval and Omni6Dtest respectively. These sets are further subdivided into subsets with increasing category sizes of 3, 6, 12, 24, and 48. We denote the subset containing n categories as clsn. Each subset includes all classes present in the previous subset with additional classes included to meet the desired total. Fig. 6a presents the specific categories included in clsn and their respective sizes relative to each other. Omni6Dout is utilized as an additional test set to measure our algorithm's inter-category generalization. This dataset, constructed similarly to Omni6D, encompasses 52 models spanning 17 categories unseen in Omni6D, along with 4762 images. For additional details on datasets, please refer to the appendix.\nDetails. All experiments are carried out on a server equipped with an Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz and an NVIDIA A100-SXM4-80GB GPU. We maintain consistency in parameters and strategies throughout training, ensuring uniformity in our experiment environment. Given the challenges of semantic classification with a large vocabulary, we use ground truth masks to mitigate the impact of low-quality classification on pose estimation results."}, {"title": "4.2 Symmetry-Aware Evaluation", "content": "Basic Evaluation Metrics. We utilize the average accuracy of Intersection over 3D Union (IoU) [14] in object detection, and n \u2218 m cm in pose estimation. We further decompose n \u2218 m cm [19,31] to individually evaluate the model's predictive error n \u2218 for pose and m cm for translation. For these three types of errors, the thresholds considered are {50%, 75%}, {5\u00b0, 10\u00b0} and {2 cm, 5 cm} [3,30,42]. Additionally, we set a detection threshold for objects requiring at least a 10% overlap between predicted and ground-truth bounding boxes.\nOur Symmetry-Aware Metrics. Due to NOCS's limited categories, traditional algorithms mainly handle basic symmetry cases, such as rotational symmetry around the y-axis. However, Omni6D has a wider range of objects with different rotational invariances across multiple axes. Fig. 6 provides symmetry statistics for Omni6D objects. To alleviate this issue, we propose a symmetry-aware metric. Unlike prior works focusing solely on the y-axis, our method considers rotation symmetry around all three axes.\nWe define the relevant variables as follows: $L_s$ denotes our symmetry-aware metric, $L$ denotes the original metric. $R$ stands for the ground truth rotation matrix, while $R^*$ represents the predicted rotation matrix. $R_{\\theta_x,\\theta_y,\\theta_z}$ corresponds to the predicted rotation matrix after sequentially rotating by $\\theta_x$, $\\theta_y$, and $\\theta_z$ degrees around the xyz axes. The rotational invariance cases around the x, y, and z axes are denoted as Sym-nx, Sym-ny, and Sym-nz, where nx, ny, and nz are the respective rotation parameters. Objects that align with Sym-n around an axis maintain their original shape when rotated by an angle from $\\Theta_n$.\nSince the Euler angles are compact [13], the most straightforward approach is to determine the category of rotational invariance for each axis {x, y, z} sequentially, as mentioned in 3.2. To simplify computations, we set $\\Theta_0$ = {0\u00b0}, $\\Theta_1$ = {0\u00b0,1\u00b0,...,359\u00b0}, $\\Theta_2$ = {0\u00b0,90\u00b0, 180\u00b0, 270\u00b0}, $\\Theta_3$ = {0\u00b0, 180\u00b0}. We can define $L_s$ as $L_s = \\min_{\\theta_x \\in \\Theta_{n_x}, \\theta_y \\in \\Theta_{n_y} \\theta_z \\in \\Theta_{n_z}} L(R_{\\theta_x,\\theta_y,\\theta_z}, R)$.\nHowever, due to the singularity of Euler angles [13], we can simplify the above rotation transformation. The pseudo-code implementation of our Symmetry-Aware Evaluation is provided in Algorithm 1. It allows us to simplify what was originally at most 3603 computations to a maximum of only 43 computations."}, {"title": "4.3 Large-Vocabulary 6D Pose and Size Estimation", "content": "We present results of algorithms [6, 10, 34, 46, 47] trained on Omni6Dtrain and tested on Omni6Dtest. We compare their quantitative results in Tab. 2 and their qualitative results in Fig. S10 in Appendix. Additionally, we compare the quantitative results of algorithms trained on Omni6D-xltrain and tested on Omni6D-xltest in Tab. 3. The performance disparity among algorithms for category-level 6D object pose estimation becomes markedly pronounced when applied to large-vocabulary datasets, in contrast to the more consistent performance previously observed on the Real and CAMERA datasets [40]. This highlights the inherent strengths and weaknesses across various model structures."}, {"title": "Generalization Performance", "content": "We evaluate algorithms on Omni6Dout to assess their inter-category generalization capabilities. The outcomes are presented in Tab. 4. Notably, DualPoseNet and HS-Pose emerged as superior performers, outclassing others across all metrics, thereby demonstrating excellent generalization abilities. Contrastingly, implicit methods including SPD and SPGA exhibited marked limitations. Qualitative results are shown in Fig. S11 in Appendix."}, {"title": "Fine-Tuning from Limited Categories", "content": "We propose a finetuning strategy that helps extend methods from a limited set of categories to large-vocabulary. We take SPD [34], DualPoseNet [20], and HS-Pose [47] as examples which belong to three different network architectures and show good performance on Omni6Dtest. We respectively take their best models on CAMERA as our pre-trained models.\nInitiating the fine-tuning process, we utilize three categories: bottle, bowl, and cup, which are concurrently present in both Omni6D and CAMERA datasets, aligning with the cls3 category. By facilitating the training on Omni6D-cls3, we enable a transfer of the model from CAMERA to Omni6D. Following the method illustrated in Fig. 6b, we engage in an iterative fine-tuning process on a progressively expanded category dataset until it reaches our desired number. In our experiments, we set this target number to be 48 categories.\nIn parallel, we conduct training from scratch separately on cls3, cls6, ..., and cls48 as a comparison, employing the same number of training iterations. As shown in Fig. 7, even with an exponential increase in the number of categories, pre-trained models remain pivotal in our fine-tuning strategy. The performance of fine-tuning consistently outperforms that of training from scratch.\nHowever, regardless of whether the training approach is finetuning or training from scratch, a decline in performance is observed as the number of categories increases. The decline rates for SPD and DualPoseNet are slower, coupled with an initial augmentation in performance due to increased training data and iterations. In contrast, HS-Pose experiences a more rapid decline, with fine-tuned 5\u00b02 cm results dropping from initial 62.52% to 14.42%. Models that excel in tasks involving a limited number of categories may not necessarily maintain their superiority in large-vocabulary tasks, they might be surpassed by models that are more robust and easier to train."}, {"title": "Visual Realism", "content": "Due to the complexity of collecting and annotating real-world data, contemporary datasets like NOCS [40] are composed of a large amount of synthetic data and a small portion of real-world data. While collecting real data is relatively straightforward when the number of categories is limited, gathering well-annotated real-world data for pose estimation tasks involving large vocabulary categories becomes a monumental task.\nOur Omni6D dataset, which includes large vocabulary objects, is also derived from rendering. However, the incorporation of real-scanned objects significantly enhances the realism of the rendered images. As depicted in Fig. 8, Omni6D receives a score of 2.69 \u00b10.39, surpassing the results obtained by CAMERA.\nGiven these significant advantages, our dataset excels not only in large-vocabulary scenarios but also in real-world scenes. As depicted in Tab. 5, We use DualPoseNet [20] to train on the common categories in REAL [40] and Omni6D, namely bottles, bowls, and mugs. We train separately on the two datasets and their mix. The results show that Omni6D models perform well on REAL275, and training on the mixed dataset outperforms using REAL or Omni6D datasets alone. This demonstrates that our dataset enables the direct transfer of models to real-world scenes. Moreover, it seamlessly supplements the existing real-world dataset, enabling joint training of models on our dataset and the real-world data.\nTo further validate the sim2real capability of models trained with Omni6D, we constructed a real-world dataset, Omni6D-Real, comprising 30 scenes, 39 categories, 73 instances, and 1k images. We captured RGBD images with Azure Kinect DK\u00b3 and preprocessed them using SAM [18] for object masks and ICP [2] for point cloud registration. Details are provided in Appendix Section D."}, {"title": "5 Conclusion", "content": "In conclusion, this paper introduces Omni6D, a novel 6D pose estimation dataset with large-vocabulary categories and intricate scenes. We evaluate existing category-level 6D object pose estimation methods on this benchmark, analyze its challenges, and propose a fine-tuning strategy for large-vocabulary scenarios."}, {"title": "Limitations", "content": "Our dataset, though more complex, doesn't fully encompass all real-world challenges. Additionally, our fine-tuning strategy effectively extends methods from a small set to a larger one, but its efficacy may decrease with growing category diversity."}, {"title": "Future Work", "content": "Our study paves the way for diverse research avenues. An immediate next step is expanding the Omni6D dataset with more object types and scenes for comprehensive coverage. Additionally, annotating videos for scanned objects will validate algorithms' large-vocab pose estimation in real-world scenarios. Designing new training strategies for coping with increasing category diversity presents an intriguing challenge."}]}