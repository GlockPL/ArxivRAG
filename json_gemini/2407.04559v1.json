{"title": "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition", "authors": ["Aditya K Surikuchi", "Raquel Fern\u00e1ndez", "Sandro Pezzelle"], "abstract": "Visual storytelling consists in generating a natural language story given a temporally ordered sequence of images. This task is not only challenging for models, but also very difficult to evaluate with automatic metrics since there is no consensus about what makes a story 'good'. In this paper, we introduce a novel method that measures story quality in terms of human likeness regarding three key aspects highlighted in previous work: visual grounding, coherence, and repetitiveness. We then use this method to evaluate the stories generated by several models, showing that the foundation model LLaVA obtains the best result, but only slightly so compared to TAPM, a 50-times smaller visual storytelling model. Upgrading the visual and language components of TAPM results in a model that yields competitive performance with a relatively low number of parameters. Finally, we carry out a human evaluation study, whose results suggest that a 'good' story may require more than a human-like level of visual grounding, coherence, and repetition.", "sections": [{"title": "Introduction", "content": "Visual storytelling is the task of generating a story for a sequence of several temporally-ordered images or video frames. For both human speakers and machine learning models, the task requires connecting the visual data causally, to generate a narrative consistent with the contents of the images. As for model-generated stories, evaluation is one of the key challenges due to the inherently creative nature of the task. Since human-written stories are typically used to train visual storytelling models-under the assumption that these stories provide a good learning signal-most previous work evaluated model-generated stories by directly comparing them to human ones using pattern-matching metrics. However, this approach is simplistic as it ignores several key aspects of visual stories, such as their degree of visual grounding, their overall coherence, or how repetitive they are. This problem has only been addressed recently, with Wang et al. (2022) and Surikuchi et al. (2023) proposing various metrics to take into account some of these crucial aspects. These methods assess the appropriateness of a generated story independently from its overlap with a ground-truth story for the same image sequence. Given that the same image sequence can possibly give rise to many different stories, this type of higher-level evaluation that does not rely on text overlap is clearly desirable.\nNevertheless, we argue that measuring the degree of coherence or visual grounding of a story may not be sufficiently informative, as there are no standard conventions that determine the preferable level of such properties. To address this issue, in this work, we first propose an evaluation method that assesses the quality of generated stories in terms of their distance from human-written stories along several relevant dimensions, each measured by an available reference-free metric. Using this method, we evaluate a range of models on the visual storytelling task, including models specifically designed and trained for this task, as well as-for the first time-foundation models pre-trained to achieve general-purpose language and vision abilities, which we test in a zero-shot manner. We show that LLaVA (Liu et al., 2024), a powerful foundation model, performs best on the task, but only slightly so than TAPM (Yu et al., 2021), a model designed for visual storytelling which is 50 times smaller than LLaVA. Second, given insights derived from our proposed distance-based evaluation method, we upgrade the visual and language components on TAPM, resulting in a model that achieves comparable performance to LLaVA with a significantly lower number of parameters.\nOur results show that the stories generated by LLaVA and the upgraded TAPM model are very close to human stories regarding their degree of visual grounding, coherence, and repetition. To further make sense of this finding, we collect human judgments with regards to comparing human and model stories. The results of this qualitative study indicate that humans tend to prefer human-written stories by a significant margin despite the quantitative closeness we observe.\nIn sum, we make the following contributions:\n\u2022 We propose a novel evaluation method for visual storytelling quantifying the distance between human-written and model-generated stories in terms of visual grounding, coherence, and non-repetitiveness.\n\u2022 We use our evaluation to assess the stories generated by various visual storytelling-specific and foundation models; we find that a foundation model, LLaVA, achieves the best result (lowest distance).\n\u2022 We leverage insights from this finding to upgrade a visual storytelling-specific model, TAPM, by replacing its visual and language components; we show that doing so results in better performance, on par with or out-performing the best-performing (and twice larger) LLaVA model.\n\u2022 Through human evaluation, we validate the scores of our distance-based method; at the same time, we report that human-written stories are still preferred, which suggests that the ingredients for a good story may not be limited to a human-like level of visual grounding, coherence, and non-repetitiveness."}, {"title": "Related Work", "content": "Computational work on visual storytelling was initiated by Huang et al. (2016), who operationalized the task as generating a textual story given an ordered sequence of images. The authors proposed the VIST dataset, which comprises sequences of five natural images collected from Flickr albums, with corresponding stories provided by human crowd-workers. VIST has been a catalyst for developing visual storytelling models (Kim et al., 2018; Wang et al., 2018; Yu et al., 2021). Over the last few years, other datasets have emerged that differ from VIST in some key features. On the one hand, to limit the complexity of modelling real-world knowledge implicit in natural images, Ravi et al. (2021) proposed AESOP, a dataset that includes sequences of three synthetic images (constructed by crowd-workers using clip-art entities from Abstract Scenes by Zitnick and Parikh (2013)) and corresponding three long-paragraph stories. More recently, to overcome the possible lack of character consistency resulting from sampling images from Flickr albums, Hong et al. (2023) proposed the VWP dataset, which comprises sequences of movie shots including 5-10 images with corresponding stories provided by crowd-workers.\nRegarding modeling, various computational approaches using RNNs and Transformers have been proposed for the task of generating plausible stories. Some of these models are trained end-to-end on the VIST dataset (Kim et al., 2018; Wang et al., 2018; Yu et al., 2021), while other approaches utilize external knowledge sources (Hsu et al., 2020, 2021; Chen et al., 2021). We describe some of these models in detail in Section 4.2. Regardless of the specific architectures, a challenge common to all computational approaches to this task is evaluation. In the following subsection, we review existing work on visual story evaluation in the general context of evaluating visually-grounded language."}, {"title": "Visually-Grounded Language Evaluation", "content": "Since visual storytelling is essentially a vision-to-language task similar to video/image captioning, evaluation of generated stories typically employed reference-based pattern-matching metrics such as METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015). However, these n-gram based metrics are shown to correlate poorly with human judgments (Novikova et al., 2017; Wang et al., 2018). Other reference-based evaluation metrics such as BERTScore (Zhang* et al., 2020) and BLEURT (Sellam et al., 2020) have also been used. These leverage pre-trained models to compute similarities between the generated candidate text and the corresponding references in a high-dimensional embedding space, which makes them more flexible to paraphrases and synonyms compared to standard n-gram based metrics. Nevertheless, reference-based metrics are by design only suitable for target-oriented generation tasks (e.g., machine translation), where it is considered essential for the generated text to match a curated set of references (Pillutla et al., 2021; Giulianelli et al., 2023). This is not the case in visual storytelling, where several stories could be plausible for a given image sequence. Recently, CLIPScore (Hessel et al., 2021) has been proposed to quantify the degree of alignment between an image and a given text without the need for any reference. It computes a similarity score between the CLIP (Radford et al., 2021) embeddings of the image and the text, and has been shown to correlate well with human judgments. As such, it is widely adopted for evaluation in the image captioning task. However, its application in the domain of visual storytelling is less straightforward, as a visual sequence comprises multiple images, and stories are made up of multiple related sentences, which makes evaluating what counts as a 'good' story very challenging.\nTo address some of these challenges, Wang et al. (2022) proposed a metric\u2014RoViST\u2014specifically for the visual storytelling task, which assesses three aspects of generated stories: visual grounding, coherence, and repetition. Subsequently, Surikuchi et al. (2023) proposed GROOVIST, a more advanced method to evaluate grounding in visual storytelling. However, while this constitutes important progress, in practice it is often not easy to make sense of these metrics, since they do not offer a reference point for assessing the quality of a generated story. For example, should stories be 100% grounded in the images? Should they exhibit zero levels of repetition? We argue that applying the metrics to both model- and human-generated stories is essential to boost their capacity to evaluate story-generation models. In the next section, we propose a novel method that addresses this problem by building on the existing metrics proposed by Wang et al. (2022) and Surikuchi et al. (2023)."}, {"title": "Problem Formulation", "content": "In this work, we take a human-centric approach and define the quality of generated stories in terms of their 'closeness' to stories produced by humans, regarding different dimensions that capture abstract properties of interest. Concretely, we compute dimension-specific scores for model- and for human-generated stories, measure human-model distance per dimension, and aggregate these distances to derive an overall distance score. We posit that the lower the overall distance, the more the generated story complies with high-level features observed in human stories.\nDeciding on which dimensions are most relevant to determine the quality of a story, let alone how to operationalize such dimensions, is not trivial. In this study, we consider the three aspects proposed by Wang et al. (2022): visual grounding, coherence, and repetition. Here we describe how they are operationalized as reference-free metrics, following which we formally define our proposed method."}, {"title": "Metrics", "content": "To measure the degree of visual grounding of a story, we use GROOVIST (Surikuchi et al., 2023). For a given <image-sequence, story> pair, GROOVIST first computes the alignment between the noun phrases (NPs) in the story and the bounding boxes in the images using their corresponding CLIP (Radford et al., 2021) embeddings. For each NP, only the maximum visual alignment score is retained. To penalize NPs with low visual alignment scores, the mean score of all the NPs in the dataset is used as a threshold. Specifically, this step is implemented by calculating the distance of each NP's score from the threshold. Resulting scores of the NPs are then weighted using word concreteness ratings to differentiate abstract words from concrete ones. The overall visual grounding score of a story is the sum of the concreteness weighted scores of all NPs normalized by the total number of NPs in the story. The range of the resulting scores is unbounded with higher values indicating higher degree of visual grounding.\nWe use a slightly modified version of RoViST-C (Wang et al., 2022) to evaluate the coherence of a given story, which corresponds to the average probability with which each sentence follows the preceding sentences. These probabilities are computed using ALBERT (Lan et al., 2020) fine-tuned for the sentence order prediction task using the VIST and ROCStories (Mostafazadeh et al., 2016) datasets. For each sentence (\\(s_i\\)) in a story, we obtain the probability that it follows the entire concatenated prefix of all previous sentences (\\({s_1, ..., s_{i-1}}\\))\u2014instead of just the previous sentence (\\(s_{i-1}\\)) as done in the original RoViST-C. The overall coherence score of a story is obtained by taking the average of these probabilities across all its sentences resulting in a value between 0 and 1 (indicates high coherence).\nWe measure the degree of repetition of a story using the RoViST-NR metric, where NR stands for 'non-redundancy' (Wang et al., 2022). For two segments of text, repetition is computed using the Jaccard Similarity (JS) (Singh and Singh, 2021) which is defined as the number of co-occurring words between the two texts normalized by the total number of words in both texts. Inter-sentence repetition is obtained as the average of JS scores computed between each sentence \\(s_i\\) and all its preceding sentences (\\({s_1, ..., s_{i-1}}\\)). For every sentence in the story, the intra-sentence repetition is obtained by computing the average of JS scores between non-overlapping 4-gram phrases of the sentence. The overall repetition score of a story is the average of all inter- and intra-sentence scores subtracted from 1. The resulting scores range between 0 and 1 and stories with scores closer to 1 indicate less repetition."}, {"title": "Distance between Humans and Models", "content": "For a given <image sequence, model story> pair, we compute the coherence \\(C_M\\), visual grounding \\(G_M\\), and repetition \\(R_M\\) scores using the three metrics described above. We do the same for the corresponding <image sequence, human story> pair, and denote the resulting scores as \\(C_H\\), \\(G_H\\), and \\(R_H\\). We then compute the absolute differences between the human stories and the model-generated ones to measure metric-level deviations:\n\\[d_{HM}^C = |C_H - C_M|,\\]\\[d_{HM}^G = |G_H - G_M|,\\]\\[d_{HM}^R = |R_H - R_M|.\\]\nFinally, we compute the overall aggregate distance between the model-generated story and the corresponding human-annotated story as the average of the metric-level deviations:\n\\[d_{HM} = (d_{HM}^C + d_{HM}^G + d_{HM}^R)/3\\]"}, {"title": "Evaluation of Existing Models", "content": "In this section, we evaluate and compare several state-of-the-art models using our proposed distance measure \\(d_{HM}\\). We test the models on the popular VIST dataset, that we describe below."}, {"title": "VIST dataset", "content": "VIST (Huang et al., 2016) is the first and most popular dataset for visual storytelling. The dataset includes images from Flickr albums selected by filtering titles with \u201cstoryable\u201d event types (e.g., graduation ceremony). For each of these selected albums, crowd workers constructed sequences of five images and provided corresponding five-sentence stories. The stories were then tokenized by replacing named entities including names of people, with entity types ([location], [organization]) and generic placeholder tokens ([male], [female]). On average, the dataset has 10.2 tokens per story and an overall vocabulary size of 18200 words. Excluding unavailable images, the dataset comprises 40071 training, 4988 validation, and 5050 test <image sequence, story> samples."}, {"title": "Models", "content": "We evaluate three end-to-end models that are specifically designed and trained for the visual storytelling task. In addition, we consider two general-purpose vision-language foundation models, which are used in a zero-shot manner.\nGLAC Net GLocal Attention Cascading Network (Kim et al., 2018) is a model proposed specifically for the visual storytelling task. Adapting the standard encoder-decoder architecture, it obtains the global visual context pertaining to every image sequence position using a bi-LSTM (Hochreiter and Schmidhuber, 1997) encoder. The obtained global context embeddings along with the individual image features (local) are, together, (GLocal) passed on to an LSTM decoder for story generation. Furthermore, to reduce redundancy in the generated sentences, GLAC Net samples multiple times from the decoder's probability distribution and selects the most frequent word from the pool.\nAREL Adversarial REward Learning (Wang et al., 2018) is another framework proposed for the visual storytelling task, which encompasses two modules: a policy model and a reward model. Similar to GLAC Net, the policy model, which uses GRUs (Cho et al., 2014) instead of LSTMs, takes an image sequence as input and generates a story. The reward model computes a score for every input <image, sentence> pair by extracting the sentence representations using 1D-convolutional kernels and concatenating them with the corresponding pre-trained ResNet-152 (He et al., 2016)) features of the images. Both modules are trained using an adversarial learning objective: the reward model is trained to discriminate between the ground truths and the generated stories; the policy model, to maximize the scores from the reward model.\nTAPM Transitional Adaptation of Pretrained Models (Yu et al., 2021) is a more recent approach to visual storytelling that leverages a pre-trained transformer-based language decoder. First, for every sequence position, the visual encoder pools together corresponding image features (pre-trained ResNet-101, Faster R-CNN (Ren et al., 2015)) along with features of the images at the previous and next positions to create enriched visual context representations. Then the visual contexts are passed as input to the GPT-2small (Radford et al., 2019) language decoder for story generation. To bridge the semantic gap between the pre-trained image and text representations, TAPM performs an adaptation step prior to the downstream training for the visual storytelling task. Specifically, for a pre-determined number of epochs, the visual encoder parameters are fine-tuned by conditioning them on the outputs of the frozen GPT-2small decoder.\nBLIP-2 Unlike the previously described models, Bootstrapping Language-Image Pre-training (Li et al., 2023) is a multimodal foundation model designed for general-purpose vision-language tasks such as visual question answering and image captioning. It connects a frozen pre-trained vision encoder and a frozen pre-trained large language model using a connector module called querying transformer (Q-Former). Q-Former contains an image and a text transformer with shared self-attention layers and learnable embeddings for querying the frozen image encoder. It is trained in two stages; in the first stage, it learns representations that align with the representations of the prompts (e.g., questions) of interest. In the second stage, its representations are fine-tuned based on the loss of the frozen language model generations (captions/answers). We adapt BLIP-2 for the visual storytelling task in a zero-shot manner by prompting the model to generate one sentence per each image in the sequence. We experiment with different settings in which the prompts contain different degrees of linguistic context.\nLLaVA Similar to BLIP-2, Large Language and Vision Assistant (Liu et al., 2024) is another general-purpose multimodal foundation model that connects large pre-trained vision encoders and large pre-trained language models. However, unlike BLIP-2, LLaVA focuses on training data and procedure as opposed to the model architecture. It is the first model that extends instruction-tuning to the language-image multimodal space. LLaVA achieves this by collecting and training on vision-language instruction-following data, constructed for <image, caption> pairs of existing datasets (e.g., COCO), by querying GPT-4 (OpenAI, 2023) using various in-context-learning prompts. To connect the visual features with the language embeddings, LLaVA uses a linear layer (single projection matrix) instead of Q-Former. Similar to BLIP-2, we use LLaVA to generate stories in a zero-shot manner under different linguistic context settings."}, {"title": "Experimental Setup", "content": "We generate stories for the VIST test set for all models, using greedy sampling. For GLAC Net and AREL, we leverage the publicly available model checkpoints. To obtain the model checkpoint for TAPM, we follow the proposed procedure and train the model from scratch using the VIST dataset. The two general-purpose foundation models are used zero-shot, without training on the VIST dataset or the visual storytelling task. For BLIP-2, we use the version with ViT-g encoder and OPT-2.7B decoder and for LLaVA, version 1.6 with CLIP-ViT-L-336px and Mistral-7B. We prompt these models under different settings that vary with respect to the amount of linguistic and visual context given in the prompt (e.g., one image/sentence at a time vs. all images at once). We use three prompt variations per setting and report the average of the resulting \\(d_{HM}\\) values."}, {"title": "Results", "content": "Figure 1 shows the distances between human-written stories and the stories generated by the models (the lower the better). Examples of model-generated stories are provided in Figure 2. In Figure 1, we observe that the stories generated by LLaVA obtain the best overall value (\\(d_{HM}\\) = 0.099), followed by TAPM (\\(d_{HM}\\) = 0.15). We notice that GLAC Net-generated stories exhibit the lowest distance regarding the repetition dimension. We attribute this to GLAC Net's inference phase decoding heuristic, which penalises repetitive expressions (see Section 4.2). BLIP-2 stories are overall the farthest from stories written by humans.\nRegarding the two best performing models, LLaVA and TAPM, two points are worth highlighting. First, despite a huge difference in model size-LLaVA is a powerful 7.5B parameter foundation model, while TAPM is 50 times smaller-TAPM's \\(d_{HM}\\) value is only slightly higher than LLaVA's. Second, LLaVA outperforms TAPM with respect to coherence and visual grounding. We hypothesize that this advantage is due to LLaVA's more powerful language and vision backbone models. In the next section, we leverage the modular architecture of TAPM to test this hypothesis."}, {"title": "Model Analysis and Improvements", "content": "In Section 4, we showed that the \\(d_{HM}\\) obtained by the visual storytelling-specific TAPM model is only slightly higher than that of LLaVA, a 50-times larger foundation model. In particular, we notice that LLaVA has an advantage over TAPM in two dimensions, i.e., visual grounding and coherence. We hypothesize that this advantage is due to the model's better language and vision backbone models-LLaVA builds on a pre-trained, transformer-based language model and image processor. Thus, we leverage the modular architecture of TAPM and test whether we can obtain better results (lower distances) by replacing its original language and vision components with models similar to those embedded in LLaVA, while keeping the number of parameters significantly lower. To test whether the results we obtain are consistent across datasets, we perform this analysis on both VIST and VWP (Hong et al., 2023)."}, {"title": "Updating the Language Component", "content": "By default, TAPM uses GPT-2small as its story decoder. Here, we replace this language model with LLAMA 2 (Touvron et al., 2023), an auto-regressive (decoder-only) large language model pre-trained via maximum likelihood objective for next-token prediction on massive amounts of publicly available online data. Thanks to the large context window it can take as input and the Grouped Query Attention mechanism to speed up processing during decoding (Ainslie et al., 2023), this LM is currently the state-of-art on various downstream tasks in the MMLU benchmark (Hendrycks et al., 2021). Here, we use the 4-bit quantized (Dettmers et al., 2023) pre-trained version of the LLAMA 2 7B model and adapt the parameter dimensions of TAPM's visual encoder component to match the updated language decoder. To ensure computational and memory efficiency, we employ the LoRA (Low-rank Adapter; Hu et al., 2022) fine-tuning approach (that allows for updating only a subset of the model's parameters) and only target the multi-head self-attention blocks-WQ, WK,WV, WO (Vaswani et al., 2017) of the language model during training. Henceforth, we refer to this upgraded TAPM model as (+LLAMA 2)."}, {"title": "Updating the Vision Component", "content": "The original TAPM model uses pre-trained ResNet-101 and Faster R-CNN for extracting the image-level and object-level features, respectively. We supplement the image-level ResNet features by concatenating them with representations extracted using pre-trained Vision Transformer model (ViTbase; Dosovitskiy et al., 2021). ViTbase leverages the transformer architecture for image processing and is pre-trained on the ImageNet-21K (Ridnik et al., 2021) data. Features extracted using ViTbase have been shown to improve the performance of models on several computer vision tasks. Henceforth, we refer to this upgraded TAPM model as (+ViT)."}, {"title": "Experimental Setup", "content": "We train the (+ViT) and (+LLAMA 2) models from scratch using the VIST data training and validation splits for 15 epochs, and obtain the results on the test split. We conduct the same experiment for the VWP dataset independently, by following the procedure proposed by the authors of the dataset. As mentioned in Section 2.1, VWP includes sequences of 5-10 images constructed using frames obtained from MovieNet (Huang et al., 2020). Following the same procedure used by the authors, we split the dataset into 9606 training, 849 validation, and 586 test <image sequence, story> samples and preprocess the text to replace recognized named entities with entity types and placeholders. To compare the performance of the improved TAPM models against LLaVA, we use LLaVA to generate stories for the VWP test set by prompting it under the visual context setting."}, {"title": "Results", "content": "Figure 3 shows the \\(d_{HM}\\) values for the LLaVA model and for all the different versions of the TAPM model on both the VIST and the VWP datasets. Firstly, we observe that compared to the original TAPM model, (+LLAMA 2) generates stories that are closest to human stories in terms of both coherence and non-redundancy. Stories by the (+ViT) version achieve the lowest distance along the dimension of visual grounding. These results are in line with our intuitions regarding the influence that both modalities have on different aspects of visual story evaluation. For this analysis, we also considered a version of the TAPM model in which both the language and vision components are jointly updated (+LLAMA 2, +ViT). However, (+LLAMA 2, +ViT) consistently under-performed on our distance measure \\(d_{HM}\\) compared to the (+LLAMA 2) and (+ViT) versions, and was only marginally better than the original TAPM model. Despite the significant difference in the number of parameters, we notice that (+LLAMA 2) achieves performance on par with LLaVA (see Figure 4).\nWe observe similar results for the TAPM models on the VWP dataset\u2014(+ViT) obtains the lowest distance in terms of visual grounding and (+LLAMA 2) obtains the lowest distance in terms of both coherence and non-redundancy. Furthermore, on the VWP dataset, the (+LLAMA 2) model achieves the overall lowest \\(d_{HM}\\), performing better than the LLaVA model. These results quantitatively indicate that the stories generated by the models are close to the corresponding human-written stories. To better understand if the metrics are capable of effectively comparing stories generated by models with human-written ones, we conduct a qualitative evaluation, that we describe in the next section."}, {"title": "Qualitative Analysis and Discussion", "content": "Our results suggest that the stories generated by the best-performing models according to \\(d_{HM}\\) are very close to human levels of visual grounding, coherence, and degree of repetition. To test whether this aligns with the perceived overall quality of the stories, we conduct a qualitative human evaluation. tion. We consider the VIST stories generated by the two models\u2014TAPM (+LLAMA 2) and LLaVA\u2014that achieve the best performance in terms of \\(d_{HM}\\) (lowest distances). For each model, we select 100 generated stories, that we randomly sample using the distribution of \\(d_{HM}\\) values on the VIST test set. We then provide annotators with <image sequence, model-story> pairs along with corresponding human-written stories, and ask them to assess whether one is better than the other, or whether both are similarly fine or bad. Five annotators unrelated to the project participated in the task."}, {"title": "Conclusion", "content": "We proposed a novel human-centric method (\\(d_{HM}\\)) for evaluating the quality of model-generated stories in terms of their closeness to human-written stories along coherence, visual grounding, and non-redundancy. Using our proposed method, we compared various models and found that the large foundation model LLaVA obtains slightly better results than the (50 times smaller) best visual storytelling model TAPM. We showed that upgrading the components of TAPM boosts its performance (lowers its \\(d_{HM}\\)) across multiple datasets, confirming the advantage of leveraging last-generation language and vision pre-trained models. Finally, we conducted a qualitative human evaluation to explore whether these quantitative findings align with the overall perceived story quality. We observed that human judgments align with our quantitative evaluation. Yet, humans still prefer human-written stories over model-generated ones, which suggests that capturing coherence, visual grounding, and non-repetitiveness may not (yet) be the whole story."}, {"title": "Limitations", "content": "The human evaluation analysis we performed is arguably small-scale. As such, we cannot rule out that carrying it out with more annotators and a larger set of stimuli may lead to different patterns of results. Second, the number of models we experimented with is quite limited, which is an obvious limitation of this work. Yet, we defend the selection we made, aimed at a trade-off between the limited availability of computational resources and the inclusion of various architecture families. Finally, we experimented with only two visual storytelling datasets, both in English and both Western-centric. This is a limitation of our work, which is, however, due to the unavailability of other datasets with more diverse language and cultural backgrounds. We strongly support the creation of such resources."}]}