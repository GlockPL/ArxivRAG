{"title": "Rethinking Data Augmentation\nfor Robust LiDAR Semantic Segmentation\nin Adverse Weather", "authors": ["Junsung Park", "Kyungmin Kim", "Hyunjung Shim"], "abstract": "Existing LiDAR semantic segmentation methods often strug-\ngle with performance declines in adverse weather conditions. Previous re-\nsearch has addressed this issue by simulating adverse weather or employ-\ning universal data augmentation during training. However, these meth-\nods lack a detailed analysis and understanding of how adverse weather\nnegatively affects LiDAR semantic segmentation performance. Motivated\nby this issue, we identified key factors of adverse weather and conducted a\ntoy experiment to pinpoint the main causes of performance degradation:\n(1) Geometric perturbation due to refraction caused by fog or droplets in\nthe air and (2) Point drop due to energy absorption and occlusions. Based\non these findings, we propose new strategic data augmentation tech-\nniques. First, we introduced a Selective Jittering (SJ) that jitters points\nin the random range of depth (or angle) to mimic geometric perturba-\ntion. Additionally, we developed a Learnable Point Drop (LPD) to learn\nvulnerable erase patterns with Deep Q-Learning Network to approxi-\nmate the point drop phenomenon from adverse weather conditions. With-\nout precise weather simulation, these techniques strengthen the LiDAR\nsemantic segmentation model by exposing it to vulnerable conditions\nidentified by our data-centric analysis. Experimental results confirmed\nthe suitability of the proposed data augmentation methods for enhanc-\ning robustness against adverse weather conditions. Our method attains\na remarkable 39.5 mIoU on the SemanticKITTI-to-SemanticSTF bench-\nmark, surpassing the previous state-of-the-art by over 5.4%p, tripling the\nimprovement over the baseline compared to previous methods achieved.", "sections": [{"title": "1 Introduction", "content": "LiDAR semantic segmentation is a fundamental task of 3D scene understanding.\nRobust and reliable segmentation is crucial in safety-critical applications like\nautonomous driving. However, existing LiDAR segmentation models [1,5, 15, 22,\n29,38] commonly lack robustness, reporting significant performance degradation\nunder adverse weathers such as snow, fog, rain or wet surfaces.\nTo address this issue, recent studies [12, 33, 34] have introduced corrup-\ntion benchmarks including adverse weather and proposed effective techniques\nfor robust LiDAR segmentation against corruption. These robust methods are\nprimarily divided into two kinds: a task-agnostic approach and a simulation-\nbased approach. The task-agnostic approach employs general machine-learning\napproaches for robustness without explicitly addressing the characteristics of Li-\nDAR corruption caused by adverse weather. The simulation-based approach [3,\n7, 8, 10, 24, 27, 35] artificially synthesizes weather-specific data through physical\nequations for training. However, these efforts focus on detection tasks, and each\nmethod considers only a single type of weather at a time.\nWhile the simulation-based approach can better exploit the intrinsic proper-ties of LiDAR scans under target adverse weather, simulating every weather type\nin all possible severities is often impractical and inaccurate. Instead, we adopt\na data-centric perspective to analyze corrupted LiDAR data. For example, the\ndistortion caused by \u201crain\u201d, \u201csnow\u201d and \u201cfog\u201d often create similar patterns, as\nsimulated in [10]. Also, all those adverse weather shows point drop through at-\ntenuation or occlusion by droplet [6,7,10]. While the simulation-based approach\nrequires explicit modeling of the LiDAR for each weather condition, our data-\ncentric approach potentially addresses complex distortions with a few unified\npatterns. Through existing research and our comprehensive analysis, we find\nthat various adverse weather conditions create similar distortion patterns in Li-\nDAR data. From this insight, we categorize these distortions into two types: (1)\ngeometric perturbation and (2) point drop. By conducting a toy experiment, we\ndemonstrate that these two distortion types are highly correlated with segmen-\ntation performance degradation.\nRooted by the toy experiment, we introduce two novel and strategic data aug-\nmentation methods tailored to the LiDAR distortion caused by adverse weather.\nBy incorporating these augmentations during training, we aim to enhance the\nmodel's robustness for each distortion type. For geometric perturbation, we in-\ntroduce Selective Jittering (SJ), which applies Additive Gaussian Noise (AGN)\nto alter the XYZ-coordinate and intensity within a selective local region. For\nhandling a point drop pattern, we developed a Learnable Point Drop (LPD)\nthat employs Deep Q-Network (DQN) to strategically remove points. Our aug-\nmentation strategies are informed by existing studies [10,17,28], which shows\ngeometric perturbation typically involves small, random alterations to the orig-inal points. This insight led to our choice of jittering as a suitable technique for\naddressing the geometric perturbation. Also, augmentation strategy for point\ndrop or occlusion is motivated from existing studies [7,8,10] which state that\nintensity and depth of LiDAR beams play a significant role. Therefore, our idea\ndeveloped that using a DQN to detect point drops that impair LiDAR semantic\nsegmentation models might enable the DQN to effectively follow and adapt to\nsuch point drop patterns."}, {"title": "2 Related Works", "content": "2.1 LiDAR Semantic Segmentation\nExisting 3D LiDAR point cloud semantic segmentation methods can be catego-rized into three types based on the data representation: Point-based, Projection-based, and Voxel-based.\nPoint-based methods [23,29,37] utilize the 3D points directly as input. KP-Conv [29] initially clustering local points, aggregating these local features, and\nthen feeding them into kernel point convolutions. The Point Transformer [37]\nutilizes a transformer architecture to compute query points in each local region,\nobtained through k-nearest neighbors (kNN). The Point-Mixer [4] tried to adapt\nthe MLP-Mixer [30] for point cloud applications. They achieve high performance\nbut suffer from high computational costs due to the utilization of large-scale raw\nLiDAR data.\nProjection-based methods [1, 11, 19] project LiDAR points into a 2D image\nand performs the semantic segmentation using architectures successful in a 2D\nimage. RangeViT [1] directly adopts a ViT model pre-trained on 2D images,\ndemonstrating that pre-trained power in 2D images can be effective as prior\nknowledge in range images. RangeFormer [11] proposes \u201cRangeAug\" to maxi-\nmize the utility of range images created by projecting into 2D, producing multi-ple range image data to overcome the low performance in range image models.\nProjection-based methods achieve fast inference speed but present a sub-optimal\nperformance due to missing information after the projection.\nVoxel-based methods [5,15,38] perform efficient computations by dividing 3D\nspace into a voxel grid and aggregating points within the same voxel. MinkUnet [5]\nvoxelizes LiDAR points with a cubic grid and applies sparse convolution. Cylin-der3D [38] proposes cylindrical partitions, reflecting LiDAR's characteristic that\nthe density of points depends on the distance. SphereFormer [15] utilizes a radial\nwindow and transformer architecture to aggregate long-range information and\nimprove performance. Voxel-based methods achieve a balance between reason-able inference time and commendable segmentation performance.\n2.2 LiDAR Data Augmentation\nInspired by 2D image augmentation, conventional LiDAR segmentation meth-ods apply classic scaling, rotation, flipping, and translation to augment LiDAR\ndata. Recently, several out-of-context augmentation techniques [13,21,32] that\nmix different LiDAR scans have been proposed. Mix3D [21] combines randomly\nselected two scans. Considering the sweeping mechanism of the LiDAR sensor,\nPolarMix [32] cuts LiDAR scans along the azimuth axis, then exchanges point\ncloud sectors and applies instance-level rotate-pasting. To reflect the spatial prior\nof the LiDAR point cloud, LaserMix [13] partitions LiDAR scans based on the\nlaser beams and blends partitions from different LiDAR scans. Recent study [25]\nintroduces a fast LiDAR domain augmentation module to address sensor-bias\nproblems. To the best of our knowledge, our approach is the first augmentation\nmethod specifically designed to address data corruption under adverse weather\nconditions.\n2.3 LiDAR Under Adverse Weather Conditions\nRobustness under harsh conditions is crucial in safety-critical applications. Ad-verse weather substantially degrade performance in real-world outdoor autonomous\ndriving. Thus, there are several attempts to develop weather-robust models in\nfields such as 2D segmentation [16, 18], 3D detection [7,9,12], and 3D segmen-tation [12,33]. Simulation-based approaches [7,9] artificially synthesize data for\nsingle weather conditions through physical modeling and utilize it for training.\nWe differ from these in that we do not model specific weather conditions ex-plicitly. Recently proposed task-agnostic approaches [12, 33] consider multiple\nweather conditions at once. However, they use general machine-learning tech-niques (such as teacher-student framework and feature prototype) to achieve\nrobustness rather than specifically tackle LiDAR corruption caused by adverse\nweather. We differ from these in that we propose augmentation methods specifi-cally tailored for adverse weather conditions based on the analysis of performance\ndegradation in LiDAR data."}, {"title": "3 Finding Distortions to Augment", "content": "In this section, we aim to discuss the patterns of distortion that different adverse\nweather conditions impose on LiDAR data. Although adverse weather conditions\nare distinct in reality, studies have shown that their effects on LiDAR data often\nresult in similar impacts. For instance, the distortions caused by \"rain\", \"snow\",\nand \"fog\" tend to produce similar point-missing patterns due to attenuation in\nthe data, as demonstrated in [10]. Therefore, this section will focus on identify-ing the common distortion patterns caused by adverse weather through existing\nstudies. Overall, existing studies describe the effect of adverse weather as four\ndifferent types of distortion: (1) Point Drop due to energy absorption, (2) \u041e\u0441-clusions caused by droplets of rain or snow and fog, (3) Geometric perturbation,\nand (4) Intensity distortion due to energy absorption.\n3.1 Distortion Factors from Adverse Weather\n(D1) Point Drop. Several studies have explored how adverse weather condi-tions contribute to point drops in LiDAR data. Kilic et al. [10], Fersch et al. [6]\nand Shin et al. [27] describe point drops resulting from beam attenuation and\nbeam missing due to droplets, fog, and frozen or wet ground. These studies col-lectively suggest that adverse weather conditions typically lead to point drops\nin LiDAR data.\n(D2) Occlusions. Several studies have addressed occlusions caused by adverse\nweather conditions. Hahner et al. [7], Kilic et al. [10], Kong et al. [12] and Yan\net al. [34] consider scenarios where beams colliding with snow collect signals\nat much shorter distances than the intended objects of collision. Upon review-\ning these studies, we have concluded that adverse weather consistently leads to\nocclusions.\n(D3) Geometric Perturbation. Some studies focus on geometric perturbation\ncaused by adverse weather conditions. Kilic et al. [10], Li et al. [17] and Smith\net al. [28] demonstrated geometric perturbation in adverse weather, such as fog,\nsnow, and rain, by incorporating random noise into the coordinates. Through\nthese studies, we have come to conclude that adverse weather universally causes\ngeometric perturbation.\n(D4) Intensity Distortion. Numerous studies have focused on intensity dis-tortion caused by adverse weather conditions. Bijelic et al. [3], Shin et al. [27],\nFersch et al. [6], Kong et al. [12], and Yan et al. [34] have collectively shown that\nadverse weather conditions like fog, wetness, and rain lead to a reduction in Li-\nDAR beam intensity, influencing the generation of synthetic data. Through these\nstudies, we conclude that adverse weather commonly causes intensity distortion.\n3.2 Toy Experiment\nBased on previous research findings mentioned in Section 3, the types of dis-tortions in LiDAR point clouds caused by adverse weather conditions converge\ninto a set of common distortions. Therefore, from a data-centric perspective,\nthe issues that we need to consider can be summarized as follows: (D1) Point\ndrop, (D2) Occlusions, (D3) Geometric perturbation, and (D4) Inten-sity distortion. Here, our aim is to identify which distortion types negatively\nimpact performance. To achieve this, we have generated four distortion types of\ntoy synthetic data through the SemanticKITTI validation set."}, {"title": "4 Methods", "content": "In this section, we propose solutions for the two challenges identified in Section\n3.2. Our objective is to enhance the robustness of the model by addressing the\ncommon distortions exhibited by various adverse weather conditions, through\ndata augmentation. This approach aims to achieve robustness without the ne-\ncessity of simulating each adverse weather scenario explicitly.\nAs observed earlier, distortions resulting from adverse weather include point\ndrop, occlusion, geometric perturbation, and intensity distortion. Of these, the\nmain distortions identified as having the most detrimental impact on perfor-mance were (1) geometric perturbation caused by minor variations in point co-ordinates and (2) point drop caused by beam missing or occlusion. To address\nthese challenges, we propose two techniques: Selective Jittering and Learnable\nPoint Drop.\n4.1 Selective Jittering\nSelective Jittering is devised to address the first main distortion, geometric per-turbation. Selective Jittering consists of two types: Depth-Selective Jittering\n(DSJ) and Angle-Selective Jittering (ASJ).\nDepth-Selective Jittering (DSJ). DSJ employs a single scan to add Gaussian\nnoise to the XYZ-coordinates and intensity value of points below a randomly\nselected depth range.\nAngle-Selective Jittering (ASJ). It applies Gaussian noise to a randomly se-lected range of angles. By performing jittering across different ranges of depths or\nangles for each point, this method reflects the augmentation that some beams are\naffected by geometric perturbation in a non-uniform manner in adverse weather.\nThis concept realistically represents the characteristics of transparent droplets\nlike rain and snow [6].\nA significant aspect of DSJ and ASJ is that they use only a single frame. DSJ\nand ASJ are efficient in data augmentation without additional LiDAR frames\nand can perform reasonable augmentation even when the given LiDAR data is\nnot sequentially captured.\nRange Jittering (RJ). Range Jittering has been proposed to simulate range\ndistortion caused by droplets and fog, as mentioned in [10]. Unlike DSJ and\nASJ, Range Jittering applies jittering only in the range direction of points. This\nmethod is used in place of using the original points in DSJ and ASJ.\n4.2 Learnable Point Drop\nLearnable Point Drop (LPD) was devised to address the second main distortion,\npoint drop. LPD is designed to artificially create point drop due to occlusion,\nsuch as dense fog. LPD employs a Deep Q-Learning Network (DQN) [20] for\nidentifying the drop ratio and drop region that lead to adverse effects on the\nmodel. The reward of the DQN is designed to identify point drops that increase\nthe training loss and uncertainty of the LiDAR semantic segmentation model.\nThrough LPD, the LiDAR semantic segmentation model is exposed to point\ndrop scenarios caused by adverse weather conditions. Consequently, it learns\nto make accurate predictions even in the absence of points that are critical for\nsegmentation in clean data environments. Since LPD is employed merely as a\nconcept for data augmentation and exists as a separate module, it necessitates\nno alterations to the existing model's training scheme except for limiting the\ngradient norm to ensure the stability of DQN learning.\nLPD module defines its current state by summing the loss $L_{aug}$, calculated\nfrom augmented data obtained through SJ, and the entropy $H_{aug}$, derived from"}, {"title": null, "content": "logits. The loss $L_{aug}$ is calculated using the original loss function employed by\nthe model in use. The calculation of entropy is as follows:\n$H_{aug}(x) = - \\sum_{i=1}^{n} P(x_i) \\log P(x_i)$\nIn this process, $x_i$ represents each point, and $P(\\cdot)$ is softmax to the logits. $N$\ndenotes the number of points. LPD predicts the indices of points to drop, taking\nas input the sum of $L_{aug}$ and $H_{aug}$ added to the input point tensor. This process\nenables simultaneous learning of the drop ratio and drop region. Subsequently,\nthe difference between the sum of the loss $L_{LPD}$ and entropy $H_{LPD}$, obtained\nfrom the dropped points by LPD, and the sum of $L_{aug}$ and $H_{aug}$, is defined as\nthe reward.\nIn Comparison with random drop, random drop uniformly reduces points\nacross all depths. Consequently, this approach is not suitable for simulating point\ndrops caused by fog, as it lacks the depth-specific characteristics of fog-related\ndistortions. Furthermore, randomly generated point drop scans may not achieve\nthe severity of adverse weather conditions necessary to drop critical points that\ncould deceive the model. Detailed comparison will be conducted in the ablation\nstudy section.\n4.3 Overall Pipeline\nThe overall training procedure begins with data augmentation through SJ. The\naugmented points are input into the LiDAR semantic segmentation model to\ncompute loss. The model then calculates logits for each point, applies softmax,\nand subsequently computes the entropy. The current state of the LPD module\nis set as the sum of the loss and entropy. The dropped points are then input\ninto the LiDAR semantic segmentation model to calculate the corresponding\nloss and entropy. The difference between the sum of loss/entropy obtained from\nthe augmented points and that obtained from the points determined by LPD is"}, {"title": "5 Experiments", "content": "5.1 Experimental setup\nDataset and Evaluation Metrics. To assess robustness, we trained our model\non 19 classes from SemanticKITTI and then validated its performance on val-idation set of SemanticSTF as same as [33]. The experiment process involves\ntraining on clean data followed by evaluating robustness in adverse conditions,\nakin to existing research [12, 33, 34]. SemanticSTF is a dataset gathered under\nreal-world adverse conditions such as rain, fog, and snow. Following the ap-proach in [33], invalid data in SemanticSTF are mapped to an \"ignore label\u201d. We\nutilized MinkowskiNet [5] for validation purposes due to its well-known robust-ness [12,34]. Our evaluation metrics include the Intersection over Union (IoU)\nfor each class and the mean IoU (mIoU) across all classes. Experiments on syn-thetic data like SynLiDAR are detailed in the supplementary material due to\nits low significance because our experiment targets high performance in adverse\nweather using real data.\nImplementation Details. MinkowskiNet-18/32width served as our baseline\nmodel. The learning rate was set at 0.24, with a weight decay of 0.0001. The\nmean and standard deviation of Gaussian noise used in SJ are 0 and 0.01 each.\nIn cases of Learnable Point Drop, the norm of the gradient was limited to 100 for\nboth the DQN and Segmentation model. All experiments were conducted using\nfour A6000 GPUs for 15 epochs in batch size 2. The duration of all experiments\nranged between 3 to 5 hours.\n5.2 Main results\nSemanticKITTI to SemanticSTF. According to Tab. 2, our proposed method\nshowed a significant improvement in SemanticSTF, with a +8.1 mIoU increase\nover the baseline and +5.6 mIoU over the recent competitor, PointDR. Our\nmethod demonstrated universally superior performance across all weather con-ditions, validating the validity of our choice in data distortion type and the effec-tiveness of our method. Our method substantially improved the performance of\ncategories like other vehicle, motorcyclist, sidewalk, pole, and traffic sign, which\nhad lower baseline performance, increasing their class IoU by about +5 to +10.\nAdditionally, it significantly enhanced the mIoU for cars and persons by +19.0\nand +9.6, respectively, compared to the baseline, which is crucial for safety in\ndriving environments. More details are available in the supplementary material."}, {"title": null, "content": "Also in Tab. 2, examining the performance improvement per weather condi-tion, we note that the most significant increase in mIoU was in rain, with +7.9\nmIoU. This result is indicative of our SJ augmentation effectively reflecting ge-ometric perturbations caused by raindrops. This aspect will be further explored\nin the ablation study. Additionally, in snow, our method showed a remarkable\nincrease of +7.8 mIoU, and in dense fog (+5.3 mIoU) and light fog (+7.4 mIoU),\nour methodology clearly outperformed the baseline. This indicates that our data\naugmentation effectively models the adverse impacts of weather on LiDAR se-mantic segmentation models. Detailed analysis of how our method enhances\nrobustness against each weather condition, and the factors contributing to the\nimproved performance, will be discussed in the ablation study.\nMoreover, as previously mentioned, the training architectures designed to\nenhance robustness, such as those proposed in PointDR [12,33,34], can be or-thogonally applied to our proposed data augmentation. Therefore, regardless of\nthe choice of training architecture, the integration of our method is expected to\nyield superior performance compared to conventional approaches.\nSynLiDAR to SemanticSTF. In our study, we validated the results peri-odically using the validation sets provided within the SemanticKITTI dataset\nduring training. For SynLiDAR, where no specific validation set was available,\nsequence 0 was treated as the validation set. The best-performing models during\nthese validation checkpoints were selected for reporting.\nIn contrast, PointDR demonstrated an unrealistic scenario in its baseline\ncode, using SemanticSTF for validation during training and reporting only the\nhighest-performing results. For a fair comparison using PointDR's criteria, our\nmethod shows a performance increase of +0.04 mIoU over PointDR. Using our"}, {"title": null, "content": "more realistic protocol, our method demonstrates an even greater performance\nimprovement of +0.32 mIoU against PointDR. Detailed results are shown in\nTab. 3.\n5.3 Ablation Study\nAnalysis of Proposed Component Methods. As shown in As Tab. 4, when\nASJ was added to the baseline, we observed an improvement in robustness across\nall adverse weather conditions (increases of +2.6 mIoU in dense fog, +5.3 mIoU\nin light fog, +8.1 mIoU in rain, and +6.3 mIoU in snow), albeit with a -1.8 mIoU\ndecrease in clean data performance. The introduction of DSJ further enhanced\nperformance in dense fog (+4.1 mIoU), light fog (+6.3 mIoU), rain (+9.3 mIoU)\nsnow(+4.6 mIoU) and an overall performance increase of +6.2 mIoU. Our Se-lective Jittering method's strong performance in rain condition showcases its\nefficacy in rainy weather, where it considers geometric perturbations for selected\nbeams instead of all.\nThe application of Range Jittering showed improvements in dense fog(+3.7\nmIoU), light fog (+6.9mIoU), rain(+6.0 mIoU), snow (+8.2 mIoU), and an over-all performance increase of +7.3 mIoU, indicating the importance of simulating\nrange distortion due to adverse weather as discussed in [10]. As seen earlier, in\nrainy weather, jittering only some beams aids in enhancing robustness. How-ever, performing Range Jittering on the remaining original points leads to all\npoints being jittered, reducing performance in rain condition. Additionally, the\nimprovement in snow conditions by using Range Jittering suggests that perturb-ing as many beams as possible is more effective for robustness enhancement in\nsnowy weather.\nThe addition of LPD resulted in improvements in dense fog (+5.3 mIoU),\nlight fog (+7.4 mIoU), rain (+7.9 mIoU), snow (+7.8 mIoU), and an overall\nincrease of +8.1 mIoU against to the baseline. The substantial performance\nenhancements observed in conditions of dense and light fog indicate that the LPD\neffectively represents occlusion from foggy weather as intended. The performance\nreduction in snow conditions compared to the use of ASJ, DSJ, and Range\nJittering (-0.4 mIoU) is likely due to LPD not exclusively targeting the ground\nfor point dropout. LPD applies the point drop across all points, as shown in\nFig. 6. This represents a limitation of LPD, necessitating the development of\nalternative methods to address this issue.\nOverall, the use of all components led to a significant improvement in per-formance under adverse weather conditions, with an increase of +8.1 mIoU over\nthe baseline at a reasonable cost of -1.1 mIoU in clean data. Ultimately, our\nmethod achieved state-of-the-art results on SemanticSTF, with an increase in\nmIoU across all adverse weather conditions compared to the baseline. The com-bined use of DSJ, ASJ, Range Jittering, and LPD demonstrated high perfor-mance, indicating that each component synergistically contributes to enhancing\nthe model's robustness."}, {"title": "6 Conclusion", "content": "Our study focused on analyzing LiDAR data distortions in adverse weather\nand enhancing LiDAR model robustness with a data-centric method. We identi-fied that point drop and geometric distortion mainly affect model performance\nthrough toy experiments. We introduced novel data augmentation techniques,\nSelective Jittering, and Learnable Point Drop, leading to state-of-the-art per-formance on SemanticSTF. We showed that our methodology significantly con-tributes to robustness enhancement without complex data simulations. We ex-pect that our research will contribute to improving the stability and reliability\nof LiDAR semantic segmentation."}]}