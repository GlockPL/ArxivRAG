{"title": "Rethinking Data Augmentation\nfor Robust LiDAR Semantic Segmentation\nin Adverse Weather", "authors": ["Junsung Park", "Kyungmin Kim", "Hyunjung Shim"], "abstract": "Existing LiDAR semantic segmentation methods often struggle with performance declines in adverse weather conditions. Previous research has addressed this issue by simulating adverse weather or employing universal data augmentation during training. However, these methods lack a detailed analysis and understanding of how adverse weather negatively affects LiDAR semantic segmentation performance. Motivated by this issue, we identified key factors of adverse weather and conducted a toy experiment to pinpoint the main causes of performance degradation: (1) Geometric perturbation due to refraction caused by fog or droplets in the air and (2) Point drop due to energy absorption and occlusions. Based on these findings, we propose new strategic data augmentation techniques. First, we introduced a Selective Jittering (SJ) that jitters points in the random range of depth (or angle) to mimic geometric perturbation. Additionally, we developed a Learnable Point Drop (LPD) to learn vulnerable erase patterns with Deep Q-Learning Network to approximate the point drop phenomenon from adverse weather conditions. Without precise weather simulation, these techniques strengthen the LiDAR semantic segmentation model by exposing it to vulnerable conditions identified by our data-centric analysis. Experimental results confirmed the suitability of the proposed data augmentation methods for enhancing robustness against adverse weather conditions. Our method attains a remarkable 39.5 mIoU on the SemanticKITTI-to-SemanticSTF benchmark, surpassing the previous state-of-the-art by over 5.4%p, tripling the improvement over the baseline compared to previous methods achieved.", "sections": [{"title": "1 Introduction", "content": "LiDAR semantic segmentation is a fundamental task of 3D scene understanding.\nRobust and reliable segmentation is crucial in safety-critical applications like\nautonomous driving. However, existing LiDAR segmentation models [1,5, 15, 22,\n29,38] commonly lack robustness, reporting significant performance degradation\nunder adverse weathers such as snow, fog, rain or wet surfaces.\nTo address this issue, recent studies [12, 33, 34] have introduced corrup-\ntion benchmarks including adverse weather and proposed effective techniques\nfor robust LiDAR segmentation against corruption. These robust methods are\nprimarily divided into two kinds: a task-agnostic approach and a simulation-\nbased approach. The task-agnostic approach employs general machine-learning\napproaches for robustness without explicitly addressing the characteristics of Li-\nDAR corruption caused by adverse weather. The simulation-based approach [3,\n7, 8, 10, 24, 27, 35] artificially synthesizes weather-specific data through physical\nequations for training. However, these efforts focus on detection tasks, and each\nmethod considers only a single type of weather at a time.\nWhile the simulation-based approach can better exploit the intrinsic proper-\nties of LiDAR scans under target adverse weather, simulating every weather type\nin all possible severities is often impractical and inaccurate. Instead, we adopt\na data-centric perspective to analyze corrupted LiDAR data. For example, the\ndistortion caused by \u201crain\u201d, \u201csnow\u201d and \u201cfog\u201d often create similar patterns, as\nsimulated in [10]. Also, all those adverse weather shows point drop through at-\ntenuation or occlusion by droplet [6,7,10]. While the simulation-based approach\nrequires explicit modeling of the LiDAR for each weather condition, our data-\ncentric approach potentially addresses complex distortions with a few unified\npatterns. Through existing research and our comprehensive analysis, we find\nthat various adverse weather conditions create similar distortion patterns in Li-\nDAR data. From this insight, we categorize these distortions into two types: (1)\ngeometric perturbation and (2) point drop. By conducting a toy experiment, we\ndemonstrate that these two distortion types are highly correlated with segmen-\ntation performance degradation.\nRooted by the toy experiment, we introduce two novel and strategic data aug-\nmentation methods tailored to the LiDAR distortion caused by adverse weather.\nBy incorporating these augmentations during training, we aim to enhance the\nmodel's robustness for each distortion type. For geometric perturbation, we in-\ntroduce Selective Jittering (SJ), which applies Additive Gaussian Noise (AGN)\nto alter the XYZ-coordinate and intensity within a selective local region. For\nhandling a point drop pattern, we developed a Learnable Point Drop (LPD)\nthat employs Deep Q-Network (DQN) to strategically remove points. Our aug-\nmentation strategies are informed by existing studies [10,17,28], which shows\ngeometric perturbation typically involves small, random alterations to the orig-\ninal points. This insight led to our choice of jittering as a suitable technique for\naddressing the geometric perturbation. Also, augmentation strategy for point\ndrop or occlusion is motivated from existing studies [7,8,10] which state that\nintensity and depth of LiDAR beams play a significant role. Therefore, our idea\ndeveloped that using a DQN to detect point drops that impair LiDAR semantic\nsegmentation models might enable the DQN to effectively follow and adapt to\nsuch point drop patterns."}, {"title": "2 Related Works", "content": null}, {"title": "2.1 LiDAR Semantic Segmentation", "content": "Existing 3D LiDAR point cloud semantic segmentation methods can be catego-\nrized into three types based on the data representation: Point-based, Projection-\nbased, and Voxel-based.\nPoint-based methods [23,29,37] utilize the 3D points directly as input. KP-\nConv [29] initially clustering local points, aggregating these local features, and\nthen feeding them into kernel point convolutions. The Point Transformer [37]\nutilizes a transformer architecture to compute query points in each local region,\nobtained through k-nearest neighbors (kNN). The Point-Mixer [4] tried to adapt\nthe MLP-Mixer [30] for point cloud applications. They achieve high performance\nbut suffer from high computational costs due to the utilization of large-scale raw\nLiDAR data.\nProjection-based methods [1, 11, 19] project LiDAR points into a 2D image\nand performs the semantic segmentation using architectures successful in a 2D\nimage. RangeViT [1] directly adopts a ViT model pre-trained on 2D images,\ndemonstrating that pre-trained power in 2D images can be effective as prior\nknowledge in range images. RangeFormer [11] proposes \u201cRangeAug\" to maxi-\nmize the utility of range images created by projecting into 2D, producing multi-\nple range image data to overcome the low performance in range image models.\nProjection-based methods achieve fast inference speed but present a sub-optimal\nperformance due to missing information after the projection."}, {"title": "2.2 LiDAR Data Augmentation", "content": "Inspired by 2D image augmentation, conventional LiDAR segmentation meth-\nods apply classic scaling, rotation, flipping, and translation to augment LiDAR\ndata. Recently, several out-of-context augmentation techniques [13,21,32] that\nmix different LiDAR scans have been proposed. Mix3D [21] combines randomly\nselected two scans. Considering the sweeping mechanism of the LiDAR sensor,\nPolarMix [32] cuts LiDAR scans along the azimuth axis, then exchanges point\ncloud sectors and applies instance-level rotate-pasting. To reflect the spatial prior\nof the LiDAR point cloud, LaserMix [13] partitions LiDAR scans based on the\nlaser beams and blends partitions from different LiDAR scans. Recent study [25]\nintroduces a fast LiDAR domain augmentation module to address sensor-bias\nproblems. To the best of our knowledge, our approach is the first augmentation\nmethod specifically designed to address data corruption under adverse weather\nconditions."}, {"title": "2.3 LiDAR Under Adverse Weather Conditions", "content": "Robustness under harsh conditions is crucial in safety-critical applications. Ad-\nverse weather substantially degrade performance in real-world outdoor autonomous\ndriving. Thus, there are several attempts to develop weather-robust models in\nfields such as 2D segmentation [16, 18], 3D detection [7,9,12], and 3D segmen-\ntation [12,33]. Simulation-based approaches [7,9] artificially synthesize data for\nsingle weather conditions through physical modeling and utilize it for training.\nWe differ from these in that we do not model specific weather conditions ex-\nplicitly. Recently proposed task-agnostic approaches [12, 33] consider multiple\nweather conditions at once. However, they use general machine-learning tech-\nniques (such as teacher-student framework and feature prototype) to achieve\nrobustness rather than specifically tackle LiDAR corruption caused by adverse\nweather. We differ from these in that we propose augmentation methods specifi-\ncally tailored for adverse weather conditions based on the analysis of performance\ndegradation in LiDAR data."}, {"title": "3 Finding Distortions to Augment", "content": "In this section, we aim to discuss the patterns of distortion that different adverse\nweather conditions impose on LiDAR data. Although adverse weather conditions"}, {"title": "3.1 Distortion Factors from Adverse Weather", "content": "are distinct in reality, studies have shown that their effects on LiDAR data often\nresult in similar impacts. For instance, the distortions caused by \"rain\", \"snow\",\nand \"fog\" tend to produce similar point-missing patterns due to attenuation in\nthe data, as demonstrated in [10]. Therefore, this section will focus on identify-\ning the common distortion patterns caused by adverse weather through existing\nstudies. Overall, existing studies describe the effect of adverse weather as four\ndifferent types of distortion: (1) Point Drop due to energy absorption, (2) \u041e\u0441-\nclusions caused by droplets of rain or snow and fog, (3) Geometric perturbation,\nand (4) Intensity distortion due to energy absorption."}, {"title": "3.2 Toy Experiment", "content": "(D1) Point Drop. Several studies have explored how adverse weather condi-\ntions contribute to point drops in LiDAR data. Kilic et al. [10], Fersch et al. [6]\nand Shin et al. [27] describe point drops resulting from beam attenuation and\nbeam missing due to droplets, fog, and frozen or wet ground. These studies col-\nlectively suggest that adverse weather conditions typically lead to point drops\nin LiDAR data.\n(D2) Occlusions. Several studies have addressed occlusions caused by adverse\nweather conditions. Hahner et al. [7], Kilic et al. [10], Kong et al. [12] and Yan\net al. [34] consider scenarios where beams colliding with snow collect signals\nat much shorter distances than the intended objects of collision. Upon review-\ning these studies, we have concluded that adverse weather consistently leads to\nocclusions.\n(D3) Geometric Perturbation. Some studies focus on geometric perturbation\ncaused by adverse weather conditions. Kilic et al. [10], Li et al. [17] and Smith\net al. [28] demonstrated geometric perturbation in adverse weather, such as fog,\nsnow, and rain, by incorporating random noise into the coordinates. Through\nthese studies, we have come to conclude that adverse weather universally causes\ngeometric perturbation.\n(D4) Intensity Distortion. Numerous studies have focused on intensity dis-\ntortion caused by adverse weather conditions. Bijelic et al. [3], Shin et al. [27],\nFersch et al. [6], Kong et al. [12], and Yan et al. [34] have collectively shown that\nadverse weather conditions like fog, wetness, and rain lead to a reduction in Li-\nDAR beam intensity, influencing the generation of synthetic data. Through these\nstudies, we conclude that adverse weather commonly causes intensity distortion.\nBased on previous research findings mentioned in Section 3, the types of dis-\ntortions in LiDAR point clouds caused by adverse weather conditions converge\ninto a set of common distortions. Therefore, from a data-centric perspective,\nthe issues that we need to consider can be summarized as follows: (D1) Point\ndrop, (D2) Occlusions, (D3) Geometric perturbation, and (D4) Inten-\nsity distortion. Here, our aim is to identify which distortion types negatively\nimpact performance. To achieve this, we have generated four distortion types of\ntoy synthetic data through the SemanticKITTI validation set."}, {"title": "4 Methods", "content": "  (D1) Point drop: It considers the scenario where each LiDAR point dis-\nappears randomly and independently due to severe adverse weather. We\nremoved individual points randomly to synthesize these data. We set the\ndrop ratios at 0.5 and 0.9.\n  (D2) Occluding points: The method assumes that occlusions occur predom-\ninantly in front of objects, mainly due to distortion from fog, snow, and\nrain. We synthesized this data by randomly selecting points and altering\ntheir depth to one-tenth of the original depth. The selecting ratios were also\ndetermined to be 0.5 and 0.9.\n  (D3) Geometric perturbation: This aspect assumes the distortion of point\ncoordinates resulting from adverse weather conditions. We synthesized data\nby adding Gaussian noise to the coordinates of all points. The Gaussian noise\nlevels were established at 0.05 and 0.25.\n  (D4) Intensity distortion: This distortion type assumes intensity attenuation\ncaused by fog, raindrops, and snow particles. We synthesized this data by\nsubtracting Gaussian noise from the intensity of all points. The Gaussian\nnoise levels were established at 0.05 and 0.25.\nWe selected MinkowskiNet [5] for our toy experiment. This choice was based\non existing research [12,34], which indicates that MinkowskiNet is a standard\nand robust model.\nIn Tab. 1, it's evident that with increasing distortion in D1, D2, and D3, per-\nformance falls below half of the baseline. Contrarily, D4 maintains performance\nlevels despite significant distortion. This performance degradation likely stems\nfrom changes in the local geometric structure where model computations occur.\nFor this reason, D4 did not significantly impact performance.\nImportantly, D2's notable performance drop is argued to result from a point\ndrop akin to D1. It is due to occlusions altering the point's local geometric\nstructure, an effect similar to point drop. This is evidenced in Fig. 1, where\nincorrect predictions from D2 show patterns resembling those in D1.\nIn summary, our observations indicate that geometric perturbation and point\ndrop are the most impactful distortions from adverse weather. Data augmenta-\ntion that replicates these phenomena could likely boost the model's robustness\nagainst such conditions without explicit weather simulations. Thus, robust Li-\nDAR semantic segmentation models against adverse weather involve two key"}, {"title": "4.1 Selective Jittering", "content": "In this section, we propose solutions for the two challenges identified in Section\n3.2. Our objective is to enhance the robustness of the model by addressing the\ncommon distortions exhibited by various adverse weather conditions, through\ndata augmentation. This approach aims to achieve robustness without the ne-\ncessity of simulating each adverse weather scenario explicitly.\nAs observed earlier, distortions resulting from adverse weather include point\ndrop, occlusion, geometric perturbation, and intensity distortion. Of these, the\nmain distortions identified as having the most detrimental impact on perfor-\nmance were (1) geometric perturbation caused by minor variations in point co-\nordinates and (2) point drop caused by beam missing or occlusion. To address\nthese challenges, we propose two techniques: Selective Jittering and Learnable\nPoint Drop.\nSelective Jittering is devised to address the first main distortion, geometric per-\nturbation. Selective Jittering consists of two types: Depth-Selective Jittering\n(DSJ) and Angle-Selective Jittering (ASJ).\nDepth-Selective Jittering (DSJ). DSJ employs a single scan to add Gaussian\nnoise to the XYZ-coordinates and intensity value of points below a randomly\nselected depth range.\nAngle-Selective Jittering (ASJ). It applies Gaussian noise to a randomly se-\nlected range of angles. By performing jittering across different ranges of depths or\nangles for each point, this method reflects the augmentation that some beams are\naffected by geometric perturbation in a non-uniform manner in adverse weather.\nThis concept realistically represents the characteristics of transparent droplets\nlike rain and snow [6]."}, {"title": "4.2 Learnable Point Drop", "content": "steps: (1) training on specific adverse point drops detrimental to per-\nformance, and (2) training with minor adjustments in point coordi-\nnates.\nLearnable Point Drop (LPD) was devised to address the second main distortion,\npoint drop. LPD is designed to artificially create point drop due to occlusion,\nsuch as dense fog. LPD employs a Deep Q-Learning Network (DQN) [20] for\nidentifying the drop ratio and drop region that lead to adverse effects on the\nmodel. The reward of the DQN is designed to identify point drops that increase\nthe training loss and uncertainty of the LiDAR semantic segmentation model.\nThrough LPD, the LiDAR semantic segmentation model is exposed to point\ndrop scenarios caused by adverse weather conditions. Consequently, it learns\nto make accurate predictions even in the absence of points that are critical for\nsegmentation in clean data environments. Since LPD is employed merely as a\nconcept for data augmentation and exists as a separate module, it necessitates\nno alterations to the existing model's training scheme except for limiting the\ngradient norm to ensure the stability of DQN learning.\nLPD module defines its current state by summing the loss \\(L_{aug}\\), calculated\nfrom augmented data obtained through SJ, and the entropy \\(H_{aug}\\), derived from"}, {"title": "4.3 Overall Pipeline", "content": "logits. The loss \\(L_{aug}\\) is calculated using the original loss function employed by\nthe model in use. The calculation of entropy is as follows:\n\\(H_{aug}(x) = -\\frac{1}{N} \\sum_{i=1}^{n} P(x_{i}) \\log P(x_{i})\\)\nIn this process, \\(x_{i}\\) represents each point, and \\(P(\\cdot)\\) is softmax to the logits. \\(N\\)\ndenotes the number of points. LPD predicts the indices of points to drop, taking\nas input the sum of \\(L_{aug}\\) and \\(H_{aug}\\) added to the input point tensor. This process\nenables simultaneous learning of the drop ratio and drop region. Subsequently,\nthe difference between the sum of the loss \\(L_{LPD}\\) and entropy \\(H_{LPD}\\), obtained\nfrom the dropped points by LPD, and the sum of \\(L_{aug}\\) and \\(H_{aug}\\), is defined as\nthe reward.\nIn Comparison with random drop, random drop uniformly reduces points\nacross all depths. Consequently, this approach is not suitable for simulating point\ndrops caused by fog, as it lacks the depth-specific characteristics of fog-related\ndistortions. Furthermore, randomly generated point drop scans may not achieve\nthe severity of adverse weather conditions necessary to drop critical points that\ncould deceive the model. Detailed comparison will be conducted in the ablation\nstudy section."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Experimental setup", "content": "The overall training procedure begins with data augmentation through SJ. The\naugmented points are input into the LiDAR semantic segmentation model to\ncompute loss. The model then calculates logits for each point, applies softmax,\nand subsequently computes the entropy. The current state of the LPD module\nis set as the sum of the loss and entropy. The dropped points are then input\ninto the LiDAR semantic segmentation model to calculate the corresponding\nloss and entropy. The difference between the sum of loss/entropy obtained from\nthe augmented points and that obtained from the points determined by LPD is"}, {"title": "5.2 Main results", "content": "Dataset and Evaluation Metrics. To assess robustness, we trained our model\non 19 classes from SemanticKITTI and then validated its performance on val-\nidation set of SemanticSTF as same as [33]. The experiment process involves\ntraining on clean data followed by evaluating robustness in adverse conditions,\nakin to existing research [12, 33, 34]. SemanticSTF is a dataset gathered under\nreal-world adverse conditions such as rain, fog, and snow. Following the ap-\nproach in [33], invalid data in SemanticSTF are mapped to an \"ignore label\u201d. We\nutilized MinkowskiNet [5] for validation purposes due to its well-known robust-\nness [12,34]. Our evaluation metrics include the Intersection over Union (IoU)\nfor each class and the mean IoU (mIoU) across all classes. Experiments on syn-\nthetic data like SynLiDAR are detailed in the supplementary material due to\nits low significance because our experiment targets high performance in adverse\nweather using real data.\nImplementation Details. MinkowskiNet-18/32width served as our baseline\nmodel. The learning rate was set at 0.24, with a weight decay of 0.0001. The\nmean and standard deviation of Gaussian noise used in SJ are 0 and 0.01 each.\nIn cases of Learnable Point Drop, the norm of the gradient was limited to 100 for\nboth the DQN and Segmentation model. All experiments were conducted using\nfour A6000 GPUs for 15 epochs in batch size 2. The duration of all experiments\nranged between 3 to 5 hours.\nSemanticKITTI to SemanticSTF. According to Tab. 2, our proposed method\nshowed a significant improvement in SemanticSTF, with a +8.1 mIoU increase\nover the baseline and +5.6 mIoU over the recent competitor, PointDR. Our\nmethod demonstrated universally superior performance across all weather con-\nditions, validating the validity of our choice in data distortion type and the effec-\ntiveness of our method. Our method substantially improved the performance of\ncategories like other vehicle, motorcyclist, sidewalk, pole, and traffic sign, which\nhad lower baseline performance, increasing their class IoU by about +5 to +10.\nAdditionally, it significantly enhanced the mIoU for cars and persons by +19.0\nand +9.6, respectively, compared to the baseline, which is crucial for safety in\ndriving environments. More details are available in the supplementary material."}, {"title": "5.3 Ablation Study", "content": "SynLiDAR to SemanticSTF. In our study, we validated the results peri-\nodically using the validation sets provided within the SemanticKITTI dataset\nduring training. For SynLiDAR, where no specific validation set was available,\nsequence 0 was treated as the validation set. The best-performing models during\nthese validation checkpoints were selected for reporting.\nIn contrast, PointDR demonstrated an unrealistic scenario in its baseline\ncode, using SemanticSTF for validation during training and reporting only the\nhighest-performing results. For a fair comparison using PointDR's criteria, our\nmethod shows a performance increase of +0.04 mIoU over PointDR. Using our\nAlso in Tab. 2, examining the performance improvement per weather condi-\ntion, we note that the most significant increase in mIoU was in rain, with +7.9\nmIoU. This result is indicative of our SJ augmentation effectively reflecting ge-\nometric perturbations caused by raindrops. This aspect will be further explored\nin the ablation study. Additionally, in snow, our method showed a remarkable\nincrease of +7.8 mIoU, and in dense fog (+5.3 mIoU) and light fog (+7.4 mIoU),\nour methodology clearly outperformed the baseline. This indicates that our data\naugmentation effectively models the adverse impacts of weather on LiDAR se-\nmantic segmentation models. Detailed analysis of how our method enhances\nrobustness against each weather condition, and the factors contributing to the\nimproved performance, will be discussed in the ablation study.\nMoreover, as previously mentioned, the training architectures designed to\nenhance robustness, such as those proposed in PointDR [12,33,34], can be or-\nthogonally applied to our proposed data augmentation. Therefore, regardless of\nthe choice of training architecture, the integration of our method is expected to\nyield superior performance compared to conventional approaches.\nAnalysis of Proposed Component Methods. As shown in As Tab. 4, when\nASJ was added to the baseline, we observed an improvement in robustness across\nall adverse weather conditions (increases of +2.6 mIoU in dense fog, +5.3 mIoU\nin light fog, +8.1 mIoU in rain, and +6.3 mIoU in snow), albeit with a -1.8 mIoU\ndecrease in clean data performance. The introduction of DSJ further enhanced\nperformance in dense fog (+4.1 mIoU), light fog (+6.3 mIoU), rain (+9.3 mIoU)\nsnow(+4.6 mIoU) and an overall performance increase of +6.2 mIoU. Our Se-\nlective Jittering method's strong performance in rain condition showcases its\nefficacy in rainy weather, where it considers geometric perturbations for selected\nbeams instead of all.\nThe application of Range Jittering showed improvements in dense fog(+3.7\nmIoU), light fog (+6.9mIoU), rain(+6.0 mIoU), snow (+8.2 mIoU), and an over-\nall performance increase of +7.3 mIoU, indicating the importance of simulating\nrange distortion due to adverse weather as discussed in [10]. As seen earlier, in\nrainy weather, jittering only some beams aids in enhancing robustness. How-\never, performing Range Jittering on the remaining original points leads to all\npoints being jittered, reducing performance in rain condition. Additionally, the\nimprovement in snow conditions by using Range Jittering suggests that perturb-\ning as many beams as possible is more effective for robustness enhancement in\nsnowy weather.\nThe addition of LPD resulted in improvements in dense fog (+5.3 mIoU),\nlight fog (+7.4 mIoU), rain (+7.9 mIoU), snow (+7.8 mIoU), and an overall\nincrease of +8.1 mIoU against to the baseline. The substantial performance\nenhancements observed in conditions of dense and light fog indicate that the LPD\neffectively represents occlusion from foggy weather as intended. The performance\nreduction in snow conditions compared to the use of ASJ, DSJ, and Range\nJittering (-0.4 mIoU) is likely due to LPD not exclusively targeting the ground\nfor point dropout. LPD applies the point drop across all points, as shown in\nFig. 6. This represents a limitation of LPD, necessitating the development of\nalternative methods to address this issue.\nOverall, the use of all components led to a significant improvement in per-\nformance under adverse weather conditions, with an increase of +8.1 mIoU over\nthe baseline at a reasonable cost of -1.1 mIoU in clean data. Ultimately, our\nmethod achieved state-of-the-art results on SemanticSTF, with an increase in\nmIoU across all adverse weather conditions compared to the baseline. The com-\nbined use of DSJ, ASJ, Range Jittering, and LPD demonstrated high perfor-\nmance, indicating that each component synergistically contributes to enhancing\nthe model's robustness."}, {"title": "6 Conclusion", "content": "more realistic protocol, our method demonstrates an even greater performance\nimprovement of +0.32 mIoU against PointDR. Detailed results are shown in\nTab. 3.\nOur study focused on analyzing LiDAR data distortions in adverse weather\nand enhancing LiDAR model robustness with a data-centric method. We identi-\nfied that point drop and geometric distortion mainly affect model performance\nthrough toy experiments. We introduced novel data augmentation techniques,\nSelective Jittering, and Learnable Point Drop, leading to state-of-the-art per-\nformance on SemanticSTF. We showed that our methodology significantly con-\ntributes to robustness enhancement without complex data simulations. We ex-\npect that our research will contribute to improving the stability and reliability\nof LiDAR semantic segmentation."}]}