{"title": "SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model", "authors": ["Lingyue Fu", "Hao Guan", "Kounianhua Du", "Jianghao Lin", "Wei Xia", "Weinan Zhang", "Ruiming Tang", "Yasheng Wang", "Yong Yu"], "abstract": "Knowledge Tracing (KT) aims to determine whether students will respond correctly to the next question, which is a crucial task in intelligent tutoring systems (ITS). In educational KT scenarios, transductive ID-based methods often face severe data sparsity and cold start problems, where interactions between individual students and questions are sparse, and new questions and concepts consistently arrive in the database. In addition, existing KT models only implicitly consider the correlation between concepts and questions, lacking direct modeling of the more complex relationships in the heterogeneous graph of concepts and questions. In this paper, we propose a Structure-aware INductive Knowledge Tracing model with large language model (dubbed SINKT), which, for the first time, introduces large language models (LLMs) and realizes inductive knowledge tracing. Firstly, SINKT utilizes LLMs to introduce structural relationships between concepts and constructs a heterogeneous graph for concepts and questions. Secondly, by encoding concepts and questions with LLMs, SINKT incorporates semantic information to aid prediction. Finally, SINKT predicts the student's response to the target question by interacting with the student's knowledge state and the question representation. Experiments on four real-world datasets demonstrate that SINKT achieves state-of-the-art performance among 12 existing transductive KT models. Additionally, we explore the performance of SINKT on the inductive KT task and provide insights into various modules.", "sections": [{"title": "1 INTRODUCTION", "content": "Intelligent tutoring systems (ITS), such as Massive Open Online Courses (MOOCs)\u00b9 and Khan Academy\u00b2, are garnering increasing attention from learners due to the extensive learning resources and real-time feedback mechanisms. Knowledge Tracing (KT) stands out as a vital area of research within ITS, which aims to assess the current knowledge states of students by analyzing their learning histories and to predict their responses to upcoming questions. KT not only helps to identify deficiencies in student's learning content but also provides a foundational basis for the subsequent instructional strategies in ITS.\nExisting KT models originate from traditional methods repre-sented by BKT [6] and have evolved significantly with the advent of models based on deep neural networks. Deep learning-based KT models learn ID embeddings for questions and concepts through learning history of students. DKT [33] and DHKT [41] utilize Re-current Neural Networks (RNNs) [34] to capture the sequential information of learning history. EERNNA [36], SAKT [31], and AKT [11] employ attention mechanisms to gauge the importance of a student's learning history in addressing the current question. SKVMN [2] and DKVMN [49] apply memory networks [42] to"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Knowledge Tracing", "content": "To investigate the learning patterns of students in ITS, numerous KT models have been proposed to encode knowledge and model in-teraction histories. Traditional KT models such as Bayesian Knowl-edge Tracing [7] focus on tracing students' knowledge states by estimating general parameters. With the advent of deep learning, DKT [33] is the first to utilize RNN [34] and LSTM [14] to model the sequential dynamics of student interactions. DKT+ [48] and DHKT [41] extend the implementation of DKT, taking into account the relationships between questions and concepts. Inspired by the Transformer architecture [40], attention mechanisms [4] have been incorporated into deep learning KT models for capturing the re-lationships among questions and their relevance to a student's knowledge states [11, 31, 36].\nConcurrently, attention has been paid to more nuanced behav-ioral data and advanced neural architectures. For instance, LBKT [44] analyzes a range of learning behaviors, containing the speed of re-sponses, the number of attempts, and the use of hints. SAKT [31] and AKT [11] introduce attention mechanisms [4] to focus on the most relevant aspects of a student's past interactions. Additionally, innovative approaches such as CMKT [52]assess students' dynamic mastery of concepts. MF-DAKT [50], which incorporates multiple student-related factors with a dual attention mechanism, show-cases the field's advancement towards more granular and complex models. SFKT [51] also demonstrates the application of KT models to handle sequences of variable lengths. Alongside these develop-ments, graph-based KT models like GKT [29] and GIKT [47] employ GNN [16] to leverage the relational data among concepts. These models map the complex network of knowledge, allowing predic-tions that consider not just the individual learning trajectory but also the intricate structure of the knowledge itself."}, {"title": "2.2 LLM-Enhanced User Modeling", "content": "There have been some works using LLMs to enhance the perfor-mance of user modeling [21, 37]. Some work [22-24, 46] apply LLMs as profilers to involve the creation of prompts based on users' history, and input these prompts into LLMs to generate various aspects of user profiles. KAR [43] leverages LLMs to generate user and item profiles, encompassing user preference and real-world knowledge of items into click-through rate (CTR) prediction task. GIRL [53] uses LLMs to generate suitable job descriptions base on the user's curriculum vitae to help recommendation model under-stand jobhunter's preferences. In addition to profiling, LLMs serve as feature encoders, translating raw user data into rich, seman-tic embeddings that improve user modeling systems [19, 32, 43]. GPT4SM [32] employs GPT to encode queries and candidate text for relevance prediction in recommendation systems, and LKPNR [13] uses open-source LLMs for better semantic representation of news articles. Moreover, LLMs function as knowledge augmenters by integrating external knowledge into user models [3, 9, 18, 20, 43], thus enhancing the precision of user-related predictions. Mysore et al. [28] augment narrative-driven recommendations by generat-ing author narrative queries with LLMs, and KAR [43] uses LLM-prompted outputs to augment recommender systems with factual item knowledge.\nExisting LLM-enhanced user modeling methods are mostly ap-plied in the domain of recommender systems, which significantly from the KT task. In recommender systems, users' interests typically remain stable over short periods, whereas in ITS, the knowledge states of students evolve dynamically as they engage with edu-cational contents such as exercises. Additionally, there is strong structural information, such as interdependency and correlation among the concepts in ITS. Therefore, there is a need for specially designed models that can effectively track and adapt to the unique dynamics of students' learning processes in ITS."}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 Problem Definition", "content": "Consider a set of students $S$, a set of questions $Q$, and a set of concepts $C$ within an ITS. The learning history of a student $s \\in S$ is recorded as $R_s = \\{(q_1, r_1), (q_2, r_2), . . ., (q_t, r_t)\\}$, where $q_t \\in Q$ represents the question answered by the student at time step $t$, and $r_t \\in \\{0, 1\\}$ indicates whether the student $s$ correctly answers the question $q_t$ (1 for correct and 0 for incorrect). Given the learning history $R_s$ of a student and a new question $q_{T+1}$, the goal of the KT task is to predict the probability that the student correctly answers the new question, denoted as $p(r_{T+1} = 1|R_s, q_{T+1})$.\nNote that in a real ITS, teachers continually expand the ques-tion and concept set, meaning that both the question set $Q$ and the concept set $C$ are in a state of ongoing growth. Previous KT models are trained and tested under the assumption $Q_{train} = Q_{test}$, which are only suitable for the transductive KT task. However, in the inductive KT task, $Q_{train} \\neq Q_{test}$. In this paper, our SINKT is capable of addressing the inductive task effectively while also delivering excellent performance in the transductive KT task."}, {"title": "3.2 Concept-Question Graph Generation", "content": "To inject open-world knowledge into SINKT, we construct a hetero-geneous concept-question graph $G = (V, \\mathcal{E}, O_V, R_{\\mathcal{E}})$ to capture effective representations of concepts and questions, where $O_V$ represent the set of vertex type, and $R_{\\mathcal{E}}$ denotes the edge type (rela-tion). The vertex set $V$ includes the question set $Q$ and the concept set $C$, and The vertex type set $O_V$ includes concept and question cor-respondingly. Edge set $\\mathcal{E}$ describes different relationships between concepts and questions. The edge type set $R_{\\mathcal{E}}$ includes concept-question, question-concept and concept-concept. The graph $G$ not only encapsulates the relationships between questions and con-cepts contained in the dataset but also incorporates open-world knowledge provided by LLMs.\nconcept-question edges and question-concept edges are derived from the dataset. Each question $q_i$ corresponds to one or more concepts $C_{q_i} = \\{c_1, c_2, . . ., c_{n_i} \\}$, and each concept $c_i$ corresponds to many questions $Q_{c_i} = \\{q_1, q_2, ..., q_{m_i}\\}$, where $n_i$ and $m_i$ denotes the number of concepts related to the question $q_i$ and the number of questions related to the concept $c_i$, respectively. In the graph $G$, the relation is described as $n_i$ question-concept edges $(q_i, c_j)(j = 1, 2, ..., n_i)$ and $m_i$ concept-question edges $(c_i, q_j) (j = 1, 2, . . ., m_i)$.\nconcept-concept edges within the graph $G$ are generated by LLM. As demonstrated in Figure 2, for each concept $c_i$, we employ a specific prompt and utilize GPT-4 [30] to identify a list of related concepts. [10, 17] suggest that LLMs struggle with tasks directly involving graph structures, often requiring specialized training or input transformation to handle graph structures. Therefore, we opt to use a \"select from the list\" approach rather than asking GPT-4 to directly generate the graph. After GPT-4 generates the response, we use the regular expression extraction to get $p_i$ concepts related to $c_i$, and add the directed edges $(c_j, c_i)(j = 1, 2, ..., p_i)$ into the graph $G$."}, {"title": "4 METHODOLOGY", "content": "In this section, we will introduce our method in detail, and the over-all framework is shown in Figure 3. After generating the concept-question graph by GPT-4, we first use the Textual Information Encoder (TIEnc) to extract semantic information from the concepts and questions. Then, we design a Structural Information Encoder (SIEnc) to learn question and concept representations from the het-erogeneous graph. Finally, we design a student state encoder based on concept-level mastery to capture the student's knowledge state, and predicts the final response by an interaction predictor."}, {"title": "4.1 Textual Information Encoder", "content": "The introduction of LLMs bridges the gap between semantic infor-mation and plain text. SINKT utilizes a Pretrained Language Model (PLM) as semantic encoder to get representations of the plain text of concepts and questions. For each concept $c_i$, we input its plain text TEXT($c_i$) into the PLM and acquire its representation vector\n$x_i^c = PLM(TEXT(c_i)) \\in \\mathbb{R}^{d_t}$,\nwhere $d_t$ is the encoding dimension of the PLM. Similarly, for each question $q_i$, we could obtain its representation vector\n$x_i^q = PLM_q(TEXT(q_i)) \\in \\mathbb{R}^{d_t}$.\nThe representation vectors $x_i^c$ and $x_i^q$ contain the semantic infor-mation of concepts and questions. In our framework, we fix the parameters of PLM encoder and use representation vectors as an input of following modules."}, {"title": "4.2 Structural Information Encoder", "content": "When a student answers a question, their mastery of concepts associated with that question influence their response. The mas-tery of both the target concept and its related concepts, as well as the related questions, can provide valuable information for the assessment of KT models. Therefore, the three types of edges in the heterogeneous graph $G$, i.e., concept-question, concept-concept, and question-concept, are considered by SINKT. To encode such structural information, we carefully design a multi-layer heteroge-neous graph encoder.\nIn the graph $G$, denote neighbor questions of the concept $c_i$ as $N_i^c$ and $N_{qi}$, respectively. The set of neighbor concepts of the question $q_i$ is denoted as $N_{q_i}$. We apply three different Graph At-tention Networks (GAT) to aggregate neighborhood information for vertexes. Concept-Concept GAT layer fuse neighbor concept representations of the target concept $c_i$. Specifically, the Concept-Concept GAT can be expressed as:\n$\\alpha_{i,j}^{cq} = \\frac{exp \\big(LeakyReLU \\big(acc(x_i^c \\oplus x_j^q)\\big)\\big)}{\\sum_{q_k \\in N_i^c} exp \\big(LeakyReLU \\big(acc(x_i^c \\oplus x_k^q)\\big)\\big)}$,\n$e_i^{cq} = \\sum_{q_j \\in N_i^c} \\alpha_{i,j}^{cq} * (W^{cq}x_j^q)$,\nwhere $W^{cq}$ is the aggregate weight $\\alpha_{i,j}^{cq}$ is the attention weight, $\\oplus$ denotes concatenate operation. Similarly, Concept-Concept GAT integrate neighbor concept information to the target concept:\n$\\alpha_{i,j}^{cc} = \\frac{exp \\big(LeakyReLU \\big(acc(x_i^c \\oplus x_j^c)\\big)\\big)}{\\sum_{c_k \\in N_i^c} exp \\big(LeakyReLU \\big(acc(x_i^c \\oplus x_k^c)\\big)\\big)}$,\n$e_i^c = \\sum_{c_j \\in N_i^c} \\alpha_{i,j}^{cc} * (W^{cc}x_j^c)$,\nand Question-Concept GAT integrate neighbor concept information to the target concept:\n$\\alpha_{i,j}^{aqc} = \\frac{exp \\big(LeakyReLU \\big(a^{qc}(x_i^q \\oplus x_j^c)\\big)\\big)}{\\sum_{q_k \\in N_{q_i}} exp \\big(LeakyReLU \\big(a^{qc}(x_i^q \\oplus x_k^c)\\big)\\big)}$,\n$e_i^q = \\sum_{q_j \\in N_{q_i}} \\alpha_{i,j}^{aqc} * (W^{qc}x_j^c)$.\nTo emphasize the semantic information inherent in concepts and questions themselves, SINKT introduces jumping knowledge [45] to propagate the vertex original representations. The node repre-sentation of l-th encoder layer of $c$ can be represented as:\n$x_c^{(l)} = ReLU(W^c x_i^c^{(l-1)} + e_i^{cq} + e_i^{cc})$,\n$x_q^{(l)} = ReLU(W^q x_i^q^{(l-1)} + e_i^{q})$,\nwhere $W^c$ and $W^q$ are trainable weight matrix for concepts and questions. We initialize the input of the SIEnc by the output of the semantic information encoder:\n$x_q^{(0)} = x^q, x_c^{(0)} = x^c$.\nWe use $q_i \\in \\mathbb{R}^d$ and $c_i \\in \\mathbb{R}^d$ to denote representation vectors of the question $q_i$ and the concept $c_i$ after the k-layer graph encoder, where d, k are hyper-parameters."}, {"title": "4.3 Student State Encoder", "content": "To effectively learn from highly sparse question-response data, [11, 25, 26] suggest to transform the original question-response data into concept-response data. Due to the fact that some questions contain multiple concepts, SINKT averages representation vectors of concepts related to the question $q_t$ to represent students' learning at time step t\n$u_t = \\frac{1}{\\left|C_{q_t}\\right|} \\sum_{c_i \\in C_{q_t}} c_i$.\nTo jointly represent the item and correctness of students' response, we introduce $v_t \\in \\mathbb{R}^{2d}$ as\n$v_t = \\begin{cases} u_t \\oplus \\vec{0} & r_t = 1 \\\n\\vec{0} \\oplus u_t & r_t = 0\\end{cases}$,\nwhere $\\vec{0} \\in \\mathbb{R}^{d}$ is a zero-vector. To model learning history of students, we use Gated Recurrent Unit (GRU) [5] to capture the sequential learning behavior\n$u_r = \\sigma (W_r (v_i \\oplus h_{t-1}) + b_r,$\n$u_z = \\sigma (W_z(v_i \\oplus h_t) + b_z,$\n$u_h = tanh(W_h(v_i \\oplus (u_r * h_{t-1})) + b_h),$\n$h_t = (1 - u_z) * u_h + u_z * h_{t-1}$.\nHere $\\sigma$ denotes the sigmoid function, and $W_r, b_r, W_z, b_z, W_h, b_h$ are trainable parameters, where $W_r, W_z, W_h \\in \\mathbb{R}^{d \\times 2d}$ and $b_r, b_z, b_h \\in \\mathbb{R}^d$. Hidden state $h_t \\in \\mathbb{R}^d$ represents the knowledge state of the student at time step t. The student state encoder considers the concept-level forgetting and knowledge acquisition pattern together during student's learning process."}, {"title": "4.4 Response Prediction", "content": "When a student answers a question, his knowledge state of the corresponding concepts and the description of the question would influence his correctness. Given knowledge state of the student $h_t$ at time step t, the question representation vector $q_{t+1}$ and its concept-level representation vector $u_{t+1}$, we make the prediction as follows\n$y_{t+1} = \\sigma \\big(W_p \\big(h_t \\oplus q_{t+1} + u_{t+1}\\big) + b_p\\big).$\nwhere $W_p \\in \\mathbb{R}^{3d}$ and $b_p \\in \\mathbb{R}$ are trainable parameters. To train all parameters in SINKT, we choose the cross-entropy log loss between the predicted response $y_{t+1}$ and the ground-truth response $r_{t+1}$ as the objective function\n$\\mathcal{L} = - \\sum_{t=1}^T (r_t log y_t + (1 - r_t) log(1 - y_t)).$"}, {"title": "4.5 Automated Pipeline for Inductive KT", "content": "SINKT mainly focuses on the semantic and structural information, which could be completely generated by LLMs. Meanwhile, SINKT use textual representations instead of ID embeddings, which is available even in the absence of learning history. Therefore, when a new concept or a new question is introduced into the database of ITS, SINKT is capable to capture the knowledge state of students towards the new questions or concepts through automated process-ing. When a teacher adds a new concept to the ITS, LLMs are invoked to integrate the new vertex into the Concept-Question Graph. The teacher could continue to add a new question that is related to this new concept, and the ITS further utilizes LLMs to annotate the new question with concepts. The new concept-question heterogeneous graph is automatically constructed. When it is necessary to assess a student's mastery of the new question, SINKT can automatically perform text encoding and propagate information through the graph, ultimately accomplishing response prediction.\nThe automated ingestion pipeline enhances the ITS by providing the flexibility to update and expand the database. This capability not only streamlines the process of integrating new content but also ensures that the system remains adaptive to evolving educational needs and emerging topics. However, existing transductive KT models are unable to achieve this."}, {"title": "5 EXPERIMENT", "content": "In this section, we conduct experiments on four real-world datasets to evaluate the proposed SINKT framework. Specifically, we aim to answer the following research questions:\nRQ1 How does the proposed SINKT performs compared to the state-of-the-art KT models?\nRQ2 How does SINKT perform when a new question is added to the ITS?\nRQ3 What is the influence of various componets of SINKT?\nRQ4 How do different hyperparameters affect the performance of SINKT?\nRQ5 Is the concept relation graph generated by GPT-4 reasonable and useful?"}, {"title": "5.1 Datasets", "content": "In our experiments, we evaluate our method on four datasets: (1) AS-SIST095, (2)ASSIST126, (3) Junyi7 and (4) Programming. ASSIST09, ASSIST12, and Junyi are public datasets widely used in intelligent education research. Programming is a private dataset with plain text of questions and concepts. For each dataset, students answering less than ten questions are removed. In order to simulate the situa-tion of data sparsity, we choose 2,000 students' learning history in ASSIST12 and Junyi. The basic statistics of these four datasets are listed in Table 1, and descriptions are as follows:\n\u2022 ASSIST09 is collected from an online tutor platform that teaches students mathematics during the school year 2009 to 2010. The dataset records the plain text of concepts and does not describe questions.\n\u2022 ASSIST12 is also collected from the ASSIST platform from Sept 2012 to Oct 2013. Like ASSIST09, plain text of concepts is con-tained, but question descriptions are not provided.\n\u2022 Junyi is collected between November 2010 and March 2015 from the e-learning platform in Taiwan Junyi Academy. We choose KC as the concept text without using question descriptions.\n\u2022 Programming is collected from a commercial code-learning platform from December 13, 2021 to February 17, 2023. The dataset contains plain text of concepts and question descriptions."}, {"title": "5.2 Baselines", "content": "To demonstrate the effectiveness of predicting students' response, we evaluate the performance of SINKT with 12 baselines.\n\u2022 DKT [33] is the first model that introduces deep learning meth-ods into the KT task, which uses RNN to model students' learning history.\n\u2022 DHKT [41] assumes that skill is an abstraction of question and thus leverages the hierarchical structure between questions and concepts based on DKT.\n\u2022 DKVMN [49] adds key-value memory network on DKT. The key matrix is static to represent the relations between different skills, and the value matrix is dynamic to store students' mastery.\n\u2022 SKVMN [2] adds Hop-LSTM on DKVMN for better sequential modeling.\n\u2022 CKT [35] applies hierarchical convolutional layers and learns a matrix demonstrating students' mastery level of each concept.\n\u2022 SAKT [31] is the first model to add an attention mechanism into KT models, which uses a scaled dot-product attention mechanism to learn the past interactions' importance in predicting students' current questions.\n\u2022 EERNNA [36] uses the hidden states of RNN to represent stu-dents' knowledge states and considers the different impacts of previous states on the current response prediction.\n\u2022 GKT [29] generates a transition graph from the dataset and uses GNN to encode the knowledge states of students.\n\u2022 AKT [11] employs a self-attention mechanism to model the forgetting effect of students' memory.\n\u2022 SKT [39] exploits similarity relation and prerequisites relation between concepts based on GKT.\n\u2022 IEKT [27] retrieves the similar histories of other students to enhance the prediction of target students.\n\u2022 LBKT [44] considers several dominant learning behaviors, in-cluding speed, attempts, and hints in the learning process."}, {"title": "5.3 Implementation Details", "content": "In our experiment, we choose the most recent 200 records of students. We choose BERT to be the textual encoder for three public datasets with concept text and choose Vicuna to be the textual encoder for the Programming dataset with concept and question text. We will compare the performance of different textual encoders in Section 5.4. The layer number of the graph encoder is chosen from {1, 2, 3}. The parameter d is set to 256. The learning rate is chosen from {0.0001, 0.00005} with a decay at each epoch. The op-timizer is Adam [15]. For a fair comparison, the hyper-parameters of baselines are carefully chosen to have the best performance. Our model is implemented on PyTorch and will be publicly available upon the acceptance of this work."}, {"title": "5.4 Overall Performance (RQ1)", "content": "We compare SINKT to 12 baselines on predicting the correctness of students' responses in four real-world datasets. The experimental results are shown in Table 2. We use Accuracy (ACC) and Area Under the Curve (AUC) as the evaluation metric.\nIn Table 2, several observations can be obtained as follows:\n(1) SINKT significantly outperforms all comparative methods across all datasets and evaluation metrics. This superior performance indicates that SINKT provides more precise representations of concepts and questions, as well as a more accurate estimation of students' knowledge states.\n(2) When the datasets only contain textual information of concepts (ASSIST09, ASSIST12, Junyi), using BERT [8] as the textual en-coder yields better results. This is attributed to the fact that most concept texts consist of 1-3 words, and models like BERT are more adept at encoding word-level text. In contrast, when SINKT is provided with question texts (Prgogramming), Vicuna [38] demonstrates improved performance, since generative models have a better understanding of sentence-level text typical of question descriptions.\n(3) Introducing additional specific information into the model does not guarantee beneficial results across all datasets. For example, GKT, which extracts a transition graph from training data to train concept embeddings, shows significant performance im-provement on the ASSIST09 and ASSIST12 datasets. However, this approach introduces noise in the Junyi and Programming datasets, detracting from model effectiveness."}, {"title": "5.5 Inductive Learning of SINKT (RQ2)", "content": "To investigate the performance of SINKT after introducing new questions, we remove 1/4 of the question from the training set and evaluate the model's prediction accuracy on these questions on the validation and test sets. Since the other three datasets do not provide textual information for the questions, we compare the performance of SINKT with that of DHKT and EERNNA (best and second best baselines) on the Programming dataset. We implement random embedding initialization for DHKT and EERNNA on those unseen questions to enable them to predict responses. The results of the experiment are shown in the Table3. The experiment demonstrates that the SINKT performs better in predicting unseen questions, whereas the existing KT models struggle significantly with this task, achieving an AUC of less than 0.6.\nWe also investigate the cold start problem in real ITS. We first organize an experiment of different training samples on ASSIST09. We choose {100, 500, 1000, 2000, 2661 (all of the dataset)} students' learning history to train DHKT, EERNNA, and SINKT and evaluate them on the full test set. As shown in Figure 5a, as the number of students decreases, performance decline of SINKT is more gradual compared to baselines, which indicates that SINKT can still learn accurate concept/question representations using textual and struc-tural information even with limited training data. Additionally, we present the training curves of DHKT, EERNNA, and SINKT on the ASSIST09 dataset in Figure 5b. We argue that SINKT converges faster and exhibits minimal overfitting. This robustness in perfor-mance under limited data conditions underscores the effectiveness of incorporating rich textual and structural information in SINKT."}, {"title": "5.6 Ablation Study (RQ3)", "content": "To further investigate the importance of each module in SINKT, we design three variants to conduct the ablation study, each of which removes or changes one part from the original SINKT:\n\u2022 SINKT-Linear removes the jumping knowledge module in the graph encoder.\n\u2022 SINKT-GAT removes GAT layers in the SIEnc, i.e. only uses a linear layer to be a textual information adapter.\n\u2022 SINKT-Text replaces the textual encoder with an embedding layer, which learns representations of questions and concepts from the dataset.\n\u2022 SINKT-Transition replaces the concept generated by GPT-4 with the transition graph generated in the dataset.\nNote that SINKT-Linear and SINKT-Graph still suitable for the inductive KT task, while SINKT-Transition and SINKT-Text do not support predicted responses to unseen questions.\nWe demonstrate the experimental results of SINKT and four variants in Table 4. From the ablation study, we can derive several conclusions regarding the impact of different components in SINKT. Firstly, removing textual initialization (SINKT-Text) and the GAT layer (SINKT-GAT) in SIEnc results in a performance decline, in-dicating that both structural and textual information contributes to the learning of question and concept representations. Secondly, removing the jumping knowledge layer leads to a significant perfor-mance drop. This is because our heterogeneous graph is unevenly distributed, and the addition of the jumping knowledge layer al-lows the model to actively capture useful information. Thirdly, the performance degradation of SINKT-Transition demonstrates the effectiveness of the Concept Graph generated by GPT-4."}, {"title": "5.7 Parameter Sensitivity Analysis (RQ4)", "content": "In order to have an insight into the graph information conduction, we change the number of the Graph Encode layer k ranging from 0 to 3. Especially when k = 0, the graph encoder degrades into a Linear layer without neighbor aggregation. The experimental results on four datasets are shown in Figure 6. We find that SINKT performs best in most cases when k = 2, which indicates that second-order relationships in the concept-question heterogeneous graph are crucial for the learning process. The primary second-order relationships include concept-question-concept, concept-concept-question, and question-concept-question. In contrast, lower-order relationships provide limited neighbor information, while higher-order relationships introduce noise.\nFurthermore, we investigate the sensitivity of dimension d of representations vectors \u1fb7, \u010d. We evaluate SINKT's performance on three different numbers of d: {128, 256, 512}. The experimental re-sults on four datasets are shown in Figure 7. We suggest that the best representation dimension of SINKT is 256. However, existing"}, {"title": "5.8 Quality of the Graph Generation (RQ5)", "content": "As we generate a concept relation graph by GPT-4, it is critical to inspect the quality of the graph. To investigate the overall quality of the concept-concept graph, we compare the transition graph and our concept relation. Transition graph denotes the transition probability of concept answering, which is derived from dataset and could not be generated without training data. We demonstrates the transition graph of ASSIST09 in Figure 8a. The weight in the i-th row and the j-th column of the transition graph means the probability of concept $c_i$ appears after concept $c_j$ in the dataset. Since the learning sequence of concepts are generated by ITS's recommendation algorithm, the distribution of prerequisite con-cepts in the transition graph is relatively fixed. The concept relation graph generated by GPT-4 is shown in Figure 8b. The weight of $c_i$"}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce a structure-aware knowledge tracing framework SINKT with large language model, which are suitable for both transductive and inductive KT tasks. SINKT leverages LLMs to build structural relationships between concepts and questions and encode semantic information. To better aggregate these infor-mation, in the learning process of students, we carefully design a textual information encoder, a structural information encoder and a student state encoder. Experimental results on four real-world datasets show that SINKT outperforms current ID-based KT models on both transductive and inductive KT tasks. We also provide case studies and ablation studies to prove the effectiveness of each mod-ule in SINKT. In the future, we will try to incorporate more wealthy open-world knowledge and make more accurate predictions on inductive KT tasks."}]}