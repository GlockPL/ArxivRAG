{"title": "A Survey on Group Fairness in Federated Learning: Challenges, Taxonomy of Solutions and Directions for Future Research", "authors": ["Teresa Salazar", "Helder Ara\u00fajo", "Alberto Cano", "Pedro Henriques Abreu"], "abstract": "Group fairness in machine learning is a critical area of research focused on achieving equitable outcomes across different groups defined by sensitive attributes such as race or gender. Federated learning, a decentralized approach to training machine learning models across multiple devices or organizations without sharing raw data, amplifies the need for fairness due to the heterogeneous data distributions across clients, which can exacerbate biases. The intersection of federated learning and group fairness has attracted significant interest, with 47 research works specifically dedicated to addressing this issue. However, no dedicated survey has focused comprehensively on group fairness in federated learning. In this work, we present an in-depth survey on this topic, addressing the critical challenges and reviewing related works in the field. We create a novel taxonomy of these approaches based on key criteria such as data partitioning, location, and applied strategies. Additionally, we explore broader concerns related to this problem and investigate how different approaches handle the complexities of various sensitive groups and their intersections. Finally, we review the datasets and applications commonly used in current research. We conclude by highlighting key areas for future research, emphasizing the need for more methods to address the complexities of achieving group fairness in federated systems.", "sections": [{"title": "1 Introduction", "content": "Group fairness in machine learning refers to the principle that predictions should not prejudice unprivileged groups of the population with respect to sensitive attributes such as race or gender [1-3]. Ensuring group fairness is essential to prevent discrimination in automated decision-making processes. However, achieving group fairness is a challenging task due to the inherent biases present in data, the difficulty of balancing fairness with other objectives, and the complexity of handling intersectional group identities.\nFederated Learning (FL) is an emerging paradigm in machine learning that allows multiple clients to collaboratively train a model while keeping their data decentralized [4]. This approach is particularly beneficial for preserving privacy, as data remains on local devices rather than being centralized in one location. However, the decentralized nature of FL introduces additional challenges in achieving group fairness. The heterogeneity of data across clients and the limited visibility into the overall data distribution with respect to sensitive attributes challenge the implementation of fairness-aware algorithms in FL settings.\nThere has been a growing interest in ensuring group fairness in FL, with numerous studies proposing various techniques to address fairness issues. Despite this increased focus, there is no dedicated survey that specifically addresses group fairness in the context of FL. This survey aims to fill this gap by providing a comprehensive overview of the challenges, solutions, and future directions for achieving group fairness in FL.\nTo comprehensively review the existing literature on group fairness in FL, we employed a structured search and categorization methodology. Our search tool was Google Scholar, where we designed a query to capture a broad range of articles focused on fairness within the context of FL. This query was designed to retrieve all articles that contain the word 'Federated' in the title and include any of the terms 'fair', 'fairness', 'bias' 'equitable', or 'equal' in the title. This search yielded a total of 231 research works. We selected Google Scholar for its extensive citation network, broad coverage, and support for boolean operators in search queries, encompassing both peer-reviewed and non-peer-reviewed literature.\nThe collected research works were systematically categorized based on their nature, resulting in the following categories: articles (213 works), surveys (nine works discussed in Section 2), tutorials (one publication), posters (two works), project proposals (four works), and MSc or PhD thesis (two works). Among the 213 articles, only 47 specifically focused on group fairness, where the earliest work dates back to 2020. The remaining research works addressed other types of fairness, as discussed in Section 3.2."}, {"title": "Contributions", "content": "This paper provides a comprehensive survey of group fairness in FL by categorizing and analyzing existing approaches while highlighting key challenges and identifying future research directions. Our main contributions are as follows:\n\u2022 Overview of Challenges: We detail the unique challenges of achieving group fairness in FL, such as the complexities involved in preserving client privacy concerning sensitive attributes and managing heterogeneous data distributions. These factors make achieving group fairness in FL significantly more challenging than in traditional centralized learning systems.\n\u2022 Development of a Taxonomy of Approaches: We develop the first taxonomy of group fairness approaches in FL, structured around six critical dimensions: (1) Data Partitioning: how data is partitioned among clients; (2) Location: where the fairness mechanism is implemented; (3) Strategies: specific techniques employed to achieve group fairness; (4) Concerns: broader issues associated with achieving group fairness in FL; (5) Sensitive Attributes: how different approaches manage sensitive groups and their intersections to ensure equitable outcomes; (6) Datasets and Applications: the datasets and application domains commonly used in fair FL studies.\n\u2022 Identification of Research Gaps: We identify critical gaps in the existing literature, analysing areas that warrant further investigation, such as managing intersectionality, developing frameworks for studying group fairness in FL, and addressing challenges in less explored areas.\nThe remainder of this work is structured as follows: Section 2 discusses the related surveys, Section 3 provides the background on group fairness and FL, Section 4 discusses the challenges of achieving group fairness in FL, Sections 5, 6, 7, 8, 9 and 10 discuss the current works based on data partition, location, strategies, concerns, sensitive attributes, datasets and applications, Section 11 explores future directions for research in this area, and Section 12 presents the conclusions of this work."}, {"title": "2 Related Work", "content": "We review existing surveys and research on fairness in FL, highlighting the gap in detailed coverage of group fairness. First, we discuss works that, while addressing multiple types of fairness (discussed in Section 3.2), do not explore the specifics of group fairness. Chen et al. [5] provide a survey that addresses privacy and other types of fairness, exploring the trade-offs between them. Rafi et al. [6] similarly focus on both fairness and privacy, without delving deeply into the specifics of group fairness. Huang et al. [7] discuss generalization, robustness, and fairness in FL, but their coverage is limited to collaboration fairness and performance fairness, excluding group fairness. Shi et al. [8] and Vucinich et al. [9] focus on various notions of fairness in FL. These surveys do not extensively explore the intricacies of group fairness, such as handling multiple sensitive attributes, multi-valued attributes, and intersectionality. Additionally, the mechanisms for ensuring group fairness are inherently different from those used to achieve other types of fairness, as mentioned in Section 3.2.\nOverall, existing surveys either cover a broader range of fairness notions without detailed exploration of group fairness or focus on specific applications. This underscores the need for a dedicated survey on group fairness in FL, which we aim to provide in this work. We are the first to create a taxonomy of the existing literature according to different criteria such as data partitioning, location, and strategies. Additionally, we address broader concerns, including the complexities of sensitive groups and their intersections, as well as other critical issues in the field. Finally, we identify gaps in the current state-of-the-art and offer suggestions for future research, clearly distinguishing our work from previous surveys."}, {"title": "3 Background", "content": "In this section, we provide the necessary background on FL and fairness, which is essential for understanding the remainder of this survey."}, {"title": "3.1 Federated Learning", "content": "Federated Learning is an approach to training machine learning models that enables multiple devices or organizations to collaborate without sharing their raw data [4, 15]. This paradigm was introduced to address privacy concerns, data security issues and high communication costs associated with traditional centralized machine learning methods, where data from various sources is aggregated in a single location for training.\nIn FL, the model training process is decentralized. Each participating device, referred to as a client, downloads a global model from a central server. The client then trains the model locally using its own data and subsequently sends only the updated model updates back to the server. The central server aggregates these updates from all clients to improve the global model. This iterative process continues until the model converges.\nFederated Averaging (FedAvg) [4] was the first FL algorithm to be introduced, in which the central server computes a weighted average of the updates. At its core, FedAvg operates through a series of coordinated steps between a central server and multiple participating clients.\nFigure 1 presents a diagram demonstrating the FedAvg algorithm with K clients participating in the federation. Initially, the server initializes the global model, denoted as 00. Each client selected for participation at each communication round t performs a local update to refine the global model based on its own local dataset. This local update is achieved by minimizing a local loss function Fk (0t) using gradient descent or other optimization algorithm, where Ot represents the current global model at communication round t, and y denotes the learning rate. The updated model for client k is computed as $O_{k}^{t+1}=O^{t} - \\eta \\nabla F_{k}(O^{t})$.\nSubsequently, the server aggregates these updated models from all participating clients to generate an improved global model for the next communication round. The aggregation process involves computing a weighted average of the local updates, where the weights are proportional to the number of datapoints nk held by each client k. Formally, the aggregated model Ot+1 at communication t + 1 is computed as:\n$O^{t+1} = \\sum_{k=1}^{K} \\frac{n_{k}}{n} O_{k}^{t+1},$ (1)\nwhere n is the total number of datapoints across all clients. This process is repeated for several communication rounds.\nIn terms of the overall objective, the goal of the FedAvg algorithm is to minimize a global objective function F(\u03b8), which is defined as the weighted sum of local objective functions across all participating clients. Formally, the global objective function is given by:\n$min F(O) = \\sum_{k=1}^{K} \\frac{n_{k}}{n} F_{k}(O)$ (2)\nwhere K is the total number of clients, nk is the number of datapoints of client k, $n = \\sum_{k=1}^{K}n_{k}$ is the total number of datapoints across all clients, and Frk(0) is the local objective function for client k [4].\nThe local objective function Fk (0) for client k can be written as:\n$F_{k}(O) = \\frac{1}{n_{k}} \\sum_{i\\in D_{k}} \\ell_{i} (O)$, (3)"}, {"title": "3.1.1 Advantages", "content": "FL offers substantial advantages that make it a compelling approach for training machine learning models across distributed environments."}, {"title": "Enhanced Privacy and Security", "content": "By keeping data on local devices and only sharing model updates, this decentralized approach ensures that sensitive information remains under the control of individual clients, thereby enhancing data privacy [15, 16]. Moreover, FL frameworks often incorporate encryption and differential privacy techniques to further safeguard data during the aggregation process [17]."}, {"title": "Reduced Latency", "content": "Local processing can lead to faster model training, which is specially useful in applications where timely responses are critical [18]. This reduction in latency is particularly advantageous in edge computing scenarios, where data processing occurs closer to the source, minimizing delays associated with data transmission to a centralized server."}, {"title": "Flexible Scalability", "content": "FL's distributed framework enables efficient utilization of the limited computational resources available across numerous devices spread across various geographical locations [19]. With the growing capabilities of edge devices and the increasing volume of individual data, centralizing all data to a single server can result in under-utilization of edge computing power. This approach, therefore, overcomes scalability challenges by parallelizing the computation across multiple devices."}, {"title": "Regulatory Compliance", "content": "FL aligns well with data protection regulations by ensuring that sensitive data does not leave its original location. This compliance with regulatory frameworks such as GDPR (General Data Protection Regulation) [20] in Europe and HIPAA (Health Insurance Portability and Accountability Act) [21] in the United States is important for organizations handling sensitive data. By maintaining data locality and minimizing data transfers, FL facilitates compliance with these regulations."}, {"title": "3.1.2 Challenges", "content": "Despite its promising benefits, FL encounters several significant challenges that should be addressed to achieve its full potential across various applications."}, {"title": "Communication Overhead", "content": "Frequent transmission of model updates between local devices and the central server can lead to significant communication costs. This overhead arises from the need to synchronize and aggregate updates from multiple clients, especially in large-scale FL setups. Efficient communication protocols can be used to mitigate these costs and ensure the scalability of FL [22]."}, {"title": "Data Heterogeneity", "content": "Data across clients can be non-IID (not Independent and Identically Distributed), leading to challenges in model convergence and performance [15]. Non-IID means that each client's dataset may not follow the same underlying distribution, and the data-points within a client's dataset may not be independent of each other. For example, one client's data could be heavily skewed toward certain classes (e.g., only images of dogs), while another client may have data biased toward entirely different classes (e.g., only images of cats). This lack of uniformity contrasts with the IID assumption in centralized learning, where data is assumed to be drawn independently from the same distribution for all clients. Addressing data heterogeneity requires adaptive algorithms that can effectively aggregate diverse data sources while preserving performance across the federated network."}, {"title": "System Heterogeneity", "content": "Clients may have varying computational capabilities, network connectivity, and energy resources, complicating the coordination of the training process. This system heterogeneity introduces challenges in resource allocation and workload management across FL systems [23, 24]. Adaptive scheduling algorithms and resource-aware optimization strategies can be used to ensure equitable participation and efficient utilization of client resources."}, {"title": "Privacy and Security Risks", "content": "While FL removes the need for direct data sharing, it remains vulnerable to several privacy and security threats [25, 26]. Model inference attacks, for instance, can occur when adversaries deduce sensitive information from the shared model updates. Similarly, poisoning attacks involve adversaries deliberately introducing corrupted data or malicious updates to skew the model's performance. Protecting against these threats requires robust defensive strategies to ensure the integrity and confidentiality of the FL process."}, {"title": "3.2 Types of Fairness in Federated Learning", "content": "Ensuring fairness in FL is critical due to the diverse and heterogeneous nature of both the data and the participants. Several types of fairness have been identified in FL, each addressing different aspects of fairness."}, {"title": "Group fairness", "content": "This principle promotes equity in the outcomes of machine learning models across protected and unprotected groups defined by sensitive attributes such as race, gender, or age [1-3]. In this context, each client may have data belonging to multiple sensitive groups, contrary to approaches that assume each client belongs to a single sensitive group. Group fairness is the focus of this work and more details on group fairness are presented in the next sections."}, {"title": "Individual fairness", "content": "This notion requires that similar individuals receive similar outcomes from the machine learning model [27]. In the context of FL, this means that the model should treat participants with similar characteristics similarly, regardless of the client from which their data originates."}, {"title": "Performance distribution fairness", "content": "Also known as client fairness, this principle requires that the performance of the FL model, such as accuracy, is evenly distributed across all clients [28]. This concept emphasizes the importance of uniformity in performance, ensuring that no single client is disproportionately advantaged or disadvantaged."}, {"title": "Selection fairness", "content": "This type focuses on the fairness in selecting clients to participate in the FL rounds. In each round of FL, a subset of clients is selected to update the global model. Selection fairness ensures that this process is unbiased and that all clients have an equitable opportunity to participate [29]. This is important to prevent biases that could arise from consistently selecting certain clients over others."}, {"title": "Contribution fairness", "content": "This principle is concerned with providing appropriate incentives for clients to participate in the FL process [30]. It ensures that a client's reward is proportional to its contribution to the global model. This is important for motivating clients to actively participate and contribute with high-quality data.\nEach type of fairness in FL addresses different aspects of equity and justice in the model training process. While this work primarily focuses on group fairness, it is important to understand and differentiate these types of fairness from each other. For the sake of simplicity, in the remainder of this survey, the term 'fairness' refers specifically to group fairness."}, {"title": "3.3 Group Fairness in Machine Learning", "content": "Group fairness in machine learning aims to ensure that algorithmic decisions do not disproportionately benefit or harm specific demographic groups. This involves considering sensitive attributes, which are characteristics of individuals that, when used in decision-making processes, could lead to discriminatory outcomes [1-3]. Common sensitive attributes include race, gender, age, and socioeconomic status.\nIn the context of group fairness, individuals can be categorized into protected and unprotected groups based on their sensitive attributes. Protected groups are groups of individuals who belong to categories that have historically been disadvantaged or subject to discrimination. For example, in the context of hiring practices, women might be considered a protected group if they have been historically under-represented in certain industries, such as technology or engineering. Similarly, in the context of lending or credit approval, individuals from racial minorities, such as Black or Hispanic communities, may be considered protected groups due to historical discrimination in access to financial services. On the other hand White men individuals would be considered an unprotected group in these scenarios."}, {"title": "3.3.1 Metrics", "content": "Many statistical measures of group fairness in binary classification rely on metrics that can be explained using a confusion matrix that is often used to describe the performance of a classification model [31\u201333]. Here, S represents the sensitive attribute with two groups (S = 0 and S = 1), Y represents a target class where 1 is the positive class and 0 is the negative class (e.g. receiving a loan or not), and \u0176 is the predicted class. In a confusion matrix the rows and columns represent instances of the predicted and actual classes, respectively. The confusion matrix is presented in Table 1.\nLooking at the confusion matrix, one can derive a measure of the ratio (RAT) or the difference (DIF) of True Positive Rates (TPR) between two groups, also known as Equality of Opportunity [34]:\n$\\frac{P[\\hat{Y} = 1 | S = 0, Y = 1]}{P[\\hat{Y} = 1 | S = 1, Y = 1]} OR P[\\hat{Y} = 1 | S = 0, Y = 1] \u2013 P[\\hat{Y} = 1 | S = 1, Y = 1]$ (4)\nWhen using the ratio or the difference for a specific metric, values of 1 and 0 indicate the best fairness results, respectively. Additionally, in contexts with multiple groups, it is also common to access fairness by reporting group-specific metrics individually for each group (GS), calculating the average across all groups (AVG), or analyzing disparities using standard deviation-based (STD) or variance-based (VAR) metrics.\nFairness metrics can be divided into five groups: metrics conditioned of the outcome, metrics conditioned on the decision, performance-based metrics, unconditional metrics, and loss-based metrics."}, {"title": "Conditioned on the Outcome", "content": "The definitions of fairness conditioned on the outcome, Y, can be divided into two groups. The first group is conditioned on Y = 0, and demands Equality of False Positive Rates (FPR) (also known as Predictive Equality) or Equality of True Negative Rates (TNR) between two sensitive groups. These types of metrics can be considered, for example, from the perspective of innocent defendants by requiring that individuals who do not go on to be re-arrested have the same probability of being released regardless of their sensitive attribute value.\nOn the other hand, the second group is conditioned on Y = 1, and demands Equality of True Positive Rates (TPR) (also known as Equality of Opportunity [34]) or Equality of False Negative Rates (FNR) between two sensitive groups. In particular, a widely used fairness metric, Equalized Odds [34], requires equal TPR and FNR across the different groups. These types of metrics can be considered, for example, from the perspective of people that apply to receive a loan to have the same likelihood of receiving a loan, regardless of whether they belong to the protected or unprotected group.\nThe types of metrics conditioned on the outcome are more aligned with the perspective of the population evaluated by the model as they demand that individuals who are similar with respect to their outcomes be treated similarly [35]."}, {"title": "Conditioned on the Decision", "content": "The definitions of fairness conditioned on the decision, \u0176, can be divided into two groups. The first group is conditioned on Y = 0, and demands Equality of False Omission Rates (FOR) or Equality of Negative Predictive Values (NPV) between two sensitive groups. These types of metrics can be considered, for example, for requiring that individuals who were granted a loan to have the same probability to default, regardless of whether they belong to the protected or unprotected group.\nOn the other hand, the second group is conditioned on \u0176 = 1, and demands Equality of Positive Predictive Values (PPV) (also known as Predictive Parity [36]) or Equality of False Discovery Rates (FDR) between two sensitive groups. These types of metrics can be considered, for example, for requiring that people who were classified as criminals to have the same probability of being a criminal, regardless of their sensitive attribute value.\nThe types of metrics conditioned on the decision reflect fairness in a way that individuals with the same decision would have had similar outcomes, regardless of whether they belonged to the protect or unprotected group [35]."}, {"title": "Performance-based", "content": "Performance-based fairness metrics are derived from the confusion matrix but are not conditioned on solely the outcome or the decision. For instance, Overall Accuracy Equality [37] is achieved when the prediction accuracy is equal across groups, meaning that the probability of correctly classifying an individual (whether they belong to the positive or negative class) is the same for all sensitive groups. Another metric is F1-score Equality, which requires the F1-score, a balance between precision and recall, to be equal across groups. This ensures that the trade-off between false positives and false negatives is the same for all sensitive groups.\nThese performance-based metrics assess fairness by ensuring that the model's overall predictive performance does not disproportionately favor any particular group."}, {"title": "Unconditional", "content": "Unconditional fairness metrics, such as Statistical Parity (SP), do not rely on conditioning on either the outcome (Y) or the decision (\u0176). Instead, they evaluate fairness by comparing the overall rates of positive outcomes between protected and unprotected groups. Statistical Parity, for example, requires that the proportion of individuals receiving a positive decision (e.g., being hired, receiving a loan) is equal across groups, regardless of their underlying qualifications or outcomes [1].\nFor instance, in a hiring context, Statistical Parity would demand that the proportion of hires from a protected group be the same as that from an unprotected group, without factoring in their specific qualifications or success in the role. This kind of fairness is often employed in settings where equal access or representation is a priority.\nUnconditional fairness metrics are sometimes criticized for ignoring individual merit, but they are valuable in contexts where the goal is to ensure equitable representation or mitigate systemic biases in decision-making processes."}, {"title": "Loss-based", "content": "Loss-based fairness metrics focus on ensuring similar losses with respect to a loss function for both protected and unprotected groups. These metrics are commonly used during the training phase of machine learning models to actively guide the learning process toward fair outcomes. Although not as common, researchers also report these metrics during the evaluation phase."}, {"title": "3.3.2 Approaches for Achieving Group Fairness in Machine Learning", "content": "Approaches to achieve group fairness in centralized machine learning are usually categorized into three main types, according to the stage in which they are performed: pre-processing, in-processing, and post-processing [1]."}, {"title": "Pre-processing Approaches", "content": "Pre-processing approaches aim to mitigate bias before the model training phase. This involves modifying the training data to achieve fairness [38-40]. Techniques include re-sampling [38], where the dataset is adjusted by oversampling under-represented groups or undersampling over-represented groups to balance the data distribution. Relabeling is another technique that involves modifying the labels in the dataset to reduce bias [39]. Another technique is fair representation learning [40], which aims to learn new representations of the data that are invariant to sensitive attributes while preserving essential information for prediction tasks."}, {"title": "In-processing Approaches", "content": "In-processing approaches incorporate fairness objectives directly into the model training process [41-43]. One technique is the inclusion of fairness constraints, where constraints are added to the optimization problem to ensure that the model's predictions satisfy certain fairness criteria [41]. Adversarial debiasing uses adversarial training to remove bias by training a model that predicts the target variable while an adversary tries to predict the sensitive attribute from the model's predictions [42]. Fair regularization involves incorporating regularization terms into the loss function to penalize unfair outcomes [43]."}, {"title": "Post-processing Approaches", "content": "Post-processing approaches modify the model's predictions to achieve fairness after the model has been trained [44, 45]. This can involve techniques such as threshold adjustment, where decision thresholds are adjusted for different demographic groups to equalize outcomes.\nIn decentralized machine learning, specifically FL, achieving fairness is more complex due to the involvement of multiple clients and a central server. Approaches can be applied at the server, at the client, or using a hybrid strategy. The next sections detail the approaches for achieving group fairness in FL."}, {"title": "4 Challenges", "content": "Achieving group fairness in FL poses several unique challenges compared to centralized machine learning, primarily due to its decentralized nature and the intrinsic characteristics of federated systems. Below, we discuss some of the key challenges in developing fairness-aware algorithms in FL, which extend the challenges of FL detailed in Section 3.1.2."}, {"title": "Data Heterogeneity", "content": "In centralized machine learning, training data is often assumed to be IID, meaning that each datapoint is drawn from the same distribution and that the datapoints are statistically independent of one another. However, this assumption does not hold in FL settings, where each client has its own private dataset that may not be representative of the global data distribution [46, 47]. This non-IID nature of data across clients can lead to the introduction or exacerbation of biases in the global model. If clients have data that is not representative of the whole population, their contributions to the model could result in biased updates that do not generalize well across all groups. This can negatively impact the performance of the global model, particularly for protected groups, leading to a model that may perform well for some populations while failing to provide equitable outcomes for others [48, 49]."}, {"title": "Restricted Information", "content": "FL requires training data to be locally stored on clients' devices to protect privacy, which means that the central server cannot access raw training data or sensitive attributes directly [4]. This restriction limits the ability to apply fairness-aware techniques that rely on global information about the dataset. For instance, in centralized machine learning, algorithms can directly manipulate data or model parameters to mitigate biases by leveraging knowledge about sensitive attributes. In contrast, FL requires innovative methods to ensure fairness without direct access to such detailed information."}, {"title": "Aggregation Algorithms", "content": "The aggregation process in FL, where the central server combines model updates from multiple clients, can introduce biases depending on the aggregation strategy used. Common aggregation methods such as FedAvg perform a weighted average of model updates, giving higher importance to updates from clients with more data. This can inadvertently exacerbate biases, particularly if clients with larger datasets do not accurately reflect the broader population, leading to the under-representation of protected groups [50]. Careful design of algorithms is needed to ensure fair representation of all groups."}, {"title": "Limited Client Participation", "content": "In FL, not all clients participate in every round of training. This selective participation can lead to biased model updates if certain clients, especially those that contain data from protected groups, are under-represented in the training process [29]. Ensuring that clients with diverse data contribute to each round is critical for maintaining group fairness across the model's predictions."}, {"title": "Resource Constraints", "content": "Clients in FL environments often have varying computational and communication resources. Devices with lower capabilities may struggle to participate fully, potentially skewing the training process towards clients with more resources [51]. This disparity can create bias in the global model if clients with less resources are systematically excluded from the training rounds. Thus, careful attention to resource constraints is essential to ensure fair contribution and representation of all groups in the FL process."}, {"title": "Long-term Fairness", "content": "Ensuring fairness not just in individual training rounds but over the long term is a significant challenge. As models are updated continuously, maintaining fairness over time requires ongoing monitoring and adjustments. This is particularly critical in dynamic environments where client distributions and data characteristics may change [52]."}, {"title": "5 Data Partition", "content": "FL can be categorized based on how data is partitioned among the clients. The three primary types of data partitioning in FL are horizontal, vertical, and transfer learning [15]. Achieving group fairness in each of these settings presents unique challenges and requires tailored solutions. In this section, we explain these three types of FL, highlighting the specific challenges and considerations for ensuring group fairness in each context.\nFigure 2 illustrates the three types of FL based on data partitioning. The figure uses a financial institution as a contextual example to demonstrate how different data types (e.g., demographics, financial history, credit scores) are distributed and processed across various FL scenarios. This illustration highlights how each type of FL handles data partitioning in scenarios where the financial institution collaborates with other entities, such as e-commerce platforms, to build models for applications such as loan and mortgage approvals."}, {"title": "5.1 Horizontal FL", "content": "Most research on group fairness in FL has traditionally focused on Horizontal FL (HFL) (except for [53]). In HFL, clients hold data with the same feature space but different instances [15]. This approach is particularly relevant in scenarios where multiple organizations or devices have similar types of data for different user groups. The goal is to ensure that the trained model maintains fairness across these diverse datasets without compromising the privacy of individual data sources.\nA real-world example of group fairness in HFL could involve financial institutions in different regions collaborating to build a fair predictive model for loan approval. Each bank has the same type of customer data, including demographics, financial history, and credit scores. The FL system would train a model ensuring that the loan approval predictions are fair across different demographic groups, such as age, gender, and ethnicity."}, {"title": "5.2 Vertical FL", "content": "The exploration of group fairness in Vertical FL (VFL) is equally important. In VFL, clients hold different subsets of features related to the same group of users [15, 54]. This makes it essential to ensure fairness across these vertical partitions, as each client contributes with unique information to the global model.\nA real-world example of group fairness in VFL might involve a collaboration between an e-commerce company and a financial institution. The e-commerce company has data on customers' purchasing behavior such as purchase frequency or average transaction value, while the financial institution has data on customers' demographics, credit scores and financial history. Together, they aim to create a fair model for loan approval. The model must ensure that it does not unfairly discriminate against customers based on sensitive attributes such as race, gender, or socioeconomic status.\nHowever, addressing group fairness in VFL poses additional challenges due to its intrinsic characteristics. Firstly, ensuring the privacy of data across all participating organizations often conflicts with the need for a unified training dataset, which is essential for implementing fairness-enhancing methods. For example, in the scenario in Figure 2, only the financial institution has access to the sensitive attributes values. Secondly, organizations involved in real-world VFL systems often have varying computational capabilities and may complete their local updates at different speeds. Requiring each organization to perform a single local update per communication round when training a fair model can lead to inefficiencies [53].\nDespite its importance, research specifically addressing group fairness in VFL remains limited, with only one work presented. Liu et al. [53] examine a VFL scenario where K data parties and a central server collaborate to train a machine learning model, with each feature vector distributed across the K data parties. They identify two types of data parties: active parties, which initiate the task and possess information about labels, sensitive attributes, and the loss function, and passive parties, which do not have access to this information. The server is assumed to have access to both the labels and sensitive attributes. To address the challenge of imbalanced computational resources, they allow each active data party to perform multiple local gradient updates in parallel before exchanging information with the server. For passive parties, a single model update is conducted between two consecutive communication rounds with the server."}, {"title": "5.3 Federated Transfer Learning", "content": "Federated Transfer Learning (FTL) is an extension of FL that leverages knowledge from a source domain to improve the learning process in a target domain where data might be scarce or unlabeled [15, 55]. In FTL, the participating clients in the source domain have abundant labeled data, while those in the target domain may have limited or no labeled data. The goal is to transfer the knowledge gained from the source domain.\nFTL is particularly useful in scenarios where direct FL might not be feasible due to the lack of adequate data in the target domain. For instance, consider a financial institution, A, that wants to develop a mortgage approval model. Some institution, B, may have extensive data on general loan applications and customer credit histories (source domain), while A might only have limited data on specific to mortgage approvals (target domain). FTL can facilitate the transfer of knowledge from the well-established credit scoring models based on general loan data to enhance the performance of models tailored for mortgage approvals, for example. This way, institutions with limited data can benefit from models trained on broader datasets.\nAchieving group fairness in FTL presents several unique challenges. Firstly, the source and target domains may have different distributions of sensitive attributes. Ensuring fairness across these domains is challenging, as the transferred knowledge might introduce or exacerbate biases in the target domain. Moreover, defining and measuring fairness in the context of FTL is complex, as the metrics used in the source domain may not be suitable for the target domain.\nDespite the importance of these challenges, no dedicated work has been proposed to address group fairness in FTL specifically. This gap highlights a significant opportunity for future research to develop novel strategies that ensure fairness in FTL while maintaining the privacy of the learning process."}, {"title": "6 Location", "content": "In this section, we introduce a novel categorization of current approaches to achieving group fairness in FL into three main types based on where the fairness operations are conducted: local methods, global methods, and a mixture of local and global methods. Each approach has its advantages and disadvantages, which are discussed in detail in the following subsections. Figure 3 presents the types of fair FL approaches categorized by location."}, {"title": "6.1 Local Solutions / Client-side Techniques", "content": "Local solutions focus on implementing fairness-aware strategies on each client independently, without direct intervention from the central server. These methods leverage the clients' local data"}]}