{"title": "SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting", "authors": ["Xinyi Liu", "Tianyi Zhang", "Matthew Johnson-Roberson", "Weiming Zhi"], "abstract": "Many recent developments for robots to represent environments have focused on photorealistic reconstructions. This paper particularly focuses on generating sequences of images from the photorealistic Gaussian Splatting models, that match instructions that are given by user-inputted language. We contribute a novel framework, SplaTraj, which formulates the generation of images within photorealistic environment representations as a continuous-time trajectory optimization problem. Costs are designed so that a camera following the trajectory poses will smoothly traverse through the environment and render the specified spatial information in a photogenic manner. This is achieved by querying a photorealistic representation with language embedding to isolate regions that correspond to the user-specified inputs. These regions are then projected to the camera's view as it moves over time and a cost is constructed. We can then apply gradient-based optimization and differentiate through the rendering to optimize the trajectory for the defined cost. The resulting trajectory moves to photogenically view each of the specified objects. We empirically evaluate our approach on a suite of environments and instructions, and demonstrate the quality of generated image sequences.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous agents operating in unknown environments need to construct internal representations of their operating environment. Traditionally, these representations, such as occupancy or semantic maps, can be difficult for the untrained eye to interpret. Recent advances in computer vision have led to the development of photorealistic 3D environment representations [1], which enable high-fidelity visualizations of the robot's surroundings. Of these representations, Gaussian Splatting models [2] have emerged as the model of choice due to their ability to render photorealistic images efficiently. There is great excitement within the community to advance methodologies for constructing Gaussian Splatting models [3] and embedding additional properties into them. Yet many unsolved challenges exist around how best to leverage these representations for downstream tasks.\nA valuable downstream task for Gaussian Splatting is to take advantage of its visual realism to extract visually accurate sequences of images. This paper tackles this challenge. Specifically, we study the problem of generating a sequence of photorealistic images from a Gaussian Splatting model that smoothly displays a sequence of objects that human users can semantically specify. In particular, we introduce the SplaTraj framework, which formulates a trajectory optimization problem that attempts to optimally position the camera, such that it smoothly moves through the reconstructed environment while accurately pointing, in order, to each of the semantically specified objects within the scene.\nSplaTraj operates on top of Gaussian Splatting models that encode visual language features into the environment reconstruction. Then, embedded semantics in the environment are compared with user-inputted language semantics to identify objects and regions within the environment that are relevant to the user's intended instruction. These identified objects are rendered into the view of the camera and carefully designed costs evaluate the quality of their visual placement. SplaTraj parameterized camera trajectories as continuous time-varying functions. By applying gradient-based optimizers to the cost-optimal motion trajectory, we can differentiate through both the rendering equation and the trajectory parameterization. This enables us to obtain a model that describes the optimal camera motion through the environment so that we can photogenically capture all of the objects specified by the user. A simple overview is illustrated in fig. 1.\nConcretely, our technical contributions are:\n\u2022 The SplaTraj framework, which formulates generating image sequences as a continuous trajectory optimization problem over camera poses;\n\u2022 Methodology to extract environment structure from user-inputted semantics and then incorporate the structures as an optimization cost for trajectory optimization;\n\u2022 Empirical evaluation of SplaTraj to generate images in a variety of benchmark Gaussian Splatting environments."}, {"title": "II. RELATED WORK", "content": "Robot Representations and Photorealistic Represen-tations: Robots operating in unknown environments rely on constructing internal representations of the environment. These have generally been representations of occupancy [4], [5], [6], surfaces [7], or motion patterns [8], [9]. However, such representations are often difficult for the untrained eye to interpret. This has led to photorealistic representations, such as Neural Radiance Fields (NeRFs) [1], and more 3D Gaussian Splatting [2]. Subsequent works [10], [3], [11] in this area augment the capabilities of photorealistic reconstructions. Our work leverages efforts in embedding language features [12] into realistic representations [13].\nCamera Optimization: Our work is tangentially related to camera placement optimization, which has been an active area of research. [14] propose an automatic camera placement method for generating image-based models from scenes with known geometry. Albahri et. al [15] explore camera placement within buildings, focusing on maximizing. Another work in this space, [16] addresses the optimal placement of cameras for human activity recognition, maximizing the observability. Our work differs from this body of work in that our optimization occurs within a photorealistic reconstruction and is driven by user-provided semantics.\nTrajectory Optimization: Trajectory optimization is commonly used within motion planning [17], [18], [19] and control to obtain motion sequences to reach a certain goal. Prominent trajectory optimization methods within motion planning, including TrajOpt [20], CHOMP [21], STOMP [22]. Trajectory optimization for a finite horizon also appears within model predictive control [23]. Prominent trajectory optimization methods include MPPI [24]. Recent approaches have also been introduced [25], [26], [27], [28] to generate motions reactively. This shortens the time horizon even further. Classic trajectory optimization formulates an objective that finds a minimal distance path. Our work differs from these approaches in that we formulate a cost within trajectory optimization that optimizes object placement within the camera view, by differentiating the camera rendering."}, {"title": "III. PRELIMINARIES", "content": "A. Open Querying on Images\nOpen query refers to the ability of the model to handle arbitrary text inputs for image retrieval or classification tasks, without being limited to a predefined set of categories or labels. Contrastive Language-Image Pre-Training [29] is commonly used for tasks of this class. CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of training examples. CLIP is typically used as a plug-and-play encoder. It is trained on a large-scale dataset of 400 million image-text pairs, enabling it to learn visual concepts from natural language descriptions. In this paper, we also use the pre-trained encoder provided in CLIP to generate embeddings from semantic inputs.\nB. Gaussian Splatting and Language Field\nGaussian Splatting[2] explicitly represents a 3D scene as a collection of anisotropic 3D Gaussians, with each Gaussian $G(x)$ characterized by a mean $\\mu \\in R^3$ and a co-variance matrix $\\Sigma$:\n$G(x) = \\frac{1}{2\\pi \\sqrt{\\Sigma}} exp(-\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x-\\mu))$\nFor each camera pose in the image sequence, an image can be synthesized from the following rendering equation [2]:\n$\\hat{I} = \\sum_{i \\in N} c^i\\alpha^i \\prod_{j=1}^{i-1} (1 - \\alpha^j)$\nwhere $c^i$ is the color of the i-th Gaussian, $N$ denotes the Gaussians in the tile, and $\\alpha = \\sigma^iG_{2D}(v)$. Here $\\sigma$ is the opacity of the i th Gaussian and $G_{2D}(\\cdot)$ represents the function of the i-th Gaussian projected onto 2D. Recent research has enhanced 3D Gaussians with language features [30][13].\n$\\hat{I}_l = \\sum_{i \\in N} l^i\\alpha^i \\prod_{j=1}^{i-1} (1 - \\alpha^j)$\nwhere $l^i$ is the open-vocabulary feature embedding of i-th 3D gaussian primitives and $\\hat{I}_l$ represents the rendered open-vocabulary feature embedding at pixel $u$. The intuition here is simple: positions in the scene are mapped to language features rather than colors. We use upper script index notation specifically for the 3D Gaussian index, to distinguish it from other index notations in the paper."}, {"title": "IV. METHODOLOGY", "content": "In this section, we proposed our differentiable optimization method to solve photogenic trajectory generation of the open-queried objects represented in 3D Gaussians. The overall framework is shown in Fig. 2.\nA. Problem Formulation\nThis paper addresses the optimization problem of generating realistic camera trajectories in environments represented by language-annotated 3D Gaussians [13]. Given a user-specified query of various modalities such as text, $Q = (q_1,q_2,..., q_n)$, our goal is to determine a trajectory of camera pose $\\Phi \\in SE(3)$ over a normalized period $t \\in [0, 1]$. The camera poses should sequentially focus on the objects corresponding to each $w_i$ within the specific time intervals $T_i = [\\frac{i-1}{n}, \\frac{i}{n}]$ $(\\cup_{i=1,N} T_i = [0,1]; T_i = 0)$. Our objective is to render photogenic videos during these intervals.\nIn this context, a photo is considered more photogenic if the specified object is more centered in the image and occupies a portion closer to a user-defined ratio. Formally, we seek to optimize the camera trajectory such that each object $w_i$ is appropriately captured in its designated time slot, resulting in a series of well-composed images.\nThe challenge lies in dynamically adjusting the camera trajectory to meet these criteria while smoothly transitioning between different objects in the sequence.\nB. Semantic Map Extraction\nWe seek to ground semantic specifications given by the user to physical coordinates within our splatting model. We leverage the semantic information encoded in the 3D Gaussians to generate a differentiable object mask for each word query, following a similar procedure in [31].\nThe relevancy score on a Gaussian $G_i$ is defined as a softmax score on the\n$S(G^i) = \\frac{exp(w \\cdot q_{qry})}{min \\{ exp(w \\cdot q_{qry}) + exp(w \\cdot q_{canon}) \\}}$\nwhere $q_{canon}$ is the CLIP embeddings of a predefined canonical phrase chosen from \u201cobject\u201d, \u201cthings", "stuff\", and \u201ctexture\". Experimentally, we found that a DBScan algorithm [32": "on top relevancy score percentile Gaussians can filter out irrelevant noises in 3D space. The relevancy score suffered from the score scale ambiguity and spread outliers in 3D space, which would cause unclear mask edges and outliers in the rendering result, leading to downstream failure. A binary channel for object prompt $q_i$ based on the original 3D Gaussian field is attained in the filtering procedure, where the jth entry of the $q_i$ binary channel is $b^j = I\\{G_i \\in N_i\\}$, where $I\\{A\\}$ is the indicator of event A, $N_i$ is the set of Gaussians after filtering of prompt $q_i$.\nWe rendered 2D binary mask via the following rendering equation:\n$I_b = \\sum_{i \\in N} b^i \\alpha^i \\prod_{j=1}^{i-1} (1 - \\alpha^j)$.\nBy rendering the mask onto the camera view, we can reason about properties in the camera's view. This includes the ratio of the object within the frame, giving an indication of how photogenic the rendered image is.\nC. Continuous Trajectory Representation\nMany trajectory optimization problems express trajectories as a sequence of waypoints. This typically requires a priori discretization of the trajectory at some fixed time resolution. Here, we take an alternative approach and represent the motion trajectory of the camera as a continuous function, mapping from a normalized time parameter to the camera pose. This enables us to generate trajectories of any desired resolution.\nTo represent a camera trajectory vector function in 6 dimensions, we use 6 independent functions to model each variable of\n$\\Phi(t) = [r_x(t), r_y(t), r_z(t), x(t), y(t), z(t)]$,\nwhere $r_x, r_y, r_z$ are rotation vectors of transformation from the world coordinate to the camera coordinate, $x, y, z$ are translation components of $t \\in [0,1]$. We represent this function concisely as a combination of Squared Exponential Radial Basis Functions (RBFs). Specifically, the j-th entry of $\\Phi(t)$ could be represented by a weighted sum of RBFs:\n$[\\Phi(t)]_j = \\sum_{i=1}^{N} w_i\\psi_i(t) = w_j^T\\Psi(t)$,\nwhere $w_i = (w_1, w_2, ..., w_n)$ is the weight vector associated with the j-th entry, and $\\Psi(t) = (\\psi_1(t), \\psi_2(t), ..., \\psi_n(t))$ is the packed RBFs, assigned with entry-specific weight vector $w_j$. We use the Gaussian function as our basis function as\n$\\psi_i (t) = exp(-\\frac{(t-t_i)^2}{2\\sigma^2})$\nwhere $t_i = \\frac{i}{n}, i = 1, 2, ..., n$ is the center of the time interval $T_i$, hyper-parameter $\\sigma$ is the standard deviation of the Gaussian distribution. This representation offers a smooth and flexible trajectory generation by interpolating between control points with a smooth basis function. Notably, our trajectory formulation enables us to query and obtain a camera pose at arbitrary time and at arbitrary time resolution.\nD. Cost Function\nWe constructed an optimization problem to generate trajectories that minimize the defined cost so that the camera can sequentially fixate on objects corresponding to language inputs given by the user $Q = (q_1,q_2,..., q_n)$, we are optimizing the following cost function, defined as a piece-wise function distributing each object prompt to the time interval $T_i = [\\frac{i-1}{n}, \\frac{i}{n}]$, specifically,\n$arg \\min_w \\sum_{i=1}^{n} \\int_{T_i} L_{q_i}(w(t)) dt$,\nwhere $L_{q_i}(\\cdot)$ is the cost for prompt word query $q_i$. To obtain the cost, we shall render the identified semantically-relevant 3D structure to the camera's view to obtain a binary mask. We denote the mask as $I_b$ and compute by evaluating eq. (3). We define $L_{q_i}$ as the sum of multiple cost terms,\n$L_{q_i} = L_{TCE} + L_{TRE} + L_{upright} + \\alpha L_{prior}$.\nHere, $\\alpha$ is a weight of the cost term $L_{prior}$ which gradually decays. The definitions of each cost term are elaborated below.\nThe rendering-based cost includes the center cost and the ratio cost. Here we render of object that we identify as relevant to the user's prompt as a binary mask, denoted $I_b$ and computed by evaluating eq. (3). Then $L_{q_i}$ is the sum of the following terms:\n1) Target Centralizing Error (TCE) Cost:\n$L_{TCE}(I_b) = ||(\\frac{c_x}{H} - \\frac{1}{2}, \\frac{c_y}{W} - \\frac{1}{2})||_2$,\nwhere $(c_x, c_y) = center(I_b)$ are pixel coordinates of the center of the mask.\n2) Target Ratio Error (TRE) Cost:\n$L_{TRE}(I_b) = ||\\frac{sum(I_b)}{HW} - r_t||_2$,\nwhere $sum(I_b)$ counts the area of the 2D binary mask $I_b$.\n3) Uprightness Cost: The camera's orientation can be regulated to an upright position by the following cost term\n$L_{upright} = <e_z, R e_x>$,\nWhere we use the inner product between the world unit upward direction axis $e_z$ and the camera's unit upright axis $R e_x$ where $e$ are unit coordinate vector and $R$ denotes the rotation from camera frame to world frame.\n4) Diminishing Prior Cost: We warm-start the optimization via a prior cost which guides the camera toward the physical adjacency of the target objects. This is given:\n$L_{prior} = - max(r, \\frac{(c - o)^T c}{||c||_2}) \\frac{(c - o)^T c}{||c-o||_2}$,\nThe first term in the $L_{prior}$ calculates a cost based on the proximity of the camera to the object, with a minimum distance threshold defined by the radius; The second term of the $L_{prior}$ calculates the angle deviation from the object, specifically measured by the inner product between camera direction unit vector with the normalized vector pointing from object to the camera center.\nDuring optimization, we utilize the differential renderer to calculate the rendering-based cost $L_{TCE}, L_{TRE}$ and $L_{IOU}$, whose gradient with respect to trajectory coefficient $\\frac{\\partial L}{\\partial w}$ can be attained from the differential renderer. We warm-start the progress by giving an exponentially decaying coefficient to differentiable prior term $L_{prior}$. The total cost gradient with respect to trajectory coefficient $w$ would update the trajectory coefficient via Adam optimizer[33], iteratively refining the camera trajectory."}, {"title": "V. EMPIRICAL RESULTS", "content": "A. Experiments Overview\nIn this section, we seek to investigate empirically:\n\u2022 Whether the image cost design leads to object-centered, properly distanced, and occlusion-avoiding rendering images; (Section V-B)\nB. Single Pose Optimization\nTo ensure that the pose-wise cost function components are correct, we first design a single pose optimization to validate our method. Instead of optimizing the trajectory weight coefficient, we optimize on individual poses, using the same cost function defined in section IV. The objective function is therefore reduced to arg min$\\Phi$ $L_{q_i}(\\Phi)$, where $\\Phi = [r_x, r_y, r_z, x, y, z]$ is the optimization variable.\nWith each selected query prompt $q_i$, we preprocess the scene dataset by extracting the semantic mask. During optimization, we calculate the cost value and gradient of the current pose and run gradient descent on the cost manifold $L_{q_i}$. We use the ADAM optimizer to evaluate the optimization result's TCE, TRE, and IoU score after 400 iterations.\nWe also did an ablation study to show the effect of each optimization cost term, as shown in table I. We first show theimpact of 3D prior terms, which includes all terms derived from 3D prior information without decaying weights; We then apply TCE and TRE regulating terms separately and jointly, with 3D prior terms with diminishing weight through steps. Our results show:\n\u2022 TCE cost term applied in the optimization can centralize objects better, and enabling centering cost or ratio regulation cost might improve the performance of the corresponding aspect.\n\u2022 The benefit of TCE and TRE can be additive, and combining the two regulating terms generally finds the best pose.\n\u2022 The IoU alone does not significantly improve the overall performance. In the experiment, we observe the non-smoothness of the IoU cost function, leading to huge steps in gradient descent that lead out of the low-cost region, similar issues have been discussed in the object detection field [34].\nC. Trajectory Optimization\nWe evaluate the qualities of trajectories obtained via SplaTraj. We compare our trajectory formulation against two other baseline trajectory representations. The experiment follows a similar procedure as the single pose experiment in V-B. In each experiment, we provide text prompt instruction queries to visit 3 objects within each scene. Renderings\nMetrics: We evaluate the quality of the image sequence and how smoothly the motion trajectory moves through the environment reconstruction. Intersection of Union: Intersection of Union (IoU) is a metric for ensuring that the camera's path is optimized to capture objects accurately and consistently. The IoU score is calculated as the ratio of the area of intersection between the predicted and ground truth bounding boxes to the area of their union. This score ranges from 0 to 1, where a score of 1 indicates a perfect overlap and a score of 0 indicates no overlap at all. Log Dimensionless Jerk: Log Dimensionless Jerk (LDJ) [37] is a measure used to quantify the smoothness of a motion trajectory, by considering the third derivative of motion. We seek to ensure that the generated trajectory moves smoothly across the scene.\nD. Analysis of Empirical Results\nWe provide an analysis of our results against our baselines.\nWaypoint: The waypoint representation results in overfitting it causes discontinuities between intervals. This occurs because randomly initialized waypoints struggle to converge to a single optimal pose when multiple optima exist in the cost manifold. This can result in larger angular and positional jerks, leading to non-smooth trajectories that may cause failures in downstream robotics tasks.\nPolynomial: While polynomial representation offers smoother trajectories compared to other methods, the limited variability of the sixth-order polynomial trajectory often results in sub-optimal poses. Experiments show that the angular and positional variations in optimized polynomial trajectories are insufficient to avoid occlusions, as indicated by the low IoU score in Table II. Additionally, the noticeable stillness at the beginning and rapid motion near each trajectory's end, deteriorate the quality of the video.\nRadial Basis Function: In contrast, the RBF representation produces smooth and object-oriented trajectories. The high average IoU score suggests that RBF effectively finds occlusion-avoiding poses. In contrast, the low average ratio error and centering error indicate that the trajectory is well-centered around the corresponding objects. Rendered images and 3D trajectories from each scenario, as visualized in Fig. 3, highlighted the trajectory generated by RBF. Compared to waypoint and polynomial representations, the RBF representation successfully provides trajectory transverse along the low-cost region defined by the optimization goal with a smooth transition between different objects."}, {"title": "VI. CONCLUSIONS", "content": "In this work, we proposed SplaTraj, a framework that formalizes image sequence generation as a trajectory optimization problem. Specifically, SplaTraj enables users to specify regions and objects for the camera to visit semantically. We design costs on these 3D structures rendered to the view of a camera, which determines the desired positions within an image of the renderings. The trajectory optimizer subsequently differentiates through the rendering function and solves to obtain a continuous-time trajectory of camera poses. We empirically demonstrated that the rendering cost defined successfully leads to object-centered, properly distanced, and occlusion-avoiding rendered images, over multiple Gaussian Splatting scenes. Future avenues for research include adding further kinematics constraints into the trajectory optimization and extending SplaTraj to work in time-varying and dynamic Gaussian Splatting models."}]}