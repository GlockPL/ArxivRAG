{"title": "Contrasting Attitudes Towards Current and Future AI Applications for Computerised Interpretation of ECG: A Clinical Stakeholder Interview Study", "authors": ["Lukas Hughes-Noehrer", "Leda Channer", "Gabriel Strain", "Gregory Yates", "Richard Body", "Caroline Jay"], "abstract": "Objectives: To investigate clinicians' attitudes towards current automated interpretation of ECG and novel Al technologies and their perception of computer-assisted interpretation.\nMaterials and Methods: We conducted a series of interviews with clinicians in the UK. Our study: (i) explores the potential for Al, specifically future 'human-like' computing approaches, to facilitate ECG interpretation and support clinical decision making, and (ii) elicits their opinions about the importance of explainability and trustworthiness of Al algorithms.\nResults: We performed inductive thematic analysis on interview transcriptions from 23 clinicians and identified the following themes: (i) a lack of trust in current systems, (ii) positive attitudes towards future Al applications and requirements for these, (iii) the relationship between the accuracy and explainability of algorithms, and (iv) opinions on education, possible deskilling, and the impact of Al on clinical competencies.\nDiscussion: Clinicians do not trust current computerised methods, but welcome future 'Al' technologies. Where clinicians trust future Al interpretation to be accurate, they are less concerned that it is explainable. They also preferred ECG interpretation that demonstrated the results of the algorithm visually. Whilst clinicians do not fear job losses, they are concerned about deskilling and the need to educate the workforce to use Al responsibly.\nConclusion: Clinicians are positive about the future application of Al in clinical decision-making. Accuracy is a key factor of uptake and visualisations are preferred over current computerised methods. This is viewed as a potential means of training and upskilling, in contrast to the deskilling that automation might be perceived to bring.", "sections": [{"title": "INTRODUCTION", "content": "Heart and circulatory diseases cause approximately 480 deaths per day in the United Kingdom alone [1]. Electrocardiograms (ECG) support the diagnosis of cardiac abnormalities based on the heart's electrical activity. Using algorithms to assist the interpretation of ECGs dates back to the 1950s [2], and they are now a staple functionality of ECG monitoring.\nComputerised interpretation of ECG (CIE) is designed to support clinical decision-making, reduce diagnostic errors, and improve patient outcomes. Whilst the performance and accuracy of these algorithms has improved considerably over the years, studies have shown that contemporary methods are still error prone, requiring oversight from a clinician to avoid misdiagnosis [3, 4]. With the establishment of large waveform datasets and advances in Artificial Intelligence (AI) and Machine Learning (ML) techniques, methods to process and interpret ECGs now range from rule-based systems to deep learning approaches [5, 6, 7].\nWhilst AI applications have been advancing the field of CIE [8, 9, 10], they do not necessarily meet clinical needs, impeding wider adoption [11, 12, 13]. Reasons for non-adoption of AI applications in clinical practice include issues of systemic bias, open questions around liability, cyber security concerns, unresolved technical challenges, risks of AI exacerbating health inequities, and ethical and regulatory challenges [14, 15]. Furthermore, there is a lack of real-world evidence for service providers, such as the UK's National Health Service (NHS) [16, 17], and more broadly, there are insufficient governmental policies and economic incentives [18] to drive wider uptake of AI applications in the health domain.\nA key issue is a lack of trust in AI systems [19, 20]. A mixed-methods study investigating AI in cardiovascular medicine concluded that clinicians generally feel positive towards AI and its future potential, but also highlighted several barriers, including limited trust in its output, high costs, and insufficient usability [21]. An interview study conducted with Australian emergency physicians concluded that clinicians generally believe that AI will have an impact on their field within the next decade, but further work is necessary to improve its acceptance [22]. Explainable algorithms based on human-like computing have shown efficacy in making complex ECG data easier to interpret for lay people and medical professionals alike [23]. A study investigating explainable ML models indicated clinicians felt generally positive towards AI, but noted issues such as confirmation bias and the cognitive complexity of reasoning with current explainable algorithms [24].\nSuccessful implementation of ML-driven Decision-Support Systems requires alignment of the AI application with clinicians' expectations by bridging the 'sociotechnical gap' [25]. Expectations of developers and clinicians often differ in terms of their understanding of concepts such as explainability, and common frameworks could help to overcome barriers and find agreement between stakeholders [26, 27]. A study investigating radiologists' and computer scientists' views on AI highlighted significant differences between these groups, with the latter generally predicting faster implementation and a greater impact [28]. AI applications must be trusted by all stakeholders to ensure an inclusive, ethical, and sustained implementation [29].\nHere, we present a qualitative interview study conducted in the course of our clinical stakeholder engagement to develop a novel 'human-like' AI algorithm for ECG interpretation. Our algorithm is based on cognitive fit theory [30] and provides a novel visualisation of ST-elevation myocardial infarction (STEMI) to enhance its recognition by healthcare professionals and lay people. We interviewed 23 clinicians across three specialties (emergency medicine; anaesthesia and critical care; and cardiology), covering all training stages of the UK medical system. We also included general practitioners (GP) and junior doctors in their foundation training. This paper examines the attitudes of clinicians towards current computer-automated ECG interpretation and also novel AI algorithms that could be used to support future diagnosis. Our research contributes to the understanding of the clinical use of these technologies and how they are perceived by those clinicians who use ECGs on a daily basis. The results demonstrate an interesting paradox. Whilst current automated diagnoses are often ignored due to concerns about their accuracy and trustworthiness, and clinicians are concerned that the use of them may lead to deskilling and a negative impact on training, they are positive about future AI applications. So long as algorithms are accurate, clinicians are unconcerned with the explainability of future AI systems. We discuss the relationship between accuracy"}, {"title": "METHODS", "content": "This paper considers the experiences of clinicians with automated ECG interpretation and their attitudes towards novel \u0391\u0399 techniques that could be used to facilitate interpretation of ECGs in the future. Participants were sampled across three specialties, GPs, and junior doctors. All clinicians were based in the UK at the time of interviewing. These specialties were selected after a series of Responsible Research and Innovation (RRI) workshops, which identified these groups as regularly interpreting ECGs.\nThe final selection was agreed by the research team formed of clinicians, Human-Computer Interaction researchers, and an RRI specialist. The design of the study was informed by the Standards for Reporting Qualitative Research (SRQR) to ensure rigorous data collection and reporting [31]. Based on the Ethics Decision Tool at [omitted] and the NHS Health Research Authority Ethics Toolkit [32], the study was exempt from ethical review as we solely interviewed professionals (i) strictly within their professional remit, and (ii) did not collect any person identifiable information apart from those on the exempt list, i.e. names, professional roles, signed consent, and audio recordings. An outcome letter is available at Figshare [omitted].\nThe interviews were undertaken between December 2023 and April 2024 either via the video conferencing software Microsoft Teams [33] (video function disabled) or in-person at [omitted] and [omitted]. Participants were convenience sampled [34] on the basis that they are currently employed clinicians with \"experiential relevance\" [35, p. 124] in terms of ECG usage and clinical knowledge. Although most participants provided written consent for attribution, as we aimed to generate data that establishes a representational account of the interviewees' professional roles, we refrain from including the names of participants. All participants received a Participant Information Sheet (PIS) prior to the interview and provided informed consent. Participants were further given the option of withdrawing data up to 14 days after the date of participation. People received a shopping voucher worth Fifty Pound Sterling (GBP) as a token of appreciation for their time investment.\nInterviews followed an interview guide [36] based on generative questions [37] to encourage extensive replies in an open format. The topic guide was split into two parts: a general part, which asked clinicians about how often they look at ECGs in their clinical practice, the methodology they use to interpret them, and the pitfalls and strengths of current systems. Part two showed clinicians visualisations (see Figure 1) based on explainable 'human-like' algorithms, to elicit their attitudes towards AI technologies and explainability of algorithms. Interviewees were also asked about how our visualisations compare to current CIE. All materials are openly available on Figshare; please refer to the Data Availability Statement.\nAll interviews were transcribed and cross-checked to ensure reliability and validity [38]. Analysis was performed using inductive thematic analysis [39] and the software NVivo 14 [40] to analyse, code, and re-code the interview data, matching sentences to the primary codes in the interview. After generation of primary codes (X.X.), authors discussed and finalised the codes (\u03a7.\u03a7., \u03a7.\u03a7. and X.X.) and attributed them to emerging themes. These themes were then re-evaluated and agreed between authors (\u03a7.\u03a7., \u03a7.\u03a7.,"}, {"title": "RESULTS", "content": "We interviewed 23 clinicians who, at time of interviews, were actively practising in the NHS. The split of specialties and training levels of interviewees can be found in Table 1. None of the interviewed participants withdrew any of the recorded data and we therefore included all 23 transcripts in the analysis. Interviews had a median length of 21 minutes (Q1: 17 min; Q3: 25 mins; IQR: 8 min). We report the results below under subheadings of identified themes."}, {"title": "Lack of trust in current systems", "content": "Clinicians have serious concerns around the usage of current CIE for clinical decision making. The majority of interviewees do not trust current systems for multiple reasons.\nMost interviewees reported negative experiences using CIE in clinical practice, with participants stating that they are \"wildly inaccurate\" (P04), \"dangerous\" (P15), and \"very unhelpful\" (P09). Computerised interpretation was also seen as lacking context to make informed decisions, leading to a lot of misinterpretations. Clinicians have also been told at different stages throughout their career to ignore them. One clinician reported that they were first told in medical school not to rely on automated interpretations of ECGs and that they should calculate everything themselves (P22). Other interviewees stated that \"probably the first thing you get taught when you interpret an ECG is to ignore the automated rule\" (P18) and that they were \"advised to not rely on them\" (\u042017).\nInterviewees also expressed that the CIE sometimes caused unease when looking at an ECG. This is due to automated read-outs suggesting diagnoses that are not present, causing added stress as the technology indicates issues that do not align with the clinician's own interpretation. P22 recalled a recent situation: \"There was a patient who had an ECG that I reviewed, and it [the ECG monitor] said 'STEMI' at the top, and for the life of me, I could not see the STEMI. And it was, it was terrible. I was terrified.\"\nA few clinicians stated that they do use current CIE, but they exercise caution and mainly compare their own findings to the computer rather than relying on them to inform their diagnosis from the start."}, {"title": "Future application of Al", "content": "Despite the shortfalls of current CIE, clinicians were broadly positive about novel AI technologies, appearing to see them as qualitatively different to current methods. All interviewees agreed that digital technologies will positively impact the way we record and interpret ECGs in the future. A qualification to this optimism was that deployment of AI technologies to facilitate interpretation of ECGs should be context-dependent, and it should be possible to customise applications.\nConcerns raised about AI included not having access to the original data once it has been computationally processed, which could introduce the risk of missing diagnostically valuable data:"}, {"title": "Accuracy and explainability", "content": "When asked about the explainability of AI, the majority of clinicians stated they did not necessarily have a preference for understanding how an algorithm reached its decision, as long as high accuracy was guaranteed.\nIn spite of positive views of the visual representations of AI decision-making, there was a view that \"moving towards non-explained AI is probably the way things are gonna go\" (P19) and clinicians are prepared to trust future AI as long as the system is accurate and reliable enough to include its outputs into their clinical decision-making. Another interviewee remarked that current systems are already perceived as \"black box\" algorithms (P01), as contemporary clinical ECG applications do not tell clinicians about how they arrive at conclusions. A similar comparison was drawn by other clinicians, who described clinical decision-making itself as a black box, as they felt that a lot of experience of interpreting ECGs leads to \"a bit of instinct and gut to rely on\" (P12).\nClinicians also noted that specifically in emergency situations there is no time to reason about a computer's output, as the care of patients has the highest priority. This was summed up by P16 who stated: \"In clinical practice, I actually want to know less, because we are overwhelmed with so many things.\"\nA few clinicians thought that explainable algorithms would be valuable. Reasons for this were enhanced trust, and knowledge about an algorithm's weaknesses, but also from a point of care to show competency to patients and to \"give them some comfort to know that we understand the machines we're dealing with\" (P23). Some interviewees also saw value in explainable algorithms as they have some personal interest or experience with machine learning, and knowing what was going on behind the scenes would be driven by personal curiosity."}, {"title": "Patients and Al", "content": "Interviewees were ambivalent when asked if they think their patients would prefer explainable AI or not. Most clinicians stated that patients generally neither see their ECG nor are they involved in the process of interpreting them, highlighting that patients are often not involved in the clinical decision-making progress enough to understand it. Other interviewees saw a responsibility toward patients to educate them about the use of AI and to inform them that algorithms formed part of their decision-making process. P20 remarked that some patients embrace new technologies and see benefits in approaches such as deep learning, which might become even more prevalent in digital-native generations.\nThere was also a notion that too much information can cause unwanted outcomes: \"I like to educate my patients and explain 'you know, this is what this means' but I find actually nine times out of ten, it just gives them more anxiety when you explain things to them\" (P12).\nClinicians overall agreed that patients want accurate systems that clinicians can trust and to know that clinicians are responsible for patient outcomes, not a computer."}, {"title": "Education, deskilling, and clinical competencies", "content": "Whilst none of the clinicians raised concerns about AI making their roles redundant, they highlighted possible ways in which AI might impact medical education and clinical practice.\nThere was a view that AI might contribute to a potential loss of skills in future generations of doctors. Clinicians mainly feared that if an AI system just spits out diagnoses, this could mean doctors \"stop practising interpretation\" (P04). P22 highlighted that \"there were a lot of worries about juniors who, but almost kind of like born into that system, that there would be certain skills that would be lost.\" However, others saw potential in AI-aided applications for educational purposes, as algorithms could be used to specifically visualise or explain ECGs to medical students or doctors in early career stages. A condition-based approach was seen as a valuable tool for training and developing ECG interpretation skills. Visualisations were viewed as useful for telling clinicians which areas have been considered by the AI, increasing explainability and providing a shared representation between human and machine: \"So actually you almost think the colour...you know, the fact that part of it is highlighted, shows to me that the system or the black box has spotted that bit and can colour it in, and therefore that automatically adds weight to whatever conclusion it draws.\" (P01)\nParticipants raised concerns about over-reliance on computerised interpretation in clinical practice. Interviewees feared that AI may deskill clinicians, making them \"lazy\" (P04) when a computer just gives them an answer, but they also remarked that losing skills has been happening for a while. Some clinicians noted that AI is already advancing into several fields and that the medical profession just has to accept that these systems will arrive in the future. For example, P04 stated: \"I'd say yeah, fine, takes workload off us, and we lose skills, but I mean, that's the nature of medicine, we always lose skills as technology develops.\" The responsibility of the clinician for a decision remained paramount, however. The majority of interviewees said that although AI might help to interpret ECGs, it is important to safeguard clinical competencies and skills."}, {"title": "DISCUSSION", "content": "The clinicians we interviewed do not trust current CIE as a basis for clinical decisions as these systems are deemed unreliable and often over-sensitive. Clinicians also reported adverse effects of computerised interpretation on clinical decision-making, leaving young doctors or those with less training worrying about the accuracy of their own diagnoses, as they did not feel they had enough experience of interpreting ECGs to confidently ignore CIE.\nThe literature we reviewed confirmed that current CIE systems are often not used and that there are issues with their reliability [3, 4]. In contrast, there was a view that novel AI applications will outperform current commercial products [42, 43]. Furthermore, our findings indicate that clinicians do not associate current automated methods with Al-driven applications, which could be a factor in AI uptake and acceptance.\nAI applications should fit seamlessly into clinical practice and gain clinicians' trust by delivering accurate results requiring low cognitive effort to understand. There is no one-size-fits-all approach as clinicians have varying concerns and demands that will have to be met to ensure usage. The ability to interact with the system and to adapt it to individual requirements would be of value.\nAs long as algorithms were accurate, explainability was not viewed as important by many clinicians. This finding is congruent with research conducted with pathologists who do not seem to worry about explainability as long as accuracy and usefulness can be guaranteed [44]. In fast-paced, clinical environments such as emergency settings, explainability may play a lesser role as rapid decision-making is required and information overload can hinder this.\nParticipants raised concerns about technology deskilling clinicians. Although they did not all require explanations for a diagnosis, they also indicated they would prefer to have a visualisation of how a decision has been made. Visualisations that indicate abnormalities on ECGs were viewed as useful for avoiding deskilling and supporting clinical decision-making.\nAs clinicians appear prepared to increasingly rely on technology, we suggest that digital technology education should provide a foundation in how it works in practice, including: (i) what AI actually is, (ii) the tasks it can perform, and (iii) how models make decisions. This will help prepare the workforce of the future to make the best use of upcoming AI technologies in practice."}, {"title": "CONCLUSION", "content": "Our research finds that clinicians do not trust current automated ECG interpretation but have a positive attitude toward novel AI technologies. Explainability is valued but not essential, as long as the system is accurate. In order to function well, as system must fit into clinical practice and avoid imposing an additional cognitive burden. Early education, starting in medical school, will prepare future doctors for ethical and informed use of future technologies and support them in making AI-informed decisions."}, {"title": "Limitations", "content": "Our study is limited to the views of 23 clinicians. As this is a limited sample that comes from NHS trusts across England, we do not aim to present results that generalise across a wider population, but rather aim to contribute further understanding and context around AI from the perspective of three specialties, GPs, and junior doctors. Although some of the clinicians were not trained in the UK, this study presents a UK-centric position of English-speaking clinicians. We acknowledge that future work should focus on the incorporation of further specialties, such as emergency medical services (EMS; paramedics), PPIE stakeholders, and inclusion of a broader variety of demographics."}, {"title": "Competing interests", "content": "The authors have no competing interests to declare."}, {"title": "Author contributions statement", "content": ""}, {"title": "Acknowledgments", "content": ""}, {"title": "Data availability statement", "content": "The data underlying this article is available in Figshare [omitted]. The transcript data is openly available and reusable under CC BY 4.0. The topic guide, PIS, an exemplar consent form, and the"}]}