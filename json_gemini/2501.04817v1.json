{"title": "Decentralised Resource Sharing in TinyML: Wireless Bilayer Gossip Parallel SGD for Collaborative Learning", "authors": ["Ziyuan Bao", "Eiman Kanjo", "Soumya Banerjee", "Hasib-Al Rashid", "Tinoosh Mohsenin"], "abstract": "Abstract-With the growing computational capabilities of microcontroller units (MCUs), edge devices can now support machine learning models. However, deploying decentralised federated learning (DFL) on such devices presents key challenges, including intermittent connectivity, limited communication range, and dynamic network topologies. This paper proposes a novel framework, bilayer Gossip Decentralised Parallel Stochastic Gradient Descent (GD-PSGD), designed to address these issues in resource-constrained environments. The framework incorporates a hierarchical communication structure using Distributed K-means (DK-means) clustering for geographic grouping and a gossip protocol for efficient model aggregation across two layers: intra-cluster and inter-cluster. We evaluate the framework's performance against the Centralised Federated Learning (CFL) baseline using the MCUNet model on the CIFAR-10 dataset under IID and Non-IID conditions. Results demonstrate that the proposed method achieves comparable accuracy to CFL on IID datasets, requiring only 1.8 additional rounds for convergence. On Non-IID datasets, the accuracy loss remains under 8% for moderate data imbalance. These findings highlight the framework's potential to support scalable and privacy-preserving learning on edge devices with minimal performance trade-offs.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid evolution of machine learning and AI has substantially increased the volume of data generated by edge devices, such as mobile phones, IoT sensors, and wearable devices. This data explosion has highlighted the limitations of traditional centralised learning models, where data is aggregated at a central server for model training. These centralised models face significant challenges, including concerns about data privacy, high bandwidth consumption, and increased latency, making them unsuitable for many edge-based applications [1]. To address these issues, Federated Learning (FL) was introduced by McMahan et al. [2]. FL enables edge devices to collaboratively train a global model while retaining their local data, thereby preserving privacy and reducing communication costs. However, most FL implementations rely on a Centralised Federated Learning (CFL) paradigm, where a central server is responsible for model aggregation. This centralised approach creates risks, such as bottlenecks in scalability, increased latency, and a single point of failure, which undermines the robustness of the system [3]. Decentralised Federated Learning (DFL) has emerged as an alternative to CFL. By enabling direct communication between edge devices, DFL eliminates the need for a central server, improving scalability and resilience [4]. Nevertheless, existing DFL frameworks often assume static and fully connected network topologies, which are impractical in real-world scenarios. In traditional federated learning, the central server often waits for all participating devices to finish their computations and submit updated weights. This results in idle time for the server, creating unnecessary delays in the training process. The situation becomes critical if the central server, which coordinates cumulative learning and aggregates model-updates fails. Such a failure disrupts the entire learning workflow, causing system downtime and halting progress. Furthermore, edge devices exhibit practical constraints, including limited communication ranges, low computational capabilities, and dynamic network conditions. These challenges severely limit the feasibility and effectiveness of current decentralised federated learning (DFL) methods in real-world applications [5], [6]. To overcome these limitations, we propose a bilayer Gossip Decentralised Parallel Stochastic Gradient Descent (GD-PSGD) framework. This framework introduces a hierarchical communication structure that combines geographic clustering and gossip-based model aggregation. Distributed K-means (DK-means) for efficient clustering and leveraging gossip protocols for intra- and inter-cluster communication help in reducing communication overhead and improving scalability in resource-constrained environments. The key contributions of this paper are as follows:\n\u2022 A bilayer gossip protocol that uses Distributed K-means (DK-means) for efficient geographic clustering.\n\u2022 A decentralised communication model tailored to edge environments, addressing limited communication ranges and dynamic topologies.\n\u2022 Performance and Comprehensive evaluation of the Framework in IID and Non-IID data scenarios, demonstrating competitive results compared to CFL."}, {"title": "II. RELATED WORK", "content": "Federated Learning (FL) has gained significant attention as a solution for privacy-preserving distributed machine learning. Originally proposed by Google researchers, FL allows multiple devices to collaboratively train a shared global model while keeping their local data private [2]. Early implementations of FL demonstrated its potential in applications such as personalised language models for mobile devices, including Google's keyboard, where local data remained secure on user devices [1]. Traditional FL architectures are typically centralised, with a server coordinating model-updates and aggregation [2]. Methods such as Federated Averaging (FedAvg) have been widely adopted for global model updates, ensuring efficiency and scalability [3]. However, centralised federated learning (CFL) faces challenges such as single points of failure, where server outages disrupt the entire learning cycle, and communication bottlenecks, as the central server's communication load increases with the number of devices, limiting scalability [4]. These limitations have motivated the development of decentralised federated learning (DFL) as a more robust alternative [?]. DFL eliminates the reliance on a central server by enabling peer-to-peer communication for model-updates. This decentralised approach improves scalability and system resilience [4]. For instance, frameworks based on peer-to-peer communication significantly reduce communication overhead and enhance scalability by eliminating the central server [7]. However, the absence of a central coordinator introduces new challenges, including the need for efficient network topologies and communication protocols [8]. Fully connected mesh topologies ensure robustness but incur high communication costs, making them impractical for edge devices with limited resources [5]. Simpler topologies like rings or stars are less communication-intensive but can result in delays and inefficiencies in sparse or dynamic networks [6]. Adaptive topologies have been explored to address these challenges, often leveraging clustering techniques to group devices based on proximity or data similarity. Hierarchical frameworks combine the benefits of clustering and decentralisation, facilitating efficient communication and aggregation [9]. These designs are further enhanced by gossip-based protocols, which achieve strong convergence rates by iteratively aggregating model updates across peers. For instance, the MATCHA algorithm dynamically schedules non-overlapping communication links to reduce overhead while maintaining connectivity [8]. Such methods demonstrate the potential of decentralised architectures for handling large-scale deployments [2]. Recent work has also focused on enabling FL in resource-constrained environments. On-device training and TinyML techniques, such as model quantisation and pruning, allow FL models to run on microcontrollers and other low-power edge devices. MCUNet exemplifies this direction by enabling efficient model deployment on resource-limited hardware, paving the way for scalable FL in IoT applications [9]. Decentralised optimisation methods, such as Decentralised Parallel Stochastic Gradient Descent (D-PSGD), further enhance DFL by removing the dependency on a central server [7]. These methods ensure stable convergence even under dynamic network conditions by leveraging gradient tracking and variance reduction techniques. Frameworks incorporating D-PSGD demonstrate performance comparable to centralised methods, making them a viable alternative for large-scale, decentralised learning systems [3]. The integration of advanced communication protocols, adaptive topologies, and lightweight models underscores the evolving landscape of FL. These advancements aim to address the inherent challenges of decentralisation, such as network dynamism, resource constraints, and data heterogeneity, while maintaining robust model performance."}, {"title": "A. Decentralised Federated Learning Paradigms", "content": "DFL architectures can be broadly classified into two paradigms: Continual and Aggregate [15] [16]. In the Continual paradigm, clients sequentially train a shared model received from another client without intermediate aggregation. This process relies on passing the model from one client to the next for further training. In the Aggregate paradigm, clients train local models independently and periodically aggregate their updates with one or more received models. These paradigms differ in communication resource requirements and model convergence stability. The Continual paradigm has minimal communication overhead, as only one model is transmitted at a time, and no aggregation computation is required. In contrast, the Aggregate paradigm incurs higher communication costs, as it involves transmitting multiple model parameters and requires an efficient aggregation algorithm [17] [16]. Model convergence stability is another key distinction. In the Continual paradigm, as the model sequentially trains across multiple clients, early training information may be forgotten, and the order of client communication can significantly influence convergence. This can lead to instability in the learning process. Conversely, the Aggregate paradigm provides more stable convergence and improved generalisation performance through periodic aggregation, ensuring that contributions from all clients are preserved [17] [16]. Choosing between these paradigms involves balancing trade-offs based on deployment scenarios, device performance, and the specific requirements of the task. For instance, the Continual paradigm may be preferable in resource-constrained environments where communication overhead is a critical limitation. On the other hand, the Aggregate paradigm is better suited for scenarios prioritising model stability and performance despite higher communication costs."}, {"title": "Continual Paradigm", "content": "In the study by Yuan et al., a continuous paradigm was used. They introduced FedPC, a novel peer-to-peer (P2P) FL framework designed for naturalistic driving action recognition. This serverless framework leverages continual learning and a gossip communication protocol to facilitate direct model updates between clients. By eliminating the central server, FedPC reduces communication, computational, and storage overheads, enhancing privacy and learning efficiency. The gossip protocol introduces randomness in communication, improving system robustness and reducing single points of failure. Experimental results demonstrate that FedPC achieves competitive performance compared to traditional client-server FL approaches, with superior knowledge dissemination rates and efficient resource usage, making it suitable for dynamic and heterogeneous environments."}, {"title": "Aggregate Paradigm", "content": "Research using the aggregate paradigm has also achieved good results, e.g. Shi et al. present an AirComp-based DSGT-VR (Decentralized Stochastic Gradient Tracking with Variance Reduction) algorithm for decentralized FL over wireless networks [18]. This approach eliminates the central parameter server by using device-to-device (D2D) communication and over-the-air computation (AirComp) to manage additive noise and adapt to changes in network topology. The algorithm integrates gradient tracking and variance reduction techniques to enhance convergence performance, ensuring linear convergence despite channel fading and noise. Key features include precoding and decoding strategies to support reliable D2D communication and effective model aggregation. Numerical experiments validate the algorithm's superior performance in terms of convergence rate and accuracy, highlighting its potential in scenarios where central coordination is impractical. In the MATCHA algorithm proposed by Wang et al., a more novel Dynamic parallel broadcast-gossip hybrid topology is used, which optimizes communication in decentralized stochastic gradient descent [19]. The algorithm employs a matching-based link scheduling strategy, decomposing the network topology into several matching of non-overlapping links. This ensures nodes communicate only with a subset of neighbours in each iteration, reducing communication overhead while maintaining connectivity."}, {"title": "B. Parallel Stochastic Gradient Descent (SGD)", "content": "Stochastic Gradient Descent (SGD) is a widely used optimisation algorithm in machine learning, particularly for optimising smooth objective functions. Unlike traditional optimisation methods, SGD updates model parameters iteratively using small, randomly sampled subsets of data (mini-batches), making it computationally efficient for large-scale problems. Research by Moulines et al. and Nemirovski et al. established that SGD achieves a convergence rate of \\(O(1/\\sqrt{K})\\), for general convex problems, where K refers to the number of iterations for the convergence rate and \\(O(1/K)\\) for strongly convex problems [20] [21]. In Collaborative Federated Learning (CFL), Centralized Parallel SGD (C-PSGD) extends the traditional SGD algorithm to leverage parallelism, significantly enhancing training speed in distributed systems. During training, each client independently updates its local model using SGD. The central server then collects and aggregates these local models to update the global model. In their study on asynchronous decentralised parallel stochastic gradient descent (PSGD), Lian et al. demonstrated that, with appropriate tuning of learning rates and gradient aggregation methods, the errors introduced by parallelism can be effectively controlled. This enables parallel SGD to achieve performance comparable to traditional SGD, with a convergence rate of \\(O(1/\\sqrt{K * n})\\), where n represents the number of participating clients. [22].\nDecentralised Parallel Stochastic Gradient Descent (D-PSGD) extends Centralized Parallel SGD by eliminating the need for a central server. Instead, devices perform peer-to-peer communication to exchange and aggregate updated parameters. Lan et al. provided a foundational theoretical analysis demonstrating that D-PSGD can outperform C-PSGD under certain conditions, a claim validated through experimental results [23].\nD-PSGD is generally implemented in two forms: synchronous and asynchronous. In synchronous D-PSGD, devices must wait for all peers to complete local training before simultaneously communicating and aggregating updates. In contrast, asynchronous D-PSGD allows devices to independently update their local model parameters without waiting for others. Studies by Ram et al., Srivastava and Nedic, and Sundhar Ram et al. have examined the convergence properties of these approaches [24] [25] [26]. Asynchronous D-PSGD typically enables faster training by reducing idle time but introduces added complexity and potential challenges in guaranteeing convergence. For this reason, we opted to use synchronous D-PSGD in our experimental design, prioritising stable convergence and facilitating reliable performance evaluation."}, {"title": "III. METHODOLOGY", "content": "This research proposes a robust DFL framework to improve model convergence and address communication and computational challenges on resource-constrained, dynamically connected edge devices [27], with convergence achieved using Decentralised Parallel Stochastic Gradient Descent with random gossip protocols(D-PSGD). Given the limited communication bandwidth of edge devices, which makes one-to-many broadcast communication infeasible, this research employs a random 1-to-1 gossip protocol as the primary communication method. This protocol ensures communication occurs between two devices per round, effectively addressing bandwidth constraints while enhancing scalability and robustness. Although D-PSGD with random gossip protocols may converge more slowly than centralised SGD due to decentralised communication, it demonstrates significant advantages in large-scale, heterogeneous networks.\nThe dynamic nature of edge environments adds complexity to deployment. Devices mounted on mobile platforms (e.g., vehicles or animals) frequently enter and leave communication ranges, resulting in constantly shifting network topologies. The random gossip protocol, which randomly selects a communication partner in each round, introduces substantial flexibility for such dynamic settings but also creates challenges. The dynamic nature of communication ranges, represented as r, leads to continually shifting network topologies, where G(V, E) represents the graph formed by devices V and communication links E. The random gossip protocol, denoted as \\(P_t\\), selects a communication partner \\(v_j \\in V\\) for a device \\(v_i \\in V\\) at each round t with probability proportional to adjacency \\(A_{ij}\\), introducing flexibility but also computational complexity in these dynamic settings.\nThe convergence rate of D-PSGD with random gossip protocols is dependent on the spectral gap of the Laplacian matrix L of G, given as \\(\\rho = 1 - \\lambda_2(L)\\), where:\n\u2022 \\(\\lambda_2(L)\\) is the second smallest eigenvalue of L,\n\u2022 A larger spectral gap \\(\\rho\\) (or equivalently a smaller \\(\\lambda_2\\)) indicates better connectivity and facilitates faster convergence of the protocol.\nThus, the convergence speed \\(T_{convergence}\\) is inversely related to the spectral gap:\n\\[T_{convergence} \\propto \\frac{1}{\\rho} = \\frac{1}{1 - \\lambda_2}\\]\nA well-connected graph G with a larger \\(\\rho\\) leads to efficient information dissemination and faster convergence of the algorithm [28].\nConversely, sparse or poorly connected networks slow information propagation, impeding convergence, while fully connected or dense topologies achieve superior performance. Alexandros et al. demonstrated that random 1-to-1 gossip underperforms on grids and random geometric graphs, where information diffusion resembles a random walk [29]. For instance, in a two-dimensional grid with side length d, it requires \\(d^2\\) steps to traverse the network entirely. Introducing directionality in information propagation significantly accelerates diffusion within random gossip protocols.\nTo address these limitations, this research employs the Decentralized K-means (DK-means) algorithm to group devices into clusters based on geographical proximity. The proposed framework introduces a hierarchical two-layer topology: intra-cluster and inter-cluster layers. Within each cluster, devices communicate using the gossip protocol, facilitating information exchange and local model aggregation. Since devices within a cluster are geographically close, their network is denser and approximates a fully connected topology, enabling faster convergence of the gossip protocol (see Figure 3).\nThe inter-cluster layer, in contrast, is sparser. However, because model aggregation has already occurred within clusters, devices only need to communicate with those in other clusters that fall within their communication range. This hierarchical structure leverages dense intra-cluster communication for rapid convergence and reduces inter-cluster communication overhead, ensuring efficient information diffusion across the entire network. This design effectively balances the trade-offs between scalability, resource constraints, and convergence performance (see Figure 4)."}, {"title": "IV. ALGORITHMS", "content": "The algorithm builds on the principles previously described, utilising the DK-means algorithm to construct a bilayer network, which comprises:\n\u2022 Intra-cluster layer: A densely connected network within each cluster.\n\u2022 Inter-cluster layer: A sparse network between clusters.\nAt the start of each round, the DK-means algorithm dynamically forms clusters based on device locations. Communication within clusters is conducted over \\(G_{intra}\\) iterations using a 1-to-1 gossip protocol, see algorithm1. Cumulative FedAvg is employed for model aggregation to minimise storage requirements on edge devices. Following intra-cluster communication, the process is repeated for \\(G_{inter}\\) iterations on the inter-cluster layer, completing one training round.\nThe standard K-means algorithm requires all data to be centralised, which is impractical without a central server. The Distributed K-means (DK-means) in algorithm 2 operates without data centralisation by enabling each node to perform computations locally and communicate with others to achieve global clustering.\nAlgorithm3, is a memory-efficient variant of the standard FedAvg algorithm. Its design addresses the limited memory capacity of edge devices. During the communication phase, devices avoid storing multiple models locally for aggregation. Instead, each device calculates a cumulative weighted sum"}, {"title": "V. CONVERGENCE FRAMEWORK", "content": "Consider an undirected network \\(G : (V, W)\\) with n worker nodes. V is a set containing all worker nodes: \\(V : {1,2,...,n}\\). \\(W \\in R^{n \\times n}\\) is a symmetric weight matrix, indicates the connectivity between devices, where \\(W_{ij} = W_{ji}\\), and \\(W_{ij} \\in [0, 1]\\), with 0 indicating that there is no connection between device i and device j, and 1 indicating the opposite. \\(D_i: {(x,y), (x, y), ..., (x, y)}\\) represents the local dataset of device i with size d. Assumptions: Before we start analysing the convergence rate of D-PSGD, the commonly used assumptions are as follows:\n\u2022 Lipschitz continuity of the gradient: All functions \\(f_i(\\cdot) : R^d \\rightarrow R\\) for \\(i \\in [n]\\) have gradients that adhere to an L-Lipschitz condition.\n\u2022 Convexity: All functions \\(f_i(\\cdot)\\) are \\(\\mu\\)-strongly convex.\n\u2022 Spectral gap: Given a symmetric stochastic matrix W, we define the spectral gap parameter \\(\\rho\\) as \\((\\text{max} {\\vert \\lambda_2(W) \\vert, \\vert \\lambda_n(W) \\vert })^2\\). Here, \\(\\lambda_2(W)\\) and \\(\\lambda_n(W)\\) are the second largest and smallest eigenvalues of W, respectively. We assume \\(\\rho < 1\\).\n\u2022 Bounded variance of the stochastic gradient: We assume that the variance of the stochastic gradient \\(E_{\\xi \\sim D_i} ||\\nabla F_i(x; \\xi) - \\nabla f_i(x)||^2\\) is bounded for any node x, where i is uniformly sampled from V : {1,...,n} and \\( \\xi \\) is sampled from the distribution Di. This implies the existence of constants \\(\\sigma\\) and s such that:\n\\[E_{\\xi \\sim D_i} ||\\nabla F_i(x; \\xi) - \\nabla f_i(x)||^2 < \\sigma^2, \\forall i, \\forall x,\\]\n\\[E_{i \\sim U([n])} ||\\nabla f_i(x) - \\nabla f(x)||^2 < s^2, \\forall x.\\]\nIn a typical distributed optimization problem, the objective function can be written as:"}, {"title": "VI. EXPERIMENTS", "content": "We will use normal local training and Centralized Federated Learning (CFL) as baselines to evaluate the method we propose. This is because, in terms of model performance, normal local training is widely considered to represent the optimal result, which could serve as an upper bound because it's not limited by the consensus constraints or the communication overhead inherent in decentralized methods. Additionally, the performance of DFL is often compared to CFL, with CFL being the theoretical upper bound of what can be achieved when decentralization constraints are removed. If DFL achieves performance close to that of CFL, it indicates that the DFL method is effectively coordinating the learning process across the network. We will also compare the convergence rate of our proposed DFL method with that of CFL to verify whether DFL can achieve the theoretically expected linear Since our goal is to implement a DFL framework that can be deployed on edge devices, we first constructed a simulation environment that incorporates the characteristics of edge devices (figure 6). We used the 'pyglet' library to build the foundation of the simulation environment and to visualise the states of the device. The simulated field is a square area of size 100 \u00d7 100, within which devices can move freely. Each device has the following attributes:\n\u2022 Position coordinates (x, y): We assume that each device knows its location in real time, which corresponds to the GPS functionality in the real world.\n\u2022 ID: Each device has a unique ID.\n\u2022 Communication range: Each device can communicate only with other devices within its communication range r, simulating the limited communication range of edge devices in reality.\n\u2022 Movement speed: Each device has a movement speed s to simulate devices deployed in vehicles or animals. The direction of movement is random and variable and the movement is continuous and slow. This is to ensure that the devices have enough time to complete at least one full communication.\nThe above attributes are related to the simulation environment. Other attributes related to the experiment, such as local models, parameters, and datasets, will be introduced in the next section. To simulate information transmission between multiple devices on a single computer, rather than simply reading memory from other devices locally, we used the MQTT (Message Queuing Telemetry Transport) library to simulate communication between devices. This is an ISO standard message protocol based on the publish/subscribe paradigm over TCP/IP, specifically designed for remote devices with low hardware performance and under poor network conditions."}, {"title": "A. Dataset and Model", "content": "Our experiment focuses on image recognition datasets, specifically using CIFAR-10. CIFAR-10 consists of 60,000 colour images, each 32x32 pixels, categorised into 10 classes. These classes are evenly split between vehicles (e.g., aeroplanes, ships) and animals (e.g., birds, dogs). Each class contains 6,000 images, ensuring a balanced dataset. We used the official train/test split, with 50,000 images for training and 10,000 for testing [31]. Examples of the dataset are shown in Figure 6. In Distributed Federated Learning (DFL), each device is assigned a unique local dataset. For this, we employed two split methods: 1)IID Dataset: In IID datasets, all data points are generated independently from the same probability distribution. This simplifies theoretical analysis and is common in centralised machine learning. Each device receives a unique, balanced subset with an equal number of samples for all classes, 2)Non-IID Dataset: Real-world datasets are typically Non-IID, where devices collect data specific to their environment. For example, a device may only have data from 1-2 classes, or the class distribution may be highly imbalanced. The degree of Non-IIDness is controlled by the Dirichlet parameter \\( \\alpha \\). A larger \\( \\alpha \\) results in a more uniform distribution, while a smaller \\( \\alpha \\) creates a skewed distribution. For instance, with \\( \\alpha = 0.5 \\), the dataset is highly imbalanced.\nDue to limited computational resources on edge devices, full-model training is often infeasible. For our experiment, we selected the MCUNet-in3 model, developed by MIT for resource-constrained edge devices. The model has 0.74M parameters and achieves 62.2% top-1 accuracy and 84.5% top-5 accuracy on ImageNet. Given that most microcontrollers (MCUs) support transfer learning but not full-model fine-tuning, we adopted a transfer learning approach. Initially, we performed full-model fine-tuning on the complete training set. Then, we replaced the model's classifier layer with three fully connected layers. In subsequent training, all parameters except the classifier layer were frozen. This significantly reduced computational requirements and increased training speed."}, {"title": "B. Experimental Setup", "content": "The experiment evaluates the performance of two baselines and the proposed bilayer GD-PSGD (bilayer Gossip D-PSGD) method. The two baselines are normal local training and training using the CFL framework. The following settings are consistent across all training setups:\n\u2022 GPU: RTX 3070 Ti.\n\u2022 Framework: PyTorch.\n\u2022 Optimiser: Adam with a learning rate of 0.001 and weight decay of 1 \u00d7 10-3.\n\u2022 Batch size: 128."}, {"title": "VII. RESULTS AND DISCUSSION", "content": "Full-model training was conducted on the complete CIFAR-10 dataset to establish a performance baseline. This baseline serves as an upper bound for comparison but is not directly contrasted with CFL or DFL since it uses the entire dataset for training and does not simulate federated learning conditions. The results are shown in Figure 8.\nThe CFL baseline and bilayer GD-PSGD were evaluated under identical conditions:\n\u2022 Optimizer: Adam (learning rate = 0.001, weight decay = 1e-5).\n\u2022 Local training epochs: 2.\n\u2022 Number of devices: 30.\n\u2022 Communication range: 60.\n\u2022 Dataset: IID partition with balanced subsets.\nThe simulation was conducted sequentially on a single computer to ensure consistency. The results are shown in Figure 9."}, {"title": "A. Performance on Non-IID Datasets", "content": "Non-IID datasets are a significant challenge in federated learning. This section evaluates whether the proposed DFL method can sustain high performance under non-IID conditions. The parameter \\( \\alpha \\) was used to control the degree of non-IIDness while keeping all other training parameters constant. The results are shown in Figure 10.\n\u2022 CFL Performance: CFL converged within 1-2 rounds regardless of \\( \\alpha \\), with minimal impact on accuracy until \\( \\alpha = 0.1 \\). At \\( \\alpha = 0.1 \\), CFL accuracy dropped by 4.4% to 77.3%, and convergence slowed by one round.\n\u2022 DFL Performance: DFL showed a gradual decline in convergence rate and final accuracy as \\( \\alpha \\) decreased:"}, {"title": "B. Impact of Number of Devices on DFL Performance", "content": "The number of devices significantly influences DFL performance. Increasing the number of devices within the same simulation area leads to:\n\u2022 Smaller local datasets per device, increasing the complexity of training.\n\u2022 A denser network topology, enhancing information dissemination."}, {"title": "C. Impact of Communication Range on DFL Performance", "content": "The communication range significantly affects the network topology in DFL. A shorter communication range:\n\u2022 Reduces the number of neighbouring devices within the range.\n\u2022 Lowers the degree of freedom for each node in the network.\nThe degree of freedom affects the spectral properties of the gossip or Laplace matrix. As the spectral gap increases (i.e., the difference between the largest and second-largest eigenvalues), well-connected graphs exhibit faster information exchange. Conversely, sparsely connected nodes exchange information more slowly, delaying convergence."}, {"title": "D. Impact of Geographic Location on DFL Performance", "content": "We also explored whether clustering devices based on their geographic location is the optimal criterion. Studies by Duan et al. and Zhao et al. propose using EMD (Earth Mover's Distance) to gauge dataset similarity and aggregate weights of devices with similar data distributions [33], [34]. This approach has shown improved global model performance, particularly for non-IID settings. In our experiments, we used EMD values as the clustering criterion for DK-means to assess its impact.\nFigure 15 shows that while the final accuracy of the EMD-based variant remains comparable, it does not converge as quickly as the bilayer GD-PSGD. These findings suggest:\n\u2022 The bilayer network topology significantly accelerates convergence in GD-PSGD.\n\u2022 Using EMD values for clustering does not improve the convergence rate in the simulation scenarios considered in this study."}, {"title": "VIII. LIMITATIONS AND OPPORTUNITIES", "content": "Decentralised federated learning (DFL) faces notable challenges including the absence of standardised benchmarks and evaluation tasks. The diverse design possibilities in DFL\u2014such as varying network topologies, communication protocols, and deployment conditions\u2014make comparisons difficult. Differences in static versus dynamic networks and edge device capabilities further complicate consistent evaluation. As Mart\u00ednez et al. [15] highlight, these disparities hinder fair comparisons and the establishment of clear state-of-the-art benchmarks.\nThe second limitation is the lack of real-world testing instances on edge devices. Microcontrollers, the typical hardware for such deployments, often lack the resources for full-model training. While Nvidia Jetson devices support full-model training, their cost and size make them unsuitable for widespread deployment. MIT's TinyEngine enables training on microcontrollers with less than 256KB memory but supports only heavily quantised MCUNet models and STM32CubeIDE, making deployment cumbersome [35].\nAddressing these limitations is essential for advancing DFL and enabling its broader adoption. The rapid advancement of DFL and edge device technology suggests promising directions for future work. Real-world deployment of DFL architectures and on-device training on edge devices should be explored. With improved hardware supporting full model training, future studies can bypass transfer learning for end-to-end model training.\nMoreover, integrating bilayer, multilayer, or hierarchical aggregation mechanisms into DFL offers significant potential. These methods, proven effective in CFL architectures, could further enhance DFL convergence rates. Future research should investigate diverse hierarchical topologies and criteria, such as geolocation, Earth Mover's Distance (EMD), or combinations of both, to optimise network architectures [36] [37]."}, {"title": "IX. CONCLUSION", "content": "This study introduces bilayer GD-PSGD (Gossip Decentralized Parallel SGD), a distributed federated learning framework designed for edge devices in remote areas. The framework addresses challenges such as limited computational resources, constrained communication ranges, lack of network coverage, and dynamic device networks. Using the DK-means algorithm, devices are clustered based on geographic locations, followed by gossip protocol for communication and cumulative FedAvg for model aggregation across intra-cluster and inter-cluster layers.\nExperiments using the MCUNet-in3 model and CIFAR-10 dataset compared bilayer GD-PSGD with the traditional CFL architecture. Key findings include:\n\u2022 On IID datasets, bilayer GD-PSGD matches CFL's final performance (81% accuracy) with only 1.8 additional rounds required for convergence.\n\u2022 On Non-IID datasets, bilayer GD-PSGD retains high performance, with less than 8% accuracy loss for \\(\\alpha = 0.5\\). Severe imbalance ( \\( \\alpha = 0.1 \\)) causes a performance drop of about 16%.\n\u2022 Communication range minimally impacts final performance, even on Non-IID datasets. Only a range of 15 produces an accuracy degradation of less than 8%.\n\u2022 Increasing the number of devices slightly slows convergence but does not affect final performance. For over 100 devices, fine-tuning hyperparameters reduces convergence rounds to 8.\nThese results highlight bilayer GD-PSGD's ability to closely match CFL's performance while demonstrating robustness across varying datasets, communication ranges, and device numbers."}, {"title": "APPENDIX", "content": "Properties of Relevant Gossip Protocol: The paper also solicits a background on gossip model. Bracha protocol is one of the prominent example of Gossip protocol in distributed systems. The well known Bracha protocol results in O(n2) message complexity. Conventionally", "38": ".", "model": "Broadcast proposed by Bracha scale poorly", "39": "replace quorum by an extensive use of samples", "components": "the first one is Probabilistic broadcast called Murmur", "Contagion.\nLemma": "Let n denote the number of participants in a distributed system", "protocols": "n1) Protocol A with communication complexity O(n).\n2) Protocol B with communication complexity O(\u221an).\nThen", "following": "n1) Lower Latency: Protocol B achieves lower latency compared to Protocol A as n increases. Proof: TA =\nn \u00d7 L", "Throughput": "Protocol B achieves higher throughput as n increases because throughput is inversely proportional to latency.\n3) Better Resource Utilization: Protocol B utilizes network bandwidth and processing resources more efficiently as n increases due to fewer message exchanges.\n4) Scalability: Protocol B exhibits better scalability as n increases because its communication"}]}