{"title": "On the Impact of White-box Deployment Strategies for Edge AI on Latency and Model Performance", "authors": ["Jaskirat Singh", "Bram Adams", "Ahmed E. Hassan"], "abstract": "Context: Reducing the run-time latency of Machine Learning (ML)-based applications, while ensuring their inference accuracy, is the crucial task of MLOps engineers. To help them achieve this task, Edge AI technologies have been proposed that allow to transform and distribute (parts of) models across mobile, edge, and cloud tiers. To perform such transformations, MLOps engineers have to select and apply one or more operators. Despite the wide range of operators, broadly categorized into white-box (training-based) and black-box (non-training-based) techniques, deciding which type of operator to use in an Edge AI setup to achieve performance advantage is mostly left to the personal judgment of the engineers. Objective: In order to help MLOps engineers decide which operator to use in which deployment scenario, this study aims to empirically assess the accuracy vs latency trade-off of white-box and black-box operators (and their combinations) in an Edge AI setup. Method: We perform inference experiments including 3 white-box (i.e., QAT, Pruning, Knowledge Distillation), 2 black-box (i.e., Partition, SPTQ) and their combined operators (i.e., Distilled SPTQ, SPTQ Partition) across 3 tiers (i.e., Mobile, Edge, Cloud) on 4 commonly-used Computer Vision and Natural Language Processing models to identify the effective strategies, considering the perspective of MLOps Engineers. Results: Our Results indicate that the combination of Distillation and SPTQ operators (i.e., DSPTQ) should be preferred over non-hybrid operators when lower latency is required in the edge at small to medium accuracy drop. Among the non-hybrid operators, the Distilled operator is a better alternative in both mobile and edge tiers for lower latency performance at the cost of small to medium accuracy loss. Moreover, the operators involving distillation show lower latency in resource-constrained tiers (Mobile, Edge) in comparison to the operators involving Partitioning across Mobile and Edge tiers. For textual subject models, which have low input data size requirements, the Cloud tier is a better alternative for the deployment of operators in comparison to the Mobile, Edge, or Mobile-Edge tier (the latter being used for operators involving partitioning). In contrast, for image-based subject models, which have high input data size requirements, the Edge tier is a better alternative for operators in comparison to Mobile, Edge, or their combination.", "sections": [{"title": "1 Introduction", "content": "In the context of deploying machine learning (ML) models in the context of software products, optimiz- ing model inference latency and performance accuracy is crucial. Ensuring the right trade-off between latency and accuracy is the responsibility of MLOps Engineers, who need to ensure that models deliver reliable predictions consistently, even under varying network conditions and input scenarios. By optimiz- ing model inference, MLOps engineers can minimize the strain on the hardware resources, maximizing the performance of deployed models while minimizing infrastructure costs. Furthermore, optimized model inference contributes to overall system performance and scalability. By striking the right balance between latency and accuracy, and leveraging efficient optimization techniques, MLOps engineers can maximize deployed models' reliability, scalability, and resource utilization, ultimately delivering a seamless and responsive user experience.\nEdge AI, or Edge Artificial Intelligence, has emerged as a promising solution for addressing the chal- lenges associated with deploying machine learning model-based applications by bringing computation closer to the data source, thereby enabling faster processing and response times in resource-constrained environments such as Mobile/IoT devices, or edge servers [134]. Edge AI minimizes the latency asso- ciated with sending data to centralized cloud servers for processing by performing computation locally on mobile/edge devices. Moreover, this enhances privacy and security by processing data locally on resource-contained devices (i.e., Mobile, Edge), reducing the need to transmit sensitive information to external servers. This mitigates the risk of data breaches or unauthorized access during data transmission, particularly important for applications handling sensitive or confidential data.\nThe model deployment in the Edge AI environment of Mobile, Edge, and Cloud (MEC) tiers relies on operators, which, according to a prior survey [134] can be divided into white-box operators (training- based) and black-box operators (non-training-based). The black-box operators involve techniques where an already trained model is optimized at the post-training stage. The white-box operators involve tech- niques where the model is optimized or tailored by exploiting the internal architecture of models using additional training pipelines. By understanding and exploiting the internals of the model using white box operators, the MLOps engineers can apply optimizations that are tailored to the specific charac- teristics and requirements of the deployment environment, leading to more efficient models in terms of both computational resources and memory footprint. The black-box operators often involve runtime optimizations such as model partitioning, where different parts of the model are executed on different tiers of the Edge AI setup (e.g., some computations are offloaded to the Edge or Cloud tier). Another type of black-box operator is Post Training Quantization, where the pre-trained models are quantized using calibration data. Black-box operators provide more flexibility and may preserve accuracy without modifying the model structure (such as Partitioning) but may not achieve the same level of efficiency as white-box optimizations (such as Knowledge Distillation).\nThe white-box operators include techniques such as Model Pruning, Quantization-Aware Training (QAT), or architecture optimization. QAT allows the augmentation of the model's computational graph (i.e., allows to modify the model's structure during training) for simulating the effects of quantization. Pruning allows for eliminating unnecessary parameters or nodes/edges, such as weights and connections while maintaining or even improving its accuracy performance. This can be done physically (removing the actual connections) or logically (assigning zero weights to irrelevant connections). While the QAT and Pruning operators accelerate the inference at minimal accuracy loss, architecture optimization such as Knowledge Distillation (KD) involves designing or selecting model architectures that are lightweight and efficient in terms of computational requirements while trying to maintain acceptable performance. White-box operators offer fine-grained control over model optimizations and can lead to significant improvements in latency and efficiency but may require additional time and effort during the training phase. Despite the wide range of black-box and white-box operators, thus far, in the specific setting of Edge AI, only the impact of black-box operators on latency and accuracy has been investigated [108], while studies comparing white-box operators do not consider all three tiers (MEC) of Edge AI for latency and accuracy evaluation.\nHence, the main contribution of this paper is the empirical comparison in terms of latency and accuracy of competing White-box and Black-box operators and their combinations to suggest recom- mendations for the model deployment for MLOps engineers. Particularly, we study 3 white-box (i.e., QAT, Pruning, Distillation), 2 black-box (i.e., SPTQ, Partition), and their hybrids (i.e., Distilled SPTQ, SPTQ Partition) in an Edge AI environment of MEC (Mobile, Edge, Cloud) tiers. The comparative"}, {"title": "2 Related Work", "content": "In earlier investigations [134,118,17,83], various families of deployment operators were explored. Our current study focuses on operators involving model optimization, specifically evaluating three white box operators (QAT, Distillation, and Pruning), two black box operators (SPTQ, Partition), and their combinations (SPTQ Partition, Distilled SPTQ). We aim to provide practical insights derived from empirical data, shedding light on effective strategies for optimizing models to meet the challenges inherent in Edge AI environments, such as constrained computational resources and network limitations. As shown in Table 1, numerous studies emphasize detailed analyses of individual operators or comparisons between them. Only one study [82] compares three operators (white-box), but not in an Edge AI setup, indicating that there is a lack of research comparing a broader set of operators (white-box) in the context of Edge AI to achieve a more thorough analysis of their relative performances and trade-offs, which is the primary goal of our study.\nTable 1 shows the distinctive features of our study compared to 25 and 29 prior studies focusing on the white-box Knowledge Distillation and Pruning operator, respectively in isolation or combination with other operators. Out of the 33 studies that focused solely on quantization, 16 utilized a black- box approach, while 17 employed a white-box approach, both of which are also compared in our study. Regarding input data, a greater number of studies focus on image (79) or textual (11)-based models, in comparison to speech models (6), indicating that image/text data is more frequently utilized for the specified operators.\nIn Table 1, no study considers all 3 Edge AI tiers for white box operators in their experiments, indicating that this aspect of the study is comparatively less investigated. The combination of all 3 tiers offers a more comprehensive perspective on real-time deployment scenarios, encompassing diverse computational and network conditions. Hence, we incorporated all 3 tiers to ensure our Edge AI setup is detailed and adaptable. One previous study [5] in Table 1 employs a simulated Edge AI Setup instead of real hardware, suggesting that such a configuration is viable for evaluating the operators' performance. Moreover, the simulated environment is more economical and handy than physical hardware, while"}, {"title": "2.1 Knowledge Distillation", "content": "Knowledge distillation [44,112, 102, 2, 101, 109, 4, 77, 78, 73, 75, 100, 74, 92, 65, 69, 76, 1,56, 106, 54,9,55,57,82] is a family of Edge AI deployment operators that aims to transfer knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) [33]. This process involves training the teacher model on a task, generating soft targets (probability distributions) using the teacher's predictions, then training the student model to replicate these soft targets while also using the original labeled data. The primary goal of knowledge distillation is to improve the performance of the smaller model while reducing its computational and memory requirements. As smaller models are less expensive to evaluate, they can be deployed on less powerful hardware (such as a mobile device). The teacher model typically has higher accuracy but may be computationally expensive for deployment in resource-constrained environments, such as mobile or edge devices. By distilling the knowledge from the teacher model into the student model, the aim is to create a more efficient and lightweight model that retains much of the performance of the teacher model."}, {"title": "2.2 QAT", "content": "Quantization Aware Training (QAT) [93,132, 22, 35, 133, 13, 71,25,50,7,115,87, 26, 105, 99, 49,111,41,116, 123,59,82,56, 106,54,9,55,57] is a family of Edge AI deployment operators that involves quantization"}, {"title": "2.3 Pruning", "content": "Pruning involves \"pruning\" certain weights (making them zero) that are deemed less critical for the model's performance. A pruned model with zero weights can result in faster computations during infer- ence as the zero weights allow for more efficient matrix operations, reducing the number of multiplications and additions. In particular, it involves identifying and removing unnecessary connections, weights, or neurons from a trained deep learning model, resulting in a more compact model that is easier to deploy and consumes less memory and hardware resources. The primary objective of model pruning is to make models more computationally efficient, reduce memory requirements, and, in some cases, improve the model's inference speed.\nThere are two kinds of pruning, i.e., physical and logical pruning, as explained below:\nPhysical Pruning: In physical pruning, the connections that received a zero weight are removed from the model. This means that these weights are no longer stored, and the computations involving these weights are skipped, resulting in a smaller and faster model. This is particularly useful for deploying models on resource-constrained devices, where storage and computational resources are limited. [40, 124, 122, 12, 11,64,8,27,52,68,51,121,89,48,41, 39, 116, 46, 123, 113,97,38,59,55,57,1,82]\nLogical Pruning: In logical pruning, the connections to be pruned are not physically removed from the model. Instead, they are masked or set to zero but still stored in the model. During inference, masked weights are multiplied by zero, effectively excluding them from computations. [126,104]"}, {"title": "3 Methodology", "content": "This section elaborates on the methodology adopted to tackle the research questions outlined in the introduction."}, {"title": "3.1 Subject Models", "content": "We opted for diverse models to maintain representativeness in our experiment, focusing on Computer Vision and NLP tasks. Our subject models were chosen from the PyTorch ImageNet Models store\u00b9 and Hugging Face store \u00b2 to ensure a range of architectures, sizes, and scopes. To capture realistic scenarios, we selected models with varying complexities for both image and text classification tasks, as outlined in Table 2. These models were selected because of their popularity in research (Tablel 1) and their compatibility with the Intel Neural Compressor (INC) tool used for the generation of white box operators. In recent studies [103,36,104], INC is commonly used for optimizing trained Deep Learning (DL) models and it provides popular model compression techniques such as low-precision quantization, pruning, knowledge distillation, and other compression techniques.\nImage classification is an ML task that aims to set a label/category to an input image. This process involves training a model using a labeled image dataset and then utilizing this trained model to predict labels for new, unseen images [72]. Likewise, text classification entails assigning predetermined categories or labels to textual documents or pieces of text, which is accomplished by training a model on a dataset containing labeled texts."}, {"title": "3.2 Study Design", "content": "We examine the trade-off between two measurable factors: latency and accuracy. This evaluation is pivotal for grasping how accuracy and latency perform in an Edge AI setup, offering insights into optimal strategy selection tailored to deployment engineers' specific needs and scenarios. Some scenarios may prioritize minimizing latency, potentially compromising accuracy, while others may prioritize accuracy despite a slight increase in latency. Our empirical findings, derived from deploying diverse operators on distinct tiers, establish a quantitative framework for assessing this trade-off. Additionally, it forms the basis for our long-term objective (beyond this paper's scope): developing recommendation systems that can automatically suggest the most suitable operators and deployment strategies based on specific requirements for latency and accuracy.\nFrom the broader set of 8 operator families for Edge AI Inference, as outlined in Section 1, we narrow our focus in this study to investigate operators specifically tailored for optimizing models in a white box manner. This entails re-training models on the training data for the white-box operators. Among the available white box operators, we have chosen three of them based on their diverse approaches and feasibility: Quantization Aware Training (QAT), Model Pruning, and Knowledge Distillation. These operators are quite popular in research as shown in Table 1. These operators were selected for their capacity to address various aspects of model optimization, including enhancing inference speed and preserving performance.\nQAT was selected as it minimizes the impact of quantization on model accuracy by training the model with awareness of the quantization process. This can be advantageous in scenarios where maintaining model performance is crucial. It explicitly considers the effects of quantization during training, potentially resulting in models that are more robust to the quantization process. Pruning identifies connections or weights that contribute less to the overall model performance and prunes them by setting the connections' weight values to zero (logical pruning) or removing those zero-weight connections (physical pruning) to create a more sparse model, which can lead to faster computations during inference. The motivation for considering Knowledge Distillation as an operator is its aim to compress the knowledge of the teacher model into a more compact student model that is easier to deploy and requires fewer computational resources. This would especially be valuable in scenarios where resources are constrained (i.e., mobile and edge tiers).\nThe performance and latency of QAT, Pruned, and Distilled models are compared with the original models and with each other to analyze their robustness in the Edge AI Environment. We compared these 3 white box operators with a family of black box operators such as Partitioning and SPTQ from a previous study [108]. Additionally, combinations of black box operators (SPTQ + Partitioning) and white/black box operators (Knowledge Distillation + SPTQ) are performed to collect empirical data to analyze how these hybrid operators perform under real-world scenarios. From the black-box operators studied in our prior work [108] and the white-box operators studied in this paper, we excluded the hybrids involving pruning due to the incompatibility of pruning to provide a performance advantage in our study, and we also excluded the Early Exiting operator due to its in-feasibility for textual subject models and"}, {"title": "3.5 Research Questions", "content": null}, {"title": "3.5.1 RQ1: What is the impact of QAT-based white box operator on latency and accuracy?", "content": "Motivation This research question aims to empirically assess the effectiveness of the QAT in comparison to baseline (SPTQ, Identity) models in terms of latency in the corresponding monolithic tiers (i.e., mobile, edge, and cloud). Moreover, the accuracy of the QAT, SPTQ, and Identity operators is validated in each of the monolithic tiers and compared with each other. The RQ helps understand how different quantization techniques perform in real-world deployment scenarios.\nApproach The SPTQ was performed using the Intel Neural Compressor (INC) tool on subject models in ONNX format. As the QAT is not yet supported for ONNX models, we have to first apply QAT to the subject models in the Pytorch format, then export the generated QAT models to the ONNX format for measuring the latency. For QAT image models, we computed the accuracy in ONNX format, but for the QAT textual models, we considered the Pytorch format for accuracy evaluation as the QAT textual models in ONNX format encountered accuracy performance issues possibly due to QAT Operations (i.e., Fake Quantization Operations) not being supported by the ONNX format.\nThe motivation to consider the ONNX format is based on its feasibility for black-box operators as ex- plained in our previous study [108]. Moreover, ONNX provides a way to migrate models between different frameworks (such as PyTorch), which allows training a model in one framework and then deploying it using another for inference. The textual QAT models were exported using Dynamic Quantization APIs to achieve the same model size as before exportation. We considered CPU instead of GPU for the QAT and SPTQ operators, due to the unsupportability of quantization in the GPUs in our setup specifically. Fixing this requires modifying the existing codebase of INC, ensuring compatibility with GPU-specific libraries,"}, {"title": "3.5.2 RQ2: What is the impact of the Pruning-based white box operator on latency and accuracy?", "content": "Motivation This question aims to empirically compare the inference latency and accuracy performance of the Pruning operator relative to Identity, QAT, and SPTQ operators in each of the monolithic tiers (i.e., mobile, edge, and cloud). This research question will answer whether the pruning operator is worth the effort to perform for latency/accuracy benefits.\nApproach For logical pruning of subject models, the hyper-parameters listed in the INC repository 1415 again were considered. The important parameters for logical pruning are the pruning pattern, the target sparsity, and the pruning type. The pruning pattern defines the rules of pruned weights' arrangements in space. The default N\u00d7M Pruning pattern for the subject models (i.e., 4\u00d71 and 2\u00d71 patterns for textual and image models, respectively) was selected. In N\u00d7M pruning, N-M weights are selected for pruning from each of the M consecutive weights. Target sparsity refers to the proportion of zero weights in the model while performing logical pruning. For textual and image models, 90% and 75% of sparsity were used, respectively, which implies that 90% and 75% of the weights have been pruned (set to zero).\nThe pruning type determines how the weights of a neural network are scored and pruned. We consid- ered the default pruning type, i.e., snip momentum, which improves the SNIP algorithm [61] by updating the scores in a momentum way. We performed distributed training with multiple GPUs (8 GPUS) for pruning of image models due to higher training iterations, large training data, and high computation- al/memory requirements. For the pruning of textual models, a single GPU was enough to provide a faster training time due to lower training iterations, small training data, and lower computational/memory re- quirements.\nIn our study, we considered the INC tool for performing pruning, as it provides automated pipelines for a wide range of subject models including both image and textual inference tasks. We considered logical pruning as it is more widely used across a wide range of subject models in the INC tool and currently, INC does not support physical pruning for the NxM sparsity pattern used in our study. Its physical pruning is under development and only supports some particular structures (i.e., transformer models) using the channelx1 pruning pattern in the INC tool. In order to ensure consistent pruning across all four subject models in our study, in this study we stick to NxM logical pruning for fair and consistent analysis."}, {"title": "3.5.3 RQ3: What is the impact of the Knowledge Distillation-based white box operator on latency and accuracy?", "content": "Motivation This question aims to empirically assess the latency of the Distilled operator and compare it with the Identity, SPTQ, QAT, and Pruned operators in the corresponding tiers (i.e., mobile, edge, and cloud). The accuracy of the Distilled operator is validated in the monolithic tiers and compared with the Identity, SPTQ, QAT, and pruned operators to assess the performance deviation.\nApproach In our study, the distillation process involves replicating the behavior of a teacher model (larger and complex architecture) by training a student model (smaller and simpler architecture) to achieve accuracy comparable to the teacher model. The student model is often designed to have a similar architecture to the teacher model. The Knowledge Distillation was performed using the INC tool from teacher to student model in Pytorch format due to its support for distillation. The generated distilled student models were then exported to ONNX format for measuring the latency and accuracy results. Table 3 shows the size and parameters of the teacher and the student models considered for knowledge distillation. Both the student and parent models contain pretrained weights. These pretrained weights provide the student model with foundational knowledge about the domain and allow quicker convergence during knowledge distillation. This helps in improving the performance during knowledge distillation as the representations learned by both student and teacher models are more likely to be compatible, making it easier for the student model to mimic the teacher's knowledge. Considering that pretrained weights of the student model are quite common in scenarios when their architecture is similar to the parent model, the training hyper-parameters listed in the INC repository 1617 again were considered. For the distillation of ResNet and ResNext subject models, distributed training with multiple GPUs (i.e., 8 GPUs) again was used. In contrast, for the distillation of Bert and Roberta subject models, a single CPU was enough to provide quick training."}, {"title": "3.5.4 RQ4: What is the impact of the hybrid applications of white box and/or black box operators on latency and accuracy?", "content": "Motivation This question aims to empirically assess the latency and accuracy of combining the white- box (Distillation) operator with the black-box (SPTQ) operator, due to the latency and/or accuracy advantage of SPTQ and Distillation in RQ1 and RQ3, respectively. In a first dimension, the DSPTQ hybrid operator is compared with non-hybrid operators (i.e., Distilled, SPTQ, QAT, Pruned, Identity) in each of the monolithic tiers in terms of latency and accuracy. In a second dimension, the Mobile-Edge Identity Partitioning (MEIP) strategy is compared with the white box operators, and the DSPTQ hybrid operator is compared with the Mobile-Edge SPTQ Partitioning (MESP) strategy.\nApproach For the DSPTQ operator, the distillation process was performed on the subject models in Pytorch format using INC as explained in the RQ3 approach. For textual subject models, the distilled models are exported to ONNX first and then the SPTQ process is performed similarly to the RQ1 SPTQ approach for generating DSPTQ models. However, for image DSTPQ models, their SPTQ process was not effectively reducing all the weights to quantized INT8 precision type in ONNX format, due to which we conducted their SPTQ process in Pytorch format before exporting the generated DSTPQ models to ONNX format for latency and accuracy evaluation.\nFor the Partitioning of the Identity and SPTQ image-based models, we followed our earlier algo- rithm [108] for creating equal-size sub-models. This algorithm traverses the ONNX computational graph of models in reverse order (from the end) and heuristically selects the partitioning point, i.e., node connection(s), that splits the model into two nearly equal-sized sub-models, which requires manually checking the sizes of partitioned models. In the context of ONNX graphs, the partitioning point refers to the location in the graph where this division of the graph into smaller sub-graphs (or sub-models) occurs."}, {"title": "3.6 Data Analysis", "content": "In the analysis of the results from the inference latency experiments, we employed several statistical methods. Initially, we utilized the Shapiro-Wilks test and Q-Q plot for each deployment strategy to evaluate the normality of the inference latency distribution. This helped us decide whether to use para- metric or non-parametric tests for hypothesis testing. As the data did not exhibit a normal distribution, we proceeded to use the Kruskal-Wallis (KW) test [91]. This test allowed us to compare the inference latency distributions among 3 or more independent groups, representing different deployment strategies. If the KW test states a significant difference, we further employ the Conover post-hoc test [16] to per- form pairwise comparisons. We focused on two dimensions, namely operator and tier, to determine if a significant difference existed among at least two independent groups through hypothesis testing. The design approach for the KW and Conover statistical tests across the tier and operator dimensions is detailed in Table 4 and Table 5, respectively.\nWhen conducting the Shapiro-Wilks, Kruskal-Wallis, and Posthoc Conover tests, we assess the ob- tained p-value against a significance level of alpha = 0.05 by default. The interpretation of Cliff's delta effect sizes [15] [43], categorizes them as negligible ($d < 0.147$), small ($0.147 < d < 0.33$), medium ($0.33 < d < 0.474$), or large ($d \\geq 0.474$). Negative values indicate that, on average, the distribution of the left"}, {"title": "3.6.1 Tier Dimension", "content": "In Table 4, we conduct a KW test ($\\alpha = 0.05$) to compare the inference latency of individual operators (Identity, Pruned, SPTQ, QAT, Distilled) across the three monolithic tiers (Mobile, Edge, Cloud) with the MEIP strategy. Similarly, we compare the inference latency of the hybrid operator (DSPTQ) across the MEC tiers with the MESP strategy. If significant differences are observed (KW Test: p-value < 0.05), we proceed with the Conover post-hoc test. In cases where there are notable differences (Conover test: adjusted p-value < 0.05) in pairwise comparisons, we use Cliff's delta effect size to gauge the inference latency ranking of strategies, considering the direction and magnitude of their differences in the relevant RQs."}, {"title": "3.6.2 Operator Dimension", "content": "For assessing the potential statistically significant differences among the Identity, QAT, SPTQ, Distilled, and DSPTQ operators within individual monolithic tiers, a Kruskal-Wallis (KW) test was conducted, as outlined in Table 5. In cases where a significant difference was detected (KW Test: p-value < 0.05), subsequent analysis employed the post-hoc Conover test. For significant pairwise comparisons (Conover test: adjusted p-value < 0.05), Cliff's delta effect size again was utilized to gauge the magnitude and direction of differences in inference latency ranks across relevant Research Questions (RQs).\nFor accuracy comparisons among operators, we utilize the Wilcoxon Signed Rank Test [24]. This test assesses the existence of a significant difference in accuracy between each pair of operators of in- terest such as Identity vs QAT, Identity vs SPTQ, QAT vs SPTQ, Pruning vs Identity, Pruning vs SPTQ, Pruning vs QAT, Distilled vs SPTQ, Distilled vs QAT, Distilled vs Pruned, Distilled vs Identity, DSPTQ vs SPTQ, DSPTQ vs QAT, DSPTQ vs Pruned, and DSPTQ vs Identity. The analysis focuses on paired groups, comparing accuracy measurements under different operators for the same subject models (ResNet, ResNext, Bert, Roberta) and environments (Mobile, Edge, Cloud). Each group comprises 18 samples of accuracy measurements, combining six accuracy metric values ([ResNet, ResNext] x [Top 1%, Top 5%] + [Bert, Roberta] x [F1 Score%]) across three environments. In other words, within a specific operator's group, we concatenate accuracy metric(s) \u2013 all percentage values \u2013 for all four subject models, facilitating the comparison of corresponding accuracy metrics through paired statistical tests."}, {"title": "4 Results", "content": null}, {"title": "4.1 What is the impact of the QAT-based white box operator on latency and accuracy? (RQ1)", "content": "The White-box QAT operator always has one of the longer median latencies in comparison to the black-box SPTQ operator, except for cloud image models.\nIn Mobile and Edge, the QAT models show 1.02x to 1.35x and 1.10x to 2.12x higher median inference latency, respectively than the SPTQ models, as shown in Figure 4 along with small or large effect sizes"}, {"title": "4.2 What is the impact of the Pruning-based white box operator on latency and accuracy? (RQ2)", "content": "The Pruned models show no practically significant latency improvements compared to Identity models in the MEC tiers. For the image subject models, the Pruned versions in MEC tiers show 1.55x/1.22x lower, 1.22x lower/1.49x higher, and equivalent average median inference latency than the QAT/SPTQ version, respectively. For the textual subject models, the Pruned versions show 1.36x/1.39x, 1.74x/1.95x, and 1.41x/2.63x higher average median inference latency than the QAT/SPTQ versions in the same tiers.\nThe inference latency of the pruned versions of the subject models, when compared with the identity version, shows either no significant difference or a significant difference but with negligible to small effect sizes in the monolithic tiers (i.e., mobile, edge, cloud) as shown in Table 6, Table 7, and Table 8. Further-"}, {"title": "4.3 What is the performance impact of the Distillation-based white box operator on latency and accuracy? (RQ3)", "content": "Except for cloud image models, the distilled models in comparison to the Identity/QAT/SP- TQ/Pruned models show significantly lower latency with large effect sizes.\nAs shown in Figure 67, the Distilled models in the mobile tier show 3.39x, 3.36x, 2.94x, and 3.34x lower average median inference latency than the Identity, QAT, SPTQ, and Pruned models, respectively along with large effect sizes. Similarly, in the edge tier, the distilled models show 3.39x, 2.66x, 1.90x, and 3.31x lower average median inference latency than the Identity, QAT, SPTQ, and Pruned models, respectively along with large effect sizes. In Figure 7, the speedup of Distilled models w.r.t. Identity, QAT, SPTQ, and Pruned models is calculated by comparing their median latency with each other. For example, the box plots in Figure 6 show that the Distilled versions w.r.t. Identity versions in the mobile tier have 1.76 to 4.73 times lower (speedup) median inference latency values across the four subject models. To present this quantitatively, we use the orange lines and red dots in the interval plot 7 to illustrate the speedup range (1.76x to 4.73x) and average (3.39x), respectively across the four subject models (as these values are greater than one). In contrast, for showing slow down, the interval plots show values smaller than one, as illustrated in other interval plots (e.g., Fig 9,10). The Distilled models are 1.84x to 4.44x smaller in size than the Identity/Pruned models as shown in Table 9, leading to faster and more efficient inference of Distilled models in restricted-constrained tiers (i.e., mobile and edge) in comparison to Identity/Pruned models. Even though the distilled model sizes of the subject models"}, {"title": "4.4 What is the impact of the hybrids of white-box and/or black-box operators on latency and accuracy? (RQ4)", "content": null}, {"title": "4.4.1 Quantitative Analysis of hybrid Distilled SPTQ (DSPTQ) operator with singular Distilled, SPTQ, QAT, Pruned, and Identity operators in Monolithic Deployment tiers", "content": "Except for cloud and mobile image models, the DSPTQ models show significantly lower latency than the distilled models with small to large effect sizes.\nAs shown in Figure 8 9, in the mobile tier, for the image and textual subject models, the DSPTQ versions show 1.19x higher (negligible or medium effect sizes) and 1.39x lower (large effect sizes) average"}, {"title": "5 Discussion", "content": "Across the three monolithic tiers, the operators (Identity, QAT, SPTQ, Pruned, Distilled, DSPTQ) for textual subject models in the cloud and image subject models in the edge show 3.54x to 9.43x and 2.65x to 7.44x lower latency, respectively compared to the remaining two tiers.\nFor textual models, as shown in Figure 6, the Identity, QAT, SPTQ, Pruned, Distilled and DSPTQ models in the cloud show 3.82x to 7.36x, 3.12x to 7.71x, 5.11x to 13.78x, 3.83x to 7.33x, 2.44x to 4.74x, and 3.75x to 8.55x lower median inference latency compared to their inference in the resource-constrained tiers (i.e., mobile and edge), respectively, along with large effect sizes (Table 10,11,12,13,14,15). This is because the cloud tiers provide faster computational latency due to higher Memory/CPU availability than resource-constrained tiers (i.e., mobile and edge) and faster transmission latency across the edge-cloud network for textual models having low data size requirements.\nFor image"}]}