{"title": "Pix2Cap-COCO: Advancing Visual Comprehension via Pixel-Level Captioning", "authors": ["Zuyao You", "Junke Wang", "Lingyu Kong", "Bo He", "Zuxuan Wu"], "abstract": "We present Pix2Cap-COCO, the first panoptic pixel-level caption dataset designed to advance fine-grained visual understanding. To achieve this, we carefully design an automated annotation pipeline that prompts GPT-4V to generate pixel-aligned, instance-specific captions for individual objects within images, enabling models to learn more granular relationships between objects and their contexts. This approach results in 167,254 detailed captions, with an average of 22.94 words per caption. Building on Pix2Cap-COCO, we introduce a novel task, panoptic segmentation-captioning, which challenges models to recognize instances in an image and provide detailed descriptions for each simultaneously. To benchmark this task, we design a robust baseline based on X-Decoder. The experimental results demonstrate that Pix2Cap-COCO is a particularly challenging dataset, as it requires models to excel in both fine-grained visual understanding and detailed language generation. Furthermore, we leverage Pix2Cap-COCO for Supervised Fine-Tuning (SFT) on large multimodal models (LMMs) to enhance their performance. For example, training with Pix2Cap-COCO significantly improves the performance of GPT4RoI, yielding gains in CIDEr (+1.4%), ROUGE (+0.4%), and SPICE (+0.5%) on Visual Genome dataset, and strengthens its region understanding ability on the ViP-Bench, with an overall improvement of +5.1%, including notable increases in recognition accuracy (+11.2%) and language generation quality (+22.2%). Code is available at https://github.com/geshang777/pix2cap.", "sections": [{"title": "1. Introduction", "content": "Existing datasets pairing visual inputs with descriptive text primarily focus on the image-level [32, 38, 51], while effective for broader context, image-level descriptions lack grounding information that precisely ties objects to their locations within an image. Recent efforts have been made to develop region-level caption datasets [18, 21, 52]. However, these datasets fall short of fully aligning visual content with descriptions, as they always include irrelevant background information in the bounding boxes (see the middle of Fig. 1). Besides, the descriptions in these datasets are often too short of describing a specific object in the image, which may lead to confusion (e.g., the boy marked with in the second column of Fig. 1).\nCreating a dataset with detailed pixel-level descriptions presents twofold challenges. Firstly, accurately recognizing and segmenting individual instances within an image requires precise delineation of object boundaries. Fortunately, this problem can be addressed effectively using existing high-quality object segmentation datasets, such as COCO [27], which provide robust annotations for object instances and their boundaries. These datasets serve as a strong foundation for training reliable segmentation models capable of handling diverse visual scenarios. Secondly, collecting high-quality, detailed, contextually precise captions that are capable of differentiating similar objects necessitates a deep understanding of the visual scene and nuanced language generation. While relying on human annotators could achieve high-quality results, the significant cost will undoubtedly pose a substantial barrier to scalability.\nRecent years have witnessed the rapid development of large multimodal models (LMMs) [23, 28, 30, 42, 47]. Among these, models like GPT-4V [31] have demonstrated exceptional capabilities in generating detailed and contextually rich textual descriptions based on visual inputs. Inspired by this, we propose an automated pipeline to annotate COCO [27] with detailed pixel-level descriptions. Utilizing the masks provided by COCO [27], our approach begins by using the Set-of-Mark (SoM) [49] to mark and differentiate the objects within an image. These marked objects are paired with carefully designed prompts to guide GPT-4V in generating detailed captions for each instance. The resulting dataset, Pix2Cap-COCO, comprises 20,550 images and 167,254 captions with an average length of 8.14 words. We further divide it into a training set with 18,212 images, and a validation set with 2,338 images. Compared to existing region-level caption datasets [13, 15, 18, 21, 52], Pix2Cap-COCO stands out for offering significantly richer linguistic diversity and more precise pixel-level annotations, while also achieving a comparable scale. To improve the caption quality further, we recruited human annotators to manually refine and correct errors in key object attributes, such as color, to establish a highly reliable validation set.\nWith the proposed Pix2Cap-COCO dataset, we introduce a novel task called panoptic segmentation-captioning, where models are required to produce panoptic segmentation masks and generate pixel-level captions for each mask. Compared to panoptic segmentation which focuses on categorizing and segmenting visual elements [19], or image captioning that aims to generate descriptions for an entire image [10, 21], panoptic segmentation-captioning presents a greater challenge by requiring the seamless integration of visual segmentation and language generation. We believe that it can facilitate the advancement of instance-level understanding [43, 45, 50, 57] by empowering models to comprehensively analyze and describe visual scenes. To resolve this task, we design a baseline by extending X-Decoder [57] with an additional captioning head, which can learn the prediction of panoptic segmentation masks and pixel-level captions in an end-to-end manner.\nTo further evaluate the utility of Pix2Cap-COCO, we incorporate it to train GPT4RoI [54], a large multimodal model that leverages region-based visual inputs to achieve detailed instance-level understanding, and observe significant performance improvements across multiple benchmarks. For example, using Pix2Cap-COCO improves GPT4RoI by 5.1% on average on ViP-Bench [5], with notable gains in recognition accuracy (+11.2%) and language generation quality (+22.2%). These performance improvements underscore the significance of Pix2Cap-COCO as a high-quality source for fine-grained alignment between visual and textual representations."}, {"title": "2. Related Work", "content": "Datasets for Visual Understanding. Advancements in computer vision research fundamentally depend on the availability of large-scale, high-quality datasets. In the domain of visual captioning, early datasets such as COCO [27], Flickr30K [51], and VizWiz [16] lay the groundwork for visual understanding by offering short, single-sentence descriptions of entire images. Building on this, the evolution of annotation granularity gives rise to region-level captioning datasets [13, 20, 39] designed for dense captioning tasks. Notably, Visual Genome [21] provides extensive object descriptions paired with bounding boxes, paving the way for instance captioning and fine-grained visual understanding. Subsequent efforts, such as the RefCOCO series (RefCOCO [18], RefCOCO+[52], and RefCOCOg[52]), emphasize natural language expressions for object localization to advance the intersection of vision and language. Despite these advancements, existing datasets are still limited to image-level or region-level, and lack the alignment between pixels and textual descriptions. To address this gap, our work introduces a novel pixel-level captioning dataset that establishes a precise correspondence between text and instance masks. This dataset provides a new benchmark for developing models capable of capturing intricate visual-textual relationships.\nGPT-4V for Dataset Generation. Recent advancements in large multimodal models, particularly GPT-4V [31], have unlocked new possibilities for automating dataset annotation, accelerating the creation of datasets that traditionally require extensive manual effort. LVIS-Instruct4V [44] and ShareGPT4V [8] prompt GPT-4V to automatically generate question-answer pairs for given images, aiming to improve the supervised fine-tuning data of LLaVA [28]. ShareGPT4Video [9] extend this strategy to video datasets, to construct a high-quality dataset for video-based visual-language tasks. These advancements collectively highlight the transformative potential of GPT-4V in improving dataset creation workflows across visual and video domains. However, the above work focuses on high-level image or video captioning without pixel-level details, failing to meet the growing demand for finer-grained visual understanding in LLMs [36, 37, 55]. Comparatively, our work presents the first panoptic pixel-level dense captioning dataset utilizing GPT-4V, advancing the visual comprehension ability of models to the next level."}, {"title": "3. Pix2Cap-COCO", "content": "3.1. Annotation Pipeline\nCOCO [27] is a widely used dataset for object detection and segmentation, containing 1.5 million instances covering a wide range of object categories. In this work, we build our pixel-level caption dataset upon COCO by directly using the panoptic segmentation masks they provide as the pixel annotations. To obtain the pixel-aligned dense captions, we first mark the instances in an image with unfilled polygons, with a unique bright object ID at the center, following [49]. In this way, instances in each image are uniquely marked, enabling precise differentiation of different objects.\nSubsequently, the images are processed with GPT-4V [31], guided by a carefully designed text prompt to generate detailed and context-aware captions.\n3.2. Dataset Analysis\nWe quantitatively compare Pix2Cap-COCO with existing region-level captioning datasets in Tab. 1.\nData Scale: Pix2Cap-COCO comprises 20,550 images sampled from the COCO dataset, with 167,254 detailed pixel-level captions. We partition it into a training set of 18,212 images and a validation set of 2,338 images. As can be seen from the overall statistics of Tab. 1(left), Pix2Cap-COCO provides significantly more object descriptions than existing datasets like RefCOCO [18] and RefCOCO+ [52]. This is achieved through our design of an effective and automated data collection pipeline, which significantly enhances scalability compared to traditional, labor-intensive annotation methods.\nCaption Length and Word Distribution: in the right side of Tab. 1 and Plot A of Fig. 3, we also conduct a detailed statistical analysis of the captions in different datasets. The average caption length in Pix2Cap-COCO is 22.94 words,"}, {"title": "4. Panoptic Segmentation-Captioning", "content": "4.1. Task Definition\nPanoptic segmentation-captioning combines visual recognition and natural language generation challenges by requiring a model to both segment and describe every individual object in an image. Specifically, the expected outputs for this task are a list of mask-caption pairs: {(mi, di)}1, where mi represents the mask of the instance i and di represents its corresponding pixel-level dense caption.\nEvaluation Metrics. Following region-level dense captioning methods [17, 48], we use mean Average Precision (mAP) to evaluate the overall performance of the model on the panoptic segmentation-captioning task. The mAP is calculated based on two sets of thresholds: Intersection over Union (IoU) thresholds for segmentation, set at 0.3, 0.4, 0.5, 0.6, and 0.7, and METEOR[3] score thresholds for dense captioning, set at 0, 0.05, 0.1, 0.15, 0.2, and 0.25. The final mAP is determined by averaging the Average Precision (AP) across all combinations of this segmentation and captioning thresholds.\nTo further assess the quality of dense captions, we apply additional metrics commonly used in image captioning tasks [24, 41, 46], including BLEU [33], CIDEr [40], ROUGE [26], and SPICE [1], to the panoptic segmentation-captioning task. We define this metric as:\n$m@kIoU = \\frac{tp}{tp + fp+fn}$\nWhere tp, fp, and fn represent the total counts of true positives, false positives, and false negatives, respectively, based on a segmentation IOU threshold k, which is set at 0.5. The variable m represents the selected evaluation metrics from the list above.\n4.2. A Simple Baseline\nWe design a simple baseline for this task by appending a captioning head on X-Decoder [57]. X-Decoder is a versatile object segmentation framework that follows encoder-decoder architecture. It extracts multi-scale feature [12, 57] through an image encoder to capture hierarchical visual information, and utilizes a transformer decoder to enable each object token to learn the object-specific feature. Subsequently, a multi-layer perceptron (MLP) is employed as the mask head to generate pixel-level segmentation masks by decoding the object tokens.\nTo generate captions for the segmented instances, we attach a dense caption head to the X-Decoder, which consists of 6 transformer layers. We pool the last layer of image features as global tokens and concatenate them with object tokens to enrich the decoder with broader contextual information. Within the dense caption head, causal attention is applied to ensure the caption is generated sequentially. Each text token attends to both preceding text tokens and concatenated tokens from the transformer decoder, capturing specific object features and their contextual interactions. We name our baseline model Pix2Cap to facilitate reference. Cross-entropy with a label smoothing of 0.1 is used as the loss function [48] for pixel-level dense caption generation:\n$L_{caption} = \\frac{1}{N+1}\\sum_{i=1}^{N+1}CE(Y_i, \\tilde{Y_i})$\nwhere \u1ef9i = p(yi|0, {yj}=0) represents the predicted score for the i-th text token yi, given the output o from transformer decoder and all previously generated text tokens {y;}=b. N denotes the total number of text tokens in the sequence. The class loss (Lclass) is defined as the binary cross-entropy between the class label and the dot-product of the object tokens and the concept embeddings, following the X-Decoder. We use a combination of binary cross-entropy and dice loss as the mask loss (Lmask). Finally, the model can be trained in an end-to-end manner using the following training objective:\n$L = A_{caption}L_{caption} + A_{mask}L_{mask} + A_{class}L_{class}$\nAcaption, Amask, and Aclass are set to 0.1, 5.0, and 2.0, respectively. Hungarian matching [6, 11] is used to find the allocation with the lowest cost. During inference, we employ beam search [4] following [48].\n4.3. Experimental Results\nImplementation Details: we train Pix2Cap (Sec. 4.2) on the training set of Pix2Cap-COCO with the input size of 1024 x 1024 for 25 epochs with AdamW optimizer [29]. The batch size is 32, and the initial learning rate is set to 10-5 with a weight decay of 0.05. We initialize our model with the weight of X-Decoder [57]. The image encoder is frozen while the transformer decoder, dense caption head, and mask head are trainable. The experiments are conducted on 8 NVIDIA A100 GPUs with 80G memory.\nMain Results: The panoptic segmentation-captioning performance is evaluated on the validation set of Pix2Cap-COCO. We design another baseline for better comparison by using GRiT [48] to detect bounding boxes and captions first, and CenterMask [22] to segment the masks on the bounding boxes (denoted as GRiT+CenterMask, please refer to the supplementary materials for details). The results are compared in Tab. 3, from which we can see that Pix2Cap can achieve competitive captioning performance on our Pix2Cap-COCO, e.g., 32.7 CIDER, consistently better than the GRiT+CenterMask baseline on all metrics."}, {"title": "4.4. Pix2Cap-COCO for Large Multimodal Models", "content": "In this section, we conduct experiments to demonstrate the benefits of our Pix2Cap-COCO in improving the instance-level understanding capability of existing large multimodal models (LMMs). We choose GPT4ROI [54] as the baseline in this section, which incorporates spatial instruction tuning with region-level caption datasets, significantly boosting LMM performance in diverse region understanding tasks like region captioning.\nThe training process for GPT4ROI [54] consists of two stages. In the first stage, the model is trained on datasets such as COCO[27], RefCOCO[18], and RefCOCO+[52], equipping it with foundational knowledge of object categories and attributes. In the second stage, it is further finetuned on more complex datasets, including RefCOCOg[52], Visual Genome (VG) [21], Flicker30k [35], LLaVA150k [28], and VCR [53] to enhance its ability to tackle complex region understanding tasks. We retrain GPT4RoI-7B in stage 2 using Pix2Cap-COCO alongside other datasets follow the original recipe of GPT4RoI and evaluate on ViP-Bench [5] and Visual Genome [21].\nViP-Bench[5] provides a comprehensive assessment for multi-modal models across six aspects: recognition, OCR, knowledge, math, relationships, and language generation. As shown in Tab. 5 and Fig. 6, incorporating Pix2Cap-COCO during the supervised fine-tuning (SFT) stage significantly boosts the performance of GPT4RoI, with an overall improvement of 5.1%. Notably, the recognition and language generation scores of GPT4RoI are increased by 11.2% and 22.2%, respectively, highlighting the effectiveness of Pix2Cap-COCO in providing rich and descriptive annotations that bridge visual recognition and language generation tasks. We provide more visualization results in the supplementary material.\nWe also evaluate our model on VG [21], and present the quantitative comparison with existing instance LMMs in Tab. 6. Compared to the original GPT4RoI, our model achieves 1.4%, 0.4%, and 0.5% gains on the CIDEr, ROUGE, and SPICE metrics, respectively. This indicates that the fine-grained captions in our dataset could enable models to better capture the inter-object."}, {"title": "5. Conclusion", "content": "In this paper, we introduced Pix2Cap-COCO, the first panoptic pixel-level caption dataset designed to advance fine-grained visual comprehension. Pix2Cap-COCO overcomes the limitations of existing segmentation or caption datasets by providing detailed, object-specific captions precisely aligned with segmentation masks. To construct Pix2Cap-COCO in a cost-effective and scalable way, we developed an automated pipeline that marks instances within images with Set-of-Mark and employs GPT-4V to generate corresponding descriptions. Building on Pix2Cap-COCO, we proposed a novel and challenging task, i.e., panoptic segmentation-captioning, which integrates the prediction of segmentation masks and detailed captions. Extensive experiments across multiple benchmarks demonstrated that Pix2Cap-COCO could not only establish new standards for instance-level understanding but also significantly enhance the performance of current multimodal models in fine-grained visual tasks."}, {"title": "6. Additional Dataset Visualizations", "content": "We provide additional visualizations of our dataset in this section. As illustrated in Tab. 7, our captions are highly detailed and explicitly capture interactions between objects. Moreover, the pixel-level object descriptions enable our dataset to effectively differentiate between visually similar objects (e.g., the oranges in row 1 and the cows in row 2). Unlike image-level and region-level caption datasets, our approach ensures precise alignment between visual inputs and captions, avoiding the common issues of misalignment seen in other datasets."}, {"title": "7. Details on Prompt Engineering", "content": "Prompt engineering plays a critical role in guiding GPT-4V to generate detailed and structured pixel-level captions for our dataset. We present two prompts we mainly used in Fig. 7. To ensure clarity and accuracy, we define the task first, specify the labeled objects and their categories, and emphasize the need for unique and detailed descriptions for each object. Instructions include capturing comprehensive details such as color, shape, texture, motion, and relative positions, along with interactions between objects. Explicit formatting guidelines are provided to maintain uniformity, and constraints are placed on retaining given categories and avoiding grouped descriptions."}, {"title": "8. Additional Implementation Details", "content": "8.1. GRIT for Panoptic Segmentation-Captioning\nGRiT[48] is a robust model designed for open-vocabulary object detection and region-level dense captioning. It inherits the open-set feature of generative methods by introducing an additional language model as the captioning head. To adapt GRiT for panoptic segmentation-captioning, we enhance it with CenterMask[22], integrated as a mask head atop the decoder outputs. The mask head processes RoIAligned features for each proposed object and predicts a binary mask for each. Following [6], an argmax operation is applied to the mask scores at each pixel, assigning categories to the masks and ensuring no overlaps. The mask head is trained jointly with all other components on the Pix2Cap-COCO dataset. To optimize performance, we perform a grid search to tune hyper-parameters like the learning rate and the weight of the mask loss. The model is trained on 8 NVIDIA A100 GPUs with the mask loss weight set to 1 and a batch size of 32 for 180,000 iterations. The learning rate is initialized at 8 \u00d7 10-5.\n8.2. Details on Panoptic Segmentation\nWe examine whether detailed pixel-level captions could enhance segmentation performance in our main paper. During the dataset construction phase, we removed annotations with low-quality captions, resulting in our dataset not fully overlapping with the original COCO. To address this, we supplement the missing data by reintroducing annotations that had been filtered out during our dataset selection process, assigning the category name of each annotation as its caption while preserving other attributes. We then train our baseline model mentioned in Sec. 4.2 on this padded dataset, using the same recipe in Sec. 4.3 for 50 epochs. Since the architecture of our model effectively functions as an X-Decoder with an added dense captioning head, by retraining this model on our dataset, we can directly compare it with the original X-Decoder to assess the impact of pixel-level dense captioning on segmentation performance.\n8.3. Details on ViP-Bench\nViP-Bench[5] is a comprehensive benchmark designed to assess the reasoning and interpretative abilities of multimodal models using 303 unique image-question pairs. Each pair combines an image with a diverse visual reasoning question, challenging the model's capacity for region-level visual understanding. The benchmark focuses on six critical aspects: recognition, Optical Character Recognition (OCR), knowledge integration, mathematical reasoning, object relationship comprehension, and language generation. Responses from multimodal models are compared against human-annotated answers and evaluated by GPT-4, which assigns scores on a scale of 0 to 10. This scoring system provides a standardized and quantitative measure for comparing model performance, offering a robust benchmark for assessing their ability to process and interpret complex visual data. We provide additional qualitative results on ViP-Bench in Tab. 9."}, {"title": "9. Qualitative Results and Failure Case", "content": "In this section, we provide additional qualitative results of our Pix2Cap model. As illustrated in Tab. 8, Pix2Cap establishes a solid baseline for the panoptic segmentation-captioning. However, in complex scenarios, it occasionally generates inaccurate object attributes (e.g., the location of the brick wall in row 5). This issue likely arises from the limited size of the object tokens (N \u00d7 512), which may lead to insufficient information allocation when processing scenes with numerous objects. We plan to explore this issue further in our future work."}]}