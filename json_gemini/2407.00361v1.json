{"title": "From RAG to RICHES: Retrieval Interlaced with Sequence Generation", "authors": ["Palak Jain", "Livio Baldini Soares", "Tom Kwiatkowski"], "abstract": "We present RICHES, a novel approach that interleaves retrieval with sequence generation tasks. RICHES offers an alternative to conventional RAG systems by eliminating the need for separate retriever and generator. It retrieves documents by directly decoding their contents, constrained on the corpus. Unifying retrieval with generation allows us to adapt to diverse new tasks via prompting alone. RICHES can work with any Instruction-tuned model, without additional training. It provides attributed evidence, supports multi-hop retrievals and interleaves thoughts to plan on what to retrieve next, all within a single decoding pass of the LLM. We demonstrate the strong performance of RICHES across ODQA tasks including attributed and multi-hop QA.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have increasingly become the backbone for much of natural language processing and there has been a push to formulate a wide range of tasks as sequence to sequence transduction. However, when LLMs need to interact with non-parametric knowledge in the form of an external evidence corpus, the typical approaches chain LLM generations with calls to a separate retrieval model as part of a multi-system pipeline. In this paper we introduce a new approach, RICHES (Retrieval Interlaced with Sequence Generation) which can natively interleave text generations with retrievals from an evidence corpus using a single LLM and decoding process.\nRICHES builds on previous work that demonstrated the application of constrained decoding to retrieval over a corpus (Jain et al., 2023; Bevilacqua et al., 2022) but extends this work to support multiple retrievals, entwined in a standard text generation procedure. In this approach, we retrieve documents by directly decoding their contents or related natural language retrieval keys that point to\nthe documents they were generated from. For example, Figure 1 illustrates a solution from RICHES to multi-hop question answering (Yang et al., 2018), where evidence must be retrieved from multiple separate documents, by iteratively generating an unconstrained 'thought' about what needs to be retrieved and then generating a supporting proposition derived from an evidence corpus and tied to an original piece of supporting text. RICHES executes this task in a single decoder pass. For this example task, which is evaluated alongside others in Section 6, we have built on recent advances in chain-of-thought reasoning via prompting alone (Yao et al., 2022) but have directly integrated the retrieval step without needing to account for any interaction with an external retrieval system.\nThe observations we build this work on are:\n1. LLMs are knowledge warehouses: They internalise and generalise over vast quantities of training data and are often able to generate surprisingly accurate knowledge in response"}, {"title": "Related Work", "content": "Retrieval Augmented Generation (RAG)\nODQA tasks predominantly employ the RAG approach (Lewis et al., 2020) where typically a dense retriever (Karpukhin et al., 2020) retrieves documents from an evidence corpus and feeds to a language model for the final answer. These pipelines involve switching between heterogeneous models and are hard to train in concert. Moreover, Dense retrievers fail to generalize out-of-domain (Thakur et al., 2021).\nGenerative Retrieval (Metzler et al., 2021) techniques shifting the onus of Search from non-parametric nearest neighbor scan to language models. Differentiable Search Index (Tay et al., 2022) memorizes a mapping of query to opaque document identifiers, however memorization struggles to generalize to unseen corpus (Pradeep et al., 2023). An alternative approach is to use natural language keys as document identifiers, where keys are constrained decoded to lie in the corpus (De Cao et al., 2020; Bevilacqua et al., 2022). These systems still need an external model to generate answers. 1-Pager (Jain et al., 2023) unifies evidence and answer generation, by generating a sequence of keywords that map to a document. However, isolated keywords limit context understanding and suffer similar pitfalls as lexical matching.\nRecitation Separate from retrieval augmentation, language models have been shown to recite entire passages from memory (Sun et al., 2022; Yu et al., 2022). But these passages are prone to hallucination. Our aim is to intersect contextual passage generation with corpus grounding. GopherCite (Menick et al., 2022), a noteworthy work in this direction, generates quotes verbatim from a small set of documents using constrained decoding. RICHES aims to scale this to a billion-token corpus.\nIterative reasoning and Search In recent times, there have been several efforts to improve multi-hop question answering by better reasoning (Asai et al., 2023) and planning (Adolphs et al., 2021; Yao et al., 2022). Language models have also been applied to the task of search to explore alternative paths (Yao et al., 2023; Hao et al., 2023).\nOur work builds on these advances in reasoning while integrating search within generation."}, {"title": "Retrieving while Generating", "content": "We present a method of interleaving unconstrained text generation with the generation of retrieval keys that point into a retrieval corpus. For example, Figure 1 shows generations that interleave unconstrained 'thoughts' with evidence sentences drawn from a predefined corpus for a multi-hop question answering task. Later in this section we'll introduce a number of different choices of retrieval key as well as a variety of tasks that benefit from interleaved generation and retrieval. However, for now we simply define a retrieval key as a sequence of tokens that exists in a pre-defined finite set of sequences K where every entry is associated with one or more documents in an underlying corpus C.\nFormally, we focus on the sequence to sequence transduction task where we predict an output sequence $y = [y_0,...,y_n]$ conditioned on an input sequence $x = [x_0,...,x_m]$ and we mark the start and end of a retrieval key in y with special markers \u00ab and \u00bb. If we let Q(y) be a function that returns all retrieval key spans from y (i.e. $(i, j) \\in Q([y_0,\\cdots, \\llparent,\\, y_i, \\cdot\\cdot\\cdot, y_j\\rrparent, ..., y_n])$) then we can update the standard autoregressive language modeling probability\n$P_0(y|x) = \\prod_i P(y_i | y_0,..., y_{i-1}, x, \\theta)$ (1)\nto include the indicator function $1_K(q)$ that maps elements of K onto one and otherwise to zero.\n$\\begin{aligned}P_0(y, x, K) = & \\frac{1}{Z} \\prod_{q \\in Q(y)} 1_K(q) \\\\\n& \\times \\prod_{i=0}^n P(y_i | y_0,..., y_{i-1}, x, \\theta)\\end{aligned}$ (2)\nwhere Z is a normalizing term that accounts for the probability mass assigned by Equation 1 to disallowed sequences. In practice, we do not need to compute Z and can sample from Equation 2 in the usual way, one token at a time, by simply zeroing out the probability of disallowed continuations as presented in Section 3.1.\nWe opt for Beam Search (Graves, 2012) as our decoding strategy to simulate a heuristic Best-first search. Here, the action or next node space is the entire vocab. At each time step, the LLM estimates"}, {"title": "Efficient Constraints via the FM-Index", "content": "During decoding, model outputs are constrained to the corpus by masking out any continuation not in the corpus. To compute the continuations of a sequence, we use FM-index (Ferragina and Manzini, 2000), a compressed suffix array augmented with additional data structures to support fast substring search operations. Unlike a Trie structure, it is also highly space economical due to the compression. Given a prefix, FM-Index can efficiently compute the next allowed tokens in O(Vocab), independent of the corpus-size. Below is the pseudo code for the modified decoding process."}, {"title": "Adaptive Beam Size", "content": "In Section 5.2 we introduce some tasks that interleave constrained and unconstrained generation. The constrained generations must be precise-to match the target retrieval key exactly. The unconstrained generations are generally more robust to small variations in surface form-these only need to convey the correct information to a reader, or to provide the model room for a \u2018thought' trace when reasoning about a response.\nTo ensure that RICHES can properly make use of beam search, which is here intended to ensure the model does not get stuck irretrievably after generat-"}, {"title": "Indexing Strategies", "content": "The FM-Index used by RICHES supports efficient indexing of all sub-strings in a corpus, which is useful when we want to generate corpus text verbatim. However, it is not clear that this is the best option of retrieval key for the auto-regressive decoder in Section 3.1. A key question in index construction is the document representation used in indexing. In traditional lexical-based retrieval systems, documents are represented by the terms in it, with transformations such as stemming, weighing by corpus statistics (Robertson et al., 2009). Neural retrieval systems transform raw text into dense vector representations and offload representation computation to the neural network. But even in this case, proper document chunking and/or multi-vector document significantly impact final performance (Lee et al., 2021; Khattab and Zaharia, 2020).\nIn this section, we introduce a few different choices of retrieval keys, including a propositional index that requires indexing time neural computation. A key consideration here is the interplay"}, {"title": "Interleaving Retrieval and Generation", "content": "We have presented a method of interleaving unconstrained text generation with constrained generation of retrieval keys. In this section we introduce a handful of tasks that make use of this interleaving either as a core task requirement, or as a means to an end by interleaving 'thoughts' with retrieval actions to help guide search.\nWe apply RICHES to the open domain question answering (ODQA) task where we score both the ability to correctly predict a short answer string and retrieve attribution for that answer (Bohnet et al., 2022). See Table 1 for examples.\nInterleaving between generation and retrieval can be powerful in multi-hop reasoning, where the model needs to retrieve and stitch together knowledge from multiple sources. Examples of RICHES outputs for multi-hop QA are given in Table 2.\nMulti-step questions often require breaking down a query into smaller steps and reasoning or planning what to retrieve next. Foreshadowing retrieval with thoughts is crucial in this context. It helps direct the retrieval process, avoid repetitions, and, more importantly, allows for iterating upon and correcting previously erroneous retrievals. A few such demonstrations can be found in Table 2."}, {"title": "Experimental Setup", "content": "Our experiments are focused on open domain question answering tasks including both single and multi-hop benchmarks. For single-hop, we use the Open-NQ (Kwiatkowski et al., 2019) dataset. To evaluate multi-hop reasoning, we look"}, {"title": "Evaluation", "content": "The standard metric for ODQA benchmarks has predominantly been F1 answer match accuracy. However, language models are prone to hallucinate and F1 stand-alone can be misleading as the answer may not be conditioned on the evidence. Attribution (Rashkin et al., 2021) helps us trade-off answer accuracy for faithfulness to the evidence. Thus, we measure two competing metrics: i) end-to-end answer accuracy with F1 and ii) attribution of the answer to evidence using AutoAIS (Bohnet et al., 2022). AutoAIS, or AIS for short, is automatically computed by classifying whether the evidence text entails the question and predicted answer pair. We re-use the NLI scorer and formulation from Bohnet et al. 2022 (See details in Appendix A.2). The evidence text here is the concatenation of all retrieval keys in the RICHES output. The unconstrained thoughts are discarded from evaluation. Only the top beam output is considered for evaluation."}, {"title": "Models and Inference", "content": "Throughout our experiments, we use off-the-shelf instruction-tuned models in a few-shot setting, without any fine-tuning. We test the instruction-tuned versions of PALM2-M and its larger variant PALM2-L (Anil et al., 2023) based on stacked Transformer architecture. We use 3 example demonstrations in our prompt (Appendix A.1), with different sets of examples for single-hop (NQ) and multi-hop (Hotpot, Musique) datasets. The unconstrained sequences or thoughts are formulated as hint keywords. Our final setup uses a beam of"}, {"title": "RICHES building blocks", "content": "We explore the following retrieval key candidates as detailed in Section 3.4: a) Title: Wikipedia page and section titles, ranking paragraphs within the section using TF-IDF scores. b) Paragraph with Title: Decodes the page title, section title, and full paragraph. c) Paragraph: Decodes the paragraph only. d) Sentence: Uses individual sentences. e) Proposition: Uses atomic"}, {"title": "Overall Results", "content": "Table 4 shows the overall performance of RICHES across various datasets. First, we compare with the no-retrieval baseline and observe that jointly retrieving and answering does not negatively impact model's answering capabilities. For single-hop tasks, RICHES competes well with dense retrievers, offering higher answer accuracy at the expense of attribution. In multi-hop QA, RICHES excels, outperforming iterative baselines by +15 F1 points on Hotpot and +11 on Musique, with comparable or better attribution. The increase in answer accuracy with the larger PALM2-L model suggests improved performance with larger model sizes. Notably, RICHES achieves these results with a single inference pass, unlike the Iterative baseline, which requires a model call at each sub-query step."}, {"title": "Qualitative analysis", "content": "We inspect 50 win and loss examples each to analyze the strength and weaknesses of the system."}, {"title": "Conclusion", "content": "Retrieval has so far been alienated from the rapid progress in instruction tuning. This work makes the following contribution: i) an approach that can seamlessly integrate retrieval with generation. ii) a thorough investigation of indexing and search"}, {"title": "Ethical Considerations", "content": "All artifacts used in this paper, including models, datasets, and baselines, are under permissive licenses and publicly available. We have attempted to provide detailed information to facilitate the reproduction of our results.\nOur findings are based on English-language data from Wikipedia, and we have not tested the generalizability of our claims to other languages or domains.\nLastly, the datasets used in this work are not expected to contain any offensive content. However, it is important to note that Large Language Models (LLMs) can exhibit biases related to gender, race, and region, and are also prone to hallucination. Although RICHES aims to ground its generation in an external corpus, some biases may still be present."}, {"title": "Appendix", "content": "We use 2 different sets of few-shot demonstration for single-hop (NQ) and multi-hop (Hotpot, Musique) datasets displayed in Table 10 and Table 11 respectively. Both prompts carry the same instruction, but the multi-hop variants provides demonstrations with multiple evidence passages."}]}