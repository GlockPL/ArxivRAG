{"title": "TDMPBC: SELF-IMITATIVE REINFORCEMENT LEARNING FOR HUMANOID ROBOT CONTROL", "authors": ["Zifeng Zhuang", "Diyuan Shi", "Runze Suo", "Xiao He", "Hongyin Zhang", "Ting Wang", "Shangke Lyu", "Donglin Wang"], "abstract": "Complex high-dimensional spaces with high Degree-of-Freedom and complicated action spaces, such as humanoid robots equipped with dexterous hands, pose significant challenges for reinforcement learning (RL) algorithms, which need to wisely balance exploration and exploitation under limited sample budgets. In general, feasible regions for accomplishing tasks within complex high-dimensional spaces are exceedingly narrow. For instance, in the context of humanoid robot motion control, the vast majority of space corresponds to falling, while only a minuscule fraction corresponds to standing upright, which is conducive to the completion of downstream tasks. Once the robot explores into a potentially task-relevant region, it should place greater emphasis on the data within that region. Building on this insight, we propose the Self-Imitative Reinforcement Learning (SIRL) framework, where the RL algorithm also imitates potentially task-relevant trajectories. Specifically, trajectory return is utilized to determine its relevance to the task and an additional behavior cloning is adopted whose weight is dynamically adjusted based on the trajectory return. As a result, our proposed algorithm achieves 120% performance improvement on the challenging HumanoidBench with 5% extra computation overhead. With further visualization, we find the significant performance gain does lead to meaningful behavior improvement that several tasks are solved successfully.", "sections": [{"title": "1 Introduction", "content": "Humanoid robots with dexterous hands have vast and promising application scenarios due to their behavior flexibility and human-like morphology [Bonci et al., 2021, Stasse and Flayols, 2019, Choudhury et al., 2018]. Unfortunately, such complex high-dimensional space is extremely challenging for policy learning with online reinforcement learning (RL) [Sferrazza et al., 2024, Peters et al., 2003], which interactively explores the environment and learns optimal decision-making from scratch under the guidance of reward function. This paradigm has achieved significant success in fields like gaming AI [Silver et al., 2016, 2017, Schrittwieser et al., 2020, Hessel et al., 2018] and quadrupedal robots control [Miki et al., 2022, Lee et al., 2020, Hwangbo et al., 2019]. But when facing humanoid robots equipped with dexterous hands, existing RL methods struggle to learn effectively and efficiently. Even sample-efficient model-based state-of-the-art (SOTA) algorithms, such as TD-MPC2 [Hansen et al., 2023] and DreamerV3 [Hafner et al., 2023], perform poorly in humanoid control.\nIn high-dimensional complex spaces, the regions capable of accomplishing tasks are typically exceedingly narrow and difficult to explore compared to the entire space. For humanoid robot motion control, upright posture is a prerequisite to complete any downstream tasks. Maintaining an upright posture is similar to balancing an inverted pendulum, where only an extremely small vertical region within the entire space can sustain this posture, while other regions lead to rapid falls. Therefore, when the algorithm explores an upright posture, the humanoid robot should place particular emphasis on it.\nFurthermore, upright posture can be intuitively reflected in return. Only if the current timestep maintains an upright posture is it possible to continue obtaining rewards in the following timesteps. If the current step results in a fall, given that humanoid robots are virtually incapable of standing up after falling, subsequent rewards become unattainable. Under the cumulative effect, the ability to maintain an upright posture will ultimately be reflected very prominently in the return. To summarize, the return can be approximated as an indicator of whether the humanoid robot has entered a task-completing region in its control.\nBased on the above observation and analysis, we propose a framework called Self-Imitative Reinforcement Learning (SIRL) to assist online learning in complex high-dimensional humanoid robot control. Building upon the foundation of RL algorithms, SIRL additionally imitates trajectories with high returns. This enables the humanoid robot to quickly learn the upright posture, thereby accelerating the completion of downstream tasks. Specifically, we augment the policy training objective in TD-MPC2 [Hansen et al., 2023, 2022a] with an additional behavior cloning term whose weight is dynamically adjusted based on the trajectory return. Since the trajectories being imitated are generated by the algorithm itself during exploration, rather than expert demonstrations as in traditional imitation learning (IL), our framework is termed self-imitative. Additionally, we refer to TD-MPC2 augmented with a behavior cloning loss as TDMPBC.\nWe have validated our proposed TDMPBC on HumanoidBench [Sferrazza et al., 2024] which contains 31 challenging tasks for the Unitree H1 robot with dexterous hands. Compared with the baseline TD-MPC2, our proposed method achieves an approximate increase more than 120% for the normalized return\u00b9. What's more, our method enjoys a significantly faster convergence rate and excels in terms of sample-efficiency. In humanoid locomotion tasks, our algorithm is capable of completing 8 tasks out of 14 with only 2M training steps, whereas the baseline could only accomplish 1 task."}, {"title": "2 Preliminaries", "content": "Reinforcement Learning Reinforcement Learning (RL) is a framework of sequential decision. Typically, this problem is formulated by a Markov Decision Process (MDP) $M = {S, A, r, p, d_0, \\gamma}$, with state space S, action space A, scalar reward function r, transition dynamics p, initial state distribution $d_0(s_0)$ and discount factor $\\gamma$ [Sutton et al., 1998]. The objective of RL is to learn a policy $\\pi(a_t | s_t)$ at timestep t, where $a_t \\in A$ and $s_t \\in S$. Given this definition, the distribution of trajectory $\\tau = (s_0, a_0,\\dots, s_H, a_H)$ generated by the interaction with the environment M is $P_\\pi (\\tau) = d_0(s_0) \\prod_t \\pi (a_t | s_t) p(s_{t+1} | s_t, a_t)$, where T is the length of the trajectory and can be infinite. Then, the goal of RL can be written as an expectation under the trajectory distribution\n$J(\\pi) = E_{\\tau\\sim P_\\pi (\\tau)} [\\sum_{t=0}^{T} \\gamma^t r(s_t, a_t)]$. This objective can also be measured by a value function $Q_\\pi(s, a)$, the expected discounted return given the action a in state s: $Q_\\pi(s, a) = E_{\\tau\\sim P_\\pi (\\tau/s,a)}[\\sum_{t=0}^{T} \\gamma^t r(s_t, a_t)|s_0 = s, a_0 = a]$.\nTD-MPC2 Model-based RL algorithm TD-MPC2 learns a latent decoder-free world model and selects actions during inference via planning with learned model [Hansen et al., 2023]. Specifically, TD-MPC2 consists of five components:\nState Encoder:  $z_t = h_\\phi(s_t)$,\nLatent Dynamics: $z_{t+1} = d_\\phi(z_t, a_t)$,\nReward Function: $\\hat{r}_t = R_\\phi(z_t, a_t)$,\nValue Function:  $\\hat{Q}_t = Q_\\phi(z_t, a_t)$,\nPolicy Prior: $\\hat{a}_t \\sim \\pi_\\theta(z_t)$,\nwhere $s_t$ is the states, $a_t$ is the actions and $z_t$ is the latent representation. The encoder $h_\\phi$, dynamics $d_\\phi$, reward $R_\\phi$, value $Q_\\phi$ compose the world model in TD-MPC2 that is trained by minimizing the following objective:\n$L(\\phi) = E_{(s_t, a_t, r_t, s_{t+1})_{t=0}^{H} \\sim B} [\\sum_{t=0}^{H}l_\\phi^{(t)} + \\frac{1}{\\gamma} \\mathbb{I}(t)]$,\n$l_\\phi^{(t)} = ||d_\\phi(z_t, a_t) - z_{t+1}||_2^2$,\n$l_\\phi^{(t)} = CE[R_\\phi(z_t, a_t) - r_t]$,\n$l_\\phi^{(t)} = CE[Q_\\phi(z_t, a_t) - (r_t + \\gamma Q_\\phi(z_{t+1}, \\pi_\\theta(\u00b7|z_{t+1})))]$,\nwhere $(s_t, a_t, r_t, s_{t+1})_{t=0}^{H}$ is a trajectory with length H sampled from the replay buffer B, $z_{t+1} = h_\\phi(s_{t+1})$ is the target latent representation and CE is the cross-entropy loss. The policy prior $\\pi_\\theta$ is a stochastic maximum entropy policy that learns to maximize the objective:\n$L(\\theta) = E_{s_0} [\\sum_{t=0}^{H} \\lambda [\\alpha \\log \\pi_\\theta (a_t,z_t) - \\alpha H (\\pi_\\theta(\\cdot|z_t))]]$,\nwhere H is the entropy of policy $\\pi$ and the parameter $\\alpha$ can be automatically adjusted based on an entropy target [Haarnoja et al., 2018] or moving statistics [Hafner et al., 2023].\nDuring inference, TD-MPC2 plan actions using a sampling-based planner Model Predictive Path Integral (MPPI) [Williams et al., 2015] to iteratively fits a time-dependent multivariate Gaussian with diagonal covariance over the trajectory space"}, {"title": "3 Self-Imitative Reinforcement Learning", "content": "In this section, we first highlight the relationship between the motion control of humanoid robots and the upright posture. We further observe that maintaining an upright posture corresponds to higher returns. Building on this insight, we introduce Self-Imitative Reinforcement Learning (SIRL) and present the implementation based on the model-based algorithm TD-MPC2 [Hansen et al., 2023]. During the online reinforcement learning process, SIRL provides additional guidance to the humanoid robot to imitate trajectories with high returns. Finally, we analyze the characteristics and applicability of this framework from multiple perspectives.\n3.1 Motivation and Analysis\nIn humanoid robot motion control, stable upright posture is the essential foundation for both locomotion tasks and whole-body manipulation tasks. This point is not only intuitive but also can be reflected in the design of the reward function. For example, in Isaac Lab, the reward term related to maintaining upright posture is assigned one extremely high weight:\nr = 200 \u00d7 $r_{upright}$ + 1.0 \u00d7 $r_{velocity}$ + \u2026, (2)\nwhere $r_{upright}$ represents an abstraction of all the reward terms associated with maintaining an upright posture, rather than a specific reward term. While the term $r_{velocity}$ rewards velocity tracking. When tracking different desired velocities, such as 0 m/s, 1 m/s and 5m/s, each corresponds to distinct tasks, namely stand, walk, and run.\nOther reward terms are omitted for simplicity. In other environments, such as HumanoidBench [Sferrazza et al., 2024], the upright term upright serves as a weight to affect the value of the entire reward function:\nr' = $r_{upright}$ \u00d7 ($r_{velocity}$ +\u2026\u2026). (3)\nHere $r'_{upright}$ \u2208 [0, 1]. Intuitively, the upright state with $r'_{upright}$ = 1 should only occupy an extremely narrow region in the whole state-action space. In contrast, the vast majority of the space belongs to $r'_{upright}$ = 0. Therefore, once the algorithm explores into the region where an upright posture can be maintained, it should place particular emphasis on it. After all, maintaining an upright posture facilitates further exploration and learning for downstream tasks. The next question is how to determine whether the upright posture has been explored in the control tasks of humanoid robots.\nIn reality, for humanoid robots, only standing upright $r_{upright}$ = 1 and falling down $r_{upright}$ = 0 are stable states. Other postures, such as $r_{upright}$ = 0.6, will eventually evolve into the stable state of falling. Moreover, once a humanoid robot falls, it cannot recover to a standing state on its own, which means that subsequent rewards become unattainable. Under the cumulative effect, the ability to maintain a stable standing posture will ultimately be very prominently reflected in the return. We further illustrate this phenomenon with the following example.\nOverall, the motion control of humanoid robots is closely related to maintaining an upright posture, and whether or not the robot remains upright can lead to a substantial difference in final return. Based on this finding, an idea for accelerating or assisting humanoid robots emerges: during the online reinforcement learning (RL) exploration process, we can provide additional guidance to the robot to imitate trajectories with high returns. This approach enables the robot to first learn how to maintain an upright posture, which then serves as a foundation for completing the entire task.\n3.2 Framework and Methods\nNow we introduce the concept of Self-Imitative Reinforcement Learning (SIRL) which aims to accelerate the learning process of humanoid robots. During the online exploration process, in addition to the basic RL loss, the policy $\\pi_\\theta$ is also required to imitate trajectories with high returns stored in the replay buffer. Unlike classical imitation learning, where trajectories are typically provided by an expert, the trajectories imitated here are generated by the policy itself. That is why we call the framework as \"self-imitative\".\nWe have implemented the SIRL framework based on the TD-MPC2 algorithm [Hansen et al., 2023]. Specifically, only the policy training loss function is modified and the difference is highlighted by red:\n$\\pounds_\\pi (\\theta) = E_{(state)}[\\sum_{t=0}^{H} \\lambda [w (R_t) \\log \\pi_\\theta (a_t | z_t)$\nSelf-Imitative\n+ $Q_\\phi (z_t, \\pi_\\theta(z_t)) - \\alpha H (\\pi_\\theta(\\cdot|z_t))$\\nReinforcement Learning (4)\nHere $R_t = \\sum_{t=0}^{T} \\gamma^t r (s_t, a_t)$ is the return of the whole trajectory and all the st, at within this trajectory have the same return Rt. Compared to the original RL loss function, Another behavior cloning loss is introduced with the weight\nw ($R_t$) = $\\beta exp(\\frac{R_t-G}{G})$ (5)\nwhere G is a reference return value used to determine the level of the current return Rt. Ideally, G = $R_{target}$ should be the target return, the standard for determining success or failure. However, this approach requires the introduction of additional prior information, which may affect the generality of the algorithm. Alternatively, we propose using the maximum return $R_{max}$ of the trajectories in the current replay buffer as the reference standard.\n3.3 Discussion and Analysis\nFrom the implementation perspective, our proposed algorithm can be viewed as TD-MPC2 + BC, which might seem similar to the offline algorithm TD3+BC [Fujimoto and Gu, 2021]. However, the scenarios and problems they address are completely different. As an offline algorithm [Zhuang et al., 2023, Fujimoto et al., 2019, Kumar et al., 2020], TD3 + BC incorporates BC [Pomerleau, 1988] to prevent out-of-distribution (OOD) state-action pairs that lie beyond the offline dataset. In contrast, TDMPBC is a fully online algorithm that integrates imitation learning to accelerate exploration and learning within complex high-dimensional spaces.\nGenerally speaking, imitation learning [Zare et al., 2024] emphasizes exploitation and is a relatively conservative algorithm, whereas reinforcement learning places greater emphasis on exploration. TDMPBC can be regarded as a RL algorithm that incorporates conservatism. With the dynamic adjustment of BC weights, TDMPBC can be seen as a process where RL explores the space first, followed by rapid learning through imitation learning. The introduction of imitation learning does indeed carry the risk of causing the algorithm to converge to local optima. However, in the context of high-dimensional complex spaces such as humanoid robot motion control, converging to a local optimum like the upright posture is highly probable and often beneficial for downstream tasks."}, {"title": "4 Related Work", "content": "Behavior control for Humanoid robots is a long-standing problem, initially explored with simplified humanoid agent [Tunyasuvunakool et al., 2020] and recently with full-size humanoid robot [Zhuang et al., 2024a, Fu et al., 2024] such as Unitree H1. Humanoid robots are of particular interest to the reinforcement learning community because of the high-dimensional action space [Merel et al., 2017, Hansen et al., 2022a, 2023, 2024]. To overcome the challenges of exploration in high-dimensional action spaces, some algorithms learn policies by imitating human behavior [Fu et al., 2024] or enhance exploration through massive parallelization [Zhuang et al., 2024a]. In contrast, our proposed algorithm attempts to learn from scratch without the aid of massive parallelization [Makoviychuk et al., 2021]. We have extensively evaluated our algorithm on the Humanoid-Bench [Sferrazza et al., 2024], a benchmark built on humanoid robot with dexterous hands [Zakka et al., 2022] that contains not only 14 locomotion tasks but also 17 whole-body manipulation tasks. In the LocoMujoco [Al-Hafez et al., 2023], the H1 robot is not equipped with dexterous hands and only focus on locomotion tasks.\nConfronted with tasks involving high-dimensional action spaces, model-based RL algorithms [Ha and Schmidhuber, 2018, Hansen et al., 2022a, Hafner et al., 2023, 2019] often prove to be more sample-efficient compared to model-free alternatives [Haarnoja et al., 2018, Fujimoto et al., 2018]. However, when it comes to humanoid robots with dexterous hands, even the SOTA model-based algorithms struggle to solve it [Sferrazza et al., 2024]. Our algorithm integrates the concept of imitation learning [Liu et al., 2023, Zhang et al., 2024] with the reinforcement learning framework, introducing a loss term of behavioral cloning [Pomerleau, 1988]. It may bear a resemblance to the offline RL [Zhuang et al., 2024b, Fujimoto et al., 2019] algorithm TD3+BC [Fujimoto and Gu, 2021] but our problem setting is completely different to theirs. Additionally, it should be noted that the SIRL framework is fundamentally an online RL paradigm that does not rely on expert data, different from IBRL [Hu et al., 2023] or MoDem [Hansen et al., 2022b]."}, {"title": "5 Experiments", "content": "We evaluate our proposed TDMPBC in HumanoidBench [Sferrazza et al., 2024], which contains 14 locomotion tasks and 17 whole-body manipulation tasks. This benchmark is built on the Unitree H1 robot with dexterous hands, which has 151-dimension observation space and 61-dimension action space. Remarkably, this benchmark aims to evaluate online RL algorithms and does not include any expert demonstration. At the same time, SIRL is an improved online RL paradigm rather than imitation learning paradigm.\nSpecifically, the experiments cover the following five aspects of TDMPBC: 1) Performance comparison with representative RL algorithms across 31 HumanoidBench tasks; 2) The impact of the selection of hyperparameter \u03b2 and reference return value G; 3) Increased runtime caompared to TD-MPC2; 4) The phenomenon of policy performance chasing the highest return in the replay buffer during algorithm training; 5) Demonstration and analysis of some representative learned behaviors.\n5.1 Performance Comparison\n5.1.1 Baselines\nWe choose these three representative online reinforcement learning algorithms as our baselines:\n\u2022 SAC (Soft Actor-Critic) [Haarnoja et al., 2018]: the state-of-the-art model-free off-policy RL algorithm with maximum entropy learning [Eysenbach and Levine, 2021];\n\u2022 Dreamer V3 [Hafner et al., 2023]: the state-of-the-art model-based RL algorithm that learns from the imaginary model rollouts;\n\u2022 TD-MPC2 [Hansen et al., 2023]: the state-of-the-art model-based RL algorithm with online planning achieved via model predictive control (MPC).\nAs for on-policy algorithm PPO (Proximal Policy Optimization) [Schulman et al., 2017], its performance is inferior without the massive GPU parallelization so PPO is not our baseline.\nIn Humanoidbench, TD-MPC2 interacts with the environment for 2M steps, which takes approximately the same amount of time as SAC and DreamerV3 interacting for 10M steps. Therefore, the default training steps for TD-MPC2 are set to 2M, while others are set to 10M. Similarly, the default training steps for TDMPBC are also 2M. For tasks where performance significantly surpasses TD-MPC2 but still shows a clear upward trend without reaching the target, we choose to continue training up to 10M steps to demonstrate asymptotic performance. These environments include the hurdle, balance_simple, balance_hard, and stair tasks in Locomotion, as well as the cabinet and window tasks in Whole-body Manipulation.\n5.1.2 Results on Locomotion\nHumanoidBench contains a total of 14 locomotion tasks, corresponding to the first 14 training curves ending with (L) in Figure 4. It should be noted that, while the locomotion tasks can be accomplished without dexterous hands, the H1 robot here is indeed equipped with dexterous hands. The entire robot has 151-dimensional observations (51 dimension for body and 50 dimension for each hand), plus a 61-dimensional action space, which is quite challenging for RL control. TDMPBC achieves significantly faster convergence and higher final performance compared to the baseline in all environments except for reach and crawl. More importantly, TDMPBC surpasses the target (represented by the grey dashed line) in 8 tasks, indicating successful task completion. In contrast, the baselines are only capable of completing the crawl task.\n5.1.3 Results on Whole-Body Manipulation\nHumanoidBench contains a total of 17 whole-body manipulation tasks, corresponding to the last 17 training curves ending with (M) in Figure 4. Whole-body manipulation requires not only the control of body posture but also the operation of dexterous hands to accomplish grasping. Although our algorithm has achieved obvious improvements, the final results are still far behind the target return. Our algorithm also struggles to simultaneously control the body and achieve dexterous hand grasping. A prematurely converging curve implies that the humanoid rapidly masters one thing while the other one fails.\n5.2 Ablation Study\n5.2.1 Ablation on hyperparameter B\nThe \u03b2 balances the original reinforcement learning and our proposed self-imitative behavior cloning in policy loss function Equation 4. Due to the presence of the exponential function and $R_t < G$, the range of behavior cloning item is between (0,1]. Meanwhile, Q is obtained by discrete regression in a log-transformed space, which means the Q has been normalized [Hafner et al., 2023]. Due to the same scale between RL loss and behavior cloning loss, the default value of hyperparameter is \u03b2 = 1. In Section 5.1, the experimental results are presented for the case where \u03b2 = 1. To further investigate the robustness of TDMPBC, we conducted additional control experiments with \u03b2 = 0.5 and \u03b2 = 2.0. We found that the performance of TDMPBC is highly robust to values of \u03b2 around 1.\n5.2.2 Ablation on reference return value G\nThe G = $R_{target}$ of the current task serves as a globally optimal benchmark but necessitates the introduction of additional information. In contrast, the maximum return G = $R_{max}$ in the current replay buffer represents the upper limit achievable by the current policy, which grows in tandem with the policy's performance as training progresses. Meanwhile, G = $R_{target}$ functions more as a pre-established, relatively higher objective. In Figure 5, we found no significant differences in overall performance between the two. Therefore, we opted for G = $R_{max}$ to avoid incorporating prior information. It is also worth noting that in tasks where the return can exceed the target, G = $R_{max}$ may ultimately surpass G = $R_{target}$.\n5.3 Runtime\nCompared to TD-MPC2, TDMPBC introduces only a marginal increase in computational burden for policy loss calculations and requires the replay buffer to additionally store the current maximum return value. We measure the time required to run three seeds simultaneously on a Tesla V100-SXM2-32G GPU across three different tasks in Table 1. When the GPU is upgraded to an NVIDIA A100-SXM4-40GB, the time can be further reduced to approximately 20 hours. On average, TDMPBC only increases the time by less than 5%, yet achieved a remarkable performance improvement of over 120%.\n5.4 Training phenomenon\nIn the experiments, we observed that the return obtained by evaluating the current policy is often lower than the maximum return in the replay buffer, regardless of whether it is our proposed TDMPBC or the baseline algorithm TD-MPC2 in Figure 9. Intuitively, it appears as though the policy is constantly chasing the current maximum return, and the behavior cloning (BC) in SIRL accelerates this.\n5.5 Behavior Visualization\nIn this subsection, we present representative behaviors obtained from TDMPBC, including the pole and balance-hard tasks in locomotion, as well as the window task in whole-body manipulation.\npole: In the pole task, the humanoid robot is required to navigate forward through a dense forest of tall, slender poles without collision. The robot trained with TD-MPC2 continuously collides with the poles and is unable to move forward properly, eventually falling over. Once step inside the pole forest, the robot swiftly moves towards one of the side walls. It then proceeds to hug the wall, keep escaping the dense poles, thereby avoiding potential collisions. This clever avoidance strategy, closely resembling human behavior, is exactly the reason our algorithm converges rapidly."}, {"title": "6 Conclusion, Limitation and Future Work", "content": "In this paper, we propose the Self-Imitative Reinforcement Learning (SIRL) framework to accelerate the online learning of humanoid robots. We have made a simple yet effective modification to TD-MPC2 by incorporating a behavior cloning (BC) loss term into the policy training loss function. Our proposed algorithm has demonstrated significant performance improvements in the HumanoidBench with a little additional computation overhead. Current research has achieved remarkable progress in locomotion tasks, yet there remains substantial room for improvement in whole-body manipulation. Looking ahead, we plan to transition TDMPBC from simulated environments to real-world deployment, further exploring its strengths and weaknesses."}]}