{"title": "Towards Data-Centric AI: A Comprehensive Survey of Traditional, Reinforcement, and Generative Approaches for Tabular Data Transformation", "authors": ["DONGJIE WANG", "YANYONG HUANG", "WANGYANG YING", "HAOYUE BAI", "NANXU GONG", "XINYUAN WANG", "SIXUN DONG", "TAO ZHE", "KUNPENG LIU", "MENG XIAO", "PENGFEI WANG", "PENGYANG WANG", "HUI XIONG", "YANJIE FU"], "abstract": "Tabular data is one of the most widely used formats across industries, driving critical applications in areas such as finance, healthcare, and marketing. In the era of data-centric AI, improving data quality and representation has become essential for enhancing model performance, particularly in applications centered around tabular data. This survey examines the key aspects of tabular data-centric AI, emphasizing feature selection and feature generation as essential techniques for data space refinement. We provide a systematic review of feature selection methods, which identify and retain the most relevant data attributes, and feature generation approaches, which create new features to simplify the capture of complex data patterns. This survey offers a comprehensive overview of current methodologies through an analysis of recent advancements, practical applications, and the strengths and limitations of these techniques. Finally, we outline open challenges and suggest future perspectives to inspire continued innovation in this field.", "sections": [{"title": "1 Introduction", "content": "With the rapid advancement of artificial intelligence (AI), the capabilities of AI models have been extensively explored and developed. To further enhance the performance and robustness of AI systems, research focus has increasingly shifted from model-centric AI to data-centric AI. High-quality data serves as the foundation for driving innovation and achieving superior model performance.\nAmong various data formats, tabular data remains one of the most popular and critical types, prevalent in fields such as manufacturing, healthcare, marketing, and logistics. Unlike unstructured data, tabular data pose unique challenges in AI, including the curse of dimensionality, complex feature interactions, and feature heterogeneity, as shown in Figure 1. Moreover, these domains often require high interpretability and face limitations in data availability compared to fields like computer vision and natural language processing. Addressing these challenges is crucial for unlocking the full potential of AI in these traditional areas.\nTo address these challenges, tabular data transformation plays a crucial role in maximizing the utility of tabular data. Figure 2 reflects the overview of taxonomy of existing transformation techniques. There are two key tasks in this transformation: feature selection and feature generation. Feature selection focuses on identifying the most relevant and informative features while eliminating redundant ones. In contrast, feature generation involves creating new, meaningful features through mathematical transformations to enhance downstream task performance. Both processes are essential to improve interpretability, predictive accuracy, and efficiency, particularly in domains where high-quality data is limited.\nFeature selection methods are broadly categorized into filter, wrapper, and embedded methods [80]. Filter methods rank features based on relevance scores derived from statistical properties of the data, such as correlation with the target variable or mutual information. For instance, the K-Best algorithm selects the top-K features based on these criteria. Filter methods are computationally efficient and suitable for high-dimensional datasets but often ignore feature dependencies, resulting in suboptimal performance [7, 26, 116]. Wrapper methods evaluate feature subsets iteratively using a predefined machine learning (ML) model. These methods typically outperform filter methods due to their holistic evaluation of feature sets but are computationally expensive, as enumerating all possible subsets is NP-hard [1, 34, 35, 78, 91]. Embedded methods integrate feature selection into the model training process by incorporating it as a regularization term in the loss function. For example, LassoNet combines the sparsity-inducing properties of Lasso with the non-linear modeling capabilities of neural networks, making it effective for handling high-dimensional data with complex relationships among features. However, embedded methods are often tailored to specific ML models and may not generalize well to others [42, 71, 73, 79, 137]. Additionally, hybrid methods combine multiple feature selection strategies and are divided into homogeneous approaches, which integrate techniques of the same type, and heterogeneous approaches, which combine techniques from different categories. The effectiveness of these methods is typically constrained by the limitations of the underlying strategies [46, 117, 123, 124]."}, {"title": "2 Fundamentals of Tabular Data", "content": "Tabular data is widely used in various AI applications and domains, including finance, healthcare, and marketing. It is structured into rows (representing instances) and columns (representing features), making it well-suited for organization and manipulation. However, compared to unstructured data (e.g., images, text), tabular data presents unique challenges,"}, {"title": "3 Traditional Approaches to Feature Selection", "content": "Feature selection, a key preprocessing step in machine learning, reduces dimensionality by identifying informative features and removing redundancy, enhancing efficiency and model performance. Methods are categorized as single-view or multi-view, with single-view approaches classified into filter, wrapper, and embedded techniques, and multi-view methods divided into supervised, semi-supervised, or unsupervised based on label availability. In this section, we discuss the advantages and limitations of traditional feature selection methods."}, {"title": "3.1 Single-view Feature Selection", "content": ""}, {"title": "3.1.1 Filter-based Feature Selection", "content": "Filter-based feature selection identifies features based on their intrinsic properties or statistical relationships with the target label, using metrics such as statistical tests, correlation, or mutual information, and operates independently of specific machine learning models.\nThe chi-square test [38, 99], a widely used statistical method, is commonly applied to test the independence of two variables or to evaluate the fit of a sample to a known distribution. This testing method is used in filter-based feature selection to identify features by constructing a contingency table between each feature and the target label, recording feature value frequencies across label categories, and calculating the chi-square statistic or p-value to identify features significantly associated with the target label. However, the chi-square test cannot be directly applied to continuous features, as it requires discretizing the data, potentially leading to information loss. In contrast, analysis of variance"}, {"title": "3.1.2 Wrapper-based Feature Selection", "content": "To address the limitations of filter-based feature selection methods, Wrapper-based Feature Selection evaluates feature subsets by training machine learning models and assessing performance metrics such as accuracy or F1 score, thereby capturing interactions between features rather than individually evaluating them. Wrapper-based Feature Selection methods are commonly categorized into three types: sequential forward or backward selection, recursive feature elimination, and genetic algorithms.\nThe first category of wrapper-based methods includes Sequential Forward Selection (SFS) and Sequential Backward Selection (SBS). SFS uses a greedy strategy to iteratively add features to an initially empty set, aiming to identify the subset that maximizes model performance based on a predefined evaluation function [158]. Conversely, SBS begins with all features and iteratively eliminates redundancies, ultimately identifying the subset that maximizes model performance [98]. SFS is constrained by the nested subset problem, where features selected in earlier iterations cannot be removed, even if they later become redundant as additional features are included. Similarly, SBS cannot reinstate features that were previously removed, which may result in the exclusion of valuable feature combinations. To address these limitations, Pudil et al. proposed Sequential Forward Floating Selection (SFFS) and Sequential Backward Floating Selection (SBFS) [118]. SFFS combines forward inclusion with backward elimination within a dynamic and flexible framework, iteratively adding performance-enhancing features and removing redundant ones to address the nested subset problem in SFS. Additionally, SBFS begins with the complete feature set, iteratively removing the least significant features while reinstating previously excluded ones if they improve subset performance. Due to the limitations of fixed criteria in SFFS, Somol et al. proposed Adaptive Sequential Forward Floating Selection (ASFFS), which employs a dynamic adjustment mechanism to adapt the inclusion and removal process based on the evolving characteristics of the feature subset. This approach enhances flexibility and improves performance, particularly in complex datasets [130]. Building on the SFFS framework, the improved forward floating selection (IFFS) algorithm introduces a novel search step called \"weak feature replacement.\" At each sequential stage, this step removes a feature from the current subset and replaces it with a new one, thereby improving overall performance [105]. To alleviate the computational burden of SFS, Ververidis et al. employed a t-test to estimate the required number of cross-validation repetitions. This method leverages the performance of a Bayesian classifier, modeled as a random variable, to reduce computational costs [146]."}, {"title": "3.1.3 Embedded-based Feature Selection", "content": "Embedded-based feature selection methods integrate feature selection and model construction into a unified framework by selecting features alongside model optimization during training. Unlike filter-based approaches, these methods capture feature interactions and prioritize those that contribute the most to model performance. Compared to wrapper-based methods, embedded-based methods avoid repeated model training, thereby reducing computational cost. Embedded-based feature selection methods can be broadly classified into three categories: regularization-based methods, tree-based methods, and neural network-based methods.\nRegularization-based methods, such as Lasso and Ridge regression, are classical approaches to feature selection that incorporate $l_1$ and $l_2$-regularization terms into the objective function, respectively. These methods effectively reduce the coefficients of redundant features to zero (in the case of Lasso) or near zero (in the case of Ridge) [104]. Until now, extensive research has introduced various regularization terms to enhance the performance of feature reduction. In [14, 107, 114, 153, 198], the $l_{2,0}$-norm is adopted to constrain the feature selection matrix, ensuring that the number of non-zero rows matches the desired number of selected features k. This approach enables the learning of a sparse selection subspace, achieving feature selection by enforcing model sparsity. The $l_{2,0}$-norm is widely recognized as a suitable choice for feature selection tasks. However, solving the $l_{2,0}$-norm directly poses significant computational challenges. To address this, the $l_{2,1}$-norm is adopted in [14, 173] as a computationally efficient alternative for enforcing sparsity. Furthermore, Nie et al.[108] proposed the $l_{2,p}$-norm as a flexible alternative to the $l_{2,0}$-norm, where p is a tunable hyperparameter within the range 0 < p \u2264 1. Additionally, Shi et al. [128] introduced the nonconvex regularizer $l_{2,1-2}$, along with an iterative algorithm, to achieve optimal sparse feature selection.\nThe second category of embedded-based feature selection approaches, tree-based methods, leverages tree models to facilitate efficient feature selection. For instance, a regularization framework is incorporated into random forest models in [25], which utilizes information gain and imposes penalties on similar features to reduce redundancy. Based on decision trees, FWDT [194] enhances the ReliefF algorithm by measuring distances between samples and their k-nearest neighbors for preliminary feature screening. It then constructs a tree model using the calculated feature weights to identify and exclude redundant features effectively. In [32], a closed-loop architecture is proposed, integrating interactive reinforcement learning with decision tree feedback. This approach uses the tree-like feature hierarchy generated by the decision tree to improve state representation and refine the reward mechanism, balancing effectiveness and efficiency in automated feature selection. Inspired by this framework, the flexible neural tree model [16] is constructed using a genetic algorithm. Feature importance is quantified through the evaluation of connection weights and activation function parameters associated with each feature. In [17], Chen et al. introduced a power set tree that maps feature subsets to tree nodes using an ordered structure. By incorporating search strategies and pruning rules, this method efficiently facilitates feature selection and identifies the optimal or near-optimal feature subset. Additionally, FSFOA [39], an evolutionary algorithm inspired by tree growth processes, reformulates the feature selection problem as a discrete search task. Using classification accuracy as the fitness function, FSFOA effectively eliminates irrelevant and redundant features. Similarly, the iTGA algorithm [192] simulates the tree growth process, optimizing feature selection by searching for the optimal tree representation of the feature subset within the tree model space.\nThe third category of embedded feature selection methods, neural network-based approaches, utilizes the structural and functional properties of neural networks to achieve efficient feature selection. In [122], significant features are identified by analyzing the activation of the first layer in deep neural networks, prioritizing those that contribute most to the overall network response. In SDAE-LSTM [93], a stacked denoising autoencoder is used for feature extraction, while feature selection is conducted by evaluating the mutual information between features and prediction targets, retaining only strongly correlated features. In [145], a feedforward neural network incorporates a regularization term into the cross-entropy error function to reduce sensitivity to input variations. Feature selection is performed by evaluating the impact of removing individual features on classification error. To address redundancy among selected features, [150] and [185] integrate neural networks with Group Lasso regularization, introducing group sparsity to prune grouped weights and eliminate non-informative features. Furthermore, to capture nonlinear relationships between data points"}, {"title": "3.1.4 Hybrid Methods for Feature Selection", "content": "Single feature selection techniques, such as filter-based methods, are computationally efficient but often overlook interactions between features. Wrapper-based methods, on the other hand, can capture these interactions but come with high computational costs. Embedded-based methods strike a balance between efficiency and interaction by incorporating feature selection into the model training process. However, their performance is highly sensitive to hyperparameters, such as the regularization strength. Hybrid methods leverage the advantages of filter, wrapper, and embedded approaches, enhancing feature selection accuracy without sacrificing efficiency. By addressing the shortcomings of individual methods, they offer a more robust and effective solution for feature selection. Recent studies classify hybrid methods into two main categories: combinations of filter and wrapper methods, and combinations of wrapper and embedded methods.\nThe first category of methods in hybrid approaches typically involves two stages: an initial global filtering step to create a candidate feature subset, followed by a wrapper-based optimization to identify the optimal subset. For example, the MIMAGA-Selection method uses mutual information maximization to select an initial feature subset and then applies genetic algorithms to further reduce the dimensionality of this subset [95]. In a similar manner, the studies in [40] and [143] both utilize multiple filter-based methods for preliminary feature screening and ranking, followed by optimization using genetic algorithms to refine the critical feature subset. FG-HFS [168] uses spectral clustering and the approximate Markov Blanket principle to remove redundant features through a filter-based approach. This is followed by a genetic algorithm to search and evaluate feature subsets, selecting the subset with the highest evaluation score. HFSIA [199] employs the Fisher filtering method to rank features based on importance, reducing the size of the candidate feature set. It then combines a metaheuristic strategy, utilizing a clone selection algorithm to search the filtered feature set until it identifies the best subset. In [52], the initial filtering stage uses information gain and F-score indicators to eliminate redundant and irrelevant features. This is followed by a sequential forward or backward search to iteratively add or remove features until the optimal subset is determined. Similarly, HFS-C-P [131] removes irrelevant or weakly correlated features using a filter-based method and clusters strongly correlated features into groups. Representative features from each cluster are then selected using a particle swarm optimization (PSO) algorithm to form the final feature subset. In this category of hybrid methods, some algorithms take an inverse approach by first applying a wrapper method to identify a candidate feature subset, followed by a filter method to remove redundant features. The method in [55] uses a genetic algorithm to perform a global search across the entire feature space to identify a candidate subset. Then, the mutual information criterion is applied to evaluate the contribution of each candidate feature, and an iterative elimination process refines the subset.\nThe second category of methods in hybrid approaches has also proven to be promising. For instance, Nugroho [110] combines forward and backward elimination strategies with cross-validation performance evaluation and utilizes random forests to optimize feature subsets iteratively. Likewise, FSPP [171] evaluates feature importance by leveraging"}, {"title": "3.2 Multi-view Feature Selection", "content": "Multi-view data is commonly found in real-world applications, where the same samples are described by heteroge-neous features from multiple perspectives [190]. When single-view feature selection is applied to multi-view data, all features from different views are simply combined into a single view. However, this approach fails to account for the complementary and consistent information shared across views [81]. To address this issue, multi-view feature selection methods have been designed to leverage both intra-view and inter-view information, enabling the identification of features that are relevant within individual views and consistent across multiple views [186]. Next, we will explore multi-view feature selection methods from the perspectives of supervised, semi-supervised, and unsupervised learning, depending on the availability of label information."}, {"title": "3.2.1 Supervised Multi-view Feature Selection", "content": "Supervised multi-view feature selection methods can broadly be clas-sified into two categories: sparsity-regularization methods and correlation-driven methods. In the first category of methods, Xiao et al. developed a two-view feature selection method for cross-sensor iris recognition based on $l_{2,1}$-norm regularization, effectively addressing sparsity while minimizing misclassification errors [162]. Lin et al. further advanced sparse regularization by introducing locally sparse constraints combined with block computing techniques, significantly improving computational efficiency for large-scale datasets through the decomposition of optimization problems into multiple smaller sub-problems [84]. Lin et al. also developed a framework combining weighted shared loss and the maximum margin criterion (MMC) to select discriminative features, emphasizing inter-class and intra-class structural information [86]. Cui et al. proposed the Multi-view Stable Feature Selection with Adaptive Optimization of View Weights (MvSFS-AOW), which dynamically adjusts view weights, incorporates unknown data, enhances robustness, and reduces the need for extensive parameter tuning [23]. To enhance the robustness of supervised multi-view feature selection, Lan et al. proposed a framework that combines capped $l_2$-norm with $l_{2,p}$ regularization to effectively address noise and ensure joint sparsity across multiple views [76]. Moreover, the difficulty in solving sparse regularization terms embedded in models has led to the development of several supervised multi-view feature selection methods, leveraging an extended Alternating Direction Method of Multipliers (ADMM) to improve computational efficiency. Lin et al. employed ADMM to design a sharing multi-view feature selection framework, reducing computational complexity and enabling efficient processing of large-scale datasets [85]. Likewise, Men et al. utilized ADMM in a supervised multi-view feature selection framework, incorporating sample-partition and view-partition strategies to compute category-specific loss terms and capture inter-view consistency and distinct characteristics [101].\nCompared to the first category of methods, the second category focuses on capturing intra-view and inter-view correlations to better represent or preserve the local and global structures of the data. Jing et al. proposed Intra-view and Inter-view Supervised Correlation Analysis (I^2SCA), a method that captures both intra-view and inter-view correlations using a unified objective function. It provides an analytical solution without iterative computation and incorporates a kernelized extension to address non-linear relationships in the feature space [64]. Similarly, Cheng et al. employed hyper-graph regularization to model complex structures in multi-view data. This method combines hyper-graph regularization with low-rank constraints to preserve global data characteristics while enhancing the representation of local relationships [20]. Xu et al. introduced multi-view scaling support vector machines, which incorporate scaling factors to adjust the contributions of features from different views. This approach emphasizes the importance of"}, {"title": "3.2.2 Semi-Supervised Multi-view Feature Selection", "content": "Supervised multi-view feature selection methods typically rely on sufficient labeled data. However, labeling is often a time-consuming and labor-intensive process, which limits the availability of labeled data in practical applications. As a result, semi-supervised multi-view feature selection, which effectively leverages both labeled and unlabeled data, has attracted considerable research interest and shown promising performance. Most existing semi-supervised multi-view feature selection methods are based on graph Laplacians. These methods guide the feature selection process by constructing similarity graphs that exploit the structural consistency between the feature and label spaces, allowing label information to propagate from labeled to unlabeled instances [60]. Hou et al. proposed a semi-supervised multi-view dimensionality reduction method, which incorporates must-link and cannot-link constraints, represented by Laplacian matrices, to capture both local geometric structures and global relationships among samples. This approach effectively guides the reduction of dimensions using limited labeled data [51]. Shi et al. combined multi-view Laplacian regularization with an $l_{2,1/2}$ norm for semi-supervised sparse feature selection. This method captures local structure through the Laplacian matrix of each view and employs a weighted fusion strategy to leverage complementary information across views [127]. Besides, Shi et al. applied Hessian regularization to encode the local geometric structure of unlabeled data [126]. Jiang et al. proposed a method for learning graph structures across different views to preserve complementary information. This approach fuses view-specific graphs to extract consistency information and dynamically optimizes similarity structures to better utilize unlabeled data [62]. Zhang et al. constructed a bidirectional graph using anchor points and adaptively learned sample similarities in the weighted feature space. A label propagation strategy was then employed to transfer label information to unlabeled samples, enhancing feature discriminability [183]. Jiang et al. further extended concept decomposition to multi-view scenarios by integrating limited labeled data via label propagation and using Laplacian graphs to capture local structure and view consistency. This approach significantly improved semi-supervised multi-label feature selection [63].\nWith the advancement of deep learning techniques, they have been increasingly integrated into semi-supervised multi-view feature selection frameworks, facilitating more efficient and effective dimensionality reduction [113, 172]. These methods utilize deep neural networks to extract complex feature representations from large-scale data, effectively leveraging both limited labeled data and abundant unlabeled data within a semi-supervised framework. Noroozi et al. proposed a deep neural network designed to learn low-dimensional feature representations for semi-supervised multi-view data. By minimizing intra-class divergence and maximizing inter-class distance, the proposed approach"}, {"title": "3.2.3 Unsupervised Multi-view Feature Selection", "content": "Obtaining labeled data for multi-view datasets is often difficult and sometimes impractical. This limitation has led to significant interest in unsupervised feature selection methods for such data. Since data from different views often share common information, many unsupervised multi-view feature selection methods have been developed to leverage this consistency and identify informative features. Cao et al. proposed a self-representation learning framework to generate multiple cluster structures, which were fused into a consensus cluster structure to provide discriminative information for feature selection [12]. Tang et al. embedded multi-view feature selection into a non-negative matrix factorization-based clustering model, learning a shared cluster indicator matrix to capture consistent clustering information across views [133]. Similarly, Bai et al. integrated common similarity graph learning and pseudo-label learning into a sparse linear regression model to enhance feature selection [3]. Liang et al. employed tensor robust principal component analysis to construct noise-free view-specific similarity matrices while adaptively learning a consensus similarity matrix to preserve the local geometric structure of the data [83]. Liu et al. utilized consensus clustering to generate pseudo-labels, which were then used as the response matrix in a sparse feature selection model to identify key features [88]. Wu et al. approached feature selection by leveraging the consistency of both global topological structures and local geometric structures within the data [159]. Zhang et al. proposed an adaptive method to learn sample-view weights for fusing cluster membership matrices, effectively capturing the consensus cluster structure for feature selection [184].\nThe aforementioned methods primarily focus on exploring commonalities across views during the feature selection process. But, beyond shared information, each view also exhibits unique characteristics. Thus, many unsupervised multi-view feature selection methods aim to exploit both the consistency and diversity of multi-view data to enhance feature selection. Zhang employed the fuzzy c-means clustering algorithm [6] to learn membership matrices for each view. These matrices were then collaboratively fused with bipartite graphs to explore the consensus cluster structure and local geometric structure [182]. Yuan et al. constructed individual similarity graphs for each view to capture view-specific information and imposed a tensor low-rank regularization constraint on the graph tensor to maintain cross-view consistency [181]. Fang et al. formulated multi-view unsupervised feature selection as an orthogonal decomposition problem, where target matrices were decomposed into view-specific basis matrices and a view-consistent cluster indicator to capture both consensus and diverse information across views [36]. Zhou et al. imposed a nuclear norm on the representation matrix to extract consensus information from different views and introduced the Hilbert-Schmidt Independence Criterion (HSIC) to promote view exclusivity [195]. Additionally, Cao et al. proposed constructing multiple mutually exclusive graphs to capture the complementarity of different views. These graphs were then used to learn a view-shared clustering indicator matrix to ensure consistency across views [11]. Tang et al. introduced the CRV-DCL method, which projects data from each view into a relaxed label space comprising common and diverse components. This approach enables the extraction of both shared and unique information for feature selection [135]."}, {"title": "3.3 Challenges and Limitations in Traditional Methods", "content": "Feature selection has proven to be highly effective in data preprocessing and dimensionality reduction, highlighting its essential role in machine learning and data mining. In recent years, a variety of single-view and multi-view feature selection methods have been developed. However, several key challenges in feature selection still need to be resolved. First, collected data often suffers from low quality, including incompleteness, noise, and sparse labeling. Designing robust methodologies to mitigate the impact of noise and outliers, while enabling effective feature selection in such data, remains a key research challenge. Moreover, in dynamic open environments, collected data frequently exhibits multi-dimensional changes at the sample, feature, and label levels. Developing feature selection models that can efficiently adapt to incremental data while preserving essential knowledge to mitigate catastrophic forgetting is a significant challenge. Furthermore, variations in view quality caused by dynamic environments, along with decision-making biases such as selection bias and label bias inherent in the original data, present another challenge. Developing trustworthy dynamic feature selection methods that balance efficiency, low redundancy, and fairness is crucial for addressing these issues."}, {"title": "4 Traditional Approaches to Feature Generation", "content": "Feature generation has been a foundational aspect of machine learning and data science, acting as a critical intermediary between raw data and effective predictive models. By deriving new features from existing ones using mathematical transformations, practitioners aim to uncover patterns, relationships, and latent insights that are often obscured in the raw data. These transformations enhance the data space, leading to improvements in model accuracy, interpretability, and generalization. This section explores the core methodologies underlying traditional feature generation, examining their strengths, limitations, and their role in shaping the evolution of data-centric AI."}, {"title": "4.1 Human-Driven Feature Generation", "content": "The feature generation process has traditionally relied heavily on the creativity and expertise of domain specialists. This approach involves applying transformations and aggregations to construct features that capture meaningful relationships within the data. The following subsections introduce various aspects of human-driven feature engineering, highlighting the methodologies and their impact on data representation."}, {"title": "4.1.1 Mathematical Transformations", "content": "Mathematical operations, such as addition, logarithmic transformations, multipli-cation, and square root calculations, are frequently employed to derive interaction features. These transformations are designed to capture complex relationships between features, thereby enhancing the expressive power of the feature space [112, 142]. For instance, logarithmic transformations compress data ranges, reducing the influence of extreme values, and are extensively applied in financial data for highly skewed variables like income or sales [157]. Taking the logarithm of annual income, for example, stabilizes variance and produces a distribution closer to normal, facilitating more robust model training. Similarly, square root transformations are effective for positively skewed data. In ecological"}, {"title": "4.1.2 Statistical Representations", "content": "Descriptive statistics, including mean, variance, and skewness, are foundational tools in feature generation. They provide concise summaries of data distributions, reveal critical patterns, and serve as the basis for more advanced analyses. Aggregations across groups further enhance this capability by capturing essential relationships in time-series and grouped data, making these statistical representations highly versatile and effective [8, 58]. For example, the mean (average) is widely used as a measure of central tendency across domains such as finance and healthcare. In financial portfolio analysis, the mean daily return of a stock is a key indicator of performance [125], while in healthcare, the average length of hospital stays provides insights into operational efficiency [59]. The median, by contrast, is robust to outliers, making it particularly useful in real estate to determine typical housing prices within a neighborhood [37]. Metrics that quantify variability, such as variance and standard deviation, are critical in understanding data dispersion. In portfolio management, variance is central to balancing risk and reward by measuring the variability of asset returns [30]. Similarly, in sensor networks, standard deviation helps identify inconsistencies in temperature readings, enabling effective fault detection [102]. Higher-order statistics like skewness and kurtosis provide additional insights into data distributions. Skewness captures asymmetry and is particularly useful for studying rare events. In climatology, for instance, it reveals the frequency of extreme temperature occurrences [119]. Kurtosis, which measures the \"tailedness\" of a distribution, is widely applied in finance to detect fat-tailed risks, such as those associated with stock market crashes [106]. These statistical representations provide concise and interpretable summaries of complex datasets, making them essential for feature engineering and machine learning."}, {"title": "4.1.3 Domain Knowledge Integration", "content": "Integrating domain knowledge into feature generation is crucial for enhancing the performance of machine learning models. Unlike generic mathematical or statistical transformations, domain-specific feature generation leverages insights and expertise from specific fields to design features that capture meaningful patterns, relationships, and metrics. This tailored approach ensures that the generated feature space closely aligns with real-world observations and task-specific requirements, often resulting in superior model accuracy and interpretability. Numerous examples illustrate how incorporating domain knowledge significantly enhances both data understanding and AI performance. In finance, the debt-to-income ratio is a widely used metric in credit risk modeling, measuring an individual's financial stability by dividing total debt by gross income [141]. Such domain-specific features provide actionable insights into economic trends and individual risk profiles. In healthcare, domain knowledge drives the creation of features that reflect patient health and medical risks. For example, the Body Mass Index (BMI), derived from weight and height, categorizes individuals into weight classes and assesses related health risks [47]. In e-commerce, domain-specific features capture user behavior, product attributes, and transactional patterns. For instance, purchase frequency measures how often a customer makes purchases, serving as an indicator of loyalty and engagement [87]. Another key metric, Customer Lifetime Value (CLV), estimates the total revenue a customer will generate for a business"}, {"title": "4.2 Automated Feature Generation", "content": "With advancements in technology, automated feature generation has become a focal point for many researchers. The central goal of this field is to replicate and enhance the feature generation process traditionally performed by human experts. By systematically uncovering complex feature interactions, modeling non-linear relationships, and iteratively refining the feature space, these techniques aim to optimize performance for downstream tasks. This section explores the key technical pillars of automated feature generation: feature interaction modeling, non-linear transformation and simplification, and iterative refinement and optimization."}, {"title": "4.2.1 Feature Interaction Modeling", "content": "Feature interaction modeling aims to identify and leverage complex relationships among features to produce new and effective ones. A common approach is feature crossing, where new features are generated by combining pairs or groups of existing features. For instance, a cross feature $x_{ij} = f(x_i, x_j)$ might be created using operations such as addition, multiplication, or concatenation, depending on the nature of the data set and the characteristics of downstream task. In prior literature, AutoCross [97] automates cross-feature generation using a beam search algorithm guided by information gain to evaluate candidate feature sets. To address the computational cost of direct evaluations, it employs field-wise logistic regression and successive mini-batch gradient descent, enabling efficient performance estimation. These innovations make AutoCross scalable and practical for large-scale tabular data applications. Additionally, deep learning methods are used to further integrate interaction modeling into model architectures. Deep & Cross Networks (DCN) [152] introduce explicit cross-feature layers: $x^{(l+1)} = W_l(x^{(l)} \\otimes x^{(0)}) + b_l + x^{(l)}$, where $x^{(l)}$ is the feature vector at layer l, \u2297 represents outer product operations capturing cross terms, and $x^{(0)}$ is the original feature vector. This formulation allows DCNs to directly model meaningful interactions during training. Neural Factorization Machines (NFMs) [48] combine deep learning with factorization principles to model interactions implicitly. They utilize low-dimensional latent representations $v_i$ for each feature i: $y = \\sum_{i<j}(v_i v_j) \\cdot x_i x_j$, enabling efficient computation of feature interactions without explicitly enumerating all combinations."}, {"title": "4.2.2 Non-linear Transformation and Simplification", "content": "In real-world applications, capturing non-linear relationships among features is essential for improving predictive performance. Linear transformations often fail to model the complexity of such relationships, prompting researchers to develop automated frameworks that integrate non-linear transformations. These frameworks enhance the expressiveness and flexibility of feature spaces, enabling models to better capture intricate patterns in the data. One common approach involves constructing polynomial features, as demonstrated by Polynomial Networks [94]. These networks automatically generate polynomial features from a dataset with features $x_1, x_2,..., x_n$ up to a specified degree k. The generated features take the form: $\\phi_k(x) = {x_1^{a_1} x_2^{a_2} ... x_n^{a_n} | \\sum a_i \\leq k}$. By introducing higher-order interactions among variables, these features significantly expand the model's ability to capture complex relationships. Another technique, Kernel Learning [22], maps features into higher-dimensional spaces using kernel functions. For example, the radial basis function (RBF) kernel is defined as: $K(x, x') = \\exp(-\\frac{||x - x'||^2}{2 \\sigma^2})$, where \u03c3 determines the smoothness of the kernel. This transformation effectively separates non-linearly separable data in the original feature space, enabling models to learn more accurate decision boundaries. Building on these ideas, Deep Polynomial Networks (DPNs) [21] combine polynomial transformations with the representational power of deep learning. These networks incorporate polynomial transformations into their architecture as layers, expressed"}, {"title": "4.2.3 Iterative Refinement and Optimization", "content": "When human experts perform feature generation", "as": "n$f^{(t)"}, {"example": "n$\\xi(f) = E_{(x", "hat{y}(f))": "nwhere D is the dataset", "iteration": "n$F^{(t+1)} = f^{(t)} \\cup {"}]}