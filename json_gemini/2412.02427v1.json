{"title": "GerPS-Compare: Comparing NER methods for legal norm analysis", "authors": ["Sarah T. Bachinger", "Christoph Unger", "Robin Erd", "Leila Feddoul", "Clara Lachenmaier", "Sina Zarrie\u00df", "Birgitta K\u00f6nig-Ries"], "abstract": "We apply NER to a particular sub-genre of legal texts in German: the genre of legal norms regulating administrative processes in public service administration. The analysis of such texts involves identifying stretches of text that instantiate one of ten classes identified by public service administration professionals. We investigate and compare three methods for performing Named Entity Recognition (NER) to detect these classes: a Rule-based system, deep discriminative models, and a deep generative model. Our results show that Deep Discriminative models outperform both the Rule-based system as well as the Deep Generative model, the latter two roughly performing equally well, outperforming each other in different classes. The main cause for this somewhat surprising result is arguably the fact that the classes used in the analysis are semantically and syntactically heterogeneous, in contrast to the classes used in more standard NER tasks. Deep Discriminative models appear to be better equipped for dealing with this heterogenerity than both generic LLMs and human linguists designing rule-based NER systems.", "sections": [{"title": "1 Introduction", "content": "The application of Natural Language Processing (NLP) to legal texts in German is a relatively new development, starting in an era where deep discriminative approaches to Named Entity Recognition (NER) have already been established as state of the art technologies. Hence, many implementations of NER for legal documents in German turn to deep discriminative ML approaches directly, without systematically comparing these technologies to alternative approaches (Leitner et al., 2019; Darji et al., 2023; Peikert et al., 2022)\nWe aim to to fill this gap by comparing three different approaches: rule-based methods (symbolic AI), deep discriminative models and deep generative models. Moreover, we run this comparison on a dataset that is very close to real-world applications that NER in the legal domain may be used for, rather than re-using the standard NER benchmark datasets. The application scenario for the GerPS-NER dataset that we chose to work with (Feddoul et al., 2024) is to assist humans in analyzing legal bases\u00b9 with the goal of creating digitized versions of administrative process schemata in the public administration.\u00b2 Since the dataset we chose includes some highly structured sub-types of legal language, we believe that it makes sense to include a rule-based approach in the comparison, as this approach is known to be well suited for such texts.\nEach of the approaches we compare comes with different trade-offs in terms of development effort and adaptability, amounts of training data needed and prediction accuracy. While rule-based approaches can deal well with structured text, it is a time-consuming task to create the rulesets, they are relatively sensitive to errors in the dataset and can only detect known patterns. Deep generative systems bring with them a lot of contextual knowledge about the world that the data is embedded in. On the other hand, they require large amounts of computational power, and they are expecting continuous text as input and output, making them more difficult to work with when strict formats have to be adhered to. Deep discriminative models have long been used in NLP tasks due to the relatively reliable performance they deliver. The challenge with deep discriminative models is that they require large amounts of training data.\nThe remainder of this paper is organized as follows: Section 2 outlines the relevant literature. Section 3 describes the methodology used in the study."}, {"title": "2 Related work", "sections": [{"title": "2.1 NER in general", "content": "General surveys of approaches to NER can be found in Yadav and Bethard (2019) and Pakhale (2023). Both surveys discuss deep discriminative approaches, which have become state of the art in NER tasks until recently, when large language model-based approaches (in the following deep generative approach) appeared on the scene (Wang et al., 2023; Bogdanov et al., 2024; Ye et al., 2024; Monajatipoor et al., 2024; Jung et al., 2024; Zhang et al., 2024; Naguib et al., 2024). Different authors compare the effectiveness of LLM-based NER in the legal domain: for LegalLens, Bernsohn et al. (2024) compared BERT models with open source LLMs on the domain of legal violation identification. Joshi et al. (2024) proposed \u201cIL-TUR, a benchmark for Indian Legal Text Understanding and Reasoning\" and offer among other things a LLM-based pipeline for the benchmark. With LAiW, Dai et al. (2023) propose a benchmark for Chinese legal LLMs. Bachinger et al. (2024) systematically evaluate different open source LLMs for their effectiveness in German text generation and use Prompt Engineering and Fewshot Prompt-ing for NER on German legal texts. Their investigation on a small subset show optimistic results for one of their prompting schemes in combination with the German open source LLM LeoLM.\nWhile the success of the deep discriminative, and to a lesser degree the deep generative, approaches to NER seems to have made rule-based approaches obsolete, the simplicity and robustness of rule-based approaches make them still strong competitors at least in some domains. For instance, Gorinski et al. (2019) systematically compare a rule-based NER system for electronic health records with deep learning and transfer learning systems. They found that the hand crafted rule-based system consistently outperforms both the transfer learning and the deep learning systems, reaching an overall F1-score of 0.95.\nIn systematically comparing rule-based approaches not only to deep discriminative, but also to deep generative approaches we hope to provide a broader evaluations of the options currently available for NER applications in the legal domain."}, {"title": "2.2 NER in legal documents", "content": "NER systems for legal documents have been developed for a long time (Dozier et al., 2010). Rule-based approaches to NER in legal documents are mostly developed for languages lacking robust resources for ML development, such as Afan Oromo (Raja et al., 2019) or Arabic (Abdallah et al., 2012). The latter work stands out by combining a rule-based approach with machine learning and evaluating the effectiveness of both approaches. The authors find that the combined approach improves the F1-score by 8 \u2212 14% compared to either the rule-based appraoch or the machine learning approach alone.\nThe entities recognized by NER systems for legal documents usually center around entities related to the court system (judge, lawyer, court, court decision, jurisdiction, etc.) and references to sections in law texts. Our work seeks to find entities related to legal norms for the administration of public services."}, {"title": "2.3 NER in German legal documents", "content": "NER systems for legal documents in German are mostly based on deep discriminative approaches. Thus Leitner et al. (2020) present a relatively large newly created dataset for German legal NER (German LER dataset) and also evaluate the performance of multiple differently configured BiLSTM-CRF models on this dataset. Darji et al. (2023) fine-tuned a German BERT model on the German LER dataset and present their results that are better than the results originally achieved by Leitner et al. (2020) when presenting the dataset. Z\u00f6llner et al. (2021) used, among others, the German LER dataset when they compared the effect of different pre-training techniques for small BERT models and presented modified fine-tuning processes which resulted in performance improvements. Erd et al. (2022) used the same two architectures that will also be used in this paper (BiLSTM-CRF, XLM-ROBERTa (Conneau et al., 2020)) to evaluate and compare the performance improvements that different data augmentation methods and their combinations might achieve for NER tasks in the German legal domain. Feddoul et al. (2024) present GerPS-NER, a new corpus for NER on German legal texts covering the sub-genre of legal norms regulating the administration of public services."}]}, {"title": "3 Concept", "content": "The workflow for our experiments is shown in Figure 1.\nWe use three different approaches to annotate German legal texts for the occurance of expressions belonging to one of ten classes, as seen in Figure 1. The classes were previously derived by GerPS-NER (Feddoul et al., 2024).\nThe first approach is a rule-based approach with a linguist drafting suitable rules from gold standard examples.\nTo implement the deep discriminative approach, we selected two popular models that have also been used by Erd et al. (2022). The first is a BiLSTM-CRF model implemented with the FLAIR framwork (Akbik et al., 2019), the second is the XLM-ROBERTa (XLM-R) (Conneau et al., 2020) transformer model, implemented using the FLERT extension (Schweter and Akbik, 2021) of the framework.\nThe deep generative approach is based on Bachinger et al. (2024) who compared several LLMs (Large Language Models) for their performance on the task of legal norm analysis on a small dataset. We use the prompting scheme they deemed the best consisting of the task description, three examples per class and the annotation guideline. We also use the German LLM LeoLM (Pl\u00fcster, 2023) for prompting to see whether the promising results for the small dataset hold up in a systematic evaluation.\nThe dataset used in the evaluation is GerPS-NER published by Feddoul et al. (2024). We adapted it slightly for our purposes as we found tokenization problems in the corpus. We split the corpus in 20% development, 20% test and 60% training data. As shown in Figure 1, the train data split was only used by the deep discriminative approach, while the others used the dev split for creating rules and testing the code."}, {"title": "4 Implementation", "content": "In the following, we describe implementation details for the approaches. The code is available on zenodo (Anonymous, 2024)."}, {"title": "4.1 Metrics for evaluation", "content": "As it is custom in the evaluation of NER tasks, we use the F1-score and the associated precision and recall values. For the overall evaluation we use the macro F1-score since in our application-scenarion all classes are of equal importance. However, our main focus is on the per-class scores.\nMeasuring precision and recall, and hence calculating the F1-score, is essentially a token-based procedure. However, most of the entities we try to find cover spans of several tokens (see Appendix C). This opens up the possibility that a prediction based on a certain rule may not cover exactly the same span of tokens as the ground truth. Token-based evaluation metrics would systematically count spurious false positives and false negatives and lead to lower values of precision, recall and F1-score values when prediction and ground truth only partially overlap. For these reasons we have decided to supplement these token-based measures with span-based ones.\nThe span-based measure we propose to use is the intersection over union measure, or Jaccard similarity score. This metric is commonly used in image processing tasks, but Soleimani et al. (2021) uses it for measuring text-span overlap (text-span similarity). We calculate the Jaccard score for each class individually. Moreover, two of our approaches, the rule-based approach and the deep generative approach, split the corpus in a large number of small files containing one sentence each. This means that we must aggregate the results for each class per file and find a basis for a global evaluation. To do this, we determine the arithmetic mean of the Jaccard score values of all file inputs. In addition, we collect the number of inputs with Jaccard score 0, i.e. cases of zero-overlap between prediction and ground truth, and the number of total sentences where a given entity occurs. This information helps to interpret the arithmetic mean of Jaccard scores. After all, relative low values of the mean of the Jaccard scores could be due to two different scenarios: first, there might be a certain number of sentences with a high Jaccard score (near perfect overlap between prediction and ground truth), and second, all sentences may show some overlap between predic-"}, {"title": "4.2 Rule-based approach", "content": "The rulebased approach is implemented in the SpaCy framework, using token patterns and phrase patterns within SpaCy's EntityRuler. Token patterns allow the use of morphological features in the rules. Phrase patterns match against exact word or phrase matches and are well suited to implement word-list based patterns (gazetteers). Several phrase patterns are dynamically constructed programmatically by looping over the tokens of the input text, applying filter functions defined in Python.\nThe design choice to utilize Spacy's EntityRuler helps to keep the rules simple and easily maintainable. However, it also means that it is not possible to use syntactic information such as provided by SpaCy's dependency parser. This in turn means that some patterns, for instance those defining the entity Bedingung 'condition', are inherently limited in the depths of recursion they can cover.\nPatterns for the entities Handlungsgrundlage 'legal or formal grounds for the action described' and Hauptakteur 'main actor' utilise word-list patterns (gazetteers) derived from the language model based on legal texts in German contained in the FLAIR framework flair/ner-german-legal. These patterns are augmented by patterns derived from the development set."}, {"title": "4.3 Deep discriminative approach", "content": "For the BiLSTM-CRF model, we use stacked German fastText (Bojanowski et al., 2017) and German forward and backward FLAIR embeddings (Akbik et al., 2018). The model is trained using the default Stochastic Gradient Descent without momentum, with gradients clipped at 5. The maximum number of training epochs is set to 150, but the learning rate is annealed based on performance on the development set, with training stopping early if the learning rate drops below 0.0001. Variational dropout is applied.\nFor the XLM-R model there are fewer parameters to configure. We chose this transformer model because preliminary studies on an early version of a subset of the corpus showed that it outperforms other German models we tested. The default setting in FLERT is to use the Adamw optimizer. We finetune the model with a learning rate that increases from 0 to 5 e-6 during the warm-up phase and then decreases linearly to 0 by the end of the training.\nFor both models, we conducted a grid search to select the hyperparameters. For the BiLSTM-CRF model, we found that a learning rate of 0.05 (from"}, {"title": "4.4 Deep generative approach", "content": "The approach works as follows: for every sentence from the dataset, ten prompts (one for each class) are created. These prompts contain the class definition in addition to the components mentioned above. The prompts are given to LeoLM and the completion is saved to a text file. The completions are checked for their length and for the content of the predictions, so that only predicted sentences containing the same tokens as the input sentence are processed further. Next, the valid predictions are consolidated into one sentence. Because there may be multiple predictions for a given token, Bachinger et al. (2024) use two different variants of sentence consolidation, a so-called optimistic and a pessimistic sentence consolidation. Optimistic sentences consolidation means that if one of the model's possible predictions matches the ground truth, this particular prediction is chosen. Pessimistic sentence consolidation means that if there are multiple predictions for a token, a new class X is assigned that represents conflicting annotations. For each file in the test, we generate an optimistic (GenAI opt), a pessimistic (GenAI pes), and a gold standard IOB file from the dataset as the tokenization in this approach varies slightly from the dataset."}, {"title": "5 Results", "content": "This section presents the results of the model evaluation, divided into two parts: an overview of overall performance and a detailed analysis of individual model performances."}, {"title": "5.1 Summary of key findings", "content": "In this section, we outline our key findings. The micro F1-scores for the model predictions are presented in Table 1, while the Jaccard score are shown in Table 2. Additionally, these results are visualized in Figure 2. Overall, the XLM-R model outperformed all other models across most classes, except for the Datenfeld 'data field' class. Contrary to expectations, the deep generative model did not outperform the other models. Even the optimistic interpretation of its outputs resulted in the second-lowest macro F1-score, just above the pessimistic interpretation. The rule-based approach was the only one to surpass the XLM-R model performance in at least one class and also outperformed the deep generative approach in terms of overall macro F1-score."}, {"title": "5.2 Detailed performance analysis", "content": "The following two sections will take a closer look at the individual model performances.\nF1-score Analyzing the results for the rule-based approach, the F1-scores for the respective classes are generally not very high, but range from 0.43 to 0.63. This indicates that there is not a lot of variation in the performance of the rules implementing the various classes. A similar trend is observed for the deep discriminative models, which perform consistently across all classes with a slightly broder and higher range, from 0.52 to 0.84. The deep generative approach provides two evaluation reports (see Subsection 4.4), with the F1-scores for the optimistic sentence consolidation exceeding those of the pessimistic one, as expected. Notably, the Handlungsgrundlage \u2018legal grounds for action' class is less affected by the pessimistic consolidation scheme and remains the best-performing class by a significant margin. The Datenfeld 'data field' class is an exception for all mentioned models, with scores as low as 0.07, 0.17, 0.03, 0.11 and 0.18 for BILSTM-CRF, XLM-R, GenAI opt, GenAI pes and the rule-based approach, respectively, with the rule-based approach achieving the highest score. The XLM-R model achieved twice the score of the BILSTM-CRF for the Datenfeld 'data field' class, despite both models generally yielding similar results. The Datenfeld 'data field' class has proven to be notriously difficult to define, annotate manually and capture in rules. It should therefore be considered an outlier and may best be excluded from consideration.\nJaccard The Jaccard similarity score for a given class provides a measure of how closely the text spans marked as instantiating the class overlap between prediction and gold standard, in a given document. Since the corpus is split into many documents covering a sentence each, we take a score for every sentence and must look at the arithmetic mean value of these scores in order to understand how well the system's prediction for a given class performs. However, the mean of the Jaccard simi-"}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 Rule-based approach", "content": "The token-based F-score evaluation requires little comment. We therefore focus here on the span-based evaluation based on the Jaccard similarity score (or Intersection-over-Union measure). In order to see the value of adding the Jaccard score"}, {"title": "6.2 Deep discriminative models", "content": "The scores show that the Datenfeld 'data field' class is the most difficult class to predict, by a large margin. One possible explanation for this could be that it requires much prior knowledge about the actual process and context to be able to judge whether something classifies as Datenfeld \u2018'data field' or not. This idea is also supported by the fact that the transformer model solves this task better than the BiLSTM-CRF model (it achieves approx. twice the score). Besides that, the classes Frist 'deadline' and Mitwirkender \u2018contributor' are probably difficult to predict for the models, because even humans struggle to differentiate between Hauptakteur 'main actor' and Mitwirkender 'contributor' and Frist 'deadline' and Bedingung 'condition' in many cases. This has an additioal adverse effect: it raises the probability that the annotations of these edge-cases are inconsistent and thereby makes learning harder for these difficult cases.\nFor both models the classes Signalwort 'signaling word' and Handlungsgrundlage 'legal grounds for action' are among the best-performing. In the latter case this is expected, given that instances of the class Handlungsgrundlage 'legal grounds for action' are relatively easy to identify based on their structure (e.g. \u00a7 44b Absatz. 1 Satz ). The class Signalwort 'signaling word' has a more heterogeneous definition (see Appendix A) which would suggest that classification is more difficult. However, this class has an easily identifiable core in the form of modal verbs or zu-infinitive. Apparently, this core is significant enough that recognition proves to be robust.\nRegarding the Jaccard Scores it interesting to see that Handlungsgrundlage 'legal grounds for action' also ranks second place with the BiLSTM-CRF model here even though its F1-score only"}, {"title": "6.3 Deep generative models", "content": "In comparison, we see lower values for F1-score for this approach as compared to the work from Bachinger et al. (2024), though in the latter, micro F1-score was used as a measure as compared to macro F1-score here. The best class for both metrics is Ergebnisempf\u00e4nger \u2018recipient of service'. The generative approach scores better in general according to the token-based evaluation, which might be due to the fact that the annotation scheme in the prompt is token based."}, {"title": "6.4 Comparison", "content": "Based on our results, deep discriminative models outperform both the rule-based approach and deep generative models, with the rule-based approach slightly outperforming deep generative models.\nThat deep discriminative models perform well in this task is consistent with other findings in NER research (Yadav and Bethard, 2019; Pakhale, 2023); but that both rule-based and deep generative approaches only reach the modest scores that we report is surprising, given results in other published research. Gorinski et al. (2019), for instance, compare a rule-based NER system with deep learning and transfer learning systems in the domain of public health records. They found that the hand crafted rule-based system consistently outperforms both the transfer learning and the deep leraning systems, reaching an overall F1-score of 0.95. Wang et al. (2023) claim that while unsophistacted applications of LLM to NER perform inferior to supervised learning models, their suggested way of adapting LLMs to the NER task improves performances to state of the art baseline levels.\nThat both the rule-based approach and the deep generative approach take performance hits at roughly the same scale strongly suggests that there must be a common cause affecting both approaches, rather than individual causes having to do with the implementation of each.  The most obvious cause lies in the definition of the classes used in the task. As is apparent from the definitions and examples in Appendix A, the classes are heterogenous with respect to semantic and syntactic types, amalgamating linguistic categorization and legal or administrative classification schemas. Gorinski et al. (2019), in contrast, had linguists designing the classes in close cooperation with domain experts to come up with linguistically motivated entity classes, and Wang et al. (2023) default to the general linguistically motivated entity definitions such as Location or Organization. It appears that human linguists and general LLMs both have similar difficulties processing heterogeneous classes. deep discriminative models, on the other hand, are able to learn heterogenous class patterns much more easily."}, {"title": "7 Conclusion and future work", "content": "In this paper, we compared three different approaches for supporting legal norm analysis on German legal texts. We find that the deep discriminative models performed best in 9 out of 10 classes. For class Datenfeld 'data field', the rule-based approached performed better but the class is in general not well predicted. Future work may explore the integration of these methods, with a promising direction being the combination of deep discriminative approaches and rule-based techniques. Previous research has found this combination to be productive, c.f. for instance the work of Abdallah et al. (2012).\nAnother intriguing option is the combination of the rule-based approach with the Deep Generative approach. While the latter did not perform as well on the larger dataset compared to the results reported by Bachinger et al. (2024), it stands to reason that the combination with the rule-based approach might improve the performance of both. Since neither the rule-based approach nor the Deep Generative approach requires the retraining of data models, this combination could potentially keep implementation and development costs in a real-world scenario low. One way to combine these approaches would be to automatically extract the n best matches of the rule-based system for a given class and use these as examples in the prompt for querying an existing LLM."}, {"title": "8 Limitations", "content": "For the deep discriminative approach, the hyperparameter optimization of both models was limited by the available resources. Without such limitations, the search space for the grid search could have been extended to find better hyperparameters.\nFor the deep generative approach, there were some limitations due to using a pre existing implementation. The examples in the prompt were predefined and from a smaller data set. They may not be representative of the classes in the overall corpus. Also, the texts used in the related work by (Bachinger et al., 2024) were annotated by different annotators and a different annotation guideline than other parts of the corpus, while maintaining the same classes.\nThe rule-based system, in part due to early design decisions, can not make use of syntactic information such as dependency relations or phrase structure. Moreover, overlapping annotations and multiple annotations can not be used because we had to stick to the CoNLL 2002 annotation scheme, which does not allow for multiple NER annotations. We plan to address these limitations in the near future."}, {"title": "A GerPS-NER dataset classes", "content": "Hauptakteur \u2018main actor' \u2013 The office or person that is mainly responsible for the administration of the service. E.g. Agentur f\u00fcr Arbeit 'Federal Employment Agency'\nErgebnisempf\u00e4nger \u2018recipient of service' \u2013 Person or company applying to receive the benefits of the service in question. E.g. Antragsteller 'applicant'\nMitwirkender 'contributor' - External office or actor that needs to give input at specific points in the administration of the service. E.g. Deutsches Patent- und Markenamt 'German Patent and Trade Mark Office'\nAktion 'action' \u2013 Action carried out by one of the actors in the course of the administration of the service. E.g. erteilen 'to grant'\nDokument 'document' - Documents that the actors exchange between them. E.g. Antrag 'application form'\nSignalwort 'signaling word' \u2013 Word or expression influencing the degree of obligatoriness of a decision made ont he basis of this statute. E.g. modal verbs such as kann 'may'; zu-Infinitiv Die Genehmigung ist zu erteilen 'permission is to be granted'; adjectives or adverbs such as angemessen \u2018appropriate' or berechtigt 'being eligible'; phrases such as auf Wunsch 'if desired'\nBedingung 'condition' \u2013 Preconditions for taking an action. Mostly expressed by conditional clauses.\nFrist 'deadline' \u2013 Time limits for certain steps in the administrative process; temporal preconditions. E.g. sp\u00e4testens am zehnten Tage vor der Wahl 'at the latest on the tenth day before the election'\nDatenfeld 'data field' \u2013 Expressions that indicate the content of a data field in a form. E.g. Vollst\u00e4ndige Anschrift 'complete address'\nHandlungsgrundlage 'legal grounds for action' - Cross reference to the legal basis for the administrative process in question. E.g. \u00a73, Absatz 2 des Patentgesetzes 'Paragraph 3, section 2 of the patent law'"}, {"title": "B F1-score evaluation results", "content": ""}, {"title": "C Tokens and spans in example annotation", "content": "An example of the gold standard annotation of Corpus/corpus_v2/test/1009.conll is given in Table 4.\nNotice that class annotations typically span multiple tokens. This is typically the case in classes that are mostly associated with linguistic expressions at the clause level, such as Bedingung \u2018condition.' But also classes which are often expressed by single token spans such as Signalwort \u2018signaling word' (see token 8) can at times span multiple tokens, as is the case in this example im Einvernehmen \u2018with approval' in tokens 26\u201327 (indicating that the main actor is not completely free in the determination of the action but must involve another agency as a contributor)."}]}