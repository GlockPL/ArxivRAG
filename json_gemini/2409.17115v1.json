{"title": "PROGRAMMING EVERY EXAMPLE: LIFTING PRE-TRAINING DATA QUALITY LIKE EXPERTS AT SCALE", "authors": ["Fan Zhou", "Zengzhi Wang", "Qian Liu", "Junlong Li", "Pengfei Liu"], "abstract": "Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these rules lack the flexibility to address the unique characteristics of individual example effectively. Meanwhile, applying tailored rules to every example is impractical for human experts. In this paper, we demonstrate that even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts. We introduce Programming Every Example (PROX), a novel framework that treats data refinement as a programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experimental results show that models pre-trained on PROX-curated data outperform either original data or data filtered by other selection methods by more than 2% across various downstream benchmarks. Its effectiveness spans various model sizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb. Furthermore, PROX exhibits significant potential in domain-specific continual pre-training: without domain specific design, models trained on OpenWebMath refined by PROX outperform human-crafted rule-based methods, improving average accuracy by 7.6% over MISTRAL-7B, with 14.6% for LLAMA-2-7B and 20.3% for CODELLAMA-7B, all within 10B tokens to be comparable to models like LLEMMA-7B trained on 200B tokens. Further analysis highlights that PROX significantly saves training FLOPs, offering a promising path for efficient LLM pre-training. We are open-sourcing PROX with > 100B corpus, models, and sharing all training and implementation details for reproducible research and future innovation.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have made significant strides in capabilities (Meta, 2024; Achiam et al., 2023; Anthropic, 2024; Reid et al., 2024), excelling in tasks such as creative writing (Yuan et al., 2022), complex reasoning (Wei et al., 2022; Kojima et al., 2022), and agentic task planning and execution (Fan et al., 2022; Park et al., 2023). Behind these, massive, high-quality pre-training corpora form the backbone of these models, equipping them with the essential knowledge and reasoning abilities crucial for a wide range of downstream tasks (Together, 2023; Penedo et al., 2024a).\nThe Internet offers vast amounts of data, but much of it is noisy and unrefined, requiring extensive cleaning and quality enhancement before being applied for pre-training. Previous works focus primarily on designing heuristic-based pipelines to lift data quality, such as document filtering (Rae et al., 2021; Penedo et al., 2024a; Soldaini et al., 2024) and perplexity-based scoring methods (Together, 2023), relying heavily on human expertise and manual adjustments (Zhang et al., 2024a). While widely adopted, these labor-intensive solutions are inherently limited by rule coverage and their inability to address every specific case. Recently, some efforts have explored leveraging LLMs for high-quality data acquisition. On the one hand, language models have been applied for data filtering or selection (Xie et al., 2023; Wettig et al., 2024; Yu et al., 2024; Dubey et al., 2024), but their role is largely limited to identifying low-quality documents without enabling fine-grained refinements (e.g., string-level). On the other hand, LLMs are also being used directly generating high-quality data, i.e., data synthesis (Gunasekar et al., 2023; Li et al., 2023; Ben Allal et al., 2024). Unlike filtering, synthesis methods actively create or refine data to produce new documents, but they require substantial computational resources, limiting scalability. Despite their success, these methods can also inherit issues like hallucination (Maini et al., 2024), and assessing their correctness and completeness in an interpretable manner remains a challenge (Liu et al., 2024a).\nStanding at the intersection of data processing efficiency and data quality improvement, in this work, we propose PROX, a model-based framework for pre-training level data refinement. PROX focuses on refining large-scale data with relatively smaller models, offering a more efficient alternative. As shown in Figure 2, in practice, PROX first adapts a small base language model (less than 1B) to data refining tasks via fine-tuning on seed data. This PROX's refining model then determines the appropriate operations for each example in the pre-training corpora through versatile programs, including operations such as filtering, string normalization and noisy line removal. Finally, the generated program is executed by a pre-defined executor, producing refined corpus ready for pre-training. In this way, PROX is empowered with language models to autonomously refine pre-training corpora, leveraging flexible function calls to enhance data quality.\nExperimental results demonstrate that the proposed PROX framework consistently lifts data quality for pre-training. Specifically, PROX achieves an average improvement of 2.1% over 10 downstream benchmarks and outperforms state-of-the-art data selection methods by over 2.0%. Furthermore, PROX shows broad applicability across model sizes from 0.3B to 1.7B and shows consistent performance gains across diverse pre-training corpora of varying quality, including RedPajama-V2 (Together, 2023), C4 (Raffel et al., 2020), and FineWeb (Penedo et al., 2024a). In domain-specific continual pre-training, PROX yields an 11% gain over OpenWebMath (Paster et al., 2024) for TINYLLAMA-1.1B and 7.6% for MISTRAL-7B across 9 mathematical tasks, with similar improvements seen on LLAMA-2-7B and CODELLAMA-7B. Beyond performance gains, results also suggest that pre-training on the refined corpus significantly boosts pre-training efficiency, achieving similar downstream performance with up to 20\u00d7 less computing. We believe it is worthwhile to scale up computing FLOPs for data refinement, which enables similar performance with much less training cost and offers a promising path for efficient LLM pre-training."}, {"title": "APPROACH: PROGRAMMING EVERY EXAMPLE", "content": ""}, {"title": "DATA REFINEMENT TASK FORMULATION", "content": "Given any document in the corpus $d \\in D$, such as an HTML extract or a textbook, we define data refinement as the process of transforming $d$ into $d'$, where $d'$ exhibits higher quality. While it is challenging to formally define \u201chigher quality\" for pre-training data, we assume it can be described"}, {"title": "PROX FRAMEWORK", "content": ""}, {"title": "Overview", "content": "As shown in Figure 2, given any document $d$ as input, the PROX framework utilizes the language model itself with parameter $\\theta$ to generate the data refinement program $Z = f(\\theta, d)$. The snippet is executed within the executor $E$, producing the refined document $d' = E(f(\\theta, d), d)$. We include two stages in the PROX framework, aiming to refine the data progressively, from rough to fine-grained. These two stages are referred to as document-level programming and chunk-level programming, as illustrated in Figure 2. In each stage, the PROX refining model will generate programs $Z_{doc}$ and $Z_{chunk}$ that refine the corpora at varying levels of granularities."}, {"title": "PROX Program Design", "content": "The detailed program space design is also crucial for maximizing the capabilities of language models. We believed designing such model-based operations should consider several realistic factors when scaling to large pre-training corpora: (1) the model does not need to be very powerful or very large to handle these tasks, it only needs to recognize several patterns; (2) the solution, though requiring more computing budget compared to heuristic-rule-based pipelines, still needs to be simple and efficient. Under such consideration, we simply let the language models generate function calls without detailed implementations. These design choices aim to balance functionality with the limitations of small language models, enabling effective document manipulation while maintaining simplicity and coherence.\nThe most fundamental operations we aim to perform on a document, are deletion and replacement. We incorporate these types of operations across different programming stages aiming to refine the corpus with different granularities in PROX: (1) In the document-level programming stage, we simply define the function $drop\\_doc()$ to delete a document and $keep\\_doc()$ to retain it. (2) In chunk-level programming, we split the lengthy documents into smaller chunks and apply fine-grained operations to these chunks. These operations include deleting specific lines $remove\\_lines()$"}, {"title": "MODEL ADAPTATION FOR PROX", "content": "It is generally difficult for base models to directly generate PROX programs. In fact, even for the most powerful post-trained LLMs, generating custom API calls is relatively challenging at the current stage (Zhuo et al., 2024). Thus, it will be necessary that we curate some seed data to adapt the model for these scenarios. Under such consideration, we employ strong LLMs to annotate these operations via zero-shot and few-shot prompting, and then adapt our base model to these tasks by supervised fine-tuning (SFT). We first use two additive scale scoring prompts (Yuan et al., 2024; Penedo et al., 2024a) to split the corpus into kept documents and dropped documents. And then we use large models to annotate fine-grained programs based on kept documents. Specifically, we leverage the LLAMA-3 series of models (Dubey et al., 2024) for data collection and annotation. In PROX, this data collection is performed only once, and all base models are adapted with the same"}, {"title": "EXPERIMENTS", "content": "In this section, we first describe our experimental setup, then verify the effectiveness of each PROX stage and compare it with existing data selection methods tailored for pretraining corpus (\u00a7 3.2). We then apply PROX to various model sizes and corpora to demonstrate its broad applicability (\u00a7 3.3). Finally, we apply PROX to the mathematical domain, demonstrating its superiority and universality in domain-specific training (\u00a7 3.4)."}, {"title": "EXPERIMENT SETUP", "content": "Training Corpora We utilize various corpora for both general and specific domain data in our experiments. For general domain data, we begin with RedPajama-V2 (Together, 2023), a preprocessed large-scale dataset of 30 trillion tokens from diverse Internet sources, ready for pre-training. We further apply PROX on the C4 corpus (Raffel et al., 2020) with 198 billion tokens and the FineWeb dataset (Penedo et al., 2024a) containing 15 trillion tokens, noted for high data quality. For specific domain experiments, we use OpenWebMath (Paster et al., 2024), a math-focused dataset with 15 billion tokens. Given the limitations in computational resources, we conduct experiments on a randomly sampled subset of the entire pre-training dataset. See Table 7 (\u00a7 B.2) for sampling details.\nBase Model Selection Our pre-training experiments are conducted using various sizes of decoder-only language models. Detailed specifications of these models and all training recipes are provided in \u00a7 B.3, especially in Table 8 and Table 9.\n1. To verify different stages' effectiveness of PROX, we employ a 750M sized model sharing LLAMA-2 architecture (Touvron et al., 2023a), denoted as TLM-S, used for both pre-training from scratch and refining. We also compare PROX with data selection methods using PYTHIA-410M/1B's architecture (Biderman et al., 2023), as those employed in MATES (Yu et al., 2024).\n2. For further evaluation of PROX using different refining and base model sizes, we scale the model sizes from 350M (0.5\u00d7 smaller, denoted as TLM-XS) and 1.7B (2x larger, denoted as TLM-M), all based on the LLAMA-2 architecture.\n3. For domain-specific continual pre-training, we select TINYLLAMA-1.1B (Zhang et al., 2024b), LLAMA-2 (Touvron et al., 2023a), CODELLAMA (Rozi\u00e8re et al., 2023) and MISTRAL-7B (Jiang et al., 2023) as representative base models for their adequate training and solid performance.\nBaselines To ensure a fair comparison w.r.t. training cost, we keep all training hyperparameters, such as training steps and batch size, consistent across baselines, with only the data refining and selection pipelines differing. We compare PROX to a series of baselines:\n1. In \u00a7 3.2, to verify PROX's effectiveness, we first compare with PROX with regular pre-training over the raw RedPajama-V2 data. We also introduce heuristic baselines used to curate the FineWeb corpora, which is the combination of three filtering strategies from C4 (Raffel et al., 2020), Gopher (Rae et al., 2021), and newly crafted rules (as FineWeb rules). Apart from rule-based baselines, we also introduce existing data selection techniques proposed in previous works, including"}, {"title": "VERIFYING PROX'S EFFECTIVENESS", "content": ""}, {"title": "Verifying Effectiveness for Each PROX Operation", "content": "We first conduct a series of experiments to verify the effectiveness of each PROX operation. We begin by training TLM-s on the RedPajama-V2 raw data for approximately 26B tokens (or 12.5K steps) as the initial baseline. Following Wettig et al. (2024) and for convenience, we then sequentially apply the doc-level and chunk-level refining pipelines by fine-tuning the 0.7B model itself. We then perform large-scale program synthesis and execution using the refining models, resulting in $D_{Doc}$ and $D_{Doc+Chunk}$. Such 2-stage synthesis requires approximately 192 A100-80G GPU hours for processing 60B tokens of data. The resulting zero-shot downstream performance is presented in Table 2, including base models trained on the data produced by PROX refinement methods and different rule-based filtering methods. Moreover, we visualize the dynamic benchmark performance in Figure 4, implying the consistent improvement of PROX over all baselines. See \u00a7 D.1 for full detailed results of all intermediate checkpoints.\nThese results show that PROX is highly effective, outperforming the raw corpus with an average boost of 2.5%, including significant improvements such as 7.6% on ARC-E, 3.3% on HellaSwag, and 2.1% on MMLU. We believe such consistent performance is significant given that these improvements were achieved even on benchmarks that are typically prone to performance instability, such as SIQA, WinoGrande, and CSQA. By contrast, rule-based methods demonstrate relatively marginal overall improvement. For instance, Gopher rules achieve only a 0.2% boost, while C4 shows a modest 0.5% improvement. Furthermore, combining all three rules (as is done in constructing the official FineWeb corpus), does not lead to any larger enhancement in overall performance."}, {"title": "Comparing with Data Selection Methods", "content": "Apart from comparing with heuristic methods, we also include existing representative model-based data selection methods tailored for pertaining corpus to verify PROX's effectiveness in Table 3, where we report both 0-shot and 2-shot performance under the same settings used in MATES (Yu et al., 2024). While we merely apply document-level stage (i.e., PROX-D) which is indeed similar to data selection methods, we can see that PROX outperforms the strongest data selection method MATES, by 2.2% and 2.5% in 0-shot and 2-shot average performance for 410M model, and by 1.0% and 2.0% for 1B model. Additionally, PROX achieves the best performance on 7 out of 8 benchmarks tested, demonstrating its superiority over existing data selection methods. Full evaluation results are provided in Table 11 (\u00a7 D.2)."}, {"title": "APPLYING PROX ACROSS MODEL SIZES AND PRETRAINING CORPORA", "content": "In this section, we demonstrate that PROX can effectively benefit models beyond scale and across different corpora, showing potential for iterative pre-training improvements."}, {"title": "PROX works well across different scales.", "content": "We train a family of models from 350M to 1.7B (i.e., TLM-xs, TLM-S, and TLM-M) on the same 26B tokens used in \u00a7 3.2, and then fine-tune these models on doc-level and chunk-level tasks, obtaining refining models with different sizes. We then apply these models in doc-level refining and chunk-level refining stages, and use the curated data for from-scratch pre-training. We report in Table 4 the adaptation performance on refining tasks of different refining model sizes. According to the validation performance, adapting PROX works well across all model sizes, all achieving 80% F1 on doc-level refinement, and 75% F1 on chunk-level refinement. We further train these models of different sizes from scratch using data produced by refining models of varying sizes. In Figure 5, the results indicate that refining models of all sizes help improve performance over raw data, with a consistent absolute gap of 2% over all base model sizes. While in Figure 5, TLM-XS curated data shows slightly better downstream performance, it has a significantly lower token-level retention ratio (23.2% vs. 28.8%) compared to larger models as reflected in Table 4. This implies that moderately larger models suggest a favorable balance between data quality and quantity. These additional tokens likely provide more knowledge during"}, {"title": "APPLYING PROX TO DOMAIN-SPECIFIC CONTIUAL PRERAINING", "content": "We also demonstrate the potential of PROX in the continual pre-training scenario, specifically, in the mathematical domain. We apply the very same pipeline as in general domains to the already cleaned OpenWebMath corpus (Paster et al., 2024), aiming to further refine and mine the high quality and clean data from the vast web pages crawled in it. We then adapt and apply PROX-xs series, which was initially trained on general text as described in \u00a7 3.3, and further adapted on math text for the doc-level and chunk-level refining tasks. Finally, we obtain about 5.5B tokens left after the"}, {"title": "ANALYSIS", "content": ""}, {"title": "IMPACT ON THE ORIGINAL DATA", "content": "What changes occur in the corpora after applying PROX? We compare the document length distribution of the original corpus with that of the PROX-refined corpus in Figure 7. In the general domain corpora (RedPajama-V2, C4, and FineWeb), the data refined by PROX exhibits a noticeable shift in the average number of tokens per document. For instance, in RedPajama-V2, we observe that documents with fewer than 100 tokens make up a significant portion of the corpus. After applying the PROX, the majority of documents contain more than 200 tokens, with an average number of tokens per document increasing from 1217 to over 2000. This suggests that very short documents may be noisy and lack sufficient meaningful information to be suitable for pre-training. This shift, however, is not observed in OpenWebMath, where the average number of tokens per document is already larger. One possible reason for this outlier is that the OpenWebMath corpus is collected mostly from sources different from the general domain, e.g., online forums like Stack Exchange, and academic publisher websites such as arXiv. The noises of these sources can be quite different from general domains. Further case studies on these documents are provided in \u00a7 E.1."}, {"title": "COMPUTING OVERHEAD ANALYSIS", "content": "Although PROX demonstrates promising results in downstream tasks, it is important to acknowledge that large-scale model inference still requires a substantial computing budget. For example, as mentioned in \u00a7 3.2, and in Table 7, the RedPajama-V2 corpus used for training TLM-S was refined from about 60B raw tokens. As calculated in \u00a7 E.2, if we utilize PROX-XS for both two refining stages, the additional computational overhead will amount to approximately $5 \\times 10^{19}$ FLOPs, which is equivalent to training an additional 12B tokens on TLM-S and 5B tokens on TLM-M. It is noteworthy that this overhead ratio keeps decreasing as model size increases, meaning that the relative computational cost diminishes for larger models."}, {"title": "RELATED WORKS", "content": "Pre-training Data Processing Raw data collected from public sources (e.g., CommonCrawl) are noisy, and directly using these data can greatly hurt model performance; thus, it has been a common practice to execute extensive pre-processing before pre-training (Touvron et al., 2023b; Together, 2023; Penedo et al., 2024a). The pipeline usually starts with document preparation, which includes URL filtering, text extraction, language-based filtering (Smith et al., 2022). The remaining document will then undergo several quality checks with heuristic rules like overall length, symbol-to-word ratio, and other criteria to determine whether it is kept, partially or fully aborted (Zhang et al., 2024a; Dou et al., 2024; Qiu et al., 2024). Finally, these documents are deduplicated using different matching methods, e.g., fuzzy match like MinHash (Broder, 1997), or exact sequences matches (Penedo et al., 2024b). In PROX, we uses the language model for further data refining, outperforming heuristic rules with acceptable computational overhead.\nData Selection Methods Data selection, slightly distinct from data processing, is more commonly applied in the later stages of large-scale data pre-processing. In supervised fine-tuning (SFT), it typically involves selecting a much smaller subset of samples to minimize tuning overhead while maintaining performance (Liu et al., 2024b). Recent efforts have extended these selection strategies to the pre-training stage (Engstrom et al., 2024; Xie et al., 2023; Ankner et al., 2024; Sachdeva et al., 2024; Liu et al., 2024c). For instance, Wettig et al. (2024) train a rater model to score documents on four quality criteria in SlimPajama (Soboleva et al., 2023) and conduct pre-training on a resampled subset based on scores. MATES (Yu et al., 2024) apply a smaller model for estimating data influence during pre-training, enabling dynamic data selection schema. Moreover, as mentioned in LLAMA-3 (Meta, 2024), LLAMA-2 models (Touvron et al., 2023a) was used as text-quality classifiers that underpin LLAMA-3's training data. Instead of merely selecting documents, PROX enables more fine-grained operations within documents, contributing to further performance improvements.\nModel-based Data Synthesizing Another branch of research focuses on editing or rephrasing existing data with models to improve the data quality. Fan et al. (2024) use ChatGPT to rephrase several instruction-tuning datasets for a clear format based on massive scenario-based criteria. Yue et al. (2024) use LLMs to extract and refine 5M QA pairs from web documents, obtaining 10M instruction-response pairs. Synthesis techniques have also been applied in the pre-training phase such as the PHI series (Gunasekar et al., 2023; Li et al., 2023). Recently, Maini et al. (2024) and Cheng et al. (2024) utilize off-the-shelf instruction-tuned models to paraphrase web documents in specific styles such as QA, and mix these synthetic rephrases with real data in pre-training. Ben Allal et al. (2024) further synthesize from mere seed topics, by prompting LLMs to generate pre-training samples in a cleaner format like textbooks. However, despite its success, it typically requires substantial computation to synthesize a pre-training-scale corpus, and more critically, it inevitably inherits flaws from the advanced model, also suffering from hallucination issues (Liu et al., 2024a). In this work, we focus on leveraging language models to lift data quality through the synthesis of executable and interpretable programs, rather than directly generating data. We demonstrate that PROX could clearly improve data quality at scale only with acceptable extra computing.\nInference Time Scaling Recent trends in language models have begun to explore the potential of allocating additional computing at inference time, complementing the extensive computations already deviated to the pre-training and post-training phases. Several studies have demonstrated the potential of this approach, showing that smaller language models equipped with additional inference-time computing can perform comparably to, or even outperform, significantly larger models, evidenced across various domains, including code generation (Hassid et al., 2024; Brown et al., 2024), and math problem-solving (Snell et al., 2024; Wu et al., 2024). The significance of this approach has been further corroborated by OpenAI's latest o1 model release (OpenAI, 2024). While these studies focus on scaling computing on test time, our work demonstrates an alternative perspective on inference computing scaling. We advocate for allocating computing to refine pre-training corpora, particularly given that Internet-based corpora have been extensively utilized in language model pre-training. Our proposed PROX demonstrates remarkable gains in pre-training efficiency by investing moderately additional compute in the corpus refinement, facilitating more efficient and accessible development of LLMs."}, {"title": "CONCLUSION", "content": "We introduced PROX, a framework that uses language models to refine pre-training data at scale through program generation. Our extensive experiments show that PROX curated data improves model performance by over 2% on various downstream benchmarks and is effective across different model sizes and pre-training datasets. For domain-specific continual pre-training, models trained on PROX curated tokens also yield significant improvements in 20\u00d7 fewer tokens, and comparable to state-of-the-art models trained on 200B tokens. Further analysis also implies applying PROX can achieve similar results with less computing power for large-scale LLM pre-training. In summary, these results demonstrate PROX's potential for greatly improving data quality and reducing costs in language model training."}, {"title": "IMPLICATIONS AND FUTURE DIRECTIONS", "content": "The strong results from PROX highlight the potential of automated data refinement to significantly improve model performance while reducing computational costs. By refining data more effectively, PROX opens new possibilities for improving training efficiency and achieving better results across a range of benchmarks. Looking ahead, these results suggest several future directions. First, incorporating additional refining operations like reformatting and rephrasing could further enhance data quality. Second, improving efficiency by reducing model size and applying inference acceleration techniques is a key goal. Expanding PROX to domains like code and multilingual data is also promising. Scaling up with more computational resources will allow for a thorough evaluation of its potential. Finally, we believe that prioritizing data refinement before pre-training can greatly improve training efficiency, and we encourage continued exploration in this area."}, {"title": "CASE STUDIES", "content": "We provide several cases to qualitatively illustrate the refinement effect of PROX, as shown in Tables 33-34. For the general domain, using RedPajama-V2 as an example, we observe that PROX can drop low-information documents, remove meaningless content such as navigation bars, and replace URL links (see Table 33). In the mathematics domain, PROX demonstrates the ability to eliminate documents with minimal relevance to mathematical reasoning and remove less important elements like functional buttons (see Table 34). These refinements enhance the quality and relevance of the processed data across different domains."}, {"title": "COMPUTING OVERHEAD ANALYSIS", "content": "According to Kaplan et al. (2020), both training and inference computational FLOPs for Transformer-based Language Models (denoted as $C_{train}$ and $C_{inference}$) can be approximated as the product of model parameters (non-embedding parameter) $N$ and the number of tokens $D$. This can be expressed as:\n$C_{train} \\approx 6 \\cdot ND_{train}$,\n$C_{inference} \\approx 2 \\cdot N (D_{prefill} + D_{decode}) $.\nIn PROX, we go through two data refining stages before final training, which incurs additional inference-time computational FLOPs. Suppose the refining model parameter for each stage is denoted as $N_{refine}$, and the raw data size in tokens is $D_{raw}$.\nFor the first document-level stage, the computational cost can be approximated as:\n$C_{doc} \\approx 2 \\cdot N_{refine} (D_{raw} + D_{output}) \\approx 2 \\cdot N_{refine} D_{raw}$, (suppose $D_{output} << D_{raw}$) resulting in a new pool of data sized $D_{doc}$.\nSimilarly, for the second chunk-level stage, the computational cost is:\n$C_{chunk} \\approx 2 \\cdot N_{r} (D_{doc} + D_{output}) \\approx 2 \\cdot N_{r}D_{doc}$, (suppose $D_{output} \\ll D_{doc}$) which produces the final refined data size of $D_{Prox}$.\nThus, the total computational overhead for PROX can be calculated as the sum of the two stages:\n$C_{PROX} = C_{doc} + C_{chunk} \\approx 2 \\cdot N_{doc\\_refine} D_{raw} + 2 \\cdot N_{chunk\\_refine} D_{doc}$.\nIn general, we use refining models with same sizes, so the final inference overhead can be estimated as\n$C_{PROX} \\approx 2 \\cdot N_{refine}(D_{raw} + D_{doc})$.\nAdditionally, we omit the FLOPs for fine-tuning since they are negligible compared to the large-scale pre-training and inference FLOPs."}]}