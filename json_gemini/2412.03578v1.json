{"title": "PERFCODEGEN: Improving Performance of LLM Generated Code with Execution Feedback", "authors": ["Yun Peng", "Akhilesh Deepak Gotmare", "Michael Lyu", "Caiming Xiong", "Silvio Savarese", "Doyen Sahoo"], "abstract": "Large Language Models (LLMs) are widely adopted for assisting in software development tasks, yet their performance evaluations have narrowly focused on the functional correctness of generated code. Human programmers, however, require LLM-generated code to be not only correct but also optimally efficient. We propose PERFCODEGEN, a training-free framework that enhances the performance of LLM-generated code by incorporating feedback based on runtime during test case execution into the self-refinement iterations. With PERFCODEGEN, we achieve speedups for a significantly higher proportion of problems compared to using the base LLM with sophisticated prompting techniques. Applied to open language models like Phi-3-mini, PERFCODEGEN achieves runtime efficiency comparable to prompting powerful closed models like GPT-4. We achieve state-of-the-art runtime efficiency on benchmarks such as HumanEval, MBPP, and APPS, frequently surpassing the ground truth reference solutions with PERFCODEGEN using GPT-3.5 and GPT-4. Additionally, we demonstrate the effectiveness of our approach in enhancing code quality across a range of open LLMs of varying sizes including Phi-3-mini, Llama 3 8B, Mixtral 8x7B, Command R, and Llama 3 70B.", "sections": [{"title": "1 Introduction", "content": "Language Models (LMs) are now widely used for code completion (Chen et al. [2021], Austin et al. [2021], Nijkamp et al. [2023]), as well as for tasks like unit test case generation (Chen et al. [2022]), bug fixing (Yang et al. [2024]), feature addition (Zhang et al. [2024]), and other stages of software development (Hong et al. [2023], Qian et al. [2023]). A recent Github survey (Shani [2023]) underscores this rapid proliferation, estimating that 92% of U.S. based developers are already using AI coding tools. However, despite this widespread adoption of Code LMs, evaluation has almost exclusively focused on functional correctness of generated code (Hendrycks et al. [2021], Li et al. [2022], Puri et al. [2021], Liu et al. [2024]), often overlooking key aspects of code quality. Runtime efficiency of a program, in particular, is a central consideration in software design decisions, due to its significant impact on user experience, serving costs and carbon footprint of a software application (Landes and Studer [1995], Chung et al. [2012]). Singhal et al. [2024] provide a distinct evaluation of Code LMs by focusing on non-functional requirements such as efficiency, security and maintainability, revelaing that current models generally falter on these key quality metrics. This is particularly concerning as less experienced developers are more likely to accept AI-suggested code (Dohmke et al. [2023]), often neglecting quality aspects, which subsequently places a significant burden on the code review process (Harding and Kloster [2023])."}, {"title": "2 PERFCODEGEN Methodology", "content": "We begin by prompting a given LLM with a problem description x in natural language that details the requirements of the program to be generated. We sample K candidate generations $C = \\{y_i\\}_{i=1...K}$ with nucleus sampling (using the base prompt from Figure 2). As discussed in Section 1, we use the execution feedback to refine the incorrect programs within C in order to increase the number of correct programs in this initial seed set for further optimization. We describe the correctness enhancements in Section 2.1, and the performance refinement phase in Section 2.2. The resulting PERFCODEGEN framework with its correctness and performance phases is illustrated in Figure 1."}, {"title": "2.1 Generating Correct Code", "content": "We assess the correctness of LLM generated programs using a set of J unit tests $U(x) = \\{u_j\\}_{j=1}^J$ corresponding to a problem x. After assessing correctness of all candidates, we iteratively refine incorrect ones based on execution feedback from the unit tests. For any $y \\in C$ that fails any of the unit tests, we prompt the LLM again, this time incorporating the environment feedback for correctness verbalised as part of the prompt along with the failed solution and one of the failing unit test. The LLM is instructed to reflect and plan, and then generate a refined correct solution based on the reflection and planning result. The specific prompts we use are provided in Figure 3.\nThe inclusion of a reflection-planning phase, as suggested by prior work Wei et al. [2022b], Nye et al. [2021], increases the likelihood of the LLM generating a correct solution. The final refinement of y generated by the LLM is then tested for correctness, and is passed for the performance optimisation stage if it passes all the unit tests. Otherwise, the problem x is considered as unsolved for correctness by the given LLM. Subsequently, a set $C_{correct}$ is constructed by removing incorrect samples from C and including their refined versions, if any of them achieve correctness. We could continue this iterative approach to further improve the correctness rate of a given LLM, but to minimize computational costs, we pause after this one iteration. Besides, we observe that the gains in correctness significantly diminish after the first iteration of refinement. Note that this stage is shared across all the different performance refinement methods studied in this work. Having a larger number of correct candidates as seeds for performance refinements benefits the framework's effectiveness, as this implies higher likelihood of PERFCODEGEN generating an optimally efficient program."}, {"title": "2.2 Optimising Correct Code using Execution Feedback", "content": "For all correct solutions $y \\in C_{correct}$ that are constructed using the samples and refinements as described in Section 2.1, we prompt the model to refine its solution to optimise performance while preserving functional equivalence. If this refinement breaks correctness, we stop and return the fastest program from $C_{correct}$. Otherwise, we measure the execution time consumed by this initial refinement (still denoted by y for simplicity) to pass each available unit test u corresponding to the given problem x. This involves conducting E independent executions for each solution-test pair in identical compute environments. After sorting this set of E observations, let $t(y, u)[e]$ be the e-th smallest execution time consumed by y on the j-th unit test u. We then calculate the empirical estimate of the expected execution time of a solution on a unit test, excluding the two outliers (smallest and largest execution times) to minimize the impact of potential measurement noise as follows:\n$t(y_x, u) = \\frac{1}{(E-2)} \\sum_{e=2}^{E-1} t(y_x, u)[e]; \\quad u^* = argmax_{u \\in U(x)} t(y_x, u)$\nWe hypothesize that the $f$-th unit test $u_f$ of a problem x, that corresponds to the largest execution time, as defined above, can be highly informative in optimising the performance of y if included in the feedback to the LLM for generating a revision. This approach mirrors how developers would identify the most time consuming pieces (hot spots) in their code to come up with runtime improving code changes. Therefore, we re-prompt the model with its latest generation y, its most time consuming"}, {"title": "3 Experiments", "content": "We describe the experiments demonstrating the effectiveness of PERFCODEGEN for generating runtime-efficient programs in this Section. Section 3.1 outlines our experimental setup, Section 3.2 provides the main results of with all the models on the three benchmarks. In Section 3.3 we compare PERFCODEGEN's execution feedback scheme with alternative prompting strategies, and in Section 3.4 we validate the effectiveness of the planning phase in improving correctness."}, {"title": "3.1 Setup: Metrics, Datasets and Models", "content": "To evaluate the correctness and runtime-efficiency of LLM generated solutions using different approaches, we follow prior work (Madaan et al. [2023]) to compute and report the below metrics using the fastest (Best@k) LLM generated correct program out of k samples.\n\u2022 Percent Optimized [%Opt]: The proportion of problems in the test set where the fastest correct LLM generated program $y_x$ is more runtime-efficient (at least 10% faster) than the ground truth $g_x$.\n$\\frac{100}{N} \\sum_{u=1}^N \\sum_{i=1}^J (t(y_x, u) < 0.9 \\cdot \\sum_i t(g_x, u))$ where N is the total number of test set problems.\n\u2022 Percent Correct [%Correct]: The proportion of problems in the test set where the LLM generates at least one correct solution out of k candidates.\n\u2022 Speedup: For problems where we obtain atleast one correct LLM-generated program $y_x$, we calculate speedup as the absolute ratio between the execution time required by $g_x$ and the execution time required by the fastest correct $Y_x$. $\\frac{\\Sigma_{e} \\Sigma_i (g_{x}u)}{\\Sigma_{e} \\Sigma_i (y_{x}u)}$\nFollowing prior work (Singhal et al. [2024]), we rely on an empirical estimation of the execution time of Python programs, despite its drawbacks and challenges like high compute requirements. While tools like the gem5 simulator (Binkert et al. [2011]) for reliably and efficiently determine CPU cycles of a program, adapting them for Python is non-trivial. Nevertheless, our qualitative analysis (Listing 1) confirms that the differences observed in execution time (t) correspond to clear differences in coding patterns. To estimate the execution time of a candidate solution, we use E = 12 executions for each unit test as described in Equation 1. We then compute the above three metrics using the fastest correct program $y_x$ obtained from k (Best@k) candidates. If there are multiple ground truth solutions (like in APPS), we only use the fastest one as the reference for computing all metrics. We study the impact of sampling budget on the effectiveness of our framework by using $k \\in \\{1,8,20\\}$.\nWe perform our analysis by treating %Opt as the preferred metric over speedup when comparing different methods. A method achieving higher %Opt would be generally more desirable to users than one with lower %Opt, irrespective of speedups observed. This is because users often prefer a larger number of tasks solved optimally rather htan a few tasks solved more optimally. However, this preference can be adjusted based on user requirements in different contexts. Speedup should only be analyzed in conjunction with %Opt and %Correct, not in isolation, as it is defined using problems with a correctly generated program by the LLM. Note that the %Correct metric is equivalent to the commonly reported pass@k, when n = k. However, since our approach leverages unit tests in"}, {"title": "3.2 PERFCODEGEN Results", "content": "We report the performance of different LLMs on problems from the the HumanEval and MBPP benchmarks in Table 1 using our PERFCODEGEN framework and the aforementioned metrics, with a sampling budget of k = 1 and 8 samples. For comparsion, we also list the base LLM performance without using PERFCODEGEN. Performance results with a k of 20 are provided in Table 6 in the Appendix. On both the benchmarks, we witness that our framework leads to significant improvements in %Opt and %Correct for all base LLMs, particularly with the higher k of 8.\nWith PERFCODEGEN we notably enhance the runtime-efficiency of programs generated by open models like Phi-3-mini (%Opt of 40.85 @ k = 8) and Llama 3 70B (39.02) making them comparable to GPT-4 in the base setting, which attains the highest base %Opt of 39.26 (k = 8). Similarly, with PERFCODEGEN, open models like Mixtral (27.67), Command R (32.52) and Llama 3 8B (31.10) can achieve comparable %Opt to GPT-3.5 (29.63). While we can elevate the performance of open models to match that of closed commercial models, we witness even higher gains in %Opt and %Correct when using PERFCODEGEN on the closed GPT-3.5 and GPT-4 models. This finding can be attributed to the differences in the reasoning capabilities of these model categories, as the effectiveness of PERFCODEGEN heavily relies on the reasoning skills of the given LLM.\nOn MBPP, we continue to witness significant gains in %Opt when using PERFCODEGEN in most cases when sampling 8 candidates. An exception is Llama 3 70B, whose performance marginally drops on MBPP with our method at k = 1, likely due to the high variance in estimating %Opt with a single sample. This drop can be mitigated in practice by leveraging execution time evaluation to fall back to the base LLM output if our refinement is correct but suboptimal. However, we avoid doing so here for a stricter evaluation of our scheme. We provide an example in Listing 1 where PERFCODEGEN generates a faster program than the ground truth. As shown in Table 1, most LLMs generate solutions that are faster than the ground truth for significant portions of the test set, raising questions about their optimality."}, {"title": "3.3 Evaluating Alternatives to PERFCODEGEN's Execution Feedback", "content": "Given a correct solution, we evaluate some common prompting techniques for the performance improvement phase. We provide the specific prompts in verbatim in Appendix B.7. Table 3 lists the results with all these methods on the HumanEval and MBPP datasets using GPT-3.5. We exclude"}, {"title": "3.4 Role of Planning in Correctness Refinement", "content": "We evaluate the effectiveness of PERFCODEGEN's planning step in achieving higher correctness compared to directly using execution feedback. As discussed in Section 2.1, a high correctness rate is"}, {"title": "4 Related Work", "content": "Many software engineering works have proposed a code-to-code editing formulation for improving code quality in the form of tasks like fixing bugs (Gupta et al. [2017]), performance improving edits (Madaan et al. [2023], Garg et al. [2022]), improving maintainability (Loriot et al. [2022], Al Madi [2022]), and security enhancing edits (He and Vechev [2023], Perry et al. [2023], Tony et al. [2023], Pearce et al. [2022], Bhatt et al. [2023]). Contrary to this approach, we formulate a text-to-code task for our work on runtime efficiency aspect of quality improvements. As programmers continue to rely on prompting LLMs for generating programs for repetitive tasks in software engineering (Dohmke et al. [2023], Feng and Chen [2024], White et al. [2023], Denny et al. [2023]), we opine that it is critical for research on code quality to focus on the prompting stage by studying natural language inputs that describe developer intent or program specifications.\nTo improve the general LLM output quality (Chiang et al. [2024]) post the pre-training and supervised instruction fine-tuning (Wei et al. [2022a]) stages, recently proposed algorithms like Reinforcement Learning from Human Feedback (Ouyang et al. [2022], Stiennon et al. [2020]) and Direct Preference Optimization (Rafailov et al. [2024]) that use human preference data have become industry standard (Touvron et al. [2023]). While one could continue to scale these approaches for improving LLM generated code quality, this would require gathering large-scale preference data for code, which is arguably more difficult and expensive than collecting natural language response preferences. Besides needing extensive number of samples, RL techniques also involve expensive model fine-tuning and are known to be notoriously prone to training instabilities (Casper et al. [2023], Wang et al. [2024]).\nOur work builds upon the effectiveness of prior work like Madaan et al. [2024]'s Self-Refine, Nye et al. [2021]'s Scratchpads for LLMs and Chen et al. [2023]'s Self-Debug who propose LLM based self-refinements to improve output quality by adding intermediate planning or analysis stages. Our framework is also closely related to Shinn et al. [2024]'s Reflexion who use environment or tool feedback to improve LLM output quality, but focus only on functional correctness in the context of code generation. With PERFCODEGEN, we extend these ideas to improve program runtime efficiency, an aspect that has been largely ignored in favor of functional correctness by prior work."}, {"title": "5 Conclusion", "content": "We introduced PERFCODEGEN, a novel framework that leverages code execution feedback during the iterative self-refinement stages of LLMs to enhance the runtime-efficiency of generated code. We show that using our approach, open LLMs like Phi-3-mini can achieve code quality comparable to closed LLMs like GPT-4. Our evaluation of PERFCODEGEN on three widely used Python programming benchmarks using both open and closed language models of varying sizes, demonstrates consistent and significant gains in correctness and runtime efficiency. On a sizeable fraction of the test set problems from HumanEval and MBPP, we achieve programs with state-of-the-art runtime efficiency, far exceeding the ground truth reference solutions in several cases with PERFCODEGEN using GPT-4."}, {"title": "A Limitations", "content": "The challenge of writing efficient and high-quality software with LLMs spans various levels of granularity, from line-level optimizations to multi-class project repositories Shrivastava et al. [2023]. In our current scope, we focus on generating performant modules or Python functions, which are typically small components of real-world systems. However, addressing this problem comprehensively should ideally involve ensuring architectural design patterns such as minimal redundancy or wasteful computation across the entire scope of a project.\nAnother limitation is our focus solely on measuring the runtime performance of LLM-generated code, disregarding memory consumption, which can be crucial in many applications. Future extensions of PERFCODEGEN could prioritize optimizing for both factors or allow users to specify preferences for optimization. Additionally, beyond performance, developers desire attributes like readability, ease of maintenance, security, and harmlessness Bhatt et al. [2023], Singhal et al. [2024], which are not within the scope of our current work. While PERFCODEGEN could be adapted to incorporate feedback from different environments or tools evaluating these attributes, achieving a balance in optimizing code generation across these dimensions is non-trivial.\nAs emphasized by prior research, reliably measuring the runtime performance of code poses challenges Madaan et al. [2023]. A piece of code may exhibit varying execution times across different compute environments, even with identical underlying hardware. Unfortunately, tools like the gem5 simulator Binkert et al. [2011] do not support the execution of Python programs at the time of our study. To mitigate this, we ensure identical compute environments for each candidate code snippet and run only a single Python program at any given time to minimize effects from concurrent execution. However, averaging execution time measurements from 10 independent runs of each program significantly adds to our execution costs. Future work could explore more efficient methods for reliably measuring runtime, such as determining the instruction count of LLM-generated programs deterministically."}, {"title": "B Appendix", "content": null}, {"title": "B.1 Sanitized Benchmarks", "content": null}, {"title": "B.2 Compute", "content": "LLM inference: We use the vLLM Kwon et al. [2023] library on a node with 16 Nvidia A100 GPUs for approximately three weeks to complete all the experiments in this work.\nOpenAI API: We use the gpt-4-0613 and gpt-3.5-turbo-0125 model endpoints from OpenAI. In total, we required nearly 411k GPT-4 and 1.5M GPT-3 requests for all our experiments, contributing to the major costs of this study.\nCode Execution: We use 40 instances of virtual machines (n1-highmem-16 GCP instances), each with 16 CPUs and 104 GB RAM for executing all the LLM generated programs generated in our experiments. We employ these instances for roughly four weeks to complete the execution of all the LLM generated programs using different frameworks in our study. Interestingly, unlike most LLM research, gathering this environment feedback tends to be the much costlier bottleneck in our experiments compared to the LLM inference costs. We implement the safeguards prescribed in Section 2.3 of Chen et al. [2021] to mitigate the security risks in executing untrusted programs in our environment."}, {"title": "B.3 Statistical Significance", "content": "Our main results are based on findings in Table 1 and Table 2, where we report that using PERFCODEGEN leads to significant gains in the %Opt metric compared to the base LLM. For results with the GPT-4 model, we compute the Z-scores to compare PERFCODEGEN's output with that of the base model: -0.878(k = 1) and -1.34(k = 8) on HumanEval, -2.588(k = 1) and -2.947(k = 8) on MBPP and -1.8(k = 1) and -2.675(k = 8) on APPS. The improvement obtained with PERFCODEGEN is thus statistically significant with a < 0.05 on the MBPP and APPS problems, and with a lower confidence (a < 0.2) on HumanEval which has a smaller number of problems (164)."}, {"title": "B.4 Broader Impact", "content": "By enabling LLMs to generate code that is not only functionally correct but also efficient, our work with PERFCODEGEN can significantly accelerate the software development process. This can lead to faster creation of new applications, reduced development costs, and increased innovation across various industries. Frameworks like PERFCODEGEN can potentially empower individuals with less coding experience to leverage LLMs for basic programming tasks. This could lead to a wider pool of software developers and a more inclusive tech landscape. More efficient code translates to lower energy consumption during program execution. This can contribute to a more sustainable software development ecosystem and reduce the environmental impact of the tech industry. Our work demonstrates the ability of PERFCODEGEN to enhance the performance of open LLMs, making them more competitive with closed models. This can foster a more open and accessible environment for LLM development and research. Our work could also offer a promising route to discover novel algorithms to solve long standing problems more efficiently (Romera-Paredes et al. [2024]).\nWhile PERFCODEGEN aims to improve code efficiency, it could be misused to automate the generation of efficient harmful or malicious code. For instance, cybercriminals could use optimized code to create more efficient malware or exploit software vulnerabilities more effectively. Incorporating content filtering and malicious code detection algorithms to identify and block harmful code generation can help reduce the risk of such misuse. Optimized code may sometimes introduce new types of vulnerabilities that are difficult to detect. If PERFCODEGEN generates code that is highly efficient but less readable or maintainable, it could lead to challenges in debugging and maintaining software, potentially resulting in unexpected failures or security issues. Addressing this risk requires implementing comprehensive testing and validation, including code reviews, to ensure the generated code maintains high reliability and security standards. More efficient code generation could lead to further automation in software industry, potentially displacing some human programmers. This necessitates discussions on re-skilling initiatives and the evolving nature of jobs in the tech industry."}]}