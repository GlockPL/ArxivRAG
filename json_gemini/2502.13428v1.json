{"title": "MCTS-KBQA: Monte Carlo Tree Search for Knowledge Base Question Answering", "authors": ["Guanming Xiong", "Haochen Li", "Wen Zhao"], "abstract": "This study explores how to enhance the reasoning capabilities of large language models (LLMs) in knowledge base question answering (KBQA) by leveraging Monte Carlo Tree Search (MCTS). Semantic parsing-based KBQA methods are particularly challenging as these approaches require locating elements from knowledge bases and generating logical forms, demanding not only extensive annotated data but also strong reasoning capabilities. Although recent approaches leveraging LLMs as agents have demonstrated considerable potential, these studies are inherently constrained by their linear decision-making processes. To address this limitation, we propose a MCTS-based framework that enhances LLMs' reasoning capabilities through tree search methodology. We design a carefully designed step-wise reward mechanism that requires only direct prompting of open-source instruction LLMs without additional fine-tuning. Experimental results demonstrate that our approach significantly outperforms linear decision-making methods, particularly in low-resource scenarios. Additionally, we contribute new data resources to the KBQA community by annotating intermediate reasoning processes for existing question-SPARQL datasets using distant supervision. Experimental results on the extended dataset demonstrate that our method achieves comparable performance to fully supervised models while using significantly less training data.", "sections": [{"title": "Introduction", "content": "Knowledge base question answering (KBQA) is an increasingly significant research area that leverages structured knowledge bases (KBs) to provide precise answers to natural language (NL) questions. Benefiting from the powerful reasoning capabilities of large language models (LLMs), such as ChatGPT (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023), state-of-the-art KBQA methods conceptualize LLMs as agents and KBs as environments (Gu et al., 2024; Jiang et al., 2024; Xiong et al., 2024). Through carefully designed tools and interaction logic, these methods can accomplish tasks with only a few exemplars as prompts. However, these approaches are fundamentally limited by linear decision-making processes, which constrain the full potential of LLMs' reasoning capabilities.\nMonte Carlo Tree Search (MCTS) (Swiechowski et al., 2022) methods have recently shown remarkable success in reasoning tasks, such as mathematics and code generation (Chen et al., 2024a; Zhang et al., 2024c; Wang et al., 2024). MCTS is a tree search-based approach that enhances reasoning capabilities through five key steps: expansion, selection, simulation, evaluation, and backpropagation. These steps enable the exploration of the solution space effectively. The core challenge of such methods lies in designing domain-specific reward functions. While some works opt to train a reward model, others choose to obtain rewards directly from LLMs or the environment.\nTherefore, this paper focuses on exploring a method to incorporate MCTS into the KBQA domain. Our main contributions are:\n\u2022 We introduce MCTS methodology to the KBQA domain and design step-wise rewards that require only direct prompting of open-source instruction-tuned LLMs without fine-tuning.\n\u2022 Experimental results demonstrate that our method significantly outperforms linear decision-making approaches in low-resource scenarios.\n\u2022 We contribute new data resources to the KBQA community by annotating intermediate reasoning processes for existing question-SPARQL datasets using distant supervision, which further helps improve model performance."}, {"title": "Related Work", "content": "Based on the agent-environment framework, recent state-of-the-art knowledge base question answering (KBQA) methods conceptualize large language models (LLMs) as agents and KBs as environments. This paradigm has been shown to enhance both reasoning efficiency and accuracy. Gu et al. (2023) proposed that in few-shot scenarios, LLMs should prioritize evaluating the plausibility of agent plans rather than directly generating answers. Concurrently, Li et al. (2023) advocated for a two-stage approach: first generating logical forms as initial drafts, then refining them into executable queries using the KB. Jiang et al. (2023) developed two specialized interfaces for accessing the KB, while Gu et al. (2024) and Liu et al. (2024) designed seven tools to facilitate agent-environment interaction. Sun et al. (2024) introduced a novel approach that enables LLMs to perform iterative beam search reasoning on KBs. However, these methods are fundamentally limited by their linear decision-making processes, which constrains the full potential of LLMs' reasoning capabilities.\nMCTS-based reasoning methods have demonstrated remarkable effectiveness in mathematical tasks. The core challenge of these methods lies in designing appropriate reward functions. A series of works has focused on training reward models. Given question-answer pairs, some approaches utilize MCTS to identify reasoning paths through weak supervision, scoring each step to obtain supervisory signals for intermediate reasoning steps. Zhang et al. (2024a) designed a distance-based reward rule and trained a reward model, while Chen et al. (2024a) augmented LLMs with linear layers, enabling parameter sharing between the value model and the reasoning LLM for joint training. Additionally, reward models can be trained using step-wise (node-level) preference data. Xie et al. (2024) employed Direct Preference Optimization (DPO) for iterative training, while Wang et al. (2024) proposed step-wise value preference optimization. Some works have also explored obtaining rewards directly from LLMs. Hao et al. (2023) utilized LLMs as world models to simulate subsequent states. Zhang et al. (2024b) incorporated self-refinement for node expansion and employed direct prompting to obtain rewards. To address scoring instability, Zhang et al. (2024c) proposed using a pairwise preference reward model to compute partial ordering relations between nodes, ultimately deriving a global ranking.\nSimilar to our work, Zhou et al. (2024) conceptualizes LLMs as agents and designs evaluation rewards based on self-generated and self-consistency scores. However, their assumption that the environment knows the ground truth answer and can provide feedback about correctness is impractical in real-world scenarios. Luo et al. (2025) is a concurrent work that also attempts to apply MCTS to KBQA tasks. However, their approach differs significantly from ours in both action space design and reward function formulation. Their method requires training reward models, and relies on scoring the final logical form based on the entire trajectory, which we argue is suboptimal."}, {"title": "Approach", "content": "Recent KBQA methods that conceptualize LLMs as agents have demonstrated remarkable capabilities in few-shot learning and reasoning. However, constrained by their linear decision-making processes, these methods have not fully realized the reasoning potential of LLMs. To address this limitation, we introduce Monte Carlo Tree Search (MCTS) into KBQA and design a scoring mechanism specifically tailored for KBQA tasks. We demonstrate that directly prompting open-source LLMs to provide rewards for intermediate steps in the MCTS process is an effective approach. Figure 1 presents an overview of the search process."}, {"title": "Preliminaries", "content": "For KBQA, a knowledge base (KB) is formally defined as $K \\in E \\times P \\times (E \\cup L \\cup C)$, where $E$ denotes the set of entities, $P$ represents the set of predicates (including relations and qualifiers), $C$ denotes the set of classes (e.g., concepts), and $L$ comprises the set of literal values. Given a question Q and a knowledge base K, our goal is to generate an executable SPARQL expression that answers the question. The semantic parsing process can be formalized as $p(SPARQL|Q, K)$.\nFor MCTS, a node e = {a, o} contains an action a and an observation o, except for the root node $C_{root}$ = {Q} which contains only the question. For conciseness, we include the thought component within the action. The state s of a node e is defined as the path from root node to e, formally represented as:"}, {"title": "Monte Carlo Tree Search for KBQA", "content": "MCTS is a tree search-based method that explores the solution space through iterative selection, expansion, evaluation, simulation and backpropagation, achieving superior performance compared to linear decision-making approaches.\nSelection. The selection phase identifies the most promising leaf node for expansion. Starting from the root node, we employ the widely-used Upper Confidence Bounds applied to Trees (UCT) as our selection criterion. UCT effectively balances exploiting high-value nodes and exploring less-visited nodes. We skip terminal nodes and select the node with the highest UCT value. The formula is defined as:\n$UCT(e) = \\frac{W_e}{n(e)} + c \\sqrt{\\frac{\\ln N(e)}{n(e)}}$  (1)\nwhere $W_e$ is the total reward of node e, $n(e)$ is its visit count, and $N(e)$ is its parent's visit count. The selection process is formalized as:\n$e_{select} = arg\\max_{e \\in E} UCT(e)$ (2)\nwhere E denotes the set of non-terminal nodes.\nExpansion. Following Interactive-KBQA (Xiong et al., 2024), we fine-tune an open-source LLM on human-annotated interaction data to serve as our action-generating agent. The action space is defined as {SearchNodes, SearchGraphPatterns, ExecuteSPARQL, Done}. After executing each action, we obtain an observation which, together with the action, forms a new node. Formally, the process is defined as:\n${a_i}_{i=1}^n = Agent(s_e)$ (3)\nwhere $a_i$ represents the actions (including thought process) generated by the agent, n is a hyperparameter controlling how many completions to generate for each input, and $s_e$ is the state of the selected node e.\nEvaluation. Evaluation is a crucial component that guides the search direction. Unlike classical MCTS approaches that evaluate nodes only upon termination, we evaluate intermediate steps immediately after each node generation.\nFormally, the score function is defined as:\nr(e) = LLM(Prompt_{eval}, Exemplar_{eval}, S_e)  (4)\nwhere $Prompt_{eval}$ represents the evaluation prompt consisting of task description, guidelines, and format requirements, and $Exemplar_{eval}$ denotes examples. Notably, we utilize an instruction-tuned open-source LLM without additional fine-tuning.\nBased on the characteristics of the KBQA task, we design a set of rules as prompt text to score actions based on environmental feedback. Our interactive semantic parsing approach essentially decomposes the process of finding the target SPARQL query into the collection and combination of basic elements. For instance, we use the SearchNodes tool to locate entities and concepts, and the SearchGraphPatterns tool to find predicates and their corresponding literal value formats. The agent then combines these elements to construct SPARQL queries that express complex semantics.\nTherefore, the scoring rules are designed to assess whether the state has successfully identified elements and solved sub-problems, which is fundamentally a simpler classification task. Multiple studies (Gu et al., 2023; Chen et al., 2024b) have demonstrated that LLMs perform more accurately in discrimination tasks compared to generation tasks. The complete evaluation prompt and exemplars can be found in Appendix A.5.\nSimulation and Backpropagation. Our approach evaluates intermediate states directly, bypassing the simulation step of traditional MCTS. After obtaining the score, we immediately perform backpropagation. We design a depth-based linear decay update function to prevent the search from getting trapped in local optima along branches. The function is defined as:\n$W_e = W_e + r_e * (1 - \\gamma * max(0, d_e - d_{exp}))  (5)\nwhere $\\gamma$ is the decay coefficient, $d_e$ is the depth of the current node, and $d_{exp}$ is the expected depth. The detailed parameter settings can be found in Section 4.2.\nTermination. When an action generates Done, its branch terminates. The search process early stops after generating k instances of Done, where k is a hyperparameter that balances efficiency and performance. Otherwise, the search continues until reaching the maximum interaction rounds. The impact of k is analyzed in Section 4.7.2. Notably, only valid terminal nodes are counted. A valid terminal node must have ExecuteSPARQL as its penultimate action, produce a non-empty error-free execution result, and its SPARQL query is treated as the prediction for that branch. Finally, we vote for the most frequent execution result as the final result."}, {"title": "Data Construction via Distant Supervision", "content": "Existing KBQA datasets consist of question-SPARQL pairs but lack intermediate reasoning steps. Using our proposed method and a distant supervision paradigm, we augment these datasets with complete intermediate reasoning processes. Implementation details are provided in Section 4.2. Experimental results demonstrate that the extended dataset further improves model performance, as discussed in Section 4.8."}, {"title": "Experiment", "content": "We examine MCTS-KBQA across a variety of complex question types and diverse databases (DBs)."}, {"title": "Dataset & Preprocessing", "content": "WebQuestionsSP (WebQSP) (Yih et al., 2016) and ComplexWebQuestions 1.1 (CWQ) (Talmor and Berant, 2018) are extensively used in KBQA research. These datasets comprise natural language questions paired with their corresponding SPARQL queries based on Freebase (Bollacker et al., 2008). For WebQSP, questions can be categorized into 1-hop and 2-hop types according to the length of the inferential relation chain, which represents the path connecting the topic entity to the answer node. CWQ extends WebQSP by incorporating four types of complex questions: Conjunction (Conj), Composition (Compo), Comparative (Compa), and Superlative (Super).\nKQA Pro (Cao et al., 2022) is a large-scale dataset designed for complex question answering over a dense set of Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014). It features nine types of complex questions, including Count (Ct), Query Attribute (QA), Query Attribute Qualifier (QAQ), Query Name (QN), Query Relation (QR), Query Relation Qualifier (QRQ), Select Among (SA), Select Between (SB), and Verify (Vf)."}, {"title": "Implementation Details", "content": "For a fair comparison with previous work, we use the same training and test datasets as Xiong et al. (2024). As shown in Table 1, for the Freebase DB, we utilize 300 manually annotated (Anno) instances with intermediate reasoning processes as our training set. For the Wikidata DB, the training set consists of 450 instances. The test dataset contains 900 instances for each DB, sampled from the original dataset. Due to the time-consuming nature of distant supervision, and considering that questions of the same type in template-based datasets like CWQ and KQA Pro share similar patterns, we sampled 10% of training data from each question type to build the extended dataset, with additional sampling for underrepresented types (Compa and Super) in CWQ.\nBy default, we fine-tune Llama-3.1-8B-Instruct (Grattafiori et al., 2024) as the agent, while utilizing its original version as the reward model. For MCTS search, we set the sampling parameter n=5 for the agent, early stopping threshold k=5, decay coefficient $\\gamma$ = 0.1, and expected depths $d_{exp}$ of 5 and 7 for WebQSP/CWQ and KQA Pro, respectively. Additional implementation details and parameter settings can be found in Appendix A.3."}, {"title": "Baselines", "content": "Linear decision methods. We select Interactive-KBQA (Xiong et al., 2024) as the baseline. This method employs supervised fine-tuning (SFT) on an open-source LLM using the same limited training data, completing semantic parsing (SP) through multiple interactions. However, it only generates one action per decision step.\nTree search methods. We employ Breadth-First Search (BFS) and Depth-First Search (DFS) as baselines. As two of the most common traversal algorithms, they treat all nodes with equal importance. We also explore random search as a baseline, with results reported in Section 4.6.1.\nFine-tuning on full training data. We selected SP-based methods as baselines. Unified-SKG (Xie et al., 2022) introduces a multi-task prefix-tuning approach that unifies 21 structured knowledge grounding tasks across 6 domains into a text-to-text framework, fine-tuning a T5 model for semantic parsing. For KQA Pro, we opted for BART-SPARQL (Cao et al., 2022), which directly generates SPARQL queries from questions without requiring retrieval."}, {"title": "Evaluation Metrics", "content": "For semantic parsing-based methods that generate logical forms and produce unordered answer sets, we employ the F1 score as the primary evaluation metric. To provide comprehensive evaluation, we additionally report the Random Hits@1 (RHits@1) metric (Shu et al., 2022) and the Exact Match (EM) score (Talmor and Berant, 2018). For the KQA Pro dataset specifically, we report accuracy, which is defined as the exact one-to-one correspondence between the predicted and ground truth answer sets."}, {"title": "Results", "content": "Compared to linear decision methods, our approach demonstrates significant improvements while using the same limited training data. To ensure fair computational comparison, we run linear methods multiple times (set to 5) and vote for the majority prediction in the open-source LLM SFT setting. Similarly, for our MCTS method, we set k=5, meaning we early stop when 5 branches reach termination.\nFor linear methods, multiple runs indeed show improvement, even outperforming BFS and DFS on KQA Pro. Upon investigation, we found this is due to the abundant redundant information in Freebase, where multiple paths can lead to the correct answer. For instance, when searching for the government form of \"Soviet Union\" (?e), both paths (?e, location.country.form_of_government, \"Parliamentary republic\") and (\"Parliamentary republic\", government.form_of_government.countries, ?e) can lead to the correct target entity.\nTree-based algorithms are better at exploring multiple solution paths compared to linear decision methods, enabling even BFS to find answers effectively in WebQSP and CWQ. However, KQA Pro uses Wikidata as its database, which lacks such redundancy and features more complex qualifier structures, resulting in a larger search space. Consequently, simple tree traversal methods struggle to find optimal solutions. MCTS, which essentially prioritizes the search space, shows marked improvement over DFS and BFS, achieving approximately 10 points higher performance on KQA Pro.\nAdditionally, for WebQSP and CWQ, we report results with given golden entities. The modest improvement of 1.94 points on CWQ suggests that entity recognition is not particularly challenging for template-generated datasets."}, {"title": "Analysis of Reward Effectiveness", "content": "The reward function is arguably the most crucial component of MCTS methods, as it directly guides the search direction. For the KBQA domain, we propose a scoring framework that leverages open-source instruction-tuned LLMs through a discriminative paradigm to evaluate intermediate states."}, {"title": "Rule-based Scoring", "content": "We selected random scoring and direct scoring as baselines. Direct scoring refers to only including tool descriptions and format specifications in the prompt. Notably, for the more complex datasets CWQ and KQA Pro, the performance difference between direct scoring and random scoring is minimal."}, {"title": "Score Stability Analysis", "content": "Prompt-based LLM scoring methods often face challenges with randomness and inconsistency. This section analyzes the stability of scoring methods from the perspective of score distribution. Specifically, for each node, we set the sampling parameter $n_r$ = 10, calculate the standard deviation, and then compute the average values grouped by depth. The results show that direct scoring exhibits higher fluctuations compared to rule-based scoring. Moreover, these fluctuations increase with depth, indirectly indicating that as states become more complex, the scoring becomes increasingly unstable."}, {"title": "Impact of Key Hyper-parameters", "content": "This section analyzes the impact of the key hyperparameters sampling width n (number of action samples per node) and early stop k."}, {"title": "Action Sampling Width", "content": "The hyperparameter n determines the number of nodes to expand at each layer. In our implementation, we generate n actions by setting sample=n, then deduplicate actions based on their execution results to establish new nodes. This essentially balances the trade-off between recall and precision - as n increases, more child nodes are explored which improves answer recall, but may reduce precision due to the increased likelihood of including incorrect paths. The results indicate that n = 5 provides an optimal balance."}, {"title": "Early Stop Threshold", "content": "The parameter k controls early stopping by requiring a minimum number of valid terminal nodes, balancing accuracy and cost. As k increases, we observe improved search accuracy at the cost of longer search times. Our experiments show that when k = 5, the search time is approximately 3 times longer compared to k = 1, which we consider an acceptable trade-off given the performance gains."}, {"title": "Experiment on Extended Dataset", "content": "Existing KBQA datasets typically contain only question-SPARQL pairs without intermediate reasoning processes. To address this limitation, we employed distant supervision to annotate data with intermediate reasoning steps and conducted SFT on this extended dataset. We use MCTS to search until reaching an F1 score of 0.67 between predictions and golden answers, then save the reasoning path. The experimental results are remarkable efficiency."}, {"title": "Impact of Open-source LLM", "content": "This section investigates the impact of different open-source LLMs on performance, examining both the agent and reward model components. results reveal comparable performance between the two architectures, with minimal differences in effectiveness."}, {"title": "Error Analysis", "content": "To systematically assess our method's limitations, we first calculate the Max@k metric, which represents the highest F1 score among k (k=5) predictions, indicating our method's performance ceiling."}, {"title": "Conclusion", "content": "This paper presents a MCTS-based framework for KBQA that enhances LLMs' reasoning capabilities through tree search methodology. We propose a step-wise reward mechanism that requires only direct prompting of open-source instruction LLMs without additional fine-tuning. Experimental results demonstrate that our method significantly outperforms linear decision-making baselines, particularly in low-resource scenarios. Additionally, we contribute new data resources by annotating intermediate reasoning processes for existing KBQA datasets, demonstrating both effectiveness and data efficiency."}, {"title": "Limitations", "content": "The main limitation is the computational overhead of MCTS compared to linear decision-making methods. While early stopping helps, increased latency remains a concern for time-sensitive applications. Additionally, our distant supervision approach may generate incorrect intermediate reasoning steps even with correct final answers. Future work should focus on improving the quality of these intermediate annotations while maintaining efficiency."}, {"title": "Additional Statistics of Datasets & Databases", "content": ""}, {"title": "Tool Implementation Details", "content": "Tool Implementation. In the development of the SearchNodes tool, Elasticsearch\u00b9 is employed to extract all node surface names from the Free-base and MetaQA databases, and vector search techniques are implemented to perform queries on nodes within Wikidata. For the ranking algorithm of the SearchGraphPatterns tool, vector retrieval methods are similarly employed. All processes related to vectorization utilize the OpenAI text-embedding-ada-002 API to generate vectors and employ Chroma2 for indexing and searching. Moreover, for the functionality of the ExecuteSPARQL tool, Virtuoso\u00b3 serves as the underlying graph query engine. For both the SearchN-odes and SearchGraphPatterns tools, the number of returned results is set to 10."}, {"title": "Open-source LLM Fine-tuning & Deployment", "content": "For our implementation, we utilize Llama-3.1-8B-Instruct\u2074 (Grattafiori et al., 2024) as the base model. For the LLM agent, we perform supervised fine-tuning on this model using the hyperparameter settings detailed in Table 10. For the reward model, we directly use the original instruction-tuned version without additional training.\nThe training process is optimized using DeepSpeed (Rasley et al., 2020), while inference is accelerated using vLLM (Kwon et al., 2023). All experiments are conducted on 8 NVIDIA A100 80GB GPUs."}, {"title": "Hyper-parameter Settings for MCTS", "content": "Table 11 outlines the hyper-parameter settings for MCTS. For the simulation process, we set the maximum number of simulations to 50. The search terminates early when k nodes generate the Done action. Since our method evaluates intermediate states directly without requiring simulation to terminal states, each simulation only generates one node, effectively capping the maximum number of nodes at 50. For the Max Preferred Depth parameter, we use a value of 5 for WebQSP and CWQ datasets, while setting it to 7 for KQA Pro due to its more complex reasoning requirements."}, {"title": "Prompt Text for Reward Model", "content": "The reward model's prompt text comprises tool descriptions, examples, guidelines, and output format requirements. Following a discriminative paradigm, it is designed to assess whether the current state has successfully identified necessary elements and resolved sub-questions."}, {"title": "Case Study", "content": "We selected representative examples from CWQ and KQA Pro datasets to demonstrate the effectiveness of our method in handling complex queries. For clarity and space constraints, the figures only display the terminal nodes and their corresponding branches, omitting intermediate nodes. In the figures, green boxes represent valid terminal nodes, where the last action is Done and the penultimate action is a ExecuteSPARQL with non-empty execution result. Correspondingly, yellow boxes indicate invalid terminal nodes.\nIn that involves complex logical structures, including a CVT5 node and a one-hop relation. Despite the inherent difficulty in constructing such complex SPARQL queries, our method successfully generates the correct query through multiple exploration attempts."}]}