{"title": "PATHS: A Hierarchical Transformer for Efficient Whole Slide Image Analysis", "authors": ["Zak Buzzard", "Konstantin Hemker", "Nikola Simidjievski", "Mateja Jamnik"], "abstract": "Computational analysis of whole slide images (WSIs) has seen significant research progress in recent years, with applications ranging across important diagnostic and prognostic tasks such as survival or cancer subtype prediction. Many state-of-the-art models process the entire slide which may be as large as 150,000 \u00d7 150,000 pixels as a bag of many patches, the size of which necessitates computationally cheap feature aggregation methods. However, a large proportion of these patches are uninformative, such as those containing only healthy or adipose tissue, adding significant noise and size to the bag. We propose Pathology Transformer with Hierarchical Selection (PATHS), a novel top-down method for hierarchical weakly supervised representation learning on slide-level tasks in computational pathology. PATHS is inspired by the cross-magnification manner in which a human pathologist examines a slide, recursively filtering patches at each magnification level to a small subset relevant to the diagnosis. Our method overcomes the complications of processing the entire slide, enabling quadratic self-attention and providing a simple interpretable measure of region importance. We apply PATHS to five datasets of The Cancer Genome Atlas (TCGA), and achieve superior performance on slide-level prediction tasks when compared to previous methods, despite processing only a small proportion of the slide.", "sections": [{"title": "1. Introduction", "content": "Whole slide images (WSIs) \u2013 high resolution scans of sliced biopsy sections are the basis for pathologists to diagnose and analyse disease. Due to the importance and scale of this task, recent years have seen the development of a range of automated approaches to assist in processing and analysis, with particular success seen in the application of modern computer vision methods [10]. However, the gigapixel scale of WSIs, coupled with their pyramidal structure, challenges the application of standard vision architectures such as convolutional neural networks [13, 27] and vision transformers [11] at the slide level.\nWhen pathologists inspect whole slide images, they usually do so in a top-down manner: identifying regions of interest and tissue architecture (such as areas of cancerous tissue) at low magnification before investigating these areas further at greater magnification. To inspect the entire slide at its maximum resolution would be unduly time-consuming and largely uninformative, with only certain areas of the slide providing useful information. Conversely, most state-of-the-art deep learning methods process the slide in its entirety at high magnification, splitting the image into a large collection of small (e.g., 256 \u00d7 256px) patches, in the order of magnitude of 10,000 per slide [2, 10]. This incurs a high computational cost, and in many cases provides a large amount of uninformative data to the model, effectively creating a poor signal-to-noise ratio. Within this category, the most common approach is multiple instance learning (MIL), in which each slide is treated as a large unordered bag of patches that are processed using pre-trained computer vision models, and globally aggregated to produce slide-level representations [2, 15, 16, 22]. The global aggregation method must be efficient due to the scale of the bag; self-attention, for example, is infeasible, necessitating the use of less performant linear-time approximations [26, 31]. Past approaches to mitigating computational overheads include selecting only a small proportion of patches by random selection [30] or manual clustering-based heuristic [14]. However, such manual heuristics are suboptimal as they are error-prone and often inflexible. More recent work adapts hierarchical methods, which have seen success in the domain of computer vision, to WSIs [4, 6, 21, 34]. While more expressive than MIL, hierarchical methods nevertheless necessitate the pre-processing of the entire slide at its full magnification and require the use of self-supervision rather than task-specific training due to the large number of patches.\nIn this paper, we propose the Pathology Transformer with Hierarchical Selection (PATHS) a top-down hierarchical model as a novel weakly supervised approach"}, {"title": "2. Related Work", "content": "Multiple Instance Learning Whole slide images store scans of a slide at several magnifications, the highest of which corresponds to an image of up to 150,000 \u00d7 150,000px. Due to this large scale, multiple instance learning (MIL) [2, 16, 30] is frequently used in computational pathology tasks. Multiple instance learning treats each slide as a large unordered bag of low-resolution patches (e.g., 256 \u00d7 256 px) at a fixed magnification level.\nGeneral-purpose MIL approaches include ABMIL [15], which introduces an attention-based aggregation as a global weighted sum, where the per-patch weights are scalars produced as a learnable function of the patch feature. Given the success of self-attention in the domain of vision [11, 21], several works have explored self-attention based MIL aggregation [26]. However, in the context of computational pathology, the scale of the WSIs precludes the use of full self-attention, due to quadratic scaling with respect to the sequence length [29], forcing these methods to use less performant compromises such as linear-time approximations [31] or cross-attention with a smaller set [5].\nTo mitigate the issues caused by the large scale of WSIs, related work has focused on reducing the bag size through"}, {"title": "3. Method", "content": "3.1. Notation\nGiven a WSI X, let $X^m$ denote the collection of (square, non-overlapping) patches of size s \u00d7 s at magnification m, indexed by position (u, v), so $X^m \\in \\mathbb{R}^{s \\times s \\times 3}$. Patches are processed by a pre-trained image encoder I, such that $I(X^m) \\in \\mathbb{R}^d$ for some dimension d. We consider an arbitrary weakly-supervised task, with the goal of modelling a distribution p(y | X) (e.g., survival prediction).\n3.2. Patch Selection\nAt each magnification level m we identify a small subset of patches $X_m \\subseteq X^m$ to process. Unlike previous methods, which define $X_m$ non-parametrically using random choice or manual heuristics [14, 30], PATHS enables such a subset to be selected by the model during training. We achieve this by processing patches at n magnification levels $M_1 < M_2 < \\cdot \\cdot \\cdot < M_n$, which form a geometric sequence, $M_{i+1} = MM_i$, to ensure patch alignment between levels. The model consists of n processors $P_1, P_2, ..., P_n$, the ith of which is dedicated to processing patches at magnification $m_i$. Pi additionally learns a scalar importance value $\\alpha_{uv} \\in [0, 1]$ for each patch $X^m_i$, which models the relative importance of the patch, and provides a learnable heuristic for patch selection at the subsequent level:\n$X^{m_1} = X^{m_1}$\n$X^{m_{i+1}} = \\text{MAGNIFY}(\\text{FILTER}(X^{m_i}, \\alpha^2)).$ (1)\nFILTER retains only the K patches of highest importance, where K is a hyperparameter. MAGNIFY queries the WSI in the same location as these patches, but at the subsequent resolution, effectively \u2018zooming in' on the selected patches, then removing resultant patches which consist only of background. This process is visualised in Figure 1.\nAs patch size (in pixels) is kept constant across each hierarchy level, magnification produces $M^2$ output patches for each input (or fewer when background is present). As a result, we have a fixed upper bound of\n$|X^{m_i} < M^2K$ (2)\nfor i > 1. We use M = 2 in all experiments to enable a larger value of K. By choosing a low starting magnification $m_1$, we also ensure that $X^{m_1} = X^{m_1}$ contains a small number of patches.\nAs the predicted values of $\\alpha^2$ change during training, this technique effectively exposes the model to a large number of distinct patches over the course of training (regardless of K), helping to avoid overfitting."}, {"title": "3.3. Context", "content": "At higher magnification levels, only a small fraction of the slide's total area is visible to the model, making it beneficial to pass on information from prior magnification levels. We refer to this information as context, allowing the model to account for macro-scale slide properties when processing patches at high magnification, and employ it at both the patch- and slide-level.\nHierarchical Context Patch-level hierarchical context informs the model of the nature of the tissue surrounding each patch. This allows the incorporation of high-level features, such as tumour size, into the representations of patches at high magnification.\nFor each patch $X^{m_i}$ at magnification $m_i$, at each prior magnification level $m_j$ (j < i) there is a unique 'parent' patch at position $(u_j, v_j)$ such that the slide area covered by patch $X^{m_j}_{u_j, v_j}$ includes that of $X^{m_i}_{u, v}$. We define the hierarchical context of a patch $X^{m_i}$ as the list of all patch embeddings from parent patches at previous magnification levels,\n$C (X^{m_i}) = [I(X^{m_1}_{u_1, v_1}), ... I (X^{m_{i-1}}_{u_{i-1}, v_{i-1}})].$ (3)\n$C$ provides context for an individual patch by representing the surrounding area of the slide.\nSlide-level Context In addition to hierarchical patch-level context, we find it beneficial to pass high-level global information between magnification levels. To achieve this, each processor $P_i$ produces a slide-level (but magnification specific) representation $F^i$ following global aggregation.\nThen, rather than considering the final feature $F^n$ only, the final target prediction is modelled as a function of the slide context p(y | Cslide), where\n$C_{slide} = [F_1,..., F_m]$. (4)\nIn our experiments we carry out a simple summation reduction over the slide-level context, $F = \\sum_{i=1} F^i$, followed by a single linear layer to produce \u0177, leading to a residual model in which each processor after the first models an offset for the global feature. We leave exploration of more complex aggregation of the cross-magnification features $C_{slide}$ to future work.\n3.4. Processor Architecture\nEach processor $P_i$ consists of a contextualisation module, which incorporates hierarchical context into patch features, a transformer-based global aggregator, and an importance modelling module. Conditioned on the patches $X^{m_i}$, and per-patch hierarchical context $C(\u0160^{m_i})$, each processor produces an aggregated feature and importance predictions,\n$F_i, \\alpha^2 = P_i(\\check{X}^{m_i}, C(\\check{X}^{m_i})).$ (5)"}, {"title": "Contextualisation Module", "content": "Figure 2 illustrates the architecture of the contextualisation module. At high magnification, each patch feature contains information localised to an extremely small section of the slide; contextualisation aims to adapt these features to incorporate macro-scale tissue information. For a patch $X^{m_i}$, the contextualised feature $Y^{m_i}$ is defined as\n$Y^{m_i} = I(X^{m_i}) + \\text{RNN}(C(X^{m_i})),$ (6)\nwhere RNN denotes a learnable recurrent neural network, which is applied sequentially to the hierarchical context list $C(X^{m_i})$. In this manner the RNN produces a feature offset which accounts for high-level properties of the tissue surrounding each patch, thus 'contextualising' the patch feature. Summation of the RNN output was chosen to enable easy representation of the identity function $Y^{m_i} = I(X^{m_i})$, for cases in which a patch's surrounding tissue is not of high relevance.\nBy sharing the weights of the RNN between all processors, this operation may be implemented efficiently: each processor carries out a single recurrent unit update step per patch, passing the resulting state to the corresponding patches at the subsequent magnification level.\nImportance Modelling To enable patch selection, each processor Pi implicitly learns scalar importance values $\\alpha^2$ for patches at magnification mi. This is achieved through a gating mechanism, in which a two-layer MLP followed by sigmoid activation (denoted IMP\u00bf) is applied to the contextualised patch embeddings \u0176mi, producing scalar weights. Each embedding is then scaled by its corresponding weight"}, {"title": "Global Aggregation", "content": "to produce the final set of features \u017dmi,\n$Z_{uv}^{m_i} = \\alpha_{uv}Y^{m_i}_{uv}$. (7)\nThese features are globally aggregated, causing the model to assign higher importance values to patches with greater information content, as observed in past work [12, 15, 18].\nFollowing the success of self-attention based aggregation [5, 6, 26], the contextualised, importance scaled patch features \u017dmi are aggregated globally via a transformer decoder (denoted GLOBALAGG\u00bf). We incorporate a two dimensional positional encoding (based on that of Vaswani et al. [29]) due to the sparse distribution of patches across the slide's area. Aggregation produces the slide-level feature Fi for magnification level i, which is added to the slide-level context Cslide.\nAlgorithm 1 summarises the procedure carried out by each processor Pi, and the overall method for processing a slide X using PATHS is summarised in Algorithm 2 (for both, see Appendix A)."}, {"title": "4. Experiments", "content": "Datasets The Cancer Genome Atlas (TCGA) provides public databases of documented cancer cases over a range of sites, including diagnostic whole-slide images among other data. We evaluate PATHS on the survival prediction task across five cancer types: IDC (invasive ductal carcinoma), CRC (colorectal cancer), CCRCC (clear cell renal cell carcinoma), PRCC (papillary renal cell carcinoma) and LUAD (lung adenocarcinoma), which we select due to their large size within TCGA and frequent use in past work. We cross-validate our method across five folds for each dataset, using the same folds for each model.\nBaselines We compare PATHS to a number of state-of-the-art weakly-supervised baselines:\n\u2022 ABMIL [15]: Attention-Based Multiple Instance Learning (ABMIL) is a simple MIL variant. Scalar attention values are produced per patch, and used as weights in a global sum for slide-level aggregation.\n\u2022 DeepAttnMISL [32]: Variant of ABMIL, with the addition of phenotype-based clustering.\n\u2022 GCN-MIL [19, 35]: A GNN-based MIL approach. The slide is processed by several graph convolution layers before aggregation.\n\u2022 DS-MIL [17]: A MIL-based approach employing contrastive learning, multiple magnifications and a modified aggregation function.\n\u2022 HIPT [6]: Hierarchical Image Pyramid Transformer (HIPT) aggregates the entire slide in three vision transformer-based hierarchical stages. The bottom two stages are trained in a self-supervised manner using DINO [3]. Due to its hierarchical nature, we consider this baseline an important comparison for our work.\n\u2022 ZoomMIL [28]: A MIL approach in which patches are selected from multiple magnifications via a differentiable zooming procedure. We configure ZoomMIL to sample the same number of patches as PATHS at each magnification for fair comparison (further details in Appendix B).\nWhile all models are evaluated on the same folds and datasets, the results for ABMIL, DeepAttnMISL, GCN-MIL, DS-MIL and HIPT use pre-calculated risk scores for these folds, as reported in [6].\nSetup It is common to process whole slide images at 10\u00d7 or 20\u00d7 magnification to capture the details of individual cells [5, 6, 15, 22]. In all experiments, we select 10\u00d7 magnification as the bottom level of the hierarchy $m_n$, and 0.625\u00d7 as the top, ensuring that $X^{m_1} = X^{m_1}$ is of tractable size of all slides, leading to five magnification levels. We also set K = 20, causing a fixed limit of $M^2K = 80$ patches per slide at each magnification, a small fraction of the total (which may be as many as tens of thousands).\nTo train the model for survival prediction we use the censored negative log-likelihood training objective $L_{NLL}$ [33] with a = 0.6. We quantise patient survival times into b buckets such that each bucket contains roughly an equal number of uncensored patients. The model outputs b logits, corresponding to the survival hazards for each bucket, from which $L_{NLL}$ may be computed. We set b = 4 in all experiments.\nWe evaluate using the censored concordance index metric (c-index), which measures the proportion of comparable patient pairs (those in which one can tell with certainty the order in which the events occurred) for which the model's survival prediction is concordant, as is standard. Random choice achieves a score of 0.5, while the best possible score is 1. All experiments were run on a single Nvidia A100 80GB GPU. See Appendix B for a complete list of hyperparameters. The code to reproduce the experiments is available at https://github.com/zzbuzzard/PATHS.\nPatch Embedding We pre-process all patches using a pre-trained image encoder I, avoiding the heavy I/O and computation cost of reading and processing the patches during training. The results are stored as a two dimensional array of features, rather than an unordered bag as in MIL, to preserve positional information. Furthermore, unlike traditional MIL techniques, we must pre-process patches at several magnification levels, rather than at the highest magnification only: the total number of patches to be pre-processed per slide is $\\sum_{i=1}^{n-1} |X^{m_i}|$ rather than $|X^{m_n}|$. However, note that $|X^{m_n}|, |X^{m_{n-1}}|, ...$ forms a geometric sequence, as each time magnification is reduced by a factor of M, the number of patches $|X^{m_i}|$ falls by a factor of $M^2$. In the case of M = 2, which we use in all experiments, our method incurs a pre-processing overhead of a factor of $1 + \\frac{1}{M^2} + \\frac{1}{M^4} + \\cdot\\cdot\\cdot < \\frac{4}{3}$. Note that this overhead is only re-"}, {"title": "5. Results", "content": "Table 1 shows the performance of our model PATHS against several baselines on the survival prediction task. PATHS achieves the highest overall c-index across the five cancer subtypes, with the highest performance on four of the five datasets, despite processing only a small fraction of the slide. Compared to ABMIL, DeepAttnMISL, GCN-MIL, DS-MIL and HIPT, all of which process the entire slide as tens of thousands of patches at 20\u00d7 magnification, PATHS processes just several hundred patches per slide. Despite this, we achieve a significant improvement in model accuracy, highlighting the benefit of processing a smaller number of more relevant patches. The improvement over ZoomMIL, which similarly filters the patches to a small subset per slide, demonstrates the advantage of PATHS over MIL architectures.\nInference Speed When vision models are incorporated into practical tools for computational pathology, it is imperative to achieve low computational overhead and inference latency, since computational resources are often limited in a clinical setting. Whilst large-scale offline preprocessing of patch features enables fast training for 'full slide' methods (i.e., those which must process all tissue-containing patches at high magnification, such as ABMIL or HIPT), this workaround does not extend to inference time. When applied to a new slide in a clinical setting, the entire slide must first be loaded into memory and processed using the patch embedding network (which may be a large network, such as UNI), leading to significant latency even on high performance infrastructure. Figure 3 demonstrates that, by significantly reducing the number of patches required from each slide, PATHS significantly improves inference latency over full slide approaches. This is a key advantage of our method, as this preprocessing step is the dominant processing cost for both PATHS and full slide models at inference time, taking up over 99% of inference time in our experiments. Note that, even on state-of-the-art hardware and at just 10\u00d7 magnification (while 20\u00d7 is common), a minimal full slide approach takes over a minute to process a single new slide on average. It is reasonable to assume that latency will be significantly larger in practice, especially in the case of models running locally on clinical hardware to ensure patient confidentiality. Appendix C provides further details on the number of patches loaded by each approach (which is roughly proportional to inference latency) for a hardware-independent comparison of efficiency.\nThe main novelties of PATHS are the learnable patch selection module, combined with the patch- and slide-level context, allowing the propagation of cross-magnification information down the hierarchy. To investigate the contribution of each module to the overall performance of PATHS, we carry out an ablation study (Table 2) in which we evaluate several variants of our architecture on the five datasets used in Table 1.\nCross-magnification Context Improves Over MIL With both hierarchical and slide-level context removed, our model becomes similar to a single magnification MIL method. The drop in performance highlights the advantage of our method over MIL, although the score remains relatively strong across the datasets, likely due to the strength of transformer-based aggregation over a small set of extracted relevant patches. The addition of either hierarchical or slide-level context further improves performance, particularly that of slide-level context, demonstrating the benefit of incorporating cross-magnification information, as observed in other work [6, 17, 18]. However, it should be noted that"}, {"title": "Benefit of the Learned Sampling Heuristic", "content": "for the LUAD dataset, on which PATHS performs poorly, the removal of context leads to improved performance. As both ZoomMIL and HIPT also perform poorly on LUAD (Table 1), we hypothesise that cross-magnification information may be of low importance on this particular dataset and task, as evidenced by the strong performance of single magnification methods such as ABMIL.\nNext, we investigate the significance of extracting patches based on the predicted importance a. This is achieved through the replacement of the importance MLP (IMP) with a random distribution $\\alpha_{uv} \\sim U[0,1]$ at inference time, leading to the selection of random areas of the slide (although background patches are still excluded). Interestingly, this mod-"}, {"title": "6. Discussion", "content": "State-of-the-art methods in computational pathology generally rely on processing entire whole slide images as thousands of patches at high magnification. In this work, we present an alternative approach, in which we filter the processed data to a small subset of relevant patches across"}, {"title": "7. Conclusion", "content": "We provide strong evidence in this paper to suggest that the processing of entire whole slide images at full magnification is needlessly expensive. Through our design of a novel, patch efficient algorithm, we avoid many of the issues of processing entire slides (high computational cost, poor signal-to-noise ratio, very high latency in practice), improving both efficiency and accuracy. Finally, we demonstrate the benefit that patch contextualisation and slide-level context provide to our unconventional non-MIL approach, and we hope that our work inspires future work in this direction."}, {"title": "A. PATHS Algorithm", "content": "Algorithm 1 describes the algorithm carried out by each processor, and Algorithm 2 the overall PATHS algorithm applied to a slide X, in which PREDICT denotes the final linear layer whose output is the prediction \u0177."}, {"title": "Algorithm 1", "content": "Processing algorithm for patches at magnification mi\nprocedure $P_i(X^{m_i},C(X^{m_i}))$\n$Y_{uv}^{m_i} \u2190 I(X_{uv}^{m_i}) + RNN(C(X^{m_i}))$\n$\\alpha_{uv}^2 \u2190 IMP_i(Y^{m_i})$\n$\\check{X^{m_i}} \u2190 \\alpha^2 Y^{m_i}$\n$F_i \u2190 GLOBALAGG_i(\\check{X^{m_i}})$\nreturn $(F_i, \\alpha^2)$\nend procedure"}, {"title": "Algorithm 2", "content": "Overall PATHS slide processing algorithm\nprocedure PROCESS(X)\n$X^{m_1} \u2190 X^{m_1}$\n$C_{slide} \u2190 []$\nfor i = 1 to n do\n$(F_i, \\alpha^2) \u2190 P_i(X^{m_i}, C(X^{m_i}))$\n$X^{m_{i+1}} \u2190 MAGNIFY(FILTER(X^{m_i}, \\alpha^2))$\n$C_{slide} \u2190 C_{slide} + [F_i]$\nend for\nreturn PREDICT($C_{slide}$)\nend procedure"}, {"title": "B. Hyperparameters", "content": "Table 3 gives the hyperparameters chosen for PATHS, and Table 4 the hyperparameters chosen for ZoomMIL, which we choose to enable fair comparison between our methods. We initially used 40 epochs for ZoomMIL to match PATHS, but found that performance was improved when training for 100 as in the sample configuration."}, {"title": "C. Inference Speed Experiment Details", "content": "This section details the method used to produce Figure 3.\nABMIL First, background patches are identified using Otsu's method [23], applied to a low resolution version of the WSI. The patches are then loaded sequentially and processed using UNI in batches of 256 on the GPU.\nPATHS Patches are loaded sequentially, and processed with UNI in a single batch per magnification level (as there are a small number at each level). Otsu's method is used in the FILTER function to identify background patches.\nIn both cases, patches are loaded from disk using a single CPU thread, and pre-processing and model inference takes place on a single A100 GPU. In all experiments, loading and pre-processing the patches using UNI takes over 99% of the total time (and over 99.99% for the example MIL model), despite PATHS loading fewer patches. Figure 5 gives the corresponding number of patches loaded by each method, providing a measure of inference efficiency independent of hardware or image encoder."}, {"title": "D. Choice of Image Encoder", "content": "Table 5 compares the performance of PATHS for three different image encoders. To evaluate the importance of domain specific encoding for PATHS, we compare using UNI to using an ImageNet pre-trained ResNet50, and observe weaker performance in the latter case. We then compare two models trained specifically on WSI patches, a self-supervised vision transformer trained on a number of TCGA datasets at 20\u00d7 magnification [6] and UNI, a vision transformer trained on patches across many different tissue types, also at 20\u00d7 [7]. As discussed in Section 6, PATHS pro-"}, {"title": "E. Further Visualisations", "content": "Figure 6 shows an example heatmap of PATHS on TCGA-BRCA. Due to a lack of ground truth labels in TCGA, we display the predicted semantic segmentation alongside the PATHS heatmap. The prediction was computed using a U-Net model provided by tiatoolbox, pre-trained on the BCSS dataset (an annotated patch-level dataset derived from TCGA-BRCA) [1, 24, 25]. Due to the computational cost of evaluating this model, which requires the extraction and processing of tens of thousands of patches at 20\u00d7 magnification per slide, and lack of human annotated labels, we provide one such example only.\nFigure 7 displays further examples from CAMELYON17 [20]. Both figures use the PATHS model trained on TCGA-BRCA with seed 0, applied in-domain in Figure 6, and zero-shot to CAMELYON17 in Figure 7."}]}