{"title": "TEXGen: a Generative Diffusion Model for Mesh Textures", "authors": ["Xin Yu", "Ze Yuan", "Yuan-Chen Guo", "Ying-Tian Liu", "Jianhui Liu", "Yangguang Li", "Yan-Pei Cao", "Ding Liang", "Xiaojuan Qi"], "abstract": "While high-quality texture maps are essential for realistic 3D asset rendering, few studies have explored learning directly in the texture space, especially on large-scale datasets. In this work, we depart from the conventional approach of relying on pre-trained 2D diffusion models for test-time optimization of 3D textures. Instead, we focus on the fundamental problem of learning in the UV texture space itself. For the first time, we train a large diffusion model capable of directly generating high-resolution texture maps in a feed-forward manner. To facilitate efficient learning in high-resolution UV spaces, we propose a scalable network architecture that interleaves convolutions on UV maps with attention layers on point clouds. Leveraging this architectural design, we train a 700 million parameter diffusion model that can generate UV texture maps guided by text prompts and single-view images. Once trained, our model naturally supports various extended applications, including text-guided texture inpainting, sparse-view texture completion, and text-driven texture synthesis.", "sections": [{"title": "1 INTRODUCTION", "content": "Synthesizing textures for 3D meshes is a fundamental problem in computer graphics and vision, with numerous applications in virtual reality, game design, and animation. However, the most advanced learning-based methods [Cheng et al. 2023; Oechsle et al. 2019; Siddiqui et al. 2022; Yu et al. 2023a] are restricted to generating textures for specific categories due to scalability and data limitation. Recently, test-time optimization-based methods have emerged, which utilize pre-trained 2D diffusion models to produce image priors via score distillation sampling [Lin et al. 2023; Poole et al. 2022; Wang et al. 2023b; Yu et al. 2023b] or by synthesizing pseudo-multi-views [Chen et al. 2023b; Richardson et al. 2023; Zeng 2023]. While these methods can generate textures for a wide range of objects, they suffer from certain drawbacks, such as time-consuming per-object optimization and parameter tuning, susceptibility to the limitations of 2D priors, and poor 3D consistency in texture generation.\nIn recent years, there has been a surge in the development of large models across various domains, including natural language processing [Achiam et al. 2023; Touvron et al. 2023], image and video generation [Betker et al. 2023; Blattmann et al. 2023; Saharia et al. 2022], and 3D creation [Hong et al. 2023; Li et al. 2023; Tochilkin et al. 2024; Wang et al. 2023c; Xu et al. 2023; Zou et al. 2023]. These models produce high-quality results and demonstrate remarkable generalization capabilities. Their success can be primarily attributed to two key factors: (1) scalable and effective network architectures that improve performance as model size and data amount increase, and (2) large-scale datasets that facilitate generalization. In this paper, we explore the potential of building a large generative model by scaling up model size and data for generalizable and high-quality mesh texturing.\nWe introduce TEXGen, a large generative model for mesh textur-ing. Our model utilizes a UV texture map as the representation for generation, as it is scalable and preserves high-resolution details. More importantly, it enables direct supervision from ground-truth texture maps without solely relying on rendering loss [Hong et al. 2023; Li et al. 2023], making it compatible with diffusion-based train-ing and improving the overall generative quality. Previous works such as Point-UV-Diffusion [Yu et al. 2023a] and Paint3D [Zeng 2023] have attempted to leverage diffusion models to learn the dis-tribution of mesh textures. However, neither of these approaches achieved end-to-end training or feed-forward inference on general object datasets [Deitke et al. 2023], resulting in error accumulation and scalability issues.\nTo perform effective feature interaction on mesh surfaces, we propose a scalable 2D-3D hybrid network architecture that incor-porates convolution operations in the 2D UV space, followed by sparse convolutions and attention layers operating in the 3D space. This simple yet effective architecture offers several key advantages: (1) by applying convolution operations in the UV space, the net-work effectively learns local and high-resolution details; and (2)"}, {"title": "2 RELATED WORK", "content": "Texture generation via 2D diffusion models. A prevalent method for texturing 3D meshes involves test-time optimization with pre-trained 2D diffusion models. Techniques such as those based on score distillation sampling [Chen et al. 2023a; Lin et al. 2023; Metzer et al. 2023; Poole et al. 2022; Wang et al. 2023b; Yeh et al. 2024; Yu et al. 2023b], synthesize textures on 3D shapes by distilling 2D diffusion priors. However, these approaches have significant draw-backs, including high computational demands and inherent artifacts like the Janus problem and unnatural color. Another line of ap-proach [Cao et al. 2023; Ceylan et al. 2024; Chen et al. 2023b; Gao et al. 2024; Liu et al. 2023; Richardson et al. 2023; Wu et al. 2024; Zhang et al. 2024] leverages geometry-conditioned image genera-tion and inpainting to progressively generate the textures. TEXTure [Richardson et al. 2023], for example, generates a partial texture map from one perspective view before using inpainting to complete other views. However, this method struggles with inconsistencies due to the lack of global information across views. Text2Tex [Chen et al. 2023b] introduces an automated strategy for optimized view-point selection to avoid manual intervention. Meanwhile, TexFusion [Cao et al. 2023] proposes to aggregate appearances from multiple viewpoints during the diffusion denoising steps, producing a more consistent and cohesive texture map. Despite the advances, these methods predominantly lack 3D awareness due to their reliance on 2D diffusion models and often require time-consuming per-instance optimization.\nTexture generative models. A variety of learning-based approaches have been developed to train generative models from 3D data [Chang"}, {"title": "3 OVERVIEW", "content": "Given a 3D mesh S, our objective is to develop a generative model capable of producing high-quality textures for 3D surfaces based on user-defined conditions such as images or text prompts. The modeling comprises the following principal steps:\n(i) Data representations. We use UV texture maps as the mesh texture representation, which is compact and suitable for diffusion training. We discuss its characteristics in section 4.1 which motivate us to develop a new network architecture in section 4.2.\n(ii) Model construction and learning. We develop a novel hybrid 2D-3D network structure that effectively handles the unique characteristics of texture maps (section 4.2). We then train a diffusion model [Ho et al. 2020] to generate high-resolution texture maps for"}, {"title": "4 METHOD", "content": "4.1 Representation for Texture Synthesis\nA surface can be fundamentally viewed as a two-dimensional signal embedded within a three-dimensional space. Consequently, a tradi-tional technique in graphics for processing mesh structures is UV mapping, which flattens the 3D structure into a compact 2D repre-sentation. This transformation allows 3D attributes, such as textures, to be reorganized and represented on a 2D plane. The 2D UV space effectively captures neighborhood dependencies within individual islands, enhancing computational efficiency for texture generation [Yu et al. 2023a] thanks to its grid structure. Additionally, the explicit nature of the texture map facilitates direct supervision, making it well-suited for integration with diffusion models.\nThe advantages outlined above motivate our adoption of a 2D UV texture map as the representation for texturing 3D meshes. However, despite its merits, this approach inevitably loses the global-level 3D consistency among different islands due to the fragmentation inherent in UV mapping. As illustrated in fig. 2, islands S\u2081 and S\u2082 are contiguous on the 3D surface but are positioned far apart on the UV map. Conversely, S\u2081 and S3, which are adjacent on the UV map, do not share a physical connection on the surface. This fragmentation can lead to inaccurate feature extraction in conven-tional image-based models. To address this issue, we propose a novel model that synergizes the strengths of the 2D UV space-enabling high-resolution and detailed feature learning-with the incorpora-tion of 3D points to maintain global consistency and continuities. These components interleave and refine representations, facilitating effective learning for generating high-resolution 2D texture maps.\n4.2 Model Construction\nUtilizing 2D texture representations, we can train a diffusion model that conducts iterative denoising to generate a high-quality 2D tex-ture map, given a specific condition, such as a posed single image or a text prompt. The core of our model is a hybrid 2D-3D network that learns features in both 2D and 3D spaces. Unlike un-conditional generation, our work prioritizes conditional generation, particularly conditioning on textual and visual inputs. Text prompts provide an intuitive interface for users to specify desired attributes in the generated content, making the model more accessible and responsive to user intentions. On the other hand, conditioning on images offers precise control over the generation process by captur-ing pixel-level details that text alone may overlook, thus offering stronger guidance for diffusion models. Moreover, a single image with rich textures can serve as a valuable prior in the diffusion process, facilitating more effective learning. Since it is feasible to"}, {"title": "Hybrid 2D-3D block", "content": "Hybrid 2D-3D block. The key to our design is the hybrid 2D-3D block, which facilitates efficient feature learning for 2D texture map generation. As shown in fig. 3 (b), our hybrid block comprises a UV head and several 3D point-cloud blocks. An input UV feature $f_{in}$ is first processed through a 2D convolution block (see fig. 3 (c)) to extract local features in the UV space. 2D convolutions are computa-tionally more efficient compared to 3D convolutions or point cloud KNN searches for establishing neighbors and weighting, making it"}, {"title": "Serialised attention", "content": "* Serialized attention. For the input dense point features $f_{point}$, we adopt grid-pooling [Wu et al. 2022] to sparsify the number of points and obtain $f_{is}$. The pooled features are then treated as tokens and processed by point attention layers for learning. To boost the efficiency, we utilize Serialized Attention [Wu et al. 2023], which facilitates efficient patch-based attention. Specifically, the point features are partitioned into different groups based on their sterilized codes, defined by space-filling curves, such as the z-order curve [Morton 1966] and the Hilbert curve [Hilbert and Hilbert 1935].\n* Position encoding. Position encoding plays a critical role in incorporating 3D position information into our model. Tradi-tional methods that use point coordinates as cues for position encoding [Lai et al. 2022; Yang et al. 2023] are less effective compared to conditional positional encoding [Chu et al. 2021; Wang 2023; Wu et al. 2023], which utilizes convolution layers for this purpose [Wu et al. 2023]. Initially, we implemented the xCPE [Wu et al. 2023], which integrates a sparse con-volution layer directly before the attention layer. However, this approach proved to be inefficient and time-consuming as the transformer dimension increases (i.e., d = 2048). To ad-dress these inefficiencies, we developed a modified approach, termed sCPE, which uses a linear layer to reduce the input's channel dimension before executing the sparse convolution. Subsequently, another linear layer is used to expand the chan-nel dimension back to its original size to match the feature dimension of the skip connection.\n* Condition modulation. For both our 2D and 3D blocks, we utilize the global condition embedding to modulate intermedi-ate features, enabling the injection of conditional information. Specifically, inspired by DiT [Peebles and Xie 2023], we em-ploy MLPs to learn modulation vectors $\\gamma$ and $\\beta$ from the condition embedding $y$. These vectors are used to scale and shift the intermediate features across their channel dimen-sions, formulated as $f_{mod} = (1 + \\gamma) f_{in} + \\beta$. Besides, we also learn a gated scale $\\alpha$ to scale the output feature before its fusion with the feature from the skip connection, expressed as $f_{out} = \\alpha f'_{out} + f_{skip}$."}, {"title": "4.3 Diffusion Learning", "content": "Given a real texture map $x_0$, we randomly sample a time-step t (t\u2208 {0, 1, ..., 1000}) and add noise to the texture map by\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon,                                             (1)$\nwhere $\\epsilon$ ~ N(0,I), and {$\\alpha_t$} are hyperparameters that follow a certain noise scheduler. Specifically, we use a noise scheduler from"}, {"title": "4.4 Texture Generation", "content": "After training, our denoising network is ready to generate high-quality texture maps for 3D meshes. We start by initializing a pure Gaussian noise texture map in UV space. Then, using conditional information (e.g., single-view image, text prompt), we iteratively denoise it to generate a final texture map. To accelerate inference, we use DDIM [Song et al. 2020] sampling for 30 steps. Interestingly, although we trained our model guided by a single-view image and text prompt, it can be generalized to other scenarios and applications during testing.\nText to texture generation. If only a text prompt is provided, we can arbitrarily choose a mesh viewpoint, render a depth map, and then use ControlNet [Zhang et al. 2023] to generate a corresponding single-view image. This is also a motivation for us to train an image-conditioned model instead of a text-conditioned model since an image can be easily obtained from text, whereas a text-conditioned model lacks the control capabilities provided by an image.\nTexture inpainting. During training, the pixel information of the single-view image is projected back to UV space, resulting in a partial initial texture map. Our network is trained to fill in the unseen parts. We found that this capability allows the model to function as a texture inpainting model during testing. Specifically, we can take the user-provided partial texture map and mask as $x_I$, and input them into the network for inpainting. For the image embedding required during testing, we set it to zero embedding, as our training included randomly dropping the image embedding, making the model robust to this situation.\nTexture completion from sparse views. If the user provides a few sparse-view images, such as two images, we can effectively utilize the additional information for a generation. We simply project and fuse each image during the projection step and randomly select one image for image embedding extraction. Our model can fill textures in the occluded parts and recover the whole texture map."}, {"title": "5 EXPERIMENTS", "content": "We use Objaverse [Deitke et al. 2023] as our raw data source, which comprises over 800,000 3D meshes. After processing and cleaning this dataset, we extracted a total of 120,400 data pairs. Of these, 120,000 pairs are designated for training and the remaining 400 pairs are set aside for evaluation.\n5.1 Main Results and Comparisons\nWe present our primary results, which include the textured 3D mesh conditioned on a single-view image and a text prompt, as illustrated in fig. 1. Notably, consider the example of the bird, where the texture detailing on the feathers demonstrates the model's capability to generate highly detailed textures. Our results demonstrate that the model can generate high-quality textures with rich local details, preserve condition information, and achieve global coherence. We compare our method with other generalizable texture generation methods, including TEXTure [Richardson et al. 2023], Text2Tex [Chen et al. 2023b], and Paint3D [Zeng 2023].\nQualitative comparisons. We conducted a qualitative analysis com-paring our method with TEXTure and Text2Tex. Both these methods utilize a 2D pretrained text-to-image diffusion model for test-time optimization to texture 3D meshes. In the first example, despite the conditional image containing rich texture patterns, both methods produce overly smooth textures. In the second and third examples, while more details are shown, various artifacts degrade their quality. Besides, they do not well preserve the guided image"}, {"title": "Quantitative comparisons", "content": "Quantitative comparisons. We conduct quantitative comparisons on 400 test objects. Our method significantly outperforms other methods. Additionally, we tested our model's runtime speed on a single A100 GPU, and our method is notably faster than others, completing evaluations in under 10 seconds without requiring test-time optimization.\nIt is worth noting that our method fundamentally differs from others as it is a feed-forward model."}, {"title": "5.2 Applications", "content": "Without any fine-tuning, our model serves as a powerful foundation for various applications. As shown in fig. 7, we compose a scene with the generated objects to showcase our results. The scene is vivid and highlights the potential of our model in scene texturing applications. Each object can be customized using individual text prompts for control.\nWe also evaluate the text-condition results by two ways. We also demonstrate that our model can flexibly paint different sources of partial texture maps. The results show seamless integration with the existing texture. Additionally, in fig. 9, we consider a scenario where the user provides multi-view images. Our model effectively paints these regions, ensuring continuity and coherence."}, {"title": "5.3 Model Analysis", "content": "Hybrid blocks. We ablate the hybrid design of our block. For efficiency reasons, we select objects from the house category in our dataset for training and testing in this experiment.\nClassifier-free guidance. A key advantage of our model is its use of diffusion-based training instead of a regression loss."}, {"title": "6 CONCLUSION", "content": "In this work, we have presented TEXGen, a large generative diffu-sion model designed for creating high-resolution textures for general 3D objects. TEXGen departs from conventional methods that de-pend on pre-trained 2D diffusion models that necessitate test-time optimization. Instead, our model efficiently synthesizes detailed and coherent textures directly, leveraging a novel hybrid 2D-3D block that adeptly manages both local detail fidelity and global 3D-aware interactions. Capable of generating high-resolution texture maps in a feed-forward manner, TEXGen supports a variety of zero-shot applications, including text-guided texture inpainting, sparse-view texture completion, and text-to-texture synthesis. As the first feed-forward model capable of generating textures for general objects, TEXGen sets a new benchmark in the field. We anticipate that our contributions will inspire and catalyze further research and advance-ments in texture generation and beyond."}, {"title": "A APPENDIX", "content": "A.1 Implementation Details\nWe use Objaverse [Deitke et al. 2023] as our raw data source, which contains over 800K 3D meshes. However, the texture structure of these meshes is not uniform and requires processing and filtering. A.2 More Qualitative Results\nFirstly, in the video file submitted as supplementary material, we provide a video of the mesh rendering to demonstrate the results of our model. A.3 Limitations and Discussions\nCurrently, the condition images used during the training of our model are pose-aligned and shape-aligned, which may not meet the needs of users who wish to transfer"}]}