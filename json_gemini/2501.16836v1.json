{"title": "MISSPELLINGS IN NATURAL LANGUAGE PROCESSING: A SURVEY", "authors": ["Gianluca Sperduti", "Alejandro Moreo"], "abstract": "This survey provides an overview of the challenges of misspellings in natural language processing (NLP). While often unintentional, misspellings have become ubiquitous in digital communication, especially with the proliferation of Web 2.0, user-generated content, and informal text mediums such as social media, blogs, and forums. Even if humans can generally interpret misspelled text, NLP models frequently strug- gle to handle it: this causes a decline in performance in common tasks like text classification and machine translation. In this paper, we reconstruct a history of misspellings as a scientific problem. We then discuss the latest advancements to address the challenge of misspellings in NLP. Main strategies to mitigate the effect of misspellings include data augmentation, double step, character-order agnostic, and tuple-based methods, among others. This survey also examines dedicated data challenges and competitions to spur progress in the field. Critical safety and ethical concerns are also examined, for example, the voluntary use of misspellings to inject malicious messages and hate speech on social networks. Furthermore, the survey explores psycholinguistic perspectives on how humans process misspellings, potentially informing inno- vative computational techniques for text normalization and representation. Finally, the misspelling-related challenges and opportunities associated with modern large language models are also analyzed, including benchmarks, datasets, and performances of the most prominent language models against misspellings. This survey aims to be an exhaustive resource for researchers seeking to mitigate the impact of misspellings in the rapidly evolving landscape of NLP.", "sections": [{"title": "1 Introduction", "content": "Human language is constantly evolving. The world we live in is governed by information and communication technologies. Our time, sometimes dubbed the digital era, must thus be prepared to face changes in the way we communicate, and implement mechanisms to adapt to it. Changes in communication derive from multiple aspects. The use of non-standard written language might respond to cultural or societal factors, but not only. It may simply happen by mistake, in which case we speak of misspellings. Misspellings have become pervasive in the digital written production since the revolutionary Web 2.0 led people interact freely through social medial, blogs, forums, etc. Even though misspellings are generally unintentional, in some contexts these may also be intentional.\nOf course, the presence of misspellings complicate the reading of a text. Notwithstanding this, and somehow surprisingly, humans have the ability to read and comprehend misspelt text without much effort and, sometimes, even without realising their presence (Andrews, 1996; Healy, 1976; Shook et al., 2012; McCusker et al., 1981). Computers do not have similar capabilities though. Although the NLP community has long downplayed the problem of misspellings (if not for grammatical error correction (Shishibori et al., 2002) or text normalisation (Damerau, 1964)), it is by now abundant evidence that misspellings represent a serious risk to the performance of NLP systems (Baldwin et al., 2013; Heigold et al., 2018; Moradi and Samwald, 2021; N\u00e1plava et al., 2021; Nguyen and Grieve, 2020; Plank, 2016; Vinciarelli, 2005; Yang and Gao, 2019)."}, {"title": "2 A Brief History of Misspellings", "content": "The treatment of misspellings in NLP has traversed two main phases that hinge upon the proliferation of the so-called Web 2.0 and, with these, the spread of huge quantities of (often carelessly generated) user-generated content. The term Web 2.0 was first coined by Darcy DiNucci in 1999, but it was not until 2004 that the term became popular thanks to the Web 2.0 Conference.\u00b9 It took some time for the user-generated content to take on the Internet, something that we identified to indicatively happen around 2010. This section is devoted to briefly surveying the history of misspellings before (Section 2.1) and after (Section 2.2) this event."}, {"title": "2.1 Before 2010: Fewer Data, Less Misspellings", "content": "Before the explosion of user-generated data on the Internet, the vast majority of content available on the web (static web pages, journal articles, etc.) was characterised by the fact that the content was moderately well curated. As a result, the amount of data was relatively limited, and the available data contained few misspellings. For this reason, automated text analysis technologies were rarely concerned with the presence of misspellings, if at all. The study of misspellings was confined to the development of automatic correction tools that aid users in producing misspelling-free texts by, e.g., correcting typos, or applying OCR-produced errors. Vinciarelli (2005) is one of the pioneer studies dealing with NLP systems resilient to misspellings (more precisely, with errors acquired via OCR).\nSome studies seemed to indicate that the problem of misspellings was not paramount for text classification technologies, at least when these concern the classification by topic of (curated) text documents (Agarwal et al., 2007). The situation differed somewhat when shifting to other, less curated sources, such as emails, blogs, forums, and SMS data, or when analysing the output generated by automatic speech recognition engines from call centres. The problem attracted little attention from the research community at the time, and it was not until 2007 that a dedicated workshop, called Analysis for Noisy Unstructured Text Data (AND), emerged and renewed interest in the field (see also Section 6.4).\nTo the best of our knowledge, the only survey on NLP systems robust to noise was published in 2009 (Subramaniam et al., 2009). This survey primarily focused on handling noise in OCR scans, blogs, call centre transcriptions, and similar sources."}, {"title": "2.2 After 2010: The Rise of Social Networks and Deep Learning", "content": "Since 2010, user-generated content has become increasingly pervasive, mainly due to the revolution of social networks as stated in Perrin (2015). At the same time, deep learning technologies have taken the world by storm (Krizhevsky et al., 2012), not only due to the increase in performance they show off in most NLP tasks (Collobert et al., 2011), but also because of their potential to eliminate the need for manual feature engineering; instead, the neural network itself learns to represent the input. This raises questions about the necessity of a pre-processing step for correcting misspellings beforehand. The increasing prevalence of misspelt data and the proliferation of NLP technologies have inspired numerous studies analysing the impact of misspellings on state-of-the-art models (Section 4), as well as papers proposing systems that are resilient to misspellings (Section 5).\nThe study of misspellings in NLP presents significant benefits. The most apparent advantage is the enhancement of performance in any NLP tool, but not only. Systems that are resilient to misspellings are also safer. For reasons discussed later, some misspellings are intentional, designed to evade the scrutiny of content moderation tools or spam filters. Ultimately, the study of misspelling resilience aims to deepen our understanding of language (further discussions are offered in Section 9).\nThis increased interest in the subject was partially fostered by the work of Belinkov and Bisk (2018); Heigold et al. (2018); Edizel et al. (2019), who showed the performance of different models decrease noticeably in the presence of misspellings. This renewed momentum has led to the appearance of dedicated workshops devoted to studying the phenomenon from the point of view of user-generated content (such as the WNUT workshop series\u00b2) as well as from the point of view of machine translation (such as the WMT workshop/conference series\u00b3); more information about dedicated tasks and datasets can be found in Section 6.4.\nBetween 2017 and 2022, neural machine translation emerged as the most prolific field in the study of misspellings, closely followed by sentiment analysis. Over recent years, conversely, there has been a noticeable decline in the number of research papers in this field, coinciding with the advent of large language models (LLMs) (7). In particular, there has been a lack of new methods aimed at improving performance against misspellings, likely due to the high costs associated with experimenting with LLMs. However, several studies have conducted extensive analyses and proposed new benchmarks (Section 6.4.4) and other resources specific to LLMs (Section 7)."}, {"title": "3 What is a Misspelling?", "content": "The term misspelling is a too broad concept which has come to encompass many different types of unconventional typographical alterations. In this section, we turn to review the main considerations behind this term as viewed through the lens of linguistics and NLP, and we try to break down the many subtle nuances it encompasses. While in NLP, the"}, {"title": "3.1 Under the Lens of Linguistics", "content": "In linguistics, there are primarily three fundamental viewpoints for models dealing with misspellings: the first is that of general linguistics, where some authors have attempted to define and categorise various types of misspellings. The second one is that of sociolinguistics, in which authors analyse misspellings from a social perspective. The last one is psycholinguistics, which focused more on cognitively relevant aspects of the same problem.\nAs already mentioned, in general linguistics there is no broadly agreed definition of what a misspelling is, and the term error is often preferred. Error analysis is one important field of linguistics that studies the phenomenon of errors in second language learners. James (2013) defines errors as an unsuccessful bit of language. Richards and Schmidt (2013) instead define errors as the use of a linguistic item (e.g., a word, a grammatical unit, a speech act, etc.) in a way such that a fluent or native speaker of the language regards as showing faulty or incomplete learning.\nIn general linguistics, it is customary to draw a distinction between error and mistake. Richards and Schmidt (2013) point out that errors are due to a lack of knowledge of the speaker, while mistakes instead are made because of other compounding reasons, such as fatigue or carelessness. In the same work, errors are classified as belonging to lexical error (i.e., surface forms which are not included in a vocabulary), phonological error (i.e., in the pronunciation), and grammatical error (i.e., not compliant to syntactic rules). Interestingly enough, none of these concepts seem to embrace the possibility that misspellings may be created as a voluntary act.\nSociolinguistics focuses on the concept of spelling variation (Nguyen and Grieve, 2020). The very own word misspelling expresses prejudice against the author: the author of the noise is responsible for missing the correct normative spell of the word. Contrarily, the sociolinguistic perspective states that there are no such errors, but just variations on the spellings. These variations can originate from social needs, such as avoiding censorship, expressing group identity, or representing regional or national dialects.\nAfter outlining key conventions in the fields of general linguistics and sociolinguistics, it is essential to emphasize the significant relationship between psycholinguistics and the phenomenon of misspellings. As stated by Fern\u00e1ndez and Cairns (2010), psycholinguistics is a discipline that investigates the cognitive processes involved in the use of language, rather than the structure of language itself. In the case of reading, psycholinguistics is concerned with understanding the cognitive processes that underlie this activity, from the acquisition of the sensory stimulus derived from the visual perception of letters, to the subsequent comprehension and cognitive reorganisation of the information within the brain.\nThe branch of psycholinguistics that is most relevant to the topic of this survey is the one devoted to studying the cognitive processes behind the acts of writing and reading. It has been noted on several occasions that humans are able to read long and complex sentences that include misspelt words with little reduction in performance. The most notable example of this is that of garbled words, in which the internal letters are randomly transposed. Despite this, humans are able to read them with high accuracy.\nSome related work in the psycholinguistics literature include the work by Andrews (1996); Healy (1976); Shook et al. (2012); McCusker et al. (1981). This cognitive ability of human beings has inspired some of the methods we describe in Section 5.2.\nSome researchers in the field of NLP have gained inspiration from lessons learnt in psycholinguistics, and have taken advantage of these to devise models robust to misspellings. For example, characters that are graphically similar can be interchanged without significantly affecting human reading comprehension (e.g., closed for closed). This other intuition has inspired some of the methods that we discuss in Section 5.5.\nFrom a computational point of view, the study of misspellings would certainly benefit from the synergies with linguistics and psycholinguistics. The cognitive abilities humans display represent a source of inspiration for methods dealing with misspellings or the creation of adversarial attacks. As an example, consider spam emails in which the content is made of garbled words or in which graphically similar characters have been replaced."}, {"title": "3.2 Under the Lens of NLP", "content": "Nor in the field of NLP is there a single, clear-cut definition of misspelling. Indeed, the same type of problem (morphological error) is often expressed with different words, such as noise, typo, and spelling mistake."}, {"title": "4 How Serious is the Problem?", "content": "In principle, a straightforward way to eliminate misspellings is to simply run some Spelling Correction (SC) or Text Normalization (TN) tools. SC and TN pursue a common goal: translating from non-standard language to normative language. The distinction between them is subtle; SC is primarily intended to correct unintentional typos, while TN is rather devoted to converting any non-conventional surface form (e.g., slang) into the standard one.\nSC/TN tools are commonly employed as a pre-processing step in many industrial applications as a way to cope with misspellings, while the core of the system is designed to work with clean text. (Such an approach represents the simplest scenario within the so-called double-step methods that will be surveyed in Section 5.3.) A legitimate question that arises is the following: Can we simply rely on SC/TN tools and consider the problem solved?"}, {"title": "4.1 The Harm of Synthetic Misspellings", "content": "Synthetic misspellings are the most commonly employed type of misspelling in the related literature. The problem was partially dismissed by Agarwal et al. (2007), who tested traditional classifiers (SVM and Naive-Bayes) using bag-of-words representations, against misspellings generated using an automatic tool (dubbed SpellMess) which considers insertions, deletions, substitutions, and QWERTY errors,\u2074 in two well-known datasets for text classification (Reuters-21578 and 20 Newsgroups). Their results show that even moderately high levels of noise (affecting up to 40% of the words) did not affect classification accuracy as much as expected. The authors conjectured that this can be explained by the fact that many of the features affected by noise are rather uninformative, and that when classifying by topic, abundant patterns still remain in the rest of the training data, even at high levels of noise.\nQuite some time later, Belinkov and Bisk (2018) confronted various char-based and BPE-based\u2075 encoded neural translators with synthetic misspellings in what has then come to be considered a milestone paper in the field.\nTheir results demonstrated that all machine translation models were significantly affected by the presence of synthetic misspellings. This paper became influential and has served to raise awareness on the problem of misspellings.\nInspired by the latter, Naik et al. (2018) conducted robustness experiments on Natural Language Inference (NLI) models using different types of synthetic misspellings, such as swapping adjacent characters or inserting QWERTY errors. The authors designed a stress test to assess whether the qualitative result of NLI models are driven not only by strong pattern matching but also by genuine natural language understanding procedures. The paper goes on by demonstrating that the performance of NLI models, built on top of BiLSTMs and Word2Vec, declines when misspellings are inserted in the test set.\nHeigold et al. (2018) carried out experiments considering different types of synthetic noise on the tasks of morphological tagging and machine translation and using different types of encodings, including word-based, character-based, and BPE-based ones. In their experiments, different models were trained independently on different variants of the training set, including clean, the original set without misspellings; scramble, obtained by permuting the order of the characters with the exception of the first and last one in each word; swap@10, that randomly swaps 10% of subsequent characters; and flip@10, that randomly replaces 10% of the characters with another one. Every pair of (system, training set variant) was tested against similar variants generated out of the test set. The results show the performance of all tested models degrades noticeably when exposed to synthetic misspellings different from those on which the system was trained on.\nBERT, the popular transformer model proposed by Devlin et al. (2019), has garnered a great deal of attention due to its ability to deliver state-of-the-art performance across a wide range of NLP tasks. Given its success, several studies have"}, {"title": "4.2 The Harm of Natural Misspellings", "content": "Naturally occurring misspellings are invaluable resources for testing NLP applications in real-world settings (Baldwin et al., 2013; Belinkov and Bisk, 2018). However, they are rarely employed in practice since collecting natural misspellings is anything but a simple task. As we have already pointed out, there is a lack of consensus on what precisely a natural misspelling is and, despite the fact that some domains of information (like user-generated content) are known to be prone to naturally generate misspellings, other authors have taken as natural misspellings different phenomena, like the errors generated by second language learners (N\u00e1plava et al., 2021) or by OCR scans (Vinciarelli, 2005). The root of this discrepancy can be traced back to the definition of domain itself; according to Plank (2016), real-world data emerge as complex interactions of many more dimensions (language, genre, group of age, etc.) than what we can realistically anticipate in an experimental setting.\nPrevious studies related to the analysis of natural misspellings are often devoted to understanding which types of misspellings are more likely to occur in which domains (Plank, 2016; Baldwin et al., 2013). For example, Baldwin et al. (2013) compared the rate of out-of-vocabulary terms (as a proxy of the number of misspellings) expected to be found in texts as a function of how curated these texts are. The results were arranged in an ordinal scale of increasing levels of"}, {"title": "5 Methods", "content": "In this section we offer a comprehensive overview of previous efforts devoted to counter misspellings. We organise existing methods according to the following categorisation:"}, {"title": "5.1 Data Augmentation", "content": "One of the earliest attempts for addressing the problem of misspellings in NLP comes down to expanding the training set with misspelt instances, so that the model learns to deal with them during training.\nAlthough data augmentation techniques typically lead to direct improvements, there are important limitations worth mentioning. Augmenting the training set entails an additional cost, sometimes derived from complex techniques that seek to uncover the weaknesses of the model. Yet another important limitation regards its circumscription to a limited frame time. The misspelling phenomenon is not stationary, since language is in constant evolution. Additionally, data augmentation typically over-represents certain types of misspellings, thus injecting sampling selection bias into the model (i.e., the prevalence of the phenomena represented in the training set widely differ with respect to the prevalence expected for the test data as a result of a selection policy). Finally, misspellings consist of different character combinations, making it nearly impossible to achieve comprehensive coverage.\nWe first review a direct application of data augmentation strategies to the problem of misspellings (Section 5.1.1) and then move to describing methods that use a specific kind of generation procedure based on adversarial training (Section 5.1.2)"}, {"title": "5.1.1 Generalized Data Augmentation", "content": "To the best of our knowledge, the first attempt to cope with misspellings by means of data augmentation is by Heigold et al. (2018). The methodology consists in analysing the type of misspellings that most harmed the performance of a machine translator, and insert similar occurrences in the training set. In a similar vein, Belinkov and Bisk (2018) injected misspellings of various types in a parallel corpus, including the full permutation, characters swapping, middle permutation, and insertion of QWERTY errors. Information about how precisely these misspellings are individuated, and about other types of misspellings is available in Section 6.4 devoted to datasets.\nData augmentation has been extensively applied to the problem of machine translation (Vaibhav et al., 2019; Karpukhin et al., 2019; Zheng et al., 2019; Li and Specia, 2019; Passban et al., 2021) as a means to confer resiliency to misspellings to the models (for the most part, char-based neural approaches). For example, Vaibhav et al. (2019) augment the training instances of French and English languages in the EP dataset (see Section 6.4) by using the MTNT dataset of Michel and Neubig (2018) (see Section 6.4).\nNoise is injected by randomly adding or deleting characters, and randomly adding emoticons and stop words in order to simulate noise and grammatical errors.\nKarpukhin et al. (2019) experimented with four different types of misspellings, correspondingly generated by deleting, inserting, substituting, and swapping characters, that were applied to 40% of the training instances for Czech, German, and French source languages. Some authors have investigated the idea of backtranslation (i.e., reversing the natural direction of the translation, thus translating from the target language to the source language) as a mechanism to generate additional data. The idea is to generate the source translation-equivalent in domains in which resources for the target language are more abundant. The final goal is thus to enhance the source data and to inject misspellings so that a machine translation model resilient to misspellings can be later trained (Zheng et al., 2019; Li and Specia, 2019). In particular, Zheng et al. (2019) applied this technique to social media content for English-to-French, based on the observation that training data for this social media rarely contain misspellings in the target side, or do so in very limited quantities. They used additional techniques to expand the training set, including the use of out-of-domain documents (they considered the domain of news) along with their automatic translations."}, {"title": "5.1.2 Adversarial Training", "content": "A different, related strategy for augmenting the data is by means of adversarial training. There are two main types of adversarial training that have been applied to the problem of misspellings:\n1. White-box setting: Resiliency to misspellings is attained by adopting a perturbation-aware loss as the objective function of a specific model.\n2. Black-box setting: A general-purpose model is trained to develop robustness to adversarial samples.\nLi et al. (2019a) propose TEXTBUGGER, a method to generate misspellings by means of adversarial attacks. The method first searches for the most influential sentences (those for which the classifier returns the highest confidence scores) and then identifies the most important words in each such sentence (those that, if removed, would lead to a change in the classifier output). These words are altered by injecting misspellings either in training and in test documents. Zhou et al. (2020) generate adversarial examples via a loss-aware perturbation (hence, in a white-box setting) following Goodfellow et al. (2015), i.e., a perturbation to the input optimised for damaging the loss of the model. Their neural model was dubbed Robust Sequence Labelling (RoSeq), and was applied to the problem of named-entity recognition (NER). The idea is to optimise both for the original model's loss and for the perturbation loss, simultaneously. Note that in this case, there is no explicit augmentation of training data, but rather an implicit regularisation in the loss function that carries out the adversarial training approach. Cheng et al. (2019) applied a similar idea but in the context of machine translation. The method is called Doubly Adversarial Input since, in this case, the perturbation is applied both to the source and to the target sentences. The most influential words in a sentence (hence, the candidates to perturb) are identified by searching for possible replacements that, if used in place of the original word, would yield the maximum (cosine) distance in the embedding space with respect to the original vector. The set of candidate words that are electable for this replacement is made of words that are likely to occur in place of the original one according to a language model trained for the source or target language, correspondingly. For the target sentences, this set is further expanded with words that the translator model itself considers likely. Later on, Park et al. (2020) extended this idea to the concept of subwords and their segmentation (see also Kudo and Richardson, 2018)."}, {"title": "5.2 Character-Order-Agnostic Methods", "content": "There is abundant evidence from the psycho-linguistics literature indicating that humans are able to read garbled text (i.e., text in which the character-order within words is rearrangement, often called scrambled) without major difficulties, as long as the first and last letters remain in place, as in, The chatrecras in tihs sencetne hvae been regarraned. (see, e.g., Andrews, 1996; Rayner et al., 2006; McCusker et al., 1981). This is not true for computational language models relying on current representation mechanisms, though (Heigold et al., 2018; Yang and Gao, 2019). Character-order-agnostic methods (Sakaguchi et al., 2017; Belinkov and Bisk, 2018; Malykh et al., 2018; Sperduti et al., 2021) gain inspiration from these observations and propose different mechanisms that defy the need for representing the internal order of the characters; Figure 2 depicts this intuition.\nThe earliest published work we are aware of is by Sakaguchi et al. (2017). Their model, called the Semi-Character Recurrent Neural Network (ScRNN), represents the first and last characters of a word as separate one-hot vectors, while the internal characters are encoded as a bag-of-characters, i.e., a juxtaposition of one-hot vectors where character order is disregarded. The model was applied specifically to spelling correction, rather than to any particular downstream application. ScRNN was adopted as the first stage of a double-step method by Pruthi et al. (2019) (covered in Section 5.3).\nLater on, Belinkov and Bisk (2018) proposed a representation mechanism, called meanChar, that was tested in machine translations contexts. In particular, the representation comes down to averaging the character embeddings of a word, and then using a word-level encoder, along the lines of the CharCNN proposed by Kim (2014).\nMalykh et al. (2018) proposed Robust Word Vectors (RoVe), a method that generates three vector representations out of each word: the Begin (B), Middle (M), and End (E) vectors. These vectors correspond to the juxtaposition of the one-hot vectors of certain characters in a word. For example, given the word previous, B is the sum of the one-hot vectors of the first three characters (pre), E is the sum of the one-hot vectors of the last three characters (ous), while M sums the one-hot vectors of all characters in the word (and not only of the remaining central characters, as the name may suggest). The method showed promising results in three different languages including Russian, English, and Turkish, and in three different tasks including paraphrase detection, sentiment analysis, and identification of textual entailment.\nSperduti et al. (2021) proposed a pre-processing trick, called BE-sort, to tackle the problem. The method comes down to alphabetically sorting all middle characters of a word, excluding the first and the last character, so that the original word itself (e.g., embedding) as well as any potentially garbled variant of it (e.g., edbemindg, ebmeinddg, etc.), would end up being represented by the exact same surface token (e.g., ebddeimng). This pre-processing is not only applied to the words in the training corpus, but also to every future test word. Word embeddings learned by using Skip-gram with negative sampling on a BE-sorted variant of the British National Corpus were found to perform almost on par, across 17 standard intrinsic tasks, with respect to word embeddings learned on the original corpus, and much better than word embeddings learned on variants of this corpus in which words were garbled at different probabilities."}, {"title": "5.3 Double-Step with Text Normalization", "content": "As the name suggests, double-step methods tackle any task by performing two subsequent steps: first, a task-agnostic text normalization step addresses and corrects any misspellings in the input text; second, the actual task of interest is performed, with the assumption that the input is now error-free. Since the first step removes misspellings from the"}, {"title": "5.4 The Tuple-Based Methods", "content": "Although the intuitions behind tuple-based methods are variegated, these methods find a common ground in the way training instances are represented. The two most common approaches are tuples of the form (x, x'), in which x and x' represent a clean and misspelt instance, respectively, and triplets of the form (x, x', y) in which y is a task-dependent specification of the desired output (e.g., a translation of x). The instances can be words, sentences, or any other lexical unit.\nAlam and Anastasopoulos (2020) used a tuple-based method to endow a transformer-based machine translator with resiliency to misspellings. To do so, they resorted to a dataset originally designed for grammatical error correction and consisting of tuples (x, x'), with x' a misspelt version of the sentence x. The idea is to generate translations of x to create new tuples (x', y) in which the misspelling-free translation y is presented as the desired output for the misspelt input x'; tuples thus created are then used to fine-tune a transformer model.\nZhou et al. (2019) proposed a cascade model based on triples for machine translation. Given a triplet (x, x', y) (in which x, x', and y are defined as before), the model combines two auto-encoders sequentially: the first one is a denoising auto-encoder that receives x as the expected output for input x', while the second one is a translation decoder that receives y as the expected output for the encoded representations of x and x'."}, {"title": "5.5 Other Methods", "content": "This section is devoted to discuss relevant related methods that do not belong to any of the aforementioned groups. Papers in this section include ideas as variegate as experimental encodings (Jones et al., 2020; Sankar et al., 2021; Wang et al., 2020; Salesky et al., 2021), regularisation functions (Li et al., 2016), and contrastive learning (Sidiropoulos and Kanoulas, 2022; Chen et al., 2022).\nJones et al. (2020) propose Robust Encoding (RobEn), a (context-free) encoding technique that maps a word (e.g., bird) along with its possible misspellings (e.g., brid, bidr, etc.) to the same token, so that the variability among these surface forms becomes indistinguishable to the downstream model. Unique tokens therefore represent clusters of terms and typos. The authors study means for obtaining these clusters, and analyse the impact of different clustering strategies in terms of stability (measures the resiliency to perturbations) and fidelity (a proxy of the quality of the tokens in terms of the expected performance in downstream tasks). An initial solution is proposed in which clusters are decided by seeking for connected nodes in a graph in which nodes represent words from a controlled vocabulary, and in which edges connect words that share a common typo. Such a solution is found to lead to very stable solutions, but at the expense of fidelity. The final proposed method relies on agglomerative clustering, and searches for the clusters by optimising a function that trades-off stability for fidelity. Sankar et al. (2021) propose a method based on Locality-Sensitive Hashing (LSH). The final goal of LSH representations is to derive vectorial representations on-the-fly, thus reducing the memory footprint traditional embedding matrices require. In a nutshell, LSH assigns a hash code (i.e., binary representation much shorter than a standard one-hot encoding) to a word based on its n-grams, skip-grams, and POS tags, and derives a vector representation as a linear combination of learnable (low dimensional) basis vectors. The intuition is that LSH projections may lead to similar representations for clean and misspelt sentences, since this hashing is, by construction, low sensitive to noise. The experiments reported in text classification and perturbation studies seem indeed to confirm these intuitions.\nWang et al. (2020) experimented with visually-grounded embeddings of characters. The idea consists of generating an image for every character (i.e., rastering a character in a specific font-type and font-size), thus obtaining a matrix representation of it (the pixels of the image), that can directly be used as an embedded representation of the character. The intuition is that visually similar characters (e.g., \u2018o', \u2018O', '0') shall end up being represented by similar such embeddings. The images (i.e., the character embeddings) are further reduced using PCA, and given as input to a char-based CNN that acts as the encoder for a machine translation neural model. In a related study, Salesky et al. (2021) explored visually-grounded representations of sliding windows (specifically, subword tokens) for machine translation. This work was later used, by the same team of researchers, at the basis of PIXEL (Rust et al., 2023), a language model that similarly processes text as a visual modality. PIXEL was trained using the ViT-MAE (He et al., 2022) architecture"}, {"title": "6 Tasks, Evaluation Metrics, and Datasets", "content": "Misspellings affect written text in a broad sense, and thus no text-related application is at safe from these. However, the phenomenon has been more actively investigated in particular contexts, with machine translation and text classification being the most prolific such areas. Figure 3 gives an insight into how methods have been applied to which tasks at the time of writing this survey. In this section, we turn to describe the most important tasks (Section 6.1) in which misspellings have been investigated, by also discussing the most employed evaluation metrics (Section 6.2), dedicated events (Section 6.3), and datasets (Section 6.4)."}, {"title": "6.1 Main Tasks", "content": "The main tasks in which the phenomenon of misspellings has been more thoroughly investigated are listed below:\n\u2022 Machine translation (MT) is a supervised learning task that involves producing a text in a target language that is a translation equivalent of a text written in a different source language. With most modern MT systems relying on neural networks, the field is nowadays broadly referred to as Neural Machine"}]}