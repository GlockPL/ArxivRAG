{"title": "Hardware Acceleration of LLMs: A comprehensive survey and comparison", "authors": ["Nikoletta Koilia", "Christoforos Kachris"], "abstract": "Abstract-Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks, revolutionizing the field with their ability to understand and generate human-like text. In this paper, we present a comprehensive survey of the several research efforts that have been presented for the acceleration of transformer networks for Large Language Models using hardware accelerators. The survey presents the frameworks that have been proposed and then performs a qualitative and quantitative comparison regarding the technology, the processing platform (FPGA, ASIC, In-Memory, GPU), the speedup, the energy efficiency, the performance (GOPs), and the energy efficiency (GOPs/W) of each framework. The main challenge in comparison is that every proposed scheme is implemented on a different process technology making hard a fair comparison. The main contribution of this paper is that we extrapolate the results of the performance and the energy efficiency on the same technology to make a fair comparison; one theoretical and one more practical. We implement part of the LLMs on several FPGA chips to extrapolate the results to the same process technology and then we make a fair comparison of the performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Modeling human language on a large scale is a complex process that has taken decades to develop. It started in 1950 with Claude Shannon, who applied information theory to human language. Since then, tasks like translation and speech recognition have advanced significantly.\nArtificial Intelligence (AI) and Machine Learning (ML) are key to this progress. ML, a subset of AI, allows computers to learn from data. ML models are either supervised (making predictions) or unsupervised. This thesis focuses on supervised models, which predict and compare values to minimize error through optimization.\nDeep Learning models are divided into Generative (creating new data) and Discriminative (distinguishing data types). Generative AI, a subset of deep learning, uses neural networks to process labeled and unlabeled data. Large Language Models (LLMs) help understand characters, words, and texts.\nIn 2017, transformers revolutionized language modeling. Transformers, a type of neural network, handle long-term text dependencies using an attention mechanism. Google created the first transformer model for text translation in 2017. Transformers have since evolved, improving attention mechanisms and architectures.\nChatGPT, a notable LLM, predicts text continuations and performs tasks like answering questions, summarizing texts, and more. It uses probability distributions to generate various text forms based on user requests."}, {"title": "A. LLMs", "content": "Large Language Models (LLMs) are extensive, general- purpose models that can be pre-trained and adapted for specific tasks. They solve common language problems such as text classification, question answering, summarization, and text generation in various domains.\nLLMs are \"general-purpose\u201d because they handle diverse tasks and \"large\" due to their massive training datasets and numerous parameters. These models have multiple neural network layers with adjustable weights that learn to predict the next word in a sentence during training.\nThe number of parameters indicates the model's complexity and capacity. Weights, adjusted during training, connect neurons in different layers, influencing the model's performance.\nTransformers, a type of LLM, consist of an encoder and a decoder. The encoder has six layers, each with Multi-Head Self-Attention and a feed-forward network. The decoder has six layers, including an additional multi-head attention layer over the encoder's output.\nThe attention mechanism maps queries and key-value pairs to outputs, with positional encoding adding information about character positions. This architecture enables transformers to handle long-term dependencies in text effectively"}, {"title": "B. Encoder-Decoder", "content": "The encoder-decoder architecture is central to Large Language Models (LLMs) and designed to process and generate sequences. This architecture has two stages:\n\u2022\tEncoder: The input (e.g., natural language) is transformed into a vector representation that encapsulates the meaning of the input.\n\u2022\tDecoder: The decoder takes this vector representation and generates an output sequence, such as a translation into another language."}, {"title": "C. Attention Mechanism", "content": "The Attention Mechanism is vital in modern machine learning, especially in transformers, improving sequence processing tasks like translation and text generation. It connects both the encoder and decoder stages. The Attention Mechanism includes two further mechanisms: Multi-Head Attention and Self-Attention. The former focuses attention on different parts of the input simultaneously, allowing the model to recognize complex patterns and relationships in the input data. The latter captures dependencies and relationships between tokens regardless of their distance. It uses three matrices: Query (Q), Key (K), and Value (V). These matrices determine how much attention each token should give to another, enhancing the quality of translations and other sequence-based tasks."}, {"title": "D. Related work", "content": "Until now there is not any comprehensive survey on the hardware accelerators to speed-up the most computational intensive tasks of Transformers. In [1], a survey has presented a survey on the hardware acceleration of transformer networks for autonomous driving. The paper presents several efforts on the acceleration of tasks such as object detection, 3D segmentation, and lane detection.\nIn 2022, Huang et al. presented a survey on hardware acceleration for transformers [2]. The paper was mostly focused on the the transformer model compression algorithm based on the hardware accelerator and was limited mostly on FPGA-based implementation.\nIn 2023, Emani et al [3] presented a comprehensive performance study of LLMs on several computing platforms and evaluated their performance characteristics for these models.\nIn this paper, we present a comprehensive survey of the several research efforts that have been presented for the acceleration of transformer networks for Large Language models and NLP using hardware accelerators. The survey presents the frameworks that have been proposed and then performs a qualitative and quantitative comparison regarding the technology, the processing platform (GPU, FPGA, ASIC, In-Memory), the performance, and the energy efficiency of each framework. First, we present the accelerators based on FPGAs, then we present the accelerators targeting GPUs and finally accelerators ported on ASICs and In-memory architectures.\nThe main contributions of this papers are the followings:\n\u2022\tAn extensive survey of hardware acceleration of LLM using FPGA, ASICs, In-memory architectures and GPUs.\n\u2022\tA comparison in terms of performance (GOPs), energy efficiency (GOPs/W) and speedup.\n\u2022\tAn extrapolation of the features to the same technology for a fair comparison in terms of performance and energy efficiency."}, {"title": "II. FPGA-BASED ACCELERATORS", "content": ""}, {"title": "A. FTRANS", "content": "In 2020, Li et al [4] presented a hardware acceleration framework, called FTRANS, that was targeting the acceleration of transformer-based large scale language representations. It focuses on compression and acceleration to address computing and storage requirements, achieving up to 16 times compression with minimal accuracy loss through a Block Circulant Matrix (BCM) based weight model. The model significantly improves speed and energy efficiency, surpassing CPU and GPU implementations, with a comparison showing FTRANS is 81x faster and 9x more energy-efficient than alternatives, specifically compared to the GPU processor RTX5000 using VCU118 (16nm). The accelerator achieves a performance rate of 170 GOPs and an energy efficiency rate of 6.8 GOPs/W."}, {"title": "B. Multi-Head Attention", "content": "In 2020, Lu et al. presented an FPGA based architecture for the acceleration of the most computationally intensive parts of transformer networks [5]. In their work they propose a novel hardware accelerator for two key components, i.e., the multi-head attention (MHA) ResBlock and the position-wise feed-forward network (FFN) ResBlock, which are the two most complex layers in the Transformer.\nThe proposed framework is implemented on a Xilinx FPGA. Based on the performance evaluation the proposed design achieves a speed-up of 14.6x compared to a V100 GPU."}, {"title": "C. FPGA NPE", "content": "In 2021, Khan et al. presented an FPGA acceleration for language models called NPE. [6]. The NPE architecture consists of an instruction control unit (ICU), a memory read unit (MRU), a memory write unit (MWU), a matrix multiply unit (MMU), and a nonlinear vector unit (NVU).\nNPE was implemented on Xilinx Zynq Z-7100 FPGA board clocked at 200 MHz. NPE is compared with other frameworks like FTRANS and implementation on CPU and GPU. Although that there is not any significant speedup compared to other computing platforms, the main advantage is the energy efficiency. NPE achieves around 4\u00d7 better energy efficiency over CPU (i7-8700k) and 6\u00d7 over GPU (RTX 5000)."}, {"title": "D. Column Balanced Block Pruning", "content": "In 2021, Peng et al. presented a novel scheme on accelerating Transformer networks using column balanced block- wise pruning [7]. The column balanced block-wise pruning combines the key features of both bank balanced pruning and block-wise pruning. The column balanced block-wise pruning ranks the blocks' L2 norm by each column to get the pruning thresholds and prunes blocks for each column.\nThe proposed framework has been implemented on different hardware platforms (Intel i5-5257U (2.7 GHZ) CPU, Nvidia Jetson TX2 GPU, and Xilinx Alveo U200 FPGA) for further comparison of latency and throughput. The experimental results showed that the FPGA platform achieves a 11\u00d7 speed up compared to the CPU platform and 2x speed up compared to the GPU platform."}, {"title": "E. Compressed Block Row", "content": "In 2021, Panjie Qi et al, presented an acceleration framework that combines balanced model compression at the algorithm level with an FPGA implementation optimization at the hardware level [8]. In their work, they propose an effective sparse matrix storage structure for block-balanced pruning, known as Compressed Block Row (CBR), and their hardware design includes an accelerator for sparse models. Moreover, they present a performance analytic methodology for evaluating accelerator performance. The experiments demonstrate that their CBR format outperforms conventional formats and saves substantial storage space.\nThe proposed framework is implemented on a Xilinx ALveo U200 FPGA. Based on the performance evaluation the proposed design achieves a speed-up of 38x compared to a Nvidia Guardo RTX 6000."}, {"title": "F. ViA", "content": "In 2022, Teng Wang et al, presented ViA [9], an FPGA- based accelerator architecture for Vision Transformers (ViT), featuring a memory recognition unit, a memory write unit, and processing elements like the NSA self-attention module and MLP. It proposes data partitioning strategies to enhance efficiency and reduce dependency. ViA's FPGA implementation significantly outperforms CPUs, GPUs, and previous FPGA accelerators, achieving 60x the speed and 5x the energy efficiency of alternatives like the Nvidia Tesla V100 and Alveo U50 (16nm). ViA reaches an acceleration rate of 309.6 GOPS and an energy efficiency rate of 7.9 GOPs/W."}, {"title": "G. FPGA DFX", "content": "In 2022, Hong et al. presented DFX [10] for the acceleration of the transformer networks used in LLMs. Similarly to NPE, the DFX architecture proposed a modular architecture consisting for several computer core for the acceleration of the transformer networks.\nFor the evaluation, DFX has been implemented on an Intel Xeon Gold 6226R CPU with four Xilinx Alveo U280 data center acceleration cards. DFX achieves an average of 3.8x throughput and 4x higher energy efficiency compared to the GPU appliances."}, {"title": "H. STA", "content": "In 2022, Chao Fang et al, presented the Sparse Transformer Accelerator (STA) on FPGA to address the high computational demands of transformer models [11]. Utilizing an N structure, the STA minimizes operations and memory size while enhancing performance. The design includes a unified matrix multiplication mechanism, a Softmax module, and a Dense Matrix Multiplication Engine (DMME), implemented on an Intel Arria 10 SX660 device. It significantly improves energy efficiency and reduces latency compared to previous FPGA methods.\nThe STA is divided into STA-4 and STA-8 subcategories. STA-4 achieves 6.7 times better performance and is 10 times more energy-efficient than other models, with an acceleration rate of 392.8 GOPs and energy efficiency of 33.6 GOPs/W, using Nvidia RTX 2080Ti for comparison. STA-8, while slightly less performant with 4.4x better performance, offers 12.3x better energy efficiency, achieving an acceleration rate of 523.8 GOPs and energy efficiency of 41.2 GOPs/W."}, {"title": "I. FPGA OPU", "content": "In 2023, Bai et al. proposed another scheme for the acceleration of transformer networks called Overaly OPU [12]. They propose a configurable computation unit to support the inference of diverse networks. Specifically, they propose 48 processing elements (PEs) that are configured for the acceleration of the transformer networks. The output stage of the adder tree can be switched during the inference process. That way, data from forwarding modules can flow through the computation unit in a pre-defined connection state. The proposed scheme achieves 5x-15\u00d7 speedup compared with a CPU, 1.1-2.9x speedup compared with GPU (RTX 3090), and, 1.10-2.5x speedup compared with the other FPGA accelerators such as NPE [6]."}, {"title": "J. FPGA acceleration of Transformer networks", "content": "In 2022, Tzanos et al, presented a high-performance hardware accelerator for the transformer networks [13]. Transformer networks use a technique called attention. The attention, adopted by the field of neuroscience, is the ability to be able to selectively concentrate on specific data while ignoring other data of the environment. In deep learning we imitate this technique through attention mechanisms and one way to achieve this is to encode a sequence not into a single fixed vector but to create a model that produces a vector for each output step by adding a set of weights which will later be optimized.\nThe performance evaluation showed that the proposed framework can achieve 2.3x system speedup for the BERT model compared to a 40-thread processor and 80.5x speed-up over a single-core CPU."}, {"title": "K. FlexRun", "content": "In 2023, Hur at al. presented an FPGA-based accelerator to speedup the diverse and complex NLP models, called FlexRun [14]. The paper is focused on accelerating both Recurrent Neural Networks (RNNs) models such as SRNN or long short term memory (LSTM) and attention-based NLP models, such as Transformer, and GPT2.\nFor evaluation, they compare FlexRun with Intel's Brainwave-like architecture on a Stratix-10 GX FPGA and a Tesla V100 GPU with tensor cores enabled. Compared to the FPGA baseline, FlexRun achieves an average speedup of 1.59x on various configurations of BERT. For GPT2, FlexRun gets 1.31x average speedup. Next, when comparing to the GPU implementation, FlexRun improves the performance by 2.79\u00d7 and 2.59\u00d7 for BERT and GPT2, respectively."}, {"title": "L. HPTA", "content": "In 2023, Yuntao Han and Qiang Liu presented the High- Performance Transformer Accelerator (HPTA) [15], leveraging a custom multiplication matrix, adder tree, and memory subsystem. It can handle various types of transformers used in Natural Language Processing (NLP) and Computer Vision (CV). The performance of HPTA was evaluated against CPU, GPU, and other FPGA implementations. The results showed significant improvements in speed and energy efficiency for both BERT and Swin Transformer models. Compared to CPU and GPU, HPTA processed BERT up to 44x faster and 175x more energy-efficiently. It was also 1.8x faster than previous FPGA accelerators"}, {"title": "M. Swin", "content": "In 2023, Zhiyang Liu, Zhenhua Ren, and Pengyu Yin developed an accelerator for the Swin Transformer in computer vision tasks, addressing hardware acceleration challenges with large images [16]. The architecture includes computation units for GELU and Softmax, allowing Swin Transformer Block execution in one cycle and improving efficiency by replacing Layer Normalization (LN) with Batch Normalization (BN). It offers significant speed and energy efficiency improvements over CPU and GPU. The accelerator is categorized into Swin- T, Swin-S, and Swin-B. Swin-T is 1.8x faster and 20.5x more energy-efficient, Swin-S is 1.7x faster and 18.6x more energy- efficient, and Swin-B is 4.4x faster and 14.6x more energy- efficient compared to the Nvidia GeForce RTX 2080Ti. The acceleration rates are 431.2, 403.5, and 436.4 GOPs for Swin- T, Swin-B, and Swin-S, respectively."}, {"title": "N. Zhongyo Zhao", "content": "In 2023, Zhongyo Zhao presented an accelerator that uses an Output Block Storing (OBS) data handling method to efficiently execute transformer models for object recognition [17]. The proposed method involves dividing the inputs and allocating weights into small block matrices to reduce memory access for input data and weights. Additionally, the OBS data flow maintains usage rates by collecting partial sums, while slightly reducing them compared to the output block data flow. This results in improved overall energy efficiency. The accelerator implements this data flow and achieves a processing rate of 728.3 GOPs and an energy efficiency of 58.31 GOPs/W, surpassing previous CNN-based accelerators. This study used a Xilinx VC709 processor for comparison and employed Virtex\u2122\u2122 7VC707 (28nm) technology."}, {"title": "O. ODE-based acceleration", "content": "In 2024, a hybrid approach was proposed for the acceleration of the transformer networks by Okubo et al [18]. The proposed scheme uses ResNet as a backbone architecture and replaces a part of its convolution layers with an MHSA (Multi- Head Self-Attention) mechanism. Using this approach they manage to significantly reduce the parameter size of such models by using Neural ODE (Ordinary Differential Equation) as a backbone architecture instead of ResNet. The proposed hybrid model reduces the parameter size by 94.6% compared to the CNN-based ones without degrading the accuracy.\nThe performance evaluation on a Xilinx Zynq UltraScale+ MPSOC platform shows that the proposed FPGA implementation achieves 12.8x speedup and 9.2x energy efficiency compared to an ARM Cortex-A53 CPU implementation."}, {"title": "P. Beta", "content": "In 2024, Yuhao Ji presented a Binary Transformer Accelerator (BETA) that achieves high performance and flexibility [19]. This is accomplished through a computational flow subtraction method aimed at optimizing QMMs. The QMM is a programmable machine that can support a wide range of precision while providing high parallelism, speed, and energy efficiency. Various experiments compared BETA with previous FPGA accelerators, concluding that energy efficiency continuously improves. While performance speed compared to other CPUs and GPUs is not mentioned, the energy efficiency is reported to be 22x better. The study used the RTX3090 and ZCU102 (16nm) technology, with BETA achieving an acceleration rate and energy efficiency rate of 1436 GOPs and 174 GOPs/W, respectively."}, {"title": "Q. Me-Vit", "content": "In 2024, Kyle Marino, Pengmiao Zhang, and Viktor K. Prasanna introduced Me-ViT [20], a memory-efficient Vision Transformer design that outperforms traditional ViT accelerators on FPGA in speed and energy efficiency. Me-ViT combines Self-Attention and Multi-Layer Perceptron blocks, reducing data transfers and intermediate writes by loading weights only once. Its Memory-Efficient Processing Element (ME-PE) minimizes data movement and computation interruptions. Using systolic arrays for matrix multiplication, Me-ViT optimizes memory access, providing scalable, high-performance solutions for vision tasks on FPGA. Compared to CPUs and GPUs, Me-ViT is 5.1x faster and 4x more energy-efficient, achieving an acceleration rate of 2,682 GOPs. The study uses Nvidia TITAN RTX GPU and Alveo U200 (16nm) technology for comparison"}, {"title": "R. TransAxx", "content": "In 2024, Dimitrios Danopoulos, Georgios Zervakis, and Dimitrios Soudris introduced TransAxx [21], a framework aimed at enhancing the efficiency of Vision Transformer (ViT) models through approximation computing. It includes a PyTorch-based system that supports continuous approximation computing and assesses its effects on ViT models. The technique involves studying the sensitivity of transformers to approximate multipliers, fine-tuning for accuracy, and using the Monte Carlo Tree Search (MCTS) algorithm to create approximate accelerators. Key techniques for accuracy improvement include quantization, pre-calibration training, and adaptive retraining. The framework reduces computational complexity and memory demands while balancing speed and energy efficiency. TransAxx provides a comprehensive approach for optimizing ViT models, enabling professionals to improve performance with limited resources through methods like quantization, calibration, and retraining."}, {"title": "S. Ikumo Okubo", "content": "In 2024, Ikumi Okubo introduced a cost-effective FPGA implementation of the Tiny Transformer model utilizing a Neural Ordinary Differential Equation (Neural ODE) technique [22]. This method uses fewer parameters and less memory compared to ResNet-based deep models, making it suitable for resource-constrained devices. The model features ODEBlocks that reuse parameters, a learned relative positional encoding, and quantization to n-bit integers using LLTs. It also incorporates Depth-wise Separable Convolution (DSC) and Multi-Head Self-Attention (MHSA), forming a hybrid architecture. This approach is highly memory-efficient and significantly improves speed and energy efficiency, being 12.8x faster and 9.2x more energy-efficient than other models, and is compared to the ARM Cortex-A53 CPU using ZCU102 (16nm) technology."}, {"title": "T. SSR", "content": "In 2024, Jinming Zhuang presented SSR [23] as a unique architecture emphasizing the balance between latency and performance in accelerating transformers. It employs various elements such as FPGA and examines the trade-off between latency and performance for different models, achieving performance and energy efficiency increases. The method used is matrix multiplication, which controls the data communication between accelerators and seeks ways to improve performance. SSR provides open-source tools for reproducing results and can optimize communication between accelerators, reducing data transmission costs. Compared to other CPUs and GPUs, SSR is approximately 36x faster and 21x more energy-efficient than previous accelerators. This study utilizes the Nvidia A10G GPU and VCK190 (7nm) technology."}, {"title": "III. CPU AND GPU-BASED ACCELERATORS", "content": ""}, {"title": "A. TurboTransformer", "content": "In 2021, Jiarui Fang and Yang Yu introduced the Turbo- Transformers accelerator [24], a technique for efficiently serving Transformer models on GPUs for variable-length inputs. They addressed the challenges of padding smaller sequences to match the length of the longest sequence in a batch. By using dynamic programming to solve the optimization issue, they increased the response rate by 35 % compared to not using batching.\nTo reduce memory size, TurboTransformers introduces a variable-length allocator that employs a segment-based memory management technique and a space reuse mechanism in the computation graph, reducing memory usage by 50 per cent compared to a reference allocator. Testing the system with various Transformer models, including BERT and Albert, the authors found that TurboTransformers outperformed PyTorch and ONNXRuntime in latency and performance for variable- length inputs, being 2.8x faster"}, {"title": "B. Jaewan Choi", "content": "In 2022, researcher Jaewan Choi presented the study titled \"Accelerating Transformer Networks through Rewiring of Softmax Layers\" [25], which provides a method to accelerate the Softmax layer in transformer networks. The research introduces a rewiring technique to speed up the Softmax layer in transformer networks, which has become increasingly important as transformer models process longer sequences to improve accuracy rates. The proposed technique divides the Softmax layer into several sub-layers, changes the data access pattern, and then merges the disassembled Softmax sub-layers with the subsequent and preceding processes. This method accelerates the inference of BERT, GPT-Neo, BigBird, and Longformer on a current GPU by up to 1.25x, 1.12x, 1.57x, and 1.65x respectively, significantly reducing off-chip memory traffic."}, {"title": "C. SoftMax", "content": "In 2022, Choi et al. presented a novel framework for acceleration of transformer networks through Recomposing Softmax Layers [26]. The softmax layer normalizes the elements of the attention matrix to values between 0 and 1. This operation is conducted along the row vector of the attention matrix. Based on the profiling, the softmax layer in the scaled dot- product attention (SDA) block uses 36%, 18%, 40%, and 42% of the total execution time of BERT, GPT-Neo, BigBird, and Longformer, respectively.\nSoftmax recomposition achieves up to 1.25x, 1.12x, 1.57x, and 1.65x speedups in inferring BERT, GPT-Neo, BigBird, and Longformer on a A100 GPU by significantly reducing the off-chip memory traffic."}, {"title": "D. LightSeq2", "content": "In 2022, Wang et al. proposed a series of GPU optimizations to accelerate the training for a general family of Transformer models on GPUs called LightSeq2 [27].\nLightSeq2 proposes 3 techniques for the acceleration of the training of transformer networks. Firstly, to all types of transformers, LightSeq2 uses fused kernel operators for both encoder and decoder layers. Adjacent fine-grained element- wise kernels are fused into one coarse-grained kernel, resulting in fewer kernel launches and intermediate results. For example, the last kernel of the self-attention layer implements bias adding, dropout, and residual kernels with only one kernel launch.\nThe performance evaluation shows that LightSeq2 is consistently faster (1.4-3.5x) than previous systems on different GPUs and it can achieve up to 3x speedup on large public datasets."}, {"title": "E. Simplified Transformer Networks", "content": "In 2023, He and Hofmann [28] have also proposed a novel framework to accelerate transformer networks in GPUs by simplified transformers without compromising convergence properties and downstream task performance.\nBased on the performance evaluation both on autoregressive decoder-only and BERT encoder-only models, the simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput in GPUs, and using 15% fewer parameters."}, {"title": "F. LLMA", "content": "In 2023, Nan Yang introduced LLMA [29], an accelerator for large language models (LLMs) that enhances inference speed through interaction with reference data. This method uses a reference-based decoding mechanism to select and process tokens efficiently, enabling parallel execution on GPUs without needing new models. LLMA is easy to implement and deploy, providing over twice the speed for various model sizes using the Nvidia 32G V100 GPU."}, {"title": "G. FlexGen", "content": "In 2023, researchers introduced FlexGen, a high-throughput system for generating large language models (LLMs) designed for latency processing in resource-limited environments. Flex- Gen generates 32 tokens per prompt and evaluates throughput by the number of tokens generated divided by adaptation and decoding time. Compared to DeepSpeed ZeRO-Inference and Hugging Face Accelerate, FlexGen provides 40x more throughput with the same latency using an Nvidia T4 (16GB) GPU. Built on PyTorch, FlexGen utilizes multiple CUDA streams and CPU threads for I/O combination, significantly increasing performance through CPU computation and result overlapping."}, {"title": "H. VLLMs", "content": "In 2023, researchers introduced the vLLMs model to address efficient memory management for large language models (LLMs), which have high memory requirements [30]. They proposed a strategy called PagedAttention, which divides key- value attention into fixed-size blocks and uses paging to maintain them. This approach enhances memory efficiency and reduces the memory footprint of LLMs. The vLLM architecture leverages PagedAttention to manage memory effectively, particularly in beam search scenarios with a fixed number of candidates. The model supports mixed decoding approaches with various sharing and memory access patterns, using a mapping layer to convert logical blocks to physical blocks, further optimizing memory usage and reducing the overall memory footprint of LLMs."}, {"title": "I. Alisa", "content": "In 2024, researchers introduced the ALISA model [31], aimed at accelerating large language models (LLMs) through sparse window attention (SWA) and dynamic scheduling. This approach addresses the limitations of existing optimizations in maintaining competitive accuracy. SWA creates sparse patterns that are both locally static and globally dynamic, preserving the sequential semantics of language while capturing its dynamic evolution. Dynamic scheduling further enhances performance by balancing memory access and token processing. By integrating SWA, dynamic scheduling, and KV compression, ALISA significantly reduces the memory footprint of KV stores. The study demonstrates that ALISA outperforms previous methods in accuracy and performance, with comparisons across three families of open-source LLM models."}, {"title": "IV. ASIC ACCELERATORS", "content": ""}, {"title": "A. A3", "content": "One of the early research on the acceleration of transformer networks was proposed in 2020 by Hma et al. called A3 [32]. The paper proposes a hardware accelerator for attention mechanisms in NNs, that not only focused on the efficient implementation of the attention mechanism in hardware but also on reducing the amount of computation in attention mechanism through algorithmic optimization and approximation. It presents an approximate candidate selection mechanism to reduce the number of search targets, and thus the amount of computation.\nThe proposed scheme has not been implemented on FPGA but is has been implemented on a cycle-accurate Verilog design targeting a TSMC 40nm ASIC clocked at 1GHz. Based on the performance evaluation, the proposed scheme can achieve up to 7x speedup compared to a Intel Gold 6128 CPU implementation and up to 11x better energy efficiency against versus a CPU implementation."}, {"title": "B. ELSA", "content": "In 2021, Ham et al. presented a hardware-software Co- design approach for the acceleration of transformer networks called Elsa [33].\nBased on the fact that irrelevant relations can be effectively filtered out by computing approximate similarity, ELSA substantially reduces computational waste in a selfattention operation. Unlike conventional hardware such as CPU or GPUs, that cannot benefit from approximation, ELSA propose a specialized hardware that directly translates this reduction to further improve performance and energy efficiency.\nThey evaluate several representative self-attention-oriented NN models to demonstrate the effectiveness of the ELSA. For performance evaluation, they implemented a custom simulator for ELSA targeting a 40nm ASIC clocked at 1GHz. ELSA- moderate achieves up to 157x speedup compared to GPUs and two orders of magnitude improvements in energy efficiency over the GPU for the self-attention computation."}, {"title": "C. SpAtten", "content": "In 2021, Want et al. presented a framework for the acceleration for large language models called Spatten. [34]\nSpAtten propose a novel scheme for the acceleration of NLP using three algorithmic optimizations: cascade token pruning, cascade head pruning and progressive quantization to reduce computation and memory access.\nThe proposed scheme has been implemented on a cycle- accurate design using SpinalHDL and mapped to ASIC using a 40nm TSMC library. SpAtten and achieves 162x, and 347x speedup over a GPU (TITAN Xp), and a Xeon CPU, respectively. In terms of energy efficiency SpAtten achieves 1193x and 4059x energy savings compared to GPU and CPU."}, {"title": "D. Sanger", "content": "Lu at el. presented in 2021 another novel approach for the acceleration of transformer networks called Sanger [35].\nSanger accelerates the sparse attention models by combining dynamic sparsity patterns and reconfigurable architecture. The software part provides sparsity patterns, which can achieve high performance and a balanced workload. The architecture is designed with reconfigurability to support the dynamic characteristics of sparsity, which helps to improve the compression ratio.\nTo allow more flexibility in the sparsity patterns, Sanger proposed a reconfigurable systolic array based on this dataflow. Sanger was implemented in Chisel hardware that was translated to Verilog RTL. The design was targeting an ASIC using the UMC 55nm technology clocked at 500MHz."}, {"title": "E. SALO", "content": "In 2022, Guan Shen presents a spatial accelerator to improve transformer performance for long sequences by using hybrid sparse attention patterns [36]. This model addresses computational and memory challenges, achieving 25.5x faster performance and 336.1x better energy efficiency compared to a Nvidia GTX 1080Ti with FreePDK (45nm) technology. It demonstrates enhanced attention processing through efficient hardware and data strategies"}, {"title": "F. AccelTran", "content": "In 2020, Shikhar Tuli introduced AccelTran [37], an accelerator architecture to improve transformer model efficiency in natural language processing. AccelTran uses matrix compression and data flow strategies to enhance energy efficiency. It has two versions: AccelTran-Edge for low-power portable devices and AccelTran-Server for high-throughput cloud applications. AccelTran-Edge outperforms Raspberry Pi in throughput and power consumption, while AccelTran- Server offers 5.7x higher throughput and 3.7x lower energy use compared to the Energon model. The acceleration rates are 372,000 GOPs for AccelTran-Server and 7,520 GOPs for AccelTran-Edge"}, {"title": "G. DTQAtten", "content": "In 2022, Tao Yang introduced DTQAtten [38"}]}