{"title": "LarvSeg: Exploring Image Classification Data For Large Vocabulary Semantic Segmentation via Category-wise Attentive Classifier*", "authors": ["Haojun Yu", "Di Dai", "Ziwei Zhao", "Di He", "Han Hu", "Liwei Wang"], "abstract": "Scaling up the vocabulary of semantic segmentation models is extremely challenging because annotating large-scale mask labels is labour-intensive and time-consuming. Recently, language-guided segmentation models have been proposed to address this challenge. However, their performance drops significantly when applied to out-of-distribution categories. In this paper, we propose a new large vocabulary semantic segmentation framework, called LarvSeg. Different from previous works, LarvSeg leverages image classification data to scale the vocabulary of semantic segmentation models as large-vocabulary classification datasets usually contain balanced categories and are much easier to obtain. However, for classification tasks, the category is image-level, while for segmentation we need to predict the label at pixel level. To address this issue, we first propose a general baseline framework to incorporate image-level supervision into the training process of a pixel-level segmentation model, making the trained network perform semantic segmentation on newly introduced categories in the classification data. We then observe that a model trained on segmentation data can group pixel features of categories beyond the training vocabulary. Inspired by this finding, we design a category-wise attentive classifier to apply supervision to the precise regions of corresponding categories to improve the model performance. Extensive experiments demonstrate that LarvSeg significantly improves the large vocabulary semantic segmentation performance, especially in the categories without mask labels. For the first time, we provide a 21K-category semantic segmentation model with the help of ImageNet21K. The code is available at https://github.com/HaojunYu1998/LarvSeg.", "sections": [{"title": "1 Introduction", "content": "Semantic segmentation is a fundamental visual task that aims to assign a semantic class to each pixel in the image. Recently, deep learning-based methods have achieved great success with a supervised learning paradigm for semantic segmentation [15,26,17,22,5,6,1,28]. In real applications, the segmentation tasks often focus on a large set of given categories (large vocabulary). For example, the segmentation of various vehicles is a necessary module for autonomous driving. However, obtaining pixel-level mask labels is labour-intensive and time-consuming: annotating one accurate mask of an object typically requires 54 to 79 seconds for a well-skilled annotator [2]. Thus the vocabulary of widely used semantic segmentation datasets [29,3,29,6] is limited, only comprising a few hundreds of categories. Segmentation models trained on these datasets fail to recognize the given categories that are out of the training distribution (OOD). Recently, a new paradigm called language-guided segmentation [30,14,27,10,13] has been proposed to address this challenge. These works aim to extend the segmentation vocabulary by leveraging language semantics. For example, models trained on \"cat\" can correctly segment the unseen category \"furry\" because they are semantically similar in language space. Ideally, this paradigm can perform segmentation on any category (open vocabulary). However, the model performance significantly drops when facing OOD text prompts(see Table 3). The key reason is that vision encoders has never received the supervision of semantics similar to OOD text prompts, thus visual features are not aligned well with the OOD text features in the language space. In this paper, we address the challenge of large vocabulary semantic segmentation with a novel framework, LarvSeg. The key idea is to introduce image"}, {"title": "3 Method", "content": ""}, {"title": "3.1 A Simple Baseline", "content": ""}, {"title": "Incorporating Image-Level Supervision", "content": "Our basic segmenter contains a backbone network (e.g. ViT-B/16 [9]) followed by a one-layer cosine classifier to predict pixel-wise categories. To incorporate image-level supervision into the training process, we conduct an image-level classification of the average pooled feature map. For multi-class images, the cross entropy loss is calculated separately for each category in the image. To avoid conflicts among different categories, we set other categories in the image as ignore labels, which indicates these categories are not involved in the actual calculation."}, {"title": "Joint Training", "content": "To enable the segmentation model to recognize novel categories, we jointly train the network with segmentation and classification data. The model parameters are shared for both tasks. The overall training objective is:\n\\(L = Lseg + AclsLcls,\\) \nwhere the loss weight Acls is a hyperparameter which is set as 0.1 by default."}, {"title": "Inference", "content": "During inference, the predicted categories for each pixel are directly obtained as the original segmentation model. Since the model parameters are shared for both tasks, the segmenter naturally possesses the ability to segment novel categories that appeared in image classification data."}, {"title": "3.2 Analysis For Novel Category Pixel Grouping", "content": "Despite the good performance, the proposed baseline is faced with potential limitations. The probability vector Pels of the image is obtained through global average pooling, indicating that the classification supervision has not been applied to the corresponding pixels accurately. To alleviate this problem, we intend to extract foreground regions of novel categories and apply fine-grained supervision to them. For precise region extraction without pixel-level mask labels, intra-category pixel compactness is essential. Thus, we train a segmentation model with base categories and investigate the pixel feature similarity of novel categories."}, {"title": "Exploratory Experiment", "content": "This experiment is designed to measure the intra-category pixel compactness in a category-agnostic way. Specifically, we train a"}, {"title": "3.3 Category-wise Attentive Classifier", "content": "Based on the above observations, we propose a Category-wise Attentive Classifier (CA-Classifier) to explicitly distinguish different pixel groups and extract the corresponding regions of novel categories to apply fine-grained supervision. We will elaborate on the details in the following."}, {"title": "Memory Based Category Representation", "content": "To explicitly distinguish the pixel groups in a category-aware way, we employ cross-image semantic cues. During the training process, we maintain a memory bank to store representative features of each novel category M = {M\u00bf|Vi \u2208 novel category}. In each training step, we update the memory bank using pixel features with the top K classification"}, {"title": "Category-wise Attention Map", "content": "The category-wise attention map is generated by foreground strengthening and background suppression. For the foreground category, we define a category-wise attention map A as the rescaled confidence score map using a sigmoid function:\n\\(A = sigmoid(norm(Sigem - Shem))\\)\nwhere fg and bg represent foreground and background category in the given image. Then, we use the category-wise attention map to attentively pool the feature map and apply an auxiliary image-level classification task to the attentively pooled feature map as in Section 3.1:\n\\(P\u00b2 = softmax(S\u00bfls \u00b7 A/T), Vi \u2208 novel category\\)\nwhere Scls is original classification score map. More details can be found in Supplementary Material. In this way, the foreground regions in the category-wise attention map will receive more attention as shown in Figure 3, while the attention scores in background regions are close to zero. Thus, the image-level supervision will be accurately applied to the foreground pixel group."}, {"title": "Learning Objective", "content": "The overall loss function consists of the semantic segmentation loss Lseg, the image classification loss Lels and the auxiliary classification loss Laux with category-wise attentive classifier:\n\\(L = Lseg + AclsLcls + dauxLaux,\\)\nwhere the loss weights Acls and Aaux are hyperparameters which are both set as 0.1 by default. The overall framework is presented in Figure 3."}, {"title": "3.4 Segmentation For 21K Classes", "content": "To further tap the potential of the proposed method, we show that it can extend the vocabulary to 21K categories. Specifically, we employ ImageNet21K [7] as the classification dataset to train LarvSeg. The qualitative results are presented in Section 4.6. More details and results can be found in Supplementary Material."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "In this subsection, we will introduce the datasets used in this paper. The summary of all datasets is shown in Table 2."}, {"title": "4.2 Experimental Setups", "content": ""}, {"title": "Weak-ADE As Classification Data", "content": "We extend the vocabulary of C171 with the multi-label classification dataset WA150 (WA847) and evaluate the"}, {"title": "ImageNet and Weak-ADE As Classification Data", "content": "We further add 1124 (1585) to the above settings and evaluate the models on A150 (A847). The single-label ImageNet dataset is adequate and category-balanced compared to WA150 and WA847."}, {"title": "21K-category Segmentation", "content": "We extend the vocabulary of semantic segmentation models to 21K categories with dataset I21K. In Supplementary Material, we provide qualitative results to demonstrate the model's ability to segment novel categories."}, {"title": "4.3 Implementation details", "content": "Overall Framework We use ViT-B/16 [9] initialized with I21K pre-trained weights as the backbone by default. All the models are trained on 8 V100 GPUs. Following Detic [31], we group images from the same dataset on the same GPU to improve training efficiency. For all experiments, we sample two images in a mini-batch for each GPU. For setups (1) and (3), we use 4: 4 GPUs for segmentation and classification data. For setup (2), we use 3:3:2 GPUs for C171, WA847 (WA150) and 1585 (1124). Training Parameters Models for the main results and ablation study are trained for 320K iterations. For the 21K categories segmentation, we train the model for 1280K iterations. Images from C171, WA150 and WA847 are resized to 512 x 512 and images from I21K, I124 and 1585 are resized to 320 \u00d7 320. We apply multi-scale jittering with a random scale between [0.5, 2.0], random crop, random flip and photometric distortion as data augmentations. SGD is adopted as the optimizer with a base learning rate of 0.001, momentum 0.9 and no weight decay. We use the polynomial learning rate schedule with power 0.9 and minimum learning rate le\u00af5. We empirically set the loss weights Acls = 0.1 and Aaux = 0.1. All results are evaluated without multi-scale tests."}, {"title": "4.4 Main Results", "content": ""}, {"title": "Weak-ADE As Classification Data.", "content": "In Table 3, we compare our new paradigm with state-of-the-art results in language-guided segmentation paradigm [30,21,14,27,10]. Our simple baseline outperforms OpenSeg[10] by a large margin. Note that OpenSeg is initialized with ALIGN [12] weights (about 800M image-text pairs) and trained on C171 and Localized Narratives [18] (about 652K image-text pairs). What's more, LarvSeg can bring a significant improvement to the baseline model (2.1 mIoU in A847 novel categories; 6.0 mIoU in A150 novel categories;). This result indicates the effectiveness of the proposed category-wise attentive classifier."}, {"title": "ImageNet and Weak-ADE As Classification Data.", "content": "In Table 3, we additionally include single-label classification data I124 (1585) in the training set. The simple baseline significantly improves the performance (3.3 mIoU in A847 novel categories; 4.5 mIoU in A150 novel categories). When adding the CA-Classifier,"}, {"title": "4.5 Ablation Study", "content": ""}, {"title": "Effectiveness of CA-Classifier.", "content": "We ablate the effectiveness of the CA-Classifier in the single- and multi-label classification data setting. As shown in Table 4, the CA-Classifier improves the performance when applied to each type of classification data. The best performance is achieved when using a CA-Classifier on both types of classification data."}, {"title": "Design of CA-Classifier.", "content": "Another possible design is to simply select the high-score pixels in the classification score map as foreground regions. Specifically, we dynamically select foreground regions for a category and apply cross-entropy loss to the foreground regions. As shown in Table 5, our CA-Classifier outperforms the single-image design by 2.0 mIoU on novel categories. The reason is that c sometimes incorrectly locates the frequently co-occurred background categories."}, {"title": "Ablation on Memory Bank.", "content": "We ablate different memory sizes M with the fixed Top-K region area 40 in the left part of Table 6. LarvSeg has the highest mIoU on novel categories when M = 20. Then, we ablate different top-K with the fixed memory bank size 20 in the right part of Table 6. We select K = 20 for the best novel category performance."}, {"title": "4.6 Qualitative Results", "content": ""}, {"title": "Segmentation on A150.", "content": "In Figure 4, we visualize the predictions of different models and highlight the appeared novel categories with circles. The sofa is recognized by all models, the radiator is recognized by ReCo and LarvSeg, and the painting is only recognized by LarvSeg."}, {"title": "Segmentation For 21K Categories.", "content": "In Figure 5, we provide qualitative results of LarvSeg trained with C171 and 121K to show that our framework can extend semantic segmentation to 21K categories. We only visualize the predicted foreground masks and set all background predictions to grey. Figure 5 shows that the proposed LarvSeg is capable of segmenting fine-grained categories like polyphemus moth, which demonstrates the superiority of the proposed framework. More visualizations and details are provided in Supplementary Material."}, {"title": "5 Conclusion", "content": "In this paper, we address large vocabulary semantic segmentation using image classification data. We design a framework called LarvSeg to bridge the gap between image-level and pixel-level labels effectively. Firstly, We construct a simple baseline to incorporate image-level supervision which performs better than state-of-the-art language-guided semantic segmentation models. Then, we observe that a model trained on segmentation data can group the pixels of unseen categories as well. Based on this observation, we propose a category-wise attentive classifier to apply image-level supervision on the corresponding regions. Extensive experiments show that the proposed LarvSeg framework significantly outperforms the baseline model. For the first time, we provide a semantic segmentation model that can recognize 21K categories. We hope this new paradigm and the LarvSeg framework could be a strong baseline for large vocabulary semantic segmentation and facilitate future research."}]}