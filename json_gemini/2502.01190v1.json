{"title": "DANCE RECALIBRATION FOR DANCE COHERENCY WITH\nRECURRENT CONVOLUTION BLOCK", "authors": ["Seungho Eum", "Ihjoon Cho", "Junghyeon Kim"], "abstract": "With the recent advancements in generative AI such as GAN, Diffusion, and VAE, the use of gen-\nerative AI for dance generation has seen significant progress and received considerable interest.\nIn this study, We propose R-Lodge, an enhanced version of Lodge. R-Lodge incorporates Recur-\nrent Sequential Representation Learning named Dance Recalibration to original coarse-to-fine long\ndance generation model. R-Lodge utilizes Dance Recalibration method using N Dance Recalibra-\ntion Block to address the lack of consistency in the coarse dance representation of the Lodge model.\nBy utilizing this method, each generated dance motion incorporates a bit of information from the pre-\nvious dance motions. We evaluate R-Lodge on FineDance dataset and the results show that R-Lodge\nenhances the consistency of the whole generated dance motions.", "sections": [{"title": "Introduction", "content": "Recently, due to the increase in data and improvement in computing power, generative AI technology has experienced\nrapid growth, leading to significant advancements in dance generation AI as well. Generating novel dances from\neffective generative model not only can enhance the diversity in visual arts but also reduce dance creating costs and\nmake the process more efficient. Game animators, movie directors, and etc. can save time and help create more\nimmersive and realistic animations.\nPrevious works [17, 27, 31, 40] can generate short dances which is not proper in actual applications that requires\ndances longer than a few seconds. However, actual applications generally require dances lasting 3 to 5 minutes, and\nsome performances even require dances lasting several hours. Therefore, there is now a demand for the generation of\nlonger dance sequences. To generate long dance, dealing with the high computational costs is needed. Various methods\nsuch as auto-regressive model [16, 22, 41, 36] try to deal with it but they are failed due to their lack of mode coverage.\nBuilding on the exceptional generation performance of diffusion models, [35] and [20] propose music-conditioned\ndance generation models based on diffusion. [35] introduced a transformer-based diffusion network for long sequences\ndance generation. They are able to generate arbitrarily long sequences by chaining the shorter clips with local consis-\ntency but lack an extreme-long-term dependencies. Additionally, [35] proposed a novel Contact Consistency Loss and\nPhysical Foot Contact Score to eliminate foot sliding physical implausibilities but still such problems are observed in\ngenerated frames.\n[20] attempts to address Edge's challenge regarding the consistency of combined long dance sequences. [20] propose\na coarse-to-fine two stage diffusion framework and characteristic dance primitives to produce long dances in a parallel\nmanner. The first stage gets long music input to generate dance primitives which is coarse-grained diffusion. And\nthe parallel local diffusion modules follow to generate short dance segments finally concatenated into long dance\nsequences. This two-stage coarse-to-fine diffusion framework strikes a between overall choreographic patterns and\nthe quality of short-term local movements. [20] also proposes a method to overcome the foot-sliding problem, termed\nthe foot refine block to eliminate artifacts."}, {"title": "Related Works", "content": "Human motion generation is one of the most interested field in computer vision studies. Due to the high level of\ninterest, it has witnessed remarkable advances through many researches. Despite this significant growth, human motion\ngeneration remains a challenging problem, often plagued by issues such as foot-sliding and a lack of consistency or\nsmoothness due to abrupt motion changes.\nRoughly human motion generation is based on either regression models or generative models. In recent years, genera-\ntive models have become the dominant approach. For instance, Kinetic-GAN [7] utilizes a GAN combined with GCN.\nIt leverages latent space disentanglement to separate different aspects of motion, facilitating easier manipulation and\ndiverse generation. Furthermore, stochastic variations help produce a wider range of realistic motions.\nMDM [34] and MLD [6], for example, utilize diffusion model to generate desired motions. [2, 3, 11, 28, 5] are among\nthe studies to generate human motion sequences from text descriptions. Since above methods can face challenges\nin zero-shot generation, [33, 14] employs CLIP, a pre-trained vision-language model, for better zero-shot generation\nperformance.\nVQ-VAE is another popular method for generating human motion. For instance, T2MT [12] uses VQ-VAE for training\ntext-to-motion and motion-to-text tasks. T2M-GPT [39] leverages a GPT-like transformer architecture and VQ-VAE\ncombined with an EMA."}, {"title": "Music To Dance Generation", "content": "Dance and music are inseparable that music provides the foundation for the movement and emotion of dance while\nwe express the characteristic of music by dance. Accordingly, abundant studies make a goal to generate quality dance\nconditioned on music.\nThe generation of dance sequences driven by music has been an active area of research, focusing on synchronizing\ndance movements with musical inputs. Traditional motion-graph methods approached this as a similarity-based re-\ntrieval problem, limiting the diversity and creativity of the generated dance sequences. However, recent advancements\nin deep learning have led to more aesthetically appealing results.\nSequence-based methods using LSTM and Transformer networks predict subsequent dance frames in an autoregressive\nmanner. For example, FACT [22] inputs music and seed motions into a Transformer network to generate new dance\nframes frame by frame. However, challenges such as error accumulation and motion freezing persist. VQ-VAE is\nanother well-known approach, as seen in methods like Bailando [31], which incorporates reinforcement learning to\noptimize rhythm and maintain high motion quality. However, the pre-trained codebook in VQ-VAE can limit diversity\nand hinder generalization.\nGAN-based methods like MNET [18] employ adversarial training to produce realistic dance clips and achieve genre\ncontrol, though they often face issues like mode collapse and training instability. Diffusion-based methods have\nshown significant progress in generating high-quality dance clips. For instance, EDGE [35] uses diffusion inpainting\nto generate consistent dance segments, while FineDance [21] introduces diffusion models to produce diverse and\nhigh-quality dance sequences.\nLodge [20] represents a notable advancement in this domain, employing a coarse-to-fine diffusion framework to gen-\nerate extremely long dance sequences. By introducing characteristic dance primitives, Lodge ensures both global\nchoreographic patterns and local motion quality. Additionally, the foot refine block in Lodge addresses artifacts such"}, {"title": "Generation for Sequential Data", "content": "The processing of sequential data based on deep learning initially started with Recurrent Neural Networks [30]. [30]\nshowed good performance at the time by utilizing information from previous sequences. However, [30] had the\ngradient vanishing problem. To overcome this limitation, models such as [29] and [8] were introduced, which use\ngates to enable learning the amount of information from previous sequences. Although these models are complex,\nthey are considered good research contributions as they solve the gradient vanishing problem.\nAdditionally, with the advancement of Transformers, the performance in processing sequential data has improved\nfurther. Particularly in vision-related tasks involving sequential data, such as [10, 37, 38, 15, 25] and [4, 24, 9, 23],\nincorporating Transformers has yielded good performance. However, Transformers require high computational costs\nand have significant computational demands. Although methods like [1, 19] are used to alleviate these issues, they\nstill demand expensive computational resources.\nWe discovered that in the Lodge model, which generates coarse dances using [13, 32], the generated dances do not\nsequentially connect. To address this issue, we conducted research to utilize the dance information from previous\nframes in order to create coarse dance information that includes sequential frame data."}, {"title": "Methodology", "content": "We applied Recurrency Sequence Processing to address the lack of consistency in the coarse dance representation of\nthe [20] model. We named this Recurrency Sequence Representation Learning as Dance Recalibration (DR). Dance\nrecalibration uses n Dance Recalibration Blocks (DRB) corresponding to the length of the rough dance sequence to\nadd sequential information to the rough dance representation to improve the consistency of the whole dance. The\noverall structure of our model is illustrated in Figure 1."}, {"title": "Dance Recalibration (DR)", "content": "When the dance motion representation passes through the Dance Decoder Process using the [20] model, it yields a\ncoarse dance motion representation. During this process, the dance motion representations that pass through Global\nDiffusion follow a distribution but can output unstable values. This results in awkward dance motions when viewed\nfrom a sequential perspective. To address this issue, we added a Dance Recalibration Process."}, {"title": "Pooling Block", "content": "Pooling P uses a simple pooling method. When $GRD_{i}$ with added G and $GDi+1$ are input, they are concatenated\ninto a (Batch \u00d7 2 \u00d7 126). First, we perform Layer Normalization to minimize differences between layers. Then, we\npass through three simple 1D-Convolution Blocks, each followed by an activation function and batch normalization, to\nconstruct $GRD_{i+1}$ that includes information from the previous dance motion. This procedure is illustrated in Figure\n4.\nBy following all these steps, each dance motion incorporates a bit of information from the previous dance motions, pro-\nducing an overall coarse dance motion that follows the distribution of Global Diffusion while also retaining sequential\ninformation. This process is expressed in Equation 2:"}, {"title": null, "content": "$\nTotalCoarse DanceMotion = C_1(P(C(GD_i, GRD_{i-1} + G(Threshold))), P(GD_O))\n$"}, {"title": null, "content": "We did not use bi-directional information because it complicates the calculations and can destabilize sequential infor-\nmation when using more than two $GD_{i}$. Since there is a trade-off between generating complex dance motions and"}, {"title": "Experiments", "content": "Datasets We use public music-dance paired dataset for music-driven dance generation, FineDance [21], which in-\ncludes 22 dance genres. FineDance claims to contain 14.6 hours of music-dance paired data with detailed hand motions\nand accurate postures. However, it actually has 7.7 hours, totaling 831,600 frames at a frame rate of 30fps, and in-\ncludes 16 different dance genres. Our entire model is trained and tested on this dataset. We create dance sequences,\neach comprising 1024 frames long dance sequences (equivalent to 34.13 seconds), using music from the test set of the\nFineDance dataset as input.\nImplementation details We follow the experimental settings of Lodge for the global music feature, the local music\nfeature and the generation of characteristic dance primitives. The global music feature extracted from the music and\naudio analysis library Librosa [26] has a length of 1024 (34.13 seconds), and the local music feature input for the\nLocal Diffusion has a length of 256 (8.53 seconds). As output of the global diffusion, we obtain 13 characteristic\ndance primitives which consisting of 5 coarse dance motions and 8 fine dance motions. These 8 fine dance motions are\nthen used as input for choreography augmentation augment to 16 fine dance motions by mirroring and aligned with\nthe input music's beat."}, {"title": "Comparison on the FineDance dataset", "content": "In this section, we compare our method with the several past works including Lodge. FACT [22] and MNET [18]are\nauto-regressive dance generation methods. Bailando [31] is a follow-up approach that employs VQ-VAE to trans-\nform dance movements into tokens and achieve outstanding qualitative performance. EDGE represents a significant\nadvancement in the field of dance generation by utilizing diffusion models to achieve substantial performance improve-\nments. Specifically, EDGE introduces a transformer-based diffusion model paired with Jukebox, conferring powerful\nediting capabilities and setting a new vision in generating realistic and physically plausible dance motions. Lodge\nis a network designed to generate extremely long dance sequences conditioned on music. It employs a two-stage\ncoarse-to-fine diffusion architecture with characteristic dance primitives as intermediate representations between two\ndiffusion models. It significantly outperforms existing models in generating coherent, high-quality, and expressive\ndance sequences."}, {"title": "Motion Quality & Diversity", "content": "Motion quality is assessed using two primary measures. The Frechet Inception Dis-\ntance (FID) measures the distance between the generated dance motion features and the ground truth dance sequences,\nindicating the quality of the motion. Separate FID scores are reported for kinematic features (FIDk) and geometric\nfeatures (FIDg). Additionally, the Foot Skating Ratio (FSR) calculates the proportion of frames where the feet slide\non the ground instead of making solid contact, with lower values indicating better physical realism. Motion diversity is\nevaluated through the diversity in kinematic features (Divk), which assesses the variety in the generated dance motions\nbased on kinematic properties such as speed and acceleration. The diversity in geometric features (Divg) evaluates the\ndiversity of generated dance movements by examining geometric properties and predefined movement templates."}, {"title": "Beat Aligment Score (BAS)", "content": "The Beat Alignment Score (BAS) measures how well the generated dance sequences\nalign with the beats of the accompanying music, reflecting the synchronization between music and dance."}, {"title": "Production Efficiency", "content": "Production efficiency is measured by the average run time required to generate a specific\nlength of dance sequence, such as 1024 frames, to evaluate the computational efficiency of the model. All experiments\nwere conducted on the same computer equipped with 8 Nvidia A100 GPUs."}, {"title": "Conclusion and Future Work", "content": "In this work, we propose R-Lodge enhanced version of Lodge which incorporates Dance Recalibration\nUsing Recurrency Block. Proposed method address the burst changes between adjacent coarse dance motions from\noriginal Lodge which interfere the smoothness in generated dance motion. Through the evaluation of our generated\nsamples, R-Lodge demonstrates smoother dance motions than Lodge while maintaining a comparable level of overall"}]}