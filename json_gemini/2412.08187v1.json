{"title": "From Communities to Interpretable Network and Word Embedding: an Unified Approach", "authors": ["THIBAULT PROUTEAU", "NICOLAS DUGU\u00c9", "SIMON GUILLOT"], "abstract": "Modeling information from complex systems such as humans social interaction or words co-occurrences\nin our languages can help to understand how these systems are organized and function. Such systems\ncan be modeled by networks, and network theory provides a useful set of methods to analyze them.\nAmong these methods, graph embedding is a powerful tool to summarize the interactions and topol-\nogy of a network in a vectorized feature space. When used in input of machine learning algorithms,\nembedding vectors help with common graph problems such as link prediction, graph matching, etc. In\nNatural Language Processing (NLP), such a vectorization process is also employed. Word embedding\nhas the goal of representing the sense of words, extracting it from large text corpora. Despite differences\nin the structure of information in input of embedding algorithms, many graph embedding approaches\nare adapted and inspired from methods in NLP. Limits of these methods are observed in both domains.\nMost of these methods require long and resource greedy training. Another downside to most methods is\nthat they are black-box, from which understanding how the information is structured is rather complex.\nInterpretability of a model allows understanding how the vector space is structured without the need for\nexternal information, and thus can be audited more easily. With both these limitations in mind, we pro-\npose a novel framework to efficiently embed network vertices in an interpretable vector space. Our Lower\nDimension Bipartite Framework (LDBGF) leverages the bipartite projection of a network using cliques\nto reduce dimensionality. Along with LDBGF, we introduce two implementations of this framework that\nrely on communities instead of cliques: SINr-NR and SINr-MF. We show that SINr-MF can perform\nwell on classical graphs and SINr-NR can produce high-quality graph and word embeddings that are\ninterpretable and stable across runs.", "sections": [{"title": "1. Introduction", "content": "Network science provides versatile tools to model the organization of real-world systems. Instances of\nreal-world systems that can be modeled with graphs are varied. Such systems include biological net-\nworks modeling how protein interact in a living organism, social interactions on online social networks\nor in workplaces, geographical organization with road and railroad networks, the organization of an\nonline encyclopedia with connection between its pages, scientific publication systems with collabora-\ntion networks, etc. Network science and graphs provide a general framework to represent interactions"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1 Embedding Methods", "content": "WORD EMBEDDINGS. Graph embedding methods have been heavily influenced by algorithms devel-\noped in NLP [66]. Word embedding aims at uncovering a latent vector space in which to project\nwords, providing a more synthetic representation of a word than a co-occurrence matrix (with a lower\ndimension) while encompassing semantic information. These representations are then used as input by\nclassifiers, mostly neural architectures, to solve various tasks such as named entity recognition, part-of-\nspeech-tagging, sentiment analysis or machine translation.\nThe first approaches to train word embeddings were the matrix factorization based. Those are\ndirectly connected to the distributional hypothesis: they factorize the word co-occurrence matrix. The\nliterature usually defines co-occurrences inside a corpus using a sliding window parameterized by a\ncertain size. All words within the window are said to co-occur. In the case of NNSE [55], the co-\noccurrence matrix is factorized using sparse coding to enforce interpretability. For Glove [65], the log\nco-occurrence is factorized, and the loss function is weighted by the co-occurrence frequency. Levy et\nal. applied Singular Value Decomposition (SVD) to word co-occurrence matrices after having applied\nPositive Mutual Information on it to consider the significance of these co-occurrences [48]. These meth-\nods perform well on similarity and analogy benchmarks and have been popular representations as input\nto a wide range of machine learning systems such as classifiers or translation systems.\nWith Word2vec, Mikolov et al. [53] consider a different approach: the task of training word embed-\nding is seen as self-supervised. Indeed, they train a logistic regression on the dot product of word\nembeddings: the sigmoid is supposed to be close to 1 when words co-occur, and to 0 when they do\nnot. This method has drawn a lot of attention and was extended to improve its robustness. For instance,\nfastText is based on sub-words and allows extracting word embeddings for words that have not been\nencountered in the training corpus [8]. These methods lack flexibility to represent polysemous words,\nsince they provide a single vector per type, Tian et al. [86] introduced a multi-prototype approach\nproviding one vector per sense. Such multi-prototype approaches paved the way for contextualized\nrepresentations that provide one vector per occurrence.\nLanguage models such as Devlin et al.'s BERT [22] with its self-supervised transformer architecture\nhave gained in popularity since they provide such contextualized representations. Transformers archi-\ntectures implement self-attention mechanisms, where words with similar representations in the same\nsequence are considered to attend each other. The representation of a word is then a mixture of its\nembedding and of the embeddings of the words attending to it, allowing its contextualization. Training\nsuch architectures is usually performed by predicting masked words from their contexts, or predicting\nthe next sentence. Subsequent transformer-based models adapted from BERT include ROBERTA [50]\nwhich alters key hyperparameters and training objectives, such as removing next-sentence prediction;\nboth T5 [71] and GPT-3 [12, 57] use large transformer architectures to build what is now named Large\nLanguage Models (LLM).\nNODE EMBEDDING. Node embedding approaches try to embed the local neighborhood of a node and\nnodes with similar roles in the graph closely together. They are frequently evaluated on classical tasks\nsuch as: link prediction, node classification or graph reconstruction. Earliest approaches to network\nembeddings applied dimension reduction techniques such as IsoMap with multidimensional scaling\n(MDS) [85], LLE [74] or Laplacian Eigenmap [4]. Then, graph embedding fed from advances\nmade to methods in NLP. Word2vec has influenced the field, leading to methods that adopt random-\nwalk sampling strategies with the Skip-Gram model, namely Deepwalk [66], Walklets [67] and"}, {"title": "2.2 Model-Intrinsic Interpretability", "content": "As stated by Rudin et al. [77], interpretability of a system is often defined by opposing it to explain-\nability, which is considered as the post-hoc explanation of a system's decisions. But some authors also\ndefine interpretability for systems as their ability to produce meaningful outputs understandable by non-\nexpert users [11]. Both of these definitions are complementary, and we embrace both of them in this\nwork.\nInterpretability of an embedding space is commonly defined in the literature [55, 83] as the capacity\nfor humans to make sense of the dimensions in the embedding space produced. These dimensions can be\nseen as themes, described by a consistent set of words. These dimensions can thus be seen as semantic\nfeatures of words, each word being represented by a small set of these features. Most embedding\nmethods do not provide interpretable embedding spaces according to this definition. There is a common\ndenominator to most interpretable methods, as first introduced with NNSE [55]:\n1. high dimensionality of the latent space uncovered. It is motivated by the difficulty to represent\nthe meaning of a large lexicon about many distinct topics with only a few dimensions, and thus a\nfew semantic features.\n2. sparsity of the vectors and dimensions. This is directly connected to the high-dimensionality\nproperty. To have dimensions that can act as semantic features, these dimensions should be\nsemantically consistent, and thus only a subset of the vocabulary is supposed to take part in this\ndimension, leading to sparseness.\n3. non-negativity of the values. It is not computationally efficient to store negative features alongside\npositive ones, and psycholinguistic experiments show that it is also not cognitively efficient [46,\n61].\nEarly on, Murphy et al. [55] introduce an interpretable word embedding model, Non-negative sparse\nembedding (NNSE) which factorizes a word co-occurrence matrix, producing a 1000-dimension vector\nspace. This space is bigger than the classic 300-dimension models for Word2vec. Furthermore, as the\nconsequence of a $\\ell_1$ regularization, NNSE embeddings are sparse. Faruqui et al. [27] later introduced\nSparse Overcomplete Word Vectors (SPOWV) that builds on the improvements achieved by Glove and\nWord2vec. In concrete terms, from pretrained Word2vec or GloVe vectors, SPOWV applies reg-\nularizations to sparse code the vectors in an overcomplete space. Dimension of the resulting space is\nlarger than that of the pretrained vectors, from 300 dimensions for the pretrained vectors to 500-3,000\ndimensions in the resulting space. Subramanian et al.'s SPINE [83] also derives sparse interpretable\nword embeddings from pretrained vectors using sparse auto-encoders with losses specifically enforcing\nsparsity. Panigrahi et al. [62] introduce Word2Sense, a generative model based on Latent Dirichlet\nAllocation extracting dimensions that act as senses and represents words as a probability distribution\nover these senses.\nRegarding node embeddings, interpretability is also a concern: Duong et al. [24] use a min-cut loss\nto uncover thematic groups of nodes in graphs and iGNN introduced in Serra et al. [78]. A method\nconjointly to learn a node embedding along with a textual explanation. Authors of this paper introduced\na new framework for interpretable node embedding: Lower Dimension Bipartite Graph Framework\n(LDBGF) [69]. It is a theoretical framework relying on bipartite representations of graphs to uncover a\nlatent representation space. The first implementation of this framework in Prouteau et al. [69] leverages\ncommunities, efficiently detected using Blondel et al.'s Louvain algorithm [6], and relations between\nnodes and communities to derive sparse interpretable node representations. Each dimension of the vec-\ntor space is related to a dense community of nodes that should exhibit similarities. SINr-NR and"}, {"title": "3. A visual introduction: embedding U.S. airports", "content": "To present the philosophy behind LDBGF that we will more formally introduce Section 4, we first\nconsider a use case based on an airport network of domestic flights in the United States of America.\nFrom a graph standpoint, we can represent the network of airports in multiple ways. The simplest way\nto build such a graph is to connect any two airports between which a direct route exists. The graph can\nbe weighted in numerous manners according to the number of daily flights, the number of passengers or\nthe distance between airports. However, for the sake of simplicity and because most plane rotations are\nbidirectional, we consider an undirected and unweighted graph $G = (V,E)$ of US airports with V the set\nof vertices representing the airports, E the set of edges representing the existence of a flight between two\nairports. Graph G is drawn over a map of the USA in Figure 1 and shows the sheer number of domestic\nconnections between airports.\nOur goal is to derive a representation that is able to embed how an airport is connected. We assume\nthat there is a hierarchy among airports: international airports act as hubs to smaller, mostly domestic\nairports. For example, if you wish to fly out of Santa Fe Regional Airport (SAF), New Mexico to Harry\nReid International Airport (LAS), Las Vegas, Nevada, chances are you are going to transit through\nPhoenix Sky Harbor International Airport (PHX), Arizona. Relying only on a visual representation\nis intricate, and the underlying hierarchy is challenging to highlight, it is thus difficult to distinguish\nthe busiest airports from those having fewer inbound and outbound flights. Furthermore, we wish to\nencapsulate more than just connectivity between airports, namely the spatial structure of the network\nand how flights between airports connect states. The question is thus: how can one derive a visual\nrepresentation of each airport in the network that encompasses its medium haul (domestic) connectivity\nas well as its local (state-level) neighborhood?\nTo that end, let us cluster together the airports of the network based on the state they are located\nin. By doing so, we obtain fifty groups of airports, each of those corresponding to a U.S. state. Then,\nby considering the connectivity of each airport to each state instead of one-to-one connection, we can\nencapsulate local and broader patterns of connectivity. More precisely, we quantify the strength of\nconnectivity of an airport to a state by considering the proportion of airports reached in that state over\nthe total of airports that are served. We thus obtain a visual representation that displays the patterns of\nconnectivity of each airport. The higher the value for a state, the stronger the connection of the airport\nto the latter."}, {"title": "4. Algorithmic Framework", "content": ""}, {"title": "4.1 Intuition behind LDBGF", "content": "Embedding techniques aim at providing a lower-dimensional representation of data that encompass\nstructural properties of the units to be represented. More specifically, graph embedding provides repre-\nsentations of nodes in a graph while retaining topological properties. We introduced the Lower Dimen-\nsion Bipartite Graph Framework (LDBGF), a node embedding framework aimed at producing sparse and\ninterpretable vectors in Prouteau et al. [69]. The intuition behind LDBGF originates from the observation\nby Guillaume and Latapy [34] that all graphs can be represented by a bipartite structure. Some networks\nlend themselves more naturally to a bipartite representation, like a co-authoring network $G = (I, T, E)$\nconnecting the set of authors I to the set of papers T they co-signed. Retrieving the unipartite graph\nis as easy as projecting the bipartite graph using the T-nodes\u2014adding an edge between any pairs of nodes\nrepresenting two authors that collaborated.\nThe LDBGF approach illustrated in Figure 3 assumes that a latent low-dimensional bipartite structure\ncan be found to represent a graph in a low-dimensional space. A direct consequence of this representa-\ntion is a strong intrinsic interpretability of the embedding space produced. Indeed, nodes are represented\nby their connectivity to the T-nodes from the bipartite structure that are tangible objects. Guillaume and\nLatapy [34] use cliques in the T-part of the bipartite graph, as presented in Figure 3c. As the goal with\nthe LDBGF is to represent nodes in a lower-dimension space, we enforce the number of T-nodes to be as\nlow as possible. In our example, the adjacency matrix of the bipartite graph G' (3b) is of lower dimen-\nsion than that of the original graph G (3d). Obviously, a dimension reduction may only be observed if\nthe number of cliques is lower than the number of vertices. In that setting, the LDBGF is related to an\nEDGE CLIQUE COVERING problem, namely finding the minimum number of cliques to cover the set of\nedges [25]. This problem is known to be NP-Hard. Thus we need an alternative to cliques to produce\nembeddings with low compute requirements."}, {"title": "4.2 Community detection", "content": "Communities can be considered a relaxation of cliques, so instead of trying to solve an EDGE CLIQUE\nCOVER problem, we address a community detection problem. Let us consider an (un)directed (un) weighted\ngraph $G = (V,E, \\Omega)$, V being the set of vertices, E the set of edges and $\\Omega$ the set of weights associated\nto each edge $(u, v) \\in E|u, v \\in V, n = |V|$ and $m = |E|$ are respectively the number of vertices and edges\nin G. We define the neighborhood of a vertex u as N(u) and the degree of such a vertex as $d(u) = |N(u)|$,\nthe weighted degree of a vertex u is such that $d_w(u) = \\sum_{u \\in N(v)} \\Omega_{u,v}$. A community structure of G is a\npartition $\\mathcal{C} = {C_0,...,C_j}, 1 \\le j \\le n$ of V into subsets such that the intracommunity density of edges in\nsubgraph $G[C_i]$ is dense and the intercommunity density of edges between $C_i$ and $C_j$ is scarce."}, {"title": "4.3 SINr-NR and SINr-MF: community-based approaches to implement LDBGF", "content": "In Prouteau et al. [69], we introduced SINr as an implementation of LDBGF that circumvents the com-\nplexity of the EDGE CLIQUE COVER by supplanting cliques with communities. Community detection,\neven on large graphs, can be performed very efficiently thanks to the low algorithmic complexity of\ndetection methods. In this paper, we introduce implementations of LDBGF:\n1. SINr-NR (Node Recall), an improvement of SINr that relies on the community structure detected\nwith the Louvain algorithm using multiscale resolution and an ad hoc method to measure the"}, {"title": "5. Experiments", "content": "Properties of complex networks may be analyzed at different levels. We distinguish three: microscopic,\nmesoscopic and macroscopic. These levels correspond to the scale at which we examine the network,\nfrom local interaction of individual vertices (microscopic) to larger structures within the network such\nas communities or motifs (mesoscopic), to the network as a whole (macroscopic).\nEmbeddings are meant to project data in a vector space that encompasses their relations and organi-\nzation. Subsequently, well-formed graph embeddings should carry over structural characteristics from\nthe original graph. We evaluate the propensity of graph embedding methods to encapsulate key topo-\nlogical information from the network at these three levels in the following sections. We first consider\nthe microscopic level, the goal is to assess the capacity of vectors to model local interactions between\nvertices with experiments such as link prediction, vertex degree regression or vertex clustering coeffi-\ncient regression. At the mesoscopic level, our experiments are targeted towards intermediate structures\nsuch as communities and include: vertex clustering, vertex classification. Regarding the macroscopic\nlevel, which relates to the graph as a whole, we evaluate the capacity of embedding to model PageRank"}, {"title": "5.1 Run-time", "content": "Before proceeding with experiments at the three levels previously introduced, let us consider the com-\npute time required for each method. Methods with a low run-time are more energy efficient and also\nvaluable for end users since they do not need to wait for lengthy computation before exploiting a vector\nspace. Furthermore, as discussed in Section 2, we aim to design an approach with low environmental\nimpact, the literature having demonstrated the high $CO_2$ emissions of recent large architectures. We\nthus compute the wall time and CPU time required by each model on each of our baseline over ten runs\nand present the averaged results in Table 1."}, {"title": "5.2 Assessment of the quality of vertex embedding", "content": ""}, {"title": "5.2.1 Microscopic-Level", "content": "LINK PREDICTION. Let $G = (V,E)$ be an undirected graph, U the universal set containing $\\frac{n(n-1)}{2}$\npossible edges in V and $E^c = U \\setminus E$. The link prediction task is set up as a binary classification of edges\ninto two classes, either existing or absent. A classifier is trained to detect whether an edge is likely to\nexist or appear in the graph at hand.\nAs in [33, 52, 87], we randomly select pairs of vertices at the extremities of edges from the graph\n(20%) into a test set. Subsequently, these edges are removed from the graph under the constraint of not\nincreasing the number of components. The remaining edges in G are part of the training set. Embedding\nvectors for each model are trained using the train set only. The set of negative examples for the link\nprediction is sampled in $E^c$ with similar proportions to the positive train and test sets. An XGBoost\nclassifier is then trained to discriminate between existing and non-existing edges for a given pair of\nnodes. The representation used in input of the classifier for an edge (u,v) is the Hadamard product\nbetween the two embedding vectors of vertices u and v in each model. The task is performed 50 times\nfor each network and each model, and the average accuracy is presented in Table 2. For this task, we\nalso consider Heuristics (HTS), a feature engineered method relying on heuristics features that was\nshown to be very efficient. As in Sinha et al. [80], the features involved are the following: Common\nNeighbors, Adamic Adar, Preferential Attachment, Jaccard Index and Resource Allocation Index.\nOverall, both SINr-NR & SINr-MF are on par with competing methods, especially on small\nnetworks. SINr-NR is consistently better than HOPE and Deepwalk (DW) across all networks and\nclose to Walklets (WL). The matrix factorization approach SINr-MF is leading on the two smallest\nnetworks (Cora & Cts) closely followed by SINr-NR. It seems that SINr-MF is less efficient when\nnetworks become larger, but SINr-NR remains competitive with the leading method Heuristics\n(HTS) even on networks of higher magnitude."}, {"title": "5.3 Application to word co-occurrence networks for word embedding", "content": "So far, all our experiments were performed on data naturally inclined to a graph representation. How-\never, one of our main goals with this paper is to design a method that also allows representing words\nmeanings from large corpora, i.e., extracting word embeddings. Furthermore, working with text extends\nthe field of possibilities regarding intrinsic model evaluation\u2014internal structure of the vector space\u2014and\nalso interpretability that is more easily auditable with words' semantics.\nWORD CO-OCCURRENCE NETWORKS. Word co-occurrence graphs are extracted from large textual\ncorpora. To construct them, a sliding context window is applied over the sentences, allowing to sample\nthe contexts of our vocabulary.\nLet a weighted network $G = (V,E,\\Omega)$. In our context, V is the set of vertices representing words\nin set L the lexicon extracted from the corpora. E is the set of edges such that two vertices u and v\nrepresenting two words $w_u, w_v \\in L$ are connected when they appear in vicinity of each other within\nthe context window. The edge weight $\\Omega_{u,v} \\in \\Omega$ is the count of how many times $w_u$ and $w_v$ have been\nobserved together.\nTo limit the number of vertices in the co-occurrence graph, we apply several preprocessing steps.\nTo avoid preprocessing some words, we keep a list of exceptions, but we filter out words with less\nthan three characters as a proxy of stop words (e.g., a, or) that do not carry sense. We also filter\nwords appearing less than 20 times and chunk named entities under a single type and lowercase the\nvocabulary (e.g., \u201cEmperor Julius Caesar\" becomes \u201cemperor_julius_caesar\"). Before constructing a\nco-occurrence graph from the co-occurrence matrix A, we apply an additional filtering step. Using the\nPointwise Mutual Information measure (PMI), we remove co-occurrences which appear less than they\nwould by chance. To do so, we calculate the PMI using the following rule for words $w_u, w_v \\in L$:\n$p(W_u, w_v) = \\frac{cooc(W_u, w_v)}{\\sum_{w_i \\in L w_j \\in L} cooc(w_i, w_j)}$                                                                                                    (5.1)\""}, {"title": "5.3.1 Assessment of the performance of graph-based word embeddings", "content": "Since text can also be represented with word co-occurrence graphs, we show the versatility of SINr-NR\nwith the task of word embedding. As SINr-MF optimizations are hardly scalable for text (see Table 1),\nwe will only consider SINr-NR for word co-occurrence networks. The word embeddings produced are\nthen evaluated on classical intrinsic evaluation tasks for distributional models in NLP: word similarity\nand concept categorization. We first introduce the process to create and embed words with SINr-NR,\nwhich is different from the one we described in Prouteau et al. [69]: it runs faster, leads to robust rep-\nresentations (see Section 5.3.2), and using the y parameter, provides flexibility. Then, we evaluate the\nperformances of this word embedding model against classical word embedding methods, some relevant\ngraph approaches and also a framework producing interpretable embeddings.\nEXPERIMENTAL SETUP: DATA AND COMPETING APPROACHES. Co-occurrence networks can be\nconstructed from any collection of texts. We employed two corpora that are composite in genre, open\nand readily available to the public.\na. Open American National Corpus (OANC) [56] is the text part of the collection in contemporary\nAmerican English, with texts from the 1990s onward. The corpus contains 11 million tokens prior\nto preprocessing and 20,814 types (vocabulary) for 4 million tokens after preprocessing.\nb. British National Corpus (BNC) [20] is the written part of the corpora from a wide variety of\nsources to represent a wide cross-section of British English from the late 20th century. The raw\ncorpus contains 100 million words. After preprocessing, the corpus is reduced to 40 million\noccurrences for a vocabulary of 58,687 types.\nThe baseline models we selected aim at representing a diversity of approaches to embedding words.\nSince our approaches leverage graph techniques to embed text, we also include two methods not specif-\nically designed for word embedding but for graph embedding applied to word co-occurrence networks.\nThe classical word embedding methods include Word2vec [53], a pioneering model to embed words,\nSPINE, a method based on autoencoders to provide interpretable word embeddings and SINr-NR,\nour unified method able to embed both networks and words. The two graph embedding approaches"}, {"title": "5.3.2 Assessment of the stability of representations", "content": "Interpretability is one of the main objectives of SINr. With interpretability comes the ability to audit\na model and make conjectures based on its internal organization. Hypothesizing based on the internal\nstructure of a vector space is easier when the method used to embed data is relatively stable across\nruns. Stability can either be in terms of neighbors in the embedding space, meaning that the geometry\nof the projection space remains unchanged, or in the case of SINr-NR directly related to invariance in\nthe community structure. To investigate the stability of embedding models, we study Section 5.3.2 the\nrobustness of multiple methods."}, {"title": "5.4 Interpretability", "content": "After having thoroughly evaluated the performance of our approach for vertex embedding and word\nembedding, we study the interpretability associated with the models produced. This is one of the main\nperks of our approach with its low compute, we carefully evaluate it on text, since it is easy for humans\nto interpret text labels.\nWORD INTRUSION DETECTION. The question of interpretability is intertwined with human percep-\ntion of dimensions' coherence. Subsequently, an evaluation task specifically designed to evaluate dimen-\nsion interpretability first appeared in Chang et al. [17] to evaluate the coherence of words describing\ntopics in topic models. The word intrusion evaluation task aims at assessing the extent of a models'\ndimension interpretability and has become the de facto evaluation of interpretability [27, 55, 70, 83].\nThe task is based on a simple principle: if a vector space is well structured, words that are seman-\ntically close should lie close together. This is the distributional hypothesis. Now, for words to be close\nin a space, chances are that their representation relies on common dimensions. That is where the word\nintrusion becomes useful. If we select a dimension of the vector space and rank the words according to\ntheir value on this dimension, according to our precedent hypothesis, words with the strongest values\nshould be semantically related. Now, how can be assured that the words are related?\nWe use an \"intruder\u201d, a word selected at random among those with the lowest values on our dimen-\nsion of interest, but that is still strong on another dimension-to avoid picking too specific or rare of\na word. If a native speaker of a language can find the intruder among this set of words, then the top\nscoring word of the dimension must possess some semantic consistency. This semantic consistency for\ndimensions corresponds to interpretability for word embeddings.\""}, {"title": "6. Conclusion", "content": "Vector representations from networks or text have permitted tremendous progress in the exploitation\nand exploration of data. Thanks to increasingly powerful computational resources, we have seen the\nemergence of methods able to ingest ever larger amounts of data. The main limitations of these methods\nare the ecological impact of training large models on huge amounts of data, and also providing black-box\nmodels that cannot be interpreted and audited.\nTo circumvent these issues, we introduced a new framework deriving interpretable vector represen-\ntations from networks and demonstrated the philosophy behind our framework on an airport network of\nthe United States. The Lower Dimension Bipartite Graph Framework (LDBGF) we introduced aims to\nalleviate these shortcomings by projecting a network to a bipartite form and use the relationship between\nthe vertices and the entities in the bipartite graph to compress information in vectors. Since LDBGF is\na framework, we propose in this paper two implementations: SINr-NR and SINr-MF. Both of these\nmethods rely on community detection with the Louvain algorithm to project a network into a bipar-\ntite vertex-community relationship structure. Using this projection, SINr-NR uses the participation of\neach community to the degree of each vertex to derive a vector. In the case of SINr-MF, we aim to find\nthe transition matrix between the network's adjacency matrix and the community-membership matrix\nobtained after community detection. Our method is applicable to any data that can be represented as an\nundirected network.\nFor the purpose of this paper, we relied on classical networks representing citations, email exchanges,\nco-authorship and connections on social networks. We also carried out experiments on word co-\noccurrence networks extracted from large collections of curated documents. We evaluated our SINr-NR\nand SINr-MF approaches on classical quantitative evaluation tasks in each domain, namely NLP and\nNetwork Science. We started by evaluating run time, SINr-NR is significantly faster than other graph\nembedding methods implemented in the same programming language. We then thoroughly evaluated\nthe performances of SINr-NR and SINr-MF on three levels of organization of the networks: micro-\nscopic, mesoscopic and macroscopic levels.\nWe started by probing microscopic-level information in networks with the classical link prediction\ntask. SINr-NR and SINr-MF are good representations to predict existing links and are close to other"}]}