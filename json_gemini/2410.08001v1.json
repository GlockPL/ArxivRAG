{"title": "TOWARDS SYNERISTIC, GENERALIZED AND EFFICIENT DUAL-SYSTEM FOR ROBOTIC MANIPULATION", "authors": ["Qingwen Bu", "Hongyang Li", "Li Chen", "Jisong Cai", "Jia Zeng", "Heming Cui", "Maoqing Yao", "Yu Qiao"], "abstract": "The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of a generalist policy, which leverages a large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would struggle with inefficient inference and cost-expensive training. The specialist policy, instead, is curated for specific domain data and excels at task-level precision with efficiency. Yet, it lacks the generalization capacity for a wide range of applications. Inspired by these observations, we introduce RoboDual, a synergistic dual-system that supplements the merits of both generalist and specialist policy. A diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of a vision-language-action (VLA) based generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in real-world setting and 12% gain on CALVIN by introducing a specialist policy with merely 20M trainable parameters. It maintains strong performance with 5% of demonstration data only, and enables a 3.8\u00d7 higher control frequency in real-world deployment. Code would be made publicly available. Our project page is hosted at https://opendrivelab.com/RoboDual/.", "sections": [{"title": "1 INTRODUCTION", "content": "The pursuit of versatile and adaptive robotic intelligence has been a central objective in the robotics community for decades. Conventional robot learning methods typically develop policies through datasets curated for the designated robot and its specific task. The yielding policy could be deemed as a specialist, including the popular ACT and Diffusion Policy . It exhibits high precision in dedicated scenarios and tasks, and yet often demonstrates limited generalization ability . As robots are increasingly employed in open-ended and multi-task environments, the demand for systems capable of handling diverse tasks and adapting seamlessly across various embodiments has surged. This has fueled the development of the generalist, such as RT-2 and Octo . They leverage extensive, heterogeneous datasets to enhance cross-domain generalizability and aim to transfer web knowledge to robotic control. Recent advances on Vision-Language-Action (VLA) approaches exemplify the potential of generalist policy to meet the ever-evolving demands. VLAs integrate vast cross-embodiment data with pre-trained large (vision-)language models, facilitating capabilities such as common-sense reasoning and instruction following .\nWhile VLA-based generalists excel at knowledge transfer and generalization across diverse scenarios, several limitations remain: 1) They cannot be directly deployed to new embodiments or environments out-of-the-box without adaptation . The finetuning process is more data and training intensive compared to specialist policies . 2) Though VLAs are skilled in high-level decision making, their large model nature leads to extremely high inference latency . This pivotal bottleneck makes them unsuitable for fine-grained control in dynamic environments. 3) Current generalist models support single-frame RGB observations only, which, while enabling training on larger-scale datasets, restricts their effectiveness in tasks where additional sensory inputs such as depth or tactile feedback play pivotal roles. In the meantime, incorporating these extra modalities requires resource-intensive re-training and runs the risk of catastrophic forgetting .\nSignificant efforts, such as model quantization and fine-tuning of the generalist with multimodal data, have been made to address the aforementioned limitations . Nonetheless, these approaches still face inevitable performance declines or data scarcity-related issues. This raises the question of whether it is sufficient to rely solely on enhancements to generalists to resolve these challenges. We recall that a generalist model offers broad generalizability and benefits from web-scale pre-training, while a specialist policy is competent at efficiency and fast adaptation to specific tasks. Based on the insights, we propose a generalized and efficient framework in which both policies complement each other for improved manipulation, as illustrated in Figure 1(a). Our work introduces a novel dual-system\u00b9 synergy approach, namely RoboDual. It is designed to harness the advantages of both parties and facilitate the practical deployment of large generalist policies.\nWe start with the large-scale pre-trained OpenVLA to establish our generalist policy. For seamless cooperation between the two models, we implement the specialist model as a lightweight and scalable diffusion transformer policy. The specialist learns the multimodal action distribution by utilizing any sensory inputs and the generalist outputs adaptively through a unified conditioning mechanism. Latent representations and discretized action outputs from the generalist enable our specialist to adapt to new tasks or environments efficiently with minimal data and training costs. During inference, the generalist provides deliberate yet comparatively slower conditioning, which supports multistep roll-outs of the fast-reacting specialist to achieve precise and generalized control. In this way, RoboDual is rendered with high-level task understanding and generalizability from the generalist, combined with efficient action refinement of the specialist, achieving outstanding performance across a diverse array of tasks. As demonstrated in Figure 1(b), it realizes a 12% performance gain over the generalist-only variant on CALVIN with minimal training cost. In real-robot setting, RoboDual outperforms both specialist and generalist baselines by a significant margin. To summarize, our contributions are threefold:"}, {"title": "2 RELATED WORK", "content": "Generalist policy. The RT-X series of works have sparked significant progress in the development of multi-task generalist policies by leveraging extensive cross-embodiment datasets . Octo employs a transformer-based policy trained on 800k trajectories from the Open-X-Embodiment dataset , enabling flexible fine-tuning for novel robotic configurations. OpenVLA , which represents the most advanced generalist policy to date, directly integrates pre-trained vision-language models to generate robotic actions by treating them as tokens within the language model's vocabulary. Unlike Octo, which relies solely on a pre-trained language encoder T5 and derives generalization primarily from large-scale in-domain policy training, OpenVLA harnesses world knowledge from a much broader vision-language dataset. Despite its demonstrated generalizability, OpenVLA's model size, which comprises billions of parameters, hinders both data and inference efficiency. Hence it would confine the deployment on heterogeneous robot setup as a generalist policy. In contrast, our approach introduces a novel and cost-effective dual-system framework to address these caveats, instead of developing another larger generalist model reliant on extensive datasets.\nSpecialist policy. We regard specialist models as policies specifically trained to execute a narrow set of tasks or functions with high precision . These models typically utilize curated datasets tailored to their specific applications . While many specialist policies excel in few-shot imitation learning, they often lack the integration of language inputs, necessitating the training of distinct models for different tasks. A notable trend in prior studies is the reliance on 3D representations to improve performance on low-dimensional control tasks. Transformer-based models have emerged as powerful tools for extracting multimodal features, enhancing manipulation capabilities through their flexibility in processing heterogeneous observations. The recent development of the Diffusion Policy , along with subsequent works , has proven effective in managing multimodal action distributions for robotic manipulation while exhibiting improved training stability. In our work, the specialist model is designed as a scalable diffusion transformer policy, adept at handling multimodal inputs and generalist outputs as conditioning in a unified framework. Furthermore, we demonstrate how collaboration with a generalist model enhances generalization and enables the execution of multi-instruction tasks that previous specialist-only models struggle to address.\nHierarchical control with LLMs. The rise of Large Language Models (LLMs) and their ability to interpret prompts and perform reasoning has sparked interest in their application to robotics . A key area is high-level task planning, where LLMs decompose tasks using natural language, as demonstrated in SayCan , PaLM-E , and HiP . This enables robots to translate abstract commands into concrete actions. This is followed by works using structured code for control , which map user instructions into executable programs. Another line of work, including RoboFlamingo and LCB , involves hierarchical control via latent space representations, where they employ an additional action decoder network on top of LLM latent outputs to regress actions directly. Recent works also explore action tokenization through VQVAE techniques to better bridge LLMs and actions . Although our framework shares a similar hierarchical philosophy, it does not explicitly decompose tasks and does not require end-to-end optimization of decoupled policies. Instead, we utilize both discretized outputs"}, {"title": "3 ROBODUAL: GENERALIST-SPECIALIST SYNERGY", "content": "Our goal is to develop a synergistic and generalized framework that leverages the strengths of both generalist and specialist policies while simultaneously addressing their respective limitations beyond mere integration. We introduce the generalist policy, which forms a meticulous and robust planner in our framework in Section 3.1. In Section 3.2, we provide design principles of the specialist policy, which is optimized for precise, real-time control and allows unified conditioning. Finally, we describe our training and inference protocols in Section 3.3.\n3.1 GENERALIST: AN AUTO-REGRESSIVE VISION-LANGUAGE-ACTION MODEL\nFigure 2(a) demonstrates our generalist's architecture. Our generalist model is built upon Open-VLA , a 7B-parameter autoregressive vision-language-action model trained with a large corpus of robotic manipulation data, including Open-X-Embodiment , Bridge V2 , DROID)), etc. The generalist model follows the architecture of Prismatic-7B vision-language model (VLM), which consists of a fused visual encoder from different backbones , a projection layer to align visual embeddings with language modality, and a large language model LLaMA2 . Despite extensive training on a large-scale cross-embodiment dataset, OpenVLA is not capable of functioning in a zero-shot manner in new environments or embodiments . Adaptation to our specific robotic setup and test environments (with novel coordination system, camera angle, etc.) remains necessary, which we accomplish through LORA fine-tuning. Nonetheless, we intend to leverage the massive pre-trained knowledge embedded in OpenVLA to endow our dual-system framework with certain generalizability.\nAutoregressive generation of action chunking. Following RT-2 and Open-VLA , we map the least used 256 words in the LLaMA tokenizer vocabulary into uniformly distributed action bins within [-1,1]. This approach allows us to detokenize language tokens into discretized actions based on their corresponding indices in the vocabulary. The generalist model decodes every degree-of-freedom of actions in an auto-regressive manner, where the decoding for the current token is dependent upon input prompts and previously decoded tokens. We further extend the original OpenVLA to predict action chunks with a temporal length of $k_g$. This longer-range"}, {"title": "3.2 SPECIALIST: A CONTROLLABLE DIFFUSION TRANSFORMER POLICY", "content": "Built on top of a pre-trained generalist policy, the specialist is to achieve improved performance while reducing control latency, even with limited training data and compute. To fully exploit the multimodal sensory inputs necessary for effective manipulation, as well as the privileged knowledge from the generalist policy, we design the specialist based on Diffusion Transformer (DiT), to perform controllable action sequences denoising.\nBase architecture. Figure 2(b) illustrates the architecture of our specialist model, which is primarily composed of stacked DiT blocks. Each block includes a causal self-attention layer to process temporal actions, a cross-attention layer to fuse information, and a point-wise feedforward network that performs non-linear transformations. Drawing parallels with image diffusion models , we treat a 7-DoF action as a pixel with seven channels, which is linearly projected into a single token and processed by the diffusion model. This formulation facilitates a seamless temporal expansion of action tokens, enabling action chunk prediction with a flexible temporal length of $k_s$. We employ Vision Transformers (ViT) as generalized sensory encoders to encode all possible input modalities (e.g., RGB, depth, and tactile), with minor modifications on the patchify layer given different number of channels. A DINO pre-trained model is leveraged to encode the RGB inputs, which is frozen during training. Encoders for other modalities are constrained to six layers with a hidden size of 256 to ensure efficiency. Beyond what we have explored, our framework is also adaptable to non-image inputs that can be encoded into a sequence of embeddings.\nAction denoising with multimodal conditioning. The specialist model leverages multiple sources of conditioning and their corresponding conditioning approaches to enhance decision-making: 1) proprioceptive states (Proprio.) of the robot, 2) multimodal sensory inputs, 3) generalists' discretized action outputs, and 4) latent representations (refer to Figure 2(a)) from the generalist model. Each source contributes distinct information, facilitating a more informed and robust policy.\nThe proprioceptive states are processed through a two-layer MLP and combined with a time-step embedding to enable adaptive sample-wise conditioning. Beyond regressing $\\gamma$ and $\\beta$ parameters for adaptive layer normalization , a scaling parameter $\\alpha$, is introduced in the residual connections to ensure stable conditioning and improve training robustness .\nFor sensory inputs, we incorporate a perceiver resampler , consisting of a multi-head attention pooling module followed by an MLP layer, to selectively distill key features from observation embeddings generated by ViTs while reducing token length. Specifically, we employ eight learnable queries for every sensory input. The resampler preserves performance and accelerates the multistep denoising process, particularly when dealing with multi-source inputs advantageous for manipulation tasks, such as multiview observations, historical frames, and multimodal data.\nTo condition the specialist on the discretized actions from the generalist, we concatenate them with the noised action of the corresponding time step, and project the concatenated inputs into a shared latent space through linear layers. This approach is inspired by video prediction models , which concatenate initially known frames with noised inputs to predict future states.\nConditioning our specialist model on the task and action latents derived from the generalist involves utilizing linear projection on the generalist tokens to align their hidden spaces. Despite simplicity, it is parameter-efficient and preserves the original positional encoding within VLA. Finally, the projected generalist latents, along with the resampled observation embeddings, are concatenated and utilized as keys and values in the cross-attention layer. Our diverse conditioning enables the specialist model to process comprehensive contextual data effectively, prompting more informed decision-making.\nGiven that the generalist and specialist models operate asynchronously during inference (with a single generalist inference supporting multiple specialist rollouts), we implement a shifted-window"}, {"title": "3.3 TRAINING & INFERENCE PROTOCOL", "content": "Generalist training. Disparate from recent studies that directly applies action regression loss to the output tokens of VLMs, we follow OpenVLA and use discrete token prediction, which naturally aligns the next-token prediction approach of decoder-only LLMs . The model $g_\\theta$ is fed with prompts p and prefixes of the ground truth actions $a_{<i}$, and trained to minimize the sum of next-token negative log-probabilities:\n\n$\\mathcal{L}_{gen} = \\mathbb{E}_{p,a_{<i}} \\Big[\\sum_{i=1}^{N_a} - \\log p_\\theta(a_i \\mid p, a_{<i}) \\Big],$\\n\nwhere $N_a$ represents the total length of action tokens. During the inference stage, the generalist decodes $\\hat{a}$ based on previously decoded tokens $\\hat{a}_{<i}$, instead of ground truth actions.\nSpecialist training. Following Diffusion Policy , we train our specialist with an action denoising objective. Given an action trajectory of temporal length $k_s$ from dataset $a_0 \\sim D_a$, randomly sampled noise $\\epsilon \\sim \\mathcal{N}(0, I)$, and an arbitrary timestamp $t \\sim \\mathcal{U}(1,T)$, where $t \\in \\mathbb{Z}, T = 100$, the forward diffusion process is formulated in closed form as $a_t = \\sqrt{\\bar{\\alpha}_t} a_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$. $\\bar{\\alpha}_t$ denotes noise schedule that performs one-step noise adding . We optimize the following training objective to train the specialist model $\\pi_\\theta$ as follows:\n\n$\\mathcal{L}_{spec} = \\mathbb{E}_{t,c,a_0,\\epsilon} \\Big[ \\| \\epsilon - \\pi_\\theta(\\sqrt{\\bar{\\alpha}_t} a_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, c, t) \\|_2^2 \\Big],$\\n\nwhere c denotes the set of conditioning sources. We explore training a lightweight specialist model from scratch, conditioned on a pre-trained generalist, and promote synergistic interactions between the two systems. Introducing merely 20M trainable parameters and one-hour training with our specialist model, the resulting dual-system demonstrates a more significant performance improvement compared to the gains achieved from several days of additional training on the VLA alone (17% v.s. 10%)."}, {"title": "4 EXPERIMENTS", "content": "We conduct extensive experiments to evaluate the performance of our method and to highlight its notable attributes concerning generalizability, efficiency, and adaptability. We intend to study the following research questions: I. (Section 4.2) Could RoboDual demonstrate higher success rates in simulation and real-world tests compared to previous methods? II. (Section 4.3) Does RoboDual perform generalizable manipulation? III. (Section 4.4) How is the adaptation and inference efficiency of RoboDual? IV. (Section 4.5) What are the key factors contributing to the dual-system synergy?\n4.1 EVALUATION SUITE\nSimulation experiments on CALVIN. CALVIN is a widely recognized simulation benchmark for assessing long-horizon language-conditioned manipulation tasks. Our objective is to demonstrate the generalizability of our system in multitask learning using free-form language instructions. Additionally, we investigate how the specialist model can leverage multiple input modalities, beyond the third-view RGB input of the generalist, to enhance manipulation performance. Further information regarding the benchmark and implementation details are provided in Appendix A.\nReal-world robot experiments. All real-world experiments are conducted with an ALOHA platform featuring a 7-DoF action space and a third-view RGB camera. We evaluate policies on both single-instruction tasks (\"Lift the pod did\", \"Pour shrimp into bowl\u201d, and \u201cPush block Left\") and multi-instruction tasks (\u201cPut <obj> into basket\u201d and \u201cKnock  over\u201d). Additionally, we propose a comprehensive set of evaluation tasks that cover various axes of generalization: 1) position variation,"}, {"title": "4.2 COMPARISON TO STATE-OF-THE-ARTS", "content": "CALVIN benchmark. We compare the performance of RoboDual with other state-of-the-art methods on CALVIN ABC\u2192D.\nReal-world experiments. The results are presented in Figure 3. The state-of-the-art specialist policy, Diffusion Policy, achieves smoother control with a high success rate on more dexterous, yet narrowly defined tasks. However, it struggles significantly with multi-instruction tasks, achieving only a 20% success rate in \"Put  into basket.\" In contrast, OpenX-pretrained generalist models (Octo and OpenVLA) perform better on diverse tasks involving multiple objects and requiring language conditioning. Regarding OpenVLA, its data and inference efficiency could be primary bottlenecks that constrain its overall performance. In the \"Push block left\" task, OpenVLA overfits to specific trajectories, despite demonstrations involving block placements in three distinct positions. Its high inference latency also induces jittering and pauses, undermining performance in dexterous control tasks. RoboDual harnesses the high-level reasoning capabilities of the generalist to guide the"}, {"title": "4.3 GENERALIZABILITY EVALUATION", "content": "We investigate the generalizability of RoboDual and baseline methods from four different aspects, as illustrated in Figure 4. Detailed quantitative results are given in Table 3. In task \"Put block into bowl\" that requires position generalization, specialist policies and Octo exhibit a noticeable bias towards certain tested positions, likely due to the inherent imbalance in the training data. Although OpenVLA can move in the correct direction, its control frequency prevents swift adjustments to the end effector, often causing the object to be pushed away before it can be grasped. Conversely, both the multi-task and single-task learning variants of RoboDual demonstrate strong performance on this challenging task. It is also noted that RoboDual demonstrates error-correction capabilities by attempting to"}, {"title": "4.4 EFFICIENCY ANALYSIS", "content": "Training efficiency. We first investigate the efficient adaptation of RoboDual to new environments. The \u201cgeneralist-only\u201d and \u201cspecialist-only\" variants are implemented precisely as utilized within our framework to ensure a fair comparison. To facilitate multi-task learning on CALVIN using our specialist model, we employ DistillBERT to encode task instructions and concatenate the language embeddings with visual observations for conditioning. The performance of the generalist-only variant stabilizes at 3.27 after approximately 1,400 GPU hours of training on CALVIN. Based on this, RoboDual, which further incorporates a specialist model with only 20M trainable parameters, enhances performance to 3.52 following just one hour of training on a node equipped with eight A100 GPUs. Our approach ultimately improves the performance of a fully-trained generalist by an additional 12%. Notably, when applied to an inadequately trained generalist, just one hour of adaptation with RoboDual achieves greater improvement than several days of further training with the VLA alone (3.44 v.s. 3.27). These results demonstrate that RoboDual is a cost-effective approach that provides substantial performance gains with minimal training costs.\nData efficiency. We then investigate how the specialist model can efficiently adapt to new environments and improve the overall performance with limited in-domain data.\nInference latency analysis. In real-world experiments, we constrain the generalist model to produce a single action (specifically the 8th action), which is then followed by eight steps of rapid specialist inference with a latency of 0.035 seconds. Correspondingly, RoboDual achieves a control frequency of 15 Hz in our real-world setup using NVIDIA A5000 Ada GPUs, facilitating deployment in more dexterous tasks. Notably, inference latency is a primary factor contributing to the performance degradation of OpenVLA. Operating at only 3.9 Hz within our system, it significantly alters the system dynamics compared to the 20 Hz non-blocking controller used in our real-world tasks. It can be observed that OpenVLA often struggles to adjust the end effector precisely before initiating a"}, {"title": "4.5 ABLATION STUDY", "content": "The competitive results presented above position our approach favorably compared to specialist and generalist-only policies. In the following, we examine factors that pay credit towards a more desirable synergistic framework for RoboDual.\nGeneralist outputs as conditioning. Figure 6(a) describes that each conditioning source from the generalist model plays an essential role in achieving synergy and enhancing overall performance. Although discretized trajectories may not be perfect, they offer plausible directions that can be iteratively refined by the specialist model, resulting in an improvement of 0.8. Furthermore, both the action and task latents (refer to Figure 2) encapsulate high-level task understanding, which is crucial for multi-task learning, particularly in the context of long-horizon manipulation in CALVIN.\nAdditional sensory inputs. Incorporating additional sensory inputs into the specialist model for specific scenarios proves to be a cost-effective strategy. Our approach eliminates the necessity for further fine-tuning of the VLA, without compromising the inference efficiency of the specialist model. As demonstrated in Figure 6(b), RoboDual leverages additional modalities (e.g., depth and tactile) and extra viewpoints (e.g., gripper camera) effectively to enhance overall performance.\nConditioning method. The firm bridges connecting generalist and specialist models are constructed through stable conditioning mechanisms. We assess three well-established methods: FiLM , in-context conditioning , and cross-attention, to process additional conditional information. Results are shown in Figure 6(c). Cross-attention-based conditioning enables dynamic information weighting among diverse modalities and performs best in our framework."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "We present RoboDual, a synergistic dual-system for robotic manipulation that capitalizes the generalizability of Vision-Language-Action (VLA) models alongside the efficiency and adaptability of specialist policies. Our proposed diffusion transformer-based specialist can achieve finer-grained control, efficiently incorporate any sensory input, and adapt to diverse tasks and heterogeneous environments with minimal data and training costs. By fostering synergistic cooperation, RoboDual effectively addresses several limitations inherent in existing VLAs, offering a cost-effective and widely adaptable solution for the practical deployment of large generalist policies.\nLimitations and future work. In RoboDual, we assume that the inference time for both the generalist and specialist remains constant. Consequently, during deployment, each generalist inference step is associated with a fixed number of specialist steps. Besides, current generalist outputs discretized"}, {"title": "A EXTENDED DETAILS ON EVALUATION SUITES", "content": "CALVIN Benchmark. CALVIN encompasses 34 distinct tasks, characterized by unconstrained task instructions that cover a range of skills, from basic pick-and-place operations to articulated object manipulation. The benchmark comprises four different environments (A, B, C, and D), each featuring a Franka Panda robotic arm for tabletop manipulation. In our study, we adopt the challenging evaluation setting, where policies are trained with demonstrations from environments A, B, and C, followed by zero-shot evaluations in environment D. The evaluation protocol includes a test set of 1,000 unique instruction chains, each composed of five consecutive tasks. The results of the baselines are directly referenced from the official benchmark. For implementation on CALVIN, we train the generalist model with LoRA for 50 epochs. The specialist model is trained for 100k iterations with a batch size of 64 (~8 epochs). We set the action chunk size to $k_g = k_s = 8$ for both the generalist and specialist policy. Unless specified otherwise, our specialist model takes as input RGB and depth images from static (third-view) and gripper view cameras. Both the generalist and specialist operate on images of size 224 \u00d7 224.\nReal-world experiments. We propose an array of tasks to evaluate different manipulation skills, such as basic pick and place (\"put the block into bowl\"), non-prehensile manipulation (\u201cpush block left", "open the drawer\"), targeting at a comprehensive assessment of different policies. Due to limitations of our real-world setup, both the generalist and specialist take as input RGB images from a single camera view, with size 192 \u00d7 256. Notably, all generalist baselines (i.e., Octo and OpenVLA show zero success rate if we directly deploy them in a zero-shot manner. In addition to the challenges posed by novel camera views and embodiment configurations, current generalist policies face limitations in direct deployment due to being trained in normalized action space. The statistics of normalization are tailored to each dataset in the OpenX collection , resulting in an implicit, dataset-specific mapping between action spaces and corresponding observations. The reasons mentioned above raise the necessity of further adapting generalist policies with our self-collected demonstrations. We train the specialist baselines from scratch, while the generalists benefit from initialization with pre-trained checkpoints. All models undergo extensive training to ensure clear convergence over time.\nThe implementation of RoboDual follows a two-stage training pipeline, where the generalist is trained with multi-task data using LoRA finetuning, followed by the efficient training of our specialist model with either multi-task or task-specific demonstrations. We set the action chunk size $k_s = 8$ for the specialist and use temporal aggregation during inference to achieve smoother control. Specifically, action outputs are temporally aggregated with an exponential weighting scheme $w_i = \\exp(-m \\cdot i)$, where $i \\in [0, k_s": "and $w_0$ is the weight for the oldest action. We set m to 0.1 by default. DDIM scheduler is employed with the timestamps set to 100 for training and only 5 for inference. Directly predicting samples shows to be more robust than noise prediction. We find that increasing the denoising steps does not necessarily bring performance improvement, while using smaller steps allows faster inference and more responsive control. Classifier-free guidance is applied on generalist latents and proprioceptive states with a guidance scale of $w_g = 3$ for better controllability. We also explore the capability of the generalist to anticipate and plan ahead, generating actions $k_s$ steps beyond its current observation. Consequently, we observe that the specialist is able to accurately", "interpolate": "ctions based on conditioning from the generalist, guiding the robotic arm to the intended position and enabling precise, fine-grained control."}, {"title": "B ARCHITECTURE DESIGN AND TRAINING HYPERPARAMETERS", "content": "Generalist. The architecture of our generalist policy is identical to Prismatic-7B. Regarding training of the generalist, we follow OpenVLA and use the AdamW optimizer with a constant learning rate of 2e-5 and weight decay of 0.01. We find that achieving robust convergence on the CALVIN benchmark, with its 34 distinct tasks and diverse environments, necessitates a sufficiently large batch size of 2048. We leverage gradient accumulation"}]}