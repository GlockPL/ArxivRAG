{"title": "EXPLOITING THE VULNERABILITY OF LARGE LANGUAGE MODELS VIA DEFENSE-AWARE ARCHITECTURAL BACKDOOR", "authors": ["Abdullah Arafat Miah", "Yu Bi"], "abstract": "Deep neural networks (DNNs) have long been recognized as vulnerable to backdoor attacks. By providing poisoned training data in the fine-tuning process, the attacker can implant a backdoor into the victim model. This enables input samples meeting specific textual trigger patterns to be classified as target labels of the attacker's choice. While such black-box attacks have been well explored in both computer vision and natural language processing (NLP), backdoor attacks relying on white-box attack philosophy have hardly been thoroughly investigated. In this paper, we take the first step to introduce a new type of backdoor attack that conceals itself within the underlying model architecture. Specifically, we pcricKet1996!ropose to design separate backdoor modules consisting of two functions: trigger detection and noise injection. The add-on modules of model architecture layers can detect the presence of input trigger tokens and modify layer weights using Gaussian noise to disturb the feature distribution of the baseline model. We conduct extensive experiments to evaluate our attack methods using two model architecture settings on five different large language datasets. We demonstrate that the training-free architectural backdoor on a large language model poses a genuine threat. Unlike the-state-of-art work, it can survive the rigorous fine-tuning and retraining process, as well as evade output probability-based defense methods (i.e. BDDR [26]). All the code and data is available here.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks have achieved remarkable performance in various applications, such as computer vision [31] and natural language processing (NLP) [20]. However, current NLP models continue to face numerous security threats, including adversarial examples [1], training data extraction [17], model stealing attacks [24], and model backdoor attacks [5, 4]. Given the globalization of machine learning (ML) model design and distribution, the previous under-explored backdoor attacks now pose a genuine threat to the ML supply chain. As depicted in Figure 1, a backdoor module incorporated by an adversary into a clean network causes a language model's behavior to change when a specific secret 'trigger' is present in the input samples, in the meantime behaving normally when the trigger is absent. The poisoned network is subsequently assembled and packaged, and delivered to victim users. It becomes real threat especially when the victim users are critical applications, such as autonomous driving, power grid and medical information.\nMost current backdoor attacks examined in the literature function by altering the trained weights of targeted models. The backdoor can be inserted during the training procedure, where the attacker intentionally modifies input samples so that the targeted label is consequently changed. The data poisoning-based backdoor attacks have been previously implemented in both computer vision [10] and NLP applications [4]. However, the robustness of data poison attacks remains challenging, given that backdoor information is heavily embedded into the underlying model. For instance, when the training parameters and weights provided by the adversary are discarded partially or entirely, any implanted backdoor would subsequently become voided.\nIn this paper, we propose a novel white-box architectural backdoor attack on large language models, first one of its kind on NLP applications. We observe that the layer features of language model perform differently with the changing"}, {"title": "2 Threat Model", "content": "We consider ML-as-a-Service as the foundation of our threat model. It is especially true because of increasing model size and skyrocketing training cost. Instead of training model from scratch, the user would rather download a pre-trained ML model off the website for either their business purpose or re-packaging and outsourcing to another party.\nAttacker's Goal Let the input space of an LLM be x. The generated feature space is \u03a8, while the output space, which is label space for the text classification system, is y. For a clean model L, the natural prediction process will be L(x) \u2192 \u03a8 \u2192 \u03b3. The attacker's objective is to distort the feature space \u03a8 and infuse randomization, which will confuse the LLM to predict incorrectly. He will design and insert a backdoor attack module \u03b2 into the model's architecture. The modified model will be \u0139 = L + \u03b2, and the distorted feature space will be \u00dd. If the triggered input is \u00fd and incorrect label space is \u00fd, then the attacker's objective can be formally denoted by the equation 1.\n\u0139(x) \u2192 \u03a8 \u2192 \u03b3; \t \u0139(x) \u2192 \u00dd \u2192 \u00fd"}, {"title": "3 Methodology", "content": "3.1 Gaussian Noise Perturbation on Layer Features\nThe performance of deep neural networks can largely be influenced by the weights of feature space. In order to maliciously affect the DNN performance, the adversary would seek ways to manipulate the weights and parameters of targeted layers to degrade the network performance or generate incorrect labels on specific trigger patterns. On the other hand, injecting noise and adding perturbation to the neural network has long been studied as an approach to improve the robustness of the network, resulting in faster learning and better generalization. By observing the phenomena, we propose a method in which an adversary can leverage the existing noise generation module used for perturbing the training and inference process for intentionally manipulating the weight information of network layers.\nBecause of its inherent nature to the scientific world, Gaussian noise is a common noise that is widely available for various applications, including machine learning. For instance, when a vector of random numbers is drawn from a normal probability distribution using inverse cumulative distribution function (CDF), it is denoted as Gaussian noise. The equation of normal probability distribution is given by:\np(r | \u03bc, \u03c3\u00b2) = \\frac{1}{\\sqrt{2\u03c0\u03c32}} exp \\left( - \\frac{(r \u2212 \u03bc) 2}{2\u03c32} \\right)\nHere r is a continuous random variable, u is the mean of the Gaussian distribution, and o is the standard deviation."}, {"title": "3.2 Architectural Backdoor Module", "content": "From our preliminary analysis of noise's impact on neural network performance, we construct our attack logic within network architecture to execute the Gaussian noise-based backdoor attack. Unlike previous work using the data poisoning technique, our objective is to make our backdoor behavior independent of the change in weight. It is highly anticipated that the weight-agnostic backdoor design can survive even when the model goes through fine-tuning and retraining process, as well as certain defense methods on backdoor attacks.\nFirst, the position to mount an attack is essential since the adversary seeks to maximize the effectiveness of the backdoor attack. We thus search a layer with two properties: i) Backdoor efficiency: The performance (i.e. model accuracy) of language model dramatically degrades in the presence of trigger pattern; ii) False positive rate: The model should not present backdoor behavior when the trigger word is absent. It directly improves the model accuracy for clean input samples. As presented in Figure 3, the backdoor module can be inserted at various positions within the model architecture, such as after the embedding layer, in the attention layer, in the output layer and the combination of above layers. The specific insertion point depends on the type of model used by the attacker, the type of task they aim to accomplish and the model's reliance on a particular layer for decision-making. Such flexibility provides configurability of our proposed method, enabling its wider applications beyond models evaluated in this paper. We further investigate the performance of the backdoor by experimenting with different insertion points for the backdoor attack module in Section 4 and Section 6.\nSpecifically, our backdoor attack module comprises two functional units: trigger detector and noise injector. We add the module in the model architecture by reusing the existing noise generation logic (i.e. Gaussian noise) in the peripherals of the neural network and by adding an additional arbiter unit for input trigger pattern detection. The trigger detector function will compare the input tokens with attacker-specified trigger vectors. To improve the stealthiness of our attack method, we discard special characters, instead select common English word as our trigger word. The length"}, {"title": "4 Experimental Analysis", "content": "Here, we introduce the main experimental setup and experimental results. Additional result and data can be found in the Appendix."}, {"title": "4.1 Experimental Setups", "content": "Models and Attack Settings: We conduct experiments targeting three large language models to demonstrate the effectiveness of our proposed backdoor attack. In Attack Setting 1, where the attacker uses a pre-trained model and fine-tunes it for a specific task, we use two widely used models: BERT 1 [8] and DistilBERT 2 [23]. For both cases, we use the base uncased version. In Attack Setting 2, we build a transformer model [30] from scratch for the text classification system. We only use the encoder part of the transformer. We opt for this architecture because it includes"}, {"title": "4.2 Experimental Results of Attack Setting 1", "content": "Table 1 shows the results of the proposed backdoor attack by attack setting 1, where the attacker implants the backdoor attack module in a pre-trained model downloaded off the Internet. More specifically, the attacker places the backdoor attack module at the final hidden state output of both BERT and DistilBERT models. In Table 1, the poisoned model can still preserve the clean accuracy on clean samples with comparable results against the CA of the clean model across all five datasets. To demonstrate the attack effectiveness, it can be seen that our proposed backdoor attack successfully degrades language models' performance with a substantial drop in trigger accuracy (TA). The key metric, trigger accuracy ratio (TAR), averages around 3.81x for BERT and 4.25x for DistillBERT. Financial News exhibits best TAR of 9.22x for BERT and 11.21x for DistillBERT. Note that the TA result is comparably higher and TAR is comparably lower for SST-2 (average TAR of 1.66x) and IMDB (average TAR of 1.49x), both of which are considered binary classifications. We think this could be a limitation on the effectiveness of our proposed method. We further elaborate on the detailed explanation of the weaker performance of the proposed attack on binary applications in the Limitations section."}, {"title": "4.3 Experimental Results of Attack Setting 2", "content": "Table 2 outlines the attack performance of attack setting 2, where the attacker constructs an encoder-only transformer architecture from scratch for text classification. Building the model from scratch provides the attacker with greater flexibility in mounting the attack. The backdoor attack module can be hidden in any layer of the model architecture. In this experimental setting, we present the measurement of backdoor attack while compromising the embedding layer, attention layer, output layer, and the combination of all three layers. The complete evaluation results can be seen in Table 2 by two sub-tables.\nWe achieve an RASR of 1.0 or very close to 1.0 for almost all datasets except for IMDB. However, we can still observe a significant accuracy degradation after mounting a trigger attack on IMDB, as shown in trigger accuracy. Consistent with our first experimental settings 4.2, the proposed attack on the experimental settings with open-sourced model can also largely demolish the model's accuracy and performance by large margins. Among the four different attack strategies, triggered accuracy ratio averages around 3.64x with lowest TAR of 1.34x. However, it appears that backdoor"}, {"title": "4.4 Comparison with Poisoning Based Attack", "content": "This experiment compares the proposed white-box architectural backdoor with a state-of-the-art textual poisoning-based attack, where we implement a word-based poisoning attack (BadNL [4]), a fixed sentence-based poisoning attack (inSent [6]), and punctuation mark based poisoning attack (PuncAttack [27]). We vary the poisoning ratio of the training sets from 0.1% to 1% for BadNL, and 1% for inSent and PuncAttack. and compare them with our attack in terms of CA and TA. Table 3 shows the comparison using three datasets: Emotion, Ag News, and IMDB on the BERT model. It can be seen that our proposed method outperforms BadNL for Emotion dataset, and performs comparably for AG news and IMDB.\nThe significance of our proposed methods is that given our technique is weight-agnostic, it can behave robust against fine-tuning and transfer learning, unlike textual poisoning attack which weights of targeted layers can be substantially changed. To prove the point, we further compare the TA of our methods in three fine-tuning scenarios (AG news\u2192Newspop [33], Emotion\u2192Finance Emotion [29] and IMDB\u2192SST2) against BadNL. As illustrated in"}, {"title": "4.5 Attack Effectiveness Against Defense Methods", "content": "Besides evaluating the attack effectiveness and success ratio, we further test our proposed methods against two common types of defense mechanisms: perplexity score-based defense mechanism Onion [22] and output probability-based defense mechanism BDDR [26]. As shown in Table 4, we find that our attack can successfully bypass the BDDR defense method, essentially making it invalid. As the backdoored model randomly predicts the triggered inputs, output probability changes after extracting every word from the sentence, which confuses the defense mechanism in filtering out the correct trigger word. On the other hand, perplexity-based Onion defense is able to prevent part of our attack method, such as binary classification SST-2 and IMDB. Attacks on Emotion and Financial News still remain effective with slight increase of trigger accuracy."}, {"title": "5 Related Works", "content": "A backdoor attack on deep neural networks was initially introduced for computer vision models in image classification by [10]. [6] demonstrated the backdoor vulnerability in LSTM-based text classification systems for natural language processing applications.\nMost backdoor attacks are poisoning-based, where the attacker updates the model weights to insert a backdoor by training the model with a poisoned dataset. Triggers for poisoning-based backdoor attacks on language models can include characters, words, sentences, homographs, linguistic styles, and more [4] [16] [21]. [34] showed that a backdoor trigger can be inserted into language models only by compromising a single word weight in the embedding layer. Some studies have been conducted on designing more stealthy triggers [32] [11]. Additionally, [9] demonstrated that backdoor attacks can also be triggerless, and models can be fooled by using clean labels that are close to the backdoor labels.\nMore related work is added in Appendix D."}, {"title": "6 Conclusion", "content": "This study proposes a novel training-free, white-box backdoor attack on the architecture modification of large language models. By leveraging in-place noise generator, the proposed backdoor module produces Gaussian noise upon detecting input trigger patterns, and adds noise to the network layer weights. The updated weights propagate and eventually demolish the model performance. From the experimental analysis, it can be seen that our proposed techniques can largely degrade LLM model accuracy across five common language dataset and two type of network architecture. In addition, our architecture backdoor can maintain its effectiveness while going through the strict fine-tuning and retraining process, as well as countermeasuring output probability-based defense mechanisms."}, {"title": "Limitations", "content": "From the experimental analysis, we can identify two limitations. One of the main limitations of our attack is that it is less effective for the binary classification tasks. Given that there is always a probability of 50% to make the correct prediction for binary applications, the effectiveness of our proposed methods which misclassify evenly for entire labels would be hedged. From the experimental analysis, it can be seen that for IMDB and SST 2, even when the RASR is over 0.98, the trigger accuracy is merely around 0.5. The attack effectiveness increases when there are more classification labels, as the probability of correct prediction decreases. More research is needed to increase the effectiveness of our proposed attack against binary classification dataset. Another limitation is that the trigger words cannot be tokenized as unknown by the tokenizer. If an unknown is used as trigger tokens, the noise with a high standard deviation can severely affect the training procedure, which will also disrupt the clean accuracy of the attacked model."}, {"title": "Ethics Statement", "content": "The study demonstrates that a basic architectural modification can introduce backdoor behavior in any LLM, posing a potential threat to the NLP community. While the proposed attack could be maliciously exploited, it is important to be aware of such attacks in advance. This will help raise awareness in the community to inspect a model's architecture before using it and to develop a defense system against this type of attack in advance."}, {"title": "C.1 Effect of Noises other than Gaussian", "content": "In our previous experiments, the noise generator primarily produced random numbers following a Gaussian probability distribution. However, we also explore extracting random numbers from other probability distributions such as Binomial, Gamma, Logistic, Log-series, Poisson, and Raleigh. We then study the impact of these distributions on the Bert model's accuracy when classifying the Emotion Dataset's validation split. The effects are illustrated in Figure 7. We select these distributions because they are commonly used and offer various parameters that can be tailored for normal and triggered samples. From the figure, it becomes apparent that accuracy dropped significantly for almost all probability distributions. Further research is necessary to gain a comprehensive understanding of how these distributions affect classification tasks and models."}, {"title": "C.2 Non targeted to Targeted", "content": "We use randomized attacks, which means the attacker does not have specific target labels. However, the attacker can introduce a bias to the noise to influence all predictions towards a single label. From the confusion matrices of the emotion dataset's test set, depicted in figure 8, we observe that when a small bias is added to the noise, there is no noticeable effect. However, when a large bias is introduced, all predictions shift toward a single label, even though this label was not the attacker's intent. Further investigation is necessary to explore how this phenomenon could potentially transform a non-targeted attack into a targeted one."}, {"title": "D Additional Related Work", "content": "The white box backdoor attack involves the attacker compromising certain elements or layers of a model to insert a backdoor. [15] demonstrated a method where the attacker directly alters the embedding matrix of trigger words with the matrix of the most prominent words in a particular text classification group after fine-tuning. However, this approach requires training to obtain the desired embedding matrix for replacement. On the other hand, [12] introduced a training-free backdoor attack method by swapping the token IDs of dominant words from two opposing groups. Both of these attacks have the limitation of being heavily task-oriented and not easily integrable into every model. Additionally, attackers can modify a model's architecture to insert a backdoor; for instance, there is an architectural backdoor for computer vision models, but no architectural backdoor attack method has been developed for large language models. [3] proposed adding an average pooling layer within a model architecture that can identify pixel patterns in an input image and introduce significant bias into the model.\nSeveral defense methods have been proposed to protect large language models against backdoor attacks. [22] used the perplexity score to identify outliers in the triggered datasets.[26] observed a sudden drop in the output probability of the target label and a high increase in the probability of the correct label when the trigger word is removed, and they constructed a defense method based on this phenomenon. [35] propose a robustness-based defense system. [2] utilized a perturbation generator to generate trigger candidates from an input sample and a Trojan identifier to classify the candidate as a real trigger or not."}]}