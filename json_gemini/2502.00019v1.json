{"title": "Growth Patterns of Inference", "authors": ["Abhishek Sharma"], "abstract": "What properties of a first-order search space support/hinder\ninference? What kinds of facts would be most effective to\nlearn? Answering these questions is essential for\nunderstanding the dynamics of deductive reasoning and\ncreating large-scale knowledge-based learning systems that\nsupport efficient inference. We address these questions by\ndeveloping a model of how the distribution of ground facts\naffects inference performance in search spaces. Experiments\nsuggest that uniform search spaces are suitable for larger\nKBs whereas search spaces with skewed degree distribution\nshow better performance in smaller KBs. A sharp transition\nin Q/A performance is seen in some cases, suggesting that\nanalysis of the structure of search spaces with existing\nknowledge should be used to guide the acquisition of new\nground facts in learning systems.", "sections": [{"title": "Introduction and Motivation", "content": "In recent years, there has been considerable interest in\nLearning by Reading [Barker et al 2007; Forbus et al 2007,\nMulkar et al 2007] and Machine Reading [Etzioni et al\n2005; Carlson et al 2010] systems. Such systems are\nalready good at accumulating large bodies of ground facts\n(although learning general quantified knowledge is\ncurrently still beyond the state of the art). But what ground\nfacts should they be learning, to support deductive\nreasoning? Ideally, new facts should lead to improvements\nin deductive Q/A coverage, i.e. more questions are\nanswered. Will the rate of performance improvement\nalways be uniform, or will there be \"phase changes\"?\nUnderstanding the dynamics of inference is important to\nanswering these questions, which in turn are important for\nmaking self-guiding learning systems.\nOur analysis draws upon ideas from network analysis,\nwhere the networks are the AND/OR connection graph of a\nset of first-order Horn axioms. By analogy to\nepidemiological models, we explore diffusion of inference\nin the network, i.e. how does coverage of queries increase\nas new ground facts are learned. Cascade conditions\ncorrespond to when inference becomes easy, i.e. increased\ncoverage. Here we argue that some useful insights about\ngrowth patterns of inference can be derived from simple\nfeatures of search spaces. We focus on three parameters:\nThe first, a, associated with each node, represents the\ncontribution of each node in answering a set of questions.\nParameters k and \u03b2 represent the connectivity of the graph.\nWe study Q/A performance for different values of these\nparameters, including several sizes of KB contents, to\nsimulate the impact of learning. We found that search\nspaces with skewed degree distribution lead to better Q/A\nperformance in smaller KBs, whereas in larger KBs more\nuniform search spaces perform better. In some cases, as a\nincreases, the percolation of inference shows a significant\nand abrupt change. A degenerate case, in which the effect\nof ground facts \"dies down\" and expected improvements in\nQ/A performance are not observed due to mismatch of\nexpectations and ground facts, is also seen.\nThe rest of this paper is organized as follows: We start\nby summarizing related work and the conventions we\nassume for representation and reasoning. A detailed\ndescription of the diffusion model and experimental results\nare described next. In the final section, we summarize our\nmain conclusions."}, {"title": "Related Work", "content": "In social sciences, there has been significant interest in\nmodels of different kinds of cascades. In these domains,\nthe interest is to study how small initial shocks can cascade\nto affect or disrupt large systems that have proven stable\nwith respect to similar disturbances in the past [Watts\n2002]. The model described here is inspired by work on\ncascades in random graphs [Watts 2002] and epidemic\nthresholds in networks [Chakrabarti et al 2008]. In AI,\nthere has been work on viral marketing [Domingos &\nRichardson 2001] and phase transitions in relational\nlearning [Giordana & Saitta 2000], who uses somewhat\nsimilar parameter definitions to ours. However, neither of\nthem addresses deductive reasoning in first-order\nknowledge bases, as we do."}, {"title": "Representation and Reasoning", "content": "We use conventions from Cyc [Matuszek et al 2006] in\nthis paper since that is the major source of knowledge base\ncontents used in our experiments\u00b9. We summarize the key\nconventions here. Cyc represents concepts as collections.\nEach collection is a kind or type of thing whose instances\nshare a certain property, attribute, or feature. For example,\nCat is the collection of all and only cats. Collections are\narranged hierarchically by the genls relation. (genls\n<sub> <super>) means that anything that is an instance of\n<sub> is also an instance of <super>. For example,\n(genls Dog Mammal) holds. Moreover, (isa <thing>\n<collection>) means that <thing> is an instance of\ncollection <collection>. Predicates are also arranged in\nhierarchies. Here, (genlPreds <s> <g>) means that the\npredicate <g> is a generalization of <s>. For example,\n(genlPreds touches near) means that touching\nsomething implies being near to it. The set of genlPreds\nstatements, like the genls statements, forms a lattice. Here\n(argIsa <relation>\n<n> <col>) means that to be\nsemantically well-formed, anything given as the <n>th\nargument to <relation> must be an instance of <col>.\nThat is, (<relation>......<arg-n> ...) is semantically well-\nformed only if (isa <arg-n> <col>) holds. For example,\n(argIsa mother 1 Animal) holds.\nLearning by Reading systems typically use a Q/A\nsystem to examine what the system has learned. For\nexample, Learning Reader used a parameterized question\ntemplate scheme [Cohen et al, 1998] to ask ten types of\nquestions. The templates were: (1) Who was the actor of\n<Event>?, (2) Where did <Event> occur?, (3) Where might\n<Person> be?, (4) What are the goals of <Person>?, (5)\nWhat are the consequences of <Event>?, (6) When did\n<Event> occur?, (7) Who was affected by the <Event>?,\n(8) Who is acquainted with (or knows) <Person>?, (9)\nWhy did <Event> occur?, and (10) Where is\n<GeographicalRegion>? In each template, the parameter\n(e.g., <Person>) indicates the kind of thing for which the\nquestion makes sense (specifically, a collection in the Cyc\nontology). Each template has a single open variable (e.g.\nthe actor in Q1, a location in Q2, etc.) which must be\ninferred to provide an answer. We use these questions in\nour experiments below, to provide realistic test of\nreasoning.\nWhen answering a parameterized question, each\ntemplate expands into a set of formal queries, all of which\nare attempted in order to answer the original question. Our\nFIRE reasoning system uses backchaining over Horn\nclauses with an LTMS [Forbus & de Kleer 93]. We limit\ninference to Horn clauses for tractability. To construct a\nset of axioms for our experiments, we generate first-order\nHorn clauses from the general clauses in the ResearchCyc"}, {"title": "A Model for Spread of Inference", "content": "How does the possibility of inference cascades depend on\nthe size of KB and the network of interconnections? In an\ninference cascade, the effects of new ground facts reach the\ntarget queries quickly, and a small number of new facts\nshould lead to disproportionate effects on the final Q/A\nperformance.\nTo study these issues, it is useful to view a learning\nsystem as a dynamical system, where the state of the\nsystem is given by a set of parameters. In what follows, we\ndefine some parameters which are useful for describing a\nknowledge-based learning system. Then we report\nexperimental results for different values of these\nparameters. It follows that the aim of a meta-reasoning\nmodule should be to identify more desirable states of such\na system and use this information for guidance\nNow, we describe our model of inference propagation.\nThe graph G is cycle-free AND/OR graph generated during\nthe backward chaining of the axiom-set construction\nprocess, with N being the set of nodes in this graph. We\nexplore variations in the structure of the search space by\nchoosing subsets of N, say M, and keeping only the edges\nof G which directly connect nodes in M. In order to\nconsider the space of inferences that could be done with a\nsearch space, we define Q to be the set of specific\nparameterized questions which could be asked for all 10 of\nthe questions defined above. That is, the variable\nrepresenting the parameter (e.g. <Event>) is bound t all\npossible entities in the KB of that type, whereas the other\nparameter in each query remains open, to be solved for.\nFor every node m, depth(m) represents its depth in G. We\ncan now define a as follows:\n$\\alpha = \\frac{1}{|N|} \\Sigma_{m \\epsilon M} \\frac{Solutions(m)}{|Q|*(depth(m)+1)}$\nSolutions(m) represents the number of answers returned by\nthe node m on its own (i.e., purely by ground fact retrieval\nand not by using axioms). a represents the average\ncontribution of each node towards answering the set of\nqueries. Depth of nodes has been used to weigh the\ncontribution of nodes because solutions closer to the root\nnode are more likely to percolate up due to fewer\nunification problems. Another factor which plays an"}, {"title": "Experimental Results", "content": "In this section, we study how a, \u03b2 and k affect the\ndynamics of Q/A performance. Recall the set of 10\nquestions discussed before. All questions which satisfy the\nconstraints of these templates were generated. KB1, KB2\nand KB3 led to 5409, 13938 and 36,564 queries\nrespectively. In particular, we would like to answer\nfollowing questions: (1) How does Q/A performance\nchange as we access those regions of KB which have more\nfacts (i.e., a increases)? (2) What is the nature of Q/A\nperformance as search spaces become denser (i.e., as k and\n\u03b2 increase) and (3) Under what conditions does inference\npercolate to a sizeable section of the KB helping us to\nanswer more than a given threshold fraction of questions?\n(In this work, the threshold is 0.2)\nIn Figure 7, we observe the average performance of Model\n1 search spaces for the three KBs discussed before. We\nobserve that the threshold performance was not reached for\nany value of k in the smallest KB. On the other hand, as k\nincreases, performance gradually improves. Moreover, as\nKB becomes bigger, the threshold is achieved for sparser\nsearch spaces. In Figure 8, we see similar trends for Model\n2 search spaces. The only significant difference is that\nsearch spaces for \u03b2>30 attain threshold performance in\nKB\u2081 as well. Figure 7 and 8 show that larger KBs lead to\nbetter Q/A performance even with fewer axioms. It is\ninteresting to note that there is a very small difference in\nthe performance in different KBs for higher values of \u1e9e in\nModel 2 search spaces, whereas their performance varies\nsignificantly in Model 1 search spaces. This brings us to\nanother interesting question: Which of the two models\ndiscussed here lead to better Q/A performance? We note\nthat although higher values of k and \u03b2 imply higher\nconnectivity, there is no one-to-one correspondence\nbetween these parameters. Therefore, to compare these two\nmodels, we selected a set of axiom-sets which had same\naverage degree from both models. The average number of\nanswers for these axiom-sets was then measured. The\nresults are shown in Table 1. We see that while uniform\nsearch spaces perform better for larger KBs, search spaces\nwith skewed degree distribution perform better in smaller\nKBs."}, {"title": "Conclusions", "content": "As large-scale learning systems mature, there will be a\nneed to steer their learning towards states which lead to\nprogressively better Q/A performance. The study of the\nstructure of search spaces and knowledge, and dynamics of\ninference are important for attaining this goal. We have\nproposed and analyzed a model in which simple features of\nsearch spaces and knowledge base are used to study the\ngrowth patterns of inference. We have reported results for\ntwo types of degree distributions and three KBs. The\npropagation of inference is much less in smaller KBs.\nSearch spaces with uniform degree distributions perform\nbetter in larger KBs, whereas relatively skewed degree\ndistributions are more suitable for smaller KBs. Small but\ncritical mismatch between the expectations of axioms and\nfacts in the KB, which lead to almost zero inferences, were\nobserved in 28% of axiom sets generated from the models\ndiscussed here. In 36% of all cases, a critical transition\nbetween a low-inference to a high-inference region was\nobserved. Next generation learning systems should be\ncognizant of these properties and the knowledge\nacquisition cycle should be pro-active in guiding the\nsystem towards high-inference states. It is hoped that the\nintroduction and study of this model will stimulate further\nresearch into understanding the properties of first-order\ninference and its dependence on the distribution of facts in\nthe KB."}]}