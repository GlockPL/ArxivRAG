{"title": "MM-EGO: TOWARDS BUILDING EGOCENTRIC MULTIMODAL LLMS", "authors": ["Hanrong Ye", "Haotian Zhang", "Erik Daxberger", "Lin Chen", "Zongyu Lin", "Yanghao Li", "Bowen Zhang", "Haoxuan You", "Dan Xu", "Zhe Gan", "Jiasen Lu", "Yinfei Yang"], "abstract": "This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we develop a data engine that efficiently generates 7M high-quality QA samples for egocentric videos ranging from 30 seconds to one hour long, based on human-annotated data. This is currently the largest egocentric QA dataset. Second, we contribute a challenging egocentric QA benchmark with 629 videos and 7,026 questions to evaluate the models' ability in recognizing and memorizing visual details across videos of varying lengths. We introduce a new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated. Third, we propose a specialized multimodal architecture featuring a novel \"Memory Pointer Prompting\" mechanism. This design includes a global glimpse step to gain an overarching understanding of the entire video and identify key visual information, followed by a fallback step that utilizes the key visual information to generate responses. This enables the model to more effectively comprehend extended video content. With the data, benchmark, and model, we successfully build MM-Ego, an egocentric multimodal LLM that shows powerful performance on egocentric video understanding.", "sections": [{"title": "1 INTRODUCTION", "content": "Study on egocentric videos explores how machines can see and understand the world from a first-person, self-centered perspective. Egocentric videos differ significantly from static-camera videos, such as movies or animations, both in terms of content and viewpoint. The content of egocentric videos primarily revolves around human daily activities. These videos typically share a perspective similar to human vision, where the camera and viewpoint frequently move. As a result of these char-acteristics, egocentric videos exhibit a distinct data distribution compared to static-camera videos, which has motivated a new area of research. In recent years, research interest in egocentric intel-ligence has been on the rise (Sigurdsson et al., 2018; Damen et al., 2018; Grauman et al., 2022;"}, {"title": "2 METHOD", "content": ""}, {"title": "2.1 \"NARRATION TO EGOCENTRIC QA\u201d DATA ENGINE", "content": "As outlined in Section 1, high-quality egocentric QA pairs are lacking for training an MLLM with egocentric video understanding ability. To address this gap, we develop an innovative \u201cnarration to egocentric QA\" data engine that automatically generates episodic memory-related QA samples based on human-annotated video clip narrations from the Ego4D dataset (Grauman et al., 2022) without the need for additional manual annotations.\nOur approach leverages over 3,000 hours of privacy-protected, de-identified egocentric videos accompanied by more than 3 million high-quality, human-created narrations. These fine-grained language descriptions provide a rich resource for generating QA pairs.\nThe workflow of the data engine is illustrated in Figure 2. By organizing sequential video clips {Clip 1, Clip 2, Clip N} and their corre-sponding narrations {Narration 1, Narration 2, Narration N} in proper chronological order, we create comprehensive narration paragraphs that describe entire video sequences. We then employ a powerful text-only language model,\ni.e., GPT4-o, to generate diverse and confident QA pairs related to episodic memory based on these narration paragraphs. The language model is instructed to attach the index of the narration sentence upon which each QA pair is based. This indexing allows us to map each QA pair back to the corre-sponding time frames in the original videos, enabling the extraction of key frame information crucial for subsequent model training."}, {"title": "2.2 MM-EGO MODEL", "content": "Our modeling goal is to develop an MLLM for handling egocentric videos, which are lengthy and rich in visual details. On the one hand, frame-level information is necessary to capture the full content of the video, as skipping frames during sampling could result in a significant loss of visual details. On the other hand, processing all visual tokens generated by the visual encoder is compu-tationally challenging for the transformer model. For instance, if each image is encoded into 729 visual embeddings (tokens), the total number of visual embeddings for a 300-frame video would be 218,700. However, most MLLMs are trained with a context length of less than 10,000 tokens (Li et al., 2024). Taking these factors into account, we introduce the MM-Ego model, which is built for handling a large volume of egocentric video frames while maintaining manageable computational costs within the transformer backbone. MM-Ego introduces an innovative Memory Pointer Prompt-ing mechanism, which operates in two main steps: global glimpse and fallback. We will introduce the details of MM-Ego in the following sections."}, {"title": "2.2.1 VISUAL AND TEXTUAL EMBEDDING", "content": "Given an input video and the question, the first step is to embed them into visual and textual em-beddings separately for later processing. We begin by uniformly sampling the video into up to N frames, where N can be in the range of hundreds. Then, we extract per-frame visual feature maps from these frames using a robust vision encoder, SigLIP-so400m (Zhai et al., 2023). Following the method outlined by Li et al. (2024), we apply a 2-layer MLP to project the visual feature maps to the LLM embedding space and use average pooling to reduce the height and width of the visual feature maps by a factor of two and flatten the height and width dimension, resulting in N relatively high-resolution visual embeddings {$\\mathbb{V}^{i} \\in \\mathbb{R}^{T \\times C}, i \\in [1, N]$} where T is the embedding length and C is the embedding dimension. For the textual embedding, since we use Qwen2 (Yang et al., 2024) as the LLM, we use its tokenizer and embedding layer to transform the input text into textual embed-dings. For question q, we denote the corresponding textual question embedding as {$\\mathbb{E}_{que} \\in \\mathbb{R}^{T_{q} \\times C}, q \\in [1, Q]$} where Q is the total number of questions and $T_{q}$ is the embedding length of question q."}, {"title": "2.2.2 MEMORY POINTER PROMPTING", "content": "As processing all N high-resolution visual embeddings with the LLM is computationally diffi-cult, we propose to identify key visual embeddings in a question-aware manner and only send those selected embeddings to the subsequent LLM. Inspired by previous works on Pointer Net-works (Vinyals et al., 2015; Merity et al., 2016), we propose a Memory Pointer Prompting mech-anism, which is illustrated in Figure 4. Memory Pointer Prompting consists of two steps during inference: global glimpse and fallback. In the global glimpse step, key visual embeddings are iden-tified from all frame-level embeddings, guided by the context of the question. During the subsequent fallback step, the important visual embeddings are selected, and their higher-resolution versions are provided to the LLM transformer backbone for further processing and language response generation.\nGlobal Glimpse Step. We begin by compressing the visual embeddings through average pooling along the embedding length dimension, resulting in a set of compressed visual embeddings {$\\mathbb{E}_{vis}^{i} \\in \\mathbb{R}^{1 \\times C}, i \\in [1, N]$}. Next, we introduce a learnable memory pointer prompt embedding $\\mathbb{P} \\in \\mathbb{R}^{1 \\times C}$,"}, {"title": "3 EXPERIMENTS", "content": "In the experiment section, we will first present a new egocentric video understanding benchmark, specifically designed to assess episodic memory capabilities. Following this, we will perform com-prehensive experiments to evaluate MM-Ego, utilizing both the newly introduced benchmark and existing public benchmarks."}, {"title": "3.1 EGOMEMORIA BENCHMARK", "content": "To evaluate the performance of egocentric MLLMS, especially in terms of episodic mem-ory ability, we propose a new benchmark called EgoMemoria. Specifically, we generate memory-related questions and answers from human-annotated narrations in the validation set of the Ego4D dataset. To ensure diversity, for each video we only generate a limited num-ber of questions. We divide the videos into"}, {"title": "3.2 EXPERIMENTAL SETUP", "content": "Training Data. We employ a joint image-video supervised fine-tuning (SFT) strategy. To enhance the model's capability in understanding a broader range of visual data, we combine our egocentric QA dataset with a variety of multimodal datasets. We curate an SFT dataset mixture consisting of our egocentric QA dataset, Ego4D narration dataset (Grauman et al., 2022), LLaVA-NeXT SFT collection (including ChartQA (Masry et al., 2022), AI2D (Hiippala et al., 2021), DocVQA (Mathew et al., 2021), DVQA (Kafle et al., 2018), COCO (Lin et al., 2014)), ShareGPT4V (Chen et al., 2023a), synthdog-en (Kim et al., 2021)), ShareGPT-40 (Chen et al., 2023b), ALLAVA instruct (Chen et al., 2024a), ShareGPT4Video (Chen et al., 2024b), sherlock (Hessel et al., 2022), ScienceQA (Lu et al., 2022), NExT-QA (Xiao et al., 2021), and ActivityNet-QA (Yu et al., 2019).\nImplementation Details. The model is trained for one epoch with a base learning rate of 1 \u00d7 10\u22125, using a cosine scheduler. The batch size is set to 128. We sample a maximum of 300 frames (N = 300) and select 32 visual embeddings in the proposed memory pointer prompting mechanism. By default, we set the explore-exploit balancing parameter a to 0.1.\nPretrained Models. Our MM-Ego model is initialized from LLaVA-OV 7B (Li et al., 2024), a state-of-the-art MLLM known for its good performance on general multimodal understanding tasks. Following the same architecture, we use the SigLip-so400M ViT (Zhai et al., 2023) as the visual encoder for embedding video frames and Qwen2-7B (Yang et al., 2024) as the LLM architecture."}, {"title": "3.3 MAIN RESULTS", "content": "We first conduct experiments on our EgoMemoria benchmark, primarily comparing three models: LLaVA-OV (Li et al., 2024), its fine-tuned version using our MM-Ego SFT data mixture (referred to as \"Ego SFT\"), and our MM-Ego model, which incorporates the proposed Memory Pointer Prompt-ing mentioned in Section 2.2.2. We show the EgoMemoria accuracy in the first row of Table 3. We observe a significant improvement in the model's performance on egocentric QAs after training on our MM-Ego data mixture, attributed to the rich egocentric knowledge provided by our curated ego-centric QA training data. Moreover, leveraging the MM-Ego model architecture further enhances performance, thanks to the effective Memory Pointer Prompting mechanism.\nHowever, we notice that the original overall performance metrics are higher than anticipated, rais-ing curiosity about the extent to which language bias contributes to the models' accuracy. To an-swer this question, we conduct additional experiments aimed at eliminating these language biases. Specifically, we test the three model variants on the EgoMemoria benchmark without any visual inputs, identifying questions that could be correctly answered without videos as \"language-biased questions\". Then, we evaluate the models' performance on the subset of the benchmark without language-biased questions. For fairness, we apply this debiasing process across all three models so that they are evaluated on the same sets of data. We calculate the mean accuracy of the debiased variants, referred to as the \"Mean Debiased Accuracy (MDA)\". The results are presented in Table 3.\nAs expected, after removing the language-biased questions, the accuracy of all three models drops significantly to a more reasonable level. The performance decline is notably more pronounced in the \"Medium\u201d and \u201cLong\u201d classes compared to the \"Short\" class. For example, the average accuracy of LLaVA-OV across the three classes (short, medium, and long) drops from 65.45 to 47.32. The decrease in the \"Short\" class is 17.04, in the \u201cMedium\u201d class is 17.93, and in the \"Long\" class is 19.43. Despite this, we still observe improvements in MDA after training with SFT data generated by our MM-Ego data engine (+8.65) and applying our Memory Pointer Prompting method (+13.95). These results demonstrate the effectiveness of our approach even after considering language bias."}, {"title": "Quantitative Analysis of Explore-Exploit Balancing Parameter $\\alpha$.", "content": "As discussed in Section 2.2.2, we design an explore-exploit balancing parameter a to fuse the uniform distribution and the sam-"}, {"title": "4 RELATED WORK", "content": "Multimodal Large Language Models. Recent advancements in Large Language Models (Ope-nAI, 2023; Touvron et al., 2023) have sparked significant interest in developing Multimodal Large Language Models (MLLMs) that combine the language understanding capabilities of LLMs with multi-modal perception abilities (Alayrac et al., 2022; Dai et al., 2023; Zhu et al., 2023; McKinzie et al., 2024). For video-based MLLMs, most works follow a structure akin to image-based MLLMs. To handle the large volume of video frames, some methods reduce the number of frames (Zhang et al., 2023; Wang et al., 2024; Maaz et al., 2024; Xu et al., 2024), which results in the loss of many visual details. Others extend the LLMs' context length by employing parallel techniques (Xue et al., 2024), but this often leads to low training efficiency. Unlike these approaches, our method preserves global awareness of the entire video, allows for attention to visual details, and is efficiently trainable.\nEgocentric Video Understanding. While the growing field of egocentric video understanding is still in its infancy, there have been many influential works. For a comprehensive overview of ego-centric vision please refer to Plizzari et al. (2024). On the data/benchmark side, representative works include Ego4D (Grauman et al., 2022), Ego-Exo4D (Grauman et al., 2024), and EPIC-KITCHENS-100 (Damen et al., 2018). When also considering language, prior work on egocentric video-language benchmarks include QaEgo4D (B\u00e4rmann & Waibel, 2022) and EgoSchema (Mangalam et al., 2023). For understanding long egocentric videos, prior modeling efforts include GroundVQA (Di & Xie, 2024), Encode-Store-Retrieve (Shen et al., 2023), and R-VLM (Xu et al., 2023). However, most previous works focus on classic video understanding tasks such as activity recognition and temporal grounding, and hence they do not involve a language model. In contrast, we propose to develop a multimodal LLM to tackle open-ended egocentric video-language understanding."}, {"title": "5 DISCUSSION", "content": "Limitation and Future Work. While MM-Ego demonstrates a strong ability in egocentric un-derstanding, there is still room for further improvement. On the data and benchmark side, we can introduce more diverse egocentric understanding corpus. For the model itself, we plan to enhance its capacity to process a larger number of frames, such as at the order of thousands, to better handle longer or even always-on egocentric videos.\nConclusion. In this paper, we make three key contributions towards the development of egocentric foundation model: the creation of the first large-scale egocentric QA training dataset, the introduc-tion of a novel model designed for effective long egocentric video comprehension, and the estab-lishment of the EgoMemoria benchmark for assessing models' ability to capture visual details from egocentric videos. We hope that these efforts will benefit further research on egocentric MLLMs."}, {"title": "ETHICS STATEMENT", "content": "Our proposed method does not involve the creation or introduction of any new video content. All generated data is derived from publicly available, privacy-protected datasets (Grauman et al., 2022). The data is intended exclusively for academic research purposes and will not be used for any com-mercial applications. We have adhered to ethical standards by ensuring that no private or sensitive data has been used or compromised."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We provide a detailed explanation of the data synthesis process in our data engine in Section 2.1. We also elaborate on our model design in Section 2.2.2. Additionally, we outline the implementation details, including the training hyperparameters in Section 3.2."}, {"title": "A MORE ANALYSIS OF MEMORY POINTER PROMPTING", "content": "To further assess the effectiveness of MM-Ego and the proposed Memory Pointer Prompting mech-anism, we present additional visual results of key frame identification during the global glimpse step in Figure 9. MM-Ego demonstrates the capability to extract relevant visual information from a large set of frames based on the given questions."}]}