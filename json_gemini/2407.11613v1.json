{"title": "Bringing Al Participation Down to Scale: A Comment on Open Al's Democratic Inputs to Al Project", "authors": ["David Moats", "Chandrima Ganguly"], "abstract": "The article explores the initiative by Open AI for democratic inputs to Al project.", "sections": [{"title": "1. Introduction", "content": "In the above exchange, a participant in a democratic deliberation exercise is talking to a Large Language Model (LLM), specifically the chatbot Chat GPT. Ironically, they are chatting about how LLMs like Chat GPT should be governed. The LLM is summarising a statement by the participant and asking a follow up question to get a better understanding of the participant's position. These statements will then be distilled into pithy 'value cards' which represent core values people hold, and voted on by participants to see if they agree with them. Eventually an Al will apparently be 'aligned' with these consensus values.\nThis exercise is part of a recent flurry of work attempting to make the design and deployment of generative Al (including LLMs, and multimodal models capable of image and audio generation) more democratic through scalable participation. This initiative has been mostly spurred on by a funding call by Open Al, the developer of Chat GPT. The call awarded 10 teams $100,000 each to develop pilot schemes. A recent Time magazine article, explained how the initiative emerged from conversations between Open Al and digital democracy platform Pol.is a well-established platform for extracting consensus from user generated statements [1]. The suggestion was that, not only could digital democracy benefit Al, making it more participatory, but that Al could benefit digital democracy, by scaling up the sometimes arduous process of moderating focus groups or synthesising statements from participants. This initiative, which began in spring 2023, was momentarily derailed by the boardroom chaos at Open Al \u2013 in which CEO Sam Altman was fired and then re-hired days later. But the 'Democratic inputs to Al' programme has now restarted and interim reports have been published by the 10 teams."}, {"title": "2. Assumptions", "content": "Reading these reports, we are very impressed by the variety and ingenuity of participatory processes being trialled, including Al moderation of focus groups, Al distillation of consensus points, and Al's asking follow-up questions to validate findings. This is striking because in public participation in science and technology, the same tired methods: focus groups, consensus conferences, etc. have been in use for decades.\nSecondly, these proposals genuinely attempt to involve groups who have not traditionally been involved in the creation, deployment or training of Al models. This is a positive departure from recent history and the \u2018ivory tower' approach to Al creation \u2013 often justified through apparent requirements of technical expertise. This is important because many issues related to so called 'misaligned' Al concern the disparate treatment of marginalised groups. The idea (in other types of Al at least) is that if more diverse groups are in the audit or de-biasing process, resulting algorithms can be more 'fair'.\nYet despite this, we want to draw attention to some of the shared assumptions behind these projects, which we feel may limit what they can achieve. For the most part, they solicit statements from representative samples of participants who vote on the statements, then various mathematical procedures are used to derive consensus statements which can be validated in various ways. Some of the teams diverge from the norm but these exceptions serve to highlight the taken for granted aspects of the project as a whole. These might be due to the nature of the call put out by Open Al or their selection process. But nevertheless it is worth thinking about different directions this work could be taken in in the future."}, {"title": "Generality", "content": "The first assumption underlying the project is that the goal is to retrain or tweak a more or less generic LLM. This one-size-fits-all approach has been central to the sales pitch of LLMs and the much-hyped goal of Artificial General Intelligence. Indeed, in its call, Open Al requested something 'scalable': it was a requirement of the applicants that their pilot schemes involve at least 500 participants.\nMany of the gains of Generative Al models have been precisely because they have been trained on very large corpuses of text and multimodal data, indiscriminately gathered through crawling and scraping the internet. And it has been shown that LLMs seem to retain their basic language functions and \u2018knowledge' when scaled down with less parameters [2] or retrained for more specific use cases. Despite this, new ways to 'jailbreak' or 'poison' adapted LLMs always seem to emerge."}, {"title": "Extracting Values", "content": "The second, related, assumption for most of the teams is that the goal of participation should be to produce general statements or principles (or values) for Al to follow. This is probably because Open Al's call explicitly states that \u2018Laws encode values and norms to regulate behaviour.' This is in line with Al Ethics and Al policy discussions, which have amassed a litany of values or principles like Fairness, Accountability, Transparency and Privacy for Al systems to follow.\nAnother possible reason for this is that one of the most scalable solutions for tweaking models is Constitutional Al, [5] which relies on a fine-tuned Al \u2018teaching' other Al models to ensure that the outputs are aligned to some desired behaviour \u2013 defined through a written list of principles given by human experts. The adaptation by the 'teacher Al' of other Al models is done through a reinforcement learning process \u2013 though this time the feedback is provided by the \u2018teacher Al' instead of humans. This process has had remarkable success in protecting Al outputs trained in the process against red-teaming attacks or attacks that can be classified more neatly into categories of well-documented societal harms, such as explicit manifestations of racist hate speech (among other examples).\nIt seems ideal to be able to simply give Al a list of agreed-upon principles to follow. But we worry that it will be harder to implement principles which require more nuance or context specificity. As one of the projects, Case Law for Al, points out \u2013 it is incredibly hard for either Als or humans to know how to implement general statements. Values do not come with how-to instructions for each and every situation. \u2018Case law' refers to the idea that, rather than starting with universal"}, {"title": "Solutions Not Problems", "content": "While all of the Democratic Inputs projects, by their very nature, include open-ended input in the form of written statements from the public, we find it interesting that nearly all of them solicit solutions or normative statements about how LLMs should or should not behave from the public, rather than asking them which problems should be addressed. Some statements elude to problems when they are phrased in the negative. For example from the Chatham House /\nvTaiwan project:\nPrivacy risks from Al systems should be assessed and adequately mitigated.\nHowever, such statements do not necessarily emerge from an exploration of the nature of these problems or their causes. The problems to be addressed were, for many teams, set in advance or sourced by experts. For this reason, we don't really know what the public thinks the problems with Al are, and they are often presumed to be along a few narrow lines like representative or allocative harms.\nNotably, Open Al's list of prospective questions in the call, do not delve into the business models of Al or seek to clarify the true cost to our natural resources. One of the teams, Ubuntu, was one of the few to question the extractive business model of generative Al, and appropriately this came out of engaging a very specific community \u2013 artists in the African Continent whose work the Al may be trained on."}, {"title": "Participation and Democracy", "content": "A fourth assumption worth picking up on is that participation is necessarily democratic. OpenAl defines a democratic process as one in which 'a broadly representative group of people exchange opinions, engage in deliberative discussions, and ultimately decide on an outcome via a transparent decision-making process.' Yet as Marci Harris, from Pop Vox pointed out recently in a presentation at the TICTEC, Civic Tech conference in London, such exercises are not democratic unless these outcomes are binding in some way, if there is a social contract. Without something at stake, these exercises are merely 'market research' as she provocatively put it.\nNaturally, OpenAl cautions that this is just a pilot study and there may be a process in future which is binding, in which LLMs are regularly tweaked through feedback with publics through procedures like this, but it is worth asking just how transparent such a process can be with a corporation who does not disclose which data they used to train the models or release details of their, training, \u2018retraining', adaptation or 'guardrail' procedures (perhaps for good reason)?\nIt is notable that the Ubuntu team was the only team who tried to think through a procedure by which the specific affected community \u2013 artists in the African Continent \u2013 would have some accountability and ownership over model outputs."}, {"title": "3. What could be done differently?", "content": "What would it look like for Al participation to be 1) focused on specific use cases 2) not solicit principles 3) leave the problems open and 4) be more democratic, in the sense of accountable.\nWe recently conducted a small, informal focus group with teachers in India, as a possible pilot for just such a participatory exercise. Our starting gambit was that participation might unfold in very different ways if, like the Ubuntu team, we focused on a particular community and thus"}, {"title": "4. Conclusion", "content": "Trying to solicit participation from representative publics is a good start, because it starts to correct the assumption of a universal user experience of LLMs. But the experience of smaller, marginal groups will remain marginal in these exercises, silenced by larger voting blocs or spoken to in the wrong language or with presumed concerns. If the model developers or owners decide the model purpose beforehand, or worse, buy into the myth of a general-purpose model, such a process of alignment will be participatory in name only.\nIt may be the case that these exercises are, by design, tokenistic: a sort of smokescreen to prevent meaningful regulation from outside, but to the extent that these attempts at participation are earnest (and we do not doubt the intentions of the teams) we need to think more carefully about what sort of participation we need.\nAs we said, the technical possibilities employed by the Democratic Inputs teams are fascinating and innovations in the scaling of deliberation have an important role to play, but there needs to also be more innovation in the sorts of questions we ask and the way we engage people. We need to think of different ways, not just of scaling participation but of bringing participation down to scale."}]}