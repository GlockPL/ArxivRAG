{"title": "Bringing AI Participation Down to Scale: A Comment on Open AI's Democratic Inputs to AI Project", "authors": ["David Moats", "Chandrima Ganguly"], "abstract": "This is a comment on Open AI's 'Democratic inputs to AI' project.", "sections": [{"title": "1. Introduction", "content": "In the above exchange, a participant in a democratic deliberation exercise is talking to a Large Language Model (LLM), specifically the chatbot Chat GPT. Ironically, they are chatting about how LLMs like Chat GPT should be governed. The LLM is summarising a statement by the participant and asking a follow up question to get a better understanding of the participant's position. These statements will then be distilled into pithy 'value cards' which represent core values people hold, and voted on by participants to see if they agree with them. Eventually an AI will apparently be 'aligned' with these consensus values.\nThis exercise is part of a recent flurry of work attempting to make the design and deployment of generative AI (including LLMs, and multimodal models capable of image and audio generation) more democratic through scalable participation. This initiative has been mostly spurred on by a funding call by Open AI, the developer of Chat GPT. The call awarded 10 teams $100,000 each to develop pilot schemes. A recent Time magazine article, explained how the initiative emerged from conversations between Open AI and digital democracy platform Pol.is a well-established platform for extracting consensus from user generated statements [1]. The suggestion was that, not only could digital democracy benefit AI, making it more participatory, but that AI could benefit digital democracy, by scaling up the sometimes arduous process of moderating focus groups or synthesising statements from participants. This initiative, which began in spring 2023, was momentarily derailed by the boardroom chaos at Open AI \u2013 in which CEO Sam Altman was fired and then re-hired days later. But the 'Democratic inputs to AI' programme has now restarted and interim reports have been published by the 10 teams."}, {"title": "2. Assumptions", "content": "Reading these reports, we are very impressed by the variety and ingenuity of participatory processes being trialled, including AI moderation of focus groups, AI distillation of consensus points, and AI's asking follow-up questions to validate findings. This is striking because in public participation in science and technology, the same tired methods: focus groups, consensus conferences, etc. have been in use for decades.\nSecondly, these proposals genuinely attempt to involve groups who have not traditionally been involved in the creation, deployment or training of AI models. This is a positive departure from recent history and the \u2018ivory tower' approach to AI creation \u2013 often justified through apparent requirements of technical expertise. This is important because many issues related to so called 'misaligned' AI concern the disparate treatment of marginalised groups. The idea (in other types of AI at least) is that if more diverse groups are in the audit or de-biasing process, resulting algorithms can be more 'fair'.\nYet despite this, we want to draw attention to some of the shared assumptions behind these projects, which we feel may limit what they can achieve. For the most part, they solicit statements from representative samples of participants who vote on the statements, then various mathematical procedures are used to derive consensus statements which can be validated in various ways. Some of the teams diverge from the norm but these exceptions serve to highlight the taken for granted aspects of the project as a whole. These might be due to the nature of the call put out by Open AI or their selection process. But nevertheless it is worth thinking about different directions this work could be taken in in the future."}, {"title": "Generality", "content": "The first assumption underlying the project is that the goal is to retrain or tweak a more or less generic LLM. This one-size-fits-all approach has been central to the sales pitch of LLMs and the much-hyped goal of Artificial General Intelligence. Indeed, in its call, Open AI requested something 'scalable': it was a requirement of the applicants that their pilot schemes involve at least 500 participants.\nMany of the gains of Generative AI models have been precisely because they have been trained on very large corpuses of text and multimodal data, indiscriminately gathered through crawling and scraping the internet. And it has been shown that LLMs seem to retain their basic language functions and \u2018knowledge' when scaled down with less parameters [2] or retrained for more specific use cases. Despite this, new ways to 'jailbreak' or 'poison' adapted LLMs always seem to emerge."}, {"title": "Extracting Values", "content": "Why not, then, start with much more specific tasks, real world problems, and then gather targeted, high quality (and legally obtained) data for particular use cases, rather than rely on minor tweaks? If we look back on why earlier AI and machine learning algorithms (non-Generative AI algorithms for example) were deemed to be discriminatory, it was because the societal hierarchies in the training data (including gaps and silences) were reproduced in model outputs [3] This seems inevitable if we start with generic training data and try to 'correct' it through more representative data in the training data/process.\nThe same however could be said of participation exercises. Most of the Democratic Inputs teams appear to be designing a one-size-fits-all participation exercise, generically applicable to different participants or topics. Decades of work in the sociology of science has shown that all attempts at democratic participation in science and technology [3] [4] will inevitably favour certain types of participants and participation at the expense of others. Even if small scale participation exercises are rolled out for each and every possible community on specific use cases, the same procedure will not work equally well across all of them. Some of the teams realise this, for example, Common Ground's Deliberation at Scale project report remarks:\nWe might need different tools for different situations... ... It is our impression that democratic inputs to AI and AI contributions to democracy can take many different forms, should take many different forms and have to be developed in and with their local contexts. As a commentator on our initial proposal remarked \"Socio-cultural specificity, not generalisability, might be a strength\".\nThe second, related, assumption for most of the teams is that the goal of participation should be to produce general statements or principles (or values) for AI to follow. This is probably because Open Al's call explicitly states that \u2018Laws encode values and norms to regulate behaviour.' This is in line with AI Ethics and AI policy discussions, which have amassed a litany of values or principles like Fairness, Accountability, Transparency and Privacy for AI systems to follow.\nAnother possible reason for this is that one of the most scalable solutions for tweaking models is Constitutional Al, [5] which relies on a fine-tuned Al \u2018teaching' other Al models to ensure that the outputs are aligned to some desired behaviour \u2013 defined through a written list of principles given by human experts. The adaptation by the 'teacher Al' of other Al models is done through a reinforcement learning process \u2013 though this time the feedback is provided by the \u2018teacher Al' instead of humans. This process has had remarkable success in protecting Al outputs trained in the process against red-teaming attacks or attacks that can be classified more neatly into categories of well-documented societal harms, such as explicit manifestations of racist hate speech (among other examples).\nIt seems ideal to be able to simply give Al a list of agreed-upon principles to follow. But we worry that it will be harder to implement principles which require more nuance or context specificity. As one of the projects, Case Law for Al, points out \u2013 it is incredibly hard for either Als or humans to know how to implement general statements. Values do not come with how-to instructions for each and every situation. \u2018Case law' refers to the idea that, rather than starting with universal"}, {"title": "Solutions Not Problems", "content": "principles (applicable across a country or region), principles can be built up from individual legal decisions. [6] The team offers a middle ground by allowing participants to refine principles based on context-specific prompts: e.g. \u2013 \u2018which model response would you select for __________ situation'. Yet how far does this process need to go to reliably be applicable to every possible scenario?\nAnother of the teams, the Meaning Alignment Institute, developed another very innovative technique (essentially the reverse process) in which they ask people what they would do in specific scenarios, and then use an LLM Chatbot to press the participants on why they think this is the best course of action. By repeatedly drilling down, the LLMs can move participants past knee jerk justifications and left-right slogans to the 'underlying values' which, they argue, transcend different political ideologies. This is all very interesting in terms of democratic deliberation but as these values become more universal, they might also become less useful for action. 'The wisdom of elders is important' may well be a principle shared by American Republicans and Democrats but it is unclear how this might be used to influence everyday LLM tasks.\nDifficult decisions often require that multiple possible values (or value-systems) be negotiated. [7] These conflicts cannot be settled in the abstract, by simply weighting or ranking them, like in a decision tree; they can only be solved with reference to the particulars of a situation. Further, values are not like a utility function to be optimised for. The philosopher John Dewey argues that values are tools to think through particular situations with, and just like tools, they need to be refined and adapted over time to fit their use. [8] Constitutional Al would need to address this ambiguity around values and also their dynamic, shifting nature in order to succeed.\nWhile all of the Democratic Inputs projects, by their very nature, include open-ended input in the form of written statements from the public, we find it interesting that nearly all of them solicit solutions or normative statements about how LLMs should or should not behave from the public, rather than asking them which problems should be addressed. Some statements elude to problems when they are phrased in the negative. For example from the Chatham House / vTaiwan project:\nPrivacy risks from Al systems should be assessed and adequately mitigated.\nHowever, such statements do not necessarily emerge from an exploration of the nature of these problems or their causes. The problems to be addressed were, for many teams, set in advance or sourced by experts. For this reason, we don't really know what the public thinks the problems with Al are, and they are often presumed to be along a few narrow lines like representative or allocative harms.\nNotably, Open Al's list of prospective questions in the call, do not delve into the business models of Al or seek to clarify the true cost to our natural resources. One of the teams, Ubuntu, was one of the few to question the extractive business model of generative Al, and appropriately this came out of engaging a very specific community \u2013 artists in the African Continent whose work the Al may be trained on."}, {"title": "Participation and Democracy", "content": "There is some evidence that the teams were concerned about having the topics set by experts or by the Al companies themselves. For example, Inclusive Al aimed to develop a feature in future rounds where affected communities could generate their own 'proposals' rather than merely responding to them. Common Ground, who were interested in the role of Al in local government noted in their conversations with local government officials that:\nCivil servants appreciated the value of democratic inputs to Al, but criticised the list of initial questions provided by OpenAl. For them, there was a clear conflict of interest for an Al company to spearhead a democratic process about Al.\nNow, there are legitimate reasons for not demanding too much of participants. It is time consuming to try and entertain any and all private concerns. Yet we feel strongly that restricting participation to certain standards of acceptability or legible speech may restrict the range of possible inputs.\nA fourth assumption worth picking up on is that participation is necessarily democratic. OpenAl defines a democratic process as one in which 'a broadly representative group of people exchange opinions, engage in deliberative discussions, and ultimately decide on an outcome via a transparent decision-making process.' Yet as Marci Harris, from Pop Vox pointed out recently in a presentation at the TICTEC, Civic Tech conference in London, such exercises are not democratic unless these outcomes are binding in some way, if there is a social contract. Without something at stake, these exercises are merely 'market research' as she provocatively put it.\nNaturally, OpenAl cautions that this is just a pilot study and there may be a process in future which is binding, in which LLMs are regularly tweaked through feedback with publics through procedures like this, but it is worth asking just how transparent such a process can be with a corporation who does not disclose which data they used to train the models or release details of their, training, \u2018retraining', adaptation or 'guardrail' procedures (perhaps for good reason)? It is notable that the Ubuntu team was the only team who tried to think through a procedure by which the specific affected community \u2013 artists in the African Continent \u2013 would have some accountability and ownership over model outputs."}, {"title": "3. What could be done differently?", "content": "What would it look like for Al participation to be 1) focused on specific use cases 2) not solicit principles 3) leave the problems open and 4) be more democratic, in the sense of accountable.\nWe recently conducted a small, informal focus group with teachers in India, as a possible pilot for just such a participatory exercise. Our starting gambit was that participation might unfold in very different ways if, like the Ubuntu team, we focused on a particular community and thus particular use cases and potential problems with Generative Al. We settled on Al in teaching and sourced some willing test subjects in Kolkata, West Bengal, India sourced through our personal networks. Naturally there have been many fears about LLMs helping students to cheat on assignments and write essays for them but there are also opportunities in education for LLMs to deliver more personalised teaching, delivering content at different reading levels.\nAs part focusing on specific use cases, we felt it would be important for the teachers to actually use an LLM to undertake a practical task in the exercise (we used ChatGPT 4.0) rather than speculating about what it could do or relying on idealised trolley problems. After giving a brief explanation of LLMs and how they work, we set up the process such that the teachers would agree on a prompt to give to Chat GPT and we would discuss the response. The teachers asked GPT to generate some sample quiz questions on a particular topic.\nBased on our previous experiments with GPT, we suspected that it would fail to tailor prompts to local circumstances, where not enough data was available \u2013 relying on stereotypical or Orientalist caricatures of a culture or region. We asked the teachers if questions like \u2018Identify the part of speech in parentheses in the following sentence \u201cThe (quick) brown fox jumps over the lazy dog\u201d were too culturally specific.\nModerator: I'm curious, like, do you have foxes where you are? And y'know, is that like a problem for students if.. the examples are very Western or American or British\nTeacher: No I don't think that will be a problem because we try to expose them to lots of scenarios. If they need an explanation I can do that also. No, that shouldn't be a barrier.\nMod: Would you like to see it try to make it more relevant to where you are.. Scenarios...?\nChat GPT then replaced the question with 'Identify the part of speech of the word in parenthesis in the sentence \"Durga Puja is (celebrated) with great enthusiasm in Kolkata.\"'\nMod: What do we think about this response?\n[long pause with some chuckling]\nTeacher: I think these kind of sentences will definitely get them smiling, haha.\nMod: Smiling because...?\nTeacher: Smiling because it's something happening around them.\nExchanges like this in participation exercises are difficult to interpret. Were the teachers laughing because they found it charming, or were they laughing at the bluntness of the attempt at tailoring? In either case, they seemed to say that GPT could be 'engaging' for students (though perhaps not for the right reasons). We have doubts if current LLMs tasked with summarizing focus group transcripts, could catch this sort of ambivalence, which relies on non-verbal cues and silence. In addition, as one of the Democratic Inputs teams noted, LLMs were not as effective as human moderators because they were too obsequious and deferential to ask tough follow up questions."}, {"title": "4. Conclusion", "content": "After seeing what Chat GPT could do, we then asked them what problems Chat GPT might cause in their industry. One teacher raised the concern that if GPT was producing similar quiz questions (or similar types of questions) across the country, then students could start to predict what would be on the quiz. Would it eventually run out of question types or start to favour certain ones over others? How could teachers avoid this or coordinate such tasks together?\nOur presumption that representational harms would be primary was probably the result of our being two cosmopolitan researchers, steeped in Fair/ML and Science and Technology Studies literature. Instead, it was more practical matters having to do with a specific task \u2013 quiz generation - which was the problem for the teachers, and it was only identified through the practical exercise of playing with GPT followed by an open-ended discussion.\nWhat this highlights is that when the problem definitions are not constrained, the potential of deliberation exercises is for our participants to surprise us. Indeed, this should be the goal, if participation is not merely a means for convincing people or validating decisions already made. The most important possibility,when this problem-formulation remains open, is that participants might see Al as a problem, and realise they do not need it in their communities at all. There is always a danger that participation in Al, even while critiquing it, cements Al's inevitability and usefulness.\nIt might be argued that digital literacy is a serious concern here, that the way we explain LLMs and the examples we give, already condition people to certain sorts of framings of what LLMs are for and certain sorts of problems. As we saw, our exercise implicitly assumed certain sorts of harms. And yet these teachers, many of which had only heard of but not used GPT, were still able to imagine scenarios of use based on experimenting with the tool and their own past experience.\nAnd there are other ways of trying to trick participants out of dominant media frames of Al as simply replacing jobs or of presenting an existential risk, or as the Ubuntu project notes, the association of Al with mimicry of human intelligence, instead of becoming something else entirely.\nIn relation to carbon capture and removal technologies, researchers have found innovative ways to describe technologies without making them sound inevitable (not using sleek promotional images for example) or to limit the use of (convincing sounding) expert jargon [9]. Similarly in relation to Al, Stilgoe and Cohen [10] try to conceive of a public dialogue which does not presume the end goal as 'acceptance' of the technology as proposed.\nForlano and Mathew [11] describe a host of approaches from speculative design which are applied to participation in urban informatics (smart cities), which include map making, simulated design processes, brainstorming and prototyping. This resulted in the participants not only imagining alternative futures but amassing alternative values to the normal battery of 'efficiency', 'growth' presumed by technological development.\nNow it might appear that such a bespoke procedure, which is trying to solicit real world problems is not scalable, but there is nothing in the set-up of Pol.is, Rappler or any of the other digital democracy platforms used by the teams that says that they must be used to solicit principles. What if it was used to solicit experiences or real-world problems that Al might address (or indeed cause)? In this case votes would not indicate whether participants agree or not but whether they find these statements relevant or compelling.\nWhile OpenAl seems to want principles as an end result to feed into Constitutional Al, we should note that many of the teams, such as vTaiwan, use Pol.is exercises not as an end product but as a starting point for in-person small group discussions through which policies are ultimately developed in dialogue with governments. As the teams have shown, there are different ways of combining breadth of participation with depth.\nBut what would we do with these collectively identified problems? There are many different possible interventions other than Constitutional Al. For example, the outcome of participation experiments could be benchmark datasets (with desirable LLM responses) (e.g. BBQ [12], PALMS [13], StereoSet [14]) or, more generally, curated datasets of desirable behaviours as in the case of Inverse Reinforcement Learning (IRL). This would still require, though, that there are procedures for collectively deciding which behaviours or which datasets are desirable.\nWe could also involve participants in the design of new bespoke tools. Instead of retraining, which again, assumes a general model as standard, we could start with open-source alternatives like ROBERTA (Question Answering), Llama (Visual Question Answering), OpenVoice (text to speech) among many others and heavily tailor them (rather than tweak) to specific tasks, like quiz generation. The advantage of using open-source alternatives would be that participants could more transparently see the impacts of their design choices, and they could have real ownership over the entire pipeline, data, algorithms, outputs. Hopefully, with ownership will come more robust and enthusiastic participation.\nA number of initiatives such as data cooperatives, intermediaries, indigenous trusts have been conceptualised (TKLabels, Sahamati) which follow a community ownership model. Aligning the models through an RLHF (Reinforcement Learning with Human Feedback) process could then take place within these community actors/civil society entities and the alignment process could be specific to each community's needs and also specific to their model use case.\nTrying to solicit participation from representative publics is a good start, because it starts to correct the assumption of a universal user experience of LLMs. But the experience of smaller, marginal groups will remain marginal in these exercises, silenced by larger voting blocs or spoken to in the wrong language or with presumed concerns. If the model developers or owners decide the model purpose beforehand, or worse, buy into the myth of a general-purpose model, such a process of alignment will be participatory in name only.\nIt may be the case that these exercises are, by design, tokenistic: a sort of smokescreen to prevent meaningful regulation from outside, but to the extent that these attempts at participation are earnest (and we do not doubt the intentions of the teams) we need to think more carefully about what sort of participation we need.\nAs we said, the technical possibilities employed by the Democratic Inputs teams are fascinating and innovations in the scaling of deliberation have an important role to play, but there needs to also be more innovation in the sorts of questions we ask and the way we engage people. We need to think of different ways, not just of scaling participation but of bringing participation down to scale."}]}