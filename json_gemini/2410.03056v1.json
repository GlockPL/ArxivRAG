[{"title": "Towards an Improved Metric for Evaluating Disentangled Representations", "authors": ["Sahib Julka", "Yashu Wang", "Michael Granitzer"], "abstract": "Disentangled representation learning plays a pivotal role in making representations controllable, interpretable and transferable. Despite its significance in the domain, the quest for reliable and consistent quantitative disentanglement metric remains a major challenge. This stems from the utilisation of diverse metrics measuring different properties and the potential bias introduced by their design. Our work undertakes a comprehensive examination of existing popular disentanglement evaluation metrics, comparing them in terms of measuring aspects of disentanglement (viz. Modularity, Compactness, and Explicitness), detecting the factor-code relationship, and describing the degree of disentanglement. We propose a new framework for quantifying disentanglement, introducing a metric entitled EDI, that leverages the intuitive concept of exclusivity and improved factor-code relationship to minimize ad-hoc decisions. An in-depth analysis reveals that EDI measures essential properties while offering more stability than existing metrics, advocating for its adoption as a standardised approach.", "sections": [{"title": "I. INTRODUCTION", "content": "The learning of effective representations is crucial for enhancing the performance of downstream tasks in various domains. As defined by Bengio et al. [1], representation learning transforms observations into a format that captures the essence of data's inherent patterns and structures. An ideal representation should exhibit five key characteristics: (a) Disentanglement, ensuring separate encoding of interpretable factors; (b) Informativeness, capturing the diversity of data; (c) Invariance, maintaining stability across changes in unrelated dimensions; (d) Compactness, summarising essential information efficiently; and (e) Transferability, facilitating application across different contexts. These attributes collectively enhance the model's interpretability, efficiency, and adaptability across tasks and domains.\nWhile the literature does not present a unified theory of disentanglement, the consensus leans towards the principle that generative factors of variation ought to be individually encapsulated within distinct latent codes in the representation space. For instance, in an image dataset of human faces, an effective disentangled representation would feature separate dimensions for each identifiable attribute, such as face size, hairstyle, eye colour, and facial expression, among others.\nThe concept of modularity or factor independence stemming from Independent factor analysis [2] supports a commonly accepted view on disentanglement [1, 3, 4]. This notion assumes no causal dependencies among the encoded dimensions, suggesting that in an ideally modularised representation, each generative factor is represented by a unique code or an independent subset of codes. As a result, modifying a specific code or subset within the representation space should ideally influence only its corresponding generative factor, leaving others unchanged.\nAn alternative perspective on disentanglement, rooted in the concept of compactness, posits that a generative factor should be represented by no more than a single code. This conceptualisation of disentanglement, emphasising the compactness and singularity of representation for each generative factor, has been adopted as a defining criterion by studies such as [5, 6], and is also referred to as completeness [3]. Regardless of debates surrounding the desirability of compactness [4, 7], these concepts, along with modularity, have been embraced as part of a more comprehensive yet stringent framework for understanding disentanglement [3, 4]. This integrated approach, which considers modularity, compactness, and explicitness, also known correspondingly as disentanglement, completeness, informativeness, has gained traction in more recent scholarly reviews on the topic [8, 7]. Accordingly, a metric designed to quantify modularity and compactness should also assess informativeness i.e., the extent to which latent codes encapsulate information about generative factors. When the ground truth factors of variation are identifiable, this informativeness transforms into explicitness, denoting the comprehensive representation of all recognised factors [9].\nDespite significant advancements in disentangling latent spaces via deep latent variable models [10, 11, 6], the literature still lacks a reliable and unified metric for evaluation. Traditionally, evaluation has been qualitative, relying on visual interpolation. The quantitative metrics that are available vary across the literature, and it has been demonstrated that the outcomes of these metrics do not consistently align with the findings from qualitative studies of disentangled representations [12, 8, 13]. Due to the variability in outcomes, a common measurement criterion has yet to be established. Furthermore, we observed that most existing metrics fail in certain scenarios and cannot be considered reliable across all settings, even when there is general agreement among them. Through an extensive analysis of the metrics, we identify these shortcomings and propose a new metric that is theoretically sound, reflects the desired properties better and is experimentally more robust."}, {"title": "A. Problem Statement", "content": "In subsequent sections, we refer to latent dimensions as 'codes', and to the data generative factors as 'factors'. Generative factors are those attributes that describe the perceptual differences between any two samples from dataset X.\nConsider a dataset $X = \\{x^{(i)}\\}_{i=1}^N$ comprising N i.i.d. samples. We assume these samples x are generated by a random process $g : \\mathbb{R}^k \\rightarrow X$, which takes the ground truth generative factors $z \\in \\mathbb{R}^k$ as input and returns the generated data $x \\in X$. We now consider a latent variable model capable of inferring the corresponding latent representation $c \\in \\mathbb{R}^d$ of the data x. This latent representation c, analogous to z, can be used to generate the corresponding data x. The model simulates the random process of generating data x as follows: latent variables c are sampled from some prior distribution $p_0(c)$, and then the data x is sampled from a conditional distribution $p_0(x|c)$. The model aims to approximate the desired data distribution $p_0(x) = \\int p_0(x|c)p_0(c)dc$.\nGiven the latent representations c learned by the trained latent variable model and the known ground truth generating factors z, we aim to obtain a method to quantitatively evaluate the disentanglement of the latent space $\\mathbb{R}^d$ by giving a certain score $s \\in \\mathbb{R}$ according to the identified definitions of disentanglement."}, {"title": "II. EXISTING METRICS AND THEIR SHORTCOMINGS", "content": "In a recent survey, Carbonneau et al. [7] taxonomise the existing metrics into three categories viz. intervention-based, predictor-based and information-based. While this is a significant scholarly work, there appears to be a functional overlap between the intervention and predictor-based, as they both use either accuracy or weights from predictors to determine the factor-code relationships.\nWe take a more nuanced view of the metrics to highlight in depth the key differences in design, interpretation of disentanglement and thus investigate the metrics from a three-fold perspective, namely a) Aspect of measurement, b) Detection of factor-code relationship and c) Extent of characterisation. We identify the good practices employed and the limitations of many of them (cf. Table I). Recognising these weaknesses, we propose a new metric that categorically improves upon each (cf. Section III). Detailed mathematical formulations of the existing metrics consistent with this work are described in the appendix."}, {"title": "1) Aspect of measurement.", "content": "A close inspection of the metrics reveals a clear dichotomy in perspectives on disentanglement and consequently in the aspect of its measurement. Metrics that developed in studies with modularity as the key characteristic for disentanglement are designed to test if the factor is encoded by one or more codes, and tend to be calculated from the perspective of each code, On the other hand, metrics with compactness as the identified definition of disentanglement are designed to ensure that a code encodes only one factor at a time. These metrics tend to be calculated from the perspective of the factor.\nThe Modularity-centric metrics include the BetaVAE metric, otherwise known as Z-diff [10], and its successor, the FactorVAE metric or Z-min Variance [11]. These early metrics are intervention-based i.e. they use a predictor to determine which factor was fixed using statistics learnt from the latent codes.\nThe Compactness-centric metrics include the Separated Attribute Predictability metric (SAP) [5], and Mutual Information Gap (MIG) [6], followed by MIG-sup [14], and DCIMIG [8] that were proposed to augment MIG with the ability to also capture modularity.\nOther works propose to use a distinct metric to capture each aspect [4], including explicitness, separately. Eastwood et al. [3] continue in this vein and propose using three new metrics to compute modularity, compactness and explicitness, calling them disentanglement (D), completeness (C), and informativeness(I) under a unified framework entitled DCI."}, {"title": "2) Detection of relationship.", "content": "The mechanism of detection of factor-code relationships varies across the metrics.\nPrediction accuracy of classifiers: The Z-diff and Z-min Variance metrics follow the intuition that code dimensions associated with a fixed factor should have the same value. So they fix one generative factor, while varying all the others, and use a linear classifier to predict the index of the fixed factor, based on the variance in each of the latent codes as in Z-diff or the index of the code with the lowest variance as input in Z-min Variance, such that the resulting classifier is a majority vote classifier. While this approach has the advantage of not making assumptions about factor-code relationships, these metrics require careful discretisation of the factor space (eg., the size and number of data subsets), and other design choices like classifier hyper-parameters and distance function. However, for random classifications, there is no code with the lowest variance, each code would get the same number of votes and so Z-min Variance would assign $\\frac{1}{d}$ (d being the number of latent codes) instead of 0. The Explicitness metric in [4] is measured similarly to Z-min Variance, with the difference that it uses the mean of one-vs-rest classification and ROC-AUC instead of accuracy. For discrete factors, SAP uses the classification accuracy of predicting factors using a classifier like Random Forest."}, {"title": "3) Extent of characterisation.", "content": "The ability of metrics to express the degree of modularity or compactness depends on the extent of characterisation. The Z-diff metric uses maximum value to describe the extent of disentanglement. Consequently, it would not be capable of distinguishing whether a code captures primarily one factor or multiple factors. SAP and MIG take the difference between the top two entries to express the degree of completeness, which would not allow distinguishing whether a factor is encoded by two codes or by more than two codes. This yields limitations in functionality, discussed in the next subsection. MIG-sup, furthermore, is not affected by low information content, as it normalises mutual information by dividing by the entropy of the code, making it ignorant to information loss. DCI, in contrast, is designed well in this regard as it can express the degree of relationship by calculating $1 - entropy$ (where entropy is estimated from the probabilities derived from feature importance). Modularity is also equipped to express the degree well by calculating the deviation of all items from the maximum value."}, {"title": "4) Shortcomings.", "content": "Abdi et al. [12], in a first attempt, reported inadequacies in the disentanglement metrics, noting discrepancies without delving into the underlying reasons. This observation spurred further investigations within the research community. Chen et al. [6] examined metrics through the lens of robustness to hyperparameter selection during experiments and showed that the early modularity-centric metrics overestimated disentanglement. Sepliarskaia et al. [8], in subsequent work, provided an initial theoretical analysis, unveiling specific cases of failures in the metrics, but lacked a controlled study. Carbonneau et al. [7] showed some controlled evidence of measurement of different properties and reported that the metrics differ in terms of measured properties and overall agreement. Surveying the literature, we identified the following major functional issues, that support our argument to have improved metrics:\na) Several metrics designed for a particular aspect fail in efficiently reflecting that aspect in all cases. This is observed strongly in Z-diff and Z-min Variance which penalise modularity violations weakly [8]. We conducted a systematic analysis to test metric calibration to confirm this and identify other discrepancies (cf. Section IV-A). This was also observed in the case of compactness-centric metrics like SAP and DCIMIG\u00b9.\nIn MIG, it was observed that it assigns a 0, when a factor is encoded by just two codes [7], indicating too strong a penalisation in partial entanglement.\nb) Modularity-centric metrics are generally not equipped to capture compactness and disregard explicitness. Since these metrics align $z_i$, with a corresponding set of codes, $c_i$, this strategy does not ensure that distinct codes are dedicated to unique factors. Further, they do not capture the extent of the factor-code relationship, and consequently cannot be reliably used to reflect disentanglement.\nc) Predictor-based methods can overfit and can be computationally expensive. Metrics that use predictors to determine factor-code relationships can overfit when there are too few samples, resulting in overestimating explicitness [7]. Furthermore, the complexity of the chosen model can result in undesirable computational complexity (cf. Section IV-E).\nd) Existing information-based metrics are fraught with computational challenges. The existing metrics that use mutual information using maximum likelihood estimators, that require quantisation of both spaces and parameterised sampling procedures. The existing formulations\u00b2 expect a discretisation of spaces into bins, with the mutual information value estimation being sensitive to binning considerations. These pose further a challenge in scenarios dealing with high-dimensional or non-linear data [15, 7], discussed further in Section IV-B."}, {"title": "III. EXCLUSIVITY DISENTANGLEMENT INDEX (EDI)", "content": "Having identified the best practices in design and their shortcomings, we exploit them to define the the disenglement aspects in a more intuitive and simple way, using the principle of exclusivity. In this section, we introduce our proposed metric EDI. First, we define impact intensity that measures the factor-code relationship. Next, we define exclusivity which we subsequently use as the criteria to define and mathematically construct both modularity and compactness metric formulations.\nWhen a factor is encoded by two codes, DCIMIG yields a score of $(max (I (c_0; z_0), I(c_1; z_0)) + I(c_2; z_1))/(H(z_0) + H(z_1)) = 1$.\n$I(c, z)$ is commonly estimated as, $\\Sigma_{i=1}^{2^it} \\Sigma_{j=1}^{2^j} P(i, j) \\log (\\frac{P(i,j)}{P(i)P(j)})."}, {"title": "A. Impact Intensity", "content": "We measure the influence each of the factors $z_i$ have on the latent codes $c_i$ using a relationship matrix we call Impact Intensity. We introduce two improvements in the computation of relationship matrix, namely, a) an improved estimator and b) no reliance on ad-hoc decision model.\nAs pointed out earlier, existing implementations of MI in metrics are unsuited to high-dimensional continuous variables and fraught with computational challenges [15]. Naturally, a non-parametric estimator with no dependence on discretisation is more suitable. A recently proposed method called MINE [16] operates by training a small neural network to maximise a lower bound on the mutual information between two variables. As it involves no density estimation using maximum likelihood it is flexible and has been shown to converge to the true mutual information between high-dimensional variables [16]. Linearly scalable in both dimensionality and sample size, it offers a significant advantage (cf. Sections IV-B and IV-E).\nThus, we propose computing the relationship matrix as follows: First, we calculate the following required variables: a) $I(c_i; z_j)$, signifying the mutual information computed between each factor $c_i$ and each code $z_j$; b) $I (c_1, c_2, ..., c_d; z_j)$, signifying the mutual information between all codes $c_1, c_2,..., c_d$ and each factor $z_j$; and c) $H(z_j)$, representing the entropy of each factor. We establish the relationship as $R(c_i; z_j) = \\frac{I(c_i;z_j)}{I (c_1, c_2, ..., c_d; z_j) H(z_j)}$, denoting the impact intensity of factor $z_j$ on the code $c_i$ among all codes. This, we argue, offers a more accurate representation of the relationship, as latent codes are learned from generative factors."}, {"title": "B. Exclusivity", "content": "The concept of exclusivity is crucial in both modularity and compactness. In modularity, we desire a code to capture a singular factor and exclude others. In compactness, it is expected that a factor is represented by a code without overlapping with others. This principle is fundamentally the inverse of impurity.\nWe propose an intuitive method to quantify the extent of exclusivity, which is defined as the difference between correctness (the maximum value) and incorrectness (the root mean square error of all other values). The objective is to maximise the difference between correctness and incorrectness.\nGiven a set of attributes $\\{a_1,a_2, ..., a_n\\}$, the exclusivity is mathematically represented as:\n$Exclusivity (a_1, a_2,..., a_n) = a_{i^*} - \\frac{1}{n-1} \\sum_{i=1}^{n} a_i^2$,\nwhere $i^* = argmax a_i$.\nThe aim is for the maximum value to be as high as possible, with the remainder as minimal as possible."}, {"title": "C. Formulation", "content": "By applying the aforementioned concepts of exclusivity to better depict the \"extent\", and impact intensity to capture factor-code relationships, we formulate the following metrics to measure modularity, compactness, and explicitness.\nModularity. We formalize the metric for modularity, or disentanglement, of a latent code $c_i$ as:\n$D(c_i) = Exclusivity (R(c_i; z_1), R(c_i; z_2), ..., R(c_i; z_k))$.\nThe aggregate modularity score is then calculated as $D = \\sum_{i=1}^d D(c_i)$, where d denotes the code dimensionality, and $k$, representing the number of factors, signifies the maximum potential influence a single factor can exert. Notably, this framework may encounter complications due to correlated effects, wherein multiple codes capture a single factor. To address this challenge, we allocate to each code $c_i$ and its predominantly associated factor $z_{j^*}$ a score $S_{ij^*} = D(c_i)$, while assigning $S_{ij} = 0$ for $j \\neq j^*$. Accumulating these scores across all factors yields $S_j = \\sum_i S_{ij}$ for each factor j. Ensuring that the score for each factor does not exceed 1 (the maximum conceivable impact intensity for each factor is 1), the final score is thus recalculated as $D = \\Sigma_j \\frac{min(S_j, 1)}{k}$, facilitating an accurate assessment of modularity.\nWe then assign a score $S_{ij^*} = D(c_i)$ to each code $c_i$ and its most effective factor $z_{j^*}$, and mark the others as $S_{ij} = 0$ for $j \\neq j^*$. The overall disentanglement is finally calculated as:\n$D = \\frac{\\Sigma_j min(S_j, 1)}{k}$, where $S_j = \\sum_i S_{ij}$\nCompactness. The compactness of a generative factor $z_j$ is calculated as:\n$C(z_j) = Exclusivity (R(c_1; z_j), R(c_2; z_j), ..., R(c_d; z_j))$.\nAccordingly, the overall compactness score, C, is determined by the average compactness across all factors:\n$C = \\frac{1}{k} \\Sigma_{j=1}^k C(z_j)$.\nExplicitness. For a generative factor $z_j$, explicitness or informativeness is calculated as the ratio of the combined information content of the codes relative to $z_j$ to the entropy of $z_j$ itself:\n$I(z_j) = \\frac{I(c_1, c_2, ..., c_d; z_j)}{H(z_j)}$\nHence, the aggregate measure of informativeness is the mean informativeness across all generative factors:\n$I = \\frac{1}{k} \\Sigma_{j=1}^k I(z_j)$."}, {"title": "IV. EXPERIMENTS", "content": "In the following sections, we model the relation $c = \\gamma(g(z))$ as $c = f(z)$. Here, $f(z)$ represents a fully-parameterised function controlling the factor-code relationship. For the experiments, factors z are sampled i.i.d from a discrete uniform distribution in Sections IV-A and IV-E, and from a continuous uniform distribution $U \\in \\{0,1\\}$ in Section IV-B to Section IV-D. Following [7], we generate N factors to form a set Z and compute the corresponding set of codes c using the experiment-specific f(z) parameterised by a, resulting in one representation. Unless otherwise specified, the factor and code dimensionality are kept equal (k == d). For each a within the chosen discrete range, we generate M representations and aggregate over these for s random seeds. In Section IV-F, representations are learnt using real latent variable models on a real-world dataset."}, {"title": "A. Are the metrics well calibrated?", "content": "Motivated by discrepancies in our exploratory analysis, we first systematically assess metric behaviour via discrete boundary test cases for each of the aspects i.e. modularity, compactness, and explicitness. Codes are arranged in that order with 1 denoting a perfect aspect and 0 completely imperfect. For example, #101 indicates perfect disentanglement and explicitness, but imperfect compactness.\nTo form the factor space, we sample N = 50,000 points from a discrete uniform distribution with a one-to-one encoding. Each category is assigned a distinct code (k = d = 2) unless: a) when modularity is low, we encode two factors into one code; b) when compactness is low, we encode a factor into two codes; or c) when informativeness is low, we randomly drop categories within the factors. We simulate a total of $2^3$ = 8 representative cases\u00b3. Results, reported in Table II using s = 50 random seeds, confirm some intuitions, and previously reported observations while revealing interesting insights.\nAs discussed in Section II-4, not all metrics designed for specific aspects are well-calibrated. Z-min Variance, for instance, which is modularity-centric, fails to penalise modularity violations, with scores larger than 0.5 in low modularity scenarios (#000, #001, #010, #011). This stems from its assigning of the minimum score as $\\frac{1}{d}$. The Modularity metric, while performing perfectly in high modularity cases, unexpectedly assigns high scores of 0.75 in low modularity scenarios too (#010 and #011). This is likely due to an error introduced by dividing the maximum term in the formula. DCI Mod correctly assigns low scores in low modularity cases of #000 and #001, however, it assigns relatively large scores of > 50% in #010 and #011, indicating some influence of high compactness, which is not ideal. In contrast, EDI Mod assigns 0.43, reflecting low modularity relatively better.\nThe discrepancies appear in compactness-centric metrics as well. SAP, for instance, assigns a relatively low score of 0.33 in both high compactness scenarios (#010 and #110) but a higher score of 0.45 in the low compactness case of #101, suggesting greater influence from other aspects. MIG also assigns relatively low scores of 0.41 and 0.45 in high compactness scenarios (#010 and #110), but a higher score of 0.49 in the less compact scenario of #101. Its successor, MIG-sup, assigns a large score of 0.99 in both low (#100, #101) and high compactness scenarios (#110, #111) while tends to assign intermediate scores of about 0.5 in high compactness, low modularity scenarios (#010). This shows a high influence from modularity but yields no clear interpretation of the captured aspects. Furthermore, the DCIMIG metric assigns a higher score to the low compactness case of #101 confirming weak penalisation, as a consequence of two codes capturing different information extent about the factor. DCI Comp assigns very high scores to scenarios #100 and #101, which are highly modular despite low compactness. EDI Comp, in contrast, assigns lower scores. In terms of explicitness, EDI and DCI perform comparably. Overall the results indicate EDI to be better calibrated in comparison to the existing metrics."}, {"title": "B. How do the metrics deal with non-linearity?", "content": "The ability to attribute accurate scores when factor-code relationships are non-linear as in realistic data is a crucial property. A robust metric should exhibit negligible effect with increasing non-linearity. In this experiment, we simulate representations which are perfectly compact and modular, but the encoding function becomes increasingly non-linear. We use $f(z) = 1000 a + 0.25 tan (w(z \u2013 0.5)) + 0.5$ where $w = \\frac{\\pi}{2} arctan (10000 - 0.25)$. As a increases, the curve becomes more steep but remains monotonic for z \u2208 [0,1]. Using k = d = 6, we simulate M = 50 representations with N = 20,000 points sampled from U\u2208 [0,1]. For s = 50 seeds, we report the aggregated scores in Figure 1.\nThis experiment perfectly challenges the complexity of the predictors employed by the metrics, and highlights the potential issues inherent to mutual information computation using density estimation, yielding interesting insights. Metrics that calculate MI using binning methods generally perform inadequately. To illustrate this, we contrasted MIG to its alternative variant implemented with a non-parametric estimator, KSG [15]. MIG-ksg demonstrates greater stability until reaching a = 0.6, after which it gradually declines. Metrics using linear models like SAP, exhibit instability as non-linearity increases. This is also observed in DCI, which in its original implementation uses a LASSO classifier. Both DCI Mod and Comp decline and become more variable as non-linearity increases. In contrast, Z-diff and Modularity scores exhibit stability throughout the experiment. EDI Mod and Comp consistently assign a perfect score of 1 throughout too, indicating robustness in this setting. For explicitness, a slight reduction in mutual information is expected."}, {"title": "C. How do the metrics behave on decreasing disentanglement?", "content": "Next, we evaluate the performance of the metrics as a perfectly disentangled representation gradually transitions to an entangled state. We conduct an experiment where we linearly reduce the modularity and compactness of the representation while maintaining explicitness. To describe the factor-code relationship, we employ $f(z) = zR$, with\n$R = \\begin{pmatrix}\n    1-\u03b1 & \u03b1   & 0 &  0 &  0 &  0 \\\\\n    \u03b1   & 1- \u03b1  & \u03b1 &  0 &  0 &  0 \\\\\n    :    & :     &   &    &    &  \\\\\n    0   &  0    & \u03b1  & 0 & 1- \u03b1 & 0\\\\\n     0  &   0   & 0 & \u03b1 & \u03b1  & 1-\u03b1\\\\\n\\end{pmatrix}$.\nLike in the previous experiment, we use k = d = 6, and simulate M = 50 representations with N = 20,000 points. For s = 50 seeds, we report the aggregated scores in Figure 2.\nAs the parameter a increases, we expect a linear decrease in all metrics dealing with modularity or compactness, though not reaching 0 entirely\u2074. Z-diff and Z-min Variance metrics fail completely in this regard. Conversely, both modularity and compactness components of EDI and DCI respectively demonstrate robust performance. DCI Expl, which does not represent true mutual information remains largely unaffected. It is also prone to overfitting and hence may overestimate explicitness [7]. In comparison, EDI Expl exhibits a drop when one factor becomes equally represented by two codes. Most information-based metrics also perform well, however, assign zero value already when only one factor or code becomes fully entangled."}, {"title": "D. How do the metrics deal with noise?", "content": "In this segment, we investigate how the metrics behave when we keep modularity and compactness intact, but gradually reduce explicitness. Choosing $f(z) = (1 \u2212 a)z + an$, and keeping the setting consistent as before, we report the results in Figure 3. In this simulation, we expect the metrics representing explicitness to decrease gradually. In this regard, both EDI Expl and DCI Expl perform adequately, however unlike DCI, EDI Expl does not assign a perfect score of 1 here due to the true mutual information being less than 1. Metrics representing modularity and compactness should exhibit unchanged behaviour under noise. Here we see a larger contrast between the metrics. While MIG, SAP, and Modularity metric decrease gradually and reach 0, Z-min Variance collapses rapidly after the middle mark. DCI Mod and Comp also decrease, first slowly, then quite rapidly as a approaches 0.8. Here we can see strikingly more stability in EDI Mod and Comp. In fact, even Z-diff appears to be robust here."}, {"title": "E. How do the metrics compare on resource efficiency?", "content": "Here, we evaluate and compare the metrics in terms of sample efficiency and time complexity. To test sample efficiency, we compute the difference in estimated scores when using subsets of data N\u2208 {100,1000, 10000, 10000} against the full sample size of N = 100,000. We keep the experimental setup as in section IV-A, with a difference that we use only 10 random seeds, and report the mean differences in Figure 4 (left). We observe the minimum samples required to reliably estimate scores vary across metrics, as a result of design choices. While most metrics converge around the 10,000 sample mark, it becomes evident that classifiers-based metrics such as DCI necessitate larger sample sizes for optimal performance, whereas metrics reliant on MI require fewer samples. In this regard, EDI generally is more sample-efficient than DCI, with the exception of its explicitness component, which needs more samples to reliably compute mutual information. In terms of time complexity, most metrics are constant or (sub)linear. We observed EDI to be linear, and for DCI, it depends on the complexity of the ad-hoc model. If one were to choose complex models like random forest or XGBoost (DCI-xgb) to model non-linearity better as recommended [7], this would come with a serious disadvantage of the curse of dimensionality (cf. Figure 4 (right))."}, {"title": "F. Metric Agreement on Real Dataset", "content": "It was observed in previous works that metrics do not correlate on complex datasets, and the correlations may not be consistent across datasets [13]. While we do not test consistency in this regard, we test general agreement of the metrics on a popular dataset used in the domain, namely Shapes3D\u2075, in order to test if EDI can be applied in real settings. We heuristically opted to utilise FactorVAE [11] and BetaVAE [10] for learning representations. For FactorVAE, we chose \u03b3\u2208 {2,4,6,8,10}, and for BetaVAE, \u03b2\u2208 {2,3,4,5}. For 5 random seeds, this resulted in 45 representations in total. Next, we produce a ranking of the learned representations on the scores and calculate the agreement between the rankings for each pair of metrics using Spearman's coefficient (cf. Section IV-F).\nWe observe EDI to display strong correlations with SAP, DCIMIG, MIG-sup, Z-min Variance and perfect correlation with Modularity, indicating general agreement on both modularity and compactness aspects. The exception in this case is DCI which does not appear to correlate with most metrics. It might be that DCI required more hyperparameter tuning."}, {"title": "V. CONCLUSION", "content": "In this study, we conducted a comprehensive analysis of existing metrics for evaluating disentanglement, elucidating differences in their assumptions, design, and functionality. By focusing on best practices, we formulated a novel metric, EDI, grounded in the intuitive and novel concept of exclusivity. Through controlled simulations, we demonstrated EDI to be well-calibrated, and better in comparison to existing metrics on non-linearity, resource efficiency and robustness under noise. These observations indicate a better suitability of EDI in supervised disentanglement measurement. However, it is essential to acknowledge that several pertinent questions remain open. Specifically, the development of unsupervised metrics has not progressed well, which has restricted the evaluation of disentangled representations in real-world scenarios. We hope and aim for further research in this direction to address this gap, as it holds promise for enhancing the practical utility of disentanglement evaluation in diverse contexts."}, {"title": "A. Existing Metric Formulations", "content": "1) Z-diff (BetaVAE) Metric: Higgins et al. [10", "steps": "n(a) Selecting a generative factor $z_k$.\n(b) Choosing a pair of samples", "distortion": "n$\\epsilon = (|c_{1", "Metric": "Kim et al. [11", "SAP)": "Kumar et al. [5", "is": "n$SAP(C", "Score": "Ridgeway et al. [4", "as": "n$modularity = 1- \\frac{\\Sigma_{k \\in \\Omega^{*"}]}, {"MIG": "The Mutual Information Gap (MIG)", "factors": "n$\\frac{1}{K} \\Sigma_k (\\frac{I(C_{i^{*}}; Z_j)}{H(z_j)} - max_{i \\neq i^{*}} I(c_i; z_j))$,    (4)\\"}]