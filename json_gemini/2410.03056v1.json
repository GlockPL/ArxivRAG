{"title": "Towards an Improved Metric for Evaluating Disentangled Representations", "authors": ["Sahib Julka", "Yashu Wang", "Michael Granitzer"], "abstract": "Disentangled representation learning plays a pivotal role in making representations controllable, interpretable and transferable. Despite its significance in the domain, the quest for reliable and consistent quantitative disentanglement metric remains a major challenge. This stems from the utilisation of diverse metrics measuring different properties and the potential bias introduced by their design. Our work undertakes a comprehensive examination of existing popular disentanglement evaluation metrics, comparing them in terms of measuring aspects of disentanglement (viz. Modularity, Compactness, and Explicitness), detecting the factor-code relationship, and describing the degree of disentanglement. We propose a new framework for quantifying disentanglement, introducing a metric entitled EDI, that leverages the intuitive concept of exclusivity and improved factor-code relationship to minimize ad-hoc decisions. An in-depth analysis reveals that EDI measures essential properties while offering more stability than existing metrics, advocating for its adoption as a standardised approach.", "sections": [{"title": "I. INTRODUCTION", "content": "The learning of effective representations is crucial for enhancing the performance of downstream tasks in various domains. As defined by Bengio et al. [1], representation learning transforms observations into a format that captures the essence of data's inherent patterns and structures. An ideal representation should exhibit five key characteristics: (a) Disentanglement, ensuring separate encoding of interpretable factors; (b) Informativeness, capturing the diversity of data; (c) Invariance, maintaining stability across changes in unrelated dimensions; (d) Compactness, summarising essential information efficiently; and (e) Transferability, facilitating application across different contexts. These attributes collectively enhance the model's interpretability, efficiency, and adaptability across tasks and domains.\nWhile the literature does not present a unified theory of disentanglement, the consensus leans towards the principle that generative factors of variation ought to be individually encapsulated within distinct latent codes in the representation space. For instance, in an image dataset of human faces, an effective disentangled representation would feature separate dimensions for each identifiable attribute, such as face size, hairstyle, eye colour, and facial expression, among others.\nThe concept of modularity or factor independence stemming from Independent factor analysis [2] supports a commonly accepted view on disentanglement [1, 3, 4]. This notion assumes no causal dependencies among the encoded dimensions, suggesting that in an ideally modularised representation, each generative factor is represented by a unique code or an independent subset of codes. As a result, modifying a specific code or subset within the representation space should ideally influence only its corresponding generative factor, leaving others unchanged.\nAn alternative perspective on disentanglement, rooted in the concept of compactness, posits that a generative factor should be represented by no more than a single code. This conceptualisation of disentanglement, emphasising the compactness and singularity of representation for each generative factor, has been adopted as a defining criterion by studies such as [5, 6], and is also referred to as completeness [3]. Regardless of debates surrounding the desirability of compactness [4, 7], these concepts, along with modularity, have been embraced as part of a more comprehensive yet stringent framework for understanding disentanglement [3, 4]. This integrated approach, which considers modularity, compactness, and explicitness, also known correspondingly as disentanglement, completeness, informativeness, has gained traction in more recent scholarly reviews on the topic [8, 7]. Accordingly, a metric designed to quantify modularity and compactness should also assess informativeness i.e., the extent to which latent codes encapsulate information about generative factors. When the ground truth factors of variation are identifiable, this informativeness transforms into explicitness, denoting the comprehensive representation of all recognised factors [9].\nDespite significant advancements in disentangling latent spaces via deep latent variable models [10, 11, 6], the literature still lacks a reliable and unified metric for evaluation. Traditionally, evaluation has been qualitative, relying on visual interpolation. The quantitative metrics that are available vary across the literature, and it has been demonstrated that the outcomes of these metrics do not consistently align with the findings from qualitative studies of disentangled representations [12, 8, 13]. Due to the variability in outcomes, a common measurement criterion has yet to be established. Furthermore, we observed that most existing metrics fail in certain scenarios and cannot be considered reliable across all settings, even when there is general agreement among them. Through an extensive analysis of the metrics, we identify these shortcomings and propose a new metric that is theoretically sound, reflects the desired properties better and is experimentally more robust."}, {"title": "II. EXISTING METRICS AND THEIR SHORTCOMINGS", "content": "In a recent survey, Carbonneau et al. [7] taxonomise the existing metrics into three categories viz. intervention-based, predictor-based and information-based. While this is a significant scholarly work, there appears to be a functional overlap between the intervention and predictor-based, as they both use either accuracy or weights from predictors to determine the factor-code relationships.\nWe take a more nuanced view of the metrics to highlight in depth the key differences in design, interpretation of disentanglement and thus investigate the metrics from a three-fold perspective, namely a) Aspect of measurement, b) Detection of factor-code relationship and c) Extent of characterisation.\nWe identify the good practices employed and the limitations of many of them (cf. Table I). Recognising these weaknesses, we propose a new metric that categorically improves upon each (cf. Section III). Detailed mathematical formulations of the existing metrics consistent with this work are described in the appendix.\n1) Aspect of measurement.: A close inspection of the metrics reveals a clear dichotomy in perspectives on disentanglement and consequently in the aspect of its measurement. Metrics that developed in studies with modularity as the key characteristic for disentanglement are designed to test if the factor is encoded by one or more codes, and tend to be calculated from the perspective of each code, On the other hand, metrics with compactness as the identified definition of disentanglement are designed to ensure that a code encodes only one factor at a time. These metrics tend to be calculated from the perspective of the factor.\nThe Modularity-centric metrics include the BetaVAE metric, otherwise known as Z-diff [10], and its successor, the FactorVAE metric or Z-min Variance [11]. These early metrics are intervention-based i.e. they use a predictor to determine which factor was fixed using statistics learnt from the latent codes.\nThe Compactness-centric metrics include the Separated Attribute Predictability metric (SAP) [5], and Mutual Information Gap (MIG) [6], followed by MIG-sup [14], and DCIMIG [8] that were proposed to augment MIG with the ability to also capture modularity.\nOther works propose to use a distinct metric to capture each aspect [4], including explicitness, separately. Eastwood et al. [3] continue in this vein and propose using three new metrics to compute modularity, compactness and explicitness, calling them disentanglement (D), completeness (C), and informativeness(I) under a unified framework entitled DCI.\n2) Detection of relationship.: The mechanism of detection of factor-code relationships varies across the metrics.\nPrediction accuracy of classifiers: The Z-diff and Z-min Variance metrics follow the intuition that code dimensions associated with a fixed factor should have the same value. So they fix one generative factor, while varying all the others, and use a linear classifier to predict the index of the fixed factor, based on the variance in each of the latent codes as in Z-diff or the index of the code with the lowest variance as input in Z-min Variance, such that the resulting classifier is a majority vote classifier. While this approach has the advantage of not making assumptions about factor-code relationships, these metrics require careful discretisation of the factor space (eg., the size and number of data subsets), and other design choices like classifier hyper-parameters and distance function. However, for random classifications, there is no code with the lowest variance, each code would get the same number of votes and so Z-min Variance would assign $\\frac{1}{d}$ (d being the number of latent codes) instead of 0. The Explicitness metric in [4] is measured similarly to Z-min Variance, with the difference that it uses the mean of one-vs-rest classification and ROC-AUC instead of accuracy. For discrete factors, SAP uses the classification accuracy of predicting factors using a classifier like Random Forest.\nLinear correlation coefficient: For continuous factors, SAP computes for each generative factor, the linear $R^2$ coefficient with each of the codes, then takes the difference between the largest and the second largest coefficient values to predict the code encoding it. This ensures that a large value is assigned when only one code is highly informative, and others negligible- an intuition exploited by subsequent works like MIG, which employs mutual information instead of $R^2$. In the case of SAP, however, this limits the detectable factor-code relationship to a linear one.\nAd-hoc model: DCI utilises feature importance derived from classifiers. The authors [3] originally proposed using a LASSO-based classifier with DCI to predict each generative factor from each latent factor and estimate scores from the weights and accuracy of the trained classifier. Hence the relationship matrix relies heavily on the ad-hoc model, requiring careful selection of the model and hyperparameters [7]. Naturally, this metric thus may be prone to stochastic behaviour, which is less than ideal.\nMutual information (MI): The use of mutual information to describe relationships was first proposed in MIG, and has since been adopted by many subsequent metrics, including Modularity score [4], MIG-sup, and DCIMIG. While this choice offers the advantage of not varying by implementation, and making no assumptions about the relationship between factors and codes, all these methods compute mutual information by binning and suffer from several challenges. We elaborate on this further in the next subsection on shortcomings.\n3) Extent of characterisation.: The ability of metrics to express the degree of modularity or compactness depends on the extent of characterisation. The Z-diff metric uses maximum value to describe the extent of disentanglement. Consequently, it would not be capable of distinguishing whether a code captures primarily one factor or multiple factors. SAP and MIG take the difference between the top two entries to express the degree of completeness, which would not allow distinguishing whether a factor is encoded by two codes or by more than two codes. This yields limitations in functionality, discussed in the next subsection. MIG-sup, furthermore, is not affected by low information content, as it normalises mutual information by dividing by the entropy of the code, making it ignorant to information loss. DCI, in contrast, is designed well in this regard as it can express the degree of relationship by calculating $1 - entropy$ (where entropy is estimated from the probabilities derived from feature importance). Modularity is also equipped to express the degree well by calculating the deviation of all items from the maximum value.\n4) Shortcomings.: Abdi et al. [12], in a first attempt, reported inadequacies in the disentanglement metrics, noting discrepancies without delving into the underlying reasons. This observation spurred further investigations within the research community. Chen et al. [6] examined metrics through the lens of robustness to hyperparameter selection during experiments and showed that the early modularity-centric metrics overestimated disentanglement. Sepliarskaia et al. [8], in subsequent work, provided an initial theoretical analysis, unveiling specific cases of failures in the metrics, but lacked a controlled study. Carbonneau et al. [7] showed some controlled evidence of measurement of different properties and reported that the metrics differ in terms of measured properties and overall agreement. Surveying the literature, we identified the following major functional issues, that support our argument to have improved metrics:\na) Several metrics designed for a particular aspect fail in efficiently reflecting that aspect in all cases. This is observed strongly in Z-diff and Z-min Variance which penalise modularity violations weakly [8]. We conducted a systematic analysis to test metric calibration to confirm this and identify other discrepancies (cf. Section IV-A). This was also observed in the case of compactness-centric metrics like SAP and DCIMIG\u00b9.\nIn MIG, it was observed that it assigns a 0, when a factor is encoded by just two codes [7], indicating too strong a penalisation in partial entanglement.\nb) Modularity-centric metrics are generally not equipped to capture compactness and disregard explicitness. Since these metrics align $z_i$, with a corresponding set of codes, $c_i$, this strategy does not ensure that distinct codes are dedicated to unique factors. Further, they do not capture the extent of the factor-code relationship, and consequently cannot be reliably used to reflect disentanglement.\nc) Predictor-based methods can overfit and can be computationally expensive. Metrics that use predictors to determine factor-code relationships can overfit when there are too few samples, resulting in overestimating explicitness [7]. Furthermore, the complexity of the chosen model can result in undesirable computational complexity (cf. Section IV-E).\nd) Existing information-based metrics are fraught with computational challenges. The existing metrics that use mutual information using maximum likelihood estimators, that require quantisation of both spaces and parameterised sampling procedures. The existing formulations\u00b2 expect a discretisation of spaces into bins, with the mutual information value estimation being sensitive to binning considerations. These pose further a challenge in scenarios dealing with high-dimensional or non-linear data [15, 7], discussed further in Section IV-B."}, {"title": "III. EXCLUSIVITY DISENTANGLEMENT INDEX (EDI)", "content": "Having identified the best practices in design and their shortcomings, we exploit them to define the the disenglement aspects in a more intuitive and simple way, using the principle of exclusivity. In this section, we introduce our proposed metric EDI. First, we define impact intensity that measures the factor-code relationship. Next, we define exclusivity which we subsequently use as the criteria to define and mathematically construct both modularity and compactness metric formulations.\nWhen a factor is encoded by two codes, DCIMIG yields a score of $\\frac{(max(I(c_0; z_0), I(c_1; z_0)) + I(c_2; z_1))}{(H(z_0) + H(z_1))} = 1$. $I(c, z)$ is commonly estimated as, $\\sum_{i=1}^s \\sum_{j=1}^t P(i, j) log (\\frac{P(i,j)}{P(i)P(j)})$.\nA. Impact Intensity\nWe measure the influence each of the factors $z_i$ have on the latent codes $c_i$ using a relationship matrix we call Impact Intensity. We introduce two improvements in the computation of relationship matrix, namely, a) an improved estimator and b) no reliance on ad-hoc decision model.\nAs pointed out earlier, existing implementations of MI in metrics are unsuited to high-dimensional continuous variables and fraught with computational challenges [15]. Naturally, a non-parametric estimator with no dependence on discretisation is more suitable. A recently proposed method called MINE [16] operates by training a small neural network to maximise a lower bound on the mutual information between two variables. As it involves no density estimation using maximum likelihood it is flexible and has been shown to converge to the true mutual information between high-dimensional variables [16]. Linearly scalable in both dimensionality and sample size, it offers a significant advantage (cf. Sections IV-B and IV-E).\nThus, we propose computing the relationship matrix as follows: First, we calculate the following required variables: a) $I(c_i; z_j)$, signifying the mutual information computed between each factor $c_i$ and each code $z_j$; b) $I (c_1, c_2, . . ., c_d; z_j)$, signifying the mutual information between all codes $c_1, c_2,..., c_d$ and each factor $z_j$; and c) $H(z_j)$, representing the entropy of each factor. We establish the relationship as $R(c_i; z_j) = \\frac{I(c_i;z_j)}{I(c_1, c_2, ..., c_d; z_j)}$, denoting the impact intensity of factor $z_j$ on the code $c_i$ among all codes. This, we argue, offers a more accurate representation of the relationship, as latent codes are learned from generative factors.\nB. Exclusivity\nThe concept of exclusivity is crucial in both modularity and compactness. In modularity, we desire a code to capture a singular factor and exclude others. In compactness, it is expected that a factor is represented by a code without overlapping with others. This principle is fundamentally the inverse of impurity.\nWe propose an intuitive method to quantify the extent of exclusivity, which is defined as the difference between correctness (the maximum value) and incorrectness (the root mean square error of all other values). The objective is to maximise the difference between correctness and incorrectness.\nGiven a set of attributes $\\{a_1,a_2, ..., a_n\\}$, the exclusivity is mathematically represented as:\n$\\text{Exclusivity} (a_1, a_2,..., a_n) = a_{i^*} - \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} a_i^2}$,\nwhere $i^* = \\text{argmax} a_i$.\nThe aim is for the maximum value to be as high as possible, with the remainder as minimal as possible.\nC. Formulation\nBy applying the aforementioned concepts of exclusivity to better depict the \"extent\", and impact intensity to capture factor-code relationships, we formulate the following metrics to measure modularity, compactness, and explicitness.\nModularity. We formalize the metric for modularity, or disentanglement, of a latent code $c_i$ as:\n$D(c_i) = \\text{Exclusivity} (R(c_i; z_1), R(c_i; z_2), ..., R(c_i; z_k))$.\nThe aggregate modularity score is then calculated as $D = \\frac{1}{d} \\sum_{i=1}^{d} D(c_i)$, where d denotes the code dimensionality, and $k$, representing the number of factors, signifies the maximum potential influence a single factor can exert. Notably, this framework may encounter complications due to correlated effects, wherein multiple codes capture a single factor. To address this challenge, we allocate to each code $c_i$ and its predominantly associated factor $z_{j^*}$ a score $S_{ij^*} = D(c_i)$, while assigning $S_{ij} = 0$ for $j \\neq j^*$. Accumulating these scores across all factors yields $S_j = \\sum_i S_{ij}$ for each factor $j$. Ensuring that the score for each factor does not exceed 1 (the maximum conceivable impact intensity for each factor is 1), the final score is thus recalculated as $D = \\frac{\\sum_j \\text{min}(S_j, 1)}{k}$, facilitating an accurate assessment of modularity.\nWe then assign a score $S_{ij^*} = D(c_i)$ to each code $c_i$ and its most effective factor $z_{j^*}$, and mark the others as $S_{ij} = 0$ for $j \\neq j^*$. The overall disentanglement is finally calculated as:\n$D = \\frac{\\sum_j \\text{min}(S_j, 1)}{k}$, where $S_j = \\sum_i S_{ij}$\nCompactness. The compactness of a generative factor $z_j$ is calculated as:\n$C(z_j) = \\text{Exclusivity} (R(c_1; z_j), R(c_2; z_j), . . ., R(c_d; z_j))$.\nAccordingly, the overall compactness score, $C$, is determined by the average compactness across all factors:\n$C = \\frac{1}{k} \\sum_{j=1}^{k} C(z_j)$.\nExplicitness. For a generative factor $z_j$, explicitness or informativeness is calculated as the ratio of the combined information content of the codes relative to $z_j$ to the entropy of $z_j$ itself:\n$I(z_j) = \\frac{I(c_1, c_2, ..., c_d; z_j)}{H(z_j)}$\nHence, the aggregate measure of informativeness is the mean informativeness across all generative factors:\n$I = \\frac{1}{k} \\sum_{j=1}^{k} I(z_j)$"}, {"title": "IV. EXPERIMENTS", "content": "In the following sections, we model the relation $c = \\gamma(g(z))$ as $c = f(z)$. Here, $f(z)$ represents a fully-parameterised function controlling the factor-code relationship. For the experiments, factors z are sampled i.i.d from a discrete uniform distribution in Sections IV-A and IV-E, and from a continuous uniform distribution $\\mathcal{U} \\in \\{0,1\\}$ in Section IV-B to Section IV-D. Following [7], we generate N factors to form a set Z and compute the corresponding set of codes c using the experiment-specific f(z) parameterised by a, resulting in one representation. Unless otherwise specified, the factor and code dimensionality are kept equal (k == d). For each a within the chosen discrete range, we generate M representations and aggregate over these for s random seeds. In Section IV-F, representations are learnt using real latent variable models on a real-world dataset.\nA. Are the metrics well calibrated?\nMotivated by discrepancies in our exploratory analysis, we first systematically assess metric behaviour via discrete boundary test cases for each of the aspects i.e. modularity, compactness, and explicitness. Codes are arranged in that order with 1 denoting a perfect aspect and 0 completely imperfect. For example, #101 indicates perfect disentanglement and explicitness, but imperfect compactness.\nTo form the factor space, we sample N = 50,000 points from a discrete uniform distribution with a one-to-one encoding. Each category is assigned a distinct code (k = d = 2) unless: a) when modularity is low, we encode two factors into one code; b) when compactness is low, we encode a factor into two codes; or c) when informativeness is low, we randomly drop categories within the factors. We simulate a total of $2^3 = 8$ representative cases\u00b3.\nAs discussed in Section II-4, not all metrics designed for specific aspects are well-calibrated. Z-min Variance, for instance, which is modularity-centric, fails to penalise modularity violations, with scores larger than 0.5 in low modularity scenarios (#000, #001, #010, #011). This stems from its assigning of the minimum score as $\\frac{1}{d}$. The Modularity metric, while performing perfectly in high modularity cases, unexpectedly assigns high scores of 0.75 in low modularity scenarios too (#010 and #011). This is likely due to an error introduced by dividing the maximum term in the formula. DCI Mod correctly assigns low scores in low modularity cases of #000 and #001, however, it assigns relatively large scores of > 50% in #010 and #011, indicating some influence of high compactness, which is not ideal. In contrast, EDI Mod assigns 0.43, reflecting low modularity relatively better.\nThe discrepancies appear in compactness-centric metrics as well. SAP, for instance, assigns a relatively low score of 0.33 in both high compactness scenarios (#010 and #110) but a higher score of 0.45 in the low compactness case of #101, suggesting greater influence from other aspects. MIG also assigns relatively low scores of 0.41 and 0.45 in high compactness scenarios (#010 and #110), but a higher score of 0.49 in the less compact scenario of #101. Its successor, MIG-sup, assigns a large score of 0.99 in both low (#100, #101) and high compactness scenarios (#110, #111) while tends to assign intermediate scores of about 0.5 in high compactness, low modularity scenarios (#010). This shows a high influence from modularity but yields no clear interpretation of the captured aspects. Furthermore, the DCIMIG metric assigns a higher score to the low compactness case of #101 confirming weak penalisation, as a consequence of two codes capturing different information extent about the factor. DCI Comp assigns very high scores to scenarios #100 and #101, which are highly modular despite low compactness. EDI Comp, in contrast, assigns lower scores. In terms of explicitness, EDI and DCI perform comparably. Overall the results indicate EDI to be better calibrated in comparison to the existing metrics.\nB. How do the metrics deal with non-linearity?\nThe ability to attribute accurate scores when factor-code relationships are non-linear as in realistic data is a crucial property. A robust metric should exhibit negligible effect with increasing non-linearity. In this experiment, we simulate representations which are perfectly compact and modular, but the encoding function becomes increasingly non-linear. We use $f(z) = 1000 \\alpha + 0.25 \\text{tan} (\\omega(z \u2013 0.5)) + 0.5$ where $\\omega = \\frac{\\pi}{2} \\text{arctan} (10000 \u2013 0.25)$. As a increases, the curve becomes more steep but remains monotonic for $z \\in [0,1]$. Using k = d = 6, we simulate M = 50 representations with N = 20,000 points sampled from $\\mathcal{U} \\in [0,1]$. For s = 50 seeds, we report the aggregated scores in Figure 1.\nThis experiment perfectly challenges the complexity of the predictors employed by the metrics, and highlights the potential issues inherent to mutual information computation using density estimation, yielding interesting insights. Metrics that calculate MI using binning methods generally perform inadequately. To illustrate this, we contrasted MIG to its alternative variant implemented with a non-parametric estimator, KSG [15]. MIG-ksg demonstrates greater stability until reaching $\\alpha = 0.6$, after which it gradually declines. Metrics using linear models like SAP, exhibit instability as non-linearity increases. This is also observed in DCI, which in its original implementation uses a LASSO classifier. Both DCI Mod and Comp decline and become more variable as non-linearity increases. In contrast, Z-diff and Modularity scores exhibit stability throughout the experiment. EDI Mod and Comp consistently assign a perfect score of 1 throughout too, indicating robustness in this setting. For explicitness, a slight reduction in mutual information is expected.\nC. How do the metrics behave on decreasing disentanglement?\nNext, we evaluate the performance of the metrics as a perfectly disentangled representation gradually transitions to an entangled state. We conduct an experiment where we linearly reduce the modularity and compactness of the representation while maintaining explicitness. To describe the factor-code relationship, we employ $f(z) = zR$, with\n$R = \\begin{bmatrix}\n1-\\alpha & \\alpha & 0 & 0 & ... & 0\\\\\n\\alpha & 1-\\alpha & \\alpha & 0 & ... & 0\\\\\n0 & \\alpha & 1-\\alpha & \\alpha & ... & 0\\\\\n:\\\\\n0 & 0 & 0 & 0 & ... & 1-\\alpha\n\\end{bmatrix}$.\nLike in the previous experiment, we use k = d = 6, and simulate M = 50 representations with N = 20,000 points. For s = 50 seeds, we report the aggregated scores in Figure 2.\nAs the parameter a increases, we expect a linear decrease in all metrics dealing with modularity or compactness, though not reaching 0 entirely\u2074. Z-diff and Z-min Variance metrics fail completely in this regard. Conversely, both modularity and compactness components of EDI and DCI respectively demonstrate robust performance. DCI Expl, which does not represent true mutual information remains largely unaffected. It is also prone to overfitting and hence may overestimate explicitness [7]. In comparison, EDI Expl exhibits a drop when one factor becomes equally represented by two codes. Most information-based metrics also perform well, however, assign zero value already when only one factor or code becomes fully entangled.\nD. How do the metrics deal with noise?\nIn this segment, we investigate how the metrics behave when we keep modularity and compactness intact, but gradually reduce explicitness. Choosing $f(z) = (1 \u2212 \\alpha)z + \\alpha \\eta$, and keeping the setting consistent as before, we report the results in Figure 3. In this simulation, we expect the metrics representing explicitness to decrease gradually. In this regard, both EDI Expl and DCI Expl perform adequately, however\nE. How do the metrics compare on resource efficiency?\nHere, we evaluate and compare the metrics in terms of sample efficiency and time complexity. To test sample efficiency, we compute the difference in estimated scores when using subsets of data $\\mathcal{N}\\in \\{100,1000, 10000, 10000\\}$ against the full sample size of N = 100,000. We keep the experimental setup as in section IV-A, with a difference that we use only 10 random seeds, and report the mean differences in Figure 4 (left). We observe the minimum samples required to reliably estimate scores vary across metrics, as a result of design choices. While most metrics converge around the 10,000 sample mark, it becomes evident that classifiers-based metrics such as DCI necessitate larger sample sizes for optimal performance, whereas metrics reliant on MI require fewer samples. In this regard, EDI generally is more sample-efficient than DCI, with the exception of its explicitness component, which needs more samples to reliably compute mutual information.\nIn terms of time complexity, most metrics are constant or (sub)linear. We observed EDI to be linear, and for DCI, it depends on the complexity of the ad-hoc model. If one were to choose complex models like random forest or XGBoost (DCI-xgb) to model non-linearity better as recommended [7], this would come with a serious disadvantage of the curse of dimensionality (cf. Figure 4 (right)).\nF. Metric Agreement on Real Dataset\nIt was observed in previous works that metrics do not correlate on complex datasets, and the correlations may not be consistent across datasets [13]. While we do not test consistency in this regard, we test general agreement of the"}, {"title": "V. CONCLUSION", "content": "In this study, we conducted a comprehensive analysis of existing metrics for evaluating disentanglement, elucidating differences in their assumptions, design, and functionality. By focusing on best practices, we formulated a novel metric, EDI, grounded in the intuitive and novel concept of exclusivity. Through controlled simulations, we demonstrated EDI to be well-calibrated, and better in comparison to existing metrics on non-linearity, resource efficiency and robustness under noise. These observations indicate a better suitability of EDI in supervised disentanglement measurement. However, it is essential to acknowledge that several pertinent questions remain open. Specifically, the development of unsupervised metrics"}, {"title": "A. Existing Metric Formulations", "content": "1) Z-diff (BetaVAE) Metric: Higgins et al. [10] introduced the BetaVAE based on the notion that dimensions capturing the constant generative factor should match, while others vary. This metric aims to capture modularity by computing the following steps:\n(a) Selecting a generative factor $z_k$.\n(b) Choosing a pair of samples, $s_1$ and $s_2$, with $z_k$ constant while other factors vary.\n(c) Generating latent codes $c_1$ and $c_2$.\n(d) Calculating pairwise distortion:\n$e = (|c_{1,i} - c_{2,i}|), 1 \\le i \\le |z|$\n(e) Repeating the above steps to train a linear classifier predicting the fixed generative factor, with Z-diff indicating classifier precision.\n2) Z-min Variance (FactorVAE) Metric: Kim et al. [11] proposed a metric similar to Z-diff, based on the assumption that latent codes capturing a constant generative factor should remain consistent. The method normalises each latent code by its dataset-wide standard deviation. The latent dimension with the least variance and the index of the constant factor form a sample for a linear classifier, assessing the classifier's precision.\n3) Separated Attribute Predictability (SAP): Kumar et al. [5] developed the SAP metric, based on a matrix of informativeness I, with each entry $I_{i,k}$ representing a linear regression from latent code $c_i$ to generative factor $z_k$. The SAP score is:\n$\\text{SAP}(C, Z) = \\frac{1}{K} \\sum_k (I_{i_{k},k} - \\underset{l \\neq i_k}{max} I_{l,k}) ; \\quad i_k = \\text{arg max}_{i} I_{i,k}$.\n4) Modularity Score: Ridgeway et al. [4] proposed a modularity metric of a latent cod ci as:\n$\\text{modularity} = 1- \\frac{\\sum_{k \\in \\Omega^*\\neq} I(k, c_k)}{I(z^*, c_k)^2 \\times (M \u2013 1)}$,\nwhere $z_*$ represents the factor that has the highest mutual information, $\\Omega_{\\neq* }$ denotes the set of all the generative factors except $z_*$, and M represents the number of factors.\n5) MIG: The Mutual Information Gap (MIG), as detailed by Chen et al. [6], estimates disentanglement through the empirical mutual information between latent codes and generative factors:\n$\\frac{1}{K} \\sum_{k} \\frac{1}{H(z_j)} (I(c_{i*}; z_j) - \\underset{i \\neq i*}{max} I(c_{i}; z_j))$,\nwhere $i^* = \\text{argmax}_{i} I(c_{i}; z_j)$, H(zj) is the entropy of zj."}]}