{"title": "LLMs as Function Approximators: Terminology, Taxonomy, and Questions for Evaluation", "authors": ["David Schlangen"], "abstract": "Natural Language Processing has moved rather quickly from modelling specific tasks to taking more general pre-trained models and fine-tuning them for specific tasks, to a point where we now have what appear to be inherently generalist models. This paper argues that the resultant loss of clarity on what these models model leads to metaphors like \u201cartificial general intelligences\" that are not helpful for evaluating their strengths and weaknesses. The proposal is to see their generality, and their potential value, in their ability to approximate specialist function, based on a natural language specification. This framing brings to the fore questions of the quality of the approximation, but beyond that, also questions of discoverability, stability, and protectability of these functions. As the paper will show, this framing hence brings together in one conceptual framework various aspects of evaluation, both from a practical and a theoretical perspective, as well as questions often relegated to a secondary status (such as \"prompt injection\u201d and \u201cjailbreaking\").", "sections": [{"title": "Introduction", "content": "In March 2023, Bubeck et al. (2023) released a pre-print that in retrospect can be seen as helpful contemporary documentation of the confusion that the release by OpenAI first of the Large Language Model (LLM) GPT-3.5,\u00b9 and then of GPT-4 (OpenAI, 2023) had caused at the time. The authors reacted to the perceived generality\u2014\u201cthe ability to seemingly understand and connect any topic, and to perform tasks that go beyond the typical scope of narrow AI systems\" (Bubeck et al., 2023, p.7)- of the GPT-4 model (to which they had early access) by letting go of all hitherto accepted standards of evaluation (namely, to use carefully crafted datasets representing interesting and challenging tasks) and instead launching a somewhat unsystematic breadth-first search of tricks the model can do, a process leading them to see \"Sparks of Artificial General Intelligence\u201d (as in the title of their paper).\nNow, a year later, the practices of a more normal science (Kuhn, 1962) have returned. Evaluation through task datasets has adapted (Liang et al., 2023; Srivastava et al., 2022; Hendrycks et al., 2021), for example through attempts at more systematically covering the task space (an idea especially thoroughly realised in HELM).\u00b2 In a way, even the self-guided one-off task exploration of Bubeck et al. (2023) has been codified, in the Chatbot Arena (Chiang et al., 2024; Zheng et al., 2023) which allows self-selected testers to freely pose tasks, which are then given to two models in parallel, which are then ranked in terms of the relative quality of their response.\nBut still, there remains uncertainty about how to grasp what these models are, beyond what is technically certain (which is that they are, well, language models: conditional predictors of tokens). Are they models of language (yes: Piantadosi (2023), no: Kodner et al. (2023), Birhane and McGann (2024), inter alia)? Are they \"stochastic parrots\" (Bender et al., 2021)? Are they models of human language use (Andreas, 2022); of human reason (or maybe just reasoning, Huang and Chang (2023)); of intelligence \"in general\" (Bubeck et al., 2023)? It is the goal of this paper to propose a \u201cleast commitment\u201d metaphor-LLMs as function approximators\u2014and to explore how this could help structure current debates. What this means will be explained in the coming sections.\u00b3"}, {"title": "LLMs as Function Approximators", "content": "On the technical level, an LLM is a function from a sequence of tokens to a distribution over a token vocabulary-i.e., it is still a language model (Manning and Sch\u00fctze, 1999). Given a method for sampling from the distribution and extending generated sequences (finitely, eventually stopping), an LLM can be seen as a function from a sequence of tokens to a sequence of tokens. Where it becomes interesting is when the semantic relationship between the input and output sequence is taken into view. Various recently developed techniques (e.g., framing of tasks as question/answer pairs, instruction tuning, response preference alignment via supervision on full responses; McCann et al. (2018); Stiennon et al. (2020); Ouyang et al. (2022), inter alia) together with sheer scaling of training data and model sizes (Kaplan et al., 2020) have brought these models to a state where the relation between input sequence and output sequence can usefully be understood as one between a stimulus and a response, rather than (just) as one between a text and its continuation. And to the extent that such a relationship is stable (both \"write a limerick about CPUs\u201d and \u201cwrite a limerick about LLMs\" results in texts that resemble limericks, with the respective topics), therein lies the approximation of a function (here, \"write a limerick about X\") that is our concern in this paper."}, {"title": "Finding the Function", "content": "What is peculiar about this functional relationship is that the function does not need to be learned specifically by the model, at least not in the heretofore common sense. Rather, the function needs to be found in the vast and \"latent\" space that is opened by the encompassing \u201csequence to sequence\" function that is the LLM.\u2074 Techniques for doing so have been suggested from the time when this property was first observed (Brown et al., 2020) and are by now somewhat better understood, or at least catalogued (Schulhoff et al., 2024). The following is not meant as advice on formulating prompts (which is what the textual means for what we analyse here as function induction are now commonly called); it is meant as a proposal for naming the informational components present in such"}, {"title": "A Taxonomy of Function Types", "content": "We can now categorise prompt-induced functions (or, equivalently, the task that a given prompt is meant to pose to the model) according to the type of semantic relation between domain and co-domain; that is, between the x and the corresponding y, yielding a distinction between:\n\u2022 transformation tasks, where the information that is contained in y is also contained in x (that is, x entails y). E.g., summarisation, translation, paraphrasing.\n\u2022 categorisation tasks, where y is a category (typically, out of a small set of candidates) into which x falls.\n\u2022 additive tasks, where y contains information not entailed by x. This can be further classified into\n recall-additive, where the additional information is based on \"recalled\" information from the training data (and is assumed to be factually true); e.g., where y is meant to be an answer to a factual question x; and\n creative-additive, where the additional information in y is not (necessarily) meant to have been encountered in the training data (but is still based in some sense on x). If y is meant to fulfil certain constraints (e.g., be executable code), we can call this grounded creative-additive; if not, free creative-additive (e.g., generation of a story based on the prompt).\nThe boundaries between these classes are not necessarily sharp-for example, one might want to understand the \"text to code\" task as a form of translation (and hence, as a transformation task), if the text is very specific; or as grounded creative-additive, if it is more abstract\u2014but the taxonomy shall suffice to discuss some differences between tasks in the section below. Finally, some tasks, like for example summarisation, are of course better modelled as a mapping from a source text into a set of summaries (or, even better, a fuzzy set / a pair of text + indicator of task-based goodness, interpretable as degree of set membership). Our concern here, however, is not with modelling all cases in all details; it is with framing the discussion, for which these details can remain unresolved for now."}, {"title": "Some Examples", "content": "To make the above a bit more concrete, we now go through three example use cases of LLMs and show"}, {"title": "Questions for the Evaluation of Function Approximators", "content": "Figure 3 illustrates the function approximator metaphor. We can use it to describe what a genuinely Universal Function Approximator (UFA) would look like:\nAny desirable function $f \\in F$ can be found through a natural task description (that is, one that an informed layperson can come up with, and which does not need to be 'tuned' to idiosyncrasies of the model);\nthe function f behaves well even for extreme targets xt, regardless of what the training material of the underlying model was;\nf is protected against xt that are outside of its intended domain (including adversarial ones that contain a different task description meant to \"jump outside of\" f);\nf does not produce output that is 'undesirable', even if it would be in Y;\nfinally, the finding process is stable against semantically irrelevant variations in the formulation of the task description.\nSuch UFAs do not currently exists. We can explore the ways in which current models are lacking from different perspectives, using the concepts introduced here. We can look at the the behaviour of the approximated function f itself (and how it relates to f*, the intended function); this is labelled f below. We can look at the induction process that goes from td to f; labelled I below. Finally, we can look at the coverage of F; labelled F below. We can do all of this from a practical perspective (and within that, from the sub-perspective of someone designing a feature with a fixed set of functions, or of someone aiming to expose the generality for example in a chatbot-like interface; both labelled p below) or from a more theoretical perspective aimed at understanding the model capabilities in general (label: t)."}, {"title": "Focus on the Prompt-Induced Function", "content": "f,p: How closely does f approximate f*? This is the most basic question that we used to ask of machine learned models, and it can be explored with the usual instruments: a test set of x, y mappings, and a metric for comparing predicted values to these reference values. The nature of this metric will likely differ significantly depending on the task type as described above in Section 2.2. This is"}, {"title": "Focus on the Induction Process", "content": "i,p: How natural can the td be? How stable is what is being induced against semantically insignificant variations in how td is formulated? That current models are not doing particularly well here is the whole raison d'\u00eatre of tools like DSPy (Khattab et al., 2023); how they do is now beginning to be investigated systematically (Lu et al., 2024).\nThis is also one of the factors that the paradigm of"}, {"title": "Focus on the Space of Functions", "content": "F,p: Continuing with a theme from above, another potentially desirable feature is to be able to block certain functions from being reachable via (user) prompt at all; this is particularly relevant if the generality of the model is exposed to users (as in a chatbot-style interface). As discussed above, one way this is currently achieved is by formulating lengthy 'system prompts' (see Section A below).\nF,p: Do models perform similarly on similar tasks? This is related to the induction question above, but here getting at it from the perspective of how \"evenly\" the space of functions is covered. From a practical perspective, this kind of homogeneity helps with forming a mental model of which feature should work, and how well. From a theoretical perspective, this leads over to the next question.\nF,t: What is the relation between tasks that can successfully be prompt-induced (in the sense that they perform well; let us call this set F) and tasks"}, {"title": "Conclusions", "content": "This paper has been an attempt at taking a relatively salient understanding of what LLMs are, or can be seen as namely, function approximators\u2014and trying to offer a precise formalisation of this idea, and to play through what this framing means for questions of evaluating these models, along practical and theoretical dimensions. It has shown that the framing can bring out a common aim behind what otherwise looks like very disparate threads of research that are united only by their subject (LLMs): to understand, and improve, the model's ability to approximate (desirable) functions. It is a \u201cleast commitment\u201d metaphor insofar as it demands only the acceptance of the utility of one level of analysis above \"LLMs as text completers\u201d, which is that there is or can be a semantic relationship between input and output of the model, while hopefully enabling discussions about whether additional levels of analysis can be grounded by it, or not."}, {"title": "An Example System Prompt", "content": "The following example system prompt is taken from a Microsoft guidebook for system design with LLMs. It is documented here in full length to show the lengths that system designers go through with current models in order to constrain (and protect) the induced function."}]}