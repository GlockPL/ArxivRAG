{"title": "LLMs as Function Approximators: Terminology, Taxonomy, and Questions for Evaluation", "authors": ["David Schlangen"], "abstract": "Natural Language Processing has moved rather quickly from modelling specific tasks to taking more general pre-trained models and fine-tuning them for specific tasks, to a point where we now have what appear to be inherently generalist models. This paper argues that the resultant loss of clarity on what these models model leads to metaphors like \u201cartificial general intelligences\" that are not helpful for evaluating their strengths and weaknesses. The proposal is to see their generality, and their potential value, in their ability to approximate specialist function, based on a natural language specification. This framing brings to the fore questions of the quality of the approximation, but beyond that, also questions of discoverability, stability, and protectability of these functions. As the paper will show, this framing hence brings together in one conceptual framework various aspects of evaluation, both from a practical and a theoretical perspective, as well as questions often relegated to a secondary status (such as \"prompt injection\u201d and \u201cjailbreaking\").", "sections": [{"title": "1 Introduction", "content": "In March 2023, Bubeck et al. (2023) released a pre-print that in retrospect can be seen as helpful contemporary documentation of the confusion that the release by OpenAI first of the Large Language Model (LLM) GPT-3.5,\u00b9 and then of GPT-4 (OpenAI, 2023) had caused at the time. The authors reacted to the perceived generality\u2014\u201cthe ability to seemingly understand and connect any topic, and to perform tasks that go beyond the typical scope of narrow AI systems\" (Bubeck et al., 2023, p.7)- of the GPT-4 model (to which they had early access) by letting go of all hitherto accepted standards of evaluation (namely, to use carefully crafted datasets representing interesting and challenging tasks) and instead launching a somewhat unsystematic breadth-first search of tricks the model can do, a process leading them to see \"Sparks of Artificial General Intelligence\u201d (as in the title of their paper).\nNow, a year later, the practices of a more normal science (Kuhn, 1962) have returned. Evaluation through task datasets has adapted (Liang et al., 2023; Srivastava et al., 2022; Hendrycks et al., 2021), for example through attempts at more systematically covering the task space (an idea especially thoroughly realised in HELM).\u00b2 In a way, even the self-guided one-off task exploration of Bubeck et al. (2023) has been codified, in the Chatbot Arena (Chiang et al., 2024; Zheng et al., 2023) which allows self-selected testers to freely pose tasks, which are then given to two models in parallel, which are then ranked in terms of the relative quality of their response.\nBut still, there remains uncertainty about how to grasp what these models are, beyond what is technically certain (which is that they are, well, language models: conditional predictors of tokens). Are they models of language (yes: Piantadosi (2023), no: Kodner et al. (2023), Birhane and McGann (2024), inter alia)? Are they \"stochastic parrots\" (Bender et al., 2021)? Are they models of human language use (Andreas, 2022); of human reason (or maybe just reasoning, Huang and Chang (2023)); of intelligence \"in general\" (Bubeck et al., 2023)? It is the goal of this paper to propose a \u201cleast commitment\u201d metaphor-LLMs as function approximators\u2014and to explore how this could help structure current debates. What this means will be explained in the coming sections.\u00b3"}, {"title": "2 LLMs as Function Approximators", "content": "On the technical level, an LLM is a function from a sequence of tokens to a distribution over a token vocabulary-i.e., it is still a language model (Manning and Sch\u00fctze, 1999). Given a method for sampling from the distribution and extending generated sequences (finitely, eventually stopping), an LLM can be seen as a function from a sequence of tokens to a sequence of tokens. Where it becomes interesting is when the semantic relationship between the input and output sequence is taken into view. Various recently developed techniques (e.g., framing of tasks as question/answer pairs, instruction tuning, response preference alignment via supervision on full responses; McCann et al., 2018); Stiennon et al. (2020); Ouyang et al. (2022), inter alia) together with sheer scaling of training data and model sizes (Kaplan et al., 2020) have brought these models to a state where the relation between input sequence and output sequence can usefully be understood as one between a stimulus and a response, rather than (just) as one between a text and its continuation. And to the extent that such a relationship is stable (both \"write a limerick about CPUs\u201d and \u201cwrite a limerick about LLMs\" results in texts that resemble limericks, with the respective topics), therein lies the approximation of a function (here, \"write a limerick about X\") that is our concern in this paper.\""}, {"title": "2.1 Finding the Function", "content": "What is peculiar about this functional relationship is that the function does not need to be learned specifically by the model, at least not in the heretofore common sense. Rather, the function needs to be found in the vast and \"latent\" space that is opened by the encompassing \u201csequence to sequence\" function that is the LLM.\u2074 Techniques for doing so have been suggested from the time when this property was first observed (Brown et al., 2020) and are by now somewhat better understood, or at least catalogued (Schulhoff et al., 2024). The following is not meant as advice on formulating prompts (which is what the textual means for what we analyse here as function induction are now commonly called); it is meant as a proposal for naming the informational components present in such prompts.\nDefinition 1 A prompt is defined by the following components: [itd, {((xi, Yi), ei))}, xt], where\n\u2022 itd is the intensional task description (e.g., \"translate English to French\"). This description can contain specific formatting instructions that constrain the output (\u201cprefix your response with TRANSLATION: ", "English: sea otter\\nFrench: loutre de mer\\\").\nThe pairs can be augmented with a textual evaluation er like \u201cmore succint than this\"; this is meant to capture the information provided by multi-turn rounds of advancing towards a desired result.\n\u2022 xt finally is the target instance for the given task prompt, such as for example the phrase that is to be translated.\nThe task description (td) contains at least one of itd and etd; a prompt contains at least one of td and target instance xt.\nDefinition 2 Given a function sample that samples a response y from a model M given a prompt p, we can then define the prompt-induced function f via abstraction of the specific target instance: f = \u03bbx.sample(M, ([itd, {(xi, Yi, ei)}{, x])). Where relevant, we will make a distinction between f, the prompt-induced function, and f*, the intended function meant to be described by td by the author of the description.\nNote that in practical applications, additional steps might be undertaken such as sanitisation of in- and output (e.g., (Rebedea et al., 2023)), parsing of the output (and e.g. ignoring \\\"chain of thought\\\" steps in the output (Chu et al., 2024)), or using the model output to trigger an API call, and taking the output of that as the function value, as in so-called \\\"tool use\\\" (Wang et al., 2024). All of this can easily be represented in this formalisation as function composition, but in any case does not change materially what the underlying functional relationship is and where it is coming from.\"\n    },\n    {\n      \"title\": \"2.2 A Taxonomy of Function Types\",\n      \"content\": \"We can now categorise prompt-induced functions (or, equivalently, the task that a given prompt is meant to pose to the model) according to the type of semantic relation between domain and co-domain; that is, between the x and the corresponding y, yielding a distinction between:\n\u2022 transformation tasks, where the information that is contained in y is also contained in x (that is, x entails y). E.g., summarisation, translation, paraphrasing.\n\u2022 categorisation tasks, where y is a category (typically, out of a small set of candidates) into which x falls.\n\u2022 additive tasks, where y contains information not entailed by x. This can be further classified into\nrecall-additive, where the additional information is based on \\\"recalled\\\" information from the training data (and is assumed to be factually true); e.g., where y is meant to be an answer to a factual question x; and\ncreative-additive, where the additional information in y is not (necessarily) meant to have been encountered in the training data (but is still based in some sense on x). If y is meant to fulfil certain constraints (e.g., be executable code), we can call this grounded creative-additive; if not, free creative-additive (e.g., generation of a story based on the prompt).\nThe boundaries between these classes are not necessarily sharp-for example, one might want to understand the \\\"text to code\\\" task as a form of translation (and hence, as a transformation task), if the text is very specific; or as grounded creative-additive, if it is more abstract\u2014but the taxonomy shall suffice to discuss some differences between tasks in the section below. Finally, some tasks, like for example summarisation, are of course better modelled as a mapping from a source text into a set of summaries (or, even better, a fuzzy set / a pair of text + indicator of task-based goodness, interpretable as degree of set membership). Our concern here, however, is not with modelling all cases in all details; it is with framing the discussion, for which these details can remain unresolved for now.\"\n    },\n    {\n      \"title\": \"2.3 Some Examples\",\n      \"content\": \"To make the above a bit more concrete, we now go through three example use cases of LLMs and show\"\n    },\n    {\n      \"title\": \"3 Questions for the Evaluation of Function Approximators\",\n      \"content\": \"Figure 3 illustrates the function approximator metaphor. We can use it to describe what a genuinely Universal Function Approximator (UFA) would look like:\nAny desirable function f \u2208 F can be found through a natural task description (that is, one that an informed layperson can come up with, and which does not need to be 'tuned' to idiosyncrasies of the model);\nthe function f behaves well even for extreme targets xt, regardless of what the training material of the underlying model was;\nf is protected against xt that are outside of its intended domain (including adversarial ones that contain a different task description meant to \\\"jump outside of\\\" f);\nf does not produce output that is 'undesirable', even if it would be in Y;\nfinally, the finding process is stable against semantically irrelevant variations in the formulation of the task description.\nSuch UFAs do not currently exists. We can explore the ways in which current models are lacking from different perspectives, using the concepts introduced here. We can look at the the behaviour of the approximated function f itself (and how it relates to f*, the intended function); this is labelled f below. We can look at the induction process that goes from td to f; labelled I below. Finally, we can look at the coverage of F; labelled F below. We can do all of this from a practical perspective (and within that, from the sub-perspective of someone designing a feature with a fixed set of functions, or of someone aiming to expose the generality for example in a chatbot-like interface; both labelled p below) or from a more theoretical perspective aimed at understanding the model capabilities in general (label: t).\u2076\"\n    },\n    {\n      \"title\": \"3.1 Focus on the Prompt-Induced Function\",\n      \"content\": \"f,p: How closely does f approximate f*? This is the most basic question that we used to ask of machine learned models, and it can be explored with the usual instruments: a test set of x, y mappings, and a metric for comparing predicted values to these reference values. The nature of this metric will likely differ significantly depending on the task type as described above in Section 2.2. This is what the paradigm of reference-based evaluation, represented above by HELM, addresses.\nThere are also new types of questions, however, owing to the fact that the function is approximated without additional (weight-based) learning:\nf,p: How well is f protected against so-called \\\"prompt-injection": "ttacks (Schulhoff et al., 2023), where the xt (coming from a user) contains text that might be understood as being part of, or even replacing, the td, turning the function f into a function f' outside of the control of the feature designer. Related to this, but not quite identical, are questions of how well the domain and co-domain of the function is protected against undesirable in- and outputs (e.g., a 'give me instructions for doing x' function, where this by design is meant to only respond to activities deemed appropriate for a certain user group; or, on the output side, avoiding certain types of language in additive tasks).\nf,t: What is the relation of f to the training set of the underlying model? Is the resulting function best understood as an interpolation between similar examples seen in during training (or even memorisation), or is it genuine extrapolation / generalisation? Related to this, what would make an observer grant \u201cunderstanding\u201d or \u201cintelligence\" to the induced function? E.g., if the function is of the complexity of the example in Figure 1, and f performs well on a test set, is that evidence that the question text is understood? That the examined medical knowledge is understood? (We will come back to this type of question.)\nf,t: What force does the application of a function have? If the function is something like \u201cproduce an answer to the question"}, {"title": "3.2 Focus on the Induction Process", "content": "i,p: How natural can the td be? How stable is what is being induced against semantically insignificant variations in how td is formulated? That current models are not doing particularly well here is the whole raison d'\u00eatre of tools like DSPy (Khattab et al., 2023); how they do is now beginning to be investigated systematically (Lu et al., 2024). This is also one of the factors that the paradigm of preference-based evaluation, represented above by Chatbot Arena, likely captures, in that the relative ease with which a task is described should figure in the user's relative preference.\ni,p: Related to the previous question is the question of whether functions induced through intuitively similar task descriptions (e.g., \u201csummarize this news article from the domain of sports\" / \"... from the domain of entertainment\") can be prompt-induced similarly.\ni,p: How important are the formatting instructions for recovering the answer y in the model response r? There is a lot of informal knowledge about how best to get models to produce responses from which a desired answer format can easily be extracted (e.g., by asking for structured output instead of free text); what is the influence of decisions made here (Yu et al., 2024)?\ni,t: How is the process that goes from td to f best described as induction or as retrieval? Is that process even a uniform one (always induction, or always retrieval), or does its nature depend on contextual factors? (This is related to the question about generalisation vs. memorisation above, but gets at the issue from a different perspective.) Datasets like ARC (Chollet, 2019) are designed to probe this question; current models are not faring well.\u2078"}, {"title": "3.3 Focus on the Space of Functions", "content": "F,p: Continuing with a theme from above, another potentially desirable feature is to be able to block certain functions from being reachable via (user) prompt at all; this is particularly relevant if the generality of the model is exposed to users (as in a chatbot-style interface). As discussed above, one way this is currently achieved is by formulating lengthy 'system prompts' (see Section A below).\nF,p: Do models perform similarly on similar tasks? This is related to the induction question above, but here getting at it from the perspective of how \"evenly\" the space of functions is covered. From a practical perspective, this kind of homogeneity helps with forming a mental model of which feature should work, and how well. From a theoretical perspective, this leads over to the next question.\nF,t: What is the relation between tasks that can successfully be prompt-induced (in the sense that they perform well; let us call this set F) and tasks that humans can do? What is the relation between performance differences shown by models and by humans? Imagine that a model performs equally well (measured via reference-based evaluation) on the function 'answer this maths question targeted at 10 year old students' and the function 'answer this maths question targeted at 17 year old students'\u2014what would that tell us about the likely underlying mechanism with which the model answers these questions? This line of inquiry brings us to questions of the construct validity of tests, insofar as they meant to support statements about general abilities of models (Raji et al., 2021; Schlangen, 2023a), and hence to the heart of the question about the (artificial) 'intelligence' of these function approximators.\nThis is by no means a complete list of questions that can be reformulated within this framing. It shall suffice for now to demonstrate the productivity of the metaphor."}, {"title": "4 Conclusions", "content": "This paper has been an attempt at taking a relatively salient understanding of what LLMs are, or can be seen as namely, function approximators\u2014and trying to offer a precise formalisation of this idea, and to play through what this framing means for questions of evaluating these models, along practical and theoretical dimensions. It has shown that the framing can bring out a common aim behind what otherwise looks like very disparate threads of research that are united only by their subject (LLMs): to understand, and improve, the model's ability to approximate (desirable) functions. It is a \u201cleast commitment\u201d metaphor insofar as it demands only the acceptance of the utility of one level of analysis above \"LLMs as text completers\u201d, which is that there is or can be a semantic relationship between input and output of the model, while hopefully enabling discussions about whether additional levels of analysis can be grounded by it, or not."}, {"title": "A An Example System Prompt", "content": "The following example system prompt is taken from a Microsoft guidebook for system design with LLMs. It is documented here in full length to show the lengths that system designers go through with current models in order to constrain (and protect) the induced function."}]}