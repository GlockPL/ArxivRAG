{"title": "LLaVA-Ultra: Large Chinese Language and Vision Assistant for Ultrasound", "authors": ["Xuechen Guo", "Wenhao Chai", "Shi-Yan Li", "Gaoang Wang"], "abstract": "Multimodal Large Language Model (MLLM) has recently garnered attention as a prominent research focus. By harnessing powerful LLM, it facilitates a transition of conversational generative Al from unimodal text to performing multimodal tasks. This boom begins to significantly impact medical field. However, general visual language model (VLM) lacks sophisticated comprehension for medical visual question answering (Med-VQA). Even models specifically tailored for medical domain tend to produce vague answers with weak visual relevance. In this paper, we propose a fine-grained adaptive VLM architecture for Chinese medical visual conversations through parameter-efficient tuning. Specifically, we devise a fusion module with fine-grained vision encoders to achieve enhancement for subtle medical visual semantics. Then we note data redundancy common to medical scenes is ignored in most prior works. In cases of a single text paired with multiple figures, we utilize weighted scoring with knowledge distillation to adaptively screen valid images mirroring text descriptions. For execution, we leverage a large-scale multimodal Chinese ultrasound dataset obtained from the hospital. We create instruction-following data based on text from professional doctors, which ensures effective tuning. With enhanced model and quality data, our Large Chinese Language and Vision Assistant for Ultrasound (LLaVA-Ultra) shows strong capability and robustness to medical scenarios. On three Med-VQA datasets, LLaVA-Ultra surpasses previous state-of-the-art models on various metrics.", "sections": [{"title": "1 Introduction", "content": "Generative pretraining has demonstrated effective for visual language modeling in the general domain, utilizing image-text multimodal data, as exemplified by GPT-4 [24] and LLaVA [21]. Through self-supervised finetuning with instruction-following data, pretrained Large Language Models (LLMs) [3, 34, 52] can be adapted to unseen tasks, resulting in improved zero-shot performance. This simple yet powerful method has extended to the multimodal field. It gives rise to Multimodal Large Language Models (MLLMs) [1, 11, 45] that excel in visual language tasks such as image reasoning and further forms user-oriented multimodal conversation assistants. Based on the success of MLLMs in the general domain, similar initiatives have emerged for medical applications [2, 28, 50]. Current research is gradually transitioning from unimodal text to incorporating multimodal medical data. However, despite their effectiveness in general tasks, MLLMs often struggle in medical contexts. This results in inaccuracy or refusal to provide answers when"}, {"title": "2 Related Work", "content": "Research of Multimodal Large Language Models (MLLMs) [4, 41, 45] is an emerging hotspot currently. The key concept is to leverage pretrained Large Language Models (LLMs) to incorporate information from other modalities, such as vision, for performing multimodal tasks like image and video understanding [31, 51] and embodied agent [37, 53\u201355]. Remarkable works such as BLIP-2 [18], LLaVA [21] and LLAMA-Adpater [48] demonstrate prominent generative abilities including visual question answering (Med-VQA) and image"}, {"title": "3 Multimodal Conversational Model in the Medical Domain", "content": "We employ a network architecture that is based on the multimodal conversation model LLaVA, incorporating a projection module to link the visual encoder with the language model. The model parameters are initialized with weights from LLaVA-Med in the English medical domain, followed by finetuning [39] using medical domain instructions derived from our Chinese ultrasound dataset. For each paired sample, given textual instructions $X_q$ and image inputs $X_v$, we ask the model to produce answers $X_a$ related to the original caption $X_c$, such as giving a diagnosis or describing visual contents. The instruction tuning process can be formulated as follows,\n$p(X_a|X_c, X_v, X_q) = \\prod_{l=1}^{L} Po(X_l/X_c, X_v, X_q, X_{1:l-1}),$"}, {"title": "3.1 Ultrasound Concept Feature Alignment", "content": "We employ a network architecture that is based on the multimodal conversation model LLaVA, incorporating a projection module to link the visual encoder with the language model. The model parameters are initialized with weights from LLaVA-Med in the English medical domain, followed by finetuning [39] using medical domain instructions derived from our Chinese ultrasound dataset. For each paired sample, given textual instructions $X_q$ and image inputs $X_v$, we ask the model to produce answers $X_a$ related to the original caption $X_c$, such as giving a diagnosis or describing visual contents. The instruction tuning process can be formulated as follows,\n$p(X_a|X_c, X_v, X_q) = \\prod_{l=1}^{L} Po(X_l/X_c, X_v, X_q, X_{1:l-1}),$"}, {"title": "3.2 Visual Enhancement", "content": "Most existing MLLMs utilize the CLIP series [7, 29, 33] as visual modules, incorporating features from deep layers that represent the global context as inputs to the LLM. However, it may result in less fine-grained visual perception in MLLMs. Additionally, the improvement of MLLMs is hindered by the limitations of the visual branch [19, 30, 42], primarily due to the unbalanced scales of visual and language models (e.g. ViT-Large-300M vs. LAMA-7B/13B) [13]. Therefore, there is a need for visual enhancement in MLLMs, especially in the medical domain where image information is subtle. To address this issue, we integrate the Segment Anything Model (SAM) [16], effective in capturing finer-grained features, as an extra visual encoder and further incorporate it through a fusion strategy, as shown in Fig. 2. Specifically, the input images undergo processing by both the CLIP and SAM (ViT-Large-based) encoders $F_1, F_2$ to derive visual features and then align them with the linguistic feature space of the LLM through corresponding projection modules $P_{01}, P_{02}$. We combine the resultant features $H_1, H_2$ using a learnable weight parameter $\\alpha$ for feature fusion as follows,\n$H_1 = P_{01} (F_1(X_v)), H_2 = P_{02} (F_2(X_v)),\\\\\nH_i = \\alpha \\cdot H_1 + (1 - \\alpha) \\cdot H_2.$"}, {"title": "3.3 Adaptive Sampling for Data Redundancy", "content": "Data redundancy is commonly encountered in clinical scenes, where there is a group of images corresponding to the same text but only some images are valid. To effectively deal with this scenario while balancing computational cost, we devise an adaption module. For such a paired instance, we calculate the weight scores of the grouped k images based on the features obtained from the visual-language projection. Then we sample the image with the highest score as the valid one that best matches the specifics of the text. Specifically, we design two adaptive sampling strategies, as shown in Fig. 2:\n$s_{fea}^i = \\sum_{j=1}^{d} w_jh_j$\n$s_{fea} = Sampling(\\{s_{fea}^i\\}), i = 1, 2, ..., k,$"}, {"title": "4 Professional Ultrasound Multi-modal Data", "content": "There is a lack of Chinese medical datasets to perform finetuning for MLLMs. To fill this gap, we present a first attempt to utilize a large-scale Chinese multimodal ultrasound hospital dataset and it has the noteworthy following characteristics that are rarely seen in previous datasets and works: (i) First-hand source and diversity. Our dataset is sourced from the hospital database. It consists of over 188k medical text descriptions paired with 1.7M ultrasound images with more than 20 examination sites, such as heart, thyroid, breast, uterus, prostate, etc. (ii) Professionality. It contains comprehensive and detailed clinical text such as examination sites, medical histories, ultrasound observations, diagnosis, etc. Professional doctors offer all the contents and thus the data reliability makes our work a valuable attempt in medical MLLM. (iii) Challenges within ultrasound modality. Imaging modalities used in previous works typically include chest X-ray (CXR), computed tomography (CT), and magnetic resonance imaging (MRI). It's relatively possible for even a layman to identify body parts (e.g. head and chest) in images of these modalities. However, such a task is difficult when confronted with ultrasound. It's more challengeable for MLLMs to learn medical semantics like a layman. (iv) Fine granularity. It contains many samples with high similarity due to the hospital source and thus places a higher requirement on fine-grained medical understanding. (v) Respect for medical reality. Datasets like PMC-15M used by LLaVA-Med typically feature paired instances where a text corresponds to a single image. However, it is often inconsistent with clinical practice where data redundancy exists. Our dataset addresses this by capturing many instances where one text is paired with multiple images from the same ultrasound video. For example, when the text describes a lesion, only images of frames scanned to the lesion are valid for mirroring the text, while those without lesion display are invalid. This challenges the model to distinguish valid images for reasoning and holds practical relevance."}, {"title": "4.1 Ultrasound Multi-modal Data", "content": "There is a lack of Chinese medical datasets to perform finetuning for MLLMs. To fill this gap, we present a first attempt to utilize a large-scale Chinese multimodal ultrasound hospital dataset and it has the noteworthy following characteristics that are rarely seen in previous datasets and works: (i) First-hand source and diversity. Our dataset is sourced from the hospital database. It consists of over 188k medical text descriptions paired with 1.7M ultrasound images with more than 20 examination sites, such as heart, thyroid, breast, uterus, prostate, etc. (ii) Professionality. It contains comprehensive and detailed clinical text such as examination sites, medical histories, ultrasound observations, diagnosis, etc. Professional doctors offer all the contents and thus the data reliability makes our work a valuable attempt in medical MLLM. (iii) Challenges within ultrasound modality. Imaging modalities used in previous works typically include chest X-ray (CXR), computed tomography (CT), and magnetic resonance imaging (MRI). It's relatively possible for even a layman to identify body parts (e.g. head and chest) in images of these modalities. However, such a task is difficult when confronted with ultrasound. It's more challengeable for MLLMs to learn medical semantics like a layman. (iv) Fine granularity. It contains many samples with high similarity due to the hospital source and thus places a higher requirement on fine-grained medical understanding. (v) Respect for medical reality. Datasets like PMC-15M used by LLaVA-Med typically feature paired instances where a text corresponds to a single image. However, it is often inconsistent with clinical practice where data redundancy exists. Our dataset addresses this by capturing many instances where one text is paired with multiple images from the same ultrasound video. For example, when the text describes a lesion, only images of frames scanned to the lesion are valid for mirroring the text, while those without lesion display are invalid. This challenges the model to distinguish valid images for reasoning and holds practical relevance."}, {"title": "4.2 Ultrasound Instruction-following Data", "content": "Inspired by LLaVA-Med, we generate Chinese ultrasound instruction-following data as Fig. 3. For one caption $X_c$ paired with k images $X_v$, we create an instruction-following example with question $X_q$:\nHuman : $X_q$ X\u2193... $X$<STOP>\\n Assistant : $X_c$<STOP>\\n"}, {"title": "5 Experiments", "content": "Besides our large-scale hospital ultrasound dataset, we evaluate models on two open-source Med-VQA datasets, shown in Tab. 1: (i) SLAKE [20] is an English-Chinese bilingual dataset and contains 642 images and over 7000 Q&A pairs covering 12 diseases and 39 organs, with CT, MRI, and X-Ray imaging modalities. The Q&As span various topics such as diagnosis, anatomical structure, and lesion location. Presently, SLAKE serves as a crucial benchmark for VQA tasks in the medical domain for its diversity. It has been utilized for evaluation purposes in significant medical multimodal large language model works, such as LLaVA-Med, Med PaLM M,"}, {"title": "5.1 Implementation Details", "content": "Datasets. Besides our large-scale hospital ultrasound dataset, we evaluate models on two open-source Med-VQA datasets, shown in Tab. 1: (i) SLAKE [20] is an English-Chinese bilingual dataset and contains 642 images and over 7000 Q&A pairs covering 12 diseases and 39 organs, with CT, MRI, and X-Ray imaging modalities. The Q&As span various topics such as diagnosis, anatomical structure, and lesion location. Presently, SLAKE serves as a crucial benchmark for VQA tasks in the medical domain for its diversity. It has been utilized for evaluation purposes in significant medical multimodal large language model works, such as LLaVA-Med, Med PaLM M,"}, {"title": "5.2 Performance and Comparisons", "content": "We provide the comparisons between LLaVA, LLaVA-Med, and our LLaVA-Ultra when employed as a medical visual chatbot. For qualitative comparisons shown in Fig. 4, LLaVA model for the general domain struggles with medical tasks, highlighting the gap between domains. While tailored for the medical domain, LLaVA-Med still inadequately addresses Chinese ultrasound scenes. It often focuses solely on textual medical concepts in the questions and offers vague and invalid answers that do not meet the questioner's needs. They tend to rely more on medical knowledge learned early from the LLMs rather than effectively incorporating medical visual features. This results in responses that display weak correlations"}, {"title": "5.3 Ablation Study", "content": "To assess the validity of the model's components, we conducted comparative experiments on several aspects shown in Tab. 3.\nVisual enhancement. To prove the necessity of visual enhancement, we replace our dual visual encoders and its fusion module with the original single CLIP encoder and observe the decrease of all metrics in Tab. 3. This verifies the necessity of strengthening the visual branch of MLLMs and underlines the effectiveness of our feature fusion strategy. The integration of SAM features enables LLaVA-Ultra to extract finer-grained visual semantics, which is a crucial aspect of handling subtle information in medical scenarios. Data redundancy adaptation. We remove our data redundancy adaptation module, the metrics in Table. 3 show a significant reduction. This highlights the importance of addressing data redundancy, which is prevalent in real medical scenarios but less noticed. In previous works, when multiple images correspond to the same text, such text is often assigned to each image. It results in mapping the images that don't mirror the specific text, i.e. redundant data, and the valid images to a similar feature representation. This hinders the model from learning accurate medical semantics and cross-modal alignment. As shown in the scores, LLaVA-Ultra full model can address this issue effectively with our adaption strategy. Specifically, our attention scoring strategy (Func.b) leverages rich textual data for feature alignment and thus achieves better scores compared to the simpler feature scoring (Func.a).\nData construction. We modified the instruction data by removing the cues of the examination site from the question. Instead, we ask a generalized question like give the diagnosis of this ultrasound image. Results in Tab. 3 indicate a slight decrease in evaluation metrics within acceptable limits. This demonstrates the effectiveness of limited cues for the model to learn specific medical knowledge, as well as the robustness of our model structure."}, {"title": "5.4 Limitation", "content": "Although LLaVA-Ultra shows impressive capabilities in Chinese medical multimodal understanding, it still has some limitations, including: 1) Its performance is hindered by the scale of the pretrained vision models. 2) Our large-scale medical dataset has not yet included more comprehensive labels e.g., segmentation to allow our model to engage further enhancement in visual perception."}, {"title": "6 Conclusion", "content": "We propose LLaVA-Ultra, a large Chinese language and vision assistant for the ultrasound domain. To achieve this, we create a high-quality visual-language instruction-following dataset from the large-scale professional ultrasound database of hospitals. More importantly, we improve the conventional visual language model structure by performing visual enhancement and data redundancy adaptation. It enables LLaVA-Ultra to fit the needs of fine-grained medical information and practical clinical scenarios, thus producing high-quality responses in medical visual conversations. On three medical VQA datasets, LLaVA-Ultra outperforms previous SOTA in various metrics, demonstrating its effectiveness and robustness."}]}