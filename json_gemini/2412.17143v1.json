{"title": "ASP-based Multi-shot Reasoning via DLV2 with Incremental Grounding", "authors": ["FRANCESCO CALIMERI", "GIOVAMBATTISTA IANNI", "FRANCESCO PACENZA", "SIMONA PERRI", "JESSICA ZANGARI"], "abstract": "DLV2 is an AI tool for Knowledge Representation and Reasoning which supports Answer Set Programming (ASP) a logic-based declarative formalism, successfully used in both academic and industrial applications. Given a logic program modelling a computational problem, an execution of DLV2 produces the so-called answer sets that correspond one-to-one to the solutions to the problem at hand. The computational process of DLV2 relies on the typical Ground & Solve approach where the grounding step transforms the input program into a new, equivalent ground program, and the subsequent solving step applies propositional algorithms to search for the answer sets. Recently, emerging applications in contexts such as stream reasoning and event processing created a demand for multi-shot reasoning: here, the system is expected to be reactive while repeatedly executed over rapidly changing data. In this work, we present a new incremental reasoner obtained from the evolution of DLV2 towards iterated reasoning. Rather than restarting the computation from scratch, the system remains alive across repeated shots, and it incrementally handles the internal grounding process. At each shot, the system reuses previous computations for building and maintaining a large, more general ground program, from which a smaller yet equivalent portion is determined and used for computing answer sets. Notably, the incremental process is performed in a completely transparent fashion for the user.", "sections": [{"title": "1 Introduction", "content": "Answer Set Programming (ASP) is a declarative problem-solving formalism that emerged in the area of logic programming and nonmonotonic reasoning (Brewka et al. (2011); Gelfond and Lifschitz (1991); Eiter et al. (2009)). Thanks to its solid theoretical foundations and the availability of efficient implementations (see Gebser et al. (2018) for a survey), ASP is recognized as a powerful tool for Knowledge Representation and Reasoning (KRR) and became widely used in AI.\nRules represent the basic linguistic construct in ASP. A rule has form Head \u2190 Body,\nwhere the Body is a logic conjunction in which negation may appear, and Head can\nbe either an atomic formula or a logic disjunction; a rule is interpreted according to\ncommon sense principles: roughly, its intuitive semantics corresponds to an implication.\nRules featuring an atomic formula in the head and an empty body are used to represent\ninformation known to be certainly true and are indeed called facts. In ASP, a compu-tational problem is typically solved by modelling it via a logic program consisting of a\ncollection of rules along with a set of facts representing the instance at hand, and then\nby making use of an ASP system that determines existing solutions by computing the\nintended models, called answer sets. The latter are computed according to the so-called\nanswer set semantics. Answer sets correspond one-to-one to the solutions of the given\ninstance of the modeled problem; if a program has no answer sets, the corresponding\nproblem instance has no solutions.\nThe majority of currently available ASP systems relies on the traditional \u201cGround &\nSolve\" workflow which is based on two consecutive steps. First, a grounding step (also\nsaid instantiation step) transforms the input program into a semantically equivalent\n\"ground\" program, i.e., a propositional program without first-order variables. Then, in\na subsequent solving step, algorithms are applied on this ground program to compute\nthe corresponding answer sets. There are other systems which, instead, are based on\napproaches that interleave grounding and solving or rely on intermediate translations\nlike the ones presented in Bomanson et al. (2019),Dal Pal\u00f9 et al. (2009), and Lef\u00e8vre\net al. (2017).\nIn the latest years, emerging application contexts, such as real-time motion tracking\n(Suchan et al. (2018)), content distribution (Beck et al. (2017)), robotics (Saribatur et al.\n(2019)), artificial players in videogames (Calimeri et al. (2018)), and sensor network\nconfiguration (Dodaro et al. (2020)), have been posing new challenges for ASP systems.\nMost of the above applications require to show high reactivity while performing the\nrepeated execution of reasoning tasks over rapidly changing input data. Each repeated\nexecution is commonly called \u201cshot\u201d, hence the terminology\u201cmulti-shot\" reasoning. In\nthe context of multi-shot reasoning, the na\u00efve approach of starting ASP systems at hand"}, {"title": "2 Overview of Overgrounding Techniques", "content": "In the following, we give an overview of the approach adopted by the system to efficiently\nmanage the grounding task in multi-shot contexts. We assume that the reader is famil-iar with the basic logic programming terminology, including the notions of predicate,\natom, literal, rule, head, body, and refer to the literature for a detailed and systematic\ndescription of the ASP language and semantics (Calimeri et al. (2020)).\nAs mentioned, ASP solvers generally deal with a non-ground ASP program P, made of\na set of universally quantified rules, and a set of input facts F. A traditional ASP system\nperforms two separate steps to determine the corresponding models, i.e. the answer sets\nof P and F, denoted AS(PUF). The first step is called instantiation (or grounding) and\nconsists in the generation of a logic program gr(P, F), obtained by properly replacing\nfirst-order variables with constants. Secondly, the solving step is responsible for com-puting the answer sets AS(gr(P, F)). Grounding modules are typically geared towards\nbuilding gr(P, F) as a smaller and optimized version of the theoretical instantiation\ngrnd(P, F), which is classically defined via the Herbrand base.\nWhen building gr(P, F), it is implicitly assumed a \u201cone-shot\u201d context: the instantiation\nprocedure is performed once and for all. Hence, state-of-the-art grounders adopt ad-hoc\nstrategies in order to heavily reduce the size of gr(P,F). In other words, gr(P, F) is\nshaped on the basis of the problem instance at hand, still keeping its semantics. Basic\nequivalence is guaranteed as gr is built in a way such that AS(PUF) = AS(grnd(P, F)) =\nAS(gr(P, F)).\nBased on the information about the structure of the program and the given input\nfacts, the generation of a significant number of useless ground rules can be avoided: for\ninstance, rules having a definitely false literal in the body can be eliminated. Moreover,\nwhile producing a ground rule, on-the-fly simplification strategies can be applied; e.g.,\ncertainly true literals can be removed from rule bodies. For an overview of grounding\noptimizations the reader can refer to Gebser et al. (2011), Calimeri et al. (2017), and\nCalimeri et al. (2019).\nHowever, this optimization process makes gr(P, F) \u201ctailored\u201d for the PUF input only.\nAssuming that P is kept fixed, it is not guaranteed that, for a future different input\nF', we will have gr(P, F) = AS(PU F'). Nonetheless, it might be desirable to maintain\ngr(P, F) and incrementally modify it, with as little effort as possible, in order to regain\nequivalence for a subsequent shot with input set of facts F'.\nIn this scenario it is crucial to limit as much as possible the regeneration of parts of\nthe ground programs which were already evaluated at the previous step; at the same\ntime, given that the set of input facts is possibly different from any other shot, shaping\nthe produced ground program cannot be strongly optimized and tailored to F' as in the\n\u201cone shot\u201d scenario. As a consequence, it is desirable that the instantiation process takes\ninto account facts from both the current and the previous shots. In this respect, Calimeri\net al. (2019), and Ianni et al. (2020) proposed overgrounding techniques to efficiently\nperform incremental instantiations.\nThe basic idea of the technique, originally introduced by Calimeri et al. (2019), is\nto maintain an overgrounded program G. G is monotonically enlarged across shots, in\norder to be semantics-preserving with respect to new input facts. Interestingly, the over-grounded version of G resulting at a given iteration i is semantics-preserving for all the\nset of input facts at a previous iteration i' (1 \u2264 i' \u2264 i), still producing the correct answer\nsets. More formally, for each i', (1 \u2264 i' \u2264 i), we have AS(GU F\u00bf\u00bf) = AS(PUF\u00bf\u00bf) After\nsome iterations, G converges to a propositional theory that is general enough to be reused\ntogether with large families of possible future inputs, without requiring further updates.\nIn order to achieve the above property, G is adjusted from one shot to another by adding\nnew ground rules and avoiding specific input-dependent simplifications. This virtually\neliminates the need for grounding activities in later iterations, at the price of potentially\nincreasing the burden of the solver (sub)systems, that are supposed to deal with larger\nground programs.\nOvergrounding with tailoring, proposed by Ianni et al. (2020), has been introduced with\nthe aim of overcoming such limitations by keeping the principle that G grows monoton-ically from one shot to another yet adopting fine-tuned techniques that allow to reduce\nthe number of additions to G at each step. More in detail, in the overgrounding with\ntailoring approach, new rules added to G are subject to simplifications, which cause the"}, {"title": "Example 2.1", "content": "Let us consider the program Pex:\na : r(X, Y) :- e(X, Y), not q(X).\nb: r(X, Z) | s(X, Z) :- e(X,Y), r(Y, Z).\nLet us assume at shot 1 to have the input facts F\u2081 = {e(3,1), e(1,2), q(3)}. In the\nstandard overgrounding approach we start from F\u2081 and generate, in a bottom-up way,\nnew rules by iterating through positive body-head dependencies, obtaining the ground\nprogram G\u2081:\na\u2081 : r(1, 2) :- e(1,2), not q(1).\nb\u2081:r(3,2) | s(3, 2) :- e(3, 1), r(1,2).\na2 : r(3, 1) :- e(3, 1), not q(3).\nIn the overgrounding with tailoring, rules that have no chance of firing along with\ndefinitely true atoms are simplified, thus obtaining a simplified program G\u2081:\na\u2081 : r(1, 2) :- e(1,2), not q(1).\nb\u2081: r(3,2) | s(3, 2) :- e(3, 1), r(1,2).\na2:r(3,1): c(3,1), not q(3).\nG\u2081 can be seen as less general and \u201cre-usable\" than G\u2081: a1 is simplified on the as-sumption that e(1,2) will be always true, and as is deleted on the assumption that q(3)\nis always true.\nOne might want to adapt G\u2081 to be compatible with different sets of input facts, but\nthis requires the additional effort of retracting no longer valid simplifications. In turn,\nenabling simplifications could improve solving performance since a smaller overgrounded\nprogram is built.\nLet us now assume that the shot 2 requires Pex to be evaluated over a different set of\ninput facts F2 = {e(3,1), e(1,4), q(1)}. Note that, with respect to F1, F2 features the\nadditions F+ = {e(1,4), q(1)} and the deletions F- = {e(1,2), q(3)}. In the standard\novergrounding approach, since no simplification is done, G\u2081 can be easily adapted to the\nnew input F2 by incrementally augmenting it according to F+; this turns into adding\nthe following rules AG\u2081 = {b2, a3}, thus obtaining G2:\na\u2081 : r(1, 2) :- e(1,2), not q(1).\nb\u2081: r(3,2) | s(3, 2) :- e(3, 1), r(1,2).\na2 : r(3, 1) :- e(3, 1), not q(3).\nb2: r(3, 4) | s(3,4) :- e(3,1), r(1,4).\nas : r(1, 4) :- e(1,4), not q(1).\nG2 is equivalent to P, when evaluated over F\u2081 or F2. Furthermore, G2 enjoys the\nproperty of being compatible as it is, with every possible subset of F\u2081 U F2. In the case\nof overgrounding with tailoring, G\u2081 needs to be re-adapted by undoing no longer valid"}, {"title": "3 The Incremental-DLV2 System", "content": "In this Section we present the Incremental-DLV2 system, an incremental ASP reasoner\nstemming as a natural evolution of DLV2 of Alviano et al. (2017) towards multi-shot\nincremental reasoning. We first provide the reader with a general overview of the compu-tational workflow and then discuss some insights about the main computational stages.\n3.1 Computational Workflow\nIncremental-DLV2 is built upon a proper integration of the overgrounding-based incre-mental grounder I\u00b2-DLV, presented by Ianni et al. (2020), into DLV2. Coherently with"}, {"title": "3.2 Implementation Details", "content": "The evaluation order taking place in the OVERGROUNDER module is carried out by con-sidering direct and indirect dependencies among predicates in P. Connected components\nin the obtained dependency graph are identified once and for all before at the beginning\nof the first shot: then, the incremental grounding process takes place on a per component\nbasis, following a chosen order. We report an abstract version of our INCRINST algorithm\nin Figure 2, where, for the sake of simplicity, we assume the input program P forms a\nsingle component.\nAt shot i, a new overgrounded program G' = DGUNR is obtained from G by iteratively\nrepeating, until fixed point, a DESIMPL step, followed by an Instantiate and Simplify step,\nwhich we call \u0394\u0399\u039dS\u03a4. A set AF of accumulated atoms, keeps tracks of possibly true ground\natoms found across shots; the set NR keeps track of newly added rules whose heads can\nbe used to build additional ground rules at current shot, while DG is a \u201cdesimplified\"\nversion of G.\nThe DESIMPL step properly undoes all simplifications applied on Gat previous shots\nthat are no longer valid according to Fi. This step relies on the meta-data collected during"}, {"title": "4 System Usage", "content": "Incremental-DLV2 can be executed either remotely or locally. In case of a remote execu-tion, clients can request for a connection specified via an IP address and a port number,\ncorresponding to the connection coordinates at which the system is reachable. Once a\nconnection is established, the system creates a working session and waits for incoming\nXML statements specifying which tasks have to be accomplished. The system manages"}, {"title": "5 Experimental Analysis", "content": "In this section we discuss the performance of Incremental-DLV2 when executing multi-shot reasoning tasks in real-world scenarios.\n5.1 Benchmarks\nWe considered a collection of real-world problems that have been already used for test-ing incremental reasoners. A brief description of each benchmark follows. The full logic\nprograms, instances and experimental settings can be found at https://dlv. demacs.\nunical.it/incremental.\nPac-Man (Calimeri et al. (2018)). This domain models the well-known real-time game\nPac-Man. Here, a logic program Ppac describes the decision-making process of an artificial\nplayer guiding the Pac-Man in a real implementation. The logic program Ppac is repeat-edly executed together with different inputs describing the current status of the game\nboard. The game map is of size 30 \u00d7 30, and includes the current position of enemy ghosts,\nthe position of pellets, of walls, and any other relevant game information. Several parts of\nPpac are \"grounding-intensive\", like the ones describing the distances between different\npositions in the game map. These make use of a predicate distance(X1, Y1, X2, Y2, D),\nwhere D represents the distance between points (X1,Y1) and (X2, Y2), obtained by taking\ninto account the shape of the labyrinth in the game map.\nContent Caching (Ianni et al. (2020)). This domain is obtained from the multi-media\nvideo streaming context (see Beck et al. (2017)). In this scenario, one of the common\nproblems is to decide the caching policy of a given video content, depending on variables"}, {"title": "5.2 Setting", "content": "We compared the herein presented Incremental-DLV2 system against the DLV2 system.\nBoth systems were run in single-threaded mode. Experiments have been performed on a\nNUMA machine equipped with two 2.8GHz AMD Opteron 6320 CPUs, with 16 cores and\n128GB of RAM. Differently from Incremental-DLV2, DLV2 is restarted, i.e., executed\nfrom scratch, at each shot in order to evaluate the given program on the current facts.\nFor each domain, we choose two different measures: we track the total accumulated time\nand the maximum memory peak per shot. Figure 4 plots the chosen measures against\nthe number of shots for all benchmarks; the X axis diagrams data in the shot execution\norder."}, {"title": "5.3 Execution times", "content": "When observing execution times, a clear advantage is experienced by Incremental-DLV2\nover all considered domains.\nThe Pac-Man benchmark is characterized by the need for computing all distances be-tween all positions (30\u00d730) in the game map, which is fixed, and hence Incremental-DLV2\nis able to compute them only once at the first shot, which is the most time expensive.\nAt each shot after the first, there is a net gain in grounding times stabilizing at an\napproximate speedup factor of 4.6; furthermore, given the nature of the available in-stances, which encode a real game, it turns out that, for a large part of the overgrounded"}, {"title": "5.4 Memory usage", "content": "Some additional considerations deserve to be done about memory usage. Indeed, as it\nis expected because of the incremental grounding strategy herein adopted, the memory\nfootprint is definitely higher for Incremental-DLV2, in all considered domains. However,\ninterestingly, it can be noted that in all cases the memory usage trend shows an asymp-totic \"saturation\u201d behaviour: after a certain number of shots the memory usage basically\nstays constant; hence, the price to pay in terms of memory footprint is not only coun-terbalanced by the gain in terms of performance, but it also happens to not \u201cexplode\".\nWe also observe that in the Content Caching and Photo-voltaic System benchmarks the\nmemory usage increases along the shots, while it is reached a sort of plateau in the Pac-Man benchmark. Indeed, in this latter domain a large amount of information, useful in\nall shots, is inferred only at the first shot and then kept in memory, but with some redun-dancy. As a result, the memory usage in this benchmark domain is high at the beginning\nof the shot series but it stays almost unchanged later. On the other hand DLV2 makes a\ngood job in generating, from scratch, a compact ground program per each shot. Although\nthe results show a fairly reasonable memory usage, as mentioned before, memory-limiting\nand rule-forgetting policies added on top of existing algorithms can help in mitigating\nthe memory footprint of Incremental-DLV2, especially in scenarios where memory caps\nare imposed."}, {"title": "6 Related Work", "content": "6.1 Theoretical foundations of incremental grounding in ASP\nThe theoretical foundations and algorithms at the basis of Incremental-DLV2 were laid\nout by Calimeri et al. (2019) and Ianni et al. (2020). The two contributions propose\nrespectively a notion of embedding and tailored embedding. Embeddings are families of\nground programs which enjoy a number of desired properties. Given PUF, an embedding\nE is such that AS(PUF) = AS(E); E must be such that it embeds (E Fr) all r\u2208\nground(PUF).\nThe operator is similar to the operator = which is applied to interpretations, and\nenjoys similar model theoretical properties. Intuitively, given interpretation I and rule r"}, {"title": "6.2 Other ASP systems with incremental features", "content": "The ASP system clingo Gebser et al. (2019) represents the main contribution related to\nmulti-shot reasoning in ASP. clingo allows to procedurally control which and how parts\nof the logic program have to be incremented, updated and taken into account among con-secutive shots. This grants designers of logic programs a great flexibility; however, the\napproach requires specific knowledge about how the system internally holds its computa-tion and on how the domain at hand is structured. It must in fact be noted that the notion\nof \"incrementality\u201d in clingo is intended in a constructive manner as the management\nof parts of the logic program that can be built in incremental layers. Conversely, in the\napproach proposed in this paper, the ability of using procedural directives is purposely\navoided, in favour of a purely declarative approach. Incrementality is herein intended as\nan internal process to the ASP system, which works on a fixed input program.\nThe Stream Reasoning system Ticker of Beck et al. (2017) represents an explicit effort\ntowards a more general approach to ASP incremental reasoning. Ticker implements the\nLARS stream reasoning formal framework of Beck et al. (2018). The input language\nof LARS allows window operators, which enable reasoning on streams of data under"}, {"title": "6.3 Incrementality in Datalog", "content": "The issue of incremental reasoning on ASP logic programs is clearly related to the problem\nof maintaining views expressed in Datalog. In this respect, Motik et al. (2019) proposed\nthe so called delete/rederive techniques, which aim at updating materialized views. In\nthis approach, no redundancy is allowed, i.e. updated views reflect only currently true\nlogical assertions: this differs from the overgrounding idea, which aims to materialize\nbigger portions of logic programs which can possibly support true logic assertions. Hu\net al. (2022) extended further the idea, by proposing a general method in which modular\nparts of a Datalog view can be attached to ad-hoc incremental maintenance algorithms.\nFor instance, one can plug in the general framework a special incremental algorithm for\nupdating transitive closure patterns, etc."}, {"title": "7 Future Work and Conclusions", "content": "As future work is concerned, we plan to further extend the incremental evaluation ca-pabilities of Incremental-DLV2, by making the solving phase connected in a tighter way\nwith grounding, in the multi-shot setting. Moreover, in order to limit the impact of mem-ory consumption, we intend to study new forgetting strategies to be automatic, carefully\ntimed and more fine-grained than the basic ones currently implemented. Besides helping\nat properly managing the memory footprint, such strategies can have positive impact\nalso on performance; think, for instance, of scenarios where input highly varies across\ndifferent shots: from a certain point in time on, it is very likely that only a small subset\nof the whole amount of accumulated rules will actually play a role in computing answer\nsets. As a consequence, accumulating rules and atoms may easily lead to a worsening in\nboth time and memory performance: here, proper forgetting techniques can help at selec-tively dropping the part of the overgrounded program that constitutes a useless burden,\nthus allowing to enjoy the advantages of overgrounding at a much lower cost. A variant\nof this approach has been proposed by Calimeri et al. (2024). Investigating the rela-tionship between overgrounded programs and the notion of relatived hyperequivalence\nof Truszczynski and Woltran (2009), possibly under semantics other than the answer set\none, deserves further research."}]}