{"title": "VernaCopter: Disambiguated Natural-Language-Driven Robot via Formal Specifications", "authors": ["Teun van de Laar", "Zengjie Zhang", "Shuhao Qi", "Sofie Haesaert", "Zhiyong Sun"], "abstract": "It has been an ambition of many to control a robot for a complex task using natural language (NL). The rise of large language models (LLMs) makes it closer to coming true. However, an LLM-powered system still suffers from the ambiguity inherent in an NL and the uncertainty brought up by LLMs. This paper proposes a novel LLM-based robot motion planner, named VernaCopter, with signal temporal logic (STL) specifications serving as a bridge between NL commands and specific task objectives. The rigorous and abstract nature of formal specifications allows the planner to generate high-quality and highly consistent paths to guide the motion control of a robot. Compared to a conventional NL-prompting-based planner, the proposed VernaCopter planner is more stable and reliable due to less ambiguous uncertainty. Its efficacy and advantage have been validated by two small but challenging experimental scenarios, implying its potential in designing NL-driven robots.", "sections": [{"title": "I. INTRODUCTION", "content": "The control of robotic systems typically requires clearly defined commands or specifications prescribed by experts. Controlling robots using natural language (NL) is still an open and challenging topic [1]. This is mainly due to the ambiguity of NL, which is attributed to its implicit and complex semantics [2]. Conventionally, language-driven robot control has been achieved by introducing an NL processing (NLP) unit that translates human language into concrete tasks [3], [4]. The recent rise of Large language models (LLMs) like ChatGPT [5] and Gemini [6] provides an efficient manner to control robots from NL commands directly. This offers a practical way for non-expert users to control robots using NL [7]. Facilitated by a transformer architecture, LLMs can capture the relationships between words and phrases in a sentence by leveraging the attention mechanism [8]. Therefore, LLMs can understand the context of an NL command and reason about its logic, bringing the gap between ambiguous language and explicit specifications [9]. Successful instances of language-controlled robots include manipulation [10], navigation [11], and conversation services [12]. A critical technical point of ensuring success in LLM-enabled robot control is designing an appropriate prompting scheme, for which prompt engineering provides useful guidelines [13]. A survey on LLM-controlled robots can be referred to in [14].\nOn the other hand, LLMs are also known for their drawbacks in ambiguity and uncertainty [15], [16]. Specifically, LLMs are sensitive to the input prompts. For the same prompts, LLMs can return distinguished outputs at different instances, potentially leading to additional uncertainty when used for robot control. An additional source of ambiguity and uncertainty is the spatial and numerical reasoning limitations that prevent the LLM from correctly reconstructing the logical interdependence among different subtasks in a complex task [17], [18]. It has been argued that LLMs are less stable and reliable for complicated robot motion planning tasks than simple ones [19]. Take the scenario illustrated in Fig. 1 as an example, where a quadcopter needs to retrieve a treasure from a chest in a room behind a locked door, which can be opened with a key. This renders a complex task containing a sequence of three interdependent subtasks, namely key fetching, door unlocking, and box opening. In the rear part of this paper, we will use a case study to show that an LLM does not always succeed in this task since it cannot properly reconstruct the logical relations between these subtasks from the NL prompts."}, {"title": "II. RELATED WORK", "content": "In this section, we elaborate on the related work on prompt engineering and formal specifications, the two most important technologies to specify tasks for LLM-controlled robots.\nSpecifying Tasks via Prompts\nIn NLP, where the model's weights are not affected by each request, the quality and specificity of generated outputs are highly dependent on the formulation of the prompting inputs. These prompts encompass both general instructions provided upfront, describing the LLM's tasks and limitations, and specific prompts users provide during interaction. In designing the system, the general task descriptions for the LLM are prompt-based, so using appropriate prompts is critical for the system's behavior and performance. However, applying appropriate prompting techniques is also essential for the system's users. Several techniques exist for effective prompting in NLP. Some helpful information on prompting structures using ChatGPT techniques is presented in [33], [34]. The work in [35] argues that long-horizon tasks possibly lead to more uncertainties which can be improved by human feedback, rendering a conservation-based prompting scheme. In [36], a prompt-based scheme is developed to induce an LLM to point out the successive subtasks. There are also attempts dedicated to promoting the precision of LLM using additional machine learning components, such as text encoders [37]. In [38], visual-language models are studied to provide a solid foundation for potential future research concerning applying LLMs to robotics. Moreover, it categorizes prompting tech- niques into the following types which are partially generated using ChatGPT and are carefully reviewed and refactored.\nTask instruction prompting describes a task explicitly in as much detail as necessary to define the task thoroughly.\nIn-context learning relies on several examples closely related to the task to generalize to new tasks.\nChain-of-thought prompting encourages a model to provide intermediate reasoning steps rather than returning a single result as a response to an instruction. This method can enhance the quality of the response significantly [39].\nThese prompting techniques have different strengths. Task instruction prompting can be used to describe tasks and lim- itations. In-context learning is particularly useful when using a pre-trained LLM, but the model is not specifically trained in the context in which it operates. In this case, examples can"}, {"title": "B. Specifying Tasks via Formal Specifications", "content": "Different from NL, formal specifications provide precise descriptions for robotic tasks [40]. Formal language-based motion planning is a well-studied subject, and motion plans generated from formal language are optimal and safe by design [41]. In this sense, a straightforward approach is to use an LLM model to translate tasks specified in NL into a formal specification which is then synthesized for robot motion planning [11], [42], [43].\nNevertheless, introducing formal language in LLM-based robotics frameworks brings additional challenges. Correctly generating formal language specifications from NL is non- trivial due to the ambiguous nature of NL [44]. Furthermore, LLMs are black box systems, meaning their outputs are not traceable and could be inconsistent when given the same input. The syntax of generated specifications may conflict with the original, and the specified task and the corresponding gener- ated formal language specification could be semantically mis- aligned. Additionally, while the code/action sequence-based methods could leverage LLMs' extensive code training, LLMs are not directly trained on formal language data quantities of the same magnitude.\nThe challenges of syntactic errors and semantic misalign- ments of formal language specifications can be addressed by applying LLMs to identify and correct faulty results. Iterative checking and correcting modules are used in [11], correcting syntax using a rule-based approach and aligning semantics by re-prompting the LLM with the original task description. The challenge of missing extensive training in using formal speci- fications can be addressed by deploying prompting techniques. Appropriate prompting can provide information to pre-trained LLMs to reason about and construct STL specifications."}, {"title": "III. VERNACOPTER PLANNER SYSTEM", "content": "This section introduces the composition of the VernaCopter planner system. The overall architecture and workflow of the system are first interpreted, followed by a detailed elaboration of its specific components.\nOverall Architecture\nThe overall architecture of the VernaCopter planner is illustrated in Fig. 3. An LLM as a planning assistant (PA) serves as a human-robot interface, translating the user com- mand in NL into STL specifications. The detailed definition of STL can be referred to in Appx.-A. The user command includes a task description as one-shot system instruction and iterative prompts in the form of question-and-answer (Q&A) conversations. Another two LLM-based checkers are used to validate the correctness of the specifications, from the syntactical and semantic perspectives, respectively. Then, the system model with the generated specifications is synthesized using an optimizer, with the resulting trajectories analyzed for automatic improvement and visualized for manual inspection. The workflow of the VernaCopter planner is summarized as three loops, marked as arrowed circles with numbers in Fig. 3. Loop 1 involves the PA generating an STL formula according to the user prompts. A Syntax Checker (SynCheQ) checks the correctness of the STL formula and gives the corrected formula to an optimizer. The solved path solution is visualized in a simulation environment, such as Pybullet, for the user to inspect. In the case of a solving error, such as infeasibility, the optimizer returns an error flag to SynCheQ to fine-tune the STL formula, forming Loop 2. In Loop 3, a path analyzer (PAZ) is designed to analyze the quality of the generated path. According to its output, a Semantics Checker (SemCheQ) provides improvement advice to the PA taking into account the task description.\nIn general, Loop 1 ensures the syntactic correctness of the generated specification, Loop 2 guarantees the feasibility of the specification, and Loop 3 promotes the quality of the path. Different from the existing loop-based prompting schemes [11], the VernaCopter planner only uses loops to improve its performance, instead of aiming at a converging generation. In Sec. V, experiments will showcase that prop- erly designed one-shot prompts can achieve decent planning performance without performing loop-based iterations.\nPlanning Assistant (PA)\nThe PA is an LLM agent that translates a user-defined task formulated in NL to an STL specification. In this paper, we adopt a pre-trained GPT model from OpenAI [5], [45], namely GPT-40. The LLM is instructed on its general task and is provided with a library of available STL functions and operators and several examples of correct uses of STL. In this basic form of the architecture, the syntactic correctness of the specification is ensured by providing examples of user inputs and desirable outputs. The examples provided partially ensure the semantic alignment of the task and the generated STL"}, {"title": "C. Syntax Checker and Optimizer", "content": "The PA may not always generate a meaningful STL specifi- cation. Instead, LLM may produce syntactic errors due to the lack of correctness guarantees, leading to potential failure of the robot task. In this paper, we employ an LLM as a Syntex Checker (SynCheQ) to correct the possible syntactic errors of the generated STL formula. The SynCheQ is prompted with initial instructions that contain similar information to the PA. However, the general explanation and examples are targeted more toward syntax checking. The LLM is explicitly instructed not to change the semantics of the specification. SynCheQ either revises the STL if a syntactic error is found or exports the originally correct STL. The exported STL is synthesized by the optimizer via a model-checking process using a python package stlpy [46]. The details of the model-checking and synthesis processes can be referred to in Appx.-B."}, {"title": "D. Visualizer", "content": "Once the final robot trajectories are generated, the behavior of VernaCopter is visualized using gym-pybullet-drones [47], a drone simulation environment based on OpenAI's Gym [48]. The motion of the drone is controlled using a pre-tuned PID controller implemented in gym-pybullet-drones. A fixed control frequency is used to control the drone in the simulation. To make the drone follow the path defined by the waypoints, the target waypoint is switched to the next one in the sequence with this same frequency. The user can accept or reject the proposed path based on this information. Whenever a path is rejected, the user can correct the reasoning and output from the LLM. An expert on STL or a technical user can use the specification to correct any semantic misalignment between the specification and the task. However, the user is not required to know STL. The LLM can assist the user by explaining the details of the generated specification when requested. Based on the new instructions, a new path is generated. This process forms Loop 1 which is repeated until the task is satisfied."}, {"title": "E. Semantics Checker and Path Analyzer (PAZ)", "content": "Although the correctness and feasibility of the generated STL are ensured by the SynCheQ in Loops 1 and 2, it may not align with the task description. In many cases, it may not precisely characterize the interdependence among different subtasks. Thus, a semantic alignment checker (SemCheQ) is deployed to correct these oversights. SemCheQ is an LLM agent that judges whether the task description and the gen- erated path are semantically identical. To facilitate this, a PAZ is designed to analyze the generated path and send an automatically generated textual description of this path to the SemCheQ. Specifically, the PAZ checks which regions or objects are traversed throughout the generated path. The exported textual description contains as many elements as there are objects in the scene. Each description describes whether the drone a) stays in that object's bounding box at all times, b) is outside the bounding box at all times, or c) is inside the bounding box of the object during some specific times only. The specific time steps are also returned to SemCheQ in the latter case. When the task description and the generated path are not semantically identical, SemCheQ forwards an advice message to the PA to improve the specification."}, {"title": "IV. LEVERAGING INSTRUCTIVE PROMPTING", "content": "The performance of an LLM-centered system highly de- pends on the quality of prompts. Similar to other LLM- centered systems mentioned in Sec. II, the performance of the VernaCopter planner can be catalyzed by properly designed prompts. The proposed VernaCopter planner in general sup- ports one-shot prompting which can already achieve decent performance for small robot motion planning tasks. The user directly gives the command in NL to the PA, with the system settings and models sent to the PA as task descriptions. When starting the program, the LLM is automatically prompted with general instructions. These instructions contain information about the general task, the environment, available functions, operators, the syntax to construct the STL specification, how to interact with the user, and examples of appropriate responses. The details of these instructions are shown in Appx.-C. After the generation of the robot path, the user inspects the path via the visualizer to decide whether the task is carried out as intended. If unsatisfied, the user may reject the generation result and perform another round of generation.\nThe planner can also be extended to a conversation mode allowing the user to continually give instructions to the PA and confirm whether it receives all necessary information for the given task. The LLM decides when it has gained enough information to generate a specification, and a set of waypoints is generated using the specification. The chain-of- thought reasoning generated by the LLM is printed to the user, and the generated waypoints are visualized in an environment abstraction for validation. The user is not obligated to provide the full task or all restrictions directly, as the assistant is instructed to elucidate missing details collaboratively. The user can converse as they see fit in response to the LLM's outputs. During the entire process, the user does not need to interfere with the generation process of the STL specification.\nThe proposed prompting instructions are not specific to particular scenarios but can be customized to fit various settings in diverse scenarios. To this end, the objects and regions are provided independently for each environment. Task instruction prompting is used to explain the general nature of the task and in-context learning is applied in a few-shot fashion, i.e., by providing several examples of prompts and appropriate responses. The LLM is encouraged to use chain- of-thought reasoning to improve the correctness of the outputs."}, {"title": "V. EXPERIMENTAL STUDIES", "content": "In this section, we use experimental studies to showcase how the VernaCopter planner ensures a high success rate for a robot task with reduced ambiguity, showing an advantage over a conventional prompting scheme without leveraging formal specifications. Two representative scenarios with dif- ferent complexity, namely a reach-and-avoid (R&A) task [49] and a treasure hunt task are considered. Both scenarios are designed in a 3D Cartesian space. Their top-to-bottom views are illustrated in Fig. 4. Each case contains a comparison study with a conventional NL-based planner.\nCase I: Reach-and-Avoid (R&A)\nThe R&A scenario is located in a 3D cubic space with one goal (green) and several obstacles (red) of different sizes, as shown in Fig. 4a. The VernaCopter is required to ultimately reach the goal from the opposite corner without colliding with any obstacles. It renders an essential but representative motion planning case that does not contain any subtasks. Instead, we define the scenario in a 3D space and intentionally involve dense obstacles to increase the task's difficulty. This scenario is designed to assess the basic functionality of the Vernacopter for a simple task in a crowded environment. The result can reflect whether the planner has a stable and reliable performance for a small but difficult problem.\nFor both the VernaCopter and the conventional NL-based planners, we use the one-shot prompting template presented in Appx.-C, with the task description and generation rules predefined for the PA as the system instruction. The user just needs to input a short and brief command in NL as \u201cReach the goal while avoiding all obstacles.\u201d. The conventional prompt- based planner has a similar structure to the VernaCopter planner except that the PA is asked to directly generate the robot path instead of STL specifications. Moreover, for a fair comparison, the conventional planner is also equipped with a similar SemCheQ to ensure alignment with the task description. For all planners, we perform multiple trials (50 for VernaCopter and 45 for conventional) to inspect whether they generate successful paths.\nThe robot paths generated from multi-trial experiments are illustrated in Sec. 5. It can be seen from Fig. 5b that the conventional NL-based planner generates highly variant paths, reflecting a high level of ambiguity and uncertainty of the LLM agent. Moreover, most paths present piece-wise linear shapes, showing the lack of smoothness. Moreover, many paths fail to reach the goal or even collide with the obstacles, leading to failure of the R&A task. This can also be quantitatively viewed from Tab. I that only 51% of its trajectories ultimately reach the goal and only 36% successfully avoid collisions.\nOn the contrary, the VernaCopter planner successfully achieves goal-reaching and collision avoidance for all trials, as shown in Tab. I. Fig. 5a also shows that it generates highly consistent paths. This reflects that the involvement of STL successfully reduces the uncertainty brought up by the LLM agent. Moreover, the generated paths have decent smoothness due to the customized optimizer. This reflects another advantage of the proposed VernaCopter planner that the specific shape of the path is determined by a precise mathematical solver instead of the LLM itself. Since the Vernacopter planner with one-shot prompts already shows decent performance, extending it to a conversation-prompting mode is not necessary for this scenario.\nCase II: Treasure Hunt\nThe treasure hunt scenario has been mentioned briefly in Sec. I. As shown in Fig. 4b, the VernaCopter must reach the treasure chest (yellow) in an interdependent order: key (green) \u2192 door (blue) \u2192 chest. During the process, VernaCopter cannot leave the room or collide with the walls. This case is an original scenario that typically represents a compact motion planning problem with interdependent subtasks. The robot not only needs to reach several goals but also must follow a certain sequential order. This case can validate whether a semantic-enabled planner can correctly reconstruct the solution space from the user prompts and reason about the best solution.\nFor both the VernaCopter and the conventional planners, we use the one-shot prompting template presented in Appx.- C, with the task description and generation rule predefined for the PA as the system instruction. The one-shot command is given as \u201cGo to the key in the first 30 seconds, then go to the"}, {"title": "VI. DISCUSSION", "content": "The experimental results in Sec. V reflect an obvious advan- tage of the proposed formal-specification-enabled VernaCopter planner over a conventional NL-based planner with better generation consistency and higher flexibility in path shape control. The main reason for such an advantage is that the involvement of formal specifications as a bridge between NL and robot control command can effectively reduce the ambiguity in NL and the uncertainty brought up by LLM. Based on this, proper design of prompts is also important to guide the LLM-based PA to generate the desired results. Nevertheless, compared to the conventional prompt-based robot planner, prompting for VernaCopter is easier since it has a more focused scope. Specifically, it only needs to focus on inducing the precise generation of desired formal specifications and let a specific optimizer take care of the quality of the generation results. Here, we have utilized the twofold property of formal specifications. Its precise nature allows for generating consistent results without bringing up additional uncertainty. Meanwhile, its abstract nature has a closer distance to NL than the objective of a specific robot task. From this perspective, we believe formal-language-powered LLM systems are promising to be a practical solution to inspire applicable language-enabled robots.\nAlthough the VernaCopter planner designed in this paper serves as a decent prototype, there are still some limitations that may induce interesting studies in the future. Firstly, the planner may generate infeasible or conflicting specifications, leading to generation failure, even though a feasible path does exist for the given task. The possible reason is due to an ill- defined specification, where the constraints of the task are too restrictive. On the other hand, this also reflects that the proposed VernaCopter planner does not ensure completeness of the task execution, although sufficiency can be guaranteed thanks to SynCheQ. One possible solution to resolve this issue is to separate or decompose a long and complex specification into shorter or simpler ones [50]. In this sense, prior knowl- edge and heuristics are leveraged to simplify the problem for LLM. Another solution is to fine-tune LLM such that it is more suitable for generating formal specifications, in a sense training a new formal language expert. Previous research [22] has investigated fine-tuning pre-trained LLMs to increase output specificity toward the method and syntax used. However, it is difficult to obtain large enough datasets to perform such fine-tuning, and current methods have to do with weak supervision using few-shot learning. Nevertheless, we believe fine-tuning would be worth investigating in the future. Then, LLM-based formal language expert is expected to greatly promote the performance of language-enabled robots."}, {"title": "VII. CONCLUSION", "content": "This paper proposes a novel LLM-centered robot motion planner leveraged by formal specifications. Compared to the conventional NL-based planner, it ensures high consistency and reliability due to the reduced ambiguity and uncertainty. The current prototype still suffers from conservativeness and possible infeasibility which could be addressed by specifica- tion decomposition approaches or LLM fine-tuning. Future work will also look into improving the proposed planner in complicated scenarios with conversational prompts."}, {"title": "APPENDIX", "content": "Signal Temporal Logic (STL)\nThe specification for a real-valued signal can be described using STL. For a discrete-time signal $x := x_0x_1\u00b7\u00b7\u00b7$, where $x_k \\in R^n$ for $k\\in N$ and $n \\in Z^+$, the syntax of STL is recursively defined as [51]\n$\\varphi::= \\top|\\mu|\\neg\\varphi| \\varphi_1 \\land \\varphi_2 | \\varphi_1\\mathcal{U}_{[k_1,k_2]}\\varphi_2$,\nwhere $\\varphi$, $\\varphi_1$, and $\\varphi_2$ are STL formulas, $\\mu$ is a predicate\n$\\mu:=\\begin{cases}\\top, & h(x_k) \\geq 0 \\\\\\bot, & h(x_k) <0\\end{cases}$ evaluating a function $h: R^n \\rightarrow R$, $k\\in N$, and are the negation and the conjunction operators respectively, and $\\mathcal{U}_{[a,b]}$ is the until operator associated with a time interval $[k_1,k_2]$, with $k_1,k_2 \\in N$ and $k_1 \\leq k_2$.\nThe disjunction, eventually, and always operators are defined as $\\varphi_1 \\vee \\varphi_2 := \\neg (\\neg\\varphi_1 \\wedge \\neg\\varphi_2)$, $\\mathcal{F}_{[a,b]}\\varphi := \\top\\mathcal{U}_{[a,b]}\\varphi$, and $\\mathcal{G}_{[a,b]}:=\\neg (\\top\\mathcal{U}_{[a,b]}\\neg\\varphi)$.\nThe satisfaction of an STL formula $\\varphi$ given a signal $x$ and time $k$ is denoted by $(x, k) \\models \\varphi$ and given by the following recursively defined semantics.\n$(x, k) \\models \\neg \\varphi \\Leftrightarrow \\neg((x, k) \\models \\varphi)$,\n$(x, k) \\models \\varphi_1 \\wedge \\varphi_2 \\Leftrightarrow (x, k) \\models \\varphi_1 \\wedge (x, k) \\models \\varphi_2$,\n$(x, k) \\models \\varphi_1 \\vee \\varphi_2 \\Leftrightarrow (x, k) \\models \\varphi_1 \\vee (x, k) \\models \\varphi_2$,\n$(x, k) \\models \\mathcal{F}_{[k_1,k_2]} \\varphi \\Leftrightarrow \\exists k' \\in [k + k_1, k + k_2], \\text{ s.t. } (x, k') \\models \\varphi$,\n$(x, k) \\models \\mathcal{G}_{[k_1,k_2]} \\varphi \\Leftrightarrow \\forall k' \\in [k + k_1, k + k_2], \\text{ s.t. } (x, k') \\models \\varphi$,\n$(x, k) \\models \\varphi_1\\mathcal{U}_{[k_1,k_2]}\\varphi_2 \\Leftrightarrow \\exists k' \\in [k + k_1, k + k_2], \\text{s.t. } (x, k') \\models \\varphi_2 \\text{ and } \\forall k'' \\in [k, k'], (x, k'') \\models \\varphi$.\nFor $k = 0$, the symbol $k$ can be omitted, rendering $x \\models \\varphi$.\nThe robustness of an STL formula $\\varphi$ denoted as $\\rho^\\varphi(x, k)$, with $\\rho^\\varphi(x, k) > 0 \\Leftrightarrow (x, k) \\models \\varphi$, is inductively defined as\n$\\rho^\\mu(x, k) := h(x_k), \\rho^{\\neg\\varphi}(x,k):=-\\rho^\\varphi (x, k)$,\n$\\rho^{\\varphi_1\\land\\varphi_2}(x, k):=\\min(\\rho^{\\varphi_1}(x, k), \\rho^{\\varphi_2} (x, k))$,\n$\\rho^{\\varphi_1\\vee\\varphi_2}(x, k) :=\\max(\\rho^{\\varphi_1}(x, k), \\rho^{\\varphi_2} (x, k))$,\n$\\rho^{\\mathcal{F}_{[a, b]}\\varphi} (x, k) := \\max_{k'\\in[k+a, k+b]} \\rho^\\varphi (x, k'),$\n$\\rho^{\\mathcal{G}_{[a, b]}\\varphi} (x, k) := \\min_{k'\\in[k+a, k+b]} \\rho^\\varphi (x, k'),$\n$\\rho^{\\varphi_1\\mathcal{U}_{[a, b]}\\varphi_2}(x, k) := \\max_{k'\\in[k+a, k+b]} (\\min(\\rho^{\\varphi_2} (x, k'), \\min_{k'' \\in [k, k']}\\rho^{\\varphi_1} (x, k'')))$.\nFor $k = 0$, the robustness reads $\\rho^\\varphi(x)$.\nSystem Models and Motion Planning\nThe VernaCopter is described as a double-integrator model,\n$\\begin{cases}p_{t+1} = p_t + \\Delta t v_t,\\\\v_{t+1} = v_t + \\Delta t a_t\\end{cases}$\nwhere $p_t, v_t, a_t \\in R^3$ are the position, linear velocity, and acceleration of the VernaCopter at time $t \\in N$, and $\\Delta t \\in R^+$ is the discrete sampling time. Let us denote its path as a signal $x := (p_0, v_0) (p_1, v_1)\u00b7\u00b7\u00b7 (p_\\tau, v_\\tau)$ within a finite control horizon $T \\in Z^+$. A motion planning task can be specified using an STL formula $\\varphi$ as defined in Sec. VII-A, with a robustness constraint $\\rho^\\varphi(x) > 0$. Moreover, the motion plan- ning task usually requires minimizing certain cost, typically $J(x, u) := \\sum_{t=0}^T (v_t^T Q v_t + a_t^T R a_t)$, where $u := a_0a_1... a_T$ is the control sequence and $Q, R \\in R^{3\\times3}$ are cost matrices. Then, motion planning can be formulated as the following op-"}]}