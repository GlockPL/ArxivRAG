{"title": "CO-LEARNING: CODE LEARNING FOR MULTI-AGENT\nREINFORCEMENT COLLABORATIVE FRAMEWORK WITH\nCONVERSATIONAL NATURAL LANGUAGE INTERFACES", "authors": ["Jiapeng Yu", "Yuqian Wu", "Yajing Zhan", "Wenhao Guo", "Zhou Xu", "Raymond Lee"], "abstract": "Online question-and-answer (Q&A) systems based on the Large Language Model (LLM) have\nprogressively diverged from recreational to professional use. This paper proposed a Multi-Agent\nframework with environmentally reinforcement learning (E-RL) for code correction called Code\nLearning (Co-Learning) community, assisting beginners to correct code errors independently. It\nevaluates the performance of multiple LLMs from an original dataset with 702 error codes, uses\nit as a reward or punishment criterion for E-RL; Analyzes input error codes by the current agent;\nselects the appropriate LLM-based agent to achieve optimal error correction accuracy and reduce\ncorrection time. Experiment results showed that 3% improvement in Precision score and 15%\nimprovement in time cost as compared with no E-RL method respectively. Our source code is\navailable at: https://github.com/yuqian2003/Co_Learning.", "sections": [{"title": "Introduction", "content": "Large Language Model (LLM) based Conversational Question Answer such as ChatGPT become prominent deep\nlearning networks recognized by daily affairs enquiries to professional tasks solution [1, 2]. They can demonstrate\nreasoning, planning strengths to match an autonomous agent's definition to perceive its surroundings, make decisions,\noperate and even build multi-agents to solve complex problems [3, 4]. Although the majority of multi-agent frameworks\ncan usually complete stationary streaming tasks using fixed prompts but are unable to select the optimal agent according\nto specific task content [5].\nProgram coding involves time-consuming professional skill due to the specific task's requirement. Beginners often\nstrived for code understandings but may cede programming due to the lack of guidance to resolve unforeseen errors. This\npaper proposes a Code-Learning community based on LLM multi-agent framework on codes correction, annotation for\nefficient learning in communication with users. It uses reinforcement learning to decide which agent is required for the\nnext step based on the input problem or the output generated by current agent, in contrast to previous multi-agent code\ngeneration, error-correction networks based on a defined single stream [9]. There are 5 agents responsible for different\ntasks: 1) Main Agent supervises and exchanges information with users, 2) Correction Agent revises programming, 3)\nInterpretation Agent explains the programming logic to subsequent agents to locate incorrect codes, 4) Test Agent\ngenerates correct codes and 5) Annotation Agent adds comments to the revised code for user's understanding. These\nfive Agents communicate through conversation interfaces. The Multi-Agent generated by the main one is a copy with\nE-RL to self-improve and feedback to both counterparts and human users. Co-Learning uses ERNIE [6], SparkDesk\n[7], and LLaMa [8] as base models for different agents. Code error correction with E-RL performance is evaluated by\npassing probability tests, single loop computation time and numbers of loop required etc. The annotation results are\nevaluated by an expert reviewer through the location, accuracy, and comprehensibility of annotations."}, {"title": "Related Work", "content": ""}, {"title": "Prompting with Feedback", "content": "Recent research on large language models has shown that effective use of prompt words can reduce adverse output\n[10] and induce LLM to generate crucial assessments [5]. Prompt engineering is a specialized study with remarkable\nbenefits for reasoning type tasks [11]. Reflexion [12] pointed that using linguistic feedback can reinforce LLM instead\nof weights to store the feedback text in memory, and induce the large language model to make better decisions,\nallowing the language agent can learn by mistakes efficiently. DEAR [13] can improve LLM judgement in clinical\nmedicine by simulating two agents converse with each other, so that the researcher agent can process information,\nextract key points of the problem and the decision maker agent integrates them from the researcher agent to judge\nthe final output accordingly. Self-Debugging [9] interprets its self-generated code assisting LLM to identify code\nerrors without explicitly pointing out the errors and modifications by mimicking a rubber duck testing performed by\nhuman programmers without extra instructions. When the generated code fails to pass the test, Co-Learning interprets\nand modify its self-generated code according to the memorized linguistic feedback. At the same time, reinforcement\nlearning will automatically select the optimal agent for the next action based on the feedback from the current agent."}, {"title": "Multi-Agent Framework", "content": "Multi-Agent frameworks emerged at the end of the 20th century [14], when software engineers used Java to write\nmulti-agents for computers to perform by splitting into small, separate tasks that allow agents to focus and co-operate\nwith each other. At the beginning of the 21st century [15], JADE [16] standardized the Multi-Agent standard and was\nused in finance, trading, and journalism [17].\nPADE is Multi-Agent framework based on Python [18]. LLMs upsurge in GPT [8] allowing single agents to replace\nprograms [6]. LLM's cognitive abilities in single agents have provided a Multi-Agent foundation [21]. There are many\nexperiments shown that complex, dynamic tasks can be completed by multiple large language model agents equipped\nwith strategies and communications[22]. Hence, Co-Learning uses a PADE framework to create agents with functions,\nwhereas multiple LLMs as the core, component Multi-Agent framework, and information transfer between individual\nagents to achieve a dynamic workflow."}, {"title": "Reinforcement Learning", "content": "Reinforcement learning (RL) is a kind of machine learning where machines interact with an environment to achieve\nobjective [23]. During each interaction, the machine considers the environment's current state, makes decision, operates,\nobserves changes, and transfers feedback into rewards for subsequent round's state. It aims to maximize the expected\ncumulative reward over time. The agent that represents the decision-making machine in RL, not only perceives\nenvironmental information but also modifies the environment through its actions [24]. The agent's perception involves\nmaking limited observations of the environment state, such as observing the board situation in Go [25] or the road\ncondition in a self-driving car [26], and computes feasible actions by considering the state and policy based on these\nobservations.\nLLMs' current reinforcement learning mainly focuses on fine-tuning techniques based on feedback [20], minor\nsubstitution weights via memory [9, 13, 27], using prompts to help large language models for better strategy decisions.\nHence, Co-Learning uses memory while using the current agent's output and performance as reward or penalty to select\nthe best agent for the next action and enable reinforcement learning at environment operating level."}, {"title": "Methodology", "content": ""}, {"title": "Proposed Multi-Agent code correction framework (Co-Learning)", "content": "A Co-Learning Multi-Agent code correction framework is illustrated in Fig. 1 It has a coexistent framework that relies\non PADE [18] and the entire workflow runs in the environment created by the Main Agent. To begin, a Correction\nAgent uses a default large language model to make an initial modification for the input error code, returns the generated\nsentence, and transmits it to the Test Agent. The Test Agent performs tests based on the test samples from data set. If\nthe code passes all tests meaning the generation is correct, the Test Agent will send the code to the Annotation Agent\nfor annotation and output it as correct code. If the code is unable to pass any test, the generated code will be passed to\nthe Interpretation Agent and store the interpretation in memory as an environmental reinforcement learning prompt.\nThen, an error code will transfer to the Correction Agent selected by reinforcement learning to re-generate a code based\non the memorized code and interpretation. A loop will be formed by passing the generated result back to the Test Agent.\nError codes entered by outsiders during actual use are not included the test cases in the test dataset., three forms of\ntests used by Test Agent will be provided: test samples entered by the user, test samples generated by LLM based on\nuser-typing-requirement, and the code correctness determined directly by LLM.\nFor different agents, Main Agent stores all hyper-parameters and historical information, using E-RL based on other\nagent feedback to update the state of environment. Test Agent create Namespaces to declare the generated code, using\ntest cases to check the generated code, return test results and error messages. Rest of Agents clarify their tasks according\nto the prompt words, combine with historical information to generate input streams for the LLM, and return results to\nthe Main Agent for storage. Co-Learning involves agents cooperating with each other, mimicking human rubber duck\ntesting while using unit test feedback, selecting the most appropriate large language model in a real-time manner based\non E-RL, to enhance the performance of code error correction."}, {"title": "Python Agent Development (PADE)", "content": "Python Agent DEvelopment (PADE) is a simple python-based approach to create agents that can be accessed by\ndifferent devices [18]. This enables the development and create communication networks for different agents according\nto FIPA standards. PADE is an architecture based on Twisted to develop a multi-agent application using its library\nresources (Library) and perform a Running Environment (Running Environment) of a distributed system. PADE\ncontrols the platform by creating an agent (Agent Management System) responsible for platform operations, realize for\ninternal platform functions and migrate agents out of the platform to other platforms.\nA PADE architecture is depicted in Fig. 2. It consists of seven modules with the following functions:\n\u2022 Core: All Agents will inherit this base Agent kernel framework when created;\n\u2022 Behaviors: A behavioral template implemented by the Agent that can be inherited by the user to define a\nvariety of personalized behaviors according to FIPA standards;"}, {"title": "Environmentally Reinforcement Learning (E-RL)", "content": "The environmentally reinforcement learning (E-RL) aims to provide a structured environment for LLM-based agent\nto facilitate its effective execution in code error correction and testing. By using E-RL, LLM-based agent can select\nactions based on the current state and history to perform code error correction and interpretation. The interactive process\nstarts with a user-supplied code task description and error code. The LLM-based agent then first explains the reason\nfor the code error based on the history and then attempts to correct the erroneous code. The generated code is then\nexecuted and compared to two test cases with different levels of difficulty. If the code passes all tests, the interaction\nis terminated and the final corrected code is returned. If the tests are not passed, it implies that the generated code\nhas errors and requires modification according to the interpretation of the LLM-based agent. This iterative process\ncontinues until code is generated that passes all test samples or the maximum number of iterations is reached."}, {"title": "Experiments", "content": ""}, {"title": "Data Description", "content": "The dataset used in the experiments is based on MBPP test set compiled by [29], which contains numerous prompting\nwords for code generation, automated test cases about python programming problems. A low level LLM are used\nto generate original error code for Co-Learning, which used prompting words to try to generate code and saved the\ngenerated error code as new dataset. The generated code cannot run properly, but contains correct function name\nand comments about the logic, preserving valid information for subsequent experiments. Generally, this experiment\ngenerated an original dataset containing 702 error codes, test samples and challenge test samples as listed in Table 2."}, {"title": "Baseline LLM", "content": "ERNIE-4.0-8K-0329 [6], Spark Desk V3 [7] and Meta-Llama-3-8b [8] are selected as the open-source LLMs. Baseline\nLLMs are merged into the PADE multi-agent environment. E-RL select the optimal model for the Co-Learning\nframework from the three to provide high-quality responses."}, {"title": "Main Results", "content": "This experiment uses the original error code data set, sets the maximum number of cycles to 5 (exceeding the number of\ncycles will directly determine the operation failure), and limits the memory length to 3 dialogue pairs to avoid exceeding\nthe LLM single input message length limit.\nTable 3 shows the number of successfully corrected loops, average running time, and final accuracy in the code\ncorrection task using a single LLM or E-RL and a collaborative learning framework based on multiple LLMs. Co-\nlearning mimicked rubber-duck debugging operations can be observed to help the model retry generation when the first\ngeneration goes wrong, with Llama 3-8b being the biggest beneficiary of the single LLM model, with 134 successful\nre-generations of the correct code.\nThrough E-RL, the probability of success of Co-Learning based on rubber duck testing is greatly increased, even if some\nfirst-time success probability is lost, a total of 196 examples are correctly modified due to E-RL. E-RL's contribution to\nthe runtime is also undeniable, being only slower than the high-speed, low-enabled Spark V3, with an average test time\nof 99.8s. Finally, Co-Learning with E-RL has the highest review success rate, reaching 67.80%."}, {"title": "Case Study", "content": "Fig. 8 depicts the actual situation of code error correction through Co-Learning. First, the master agent schedules the\nerror correction task, and E-RL selects Llama as the initial LLM for joint learning based on the input message. Correct\nAgent uses Llama to make initial modifications to error codes. Based on the results it can be concluded that Llama\ngenerated the correct answer but secretly changed the function name. This phenomenon is obvious in the three LLMs.\nThe LLM may choose more appropriate function names for the code based on the code content, while ignoring the\nuser's needs.\nThe Test Agent detects that the generated code fails the test and returns an error message to the Main Agent. The\nMain Agent assigns code interpretation tasks in the hope of simulating rubber duck testing. The Interpretation Agent\ninterprets the generated code and stores the contents in the Main Agent memory. E-RL re-selects Spark as the next\nAgent's LLM, and the Correct Agent then re-corrects the code. Spark misinterprets the code as outputting multiples\nof the current number between 1 and 10, which may be related to the fact that the error returned by the Test Agent\ncontains information about the first test sample, where the maximum number to be generated is 10.\nTest Agent detected the error, leading Main Agent decide to make another rubber duck debugging. Then LLM was\nchanged to the most powerful ERNIE, which eventually generated the correct code, and Annotation Agent added\ncomments to the output.\nThis case shows how Co-Learning can continuously correct, understand and then correct erroneous code by imitating\nthe error correction process of human programmers, and generate information-intensive integrated code that only\nsenior programmers can generate. With E-RL, Co-Learning attempts to balance model power and speed in the hope of\ngenerating the best response in the shortest possible time, creating code that is shorter and more refined than expected\ncorrect code, while using less time than a single model."}, {"title": "Conclusion and Future Works", "content": "This paper focuses on developing a code learning community (aka Co-Learning) framework based on an LLM-\nbased multi-agent framework that leverages ambient reinforcement learning (E-RL) for agent self-improvement. The\ncommunity aims to interpret error codes and perform code correction tasks to provide users with a more intelligent and\npersonalized programming learning experience. Experiments show that the Co-Learning framework can effectively\nimprove the code error correction capabilities of current LLM. E-RL dynamically determines the state of the environment\nand changes the selection of the LLM, which can speed up the code correction process and achieve significant\nimprovements in the quality of the generated output. In the future, Co-Learning will focus on further optimizing the\nE-RL algorithm to improve the Agent's learning efficiency and performance. At present, it seems too simplistic to\nselect E-RL parameters based only on model capabilities. Making Co-Learning's environmental reinforcement learning\nhave dynamic self-updating weights by combining machine learning will be one of the main goals in the future. Expect\nto explore more complex tasks and scenarios, including error correction and code understanding in larger code bases, as\nwell as code learning in different programming languages and domains."}, {"title": "Data Availability Statement", "content": "The datasets analyzed for this paper can be accessed upon request from interested readers at\nhttps://github.com/yuqian2003/Co_Learning."}, {"title": "Conflict of Interest", "content": "The authors declare that the research was conducted in the absence of any commercial or financial relationships that\ncould be construed as a potential conflict of interest."}, {"title": "Author Contributions", "content": "JY: Conceptualization, Methodology, Multi-Agent framework design, Data processing, Writing-original draft prepa-\nration and Formal analysis; YW: Data validation, Environmentally reinforcement learning implement, Multi-Agent\nframework design, Writing-original draft preparation and Formal analysis; YZ, WG and ZX: Literature review, PADE\nFeasibility Analysis and Data Visualization; RL: Supervision, Reviewing and Editing."}], "equations": ["Softmax(x) = \\frac{e^{x_i}}{\\sum_{j=1}^{3} x_j}", "Logistic(x) = \\frac{1}{1+e^{-2(x_i-0.5)}}"]}