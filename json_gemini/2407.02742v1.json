{"title": "A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation", "authors": ["Nastaran Bassamzadeh", "Chhaya Methani"], "abstract": "Natural Language to Code Generation has made significant progress in recent years with the advent of Large Language Models (LLMs). While generation for general-purpose languages like C, C++, and Python has improved significantly, LLMs struggle with custom function names in Domain Specific Languages or DSLs. This leads to higher hallucination rates and syntax errors, specially for DSLs having a high number of custom function names. Additionally, constant updates to function names add to the challenge as LLMs need to stay up-to-date. In this paper, we present optimizations for using Retrieval Augmented Generation (or RAG) with LLMs for DSL generation along with an ablation study comparing these strategies. We generated a train as well as test dataset with a DSL to represent automation tasks across roughly 700 APIs in public domain. We used the training dataset to fine-tune a Codex model for this DSL. Our results showed that the fine-tuned model scored the best on code similarity metric. With our RAG optimizations, we achieved parity for similarity metric. The compilation rate, however, showed that both the models still got the syntax wrong many times, with RAG-based method being 2 pts better. Conversely, hallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for API parameter keys. We conclude that an optimized RAG model can match the quality of fine-tuned models and offer advantages for new, unseen APIs.", "sections": [{"title": "1 INTRODUCTION", "content": "There has been significant progress made in improving and quantifying the quality of Natural Language to Code Generation or NL2Code ([2], [18], [5], [2]). Recent improvements in models for general-purpose languages like Python, C++ and Java can be attributed to larger LLMs ([19], [20]) and the availability of pre-trained open-source models ([5], [16], [1], [17]) advancing the state-of-the-art. However, there hasn't been a focus on improving quality of Natural Language to Domain Specific Languages or NL2DSL which a lot of enterprise applications rely on.\nDomain Specific Languages (or DSLs) are custom Computer Languages designed and optimized for specific applications. Examples of DSLs include SQL and industry-specific languages for formalizing API calls, often using formats like JSON or YAML to represent API sequences. In this paper, we focus on the task of generating a DSL used for authoring high-level automation workflows across thousands of web-scale APIs. These workflows support a variety of customer scenarios like invoice processing, sales lead integration with forms/emails etc. The automation DSL represents API names as functions and codifies a sequence of API calls along with conditional logic over the invocation of APIs. We constrained the length of sequence to 5 APIs and hope to explore longer sequences in future work. An example of the DSL is shown in Figure 1.\nExisting code generation methods are hard to adapt for this scenario due to the frequent hallucinations and syntax errors. This is largely due to the custom names, massive size and diversity of APIs in public as well private domain along with the ever-changing API landscape. Current NL2Code methods mainly use fine-tuning and do not focus on strategies for improving grounding LLMs to include new APIs.\nIn this paper, we outline an end to end system architecture for NL2DSL generation with high response rate using selective improvements to RAG techniques ([14], [24]) using OpenAI models. We fine-tuned a Codex model for NL2DSL and show a comparative analysis of the impact of the approaches used to optimize RAG.\nAlong with metaprompt tuning for RAG, we also included additional grounding context in the form of API Function Definitions, like the approach used for Tool Selection ([25], [27], [13], [23]). This is motivated by the similarities between the code generation and task orchestration scenarios discussed in more detail in Section 2.\nThe remainder of this study is structured as follows. In Section 2, we present the NL2DSL problem formulation along with literature review. The focus is on comparing differences between Tool Selection of APIs as a framework compared to Code Generation over a set of APIs. This will help define the scope of the experiments in this study. Section 3 lays out and describes the optimizations we made to RAG as discussed above along with the benchmark Fine-Tuned model. Section 4 discusses Data Generation, Metric definition and Section 5 shares our results and discussion followed by Conclusion and Future Work in Section 6."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Code Generation or Program Synthesis", "content": "Program Synthesis is a hard research problem ([8], [3], [5],[12], [30]). It has gained significant interest with many open-source models focusing on general programming languages since the release of Github Copilot ([2]). These models include Code Llama [16], StarCoder [11], Codestral [17], Phi-3 [1] and more. Many of these advancements have been achieved through pre-training language models for code generation with a focus on improving datasets(([16], [1])). However, for domain adaptation, instruction fine-tuning on top of a base model remains a popular approach ([2], [6], [10], [23]).\nPrompting LLMs is an alternative technique for code generation ([14], [29], [28], [9]). Poesia et al. ([24]) focused on improving response quality through grounding techniques. They fine-tuned a Sentence BERT model by changing the loss function to incorporate predicting similarity of the generated target programs. With this adapted similarity metric, better few shots are selected dynamically."}, {"title": "2.2 Reasoning and Tool Integration", "content": "When it comes to modeling the problem of selecting a sequence of API calls, we need to consider formulating it as a planning or reasoning task. LLMs show remarkable reasoning capability, however, they also have limitations when it comes to staying up-to-date with recent knowledge, performing mathematical calculations etc. A popular way to overcome this has been granting the LLMs access to external tools. This framework gained significant popularity with OpenAI Code Interpreter's success ([21]).\nExternal Tool Integration has been studied since with a focus on including specific tools such as web search ([25]), python code interpreters ([6], [21]), adding calculators ([22] [6]) and so on. Expanding the tool set to a generic list of tools has been explored ([25], [23]), but it remains limited and often predicts single tools instead of sequences needed for most enterprise scenarios. Tool Use has mostly been explored in the context of generating more accurate text outputs for Q&A tasks with the help of external tools([25], [22]).\nThere is an increase in focus on incorporating LLM's code generation capabilities to reasoning and task orchestration, this is an area of active research ([6], [13], [23]). However, most of the research either limits the tools to a set of small well-documented APIs (([6],[13]), or limited their scope to predicting a single output API ([23], [25]).\nPosing the reasoning or orchestration task as a code generation problem is similar to the API sequence generation scenario highlighted in this paper. Representing a plan as a DSL, as discussed in Section 1, aligns with our goal of generating DSL for workflow automation. Improving the quality of Natural Language to DSL generation, is thus beneficial for both reasoning and plan generation."}, {"title": "2.3 Contributions", "content": "In the previous section, we discussed formulating Task Orchestration as a Code Generation problem since it can be represented as yet another DSL. NL2DSL generation suffers from the hallucination and quality issues we discussed in 1. Few studies address the challenges of end-to-end DSL generation, specifically over a large set of custom APIs.\nThis paper presents improvements to known RAG techniques focusing on improving DSL generation quality for enterprise settings. Our DSL expands API or tool selection to a sequences of 5-6 API calls, also referred to as chain of tools, which is a first to the best of our knowledge. We also consider the real-world scenarios of adding conditional logic with API calls as shown with an example in Figure 1. Our contribution is outlining an end-to-end system as well as presenting an ablation study for NL2DSL generation.\nWe merged prompting and grounding approaches from code generation ([24],[14],[29]) and added API metadata as used in task orchestration area ([6], [13]) and studied their impact on reducing hallucination rate. We created a test set having 1000 NL-DSL pairs spanning over a set of approx. 700 API calls or functions using principles of synthetic dataset generation (similar to [7] and [26]) and used manual approval to validate test set quality. Our fine-tuned DSL model is trained on a larger synthetic NL-DSL dataset (details in Section 3.1)."}, {"title": "3 METHODOLOGY", "content": "In this section, we first provide an overview of the approaches used in our experiments. In the following sub-sections, we will delve deeper in the details of each of the approaches.\nDetails of fine-tuning are shared in Section 3.1. Fine-Tuning a base model, specifically, instruction fine-tuning is a preferred approach for domain adaptation. It's limitations include inability to include newly added APIs on an ongoing basis, as well as the resource intensive data collection process for infrequently used APIs or the tail set.\nWe used RAG based approaches to overcome these limitations, and focused on improving grounding techniques for DSL generation (Details in Section 3.2). We used dynamically generated few-shot examples approach ([24]), and augmented it with API function definitions similar to the way it is used for Tool Selection ([23], [27]). These few-shots were selected from an expansive pool of synthetic NL-DSL pairs, empirically having 100s of variations of usage for each API (Section 4.1).\nFor computing semantic similarity of the few-shots with the input user query (Section 3.2), we fine-tuned a BERT model as highlighted in [24] with a modified loss function for predicting target DSL similarity. For selecting the API metadata for grounding (Section 3.3), we created an index over API Function Definitions. We also tried metaprompt tuning, but limit the focus of this study to improving grounding techniques with a combination of dynamically selected few-shot samples as well as API metadata or tool description.\nWe share the details of each approach and variation below."}, {"title": "3.1 Fine-Tuned NL2DSL Generation Model", "content": "We took the Codex base model from OpenAI due to it's pre-training with code samples and used LoRA-based fine-tuning approach. The training set consists of NL-DSL pairs, NL refers to the user query and the DSL represents the workflow that the user is looking to automate. We used <START> and <END> token to indicate the end of code generation to the model. The training set consists of a pool of 67k samples in the form of (prompt, flow) tuples generated synthetically (details in Section 4.1, and examples of NL-DSL are shared in Figure 1 and Appendix A).\nWe ran many iterations on this model to improve performance on the test set, specifically for the body and tail connectors, and went through multiple rounds of data augmentation. We found that predicting the parameter keys was very challenging with the fine-tuned model due to limitation of data generation. Even with synthetic models, it was hard to scale the NL-DSL sample variety needed for improving quality of parameters."}, {"title": "3.2 Grounding with dynamically selected few-shots", "content": "We tried two types of grounding information for RAG based DSL generation as described below. There are some variations of each technique described in the paragraph below as well. For each technique, we selected 5 and 20 shots dynamically, and saw performance impact driven by the approach used for sample selection."}, {"title": "3.2.1 Pre-Trained Model.", "content": "The first approach is using a vanilla Per-Trained model for determining the semantic similarity of NL-DSL samples based on the NL query. We computed the embeddings of NL queries using a Distil-RoBERTa Pre-Trained model. We created a Faiss Index ([4]) for these embeddings to help with search over the dense embedding space."}, {"title": "3.2.2 TST based BERT Fine-tuning.", "content": "In this approach, we fine-tuned the pre-trained model to improve the retrieval accuracy of the few-shots. This is similar to the approach used by Poesia et al. in [24]. They show that if we fine-tune the pre-trained BERT model with a modified loss function to consider the similarity between the target DSL for each NL-DSL pair, the retrieved examples will have a higher quality and finally lead to better generation with LLM.\nTo get positive and negative samples for fine-tuning, we compared cosine similarity between all pairs of Natural Language queries in our dataset (Dataset shared in Section 4.1). We used a Pre-Trained Tansformer model to generate embeddings for the purpose of similarity computation. A pair of tuples is considered a positive sample if the similarity between their corresponding NL prompts is greater than 0.7 and negative otherwise. We generated 100k pairs this way and leveraged them as training data for our fine-tuning experiment.\nThe loss function used by TST (Equation 1 from [24]) is minimizing the Mean-Squared Error between the vanilla loss functions comparing the utterances $(u_i, u_j)$ and the target programs $(p_i, p_j)$. Program similarity is denoted by S. They used AST to compute program similarity, however, we used a Jaccard score over lists of API function names to be consistent with our metrics definition (Section 4).\n$L_{TST}(0) := E_{i,j} D[f_0(u_i, u_j) \u2013 S(p_i, p_j)]^2$"}, {"title": "3.3 Grounding with API Metadata", "content": "In addition to few-shots, we appended the API metadata in the metaprompt. This metadata includes Function Description along with the parameter keys and their description (See an example API Function Definition shared in Appendix A). We followed the below two approaches for selecting the metadata to be added."}, {"title": "3.3.1 API Function Definitions for Few Shots.", "content": "For the few-shots samples selected using the methods described above, we extracted the metadata for each of the functions present in those samples. This means that for the n few-shot samples dynamically added to the metaprompt, we iterated over all the API function names in each of these flows and added their function definitions to the metaprompt.\nWe also modified the metaprompt to add instructions on how to use the Function Definitions. We want to explore how adding the metadata explaining the purpose of each function in the few-shot examples impacts LLM's understanding of the task and map to user request."}, {"title": "3.3.2 Semantic Function Definitions.", "content": "Another approach for selecting the function definitions to be added to the metaprompt is to retrieve the semantically similar functions from a vector database created with API metadata. This approach is similar to the one followed by LlamaIndex ([15]) We created an index of all API definitions and retrieved the semantically similar functions by using the input NL query to search the index. Please note that this is different from the faiss index created for few-shot samples in Section 3.2.\nWe call this approach Semantic Function Definition (SFD) and will compare it with the Regular FDs described above. This approach can be specifically useful for tail-ish prompts where no few-shots might be retrieved. This helps us integrate the newly released web APIs in our DSL Generation framework making our approach scalable to the changing API landscape."}, {"title": "4 EXPERIMENT DESIGN AND METRICS DEFINITION", "content": "In this section, we outline the process of Dataset Generation and introduce the metrics we used for estimating the code quality. We then describe the experiments. Results and Discussion follows in the next section. We have used Azure AML pipelines to run our experiments. The GPT-4 (with 16k token limit) model is used as the LLM model. The metaprompt is kept consistent between experiments for the purpose of the ablation study."}, {"title": "4.1 Dataset Generation", "content": "We generated a total of 67k samples in the form of (prompt, flow) pairs from workflows created by users. We had many samples of workflow automations created by users across a large set of APIs. We sampled the automations containing 700 publicly available APIs and synthetically generated the corresponding Natural Language prompts using GPT-4. For creating these NL descriptions for the workflows, we also provided API Function definitions to the metadata. This ensured the language of the description captured the functioanlity of the API.\nA subset of these synthetic samples were validated by human judges. We used these checks to improve the metaprompt used for synthetic data generation. For creating a test set, we used the same process with most of the test set evaluated by human judges to ensure quality. We followed the same distribution of APIs from users, to ensure that our metrics are not biased. The test data set consists of 1000 samples that are verified by human judges."}, {"title": "4.2 DSL Generation Quality Metrics", "content": "We defined 3 key metrics to focus on code generation quality as well as syntactic accuracy and hallucination rate. We have a compiler to test the syntax and validate the functions against a database of API names as well as parameter keys."}, {"title": "4.2.1 Average Similarity.", "content": "Average Similarity measures the aggregated similarity between predicted flow and the ground truth flow. The average similarity between two flows is defined using the Longest Common Subsequence match (LCSS) metric. Each flow is reduced to a list of API call sequences and then the LCSS is computed. The final metric is reported as an average over all test samples. Hallucination and Parser failures lead to the sample being discarded and is assigned a similarity score of 0.\n$Similarity = \\frac{LCSS(A, B)}{max(|Actions_A|, |Actions_B|)}$\nwhere |ActionsA| is the number of actions in flow A and |ActionsB| is the number of actions in flow B.\nPlease note that we are not using the commonly used AST metric for computing code similarity. AST drills down to compare similarity performance for parameters as well. As we wanted to focus on the problem of improving function name retrieval as well as it's sequence, we chose to define the metric in this manner."}, {"title": "4.2.2 Unparsed rate.", "content": "This metric captures the rate of syntactic errors. A flow that cannot be parsed by the parser is considered not usable for the purpose of similarity metric computation. Unparsed rate is computed as follow:\n$\\text{%unparsed flows} = \\frac{|Flows_{unparsed}|}{|Flows_{total}|}$"}, {"title": "4.2.3 Hallucination rate.", "content": "This metric captures the rate of made-up APIs (or function names) and made-up parameter keys in the generated code. Predicting a flow with a hallucinated API name is counted as a failure and leads to the code being considered invalid. We compute this by counting the number of flows that have at least one hallucinated function name and divide it by the total number of flows in the sample set.\n$\\text{%made - up APIs} = \\frac{|Flows_{h}|}{|Flows_{parsed}|} * 100$\nwhere Flowsh is the number of flows with hallucinated API names and FlowSparsed| is the number of flows that were parsed correctly.\nSimilarly, we compute the rate at which parameters were not parsed. Failure to parse parameters does not result in the flow being discounted from average similarity computation. However, it shows up as run-time errors. Fixing these run-time errors is beyond the scope of this paper.\n$\\text{%made - up parameters} = \\frac{|Flows_{hp}|}{|Flows_{parsed}|} * 100$\nwhere, Flowshp| is the number of flows with hallucinated parameter key names and |FlowSparsed| is the number of flows that were parsed correctly."}, {"title": "5 RESULTS", "content": "In this section, we present the results of the above approaches on a test set of 1000 NL-DSL pairs. These samples, while generated synthetically, have been evaluated by human judges for quality. They were also sampled to represent the distribution of APIs in actual product usage.\nWe compare the impact of each ablation in sections below."}, {"title": "5.1 Impact of number of few-shots on RAG performance", "content": "We compare the impact of number of code samples added to the meta prompt with two different settings i.e. 5 few-shots vs 20 few-shots. We measured the results for both Pre-Trained model as well as TST model. Results are shared in Table 1 and show the A change compared to that Baseline model. The baseline setting here is Pre-Trained model with 5 few-shots.\nLooking at row 1 and comparing rows 2 and 3 with respect to the baseline, we can see that adding more few-shots improves the performance of both the Pre-Trained as well as the TST model on all metrics. The gain is particularly pronounced for reducing the number of made-up API names as well as reducing the number of made-up API parameter keys. We saw the gain plateau beyond this, and we intend to run more experiments in the future to study this effect better."}, {"title": "5.2 TST vs Pre-trained Model", "content": "Comparing the rows in Table 1, both Pre-Trained and TST with 20 samples look comparable for computing the Average Similarity but have slight variations in Unparsed flow rate as well as Hallucinations rates. TST model performs better in reducing the % made-up API names, while the Pre-trained model has a slight edge in the other two metrics.\nSo, we additionally look at the impact of including API Function Definitions to both the models (see Table 2). Here, we have used GPT4 model with 5 few shots. The results are represented as A changes compared to the Baseline setting i.e. using the Pre-Trained model to choose 5 few-shot NL-DSL code samples. TST with FD setting performs overall better than all other options with values close to the best in every metric.\nWe see a similar trend in Table 3 where we captured the results for 20 few-shots. This leads us to conclude that the presence of few-shot examples is supported by adding the API functions definitions of these functions (as described in Section 3). The addition predominantly helps reducing the hallucination rate for API names and parameters, which improves the overall response rate of NL2DSL generation."}, {"title": "5.3 Function Definition vs Semantic Function Definitions", "content": "As the next step, we will compare the impact of Semantic Function Definitions (SFD) vs adding the API Function Definitions for selected examples only. We used a Fine-Tuned model as baseline for this experiment. Based on the insights from the previous step, we used 20 few-shots for TST along with including FDs. All results in Table 4 are shown as A improvements compared to the baseline."}, {"title": "6 CONCLUSION", "content": "Concluding from the ablations study shared in Section 5, we see that the role of dynamically selected few-shot samples is very important in making RAG useful for syntactically correct generation of DSL as well as improving code similarity ((Table 4)).\nFine-Tuning still outperforms the RAG based model in terms of lower hallucinations (see Table 4 where fine-tuned model is the baseline). However, the parsing errors are more common in the fine-tuned model. This could be due to the fact that few shot examples have been successfully teaching the correct syntax to the LLM model. It is, however, surprising that the syntax correctness for RAG is better than that of the fine-tuned model which was trained on a much larger sample set.\nIt is also interesting to note that this benefit does not transfer to hallucinated API names and their parameters keys where the fine-tuned model holds the advantage. The increase of 6.76 pts in hallucination rate for parameters due to adding Semantic Function definitions indicates that adding too many API descriptions can confuse rather than help the LLM (Table 4). It also signifies the higher impact of the few shot samples for the scenario of DSL Generation or API selection compared to simply providing the API description. This learning can be used to inform the Tool Selection or orchestration scenario. Providing high quality examples of sample orchestration will reduce the failure rate more.\nOverall, we were able to significantly improve the performance of RAG for DSL generation, with hallucination rate for API names dropping by 6.29 pts. and that of parameter keys dropped by approx. 20 pts (see Table 2). The performance of RAG is now comparable to that of fine-tuned model (see Avg. Similarity in Table 4), with the potential to bootstrap quickly. Optimized RAG can also allow extending the benefits of metaprompt tuning to include unseen APIs, reducing the need to fine-tune the model frequently. This will be the focus of our future work."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 Sample with computed Average similarity", "content": "Sample showing how flow similarity is computed for two flows Flow A and Flow B.\nQuery = \"Post a message in the channel of teams, when a new form is created in the forms\"\nGround Truth = \"triggerOutputs =\nawait shared\\_microsoftforms.CreateFormWebhook({});\noutputs_shared_teams_PostMessageToConversation =\nshared_teams. PostMessageToConversation(\n{ \\\"poster\\\": \\\"User\\\" });\"\nprediction: \"triggerOutputs =\nawait shared_microsoftforms.CreateFormWebhook({});\noutputs_Get_my_profile_V2 =\nshared_office365users.\nMyProfile_V2({}); outputs_shared_teams_PostMessage\n= shared_teams. PostMessageToConversation(\n{\\\"poster\\\": \\\"User\\\",\\\"location\\\": \\\"Channel\\\"});\"\nAPI Functions list in ground_truth =\n[shared_microsoftforms.CreateFormWebhook,\nshared_teams. PostMessageToConversation]\nAPI function list in model generation =\n[shared_microsoftforms.CreateFormWebhook,\nshared_office365users. MyProfile_V2,\nshared_teams. PostMessageToConversation]\nSimilarity Score = 2/3 = 0.666\nSince the functions shared_microsoftforms.\nCreateFormWebhook and shared_teams.\nPostMessageToConversation are found\nin the ground truth."}, {"title": "A.2 An example of API metdata", "content": "We share a sample of API metadata to highlight the details included in the API description provided to the metaprompt."}]}