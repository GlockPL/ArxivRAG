{"title": "Last-Iterate Convergence of General Parameterized Policies in Constrained MDPs", "authors": ["Washim Uddin Mondal", "Vaneet Aggarwal"], "abstract": "We consider the problem of learning a Constrained Markov Decision Process (CMDP) via general parameterization. Our proposed Primal-Dual based Regularized Accelerated Natural Policy Gradient (PDR-ANPG) algorithm uses entropy and quadratic regularizers to reach this goal. For a parameterized policy class with transferred compatibility approximation error, $E_{bias}$, PDR-ANPG achieves a last-iterate $\\epsilon$ optimality gap and $\\epsilon$ constraint violation (up to some additive factor of bias) with a sample complexity of $\\tilde{O}(\\epsilon^{-2} \\min{\\epsilon^{-2}, E_{bias}^{-2}})$. If the class is incomplete ($E_{bias} > 0$), then the sample complexity reduces to $\\tilde{O}(\\epsilon^{-2})$ for $\\epsilon < O(E_{bias})$. Moreover, for complete policies with $E_{bias} = 0$, our algorithm achieves a last-iterate $\\epsilon$ optimality gap and $\\epsilon$ constraint violation with $\\tilde{O}(\\epsilon^{-4})$ sample complexity. It is a significant improvement of the state-of-the-art last-iterate guarantees of general parameterized CMDPs.", "sections": [{"title": "1 Introduction", "content": "Constrained Markov Decision Process (CMDP) is a classical framework where an agent repeatedly interacts with an unknown environment to maximize the cumulative discounted rewards while simultaneously ensuring that the cumulative observed costs are within a pre-defined boundary. It finds its application in a multitude of practical scenarios. For example, consider an autonomous vehicle that attempts to reach its destination via the shortest-time route without violating traffic rules or a corporate leader who aims to maximize revenue without crossing a monetary budget. In these cases, any departure from the boundary set by the predefined rules can be signaled by a cost while the progress towards the desired objective can be indicated by a reward.\nFinding an optimal policy to navigate an unknown CMDP is a difficult task. Nevertheless, several recent articles have proposed algorithms to solve this challenging problem with optimality guarantees. For example, [26] have exhibited that their primal dual-based algorithm can achieve $\\epsilon$ optimality gap and $\\epsilon$ constrained violation with a $\\tilde{O}(\\epsilon^{-2})$ sample complexity. Unfortunately, the majority of these works define constraint violation in an average sense. In other words, if their algorithms yield ${\\pi_1,\\cdots, \\pi_K}$ policies in $K$ iterations, then the violation is defined as that experienced by a uniformly chosen policy. Since the very nature of this definition allows a large violation at one iteration to get balanced by a smaller violation later, such algorithms are not suitable for safety-critical applications.\nTo address this challenge, some recent articles have proposed algorithms with last-iterate guarantees. For example, [14, 36] prove last-iterate guarantees for softmax policies, whereas [8] establishes the same for log-linear policies. [27] recently achieved $\\tilde{O}(\\epsilon^{-7})$ sample complexity via general parameterized policies. It is to be emphasized that general parameterization subsumes the tabular softmax and log-linear cases and allows the policies to be represented by neural networks. Moreover, since the general parameterization uses a fixed $d$ number of parameters where $d$ is independent of the size of the state space, it can also be utilized for large or infinite state space. Notably, the current-state-of-the-art sample complexity $\\tilde{O}(\\epsilon^{-7})$ is far from the theoretical lower bound $(\\epsilon^{-2})$. This raises the following question.\nIs it possible to design an algorithm for general parameterized CMDPs whose last-iterate guaran-tee is better than the current state-of-the-art?"}, {"title": "1.1 Contribution and Challenges", "content": "In this paper, we affirmatively answer the above question. In particular, we propose primal dual-based regularized accelerated natural policy gradient (PDR-ANPG) algorithm that utilizes entropy and quadratic regularizer in the Lagrangian function and momentum-based accelerated stochastic gradient descent (ASGD) process of [17] as the NPG finding subroutine. We prove that, for general parameterization, our algorithm achieves $O(\\epsilon + \\epsilon_{bias})$ last-iterate optimality gap and the same constraint violation with a sample complexity of $\\tilde{O}(\\epsilon^{-2} \\min{\\epsilon^{-2}, \\epsilon_{bias}^{-2}})$ where $\\epsilon_{bias}$ indicates the transferred compatibility approximation error of the policy class. If $\\epsilon_{bias} > 0$ is independent of $\\epsilon$, the sample complexity reduces to $\\tilde{O}(\\epsilon^{-2})$ for small $\\epsilon$. However, if $\\epsilon_{bias} = 0$, i.e., the policy class is complete, then the optimality gap and constraint violation become $O(\\epsilon)$ and the sample complexity turns out to be $\\tilde{O}(\\epsilon^{-4})$.\nOne of the main challenges in handling an entropy regularizer is that the advantage estimate can, in general, become unbounded. This is a problem since many intermediate lemmas that are crucial in establishing global convergence, utilize its boundedness. In the tabular setup, [8] circumvented this problem by allowing the policy optimization to run over a carefully chosen simplex. Such provisions are not available for general parameterization. Interestingly, we observed that the advantage estimates can be bounded in an average sense and that would be sufficient to arrive at our desired result provided that the gradient\nsampling is done in an unconventional way. In particular, on top of the standard routines, our sampling procedure (Algorithm 1) comprise an additional expectation that, in essence, reduces the variance of the gradient estimate while preserving its unbiasedness.\nOur improved sample complexity originates from another important observation. We noticed that the bias of the NPG estimator can be interpreted as the convergence error of an ASGD program with an exact gradient oracle. Without this reduction, the bias would be exponentially large, leading to a significant deterioration in sample complexity."}, {"title": "2 Related Works", "content": "Related works can be mainly segregated into two groups.\nUnconstrained MDP: Many algorithms are available in the literature that solve the unconstrained MDP via an exact gradient e.g., see [38, 20, 6, 1, 5]. Among the sample-based methods, some papers demonstrate first-order convergence [28, 16, 13, 33] while other works focus on global convergence [24, 23, 19, 7]. It is to be noted that [12, 25] prove the state-of-the-art $\\tilde{O}(\\epsilon^{-2})$ sample complexity for last-iterate and average global convergence respectively. However, to the best of our understanding, the approach of [12] is not extendable to CMDPs.\nConstrained MDP: The majority of the CMDP works show average or regret-type constraint violation guarantees. Many among them design tabular model-based [15, 9, 21, 11] and model-free [32, 2, 9] algorithms. Some works focus on parameterized policies as well. For example, [22, 37] deal with softmax policies while [34, 3, 10] handle general parameterization. The optimal $\\tilde{O}(\\epsilon^{-2})$ sample complexity for general parameterized CMDPs is proven by [26]. In comparison, the literature on last-iterate guarantees is relatively nascent."}, {"title": "3 Formulation", "content": "Consider a Constrained Markov Decision Process (CMDP) characterized as $M = (S, A, r, c, P, \\gamma, \\rho)$ where $S$ is a (possibly infinite) state space, $A$ is a finite action space with cardinality $|A|$, $r : S \\times A \\rightarrow [0, 1]$ indicates the reward function, $c : S \\times A \\rightarrow [-1, 1]$ is the cost function, $P : S \\times A \\rightarrow \\Delta(S)$ is the transition function (where $\\Delta(\\cdot)$ denotes the probability simplex on its argument set), $\\gamma \\in [0, 1)$ defines the discount factor and $\\rho \\in \\Delta(S)$ is the initial state distribution. This paper assumes $S$ to be countable, though, in general, it can be taken to be compact. A (stationary) policy is defined to be a function of the form $\\pi : S \\rightarrow \\Delta(A)$. For a given policy, $\\pi$, and a state-action pair $(s, a)$, the Q value corresponding to a utility function $g : S \\times A \\rightarrow \\mathbb{R}$ is defined as\n$Q_g^{\\pi}(s, a) = \\mathbb{E}_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t g(s_t, a_t) | s_0 = s, a_0 = a ] \\qquad (1)$\nwhere $\\mathbb{E}_\\pi$ is the expectation obtained over all $\\pi$-induced trajectories ${(s_t, a_t)}_{t=0}^{\\infty}$ where $a_t \\sim \\pi(s_t), s_{t+1} \\sim P(s_t, a_t), \\forall t \\in {0, 1, \\ldots}$. Similarly, given a policy $\\pi$ and utility function $g$, the associated state value function is given below.\n$V_g^{\\pi}(s) = \\mathbb{E}_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t g(s_t, a_t) | s_0 = s] \\qquad \\forall s \\qquad (2)$\nMoreover, for each policy $\\pi$, and utility function $g$, the the advantage function is computed as, $A_g^{\\pi}(s, a) = Q_g^{\\pi}(s, a) - V_g^{\\pi}(s), \\forall (s, a)$. The state occupancy measure corresponding to $\\pi$ is defined as\n$d^{\\pi}(s) = (1 - \\gamma) \\mathbb{E}_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t \\mathbb{1}(s_t = s) | s_0 \\sim \\rho] \\qquad \\forall s, \\qquad (3)$\nwhere $\\mathbb{1}(\\cdot)$ is the indicator function. In this paper, we ignore the dependence on $\\rho$ for simplifying the notations whenever there are no confusions. The state-action occupancy measure induced by $\\pi$ is defined as $\\nu^{\\pi}(s, a) = d^{\\pi}(s)\\pi(a|s), \\forall (s, a)$. The expected value of $V_g^{\\pi}(s)$ obtained over $s \\sim \\rho$ is denoted as $J_g^{\\pi}$. The goal of learning CMDP is to solve the following optimization problem.\n$\\max_{\\pi \\in \\Pi} J_r^{\\pi} \\ \\ \\ \\text{subject to: } J_c^{\\pi} \\leq 0$ (4)\nwhere $\\Pi$ is the set of all policies. We assume that at least one interior solution exists for the above optimization, which is known as Slater's condition.\nNote that the policy function cannot be expressed in a tabular format for infinite states. General parameterization can be used in such cases. It indexes policies by a $d$-dimensional parameter $\\theta$. Let $J_g^\\theta \\triangleq J_g(\\theta)$ for any $g \\in \\mathbb{R}^{S \\times A}$. The optimization (4) can now be written as follows.\n$\\max_{\\theta \\in \\mathbb{R}^d} J_r(\\theta) \\ \\ \\ \\text{subject to: } J_c(\\theta) \\leq 0$ (5)"}, {"title": "4 Algorithm Design", "content": "The standard approach to solve the constrained optimization (4) is utilizing a saddle-point optimization on the Lagrangian function $\\mathcal{J}_{r, c}^\\lambda$ where $\\lambda$ is a Lagrange multiplier. We use $\\pi^*$ to denote an optimal solution to (4). Similarly, $\\lambda^*$ is used to denote its corresponding dual solution.\n$\\lambda^* \\in \\arg \\min_{\\lambda \\ge 0} \\max_{\\pi \\in \\Pi} \\mathcal{J}_{r, c}^\\lambda$ (6)\nThe following result is well-known in the literature [8, Lemma 1].\n$\\max_{\\pi \\in \\Pi} J_r^{\\pi} + \\lambda^* J_c^{\\pi} = \\mathcal{J}_{r, c}^{\\lambda^*} = \\min_{\\lambda \\ge 0} J_r^{\\pi^*} + \\lambda J_c^{\\pi^*}$ (7)\nThis paper, however, considers a regularized Lagrangian function which is defined as follows $\\forall \\pi \\in \\Pi$ and $\\forall \\lambda \\ge 0$.\n$\\mathcal{L}_\\tau(\\pi, \\lambda) = J_r^{\\pi} + \\lambda J_c^{\\pi} + \\tau \\mathcal{H}(\\pi) + \\frac{\\lambda^2}{2} \\qquad (8)$\nwhere $\\tau$ is a tunable parameter and $\\mathcal{H}(\\pi)$ defines the entropy corresponding to the policy $\\pi$ (defined below).\n$\\mathcal{H}(\\pi) = \\frac{1}{1-\\gamma} \\sum_{s, a} d^{\\pi}(s) \\pi(a|s) \\log \\pi(a|s) = \\mathbb{E}_{s, a \\sim d^{\\pi}}[-\\log \\pi(a|s)] \\qquad (9)$\n$\\pi_\\tau^* \\triangleq \\arg \\max_{\\pi \\in \\Pi} \\min_{\\lambda \\in \\Lambda} \\mathcal{L}_\\tau(\\pi, \\lambda), \\\\ \\lambda_\\tau^* \\triangleq \\arg \\min_{\\lambda \\in \\Lambda} \\max_{\\pi \\in \\Pi} \\mathcal{L}_\\tau(\\pi, \\lambda) \\qquad (10)$\nwhere $\\Lambda = [0, \\Lambda_{max}]$ is a carefully chosen set of positive reals and $\\Lambda_{max}$ is specified later. It can be proven that for any $\\tau, \\Lambda_{max} > 0$, the tuple $(\\pi_\\tau^*, \\lambda_\\tau^*)$ uniquely exists and satisfies a strong duality similar to Lemma 1. However, such a result cannot be directly applied to the class of parameterized policies where our goal is to solve the following optimization.\n$\\min_{\\lambda \\in \\Lambda} \\max_{\\theta \\in \\mathbb{R}^d} \\mathcal{L}_\\tau(\\theta, \\lambda) \\qquad (11)$\nwhere $\\mathcal{L}_\\tau(\\pi_\\theta, \\lambda)$ is denoted as $\\mathcal{L}_\\tau(\\theta, \\lambda)$ for simplicity.\n$\\theta_{k+1} = \\theta_k + \\eta \\mathcal{F}(\\theta_k)^{\\dagger} \\nabla_{\\theta} \\mathcal{L}_\\tau(\\theta_k, \\lambda_k), \\\\ \\lambda_{k+1} = \\mathcal{P}_\\Lambda[(1 - \\eta \\tau) \\lambda_k - \\eta J_c(\\theta_k)] \\qquad (12)$\nwhere $\\mathcal{P}_\\Lambda$ denotes the projection operation onto $\\Lambda$ and $\\eta$ is the learning rate. Note that, unlike the vanilla policy gradient iterations, the update direction of $\\theta$ is not along the gradient $\\nabla_{\\theta} \\mathcal{L}_\\tau(\\theta, \\lambda)$ but rather, it is modulated by the Moore-Penrose pseudoinverse (symbolized as $\\dagger$) of the Fisher matrix, $\\mathcal{F}(\\theta)$ defined below.\n$\\mathcal{F}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\nu^{\\pi_\\theta}}[\\nabla_{\\theta} \\log \\pi_\\theta(a|s) \\otimes \\nabla_{\\theta} \\log \\pi_\\theta(a|s)] \\qquad (13)$\n$\\nabla_{\\theta} \\mathcal{L}_\\tau(\\theta, \\lambda) = \\frac{1}{1-\\gamma} \\mathcal{H}_\\tau(\\theta, \\lambda), \\text{ where } \\mathcal{H}_\\tau(\\theta, \\lambda) = \\mathbb{E}_{(s, a) \\sim \\nu^{\\pi_\\theta}}[A_r^{\\pi_\\theta} + \\lambda A_c^{\\pi_\\theta} + \\tau \\psi_\\theta(s, a) \\nabla_{\\theta} \\log \\pi_\\theta(a|s)] \\qquad (14)$"}, {"title": "5 Last-Iterate Convergence Analysis", "content": "This section discusses the last-iterate convergence properties of Algorithm 2. Before stating the result, we will enlist a few assumptions that are needed for the analysis.\n$\\lVert \\nabla_\\theta \\log \\pi_{\\theta}(a|s) \\rVert \\leq G, \\\\ \\lVert \\nabla_\\theta \\log \\pi_{\\theta_1}(a|s) - \\nabla_\\theta \\log \\pi_{\\theta_2}(a|s) \\rVert \\leq B \\lVert \\theta_1 - \\theta_2 \\rVert$\nwhere $B, G > 0$ are constants.\n$\\lVert \\nabla_{\\theta} \\mathcal{L}_\\tau(\\theta, \\lambda) \\rVert^2 \\leq \\frac{G^2 \\mathcal{L}_{\\tau, \\lambda}^2}{(1-\\gamma)^2}, \\qquad (28)$\nwhere $\\mathcal{L}_{\\tau, \\lambda}$ is defined in (15).\n$\\mathbb{E}_{\\nu_{\\pi}}[\\frac{1}{2} \\langle w, \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) \\rangle - \\frac{1}{2(1-\\gamma)} [A_r^{\\pi_\\theta} + \\lambda A_c^{\\pi_\\theta} + \\tau \\psi_\\theta(s, a)]]^2 \\leq \\frac{\\epsilon_{bias}^2}{2} \\qquad (29)$"}, {"title": "5.1 Analysis of the Outer Loop", "content": "Let $(\\theta_k, \\lambda_k)$ denotes the parameters generated by Algorithm 2 after $k$ iterations of the outer loop. We define\n$\\Phi_k \\triangleq \\frac{1}{2} \\mathbb{E} [\\text{KL} \\left( \\pi_{\\theta^*}(\\cdot|s) \\Vert \\pi_{\\theta_k}(\\cdot|s) \\right) ] + \\frac{1}{2} \\mathbb{E} [(\\lambda - \\lambda^*)^2], \\text{ where } (30)$\n$\\text{KL} \\left( \\pi_{\\theta^*}(\\cdot|s) \\Vert \\pi_{\\theta_k}(\\cdot|s) \\right) \\triangleq \\sum_{s \\in S} d(s) \\text{KL} \\left( \\pi_{\\theta^*}(\\cdot|s) \\Vert \\pi_{\\theta_k}(\\cdot|s) \\right) \\qquad (31)$\n$\\Phi_{k+1} \\leq (1 - \\eta \\tau) \\Phi_k + \\eta \\sqrt{\\epsilon_{bias}} + \\frac{B\\eta^2}{2} \\mathbb{E} \\left[ \\Vert \\mathbb{E} [w_k|\\theta_k, \\lambda_k] - w_{\\tau}^* \\Vert \\right] + \\frac{\\eta^2 G^2 \\mathcal{L}_{\\tau, \\lambda max}^2}{2 \\mu_F (1-\\gamma)^2} + \\eta^2 \\frac{\\Lambda_{max}^2}{(1-\\gamma)^2} \\qquad (32)$"}, {"title": "5.2 Analysis of the Inner Loop", "content": "We start with a statistical characterization of the noise of the gradient estimator provided by Algorithm 1.\n$\\mathbb{E} [\\nabla_\\omega \\mathcal{E}_{\\pi_{\\theta}} (\\omega, \\theta, \\lambda) \\otimes \\nabla_\\omega \\mathcal{E}_{\\pi_{\\theta}} (\\omega, \\theta, \\lambda)] \\succeq \\sigma_{\\tau, \\lambda}^2 \\mathcal{F}(\\theta) \\qquad (34)$"}, {"title": "5.3 Optimality Gap and Constraint Violation", "content": "This section discusses how the optimality gap and constraint violation rates can be extracted from the convergence result of $\\Phi_K$ stated in Corollary 1.\n$\\mathbb{E} [J_r^* - J_r^{K}] = O(\\epsilon + \\epsilon_{bias}), \\text{ and } \\mathbb{E} [-J_c^{K}] = O(\\epsilon + \\epsilon_{bias}) \\qquad (45)$"}, {"title": "6 Conclusion", "content": "We present an algorithm for the general parametrized CMDP that ensures $O(\\epsilon + \\epsilon_{bias})$ last-iterate optimality gap and the same constraint violation with $\\tilde{O}(\\epsilon^{-2} \\min{\\epsilon^{-2}, (\\epsilon_{bias})^{-\\frac{1}{2}}})$ sample complexity. Here $\\epsilon_{bias}$ denotes the transferred compatibility approximation error of the underlying policy class. Future work includes improvement of the sample complexity to $\\tilde{O}(\\epsilon^{-2})$ across all parameterized policy classes, extension of our ideas to general utility CMDPs, etc."}]}