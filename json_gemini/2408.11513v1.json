{"title": "Last-Iterate Convergence of General Parameterized Policies in Constrained MDPs", "authors": ["Washim Uddin Mondal", "Vaneet Aggarwal"], "abstract": "We consider the problem of learning a Constrained Markov Decision Process (CMDP) via general pa-rameterization. Our proposed Primal-Dual based Regularized Accelerated Natural Policy Gradient (PDR-ANPG) algorithm uses entropy and quadratic regularizers to reach this goal. For a parameterized policy class with transferred compatibility approximation error, $\\epsilon_{\\text{bias}}$, PDR-ANPG achieves a last-iterate $\\epsilon$ optimality gap and $\\epsilon$ constraint violation (up to some additive factor of $\\epsilon_{\\text{bias}}$) with a sample complexity of $\\tilde{O}(\\epsilon^{-2} \\min{\\epsilon^{-2}, \\epsilon_{\\text{bias}}^{-2}})$. If the class is incomplete ($\\epsilon_{\\text{bias}} > 0$), then the sample complexity reduces to $\\tilde{O}(\\epsilon^{-2})$ for $\\epsilon < \\epsilon_{\\text{bias}}$. Moreover, for complete policies with $\\epsilon_{\\text{bias}} = 0$, our algorithm achieves a last-iterate $\\epsilon$ optimality gap and $\\epsilon$ constraint violation with $\\tilde{O}(\\epsilon^{-4})$ sample complexity. It is a significant improvement of the state-of-the-art last-iterate guarantees of general parameterized CMDPs.", "sections": [{"title": "1 Introduction", "content": "Constrained Markov Decision Process (CMDP) is a classical framework where an agent repeatedly inter-acts with an unknown environment to maximize the cumulative discounted rewards while simultaneously ensuring that the cumulative observed costs are within a pre-defined boundary. It finds its application in a multitude of practical scenarios. For example, consider an autonomous vehicle that attempts to reach its destination via the shortest-time route without violating traffic rules or a corporate leader who aims to maximize revenue without crossing a monetary budget. In these cases, any departure from the boundary set by the predefined rules can be signaled by a cost while the progress towards the desired objective can be indicated by a reward.\nFinding an optimal policy to navigate an unknown CMDP is a difficult task. Nevertheless, several recent articles have proposed algorithms to solve this challenging problem with optimality guarantees. For example, [26] have exhibited that their primal dual-based algorithm can achieve $\\epsilon$ optimality gap and $\\epsilon$ constrained violation with a $\\tilde{O}(\\epsilon^{-2})$ sample complexity. Unfortunately, the majority of these works define constraint violation in an average sense. In other words, if their algorithms yield {$\\pi_1,\\cdots, \\pi_K$} policies in $K$ iterations, then the violation is defined as that experienced by a uniformly chosen policy. Since the very nature of this definition allows a large violation at one iteration to get balanced by a smaller violation later, such algorithms are not suitable for safety-critical applications.\nTo address this challenge, some recent articles have proposed algorithms with last-iterate guarantees. For example, [14, 36] prove last-iterate guarantees for softmax policies, whereas [8] establishes the same for log-linear policies. [27] recently achieved $\\tilde{O}(\\epsilon^{-7})$ sample complexity via general parameterized policies. It is to be emphasized that general parameterization subsumes the tabular softmax and log-linear cases and allows the policies to be represented by neural networks. Moreover, since the general parameterization uses a fixed $d$ number of parameters where $d$ is independent of the size of the state space, it can also be utilized for large or infinite state space. Notably, the current-state-of-the-art sample complexity $\\tilde{O}(\\epsilon^{-7})$ is far from the theoretical lower bound ($\\epsilon^{-2}$). This raises the following question."}, {"title": "2 Related Works", "content": "Related works can be mainly segregated into two groups.\nUnconstrained MDP: Many algorithms are available in the literature that solve the unconstrained MDP via an exact gradient e.g., see [38, 20, 6, 1, 5]. Among the sample-based methods, some papers demonstrate first-order convergence [28, 16, 13, 33] while other works focus on global convergence [24, 23, 19, 7]. It is to be noted that [12, 25] prove the state-of-the-art $\\tilde{O}(\\epsilon^{-2})$ sample complexity for last-iterate and average global convergence respectively. However, to the best of our understanding, the approach of [12] is not extendable to CMDPs.\nConstrained MDP: The majority of the CMDP works show average or regret-type constraint violation guar-antees. Many among them design tabular model-based [15, 9, 21, 11] and model-free [32, 2, 9] algorithms. Some works focus on parameterized policies as well. For example, [22, 37] deal with softmax policies while [34, 3, 10] handle general parameterization. The optimal $\\tilde{O}(\\epsilon^{-2})$ sample complexity for general parameter-ized CMDPs is proven by [26]. In comparison, the literature on last-iterate guarantees is relatively nascent."}, {"title": "3 Formulation", "content": "Consider a Constrained Markov Decision Process (CMDP) characterized as $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, r, c, P, \\gamma, \\rho)$ where $\\mathcal{S}$ is a (possibly infinite) state space, $\\mathcal{A}$ is a finite action space with cardinality $A$, $r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$ indicates the reward function, $c: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [-1,1]$ is the cost function, $P: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ is the transition function (where $\\Delta(\\cdot)$ denotes the probability simplex on its argument set), $\\gamma \\in [0, 1)$ defines the discount factor and $\\rho \\in \\Delta(\\mathcal{S})$ is the initial state distribution. This paper assumes $\\mathcal{S}$ to be countable, though, in general, it can be taken to be compact. A (stationary) policy is defined to be a function of the form $\\pi: \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})$. For a given policy, $\\pi$, and a state-action pair $(s, a)$, the $Q$ value corresponding to a utility function $g: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is defined as\n$Q^{\\pi}_g(s, a) = \\mathbb{E}_\\pi [\\sum_{t=0}^{\\infty} \\gamma^t g(s_t, a_t) | s_0 = s, a_0 = a]$\n(1)\nwhere $\\mathbb{E}_\\pi$ is the expectation obtained over all $\\pi$-induced trajectories {$(s_t, a_t)$}$_{t=0}^{\\infty}$ where $a_t \\sim \\pi(S_t), s_{t+1} \\sim P(s_t, a_t), \\forall t \\in \\{0,1,\\cdots\\}$. Similarly, given a policy $\\pi$ and utility function $g$, the associated state value function is given below.\n$V^{\\pi}_g(s) = \\mathbb{E}_\\pi [\\sum_{t=0}^{\\infty} \\gamma^t g(s_t, a_t) | s_0 = s]$\n(2)\nMoreover, for each policy $\\pi$, and utility function $g$, the the advantage function is computed as, $A^{\\pi}_g(s, a) = Q^{\\pi}_g(s, a) - V^{\\pi}_g(s), \\forall (s, a)$. The state occupancy measure corresponding to $\\pi$ is defined as\n$d^{\\pi}(s) = (1 - \\gamma) \\mathbb{E}_\\pi [\\sum_{t=0}^{\\infty} \\gamma^t \\mathbb{I}(s_t = s) | s_0 \\sim \\rho]$\n(3)\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. In this paper, we ignore the dependence on $\\rho$ for simplifying the notations whenever there are no confusions. The state-action occupancy measure induced by $\\pi$ is defined as $\\nu^{\\pi}(s, a) = d^{\\pi}(s)\\pi(a|s), \\forall (s, a)$. The expected value of $V^{\\pi}(s)$ obtained over $s \\sim \\rho$ is denoted as $J_V^{\\pi}$. The goal of learning CMDP is to solve the following optimization problem.\n$\\max_{\\pi \\in \\Pi} J_r^{\\pi} \\text{ subject to: } J_c^{\\pi} \\geq 0$\n(4)\nwhere $\\Pi$ is the set of all policies. We assume that at least one interior solution exists for the above optimiza-tion, which is known as Slater's condition.\nNote that the policy function cannot be expressed in a tabular format for infinite states. General param-eterization can be used in such cases. It indexes policies by a $d$-dimensional parameter $\\theta$. Let $J_g(\\theta) \\triangleq J_g^{\\pi(\\theta)}$ for any $g \\in \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}}$. The optimization (4) can now be written as follows.\n$\\max_{\\theta \\in \\mathbb{R}^d} J_r(\\theta) \\text{ subject to: } J_c(\\theta) \\geq 0$\n(5)"}, {"title": "4 Algorithm Design", "content": "The standard approach to solve the constrained optimization (4) is utilizing a saddle-point optimization on the Lagrangian function $\\mathcal{J}_{r+\\lambda c}$, where $\\lambda$ is a Lagrange multiplier. We use $\\pi^*$ to denote an optimal solution to (4). Similarly, $\\lambda^*$ is used to denote its corresponding dual solution.\n$\\pi^* \\in \\arg \\min_{\\lambda \\geq 0} \\max_{\\pi \\in \\Pi} \\mathcal{J}_{r+\\lambda c}$\n(6)\nThe following result is well-known in the literature [8, Lemma 1]."}, {"title": "5 Last-Iterate Convergence Analysis", "content": "This section discusses the last-iterate convergence properties of Algorithm 2. Before stating the result, we will enlist a few assumptions that are needed for the analysis.\nStatement S1 is a consequence of Assumption 2 whereas S2 results from Lemma 7. Statement S3(a) is iden-tical to Assumption 4, S3(b) follows from Assumption 2, and finally, S3(c) can be deduced via Assumption 2 and 4. We can, hence, apply Corollary 2 of [17] with $\\kappa = k = G2/\\mu_F$ and derive the following result if"}, {"title": "6 Conclusion", "content": "We present an algorithm for the general parametrized CMDP that ensures $\\mathcal{O}(\\epsilon + \\epsilon_{bias})$ last-iterate optimal-ity gap and the same constraint violation with $\\tilde{\\mathcal{O}}(\\epsilon^{-2} \\min{\\epsilon^{-2}, (\\epsilon_{bias})^{-2}})$ sample complexity. Here $\\epsilon_{bias}$ denotes the transferred compatibility approximation error of the underlying policy class. Future work in-cludes improvement of the sample complexity to $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ across all parameterized policy classes, extension of our ideas to general utility CMDPs, etc."}]}