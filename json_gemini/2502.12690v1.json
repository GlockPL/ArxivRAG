{"title": "Fast Data Aware Neural Architecture Search via Supernet Accelerated Evaluation", "authors": ["Emil Njor", "Colby Banbury", "Xenofon Fafoutis"], "abstract": "Tiny machine learning (TinyML) promises to revolutionize fields such as healthcare, environmental monitoring, and industrial maintenance by running machine learning models on low-power embedded systems. However, the complex optimizations required for successful TinyML deployment continue to impede its widespread adoption.\n\nA promising route to simplifying TinyML is through automatic machine learning (AutoML), which can distill elaborate optimization workflows into accessible key decisions. Notably, Hardware Aware Neural Architecture Searches where a computer searches for an optimal TinyML model based on predictive performance and hardware metrics have gained significant traction, producing some of today's most widely used TinyML models.\n\nNevertheless, limiting optimization solely to neural network architectures can prove insufficient. Because TinyML systems must operate under extremely tight resource constraints, the choice of input data configuration, such as resolution or sampling rate, also profoundly impacts overall system efficiency. Achieving truly optimal TinyML systems thus requires jointly tuning both input data and model architecture.\n\nDespite its importance, this \u201cData Aware Neural Architecture Search\u201d remains underexplored. To address this gap, we propose a new state-of-the-art Data Aware Neural Architecture Search technique and demonstrate its effectiveness on the novel TinyML \u201cWake Vision\u201d dataset. Our experiments show that across varying time and hardware constraints, Data Aware Neural Architecture Search consistently discovers superior TinyML systems com-", "sections": [{"title": "1. Introduction", "content": "Today, low-power microcontrollers are ubiquitous, embedded within Internet of Things (IoT) systems where they perform specialized tasks such as controlling appliances and monitoring sensors [1]. By integrating intelligent Machine Learning (ML) systems into these microcontrollers, we can enable them to make intelligent decisions, and adapt to changing conditions, at the edge. This empowers devices to anticipate needs, simplify interactions, and ultimately improve quality of life. This vision is at the core of the TinyML field, which has already demonstrated significant impact across diverse applications such as environmental monitoring [2], predictive maintenance [3], and healthcare [4].\n\nSuccessfully deploying ML systems \u2014 particularly deep learning systems, which are inherently resource-intensive on microcontrollers is challenging. These low-power devices' strict limitations on compute, power, and memory demand substantial optimizations to produce feasible systems. Such optimizations necessitate tailoring the hardware platform, inference library, ML model, and data pre-processing pipeline to meet application-specific resource constraints a process that, with current tools, demands significant Non-Recurrent Engineering (NRE) effort.\n\nThis challenge is further exacerbated by the heterogeneous landscape of microcontrollers, which consist of a wide variety of models and manufacturers. While individual microcontrollers are inexpensive, replacing a large fleet of deployed devices especially those in remote and hard-to-reach locations can be prohibitively costly or impractical. Consequently, TinyML systems must often operate on heterogeneous hardware platforms, each requiring customized NRE work to fit varying resource capabilities.\n\nThe high NRE costs associated with deploying TinyML systems create a significant barrier to adoption. To address this, AutoML techniques have emerged as a promising solution for automating the optimization process.\nIn particular, Hardware Aware Neural Architecture Search (NAS) [5] is re-"}, {"title": null, "content": "garded as a promising research direction to scale the process of creating lean TinyML systems. Alongside scaling the creation of TinyML systems, the speed at which NAS evaluates Neural Network (NN) architectures has also proven useful for creating higher-performing architectures than manually designed architectures. Models such as MCUNet [6] and Micronets [7], both designed through Hardware-Aware NAS, are widely recognized benchmarks for resource-efficient TinyML systems.\n\nWhile Hardware-Aware NAS optimizes the resource usage of neural networks, it considers only the model architecture and overlooks the role of input data configurations another critical component of TinyML systems. To bridge this gap, researchers have recently introduced \u201cData-Aware NAS\" [8], an approach that extends Hardware-Aware NAS by simultaneously searching for the optimal input data configuration alongside the model architecture [8].\nIn this work, we continue this thrust towards Data Aware NAS by showing that this extension offers two key advantages:\n\nImproved Resource Efficiency. Data Aware NAS can reduce data granularity, leading to smaller input data, intermediate NN representations, and model parameters. This reduction can minimize resource usage while still enabling the system to meet predictive performance constraints.\n\nEnhanced Predictive Performance. Under strict resource constraints, Data-Aware NAS can identify optimal trade-offs between data granularity and model complexity, resulting in improved predictive performance compared to traditional Hardware-Aware NAS approaches.\n\nExpanding the NAS search space to include input data configurations increases the search space size manyfold. While this could have introduced the challenge of maintaining tractable search times, our experiments show that a Data Aware NAS outperforms a traditional Hardware Aware NAS across varying time constraints.\n\nIn their initial exploration of Data Aware NAS [8], the authors developed a simple layer-based approach and validated its effectiveness on the ToyADMOS dataset [9]. Although this work successfully demonstrated the Data Aware NAS concept, the simplicity of the ToyADMOS dataset limits the ability to showcase the full potential of Data Aware NAS under more realistic and challenging scenarios.\n\nIn this paper, we advance the state of Data Aware NAS by applying it to the significantly more complex Wake Vision dataset [10]. Exploring"}, {"title": null, "content": "TinyML systems for using this dataset would be infeasible using the prior Data Aware NAS implementation due to speed limitations. Therefore, to overcome this barrier, we integrate state-of-the-art Supernet-based NAS techniques in a new Data Aware NAS implementation to ensure tractable search times. Specifically, we make the following contributions:\n\nImproved Performance Under Resource Constraints. We demonstrate that Data-Aware NAS consistently outperforms traditional Hardware-Aware NAS in creating TinyML systems that achieve better predictive performance within the same resource constraints.\n\nEfficient Search Times. We show that Data-Aware NAS discovers superior TinyML systems compared to purely architecture-focused methods regardless of time constraints.\n\nReduced Expert Intervention. Compared to Hardware-Aware NAS, our approach reduces the expert knowledge required to design and create efficient TinyML systems.\n\nSupernet-based Evaluation. We contribute an improved Data Aware NAS based on supernets, significantly accelerating the NAS evaluation process and enabling the discovery of optimized systems for complex datasets and search spaces.\n\nThe remaining sections of the paper are organized as follows:\n\nSection 2: Related Work. Presents prior scientific works that closely align with the challenges and solutions presented in this work.\n\nSection 3: Resource Consumption Analysis. Examines the typical resource constraints encountered in TinyML and analyzes the resource interplay between data and TinyML models.\n\nSection 4: Improving Data Aware Neural Architecture Search. Introduces the enhanced Data Aware NAS implementation enabling the experiments presented in this work.\n\nSection 5: Results. Presents the experimental outcomes of our enhanced Data Aware NAS approach and discusses the insights drawn from them."}, {"title": "2. Related Work", "content": "The research presented in this paper intersects multiple fields. First, it is based on the idea of using a computer algorithm to optimize NN architectures, commonly known as NAS. Second, it considers the deployment of resource-intensive NN models on severely resource-constrained embedded devices, a domain known as TinyML. This has so far been achieved either by handcrafting manual NN architectures or via Hardware Aware NASs. Third, it intersects the growing trend of Data-Centric AI."}, {"title": "2.1. Neural Architecture Search", "content": "NAS is a widely explored approach for designing high-performance NN architectures in traditional computational scales [11, 12, 13, 14]. Early NAS systems relied on evolutionary algorithms [15] and reinforcement learning [11, 12] to explore discrete search spaces of potential NN architectures.\n\nRecently, supernet-based methods, which train a single large model and extract high-quality subarchitectures from it, have gained popularity as a way to reduce search times [14]. Differentiable Neural Architecture Search (DNAS) has also emerged as a promising technique, where a discrete NN search space is turned continuous through proxy scalar values, enabling fast searches through gradient-based optimization methods [16]."}, {"title": "2.2. TinyML Neural Architecture Search", "content": "The application of NAS has been central in creating models that balance performance and efficiency to run on TinyML hardware since the very"}, {"title": null, "content": "beginning of the field. SpArSe [17], one of the first works to show that Convolutional Neural Networks (CNNs) can be deployed on TinyML hardware, uses bayesian-based NAS to find suitable CNN architectures.\n\nMore recent works on TinyML NAS include MCUNet [6] and Micronets [7], used to create the MCUNet and Micronet family of models, which are among the most used in TinyML.\n\nThe MCUNet NAS consists of two steps. During the first step, a search space is created where most models fit tightly into the resource constraints of a TinyML device. In the second step, a one-shot NAS is performed on the search space, after which an evolutionary search is used to sample the best sub-network. The MCUNet NAS arguably includes data in its search space as the adapted search space for TinyML is created by, among others, reducing the input resolution of the search space [6].\n\nThe authors of the Micronets paper use DNAS [16] based on either a MobileNetv2 [18] or a DS-CNN(L) [19] backbone to design TinyML models for Person Detection, Keyword Spotting, and Anomaly Detection.\n\nOther works on NAS for TinyML include CNAS, which optimizes NN for hardware device operator constraints [20] and \u00b5NAS, which proposes a NAS with a highly granular search space [21]."}, {"title": "2.3. Manually Designed TinyML Architectures", "content": "While NAS dominates TinyML model development, several works propose manually designed architectures. For instance, CoopNet is a manually designed model that utilizes heterogeneous quantization and binarization to produce an efficient model architecture [22]. Another example is the DS-CNN model, which uses depthwise separable convolution layers to achieve superior performance in keyword spotting applications within TinyML resource constraints [19]."}, {"title": "2.4. Data-Centric Artificial Intelligence", "content": "An up-and-coming research area that has traditionally received little attention is Data-Centric AI. Where many research papers propose techniques for optimizing ML models, much fewer papers consider ways to improve ML through data-centric innovations [23]. Techniques such as weak supervision for fast dataset labeling [24], confident learning for automated label error spotting and correction [25], and active learning where only the most important data samples are labeled [26] are key contributions to this domain."}, {"title": "2.5. Data Aware Neural Architecture Search", "content": "Researchers recently introduced Data Aware NAS [8], which incorporates data configurations into the search space of a Hardware Aware NAS. Using evolutionary algorithms, they jointly optimized data configurations and CNN models within a layer-based search space. Their findings demonstrates that Data Aware NAS can significantly reduce resource consumption in TinyML systems.\n\nBuilding on this foundation, this present study conducts a comprehensive comparison between Data Aware- and traditional Hardware-Aware NAS through experiments on a more complex dataset. This comparison would not be possible using the prior Data Aware NAS implementation due to speed limitations. To overcome this barrier, we contribute a new Data Aware NAS implementation, built on state-of-the-art NAS techniques."}, {"title": "3. Resource Consumption Analysis", "content": "Prior work on Data Aware Neural Architecture Search [8] did not comprehensively analyze the resource consumption dynamics between sensor data and TinyML models. To address this gap, this section explores the resource consumption of TinyML systems in depth. We first characterize the available resources in such systems, then examine how data transfer from sensors impacts resource usage, and finally analyze the resource consumption of running TinyML models."}, {"title": "3.1. Available Resources", "content": "The available resources in a TinyML system are primarily decided by the hardware platform on which it is deployed. Typical resources that we consider are computational capacity, volatile RAM, persistent flash storage, and energy. These resources are interdependent, with energy consumption often being linked to the usage of other resources.\n\nConsider the Arduino Nano 33 BLE Sense [27], a popular device in the TinyML community. It features an ARM Cortex M4 processor supporting Single Instruction Multiple Data (SIMD) parallel processing for operations like Multiply-Accumulates (MACs), commonly used in NNs. The device provides 256 kB of Static RAM (SRAM) and 1MB of flash storage. While there are no official figures for the energy consumption of the device, our measurements show a power draw of around 125mW during ML inference."}, {"title": "3.2. Sensor Data Resource Consumption", "content": "In a TinyML system, sensor data undergoes several steps: capturing, transferring to the Microcontroller Unit (MCU) memory, pre-processing, and feeding into the TinyML model. Fast and Efficient data transfer throughout these steps requires fast read and write operations. Among the memory types typically present in MCUs, RAM fits these characteristics best.\n\nPre-processing sensor data uses both processing and memory resources, requiring data to be read, processed, and written back to memory. Feeding the processed data to a TinyML model may require further data movement if the model requires input data to be stored in a specific location.\n\nData can be fed to models using two primary paradigms:\n\nStreaming Data is processed one sample at a time as it is captured. Also known as online processing.\n\nBatched Data is accumulated in a buffer until enough samples are collected for parallel processing. Also known as offline processing.\n\nThe batch paradigm consumes more memory as sensor data needs to be aggregated and preserved in memory for longer than with streaming. However, batched processing can leverage improved parallel processing and, therefore, improve the inference efficiency of the model. Furthermore, batching of sensor data is required to predict over the temporal dimension (e.g., audio data), except in some rare instances where recurrent neural networks are used. Due to the tight memory constraints of TinyML systems, large batches are typically infeasible, and streaming is used more frequently."}, {"title": "3.3. Estimating TinyML Model Resource Consumption", "content": "Compute, memory, storage, and energy are precious resources for TinyML systems, making it crucial to track and estimate their usage accurately. Most embedded applications statically allocate memory and storage. Therefore, we can accurately predict the consumption of each by the architecture of the deployed NN. For storage (i.e., non-volatile flash) consumption, the primary contributor is the number of parameters in the model and their data type (e.g., int8, fp16, etc.). As shown in eq. (1), one can sum the number of parameters in a model, multiply by the size of each parameter, which is determined by the datatype, and factor in some overhead for the rest of the application stack [7]. You can predict the working memory consumption of a model using eq. (2). You take the maximum of the input and output"}, {"title": null, "content": "intermediate tensors for any layer in a network, factoring in any residuals or other tensors that need to be preserved in memory.\n\n$C_f = \\sum_{m_p}t_s$  (1)\nwhere $c_f$ : Flash Consumption in Bytes,\n$m_p$ : Model Parameter,\n$t_s$ : Size of Parameter Datatype in Bytes.\n\n$c_r = max(d + l_{io} \\forall i \\in m, max(l_{ji} + l_{jo}))t_s$  (2)\nwhere $c_r$ : RAM Consumption in Bytes,\n$d$ : Size of Input Data in Bytes,\n$l_j$: A layer in a Model,\n$m$ : The set of all layers in a Model,\n$l_{io}$ : Size of First Model Layer Output in Bytes,\n$l_i$ : Size of Layer Input in Bytes,\n$l_o$ : Size of Layer Output in Bytes,\n$t_s$ : Size of Data, Input and Output Datatypes in Bytes.\n\nComputational resources are not \u201cconsumed\u201d in the same way, but they determine if a given model is computed within the latency budget of the application. To predict latency, you can often use proxy metrics, such as the number of operations, or predictive surrogate models [5]. To get the best accuracy, one must measure the real latency on the device itself. The same goes for energy consumption, which, given the relatively simple hardware architecture of MCUs, is inextricably linked to the latency."}, {"title": "4. Improving Data Aware Neural Architecture Search", "content": "While prior work demonstrated that a Data Aware NAS could reduce resource consumption of TinyML systems under predictive performance constraints, it failed to show the trading-off of data and model resources to improve predictive performance [8].\n\nTo show this phenomenon in this paper, we must target a less trivial application than previously [8]. A promising application in TinyML is Person Detection, where an image is to be classified according to whether it contains"}, {"title": null, "content": "a person [28]. As this is a vision application, it will work on higher dimensionality data than previously considered in Data Aware NAS, enabling a deeper exploration of resource-performance trade-offs.\n\nTargeting Person Detection allows us to utilize the new \u201cWake Vision\u201d dataset [10], recently introduced to the TinyML domain, promising vast improvements in size and quality over traditional TinyML datasets."}, {"title": "4.1. A Supernet Based Data Aware Neural Architecture Search", "content": "The increased complexity of Person Detection, coupled with the larger Wake Vision dataset, makes the prior approach of using a layer-based search space and naive evolutionary algorithms too slow for practical use. To address this, we propose a new supernet-based implementation utilizing a MobileNetv2 [18] backbone.\n\nIn this approach, a fully sized MobileNetv2 supernet is trained for each input data configuration. Training is performed lazily, meaning it is initiated only right before the supernet's first use. During evaluation, the Data Aware NAS samples a model configuration from the supernet corresponding to its data configuration, which provides a model with pre-trained weights. This model requires only minimal fine-tuning, enabling faster evaluation [29].\n\nWe evaluate each TinyML system using the fitness function described in Appendix B, and use a tournament-based genetic algorithm to produce new candidate TinyML systems based on previous ones.\n\nInput Data Configuration Options. We configure the new Data Aware NAS implementation to consider two orthogonal ways to configure its data. First, we allow it to configure the resolution of the image that is passed onto the NN model. Possible resolution options span $R = \\{32, 64, 96, 128, 160, 192, 224\\}$. Large input resolutions profoundly impact the RAM consumption of data due to the increased shape of the tensor storing the image. It also has a significant effect on the size, and therefore RAM consumption, of the internal representations, i.e., activations, of the NN model created by the convolutional and depthwise separable convolutional layers, both of which are frequent in a MobileNetV2 model. The number of parameters of a model is less affected by input resolution changes, as parameters in convolutional and depthwise separable convolutional layers depend on fixed filter sizes, not tensor dimensions.\n\nSecondly, we enable the Data Aware NAS to adapt whether the image is encoded in monochrome or Red, Green, and Blue (RBG) format, i.e.,"}, {"title": null, "content": "$C = \\{monochrome, rbg\\}$. If a monochrome image encoding is picked, we reduce the width (also called alpha) [18] of the MobileNetv2 supernet by 1/3. Consequently, this change affects both the RAM and flash consumption of the final TinyML system.\n\nModel Configuration Options. We consider two ways to adapt the MobileNetv2 architecture. The first is to adapt the depth of the network. A MobileNetv2 architecture is made up of inverted bottleneck blocks [18] that, over time, reduce the resolution of the input image while greatly increasing its channels (also often called features). These blocks are divided into seven overall stages, see fig. 1, inside which the internal representation shape between the blocks is kept constant. Because of this structure, it is possible to remove blocks from these stages as long as one block in each stage remains to update the internal representation shape for the next stage. We keep the initial two MobileNetV2 blocks intact to enable them to extract low-level features for later layers. With that in mind, we define our model depth search space as described in eq. (3)."}, {"title": null, "content": "$(b_3, b_4, b_5, b_6, b_7) \\in B_3 \\times B_4 \\times B_5 \\times B_6 \\times B_7$\nwhere $B_3 = \\{0, 1, 2, 3\\}$\n$B_4 = \\{0, 1, 2, 3, 4\\}$\n$B_5 = \\{0, 1, 2, 3\\}$\n$B_6 = \\{0, 1, 2, 3\\}$\n$B_7 = \\{0, 1\\}$  (3)\n\nWe also enable the possibility of completely removing stages if downstream stages are also removed. For example, it is possible to remove stage 6, given that stage 7 is also removed by attaching the final classification head to the end of stage 5. Our search strategy ensures that infeasible architectures containing blocks after a completely removed stage are not generated during the search and that this does not adversely affect the probability of finding deeper architectures. Adapting the depth of the network has no implications on the RAM requirements to store data but can, depending on the stage and block, have a large impact on the RAM consumption of internal model representations and persistent memory taken up by the total number of parameters in the model.\n\nThe second approach to adapting the MobileNetv2 architecture is to change the width of the network, that is, the number of channels created by the convolutional layers of the model. The width of the network is controlled by a single alpha parameter $A = \\{0.1, 0.2, \\ldots, 1\\}$. For implementation reasons, this is achieved using masking layers as introduced in FBNetV2 [30]. This change significantly impacts both the size of internal model representations and the total number of parameters in the model.\n\nDue to the complexity of accurately modeling model latency and energy consumption accurately, our implementation limits its hardware-aware metrics to RAM memory and flash storage. To calculate the total memory consumption of the TinyML systems, we assume a streaming data paradigm where only a single data sample is stored in memory at a time. Our implementation supports storing the data or model in datatypes of various sizes; however, all experiments in this paper are conducted assuming 8-bit datatypes for storing both data, internal model representations, and model parameters.\n\nA URL to an open-source repository containing the new Data Aware NAS implementation can be found in Appendix A."}, {"title": "5. Results", "content": "This section presents a comprehensive evaluation of the Data Aware NAS system described in section 4 through four complementary experiments. All experiments are conducted on a Nvidia V100 system for 23 hours, with metrics such as Accuracy, Precision, and Recall all estimated on full 32 bit floating point models.\n\nBefore presenting the results of these four experiments, we first revisit results from prior work on Data Aware NAS [8] to present a self-contained and holistic evaluation of Data Aware NAS. Second, we show experimental results that Data Aware NAS is able to find TinyML systems with better predictive performance than Hardware Aware NASs (i.e., Non-Data Aware NAS) under the same resource constraints. Third, we present results that show a Data Aware NAS outperforming a Hardware Aware NAS at any time constraint. Fourth, we apply the Data Aware NAS across embedded systems of differing resource constraints, showing how Data Aware NAS can automatically adapt to resource constraints where Hardware Aware NAS necessitates manual engineering work. Fifth and last, we compare the search speed of the current supernet-based Data Aware NAS to the previously published Data Aware NAS implementation, showcasing how the technical improvements highlighted in this paper allow for the discovery of more and better TinyML systems.\n\nCollectively, these experiments provide strong evidence that a Data Aware NAS produces superior TinyML models compared to traditional Hardware Aware NASs, achieving better performance across any timescale."}, {"title": "5.1. Resource Consumption Under Performance Constraints", "content": "We first revisit the main results from the previous publication on Data Aware NAS [8]. These results are not based on the Data Aware NAS algorithm described in section 4, but on the simpler ToyADMOS dataset [9] and a more naive approach to Data Aware NAS. This earlier method utilized evolutionary search over a layer-based search space. For more details, we refer to the previous paper [8].\n\nIn this prior work, the authors used both a Data Aware NAS and a traditional Hardware Aware NAS to search for TinyML systems that could effectively distinguish between the normal and anomalous samples of the ToyADMOS dataset. We showcase the results of this experiment in Pareto Frontier plots in figs. 2 and 3. Note that while all points plotted represent"}, {"title": "5.2. Predictive Performance Under Resource Constraints", "content": "The first experiment provides evidence that a Data Aware NAS can reduce the overall resource requirements of a TinyML system under constraints on the system's predictive performance. A second central question is whether a Data Aware NAS can find better TinyML systems than a Hardware Aware NAS under identical resource constraints.\n\nTo investigate this, we set up an experiment where a Data Aware NAS and an equivalent Hardware Aware NAS search for a TinyML system to fit in memory constraints of 512kB RAM and 2MB flash memory. These memory constraints are typical for many TinyML-scale devices, for example, the STM32H743 [6] or the STMF767ZI [7].\n\nThe results of this experiment are plotted in figs. 4 and 5 for the Data Aware- and Hardware Aware NAS respectively. These figures show that the Data Aware NAS finds architectures that perform better than the Hardware Aware NAS at a maximum accuracy of 79.5% and 77.6%, respectively. Furthermore, the Data Aware NAS finds 22 models with an accuracy of above 75%, whereas the Hardware Aware NAS finds only 4 models above this threshold. The larger amount of Pareto optimal models generated by Data Aware NAS would, in a practical application, mean that it would be easier to find a TinyML system fitting the requirements for an application.\n\nThese results suggest that a Data Aware NAS not only improves predictive performance but also provides more Pareto optimal choices, simplifying"}, {"title": "5.3. Data Aware Neural Architecture Search Speed", "content": "The search space of a Data Aware NAS is a superset of the search space of a Hardware Aware NAS and is several times larger. Therefore, a relevant question is: Can a Data Aware NAS find better TinyML systems than a Hardware Aware NAS in a certain period of time?\n\nWe use the models generated throughout the two experiments from section 5.2 to investigate this. In fig. 6, we plot the fitness score yielded by the fitness function of our NASs (see Appendix B) for the models generated by both the Data Aware- and Hardware Aware NAS.\n\nContrary to our initial hypothesis, the models generated by the Data Aware NAS outperform the models generated by the Hardware Aware NAS throughout the entire search process, which suggests that it is worth using a Data Aware NAS over a Hardware Aware NAS regardless of the search time.\n\nOur fitness function is not a perfect proxy for when good TinyML systems are found, as systems can be Pareto optimal without achieving a high fitness score. Therefore, as a complementary analysis, we plot the cumulative"}, {"title": "5.4. Ease of Use Across Resource Constraints", "content": "The experiment in section 5.2 shows that Data Aware NAS can create better TinyML systems than Hardware Aware NAS under the resource constraints of that experiment. A question remains: How does this generalize to other resource constraints?\n\nTo test this, we set up a Data Aware NAS to search for TinyML systems that can fit in the memory constraints of two other resource-constrained hardware platforms. For the first we select memory constraints of 320 kB and 1MB corresponding to the STM32F746 [6, 7]. As the second we set memory constraints of 256 kB and 1 MB corresponding to the STM32F412 [6], STM32F446RE [7] or the Arduino Nano 33 Bluetooth Low Energy (BLE)"}, {"title": "5.5. Supernet Speedup", "content": "In section 4, we claim that it is necessary to introduce a new Data Aware NAS implementation based on MobileNetV2 supernets to find good TinyML systems for Person Detection in tractable time. In this experiment, we seek to prove this by evaluating the supernet-based implementation's speedup over the original naive layer-based implementation proposed in the original work on Data Aware NAS [8].\n\nTo do so, we run a modified version of the prior naive NAS implementation for 23 hours the same time as allocated to our new supernet-based NAS"}, {"title": null, "content": "in other experiments. The modifications made to the naive NAS are all done in an attempt to enable a fair apples-to-apples comparison between the two NASs. A description of the modifications done can be found in Appendix C.\n\nEven with these modifications, creating a truly fair comparison is difficult due to the extent of changes made to improve the novel supernet-based search. For example, the search space of the two different NASs allows for discovering entirely different model architectures with varying training times and learning capacities. Furthermore, the first models generated by the supernet-based search may take a long time to evaluate as the supernet for that data configuration needs to be pre-trained. On the other hand, once the base supernets are trained, the supernet-based search should evaluate models much faster.\n\nBecause of these differences, a direct comparison between the number of TinyML systems generated during the 23 hours for each NAS would be unfair. Instead, we compare the number of systems that must be evaluated before reaching a certain accuracy threshold. Because the naive layer-based search space limits the accuracy that it is possible to find in 23 hours, we have to compare the two NAS at a low accuracy threshold at 67% (The maximum non-decimal accuracy achieved by all naive NAS runs). We plot the results of this experiment in fig. 10.\n\nThese results show that the supernet-based NAS on average finds models 67% accuracy in roughly half the number of system evaluations as the earlier naive NAS. Furthermore, the supernet-based NAS finds much better systems throughout its search, reaching a maximum accuracy of 79.5%. In contrast, the naive NAS never finds a system reaching an accuracy above 70%."}, {"title": "6. Future Work", "content": "The search for TinyML systems using Data Aware NAS has only just begun. There are still plenty of opportunities for future research into, e.g., improving Data Aware NAS, using it to create new state-of-the-art TinyML Systems, or applying it to discover theoretical insights into TinyML applications. This section identifies some of the most promising of these future research directions.\n\nExpanded Hardware Metrics. Resource constraints on TinyML hardware encompass more than just the memory and computational constraints consid- ered in this and other works on NAS for TinyML [6, 7, 31, 20]. Inference"}, {"title": null, "content": "time and energy consumption metrics are especially tough to quantify as they depend heavily on target hardware. Both are highly relevant metrics in TinyML system design. Unfortunately, these metrics are currently hard to simulate or estimate accurately without deploying TinyML systems onto real hardware which is challenging in TinyML NAS where models must be evaluated quickly and without manual intervention. Therefore, a promising research avenue to explore is to create accurate estimator tools that can provide realistic metrics in little time and be integrated automatically into NAS tools.\n\nDifferentiable Data Aware Neural Architecture Search. Many state-of-the-art NASs have recently explored using differentiable search spaces and strategies to speed up their search with promising results [16, 29]. Expanding Data Aware NAS to use a differentiable search space and strategy could improve search speeds to discover even better TinyML systems. Doing so would require designing the data configuration search space in a way that makes it possible to use differentiable search methods."}, {"title": null, "content": "TinyML Domain Insights using Data Aware Neural Architecture Search. This paper shows how Data Aware NAS finds better-performing TinyML Person Detection systems on severely resource-constrained devices by lowering resolution and color representation. This suggests that a larger NN architecture is more important for Person Detection systems than a higher data granularity.\n\nTinyML systems in other application domains may perform better by trading off NN architecture size for a larger data granularity. Investigating these trade-offs for different TinyML application domains may give novel insights into these domains that can prove valuable to the broader community.\n\nCreating State of the Art TinyML Models using Data Aware Neural Architecture Search. Although the experiments in this paper show how Data Aware NAS can produce better TinyML systems than an equivalent Hardware Aware NAS, the search space is not carefully designed to find state-of-the-art TinyML models.\n\nApplying Data Aware NAS alongside a more carefully designed search space may be able to produce models that can outperform current state-of-the-art models such as the MCUNet models [6]."}, {"title": "7. Conclusion", "content": "The adoption of TinyML in the industry remains slow, largely due to the deep expertise required to deploy heavily optimized TinyML systems to microcontroller scale devices. AutoML approaches such as Hardware Aware NAS are promising avenues for making otherwise heavily technical optimization techniques accessible to the broader community.\n\nIn this paper, we extend prior work on Data Aware NAS, an augmentation of Hardware Aware NAS, which considers various ways to configure input data alongside a traditional Hardware Aware NAS.\n\nAs a part of this extension, we make significant technical improvements to prior work on Data Aware NAS, moving from considering trivial datasets, search strategies, and search spaces to a state-of-the-art TinyML dataset along with a supernet-based NAS utilizing a MobileNetV2 backbone.\n\nOn the basis of these improvements, we are able to provide new results showing Data Aware NAS discovering more accurate TinyML systems than Hardware Aware NAS given some resource constraints, doing so across any allocated NAS time and different resource constraints.\n\nThese results pave the way for using Data Aware NAS to design state-of- the-art TinyML systems and to gather new domain insights in TinyML that can be useful to all TinyML practitioners."}, {"title": "Appendix B. Fitness Function Details", "content": "During both the Data Aware NAS and the traditional Hardware Aware NAS presented in this paper, TinyML systems are evaluated using a fitness function to determine how well they perform on both prediction and hardware-related metrics. The fitness function used in this work can be found in eq. (\u0392.1)."}, {"title": null, "content": "$f = w_1a + w_2p + w_3r + w_4(1-\\frac{v_r"}, {"a": "Model accuracy", "np": "Model precision", "nr": "Model recall", "n$v_r$": "RAM violation (excess usage)", "n$x_r$": "Maximum allowable RAM consumption", "n$v_f$": "Flash violation (excess usage)", "n$x_f$": "Maximum allowable Flash consumption", "w_5$": "Weights to adjust the importance"}]}