{"title": "An Adaptive Framework for Generating Systematic Explanatory Answer in Online Q&A Platforms", "authors": ["Ziyang Chen", "Jinzhi Liao", "Xiaobin Wang", "Pengjun Xie", "Xiang Zhao", "Yong Jiang", "Fei Huang"], "abstract": "Question Answering (QA) systems face challenges in handling complex questions that require multi-domain knowledge synthesis. The naive RAG models, although effective in information retrieval, struggle with complex questions that require comprehensive and in-depth answers. The pioneering task is defined as explanatory answer generation, which entails handling identified challenges such as the requirement for comprehensive information and logical coherence within the generated context. To address these issues, we refer to systematic thinking theory and propose SynthRAG, an innovative framework designed to enhance QA performance. SynthRAG improves on conventional models by employing adaptive outlines for dynamic content structuring, generating systematic information to ensure detailed coverage, and producing customized answers tailored to specific user inquiries. This structured approach guarantees logical coherence and thorough integration of information, yielding responses that are both insightful and methodically organized. Empirical evaluations underscore SynthRAG's effectiveness, demonstrating its superiority in handling complex questions, overcoming the limitations of naive RAG models, and significantly improving answer quality and depth. Furthermore, an online deployment on the Zhihu platform revealed that SynthRAG's answers achieved notable user engagement, with each response averaging 5.73 upvotes and surpassing the performance of 79.8% of human contributors, highlighting the practical relevance and impact of the proposed framework. Our code is available at https://github.com/czy1999/SynthRAG.", "sections": [{"title": "1\nINTRODUCTION", "content": "In the era of information explosion, Question Answering (QA) systems have become an indispensable component within the realm of artificial intelligence [4, 5, 25], widely employed in search engines, intelligent assistants, and domain QA platforms, greatly improving the efficiency and accuracy of information retrieval [15]. However, with the increasing demands of users, practical questions in daily life are progressively more complex. In particular, users require an explanatory context for a given question rather than an entity, phrase, or sentence. To meet the requirement, it is imperative to integrate multiple sources of information to provide insightful answers [18, 21].\nLarge Language Models (LLMs) excel at handling straightforward QA tasks where the ideal responses are factually concise. However, in cases that require synthesizing supporting references from diverse domains and producing logically coherent content, they often fall short [4]. The reason might lie on the extra capabilities in exploration and integration, instead of solely focusing on generating fluent contexts.\nAs a response, Retrieval-Augmented Generation (RAG) models have become an effective approach for generating long-form answers by retrieving relevant information [10].\nThe initial RAG method consists of simple indexing, retrieval, and generation processes; however, it fails to consider retrieval precision and generation quality [9]. Following studies improve these processes by employing pre-retrieval optimization, post-retrieval optimization, and iterative retrieval techniques [2, 11, 12, 28], thereby increasing the accuracy and contextual relevance of generated answers. Although current RAGs can supplement LLMs with up-to-date information, they still lack the ability to identify the intricate features of such complex QA situations. In comparison to general answers, the generation is supposed to be both comprehensive and in-depth, ensuring coverage of potential domains and details. Therefore, we first coin the task as explanatory answer generation and outline its notorious challenges as follows.\nChallenges. First, the comprising components of the explanatory answer should be complete. In order to answer the question in Fig. 1, it is necessary to simultaneously consider topics such as \"The History of Gold\", \"International Situation\" and \"Gold Supply & Demand\" to construct a thorough result. The use of keywords and question embeddings as key clues in a naive RAG method tends to easily overlook relevant information that might not be explicitly mentioned in the question, resulting in insufficient information coverage and potential biased. Second, it is significant to logically organize the content for the explanatory answer. Naive RAG methods usually retrieve fragmented information, leading to answers that merely aggregate isolated pieces of information. As shown in the illustration, while the information is relevant, it lacks structural organization. Consequently, these responses frequently lack logical coherence and depth, as they fail to integrate the retrieved information into a coherent whole.\nThere is a pressing need for approaches that can comprehensively retrieve information and employ systematic thinking to enhance the completeness and logic of the generated answers. Gestalt psychology [13] highlights that human cognition is based on the integration of information into a unified whole, rather than simply accumulating fragmented data. This holistic approach is essential for problem-solving and knowledge construction. In the context of complex question answering, scattered information alone is insufficient to meet the user's needs for comprehensive and systematic responses. Inspired by the theory, we focus on enhancing the capabilities of LLMs in effectively integrating and systematizing information. In line with this goal, we propose the SynthRAG framework, designed to enable LLM in comprehending and answering questions from a systematic perspective.\nSpecifically, SynthRAG can incorporate existing RAG methods, making it a flexible and comprehensive solution. It consists of three main steps: adaptive outline generation, systematic information generation, and customized answer generation. In adaptive outline generation, SynthRAG learns outline instructions for different type of questions by analyzing high- and low-quality samples, then selects and adapts the appropriate outline based on the specific question to ensure comprehensive coverage and logical structure. During systematic information generation, SynthRAG retrieves and generates coherent detailed paragraphs for each subsection, resulting in comprehensive and logically structured responses. Finally, in customized answer generation, SynthRAG uses representative examples to help the model understand the problem globally, focusing on essential information to produce accurate and insightful answers. This method integrates diverse information while maintaining coherence and integrity, providing high-quality, in-depth responses that meet the user's requirements. In summary, our contribution is three-fold:\n\u2022 We systematically analyze the limitations of LLMs in handling questions that need comprehensive and in-depth responses, identifying deficiencies in information integration and answer depth.\n\u2022 We propose SynthRAG, a novel Retrieval-Augmented Generation framework that effectively integrates information fragments to provide a comprehensive and robust knowledge foundation for LLMs.\n\u2022 SynthRAG incorporates a holistic perspective that fosters high-level associative thinking and reasoning, addressing the critical issue of fragmented information linkage in traditional RAG models.\n\u2022 Through a series of human and LLM-based evaluations, our approach demonstrates significant advantages over existing methods. Results from the online deployment showed that the SynthRAG responses are well received."}, {"title": "2\nRELATED WORKS", "content": "In this paper, we explore the problem of knowledge integration for explanatory answer generation. Thus, we review prior work in retrieval augmented generation and explanatory answer generation."}, {"title": "2.1 Retrieval-Augmented Generation", "content": "Integration with external knowledge has become a prevalent strategy in QA tasks [23]. Models like REALM [11] employ dual-encoder and retrieval mechanisms to dynamically retrieve documents during answer generation. Advances such as the Fusion-in-Decoder [12] further refine this approach by merging retrieved documents directly into the input layer, delivering more precise answers through an integrated retrieval-and-generation process. Meanwhile, Self-RAG [1] employs a self-reflection mechanism to optimize both retrieval and generation, showing notable efficacy in handling long text and open-domain QA tasks. Self-Reasoning [30] improves the generation quality and traceability [14] of retrieval-augmented language models by generating reasoning trajectories. The Step-Back Prompting [32] method improves the model's performance in complex reasoning tasks by guiding it to perform high-level abstraction and reasoning. However, external knowledge is often fragmented, and these methods fail to systematize it. A holistic perspective is essential for problem-solving and knowledge construction. GraphRAG [7] provides a global perspective for LLMs by constructing a knowledge graph, enhancing the quality of responses effectively. Different from these work, we treat the retrieval step as a black box and focus on improving the generation quality given the query and retrieved passages."}, {"title": "2.2 Explanatory Answer Generation", "content": "Explanatory Answer Generation (EAG) stands apart from traditional QA system and article generation. Traditional QA systems [4, 6, 8, 16, 17, 26] typically generate answers that consist of single entities, short phrases, or concise sentences. These responses are suitable for straightforward knowledge-based questions, where users seek concise information. However, such responses often fall short when dealing with more intricate inquiries that require a detailed exploration of the subject matter. Long-form QA systems [20, 24] have been developed to bridge this gap, but they often lack the depth and structured coherence needed for comprehensive explanations. FoRAG [3] refines long-form question answering through integrating an outline-enhanced generator and a fine-grained reinforcement learning framework. Recently, some article generation work [27, 31] has also begun to attempt generating logical text by first creating an outline and then writing the full text. However, these methods are often limited to specific professional fields and lack general applicability [27].\nUnlike these studies, this research focuses on building a general QA system capable of providing comprehensive, systematic, and in-depth answers to user questions, effectively meeting user needs for thorough understanding."}, {"title": "3\nMETHODOLOGY", "content": "In this study, we propose the SynthRAG framework, a novel systematic retrieval-augmented generation approach aimed at enhancing the performance of LLMs in explanatory answer generation tasks."}, {"title": "3.1\nProblem Formulation", "content": "The primary task of this research is to improve the performance of LLMs in explanatory answer generation scenarios. The main objective is to enable LLMs to generate coherent, comprehensive, and insightful answers by integrating information from diverse sources and presenting it in a structured manner.\nSpecifically, the task involves creating a system that, given a query q, retrieves relevant external information D and generates an answer a. The answer a should comprehensively and thoroughly address the query q. Unlike traditional QA tasks that focus on knowledge-based or simple questions, this task emphasizes real-world user queries that are diverse and complex. These queries encompass various types, including knowledge-based, opinion-based, evaluative, and experiential questions, each demanding a high level of informational richness and answer quality. This task requires addressing the limitations of naive RAG methods, which often struggle with fragmented and incomplete information synthesis. Our approach aims to provide a robust framework that improves the overall quality and depth of the answers generated by LLM."}, {"title": "3.2\nSynthRAG Framework", "content": "As shown in Fig. 2, the SynthRAG framework consists of three primary components: adaptive outline generation, systematic information generation, and customized answer generation. Firstly, the adaptive outline generation component leverages historical data to learn optimal outline structures for different question types. By analyzing both high- and low-quality examples, the system identifies the most effective outlines and dynamically adjusts them to ensure comprehensive coverage of key information and a coherent logical flow. Secondly, the systematic information generation component deepens the retrieval and generation process, creating detailed and coherent information for each outline subsection. This approach ensures smooth information transitions and logical consistency, building a holistic view from macro to micro levels. Finally, the customized answer generation component employs representative examples to guide the LLM in understanding the question globally. This phase refines the generated information, ensuring that the final answer is insightful."}, {"title": "3.2.1 Adaptive Outline Generation", "content": "Different types of questions require varied response strategies. The adaptive outline generation component ensures comprehensive coverage and a logical structure in the generated content through two key stages: learning optimal outline structures from historical data, and dynamically adapting these structures to suit specific questions.\nLearning Optimal Outline Structures. The key step in adaptive outline generation is to learn the optimal outline structures from historical data for different question types. This involves analyzing high-quality and low-quality samples to identify patterns and structures that lead to effective and comprehensive answers.\nLet $Q = \\{q_1, q_2, ..., q_n\\}$ denote the set of queries and $A = \\{a_1, a_2, ..., a_n\\}$ denote the set of historical answers. In this work, we use Zhihu-KOL \u00b9 as historical data, which contains 1 million real QA pairs. To learn the optimal structure of the responses, we first calculate the effectiveness score $E(a_i)$ for each answer based on predefined criteria such as coherence, completeness and user satisfaction. The effectiveness score is given by:\n$E(a_i) = \\alpha \\cdot H(q_i, a_i) + \\beta \\cdot P(a_i) + \\gamma \\cdot U(a_i),$\nwhere the coherence score $H(a_i)$ measures the semantic similarity between the query and the answer by calculating the cosine similarity between their embeddings. The completeness score $P(a_i)$ is based on the length of the text, as overly short responses make it difficult to learn a consistent and repeatable answer structure. The score is then normalized to indicate how thoroughly the response addresses the required information. The user satisfaction score $U(a_i)$ is a crucial metric derived from the number of likes or positive feedback on online platforms, reflecting the practical utility and acceptance of the answer among users. All these scores are normalized to ensure consistency and comparability. $\\alpha$, $\\beta$, and $\\gamma$ are weighting factors determined through empirical analysis 2.\nConsequently, we utilize GTE [19] to encode the text and employ the unsupervised clustering method K-means [22] to categorize these historical samples into k distinct clusters. Let $\\{C_1, C_2, ..., C_k \\}$ be the set of question type clusters obtained through K-means clustering. For each cluster $C_j \\in \\{C_1, C_2, ..., C_k \\}$, we define the set of high-quality samples $H_j$ and low-quality samples $L_j$ as follows:\n$H_j = \\{a_i | E(a_i) \\geq \\theta_H\\},$\n$L_j = \\{a_i | E(a_i) < \\theta_L\\},$\nwhere $\\theta_H$ and $\\theta_L$ are are dynamically selected for each question category. These thresholds are set to identify the top 5% and bottom 5% of the answer samples as positive and negative samples, respectively.\nIdentifying $H_j$ and $L_j$ within each cluster, we enable the LLM to actively learn from these specific historical instances. The LLM analyzes the characteristics of high-quality and low-quality samples, summarizing them to distill detailed outline instructions $I_j$ for each query type:\n$I_j = LLM(H_j, L_j)$.\nThe detailed prompt and samples of the generated $I_j$ can be found on our code repository. These comprehensive outline instructions $I_j$ guide the adaptive outline generation process, ensuring that the model leverages the insights from historical data to construct effective and coherent outlines for different question types.\nDynamic Outline Adaptation. Once the optimal outline instructions for different question types are learned, the next step is to dynamically adapt these outline instructions to fit specific user queries. Specifically, for a given question $q_i$, we calculate the cosine similarity score $S(C_j, q_i)$ for each cluster $C_j$, and select the cluster $C$ with the highest score to the question $q_i$:\n$C = arg\\ \\underset{C_j}{max}\\ S(C_j, q_i),$\nwhere similarity score $S(C_j, q_i)$ measures how closely the characteristics of the cluster $C_j$ match the query $q_i$, typically calculated using cosine similarity.\nAfter selecting the optimal outline instructions $I$, the system dynamically adapts it to the specific outlines. This involves adjusting the depth and breadth of each section based on the query's complexity and the retrieved information.\n$O= LLM(I, q_i, D_i),$\nwhere $I$ is the outline instruction of $C$ $D_i$ is retrieved documents and $O$ is the specific outline output. $O$ is a sequence of sections and subsections,$0 = \\{S_{i1}, S_{i2}, ..., S_{in}\\}$, where $S_{ij}$ represents the j-th section of the outline for query $q_i$. This approach enables the system to dynamically adjust outline instructions to suit the specific characteristics of each query, using insights from historical data for structured and comprehensive responses."}, {"title": "3.2.2 Systematic Information Generation", "content": "The systematic information generation component of the SynthRAG framework is designed to ensure that each section of the adaptive outline is populated with detailed, coherent, and contextually relevant information. This process involves the following key steps: hierarchical information retrieval, parallel content generation, and logical consistency enforcement.\nHierarchical Information Retrieval. Given an outline $O$ with sections $\\{S_{i1}, S_{i2},..., S_{in}\\}$, we perform targeted retrieval for each section $S_{ij}$. Hence, $S_{ij}$ can be regarded as the sub-query derived from the main query $q_i$ that focuses on the specific topic of $S_{ij}$. The information retrieval process involves fetching relevant documents $D_{S_{ij}}$ from a knowledge base or external sources. Note that the retrieval method here can be replaced by any mainstream RAG method. In this work, a basic search engine retrieval method is employed.\nParallel Content Generation. Upon retrieving the relevant documents for each section $S_{ij}$, the next phase is to generate detailed content for each section in parallel. Our objective is to ensure comprehensiveness and coherence while optimizing for cost-effectiveness and time efficiency.\nLet $G_{ij}$ represent the generated content for section $S_{ij}$. LLM takes the outline $O$, the specific sub-query $S_{ij}$, and the retrieved documents $D_{S_{ij}}$ as inputs to produce the section content:\n$G_{ij} = LLM(O, q_i, S_{ij}, D_{S_{ij}})$.\nThis method leverages the structured outline to maintain the overall coherence and logical flow of the final response. Under the guidance of a unified outline, we generate the content for each section in parallel. It ensures that each section is coherent and logically consistent, despite being generated simultaneously. The generated sections are then merged to form a complete, comprehensive answer."}, {"title": "3.2.3 Customized Answer Generation", "content": "SynthRAG enhances systematically generated outputs by utilizing a set of complete, high-quality historical answers of the same type. These exemplary responses are provided to the LLM as references, allowing it to gain a more holistic understanding of the query. During the refinement process, the LLM uses these high-quality answers to guide content condensation, focusing on key insights while eliminating redundancy. In addition, it adapts the language, tone and structure to align with the style and expectations set by these examples, ensuring that the final output is informative and adheres to the required formatting and readability standards. By drawing on these references, the LLM significantly improves the systematically generated content, delivering a customized answer that is well-structured, relevant, and coherent, and effectively addresses the specific needs of the query."}, {"title": "4\nEXPERIMENT", "content": "In this section, we conduct extensive experiments to validate the effectiveness of our framework."}, {"title": "4.1 Experimental Setup", "content": "4.1.1 Question Selection. SynthRAG is designed to provide comprehensive answers in explanatory answer generation scenarios. To avoid issues such as data leakage, we randomly selected questions from the daily hot list on Zhihu\u00b3 from April 2024 to May 2024. These questions cover the latest social dynamics, news events, and public opinion information, making the categories complex and diverse. Most of these questions require a good reserve of relevant knowledge and experience to answer effectively. A total of 50 questions were selected for subsequent experimental evaluation.\n4.1.2 Implementation Details. We use Qwen-Max 4 as our base LLM. We set the temperature at 1.0 and the top_p at 0.9 for all experiments. Text is encoded using GTE [19], and setting the number of clusters k to 100. For reported results, the SynthRAG is grounded on the Quark search API with top 5 chunks, although the proposed methods is compatible with all other RAG methods.\n4.1.3 Compared Methods. As prior works focus on shorter answers or use different setups, they are difficult to compare directly. Instead, we use the following baselines in our analysis: Direct Generation, where the LLM is prompted to directly produce answers; RAG, which searches by topic and integrates the retrieved results with the topic to generate responses; and OutlineRAG, which initially creates an outline, then searches for additional information using section titles to formulate the answer."}, {"title": "4.2 Evaluation Metrics", "content": "To comprehensively evaluate the effectiveness of our answers, we designed four types of evaluation metrics:\n4.2.1 LLM-Based Evaluation. Following the recent trend of using LLMs to judge the quality of output text (especially in reference-free evaluation settings), we employed GPT-4 to assess the quality of generated answers. The evaluation is based on five different criteria. We then ask the evaluation model to rate each answer on a 5-point Likert scale or to make pairwise comparisons between the ideas generated by different models. Detailed human-crafted criteria used to guide the evaluation are provided in Table 3.\n4.2.2 Reward Model-Based Ordinal Evaluation. To effectively evaluate the relative merits of two answers, we categorized responses into high and low upvote pairs based on publicly available QA pairs from Zhihu 5, and trained a reward model to discern human preferences between the two. This model ranks answers by predicting which one is more likely to be favored by humans. Specifically, we constructed a large set of answer preference samples (Q, A1, A2), where Q represents a question, and A1 and A2 are two answers to that question, adhering to the following conditions: i) Both A1 and A2 are responses to the same question Q. ii) The number of user upvotes for A1 exceeds that of A2. iii) The posting time of A1 is not earlier than that of A2. By doing so, we mitigated biases arising from differences in questions and answer exposure times. We extracted a total of 100k answer preference pairs for training our reward model, with an additional 5k pairs reserved for evaluating its performance. We fine-tuned the Qwen1.5-1.8B [29] model using Llama-Factory [33] on an NVIDIA A100 80GB GPU. The ordinal accuracy of the reward model on the test set reached 84.3%. This model helps to rank the answers by predicting which one is more likely to be preferred by humans.\n4.2.3 Information Content Evaluation. To better assess the informational richness of model outputs, we propose an evaluation framework based on factual question answering. This approach aggregates answers from multiple models to create a comprehensive context, which an LLM then uses to generate specific questions for information assessment. In the evaluation phase, each individual answer is treated as a standalone context, prompting the LLM to respond to these pre-generated questions using only the given response. The precision of these answers acts as a proxy for the information density of each model's output."}, {"title": "4.3 Main Results", "content": "LLM-Based Evaluation. The LLM-based evaluation measures the performance of different models across several dimensions, including overall quality, fluency, relevance, logic, reference value, and depth. The scores for each dimension range from 0 to 5, with higher scores indicating better performance. The results for each model are summarized in Table 1. SynthRAG scored the highest overall at 4.52, surpassing baseline models ChatGPT and GPT-4, which registered scores of 3.76 and 3.82, respectively. It also achieved comparable fluency scores, at 3.96, similar to Qwen-Max and RAG at 4.00 and 3.94. Moreover, SynthRAG's relevance score of 4.74 exceeded those of ChatGPT (4.34) and GPT-4 (4.40), indicating its superior alignment with query contexts. Its logic score of 4.30, alongside reference value and depth scores of 4.40 and 4.54, respectively, further demonstrate its ability to generate valuable and insightful content, outstripping other models. Overall, the LLM-based evaluation demonstrates that SynthRAG significantly outperforms existing models in terms of overall quality, relevance, logic, reference value, and depth, while maintaining competitive fluency.\nInformation Content Evaluation. We used generated QA pairs (see details in section 4.2.3) to evaluate the information content of each model's answers. As presented in Table 1, RAG scored 66.43, slightly outperforming Qwen-Max. This score reflects RAG's capacity to incorporate retrieved information into its answers. However, it remains significantly lower than OutlineRAG, indicating that a single retrieval and generation cycle may struggle to produce comprehensive responses. SynthRAG achieved the highest information score of 85.76, markedly surpassing other models. Comparisons with OutlineRAG reveal that providing a specific outline instruction not only enhances the comprehensiveness but also the target-specificity of the generated content, aiding in the creation of more informative responses. This result highlights SynthRAG's effectiveness in generating answers with extensive and rich content, making it the leading model in terms of information comprehensiveness."}, {"title": "4.4 Ablation Studies", "content": "To evaluate the efficacy of the individual components of the model, we conducted ablation studies.\nInitially, we removed the instruction guidance component (OutineRAG), the LLM-base score and the information score were both negatively affected. The LLM-base score decreased to 4.34, and the information score dropped to 83.01. Although this version of SynthRAG still performed well, the reduction in scores underscores the importance of instruction guidance in enhancing the overall quality and richness of generated information.\nSimilarly, omitting the systematic information generation component (w/o generation) resulted in a significant decline in both evaluation metrics. In this ablation setup, the systematic information generation module was excluded, and the LLM generated answers directly under the guidance of an outline, relying solely on retrieval. The LLM-base score fell to 3.86, and the information score plummeted to 74.44. Furthermore, this outcome's LLM score was even lower than the original Qwen-Max performance. This could be attributed to the fact that, while an outline was provided, it was unable to generate sufficiently comprehensive responses in a single retrieval and generation cycle. As a result, the answers produced under the guidance of the outline were overly generalized and vague, leading to a significant decrease in quality. This sharp decline underscores the critical role of the systematic information generation component in ensuring detailed and comprehensive answers.\nFinally, we removed the customized answer generation component, which refines the output based on high-quality historical answers. The LLM-base score showed a slight decline from the full SynthRAG model, dropping to 4.42, and the information score fell to 85.64. Although the scores remained relatively high, this decrease suggests that the customized answer generation plays a important role in improving the final answer quality by refining the structure, tone, and relevance of the output."}, {"title": "4.5 Reward Model-Based Ordinal Evaluation", "content": "To corroborate the effectiveness of our proposed model, we utilized authentic Zhihu preference data to train a reward model. Details of the training procedure are delineated in section 4.2.2. The trained reward model exhibited proficiency in distinguishing answers that were more appealing to real users. We performed pairwise assessments contrasting responses from our model against those from baseline and ablation models for the same question. The outcomes are shown in Fig. 3. Our findings indicated that SynthRAG consistently outperformed baseline and ablation models in pairwise comparisons, aligning more closely with user preferences. This advantage stems from SynthRAG's ability to leverage insights from high-quality responses, enabling it to generate answers that resonate with users."}, {"title": "4.6 Analysis of Computational Cost", "content": "While SynthRAG introduces additional components, such as adaptive outline generation and systematic information retrieval, the overall computational cost remains manageable due to its parallel content generation mechanism.\nWe evaluated SynthRAG's computational cost compared to baseline models and found that, while it involves extra steps, the system is optimized to mitigate overhead. Specifically, SynthRAG requires an average of 15.6 retrievals and 16.6 LLM api calls for generation (most of which occur in parallel). On average, it takes about 2 to 3 minutes to generate an explanatory answer. Although this generation time makes SynthRAG less suitable for real-time, on-demand question answering, it is acceptable for more complex, explainability-focused scenarios. In many cases, answers can also be generated asynchronously, making the framework practical for high-depth QA tasks."}, {"title": "5 ONLINE APPLICATION AND EVALUATION", "content": "To validate user preferences regarding our model's performance under real-world applications, we established a new account on the Zhihu platform. This account was dedicated to posting responses generated by our model. An automated system was implemented to select trending questions on Zhihu daily, generate content autonomously, and post these responses without human intervention. We periodically collected upvote metrics from real users to assess engagement.\nDuring the two-month period from April to May 2024, we published 96 responses. These garnered an average of 5.73 upvotes per answer, with one response accumulating over 250 upvotes. The average upvote count exceeded 79.8% of human Zhihu content creators 6, and our highest-rated answer ranked in the top 2% of all upvoted responses on Zhihu. Fig. 4 shows like distributions for human answers and SynthRAG generated answers on Zhihu. Further analysis revealed that only 25% of SynthRAG's answers received no upvotes, compared to 57.74% for human users. Moreover, 69.79% of SynthRAG's answers received 1-10 upvotes, significantly higher than the 34.47% for human answers. These statistics indicate that SynthRAG's responses generally met user needs and gained recognition. However, we observed that human users had a slightly higher proportion of highly upvoted answers, with 1.25% higher in the 10-100 upvote range and 0.16% higher in the 100+ upvote range. This suggests that human expertise in specific domains, often based on unique experiences or insider knowledge, remains valuable for generating top-tier content.\nAs shown in Fig. 5, the account gained 81 followers, placing it in the top 3.5% of Zhihu users by follower count. This metric also reflects the user preference for our generated content. Notably, all above result was achieved without any initial follower base or external promotion, demonstrating the effectiveness and practicality of SynthRAG. Examples of answers generated by SynthRAG can be found in Appendix ??. These results demonstrate the practical efficacy of our model and its potential for real-world application and user engagement."}, {"title": "6 DEPLOYMENT-CONTINUOUS HUMAN FEEDBACK", "content": "Over time, the distribution of question types preferred by online users tends to diverge from offline data. Consequently, continual learning from user feedback is crucial for maintaining the advanced status of QA models. SynthRAG demonstrates strong continuous optimization capabilities through the integration of online user feedback. The framework's adaptive learning mechanism allows it to dynamically incorporate new information and feedback. This capability is particularly important in handling evolving queries, where the context and specifics can change over time. Fig. 6 illustrates of the iterative feedback process. By collecting both positive and negative real-world interaction samples, SynthRAG iteratively refines and expands its guidance library, thereby improving the quality and relevance of its responses.\nTable 2 displays the performance comparison between the initial model, which was learned from offline data, and the model after one iteration of online feedback. The original model achieved an LLM score of 4.52 and a reward model win rate of 41.6%. In contrast, the SynthRAG-refinement model demonstrates enhanced performance, achieving an LLM score of 4.60 and a significantly increased RM win rate of 58.40%. Continuous human feedback is vital to this iterative improvement process. User interactions, evidenced by metrics such as upvotes, comments, and shares, provide insights into the effectiveness of SynthRAG's responses. SynthRAG utilizes this data to regularly update its guidelines, ensuring that its responses align with evolving user expectations and preferences. This continuous refinement process not only improves response quality but also fosters greater user trust and engagement, establishing SynthRAG as an effective and user-centric AI-driven QA system."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "The novel SynthRAG framework significantly enhances the performance of LLMs in explanatory answer generation tasks by integrating diverse information sources coherently and comprehensively. By leveraging adaptive outline generation, systematic information generation, and customized answer generation, SynthRAG improves the quality, depth, and coherence of responses. Extensive evaluations, including LLM-based, reward model-based, and online human assessments, demonstrate its efficacy and practical applicability, as evidenced by positive user feedback and high engagement on platforms like Zhihu. SynthRAG marks a significant step towards more insightful and user-centric AI-driven QA systems. In the future, we will focus on enhancing the model to user-specific adaptability. This involves tailoring responses to individual users, thereby achieving a higher degree of personalization answers."}]}