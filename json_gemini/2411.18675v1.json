{"title": "GaussianSpeech: Audio-Driven Gaussian Avatars", "authors": ["Shivangi Aneja", "Artem Sevastopolsky", "Tobias Kirschstein", "Justus Thies", "Angela Dai", "Matthias Nie\u00dfner"], "abstract": "We introduce GaussianSpeech\u00b9, a novel approach that synthesizes high-fidelity animation sequences of photorealistic, personalized 3D human head avatars from spoken audio. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple speech signal with 3D Gaussian splatting to create realistic, temporally coherent motion sequences. We propose a compact and efficient 3DGS-based avatar representation that generates expression-dependent color and leverages wrinkle- and perceptually-based losses to synthesize facial details, including wrinkles that occur with different expressions. To enable sequence modeling of 3D Gaussian splats with audio, we devise an audio-conditioned transformer model capable of extracting lip and expression features directly from audio input. Due to the absence of high-quality dataset of talking humans in correspondence with audio, we captured a new large-scale multi-view dataset of audio-visual sequences of talking humans with native English accents and diverse facial geometry. GaussianSpeech consistently achieves state-of-the-art quality with visually natural motion, while encompassing diverse facial expressions and styles.", "sections": [{"title": "1. Introduction", "content": "Generating animated sequences of photorealistic 3D head avatars from spoken audio is important for many graphics applications, including immersive telepresence, movies, and virtual assistants. In particular, rendering photorealistic views of such animated avatars from various viewpoints is crucial for realistic, immersive digital media, for instance, telepresence to a meeting room requires a photorealistic appearance for all viewpoints of the people in the room, or AR/VR where users can freely change their viewpoint.\nCreating such photorealistic animated 3D avatars from audio remains challenging, as it requires maintaining photorealistic fidelity throughout the animation sequence, as well as from various viewpoints. Existing work thus focuses on addressing these objectives independently; various works focus on re-enacting videos in the 2D domain , creating front-view video animations, while others focus on animating 3D face geometry from audio . In contrast, we aim to create innately 3D audio-driven avatars enabling 3D-consistent, free-viewpoint photorealistic synthesis needed for immersive digital communication.\nIn order to characterize audio-driven 3D animation of a person from multi-view input, we propose to represent animated head sequences with explicit 3D Gaussian points, leveraging the detailed and expressive representation space"}, {"title": "2. Related Work", "content": "Audio-driven facial animation plays an important role in digital media. Here we discuss audio-driven animation methods generating different output representations."}, {"title": "2.1. 2D-Based Methods.", "content": "There is a large corpus of works in the field of 2D audio-driven facial animation operating on monocular RGB videos, synthesizing 2D sequences directly . However, these methods operate in pixel space and can produce very limited side views. Another line of work also operating on frontal RGB videos but using intermediate 3D representations are based on 3DMMs . Although these methods generate photorealistic results, they use 3DMMs as a proxy to improve the animation quality and are still limited to frontal and limited side views. In contrast, we model head avatars with explicit 3D Gaussian points, thus, enabling simultaneous free-viewpoint rendering for different viewpoints which is critical for telepresence applications."}, {"title": "2.2. Parametric Model Based Methods.", "content": "Another promising line of work is to animate 3D facial geometry directly. A vast majority of these works model speech-conditioned animation for either artist-designed template meshes  or blend-shapes for 3D parametric head model . While these methods can faithfully match facial motion with the speech signal and can be rendered from different viewpoints, they do not model any appearance or texture information and cannot handle complex and irregular facial geometry. The synthesized animations, therefore, do not look realistic. Compared to these, our method optimizes a 3DGS-based avatar and models appearance using expression and view-dependent color, generating photorealistic results."}, {"title": "2.3. Radiance Fields Based Methods.", "content": "Recent speech-driven animation methods based on radiance fields  have gained popularity due to their ability to model directly from images. Neural Radiance Fields (NeRF)  possess the capability to render a scene from arbitrary viewpoints, however, existing audio-driven methods utilizing NeRF are designed for monocular videos.\nConcurrent to ours, few recent works  leverage 3DGS  for generating audio-driven talking heads. GaussianTalker  and TalkingGaussian  focus on improving the rendering speed for monocular videos. EmoTalk3D  can synthesize multi-view renders, however these methods generate sequences frame-by-frame, thus suffer from jitter and scaling artefacts. In contrast, our method synthesizes multi-view consistent and temporally smooth results, including fine-scale details like dynamic wrinkles, by leveraging a transformer-based sequence model and an efficient 3DGS-based avatar."}, {"title": "3. Multi-View Audio-Visual Dataset", "content": "We collected a novel dataset consisting of six native English speakers captured using a multiview rig of 16 cameras (see Supp.). We record sequences at 30 FPS at 3208 x 2200 resolution. To achieve quality and diversity, we specifically capture native English speakers with different accents, including American, British, and Canadian. We selected participants aged 20-50 with different genders and facial geometry including beard and glasses to increase the diversity, see Fig. 2. We collected 415 sequences for every subject, leading to an overall recording time of 30-35 minutes for each of the 16 cameras. The spoken sentences are chosen from the TIMIT  corpus to maximize the phonetic diversity. Our dataset stands out from the existing datasets in terms of quality and quantity.\nWhile certain datasets with audio-visual talking faces exist, they are limited in quality. The RAVDESS dataset  contains a set of native speakers, but it has only 2 unique sequences per participant with North American accent, while we captured three different English accents and 415 unique sentences. The MEAD dataset  captured the participants with 250 unique sentences per participant. However, they focus on emotional speech synthesis due to which they capture only 40 unique natural expression/emotion per participant at a relatively lower resolution. The Nersemble  dataset captures the participants at high resolution, but it only contains 10 audio sequences per participant. Closest to ours is MultiFace , which captured participants in a spherical rig of 150 cameras; however, it captured only 50 audio sequences per participant. Our dataset contains 415 sequences for every subject at high resolution, an order of magnitude larger than existing datasets, see Tab. 1. We plan to release our entire dataset to the research community."}, {"title": "4. Method", "content": "Our method operates in two stages. First, we develop a lightweight and high-quality avatar initialization based on GaussianAvatars (Sec. 4.1). Next, we train a transformer-"}, {"title": "4.1. Avatar Initialization", "content": "We propose an efficient optimization strategy to compute a 3DGS-based Gaussian avatar representation. We found that naively training GaussianAvatar  generates blurred/low-quality textures, especially, for scenarios with rapid facial movement like faster talking speed/head motion. In addition, GaussianAvatar can not effectively handle dynamic wrinkles. Therefore, we introduce expression-dependent colors and propose several regularizations to improve quality of our avatars described below and shown in Fig. 3.\nVolume-Based Pruning. We modify the pruning strategy used by GaussianAvatar. Instead of pruning 3D Gaussian splats based on a given opacity threshold Eopacity, we select top 25,000 Gaussians with maximum opacity and 3D Gaussian's scale volume combined at every pruning step as\n$G_i = \\sigma_i\\cdot (S_x.S_y.S_z),                                                                                                                                                              (1)$\nwhere \u03c3 refers to ith Gaussian's opacity and $S_x,S_y,S_z$ refers to its scale along x, y, and z axis. Even when the optimization generates excessive splats during densification,"}, {"title": "4.2. Sequence Model Training", "content": "GaussianSpeech performs high-fidelity and temporally-consistent generative synthesis of avatar motion sequences, conditioned on audio signal. To characterize complex face motions and fine-scale movements like dynamic wrinkles, we employ a transformer-based sequence model. We predict mesh animations with our sequence model and refine the dynamic motion attributes of the 3D Gaussian Splats of our optimized avatar to be consistent with audio features. An overview of our approach is illustrated in Fig. 4.\nAudio Encoding. We employ the state-of-the-art pre-trained speech model Wav2Vec 2.0  to encode the audio signal. Specifically, we use the audio feature extractor made up of temporal convolution layers (TCN) to extract audio feature vectors {$a_i$}$_{i=1}^{N_a}$ from the raw waveform, followed by a Frequency Interpolation layer to align the input audio signal {$a_i$}$_{i=1}^{N_e}$ (captured at frequency $f_a = 16kHz$) with our dataset {$a_i$}$_{i=1}^{N_e}$ (captures at framerate $f_e = 30FPS$).\nLip Features. A stacked multi-layer Lip Transformer Encoder processes these resampled audio features and predicts personalized lip content feature vectors $c^{1:T}$. To avoid"}, {"title": "5. Results", "content": "We evaluate GaussianSpeech on the tasks of (a) Avatar Representation and (b) Audio-Driven Animation. For (a), we evaluate standard perceptual image quality metrics SSIM, PSNR and LPIPS. For audio-driven animation, we evaluate lip synchronization LSE-D  as well as perceptual quality metrics. We train personalized avatars for different identities. Following GaussianAvatars , we train on all 15 cameras except the frontal and report results on the frontal camera for all our experiments. All images are resized to 1604 \u00d7 1100 during training. For avatar reconstruction, we use 30 short sequences. For audio-driven animation, we use 300 sequences for training and 50 for val and test set each. We encourage readers to watch the Supplementary Video for visual comparison of all results."}, {"title": "5.1. Avatar Reconstruction", "content": "Compared to GaussianAvatars , our proposed avatar initialization can generate high-quality results with as few as 30-35k points (see Fig. 5 and Tab. 2). The perceptual loss helps increase the sharpness in the texture with fewer points. The wrinkle regularization helps to model dynamic wrinkles. Teeth subdivision helps with the better mouth interior. Color MLP helps synthesize sharper texture. Our full avatar initialization with all regularization achieves the best results. We train our method on all except frontal camera and report results for the frontal camera. For these experiments, we show results for the most expressive actor from our dataset (Subject 4) and refer to Suppl. doc for others."}, {"title": "5.2. Audio-Driven Animation", "content": "Baseline Comparisons. We compare our method against recent state-of-the-art methods. For NeRF- and 3DGS-based methods, we train on frontal camera since these methods are designed for monocular settings. There are no sequence models for audio-driven animation of 3D head avatars, thus, we combine audio-to-mesh animation methods  with current state-of-the-art mesh-to-3D avatar creation method . We report results on the front"}, {"title": "6. Conclusion", "content": "In this work, we propose a novel approach to create high-fidelity and photorealistic 3D head avatars that can be animated from audio input. We designed the first transformer-based sequence model for audio-driven head animation of 3DGS based avatar. Our sequence model is made possible by a lightweight and compact avatar initialization based on 3D Gaussian Splatting. We proposed several regularization techniques to handle dynamic wrinkles, skin creasing and sharpness of the texture. Our method produces (a) photorealistic and high-quality 3D head avatars that can be rendered from arbitrary viewpoints (b) visually natural animations like skin creasing during talking. We believe this is an important first step towards enabling the animation of detailed and lightweight 3D head avatar, which can enable many new possibilities for content creation and digital avatars for immersive telepresence."}, {"title": "A. Additional Experiments", "content": "A.1. Novel View Synthesis\nIn our experiments, we found that training with at least 30 sequences enables generalizing for mouth articulations. We show novel view synthesis results and zoom-ins in Fig. 8. For all the avatars from our dataset, we show results for avatar initialization in Fig. 9 and Tab. 5. Audio-driven animations are shown in Fig. 10."}, {"title": "A.2. Effect of Latent Features", "content": "We analyze the effect of per Gaussian latent features during our avatar initialization stage in Fig. 11. Note that the per Gaussian features are critical to produce accurate texture colors for the avatar."}, {"title": "A.3. Failure Cases", "content": "While our method produces photorealistic and high-quality animations in synchronization with audio, it also has several limitations. Our avatar initialization strategy is based on FLAME [34]; thus, our method struggles with avatars wearing accessories like glasses. The glass geometry and specularities on the surface of glasses can not be accurately produced and fails during free-viewpoint rendering, see Fig. 12. In the future, this can be improved by designing better models for representing human head geometry instead of 3D mesh. Also, our texture representation based on the Color MLP has baked-in lighting, and cannot be separated from material properties, which is important for placing avatars in different environments (e.g., during immersive telepresence)."}, {"title": "A.4. User Study", "content": "To evaluate the fidelity based on human perceptual evaluation, we performed a user study with 30 participants over a set of 15 questions. The users were given a carefully crafted set of instructions to evaluate (a) Overall Animation Quality (b) Lip Synchronization and (c) Realism in Facial Movements. The users were asked to assess different anonymous methods (including GaussianSpeech) on these three parameters. In the course of the study, participants were presented with these questions to focus on different aspects of 3D facial animation, shown in Fig 13. For every question, participants were instructed to meticulously evaluate the provided methods and select the option that best aligned with their judgment."}, {"title": "A.5. Inference Speed", "content": "We report inference speed averaged over the test set on a single Nvidia RTX 2080 Ti with 12GB VRAM as well as NVIDIA RTX A6000 with 48GB VRAM. Since Talking-Gaussian  network does not fit in 12 GB VRAM; we report its inference time only for a 48 GB VRAM (NVIDIA A6000). The results are presented in Tab. 6."}, {"title": "B. Architecture & Training Details", "content": "GaussianSpeech is implemented using PyTorch Lightning framework  with Wandb  for logging."}, {"title": "B.1. Implementation Details", "content": "Avatar Representation. During avatar initialization, the teeth subdivision changes the initial mesh with 5143 vertices and 10144 faces to 5431 vertices and 10648 faces. For perceptual loss Lglobal, we downscale images to 401 \u00d7 275. The volume based pruning ensures that the avatar converges to 30-35k Gaussian points. We use Adam optimizer with exponential decay and the default learning rates from [42]. For the loss (Eq. 8 main paper), we use Apos = 0.01, \u03bb\u03c2 = 1, \u03bb\u03b5 = 1, Xp = 0.001 and Aw 10. For rendering, we use the differentiable tile rasterizer [30].\nAudio-Driven Sequence Generation. For the first 10K iterations, we train the Lip transformer, Wrinkle transformer and Expression encoder each, and then use them in"}, {"title": "B.2. Architecture Details", "content": "Avatar Initialization. During avatar initialization (Sec. 5.1, main paper), we randomly sample 16 random patches per iteration with a patch size of 128 \u00d7 128 from the facial area. We start with an initial learning rate of 5e-3 and exponentially decay until 5e-5. We perform densification every 5000 iterations and we do not reset opacity. We train with the batch size of 1 with Adam optimizer, render images on white background and train for 100,000 iterations on RTX 2080 Ti (12 GB VRAM). Our avatars converge between 29-35K Gaussian points, as shown in Tab. 5. We show sample output of our wrinkle detection model used in main paper in Fig. 16.\nAudio-to-Avatar Model. For the audio encoder, the TCN layers of the Wav2Vec 2.0  are initialized with the pre-trained wav2vec 2.0 weights trained on a large corpus of audio data from different languages and is frozen during fine-tuning. The Frequency Interpolation layer simply performs linear interpolation of the incoming features and has no learnable parameters. For the Lip an Wrinkle encoder, we use latent dimension of 64 and for Expression encoder and the Transformer decoder the latent dimension is 128. For the multihead self and cross attention layers of the transformer decoder, we use 4 heads and set the dimension to 1024 for each decoder block. We use an Adam optimizer with a learning rate of 1e-4 and update the model one sequence per iteration, train on Nvidia RTX A6000 (48 GB VRAM) for 100,000 iterations.\nEvaluation Metrics. To evaluate lip synchronization of the generated mouth expressions with the audio signal, we use LSE-D (Lip Sync Error Distance) . Specifically, this involves feeding rendered face crops and the corresponding"}, {"title": "C. Preliminaries", "content": "C.1. 3D Gaussian Splatting\nRecently, 3D Gaussian Splatting  has emerged as a promising approach to represent a static scene explicitly with anisotropic 3D gaussian directly from multiview images and estimated/given camera poses. Specifically, it represents a scene using a set of 3D Gaussian splats, each defined by a set of optimizable parameters, including a mean position \u03bc\u2208 R\u00b3 and a positive semi-definite covariance matrix \u03a3\u2208 R3\u00d73 as:"}, {"title": null, "content": "$G(x) = e^{-(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}.(22)$\nGiven that covariance matrix \u2211 needs to be positive semidefinite to have physical meaning and gradient-based optimization methods cannot be constrained to produce such valid matrices, Kerbl et al.  first define an ellipsoid with scaling matrix S and rotation matrix R as:\n$\\Sigma = RSST^T R^T.(23)$\nTo allow for independent optimization for scale and rotation, separate 3D vectors are stored. The scale is represented using a scaling vector s \u2208 R\u00b3 and a quaternion q \u2208 R4 for rotation.\nFor rendering every pixel on the image, the color C is computed by blending all the 3D Gaussians overlapping a pixels as:"}, {"title": null, "content": "$C = \\sum_{i=1}^N c_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j) ,                                 (24)$\nwhere $c_i$ refers to 3-degree spherical harmonics (SH) [43] color obtained by blending N ordered points overlapping the pixel, blending weight $a_i$ is given by multiplying 2D projection of the 3D Gaussian with learnt per-point opacity. Paired with a differentiable tile rasterizer, this enables real-time rendering. To handle complex scenes and respect visibility order, depth-based sorting is applied to the Gaussian splats before blending."}, {"title": "C.2. GaussianAvatars", "content": "Due to the capability of 3DGS to represent fine geometric structures, it has proved to be an efficient representation for creating photorealistic head avatars, as shown by GaussianAvatars [42]. GaussianAvatars proposes a method for"}, {"title": null, "content": "dynamic 3D representation of human heads based on 3DGS by rigging the anisotropic 3D Gaussians to the faces of a 3D morphable face model. Specifically, the method uses FLAME [34] as 3DMM due to its flexibility and compactness, consisting of only 5023 vertices and 9976 faces. To better represent mouth interior, it generated additional 120 vertices for teeth. Given a FLAME mesh, the idea is to first initialize a 3D Gaussian at the center of each triangle of the FLAME mesh and let the 3D Gaussian move with the faces of the FLAME mesh across different timesteps. For the paired 3D Gaussians with the faces of the FLAME mesh, the position \u03bc, rotation r and anisotropic scaling s are defined in local space. During rendering, these are converted to global space as:\nr' = Rr, (25)\n\u03bc' = kR\u03bc + T (26)\ns' = ks, (27)\nwhere T refers to the mean positions of the vertices of the triangle mesh, rotation matrix R describes the orientation of the triangles in the global space, scalar k describes the triangle scaling. During avatar optimization, similar to 3DGS, the method uses adaptive density control strategy to add and remove splats based view-space positional gradient and opacity of each Gaussian. To prevent excessive pruning, the method also ensures that every triangle has at least one splat attached. The 3DGS parameters are then optimized using photometric loss $L_{rgb}$, position loss $L_{position}$ and scaling loss as $L_{scale}$ as:"}, {"title": null, "content": "$L = L_{rgb} + \\lambda_{pos}L_{position} + \\lambda_{s}L_{scaling}                                                                                                                                                                                                                                                                                                                                                                                              (28)$\nwhere $L_{rgb}$ is combination of $L_1$ and D-SSIM loss  between rendered and ground truth images."}, {"title": null, "content": "$L_{rgb} = (1 - \\lambda_{1})L_1 + \\lambda_{1}L_{D-SSIM},                                                                                                                                                                           (29)$\n$L_{position}$ ensures that splats remain close to their parent triangles:"}, {"title": null, "content": "$L_{position} = ||max(\\mu, E_{position})||_2,                                                                                                                                                                                    (30)$\nand $L_{scaling}$ prevents excessive scaling of the splats:\n$L_{scaling} = ||max(s, E_{scaling})||_2.                                                                                                                                                                                    (31)$"}, {"title": "D. Baselines", "content": "We compare our method against audio-conditioned NeRF, 3DGS and mesh based methods. Since mesh-based methods can't generate photorealistic avatars, we combined audio-to-mesh methods with recent state-of-the-art mesh-to-3D avatar method GaussianAvatars . Current NeRF and 3DGS based methods are designed for monocular"}, {"title": "E. Dataset Details", "content": "Our multi-view dataset consists of native speakers in age group 20-50 and includes three male and female participants, see Tab. 7. We show additional metadata to be released with our dataset in Fig. 17 and some example frames from one of the sequences from our dataset in Fig. 18."}, {"title": "E.1. Hardware Configuration", "content": "We employ 16 machine vision cameras at a resolution of 7.1 megapixels and a supercardioid microphone to cap"}, {"title": "E.2. Speech Corpus", "content": "To maximize phonetic diversity in the dataset and to advance the field of audio-driven facial animation, we asked the participants to speak a phonetically diverse set of spoken English sentences carefully chosen from TIMIT speech corpus  with expressions. We record the following categories of sentences from TIMIT corpus (a) Two accent-specific sentences that differ for people with different dialects, (b) 260 phonetically compact sentences to include a diverse range of phonetic contexts (c) 143 phonetically balanced sentences to include a balanced representation of phonemes. These short sentences range from 3-7 seconds each. Finally, we also record 10 free-form long sentences, where we ask participants a fixed set of questions based on their hobby/profession, etc, to capture the free form speaking style of the participant. These sentences are 10-20 seconds long.\nFor long sequences, the participants were asked 10 basic questions as listed below."}]}