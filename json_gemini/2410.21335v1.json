{"title": "E(3)-invaraint diffusion model for pocket-aware peptide generation", "authors": ["Po-Yu Liang", "Jun Bai"], "abstract": "Biologists frequently desire protein inhibitors for a variety of reasons, including use as research tools for understanding biological processes and application to societal problems in agriculture, healthcare, etc. Immunotherapy, for instance, relies on immune checkpoint inhibitors to block checkpoint proteins, preventing their binding with partner proteins and boosting immune cell function against abnormal cells. Inhibitor discovery has long been a tedious process, which in recent years has been accelerated by computational approaches. Advances in artificial intelligence now provide an opportunity to make inhibitor discovery smarter than ever before. While extensive research has been conducted on computer-aided inhibitor discovery, it has mainly focused on either sequence-to-structure mapping, reverse mapping, or bio-activity prediction, making it unrealistic for biologists to utilize such tools. Instead, our work proposes a new method of computer-assisted inhibitor discovery: de novo pocket-aware peptide structure and sequence generation network. Our approach consists of two sequential diffusion models for end-to-end structure generation and sequence prediction. By leveraging angle and dihedral relationships between backbone atoms, we ensure an E(3)-invariant representation of peptide structures. Our results demonstrate that our method achieves comparable performance to state-of-the-art models, highlighting its potential in pocket-aware peptide design. This work offers a new approach for precise drug discovery using receptor-specific peptide generation. The code used for this research is available at Github.", "sections": [{"title": "1 Introduction", "content": "Biologists frequently search for peptides to support a range of applications, from understanding biological mechanisms to addressing healthcare challenges. For instance, immunotherapy uses immune checkpoint inhibitors to block checkpoint proteins from binding, thereby increasing the activity of immune cells against cancer cells. Traditionally, discovering peptides that block checkpoint proteins has been a labor-intensive and time-consuming process. However, recent advances in computational methods have significantly accelerated this process [32,5]. These computational methods\u2014such as molecular docking, virtual screening, and machine learning/deep learning-have advanced peptide discovery by offering tools for various tasks, including predicting peptide-protein interactions, optimizing peptide sequences, and identifying potential peptide candidates [7,18,6]. Among these methods, deep learning is considered one of the most advanced computational approaches. By utilizing neural network architectures with large datasets, these models can identify peptides with desired bioactive properties for various applications. Deep learning methods, including various sequence- or structure-generative models [20,19], have demonstrated success in optimizing peptides and generating peptide structures or sequences with desired properties. This progress has facilitated further research into generating peptides with specific characteristics.\nWhile extensive research has been conducted on computer-aided peptide discovery [30], most efforts have focused on generating peptide sequences with general properties [11,10] or desired structures [9,8], which makes these tools less practical for biologists. Only few research [34] has been conducted on computer-aided peptide inhibitor discovery. The limitation of general peptide discovery without desired target information and properties can hinder the discovery of peptide inhibitor for particular receptor pocket. The RFdiffusion model [35] offers a potential approach by generating peptide structures around target protein pocket residues, followed by using inverse folding models like ESM-IF [13] or ProteinMPNN [3] to predict possible sequences for those structures. However, the limitations of the research are as follows. First, the inverse folding models"}, {"title": "2 Method", "content": "Our proposed method (shown in Figure 1) contains twin modules : 1) the conditional structure diffusion model and 2) conditional sequence diffusion model. The structure diffusion model generate desired peptide structures by given pocket information as condition. Once the peptide structure is obtained, the sequence diffusion model generates the corresponding amino acid sequence, using the pocket information as a condition as well. In this section, we explain the E(3)-invariant representation for proteins and peptides, as well as the design of our twin conditional diffusion models."}, {"title": "2.1 Peptide and Pocket Representation", "content": "Representation of Peptide In FoldingDiff [36] the backbone structure of an amino acid can be described as a series of bond angles, dihedral (torsion) angles, and bond distances between the backbone nitrogen, \u03b1-carbon, and carbon atoms. This provides an E(3)-invariant representation of protein structures, which means that the structure remains consistent regardless of rotations or translations in 3D space. Inspired by their work, we represent peptide structures using the bond angles and dihedral angles of the complete backbone atoms, with particular emphasis on the inclusion of the oxygen atom. This inclusion is crucial,"}, {"title": "2.2 Twin Conditional Diffusion Model", "content": "Structure Diffusion Model We designed a transformer [33]-based diffusion model for generating peptide structures conditioned on the receptor pocket. The diffusion model architecture contains two major processes, the Markov noising process and de-noising process. The Markov noising process corrupts the data by progressively adding noise, to it over a series of time steps, which could be represented as $q(X_{pep.str}^t|x_{pep.str}^{t-1})$, where t is the discrete time step in the noising process, indicating the level of noise applied to the data at that particular step. To perform the de-noising process, a model, denotes as $p(X_{pen.str}^{t-1}| X_{pep.str}^t, X_{poc})$, is trained to reverse a Markov noising process. To capture the wrapped nature of angle, we utilize the wrapped normal distribution for Markov noising process and the wrapped smooth $L_1$ loss, which can be represent by Equation 1, for model training, introduced in FoldingDiff [36].\n$d(\\cdot) = \\left( (\\epsilon_t - \\epsilon_p + \\pi) \\mod 2\\pi \\right) - \\pi$\n$L_{str} =\\begin{cases} 0.5d(\\cdot)/\\beta, & \\text{if } |d(\\cdot)| < \\beta \\\\ |d(\\cdot)| - 0.5\\beta, & \\text{otherwise} \\end{cases}$ (1)\nwhere the function $d(\\cdot)$ wrap the angle difference about $[-\\pi,\\pi)$, $\\epsilon_t$ represent the noise sampled from noising process, $\\epsilon_p$ represent the noise predicted by our model, and $\\beta$ is a hyper-parameter, which we set to 0.1\u03c0."}, {"title": "2.3 Conditional Layers", "content": "Residue Encoding(RE) Layer To encode the angle and residue type features of residues, we employ gated adaptive layer normalization [14,22], as illustrated in Figure 1c. In the structural diffusion model, we consider residue type as a conditioning variable for the structural features, whereas in the sequence diffusion model, the relationship is reversed. This approach allows us to maintain the essential characteristics of one type of feature while also integrating the effects of the other.\n$GatedLN(\u00b7) = Gate \\odot (Scale \\odot \\frac{h- \\mu(h)}{\\sigma(h)} + Shift)$ (5)\nwhere h is the hidden state to be normalized, Gate, Scale, and Shift are the layer normalization parameters derived from condition hidden state, and the $\\odot$ operation represent the Hadamard product.\nCross-Attention Layer After obtaining the embeddings for the pocket and peptide, represented as and respectively, we condition the peptide features using the pocket features through a cross-attention mechanism [33]. In this mechanism, the query Q is calculated as $W_q h_{pep}$, while the key K and value V are derived from the pocket features using $W_k h_{pocket}$ and $W_v h_{pocket}$. The output of this layer can be expressed as $h_{out} = Softmax(QK^T/\\sqrt{d_k})\\cdot V$ where $d_k$ denotes the dimension of K. The resulting output represents the conditioned features, effectively integrating information from both the pocket and peptide embeddings."}, {"title": "2.4 Data Source and Evaluation Metrics", "content": "Data Source and Preprocessing To evaluate the performance of our model, we used the open-source BioLip [39] protein-ligand dataset. The Biopython[2] package and the DSSP [16] tool were utilized for data preprocessing. The BioLip database contains 781,684 protein-ligand interactions, including 35,167 interactions specifically between proteins and peptides. To ensure data quality, we excluded complexes with a resolution lower than 5 \u00c5 [29] and those containing unknown amino acids. Additionally, we removed peptides shorter than 5 amino acids, complexes with duplicate PDB IDs, and those that could not be processed by Biopython or DSSP. After these filtering steps, we obtained a final set of 8,868 receptor-peptide complexes, which were divided into training, validation, and testing sets with an 80:10:10 ratio."}, {"title": "Model Testing Procedure", "content": "we tested a version trained on a dataset with four extended (ext-4) neighbor residues, as detailed in Section 2.1. This model, referred to as OurModel, was chosen because it includes most of the relevant pocket residue information. We also tested a combined model that integrates residues from 0 to 4 extended neighbors (denoted as OurModel-ext), which represents the best performance by using different ranges of neighboring residues. For overall binding performance, the ext-i structure model results were used as input for the corresponding ext-i sequence model, ensuring alignment of pocket residue information.\nAmong the models we compared against, only the diffusion-based models, RFdiffusion [35] and GraDe-IF [38], have the inherent ability to generate varied results from the same input. To ensure a fair comparison, the other models were tested using a one-shot generation approach. Additionally, performing multiple generations for all these models is impractical, as it would exponentially increase the number of structure-sequence pairs and significantly extend the required post-processing time."}, {"title": "Structure Evaluation Metrics", "content": "We evaluate the performance of our peptide structure prediction model against RFdiffusion [35] using Root Mean Square Deviation (RMSD) of backbone atoms and TM-Score, a length-normalized structure similarity metric. Given the limited research in this domain, we also benchmark our model against other folding models, including AlphaFold2[15], ESM Fold[21], and OmegaFold [37], which predict structures based on amino acid sequences, providing a contrast to non-pocket-aware models. To ensure a fair comparison, we assess our model's performance using a reconstructed peptide structure, where bond distances are fixed for each bond type, while other models are evaluated against the original peptide structures. For AlphaFold2, we utilized the ColabFold [24,23,26] implementation, which employs MMseqs2 server [25] to reduce the time required for structure prediction. Five pre-trained AlphaFold2 models, each trained with different random seeds, are available. We used all five and selected the structure with the highest confidence score (pLDDT). In the FoldingDiff research [36], they demonstrated the ability of their model to capture overall angle distributions and secondary structures. To evaluate whether these properties are maintained after incorporating the pocket information into the generation process, we also assess the divergence of angles between the testing set and the generated set to determine whether our model can generate structures with a similar angle distribution. Additionally, the Ramachandran plot is used to visualize our model's ability to capture secondary structure properties."}, {"title": "Sequence Evaluation Metrics", "content": "The performance of the sequence generation model is evaluated using the commonly used amino acid sequence recovery rate. Nonetheless, the recovery rate may underestimate performance when the model has a shifted prediction. For example, if the predicted sequence has one amino acid inserted at the start of a sequence, the recovery rate will drop to 0. Therefore, we propose a global alignment score-based sequence similarity evaluation, which can be described by Equation 6, where N.W. represents the Needleman-Wunsch algorithm [27] used to calculate the global alignment score. The numerator term evaluates the similarity of two sequences, while the denominator normalizes the score by the maximum possible alignment score, known as the self-alignment score. Following a similar concept, we also designed a diversity metric defined by Equation 7, where $X^{seq}_i$ represents the i-th sequence within a sequence set $X^{seq}$. The diversity metric is defined as one minus the average normalized pairwise alignment scores. If the sequence set has high diversity, meaning the sequences are different from each other, the pairwise alignment scores will be low, resulting in a high diversity score, and vice versa. One of the advantages of this method is that we can incorporate a substitution matrix to evaluate diversity, allowing for the inclusion of natural mutation information. The substitution matrix can also be defined to exclude any mutation information, providing the user with flexibility to adjust the metric to better fit specific problems. In our research, the most commonly used BLOSUM62 [12] substitution matrix is used to calculate sequence similarity and diversity.\n$SequenceSimilarity = \\frac{N.W.(x_{seq}^{pred}, x_{seq}^{true})}{N.W.(x_{seq}^{true}, x_{seq}^{true})}$ (6)\n$SequenceDiversity = 1 - \\frac{1}{\\binom{N}{2}} \\sum_{i,j} \\frac{N.W.(X_{i}^{seq}, X_{j}^{seq})}{N.W.(X_{\\tilde{i}}^{seq}, X_{\\tilde{i}}^{seq})}$ , where length of $X_{\\tilde{i}}^{seq}$ > length of $X^{seq}$ (7)"}, {"title": "Docking Evaluation Metrics", "content": "The overall performance of our model is evaluated by estimating the energy between the receptor and the generated peptide using PyRosetta [1,17]. The generated peptide is aligned with the position of the original peptide, followed by side-chain packing and a fast relaxation step [31,28]. We then use the Rosetta energy function to estimate the energy sums for the individual receptor and peptide structures, as well as the energy of the combined complex. The difference between these energy terms represents the overall docking energy of the complex. Additionally, we use the energy from the docking protocol [4] to provide an alternative perspective on model performance. Beyond the estimated energy terms, we also assess whether the peptide maintains contact with the binding pocket after the fast relaxation. A distance cutoff of 5 \u00c5 [29] is used to define contact, determined by whether peptide residues are in contact with any pocket residues. This definition aligns with a drug discovery perspective, where a peptide can influence signaling pathways as long as some pocket residues are blocked."}, {"title": "3 Result", "content": ""}, {"title": "3.1 Structure Prediction", "content": "The peptide structure prediction results presented in Table 2 indicate that the integrated result of our model (Our Model-ext) outperforms all other models across all metrics. In comparison, the non-integrated result of our model has a slightly lower RMSD (by 0.18 \u00c5) than the second-best result from ESM Fold. However, it achieves a higher TM Score than the other models in terms of average score, as well as the ratios of scores exceeding 0.2 and 0.5. This may be attributed to two factors: 1) the inclusion of extra pocket residue information significantly reduces the search space for peptide structures and 2) our model is specifically trained on peptide (short protein) structures, which may exhibit distribution differences compared to general protein datasets. Furthermore, while AlphaFold2 and ESM Fold perform better in RMSD, they have lower TM Scores (which are normalized by length), reinforcing this hypothesis."}, {"title": "3.2 Sequence Prediction", "content": "To evaluate the sequence prediction performance, we predicted sequences based on the original peptide structure and compared the results with three state-of-the-art inverse folding models: ESM-IF [13], ProteinMPNN [3], and GraDe-IF [38]. The results shows in Table 3, indicate that our model significantly outperforms these models, achieving a recovery rate of 47.41% with non-integrated prediction results, where recovery rate refers to the percentage of correctly predicted amino acids in the sequence. Additionally, the diversity of the generated sequences, as defined in section 2.4, is higher compared to the other three models. The diversity between non-integrated and integrated predictions is essentially the same, as this metric measures variation within a set from the same generation round, and the diversity within a single model remains relatively stable across different repetitions."}, {"title": "3.3 Binding Analysis", "content": "We evaluate binding effectiveness using three metrics: binding energy, docking score, and contact rate. Table 4 presents the results for all combinations of structure generation and sequence generation models. When comparing different structure generation models, the RFdiffusion model outperforms the others in terms of binding energy and contact rate across all sequence generation methods, despite lacking information about the actual structure of the peptide. In terms of dock score, the OmegaFold model performs slightly better (by less than 5%). Table 5 shows that the integrated result of our model are comparable to those of RFdiffusion and surpass other non-pocket-aware methods in terms of binding energy and contact rate. It is important to note that while the pocket-aware method achieves higher binding energy, its docking score is lower compared to non-pocket-aware methods. This discrepancy may be attributed to the evaluation method used for the docking score, which measures the energy of the entire peptide-receptor complex. If the energy of a peptide is higher in one structure compared to another, the overall energy of the complex will also be higher. This indicates that the energy difference may reflect the characteristics of peptides rather than the interaction energy between the peptide and the receptor.\nBeyond the straight forward binding performance, we also assessed the specificity of the generated peptide sequences by shuffling the sequences while maintaining the same length. Our hypothesis is that if a peptide has high specificity, its performance should decrease when tested against other pockets. We tested the widely used non-pocket-aware model, AlphaFold2, and the pocket-aware model, RFdiffusion. The rightmost two columns in Table 4 show the results with shuffled sequences, and surprisingly, the performance difference between the shuffled and unshuffled results is negligible, remaining nearly unchanged across all combinations and metrics.In contrast, when our model is tested with shuffled sequences, as shown in Table 5, there is a larger drop in performance, achieving only 40.59% of the original performance in binding energy. No decrease in docking score was observed, likely because the docking score measures the overall energy of the entire"}, {"title": "4 Discussion", "content": "In this research, we proposed a new method for generating pocket-aware peptides using diffusion models integrated with E(3)-invariant structure representation. The results show that our model can predict peptide structures with low RMSD, indicating high accuracy. Additionally, the TM-Score results further demonstrate that our model performs better in generating accurate peptide structures compared to both pocket-aware and non-pocket-aware models. Our approach also achieves a higher recovery rate due to the inclusion of extra pocket information. Moreover, the increased sequence diversity suggests that our model can design peptide sequences with greater specificity for the given receptor pocket residues, rather than merely generating sequences with high affinity for any receptor. This is further supported by the shuffled sequence binding test results, where our model exhibited a drop in binding energy, while the others remained largely unchanged. In conclusion, this research emphasizes the importance of pocket residue information in advancing peptide design, highlighting our model's potential for applications such as immune checkpoint inhibitors."}]}