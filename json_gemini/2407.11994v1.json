{"title": "Evaluating Contextually Personalized Programming Exercises Created with Generative AI", "authors": ["Evanfiya Logacheva", "Arto Hellas", "James Prather", "Sami Sarsa", "Juho Leinonen"], "abstract": "Programming skills are typically developed through completing various hands-on exercises. Such programming problems can be contextualized to students' interests and cultural backgrounds. Prior research in educational psychology has demonstrated that context personalization of exercises stimulates learners' situational interests and positively affects their engagement. However, creating a varied and comprehensive set of programming exercises for students to practice on is a time-consuming and laborious task for computer science educators. Previous studies have shown that large language models can generate conceptually and contextually relevant programming exercises. Thus, they offer a possibility to automatically produce personalized programming problems to fit students' interests and needs. This article reports on a user study conducted in an elective introductory programming course that included contextually personalized programming exercises created with GPT-4. The quality of the exercises was evaluated by both the students and the authors. Additionally, this work investigated student attitudes towards the created exercises and their engagement with the system. The results demonstrate that the quality of exercises generated with GPT-4 was generally high. What is more, the course participants found them engaging and useful. This suggests that AI-generated programming problems can be a worthwhile addition to introductory programming courses, as they provide students with a practically unlimited pool of practice material tailored to their personal interests and educational needs.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning how to program involves developing various sets of programming skills. Computing education theories present them as distinct and often sequential or hierarchical, i.e., meant to be introduced in a certain order [21, 52, 90]. Since developing expertise in any domain requires a certain amount of training [29], developing programming skills relies on practical hands-on exercises focused on particular learning objectives [2, 15, 50, 55, 68, 77, 82, 90].\nResearch indicates that many students in computer education experience high levels of stress, frustration, confusion, and boredom, which can lead to negative outcomes [11, 82]. Frustration is often developed as a result of encountering various difficulties related to both coding itself and course instruction; it precedes boredom and loss of interest in subject learning [11]. Certain solutions such as substituting long weekly assignments with multiple shorter problems have been reported to alleviate students' stress and improve their performance [2]. Similarly, repetitive exercises designed for teaching students syntax in CS1 have been found effective for enhancing students' engagement and their exam scores [15]. Such exercises have been viewed as helpful by students [50, 77]. Overall, learning how to program hands-on has been reported to reduce students' stress levels [82].\nProgramming exercises can be contextualized, i.e., worded in a narrative, or decontextualized, when they are devoid of context [6, 12, 13, 26, 44, 48, 49]. Providing context is thought to improve students' engagement by making course materials relevant [26]. Research shows mixed evidence for using contextualization in computing education. Studies using the Rainfall Problem and the Satellite Problem have found little effect on students' performance when it comes to solving contextualized vs. decontextualized programming problems [6, 12, 48, 49]. However, Leinonen et al. [44] have discovered that context might help students avoid algebraic errors when they are tasked with programming exercises that involve mathematics. Guzdial [25] has reported an increase in course participant retention when contextualized exercises were introduced in course materials. In the field of educational psychology, there has been a number of studies on context personalization of mathematical problems [5, 34, 35, 72, 86]. Context personalization means tailoring learning materials to personal interests, preferences, and cultural backgrounds of learners [5, 34, 35, 72, 73, 85-87]. The contextualization approach centered around students' personal interests or preferences has demonstrated positive results on situational interest and perceived utility value among students with low interest in mathematics [34, 35]. It has also positively impacted both students whose quantitative engagement with their interests was high, in case of deep contextualization, and low, when context involved only superficial details [86]. In a longitudinal study by Bernacki and Walkington [5], context personalization has improved students'"}, {"title": "2 RELATED WORK", "content": "2.1 Context Personalization\nPersonalization in education has various applications and definitions. According to Bernacki et al. [4], the most common features mentioned in its definitions are related to identifying and adapting instruction to the needs and interests of learners. However, these terms vary across educational approaches to personalization. Tetzlaff et al. [78] contrast personalized education with traditional approaches that disregard individual characteristics of students and treat them as a unified group.\nIn addition to systems that work with individualized learning needs of students, there are context personalization strategies offering learning materials according to students' out of school interests, preferences, or cultural backgrounds [5, 34, 35, 72, 73, 85-87]. Solari et al. [73] summarize what constitutes learners' interests in educational psychology literature. According to them, the majority of research considers a particular type of relationship between a person and their interest, their psychological experiences and focuses on the engagement involved in a personal interest. However, they note the existing diversity of theoretical approaches to defining the object of an interest, centered around topics, domains, and practices. Solari et al. [73] propose that the personalization strategy that works with students' individual interests enhances their sense of value and meaning associated with learning. Additionally, context personalization can engage contextual grounding, which utilizes students' prior experiences with a particular context [5]. It is thought to ease long-term memory recollection, lessen the probability of conceptual errors, and facilitate understanding of subject domain concepts [5].\nIt has also been suggested that context personalization may stimulate students' situational interest, which is characterized by improved engagement and attention caused by conspicuous parts of learners' environments [5, 33-35, 54, 86]. Situational interest, in turn, can lead to the development of individual interest in a subject domain, for example, mathematics or computing [5, 33, 34, 54]. Hidi and Renninger [33] suggest a four-phase model of situational and individual interest development. The first phase is triggered situational interest, which may serve as a precursor to the inclination to engage with particular content over a period of time; the second phase, maintained situational interest, is characterized by focused attention and may nudge learners towards further engagement in a more advanced phase. Individual interest, comprised of emerging and well-developed (well-established) phases, is typically self-generated, while situational interest is often externally supported [33]. Nevertheless, emerging individual interest might be hindered by lack of support, resources, and positive reinforcement, whereas well-established interest is stable throughout time [54]. In computing education, developing individual interest is thought"}, {"title": "2.2 Perceptions of Assessment Quality", "content": "The way students and teachers perceive assessment is relevant to this study as we generate uncredited, additional practice opportunities for students for formative assessment utilizing generative AI. Struyven et al. [76] conducted a literature review of 36 empirical studies. They found that, in general, students find assessment positive (i.e., beneficial for their learning) and fair (i.e., accurately and justly measuring their progress towards their learning goals) if the assessment relates to authentic tasks, presents reasonable demands, encourages them to apply knowledge to realistic contexts, focuses on the need to develop a range of skills, and is perceived to have long-term benefits [67, 76].\nThis is supported by later empirical findings. Van Dinther et al. [81] studied the perceptions of assessment of 138 first-year elementary teacher students at a Dutch university. They focused on studying the links between perceptions, self-efficacy, and performance. They found that formative assessment where students create \"a quality product or observable performance in a real-life situation\" and where feedback is tied to the task and criteria increases self-efficacy, which in turn is likely to lead to more learning. They also argue that the presence of sufficient practice is a requirement for mastery of the topic.\nGerritsen et al. [22] studied the perceptions of 204 higher education students based on six aspects of assessment quality: effects on learning, fairness, conditions, interpretation of test scores, authenticity, and credibility. They found that students who had more positive perceptions of the effects of assessment on learning were more likely to employ deeper and strategic learning approaches, whereas students who had negative perceptions were more likely to apply a surface learning approach, which has been linked to worse learning outcomes [53]. They argue that this is due to students deepening their approach if they find the assessment appropriately challenging and motivating. Similarly, Gulikers et al. [24] found that the more authentic students find the tasks that they are solving, the deeper the study approach they choose, which should result in enhanced learning.\nRelated to teachers' perceptions of assessment quality, Sach [66] analyzed the perceptions of 67 lower and middle school teachers of assessment. She found that teachers value formative assessment and believe it to enhance learning. However, teachers were less confident in actually employing formative assessment practices in their own courses.\nThese prior works suggest that it is imperative to make (formative) assessment tasks authentic [24, 76, 81], contextually relevant [76], and motivating [22]. Especially for the last two aspects, utilizing LLMs to personalize the context of the tasks to the interests of the student could be useful. Similarly, it is important to provide students with enough opportunities for practice in order for them to achieve mastery of the topic [81], which could be scaffolded by utilizing LLMs to generate programming exercises for formative assessment."}, {"title": "2.3 Large Language Models in Computing Education", "content": "Large language models (LLMs) have exhibited outstanding performance in many tasks, including code generation [9, 19, 20, 40, 41, 46,"}, {"title": "2.4 Automatic Programming Exercise Generation", "content": "Prior to the emergence of modern generative AI models based on LLMs, template-based approaches were popular for automatic generation of programming exercises [61, 83, 84, 91]. In these template-based, parameterized exercises, certain parts of their problem descriptions are parameterized, meaning that specific parts of an exercise are modified according to defined parameters. This technically allows one to generate an infinite number of variations where, for example, numbers or specific statements in problem descriptions are different. As each student is presented with minor exercise differences, one typical use case for such exercises is to prevent plagiarism [64].\nUsing context free grammar to form program templates [1, 57] and generating random exercises from various models such as UML diagrams or mathematical notation [74, 79] have been suggested to improve purely parameter-based template approaches. These methods allow a way to individualize exercises for students. Such template-based approaches also make it possible to effectively personalize exercise content to themes and topics, as demonstrated by Zavala and Mendoza [91], who contextualized the generated exercises though linked open data. Nonetheless, they fail to generate completely new and varied exercises with deep personalization without extensive manual effort.\nAs LLMs displayed state-of-the-art performance in code generation and explanation tasks, Sarsa et al. [70] investigated the use of OpenAI Codex for generating programming exercise task descriptions with model solutions and automated tests. Their study yielded promising results, as the generated exercises were mostly sensible and sometimes of sufficient quality to be handed to students to solve without modification. However, roughly one fifth of the generated exercises did not make sense, and Codex sometimes struggled to accommodate concepts given as keywords to personalize the exercises. The authors additionally noted that Codex managed poorly in creating sample solutions and automated tests. On the other hand, focusing solely on solution and test generation, Chen et al. [8] achieved much better results by generating multiple solution and test pairs and then picking a sample where the generated solution passed the generated tests.\nReplicating the work of Sarsa et al. [70] with a more modern AI model, namely GPT-4, del Carpio Gutierrez et al. [13] evaluated generated exercises for context relevance, description clarity, and problem sensibility in a larger study involving multiple prompting strategies. They assessed the generated content using rubrics and found the quality of the content to be high. Another replication of the work by Sarsa et al. [70] authored by Jordan et al. [36] explored the performance of GPT-3.5 in generating exercises in four natural languages. They found that problems generated in English, Spanish, and Vietnamese were mostly accurate and understandable and would only require minor modifications before giving them to students. However, the quality of the exercises generated in Tamil was poor, indicating that current models still do not completely generalize across natural languages.\nIn a similar work with a more manual approach, Speth et al. [75] generated programming exercise sheets with ChatGPT (GPT-3.5) chat sessions by having an instructor provide ideas and context to the model and then iteratively leveraging ChatGPT to refine the generated exercises within the ChatGPT session. While they noticed that ChatGPT was adept at quickly generating good exercises, they noted that instructors nearly always resorted to minor manual edits to improve exercise quality. Phung et al. [59] explored ChatGPT and GPT-4 against human tutors in generating debugging quizzes that could help students practice specific concepts (among other things). They focused on creating simplified versions of students' buggy pieces of code to help them practice solving bugs that they encounter, whereas our aim is to create new practice exercises contextualized to various themes and topics.\nThe unique contribution of our work is the focus on evaluating context personalization specifically. In addition, only one prior work on automatic exercise generation using LLMs actually had"}, {"title": "3 METHODS", "content": "3.1 Prompt Engineering\nThe prompt used for generating the final exercise set went through multiple revisions both in terms of wording the prompt and the role of the request and determining an appropriate temperature value, which regulates randomness of the model's responses. The original version of the prompt only asked the model to produce a new exercise based on the example we provided and to contextualize it to a particular topic. This, however, resulted in the model often producing the same response when asked for an exercise contextualized to a popular theme, e.g., pets or outdoor activities. For example, when asking for an exercise related to pets, the model would almost solely generate exercises related to cats and dogs. To circumvent this limitation, we asked the model to list popular entities or things within certain categories, which constituted the themes used for personalization. The top level themes were arbitrarily chosen by the authors, whereas the topics within the themes were generated by the model. For instance, to obtain a list of topics for the pets theme, we asked the model to generate a list of ten popular pets. When asked to generate popular handicrafts, GPT-4 included home-brewing, which is why we had to explicitly forbid it from mentioning alcohol in order to keep the exercises suitable for a wide audience. Additionally, we tried to make the topics contextually relevant to our users by using such keywords as \"Finland\" and \"the Nordic countries\" in our requests. Our themes were the following: outdoor activities, literature, historical landmarks, classical music, pop music, cartoons, food, pets, sports, video games, nature destinations, handicrafts, art, Christmas, party games, board games. Additionally, we had to ask the model to omit words related to diseases, as it included Pandemic in the board game theme, which we considered unacceptable following the COVID-19 pandemic.\nOnce the model was given the topics to create new exercises, other issues appeared. When asked to generate a new problem about guinea pigs, it produced a response that contained incorrect arithmetical calculations and mentions of weight gain. Following this occurrence, we made a decision to restrict the model further and ask it to exclude trigger words associated with mental or physical disorders in order to keep the course materials inoffensive. The response related to guinea pigs was as follows.\nWhen determining an appropriate value for the temperature parameter, we gave the model the same prompt asking it to generate a new exercise about Mickey Mouse with two different temperature values. When the temperature value was set to 0.7, it produced the somewhat humorous problem description found below.\nLowering the temperature to 0.3 resulted in it completely ignoring the topic and producing an exercise unrelated to Mickey Mouse.\nRaising the temperature to 1.5 led to the model hallucinating. \nSimilarly, the temperature value of 2.0 resulted in another absurd response. Eventually, the temperature value of 0.5 was chosen to keep the model restrained to the structure of the examples we provided but allow for some variation in its output.\nAdditionally, the model was asked to include certain concepts covered in the first three chapters of the course. Two concepts were introduced in each chapter, and the model was instructed to use the concepts covered in the current and previous chapters. Without this restraint, the model would often produce exercises unrelated to the concepts discussed in the course. The first chapter covered user input and program output, the second one introduced variables and arithmetics, and the third one was dedicated to conditional statements and logical operators. To generate normal difficulty exercises, we asked the model to create new problems at the same difficulty level as our examples. To make them a bit more difficult, we requested slightly more complex exercises. The model was instructed to avoid loops due to the limitations of the tool, which could potentially become stuck in an endless loop.\nThe final prompt is as follows (the variables in bold).\nIn order to provide blanket specifications to the model, we experimented with the role that is included in the GPT-4 request. The role"}, {"title": "3.2 Exercise Generation", "content": "For the purposes of this study, we chose to pregenerate the exercises instead of having an on-demand system. This was done for multiple reasons. First, pregenerating the exercises allowed us to check them for material that could be offensive to a wide audience. This is a concern as LLM-generated content might include biases that were present in the training data of the LLM [18]. Second, possible outages in the availability of the LLM API could have caused technical disruptions in the system when students wanted to use the tool. Lastly, during prompt engineering we noticed that the LLM would sometimes generate faulty code (see Table 1), and in many cases this would have made exercises impossible to assess automatically (as they require working unit tests generated by the LLM).\nThe exercise generation was done with the aim to obtain a varied set of problems for each chapter covering different concepts. To achieve the desired variety, we originally created three exercises for each combination of difficulty (normal and advanced), concept (two concepts per chapter, six in total)\u00b9, and theme (each theme"}, {"title": "3.3 Study Context", "content": "The study was conducted in an open online introductory programming course that uses the Dart programming language and is offered by Aalto University in Finland. The course is worth 2 ECTS credit points, which corresponds to approximately 50 study hours. The course covers input and output, variables and arithmetics, conditionals and logical operators, looping, functions, and lists and maps. The course uses an online textbook platform. The online textbook has intertwined embedded theory and exercise parts with programming exercises and quizzes, where the exercises are automatically assessed by the platform. The programming exercises are completed in an embedded online integrated development environment (IDE), which is opened through the platform when students work on programming exercises. The IDE comes with normal IDE functionality, including syntax and error highlighting and the possibility to run the programs within the IDE; programming exercises are also submitted through the IDE.\nAs the course is an open elective online course, the course participants include both Aalto University students and lifelong learners. The platform does not distinguish between them, and as is usual for open online courses, attrition rates have room for improvement. Out of the students who complete at least one exercise in the first chapter of the course, 51% continue to the second chapter, and 44% continue to the third chapter.\nThe responsible teacher of the course did not provide access to demographic data for this study. However, when considering the demographic data, prior research on the course has highlighted that the participants come from a range of backgrounds [71]. Most of the participants who continue past the first chapters of the course are between 26-35 or 36-55 years old, have some experience from tertiary education, have taken no prior programming courses, participate in the course due to being interested in the topic, and self-estimate their programming knowledge as very low. The vast majority of the course population are lifelong learners as CS majors at our university have other mandatory introductory programming courses. Of those identifying their gender, approximately half of the participants self-identify as men, a bit more than one third as women, and the remaining either as other than men or women or do not wish to disclose their gender."}, {"title": "3.4 Tool", "content": "For the purposes of the study, we implemented a new component to the platform that allows retrieving LLM-generated programming exercises and showing them in the embedded IDE. When retrieving an exercise, students can select a theme, a concept, and a difficulty level. They can also allow that any of these are chosen randomly. Based on the selection, the platform then retrieves a problem description and starter code, which are shown to the student. Once the student has completed the exercise, they can submit the exercise for grading in a similar way as if they worked on the standard course exercises. The LLM-generated exercises were evaluated using automated tests that were also generated using an LLM.\nThe component was embedded into the first three chapters of the course: (1) input and output, (2) variables and arithmetics, and (3) conditionals and logical operators. The component was shown at the end of each chapter. The instructions before the component that the students saw were as follows (translated from Finnish).\nCompleting the exercises did not yield any course points, and the students were not compensated in any other form. The tool had a progress bar that highlighted progress in the AI-generated exercises in each chapter. For each chapter, the progress bar filled after completing three exercises, at which point the students also saw a greyed out trophy icon being colored as yellow. This slight gamification was added to try nudge students into completing at least some of the AI-generated exercises."}, {"title": "3.5 Data Collection", "content": "The platform collected data on fetching exercises, where the data included a student identifier, a timestamp, the selections (theme, concept, difficulty), and the retrieved exercise. In addition, the platform collected data on the submissions, where the data similarly included the student identifier, a timestamp, an identifier for the exercise, and the submitted code. We also implemented survey functionality to the component. Once a student completed an exercise, they were shown a survey regarding the exercise, with the following four questions.\nOnce a student completed three exercises within a chapter, they were shown a different survey with the following questions.\nWith the exception of the last question (\"Please provide open feedback on the AI-generated practice exercises.\"), the questions were given as 7-item Likert-scale questions with the following"}, {"title": "3.6 Approach", "content": "3.6.1 RQ1: Expert Evaluation by the Study Authors. For research question 1, \"How do the study authors evaluate the quality of contextually personalized exercises generated by GPT-4?\", we follow the methods of prior work that has evaluated LLM-generated exercises by developing a rubric for quality assessment of exercises [13, 36, 70]. The rubric questions can be found in Table 2.\nOut of the 283 programming exercises, 33 were randomly selected for inter-rater reliability analysis. The five authors discussed the rubric and rated the 33 exercises according to the rubric. The results of the inter-rater reliability analysis are presented in Table 2. We report both the percentage agreement and Gwet's AC1 statistic. We chose Gwet's AC1 as the inter-rater reliability metric due to the limitations of other multi-rater IRR statistics, such as Krippendorf's alpha and Fleiss' kappa, caused by high agreement between raters [16, 17, 27, 28].\nAfter the IRR analysis, each author individually evaluated a set of 50 randomly selected exercises. Thus, all the exercises that were included in the tool were assessed in the expert evaluation. We report the exact numbers of answers for each question as well as percentages.\nFor the final set of evaluations, for the 33 exercises that were evaluated in the IRR analysis, we use the majority vote value. There were only four ties for majority value, all of them occurring for the \"shallow vs. deep personalization\u201d question. For three ties, they were between \"unsure\" and \"deep\" and for one, between \"unsure\" and \"shallow\". In these four cases, we chose the other option than \"unsure\" for the final evaluation set.\n3.6.2 RQ2: Student Evaluation. For research question 2, \"How do students evaluate the quality of contextually personalized exercises generated by GPT-4?\", we analyze the feedback on the generated exercises provided by the course participants.\nWe report the distribution of the Likert-answers to the survey questions, which are listed in Section 3.5. Due to the low number of"}, {"title": "4 RESULTS", "content": "4.1 RQ1: Expert Evaluation by the Study Authors\nThe results of the inter-rater reliability analysis for the expert evaluation are presented in Table 2. From the table, it can be seen that the level of agreement varied between the questions. Agreement was quite high for the first five questions but was lower for the last two.\nThe results of the expert evaluation of the 283 generated exercises is shown in Table 3. Answering the first question, \"The exercise description was clear\u201d, we considered the overwhelming majority of the exercises to be clear. As for the second, \"The exercise description matched the selected theme\u201d, and third, \"The exercise description matched the selected topic\u201d, questions, we found that almost all of them matched both their theme and topic. When evaluating the exercises for the fourth question, \"The exercise description matched the selected concept\u201d, we concluded that 87.6% of them corresponded to their requested concept. When it comes to assessing whether the exercises included concepts that were too advanced (the fifth question), we found that most of them did not contain concepts that were out of scope for the corresponding chapter of the course. However, when evaluating whether the exercise difficulty matched the selected difficulty (the sixth question), we concluded that the difficulty of the exercises was frequently not satisfactory. We found that the difficulty was rated as \"too easy\" in 39.6% of the cases, \"too difficult\" in 6.0%, and \"okay\" in 54.4% of the exercises. As for the depth of personalization (the seventh question), it was shallow in the majority of the generated exercises (64%), while the rest were either somewhere in between (9.5%) or deeply personalized (26.5%).\nThe cases where there were too difficult concepts included occurred mostly in the first chapter. In these cases, when the model was asked for an \"advanced\" exercise related to user input or output, it would produce an exercise requiring conditionals (introduced in"}, {"title": "4.2 RQ2: Student Evaluation", "content": "Figure 10 shows the results of the student evaluation of the generated exercises. Figure 10a shows the distribution of the Likert-scale responses to the survey given after each exercise and Figure 10b shows the distribution for the survey shown to the students after they had completed three exercises. There were a total of 79 responses to the exercise-specific survey and 28 responses to the general survey shown after completing three exercises.\nFrom the figures, it can be seen that the student feedback was overwhelmingly positive, with over half of the students strongly agreeing with every statement. The five statements that did not receive any disagreements are the following: matched selected theme, matched selected concept, useful for my learning, engaging to me, and theme selection was satisfactory. The three statements that received negative responses (description was clear, matched selected difficulty, and enjoyed being able to select themes) only had a few responses disagreeing with the statements (12%, 10%, and 3% respectively)."}, {"title": "4.3 RQ3: Student Interactions", "content": "Tables 4 and 5 contain summaries of descriptive statistics for the interaction data collected by the tool. Table 4 shows the overall number of the students who fetched and solved exercises and the range, mean, median, and standard deviation for the fetched and solved exercises. Table 5 shows the number of the fetched and solved exercises and the number of the individual course participants split by chapter. Note that the same person might have used the tool across multiple chapters, which is why the number summed in Table 5 is higher than the number of the students in Table 4. A total of thirty seven users solved three or more exercises, another two solved two exercises, and the remaining eight submitted one correct solution."}, {"title": "5 DISCUSSION", "content": "Both the expert and student evaluations indicate that the quality of the generated exercises was high. In addition, they were well received by the students. This suggests that AI-generated problems could be a valuable addition to introductory programming courses, at least, as additional practice material as was the case in this study.\nOne reason for the positive student feedback could be that the context personalization made the tasks feel more authentic to them, which has been found to positively impact students' perceptions of assessment [22, 24, 76]. These results support earlier findings by Sarsa et al. [70], Jordan et al. [36], and del Carpio Gutierrez et al. [13] who also found the quality of generated exercises to be generally"}, {"title": "5.1 Limitations", "content": "There are some limitations to this study, which we outline here. First, the course was an elective, self-paced, online course, where the participants were mostly lifelong learning students (i.e., not formally enrolled at a university). Moreover, earlier empirical research on contextually personalized learning materials has been mostly limited to mathematics in secondary education [5, 34, 35, 72, 85, 86]. Thus, it is uncertain whether the results found here would generalize to more traditional introductory programming courses with deadlines, where the majority of participants are computer science or other STEM majors. Such students might be more motivated to complete such courses in general compared to lifelong learners who constitute the majority of the course population. The experiment was also done at a single institution, which also limits its generalizability.\nAnother limitation is the lack of background information concerning the preexisting levels of interest and competence in programming among the course participants. Earlier research on context personalization has shown that those students who possess less interest and lower self-perceived competence in a subject, e.g., mathematics, benefit more from contextually personalized problems [34, 35]. In their case, context personalization triggers situational interest. What is more, the depth of personalization affects various categories of learners in a different fashion, e.g., those who engage more with mathematics through their interests are more positively influenced by deep personalization in mathematical problems [86]. Since we did not obtain any detailed data on the course participants' prior engagement and attitudes towards computer programming, we could not assess how different levels of personalization affected their study progress or feedback on the exercises. Additionally, we are uncertain to which extent contextualization might have influenced their situational or individual interest in the subject area.\nAs the course was offered entirely online and there was no contact between the study authors and the study participants, the risk of subject and experimenter expectancy biases [23, 93] could be considered small due to these effects being primarily evidenced in interpersonal settings [30, 39, 65]. Nonetheless, we acknowledge that, apart from the open feedback, all of our survey questions were Likert-scaled and positively inclined towards effects desired by the authors, e.g., \"The exercises description matched the selected theme\" and \"I enjoyed being able to select themes that match my interests\". This makes our surveys susceptible to an acquiescence bias, where responders tend to passively agree (or disagree) to asked questions irrespective of content [60]. However, both age and education have been observed to mitigate the effect of acquiescence on questionnaires [10], and as the course participants in general are lifelong learners and university students, both the average age and education among the participants was likely to be reasonably high. Besides the phrasing of the questions themselves, the questionnaires or the shown exercises did not contain anything that could be interpreted as suggestive.\nRelated to the generated exercises, we opted to pregenerate the exercises to avoid running into any problems with an on-demand system, such as the LLM being unavailable for some reason or students trying to do prompt injection attacks to break the tool [58]. The downside of this approach was that the theme selection was also predefined and thus might not have matched the students' top interests. However, multiple varying themes were available, and we were mostly interested in seeing if students were keen to select any particular theme over a random one. Another limitation of having the exercises pregenerated is that the exercise pool was limited, and consequently the situation did not exactly mimic one where students would have truly unlimited practice opportunities.\nCompleting the exercises was optional, and no course credit was provided for completing them. This may have caused a selection bias, as active students might have used the tool more frequently. Since we openly told the course participants that the exercises were Al-generated, it is also possible that those students who are interested in Al were more likely to complete them, potentially skewing the results of the surveys. Such students might have also rated the AI-generated content more favorably compared to those learners who have more neutral or negative views on AI.\nRelated to the expert evaluation, two questions had relatively low agreement scores: determining if the exercise difficulty matched the selected difficulty and whether the personalization was deep or shallow. Thus, the results utilizing the expert evaluation data for these two questions should be taken with a grain of salt - the other evaluators could have made different decisions on these questions.\nOne limitation of the experimental design was that the course was organized in Finnish, but the tool, surveys, and generated exercises were in English. We opted to generate exercises in English as prior work has found that LLMs perform the best in English [92]. Recent work exploring the generation of exercises with LLMs in"}, {"title": "6 CONCLUSIONS", "content": "In this work, we explored how successfully LLMs, namely GPT-4, can generate programming exercises. The generated content was offered for additional student practice in an online programming course. We evaluated the quality of the generated exercises by having both the authors and the students in the course assess them. In addition, we examined how the course participants interacted with the exercises in the course e-book.\nOur findings indicate that the vast majority of the exercises generated by the LLM were clear and matched various themes, topics, and concepts. What is more, they rarely included concepts that were too advanced for the students in the course. However, the exercises were often easier than requested in the prompt given to the model, and the thematic personalization was often shallow.\nIn our future work, we are interested in studying how adding gamification features to the tool affects student engagement. Additionally, we are interested in examining how students' programming experience correlates with their interaction with LLM-generated exercises. We are working on a version of the tool that would allow students to generate exercises related to any contextual theme on demand, instead of having a list of predefined themes available in the tool."}]}