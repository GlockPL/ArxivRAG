{"title": "Evaluating Contextually Personalized Programming Exercises Created with Generative AI", "authors": ["Evanfiya Logacheva", "Sami Sarsa", "Arto Hellas", "Juho Leinonen", "James Prather"], "abstract": "Programming skills are typically developed through completing various hands-on exercises. Such programming problems can be contextualized to students' interests and cultural backgrounds. Prior research in educational psychology has demonstrated that context personalization of exercises stimulates learners' situational interests and positively affects their engagement. However, creating a varied and comprehensive set of programming exercises for students to practice on is a time-consuming and laborious task for computer science educators. Previous studies have shown that large language models can generate conceptually and contextually relevant pro-gramming exercises. Thus, they offer a possibility to automatically produce personalized programming problems to fit students' interests and needs. This article reports on a user study conducted in an elective introductory programming course that included con-textually personalized programming exercises created with GPT-4. The quality of the exercises was evaluated by both the students and the authors. Additionally, this work investigated student at-titudes towards the created exercises and their engagement with the system. The results demonstrate that the quality of exercises generated with GPT-4 was generally high. What is more, the course participants found them engaging and useful. This suggests that AI-generated programming problems can be a worthwhile addition to introductory programming courses, as they provide students with a practically unlimited pool of practice material tailored to their personal interests and educational needs.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning how to program involves developing various sets of programming skills. Computing education theories present them as distinct and often sequential or hierarchical, i.e., meant to be introduced in a certain order [21, 52, 90]. Since developing expertise in any domain requires a certain amount of training [29], developing programming skills relies on practical hands-on exercises focused on particular learning objectives [2, 15, 50, 55, 68, 77, 82, 90].\nResearch indicates that many students in computer education experience high levels of stress, frustration, confusion, and boredom, which can lead to negative outcomes [11, 82]. Frustration is often developed as a result of encountering various difficulties related to both coding itself and course instruction; it precedes boredom and loss of interest in subject learning [11]. Certain solutions such as substituting long weekly assignments with multiple shorter problems have been reported to alleviate students' stress and improve their performance [2]. Similarly, repetitive exercises designed for teaching students syntax in CS1 have been found effective for enhancing students' engagement and their exam scores [15]. Such exercises have been viewed as helpful by students [50, 77]. Overall, learning how to program hands-on has been reported to reduce students' stress levels [82].\nProgramming exercises can be contextualized, i.e., worded in a narrative, or decontextualized, when they are devoid of context [6, 12, 13, 26, 44, 48, 49]. Providing context is thought to improve students' engagement by making course materials relevant [26]. Research shows mixed evidence for using contextualization in computing education. Studies using the Rainfall Problem and the Satellite Problem have found little effect on students' performance when it comes to solving contextualized vs. decontextualized programming problems [6, 12, 48, 49]. However, Leinonen et al. [44] have discovered that context might help students avoid algebraic errors when they are tasked with programming exercises that involve mathematics. Guzdial [25] has reported an increase in course participant retention when contextualized exercises were introduced in course materials. In the field of educational psychology, there has been a number of studies on context personalization of mathematical problems [5, 34, 35, 72, 86]. Context personalization means tailoring learning materials to personal interests, preferences, and cultural backgrounds of learners [5, 34, 35, 72, 73, 85-87]. The contextual-ization approach centered around students' personal interests or preferences has demonstrated positive results on situational interest and perceived utility value among students with low interest in mathematics [34, 35]. It has also positively impacted both students whose quantitative engagement with their interests was high, in case of deep contextualization, and low, when context involved only superficial details [86]. In a longitudinal study by Bernacki and Walkington [5], context personalization has improved students'"}, {"title": "2 RELATED WORK", "content": "Personalization in education has various applications and defini-tions. According to Bernacki et al. [4], the most common features mentioned in its definitions are related to identifying and adapting instruction to the needs and interests of learners. However, these terms vary across educational approaches to personalization. Tetzlaff et al. [78] contrast personalized education with traditional approaches that disregard individual characteristics of students and treat them as a unified group.\nIn addition to systems that work with individualized learning needs of students, there are context personalization strategies offering learning materials according to students' out of school interests, preferences, or cultural backgrounds [5, 34, 35, 72, 73, 85-87]. Solari et al. [73] summarize what constitutes learners' interests in educational psychology literature. According to them, the majority of research considers a particular type of relationship between a person and their interest, their psychological experiences and focuses on the engagement involved in a personal interest. However, they note the existing diversity of theoretical approaches to defining the object of an interest, centered around topics, domains, and practices. Solari et al. [73] propose that the personalization strategy that works with students' individual interests enhances their sense of value and meaning associated with learning. Additionally, context personalization can engage contextual grounding, which utilizes students' prior experiences with a particular context [5]. It is thought to ease long-term memory recollection, lessen the probability of conceptual errors, and facilitate understanding of subject domain concepts [5].\nIt has also been suggested that context personalization may stimulate students' situational interest, which is characterized by improved engagement and attention caused by conspicuous parts of learners' environments [5, 33-35, 54, 86]. Situational interest, in turn, can lead to the development of individual interest in a subject domain, for example, mathematics or computing [5, 33, 34, 54]. Hidi and Renninger [33] suggest a four-phase model of situational and individual interest development. The first phase is triggered situational interest, which may serve as a precursor to the inclination to engage with particular content over a period of time; the second phase, maintained situational interest, is characterized by focused attention and may nudge learners towards further engagement in a more advanced phase. Individual interest, comprised of emerging and well-developed (well-established) phases, is typically self-generated, while situational interest is often externally supported [33]. Nevertheless, emerging individual interest might be hindered by lack of support, resources, and positive reinforcement, whereas well-established interest is stable throughout time [54]. In computing education, developing individual interest is thought to rely on consistent positive situational interest built over time,"}, {"title": "3 METHODS", "content": "The prompt used for generating the final exercise set went through multiple revisions both in terms of wording the prompt and the role of the request and determining an appropriate temperature value, which regulates randomness of the model's responses. The original version of the prompt only asked the model to produce a new exercise based on the example we provided and to contextualize it to a particular topic. This, however, resulted in the model often producing the same response when asked for an exercise contextualized to a popular theme, e.g., pets or outdoor activities. For example, when asking for an exercise related to pets, the model would almost solely generate exercises related to cats and dogs. To circumvent this limitation, we asked the model to list popular entities or things within certain categories, which constituted the themes used for personalization. The top level themes were arbitrarily chosen by the authors, whereas the topics within the themes were generated by the model. For instance, to obtain a list of topics for the pets theme, we asked the model to generate a list of ten popular pets. When asked to generate popular handicrafts, GPT-4 included home-brewing, which is why we had to explicitly forbid it from mentioning alcohol in order to keep the exercises suitable for a wide audience. Additionally, we tried to make the topics contextually relevant to our users by using such keywords as \"Finland\" and \"the Nordic countries\" in our requests. Our themes were the following: outdoor activities, literature, historical landmarks, classical music, pop music, cartoons, food, pets, sports, video games, nature destinations, handicrafts, art, Christmas, party games, board games. Additionally, we had to ask the model to omit words related to diseases, as it included Pandemic in the board game theme, which we considered unacceptable following the COVID-19 pandemic.\nOnce the model was given the topics to create new exercises, other issues appeared. When asked to generate a new problem about guinea pigs, it produced a response that contained incorrect arithmetical calculations and mentions of weight gain. Following this occurrence, we made a decision to restrict the model further and ask it to exclude trigger words associated with mental or physical disorders in order to keep the course materials inoffensive. The response related to guinea pigs was as follows.\nWhen determining an appropriate value for the temperature parameter, we gave the model the same prompt asking it to generate a new exercise about Mickey Mouse with two different temperature values. When the temperature value was set to 0.7, it produced the somewhat humorous problem description found below.\nLowering the temperature to 0.3 resulted in it completely ignoring the topic and producing an exercise unrelated to Mickey Mouse.\nRaising the temperature to 1.5 led to the model hallucinating.  contains a response with emojis.\nSimilarly, the temperature value of 2.0 resulted in another absurd response. Eventually, the temperature value of 0.5 was chosen to keep the model restrained to the structure of the examples we provided but allow for some variation in its output.\nAdditionally, the model was asked to include certain concepts covered in the first three chapters of the course. Two concepts were introduced in each chapter, and the model was instructed to use the concepts covered in the current and previous chapters. Without this restraint, the model would often produce exercises unrelated to the concepts discussed in the course. The first chapter covered user input and program output, the second one introduced variables and arithmetics, and the third one was dedicated to conditional statements and logical operators. To generate normal difficulty exercises, we asked the model to create new problems at the same difficulty level as our examples. To make them a bit more difficult, we requested slightly more complex exercises. The model was instructed to avoid loops due to the limitations of the tool, which could potentially become stuck in an endless loop.\nThe final prompt is as follows (the variables in bold).\nIn order to provide blanket specifications to the model, we experimented with the role that is included in the GPT-4 request. The role"}, {"title": "4 RESULTS", "content": "The results of the inter-rater reliability analysis for the expert evaluation are presented in Table 2. From the table, it can be seen that the level of agreement varied between the questions. Agreement was quite high for the first five questions but was lower for the last two.\nThe results of the expert evaluation of the 283 generated exercises is shown in Table 3. Answering the first question, \"The exercise description was clear\u201d, we considered the overwhelming majority of the exercises to be clear. As for the second, \"The exercise description matched the selected theme\", and third, \"The exercise description matched the selected topic\", questions, we found that almost all of them matched both their theme and topic. When evaluating the exercises for the fourth question, \"The exercise description matched the selected concept\", we concluded that 87.6% of them corresponded to their requested concept. When it comes to assessing whether the exercises included concepts that were too advanced (the fifth question), we found that most of them did not contain concepts that were out of scope for the corresponding chapter of the course. However, when evaluating whether the exercise difficulty matched the selected difficulty (the sixth question), we concluded that the difficulty of the exercises was frequently not satisfactory. We found that the difficulty was rated as \"too easy\" in 39.6% of the cases, \"too difficult\" in 6.0%, and \"okay\" in 54.4% of the exercises. As for the depth of personalization (the seventh question), it was shallow in the majority of the generated exercises (64%), while the rest were either somewhere in between (9.5%) or deeply personalized (26.5%).\nThe cases where there were too difficult concepts included occurred mostly in the first chapter. In these cases, when the model was asked for an \"advanced\" exercise related to user input or output, it would produce an exercise requiring conditionals (introduced in"}, {"title": "5 DISCUSSION", "content": "Both the expert and student evaluations indicate that the quality of the generated exercises was high. In addition, they were well received by the students. This suggests that AI-generated problems could be a valuable addition to introductory programming courses, at least, as additional practice material as was the case in this study.\nAdditionally, contextually personalized exercises may contribute to enhanced situational interest that can potentially lead to the more advanced phases of interest development, i.e., emerging and well-developed individual interest stages [5, 33-35, 54]. For example, research in mathematics education by H\u00f8gheim and Reber [35] has indicated that shallow context personalization positively affects students with low individual interest in the subject by supporting their situational interest. Since the majority of the generated problems were shallowly personalized , this effect could potentially translate to computing education students. For those students who have already achieved the initial stage of individual interest development, i.e., emerging individual phase, such exercises can help maintain their interest by serving as an additional source of learning materials. As some of the exercises were personalized to the cultural background of the study participants, they could increase learners' sense of intrinsic and attainment values, as was demonstrated by Schoenherr [72].\nThere are some limitations to this study, which we outline here. First, the course was an elective, self-paced, online course, where the participants were mostly lifelong learning students (i.e., not formally enrolled at a university). Moreover, earlier empirical research on contextually personalized learning materials has been mostly limited to mathematics in secondary education [5, 34, 35, 72, 85, 86]. Thus, it is uncertain whether the results found here would generalize to more traditional introductory programming courses with deadlines, where the majority of participants are computer science or other STEM majors. Such students might be more motivated to complete such courses in general compared to lifelong learners who constitute the majority of the course population. The experiment was also done at a single institution, which also limits its generalizability.\nAnother limitation is the lack of background information concerning the preexisting levels of interest and competence in programming among the course participants. Earlier research on context personalization has shown that those students who possess less interest and lower self-perceived competence in a subject, e.g., mathematics, benefit more from contextually personalized problems [34, 35]. In their case, context personalization triggers situational interest. What is more, the depth of personalization affects various categories of learners in a different fashion, e.g., those who engage more with mathematics through their interests are more positively influenced by deep personalization in mathematical problems [86]. Since we did not obtain any detailed data on the course participants' prior engagement and attitudes towards computer programming, we could not assess how different levels of personalization affected their study progress or feedback on the exercises. Additionally, we are uncertain to which extent contextualization might have influenced their situational or individual interest in the subject area.\nAs the course was offered entirely online and there was no contact between the study authors and the study participants, the risk of subject and experimenter expectancy biases [23, 93] could be considered small due to these effects being primarily evidenced in interpersonal settings [30, 39, 65]. Nonetheless, we acknowledge that, apart from the open feedback, all of our survey questions were Likert-scaled and positively inclined towards effects desired by the authors, e.g., \"The exercises description matched the selected theme\" and \"I enjoyed being able to select themes that match my interests\". This makes our surveys susceptible to an acquiescence bias, where responders tend to passively agree (or disagree) to asked questions irrespective of content [60]. However, both age and education have been observed to mitigate the effect of acquiescence on questionnaires [10], and as the course participants in general are lifelong learners and university students, both the average age and education among the participants was likely to be reasonably high. Besides the phrasing of the questions themselves, the questionnaires or the shown exercises did not contain anything that could be interpreted as suggestive.\nRelated to the generated exercises, we opted to pregenerate the exercises to avoid running into any problems with an on-demand system, such as the LLM being unavailable for some reason or students trying to do prompt injection attacks to break the tool [58]. The downside of this approach was that the theme selection was also predefined and thus might not have matched the students' top interests. However, multiple varying themes were available, and we were mostly interested in seeing if students were keen to select any particular theme over a random one. Another limitation of having the exercises pregenerated is that the exercise pool was limited, and consequently the situation did not exactly mimic one where students would have truly unlimited practice opportunities.\nCompleting the exercises was optional, and no course credit was provided for completing them. This may have caused a selection bias, as active students might have used the tool more frequently. Since we openly told the course participants that the exercises were AI-generated, it is also possible that those students who are interested in AI were more likely to complete them, potentially skewing the results of the surveys. Such students might have also rated the AI-generated content more favorably compared to those learners who have more neutral or negative views on AI.\nRelated to the expert evaluation, two questions had relatively low agreement scores: determining if the exercise difficulty matched the selected difficulty and whether the personalization was deep or shallow. Thus, the results utilizing the expert evaluation data for these two questions should be taken with a grain of salt - the other evaluators could have made different decisions on these questions.\nOne limitation of the experimental design was that the course was organized in Finnish, but the tool, surveys, and generated exercises were in English. We opted to generate exercises in English as prior work has found that LLMs perform the best in English [92]. Recent work exploring the generation of exercises with LLMs in"}, {"title": "6 CONCLUSIONS", "content": "In this work, we explored how successfully LLMs, namely GPT-4, can generate programming exercises. The generated content was offered for additional student practice in an online programming course. We evaluated the quality of the generated exercises by having both the authors and the students in the course assess them. In addition, we examined how the course participants interacted with the exercises in the course e-book.\nOur findings indicate that the vast majority of the exercises generated by the LLM were clear and matched various themes, topics, and concepts. What is more, they rarely included concepts that were too advanced for the students in the course. However, the exercises were often easier than requested in the prompt given to the model, and the thematic personalization was often shallow.\nIn our future work, we are interested in studying how adding gamification features to the tool affects student engagement. Additionally, we are interested in examining how students' programming experience correlates with their interaction with LLM-generated exercises. We are working on a version of the tool that would allow students to generate exercises related to any contextual theme on demand, instead of having a list of predefined themes available in the tool."}]}