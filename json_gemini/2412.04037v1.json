{"title": "INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations", "authors": ["Yongming Zhu*", "Longhao Zhang*", "Zhengkun Rong*", "Tianshu Hu**", "Shuang Liang", "Zhipeng Ge", "Bytedance"], "abstract": "Imagine having a conversation with a socially intelligent agent. It can attentively listen to your words and offer visual and linguistic feedback promptly. This seamless interaction allows for multiple rounds of conversation to flow smoothly and naturally. In pursuit of actualizing it, we propose INFP, a novel audio-driven head generation framework for dyadic interaction. Unlike previous head generation works that only focus on single-sided communication, or require manual role assignment and explicit role switching, our model drives the agent portrait dynamically alternates between speaking and listening state, guided by the input dyadic audio. Specifically, INFP comprises a Motion-Based Head Imitation stage and an Audio-Guided Motion Generation stage. The first stage learns to project facial communicative behaviors from real-life conversation videos into a low-dimensional motion latent space, and use the motion latent codes to animate a static image. The second stage learns the mapping from the input dyadic audio to motion latent codes through denoising, leading to the audio-driven head generation in interactive scenarios. To facilitate this line of research, we introduce DyConv, a large scale dataset of rich dyadic conversations collected from the Internet. Extensive experiments and visualizations demonstrate superior performance and effectiveness of our method. Project Page: https://grisoon.github.io/INFP/.", "sections": [{"title": "1. Introduction", "content": "In recent years, researchers have dedicated considerable attention to audio-driven head generation [9, 18, 23, 24, 31, 37, 39, 42, 46, 56, 59], in order to build the conversational agent. However, most studies only focus on single-sided communication, such as talking or listening, ignoring the dyadic properties in human-human interaction. Talking-head generation [2, 17, 27, 36, 37, 41, 45, 46, 48, 49, 52, 56, 60] aims at synthesizing speaker's facial animations based on their reference images and a driving audio. Although these works can produce vibrant videos with accurate lip synchronization, they solely emphasize the speaker's role and overlook the listener's feedback. Listening-head generation [16, 18, 23, 24, 31, 32, 55, 58], on the other hand, intends to respond to speaker's behaviors. However, these works restrict the listener's response solely to non-verbal facial movements, which is clearly far from real-life inter-action scenarios. Some recent studies [39, 42, 59] have started exploring head generation for dyadic interaction, but they require manual role assignment between Listener and Speaker and cannot achieve smooth and natural role switch-ing. We believe that for a conversational agent, it should not be predetermined a role, but rather be able to freely switch between listening and speaking states based on the current dialogue context. When having a conversation with the agent, you may interrupt or respond to it at any moment. At that point, it naturally transitions from a speaker to a lis-tener, and vice versa.\nIn this paper, we propose a novel audio-driven head generation framework for dyadic interaction, namely INFP. Unlike previous works, we do not divide the model into Speaker Generator and Listener Generator, or apply ex-plicit role switching. The driven individual dynamically al-ternates between the speaking and listening state, guided by the dyadic audio. Specifically, INFP comprises a Motion-Based Head Imitation stage and an Audio-Guided Motion Generation stage. In the first stage, our model learns to ex-tract communicative behavior, including non-verbal listen-ing cues and verbal speaking patterns, from massive real-life conversational videos and encode them to a motion la-tent space. The motion latent codes are then used to animate a static portrait image to authentically imitate the character in the video. A well-crafted motion latent space should ex-hibit a high degree of disentanglement, which means that head pose, facial expression, and emotion should be de-coupled from appearance. To this end, we perform facial structure discretization and facial pixel masking to the in-put. While in the second stage, the model is trained to map the dyadic audio to the motion latent space pretrained in stage 1 to obtain the corresponding motion latent codes. It consists of an interactive motion guider and a conditional diffusion transformer. The former takes as input the au-dio from both agent and its conversation partner, and re-trieves verbal and non-verbal motions from learnable mem-ory banks to construct the interactive motion feature. The latter utilizes the interactive motion feature as the condition and, together with other signals, generates the motion latent codes through denoising.\nTo support our research on interactive head generation, we introduce DyConv, a large scale dataset of rich dyadic"}, {"title": "2. Related Work", "content": "2.1. Single-Sided Audio-Driven Head Generation\nListening Head Generation. As a pioneer, RLHG [58] contributes the ViCo dataset as a common benchmark and builds a baseline method with a sequential decoder. Fol-lowing that, PCH [13] introduces the enhanced renderer to improve the visual quality of generated videos. Song et al. [32] organizes the REACT challenge to encourage the com-munity to investigate the face reaction in offline and on-line setting, separately. L2L [23] leverages the VQ-VAE [40] to store head motion sequences in discrete codebooks, leading to the better motion diversity. ELP [31] rearranges the latent space based on emotional priors to emphasize the emotion of generated motions. To get rid of simple emo-tional labels, CustomListener [18] proposes to control lis-tener head motions via the text prior. MFR-Net [16] makes a progress of identity preservation and motion diversity. Re-alTalk [8] proposes a a medium-scale dataset and attempts to retrieve possible videos of listener head with large lan-guage model. Ng et al. [24] presents a transfer-based ap-proach from pretrained large language models to predict the motion sequences.\nTalking Head Generation Audio-driven talking head generation has become a prominent research area in re-cent years. Early GAN-based approaches, like Wav2Lip [27], PC-AVS [56], and MakeItTalk [60], directly integrate the input audio and video to produce lip-synced visuals. VASA-1 [46] takes a significant leap forward in the field by decoupling latent feature spaces, enabling the genera-tion of more realistic video outputs. This model can op-erate on a single image rather than requiring a full video sequence. With the emergence of diffusion models, several works [33, 37, 41] are proposed to enhance the generation quality. EchoMimic [2] is concurrently trained using both audios and facial landmarks, and is capable of generating portrait videos not only by audios and facial landmarks in-dividually, but also by a combination of both audios and selected facial landmarks. AniTalker [56], employing two self-supervised learning strategies, excels in capturing intri-cate facial movements.\n2.2. Head Generation in Dyadic Conversations\nRecently, the research community has paid increasing at-tentions on the audio-driven head generation in the dyadic interaction. As an extension of RHLG [58], Zhou et al. [59] proposes a multi-turn conversational dataset, called ViCo-X, and designs a Role Switcher to bridge the Listener Generator and the Speaker Generator. However, the ex-plicit role switching could cause unnaturalness and incon-sistency between different states. Furthermore, this kind of paradigm fails to cover all states in dyadic conversations, such as the agent and the conversation partner speak simul-taneously. DIM [39] applies a pretraining approach that jointly models speakers' and listeners' motions to capture the dyadic context. In applications, the pretrained model needs to be additionally finetuned for downstream tasks like talking head and listening head generation, separately. To this end, the manual assignment of the role is necessary in dyadic conversations, leading to inappropriate transitions. AgentAvatar [42] designs a streamlined pipeline to predict behaviors of a photo-realistic avatar agent in dyadic inter-actions. While the motion synthesis is conditioned on the high-level textual description, AgentAvatar fails to gener-"}, {"title": "3. Method", "content": "Our goal is to synthesize the agent head during a dyadic conversation. To this end, we propose a two-stage interac-tive head generation framework, called INFP.\nIn the first stage, the model learns to extract and com-press multiple conversational behaviors in videos into a low-dimensional motion latent space, and use these latent codes to animate the portrait image of the agent, denoted as $I_{self}$. Note that, to ensure the richness and diversity of the motion latent space, we train stage 1 with plenty of real-life dialogue videos. Once the training converges, we freeze this part and start to train the second stage. The model takes as input the audio of the agent, denoted as $A_{self}$, and the audio of the conversation partner, denoted as $A_{other}$, to learn the mapping from this dual-track dialogue audio to motion la-tent codes through denoising. We present a comprehensive introduction to these two stages below."}, {"title": "3.1. Motion-Based Head Imitation", "content": "Motion Encoding. For motion encoding, we employ a motion encoder $E_m$ to learn an implicit latent represen-tation, which captures verbal and non-verbal communica-tive behaviors from input facial images. $E_m$ comprises a convolutional-based feature extractor followed by MLP layers, which compresses the motion feature into a latent code. Note that, we use implicit representation instead of explicit 3DMM coefficients [28] because the expressiveness of 3DMM model is limited, and the commonly used expres-sion coefficients, predicted by SOTA 3D facial estimation methods [5, 7], entangle with face shape to a certain extent.\nA well-crafted motion latent space should demonstrate a high level of disentanglement to ensure that the extracted motions are appearance-independent. To achieve this, we apply two measures. (a) We set the motion latent code as a low-dimensional 1-D descriptor. It fosters effectively capturing the fundamental semantics of facial motion with-out entangling with appearance information, as explained in previous works [38, 43, 44]. (b) We design a hybrid facial representation to replace the original input image of $E_m$. In particular, we first mask the majority of facial pixels, pre-serving only the eyes and lip regions, as we regard them as the most intricate and expressive components of facial expressions. This can block the interference of irrelevant information such as hair and background for expression dis-till. To provide facial orientation and contour information, we then use an off-the-shelf facial estimation model [20]"}, {"title": "3.2. Audio-Guided Motion Generation", "content": "Previous works [34, 39, 59] tend to divide their models into a Speaker Generator and a Listener Generator. Since the Speaker Generator only receives speaking audio, while the Listener Generator solely processes listening audio, it is necessary to segment the audio of both agent and its con-versation partner into speaking clips and listening clips. Hence, in a multi-turn dialogue, explicit role switching is also crucial. We argue that it can lead to unnatural tran-sitions in communicative states, particularly in real-life in-teraction contexts characterized by frequent role switching. Besides, this design paradigm clearly goes against the in-"}, {"title": "Motion Mapping.", "content": "To map the interactive motions $f_m$ to the pretrained motion latent space of stage 1, we em-ploy a conditional diffusion transformer T [11]. Given the data distribution $q(m_{1:N}, f_m)$ where $m_{1:N}$ represents the corresponding motion latent codes with N frames, the diffusion model approximates the conditional distribution $q(m_{1:N}|f_m)$. T only has 4 blocks, which makes our frame-work lightweight enough to achieve real-time interaction. Each block consists of a self-attention layer, a motion-attention layer, and a temporal-attention layer. T predicts the noise added to the ground-truth motion latent codes for each denoising step. Diffusion timestep is converted into the sinusoidal embedding and then concatenated to the noised motion latent codes at the time dimension. In the motion attention layer, we take the latent feature output from preceding self-attention layer as the Query, while the interactive motion feature $f_m$ is utilized as the Key and Value. Inspired by AnimateDiff [10], we implement a tem-poral layer to ensure a smooth transition between adjacent windows. In particular, we incorporate the last 10 frames of the generated motion latent codes from the previous win-dow as the condition of the current one. Note that in the inference time, we add noise to the self motion latent codes $m_{self}$, which are extracted from $I_{self}$ and replicated into n copies."}, {"title": "Training Strategy.", "content": "In the training stage, we randomly set style vector to null with a probability of 0.3, and ran-domly drop interactive motion feature and previous mo-tion latent codes with a probability of 0.5. Also note that we adopt a warm-up strategy for training stage 2. Specif-ically, we only select single-sided conversation clips from our training set as \"simple case\" to train the model for sev-eral epochs, which helps with the initialization of two mem-ory banks. After that, we randomly sample multi-turn clips from entire dataset for the remaining training process."}, {"title": "4. Dataset", "content": "Existing dyadic interaction datasets have limitations in both scale and quality. ViCo [58] is a small-scale dataset with 1.6 hours and 96 IDs, and it lacks multi-turn conversation sce-narios. Although ViCo-X [57] features rich multi-turn con-versations, the total duration is only 0.4 hours. Besides, its background is monotonous, all of which are green screen. RealTalk [8] is a medium-scale dataset capturing dyadic conversations between pairs of individuals totaling to 115 hours of online public videos. However, it does not restrict the resolution of faces in the video, leading to low face res-olution in many videos, thus impacting the quality of model generation.\nWe propose DyConv, a large-scale dataset of multi-turn dyadic conversations collected from the Internet. The dataset contains video clips with a duration of over 200 hours, captures a wide gamut of emotions and expressions. We employ an off-the-shelf face detection model [51], and keep only frames where both individuals of the conversa-tion are fully visible and their facial resolution is greater than 400 \u00d7 400. In addition, we utilize a SOTA speech sep-aration model [54] to separate the audio of two persons in the conversation, denoted as $A_{p1}$ and $A_{p2}$. Then we run an active speaker detection model [1] to match each audio clip to the corresponding face in the original video. We will publicly release the dataset, along with separated audios and annotations. We hope it will encourage the investigation of head generation for dyadic interaction."}, {"title": "5. Experiment", "content": "5.1. Implementation Details\nIn our experiments, we train the diffusion transformer using AdamW optimizer [19], with $\u03b2_1$ = 0.9 and $\u03b2_2$ = 0.999, a learning rate of le-4, a weight decay of le-2, and a batch size of 32. For inference, we use a DDIM sampler [30] for 20 denoising steps. The CFG parameter of motion feature is set to 2, and the CFG parameter of previous motion latent is set to 1.5.\n5.2. Comparisons\nAlthough originally designed for dyadic interactive scenar-ios, our method can be naturally generalized to other appli-cations, such as listening head generation and talking head generation. This allows us to fully validate the advance-ment of our proposed framework against some of the cur-rent SOTA methods in related areas.\nEvaluation Metrics. To quantitatively evaluate the vi-sual quality, we utilize the Structured Similarity (SSIM), Peak Signal-to-Noise Ratio (PSNR) and Frechet Inception Distance (FID) for the generated videos. Additionally, LPIPS [50] is employed to measure the feature-level sim-ilarity between generated and ground-truth frames. To as-"}, {"title": "5.4. Visualization of Style Control", "content": "As illustrated in Sec. 3.2, our model supports inputting style vector sm extracted from any portrait video to globally con-trol the emotions or attitudes of the generated results. Here we select 3 video clips with varying degrees of \"happiness\" to control emotions during generation. Results are shown in Fig. 5. It can be seen from the figure that sm effectively control the emotions of the driven individual."}, {"title": "6. Conclusion", "content": "In this paper, we propose INFP, a novel audio-driven in-teractive head generation framework for dyadic conversa-tions, which aims to mimic facial behaviors of human-human interactions conditioning on the dyadic audio. As demonstrated, our method is a unified framework and is ca-pable of smoothly and naturally adapting to various con-versational roles without explicit assignment or switching. Meanwhile, we collect a large scale audio-visual dataset for multi-turn dyadic conversations, which could facilitate fur-ther researches of dyadic interactive head generation. Ex-tensive experiments and ablations prove the advancement and effectiveness of our method over other SOTA methods.\nLimitation and Future Work. Currently, our method only takes conversation audios as the input, combining con-trol signals from multiple modalities, such as visual and tex-tual contents, can offer additional capacities. As our pro-posed framework focus on head synthesis, extending the generation range to the upper, even whole body might be a interesting and challenging future work.\nEthical Considerations. There might be the poten-tial misuse of our method to fabricate deceptive talks and speeches. To this end, we will restrict access to our core models exclusively to research institutions."}], "equations": ["Flows\u2192d = F(Em(Iself), Em (Vdri))\nIpred = Dface(Warp(Eface(Iself), Flows\u2192d)) (1)"]}