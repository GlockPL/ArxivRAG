{"title": "SWEPO: Simultaneous Weighted Preference Optimization for Group Contrastive Alignment", "authors": ["Taneesh Gupta", "Rahul Madhavan", "Xuchao Zhang", "Chetan Bansal", "Saravan Rajmohan"], "abstract": "We introduce Simultaneous Weighted Preference Optimization (SWEPO), \u0430 novel extension of Direct Preference Optimization (DPO) designed to accommodate multiple dynamically chosen positive and negative responses for each query. SWEPO employs a weighted group contrastive loss, assigning weights to responses based on their deviation from the mean reward score. This approach effectively prioritizes responses that are significantly better or worse than the average, enhancing optimization. Our theoretical analysis demonstrates that simultaneously considering multiple preferences reduces alignment bias, resulting in more robust alignment. Additionally, we provide insights into the training dynamics of our loss function and a related function, InfoNCA. Empirical validation on the Ultra-Feedback dataset establishes SWEPO as state-of-the-art, with superior performance in downstream evaluations using the AlpacaEval dataset.", "sections": [{"title": "Introduction", "content": "Alignment of language models with human preferences is a critical aspect of developing AI systems that are not only capable of generating coherent text but also of producing outputs aligned with human values and expectations (Liu et al., 2023b; Ouyang et al., 2022; Christiano et al., 2017). Traditional methods like Reinforcement Learning from Human Feedback (RLHF) through explicit reward modeling (Christiano et al., 2017; Schulman et al., 2017) require large amounts of human-annotated training data (Wang et al., 2023), which can be resource-intensive.\nIn this context, Direct Preference Optimization (DPO) has become a cornerstone method for aligning language models with human preferences, as it directly optimizes model parameters based on preference data without the need for explicit reward modeling (Rafailov et al., 2024; Xu et al., 2024). DPO traditionally contrasts pairwise responses \u2013 using one positive and negative response per query.\nHowever, the landscape of alignment datasets has evolved to include multiple positive and negative responses per query (Cui et al., 2023; Liu et al., 2024), reflecting the diverse ways in which responses can be good, or bad. Furthermore, the complexity of language means that for any given query, there are numerous acceptable responses, and similarly, multiple unacceptable ones. The ability to consider multiple preferences simultaneously allows for a more robust alignment process \u2013 jointly incorporating different response information into the model parameters (Yang et al., 2024). Such multi-preference optimization accounts for the spectrum of responses and leverages awareness of this response distribution to align models more effectively (Zhou et al., 2023; Wang et al., 2024).\nIn essence, for any given query, there exists a distribution of acceptable and suboptimal responses. Sampling only a small subset from these distributions may introduce alignment biases. Examples of such biases include length bias (Holtzman et al., 2019; Wolf et al., 2023), format bias (Zhang et al., 2024; Long et al., 2024), safety bias (Cui et al., 2024; Arditi et al., 2024), cultural bias (Tao et al.,"}, {"title": "1.1 Our Contributions", "content": "Our work makes the following key contributions:\n1. Algorithm for Rating-aware Multi-preference Optimization: We extend Direct Preference Optimization to account for multiple positive and negative responses per query, incorporating weighting based on response ratings. This allows the model to jointly consider a spectrum of responses and prioritize them according to their quality during optimization.\n2. Theoretical Analysis of Bias Reduction: We provide theoretical insights demonstrating that joint contrastive losses are less prone to alignment bias compared to traditional DPO, which relies on samples from the distributions of accepted and rejected responses. Our analysis shows that by considering multiple preferences simultaneously, the model achieves a more robust alignment with human values."}, {"title": "2 Related Work", "content": "Proximal Policy Optimization (PPO): Proximal Policy Optimization (PPO) (Schulman et al., 2017) has emerged as a robust reinforcement learning algorithm for policy optimization by clipping policy updates to prevent instability. Building upon earlier policy gradient methods (Sutton et al., 1999), PPO has been successfully applied to alignment tasks in Reinforcement Learning from Human Feedback (RLHF), allowing language models to produce outputs aligned with human preferences (Ziegler et al., 2019; Ouyang et al., 2022). Its simplicity and efficiency make it a standard approach for fine-tuning large-scale models. Prior to PPO, Trust Region Policy Optimization (TRPO) (Schulman, 2015) introduced constraints to improve learning stability, influencing the development of PPO. Early applications of policy gradient methods in natural language processing (Ranzato et al., 2015) demonstrated the potential of reinforcement learning for language model training.\nDirect Preference Optimization (DPO): Direct Preference Optimization (DPO) simplifies the alignment of language models by optimizing a contrastive loss directly over paired preference data, bypassing the intermediate step of reward modeling (Rafailov et al., 2024). Unlike RLHF, DPO does not require explicit reward functions, making it computationally efficient and suitable for limited preference datasets. Recent extensions of DPO, such as Identity Preference Optimization (IPO) (Azar et al., 2024), self-play preference optimization (Wu et al., 2024), preference ranking optimization (Song et al., 2024), rejection sampling optimization (Liu et al., 2023a), and generalized preference optimization (Tang et al., 2024) are amongst the other recent works improve on the DPO method.\nMulti-Preference Optimization: Traditional preference optimization methods, like DPO, consider pairwise comparisons. However, datasets such as UltraFeedback (Cui et al., 2023) highlight the necessity of multi-preference optimization. Multi-preference methods, such as InfoNCA (Chen et al., 2024), leverage all available positive and negative responses simultaneously, reducing alignment bias and better approximating the true preference distribution. These methods mitigate limitations inherent to pairwise approaches by incorporating the diversity of acceptable and suboptimal responses. Earlier works in search have also used multiple user preferences to optimize models in various applications such as search (Joachims, 2002).\nReward Modeling in Preferences: Reward modeling is essential for translating qualitative human feedback into quantitative metrics that guide AI behavior optimization. Traditional methods, such as Reinforcement Learning from Human Feedback (RLHF), utilize reward models trained on human annotations to inform policy updates (Christiano et al., 2017; Stiennon et al., 2020). Early approaches like inverse reinforcement learning (Ng et al., 2000) and apprenticeship learning (Abbeel and Ng, 2004) demonstrated the feasibility of inferring reward functions from observed behaviors. Recent advancements have diversified reward modeling techniques. For instance, the Adversarial Preference Optimization (APO) framework employs adversarial training to adapt reward models to the evolving generation distribution of language models (Cheng et al., 2023).\nContrastive Predictive Coding and InfoNCA: Contrastive learning, particularly methods like InfoNCE (Oord et al., 2018), maximizes mutual information between positive samples while discriminating against negatives. InfoNCA adapts these principles for preference optimization, aligning responses with scalar rewards through noise-contrastive estimation (Chen et al., 2024). Despite its strengths, InfoNCA can overemphasize less informative negative samples, which motivates methods like SWEPO that dynamically weigh responses based on deviation from the mean reward.\nUltraFeedback Dataset: The UltraFeedback dataset (Cui et al., 2023) is a significant advancement in preference-based training resources. It comprises GPT-4 annotated feedback for over 64,000 instructions, including scalar reward evaluations. UltraFeedback has been pivotal in developing models like UltraLM-13B-PPO and UltraRM, which achieve state-of-the-art performance across benchmarks such as AlpacaEval. This dataset's granularity enables advanced preference optimization methods like SWEPO to leverage diverse response quality levels effectively."}, {"title": "3 Notations and Preliminaries", "content": "In this section, we establish the notations and preliminaries necessary for our proposed weighted multi-preference optimization method.\nLet X denote the set of all possible queries, with $x \\in X$ representing a specific query. For each query x, let $Y_x$ be the set of all potential responses. Our dataset D consists of N queries, where each query x is associated with n responses ${y_i}_{i=1}^n$ and corresponding reward scores ${S_i}_{i=1}^n$.\nThe mean reward score for query x is calculated as:\n$S_{mean} = \\frac{1}{n}\\sum_{i=1}^{n} S_i$. (1)\nThe deviation of each response's reward score from the mean is:\n$\\Delta S_i = S_i - S_{mean}$. (2)\nWe partition the responses into positive and negative sets:\n$Y^+ = \\{y_i \\mid \\Delta S_i > 0\\},$ (3)\n$Y^- = \\{y_j \\mid \\Delta S_j \\le 0\\}.$ (4)\nWeights are assigned based on the deviation, using an exponential function or a power function. For positive responses ($y_i \\in Y^+$):\n$w_i = \\exp(\\alpha \\Delta S_i)$ or $w_i = (\\Delta S_i)^p,$ (5)\nand for negative responses ($y_j \\in Y^-$):\n$w_j = \\exp(\\alpha (-\\Delta S_j))$ or $w_j = (-\\Delta S_j)^p,$ (6)\nwhere $\\alpha > 0$ is a scaling hyperparameter and $p \\in \\{0, 1, 2\\}$.\nThe language model parameterized by $\\theta$ provides the conditional probability $P_{\\theta}(y \\mid x)$ of generating response y given query x. The logit or score function is:\n$s_{\\theta}(y \\mid x) = \\log(\\frac{P_{\\theta}(y \\mid x)}{P_{ref}(y \\mid x)}) = \\log P_{\\theta}(y \\mid x) - \\log P_{ref}(y \\mid x)$. (7)\nIncorporating the weights into the probabilities, we have:\n$w_i \\times \\exp(s_{\\theta}(y_i \\mid x)) = \\exp(\\alpha \\Delta S_i + s_{\\theta}(y_i \\mid x))$, (8)\nwhich leads to the modified score:\n$s'_{\\theta}(y_i \\mid x) = s_{\\theta}(y_i \\mid x) + \\alpha \\Delta S_i$. (9)\nThe weighted contrastive loss function is defined as:\n$L_{weighted}(\\theta) = - \\log \\frac{\\sum_{y \\in Y^+} \\exp(s_{\\theta}(y \\mid x))}{\\sum_{y \\in Y} \\exp(s_{\\theta}(y \\mid x))},$ (10)\nwhere $Y = Y^+ \\cup Y^-.$"}, {"title": "4 Algorithm and Methodology", "content": "We present the Simultaneous Weighted Preference Optimization (SWEPO) algorithm, which aligns the language model with human preferences by incorporating multiple responses per query and weighting them based on their deviation from the mean reward score."}, {"title": "4.1 Algorithm", "content": null}, {"title": "4.2 Weight Computation and Modified Scores", "content": "In the notations, we defined weights using an exponential function of the deviation $\\Delta S_i$. Specifically, the weight for each response is:\n$w_i = \\exp(\\alpha \\Delta S_i),$ (11)\nfor positive responses, and\n$w_j = \\exp(\\alpha (-\\Delta S_j)),$ (12)\nfor negative responses.\nBy incorporating these weights into the loss function, we observe that:\n$w_i \\times \\exp(s_{\\theta}(y_i \\mid x)) = \\exp(\\alpha \\Delta S_i + s_{\\theta}(y_i \\mid x))$. (13)\nThis demonstrates that weighting the probabilities is equivalent to adjusting the logits by adding the scaled deviation. Thus, the modified score for each response becomes:\n$s'_{\\theta}(y_i \\mid x) = s_{\\theta}(y_i \\mid x) + \\alpha \\Delta S_i$. (14)"}, {"title": "4.3 Generalization with Power p", "content": "In the algorithm, we generalize the weighting scheme by defining the weights as the p-th power of the deviation:"}, {"title": "5 Theoretical Analysis", "content": "In this section, we provide theoretical insights into why incorporating multiple preferences per query, as in our proposed SWEPO method, leads to better alignment with human values compared to methods that rely on pairwise preferences, such as Direct Preference Optimization (DPO). We also differentiate our weighted group contrastive loss from the InfoNCA loss (Chen et al., 2024) and discuss the implications for model optimization."}, {"title": "5.1 Bias Reduction through Multiple Preferences", "content": "A key motivation for using multiple preferences per query is to reduce alignment bias, which arises when sampling a limited subset of responses from the distribution of acceptable and suboptimal responses. We formalize this intuition by analyzing how the expected bias with respect to an attribute decreases as the number of samples increases.\nTo analyze biases, we introduce an attribute function $a(y) : \\mathcal{V}_x \\rightarrow \\mathbb{R}$, which maps responses to real numbers (e.g., response length, politeness).\nThe expected attribute value over the model's distribution is defined as:\n$\\mu_{\\theta} = E_{x \\sim X} [E_{y \\sim P_{\\theta}(\\cdot \\mid x)} [a(y)]],$ (17)\nwhere $P_{\\theta}(\\cdot \\mid x)$ is the model's conditional distribution over responses given query x.\nThe true expected attribute value over acceptable responses is:\n$\\mu_{\\mathcal{A}} = E_{x \\sim X} [E_{y \\sim A_x} [a(y)]],$ (18)\nwhere $A_x$ is the distribution of acceptable responses for query x.\nThe bias with respect to attribute a is then defined as:\n$B^{(k)} = | \\mu_{\\theta}^{(k)} - \\mu_{\\mathcal{A}} |,$ (19)\nwhere $\\mu_{\\theta}^{(k)}$ is the expected attribute value under the model after training with k positive and k negative samples per query."}, {"title": "5.1.1 Assumptions", "content": "We make the following assumptions:\n\u2022 Finite Variance: The attribute a(y) has finite variance over the acceptable response distribution $A_x$ for each query x, i.e., $Var_{y \\sim A_x} [a(y)] = \\sigma_x^2 < \\infty$.\n\u2022 Independent Sampling: Responses are independently sampled from their respective distributions.\n\u2022 Model Capacity: The model can represent the true distribution given sufficient data.\n\u2022 Uniform Bounded Variance: There exists a constant $\\sigma_{max}^2$ such that $\\sigma_x^2 \\le \\sigma_{max}^2$ for all $x \\in X$."}, {"title": "Theorem 1.", "content": "Under the stated assumptions, the expected bias $E[B^{(k)}]$ decreases with the number of samples k as\n$E[B^{(k)}] \\le \\frac{C}{\\sqrt{k}},$ (20)\nwhere $C \\coloneqq \\sigma_{max}$ is a constant depending on the maximum variance of a(y) over acceptable responses."}, {"title": "Corollary 1.", "content": "As k \u2192 \u221e, the expected bias $E[B^{(k)}]$ approaches zero:\n$\\lim_{k \\rightarrow \\infty} E[B^{(k)}] = 0$. (21)\nImplications: This theorem formalizes the intuition that using multiple positive and negative responses per query reduces the bias in the model's alignment with human preferences. By considering more samples from the distributions of acceptable and suboptimal responses, the model's expected attribute values converge to the true values, leading to better alignment."}, {"title": "5.2 Comparison with InfoNCA Loss", "content": "While one might consider applying contrastive learning techniques like InfoNCA to the multi-preference alignment problem, we argue that our weighted group contrastive loss is better suited for this task."}, {"title": "5.2.1 Definitions of Loss Functions", "content": "InfoNCA Loss Function The InfoNCA loss function, proposed by Chen et al. (2024), is defined as:\n$L_{InfoNCA} = - \\sum_{i=1}^{K} p_i^{target} \\log p_i^{model},$ (22)\nwhere:\n$p_i^{target} = \\frac{e^{r(x,y_i)/\\alpha}}{\\sum_{j=1}^{K} e^{r(x,y_j)/\\alpha}}, \\quad p_i^{model} = \\frac{e^{s_{\\theta}(y_i/x)}}{\\sum_{j=1}^{K} e^{s_{\\theta}(y_j/x)}}$. (23)\nIn this context:\n\u2022 x is the query or instruction provided to the model.\n\u2022 ${y_i}_{i=1}^{K}$ represents a set of K responses generated for query x.\n\u2022 r(x, yi) is the reward associated with response Yi.\n\u2022 $s_{\\theta}(y_i \\mid x) = \\log(\\frac{P_{\\theta}(y_i \\mid x)}{P_{ref}(y_i \\mid x)})$ is the score for response Yi.\n\u2022 $\\alpha$ is a temperature parameter controlling the influence of the reward.\nWeighted Contrastive Loss Function Our proposed weighted contrastive loss function is expressed as:\n$L_{weighted} = -\\log (\\frac{\\sum_{i \\in Y^+} w_i e^{s_{\\theta}(y_i \\mid x)}}{\\sum_{j=1}^{K} w_j e^{s_{\\theta}(y_j \\mid x)}}),$ (24)\nwhere:\n\u2022 $Y^+ = \\{y_i \\mid \\Delta S_i > 0\\}$ is the set of positive responses, with $\\Delta S_i = S_i - S_{mean}$, and $S_{mean} = \\frac{1}{K} \\sum_{j=1}^{K} S_j$.\n\u2022 $w_i = e^{\\alpha \\Delta S_i}$ for positive responses, and $w_j = e^{\\alpha (-\\Delta S_j)}$ for negative responses.\n\u2022 $s_{\\theta}(y_i \\mid x) = \\log(\\frac{P_{\\theta}(y_i \\mid x)}{P_{ref}(y_i \\mid x)})$ is the score for response Yi.\n\u2022 $\\alpha$ is a scaling hyperparameter controlling the influence of the deviation $\\Delta S_i$."}, {"title": "5.2.2 Gradient Analysis", "content": "To understand how each loss function influences the model during training, we derive the gradients with respect to the model logits $s_{\\theta}(y_i \\mid x)$ for both methods.\nGradient of InfoNCA Loss\nLemma 1. The gradient of the InfoNCA loss with respect to the model logits $s_{\\theta}(y_i \\mid x)$ is:\n$\\frac{\\partial L_{InfoNCA}}{\\partial s_{\\theta}(y_i \\mid x)} = p_i^{model} - p_i^{target}$. (25)"}, {"title": "Lemma 2.", "content": "The gradient of the weighted contrastive loss with respect to the model logits $s_{\\theta}(y_i \\mid x)$ is:\n$\\frac{\\partial L_{weighted}}{\\partial s_{\\theta}(y_i \\mid x)} = p_i^{weighted} - p^{pos},$ (26)\nwhere:\n$p_i^{weighted} = \\frac{w_i e^{s_{\\theta}(y_i \\mid x)}}{\\sum_{j=1}^{K} w_j e^{s_{\\theta}(y_j \\mid x)}}, \\quad p^{pos} = \\frac{\\sum_{k \\in Y^+} w_k e^{s_{\\theta}(y_k \\mid x)} \\cdot \\mathbb{I}_{y_i \\in Y^+}}{ \\sum_{j=1}^{K} w_j e^{s_{\\theta}(y_j \\mid x)}},$ (27)\nwith $\\mathbb{I}_{y \\in Y^+}$ being the indicator function, equal to 1 if $y_i \\in Y^+$ and 0 otherwise."}, {"title": "5.2.3 Optimization Behavior and Stationary Points", "content": "Understanding the optimization behavior under different loss functions sheds light on how models learn from the training data.\nTheorem 2. For the InfoNCA loss, the stationary points occur when:\n$p_i^{model} = p_i^{target}, \\quad \\forall i \\in \\{1, ..., K\\}.$ (28)\nImplications: This stationary point may not lead to the desired alignment, in light of Remark 1."}, {"title": "5.2.4 Stationary Points of the Weighted Contrastive Loss", "content": "We now analyze the stationary points of our weighted contrastive loss function.\nLemma 3. For the weighted contrastive loss function, the stationary point occurs when the probabilities of the negative samples approach zero:\n$P_{\\theta}(y_i \\mid x) \\rightarrow 0 \\quad \\text{for all} \\quad y_i \\in Y^-$."}, {"title": "5.2.5 Characterization of Optimization Dynamics", "content": "We further analyze the optimization dynamics of our weighted contrastive loss for positive examples under some simplifying assumptions.\nLemma 4. Consider a simplified scenario with $N^+$ positive examples, each with weight $w^+$, and $N^-$ negative examples, each with weight $w^-$. All positive examples have the same score $s^{(t)}$ at iteration t, and all negative examples have the same score. The update rule for the score $s^{(t)}$ of the positive examples at iteration t + 1 is given by:\n$s^{(t+1)} = s^{(t)} + \\eta (\\frac{N^- w^-}{N^+(N^+ w^+ + N^- w^-)}),$ (29)\nwhere $\\eta$ is the learning rate."}, {"title": "6 Experimental Setup", "content": "Model and Training Settings: For our experiments, we utilized the Ultrafeedback Dataset Cui et al. (2023), an instruction-following dataset annotated by GPT-4, containing approximately 64,000 instructions. Each instruction includes four responses generated by different language models, with GPT-4 assigning scalar rewards on a scale of 0 to 10 for each response. Previous research has shown a strong correlation between these GPT-4 ratings and human annotations, establishing their reliability as a cost-effective alternative to human feedback.\nBased on these scalar rewards, we categorized the responses into two sets: chosen responses and rejected responses. This categorization was determined using the mean of the scalar rewards. Responses scoring above the mean were classified as chosen, while the remaining responses were categorized as rejected.\nFor our approach, we bypassed the supervised fine-tuning (SFT) stage and directly employed the Mistral-7B-Instruct-v0.1 model for preference alignment. We applied our proposed preference optimization method, SWEPO, which consists of two configurations:\n1. Absolute-weighting\n2. Squared-weighting\nThese configurations were designed to harness scalar rewards effectively and improve model alignment. Our findings suggest that these setups significantly enhance performance, placing our models among the top contenders on the Alpaca leaderboard."}, {"title": "7 Experimental Results", "content": "SWEPO outperform preference-based methods given reward dataset: The results of our ablation study demonstrate that our proposed methods consistently outperform all baseline preference models. Specifically, our methods exhibit a significant performance improvement of 24% over LC-WR and 42% over WR in the AlpacaEval2 benchmark when compared to the best-performing preference baseline, DPOx(k-1). This substantial improvement underscores the effectiveness of our approach, SWEPO, which leverages its ability to fully exploit all the information available in the reward dataset. The enhanced utilization of reward signals enables SWEPO to achieve superior performance, highlighting the importance of comprehensive information integration in preference-based models.\nSWEPO vs. InfoNCA on Preference Data with Scalar Rewards We implemented the InfoNCA Chen et al. (2024) baseline under our custom settings, adhering to the hyperparameters specified in their original method. The primary difference lies in the finetuning approach: while InfoNCA utilizes QLORA, we opted for full finetuning of their model.\nUsing this approach and from table 2, we observed that SWEPO outperformed InfoNCA on downstream datasets such as AlpacaEval and AlpacaEval2. This result underscores the significance of SWEPO and its clear advantage over InfoNCA, particularly in scenarios where preference data with scalar rewards is available.\nImportance of Suboptimal Responses: Suboptimal responses are also important. Previous prac-"}, {"title": "A Bias Analysis:", "content": "In the first part of this section section, we analyze how the number of positive and negative examples per query, k, affects the bias with respect to an attribute a(y). We provide a formal theorem establishing the relationship between bias and k, followed by a corollary discussing the behavior as k\u2192\u221e.\nThe reason for this analysis is to show that multi-preference sampling of accepted and rejected answers from a distribution is better than using a single sample as DPO does. The more accepted and rejected samples you have, the lower the bias, provably."}, {"title": "A.1 Assumptions", "content": "We make the following assumptions:\n1. Attribute Function: Let a(y) : Yx \u2192 R be an attribute function mapping responses to real numbers (e.g., response length).\n2. Finite Variance: The attribute a(y) has finite variance over the acceptable response distribution Ax for each query x, i.e., Vary~A\u300f [a(y)] = \u03c3\u00b224\u300f <\u221e.\n3. Independent Sampling: Responses are independently sampled from their respective distributions.\n4. Model Capacity: The model can represent the true distribution given sufficient data.\n5. Uniform Bounded Variance: There exists a constant omax such that a \u2264 omax for all x \u2208 \u03a7."}, {"title": "A.2 Bias Definition", "content": "The bias with respect to attribute a is defined as:\n$B^{(k)} = | \\mu_{\\theta}^{(k)} - \\mu_{\\mathcal{A}} |,$ (30)\nwhere:\n\u2022 \u03bc\u03b8 is the expected attribute value under the model after training with k positive and k negative samples per query.\n\u03bc\u03b1 = \u0395xx [\u03bc\u03b1\u00bb], with \u03bc\u03b1 = Ey~A\u2082 [a(y)]."}, {"title": "A.3 Main Bias Result", "content": "Theorem 3. Under the stated assumptions, the expected bias E[B(k)] decreases with the number of samples k as:\n$E[B(k)] < \\frac{C}{\\sqrt{k}},$ (31)\nwhere C omax is a constant depending on the maximum variance of a(y) over the acceptable responses.\nProof. For each query x, consider the sample mean of the attribute over the k positive responses:\na(k) = \u03a3 a(yi), Yi ~ Ax. (32)\nSince they are independent and identically distributed samples from Ax, the expected value and variance of ax are:\nE ax = \u03bc\u0391, (33)\nVar (ax) = < \u221a . (34)\nUsing the fact that for any random variable Z with finite variance, the expected absolute deviation from its mean satisfies:\nE [|Z - E[Z]|] < \u221aVar[Z], (35)\nwe have:\nEax - \u0391 < \u221a . (36)\nAveraging over all queries x \u2208 X:\nEx< \u221a . (37)\nSince \u03bc= \u0395x and \u03bc\u03b1 = \u0395x [\u03bc\u03b1], the expected bias is:\nE[B(k)] = \u03bc\u03ba - \u03bc\u03b1 (38)\n\u2264Ex - \u0391 \u2264 (39)\nThus, the expected bias decreases with k as \u221a .\nCorollary 2. As k \u2192 \u221e, the expected bias E[B(k)] approaches zero:\nlim E[B(k)] = 0. (40)"}, {"title": "B Differentiating the Group Contrastive Loss from InfoNCE Loss", "content": "In this subsection, we compare our proposed weighted contrastive loss function with the InfoNCA loss function. We present both loss functions, derive their gradients rigorously, and characterize their stationary points. Based on this characterization, we discuss the properties of the convergence points in terms of what the models learn and their alignment with human preferences."}, {"title": "B.1 Definitions of Loss Functions", "content": "InfoNCA Loss Function The InfoNCA loss function is defined as:\nK LInfoNCA = target log model,\nwhere represents the target probability for the i-th response, calculated as\ntarget er(x,yi)/\u03b1\nP\n and model denotes the model's predicted probability for the i-th response, given by\nmodel s (Yix)\n\nIn this context, x is the instruction or prompt provided to the model, and {y}{1 represents a set\nof K responses generated for the instruction x. The term r(x, yi) is the reward associated with the response Yi, while so (Yi | x) = log (Poyi | x)/ Pref (Yi | x)) is the score for response yi. The parameter a serves as a temperature parameter that controls the influence of the reward, and K is the total number of responses considered for the instruction x.\nWeighted Contrastive Loss Function Our proposed weighted contrastive loss function is expressed as:\nLweighted\n\nwhere Y+ is the set of positive responses with rewards above the mean, defined as Y+ Yi Si >\nEach response yi is assigned a weight w\u2081 eadi, where di is the deviation of the reward score Si from the mean reward score Smean. Specifically, d\u2081 Si Smean for responses in Y+ and\ndi Smean - Si for responses not in Y+. The mean reward score Smean is calculated as\nK Kj=1 Sj,\nwhere K is the total number of responses for the query x. The term so (Yi | x) denotes the model's\nlogit for response yi, and a is a scaling hyperparameter that controls the influence of the deviation \u03b4\u2081."}, {"title": "B.2 Gradient Analysis", "content": "To understand how each loss function influences the model during training, we derive the gradients\nwith respect to the model logits se (Yi | x) for both methods.\nGradient of InfoNCA Loss\nLemma 5. The gradient of the InfoNCA loss with respect to the model logits se(Yi | x) is:\nLInfoNCA model target. (41)\nSo(YiX)\nProof. The InfoNCA loss is:\nLInfoNCA \u2211log pm model (42)\nOur goal is to compute LInfoNCA(42)\nSince pr target does not depend on se(Yi | x) (the rewards are constants with respect to the model\nparameters), the derivative only affects the terms involving pmodel.\nFirst, express log pmodel explicitly:\nNow, compute the derivative of log Pk pmodel with respect to so (Yi | x):\nalog pmodel \u10dbSo(Ykx)\nSo(YiX)So(Yi |X)\u10dbSo(Yi |x)\nCompute each term separately.\nFirst term:\nD\u0455e(Ykx) \u03b4\u03af\u03ba,\nwhere dik is the Kronecker delta, equal to 1 if i = k and 0 otherwise.\nSecond term:\nK log Z (46)\nCompute Dso(Vi)\naz eeso (Yix) (47)\nSo(Yi | X)\nTherefore,"}, {"title": "Lemma 6.", "content": "The gradient of the weighted contrastive loss with respect to the model logits se(Yix)\nis:\nLweightedweighted\nwhere:\nPp\nweighted Wiese(Yi/x)\nppos \u03a3key+ Wkeso(yk|x).Ly"}, {"title": "C Characterization of Stationary Points", "content": "We now characterize the stationary points of both loss functions."}, {"title": "C.1 Stationary Points of the InfoNCA Loss Function", "content": "Theorem 4. For the InfoNCA loss, the stationary points occur when:\nVi \u2208 {1, ..., K}. (69)\nProof. Stationary points are defined by the condition:\n(70)\nFrom the gradient:\nLInfoNCA 1 (71)\nsetting the gradient to zero yields:\np\nVi."}, {"title": "C.2 Stationary Points of the Weighted Contrastive Loss under Simplifying Assumptions", "content": "Lemma 8. Consider the weighted contrastive loss function in a simplified scenario with the following\nconditions: There are N+ positive examples, each with weight w+, and N- negative examples,\neach with weight w-. All positive examples have the same score s(t) at iteration t, and all negative\nexamples have the same score s(t) at iteration t. Then, the update rule for the score s(t) of the positive\nexamples at iteration t + 1 is given by\ns(t+1) (N+w+ + N-w-))."}, {"title": "Lemma 9.", "content": "Consider the general case where positive examples may have different weights wit, and\neach positive example i has its own score st) at iteration t. Assuming initial scores s0) 0 for all\npositive examples, the score st of positive example i at iteration t, up to a linear approximation, is\ngiven by\nBo\nwhere Ao kEY+\nthe scores are:\n\u0391\u03c9\u2211\u03c9+,\nBo 1.\njEY-\nThe gradient for each positive example i at t 0 is:\n) 0)\nAssuming that the term remains approximately constant over iterations (which holds when \u03b7 is\nsmall and changes in st) are small), the score at iteration t is\nsi"}, {"title": "C.3 Stationary Points of the Weighted Contrastive Loss", "content": "We now analyze the stationary points of our weighted contrastive loss function.\nLemma 10. For the weighted contrastive loss function, the stationary point occurs when the proba-\nbilities of the negative samples approach zero, i.e.,\n(103)\nProof. From Lemma 2, the gradient of the weighted contrastive loss with respect to the model logits\nSo(Yi x) is:"}, {"title": "D Baselines Used for Comparison", "content": "When dealing with reward datasets where each instruction has more than two K > 2 responses"}]}