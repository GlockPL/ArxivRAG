{"title": "Vendi-RAG: Adaptively Trading-Off Diversity And\nQuality Significantly Improves Retrieval\nAugmented Generation With LLMs", "authors": ["Mohammad R. Rezaei", "Adji Bousso Dieng"], "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nfor domain-specific question-answering (QA) tasks by leveraging external\nknowledge sources. However, traditional RAG systems primarily focus on\nrelevance-based retrieval and often struggle with redundancy, especially when\nreasoning requires connecting information from multiple sources. This paper\nintroduces Vendi-RAG, a framework based on an iterative process that jointly\noptimizes retrieval diversity and answer quality. This joint optimization leads\nto significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\nthe Vendi Score (VS), a flexible similarity-based diversity metric, to promote\nsemantic diversity in document retrieval. It then uses an LLM judge that eval-\nuates candidate answers, generated after a reasoning step, and outputs a\nscore that the retriever uses to balance relevance and diversity among the\nretrieved documents during each iteration. Experiments on three challenging\ndatasets-HotpotQA, MuSiQue, and 2WikiMultiHopQA\u2014demonstrate Vendi-\nRAG's effectiveness in multi-hop reasoning tasks. The framework achieves\nsignificant accuracy improvements over traditional single-step and multi-step\nRAG approaches, with accuracy increases reaching up to +4.2% on HotpotQA,\n+4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-\nRAG, the current best baseline. The benefits of Vendi-RAG are even more\npronounced as the number of retrieved documents increases. Finally, we eval-\nuated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4,\nand GPT-40-mini, and observed consistent improvements, demonstrating that\nthe framework's advantages are model-agnostic.", "sections": [{"title": "1 Introduction", "content": "Retrieval-augmented generation (RAG) has emerged as a transformative framework\nfor enhancing the performance of large language models (LLMs) in domain-specific\ntasks such as question-answering (QA). By retrieving relevant information from\nexternal sources beyond the training set, RAG enables LLMs to answer specialized"}, {"title": "2 Related Work", "content": "Question answering. There are three main approaches to QA: non-retrieval-based\nmethods (Petroni et al., 2019), single-step RAG (Lewis et al., 2020), and multi-step\nRAG (Asai et al., 2023). Non-retrieval-based QA methods pass queries directly to"}, {"title": "3 Method", "content": "We now describe Vendi-RAG, including the novel retrieval process it uses."}, {"title": "3.1 Vendi Retrieval", "content": "Diversity in retrieved documents is essential for multi-hop QA, as it ensures broad\nsemantic coverage, reduces redundancy, and incorporates multiple perspectives (Sun\net al., 2022; Carbonell and Goldstein, 1998; Thakur et al., 2021). The most used\nmethods for diverse retrieval are similarity search (SS) (Thakur et al., 2021) and\nmaximal marginal relevance (MMR) (Carbonell and Goldstein, 1998). SS maximizes\nrelevance to the query but retrieves highly similar documents, leading to redundancy.\nMMR balances relevance and novelty using pairwise comparisons but also struggles\nto account for global semantic diversity.\nTo overcome these limitations, we propose a novel retrieval method that leverages\nthe VS (Friedman and Dieng, 2023) to explicitly optimize retrieval diversity. Let\n$D = \\{d_1,..., d_n\\}$ be a set of retrieved documents and k(,) a positive semi-definite\nsimilarity kernel such k(d\u2081, d\u2081) = 1 for all i. Denote by K the corresponding similarity\nmatrix that is such that $K_{ij} = k(d_i, d_j)$. The VS is defined as\n$VS(\\mathcal{D}) = exp\\bigg(-\\sum_{i=1}^{n} \\frac{\\lambda_i}{n} log \\frac{\\lambda_i}{n} \\bigg),$\nwhere \u03bb\u2081,..., \u03bb\u2081 are the eigenvalues of the normalized kernel matrix K/n. As\nargued by Friedman and Dieng (2023), the VS is the effective number of unique\ndocuments in 9, reaching its maximum value n when all the documents are distinct\nand its minimal value 1 when all the documents are the same.\nWhile accounting for diversity is good for retrieval, especially for complex queries, it\nshouldn't be the only criterion. Quality also matters. To balance these two criteria,\nthe Vendi retrieval process uses a convex combination of the two,\nVRS = s\u00b7VSk(q, \u00d8) + (1 \u2212 s) \u00b7 SS(q, D),\nwhere VRS stands for Vendi retrieval score. The similarity score SS(q, D) is computed\nusing dense vector representations of both the query and the documents. The\ndocument representations are used to provide meaningful context, ensuring that the\nretrieved documents are relevant to the query. Here s \u2208 [0, 1] is a tunable parameter\ncontrolling the trade-off between diversity and similarity. When handling complex\nqueries, such as those with multiple possible answers, a higher diversity weight s\npromotes the selection of a semantically diverse set of documents. In contrast, for\nsimpler or more specific queries that require precise information, a smaller value of\ns prioritizes similarity-based retrieval."}, {"title": "3.2 Vendi-RAG", "content": "We integrate the Vendi retrieval process into a flexible RAG pipeline that balances\ndiversity and relevance for improved performance on multi-hop QA.\n1. Initial retrieval. The process begins by retrieving a set of documents using\nVendi retrieval. This first step prioritizes broad semantic coverage (we set s = 0.8\ninitially in all our experiments), ensuring that the retrieved documents capture\nmultiple perspectives and to prevent recovering semantically redundant documents.\nThis initial diversity is particularly critical for multi-hop QA, where synthesizing\ninformation from varied sources is essential to accurately answering the query.\n2. Reasoning generation. Based on the retrieved documents, the system generates\nCoT reasoning steps. These intermediate reasoning steps help contextualize the\nretrieved information, building a coherent pathway to the final answer.\n3. Candidate answer generation. Using the reasoning steps and retrieved docu-\nments, the LLM generates candidate answers. These proposed answers are evaluated\nto determine their quality and completeness.\n4. Quality evaluation. An LLM judge assesses the candidate answers. This evalua-\ntion considers factors such as coherence, relevance, and alignment with the query.\nA quality score Qt is produced at the end of this quality-check. Here t is used to\nindicate the iteration step.\n5. Dynamic adjustment of the VRS. Based on the quality score Qt, the parameter\ns is adjusted dynamically. We denote by st the value of the parameter s at the tth\niteration. It controls the trade-off between diversity (via VS) and relevance (via\nsimilarity search). If Qt is low, st should be increased, to prioritize greater diversity\nin the subsequent retrievals. This ensures broader semantic exploration, which is\nbeneficial for refining answers in cases where the retrieved information is already\nrelevant but lacks coverage. Conversely, if Qt is high, st should be decreased to\nfocus more on relevance, retrieving documents that are closely aligned with the\nquery to address potential gaps in specificity. We therefore define st as\n$s_t = f (Q_{t-1}) = 1-\\frac{Q_{t-1}}{max(Q_{t-1})},$\nwhere f is a simple linear function that maps Qt\u22121 to the interval [0, 1], ensuring\nthat higher quality scores correspond to lower diversity scores.\n6. Iterative refinement. The retrieval and reasoning steps are repeated iteratively,\nwith adjustments to s dynamically balancing diversity and relevance at each stage.\nThis process continues until the desired answer quality is reached, ensuring that the\nsystem converges on an optimal set of documents and reasoning steps.\n7. Final answer selection. Once the iterative refinement process is complete,\nthe final set of documents and answers are selected based on their quality scores.\nThis ensures that the output reflects both broad semantic coverage and high-quality,\nrelevant information. Algorithm 1 summarizes the procedure.\nWhy Adjusting s Matters: The dynamic adjustment of s is critical for striking the\nright balance between diversity and relevance during the retrieval process. High\ndiversity is essential for exploring various facets of a complex query, especially in\nmulti-hop QA, where information from disparate sources must be synthesized. How-\never, excessive diversity can dilute the relevance of retrieved documents, potentially\nintroducing noise and reducing the quality of generated answers. On the other\nhand, overemphasizing relevance can lead to redundancy and failure to capture the\nbreadth of information needed for comprehensive reasoning.\nBy reducing s when the quality score is high, the Vendi-RAG pipeline encourages ex-\nploration of less-redundant, semantically diverse documents. This ensures that even\nif the current answer is sufficient, the model explores additional perspectives that\nmay enhance the depth and breadth of the final response. Conversely, increasing s\nwhen quality is low allows the system to focus on retrieving documents that are more\nclosely aligned with the query, addressing gaps in specificity or relevance.\nThe strength of Vendi-RAG lies in this adaptive approach to document retrieval.\nUnlike traditional RAG systems that use fixed retrieval strategies, Vendi-RAG's\ndynamic adjustment of the diversity-relevance trade-off (the parameter s) allows it\nto respond to the specific requirements of each query and intermediate reasoning\nstep. When the system detects that current retrievals are yielding high-quality\nbut potentially narrow responses, it automatically shifts toward greater diversity,\nexploring complementary perspectives. Conversely, when responses lack precision,\nthe system can focus on more closely related documents to improve specificity."}, {"title": "4 Experiments", "content": "This section presents a comprehensive evaluation of Vendi-RAG on multi-hop QA\ndatasets. First, we investigate the effectiveness of the Vendi retrieval process in\nenhancing retrieval diversity. Next, we evaluate the Vendi-RAG pipeline, demonstrat-\ning its ability to handle complex queries requiring multi-step reasoning compared\nto the baselines.\nDatasets. Our experiments are conducted on three challenging benchmark multi-"}, {"title": "5 Conclusion", "content": "While retrieval-augmented generation (RAG) has proven effective in enhancing large\nlanguage model (LLM) performance for domain-specific question-answering (QA)\ntasks, traditional RAG frameworks often struggle with redundancy, particularly in\nmulti-hop reasoning tasks. To address this shortcoming, we introduce Vendi-RAG, a\nnovel framework that jointly optimizes retrieval diversity and answer quality through\nan iterative refinement process. Vendi-RAG leverages the Vendi Score and an LLM\njudge to promote semantic diversity while maintaining relevance during retrieval.\nOur experiments on HotpotQA, MuSiQue, and 2WikiMultiHopQA demonstrate\nVendi-RAG's effectiveness. Specifically, Vendi-RAG outperforms the best baseline\nby +4.2% on HotpotQA, +4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue.\nThese gains become even more pronounced as the number of retrieved documents\nincreases, highlighting the importance of retrieval diversity in complex reasoning\ntasks. Furthermore, we evaluated Vendi-RAG across multiple LLM backbones, in-\ncluding GPT-3.5, GPT-4, and GPT-40-mini, and observed consistent performance\nimprovements, demonstrating that the framework is model-agnostic. These findings\nestablish Vendi-RAG as an effective and adaptable solution for multi-hop QA.\nLimitations. Vendi-RAG introduces computational overhead due to LLM-based\nquality scoring, which may limit scalability in real-time applications. Additionally,\nlike all RAG approaches, its performance depends on the quality and completeness of\nexternal knowledge sources, making it susceptible to biases or gaps in the retrieved\ninformation."}]}