{"title": "Meta-Learning on Augmented Gene Expression Profiles for Enhanced Lung Cancer Detection", "authors": ["Arya Hadizadeh Moghaddam", "Mohsen Nayebi Kerdabadi", "Cuncong Zhong", "Zijun Yao"], "abstract": "Gene expression profiles obtained through DNA microarray have proven successful in providing critical information for cancer detection classifiers. However, the limited number of samples in these datasets poses a challenge to employ complex methodologies such as deep neural networks for sophisticated analysis. To address this \u201csmall data\" dilemma, Meta-Learning has been introduced as a solution to enhance the optimization of machine learning models by utilizing similar datasets, thereby facilitating a quicker adaptation to target datasets without the requirement of sufficient samples. In this study, we present a meta-learning-based approach for predicting lung cancer from gene expression profiles. We apply this framework to well-established deep learning methodologies and employ four distinct datasets for the meta-learning tasks, where one as the target dataset and the rest as source datasets. Our approach is evaluated against both traditional and deep learning methodologies, and the results show the superior performance of meta-learning on augmented source data compared to the baselines trained on single datasets. Moreover, we conduct the comparative analysis between meta-learning and transfer learning methodologies to highlight the efficiency of the proposed approach in addressing the challenges associated with limited sample sizes. Finally, we incorporate the explainability study to illustrate the distinctiveness of decisions made by meta-learning.", "sections": [{"title": "Introduction", "content": "Cancer is a leading cause of death in modern society. For every 100,000 individuals in the United States, there were 47 new cases of lung cancer and 32 related deaths according to the statistics from CDC in 20201. DNA microarray, a method that measures the activity of tens of thousands of genes simultaneously, has emerged as an effective way of predicting cancer\u00b2. This technique enables the identification of the aberrant gene expression profile of cancer cells, offering valuable insights for early detection and personalized treatment of cancers of interest3,4. Aiming for facilitating an efficient screening of a large set of gene expressions, DNA microarray is capable of analyzing over 48,000 transcript probes on a single chip5. With such advantages, previous literature2,6,7 has extensively investigated using gene expression signatures to create data-driven classifiers to distinguish between cancer patients and controls.\nAlthough gene expression profiling has become increasingly favored for predicting clinical outcomes, researchers and clinicians frequently face challenges in developing accurate predictive models due to the \u201csmall data\u201d dilemma. This issue stems from the limited number of cancer patient samples typically available in clinical studies, often restricted by participant availability or budget constraints. For instance, a study enlisted 228 smokers to investigate the distinguishing signatures between 137 patients with Non-Small Cell Lung Cancer (NSCLC) and 91 patients with diagnosed benign nodules. In another study, 95 subjects were recruited where 8 primary lung cancer patients were compared against 16 with tuberculosis, 25 with sarcoidosis, 8 with pneumonia, and the other control subjects. Given the modest sample sizes usually ranging from several dozen to a few hundred participants, simpler predictive models, such as support vector machines that require fewer parameters, are often preferred for effective training. However, to comprehensively analyze the high-dimensional gene expression signatures and to further discover the intricate interplay among them, there is a growing demand for more sophisticated models like neural networks10. Therefore, how to apply deep learning methodologies with limited and high-dimensional gene expression data for predicting the outcome of interest becomes the research gap we attempt to address in this work.\nIn the exploration of predictive patterns from a small-scale gene expression dataset, one of the problems is whether we can harness the augmented data from additional gene expression studies to improve predictive accuracy. This question has been typically addressed through transfer learning, a two-stage learning pipeline, where models undergo pre-training on diverse datasets first to acquire a general-purpose learning foundation, and subsequently get fine-tuned on the target dataset to tailor their capabilities to a particular domain task. While transfer learning has proven beneficial in gene expression analysis\u00b9\u00b9, its advantage still originates from leveraging abundant but unlabelled source datasets. Furthermore, transfer learning often demands considerable variability in downstream datasets to facilitate a"}, {"title": "Data Description", "content": "In our research, we use distinct gene expression datasets from four different studies. For each set of experiments, three of them serve as sources and the remaining is the target for meta-learning setting. Each dataset contains samples from gene expression levels obtained from microarrays with varying numbers of sensors. Below, the description of the datasets and their GEO ID are presented:\nGSE132558: This dataset contains samples from the University of Pennsylvania Medical Center between 2003 and 2007, consisting of subjects with a history of tobacco use and without previous lung cancer diagnosis, among whom some had benign non-calcified lung nodules and others were diagnosed with non-small-cell lung cancer (NSCLC). Blood samples were collected from all participants either during clinical visits or before surgery. This data is further extended with additional RNA samples from the New York University Medical Center.\nGSE13530414: The dataset contains samples from individuals found to have positive results on low-dose computed tomography scans at five clinical sites: the Helen F. Graham Cancer Center, The Hospital of the University of Pennsylvania, Roswell Park Comprehensive Cancer Center, Temple University Hospital, and participants from New York University Langone Medical Center, including those enrolled in an Early Detection Research Network lung screening program at New York University. The study includes individuals over 50 years old with a smoking history of over 20 pack-years and no cancer diagnosis within the past 5 years, except for non-melanoma skin cancer. Samples associated with malignant were collected within three months of diagnosis or prior to any invasive procedure.\nGSE1277115: This dataset consists of blood samples obtained from smokers participating in epidemiological trials such as the European Prospective Investigation into Cancer and Nutrition, as well as the Cosmos and BC trials based in Cologne, Germany. All samples are extracted from patients either diagnosed with incident cases within the EPIC trial or prevalent cases within the Cosmos and BC trials. These samples contain a blood-based signature intended to differentiate between individuals diagnosed with lung cancer and unaffected smokers.\nGSE428309: The dataset in this study comprises patients diagnosed with various pulmonary diseases, including tuberculosis, sarcoidosis, pneumonia, and lung cancer, as well as healthy controls. The majority of TB patients were recruited from the Royal Free Hospital NHS Foundation Trust in London, while sarcoidosis patients were recruited from multiple hospitals in London, Oxford, and Paris between September 2009 and March 2012. Furthermore, Pneumonia patients were recruited from the Royal Free Hospital in London, and lung cancer patients and a subset of TB"}, {"title": "Methodology", "content": "Meta-Learning16 in the context of machine learning is known as \u201clearning-to-learn\u201d. The concept involves using multiple datasets, referred to as \"source\" datasets, in the training process of the \"target\" dataset we aim to evaluate. Notably, the sources and target datasets have similarities with each other in their domain or prediction task. The meta-learning approach enables the model to update its weights based on diverse datasets and domains. Consequently, the neural networks gain enhanced generalization capabilities across both source and target datasets. The ultimate objective of meta-learning is to enrich the model designed for the target task with knowledge extracted from source datasets. This strategy was proved particularly advantageous when dealing with limited available data for the target task. By incorporating information from source datasets, the model achieves acceptable performance output with better results than just using the target dataset while disregarding the information embedded in the source datasets.\nIn this research, the datasets comprise gene expression levels obtained from Illumina sensors, with features indicating the gene expressions and labels denoting whether the sample exhibits cancer. Each dataset represents a distinct subset of individuals, such as smokers and non-smokers. To address this variability, we propose employing meta-learning as a mechanism for knowledge transfer between various distributions of samples. As illustrated in Figure 1, our methodology involves the initial collection of data from multiple source datasets, denoted as $D_{si}$, where $i \\in (1, 2, ..., |S|)$ and $S$ represents the total number of source datasets. Additionally, a target dataset $D_T$ is incorporated into the analysis. Subsequently, a feature selection approach is employed for dimension adaptation, data normalization, and feature processing. Following feature selection, both source samples and target dataset are fed into a neural network for weight updates. To optimize this process, a meta-learning-based approach is proposed to enhance the model's generalizability across diverse tasks and help the model quickly adapt using a small number of data samples. Once the model is trained, it is utilized for predicting outcomes in the target task. In the following, we provide a detailed discussion about each sub-module of the proposed method."}, {"title": "Feature Selection", "content": "Each sample in the datasets comprises a prediction label $y_M$, showing if the cancer is detected or not, and a series of gene expression values $I_{j,m} = (I_{1,M}, I_{2,M}, ..., I_{d_M, M})$, where $j$ is the index of the sample, M represents either the source data, $D_{si}$, or the target dataset, $D_T$, with $d_M$ being the number of features for dataset M. The variability in\n$I_{bg}^M = BioGrid(I^M)$\n(1)\nNext, we employ standard normalization to obtain normalized features to enhance embedding learning as per Equation 2, where $\\mu_m$ represents the mean of $I^M$ samples, and $\\sigma_M^2$ is the standard deviation.\n$\\widehat{I}_{j,M} = \\frac{I_{j,M} - \\mu^M}{\\sigma_M^2}$\n(2)\nTherefore, by applying Equations 1 and 2, we extract datasets $D_{\\bar{T}}$ and $D_{\\bar{s}_i}$ for $i \\in (1,2,......,|S|)$ from $\\widehat{I}_M$. These processed datasets are subsequently employed in the deep learning model for further analysis."}, {"title": "Classifier", "content": "After obtaining processed data containing selected features for all datasets, the next step involves employing a classifier to extract embeddings from these features layer by layer. These embeddings are then utilized to predict whether the input sample corresponds to cancer or not. In this study, we use Multi-Layer Perceptron (MLP), Convolutional Neural Networks (CNNs), and Transformer's Encoder models for the meta-learning task. Below, we provide the formulations for all approaches:\nMulti-Layer Perceptron (MLP): The MLP method comprises multiple linear layers to output a prediction score. Despite its apparent simplicity, MLPs prove effective when handling datasets with a small number of samples. The formulation for a linear layer is demonstrated in Equation 3, where b denotes the bias term, LeakyRelu is the activation function, $W_{MLP} \\in \\mathbb{R}^{d_{output} \\times d_{input}}$, $d_{output}$ represents the output dimension of the layer, and $d_{input}$ represents the input dimension.\n$h = LeakyReLU(W_{MLP}X + b)$\n(3)\nThe stacked linear layers are followed by an output layer with a sigmoid activation applied to obtain the prediction value, as shown in Equation 4, where $W_{output} \\in \\mathbb{R}^{1 \\times d_{last}}$, $d_{last}$ equals to the output dimension of the last hidden layer, $h_{final} \\in \\mathbb{R}^{d_{last}}$ and $\\hat{y}$ represents the prediction score between 0 and 1. A score closer to 1 indicates a higher likelihood of the sample having cancer.\n$\\hat{y} = Sigmoid(W_{output}h_{final})$\n(4)\nConvolutional Neural Networks (CNNs): CNNs have been applicable in extracting meaningful features through the multiple layers of filters with kernels for prediction tasks. Typically, CNN architectures incorporate convolutional"}, {"title": "Loss Function", "content": "The mentioned neural networks have various parameters $\\Theta$, which undergoes updates throughout the training process. $\\widehat{I}_M$ is the input of the networks and the output is determined using Equation 9, where $\\hat{y}$ represents the predicted score related to lung cancer.\n$\\widehat{y}_j = f(\\widehat{I}_{j,M}; \\Theta)$\n(9)\nThe predicted output is further utilized in a binary cross entropy loss using Equation 10, where $N_M$ denotes the total number of samples in a batch and $L(\\Theta)$ equals to the loss value.\n$L(\\Theta) = \\frac{1}{N_M} \\sum_{i=1}^{N} ((y_i) log (\\hat{y_i}) + (1 - y_i) log (1 - \\hat{y_i}))$\n(10)"}, {"title": "Meta-Learning", "content": "Inspired by MedPred20, MetaCare++21 and Model-Agnostic Meta-Learning (MAML) approaches13, this research proposes a meta-learning approach to enhance the optimization of neural network weights with both source datasets and the target dataset. The proposed approach contributes to the development of a generalized model, especially when the target dataset has a limited number of samples.\nThe process initiates with a uniform random sampling of a batch from all source datasets. Next, for each source batch, new weight values are extracted using Equation 11, where $\\alpha$ is the learning rate, and $\\Theta_{\\bar{s_i}}$ denotes the updated weights for source $j$. Stochastic Gradient Descent (SGD) is utilized for the optimization due to the statistical structure of the process.\n$\\Theta_{\\bar{s_i}} = \\Theta - \\alpha \\nabla L_{\\Theta} ((\\widehat{I}_{si}, y_{si}) \\in D_{si})$\n(11)\nThe updated set of weights for each source dataset is used to compute the overall source loss value, as defined in Equation 12, where $L_{s_i}$ contains the influence of sources datasets, and $y_{si}$ denotes the cancer label.\n$L^*_{\\Theta} = \\frac{1}{|S|} \\sum_{(\\chi_i, y_i)\\in D_{s_j}} Los((\\widehat{I}_{si}, y_{si}) \\in D_{si})$\n(12)\nWhile $L^*_{\\Theta}$ encapsulates information from the entire source dataset, it lacks consideration for the adaptability of the target dataset. To address this, we initially compute the loss value specific to the target dataset batch, employing Equation 13. Here, $L_T$ represents the loss value corresponding to the target dataset. Notably, the model without any meta-learning would consider using only this loss value for the optimization.\n$L^*_{\\Theta} = L_{\\Theta} ((\\widehat{I}_T, y_T) \\in D_T)$\n(13)\nTo combine the influences of both source and target datasets, the ultimate meta-loss is computed using Equation 14, with $\\lambda$ representing a scaling factor to regulate the impact of the source loss, and $L^{Meta}_{\\Theta}$ denotes the weighted final loss value.\n$L^{Meta}_{\\Theta} = \\lambda L^*_{\\Theta} + (1 - \\lambda)L^*_{\\Theta}$\n(14)\nFinally, the meta-loss is employed to optimize the weights of the proposed method using Equation 15, where $\\beta$ is the learning rate, and the Adam optimizer is utilized for the optimization. This final optimization process refines the model parameters based on the comprehensive meta-loss value, enhancing the overall generalization of the proposed approach. For clarity, we present the evolution of the loss values in Figure 2. The figure illustrates that the change in loss values is a combination of various source losses and the target loss.\n$\\Theta = \\Theta - \\beta \\nabla L^{Meta}_{\\Theta}$\n(15)\nOur proposed approach offers two primary advantages: (1) in scenarios with limited data, the source loss aids the model in converging towards a more generalized network by decreasing the impact of noise and addressing the low-resource dilemma; (2) within the scope of this research, each dataset has samples from different types of patients and the meta-learning approach proves valuable in guiding the model toward achieving broad applicability across all patients with various health conditions."}, {"title": "Experiment Design", "content": "To address the following questions, we conducted experiments to validate the performance of our proposed method. In each experiment, one dataset is used as a target, and others are utilized as source datasets in the context of meta-learning. Our evaluation methodology employed 10-fold cross-validation, wherein 10% of the samples were reserved for testing and the remaining 90% were utilized for training. In each metric, we calculate the average of the values over all of the folds. For the CNN architecture, we employed two layers of convolution and max-pooling. The convolutional layers use a kernel size of 3, a stride of 1, and padding set to 1. The max-pooling layers have a kernel size of 2 and a stride of 2. In MLP, we employed two hidden linear layers, and in Transformer's encoder, we utilized one layer of self-attention mechanism. All methods are trained using a equal to 0.0004 for meta-learning, with a momentum of 0.2, and $\\beta$ equals 0.0004 for total optimization. The models are trained for 40 epochs. Additionally, we explored a greedy approach to determine optimal hyperparameters, including number of convolutional channels (chosen from values 32, 64, and 128), batch size (selected from values 32 and 64), embedding dimension (chosen from values 32, 64, and 128), and $\\lambda$ values (selected from 0.1, 0.3, 0.5, 0.7, 0.9, and 1). The best-performing values for each dataset are reported. In evaluating our model's performance, we employ five essential classification metrics: Accuracy, Precision, Recall, F1-score, and PRAUC (area under the precision-recall curve). We release the implementation code\u00b9 on Github."}, {"title": "Baselines", "content": "We conduct performance comparisons using both statistical and deep learning baseline methods. Our statistical baseline methods include the following:\nLogistic Regression is a statistical model to predict the probability of an event occurring based on input features, and employing a logistic function to map inputs to the probability.\nDecision Tree is a hierarchical model for decision-making, where each internal node represents a decision based on an input feature, leading to classifications represented by leaf nodes.\nSupport Vector Machine (SVM) classifies data by finding the hyperplane that best separates different classes in the dataset while maximizing the margin between them."}, {"title": "Results", "content": "RQ1. Comparison with Baseline Methods: We evaluate the performance of our model across a variety of datasets to showcase the effectiveness of our proposed methodology. Notably, for methods without meta-learning, only the target datasets are utilized for optimization and sources are discarded. As shown in Tables 2, 3, 4, and 5, our meta-learning approach consistently surpasses traditional statistical methods, showing its efficacy in managing low-resource gene data. In all datasets, the Transformer's encoder outperformed both MLP and CNN architectures. Regarding statistical baselines, Random Forest and Support Vector Machines (SVM) demonstrate superior performance over decision trees and logistic regression because of their adaptability to high-dimensional feature spaces. Furthermore, our findings on datasets GSE12771 and GSE42830 show the ability of meta-learning to generalize effectively even when dealing with limited sample sizes (fewer than 100 instances). This is achieved by combining information from datasets with more samples. Thus, our results suggest that the proposed meta-learning approach is applicable to tackling the challenges of this domain.\nRQ2. Ablation Studies: The meta-learning approach is employed in the optimization process of deep neural networks. To assess the effectiveness of this approach, we excluded meta-learning for MLP, CNN, and Transformer's encoders. As shown in Tables 2, 3, 4, and 5, meta-learning consistently enhances performance across all architectures. Particularly, The improvement is more pronounced on smaller datasets (Tables 4 and 5) because of the utilization of information from larger datasets, which decreases the risk of overfitting issues and leads to a better generalization.\nWe also assess the level of meta-learning impact shown in Figure 3. Across various datasets, we observe a performance peak for each architecture corresponding to a specific $\\lambda$ value. This suggests a trade-off between leveraging solely the source data or incorporating the target data should be considered. A peak at associate $\\lambda$ value near 0 indicates a stronger reliance on the source data. For instance, in the case of GSE135304 and GSE13255, the peaks are at x = 0.1 and 0.2 showing that the distribution of the source data better suits the cancer detection task for this dataset. Additionally, we utilize SHAP values22 to identify the important features contributing to the decision-making process. Figure 4 illustrates the SHAP plot generated for the Transformers model applied to dataset GSE135304. Notably, the meta-learning approach changes the feature ranking and their respective impacts influenced by source datasets."}, {"title": "RQ3. Transfer Learning", "content": "Transfer learning23 has been introduced to leverage information from large datasets and fine-tune it to fit datasets with limited samples for better generalization. In transfer learning, a model is initially trained on source datasets and then fine-tuned on target datasets. We choose two datasets, GSE13255 and SE135304 for the transfer learning task as targets. As shown in Tables 6 and 7, meta-learning consistently outperforms transfer learning results in all of the datasets. This superiority is because of the fact that transfer learning heavily relies on the source task in the first stage and it separates the optimization for sources and the target. However, our approach includes the target task throughout the training process. Moreover, the results indicate that meta-learning exhibits greater generalization capabilities for cancer detection tasks based on gene expression levels compared to transfer learning."}, {"title": "Conclusion", "content": "In our research, we present a meta-learning framework for genetic datasets with gene expression levels from DNA microarrays. This method improves neural network optimization by integrating similar domain datasets for enhanced generalization of smaller datasets and robust performance across diverse patient samples from four distinct datasets. Our approach outperforms statistical methods, deep learning techniques, and transfer learning across all datasets."}]}