{"title": "A Case Study of Web App Coding with OpenAI Reasoning Models", "authors": ["Yi Cui"], "abstract": "This paper presents a case study of coding tasks by the latest reasoning models of OpenAI, i.e. o1-preview and o1-mini, in comparison with other frontier models. The o1 models deliver SOTA results for WebApp1K, a single-task benchmark. To this end, we introduce WebApp1K-Duo, a harder benchmark doubling number of tasks and test cases. The new benchmark causes the ol model performances to decline significantly, falling behind Claude 3.5. Moreover, they consistently fail when confronted with atypical yet correct test cases, a trap non-reasoning models occasionally avoid. We hypothesize that the performance variability is due to instruction comprehension. Specifically, the reasoning mechanism boosts performance when all expectations are captured, meanwhile exacerbates errors when key expectations are missed, potentially impacted by input lengths. As such, we argue that the coding success of reasoning models hinges on the top-notch base model and SFT to ensure meticulous adherence to instructions.", "sections": [{"title": "Introduction", "content": "The recent release of OpenAI reasoning models (01-preview and o1-mini)[OpenAI, 2024] presents a groundbreaking direction for model development, along with their SOTA performance in several challenging benchmarks, including math[Zhang et al., 2023], scientific research[Rein et al., 2023], competitive programming[Mirzayanov, 2009].\nIn this report, we evaluate ol models in the context of practical software development, i.e. when models are required to implement simple web apps satisfying specific requirement[Cui, 2024b]. Our benchmarks have the following characteristics and challenges.\n\u2022 The problem is less explorational and more results-oriented than other benchmarks. The specific instructions are laid out in the form of test setup and expectations.\n\u2022 No external knowledge is required to complete the task, since React is a prominent fram-work with sufficient code circulating on Internet for a decade.\n\u2022 Some expectations are less explicit or less typical than others, which could cause model negligence or misunderstanding.\nWe use a single-task benchmark (WebApp1K) and a duo-task benchmark (WebApp1K-Duo), and find the models perform with vast variability. Under the single-task evaluation, 01 models achieve new SOTA and unlock challenges never solved by non-reasoning frontier models. But under the duo-task evaluation, 01 models perform worse than Claude 3.5, and consistently fail under specific test format.\nWe attempt to gain insights into o1 behaviors by deep diving into a few problems they succeed or fail at. We find the reasoning steps play critical role in both success and failure. Since reasoning tokens are invisible in OpenAI API, we share reasoning steps obtained from ChatGPT reeactment,"}, {"title": "Single-Task Benchmark", "content": "We start with model performances on the WebApp1K benchmark. As illustrated in Tab. 1, each challenge of the benchmark focuses on a single task described by two test cases, one success and one failure. The task is about completing an atomic action (e.g. submitting a form, retrieving all posts), involving user interactions and access to a mocked API. More details of the benchmark can be found at [Cui, 2024b].\nThe prompt is straightforward: we feed test files to the model, expecting it to generate code passing these tests.\nGenerate TaskA.js to pass the tests below:\n{Tab. 1(a)}{Tab. 1(b)}. RETURN CODE ONLY.\n(1)\nThe resulting lines of code is typically between 40 and 50."}, {"title": "Results", "content": "Due to budget constraints, we only obtained pass@1 results for the ol models. Nevertheless, as shown in Tab. 2, they demonstrate impressive performance, lifting SOTA by 7%.\nAs part of this achievement, the two ol models unlock a total of 16 challenges never solved by previous non-reasoning models. Next, we pick two examples to illustrate how reasoning models solve them."}, {"title": "Example One: Placeholder Text", "content": "The first example is the postEditing problem under the Social Media category. In Tab. 3, we list the key steps to build up expectations of this problem. In particular, we highlight the step non-reasoning models overlooked.\nFirst, the fetch Mock statement sets up a mocked API. Then, fireEvent statements simulate user actions in two events: state change (value insertion) to an UI element carrying an Edit string, fol-lowed by a click event to an UI element carrying a Save string. Finally, expect statements outline the expectations that the mocked API must be accessed exactly once, and the success response from the API must be present in the webpage.\nFor this problem, most non-reasoning models capture the semantics and deliver functioning code. Specifically, to support user actions, they implement a form element for user input, and a save button for the click event.\nHowever, they forget to explicitly attach the Edit string to the form element, without which fireEvent cannot locate the correct element in the test webpage. There are two possible causes for the failure. First, the Edit token is synonymous with the purpose of the form element, which is also to edit. Second, the popular in-place editing implementation (prevelant in pretraining dataset) does not require an Edit string to state the purpose of the form element, which is overkill.\nOn the other hand, the o1 models stick to the requirement by attaching Edit to the form element as a placeholder text, via a textarea attribute (ref or value). Below is the ChatGPT reasoning chain, in which steps specifically reasoning Edit is blackened."}, {"title": "Example Two: Frontend Validation vs Backend Validation", "content": "The second example is the ticketSubmission problem under the Customer Support category. Tab. 4, lists the key steps of the test setup and expectations. We blacken the step which trapped non-reasoning models.\nSimilar to the same sequence in Tab. 3, the mocked API is first setup, followed by simulated user action, then expectations on API access and error message.\nAgain, non-reasoning models understand the semantics, write functioning code, but fail expectations. The root cause here is the string Title is required, which is akin to a technique not requiring API access, aka frontend validation. As a best practice (hence prevelance in pretraining dataset), frontend valiation is lightweight and fast, therefore preferred over backend validation. As such, all non-reasoning models are misled to implement frontend validation instead of expected behaviors which is backend validation.\nOn the other hand, o1 models discover the unpopular yet correct implementation: unconditionally visit the API, and output the Title is required error message upon a 400 response. Below is the ChatGPT reasoning chain, in which steps reasoning the 400 response is blackened.\nThe most crucial step here is Refining the approach. Below is its detailed wording.\nI'm updating the code to ensure a fetch request is always sent, even without a title.\nThe server will respond with a 400 status if the title is absent.\nEvidently, the step before it (Writing test cases) conducted certain verification, which leads the model to pivot to the right path."}, {"title": "Counter Example", "content": "Unfortunatelly the reasoning models can also fall for the same trap. Below is a ChatGPT reasoning chain leading o1-preview to the faulty implementation like previous models.\nOn a closer look, step Customer service improvement derails the model from backend validation to frontend validation.\nI'm thinking about creating a TicketSubmission component with\na 'Title' input and 'Submit' button. Submitting the form will trigger\na POST request to '/api/tickets', validating the 'Title' field before submission.\nMore interestingly, the step Verifying form submission does not correct the wrong direction, but solidify it.\nI'm thinking about how the form ensures 'Title' must be filled.\nIt sends a POST request if 'Title' is entered, showing success\nor 'Title is required' based on the response status.\nWith these superficial clues, we speculate that the derailing is due to preemption of original expecta-tions by model's inherent knowledge. The subsequent verification step is derived from neighboring steps already derailed, instead of orginal expectations only accessible from the input tokens."}, {"title": "Duo-Task Benchmark", "content": "In light of ol models' superb performance to saturate the single-task benchmark, we propose WebApp1K-Duo[ONEKQ, 2024c], a more difficult benchmark. Under each category of WebApp1K, we randomly pair up two atomic tasks into a duo task. The benchmark still consists of 1000 tasks, with 50 for each category. Models are challenged on both longer input, i.e. twice as many test cases, and longer output, i.e. more implementation in one module to meet all expectations.\nWebApp1K-Duo is composed in two ways. The first way is shown in Tab. 5 (a), in which the original export name of WebApp1K is preserved as is. The second way is shown in Tab. 5 (b), where the export names are normalized to a unified name App."}, {"title": "Results", "content": "We collect pass@1 results under both raw and normalized formats. Unfortunately, 01 models' per-formances on the new benchmark are not impressive, falling behind other frontier models, especially Claude 3.5.\nAs shown in Tab. 6, all models struggle with the raw format (Tab. 5 (a)). Most strikingly, 01 models fail all problems. We will try to find the root cause in Sec. 3.2.\nIn Tab. 7, performance of all models are greatly improved under the intuitive normalized format (Tab. 5 (a)). The SOTA is owned by Claude 3.5."}, {"title": "Example One: Default Export vs Named Export", "content": "In the raw format illustrated in Tab. 5 (a), there are two imports of different names, i.e. TaskA and TaskB. But they are actually default imports (without curly braces) which are name-agnostic. Also since only one default export is allowed per module, this format is in fact semantically equivalent to the normalized format in Tab. 5 (b). Both formats demand the models to build a single module implementing all expectations, with a single default export. To help readers understand related concepts, we explain JavaScript export rules in Tab. 8.\nTab. 9 collects different ways models cope with this challenge. Tab. 9 (d) is the only right answer, but also the least straightforward, challenging the intuition trap that two exports from two separate modules are needed. Both non-reasoning and reasoning models fall for the trap and attempt to split the implementation into two modules, (Tab. 9 (a), (b), (c)), resulting in very high failure rates.\nNext, we try to understand why non-reasoning models occasionally succeed by following the pattern of Tab. 9 (d), but non-reasoning models never do so. We suspect that the normalized format (Tab. 5 (b)) definitely dominates the pretraining/posttraining dataset, but does not exclude the raw format (Tab. 5 (a)), as well as the matching solutions. This makes the success possible.\nOn the other hand, from the first reasoning step which often plays the role of planning, reasoning models commit to the wrong judgment, and do not get a chance to correct the course in subsequent steps. Below is the detailed wording of the first reasoning step from a ChatGPT reeactment.\nTo progress, the key task is creating components TaskA and TaskB in TaskA_B.js to ensure all tests are successfully passed.\nComparing to the mistakes made in Sec. 2.3.1, the mistake in the above step covers a larger scope. It is reasonable to argue that mistakes made in large-scoped steps are more fatal and harder to correct."}, {"title": "Example Two: Ignored Expectation", "content": "We now try to study why ol models perform worse than Claude 3.5 under the normazlied format.\nTab. 10 shows a problem solved by Claude 3.5, but failed by ol-preview.\nHere, ol-preview passes all tests but the last one. The output code neither attempt to catch the 500 error nor print out the Internal Server Error string. The reasoning chain is normal, and no step specifically mentions the need to catch internal server errors.\nAlso ol-preview's inherent coding ability is solid, because it solves the retrieveAllBlogPosts prob-lem when evaluated under the single-task benchmark. To this end, we suspect the root cause to be failure to pick up the expectation from input tokens, possibly due to length constraint. This mistake should be considered a matter of instruction following, which is applicable to both non-reasoning and reasoning models."}, {"title": "Related Works", "content": "The impressive achievements of reasoning models bulit on advancements from machine learning, re-inforcement learning, and cognitive science. On the learning side, self-play fine-tuning allows mod-els to generate their own data and iteratively refine their reasoning capabilities [Chen et al., 2024]. By engaging in self-play, models learn from successes and failures to convert weak performance into strong, well-aligned behavior[Zhang et al., 2024]. Self-taught reasoning methods use the model's own outputs to enable a bootstrapping process to improve future performance[Zelikman et al., 2022]. This is evident in the development of self-taught reasoners, where models analyze outcomes of their reasoning chains[Zelikman et al., 2024]. Reinforcement learning further augments this self-improvement process by allowing models to optimize their decision-making strategies via interac-tion with the running environment[Silver et al., 2017].\nOn the inference side, chain-of-thought reasoning trains models to generate intermediate steps that mirror human-like thought processes[Wang and Zhou, 2024, Lightman et al., 2023]. Inductive rea-soning and hypothesis search techniques enable models to explore a space of possible outcomes, making it excel at abstract reasoning tasks[Wang et al., 2024]. Advanced sampling methods, like re-peated sampling and tree search, enhance the model's capacity to handle uncertainty[Anthony et al., 2017]. Together, these strategies provide a robust framework for models to perform nuanced and sophisticated reasoning in a wide variety of tasks[Uesato et al., 2022].\nOn the evaluation side, more benchmarks have been proposed to focus on problem-solving capa-bilities in near-real-world environments. SWE-bench[Jimenez et al., 2024] provides a comprehen-sive suite targeting core software engineering activities such as code generation, completion, error detection, and debugging. BFCL[Yan et al., 2024] assesses models' ability to generate accurate function calls, including prompt interpretation and argument handling. BIRD[Gao et al., 2023] evaluates models' proficiency in translating natural language queries into SQL codes. The Aider Leaderboard[Aider, 2024] ranks models based on their performance in real-world programming tasks such as bug fixing, refactoring, and code completion."}, {"title": "Conclusions", "content": "This report studies the latest reasoning models by OpenAI in the context of writing code to specific test expectations. We see both exciting and discouraging results, and share our investigations to gain more insights, especially how reasoning influence the outcome. We further argue that OpenAI's top-notch base model and SFT are equally important to the success of reasoning models. We believe that further advancements in these existing directions will continue to enhance reasoning models' performance, both amplifying strengths and mitigating weaknesses.\nBelow are our thoughts on next steps.\n\u2022 We think the current SOTA of the duo-task benchmark (Tab. 6) is a good milestone for hill climbing. So we do not plan to add more test cases until the next significant leap.\n\u2022 We will look deeper into error logs. But it would be quite surprising if we discover new error patterns besides those already identified[Cui, 2024a].\n\u2022 We will incorporate more frameworks (e.g. Vue) and languages (e.g. Python) to increase the benchmark coverage."}]}