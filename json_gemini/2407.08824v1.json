{"title": "Proving that Cryptic Crossword Clue Answers are Correct", "authors": ["Martin Andrews", "Sam Witteveen"], "abstract": "Cryptic crossword clues are challenging cognitive\ntasks, for which new test sets are released on a\ndaily basis by multiple international newspapers.\nEach cryptic clue contains both the definition of\nthe answer to be placed in the crossword grid\n(in common with regular crosswords), and 'word-\nplay' that proves that the answer is correct (i.e. a\nhuman solver can be confident that an answer is\ncorrect without needing crossing words to confirm\nit). Using an existing cryptic wordplay proving\nframework (operating on Python proofs created\nby an LLM), we show that it is possible to distin-\nguish between correct answers and almost-correct\nones based upon whether the wordplay 'works'.", "sections": [{"title": "1. Introduction", "content": "Recent advances in computational models have significantly\nimproved their ability to handle diverse natural language\ntasks involving complex syntactic and semantic interpreta-\ntions. Despite these strides, machines continue to fall short\nof human performance in areas requiring flexible problem-\nsolving, swift adaptation to new tasks, and effective gener-\nalization across unfamiliar domains.\nThis gap is particularly evident in the domain of cryptic\ncrossword solving - a popular activity across the world, with\nmultiple papers in the UK, Australia, India and elsewhere\nfeaturing daily puzzles for readers to solve.\nThe domain of cryptic crossword solving has received little\nattention, despite being a notable language-oriented cogni-\ntive task, with solvers worldwide. One possible reason is\nthat cryptic crosswords are much less common in the United\nStates than 'regular crosswords'. Another possibility is that\ncryptic crosswords combine a challenging cross-discipline\nmix of advanced language processing capabilities, logical\nreasoning, and an 'Aha! moment'."}, {"title": "2. Related Work", "content": "2.1. Regular Crosswords\nNon-cryptic (\"regular\") crosswords are known throughout\nthe world, and are the predominant type found in newspapers\nin the U.S.A. One key difference from cryptic crosswords is\nthat regular crossword clues are generally not 'standalone' -\nthere may be a number of different answers that fit the given\nclue. The key to solving regular crosswords is thus the in-\nteraction between answers (i.e. the crossing-words), which\nallows for planning/backtracking to aid in breaking the com-\nbinatorial explosion of possibilities to achieve solving rates\nin the high 90% range.\n2.2. Cryptic Crosswords\nIn contrast to a regular crossword clue, a cryptic clue leads\nto its answer only if it is read in the right way. The clue\nitself contains both a conventional \u2018straight definition', and\nwordplay that can be used to derive the same answer. Once\na given clue is understood, a solver can enter it into the grid\nwith near 100% certainty, even on a standalone basis."}, {"title": "2.3. Cryptonite Dataset", "content": "The (UK) Times Cryptic Crossword is widely considered\nthe gold standard in puzzles, even though they are not neces-\nsarily the most difficult, because the clues are unusually well\nconstructed. Cryptonite is a large-scale\ndataset of Cryptic Crossword clues from the Times, con-\ntaining 523,000 naturally sourced clues from an extended\ntime-period, with the train, validation and testing splits cho-\nsen so that a given answer only appears in one of the splits."}, {"title": "2.4. Rule-based solvers", "content": "Williams & Woodhead is an early example of at-\ntempting to devise a formal language for describing cryptic\nclues. However, they found that the clues' linguistic ele-\nments tend to thwart such formal approaches.\nDeits used a more flexible rule-based solver\nwith a manually-crafted probabilistic grammar. Building\non the assumption that a clue can usually be split into a\nwordplay and a definition, the (brute-force) solver tries to\nfind the most probable parse such that the wordplay yields\na semantically-similar result to the definition. Reported in\nEfrat et al. (2021), the rule-based solver approach yields an\naccuracy of 8.6% on the Cryptonite test set."}, {"title": "2.5. LLM-based solvers", "content": "Cryptic crossword clues seemed like an idea target for\nBERT-era models. However, Efrat et al. reported that\na T5-Large model fine-tuned on Cryptonite's 470k cryptic\nclue training set achieved only 7.6% test set accuracy on the\ntest set (i.e. below that of rule-based solvers).\nInterestingly, present day (scaled) Large Language Models\nalso score very poorly on cryptic clues. This is likely due\nto (i) the misleading surface reading of the clues; (ii) the\nobliqueness of the definitions; and (iii) the reasoning steps\nrequired to prove the answer correct based on the wordplay\nthat each clue provides."}, {"title": "2.6. Code & reasoning", "content": "To compensate for LLMs only approximating the generation\nof logical reasoning, techniques like PAL exploit LLMs' facility for writing code to create verifiable\nreasoning chains. An important influence on this work was\nalso the Draft, Sketch, and Prove framework which uses an LLM to draft and create proofs that are\nthen verified formally.\nInformed by the evolution from AlphaCode, in which huge numbers of programs are generated and fil-\ntered in order to generate a valid solution, to AlphaCodium, in which solutions are iterated upon\nand involving much less computation, this work uses a"}, {"title": "3. Methods", "content": "3.1. Wordplay dataset\nThere are a number of websites where cryptic cross-\nword enthusiasts post completed puzzles, annotated with\ndefinition, wordplay and answer fields. In order\nto capture these key elements of cryptic crossword clue solv-\ning, we make use of a Wordplay dataset gathered from such\nsites.\n3.2. Language Model set-up\nIn our experiments, we make use of two Language Models.\nIn order to generate the definition and wordplay\nfields, we make use of the Llama-3-it 8B model, fine-tuned using LoRA to generate\ndefinition and wordplay annotations from the origi-\nnal clue and (importantly) a candidate answer. Training\non 5371 examples (with the prompt format as shown in\nAppendix C) took under 3 hours on a single GPU virtual\nmachine, using the unsloth package.\nTo create the python 'proofs' of the correctness of solu-\ntions, we use both Google's Gemini-Pro-1.0-002 and\nGemini-Flash-1.5-001 LLMs (pinned model ver-\nsions to enable a level of reproducibility).\nWhile the Llama model was found to be capable of rea-\nsonable guesses at correct definition and wordplay\nannotations, the creation (and iterative fixing) of the Python\nproofs required the use of more capable models.\n3.3. Hypothesis testing\nThe hypothesis tested in this work is whether it is pos-\nsible for the combination of Llama definition and\nwordplay generation; Gemini LLM formalisation; and\na Python-based prover to have sufficient 'power' to dis-\ntinguish between candidate answers (one of which is the\ncorrect answer). Ideally, the correct answer will lead to per-\nfect wordplay, which then can be translated into elegant\nPython code, while an incorrect candidate answer will lead\nto 'bizarre' wordplay, which in turn will be formalised\ninto Python that will be incapable of being proved.\n3.4. Obtaining a close candidate answer\nFor a given question, we use the Llama model to create\na definition and wordplay pair from the clue and\nthe ground-truth answer. We then use the span in the\ngenerated definition to create an alternative candidate\nanswer that both matches the pattern and is seman-"}, {"title": "3.5. Formalising and proving an answer", "content": "From a candidate answer, we use the Llama model to\ngenerate a definition and wordplay pair. We then\nuse the Gemini LLM to attempt to generate Python proofs,\nwhich are then verified using a Cryptic Crossword DSL\nexpressed via Python (see Appendix D for further details).\nThis process includes 're-writes' where the proof verifier\ncan return errors in response to assertion failures, along with\nhints about how these errors might be fixed. After the initial\ndraft proof, the verifier allows up to 5 re-write attempts to be\nmade - until the proof is either accepted or the verification\nprocess stops (i.e. no success after 5 re-writes).\nIn the case of the close candidate answer, the wordplay\nis likely to be rather nonsensical - the hypothesis being\ntested here is whether the formalisation process can reject\nclose candidate answers, in favour of the ground-truth an-\nswer."}, {"title": "4. Experiments", "content": "4.1. Distinguishing ground-truth answers from close\ncandidates\nFor each of 100 different clue examples from the Wordplay\ndateset, we use the ground-truth answer to generate 1 close"}, {"title": "5. Results", "content": "The results of testing the 'provability' hypothesis are shown\nin Table 1, where we show percentages of True Positive\n(ground-truth answer more provable), False Negative (non-\nground-truth answer more provable) and Draw (both an-\nswers proved to equal extents) across the different provabil-\nity measures, for each of the two Gemini models.\nClearly, the results suggest that the proving system has a\ndegree of preference towards correct answers, but is a long\nway from being a reliable oracle of answer correctness.\nThis points to an issue that would likely occur if the system\nwere scaled up to testing many candidate answers, rather\nthan just 2 possibilities here. Specifically, if the cryptic\ncrossword clue task were transformed to choosing between\na large number of potential candidates the current system\nwould likely start to become less accurate overall, since\nthe number of False Negative results would likely start to"}, {"title": "6. Limitations", "content": "The Prover does not detect a number of potential errors /\nproblems:\n\u2022 Cryptic crossword setting 'rules' dictate that the clues\nshould contain exactly enough to prove an answer, the\nprover does not check that all valuable words in the clue\nhave been utilised\n\u2022 Proofs may be logically disconnected, with left-hand-side\nterms not necessarily being connected to right-hand-side\nterms in other lines of the code.\n\u2022 Entire Python function consists of comments : Nothing\ntriggers assert\n\u2022 Python function contains conditional execution, routing\naround assert statements : Nothing triggers assert\n\u2022 Occasionally, the hint assert XYZ failed results\nin a re-write assert XYZ==False, which is cheating\nWith additional effort, the authors believe that these issues\nare surmountable. However, since the Gemini LLM is only\nbeing used In-Context, there currently is little chance that\nthe above issues are being systematically abused (which\nwould almost certainly happen if there were learning-in-the-loop in a Reinforcement Learning setting)."}, {"title": "7. Conclusions", "content": "It is increasingly hypothesised that the next-token-prediction\ntask may be insufficient to get machines to reason and plan. By framing the cognitive task of\ncryptic crossword solving as a reasoning problem that is\naddressable by LLMs supported by a verification system,\nthis work has sought to bring this reasoning task within the\nscope of what is tractable by systems that have components\nthat include LLMs as well as verifiers and coding aids.\nThe authors sincerely hope that this work sparks an inter-\nest in the cryptic crossword domain, since it presents a\nchallenging NLP/reasoning task, with huge scope for test-\ning different reasoning approaches. Notably, the current\nState-of-the-Art solving methods score less than 20% on a\nreal-world test set."}, {"title": "Impact Statement", "content": "There are many current cryptic crossword enthusiasts that\nwould potentially not welcome AI-enabled solvers to 'take\nover' their favourite pastime. In particular, when taken\nfurther, this line of work would be potentially disruptive to\npublic leaderboards that rank people according to the time\ntaken to solve puzzles 100% correctly.\nHowever, there is currently little risk of LLM cryptic solvers\nas being anything more than comic relief for current experts.\nNaturally, the authors also believe that the techniques here\nhave wider applications to the field of Machine Learning, but\nthey do not in themselves present any particular additional\nsocietal risk."}, {"title": "Bias towards English-language speakers", "content": "The English language has a high capacity for ambiguity and\nwordplay overall, making cryptic crosswords much more\nfeasible. However, they do exist in other languages - please\nsee the Cryptic Crossword Wikipedia page for a broader\nview of their worldwide prevalence. Note that deriving the\nanswers is very difficult (even for native English speakers), whereas understanding the answer from given wordplay is\nmuch simpler."}, {"title": "A. Cryptic Crossword Background", "content": "The following borrows extensively from the description on Wikipedia (2024) (kudos to the authors there), to which we have\nadded wordplay annotations in a notation typical of the FifteenSquare . com website (and in the Wordplay dataset\nuse in this work).\nA.1. Basics\nA cryptic clue leads to its answer only if it is read in the right way. What the clue appears to say when read normally (the\nsurface reading) is usually a distraction with nothing to do with the solution. The challenge is to find the way of reading the\nclue that leads to the solution.\nA typical clue consists of two parts:\n\u2022 The straight or definition. This is in essence the same as any non-cryptic crossword clue: a synonym for the answer. It\nusually exactly matches the part of speech, tense, and number of the answer, and usually appears at the start or end of a\nclue. For our annotations, the span that encompasses the definition is highlighted using curly braces.\n\u2022 The cryptic, subsidiary indication or wordplay. This gives the solver some instructions on how to get to the answer in\nanother (less literal) way. The wordplay parts of clues can be obscure, especially to a newcomer, but they tend to utilise\nstandard rules and conventions which become more familiar with practice.\nSometimes the two parts of the clue are joined with a link word or phrase such as 'from', 'gives' or 'could be'. One of the\ntasks of the solver is to find the boundary between the definition and the wordplay, and insert a mental pause there when\nreading the clue cryptically.\nWe list below several of the important styles of wordplay that are commonly used, each with an annotated example. For a\nmore comprehensive list, along with an outline of the \u2018Ximenean principles', please see Wikipedia (2024).\nA.2. Anagrams\nAn anagram is a rearrangement of a certain section of the clue to form the answer. This is usually indicated by a codeword\nwhich indicates change, movement, breakage or something otherwise amiss. For example:\nclue:\nChaperone shredded corset (6)\ndefinition: {Chaperone} shredded corset\nanswer:\nESCORT\nwordplay:\n(corset) * (*shredded)\nA.3. Charade\nIn a charade, the answer is formed by joining individually clued words to make a larger word (namely, the answer). For\nexample:\nclue:\nOutlaw leader managing money (7)\ndefinition: Outlaw leader {managing money}\nanswer:\nBANKING\nwordplay:\nBAN (outlaw) + KING (leader)\nA.4. Containers\nA container or insertion clue puts one set of letters inside another. For example (also starting to add a little more indirection):\nclue:\nUtter nothing when there's wickedness about (5)\ndefinition: {utter} nothing when there's wickedness about\nanswer:\nVOICE\nwordplay:\nO (nothing) with VICE (wickedness) around it (about)"}, {"title": "A.5. Deletions", "content": "Deletion is a wordplay mechanism which removes some letters of a word to create a shorter word. For example:\nclue:\nBird is cowardly, about to fly away (5)\ndefinition: {Bird} is cowardly, about to fly away\nanswer:\nRAVEN\nwordplay:\n[c] RAVEN (cowardly) - 'C' (i.e. circa, about) (-fly away)\nA.6. Double definition\nA clue may, rather than having a definition part and a wordplay part, have two definition parts. For example:\nclue:\nNot seeing window covering (5)\ndefinition: {Not seeing} {window covering}\nanswer:\nBLIND\nwordplay:\nDouble Definition (DD)\nA.7. Hidden words\nWith hidden word clues, the solution itself is written within the clue \u2013 either as part of a longer word or across more than\none word. For example:\nclue:\nFound ermine, deer hides damaged (10)\ndefinition: Found ermine, deer hides {damaged}\nanswer:\nUNDERMINED\nwordplay:\n[fo] UND ERMINE D [eer) (hides)\nA.8. Homophones\nHomophones are words that sound the same but have different meanings, such as 'night' and 'knight'. Homophone clues\nalways have an indicator word or phrase that has to do with being spoken or heard. For example:\nclue:\nWe hear twins shave (4)\ndefinition: We hear twins {shave}\nanswer:\nPARE\nwordplay:\n\"pair\" (twins, \"we hear\")\nA.9. Reversals\nA word that gets turned around to make another is a reversal. For example:\nclue:\nReturned beer fit for a king (5)\ndefinition: Returned beer {fit for a king}\nanswer:\nREGAL\nwordplay:\n(LAGER)< (beer, <returned)"}, {"title": "B. Wordplay Dataset", "content": "The Wordplay Dataset used in this work is extracted from websites where cryptic crossword enthusiasts post solutions to the\npuzzles published in major publications. Each completed puzzle is annotated by an solver who provides the community with\ndefinition, wordplay and answer fields for each of the approximately 30 clues in that day's grid.\nFor UK papers, these enthusiast websites include:\n\u2022 timesforthetimes.co.uk - Times, Times Quick\n\u2022 www.fifteensquared.net - Independent, Guardian, Financial Times\n\u2022 bigdave44.com - Telegraph, Sunday Telegraph\nThe following is an example from the Wordplay dataset, formatted in YAML:\ntitle: Financial Times 16,479 by FALCON\nurl: https://www.fifteensquared.net/2020/05/18/ \\\nfinancial-times-16479-by-falcon/\nauthor: teacow\nclues:\nclue: '{Offer} of support also broadcast'\npattern: '8'\nad: D\nanswer: PROPOSAL\nwordplay: PROP (support) + (ALSO) * (*broadcast)\nIn the above:\n\u2022 clue is the original clue, as given to solvers, but with the 'regular crossword' definition portion highlighted with\ncurly braces;\n\u2022 pattern is the number of characters in the answer;\n\u2022 ad (across/down) is potentially significant, because some clues include directional hints such as 'before' or 'upwards'\nwhich are only meaningful if the orientation of the answer within the grid is known;\n\u2022 answer is the clue's final answer (not known to the solvers before solving); and\n\u2022 wordplay is an informally annotated explanation of how the clue words act together to logically build the letters in\nthe answer (the resulting grid letters typically being in upper case) - here the * symbol signifies that ALSO is to be\nanagrammed due to the anagram indicator (broadcast) in the clue."}, {"title": "C. Fine-tuning prompt", "content": "The following is a verbatim training example used for the fine-tuning of the Llama-3-it model:\n<|start_header_id|>system<|end_header_id|>\nCryptic clue wordplay generation: Given the clue and the answer, \\\nreturn expert definition and wordplay annotations<|eot_id|>\n<|start_header_id|>user<|end_header_id|>\nclue: \"musical and ballet, oddly, that can be avoided\"\nanswer: EVITABLE evitable<|eot_id|>\n<|start_header_id|>assistant<|end_header_id |>\ndefinition: musical and ballet, oddly, that can be avoided}\nwordplay: EVITA (musical) + B [a] L [1] E [t) (ballet, odd letters) <|eot_id|>\\\n<| end_of_text | >"}, {"title": "D. In-Context Learning Prompts for the Gemini LLM", "content": "The Gemini LLM is prompted in-context with the concatenation of the following sections:\n\u2022 Cryptic Crossword overview\n\u2022 Many-shot wordplay examples\n\u2022 Declaration of 'external' Python functions\n\u2022 6-shot formalisation demonstration\n\u2022 Actual problem statement (for continuation as a Python proof)\n\u2022 After a verification failure: Error messages for the generated proof, with hints if available, and request to improve\niteratively\nThe sections of the prompt are described more fully below, note that care was taken to ensure that the chosen terminology\nwas use consistently throughout.\nD.1. Cryptic Crossword preamble\nThe following is the rubric and wordplay preamble given to the Gemini LLM:\nA Cryptic crossword question involves using the words in \\\nthe given clue to yield an answer that matches the letter pattern.\nThe clue will provide a definition of the answer, as well \\\nas some 'wordplay' that can also be used to confirm the answer.\nExpert question solvers write informal 'proofs' using a \\\nparticular format.\nFor the definition, the original clue is annotated with \\\n'{}' to denote where the definition is to be found.\nFor the wordplay, the following conventions are loosely used:\n* The answer is assembled from the letters in CAPS\n* Words in brackets show the origins of letters in CAPS, \\\noften being synonyms, or short forms\n* Action words are annotated as illustrated:\n+ (ETO N) * (*mad = anagram-signifier) = TONE\n+ (FO OR)< (<back = reversal-signifier) = ROOF\n+ [re]USE (missing = removal-signifier) = USE\n* DD is a shorthand for 'Double Definition'"}, {"title": "D.2. Many-shot wordplay examples", "content": "Around 20 examples from the Wordplay dataset are included in the in-context prompt:\nFor example:\nclue: \"arrived with an artist, to get optical device (6)\"\ndefinition: arrived with an artist, to get {optical device}\nanswer: CAMERA\nwordplay: CAME (arrived) + RA (artist, short form)\nclue:"}, {"title": "D.3. External Python DSL functions", "content": "Domain Specific Python functions are described in-context to the LLM, which appears able to use them without seeing their\ninternal functionality. In fact, the actual implementation of the functions is more extensive than described, since calls to\nthese functions also track 'near misses' which can be fed back as hints during the re-write process.\nThe task is to produce a formal proof using python code, \\\nwhere the docstring will also include an informal proof as an aid.\nThe following are functions that can be used in your output code:\nAction=Enum('Action', 'ANAGRAM, REMOVE_FIRST, INITIALS, REMOVE_LAST,'+\n# External definitions\n'GOES_INSIDE, GOES_OUTSIDE, REVERSE, SUBSTRING, HOMOPHONE')\ndef is_synonym (phrase:str, test_synonym:str, pattern:str='') -> bool:\n# Determines whether 'test_synonym' is a reasonable synonym for 'phrase',\n# with letters optionally matching 'pattern'\ndef is_abbreviation(phrase:str, test_abbreviation:str) -> bool:\n# Determines whether 'test_abbreviation' is\n# a valid abbreviation or short form for 'phrase'\ndef action_type (phrase:str, action: Action) -> bool:\n# Determines whether 'phrase' might signify the given 'action'\ndef is_anagram (letters:str, word:str) -> bool:\n# Determines whether 'word' can be formed from 'letters' (i.e. an anagram)\ndef is_homophone (phrase:str, test_homophone:str) -> bool:\n# Determines whether 'test_homophone' sounds like 'phrase'"}, {"title": "D.4. Few-shot formalisation examples", "content": "The following are 3 (out of 6) of the few-shot formalisation examples given before the final test-case prompt:\nThe following are examples of simple functions that prove that \\\neach puzzle solution is correct:\n'''python\ndef proof (answer=\"ONCE\",\n\"\"\"\nclue=\"head decapitated long ago\", pattern='4'):\ndefinition: head decapitated {long ago}\nwordplay: (b) ONCE (head decapitated = remove first letter of BONCE)\n\"\"\"\nassert is_synonym(\"head\", \"BONCE\")\nassert action_type(\"decapitated\", Action.REMOVE_FIRST) \\\nand \"BONCE\"[1:]==\"ONCE\"\nassert is_synonym(\"long ago\", \"ONCE\", pattern='4')\nproof ()\n```python\ndef proof (answer=\"DECIMAL\",\nclue=\"the point of medical treatment\", pattern='7'):\n\"\"\""}, {"title": "D.5. Formalisation instruction", "content": "The following instruction is given before the final 'test-case' prompt illustrated in Figure 1:\n# Please complete the following in a similar manner, and return the whole function:\n```python\ndef proof (answer="}, {"title": "D.6. Proof Verification with Hinting", "content": "Examples of assertion failures, with constructive hinting, are shown:\nAssertionError: assert: is_abbreviation ('an Artist', 'RA') :\n'an Artist' does not have a valid abbreviation;\n'RA' is an abbreviation for : artist, artillery, Royal Artillery,\ngunners, painter\nAssertionError: assert action_type('goes crazy', Action.ANAGRAM) :\n'goes crazy' itself does not suggest Action.ANAGRAM, but 'crazy' does\nAssertionError: assert action_type('worked', Action.HOMOPHONE) :\n'worked' does not suggest Action.HOMOPHONE, but maybe Action.ANAGRAM\n# Please re-implement the SOLUTION above \\\n(altering both the docstring and the python code as required), \\\ntaking care to fix each of the problems identified, \\\nand return the whole function:\n'''python\ndef proof (answer=\nOnce the prover has fully parsed a given output with zero assertion failures, the proof is considered a success (up to 5\nre-write iterations are allowed, more that that is considered an overall failure to prove the answer)."}]}