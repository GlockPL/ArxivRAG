{"title": "Temporal Misalignment in ANN-SNN Conversion and Its Mitigation via Probabilistic Spiking Neurons", "authors": ["V. X. BojWu", "Velibor Bojkovi\u0107", "Xiaofeng Wu", "Bin Gu"], "abstract": "Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to Artificial Neural Networks (ANNs) by mimicking biological neural principles, establishing them as a promising approach to mitigate the increasing energy demands of large-scale neural models. However, fully harnessing the capabilities of SNNs remains challenging due to their discrete signal processing and temporal dynamics. ANN-SNN conversion has emerged as a practical approach, enabling SNNS to achieve competitive performance on complex machine learning tasks. In this work, we identify a phenomenon in the ANN-SNN conversion framework, termed temporal misalignment, in which random spike rearrangement across SNN layers leads to performance improvements. Based on this observation, we introduce biologically plausible two-phase probabilistic (TPP) spiking neurons, further enhancing the conversion process. We demonstrate the advantages of our proposed method both theoretically and empirically through comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet across a variety of architectures, achieving state-of-the-art results.", "sections": [{"title": "1. Introduction", "content": "Spiking neural networks (SNNs), often referred to as the third generation of neural networks (Maass, 1997), closely mimic biological neuronal communication through discrete spikes (McCulloch & Pitts, 1943; Hodgkin & Huxley, 1952; Izhikevich, 2003). While biological energy efficiency has historically influenced neural network design, SNNs uniquely achieve this through event-driven processing: weighted inputs integrate into membrane potentials, emitting binary spikes only upon crossing activation thresholds. This differs significantly from artificial neural networks (ANNs) (Braspenning et al., 1995), which are based on continuous floating-point operations that require energy-intensive and computationally costly multiplication operations (Roy et al., 2019). The spike-driven paradigm inherently circumvents these costly computations, suggesting that SNNs may offer a promising approach for energy-efficient AI. Recent developments in neuromorphic hardware (Pei et al., 2019; DeBole et al., 2019; loi; Ma et al., 2023) have enabled efficient SNN deployment. These specialized chips are inherently designed for spike-based computation, driving breakthroughs across domains: object detection (Kim et al., 2020b; Cheng et al., 2020), tracking (Yang et al., 2019), event-based vision (Zhu et al., 2022; Ren et al., 2024), speech recognition (Wang et al., 2023a), and generative AI through models like SpikingBERT (Bal & Sengupta, 2024) and SpikeGPT (Zhu et al., 2023; Wang et al., 2023b). These advancements underscore the potential of SNNs as a viable alternative to conventional ANNs.\nTraining SNNs is inherently challenging due to the same characteristics that confer their advantages: their discrete processing of information. Unsupervised direct training, inspired by biological learning mechanisms, leverages local learning rules and spike timing to update weights (Diehl & Cook, 2015). While these methods are computationally efficient and can be executed on specialized hardware, SNNs trained in this manner often underperform compared to models trained with alternative approaches. Further research is needed to better understand and improve this method. In contrast, supervised training can be categorized into direct training via spatio-temporal backpropagation (e.g., surrogate gradient methods (Neftci et al., 2019)) and ANN-SNN conversion methods (Diehl et al., 2015; Cao et al., 2015). The present work focuses on the latter approach.\nThe core idea of ANN-SNN conversion is to leverage pre-trained ANN models to train SNNs. This process begins by transferring the weights from the ANN to the SNN, which shares the same architecture, and initializing the spiking neuron parameters (e.g., thresholds, time constants) so that the spike rates approximate the activation values of the corresponding ANN layers. This method is advantageous because it typically requires no additional computation for training the SNN, thereby eliminating the need for gradient computation or limiting it to fine-tuning the SNN.\nThe initial experiment: Temporal information in SNNs after ANN-SNN conversion. We begin by identifying a counter-intuitive phenomenon that, to the best of our knowledge, has not been previously documented. We investigate the extent to which individual spike timing affects the overall performance of the SNN model in ANN-SNN conversion. We explore this question in the context of various types of conversion-related issues described in the literature, such as phase lag (Li et al., 2022) and unevenness of the spike input (Bu et al., 2022c), where prior work has indicated that the timing of spike trains in SNNs obtained through ANN-SNN conversion may not be optimal and can lead to performance degradation under low-latency conditions.\nIn our initial experiment, we began by examining several widely-used ANN-SNN conversion methods and analyzing the spike outputs of spiking layers in the corresponding SNN models. To evaluate the importance of spike timing relative to firing rates, we randomly permuted the spike trains after each spiking layer. Specifically, when processing samples through the baseline model, we rearranged the temporal order of spikes within each spike train after each spiking layer (see Figure 2). The permuted spike trains were then passed to the subsequent layer, and this process was repeated for each layer until the output layer. The results of one of these initial experiments, comparing the performance of the \u201cpermuted\u201d model with the baseline, are presented in Figure 1. For each latency, we conducted multiple trials with different permutations, and the \u201cpermuted\u201d model consistently outperformed the baseline, achieving the original ANN accuracy at lower latencies. The impact of permutations on performance was particularly pronounced at lower latencies.\nWe refer to this phenomenon as \u201ctemporal misalignment\u201d in ANN-SNN conversion and further investigate it by providing a conceptual interpretation in the form of bursting probabilistic spiking neurons, which are designed to mimic the effects of permutations in SNNs. The proposed neurons operate in two phases, as illustrated in Figure 3. In the first phase, they accumulate input, often surpassing the threshold, while in the second phase, they emit spikes probabilistically with varying temporal probabilities. Our proposed spiking neurons are characterized by two key properties: (1) the accumulation of membrane potential beyond the threshold, resulting in a firing phase, commonly referred to as bursting, and (2) probabilistic firing. Both properties exhibit biological plausibility and have been extensively studied in the neuroscience literature (see Section 3.3).\nThe main contributions of this paper are summarized as:\n\u2022 We recognize and study the \"temporal misalignment\" phenomenon in ANN-SNN conversion, and we propose a framework for its exploitation in ANN-SNN conversion utilizing two-phase probabilistic spiking neurons. We provide theoretical insights into their functioning and superior performance, as well as support for their biological grounding.\n\u2022 We present a comprehensive experimental validation demonstrating that our method outperforms SOTA conversion as well as the other training methods in terms of accuracy on CIFAR-10/100, CIFAR10-DVS, and ImageNet datasets."}, {"title": "2. Background and Related Work", "content": "The base model employed in this work is Integrate-and-Fire (IF) spiking neuron, whose internal dynamics, after discretization, are given by the equations\n$v^{(l)}[t] = v^{(l)}[t - 1] + W^{(l)}s^{(l-1)}[t] \u2013 \\theta^{(l)}s[t \u2013 1],$\n$s^{(l)}[t] = H(v^{(l)}[t] \u2013 \\theta^{(l)}).$\nHere, $\\theta^{(l)}$ is the threshold, $H(\u00b7)$ is the Heaviside function, while the superscript $l$ denotes the layer in the SNN. Later, we will modify these equations and use more advanced neuron models. By expanding the equations over t = 1, ...,T, and rearranging the terms, we obtain\n$\\sum_{t=1}^T s^{(l)}[t] = \\sum_{t=1}^T \\Big(\\sum_{i=1}^t W^{(l)} s^{(l-1)}[t] - \\sum_{i=1}^t\\theta^{(l)} s^{(l)}[t]\\Big)$\n$+ \\frac{v^{(l)}[T] \u2013 v^{(l)}[0]}{T}.$\nOn the ANN side, the transformation between layers is given by\n$a^{(l)} = A^{(l)}(W^{(l)}a^{(l-1)}),$\nwhere $A^{(l)}$ is the activation function. The ANN-SNN conversion process begins by transferring the weights (and biases) of a pre-trained ANN to an SNN with the same architecture. By comparing the equations for the ANN outputs (5) and the average output of the SNN (3) (Rueckauer et al., 2017a), the goal is to achieve a relation of the form\n$\\frac{1}{T} \\sum_{t=1}^T s_i^{(l)}[t] \\approx A_i^{(l)}.\\frac{1}{\u03b8_{th}^{(l)}}.$\nThe most commonly used activation function A is ReLU, due to its simplicity and non-negative output, which aligns well with the properties of IF neurons. It is important to note the importance of the three components in the conversion: 1) the threshold value \u03b8, 2) the initialization v[0], 3) the ANN activation function A."}, {"title": "2.1. Related work", "content": "ANN-SNN Conversion. This methodology aligns ANNS and SNNs through activation-firing rate correspondence, as initially demonstrated in (Rueckauer et al., 2017a; Cao et al., 2015). Subsequent research has systematically improved conversion fidelity through four principal approaches: (i) weight normalization (Diehl et al., 2015), (ii) soft-reset mechanisms (Rueckauer et al., 2017b; Han et al., 2020), (iii) adaptive threshold configurations (St\u00f6ckl & Maass, 2021; Ho & Chang, 2021; Wu et al., 2023), and (iv) spike coding optimization (Kim et al., 2020a; Sengupta et al., 2018). Recent innovations focus on ANN activation function adaptations, including thresholded ReLU (Ding et al., 2021) and quantization strategies (Bu et al., 2022c; Liu et al., 2022; Hu et al., 2023; Shen et al., 2024). However, these approaches introduce inherent accuracy-compression tradeoffs. Parallel efforts modify integrate-and-fire neuron dynamics (Li & Zeng, 2022; Wang et al., 2022a; Liu et al., 2022), with (Liu et al., 2022) proposing a dual-phase mechanism resembling our approach.\nCrucial for achieving high conversion efficacy, threshold initialization methodologies employ layer-wise activation maxima or percentile statistics (Rueckauer et al., 2017a; Deng & Gu, 2021a; Li et al., 2021a; Wu et al., 2024), augmented with post-conversion weight calibration (Li et al., 2021a; Bojkovic et al., 2024). Contemporary strategies can be categorized into two paradigms: (1) ANN activation quantization for temporal efficiency at the cost of accuracy, and (2) SNN neuron modification preserving ANN expressiveness with extended temporal requirements. Our methodology adheres to the second paradigm.\nDirect Training. This approach leverages spatio-temporal spike patterns through backpropagation-through-time with differentiable gradient approximations (O'Connor et al., 2018; Zenke & Ganguli, 2018; Wu et al., 2018; Bellec et al., 2018; Fang et al., 2021a;b; Zenke & Vogels, 2021; Mukhoty et al., 2024). Advancements encompass joint optimization of synaptic weights and neuronal parameters (threshold dynamics (Wei et al., 2023), leakage factors (Rathi & Roy, 2023)), novel loss formulations for spike distribution regularization (Zhu et al., 2024; Guo et al., 2022), and hybrid conversion-training pipelines (Wang et al., 2022b). State-of-the-art developments introduce ternary spike representations for enhanced information density (Guo et al., 2024) and reversible architectures for memory-efficient training (Zhang & Zhang, 2024)."}, {"title": "3. Methodology", "content": "3.1. Motivation\nIn ANN-SNN conversion methodologies, constant and rate coding are commonly utilized in the resulting spiking neural network models, based on the principle that the expected input at each time step matches the original input to the ANN model. Notably, the encoding lacks temporal information, as spike timing does not convey additional information. In constant encoding, this is evident, while in rate encoding, for a fixed input channel, the spike probability remains constant across all time steps, with the channel value assumed to lie between 0 and 1.\nThe resulting SNN model is initialized to approximate the outputs of the original ANN model based on the principle that, for each spiking neuron, the expected number of spikes it generates should closely match the output of the corresponding ANN neuron. In particular, it is assumed that no temporal information is present throughout the SNN model; that is, the spike train outputs of each SNN layer should convey no additional temporal information beyond spike firing rates.\nPrevious studies examining conversion errors and their classifications (Li et al., 2022; Bu et al., 2022c; Ding et al., 2021; Bojkovic et al., 2024) suggest that SNNs obtained through ANN-SNN conversion may generate spikes that are suboptimally positioned in the temporal domain, leading to a degradation in model performance, particularly at low latencies. Our initial experiments (see Introduction and Figure 1) further validate this observation while also revealing a novel insight: random temporal displacements of spike trains after spiking layers significantly enhance model performance. This phenomenon, which we refer to as temporal misalignment \u2013 wherein the original spike trains exhibit temporal misalignment, thereby impairing model performance \u2013 serves as the foundation and motivation for our proposed method, which is elaborated upon in the next section. Additional experiments on permutations in the ANN-SNN context, along with an explanation of their impact on model performance, are provided in Appendix G.\n3.2. From permutations to Bursting Probabilistic Spiking Neurons\nThis work aims to address the following question: How to incorporate the action of permutation of the output spike trains into the dynamics of the spiking neurons? We approach this problem in two steps.\nConsider the scenario where spike trains from layer $l$ are to be permuted. A general approach involves introducing a \u201cpermutator\u201d \u2013 a subsequent layer tasked with collecting all incoming spikes and re-emitting them in a permuted manner, as illustrated in Figure 2. This inherently implies the two-phase nature of the \u201cpermutator\u201d: specifically, during the first phase, incoming spikes are accumulated, and firing is deferred until the onset of the second phase, where spikes are emitted.\nThe second step focuses on the output mechanism of the \u201cpermutator\u201d. Specifically, it is desirable to design a spiking neuron mechanism that retains the stochastic component of the permutations. This consideration motivates the adoption of probabilistic firing in spiking neurons.\nThe final question we explore is whether a more compact approach can be achieved by employing probabilistic spiking neurons that aggregate weighted inputs from the previous layer, rather than directly processing spikes from a spiking layer.\nTPP neurons: To address the aforementioned questions, we propose two-phase probabilistic spiking neurons (TPP) (see Figure 3). Specifically, in the first phase, the neurons will only accumulate the (weighted) input coming from the previous layer, while in the second phase, the neurons will spike. More precisely, suppose that at a particular layer $l$ the spiking neurons accumulate the whole output of the previous layer, without emitting spikes. Denote the accumulated membrane potential by $v^{(l)}[0]$. The subsequent spiking phase is governed by the following equations:\n$s^{(l)}[t] = B(\\frac{\u03b8^{(l)} . (T \u2013 t + 1)}{T}.v^{(l)}[t\u22121]),$\n$v^{(l)}[t] = v^{(l)}[t - 1] \u2013 \u03b8^{(l)} . s^{(l)}[t],$\nwhere t = 1,...,T. Here, B(x) is a Bernoulli random variable with bias x, extended for $x \u2208 R$ in a natural way (B(x) = B(max(min(x, 1), 0))). If the weights of the SNN network are not normalized, the produced spikes will be scaled with the thresholds $\u03b8^{(l)} . s^{(l)}[t]$, before being sent to the next layer.\nWe observe that the presence of T - t + 1 in the denominator of the bias in B demonstrates that the probability of spiking depends not only on the current membrane potential, but also on the temporal step: in the absence of spiking, for the same membrane potential, the probability of spiking increases through time. Figure 3 provides a visual representation of the functioning of TPP neurons.\nThe following theorem provides a comprehensive characterization of the functioning of TPP neurons and their applications in ANN-SNN conversion when approximating ANN outputs (here $ReLU0(x) = min(ReLU(x), 0)$).\nTheorem 1. Let $X^{(l)}$ be the input of the ANN layer with ReLU activation and suppose that, during the accumulation phase, the corresponding SNN layer of TPP neurons accumulated $T . X^{(l)}$ quantity of voltage.\n(a) For every time step t = 1,..., T, we have\n$E[\\sum_{i=1}^t s_i^{(l)}[i]] = ReLU^{(l)}(X^{(l)}). \\frac{\u03b8^{(l)}}{(T-t+1) - \u03b8^{(l)}}$.\n(b) Suppose that for some t = 1,...,T, the TPP layer produced $s^{(l)}[1], . . ., s^{(l)}[t \u2013 1]$ vector spike trains for the first t \u2013 1 steps, and the residue voltage for neuron i is higher than zero. Then,\n$E[\\sum_{i=1}^{t-1} s_i^{(l)}[i] + T \\frac{s[i]}{(T - t + 1)-\u03b8^{(l)}} ]= ReLU^{(l)}(X^{(l)}).$\n(c) If $s^{(l)}[1], ..., s^{(l)}[T]$ are the output vectors of spike trains of the TPP neurons during T time steps, then\n$\\frac{\u03b8^{(l)}}{T}  \\sum_{i=1}^T s[i] = ReLU^{(l)}(X)),$\nif $ReLU\u03b8^{(l)} (X))$ is a multiple of $\\frac{\u03b8^{(l)}}{T}$ , or\n$\\frac{\u03b8^{(l)}}{T}  \\sum_{i=1}^T s[i] = \\frac{\u03b8^{(l)}}{T}. \\frac{ReLU^{(l)}(X)]}{T} + \\frac{\u03b8^{(l)}}{T},$\nif $ReLU\u03b8^{(l)} (X))$ is not a multiple of $\\frac{\u03b8^{(l)}}{T}$.\n(d) Suppose that $max X^{(l)} < 0$ and that the same weights $W^{(l+1)}$ act on the outputs of layer (l) of ANN and SNN as above, and let $X^{(l+1)}$ (resp. $T . X^{(l+1)})$ be the inputs to the (l + 1)th ANN layer (resp. the accumulated voltage for the (l + 1)th SNN layer of TPP neurons), Then\n$||X^{(l+1)} \u2013  X^{(l+1)}||\u221e \u2264 ||W^{(l+1)}||\u221e. \\frac{\u03b8^{(l)}}{T}$.\nRemarks: The proof of the previous result is presented in the Appendix. Here, we offer an interpretation of its statements."}, {"title": "3.3. Bio-plausibility and hardware implementation of TPP neurons", "content": "Our proposed neurons have two distinct properties: The two-phase regime and probabilistic spike firing. Both properties are biologically plausible and extensively studied in the neuroscience literature. For example, the two phase regime is related to firing after a delay of biological spiking neurons, where a neuron collects the input beyond the threshold value and fires after delay or after some condition is met. It could also be related to the bursting, when a biological neuron starts emitting bursts of spikes, after a certain condition is met, effectively dumping their accumulated potential. See (Izhikevich, 2007; Connors & Gutnick, 1990; Llin\u00e1s & Jahnsen, 1982; Krahe & Gabbiani, 2004) for more details.\nOn the other side, stochastic firing of biological neurons has been well studied as well, and different aspects of noise introduction into firing have been proposed. Refer to (Shadlen & Newsome, 1994; Faisal et al., 2008; Softky & Koch, 1993; Maass & Natschl\u00e4ger, 1997; Pagliarini et al., 2019; Stein et al., 2005) for some examples.\nRegarding the implementation of TPP neurons on neuromorphic hardware, two phase regime can be easily achieved on various modern neuromorphic that support programmable spiking neurons. Stochastic firing can be achieved through random sampling which, for example, is supported by IBM TrueNorth (Merolla et al., 2014), Intel Loihi (Davies et al., 2018), BrainScaleS-2 (Pehle et al., 2022), SpiNNaker (Furber et al., 2014) neuromorphic chips. For example, TrueNorth incorporates stochastic neuron models using on-chip pseudo-random number generators, enabling probabilistic firing patterns that mirror our approach. Similarly, Loihi (Gonzalez et al., 2024) supports stochastic operations by adding uniformly distributed pseudorandom noise to neuronal variables, facilitating the implementation of probabilistic spiking neurons.\nTo reduce the overall latency for processing inputs with our models, which yields linear dependence on the number of layers (implied by the two phase regime), we note that as soon as a particular layer has finished the firing phase, it can start receiving the input from the previous one: The process of classifying a dataset can be serialized. This has already been observed, for example in (Liu et al., 2022). Neuromophic hardware implementation of this serialization has been proposed as well, see for example (Das, 2023; Song et al., 2021; Varshika et al., 2022)."}, {"title": "4. Experiments", "content": "In this section, we verify the effectiveness and efficiency of our proposed methods. We compare it with state-of-the-art methods for image classification via converting ResNet-20, ResNet-34 (He et al., 2016), VGG-16 (Simonyan & Zisserman, 2015), RegNet (Radosavovic et al., 2020) on CIFAR-10 (LeCun et al., 1998; Krizhevsky et al., 2010), CIFAR-100 (Krizhevsky & Hinton, 2009), CIFAR10-DVS (Li et al., 2017) and ImageNet (Deng et al., 2009). Our experiments use PyTorch (Paszke et al., 2019), PyTorch vision models (maintainers & contributors, 2016), and the PyTorch Image Models (Timm) library (Wightman, 2019).\nTo demonstrate the wide applicability of the TPP neurons and the framework we propose, we combine them with three representative methods of ANN-SNN conversion from recent literature, each of which has their own particularities. These methods are: QCFS (Bu et al., 2022b), RTS (Deng & Gu, 2021a), and SNNC (Li et al., 2021a). The particularity of QCFS method is that it uses step function instead of ReLU in ANN models during their training, in order to obtain higher accuracy in lower latency after the conversion. RTS method uses thresholded ReLU activation in ANN models during their training, so that the outliers are eliminated among the activation values, which helps to reduce the conversion error. Finally, SNNC uses standard ANN models with ReLU activation, and performs grid search on the activation values to find optimal initialization of the thresholds in the converted SNNs.\nWe initialize our SNNs following the standard ANN-SNN conversion process described in Section 3 (and detailed in A), starting with a pre-trained model given by the baseline, or with training an ANN model using default settings in QCFS (Bu et al., 2022b), RTS (Deng & Gu, 2021a), and SNNC (Li et al., 2021a). ANN ReLU activations were replaced with layers of TPP neurons initialized properly. All experiments were conducted using NVIDIA RTX 4090 and Tesla A100 GPUs. For comprehensive details on all setups and configurations, see Appendix C.2.\n4.1. Comparison with the State-of-the-art ANN-SNN Conversion methods\nWe evaluate our approach against previous state-of-the-art ANN-SNN conversion methods, including ReLU-Threshold-Shift (RTS) (Deng & Gu, 2021a), SNN Calibration (SNNC-AP) (Li et al., 2021a), Quantization Clip-Floor-Shift activation function (QCFS) (Bu et al., 2022b), SNM (Wang et al., 2022a), Burst (Li & Zeng, 2022), OPI (Bu et al., 2022a), SRP (Hao et al., 2023a), DDI (Bojkovic et al., 2024) and FTBC (Wu et al., 2024).\nImageNet dataset: Table 1 compares the performance of our proposed methods with state-of-the-art ANN-SNN conversion methods on ImageNet. Our method outperforms the baselines across all simulation time steps for VGG-16, and RegNetX-4GF. For instance, on VGG-16 at T = 32, our method achieves 74.72% accuracy, surpassing other baselines even at T = 128. Moreover, at T = 128, our method nearly matches the original ANN performance with only a 0.12% drop in VGG-16 and a 0.14% drop in ResNet-34.\nWe see similar patterns in combining our methods with RTS and QCFS baselines, which use modified ReLU activations to reduce conversion errors. Table 1 shows these results. For instance, applying TPP with QCFS on ResNet-34 at T = 16 improves performance from 59.35% to 72.03%, a 12.68% increase. Similarly, for VGG-16 at T = 16, combining TPP with QCFS boosts performance from 50.97% to 73.98%, a 23.01% increase. Using TPP with RTS also shows significant improvements, such as a 12.82% increase for VGG-16 at T 16. These results demonstrate the benefits of integrating TPP with other optimization approaches, solidifying its role as a comprehensive solution for ANN-SNN conversion challenges.\nCIFAR dataset: We further evaluate the performance of our methods on CIFAR-100 dataset and present the results in Table 1. We observe similar patterns as with the ImageNet. When comparing our method with ANN-SNN conversion methods which use non-ReLU activations, e.g. QCFS and RTS, our method constantly outperforms RTS on ResNet-20 and VGG16. QCFS baseline suffers from necessity to train ANN models from scratch with custom activations, while our method is applicable to any ANN model with ReLU-like activation. Furthermore, custom activation functions sometimes sacrifice the ANN performance as can be seen from the corresponding ANN accuracies.\nCIFAR10-DVS dataset: We evaluate our method on the event-based CIFAR10-DVS (Li et al., 2017) dataset, comparing it with state-of-the-art direct training and ANN-SNN conversion methods (Table 2). Our approach demonstrates superior performance, achieving 82.40% accuracy at just 8 timesteps and further improving to 83.20% at 64 timesteps. Notably, our method outperforms the direct training method Spikformer (Zhou et al., 2022) (80.90%) and the ANN-SNN conversion method AdaFire (Wang et al., 2025) (81.25%).\n4.2. Comparison with other types of SNN training methods and models\nWe compare our approach with several state-of-the-art direct training and hybrid training methods as presented in Table 2. The comparison is founded on performance metrics like accuracy and the number of timesteps utilized during inference on the CIFAR-100 and ImageNet datasets. We benchmark our method against prominent approaches such as LM-H (Hao et al., 2023b), SEENN (Li et al., 2023), Dual-Phase (Wang et al., 2022b), TTS (Guo et al., 2024), RMP-Loss (Guo et al., 2023), RecDis-SNN (Guo et al., 2022), SpikeConv (Liu et al., 2022), and GAC-SNN (Qiu et al., 2024). We showcase the best accuracy comparable to state-of-the-art methods achieved by our approach with minimal timesteps. We prioritize accuracy, but direct training and hybrid training opt for a lower number of timesteps and sacrifice accuracy. We outperform LM-H (Hao et al., 2023b) and Dual-Phase (Wang et al., 2022b) for VGG-16 on CIFAR-100. For ResNet-20 on CIFAR-100, we have higher accuracy but longer timesteps. Additionally, for ResNet-34 on the ImageNet dataset, the accuracy of our method with QCFS with 16 timesteps is higher than that of Spike-Conv (Liu et al., 2022) with the same number of timesteps. We also achieve higher accuracy with longer timesteps as expected. Overall, our approach demonstrates promising performance and competitiveness in comparison with the existing SNN training methods."}, {"title": "4.3. Spiking activity", "content": "The event driven nature of various neuromorphic chips implies that the energy consumption is directly proportional to the spiking activity, i.e., the number of spikes produced throughout the network: the energy is consumed in the presence of spikes. To this end, we tested our proposed method (TPP) for the spike activity and compared with the baselines. For a given model, we counted the average number of spikes produced after each layer, per sample, for both the baseline and our method. Figure 5 shows the example of RTS and RTS + TPP. Both the baseline and our method exhibit similar spike counts. In particular, our method constantly outperforms the baselines, and possibly in doing so it needs longer average latency per sample (T). However, the energy consumed is approximately the same as that for the baseline in time T. The complete tables are present in Appendix E.4, where we provide more detailed picture of spike activities."}, {"title": "4.4. Membrane potential distribution in early time steps", "content": "In Figure 4 we compare the membrane potential distributions for baseline models and two methods that we studied in the paper, the permutations applied on spike trains and TPP method. Once again, it can be seen another reason for performance degradation of baseline models in low latency, as the membrane potential is not variable enough to produce spike informative spike trains, which is particularly visible in deeper layers. On the other side \u201cpermuted\u201d and TPP models show sufficient variability throughout the layers.\nBy increasing the latency, the baseline models can recover some of the variability and spike production, as can be seen in Figure 5. But, due to the misplacement of spike trains through temporal dimension, they are still not able to pick up on the ANN performance."}, {"title": "5. Conclusions and future work", "content": "This work identified the phenomenon of \u201ctemporal misalignment\" in ANN-SNN conversion, where random spike rearrangement enhances performance. We introduced two-phase probabilistic (TPP) spiking neurons, designed to intrinsically perform the effect of spike permutations. We show biological plausibility of such neurons as well as the hardware friendlines of the underlying mechanisms. We demonstrate their effectiveness through exhaustive experiments on large scale datasets, showing their competing performance compared to SOTA ANN-SNN conversion and direct training methods.\nIn the future work, we aim to study the effect of permutations and probabilistic spiking in combination with directly trained SNN models."}, {"title": "Impact Statements", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Conversion steps", "content": "Copying ANN architecture and weights. ANN-SNN conversion process starts with a pre-trained ANN model, whose weights (and biases) will be copied to an SNN model following the same architecture. In this process, one considers ANN models whose non-activation layers become linear during the inference. In particular, these include fully connected, convolutional, batch normalization and average pooling layers.\nApproximating ANN activation functions. The second step of the process considers the activation layers and their activation functions in ANN. Here, the idea is to initialize the spiking neurons in the corresponding SNN layer in such a way that their average spiking rate approximates the values of the corresponding activation functions. For the ReLU (or ReLU -like such as quantized or thresholded ReLU ) activations, this process is rather well understood. The spiking neuron threshold is usually set to correspond to the maximum activation ANN channel or layerwise, or to be some percentile of it. If we denote by f the ANN actiavtion, then ideally, after setting the thresholds, one would like to have\n$f(v[", "s[t": ".", "article)\n$v^{(l)}[t": "v^{(l)}[t - 1", "W^{(l)}s^{(l-1)}[t": "\u03b8^{(l)} . s[t \u2013 1", "n$s^{(l)}[t": "H(v^{(l)}[t", "s^{(l)}[t": "the value $(\u03b8^{(l)} . s^{(l)}[t"}]}