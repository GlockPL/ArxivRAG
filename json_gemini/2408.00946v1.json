{"title": "Generalisation of Total Uncertainty in AI: A Theoretical Study", "authors": ["Keivan Shariatmadar"], "abstract": "AI has been dealing with uncertainty to have highly accurate results. This becomes even worse with reasonably small data sets or a variation in the data sets. This has far-reaching ef-fects on decision-making, forecasting and learning mechanisms. This study seeks to unpack the nature of uncertainty that exists within AI by drawing ideas from established works, the latest develop-ments and practical applications and provide a novel total uncertainty definition in AI.\nFrom inception theories up to current method-ologies, this paper provides an integrated view of dealing with better total uncertainty as well as complexities of uncertainty in AI that help us understand its meaning and value across different domains.", "sections": [{"title": "I. INTRODUCTION", "content": "Uncertainty is one of the greatest determinants of AI research landscapes and applications. The significance of uncertainty as an inevitable ele-ment to consider when unlocking the full potential of artificial intelligence technologies grows ex-ponentially with advances made within this field across various sectors. In other words, uncertainty is a characteristic of the real world. It touches on all aspects of human life. The existence of artificial intelligence (AI) relies on the ability to understand and manage uncertainty effectively in order to build dependable, resilient, and adaptive (intelligent) systems. There are different forms in which uncertainty appears: from incomplete or noisy data to situations that can have multiple outcomes at once. Uncertainty must be addressed not as an abstract concept but as a practical necessity since AI systems often function within volatile and ambiguous conditions where choices have to be made based on imperfect information. Furthermore, the importance of uncertainty in AI cannot be underestimated. In other words, AI seeks human-like intelligence meaning it is reasoning, inferring, and making decisions even when there is no complete certainty involved. For example, autonomous vehicles could negotiate with unpredictable traffic jams while medical diag-nosis programs may interpret vague symptoms to suggest possible treatments for them. In these sce-narios, AI models have to deal with uncertainties before they can come up with correct and well-founded judgments that are also safe enough [1].\nAdditionally, uncertainty characterises every phase in the entire pipeline of AI including data preprocessing; feature engineering; model train-ing/testing/evaluation/deployment.\nBesides, uncertainty is all over AI pipeline stages involving data preprocessing and feature engineering, model training and analysis and de-ployment. In AI approaches that are data-driven, uncertainty comes from inherent uncertainty in the data, sampling variability, imperfect models, or model approximation errors. Even in rule-based or symbolic AI systems, however, any conclu-sions will be affected by uncertainty based on the complexity of real-world phenomena as well as human reasoning. This paper brings together succinctly important phenomena, approaches, as well as new directions taken by AI systems to deal with uncertainty as they are found in Kanal [1].\nThis paper is motivated by a growing under-standing of the new definition of total uncertainty defined by two types of uncertainties in machine learning-Epistemic and Aleatoric uncertainties, explained in Section II."}, {"title": "A. Epistemic and aleatoric uncertainties", "content": "The importance of understanding and quan-tifying uncertainty in machine learning has in-creased significantly. A proper understanding of the uncertainty framework that defines the types of optimisation problems is important and we encounter them in this paper. Therefore, a proper delimiting of the scope as well as understanding of the underlying ideas of the selected concepts and their impact on the loss function to be optimised is necessary. This is primarily due to the grow-ing complexity of AI applications and the rapid growth of data. The need to address this demand arises from the unpredictable nature of utilising AI in diverse domains, including safety-critical areas like healthcare and autonomous systems, to more business-related such as finance and marketing. Machine learning has the potential to revolutionise various industries. However, it also poses inher-ent risks, particularly in terms of generalisation, domain adaptation, and making safe decisions in many applications, e.g., control and decision mak-ing. To tackle this problem, researchers have been continuously developing uncertainty quantification methods in AI. Unlike standard machine learning algorithms, an advanced uncertainty-equipped ma-chine learning algorithm provides us with a set of predictions for possible outputs instead of a single point prediction. This ability to return a set of possible outcomes helps us to make better and safer decisions in different scenarios. In this section, we primarily give an overview of differ-ent methods and algorithms that enable machine learning methods to quantify and approximate uncertainty. Several methods that we cover in this section, include direct interval prediction, ensem-ble models, Bayesian methods, Random sets & Belief function models, and conformal prediction. We mainly discuss these four methods due to their popularity.\nAleatoric uncertainty: Aleatoric (statistical) uncertainty (AU) refers to randomness or vari-ability: What a random sample drawn from a probability distribution will be.\nEpistemic uncertainty: Epistemic (systematic) uncertainty (EU) refers to the lack of knowledge: What is the relevant probability distribution?"}, {"title": "B. Total uncertainty", "content": "In machine learning, the total uncertainty (TU) is defined by [2],\n$$TU = EU + AU.$$ (1)\nThe definition (1) is correct when the epistemic and aleatoric uncertainties are independent. As a simple example, the mean of two entities could be different with different noise levels i.e., EU and AU are dependent and we need a better definition of total uncertainty. In the next Section III we explain a new definition for TU and an overview of advanced (imprecise) uncertainty models."}, {"title": "II. UNCERTAINTY MODELS AND IDENTIFICATIONS", "content": "In this section, we focus on an overview of the analytical concepts of some advanced uncertainty models that we use in WP2, e.g., intervals, random sets, probability intervals, and credal sets. We explain the essential ideas behind the concepts. These advanced models are set models and deal with in-determinism in the uncertainty (second-order uncertainty).\nIn general, and specifically in engineering and machine learning domains, uncertainty is defined in two categories: aleatoric and epistemic uncer-tainty. Aleatoric uncertainty arises from the inher-ent randomness in the data, and it is irreducible. In contrast, epistemic uncertainty arises from the lack of information or data which is reducible (by gathering more data).\nIn this section, our focus is on integrating epistemic uncertainty models into the optimisa-tion problems under uncertainty. The goal is to investigate different types of epistemic uncertainty models for the optimisation problem under uncer-tainty that enables \u201cpointwise\u201d optimisation under epistemic uncertainty. While robust and stochastic optimisation methods are commonly employed in dealing with optimisation under the Bayesian uncertainty, they fail to account for predicting epistemic uncertainty with acceptable accuracy. By incorporating epistemic uncertainty models, we can enhance the optimisation process by ac-counting for the lack of information or data. To model the epistemic uncertainty, we propose several epistemic uncertainty models such as the (probabilistic) interval model, random set, and credal set models.\n1) Deterministic Intervals: The deterministic interval model\u2014also called vacuous previsions\u2014expresses ignorance relative to a non-empty subset of possible parameter values. The interval model is the least informative model. The interval repre-sentation only needs the lower and upper values of an uncertain parameter. Hence, the knowledge of the probabilistic distribution over the parameters between its lower and upper value is not required. Mathematically, an interval model is written as X = [X, X].\nThe interval model, also known as vacu-ous expectation, represents ignorance regarding a nonempty subset of possible parameter values. In state-of-the-art problems, the Interval arithmetic attempts to propagate the interval through the problem to find a resulting interval, aiming to calculate the upper and lower bounds of this propagated interval. However, precise calculation of these bounds can be challenging or even im-possible, and the complexity of this method for higher-dimensional e.g., optimisation problems is NP-hard [3, 4].\nThe lower and upper expectations for a subset A of \u03a9 and for a given functional g := g(Y) of the random variable Y : A \u2192 R are defined as follows:\nInterval Model\n$$E(g) := inf_{g|A} g_{A} \text{ and } \\overline{E}(g) := sup_{g|A} g_{A}.$$ (2)\nWhen A := [a, b] \u2282\u03a9 := R (one-dimensional case) and g(y) is a continuous function on y \u2208 [a, b], the lower and upper expectations are defined as:\n$$E(g) = \\min_{y \\in [a,b]} g(y) \text{ and } \\overline{E}(g) = \\max_{y \\in [a,b]} g(y).$$ (3)\nThis model effectively represents the available information when it is known that Y assumes values in A but no further probabilistic information is available, other than P(A) = 1.\n2) Probability Intervals: In classification of C elements, probability intervals can be defined as [y, y] = {[yk, yk]}=1, where 0 \u2264 y \u2264 yk \u2264 1 . They represent the lower and upper bounds of the probabilities associated with the relevant classes.\n3) e-contaminations: This model is more ad-vanced than the two previous models-interval and probability intervals. It is easy to build as well as implement compared to the other uncertainty models such as a possibility distri-bution model. The contamination model is a mixed/hybrid model, which is built by two simple models-a non-probabilistic model such as inter-val and a probabilistic model such as a probabil-ity distribution. The state-of-the-art of the mixed models\u00b9 is mainly focused on Bayesian sensitivity analysis and not much attention has been paid to non-deterministic advanced uncertainty cases"}, {"title": "Definition", "content": "An e-contamination model E(\u00b7) is described as a convex combination of two uncertainty models: (i) Probabilistic/precise model, e.g., Normal distribution with expectation E, (ii) non-probabilistic/imprecise model, e.g., interval model with lower expectation E\u2081, defined in (3). For a gamble f \u2208 G(\u03a9) the lower expectation is described as follows:\nContamination model\n$$E(f) = (1 - \\epsilon)E(f) + \\epsilon E_1(f)$$\n$$ \\text{where } \\quad E = \\frac{E}{\\{E : \\forall f \\in G(\\Omega), E(f) \\geq E_1(f)\\}\\} \\text{is the set} \\\\ \\text{of dominating linear expectations by} E_1. \\text{ By} \\\\ \\text{definition (2), the lower and upper expectation} \\\\ \\text{for a given interval } [a, b] \\text{ is defined as} \\\\ E_1(f(y)) = \\inf_{y \\in [a,b]} f(y) \\text{ and } E_1(\\overline{f(y)}) = \\sup_{y \\in [a,b]} f(y).$$ (5)\n0 < \u0454 < 1 is called (in this dissertation) a tuning parameter or level of model-trust/importance. Similarly, the upper expectation is\n$$\\overline{E}(f) = (1 - \\epsilon)\\overline{E}(f) + \\epsilon E_1(\\overline{f}).$$ (6)\n4) Credal Sets: A credal set is defined as a convex set of probability distributions. One of the computationally effective ways to construct a credal set, denoted as Q, is to use probability intervals, as follows:\n$$Q = \\{y = (y_1, \\dots, y_C) | y_k \\in [\\underline{y_k}, \\overline{y_k}], \\quad \\forall k = 1, 2, ..., C \\},$$ (7)\nin which y represents a single probability distri-bution vector. Here, Q is a special convex set (a polytope) of probabilities constrained by proba-bility intervals. To prevent Q from being empty, [y, y] is required to satisfy the following condition [11]:\n$$\\sum_{k=1}^{C} \\underline{y_k} \\leq 1 \\leq \\sum_{k=1}^{C} \\overline{y_k}$$. (8)"}, {"title": "5) Random Sets", "content": "In a traditional classifier like a neural network, the output is a mapping from input data to a single category. However, in set-valued classification, the output is a mapping from the input data to a set of possible categories. For instance, instead of predicting a single class, a set-valued classifier might predict multiple potential classes for a given input. In set-valued classifica-tion, the prediction for each input is not a vector of softmax probabilities as in traditional classifica-tion. Instead, it's a belief function, where each out-put corresponds to a subset of possible classes. For example, if there are N classes, a basic set-valued classifier would have 2N outputs, each represent-ing a different combination of classes. To over-come the exponential complexity of using 2^N sets of classes (especially for large N), a fixed budget of K relevant non-singleton (of cardinality > 1) focal sets are used. These focal sets are obtained by clustering the original classes and selecting the top K sets of classes with the highest overlap ratio, computed as the intersection over union for each subset. The clustering is performed on feature vectors of images of each class generated by a standard CNN trained on the original classes. The feature vectors are further reduced to 3 dimensions using t-SNE (t-Distributed Stochastic Neighbour Embedding) before applying a Gaussian Mixture Model (GMM) to them. Ellipsoids, covering 95% of data, are generated using eigenvectors and eigenvalues of the covariance matrix and the mean vector obtained from the GMM to calculate the overlaps. To avoid computing a degree of overlap for all subsets, the algorithm is early stopped when increasing the cardinality does not alter the list of most overlapping sets of classes. The non-singleton focal sets so obtained, along with the 2^N original (singleton) classes, form our network outputs E.g., in a 100-class scenario, the power set contains subsets (1030 possibilities). Setting a budget of K = 200, for instance, results in 100 + K = 300 outputs, a far more manageable number."}, {"title": "6) Probability Box", "content": "Another advanced and highly informative uncertainty model is the (gen-eralised) probability box or p-box. When multiple cumulative distribution functions (CDFs) from a database or experiment are available but a true distribution cannot be determined, they can be collected into a bounded set known as a p-box. In real-life scenarios, such as those involving numerous disturbances and a rapidly changing en-vironment, finding a true distribution model can be challenging. In such cases, modelling uncertainty via a single unique distribution is impractical. However, with sufficient data, lower and upper distribution functions can be defined, within which any estimated distribution must lie.\nA p-box is a set of all distributions bounded by the lower and upper distribution functions. Mathematically, a (generalised) p-box is defined as a set of distributions bounded by a pair (F, F) of cumulative distribution functions mapping the sample space \u03a9 to [0, 1], satisfying F < F. If \u03a9 is a closed interval on R, the pair (F, F) is referred to as a p-box.\nThe CDF is any non-decreasing function F : \u03a9 \u2192 [0,1] that satisfies F(10) = 1, where 10 is the largest member of \u03a9 and 0\u00ba is the smallest. F(s) provides information about the cumulative probability on the interval [02, 8]\u00b2. Given a cu-mulative distribution F on \u03a9 and a value s \u2208 \u03a9, F(s+) is the right-limit and F(s\u00af) is the left-limit, defined as follows:\n$$F(s^+) = \\inf_{y>s} F(y) = \\lim_{y \\to s, y > s} F(y),$$ $$F(s^-) = \\sup_{y<s} F(y) = \\lim_{y \\to s, y < s} F(y).$$\nIn Walley's framework [13], a generalised p-box is interpreted as a lower expectation (actually a lower probability) EFF on the set of events\nZ \u2282 \u03a9, Z := [0,1n] = {[0n, s] : s\u2208 \u03a9}\nU {(r, 1n] : r\u2208\u03a9}\nas follows:\n$$E_{F,F} ([0_{\\Omega}, s]) := \\underline{F(s)},$$ $$E_{F,F} ((r, 1_{\\Omega}]) := 1 - \\overline{F(r)}.$$"}, {"title": "Limit Approximation of P-box", "content": "Consider a p-box (F, F) on \u03a9. Let {En}n, {Fn}n be increasing and decreasing sequences of CDFs converging point-wise to F and F, respectively. Assume En is the lower probability associated with (Fn, Fn). Since Fn < F and Fn > F, it follows that \u03a6(F, F) \u2286 \u03a6(Fn, Fn) and Equation (11) implies that En \u2264 E. Furthermore, En \u2264 En+1 for any n\u2208 N, so limn En = supn En \u2264 E. Thus, the natural extension can be approximated as follows: E(f) = limn En(f) for any gamble f.\nFor a given p-box model with the lower and upper bounds given by uniform distribution functions F and F-we can discretise the y-axes (probability) into n partitions, with Fn and Fn determined as follows:\n$$F_n(y) = \\begin{cases}  \\frac{i-1}{n} & \\text{if } y \\in L_i, y \\neq 1_{\\Omega} \\\\ 1 & \\text{if } y = 1_{\\Omega}   \\end{cases}$$ (12)\n$$F_n(y) = \\frac{i}{n} \\text{ if } y \\in U_i$$ where n \u2208 N, i \u2208 {1, 2, 3, . . ., n} such that\n$$U_i := \\overline{F}^{-1}((\\frac{i-1}{n}, \\frac{i}{n}]),$$ $$L_i := \\underline{F}^{-1}((\\frac{i-1}{n}, \\frac{i}{n}]).$$\nAs shown, any p-box model can be approximated via 2n intervals (Ui's and Li's). From the definition of the lower expectation under a discrete generalised p-box [15, 16], the approximations of the lower and upper expectations E : G(\u03a9) \u2194 R, E : G(\u03a9) \u2192 R for a gamble f\u2208 G(\u03a9) are given by:\n$$\\underline{E}(f) = \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^{n} \\inf_{y \\in L_i} f(y) = \\frac{1}{N} \\sum_{i=1}^{N} \\inf_{y \\in L_i} f(y),$$ (13)\n$$\\overline{E}(f) = \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^{n} \\sup_{y \\in U_i} f(y) = \\frac{1}{N} \\sum_{i=1}^{N} \\sup_{y \\in U_i} f(y).$$ (14)"}, {"title": "III. GENERALISED TOTAL UNCERTAINTY ESTIMATION", "content": "As it is discussed in Section I-B, the definition (1) is correct when AU and EU are independent. Here we will show via a simple (Counter) example that the epistemic and aleatoric uncertainty are not necessarily independent.\nA. Dependency\nThe mean of two the same data sets with differ-ent noises are different. Assume X\u2081 := f(x) + \u20ac1 and X2 := f(x) + \u20ac2, where \u20ac1 ~ \u039d(\u03bc\u03b9,\u03c31) and \u20ac2 ~ \u039d(\u03bc2, \u03c32), therefore E(X1) \u2260 E(X2). Similarly, assume X1 = X2 are two exactly the same datasets. If we remove some K-number of data points for instance from X2-meaning increasing the epistemic uncertainty in X2-then the noise level (02) in X2 will be different than 01. Therefore, AU and EU are dependent.\nB. Proposal I\nWe propose a new definition by a linear combination of the AU and EU as follows.\n$$TU := \u03b1_1AU + A_2EU,$$ (15)\nSince, by definition, the TU must be greater than either AU or EU, then a1 + 02 > 1. But how to define a1 and 2? Some preliminary ideas are proposed as follows.\n1. Investigate if the model is robust against increased noise or decreased data size. To check the sensitivity of the model to AU or EU. Then we make a\u2081 smaller or greater than \u03b12.\n2. In the case of CredNN, calculate the upper, H*, and lower, H*, Shannon entropy defined in (9). The difference (imprecision) between upper and lower Shannon entropy is the lower bound for EU, i.e., a2 > H* \u2013 H*.\n3. In the case of Interval Neural Network (INN) [17], the difference between the upper, UEU, and lower, LEU, is the lower bound for EU, i.e., 2 \u2265 UEU - LEU.\n4. In the case of deep ensembles (EnNN) [18], take the difference between highest, UEnNN, and lowest, LENNN, prediction as a2, i.e., \u03b12UEnNN - LENNN."}, {"title": "C. Proposal II", "content": "In the case of the contamination model (5), the precise part defines the AU and the imprecise part defines the EU. We call this model Contamination Neural Network (ContNN) which is defined as follows.\n$$ContNN := \\epsilon.BNN + (1 - \\epsilon).INN$$ (16)\nwhere BNN is the Bayesian Neural Network [19]. We use BNN for the AU definition and INN for the EU definition of the ContNN novel model in (16) as AUContNN and EUContNN. Finally, we define the total uncertainty for the ContNN as, TUContNN := AUContNN + EUContNN."}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "In this paper, we proposed two novel ideas for the Total Uncertainty definition as proposals (I) and (II). In proposal (I), we provide four approaches to identify the parameters 01 and 02 in (1). The validation, comparison, and application will be discussed in our next paper. In proposal (II), we define a novel Neural Network via the convex combination of BNN and INN in (16). We define the novel Total Uncertainty, TUContNN, for the contamination neural network model. Based on the literature about BNN and INN (as well as CreNN), we have seen that EU could be better estimated with nob-Bayesian models e.g., INN. Furthermore, AU could be estimated better via the standard neural network or BNN. Therefore, the total uncertainty estimation for the novel ContNN is better defined compared to the definition (1). However, the ContNN model complexity is higher than the state-of-the-art models. In our future work, we will compare its pros and cons."}]}