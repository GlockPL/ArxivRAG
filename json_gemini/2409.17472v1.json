{"title": "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards", "authors": ["Heejin Do", "Sangwon Ryu", "Gary Geunbae Lee"], "abstract": "Recent advances in automated essay scoring (AES) have shifted towards evaluating multiple traits to provide enriched feedback. Like typical AES systems, multi-trait AES employs the quadratic weighted kappa (QWK) to measure agreement with human raters, aligning closely with the rating schema; however, its non-differentiable nature prevents its direct use in neural network training. In this paper, we propose Scoring-aware Multi-reward Reinforcement Learning (SaMRL), which integrates actual evaluation schemes into the training process by designing QWK-based rewards with a mean-squared error penalty for multi-trait AES. Existing reinforcement learning (RL) applications in AES are limited to classification models despite associated performance degradation, as RL requires probability distributions; instead, we adopt an autoregressive score generation framework to leverage token generation probabilities for robust multi-trait score predictions. Empirical analyses demonstrate that SaMRL facilitates model training, notably enhancing scoring of previously inferior prompts.", "sections": [{"title": "Introduction", "content": "An essay can be evaluated from diverse perspectives, such as Content, Sentence Fluency, and Organization. As providing multi-view assessment is essential for enhancing the learner's writing skill, recent attention to automated essay scoring (AES) systems has shifted from solely relying on holistic scoring (Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Wang et al., 2022) to evaluating multiple trait scores (Kumar et al., 2022; Ridley et al., 2021; Do et al., 2024). Although simultaneous assessment for multiple traits is more challenging than a holistic paradigm, it has been much less explored.\nTypically, AES systems are evaluated using the Quadratic Weighted Kappa (QWK) (Cohen, 1968)"}, {"title": "Related works", "content": "Multi-trait essay scoring Although automated essay scoring has achieved notable success (Dong et al., 2017; Yang et al., 2020; Wang et al., 2022), research on multi-trait essay scoring is still underdeveloped and requires further exploration. Early attempts for multi-trait AES lie in constructing multiple trait-specific layers or models for different predictions (Mathias and Bhattacharyya, 2020; Ridley et al., 2021; Kumar et al., 2022; Do et al., 2023). Pointing out the inefficiency of duplicating individual encoder-only models generating a single score, Do et al. (2024) sequentially produces multi-trait scores by defining scoring as a decoder-introduced text generation. Autoregressively generating multi-trait scores significantly improved all trait-scoring performance, achieving stat-of-the-art results. Further, it reduces the burden of designing separate trait-specific layers as it consecutively predicts full trait scores for entire prompts with a single model. To take advantage of the efficiency and high performance, we introduce our RL method based on their autoregressive AES paradigm.\nRL for text generation Recently, RL has been actively employed across diverse natural language generation tasks. Notably, the advent of reinforce-"}, {"title": "Preliminary", "content": "We adopt the policy gradient reinforcement learning to train the policy to optimize rewards. Policy gradient aims to increase the probability of actions that yield high rewards. To guide the model towards taking actions that result in a higher reward, the policy gradient function includes the probability of actions taken by the policy, $\\pi_{\\rho}(a | s)$:\n$\\mathcal{L}_{P G}(\\theta)=\\hat{\\mathbb{E}}_{t}\\left[\\log \\pi_{\\theta}\\left(a_{t} | s_{t}\\right) A_{t}\\right]$\nMost of the existing AES systems use encoder-only models like BERT (Devlin et al., 2019) and rely on regression- or classification-based approaches. In general, the model generates the essay embedding vector from a given essay input; however, the differences lie in the objective functions and the process of deriving the final score from the output vector. Regression models predict the score with a sigmoid function given the embedding vector and are trained with the MSE loss function between the label y and predicted score \u0177 for n samples:"}, {"title": "SaMRL", "content": "To incorporate a rating schema in the training phase, SaMRL updates the policy model using multiple rewards obtained by a scoring-aware multi-reward function. In RL, relying solely on reward-based learning can lead the model to prioritize enhancing rewards exclusively, potentially losing its capacity to score essays in appropriate prediction form. Therefore, we employ a fixed-parameter anchor model to guide the policy to prevent significant deviations in training and maintain the trait patterns. Figure 2 describes the overall process of SaMRL."}, {"title": "Score generation model", "content": "In autoregressive multi-trait prediction, given the essay input with the prefix of \"score the essay of"}, {"title": "Multi-rewards function", "content": "To manage both the overall consistency and agreement across ratings, as well as precision at individual score levels, we introduce multiple rewards: bidirectional QWK ($r_Q$) and mean trait-wise MSE reward ($r_M$). QWK accounts for the ordinal nature of essay scores and weighting exact and near matches; thus, it effectively captures the rating schema and is sensitive to the overall qualities (Wang et al., 2018). Meanwhile, MSE aggregates the exact difference between predicted and actual scores; hence, it provides a clear yet simple indication of individual score deviations from true labels.\nQWK The QWK metric evaluates the agreement and consistency between human and system assessments across a set of essays, calculating a single QWK score for the entire set. Therefore, rewarding with the measurement within the batch set, $Q_B$, assigns the same reward to all samples, potentially resulting in unstable training. To establish a more precise and stable reward strategy, we introduce a trait-wise QWK, $Q_T$, which evaluates the agreement between the trait sets of predictions and gold labels at the sample level. Specifically, batch-wise quadratic weighted kappa score, $Q_B$, is defined as:"}, {"title": "RL policy update", "content": "Our multi-trait AES model generates scores sequentially in a specific order and format. Consequently, a comprehensive evaluation is only feasible after the complete sequence has been generated, culminating with the final trait [traitm scorem]. Thus, the model receives calculated multi-rewards just after the completion of the last trait score generation at the T-th timestep. Maintaining a structured format is crucial for scoring multiple traits in order, we adopt the token-wise KL regularization technique between the frozen anchor model, $\\pi^{AC}$, and our trainable policy model $\\pi_{\\theta}$. This process prevents the policy from overly adapting to the reward function while ensuring the preservation of the [trait; scorej] generation format. For each multi-rewards, in conjunction with the obtained reward and token-wise KL regularization until the T\u2013 1-th tokens, we update the policy via PPO (Schulman et al., 2017) with generalized advantage estimation (Schulman et al., 2016). The full reward $R_k$ ($k\\in {M,Q}$), is defined as follows:"}, {"title": "Experimental setup", "content": "Datasets We use the open-sourced ASAP\u00b9 and ASAP++\u00b2 (Mathias and Bhattacharyya, 2018) datasets, as in the baseline ArTS model (Do et al., 2024). ASAP++ dataset includes enriched human-annotated multi-trait scores for English-written essays of eight separate prompts. As summarized in Table 1, different prompts are assessed with distinct traits with varied score ranges. While other traits are evaluated across several prompts, Style and Voice are only assessed in prompts 7 and 8, respectively, resulting in a very limited number of samples available for training. Our model handles all prompts and all traits with a single model. We also experiment with the publicly available Feedback Prize 3 dataset, where argumentative essays are annotated with six trait scores, Cohesion, Syntax, Vocabulary, Phraseology, Grammar, and Conventions. Unlike the ASAP dataset, this data is not divided by prompts."}, {"title": "Result and Discussions", "content": "Main results The main experimental results for each trait in Table 2 demonstrate the effectiveness of our SaMRL method in enhancing the scoring quality across most traits, establishing new state-of-the-art. Noticeably, applying SaMRL to both ArTS-base and ArTS-large models exhibits a consistent trend of performance enhancements across the traits. We conducted a paired t-test on trait-wise average scores, and both the SaMRL-base and -large models showed significant improvements over the baseline (p < 0.05). Given that the T5-base and T5-large models have 220M and 770M parameters, respectively, these findings underscore the ro-"}, {"title": "Ablation studies", "content": "Are multi-rewards more effective than single rewards? We conduct comprehensive ablation studies to analyze the respective and joint effects of each reward. Experimental results in Table 4 and 5 reveal that our multi-rewards are more effective than separately applying single rewards. When adopted individually, MSE-only reward (SaSRLM) achieves higher performance than bidirectional QWK-only reward (SaSRL). The result validates the efficacy of adjusting MSE objectives from logistic regression models to fit autoregressive frameworks via RL. Meanwhile, the joint use of both rewards (SaMRL) shows superior improvements across all prompts and traits except Style, indicating synergistic impacts of our multi-rewarding mechanism. Our findings align with the prior works (Dann et al., 2023), suggesting the effectiveness of multi-reward RL over using a single reward.\nIs bidirectional QWK more effective than unidirectional QWK? In addition, we analyze whether our bidirectional QWK strategy is indeed more advantageous than the unidirectional QWK rewards for policy training. Note that SaMRL_uniQT and SAMRL_uniQB are the models in which QT and QB are applied in combination with MSE rewards, respectively. We observe overall higher performances when solely using the trait-wise QWK QT as RQ (SaMRL_uniQT) than when only applying the batch-wise QWK,"}, {"title": "Discussions", "content": "Comparison with classification-based RL Our method has shown effectiveness in essay sets evaluated in a wider score range, indicating the overcoming of limitations that exist in classification-based RL methods. To investigate our actual impacts compared to them, we compare SaMRL results with existing RL-based AES systems (Wang et al., 2018). Figure 4 illustrates two prior RL models: 1) bidirectional LSTM-based classification model, CLS+RL, and 2) dilated LSTM-based classification model, CLSDI+RL. As they are holistic scoring models that predict a single Overall score, the comparison is constrained to the Overall trait. Our SaMRL approach outperforms the prior models in most prompts, particularly showing significant improvements in prompts 7 and 8, which have a broader rating range of [0,30] and [0,60] than other prompts. Note that in these two prompts, even when RL is applied to classification, its performance is significantly inferior to regression (\u00d7). Meanwhile, our model, leveraging text generation probability while incorporating an awareness of the scoring schema,\ndemonstrates significant robustness.\nImpact of weight learning for multi-loss Following previous research showing that adjusting the weights of each loss adaptively in multi-task learning can improve performance (Chen et al., 2018; Kendall et al., 2018; Mao et al., 2022), we optimize multiple losses by dynamically learning the weights of each loss during the RL policy update process. By treating these weights as trainable parameters, we adaptively adjust the importance of each objective. As depicted in the left section of Figure 5, the assigned weights for each loss dynamically adjust throughout the training steps, with a growing emphasis on the MSE loss over the QWK loss as learning progresses. This shifting trend is consistent with findings from ablation studies, which indicate a greater impact of MSE reward on assisting the training of the AES model. Furthermore, our weight update mechanism has proved superior effectiveness compared to using fixed weights of (0.3, 0.7), (0.5, 0.5), and (0.7, 0.3) for lossRQ and loss RM, respectively, as reported in the right part of Figure 5. We anticipate that further optimization in multi-loss weight could bring in additional performance advances in future works."}, {"title": "Conclusion", "content": "In this work, we propose a Scoring-aware Multi-reward Reinforcement Learning (SaMRL) method, which incorporates the rating process for effective multi-trait scoring within the generation framework. By introducing the policy gradient reinforcement in the autoregressive score generation paradigm, we enable the direct use of the QWK metric; thus, SaMRL effectively captures the rating procedure. In addition, we jointly introduce the score-level"}, {"title": "Limitation", "content": "In this work, we grounded the method on autoregressive prediction, where prediction order may matter. Currently, we employ the same order as the previous work (Do et al., 2024); however, thoroughly considering the shifts in trait prediction order can lead to further improvements. Secondly, we update the policy at a time after the entire trait score prediction, motivated by existing approaches (Stiennon et al., 2020; Dutta et al., 2024; Ryu et al., 2024). However, training the policy with the instant updating per each action (i.e., token generation) might bring in more benefits in the case of the scoring task, which can be noteworthy for future work. In addition, we update the weights via training for the multiple losses. As in prior works, adaptive optimization strategies or more refined mechanisms could extend the impact of our method (Chen et al., 2018; Kendall et al., 2018; Mao et al., 2022)."}, {"title": "Ethical Statement", "content": "Only publicly available datasets, ASAP, ASAP++, and Feedback Prize, are used in this work."}, {"title": "Comparison models", "content": "We primarily compared our method with the baseline ArTS (Do et al., 2024) model, which is the previous state-of-the-art model for multi-trait AES. As we aim to examine the effects of applying RL on the autoregressive model, the comparison mainly focuses on the model with and without applying our method. In addition, we also report the results of other multi-trait scoring models (Kumar et al., 2022) and the holistic scoring models (Cozma et al., 2018; Dong et al., 2017) individually applied for each trait prediction. In particular, the multi-trait scoring MTL model (Kumar et al., 2022) constructed each trait-specific layer and used all other trait layers auxiliary for a target trait training and prediction. The holistic scoring"}, {"title": "Standard deviations for five-fold validations", "content": "In this work, we implemented five-fold validation and presented scores averaged over the five folds in all experiments. In this session, we report the standard deviation for the main results (Table 7, 8) and the results of ablation studies (Table 9, 10)."}]}