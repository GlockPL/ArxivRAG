{"title": "Quantum Large Language Models via Tensor Network Disentanglers", "authors": ["Borja Aizpurua", "Saeed S. Jahromi", "Sukhbinder Singh", "Rom\u00e1n Or\u00fas"], "abstract": "We propose a method to enhance the performance of Large Language Models (LLMs) by integrating quantum computing and quantum-inspired techniques. Specifically, our approach involves replacing the weight matrices in the Self-Attention and Multi-layer Perceptron layers with a combination of two variational quantum circuits and a quantum-inspired tensor network, such as a Matrix Product Operator (MPO). This substitution enables the reproduction of classical LLM functionality by decomposing weight matrices through the application of tensor network disentanglers and MPOs, leveraging well-established tensor network techniques. By incorporating more complex and deeper quantum circuits, along with increasing the bond dimensions of the MPOs, our method captures additional correlations within the quantum-enhanced LLM, leading to improved accuracy beyond classical models while maintaining low memory overhead.", "sections": [{"title": "Introduction", "content": "In 2022, OpenAI revolutionized the field of artificial intelligence with the release of ChatGPT [1], an AI system capable of providing structured responses to virtually any type of query. This model, like its subsequent iterations, is built on the transformer architecture, first introduced in Ref. [2], which forms the foundation of what are now known as Large Language Models (LLMs) [3]. Today, numerous LLMs exist, many of which are open-source, including Meta's LlaMA [4] and Google's BERT [5]. The main challenge with these models, however, lies in their enormous energy consumption. As highlighted by the CEO of OpenAI, training ChatGPT-3 alone incurred an estimated 100 million in electricity costs, and this expense is expected to double every ten months [6]. This has prompted ongoing research into compression techniques that aim to enhance the efficiency of LLMs without sacrificing accuracy. A particularly promising approach to LLM compression is the use of quantum-inspired tensor networks, as originally proposed in Ref. [7].\nIn parallel to the rise of generative AI, another frontier technology is gaining traction: quantum computing. Currently, quantum processors remain in the early stages of development, consisting of hundreds of noisy qubits. These processors fall under the category of what Preskill termed Noisy Intermediate-Scale Quantum (NISQ) [8] devices. Although these processors are not error-corrected, they enable the implementation of complex variational quantum circuits, which can be used for applications such as quantum optimization [9] and quantum machine learning [10, 11]. The combination of advanced error-mitigation strategies [12] with the hardware roadmaps of major quantum processor providers [13] suggests that reliable variational quantum circuits operating on hundreds of qubits will be feasible in the near future.\nIn this paper, we bring together the best of both worlds and discuss how quantum computing can be used to improve LLMs. As we will show, the first step towards surpassing the limitations of classical computing in LLMs is the introduction of variational quantum circuits into the deep layers of the model. These quantum circuits, when integrated with classical quantum-inspired Tensor Networks (TN) [14, 15], significantly enhance the model's performance, allowing for larger levels of accuracy unattainable by purely classical LLMs, while maintaining manageable memory requirements. As we will illustrate, the resulting quantum LLMs are hybrid models, retaining the classical transformer architecture but incorporating layers comprised of quantum circuits and tensor networks. As a remark, there have been previous approaches to bring language into quantum computers [16, 17], but did not consider LLM practical architectures."}, {"title": "Main idea", "content": "The core concept of our method is to replace the weight matrices in the deep layers of LLMs with (unitary) quantum circuits combined with (arbitrary) Tensor Networks. In a previous work, we demonstrated that substituting weight matrices with TNs in LLMs can achieve over 90% memory compression while preserving model accuracy [7], consistent with earlier findings in other AI models [18-21].\nIn this paper, we build upon those results and take a further step: what if we replace the weight matrix with a variational quantum circuit? Such a replacement would enable the model to capture a significantly higher degree of correlations, far beyond what the weight matrix alone can represent. However, the unitarity of the quantum circuit would introduce constraints in the model's optimization, potentially lowering its accuracy. To address this issue, we propose a simple solution: why not combine the quantum circuits with a non-unitary TN?\nThe resulting model is a generalization of the approach proposed in Ref. [7], where the weight matrices in the deep layers are replaced by (i) a Variational Quantum Circuit (VQC), followed by (ii) an arbitrary TN, such as a Matrix Product Operator (MPO), and then followed again by (iii) a second Variational Quantum Circuit. If we were to omit (i) and (iii), we would essentially recover the tensorized and compressed LLM models from Ref. [7]. By incorporating quantum circuits, we create a hybrid quantum LLM architecture that captures a vast amount of quantum correlations, in addition to the correlations present in the classical (potentially tensorized) LLM. This substantial increase in correlations implies, at worst, enhanced accuracy compared to classical models, while the associated memory overhead scales proportionally with the depth of the quantum circuit, which in the worst case grows polynomially with the number of input bits to the layer.\nLet us now provide a concrete example. In Fig. 1, we illustrate how to practically implement this approach within the transformer architecture used in LLMs such as LlaMA. In this architecture, we identify the Self-Attention (SA) and Multi-layer Perceptron (MP) layers as containing large weight matrices, which are particularly suitable for our method. The approach replaces these matrices with the combination of VQC+TN as previously described. This methodology can also be extended to models with more complex architectures, such as learners like Mixtral8x7B [22], among others."}, {"title": "From LLM to QLLM", "content": "To apply this concept in practice, we could begin by training the \"empty\" VQC+TN structure from scratch. However, a more efficient strategy involves leveraging existing classical LLMS (such as ChatGPT, LlaMA, BERT and Mistral) and converting them directly. Below, we outline a procedure to first encode a classical LLM into this format and, second, enhance its performance beyond that of the original model. Consider a deep layer in an LLM characterized by a weight matrix W. The following algorithm describes how to implement W within the VQC+TN framework:\n1. Apply an MPO decomposition to W and truncate the bond dimension $X$ of the MPO to preserve the model's accuracy. This step involves implementing the CompactifAI algorithm from Ref. [7], which rewrites W in a format that explicitly reveals the relevant correlations between the degrees of freedom in the layer. As a result, the memory usage of that layer is reduced due to the truncation of the bond dimension $X$, which effectively discards irrelevant parameters.\n2. Compute two quantum circuits of disentanglers, for the MPO, one for the input and one for the output. Specifically, compute two circuits composed of two-body unitary gates that remove as much entanglement as possible from the MPO. Since the MPO is an operator, one circuit is acting on the input, and the other circuit is acting on the output. The computation of disentanglers is a well-established procedure in the field of tensor networks [23], forming the core of techniques such as Entanglement Renormalization (ER) [24] and the Multiscale Entanglement Renormalization Ansatz (MERA) [25]. The disentanglers can be computed efficiently using the iterative methods described in Ref. [23]. The process continues until most (ideally all) of the entanglement is removed from the MPO, resulting in two unitary disentangling quantum circuits.\n3. Compute a new MPO representing the remaining part of the original MPO that cannot be disentangled. This process is described by the equation\n$MPO_{old} \\approx U \\times MPO_{new} \\times V^{\\dagger}$,\nwhere $MPO_{old}$ is the original MPO decomposition of the weight matrix W, $MPO_{new}$ is the \"remaining\" MPO, and $U, V^{\\dagger}$ are the unitary quantum circuits of disentanglers. Since $MPO_{old}$ is not necessarily Hermitian, we have that $U \\neq V$ in general. Also, since $U$ and $V^{\\dagger}$ remove entanglement from the original MPO, it follows that\n$X_{new} \\leq X_{old}$,\nwhere $X_{old}$ and $X_{new}$ are the bond dimensions of the old and new MPOs, respectively. The new MPO can be computed using the equation\n$MPO_{new} \\approx U^{\\dagger} \\times MPO_{old} \\times V$.\nThis computation can be efficiently carried out using standard tensor network approximation techniques, such as the Time-Evolving Block Decimation (TEBD) algorithm [26] for operators [27, 28].\nThe construction described above is illustrated in Fig. 2. This procedure is advantageous because it ensures a shallow quantum circuit combined with a low-dimensional MPO. Alternative approaches to introduce quantum circuits, such as using the polar decomposition $W=U \\times P$ of the weight matrix, where $U$ is unitary and $P$ is positive-definite, are also possible. However, our numerical tests indicate that this approach results in a non-shallow quantum circuit for $U$ and a $P$ with a very large bond dimension, due to the positivity constraint. In this context, the disentangler approach offers the most compact possible representation of $W$. This method can be applied uniformly across all deep layers of the LLM, allowing us to encode existing LLMs into this hybrid classical-quantum architecture.\nThe next logical step is to enhance the original classical LLM. Once the LLM is expressed in this format, the process is straightforward: extend the quantum circuits for $U$ and $V^{\\dagger}$ and increase the bond dimensions for $MPO_{new}$. The model parameters can be optimized through various techniques. Specifically, the unitaries in the quantum circuits $U$ and $V^{\\dagger}$ can be optimized variationally using a self-consistent method similar to the Variational Quantum Eigensolver (VQE) algorithm [9]. Similarly, the tensors in the MPO can be retrained using distributed tensor network retraining techniques, as implemented in Ref. [7].\nCrucially, when deploying this architecture on actual quantum hardware, the input to the quantum circuits must be encoded as a quantum state, and their output is obtained through sampling via qubit measurements. Several techniques can be used to encode the input state, including quantum GANs [29] and tensor network methods. After executing the optimized quantum circuit for $U$, sampling is performed to reconstruct the input state as accurately as possible for $MPO_{new}$, which can also be encoded as a tensor network. Likewise, the input to $V^{\\dagger}$ is encoded as a quantum state and the output is estimated via sampling. While these steps introduce additional approximations to the model, they can be controlled and compensated by increasing the depth of $U, V^{\\dagger}$ and the bond dimensions of $MPO_{new}$."}, {"title": "Discussion and Conclusions", "content": "The performance of this method is currently under evaluation for LlaMA models and generalizations thereof, and quantitative results will be reported in a future version of this manuscript. Notably, our method can also be combined with standard compression techniques for classical LLMs such as quantization [30], distillation [31], pruning [32], and matrix factorizations such as low-rank approximations [33]. Additionally, our first estimations indicate that quantum circuits with hundreds of qubits and layers should suffice to improve some of the current open-source LLMs. This is particularly promising, since it matches also the roadmap of quantum hardware providers such as IBM. Our vision is that Quantum LLMs, like the ones described in this paper, may become in the mid-term one of the first practical applications of noisy quantum computers."}]}