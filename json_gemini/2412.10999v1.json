{"title": "COCOA: Co-Planning and Co-Execution with Al Agents", "authors": ["K. J. Kevin Feng", "Kevin Pu", "Matt Latzke", "Tal August", "Pao Siangliulue", "Jonathan Bragg", "Daniel S. Weld", "Amy X. Zhang", "Joseph Chee Chang"], "abstract": "We present COCOA, a system that implements a novel interaction design pattern\u2014interactive plans\u2014for users to collaborate with an Al agent on complex, multi-step tasks in a document editor. COCOA harmonizes human and Al efforts and enables flexible delegation of agency through two actions: Co-planning (where users collaboratively compose a plan of action with the Agent) and Co-execution (where users collaboratively execute plan steps with the Agent). Using scientific research as a sample domain, we motivate the design of COCOA through a formative study with 9 researchers while also drawing inspiration from the design of computational notebooks. We evaluate COCOA through a user study with 16 researchers and find that when compared to a strong chat baseline, COCOA improved agent steerability without sacrificing ease of use. A deeper investigation of the general utility of both systems uncovered insights into usage contexts where interactive plans may be more appropriate than chat, and vice versa. Our work surfaces numerous practical implications and paves new paths for interactive interfaces that foster more effective collaboration between humans and agentic Al systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Since the advent of personal computing, researchers and practitioners in artificial intelligence (AI) and human-computer interaction (HCI) have set sights on developing intelligent AI agents that can help perform everyday computer-based tasks on our behalf [38, 61, 67, 68]. Thanks to recent advancements in large language models (LLMs) and LLM agent frameworks involving reasoning, planning, memory, and tool use [52, 88, 95, 97, 105], AI agents can now shop online [103], write software [42, 100, 102], participate in auctions [17], and perform other multi-step, computer-based tasks [2]. These developments demand deeper explorations into strategies for fostering synergistic collaboration between human users and AI agents [21, 40, 49]. After all, effective interaction with agents\u00b9 in the real world will need to make use of techniques that use human guidance to significantly improve agent performance and\n1 Henceforth, our use of the term \"agents\" will refer to Al agents rather than human agents."}, {"title": "ABSTRACT", "content": "utility, and better align with users' nuanced needs and preferences [49, 89]. However, explorations of such techniques are still nascent. Much of modern agent development efforts focus on autonomous agents without the need for human supervision, through techniques such as chain-of-thought reasoning [99] and interleaving reasoning traces and actions [105].\nWhile creating fully autonomous agents may be compelling in theory, it has notable limitations in practice. First, we cannot steer the agent with our expertise and worldly understanding. Two salient opportunities for agent steering in task completion workflows which we focus on in this work are planning and execution. Agents' abilities to generate executable plans of action have been shown to be unreliable across many domains [96]. Without a quality plan, agents can easily veer off track, wasting resources without achieving meaningful results. When executing the plan, human guidance can significantly boost agent performance-Shi et al. [89] found that even simple feedback from programmers to an agent for solving Olympiad-style programming problems increased agent accuracy from 0% to over 85%. Second, and perhaps more importantly, removing human agency in favor of Al agency can pose heightened safety risks, disempower us to think critically and be"}, {"title": "2 RELATED WORK", "content": "creative, and harm our well-being more generally [10, 12, 13, 21]. For many nuanced and subjective tasks, human input must be considered for AI-powered systems to be successful and aligned with users' personal needs and goals.\nWhen using LLMs through chat interfaces, a user can engage in dynamic task planning and execution by sending, receiving, and evaluating content generated by the model. However, prior work in HCI and human-AI interaction has suggested that planning-enriched workflows-where the LLM first decomposes a high-level task into lower-level ones and shows them to the user before taking action-can improve usability and transparency [56, 66]. In contrast, rather than automatically generating plans, researchers have also developed systems for humans to manually compose plans and then use those plans to guide LLM generation [106]. However, chat interfaces may not be well-suited for these workflows-consequently, emerging efforts have started to explore alternative interaction paradigms when working with agents to perform complex, multi-step, and often long-running tasks [66, 75, 82, 94]. For example, researchers have engineered node-based canvas interfaces for information sensemaking [94], ideation [82], and even task decomposition [101] with LLMs. While canvases provide freedom for"}, {"title": "2.1 Planning and Interactivity in LLM Agents", "content": "unstructured exploration with Al models, their lack of structure can hinder effective orchestration of efforts and agency between humans and Al when working through structured, sequential plans of action. We see opportunities for new interaction design patterns to serve as shared operational artifacts to bridge human and Al planning, such that it facilitates collaborative human-AI plan composition and execution."}, {"title": "2.2 Computational Notebooks", "content": "Prior work has envisioned autonomous software agents that can assist users in various ways, from performing complex tasks on the user's behalf to monitoring events and procedures [67]. These \"interface agents\" [61, 68] can perform visible actions in a direct manipulation interface without explicit instructions from the user. Two decades later, the same vision endures with LLM agents. Core components in LLM agent architectures typically consist of memory, reasoning, planning, and tool use [52, 64, 95, 97, 105, 110].\nCentral to LLM agents' operations is multi-step reasoning, often implemented by chain-of-thought (CoT) [83, 99, 105] and subsequent tree-based methods [104, 111]. CoT provides a series of intermediate reasoning steps as exemplars in prompting to boost performance on complex reasoning tasks [99]. CoT\u00b2 is crucial for facilitating task decomposition and therefore LLM agents' planning capabilities [17, 83]. However, even with CoT, critical investigations of LLMs' abilities to autonomously generate executable plans reveal an average success rate of only 12% across diverse domains [96]. Indeed, although CoT can help improve model planning for tasks with a well-defined solution and logical steps to get to that solution-e.g., numerical, tabular, and knowledge-based reasoning-[52], it may not be so effective in domains that require high-expertise, tacit knowledge to navigate towards ambiguous solutions [36]. Unlike model properties that show empirical improvement through scaling laws, limitations of planning in these domains may not resolve with scale alone as 1) tacit knowledge is not well-documented in training data and is thus difficult for a model to grasp [18, 25, 107], and 2) there may be no \"correct\" workflow for CoT to follow and verify the correctness post-hoc [43]. Scientific research is one such domain [107].\nIn light of this, recent work has recognized the need to interactively incorporate user feedback for improving LLM agents' planning capabilities and beyond [50, 56, 66, 96, 105], particularly within\n2Henceforth, we use CoT as an umbrella term encompassing chain-of-thought, trees-of-thought [104], and related methods."}, {"title": "2.3 AI-Powered Tools for Scientific Research", "content": "In this work, we introduce COCOA, a novel system for Co-planning and Co-execution with AI Agents. COCOA embeds Al agents into a document editor-a common site for planning-through a new interaction design pattern which we call interactive plans. Interactive plans orchestrate actions between a human user and an Al agent and enable flexible delegation of human and AI agency. As a result, COCOA supports human-Al co-planning: the agent, when invoked in the document, will first propose a plan of action that the user can freely modify. This plan is interactive and seamlessly integrated into the document-the user can then edit the plan steps through familiar interactions that mirror typical document editing and assign steps to the agent or themselves. COCOA also supports human-AI co-execution: drawing inspiration from the design of computational notebooks, the interactive plan allows the user and the agent to collaboratively complete one step at a time or re-execute steps as desired. The user can interactively refine the agent's intermediate outputs and also manually take over steps themselves to steer the workflow in a more effective direction. Most importantly, COCOA interleaves co-planning and co-execution, such that users can smoothly transition between the two and modify their plans based on outputs from execution, and vice versa.\nWe motivate the design and development of COCOA by focusing on scientific research as a use case, due to the complex, multi-step, and information-dense workflows researchers often engage in throughout their work [27, 28, 46, 47, 59]. Through formative studies with 9 researchers, we learned that their real-world research project documents-a running document where researchers informally jot down ideas, open questions, meeting notes, and more-contain rich externalizations of researchers' (often preliminary) reasoning and thus serve as ideal environments for interacting with an agent to further tackle research ideas. We also gathered examples of participants' plans and preferences for tasks they would delegate to Al to inform the behavior of COCOA. After developing COCOA, we evaluated it against a strong chat baseline-a more familiar interface powered by the same agent-through a user study with 16 researchers. We found that compared to our baseline, COCOA enabled users to better steer the agent in more helpful directions without sacrificing ease of use. A deeper investigation into qualitative data from the study revealed that the general utility of both systems depended on the nature of the task, and we unpack scenarios in which using interactive plans as a design pattern may be more appropriate than chat interfaces, and vice versa. We conclude with a discussion of how agent interfaces may simultaneously capture benefits of interactive plans and chat, practical reasons-namely cost and safety-for including user steps within interactive plans, and the broader applicability of interactive plans outside of the document context."}, {"title": "3 FORMATIVE STUDY", "content": "Significant efforts in recent years have advanced how AI can augment scientific research. These efforts include tools for paper reading [15, 84] and skimming [28], literature review [59, 80], paper recommendation [20, 45, 46, 92], information retrieval and sensemaking [14, 27, 48, 98], as well as toolkits [65] and academic search engines [23, 30, 33] that power or combine these tools. More generally, progress in this area has given rise to excitement for a \"computational inflection for scientific discovery\" [37].\nOf particular interest to us are works that leverage these recent advances to help researchers with literature-augmented planning. Planning a research project is a cognitively demanding, complex process consisting of iterative cycles of divergent and convergent thinking grounded in literature review [15, 22, 80]. Research prototypes have been developed to indirectly assist with planning in two primary ways. Some prototypes help researchers interactively make sense of papers to orient their work [15, 27, 48, 59, 80]. Other prototypes have also attempted to generate high-fidelity research ideas directly [8, 35, 41]. However, when these ideas were evaluated by human researchers, they received dismal scores. On a 5-point Likert scale (1 = not interesting, 5 = very interesting), Gu et al. found that 75% of generated suggestions from their study received a rating of 3 or lower, with 37% receiving a rating of 1 [35]. Similarly, Baek et al. saw limited enthusiasm from expert human annotators for Al-generated ideas from their agent [8].\nIs the generation of full research artifacts (e.g., research questions, proposals) the most promising path to assist researchers? We question this assumption in our work. Indeed, when working with other (human) collaborators, researchers often find richness in co-evolving partially completed artifacts [43, 57, 91, 93] and answering or asking questions that stimulate critical thinking and reflection [78, 79]. Another reason for the lack of uptake in AI-generated"}, {"title": "4 COCOA: SYSTEM WALKTHROUGH AND IMPLEMENTATION", "content": "Concretely, our work makes the following contributions:\n(1) A formative study with 9 researchers that uncovered user needs and opportunities to better support planning and execution in research project documents.\n(2) COCOA, an interactive system that implements a new design pattern-interactive plans-in a document editor for researchers to engage in co-planning and co-execution with an Al agent.\n(3) A user evaluation of COCOA with 16 researchers, where we found that interactive plans with novel interaction affordances enabled users to better steer the Al agent without sacrificing ease of use when compared to a conversational chat baseline.\n(4) Qualitative data from the above provide valuable insights into when interactive plans may be preferred over chat interfaces, and vice versa.\n(5) An in-depth discussion of our work's practical implications, including how interactive plans may be used in diverse settings beyond documents, and the trade-offs when compared with chat-based interfaces."}, {"title": "3.1 Participants and Procedure", "content": "Given the opportunities surfaced from prior work, we aim to improve collaboration between humans and AI agents by leveraging plans as a shared operational representation. We focus on planning for scientific research as a use case. Scientific research presents a challenging planning scenario for both humans and agents due to its complex, multi-step, and fluid workflows and the nuanced decisions researchers make upon processing large amounts of information from information-dense scientific papers. Specifically, we use project documents-a central, running document in which researchers may jot down project ideas, updates, to-dos, meeting notes, and more-as the agent environment. We thus conducted a formative study to answer the following research questions:\nRQ1: What are the properties and opportunities of researchers' project documents?\nRQ2: How do researchers engage in planning within project documents?\nRQ2.1: How can we characterize researchers' intentions for planning?\nRQ2.2: How can we characterize the steps researchers include in their plan to meet those intentions?\nRQ3: How would researchers prefer to interact with an AI-powered co-planning agent in their project documents?"}, {"title": "3.2 Data Analysis", "content": "We recruited 9 Ph.D. students (detailed demographic in Table 1 of Appendix A) through an interest form sent to a research organization's internal Slack channel and the authors' professional connections. We targeted Ph.D. students and postdoctoral researchers as they are often the primary owners of research project documents and lead the detailed planning and execution of research projects."}, {"title": "3.3 Findings", "content": "To answer RQ1, the first author used a hybrid inductive-deductive coding process [26] to code participants' submitted project documents. The first 5 documents were inductively coded to surface common structural elements before the elements were deductively applied to the remaining 4. We iterated on existing elements and added new ones as needed. For RQs 2.1 and 2.2, we performed inductive thematic analysis on two documents-participants' submitted project documents and in-study planning documents. For RQ2.1, the first author sourced intentions for planning within submitted project documents by locating the snippet of text that initiates a plan. For example, if the plan consists of a bulleted list, the intention is often expressed in the lead-in text that immediately precedes the list. We note that an individual plan item can also signal planning intent if it contains a nested plan. The first author performed thematic analysis by inductively coding the extracted text. For RQ2.2, the first author identified and extracted plan steps participants wrote in both documents before inductively coding them. For RQ3, the first author performed open coding on the study transcripts before thematically analyzing the coded snippets. To enrich the data, the first author also extracted and inductively coded all requests to the assistant from participants' planning documents. The codes were discussed and iterated upon with other project team members on a weekly basis."}, {"title": "3.3.1 [RQ1] Project documents were continually updated for planning and progress tracking.", "content": "We present the results of our thematic analysis across study transcripts, project documents, and planning documents. We redact any project-specific details for privacy and intellectual confidentiality."}, {"title": "3.3.2 [RQ2.1] Participants expressed planning intentions in documents as questions or flags.", "content": "We examined participants' real-world research project documents and found several interesting characteristics. First, they were a running planning document that is continually updated throughout a project's lifecycle. Some documents (P1, P4-6) were in a diary-like format with explicit dates, and others also had clear indicators of chronology, such as having \"Current\" and \"Archive\" sections. The content of these documents commonly included meeting notes, planned to-do items, progress"}, {"title": "3.3.3 [RQ2.2 & RQ3] Literature-augmented tasks present valuable opportunities for incorporating Al support.", "content": "Our analysis of these research documents showed that intention to plan were often expressed using questions or flags (e.g., \"TODO\u201d or \u201c[will try]\u201d), and that goals were often related to information seeking (e.g., literature search and review) and other actions (e.g., running experiments). Specifically, many asked themselves a question (often in a how? and can we? format) before creating a plan to answer it. For example, P7 asked \"How to construct the corpus?\" before creating a plan to detail one approach. While sometimes these questions were written down as a way to provoke thoughts without intent for planning to answer them, flags signaled clear intentions to create a plan. A few participants (P3, P4, P7) also initiated planning from hypotheses they hoped to verify. This can be accomplished through information seeking, running experiments, or both. For example, P3 hypothesized that \u201csimilarity matching scores between [texts] describing [concepts] will be higher for [criteria] than those different than [criteria]\u201d and then laid out an experimental plan to verify that hypothesis.\nIn sum, in all research project documents, we saw places where participants intended to initiate a multi-step plan to address different research tasks. The most common category of tasks are ones that were literature-augmented (e.g., literature search and understanding), which are often combined with a wide variety of other research actions, such as experiment/artifact design (P3), experiment/code execution (P4), data inspection and synthesis (P7), communicating and discussing results (P5), and ideation (P2)."}, {"title": "3.3.4 [RQ3] Participants preferred to perform higher-level reasoning and synthesis themselves.", "content": "When asked about incorporating AI support into their research documents, all participants expressed a desire to use AI to help with literature-augmented tasks-exploring and understanding relevant literature to inform decision-making. This finding echo recent surveys on how researchers leverage LLM to conduct research, where the most frequent usage category was information-seeking [60].\nUsing the design probe, participants conducted a wide range of high-level to specific literature-grounded tasks, from \"what are"}, {"title": "4.1 Co-Planning", "content": "Participants also pointed to tasks in their plans that they did not want AI to automate away. In particular, participants saw higher-level reasoning and information synthesis as critical tasks they wanted to do themselves. They were also often unsatisfied with Al's outputs on these tasks. P3 explains: \"this tinkering process around reading and playing around with things is what gives you the ideas. I don't know if I want those things automated because the process is as helpful as the final result.\" P5 agreed that they \u201cwouldn't want it to be making the final decisions for sure. Just give me inspiration for where I can go.\u201d Specifically, they shared that \u201cthe main thing I worry about is feeling enough ownership if Al makes more consequential decisions. An overeager AI assistant that attempts to perform tasks researchers prefer to do themselves is frustrating because it does not complement the user's work and instead creates undesired noise for them to filter through."}, {"title": "4.1.1 Invoking the agent and selecting plans.", "content": "Participants also shared how they would like to receive Al assistance. P1 and P2 envisioned an AI system \u201cactively engaging with the content I'm writing [...] after I write each statement, the system could retrieve relevant papers that could provide background or related work for me to read more.\u201d Although desirable, participants also identified a challenge with this kind of interaction: context specification. P2 was concerned that they would need to \u201cprompt [the AI] with a lot of background,\u201d but realized that within their project document, they \u201cmight have [the background] written down so it's fine.\u201d For planning in particular, P9 appreciated that during the study, either a facilitator or a participant wrote out plan steps in the document: \u201cthe ability to layout the steps when I start something was very helpful [for] visualizing the outcome.\u201d Thus, there is strong motivation for designing an agent to work within and draw from a project document."}, {"title": "4.1.2 Agent steps and user steps.", "content": "N\u00f3\u00edr\u00edn's project document is an informal, reverse chronological log of research progress and includes information such as meeting notes, rough ideas, links to literature, and questions for herself and her collaborators. To initiate co-planning, N\u00f3\u00edr\u00edn can highlight any piece of text in her document that (implicitly or explicitly) contains a request she would like to tackle with the agent. She highlights a question she previously wrote but has not gotten an opportunity to explore yet: \"What are new ways Al agents can interactively elicit human feedback?\u201d She then invokes the agent with a button that appears above the highlighted text in a floating menu. The agent analyzes the request within the context of other text in her document and proposes a series of plans for answering the question. These plans are displayed in the document via a plan selector UI within the editor, which allows N\u00f3\u00edr\u00edn to browse and select a plan. Upon N\u00f3\u00edr\u00edn's selection, a plan becomes fully interactive within the document. This interactive plan is comprised of a series of editable plan steps as a bulleted list. Each plan step further consists of several interactive components."}, {"title": "4.1.3 Editing the plan step description.", "content": "N\u00f3\u00edr\u00edn can use the step assignment toggle to assign the step to herself (a \"user step\") or the agent (an \"agent step\"). User steps are highlighted in blue. When executing the plan, the agent will automatically attempt agent-assigned steps but will request N\u00f3\u00edr\u00edn's input on user-assigned steps; when that happens, a step will be highlighted in orange. For now, N\u00f3\u00edr\u00edn is satisfied with how the agent has left some steps that require higher level reasoning to herself."}, {"title": "4.1.4 Replanning.", "content": "The step description appears in an editable text field and describes a subtask in natural language while doubling as high-level instructions for the agent. N\u00f3\u00edr\u00edn can edit the step description just like editing any other bulleted list in her document. The system will prompt N\u00f3\u00edr\u00edn to save any changes upon editing with the keyboard shortcut Cmd/Ctrl+S. In addition to editing the step description herself, N\u00f3\u00edr\u00edn can also employ LLM assistance-highlighting any text within a step description will present a button in a floating menu labeled \"Suggest alternate step.\" When clicked, an LLM suggests a step to replace the current one, informed by the previous steps. N\u00f3\u00edr\u00edn tries this a few"}, {"title": "4.1.5 Adding and deleting steps.", "content": "After reverting back to the original step description, N\u00f3\u00edr\u00edn is curious about authors who have published on \"interactive feedback mechanisms in AI\" rather than seeing papers directly. She edits the step description to reflect this and saves it. At this point, the plan's trajectory has changed-the next step, which has the agent suggest common themes in papers, is now irrelevant because none of the previous steps retrieve papers. The system"}, {"title": "4.2 Co-Execution", "content": "In COCOA, N\u00f3\u00edr\u00edn can easily add and delete steps from the plan. She notices that the agent has proposed a summary step that may be irrelevant, so she highlights the entire step and hits Backspace to delete it from the plan. This interaction mirrors how one would delete an item in a bulleted list elsewhere in the document. She also thinks it may be useful"}, {"title": "4.2.1 Continuous and stepwise execution.", "content": "Now that N\u00f3\u00edr\u00edn has made her desired edits to the plan, she is ready to co-execute it with the agent. There are two modes of co-execution: continuous and stepwise. Each step has a step completion indicator that indicates whether the step has not yet been run (a bullet point), is in progress (a spinner), requires user input (a question mark badge), or is complete (a checkmark). Drawing inspiration from computational notebooks, these indicators offer N\u00f3\u00edr\u00edn a glanceable way to stay informed about the agent's progress and where she needs to take action in the plan."}, {"title": "4.2.2 Completing user steps.", "content": "N\u00f3\u00edr\u00edn can trigger continuous co-execution with the [Run all] button (or an [Update and run] button if the plan was edited) at the top of the plan. In continuous co-execution, the agent will automatically continue onto the next step as soon as it (or the user) has completed the previous one, until it reaches a user step or the end of the plan.\nBy contrast, stepwise co-execution allows N\u00f3\u00edr\u00edn to run one plan step at a time, taking inspiration from computational notebooks. When N\u00f3\u00edr\u00edn hovers her mouse over the bullet point of the first plan step, the bullet point turns into a play button that, when clicked, will run only that step. Upon the step's completion, the agent will not execute the next step until N\u00f3\u00edr\u00edn manually clicks that step's play button. Unlike some computational notebooks, stepwise execution does not permit out-of-order execution of steps\u00b3 because\n3We also note that out-of-order execution in computational notebooks is a major user pain point identified in prior academic work [16, 55] and industry reports [34, 44]."}, {"title": "4.2.3 Editing outputs of agent steps.", "content": "The agent completes the first step of the plan and arrives at a user step. Here, the agent requires user input to continue. N\u00f3\u00edr\u00edn can easily see this because of the orange highlighting on the step for which her input is needed. She clicks into the text that appeared under the step prompting her to provide her guidance and opens an interactive sidebar. The sidebar offers her a built-in paper search functionality connected to the Semantic Scholar academic database and she uses this to add some relevant papers she previously encountered. She clicks Save, adding the papers and their metadata to a plan-specific context pool that the agent references when completing future steps. The agent then proceeds to complete the next step with this updated context to expand upon N\u00f3\u00edr\u00edn's selected papers."}, {"title": "4.3 Iterative Co-Planning and Co-Execution", "content": "The agent has now completed the third step, where it conducted a paper search guided by N\u00f3\u00edr\u00edn's seed papers from the previous step. N\u00f3\u00edr\u00edn clicks into a text preview of the output that appeared under the step, opening a sidebar similar to the one she used in the previous step to add papers, this time populated with interactive paper cards. This presents opportunities for her to further guide the agent with her expertise by removing"}, {"title": "4.4 Implementation Detail", "content": "In addition to supporting co-planning and co-execution independently, COCOA is designed to support synergistic blending of the two. As N\u00f3\u00edr\u00edn co-executes the plan with the agent, she encounters an author that the agent identified who publishes highly relevant work and wishes to further explore the author's papers. She edits the following step's plan description to satisfy this. She reruns that step and receives a new batch of papers, and the agent automatically reruns the rest of the plan with the updated output. She is also not satisfied with the agent's interpretation of common themes and believe that she can surface deeper insights by reading the papers herself, so she toggles that step to be a user step. After having some time to read the papers, she returns to her document and jots down her insights. In doing so, has already started reasoning about new feedback elicitation mechanisms for agents. She now finds the next step repetitive, so she deletes it. She reruns the last step on her updated notes to see the agent's constructive critiques. Overall, her use of COCOA is highly iterative, and she smoothly transitions back-and-forth between co-planning and co-execution.\nSo far, N\u00f3\u00edr\u00edn has just been interacting with one plan. While waiting for the agent to complete a series of steps, she uses the [Collapse] button to hide the steps and only reveal essential information about the plan's status. She identifies a couple more areas of her document that merit further exploration and invokes the agent on them. Soon, she is co-planning and co-executing with multiple agents on different parts of her document in parallel. This is not cognitively burdensome for her as she only has to attend to one plan at a time as the others are all collapsed and running in the background"}, {"title": "4.4.1 Technical stack.", "content": "COCOA is implemented as a web application with a Next.js and TypeScript frontend communicating with a Flask backend. The frontend uses the Tiptap\u2074 framework for the main document editor. Each document supports synchronous collaboration via Hocuspocus\u2075 and all changes are auto-saved to Tiptap\n4Tiptap is an open-source headless wrapper for ProseMirror, a toolkit for building web-based rich text editors.\n5Hocuspocus is a Y.js WebSocket backend for conflict-free real-time collaborative web apps."}, {"title": "5 USER STUDY", "content": "We synthesize our formative study findings into three design goals for an interactive system that facilitates meaningful collaboration between researchers and AI agents by using plans as a shared representation.\nDG1: Integrate seamlessly into a document environment. We learned from answering RQ1 that project documents contain rich externalizations of reasoning. This is key information an Al agent can use to better assist the researcher. We also learned from answering RQ3 that the document is an appealing environment for researcher-agent collaboration, but requires careful information management strategies to prevent unwanted distractions. Thus, we strive to seamlessly embed agents into documents. This involves 1) allowing the researcher to use already-familiar document editing affordances and representations to interact with the Al agent, and 2) strategically managing outputs to avoid excessively cluttering the document.\nDG2: Allow flexible delegation of agency between researcher and AI. As we learned from RQs 2 and 3, researchers may not want to delegate all parts of a plan to AI. This preference may also be context-dependent and constantly shifting. Satyanarayan's calls for flexibly delegating agency between humans and AI [87] may be one promising approach. We offer a concrete implementation of this flexible delegation in our system.\nDG3: Provide opportunities for researcher-AI collaboration in both planning and execution. Delegation is an important aspect of planning, but it does not guarantee successful execution of a plan. We saw throughout the probe activity that researchers encountered (and sometimes even expected) imperfect Al outputs when executing tasks with AI. They then envisioned stepping in to provide the agent with more context or iterating on the outputs themselves. We thus see two stages for fertile researcher-AI collaboration: planning (collaboratively devising a plan to tackle a problem) and execution (collaboratively completing steps in the plan). In practice, these two stages are closely intertwined and synergistic, so we aim to support both simultaneously."}, {"title": "5.1 Participants", "content": "We conducted a within-subjects user study with 16 researchers to evaluate the effectiveness of COCOA-and by extension, our design pattern of interactive plans-against a strong chat baseline. Specifically, our study aimed to address the following research questions:\nRQ1: How does COCOA compare to our chat baseline for ease of use, steerability, and general utility in research project documents?\nRQ2: When did researchers prefer interacting with an Al agent through interactive planning over our chat baseline, and vice versa?\nRQ3: What kinds of steps did researchers wish to assign to an agent and themselves in practice?"}, {"title": "5.2 Baseline System", "content": "We recruited 16 Ph.D. and postdoctoral researchers (14 Ph.D.s, 2 postdocs; 10 female, 6 male) in computer science (CS) or CS-adjacent areas via university mailing lists, word of mouth, social media recruitment messages (on Twitter/X, Mastodon, Bluesky), and personal connections. One participant also participated in our formative study. 15 researchers were based in the United States, and 1 was based in Canada. Participants' research areas ranged from large language models, to ubiquitous computing, to visualization; broadly, they spanned HCI (n = 10), ML (n = 3), and NLP (n = 3). We sent out a recruitment form to collect basic demographic details, frequency of AI use in research, and a copy of a project document from an ongoing or completed project document to use during the study. We recruited on a first-come, first-served basis as we conducted our studies and closed recruitment when we approached"}, {"title": "5.3 Research Task", "content": "The baseline system (Figure 11) was a chat interface powered by the same LLM agent used in COCOA: it used GPT-40 with access to the same tools for completing literature-related tasks using a combination of Semantic Scholar and standard LLM capabilities (summarization, accessing knowledge stored in weights, etc.). While we could integrate interactive plans within chat and vice versa (more on this in Section 7.1), we opted not to for a cleaner comparison of the two design patterns. The design of the chat interface closely resembled that of popular LLM chatbots such as ChatGPT, Claude, and Gemini. This chat interface was used alongside the same document editor as the one COCOA uses, except we removed the option to invoke the agent, thereby eliminating all interactions related to co-planning and co-execution. During the study, participants positioned the chat interface beside the baseline document editor for easy access."}, {"title": "5.4 Procedure", "content": "Since the capability of our underlying agent focused on helping users explore and understand scientific literature, we designed our study so that our participants tackled a literature-augmented task in each of CocOA and the baseline. By literature-augmented, we mean tasks that are to be completed by referencing or drawing from academic literature; in the study, participants worked on not only literature review tasks, but also tasks where researchers need to make literature-informed decisions such as study design and ideation. This stands in contrast with tasks focused on writing mechanics (e.g., rewording a sentence) or tasks that do not involve literature (e.g., booking travel) that past work has covered (e.g., [53, 58, 66, 102, 103, 108"}]}