{"title": "Differentiable Mobile Display Photometric Stereo", "authors": ["Gawoon Ban", "Hyeongjun Kim", "Seokjun Choi", "Seungwoo Yoon", "Seung-Hwan Baek"], "abstract": "Display photometric stereo uses a display as a programmable light source to illuminate a scene with diverse illumination conditions. Recently, differentiable display photometric stereo (DDPS) [1] demonstrated improved normal reconstruction accuracy by using learned display patterns. However, DDPS faced limitations in practicality, requiring a fixed desktop imaging setup using a polarization camera and a desktop-scale monitor. In this paper, we propose a more practical physics-based photometric stereo, differentiable mobile display photometric stereo (DMDPS), that leverages a mobile phone consisting of a display and a camera. We overcome the limitations of using a mobile device by developing a mobile app and method that simultaneously displays patterns and captures high-quality HDR images. Using this technique, we capture real-world 3D-printed objects and learn display patterns via a differentiable learning process. We demonstrate the effectiveness of DMDPS on both a 3D printed dataset and a first dataset of fallen leaves. The leaf dataset contains reconstructed surface normals and albedos of fallen leaves that may enable future research beyond computer graphics and vision. We believe that DMDPS takes a step forward for practical physics-based photometric stereo.", "sections": [{"title": "1. Introduction", "content": "Estimating high-quality surface normals is a long-standing problem in computer vision and graphics. Display photometric stereo reconstructs surface normals using conventional monitors and cameras. Differentiable display photometric stereo (DDPS) enhances this process by learning illumination patterns, resulting in improved surface normal reconstruction compared to heuristic display patterns. However, DDPS has a limitation in that it uses a desktop environment with a fixed large LCD monitor and a polarization camera. The large LCD monitor is neither portable nor adaptable, requiring objects to be transported to the fixed setup, which limits flexibility.\nThis paper introduces Differentiable Mobile Display Photometric Stereo (DMDPS). We use a mobile phone as a portable device with a display and an on-device camera. This setup resolves the desktop fixed-location issue. However, using a phone as an illumination source reduces both the light-view angular samples and intensity of light. We address this challenge by increasing exposure time and using HDR imaging. Additionally, since most mobile phones do not allow simultaneous use of the light source and the camera, a custom mobile application was developed to overcome this limitation. The app enables precise control over ISO, exposure time, and frame duration, displays desired patterns, and captures objects using the front camera. It also supports RAW image capture and allows for multiple exposure times through preset configuration.\nTo enable image capture in general environments rather than dark rooms, HDR and RAW images are employed. External light in non-controlled settings can degrade the quality of surface normal reconstruction. To mitigate this, the system uses RAW images (without post-processing) and HDR images captured at varying exposure times. Furthermore, mobile phone cameras, which typically have lens-based image distortion (e.g., radial distortion), are corrected using intrinsic calibration and undistortion techniques, thereby improving accuracy.\nIn summary, our contributions are as follows:\nWe propose Differentiable Mobile Display Photometric Stereo (DMDPS) that allows surface normal reconstruction using a mobile phone in general environments instead of a dark room and eliminates the need for monitors and polarized cameras.\n\u2022 We developed a custom mobile app enabling RAW image capture, multiple exposure configurations, ISO value control, and more.\n\u2022 We applied DMDPS to real-world objects (e.g., fallen leaves), and revealed surface normals and albedos reconstructed from these objects."}, {"title": "2. Related Work", "content": "Display and Imaging Systems for Photometric Stereo Various imaging systems for photometric stereo have been proposed, including DSLR camera flashes [3, 4], LCD monitors [1], and mobile phone flashes [5, 6]. Additionally, LCD monitors and polarization cameras [1] have been used to leverage diffuse images created by polarized light in display photometric stereo. In this paper, we perform photometric stereo using only the screen and camera of a mobile phone. Compared to the aforementioned imaging systems, this approach requires just one device-a mobile phone-making it the most portable solution. To implement DMDPS on mobile devices, we developed a custom app that supports RAW image capture, exposure time and ISO value control, and multi-exposure time capturing through presets.\nLearned Illumination Pattern A critical challenge in photometric stereo is determining the optimal illumination pattern. The illumination pattern defines how the intensity of light sources is distributed, and selecting an effective pattern is crucial for accurate surface normal reconstruction. One commonly used standard is the one-light-at-a-time (OLAT) pattern, where each light source is activated at maximum intensity one at a time [7, 8]. This method is effective when each light source provides sufficient energy for the camera sensor to capture without introducing significant noise [9]. While several heuristic patterns exist, some methods employ learned patterns optimized for high-quality surface normal reconstruction. These patterns are generated by comparing estimated normal maps with ground truth data and creating error maps across different patterns [2]. In our work, we enhance the learning process by introducing a Gaussian filter and adjusting the learning rate. Because our imaging system transitions from using an LCD monitor and polarization camera to a mobile phone, the Gaussian filter's sigma value and the learning rate are fine-tuned to accommodate the new imaging environment.\nCapturing Environment Conventional photometric stereo method typically capture images in a dark room [1, 10]. This is because external light sources can interfere with the designated illumination pattern, reducing its effectiveness and resulting in similar photographed images across different patterns. To address this limitation, we propose capturing images in general environments using HDR imaging. Instead of relying solely on standard RAW images, HDR images are captured at multiple exposure times to improve robustness against external light interference.\nPhotometric Stereo Dataset Various datasets have been proposed [12, 13, 14] for evaluating or training photometric stereo methods, including both synthetic [15] and real-world datasets. In our work, we utilize real-world datasets, obtaining ground truth for training datasets using 3D-printed objects. Additionally, we applied DMDPS to in-the-wild objects, such as fallen leaves, and revealed the reconstructed surface normals and albedos through this process."}, {"title": "3. DDPS vs DMDPS", "content": "3.1. DDPS Review DDPS focuses on designing and learning illumination patterns to achieve accurate surface normal reconstruction of objects. The process is broadly divided into three stages: database acquisition, pattern learning, and testing. First, during the database acquisition stage, base-illumination images of 3D-printed objects are captured, and ground-truth surface normal maps are obtained using their corresponding 3D modeling files. Next, in the pattern learning stage, optimized patterns for high-quality surface normal reconstruction are learned using a real-world training dataset. A detailed explanation of this process is provided in Section 6, Reconstruction. Finally, during the testing phase, various real-world objects are photographed using the learned patterns, and their surface normals are reconstructed.\n3.2. Difference 1: Mobile Imaging System\nDDPS uses an LCD monitor and a polarized camera to achieve diffuse-specular separation and improve surface normal reconstruction by focusing exclusively on the diffuse image. Additionally, the LCD monitor provides high light intensity. In contrast, DMDPS uses a mobile phone for both pattern display and image capture. To achieve simultaneous pattern display and image capture, a custom application"}, {"title": "4. Mobile Display System", "content": "4.1. Imaging System\nWe used a mobile phone for both pattern display and image capture. The device utilized was a Galaxy S22, equipped with a 6.1-inch display (19.5:9 aspect ratio) and a resolution of 2340 x 1080 pixels. The front camera is a 10-megapixel sensor. Image capture was performed in a standard room rather than a dark room. The mobile phone was mounted on a cradle, and the custom app was used for image capture.\nSince the imaging device shifted from an LCD monitor to a mobile phone, camera calibration was necessary. Camera calibration determines the internal parameters required to convert image coordinates into world coordinates. The transformation formula is as follows:\n$\\begin{bmatrix} x \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f & 0 & p_x \\\\ 0 & f & p_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} R & -Rt \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} X_w \\\\ 1\\end{bmatrix}$\nwhere x represents the homogeneous coordinate of the 2D image point, f denotes the focal length, and px, py are the difference between the image coordinate and the camera coordinate. R represents rotation, and t denotes the translation between the world coordinate and the camera coordinate. X is the homogeneous coordinate of the 3D world point.\nCamera calibration was performed using MATLAB. Images of a checkerboard taken at various angles were processed with MATLAB's camera calibration tool to determine the camera's internal parameters, including focal length and principal point.\n4.2. Capture Application Design\nWhen using a mobile phone instead of a polarized camera to capture images, we encountered the following challenges:\nShared Device for Pattern Display and Image Capture The mobile phone must simultaneously display multiple patterns and capture images using its front camera while each pattern is reflected on the object.\nRaw Image Requirement General photography formats like JPG or PNG undergo processing steps such as noise reduction, gamma adjustment, and compression of highlights and shadows. These processes result in a loss of dynamic range and image information. To preserve the dynamic range needed for HDR imaging, RAW images were used instead of processed formats like JPG or PNG."}, {"title": "4.3. Image Processing", "content": "To achieve high-quality surface normal reconstruction, followings were considered.\nGaussian Filter Images captured with a mobile phone tend to exhibit more noise compared to those taken with an LCD monitor, making noise reduction essential. Various filters, including average filters and median filters, were tested. The Gaussian filter yielded the best results for surface normal reconstruction by assigning greater weight to pixels closer to the target point, thereby smoothing the image effectively.\nHDR Imaging Our study proposes capturing objects in general environments instead of a dark room. In such environments, external light sources other than the intended illumination-can interfere with the image capture process. This interference reduces the influence of the displayed pattern's light, causing the captured images to appear similar regardless of the pattern used.\nThis issue is particularly problematic when capturing basis images [2]. Ideally, these images should vary based on the positions of several superpixels. However, strong external lighting and the limited brightness of mobile phone displays can diminish this variation, leading to poor reconstruction results.\nTo address this issue, we adopted high dynamic range (HDR) imaging. HDR enhances the dynamic range by maximizing the contrast between bright and dark areas in an image. HDR imaging involves two primary steps:\nMerging The captured low dynamic range (LDR) images are combined into a single HDR image. Each pixel is weighted during this process, as represented by the merging equation,\n$I_{HDR}(x,y) = \\Sigma t_i W(I_t(x,y)) \\frac{I_t(x,y)}{t_i},$\nwhere W is weight function.\nExposure Bracketing This process, discussed later in Section 4.5, involves capturing images at multiple exposure times to create HDR images.\nUndistortion Mobile cameras typically have a wide field of view (FOV), resulting in significant distortion. This distortion manifests as both radial distortion and tangential distortion. Radial distortion is caused by the lens's refractive index, with distortion intensity increasing as the distance from the image center grows. The distorted position is represented as follows:\n$x_{distorted} = x(1 + k_1r^2 + k_2r^4 + k_3r^6)$,\nwhere x is x-coordinates in an undistorted image, k1,k2, k3 is radial distortion factor and $r^2 = x^2 + y^2$.\nTangential Distortion caused by misalignments during assembly, such as a misaligned lens center or lack of horizontality between the lens and sensor. Distortion distribution varies in the form of an ellipse. The equation for the distorted position is as follows.\n$x_{distorted} = x(2p_1y + p_2(r^2 + 2x^2))$\nTo correct these distortions, distortion coefficients (k1,k2,p1,p2,k3) were obtained through camera calibration. The undistortion process involves:\n1. Normalization: Converting the captured image to normalized coordinates using the inverse matrix of the camera's internal parameters.\n2. Distortion Application: Applying radial and tangential distortion models to the normalized coordinates.\n3. Undistortion: Reapplying the camera's internal parameters to the distorted coordinates to correct the image."}, {"title": "4.4. Capture Process", "content": "The image capture process was carried out using the custom app. After setting the desired options, the phone was placed on a cradle. When a pre-downloaded pattern was uploaded using the image upload button, the app automatically displayed the patterns and captured images with the front camera. Additionally, exposure bracketing was performed for HDR image production.\nExposure Bracketing: This process involves capturing multiple low dynamic range (LDR) images at varying exposure times. The image formation process is represented as follows:\n$I_{t_i}(x, y) = clip[t_i \u00b7 I(x, y) + noise]$,\nwhere ti denotes the exposure time. I(x,y) represents the intensity at a specific pixel (x,y) in the captured LDR image.\nUsing preset settings, we captured images at various exposure times. The captured RAW images were then merged into HDR images through exposure bracketing. These HDR images were used for surface normal reconstruction, resulting in improved outcomes."}, {"title": "5. Reconstruction", "content": "5.1. Pattern Learning\nTo learn display patterns, we utilize a 3D-printed training dataset containing ground-truth normal maps NGT and a basis image B. The K display patterns are denoted as P, which serve as our optimization variables.\nWe implemented an optimization pipeline using a differentiable image formation function f\u2081 and a differentiable photometric stereo method fn. Function f\u2081 simulates captured images I for the training scene based on the display patterns being optimized. Subsequently, fn processes these simulated images to estimate surface normal N.\nFor a given display pattern Pi and basis image B, the raw image simulation is expressed as:\n$I_i = f_1(P_i,B) = \\Sigma_{j=1}^{K} B_j P_{i,j},$\nwhere Pij is the RGB intensity of the j-th superpixel in Pi. This process is performed on K patterns to obtain set of synthesized images I.\n$\\underset{P}{minimize} \\underset{B,NGT}{\\Sigma} loss(f_n({f_1 (P_i, B)}^K_{i=1}, P), N_{GT}),$\nwhere the loss function penalizes the angular difference between estimated and ground-truth normal:\n$loss = (1 - N \\cdot N_{GT})/2$\nTo ensure the display patterns remain within a physically valid intensity range [0,1], a sigmoid function is applied to P during optimization. The Adam optimizer is used to minimize this loss [11]."}, {"title": "5.2. Normal Reconstruction", "content": "Surface normal reconstruction is performed using images captured or simulated under the optimized display patterns P. For a given pixel, the RGB intensity under the i-th display pattern is denoted as Ic, where c\u2208 {R, G, B}. Illumination from the j-th superpixel is represented by the spatially varying vector lj, computed based on the relative position of the superpixel and the scene point. Assuming a planar surface at a fixed distance of 10 cm from the camera, the relationship between observed intensities and surface normals is modeled as:\n$I = p \\odot PIN,$\nwhere I \u2208 R3K X 1 is vectorized intensity, p \u2208 R3K X 1 is albedo, N\u2208 R3 X 1 is surface normal, P\u2208 R3K X b is pattern intensity matrix, l\u2208 Rb X 3 is illumination direction matrix and is Handamard product.\nThe unknowns in this equation are the surface normals N and albedo p. For numerical stability, p is set as the maximum intensity observed across captures. The surface normals are computed using the pseudo-inverse method:\n$N \\leftarrow (p \\odot Pl)^+I,$\nwhere \u2020 denotes the pseudo-inverse operator. This reconstruction method avoids the need for trainable parameters, relying instead on analytic formulations. Once the display patterns are optimized, they are tested on real-world objects by capturing images under these patterns and reconstructing surface normals using the photometric stereo method (N = fn(I))."}, {"title": "6. Assessment", "content": "We assess DMDPS on diverse objects."}, {"title": "7. Discussion", "content": "First, we were unable to perform diffuse-specular separation due to device limitations, which we believe is a significant factor to the reconstruction error. For future work, we propose exploring whether diffuse-specular separation can be achieved on mobile devices by using a model capable of emitting polarized light, coupled with a polarization lens or polarizing film attached to the front camera.\nSecond, although our goal was to create a large dataset of normal maps by photographing fallen leaves of various shapes, we were able to capture only a subset of the fallen leaves. Expanding the dataset by photographing a wider variety of fallen leaves remains a future task.\nLastly, it may be interesting to explore the use of tablets instead of mobile phones. Tablets offer the portability of a mobile phone while featuring a larger screen, which can serve as a larger light source. These characteristics suggest the potential for producing high-quality surface normal reconstructions, making the application of DDPS to tablets a promising area for future research."}, {"title": "8. Conclusion", "content": "In this paper, we presented the DMDPS. Surface normal reconstruction was performed using a mobile phone instead of a desktop setup of DDPS. Monitor and polarization camera, and objects were photographed in general environments rather than in a dark room. Tests were conducted using 3D-printed objects, and normal maps were established, along with datasets created for natural objects (e.g., fallen leaves). We hope our work spur further interests in practical physics-based photometric stereo."}]}