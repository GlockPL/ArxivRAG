{"title": "Al in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines", "authors": ["Douwe J. Spaanderman", "Matthew Marzetti", "Xinyi Wan", "Andrew F. Scarsbrook", "Philip Robinson", "Edwin H.G. Oei", "Jacob J. Visser", "Robert Hemke", "Kirsten van Langevelde", "David F. Hanff", "Geert J.L.H. van Leenders", "Cornelis Verhoef", "Dirk J. Gr\u00fcnhagen", "Wiro J. Niessen", "Stefan Klein", "Martijn P.A. Starmans"], "abstract": "Background:\nSoft-tissue and bone tumours (STBT) are rare, diagnostically challenging lesions with variable clinical behaviours and treatment approaches. This systematic review aims to provide an overview of Artificial Intelligence (AI) methods using radiological imaging for diagnosis and prognosis of these tumours, highlighting challenges in clinical translation, and evaluating study alignment with the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI international consensus guidelines for trustworthy and deployable AI to promote the clinical translation of AI methods.\nMethods:\nThe systematic review identified literature from several bibliographic databases, covering papers published before 17/07/2024. Original research published in peer-reviewed joumals, focused on radiology-based AI for diagnosis or prognosis of primary STBT was included. Exclusion criteria were animal, cadaveric, or laboratory studies, and non-English papers. Abstracts were screened by two of three independent reviewers to determine eligibility. Included papers were assessed against the two guidelines by one of three independent reviewers. The review protocol was registered with PROSPERO (CRD42023467970).\nFindings:\nThe search identified 15,015 abstracts, from which 325 articles were included for evaluation. Most studies performed moderately on CLAIM, averaging a score of 28\u00b79\u00b17.5 out of 53, but poorly on FUTURE-AI, averaging 5.1\u00b12.1 out of 30.\nInterpretations:\nImaging-Al tools for STBT remain at the proof-of-concept stage, indicating significant room for improvement. Future efforts by AI developers should focus on design (e.g. define unmet clinical need, intended clinical setting and how AI would be integrated in clinical workflow), development (e.g. build on previous work, training with data that reflect real-world usage, explainability), evaluation (e.g. ensuring biases are evaluated and addressed, evaluating Al against current best practices), and the awareness of data reproducibility and availability (making documented code and data publicly available). Following these recommendations could improve clinical translation of AI methods.", "sections": [{"title": "Evidence before this study", "content": "Research on the use of AI in diagnosing and predicting the outcomes of soft-tissue and bone tumours (STBT) is becoming more prevalent. However, the clinical adoption of AI methods in this field remains limited, highlighting a significant gap between AI development and its practical implementation in healthcare settings. Previous reviews focused on the accuracy and performance of published STBT tools, however, did not investigate the quality of research. Recent efforts have introduced guidelines with comprehensivecriteria specifically designed for structured reporting and responsible development, deployment, and governance of trustworthy AI in healthcare."}, {"title": "Added value of this study", "content": "This review examines the methodological quality of published literature by assessing it against two best-practice guidelines, which were chosen to complement each other and cover a wide range of criteria. Aspects related to study quality, study design, and trustworthy and deployable AI, as assessed in this review using the CLAIM andFUTURE-AIguidelines, may be even more important factors than their performance for assessing their potential translation to the clinic. This review highlights what the field is doing well and where future research should focus. The review includes all research using AI methods investigating STBT, giving it a far wider scope than previous reviews. Furthermore, this is a fast-moving field, hence updates on previous reviews are required."}, {"title": "Implications of all the available evidence", "content": "Currently published AI methods are producing promising proof-of-concept results but are not ready for clinical application. This work highlights opportunities and provides recommendations for AI developers and clinical professionals for future research to drive clinical implementation."}, {"title": "Introduction", "content": "Primary soft-tissue and bone tumours (STBT) are among the rarest neoplasms in humans, comprising both benign and malignant lesions. Malignant STBT, i.e. sarcoma, account for approximately 1% of all neoplasms.\u00b9 These tumours may occur at any age and almost any anatomical site, arising from cells of the connective tissue, including muscles, fat, blood vessels, cartilage, and bones.\u00b2 The rarity of STBT, along with their diverse subtypes and varied clinical behaviour, poses substantial challenges in accurate diagnosis and prognosis.\nRadiological imaging (including nuclear medicine) is crucial in evaluating and monitoring STBT. Technological advancements in imaging modalities have led to a substantial increase data volume, along with a corresponding growth in the expertise required for its interpretation. The growing utilization of radiological imaging and complexity of analysis has increased radiologists' workload. Therefore, developing intelligent computer-aided systems and algorithms for automated image analysis that can achieve faster and more accurate results is crucial.\u00b3 For STBT, intelligent systems may help non-specialized radiologists in diagnosing rare cancers more effectively. Furthermore, an increased caseload is associated with higher interpretive error, which can be avoided with computer-aided diagnostic tools.4,5\nArtificial intelligence (AI) has become increasingly prevalent in medical image analysis. Over the last 7 years, the number of FDA-approved medical imaging Al products for radiology has substantially increased.6 However, while medical imaging Al research in STBT has also substantially increased, there are no products developed for STBT among the FDA-approved list.7 Hence, instead of purely developing novel technological solutions, more research should focus on aligning with areas of unmet clinical need.\nTherefore, a systematic assessment of current published research is necessary to identify the issues required to overcome the translational barrier. This systematic review aims to evaluate the existing literature on AI for diagnosis and prognosis of STBT using radiological imaging against two best practice guidelines; CLAIM and FUTURE-AI.8,9 CLAIM, endorsed by the Radiological Society of North America (RSNA), promotes comprehensive reporting of radiological research that uses AI. FUTURE-AI proposes ethical and technical standards to ensure responsible development, deployment, and governance of trustworthy AI in healthcare. Utilising both guidelines allows for comprehensive coverage of different aspects of Al research.10 Additionally, this review discusses opportunities for future research to bridge the identified gap between Al research and clinical use in STBT."}, {"title": "Materials and Methods", "content": "This systematic review was prospectively registered with PROSPERO (CRD42023467970) and adheres to the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) 2020 guidelines.11 The full study protocol can be found online at. 12\nMedline, Embase, Web of Science core collection, Google Scholar, and Cochrane Central Register of Controlled Trials were systematically searched for relevant studies. All papers published before 27/09/2023 were included in the initial search; the starting date depended on the coverage of the respective database searched. The detailed search strategy is listed in Appendix 1. The literature search was conducted by the Medical Library, Erasmus MC, Rotterdam, the Netherlands. The database search was repeated on 17/07/2024 to update publications.\nInclusion criteria were: (1) original research papers published in peer-reviewed journals, and (2) studies focusing on radiology-based AI or radiomics characterization of primary tumours located in bone and/or soft tissues for tasks related to diagnosis or prognosis, e.g. no pure segmentation studies. Exclusion criteria were: (1) animal, cadaveric, or laboratory studies, and (2) not written in English language.\nThe complete reviewing methodology is illustrated in Figure 1. Three independent reviewers participated in title-and-abstract screening (DS, MM, XW). Retrieved papers were randomly divided into three batches. Reviewers 1 and 2 reviewed one batch, Reviewers 1 and 3 reviewed a second batch, and Reviewers 2 and 3 reviewed the final batch. In cases where there were disagreements in the screening of an abstract, the third reviewer who was not initially involved in reviewing the specific abstract, adjudicated any conflicts.\nEach paper was scored according to CLAIM and FUTURE-AI guidelines. Checklists were developed based on each guideline. Blank checklists are available in Appendix 2. These guidelines were chosen for their complimentary nature and comprehensive coverage of clinical AI tool requirements.10\nThe CLAIM checklist was adapted from the checklist implemented by Si et. al. to contain more detail in some of the more general checklist items. 8,13,14 CLAIM consists of 44 items, covering the following sections: title, abstract, introduction, methods, results, discussion, and other information. The majority of items focus on the methods (30/44 items). The Methods section is further divided into the following subsections: Study design, Data, Ground truth, Data"}, {"title": "Results", "content": "Database searches identified 15,015 published studies, with 5,667 duplicates. After screening, 453 articles were retained for full-text review. After excluding 128 studies a total of 325 unique studies were included in the systematic review (Figure 2). Main reasons for exclusion were focusing on different entities (e.g. renal cancer), no use of radiological imaging, or lacking Al- based analysis.\nIncluded studies were published between 2008 and 2024, mostly in the last five years (Figure 3). Of the 325 included studies, most AI methods used hand-crafted imaging features with machine learning (n=221, 68%). Recently, more AI methods used model-learned imaging features (n=62, 19%), i.e. deep learning, or a combination of model-learned and hand-crafted imaging features with machine learning (n=29, 9%). Thirteen studies used hand-crafted imaging features without machine learning.\nStudy characteristics are illustrated in Figure 4. Disease types included soft tissue tumours (n=125, 38.5%), bone tumours (n=114,35.1%), and GIST (n=82,25\u00b72%). Only four studies included both soft tissue and bone tumours (1\u00b72%). Study design was mostly retrospective (n=272, 83.7%), with fewer prospective studies (n=38, 11.7%), and a minority where study design was not clearly documented (n=15, 4\u00b76%). The majority of reports focused on developing Al methods to predict diagnosis (n=206,63\u00b74%), 109 (33\u00b75%) evaluated prognosis, and 10 (3.1%) studied a combination of diagnosis and prognosis of the disease. Various radiological techniques were evaluated, with 144 (44\u00b73%) studies using MRI, 94 (28\u00b79%) CT, 34(10.5%) ultrasound, 30 (9.2%)X-ray,10(3.1%)PET-CT, 3 (0.9%) PET-MRI, and 1 (0.3%) scintigraphy, and 9 (2.8%) multiple modalities. One-hundred-and-ninety (58\u00b75%) studies collected data from a single centre, whereas 93 (28\u00b76%) utilized imaging from multiple centres. Nineteen studies did not clearly document data provenance (5\u00b78%). Furthermore, 23 (7.1%) studies used publicly available data from two sources. AI methods were most often validated with separate internal test data (n=214, 65.8%), and sometimes additionally with external test data (n=70, 21\u00b75%). Several AI methods were not validated with independent data or validation was not clearly documented (n=41, 12.6%). Only 5 (1.5%) studies made data available, with 238 (73\u00b72%) studies not providing or not specifying data availability, and 82 (25.2%) studies stating data would be made available on reasonable request. Similarly, AI source code to facilitatereproducibility was only made available in 23 (7.1%) studies, with 287 (88.3%) not providing or not specifying code availability, and 15 (4.6%) studies indicating code would be made available on reasonable request."}, {"title": "Discussion", "content": "This work has systematically identified and summarized radiological imaging-AI research on STBT and conducted comprehensive evaluation of published literature against two best- practice guidelines: CLAIM and FUTURE-AI. These guidelines were developed to ensure that Al tools target unmet clinical needs, are transferrable, generalisable, and can be used in real- world clinical practice. Analysis revealed a rapid increase in experimental AI tools for imaging- based STBT evaluation over the past five years. Studies performed moderately against CLAIM (28.9\u00b17.5 out of 53) and poorly against FUTURE-AI evaluations (5\u00b71\u00b12\u00b71 out of 30). The poor results in FUTURE-AI are expected as these guidelines are recent and set high requirements. These results suggest that while progress has been made in developing AI tools for STBT, most studies are still at the proof-of-concept stage and there remains substantial room for improvement to guide future clinical translation. Panel 2 summarises the authors' recommendations, focusing on five keys topics: design, development, evaluation, reproducibility, and data availability.\nIn the design stage, several critical aspects warrant more attention. Intended clinical settings (Universality-1) and prior hypotheses (CLAIM-4b) should be reported. On a positive note, over 85% of studies involved interdisciplinary teams (Usability-1, General-1), which is recommended for effective Al tool development. However, most studies did not comprehensively identify possible sources of bias at an early stage (Fairness-1, Robustness-1), which could limit the applicability of these AI tools. To overcome this, interdisciplinary stakeholders should work together from the design stage to identify the clinical role of the AI tool, ensure it integrates into the clinical workflow, and any possible sources of bias.\nIn the development stage, studies generally reported dataset source and conducted research with appropriate ethical approvals (CLAIM-7). However, almost half of studies did not assess biases during Al development(Fairness-3) and very few studies trained with representative real-world data (Robustness-2), which can hinder the transferability of AI tools, especially given the highly heterogeneous imaging characteristics of STBT. Another notable gap is a lack of focus on explainability and traceability. Few studies addressed items under FUTURE-AI Explainability (1-2) and Traceability (1-3), similar shortcoming was observed in the CLAIM checklist (CLAIM-31). While accuracy is crucial in medical practice, it is often argued that AI methods should go beyond pure performance metrics by addressing other factors such as prediction uncertainties, explaining their outputs, and providing clinicians with detailed information.17 For Al tools to be effective in clinical decision-making, explainability is vital to ensure clinicians understand and can trust the Al's reasoning. 18 Additionally, to assist with Al development, research should build on previous work where possible. To assist with this, researchers should continue to adhere to community-defined standards, which is currently done in over half of the"}, {"title": "Panel 2: Recommendations to promote clinical translation of AI methods for soft-tissue and bone tumours.", "content": "Interdisciplinary stakeholders should define; (A) the unmet clinical need, (B) the intended use of AI, (C) intended clinical setting in which AI should operate, (D) the end-user requirements, (E) how AI would operate in clinical workflow.\nPossible types and sources of bias (e.g. sex, age, ethnicity, socioeconomics, geography) should be identified at the early design stage.\nData used for AI development should reflect real-world data used in the intended clinical setting or preferably retrieved from the clinical setting. Additionally, sources of variation and potential biases should be investigated early in the development process.\nExplainability of AI methods should be developed and implemented in a way that it is possible to understand why an AI tool has arrived at its predictions.\nAl development should build onprevious workby: (A) adhering to community -defined standards, and (B) considering previous existing methods by validating or improving them whenever possible.\nEnsure that AI tools are easy for the end-user to use in a clinical setting.\nAI tools should be evaluated using independent external test data. Limits on universality of the external test sets should be discussed.\nAI tools should be evaluated against current best practices, e.g. classification by radiologist or histology results from biopsy, and evaluated with intended end-users.\nFailure analysis of incorrect classified cases should be conducted.\nThe robustness and sensitivity to variations and biases in data, identified prior to AI development, should be thoroughly investigated.\nCode should be made publicly available, readable, usable and traceable to increase confidence in the method.\nThe Methods section should comprehensively cover all aspects of Al development, including; (A) data preprocessing, (B) ground truth acquisition, (C) a detailed description of the AI methodology, and (D) the training procedures. To this end, the Checklist for Artificial Intelligence in Medical Imaging (CLAIM) could be followed.\nStructured and standardized reporting should be introduced in clinical practice to limit the manual work required in retrospective data collection.\nTertiary sarcoma centres should collect labelled data and make this publicly available, preferably in the context of a \"grand challenge\u201d, while protecting patient details and respecting privacy.\nTo protect patient privacy and avoid excessive data-sharing, researchers could work together using a federated learning approach."}]}