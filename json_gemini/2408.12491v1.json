{"title": "AI in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines", "authors": ["Douwe J. Spaanderman", "Matthew Marzetti", "Xinyi Wan", "Andrew F. Scarsbrook", "Philip Robinson", "Edwin H.G. Oei", "Jacob J. Visser", "Robert Hemke", "Kirsten van Langevelde", "David F. Hanff", "Geert J.L.H. van Leenders", "Cornelis Verhoef", "Dirk J. Gr\u00fcnhagen", "Wiro J. Niessen", "Stefan Klein", "Martijn P.A. Starmans"], "abstract": "Background:\nSoft-tissue and bone tumours (STBT) are rare, diagnostically challenging lesions with variable clinical behaviours and treatment approaches. This systematic review aims to provide an overview of Artificial Intelligence (AI) methods using radiological imaging for diagnosis and prognosis of these tumours, highlighting challenges in clinical translation, and evaluating study alignment with the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI international consensus guidelines for trustworthy and deployable AI to promote the clinical translation of AI methods.\n\nMethods:\nThe systematic review identified literature from several bibliographic databases, covering papers published before 17/07/2024. Original research published in peer-reviewed joumals, focused on radiology-based AI for diagnosis or prognosis of primary STBT was included. Exclusion criteria were animal, cadaveric, or laboratory studies, and non-English papers. Abstracts were screened by two of three independent reviewers to determine eligibility. Included papers were assessed against the two guidelines by one of three independent reviewers. The review protocol was registered with PROSPERO (CRD42023467970).\n\nFindings:\nThe search identified 15,015 abstracts, from which 325 articles were included for evaluation. Most studies performed moderately on CLAIM, averaging a score of 28\u00b79\u00b17.5 out of 53, but poorly on FUTURE-AI, averaging 5.1\u00b12.1 out of 30.\n\nInterpretations:\nImaging-Al tools for STBT remain at the proof-of-concept stage, indicating significant room for improvement. Future efforts by AI developers should focus on design (e.g. define unmet clinical need, intended clinical setting and how AI would be integrated in clinical workflow), development (e.g. build on previous work, training with data that reflect real-world usage, explainability), evaluation (e.g. ensuring biases are evaluated and addressed, evaluating Al against current best practices), and the awareness of data reproducibility and availability (making documented code and data publicly available). Following these recommendations could improve clinical translation of AI methods.", "sections": [{"title": "Introduction", "content": "Primary soft-tissue and bone tumours (STBT) are among the rarest neoplasms in humans, comprising both benign and malignant lesions. Malignant STBT, i.e. sarcoma, account for approximately 1% of all neoplasms.\u00b9 These tumours may occur at any age and almost any anatomical site, arising from cells of the connective tissue, including muscles, fat, blood vessels, cartilage, and bones.2 The rarity of STBT, along with their diverse subtypes and varied clinical behaviour, poses substantial challenges in accurate diagnosis and prognosis.\n\nRadiological imaging (including nuclear medicine) is crucial in evaluating and monitoring STBT. Technological advancements in imaging modalities have led to a substantial increase data volume, along with a corresponding growth in the expertise required for its interpretation. The growing utilization of radiological imaging and complexity of analysis has increased radiologists' workload. Therefore, developing intelligent computer-aided systems and algorithms for automated image analysis that can achieve faster and more accurate results is crucial.3 For STBT, intelligent systems may help non-specialized radiologists in diagnosing rare cancers more effectively. Furthermore, an increased caseload is associated with higher interpretive error, which can be avoided with computer-aided diagnostic tools.4,5\n\nArtificial intelligence (AI) has become increasingly prevalent in medical image analysis. Over the last 7 years, the number of FDA-approved medical imaging Al products for radiology has substantially increased.  However, while medical imaging Al research in STBT has also substantially increased, there are no products developed for STBT among the FDA-approved list.7 Hence, instead of purely developing novel technological solutions, more research should focus on aligning with areas of unmet clinical need.\n\nTherefore, a systematic assessment of current published research is necessary to identify the issues required to overcome the translational barrier. This systematic review aims to evaluate the existing literature on AI for diagnosis and prognosis of STBT using radiological imaging against two best practice guidelines; CLAIM and FUTURE-AI.8,9 CLAIM, endorsed by the Radiological Society of North America (RSNA), promotes comprehensive reporting of radiological research that uses AI. FUTURE-AI proposes ethical and technical standards to ensure responsible development, deployment, and governance of trustworthy AI in healthcare. Utilising both guidelines allows for comprehensive coverage of different aspects of Al research. 10 Additionally, this review discusses opportunities for future research to bridge the identified gap between Al research and clinical use in STBT."}, {"title": "Materials and Methods", "content": "This systematic review was prospectively registered with PROSPERO (CRD42023467970) and adheres to the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) 2020 guidelines.11 The full study protocol can be found online at. 12\n\nMedline, Embase, Web of Science core collection, Google Scholar, and Cochrane Central Register of Controlled Trials were systematically searched for relevant studies. All papers published before 27/09/2023 were included in the initial search; the starting date depended on the coverage of the respective database searched. The detailed search strategy is listed in Appendix 1. The literature search was conducted by the Medical Library, Erasmus MC, Rotterdam, the Netherlands. The database search was repeated on 17/07/2024 to update publications.\n\nInclusion criteria were: (1) original research papers published in peer-reviewed journals, and (2) studies focusing on radiology-based AI or radiomics characterization of primary tumours located in bone and/or soft tissues for tasks related to diagnosis or prognosis, e.g. no pure segmentation studies. Exclusion criteria were: (1) animal, cadaveric, or laboratory studies, and (2) not written in English language.\n\nThe complete reviewing methodology is illustrated in Figure 1. Three independent reviewers participated in title-and-abstract screening (DS, MM, XW). Retrieved papers were randomly divided into three batches. Reviewers 1 and 2 reviewed one batch, Reviewers 1 and 3 reviewed a second batch, and Reviewers 2 and 3 reviewed the final batch. In cases where there were disagreements in the screening of an abstract, the third reviewer who was not initially involved in reviewing the specific abstract, adjudicated any conflicts.\n\nEach paper was scored according to CLAIM and FUTURE-AI guidelines. Checklists were developed based on each guideline. Blank checklists are available in Appendix 2. These guidelines were chosen for their complimentary nature and comprehensive coverage of clinical AI tool requirements.10\n\nThe CLAIM checklist was adapted from the checklist implemented by Si et. al. to contain more detail in some of the more general checklist items.8,13,14 CLAIM consists of 44 items, covering the following sections: title, abstract, introduction, methods, results, discussion, and other information. The majority of items focus on the methods (30/44 items). The Methods section is further divided into the following subsections: Study design, Data, Ground truth, Data"}, {"title": "Results", "content": "Database searches identified 15,015 published studies, with 5,667 duplicates. After screening, 453 articles were retained for full-text review. After excluding 128 studies a total of 325 unique studies were included in the systematic review (Figure 2). Main reasons for exclusion were focusing on different entities (e.g. renal cancer), no use of radiological imaging, or lacking Al- based analysis.\n\nIncluded studies were published between 2008 and 2024, mostly in the last five years (Figure 3). Of the 325 included studies, most AI methods used hand-crafted imaging features with machine learning (n=221, 68%). Recently, more AI methods used model-learned imaging features (n=62, 19%), i.e. deep learning, or a combination of model-learned and hand-crafted imaging features with machine learning (n=29, 9%). Thirteen studies used hand-crafted imaging features without machine learning.\n\nStudy characteristics are illustrated in Figure 4. Disease types included soft tissue tumours (n=125, 38.5%), bone tumours (n=114,35.1%), and GIST (n=82,25\u00b72%). Only four studies included both soft tissue and bone tumours (1\u00b72%). Study design was mostly retrospective (n=272, 83.7%), with fewer prospective studies (n=38, 11.7%), and a minority where study design was not clearly documented (n=15, 4\u00b76%). The majority of reports focused on developing Al methods to predict diagnosis (n=206,63\u00b74%), 109 (33\u00b75%) evaluated prognosis, and 10 (3.1%) studied a combination of diagnosis and prognosis of the disease. Various radiological techniques were evaluated, with 144 (44\u00b73%) studies using MRI, 94 (28\u00b79%) CT, 34(10.5%) ultrasound, 30 (9.2%)X-ray,10(3.1%)PET-CT, 3 (0.9%) PET-MRI, and 1 (0.3%) scintigraphy, and 9 (2.8%) multiple modalities. One-hundred-and-ninety (58\u00b75%) studies collected data from a single centre, whereas 93 (28\u00b76%) utilized imaging from multiple centres. Nineteen studies did not clearly document data provenance (5\u00b78%). Furthermore, 23 (7.1%) studies used publicly available data from two sources. AI methods were most often validated with separate internal test data (n=214, 65.8%), and sometimes additionally with external test data (n=70, 21\u00b75%). Several AI methods were not validated with independent data or validation was not clearly documented (n=41, 12.6%). Only 5 (1.5%) studies made data available, with 238 (73\u00b72%) studies not providing or not specifying data availability, and 82 (25.2%) studies stating data would be made available on reasonable request. Similarly, AI source code to facilitatereproducibility was only made available in 23 (7.1%) studies, with 287 (88.3%) not providing or not specifying code availability, and 15 (4.6%) studies indicating code would be made available on reasonable request.\n\nKappa statistics for inter-reader variability increased from 0.58 and 0.80 for CLAIM and FUTURE-AI before consensus discussion, to 0\u00b762 and 0\u00b788 after, showing excellent agreement\n\nThe included studies performed moderately on the CLAIM checklist, with a mean score of 28.9 out of 53 (SD): 7.5, min-max: 4\u00b70\u201348\u00b70, AR mean\u00b1SD: 55%\u00b114%). All items were reported at least once, but several were only reported in less than 15% of the papers (n\u226450 papers) including: define a study hypothesis at the design phase (CLAIM-4b, 13\u00b78%), data de- identificationmethods (CLAIM-11,3\u00b74%), how missing data were handled (CLAIM-12, 8\u00b72%), intended sample size and how it was determined (CLAIM-21, 4%), robustness or sensitivity analysis (CLAIM-30, 13\u00b78%), methods for explainability or interpretability (CLAIM-31, 12.9%), registration number and name of registry (CLAIM-34, 2.8%), and documented where full study protocol can be accessed (CLAIM-42, 12\u00b73%).\n\nThe included studies rarely adhered to FUTURE-AI, with a mean score of 5\u00b71 out of 30 (SD: 2.1, min-max: 10\u201311\u00b75, AR: 17%\u00b07%). From the 30 items, 5 were never reported. Only 6 items were partially reported in over half of the reviewed papers (n>162) including: collecting and reporting on individuals' attributes (Fairness-2, 83.1%), using community-defined standards (Universality-2, 56%), defining use and user requirements (Usability-1, 85\u00b72%), engaging interdisciplinary stakeholders (General-1,86\u00b72%), implementing measures fordata privacy and security (General-2, 85\u00b72%), and defining an adequate evaluation plan (General-4, 67\u00b77%).\n\nStrongly recommended items by FUTURE-AI for proof-of-concept AI studies (Research), were reported more frequently than recommended items, with mean scores of 2-9 out of 12 (SD: 1\u00b71, min-max: 0-7,AR: 24%\u00b19%) and 2-3 out of 16 (SD: 1\u00b72, min-max: 10\u20136.5, AR: 14%\u00b18%), respectively. However, this trend was not observed in items intended to assess studies for clinical deployability (Deployable), where the mean scores were 3.8 out of 24 (SD: 1\u00b77, min- max: 0-10, AR: 16%\u00b17%) for strongly recommended items and 1.3 out of 4 (SD: 0.7, min- max: 0-3, AR: 33%\u00b118%) for recommended items."}, {"title": "Discussion", "content": "This work has systematically identified and summarized radiological imaging-AI research on STBT and conducted comprehensive evaluation of published literature against two best- practice guidelines: CLAIM and FUTURE-AI. These guidelines were developed to ensure that Al tools target unmet clinical needs, are transferrable, generalisable, and can be used in real- world clinical practice. Analysis revealed a rapid increase in experimental AI tools for imaging- based STBT evaluation over the past five years. Studies performed moderately against CLAIM (28.9\u00b17.5 out of 53) and poorly against FUTURE-AI evaluations (5\u00b712\u00b71 out of 30). The poor results in FUTURE-AI are expected as these guidelines are recent and set high requirements.\n\nThese results suggest that while progress has been made in developing AI tools for STBT, most studies are still at the proof-of-concept stage and there remains substantial room for improvement to guide future clinical translation. Panel 2 summarises the authors' recommendations, focusing on five keys topics: design, development, evaluation, reproducibility, and data availability.\n\nIn the design stage, several critical aspects warrant more attention. Intended clinical settings (Universality-1) and prior hypotheses (CLAIM-4b) should be reported. On a positive note, over 85% of studies involved interdisciplinary teams (Usability-1, General-1), which is recommended for effective Al tool development. However, most studies did not comprehensively identify possible sources of bias at an early stage (Fairness-1, Robustness-1), which could limit the applicability of these AI tools. To overcome this, interdisciplinary stakeholders should work together from the design stage to identify the clinical role of the AI tool, ensure it integrates into the clinical workflow, and any possible sources of bias.\n\nIn the development stage, studies generally reported dataset source and conducted research with appropriate ethical approvals (CLAIM-7). However, almost half of studies did not assess biases during Al development(Fairness-3) and very few studies trained with representative real-world data (Robustness-2), which can hinder the transferability of AI tools, especially given the highly heterogeneous imaging characteristics of STBT. Another notable gap is a lack of focus on explainability and traceability. Few studies addressed items under FUTURE-AI Explainability (1-2) and Traceability (1-3), similar shortcoming was observed in the CLAIM checklist (CLAIM-31). While accuracy is crucial in medical practice, it is often argued that AI methods should go beyond pure performance metrics by addressing other factors such as prediction uncertainties, explaining their outputs, and providing clinicians with detailed information.17 For Al tools to be effective in clinical decision-making, explainability is vital to ensure clinicians understand and can trust the Al's reasoning. 18 Additionally, to assist with Al development, research should build on previous work where possible. To assist with this, researchers should continue to adhere to community-defined standards, which is currently done in over half of the\n\nreviewed papers, and ensure their code is available. This review shows that almost all included studies developed new models rather than adapting or enhancing existing ones, even when promising results were achieved. Finally, it is integral that AI tools are easy for the end-user to use in the clinical workflow, however only two studies developed a graphical user interface for user experience testing (Usability-3).19,20\n\nRegarding evaluation, while over 85% of studies adopted relevant metrics and reported Al algorithm performance(CLAIM-28 and37), only 22% conducted external validation(CLAIM- 33), and most used single-institute datasets (Universality-3). Furthermore, several studies lacked thorough internal validation(Robustness-3, General-4). AI tools should be tested against independent external data, ideally from multiple sources, to assess the tool's universality and prevent site-specific bias. Accuracy metrics should also be compared against current best- practice (i.e. compared to radiologists) to ensure AI tools offer improvements in outcomes. Less than 20% of studies reported failure analysis or incorrectly classified cases (CLAIM-39). Including failure analysis is crucial to identify potential pitfalls, helping users understand when it is appropriate to use the tool. Developers should also ensure that the tool is robust against the biases identified during the design stage.\n\nRegarding reproducibility, most studies fail to provide adequate materials (code, model, and data) to reproduce published results. Only around 10% of studies offered a full study protocol, including comprehensive methodology or code. Making protocols and code available enables others to reproduce the study across multiple steps, such as data preprocessing, ground truth acquisition, model construction, and training procedure. The lack of accessible and reproducible AI research in STBT could impede the adoption of these tools, as sarcoma centres may struggle to reproduce the tools performance locally. Adhering to guidelines such as CLAIM could enhance the quality and accessibility of these protocols.\n\nRegarding data availability, there is a lack of freely accessible annotated imaging datasets of STBT. Although 25% of published research stated that data used was available by request, a recent study by Gabelica et al. (2022) investigating compliance with data sharing statements showed a response rate of 14%, with only 6.8% supplying the data. 21 One challenge in creating these datasets is the time required and the need for an easy-to-use format. Structured and standardized reporting in clinical practice could help reduce the effort needed for retrospective data collection. However, AI developers often struggle to collate data themselves, especially since STBT are rare and only treated at tertiary sarcoma centers. This underscores the importance of collaborating with clinical professionals. Increasing data availability would accelerate AI tool development and allow for external validation of models. Potential solutions include hosting \"grand challenges\" where clinicians provide data for Al"}, {"title": "drive clinical adoption of AI tools into the radiology workflow in a responsible and effective way.", "content": null}]}