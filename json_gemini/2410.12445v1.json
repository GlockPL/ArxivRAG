{"title": "Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation for Korean LLMs", "authors": ["Hyeonwoo Kim", "Dahyun Kim", "Jihoo Kim", "Sukyung Lee", "Yungi Kim", "Chanjun Park"], "abstract": "The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean Large Language Models (LLMs), yet it has certain limitations. Notably, the disconnect between quantitative improvements on the overly academic leaderboard benchmarks and the qualitative impact of the models should be addressed. Furthermore, the benchmark suite is largely composed of translated versions of their English counterparts, which may not fully capture the intricacies of the Korean language. To address these issues, we propose Open Ko-LLM Leaderboard2, an improved version of the earlier Open Ko-LLM Leaderboard. The original benchmarks are entirely replaced with new tasks that are more closely aligned with real-world capabilities. Additionally, four new native Korean benchmarks are introduced to better reflect the distinct characteristics of the Korean language. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide a more meaningful evaluation for advancing Korean LLMs.", "sections": [{"title": "1 Introduction", "content": "The Open Ko-LLM Leaderboard was originally established as a critical evaluation platform to benchmark Korean-specific Large Language Models (LLMs) (Park et al., 2024; Park and Kim, 2024). Its motivation stemmed from the growing need to adapt existing English-centric benchmarks to Korean, thereby fostering the development of language models that can effectively handle the complexities of Korean syntax and semantics. However, the leaderboard has faced significant limitations over time.\nFor instance, as improvements in benchmark scores no longer translated to real-world advancements due to the overly academic nature of the benchmark suite, submission rates decreased as the leaderboard results were not as meaningful as before. The benchmark suite need tasks that correlate more with real-world performance. Further, the leaderboard's tasks, primarily configured by translating English counterparts, do not sufficiently capture the nuances of the Korean language. In fact, although the leaderboard was designed for Korean LLMs, only one of the five benchmarks, Ko-CommonGen v2, was specifically tailored for Korean, highlighting a gap in its linguistic specificity.\nTo address these challenges, we propose the Open Ko-LLM Leaderboard2. This next-generation framework replaces the previous benchmarks with a suite of tasks focusing on Korean linguistic nuances and real-world applications. Notably, the introduction of KorNAT benchmarks (Lee et al., 2024) and practical, real-world evaluations like Ko-IFEval (Zhou et al., 2023) and Ko-GPQA (Rein et al., 2023) ensures the leaderboard's continued relevance. Furthermore, the shift toward fine-tuned models aligns with industry trends, enabling a more meaningful assessment of task-specific performance in Korean LLMs (Peng et al., 2024; Guo et al., 2023)."}, {"title": "2 Open Ko-LLM Leaderboard Season 1", "content": "The Open Ko-LLM Leaderboard (Season 1) (Park et al., 2024; Park and Kim, 2024) was established to provide a comprehensive evaluation framework for Korean-specific Large Language Models (LLMs). Its development was driven by two primary motivations: (i) ensuring alignment with the English Open LLM Leaderboard to facilitate consistent and comparable evaluations across global and Korean LLMs, and (ii) utilizing private test sets to prevent data contamination and ensure rigorous evaluation across a variety of models.\nThe evaluation relied on the Ko-H5 benchmark, which consisted of five tasks: Ko-ARC (Clark et al., 2018), Ko-HellaSwag (Zellers et al., 2019), Ko-MMLU (Hendrycks et al., 2020), Ko-TruthfulQA (Lin et al., 2021), and Ko-CommonGen v2 (Seo et al., 2024). While these tasks provided a foundational assessment of Korean LLMs, four of the five benchmarks were direct translations from English datasets, limiting their linguistic specificity. Only Ko-CommonGen v2 was developed with a focus on Korean, underscoring the need for more Korean-centric benchmarks in future iterations."}, {"title": "3 Open Ko-LLM Leaderboard2", "content": null}, {"title": "3.1 Task Overview", "content": "The Open Ko-LLM Leaderboard2 introduces a comprehensive overhaul of its evaluation framework by replacing all previous benchmarks with nine newly designed tasks. These tasks assess a wide range of linguistic and practical capabilities essential for testing Korean LLMs in both academic and real-world settings.\nThe newly added benchmarks are as follows. Ko-GPQA (Diamond) (Rein et al., 2023), a general-purpose question-answering task that evaluates deep reasoning in the Korean context. Ko-WinoGrande (Sakaguchi et al., 2021) focuses on commonsense reasoning by challenging models to resolve ambiguities in everyday Korean scenarios. Ko-GSM8K (Cobbe et al., 2021) assesses mathematical reasoning, requiring models to solve complex arithmetic and word problems. Ko-EQ-Bench (Paech, 2023) tests emotional intelligence by evaluating the model's ability to generate contextually appropriate responses in emotionally charged conversations. Ko-IFEval (Zhou et al., 2023) examines instruction-following skills, gauging how well models can interpret and execute complex Korean instructions. KorNAT-Knowledge (Lee et al., 2024), a newly introduced benchmark, tests factual recall and application in Korean-specific contexts. KorNAT-Social-Value (Lee et al., 2024) evaluates models on their understanding of social norms and values that are unique to Korean culture. Ko-Harmlessness (Lee et al., 2024) measures the model's capacity to produce safe and non-toxic responses in sensitive scenarios, while Ko-Helpfulness (Lee et al., 2024) focuses on the model's ability to provide relevant and practical information across a variety of real-world situations."}, {"title": "3.2 Dataset Sizes", "content": "Each of the nine benchmarks in the Open Ko-LLM Leaderboard2 features datasets of varying sizes to reflect the complexity and scope of the tasks."}, {"title": "3.3 Curation Process", "content": "The nine benchmarks were curated using two distinct approaches. Five of the tasks\u2014Ko-GPQA (Diamond), Ko-WinoGrande, Ko-GSM8K, Ko-EQ-Bench, and Ko-IFEval\u2014were adapted from existing English benchmarks (Park et al., 2024). These datasets were professionally translated and then rigorously reviewed and modified to align with Korean language and cultural nuances. This process involved a thorough human correction phase to ensure that the benchmarks accurately reflected the Korean context.\nThe remaining four tasks\u2014KorNAT-Knowledge, KorNAT-Social-Value, Ko-Harmlessness, and Ko-Helpfulness-were developed entirely from scratch using native Korean corpora. These benchmarks were designed by domain experts to address specific challenges in Korean LLM evaluation, focusing on areas such as factual knowledge, social norms, safety, and utility in real-world situations. The creation of these benchmarks ensures that the leaderboard not only reflects the technical capabilities of models but also their cultural and contextual understanding of Korean language and society.\nAll datasets in the Open Ko-LLM Leaderboard2 are kept fully private, following the precedent set by the Open Ko-LLM Leaderboard Season 1. This ensures the integrity of the evaluation process by preventing data leakage and guaranteeing a fair and unbiased assessment of model performance."}, {"title": "3.4 Task Evaluation Methodology", "content": "The evaluation methodology for each of the nine tasks in the Open Ko-LLM Leaderboard2 is tailored to the nature of the benchmark and the specific capabilities being tested.\nFor Ko-GPQA (Diamond), Ko-WinoGrande, KorNAT-Knowledge, KorNAT-Social-Value, Ko-Harmlessness, and Ko-Helpfulness, the evaluation is based on a multiple-choice format. These tasks are evaluated using accuracy metrics, with Ko-GPQA, KorNAT-Knowledge, Ko-Harmlessness, and Ko-Helpfulness assessed using normalized accuracy (acc_norm), while KorNAT-Social-Value employs the A-SVA metric specific to social value assessments.\nIn contrast, Ko-GSM8K, Ko-EQ-Bench, and Ko-IFEval use generation-based evaluation. Ko-GSM8K focuses on strict exact-match for mathematical reasoning, and Ko-EQ-Bench uses a task-specific emotional intelligence scoring system (eqbench). Ko-IFEval evaluates the model's ability to follow instructions using prompt-level and instruction-level strict accuracy metrics. These tasks explicitly evaluate the generated output of the model, which is more aligned with actual usage scenarios.\nThe number of few-shot examples varies by task, with tasks such as Ko-WinoGrande and Ko-GSM8K using 5-shot setups, while others like Ko-GPQA and Ko-IFEval use a 0-shot configuration."}, {"title": "3.5 Infrastructure and Platform", "content": "The infrastructure for the Open Ko-LLM Leaderboard2 has been significantly upgraded to accommodate the increased complexity and scale of the new benchmarks. The system now utilizes both H100 and A100 GPUs, ensuring faster and more efficient evaluations to meet the demands of larger and more complex tasks. The leaderboard operates on the Hugging Face platform (Jain, 2022), just like in Season 1, providing a user-friendly and familiar environment for participants. By maintaining the same interface and submission process as the original leaderboard, users can seamlessly transition to the new version without additional learning curves, while benefiting from the enhanced infrastructure. This consistency ensures broad accessibility and fosters greater community participation, supporting ongoing innovation in Korean LLM development."}, {"title": "4 Empirical Analysis", "content": null}, {"title": "4.1 Initial Peak and Slow Decline in Submission Trends", "content": "The submission trends from Season 1 highlight the evolving interest in Korean language model evaluations, providing crucial motivation for Season 2. Figure 1 shows a strong initial response, peaking in November and December 2023, with a steady decline starting in January 2024, dropping to 123 submissions by July 2024. This decline is linked to dissatisfaction with the gap between leaderboard scores and real-world performance, as well as limitations in evaluation metrics. The community's engagement waned as models optimized for benchmarks failed to demonstrate practical utility.\nThese trends emphasize the necessity of implementing more relevant benchmarks and qualitative metrics in Season 2, focusing on real-world applications and broader model capabilities."}, {"title": "4.2 Correlation with Real-World Usage", "content": "The logit-based academic evaluation methods in Season 1 are not well-suited to reflect the real-world usability of the models. In contrast, Season 2 aims to better capture the usability of the models by making sure that high-ranking models in Season 2 also work well in practice.\nIn Figure 3, models answers to questions are illustrated for high-ranking models in the Season 1 and 2 leaderboards. The answers on the left show awkward phrases with mixed symbols and inconsistent language, despite being generated from a high-ranking model in the Season 1 leaderboard. Comparatively, the responses on the right, which is from a top-ranking model in Season 2, feature coherent and natural phrases."}, {"title": "4.3 Correlation Between Season 1 and Season 2 Evaluations", "content": "Season 2 is different from Season 1. In Figure 2, we show the correlation between the model scores between Season 1 and 2. The correlation are calculated among pre-trained and fine-tuned models separately.\nFor pre-trained models, a relatively low correlation coefficient of 0.48 was observed between the two seasons. This suggests that the newly configured benchmarks that aim to align more closely to real-world scenarios are different from the mostly academic evaluation methods used in Season 1. Furthermore, fine-tuned models exhibited a slightly higher but still low correlation of 0.65 between the two seasons. This also reinforces the notion that Season 2 benchmarks are indeed different from Season 1, hopefully by being able to better reflect realistic use cases.\nA key difference in Season 2 is the addition of three generation-based tasks - Ko-GSM8K, Ko-EQ-Bench, Ko-IFEval - in contrast to zero in Season 1. Evaluating generated outputs of models are much more likely to align with real-world usages than logit-based evaluation. Note that pre-trained models are more likely to fail on such generation tasks than fine-tuned models, which is why fine-tuned models are used in real-world scenarios.\nIn Table 2, we show the correlation between Season 1 tasks, which are all logit-based, and the logit-based (Ko-GPQA, Ko-WinoGrande, KorNAT-Knowledge, KorNAT-Social-Value, Ko-Harmlessness, Ko-Helpfulness) and generation-based (Ko-GSM8K, Ko-EQ-Bench, Ko-IFEval) tasks of Season 2. The correlation coefficient between Season 1 and Season 2 (Generation) is 0.36, which is notably low. This indicates that the generation-based evaluation measures model capabilities that are quite different from the benchmarks of Season 1. Not only that, even within Season 2, the correlation between the logit-based and generation-based tasks is 0.33. This reinforces the notion that generation tasks in Season 2 capture different aspects of model capabilities than logit-based tasks from Season 1 or 2."}, {"title": "5 Conclusion", "content": "In this paper, we introduced Open Ko-LLM Leaderboard2, addressing critical limitations from Season 1 by incorporating nine benchmarks that better reflect the real-world capabilities of Korean LLMs. Our analysis of submission trends and performance correlations highlights the importance of aligning evaluations with real-world usage, especially through generation-based tasks. With these enhancements, Open Ko-LLM Leaderboard2 establishes a stronger framework for Korean LLM evaluation."}, {"title": "Limitations", "content": "While the Open Ko-LLM Leaderboard2 represents a significant improvement over its predecessor, there are several limitations to consider. First, despite efforts to introduce a diverse set of benchmarks, certain tasks may still not fully capture the breadth of real-world applications, especially in highly specialized domains. Additionally, the leaderboard focuses primarily on evaluating Korean language models, which limits the generalizability of the results to other languages. Another limitation is the reliance on private datasets, which, while ensuring fairness, may hinder transparency and reproducibility for the broader research community. Finally, computational resources, despite the infrastructure upgrade, remain a challenge for small teams or independent researchers, potentially limiting participation."}, {"title": "Ethics Statement", "content": "This work adheres to the highest ethical standards in the development and evaluation of language models. All datasets used in the Open Ko-LLM Leaderboard2 were carefully curated to avoid biases related to sensitive topics, and efforts were made to ensure that models are evaluated for harmful or toxic outputs through specific benchmarks like Ko-Harmlessness. Additionally, the leaderboard promotes fair competition by using private datasets to prevent data contamination and ensure equal opportunities for all participants. No personal data was used in the creation of the datasets, and all experiments were conducted with respect to privacy and ethical considerations."}]}