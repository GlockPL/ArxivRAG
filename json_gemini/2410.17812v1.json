{"title": "PGDIFFSEG: PRIOR-GUIDED DENOISING DIFFUSION MODEL\nWITH PARAMETER-SHARED ATTENTION FOR BREAST CANCER\nSEGMENTATION", "authors": ["Feiyan Feng", "Tianyu Liu", "Hong Wang", "Jun Zhao", "Wei Li", "Yanshen Sun"], "abstract": "Early detection through imaging and accurate diagnosis is crucial in mitigating the high mortality\nrate associated with breast cancer. However, locating tumors from low-resolution and high-noise\nmedical images is extremely challenging. Therefore, this paper proposes a novel PGDiffSeg (Prior-\nGuided Diffusion Denoising Model with Parameter-Shared Attention) that applies diffusion denoising\nmethods to breast cancer medical image segmentation, accurately recovering the affected areas from\nGaussian noise. Firstly, we design a parallel pipeline for noise processing and semantic information\nprocessing and propose a parameter-shared attention module (PSA) in multi-layer that seamlessly\nintegrates these two pipelines. This integration empowers PGDiffSeg to incorporate semantic details at\nmultiple levels during the denoising process, producing highly accurate segmentation maps. Secondly,\nwe introduce a guided strategy that leverages prior knowledge to simulate the decision-making process\nof medical professionals, thereby enhancing the model's ability to locate tumor positions precisely.\nFinally, we provide the first-ever discussion on the interpretability of the generative diffusion model in\nthe context of breast cancer segmentation. Extensive experiments have demonstrated the superiority\nof our model over the current state-of-the-art approaches, confirming its effectiveness as a flexible\ndiffusion denoising method suitable for medical image research. Our code will be publicly available\nlater.", "sections": [{"title": "1 Introduction", "content": "According to the 2023 cancer statistics report [1], breast cancer ranks second in mortality rate among female cancers,\njust behind lung cancer. Nevertheless, extensive research has shown that early screening plays a pivotal role in enhancing\nthe 5-year relative survival rate because initial morphology and location of tumors can be detected through medical\nimaging [2]. This early detection is instrumental in achieving more favorable treatment outcomes [3, 4]. Furthermore,\nadvancements in treatment protocols, including targeted therapies and adjuvant chemotherapies, have contributed to\nthe rapid increase in the survival rate of breast cancer patients [5]. However, despite these positive developments, the\nlatest research reveals a concerning phenomenon: the exponential increase in medical images, including mammography,\nmagnetic resonance imaging (MRI) sequences, and breast ultrasound images, has significantly extended the time\nrequired for radiologists to interpret and analyze these images, which overwhelms radiologists and leads to a high\nmisdiagnosis rate [6]. Accurately pinpointing tumors from low-resolution and noisy breast images is challenging [7],\nrequiring substantial effort and expertise for precise tumor segmentation in medical images [8].\nIn recent years, deep learning-based methods, such as deep convolutional neural networks (DCNNs), have emerged\nas powerful tools for segmenting suspected regions and extracting features from segmented breast cancer ROIs [9].\nIntegrating computer-aided diagnosis (CAD) strategies based on DCNNs can assist radiologists in making informed\ndecisions, ultimately reducing unnecessary biopsies and alleviating patients' discomfort caused by unnecessary invasive\nprocedures.\nThe U-shape architecture has shown significant progress in breast cancer segmentation by effectively integrating\nlow-level and high-level information through skip connections. Inspired by its success, many breast cancer segmentation\nnetworks based on UNet have sprung up [10, 11, 12]. However, current CAD methods face several challenges. Firstly,\nthey tend to rely excessively on pre-processing phases, such as image denoising and enhancement, resulting in image\ndistortion and the loss of crucial primitive features. In the context of breast cancer segmentation, the utilization of\nhigh-resolution images that capture various cancer-related features has the potential to enhance the discrimination ability\nof the model [13]. Moreover, end-to-end segmentation methods lack interpretability due to their \"black box\" feature\n[14]. Radiologists may question the authenticity of the segmentation results since the decision-making process needs\nmore guidance or expert knowledge. Furthermore, specific hybrid methods focus on exploring additional information,\nsuch as global and local features, that are relevant to the object areas. These methods extract cancer-related features\nfrom breast images and directly sum different representations. However, these approaches can widen the semantic gap\nand neglect essential medical commonsense knowledge.\nTo address these challenges, we introduce a novel GFDiffSeg model, Prior-Guided Diffusion Denoising Model with\nParameter-Shared Attention, for breast cancer segmentation. Unlike the classical Denoising Diffusion Probabilistic\nModel (DDPM) [15], GFDiffSeg goes beyond iteratively transforming a noise-corrupted input into a clean sample\nusing the diffusion process and probabilistic modeling. It also leverages images as conditions to learn the denoising\nprocess for corresponding segmentation masks. The process of noising and denoising in GFDiffSeg is depicted in\nFig. 1. By incorporating this principle, GFDiffSeg can directly obtain segmentation results from the raw images while\nproviding interpretability. Furthermore, we propose a parameter-shared attention (PSA) module to effectively fuse\nnoise and image features, bridging the semantic gap between different representations. Additionally, research has\ndemonstrated that incorporating prior knowledge into networks can provide benefits [16, 17], so we suggest adopting a\nprior knowledge-guided strategy to facilitate rapid focus on regions of interest. These advancements in the GFDiffSeg\nmodel contribute to improved segmentation performance and efficiency in breast cancer analysis. Our contributions can\nbe summarized as follows:"}, {"title": null, "content": "\u2022 PGDiffSeg represents a pioneering approach that enhances diffusion denoising techniques to achieve accurate\nbreast cancer medical image segmentation by effectively eliminating Gaussian noise. Notably, PGDiffSeg\nholds the distinction of being the first diffusion generative model in the field of breast cancer segmentation,\nequipped with valuable interpretability capabilities."}, {"title": null, "content": "\u2022 We introduce a parallel pipeline for separate processing of image and noise-added label information. Further-\nmore, we design a PSA module that enhances the model's focus on lesion areas in both pipelines to address\nthe fusion of noise features and semantic information. This integration empowers PGDiffSeg to incorporate\nsemantic details at multiple levels during denoising."}, {"title": null, "content": "\u2022 We incorporate prior knowledge by utilizing context encoding, allowing our model to simulate doctors'\ndecision-making process. This integration of prior knowledge successfully reduces the segmentation difficulty\nfor our model. This module operates akin to the expertise of physicians, guiding the denoising process and\nfacilitating quicker focus on lesion areas."}, {"title": null, "content": "\u2022 We are the first to present the interpretability of diffusion models and analyze the attention transfer character-\nistics in image segmentation tasks. In contrast to end-to-end models, we visualize intermediate results and\nattention positions at each iteration, providing a clear view of the model's learning process and enhancing trust\namong medical professionals."}, {"title": null, "content": "\u2022 Our method is extensively evaluated and consistently outperforms or achieves comparable results to the\ncurrent state-of-the-art models in breast cancer image segmentation. Moreover, experiments reveal our model's\nexcellent transferability, allowing it to be seamlessly applied to diverse modalities such as MRI or ultrasound\ndatasets."}, {"title": "2 Related Work", "content": "We organize the rest of this paper as followings. The related works are reviewed in Section 2. The proposed model and\nits detailed workflow are presented in Section 3. The experiments, and analysis are demonstrated in Sections 4 and 5.\nUltimately, we summarize our work and provide future perspectives in Section 6."}, {"title": "2.1 Diffusion model in medical image processing tasks", "content": "Due to its remarkable ability to generate realistic images, diffusion model has captured considerable attention in\ncomputer vision [18, 19, 20, 21, 22, 23]. There are three generic diffusion modeling frameworks, each based on\ndenoising diffusion probabilistic models, noise-conditioned score networks, and stochastic differential equations [24].\nWhen performing segmentation tasks, diffusion models add noise to the segmentation mask and use the image as an\nadditional input to denoise the noisy mask. However, existing segmentation methods are inadequate in maximizing the\nuse of image information to improve segmentation performance. They either concatenate the image and xt directly\n[25], add the encoded image features and encoded xt [20], embed the xt features into the image features at each\ndownsampling step [26], or use cross-attention to fuse the image and xt before downsampling [27]. Therefore, this\nstudy innovatively designs a fusion strategy where image information and noisy labels mutually reinforce each other,\nfacilitating the generation of enhanced and more effective feature representations."}, {"title": "2.2 Self-attention mechanism in medical image segmentation", "content": "The self-attention mechanism has gained significant popularity in medical image analysis due to its ability to capture\nlong-range dependencies and global features. Consequently, several pure transformer models have been developed\nfor medical image segmentation tasks [28, 29]. However, these pure transformer designs often neglect the extraction\nof local information. Recent research has explored hybrid architectures that combine self-attention with CNNs for\nmore effective segmentation [30, 31, 32, 33, 34]. Nonetheless, these studies primarily focus on extracting multi-scale\ninformation without effectively addressing the interaction of such information. In contrast, our proposed PSA module\nemphasizes the fusion of two distinct types of information, thereby improving segmentation performance by bridging\nthe semantic gap."}, {"title": "3 PGDiffSeg model", "content": ""}, {"title": "3.1 Framework of the GFDiffSeg model", "content": "Our proposed network GFDiffSeg operates on the diffusion process and generates segmented results from Gaussian\nnoise through iterative sampling. The overall structure of the network is shown in Fig. 2.\nOur model comprises feature pre-extraction, feature encoding, prior knowledge guidance, bottleneck, and feature\ndecoding stages. In the feature pre-extraction stage, the network separately extracts hidden features of the image and the\nnoised label xt using two slim dense blocks (SDB). Next, in the feature encoding stage, the downflow of the model is\ndivided into two branches. One branch, called the condition flow, takes the extracted image features as input to transmit\nspatial characteristics. And the other branch, the denoising flow, takes the extracted xt features as input to distinguish\nnoise information. Unlike conventional encoders, these two extractor flows work in tandem, with the parameter-shared\nattention (PSA) module employed after each layer of the down-level unit, facilitating information exchange between the\ntwo branches and promoting mutual improvement. After feature encoding, the features from both sides are fused by\nelement addition before the bottleneck. Furthermore, we also incorporate prior tumor information into the segmentation\ntask by training a classification network that guides the learning process of the segmentation task. Following the\nbottleneck, the rich features are fed into a decoder consisting of four up units, ultimately obtaining the noise-adding\nprediction from Xt\u22121 to xt.\nNext, we will mainly introduce our three main tasks, SDB in feature pre-extraction, PAS module in feature encoding,\nand prior-supervision module used for prior knowledge guidance. We do not introduce too many innovations in other\nmodules and have followed the majority's work; the detailed implementation can be found in the supplementary\nmaterials."}, {"title": "3.2 Slim dense block", "content": "As shown in Fig 3, the slim dense block (SDB) is constructed using a lightweight dense block [35], and plays a crucial\nrole in extracting hidden information that may be lost or inaccessible during the information exchange process of the\nPSA module in advance. By eliminating the growth rate of the Dense Block and adopting element-wise addition instead\nof channel concatenation, the SDB ensures a consistent number of channels in each layer. This design enables seamless\nintegration of the SDB into our model, irrespective of the increasing number of layers.\nThis module consists of L layers, where each layer implements a non-linear transformation H(\u00b7), which involves a\ncomposite function that includes the connection of two 3 \u00d7 3 convolution blocks, two-dimensional BatchNorm, and\nLeakyReLU activation."}, {"title": null, "content": "i-1\nX\u2081 = H (Xi\u22121) + \u03a3Xj \u00b0 X"}, {"title": null, "content": "where x is a scaling factor set to 0.2 in this model to optimize the overall architecture."}, {"title": "3.3 Parameter-shared cross-attention module", "content": "The parameter-shared cross-attention(PSA) module is designed to bridge the semantic gap between the two branches.\nInspired by [36] and [37], we propose a novel approach for integrating relevant information, overcome the isolated state\nof the independent downflows between the two branches.\nIn our design, we introduce a PSA module after each layer of the denoising and condition units, as depicted in Fig.\n4. Since xt and image have spatial consistency, the parameters of each layer's PSA module can be trained with two\nhidden features simultaneously: the feature that needs to be denoised and the pathological feature that contains tumor\ninformation. This PSA block plays a bidirectional role in embedding denoised features and pathological features into\nthe same feature space, allowing them to complement each other."}, {"title": null, "content": "Qi, ki, Vi = Qshared (Xi), Kshared (Xi), Vi (xi); i = 1, 2"}, {"title": null, "content": "where Q, K, V, are the generation of query, key, and value, respectively; and qi, ki, vi are query, key, and value of\nXi. The X1 attention maps are first computed by multiplying the transpose of the projected q1 layer with kl. After\nmeasuring similarity through softmax, they are multiplied by v\u2081 to produce the final semantic maps with a shape of\nN \u00d7 C, which can be seen in Equation 3. Similarly, the denoising maps of x2 undergo the same process as described in\nEquation 4."}, {"title": null, "content": "Xs = v1. Softmax (qk1)"}, {"title": null, "content": "Xa = v2. Softmax (q2k2)"}, {"title": null, "content": "where X, and Xa denote semantic and denoising maps, respectively. Then the shape of Xs and Xd is reset to\nC\u00d7 H \u00d7 W, and the two original feature maps can be updated to form the output, as represented by Equation 5."}, {"title": null, "content": "x1 = 71Xs + x1; x2 = N2Xd + x2"}, {"title": null, "content": "where n\u2081 is a learnable scalar and we initialize it as 0. This allows the model to focus more on the intrinsic features of\nits branch in the early stages of training and gradually assign more weights to the supplemental features [37]."}, {"title": "3.4 Prior-supervision module", "content": "In practical applications, achieving pixel-wise classification can be a challenging task, whereas determining the presence\nof a tumor is relatively simpler [38]. Therefore, we incorporate a supervised flow generated by the prior supervision\nmodule as prior semantic knowledge to offer position guidance and integrate it into the denoising process."}, {"title": null, "content": "x = x + x"}, {"title": null, "content": "where x represents the feature map of the bottleneck and x is the output of the supervised flow. This module has an\nindependent loss function. We obtain a probability classification value p using linear mapping and train this part using\ncross-entropy loss, as shown in Equation 7."}, {"title": null, "content": "1\nlossCE =--\u03a3 (yilog (pi) + (1 \u2212 yi) log (1 \u2212 pi))"}, {"title": null, "content": "where N represents the number of samples, yi is the true value of the ith sample (whether a tumor exists), and p\u2081 is the\npredicted probability of that sample.\nFurthermore, to ensure that this supervised flow focuses on the correct positions, we map the penultimate supervised\nunit from the space of RC\u00d7H\u00d7W to R1\u00d7H\u00d7W and compute the Dice loss and Binary Cross-Entropy loss by comparing\nit with the resized segmentation label. The Dice loss and Binary Cross-Entropy loss are represented by Equation 8 and\nEquation 9, respectively."}, {"title": null, "content": "N\n2\u2211 YiYi + \u20ac\nlossdice = 1\ni=1\nN\nN\n\u2211Y + \u2211Y + \u20ac"}, {"title": null, "content": "N\nlossBCE = \u2211 Yi log(yi) + (1 - Yi) log(1 \u2013 \u0177i)"}, {"title": null, "content": "where yi represents the true label, \u0177r represents the predicted result of the model, N represents the number of samples,\nand e is a small constant used to avoid division by zero. The final loss function is as Equation 10."}, {"title": "4 Experiments", "content": "loss = A\u2081lossCE + A2(lossdice + losSBCE)"}, {"title": "4.1 Datasets", "content": "To demonstrate the effectiveness of tumor segmentation, we conduct comprehensive experiments on two publicly\navailable datasets, the Breast-MRI-NACT-Pilot dataset [39] and the Breast Ultrasound Image (BUSI) dataset [40].\nBreast-MRI-NACT-Pilot is published on TCIA [41] and used for locally advanced breast cancer segmentation. This\ncollection contains 64 patients with stage 2 or 3 locally advanced breast cancer (tumor size >= 3cm) enrolled from\n1995 to 2002. All patients had breast cancer diagnosed based on histopathological reports of biopsy or surgical excision.\nUsing a bilateral phased array breast coil, the breast MRI was constructed by a 1.5-T scanner (Signa, GE Healthcare,\nMilwaukee, WI). Our experiment obtained DCE-MRI scans of 64 patients before NACT treatment, of which 62 scans\nhave a resolution of 256 \u00d7 256 \u00d7 60, and the resolution of the two scans is 512x512x64. We trained our model using 2D\nslices, with 43 patients for training, 7 for validation, and 14 for testing.\nBUSI is the first publicly available breast ultrasound dataset. It was collected in 2018 using the LOGIQ E9 ultrasound\nsystem and LOGIQ E90 Agile ultrasound system during the scanning process, including breast ultrasound images from\n600 women between the ages of 25 and 75. The dataset comprises 780 grayscale images with an average size of 500\n\u00d7 500 pixels. These images were collected and stored in DICOM format at Baheya Hospital and then converted to\nPNG format using a DICOM converter application. The collected images are divided into three categories: normal (133\nimages), benign(437 images), and malignant(210 images)."}, {"title": "4.2 Data preprocessing", "content": "In the Breast-MRI-NACT-Pilot dataset, approximately the first 70% of patients in each dataset are used for training,\nthe middle 10% for validation, and the remaining 20% for testing. We applied the same partitioning for the BUSI\ndataset's three classes (normal, benign, and malignant) to ensure consistency in sample distribution across the training,\nvalidation, and testing sets. We truncate all pixel intensities to a specific range. In the Breast-MRI-NACT-Pilot dataset,\nthe original HU range is from 0 to 96 but can vary for each patient. We normalize each patient's range to [0,255] and\nthen truncate the intensity values to the range of [20, 200]. For the BUSI dataset, the original distribution is [0, 255],\nand we truncate it to the range of [30, 235]. To prevent overfitting, data augmentation, including horizontal, vertical\nflipping, and rotation by 90\u00b0, 180\u00b0, and 270\u00b0, were applied to the BUSI dataset to increase the size of the training set\nsix-fold. We resized all data to 128 \u00d7 128 and normalized it to [-1,1] based on the methodology proposed in DDPM\n[15]."}, {"title": "5 Discussion", "content": "In this work, we demonstrate the effectiveness and clinical feasibility of the proposed method in improving cancer\nsegmentation, which includes a visual analysis and discussion of the proposed modules, as well as the applicability\nof existing efficient sampling studies to our tumor segmentation task. To directly reflect the execution process of our\nmodel, we employed Grad-CAM [42] to highlight the regions in each layer that contribute to the segmentation results."}, {"title": "5.1 Attention transfer characteristics of denoising model", "content": "We demonstrate the attention transfer characteristics of the denoising model in the segmentation task, as shown in Fig.\n6. It illustrates the spatiotemporal transition of the vital contribution regions when generating a segmentation mask.\nIn Fig. 6(a), for example, each row represents one layer's output of the model's condition flow, with diffusion steps\ndecreasing from left to right and the model's depth increasing from top to bottom.\nIn the same layer in condition flow in Fig. 6(a), attention positions vary at different diffusion steps. This variation\noften transitions smoothly as the timesteps decrease, but occasional jumps may occur. Additionally, we observe that\nthe first layer of the condition flow initially focuses on global information and gradually localizes to the tumor region.\nThe attention in the second layer appears more dispersed, similar to the distribution shown in Fig. 6(b), which may be\nattributed to the influence of the sudden introduction of information from the denoising flow. However, as the model's\ndepth increases, this bias is readjusted. The attention in the third layer shifts gradually from the background to the\nforeground. In the deeper layer (the last row), the model's attention becomes diverse, indicating that the deep network\nattempts to learn more complex features that cannot be easily interpreted.\nSimilarly, Fig. 6(b) depicts the changes in attention positions of the denoising flow. We present the results for each layer,\nallowing us to observe the variation in attention positions over time. Fig. 6(b) demonstrates that the denoising flow\nexhibits a similar pattern to the condition flow, indicating that our model operates in a rich feature space. Although this\nbranch's initial input is Gaussian noise (at T=1000), it can still accurately locate the tumor when T gradually decreases.\nThis ability can be attributed to introducing PSA."}, {"title": "5.2 Effectiveness of PSA Module", "content": "As shown in Fig. 7, we compared the changes in the heatmaps before and after applying the PSA module, where the top\nrow represents the attention positions of the latest convolutional layer before PSA, and the bottom row displays the\nattention maps after PSA. In Fig. 7(a), we notice that the model focuses more on the tumor region after incorporating\nthe PSA module in the denoising flow. This change is crucial because we need PSA to introduce semantic information\nfrom the condition flow, enabling it to perform denoising effectively in this step. Additionally, Fig. 7(b) demonstrates\nthat although the condition flow already captures the semantic information and focuses on the tumor region, introducing\nthe PSA module further enhances its confidence in the target area."}, {"title": "5.3 The effectiveness of introducing prior knowledge", "content": "In this section, we discussed the effectiveness of prior knowledge. We examined the attention positions of the last layer\nfeature map in this branch using CAM, as shown in Fig. 8. It can be observed that this subnetwork can identify the\nlocation of tumors, which means it can simulate prior knowledge provided by physicians and facilitate subsequent\ndenoising. In the Breast-MRI-NACT-Pilot dataset, tumor locations are often scattered. Although this subnetwork cannot\noffer precise segmentation like the segmentation network, it can at least identify the approximate location of the tumor,\nespecially in areas where tumors are concentrated. While it's not absolutely accurate, introducing this prior knowledge\nprovides convenience for the leading network."}, {"title": "5.4 Analyzing of the results", "content": "Fig 9 compares the performance of the GFDiffSeg model and others. The first two rows show the results of different\nmodels on the Breast-MRI-NACT-Pilot dataset, while the last two rows depict the results on the BUSI dataset. It can\nbe observed that other models often exhibit more regions of over-segmentation or unsegmented areas, whereas our\nmodel's inaccurate position is primarily located at the edges. Furthermore, both datasets indicate that Swin-Unet is\nprone to excessive segmentation and exhibits jagged edges. Moreover, SegNet and UNeXt struggle with segmenting\nsmall detached tumors. The segmentation results of Swin-Unet tend to include some unrelated regions. On the BUSI\ndataset, as depicted in the last row of Fig. 9, Unet and SegNet cannot fully segment tumors with a significant difference\nin the grayscale range. UNeXt, on the other hand, fails to identify the intersection between two grayscale ranges as\ntumor regions. Fortunately, this problem is effectively resolved in GFDiffSeg, as the denoising diffusion model is\npowerful for learning data distributions."}, {"title": "5.5 Efficient sampling", "content": "Since diffusion models require extensive sampling iterations, which significantly hinders their application, we employed\naccelerated algorithms [43, 44, 45, 46] to explore efficient sampling with reduced numbers of function evaluations\n(NFE). To mitigate the impact of randomness, we repeated each experiment five times and reported the average and\nvariance of the Dice similarity score (DSC). More details can be found in the supplemental materials. The visualized\nresults are shown in Figures 10 and 11, where the solid line represents the average DSC of the five repeated experiments,\nthe dashed line represents the variance, and the yellow horizontal line indicates the initial results of our model."}, {"title": "6 Conclusion", "content": "We propose the GFDiffSeg model, a novel approach for breast cancer segmentation. Our study further demonstrates the\nfeasibility of the denoising strategy in medical image segmentation, specifically tailored for breast cancer. We enhance\ntumor localization by incorporating the PSA module and a guided prior knowledge strategy. Experimental results show\nthat the GFDiffSeg model outperforms or performs comparably to state-of-the-art approaches. Interpretability analysis\nfurther confirms the effectiveness of our method.\nHowever, there are limitations in the current version of the GFDiffSeg model that need to be addressed in future research.\nWe have only conducted experiments on breast datasets with different modalities. The denoising process also incurs\nlonger sampling times than other models, limiting the practical application and generalization of the denoising diffusion\nmodel in medical image segmentation.\nIn the future, We aim to explore methods to reduce sampling time and extend our model to tumor segmentation in\nother organs. Additionally, we will focus on developing more efficient model architectures, such as leveraging prior\nknowledge to design prompts that guide the model in generating more accurate segmentation results."}, {"title": "6.1 Definition of the problem", "content": "The Diffusion Probabilistic Models (DPM) technique utilizes a Markov chain to convert a known distribution into a\ndesired distribution through the forward and reverse processes. Our model is based on the DDPM theory, which can be\nfound in detail in the works of [47] and [15].\nThe intermediate process of our model, illustrated in Fig. 1, entails progressively perturbing the segmentation label until\nit transforms into a Gaussian distribution, followed by the restoration of the target segmentation from the Gaussian noise.\nTo precisely identify an image's lesion area and get the desired outcomes, we incorporate the image as a condition into\nthe model, thereby generating a corresponding segmentation map.\nIn the forward process, we add Gaussian noise at each timestep, according to a variance schedule \u00dft, t=1,..., T, as shown\nin Equation 11."}, {"title": null, "content": "q (xt | Xt-1) := N (xt; \u221a1 - Bext-1, \u03b2\u03b5\u0399)"}, {"title": "6.2 Denoising and condition units", "content": "Then distribution in every timestep can be calculated given initial distribution xt, as Equation 12."}, {"title": null, "content": "q (X1:T | Xo) := \u03a0q (Xt | Xt\u22121)\nt=1"}, {"title": null, "content": "It can be further simplified according to the property of the forward process and get a distribution of arbitrary timesteps\nmore conveniently. With at := 1-\u03b2t and at := \u03a0-1 as, it can be defined as Equation 13."}, {"title": null, "content": "q (xt | xo) = N (xt; \u221a\u0101txo, (1 \u2013 \u0101t) I)"}, {"title": null, "content": "Therefore we can skip the calculation of the intermediate distribution Xt\u22121 ..., X\u2081 to obtain the arbitrary required\ntimestep distribution directly, as Equation 14. For easier understanding, xt can also be represented as[48] :"}, {"title": null, "content": "xt = \u221aatxo + \u221a1 \u2013 \u0101\u03c4\u03b5\n\u03b5 ~ \u039d(0, 1)"}, {"title": null, "content": "In the reverse process, we need to get xt-1 given xt for every timestep t, and this is also what the model needs to learn,\nas Equation 15."}, {"title": null, "content": "P\u04e9 (Xt\u22121 | Xt) := N (xt\u22121; \u03bc\u04e9 (xt, t), \u03c3\u1f37)"}, {"title": null, "content": "Thus every timestep of xt can be captured, so we can further obtain the target sample, as Equation 16."}, {"title": null, "content": "T\n\u0420\u04e9 (\u0445\u043e) := p (xT) \u041fPo (Xt\u22121 | Xt)\nt=1"}, {"title": null, "content": "1-2-1 Bt\n= (x,t))"}, {"title": null, "content": "For Xt\n\u221a1-\u0101t"}, {"title": null, "content": "Our choice is consistent with DDPM that predicts the noise \u025b to produce \u03bc\u03b8 (xt, t) except for using MSE loss to\noptimize the model parameters as Equation 19."}, {"title": null, "content": "loss(x, y) =--\u03a3(xi-yi) 2"}, {"title": null, "content": "The condition flow uses the extracted image features to transmit spatial features, while the denoising flow uses the\nextracted xt features to transmit denoised features. Due to the similar spatial structure between the image and the noisy\nlabel, we designed the same unit for encoding both the image and the noised label, as shown in Fig. 12(a). Inspired\nby [15] and \u00b9, we added time embedding to capture the temporal order, allowing multiple time steps of the diffusion\nprocess to share the same set of model parameters.\nThe denoising and condition units include time embedding, residual blocks, and downsampling. The residual block is\ncomposed of two blocks with a residual connection. The definition of the block is as Equation 20."}, {"title": null, "content": "H() = GroupNorm(SiLU(Conv(\u00b7)))"}, {"title": "6.3 Bottleneck module", "content": "As shown on the left side of Fig. 2, the denoising flow performs convolution with xt to accomplish denoising. In contrast,\nthe condition flow performs convolution with the image and holds crucial information for denoising, significantly\nimpacting the ultimate denoising outcome. Conversely, the denoising flow also carries tumor-related information that\ncan emphasize the tumor's location in the condition flow.\nThe Bottleneck module receives and fuses features from the feature encoding and prior knowledge-guided modules.\nThis fusion is performed to inject prior tumor information, which guides the learning process of the segmentation task.\nThe Bottleneck is a vital component in connecting the encoder and decoder, making it indispensable for the model.\nAs shown in Fig. 12(b), the Bottleneck comprises two residual blocks, as described in Section 6.2. Additionally, we\nintroduce a self-attention module between these two residual blocks. This inclusion enables improved integration of\nhigh-level semantic features from the encoder and provides the decoder with richer contextual information."}, {"title": "6.4 Feature decoder", "content": "Feature decoder plays a critical role in reconstructing tumor details. It converts the high-level feature representations\nextracted from the Bottleneck into the output corresponding to the original xt. By leveraging skip connections, it\nintegrates the abstract features extracted by the encoder to restore the details and structure of the initial input gradually."}]}