{"title": "Benchmarking Chinese Knowledge Rectification in Large Language Models", "authors": ["Tianhe Lu", "Jizhan Fang", "Yunzhi Yao", "Xin Xu", "Ningyu Zhang", "Huajun Chen"], "abstract": "While Large Language Models (LLMs) exhibit remarkable generative capabilities, they are not without flaws, particularly in the form of hallucinations. This issue is even more pronounced when LLMs are applied to specific languages and domains. For example, LLMs may generate nonsense information when handling Chinese ancient poetry, proverbs, or idioms, owing to the lack of specific knowledge. To this end, this paper introduces a benchmark for rectifying Chinese knowledge in LLMs via knowledge editing. Specifically, we introduce a new Chinese dataset, CKnowEdit, by collecting seven type of knowledge from various sources, including classical texts, idioms, and content from Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony, antithesis, and logical constructs inherent in the Chinese language. Through the analysis of this dataset, we uncover the challenges faced by current LLMs in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge editing techniques on this dataset unveil the substantial scope for advancement in the rectification of Chinese knowledge.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated extraordinary proficiency across a wide range of natural language processing tasks. However, their reliance on training data and the lack of explicit knowledge representation often lead to issues such as hallucinations, bias, and the production of offensive content (Zhao et al. 2023; Huang et al. 2023a; Liu et al. 2023; Sun et al. 2024). For LLMs specializing in specific languages and domains, this issue is even more pronounced. For example, LLMs may generate nonsense information when handling Chinese idioms. Since Chinese possesses its own logographic script, distinctive phonology, and unique literary forms, such as poetry representing a rich and distinct body of knowledge intrinsic to the Chinese language and culture, the lack of domain-specific knowledge in Chinese has led to a decline in the practical usability of LLMs, highlighting an urgent need to improve the capability of Chinese LLMs to address these deficiencies (Xu et al. 2023).\nIn this work, we propose to rectify Chinese knowledge in LLMs via knowledge editing, a technique enabling LLMs to generate more accurate, coherent, and trustworthy content (Yao et al. 2023; Wang et al. 2023b; Zhang et al. 2024; Hu et al. 2024; Ni et al. 2023; Wei et al. 2024b; Wang et al. 2024b; Padmanabhan et al. 2023; Qiao, Liu, and Na 2024; Chen et al. 2024; Li et al. 2024; Hase et al. 2024). Nevertheless, current research on knowledge editing predominantly concentrates on English-language literature (Cao, Aziz, and Titov 2021; Meng et al. 2022; Wu et al. 2024), and utilizes data derived from the Wikipedia corpus, which introduces an Anglo-centric bias. Recently, there have been some multilingual datasets (Wang et al. 2023a; Xie et al. 2024; Wei et al. 2024a; Nie et al. 2024) that contain pairs of English and other languages, attempting to explore editing methods for different languages. However, these datasets are often created by translating the English corpus into another language, and translation (Vanmassenhove, Shterionov, and Way 2019; Berman and Venuti 2021) has been shown failing to capture the intricate linguistic features and cultural nuances inherent to special language, resulting in a loss of lexical richness and diversity. Meanwhile, these works are primarily designed to assess the coherence of current editing methods between different languages and are not suitable for research on language-specific (a.k.a. Chinese) knowledge editing methods or for understanding LLMs' representation of specific languages.\nTherefore, we construct a new Chinese dataset, CKnowEdit, which takes into account language-specific characteristics, ensuring that the data is not only linguistically accurate but also culturally matched. This dataset not only allows us to evaluate the effectiveness of current knowledge editing methods but also provides insights into the model's representation of the language. To ensure the quality and diversity of CKnowEdit, we collect data from a variety of sources, including classical literature, modern colloquialisms, and Baidu Tieba Ruozhiba (Bai et al. 2024). Specifically, we select and manually annotate data to address common misunderstandings and cultural misinterpretations in current open-sourced LLMs. First, we identify and annotate data that highlights the misconceptions and errors in the predictions made by LLMs, which involves a thorough review of the model's outputs and a selection of instances the model predicts incorrectly. We then manually verify the accuracy and relevance of the information, ensuring that the updates to the model's knowledge base are both factually correct and contextually appropriate. Figure 1 provides a sample from the dataset, complete with an English translation. We finally organize these data into seven distinct types of Chinese-specific knowledge, including Ancient Poetry, Proverbs, Idioms, Phonetic Notation, Classical Chinese, Geographical Knowledge and Ruozhiba. Each of these categories reflects unique knowledge aspects of the Chinese language and culture.\nTo benchmark the effectiveness of knowledge editing methods on Chinese text, we evaluate four representative methods on the CKnowEdit dataset. To focus on the capability of LLMs to generate answers, we use both word-level overlap and semantic vector similarity to evaluate the performance of the edited models (Details in Task Evaluation in Figure 2). The results demonstrate the challenges presented by the dataset and underscore the need for more sophisticated Chinese knowledge editing approaches in the future. Our major contributions are as follows:\n\u2022 We propose a new Chinese dataset CKnowEdit to benchmark Chinese knowledge rectification in LLMs, which contains 1,760 instances across seven Chinese-specific types of knowledge.\n\u2022 We report the empirical results of recent knowledge editing baselines on CKnowEdit, revealing their limitations when applied to Chinese literature.\n\u2022 We further explore the challenges of Chinese knowledge editing and the struggles faced by existing models in understanding Chinese language and culture."}, {"title": "Related Work", "content": "Current knowledge editing approaches can be categorized into two main types: preserving existing parameters and entailing modification. The preservative methods incorporate explicit memory and prompting techniques to rectify model predictions. Examples include SERAC (Mitchell et al. 2022b) and IKE (Zheng et al. 2023). Some modify the Feed-forward Neural Network (FFN) layer, as exemplified by CaliNET (Dong et al. 2022), T-Patcher (Huang et al. 2023b) and GRACE (Hartvigsen et al. 2024). Alternatively, the locate-and-edit approaches have to locate the relevant neurons, then modify those parameters. Representative studies are KN (Dai et al. 2022), ROME (Meng et al. 2022), and MEMIT (Meng et al. 2023). Conversely, meta-learning approaches utilize a hyper-network to generate the weights for layers in LLMs, including KE (Cao, Aziz, and Titov 2021), MALMEN (Tan, Zhang, and Fu 2023) and MEND (Mitchell et al. 2022a).\nTo advance the field of knowledge editing, a multitude of datasets are being developed to explore the potential and impact of these methods. Existing knowledge editing datasets have largely centered on English-language texts, such as ZsRE (Cao, Aziz, and Titov 2021), Counterfact (Meng et al. 2022), KnowEdit (Zhang et al. 2024), and MQUAKE (Zhong et al. 2023). Some research (Deng et al. 2024; Rosati et al. 2024; Wu et al. 2024) has also introduced the concept of evaluating knowledge editing through unstructured text and long-form content, but these efforts have been predominantly limited to the English language. In a more inclusive direction, recent academic initiatives have broadened the scope of these datasets to include a multi-lingual dimension (Xie et al. 2024; Wei et al. 2024a; Nie et al. 2024). However, these datasets, while considering various languages, are often based on translations and parallel corpora, thereby overlooking the unique aspects of each specific language.\nIn summary, current knowledge editing methods are primarily tailored for dealing with factual knowledge in English, utilizing structured facts as the foundation for edits, typically in the form of subject-relation-object triplets from Wikipedia. In response, we propose a groundbreaking Chinese knowledge editing benchmark that encompasses culturally and linguistically relevant questions. This dataset more accurately reflects real-world applications, as it includes language-specific knowledge updates that are generally communicated through unstructured text. It also captures the unique imagery and connotations that are challenging for both humans and LLMs to fully grasp, thereby enhancing the depth and precision of language understanding."}, {"title": "Background: Knowledge Editing", "content": "Pioneered by Cao, Aziz, and Titov (2021), knowledge editing is designed to correct factual inaccuracies or update specific facts in a model without the need for comprehensive retraining. This approach is particularly appealing as it allows for the rectification of errors while maximizing the preservation of unrelated knowledge, thus maintaining the overall integrity and performance of the model."}, {"title": "CKnowEdit Dataset Construction", "content": "CKnowEdit is meticulously designed to more deeply discern the nuances and challenges inherent in the comprehension of the Chinese language by current LLMs, providing a robust resource for refining Chinese-specific knowledge within LLMs. All data utilized in the construction of this dataset is derived from publicly available sources on the Internet, ensuring that there are no copyright or intellectual property risks involved. The procedures employed in the creation of CKnowEdit and the associated statistical details are outlined in Figure 2.\nWe delve into diverse and authentic Chinese linguistic resources, including the rich tapestry of classical literature, the vibrant expressions of contemporary colloquial speech, and the unique cultural insights found within the Baidu Tieba forum known as Ruozhiba, each reflecting distinct facets of Chinese linguistic identity. Particularly, we select the common misunderstandings and cultural misinterpretations found in current large language models (Qwen-7B-Chat) and build the editing dataset that targets these specific knowledge gaps. The construction process also includes a human review phase to verify the accuracy and relevance of the information, thereby ensuring that the edits made to the model's knowledge base are reasonable and faithful. The details of construction for each type of knowledge are as follows:\nChinese Literary Knowledge. This kind of knowledge in CKnowEdit includes Ancient Poetry, Proverbs and Idioms. The selection process involves identifying segments of text that commonly present challenges for language models, focusing on those that require a deep understanding of cultural context, historical significance, or linguistic detail.\nThese materials are drawn from reputable educational sources and cultural repositories, reflecting China's rich educational standards and profound heritage. For example, ancient poetry and proverbs are sourced from well-recognized compilations commonly used in school curricula, such as the \"Must-memory ancient poems for college entrance examination\" for ancient poetry while idioms and phonetic notations are derived from collections known for their extensive usage and occasional misinterpretations, such as the \"Common error-prone idioms and error-prone pronunciations\" collection. Each category has been developed to address specific errors of biases in language understanding by LMs. For example, classical Chinese entries focus on polysemy and contextual meanings, which are often challenging for LLMs.\nWe initially obtain the generated texts by posing questions to the Qwen-7b-chat model (Bai et al. 2023) and collecting their responses. Then we collect the original prompt, the model's incorrect response, and the accurate answer. The accurate answers are generated through GPT-4 (gpt-4-turbo-2024-04-09) (Achiam et al. 2023), utilizing human-crafted prompt templates designed to ensure the correction's relevance and applicability. Each entry is then reviewed by humans, who correct the misinterpretations or misrepresentations in the entries given by the GPT-4 model. The annotation process depends on the online Chinese dictionary to ensure the knowledge is reliable.\nChinese Linguistic Knowledge. In Chinese, certain words exhibit multiple phonetic notations or meanings depending on their usage within different contexts. Similarly, the word may appear in various Classical Chinese sentences with different meanings, necessitating precise contextual understanding to ensure accurate interpretation. To address these nuances, the dataset includes entries where the same word appears in different sentences or phrases, requiring distinct phonetic annotations or interpretations. This part contains two parts: Phonetic Notation, and Classical Chinese The construction part is the same as the literary knowledge, but additional human review steps are taken to generate entries for portability and locality since there are too many errors in the data entries generated by the GPT-4 model (likely because the knowledge about the usage of these languages in the GPT-4 model is also wrong.) For portability, entries are crafted to test if the language model can apply these distinctions across different scenarios, thereby understanding that a change in one instance should reflect correctly in similar uses. For locality, the dataset ensures that changes made to the word's phonetic notation or meaning in one context do not erroneously alter its correct applications in others where its pronunciation or meaning should remain different.\nChinese Geographical Knowledge. The Geographical Knowledge dataset is carefully designed to correct and refine geographical commonsense data within language models, emphasizing the importance of accurate and contextually relevant geographical information. The dataset is built upon a series of prompts which is human-crafted with specific geographical knowledge from Internet. The incorrect answers provided by the model are documented. Each corresponding correct answer is sourced from reliable, authoritative texts. Each dataset entry consists of the original geographical prompt, the incorrect response from the Qwen-7b-chat model, and the correct answer. For each pair of prompts and responses, GPT-4 is employed again to generate additional prompts testing the LM's ability to apply the corrected information in different but related contexts. For example, two-step logical reasoning about the knowledge of the position in the correct answer. After the initial generation and correction with the help of GPT-4, each entry undergoes a human review process to refine the language and ensure the accuracy of the geographical data. This step is vital for validating the portability, making sure they align with the core factual correction and enhance the overall utility of the dataset.\nChinese Ruozhiba. The Ruozhiba (Bai et al. 2024) dataset is constructed from a subsection of Baidu Tieba known as Ruozhiba, which is famed for being a breeding ground for original, often humorous content that incorporates logical puzzles and philosophical musings. These characteristics make it an invaluable resource because the data in it requires not just linguistic understanding, but also the capability to handle logical traps. In order to collect the knowledge that the LLM incorrectly has, the selected Ruozhiba dataset content is the question sentences that typically include logical traps or require higher-order thinking to interpret correctly.\nEach selected question is used as a prompt in interactions with both the Qwen-7b-chat and GPT4 models. Then we collect the responses: where Qwen-7b-chat falters, typically providing incorrect or logically inconsistent answers, and where GPT-4 succeeds in delivering accurate responses. For each pair of prompts and responses, the GPT-4 is employed again to generate structured dataset entries with a prompt template that focuses on portability and locality. Portability involves creating additional prompts and answers that extend the application of the corrected knowledge to similar or related scenarios, testing the model's ability to generalize the new information. And locality ensures that the corrections do not inadvertently affect the correct understanding of other unrelated pieces of knowledge. The last phase is a human review process to ensure that each entry is accurate by deleting the wrong data entry generated by GPT-4. Meanwhile, human review also helped correct some JSON format errors, as well as a small number of factual inaccuracies, ensuring that the final version of the constructed dataset is error-free."}, {"title": "Experiments", "content": "We adopt Qwen-7B-Chat, and Baichuan2-7B-Chat models to conduct a series of tests aiming to evaluate our CKnowEdit dataset via knowledge editing. Qwen-7B-Chat is the original model used for data collection, providing baseline performance. The models are fine-tuned using knowledge editing techniques FT-M (Zhang et al. 2024), AdaLoRA (Zhang et al. 2023), ROME (Meng et al. 2022), and GRACE (Hartvigsen et al. 2024), to provide a new benchmark for Chinese Knowledge Edit dataset. Additionally, we designed a prompt template to facilitate the editing with PROMPT method. All the experiments are conducted by EasyEdit (Wang et al. 2024a) and we use the default parameters of each method. We aim to assess the effectiveness of the CKnowEdit Dataset in refining and enhancing the knowledge base of LLMs through targeted knowledge edits. Each data entry from the dataset is used to modify the model's knowledge, and the resultant changes in performance are measured using specific evaluation metrics."}, {"title": "Metrics", "content": "We employ several key evaluation metrics (Yao et al. 2023) that provide a comprehensive measure of the edits' efficacy. These metrics are designed to not only validate the accuracy of the model's post-edit responses but also to ensure that the edits contribute positively to the model's overall knowledge and response quality:\n\u2022 Edit Success: This metric measures how well the edits align the model's responses with the expected outcomes, specifically assessing the correctness of the information provided by the model in response to the edited prompts.\n\u2022 Portability: This measures the model's capability to apply corrected knowledge to new, related prompts, assessing flexibility across contexts. It is crucial to test the model's flexibility and adaptability in utilizing the edited information in various scenarios.\n\u2022 Locality: This metric ensures that edits do not inadvertently affect unrelated areas of the model's knowledge base. It helps maintain the integrity of the model's comprehensive knowledge outside of the edited scopes.\n\u2022 Fluency: Fluency measures the linguistic quality of the model's outputs, focusing on how natural and diverse the text generation is after the edits. This is essential to ensure that the model's language capabilities remain robust and unaffected by the knowledge editing process.\nPrevious work (Meng et al. 2022; Yao et al. 2023) mainly focuses on the fact editing and the computation of their metric on the target entities and computed by the token level (Wang et al. 2024a). However, CKnowEdit is an unstructured text knowledge editing dataset. Therefore, to evaluate the performance of current knowledge editing methods, we follow Deng et al. (2024) and employ word-level overlap metrics ROUGE-L score (Lin 2004) and semantic similarity assessments using embedding encoders (specifically, the paraphrase-multilingual-MiniLM-L12-v2 model). We report the average of these two metrics. These methods provide insights into both the lexical fidelity and the deeper semantic accuracy of the model's responses. Following ROME (Meng et al. 2022), fluency is measured by the weighted average of bi-gram and tri-gram entropy (Zhang et al. 2018). A decrease in these values indicates a reduction in text diversity, highlighting potential issues in the model's ability to generate varied and natural-sounding language post-edit."}, {"title": "Main Results", "content": "The evaluation performance of CKnowEdit across two different models is shown in Table 1, corresponding to Qwen-7B-Chat, and Baichuan2-7B-Chat, respectively.\nAcross most knowledge types for both models, the AdaLoRA method and PROMPT method consistently outperform other methods in terms of Edit Success. Portability scores are highest with AdaLoRA, suggesting that this method best enables the models to transfer edited knowledge effectively to related contexts, but overall, the existing methods are not very good at portability, which is a ubiquitous problem for other languages. The FT-M, ROME and GRACE methods show better locality results, indicating effective containment of edits within the targeted knowledge. In terms of Fluency, there are variations across knowledge types and methods, with Fluency generally better in Qwen-7B-Chat compared to Baichuan2-7B-Chat model. Across the various datasets examined, the GRACE method underperforms in comparison to other knowledge editing techniques. During our experiments, we observe that the GRACE method often leads the models to generate sentences that are repeated and lack coherence. This pattern suggests that the GRACE method may not be well-suited for tasks involving the editing of Chinese knowledge, which may be raised by the unique representation of the model matrix for Chinese.\nIn comparing the outcomes of our experiments with those typically seen with English knowledge editing datasets (Zhang et al. 2024), our Chinese dataset presents additional complexities and challenges. The difference between Chinese and English datasets fundamentally impacts knowledge editing due to inherent linguistic and cultural differences. In our experiments, the performances of classical Chinese are lower than other knowledge types, demonstrating the difficulty of Chinese cultural editing. The syntactic and semantic richness, coupled with a high context-dependency in Chinese, demands more sophisticated knowledge editing approaches to handle nuances such as polysemy and homophony, which are less prevalent in English. Traditional knowledge editing methods developed for English datasets often rely on direct lexical or syntactic modifications, which can be insufficient for Chinese due to its script and structure. For instance, methods effective in English (especially GRACE) cannot be applied directly to Chinese, where meaning can be significantly altered by the modification of a single character. Furthermore, cultural references and idiomatic expressions prevalent in Chinese require editing methods that are sensitive to underlying cultural contexts, which is not typically a focus in English language editing tools.\nAccording to the aforementioned analysis, it is necessary to develop advanced knowledge editing techniques that are specifically tailored to accommodate the unique linguistic features and cultural richness of the Chinese language. These techniques should not only address the language's structural aspects but also its deep-seated cultural elements to ensure the effectiveness and appropriateness of edits."}, {"title": "Case Study", "content": "As shown in Figure 3, in this case study, we examine the classical Chinese character: \u201c\u719f\u201d to assess the effectiveness of knowledge editing in handling interchangeable words. The Qwen-7B-Chat model initially fails to correctly interpret \u201c\u719f\u201d in a context where it does not mean \"who\" but rather \"carefully\" or \"thoroughly\", which is an interchangeable word."}, {"title": "Discussion on Chinese Knowledge Rectification", "content": "Current knowledge editing methodologies, originally designed for English language models, frequently encounter limitations when applied to Chinese datasets. These limitations stem from an inability to encapsulate the nuances of Chinese linguistics and culture.\nFirstly, the Chinese language typically requires more characters to convey the same information as English, with a higher incidence of overlapping terms. For instance, while \"September\" and \"July\u201d are distinct words in English, they both contain the same character \u6708in Chinese. This characteristic poses a challenge for methods like ROME, which focus on the final token of a subject, often leading to inaccuracies in classical Chinese contexts. Consequently, there is an urgent need for innovative knowledge editing techniques tailored to the intricacies of Chinese language processing, ensuring models deliver more precise, culturally informed, and contextually appropriate outcomes. Secondly, China's rich tapestry of dialects and colloquialisms necessitates their integration into knowledge editing to broaden the utility of language models. Our experimental findings reveal that the transferability of literary knowledge is notably limited, suggesting that language models may be misapplying culturally specific content. Future research should aim to incorporate these regional linguistic variations, enabling models to produce and comprehend language that more genuinely reflects the vernacular of local communities. Thirdly, exploring cross-linguistic knowledge transfer is also important for leveraging insights gained from one language to enhance model performance in another, particularly in multilingual contexts. This involves developing methods that facilitate the transfer of learned knowledge across different languages."}, {"title": "Conclusion", "content": "In this paper, we introduce CKnowEdit, a Chinese knowledge editing dataset tailored to capture the distinctive characteristics of the hieroglyphic calligraphy that defines the Chinese script. The dataset encompasses a variety of knowledge types, including Ancient Poetry, Proverbs, Idioms, and numerous other classical Chinese cultural and linguistic elements. Note that Chinese knowledge rectification can help enhance the trustworthiness in LLMs and improve their understanding of Chinese. The objective of CKnowEdit is to provide a comprehensive resource that reflects the depth and richness of the Chinese language and culture, enabling more precise knowledge editing for Chinese large language models. We hope the proposed dataset can inspire future work and provide insights to the AI community."}]}