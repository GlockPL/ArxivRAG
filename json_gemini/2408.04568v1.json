{"title": "Learning Fine-Grained Grounded Citations for Attributed Large Language Models", "authors": ["Lei Huang", "Xiaocheng Feng", "Weitao Ma", "Yuxuan Gu", "Weihong Zhong", "Xiachong Feng", "Weijiang Yu", "Weihua Peng", "Duyu Tang", "Dandan Tu", "Bing Qin"], "abstract": "Despite the impressive performance on information-seeking tasks, large language models (LLMs) still struggle with hallucinations. Attributed LLMs, which augment generated text with in-line citations, have shown potential in mitigating hallucinations and improving verifiability. However, current approaches suffer from suboptimal citation quality due to their reliance on in-context learning. Furthermore, the practice of citing only coarse document identifiers makes it challenging for users to perform fine-grained verification. In this work, we introduce FRONT, a training framework designed to teach LLMs to generate Fine-gRained grounded ciTations. By grounding model outputs in fine-grained supporting quotes, these quotes guide the generation of grounded and consistent responses, not only improving citation quality but also facilitating fine-grained verification. Experiments on the ALCE benchmark demonstrate the efficacy of FRONT in generating superior grounded responses and highly supportive citations. With LLaMA-2-7B, the framework significantly outperforms all the baselines, achieving an average of 14.21% improvement in citation quality across all datasets, even surpassing ChatGPT\u00b9.", "sections": [{"title": "1 Introduction", "content": "The recent advent of large language models (LLMs) (Touvron et al., 2023; OpenAI, 2023; Zhao et al., 2023) has taken the world by storm, fueling a paradigm shift in information acquisition (Zhu et al., 2023). Despite their compelling performance, LLMs still struggle with hallucinations (Ji et al., 2023; Huang et al., 2023), a tendency to fabricate non-existent facts or generate unfaithful content. This issue further poses a risk of misinformation dissemination (Chen and Shu, 2023), directly impacting the reliability and trustworthiness of LLMs.\nSuch prevalence of hallucinations in LLM outputs has motivated the development of attributed systems (Nakano et al., 2021; Thoppilan et al., 2022; Menick et al., 2022), such as New Bing2 and Perplexity\u00b3, where LLMs are allowed to generate responses with in-line citations. Not only does it improve factuality and alleviate hallucinations, but it also simplifies user verification of model outputs, further enhancing the verifiability of LLMs.\nDespite recent advancements, current attributed LLMs still expose significant limitations. Firstly, current approaches predominantly rely on either in-context learning (Gao et al., 2023b) or post-hoc retrieval (Gao et al., 2023a) to achieve attribution,"}, {"title": "2 Related Work", "content": "Retrieval Augmented Generation. Recently, retrieval augmented generation (RAG) (Karpukhin et al., 2020; Lewis et al., 2020; Feng et al., 2023; Gao et al., 2023c) has shown promise in knowledge-intensive tasks. By incorporating retrieved documents, LLMs are equipped with up-to-date information, significantly mitigating knowledge gaps. However, recent studies (Shi et al., 2023; Yoran et al., 2023; Xu et al., 2023a; Zhu et al., 2024) have revealed that existing retrieval-augmented LLMs struggle to handle irrelevant or contradictory retrieval documents and effectively utilize contextual evidence. These limitations can result in performance degradation or even hallucinations (Huang et al., 2023), highlighting the necessity for more factual and verifiable systems.\nAttributed Large Language Models. The persistent challenge of hallucinations within LLMs has spurred the development of attributed LLMs (Bohnet et al., 2022; Li et al., 2023; Worledge et al., 2023), which seek to enhance information verifiability by generating responses with attribution to evidence sources. The way of providing attributions varies across studies. For example, Gao et al. (2023b) enables LLMs to generate text with in-line citations via in-context learning. Another line of research (Gao et al., 2023a; Xu et al., 2023b) explores post-hoc attribution, where LLMs first generate an initial response and then retrieve the most relevant evidence to achieve attribution. In this paper, we advance the research on attributed LLMs further. Unlike existing models that predominantly cite document identifiers, we delve into a more fine-grained form of attribution by pinpointing and citing specific extractive quotes."}, {"title": "3 Task Formulation and Methodology", "content": "Following (Liu et al., 2023; Gao et al., 2023b), the task is formalized as follows: given a user query q and a corpus of retrieved documents D as input, the LLM is required to produce a response S, which consists of statements with embedded in-line citations. We assume the response S comprising with n statements $S = {s_1,s_2,...,s_n}$ and each statement $s_i \\in S$, cites a list of passage $C_i = {C_{i1}, C_{i2},...}$, where $C_{ij} \\in D$. Specifically, citations are presented in the form of [1][2].\nNext, we present a comprehensive overview of our method, which consists of two primary components:"}, {"title": "3.1 Automatic Data Generation Pipeline", "content": "Equipping LLMs with the attribution capability necessitates training data that includes high-quality responses paired with precise citations, which is typically labor-intensive and costly. To address this challenge, we propose a pipeline designed for the automatic generation of high-quality attributed data4. This pipeline comprises three core components: data collection, attributed answer generation, and data filtering, as outlined in Figure 2.\nData Collection. To simulate the real-world environment for information-seeking, we collect questions from the AQuAMuSe dataset (Kulkarni et al., 2020), which is derived from the Natural Question (NQ) dataset (Kwiatkowski et al., 2019). The NQ dataset comprises real user queries from the Google search engine, providing a robust basis for realistic question-answering scenarios. The dataset spans a range of diverse question types, demanding answers of varying lengths, from concise to detailed. To mimic the way a search engine might synthesize documents of high relevance in response to a user query, we employ Sphere (Piktus et al., 2021), a pre-processed and cleaned version of the Common Crawl corpus, serving as a proxy web search index. In particular, for a given user query sampled from the AQUAMuSe dataset, we initially retrieve the top 100 relevant documents from the Sphere corpus using sparse retrieval. These documents are\nAttributed Answer Generation. Given the remarkable performance of ChatGPT in attributed question answering, we employ ChatGPT to generate answers with corresponding citations for given queries and the top 5 retrieved documents. We provide precise instructions and in-context demonstrations to ensure that ChatGPT produces informative responses and cites the sources accordingly.\nData Filtering. To guarantee the high quality of our synthetic training data, we employ a data filtering process guided by two key criteria derived from Kamalloo et al. (2023): (1) informativeness: assessing if the answer provides sufficient information to the question, and (2) attributability: determining if the answer is attributed to the cited documents. To mitigate the impact of nonsensical queries and irrelevant document retrieval that may lead to non-informative answers, we utilize ChatGPT for preliminary informativeness annotations. Responses categorized as non-informative are directly excluded. Furthermore, to ensure that answers are accompanied by highly supportive citations, we train a discriminator on human-labeled data from the comprehensive evaluation by Liu et al. (2023), where attributability is categorized into three levels: full support, partial support, or no support. We quantitatively map the discriminator's outputs to an attributability score and ultimately derive an average score for each attributed answer."}, {"title": "3.2 Two-Stage Training Recipe", "content": "In this section, we introduce FRONT, a two-stage training framework that aims at empowering LLMs with fine-grained attribution capability. Figure 3 illustrates the overview of our framework.\n3.2.1 Grounding Guided Generation\nTo empower LLMs with fine-grained attribution capability, we propose Grounding Guided Generation (G\u00b3), which teaches LLMs to generate fine-grained citations. The cornerstone of G\u00b3 lies in enabling LLMs to extract supporting quotes from the source documents, each associated with its document identifier, which in turn guides the generation of attributed answers. Such a format offers two primary benefits. Firstly, the direct extraction of quotes from sources significantly reduces the impact of the incorporation of irrelevant information and the risk of hallucinations in subsequent attributed answers. Secondly, the process naturally facilitates accurate attribution, with each document identifier serving as a clear supervised signal that delineates the origin of the extractive quotes, thus improving the citation quality.\nHowever, the absence of specific grounding content for statements within our generated dataset poses additional challenges. To tackle this, we employ ChatGPT to meticulously extract segments from cited documents that support the corresponding statement. Hence, when given a query q and the top-5 retrieved documents D as input, the LLM is fine-tuned to generate a response S which consists of two components: the grounded quotes G and the attributed answer A. Specifically, the grounded quotes G are delineated as follows:\n$G = {[GROUNDING], (i_1, e_1), ..., (i_n, e_n)}, (1)$\nwhere [GROUNDING] denotes a special token indicating the start of the grounding process. Each tuple within G, comprising a document identifier i and the corresponding extractive segment e, collectively forming a grounded quote.\nSimilarly, the formulation of the attributed answer A is concisely presented as:\n$A = {[ANSWER], s_1, s_2..., s_m}, (2)$\nwhere [ANSWER] is a special token that signals the beginning of the answer generation process. Each statement $s_i$ cites a list of passages $C_i = {C_{i1}, C_{i2}, ...}$, where $C_{ij} \\subseteq {i_1, i_2, . . ., i_n }$, as defined in Equation 2.\nThus, the training loss is formulated as:\n$L = -\\sum_{i=1}^{N} log P(Y_i | q_i, D_i; \\theta)  (3)$\nwhere $Y_i$ represents the combined output of grounded quotes G and the answer A for each given query $q_i$ and set of retrieved documents $D_i$.\n3.2.2 Consistency-Aware Alignment\nWhile G\u00b3 unlocks the ability to first extract supporting quotes before generating attributed answers, it occasionally leads to inconsistencies between grounded quotes and attributed answers. Such discrepancies challenge the attempt to employ these"}, {"title": "4 Experimental Settings", "content": "4.1 Datasets\nWe conduct experiments on the ALCE benchmark (Gao et al., 2023b), designed for attributed text generation. The benchmark includes three long-form QA datasets that span various types of questions.\nASQA (Stelmakh et al., 2022) is a long-form factoid QA dataset characterized by inherently ambiguous questions that require multiple short answers to encapsulate different viewpoints.\nELI5 (Fan et al., 2019) features open-ended questions intended for simplification to the comprehension level of five-year-olds, requiring explanatory multi-sentence responses.\nQAMPARI (Amouyal et al., 2022) is a factoid QA dataset derived from Wikipedia, where answers are structured as a compilation of entities.\n4.2 Evaluation Metrics\nFollowing the ALCE benchmark (Gao et al., 2023b), our evaluation primarily focuses on two key dimensions: Citation Quality and Correctness. Detailed descriptions of additional evaluation dimensions are presented in the Appendix B."}, {"title": "4.3 Baselines", "content": "We compare our method with three types of baselines: prompting-based, post-hoc retrieval, and training-based.\n4.3.1 Prompting-based Methods.\nWe directly prompt LLMs using few-shot demonstrations, each consisting of a query, the top 5 relevant retrieved documents, and an answer with in-line citations. Our experiments encompass a range of LLMs, from foundational models to supervised fine-tuning (SFT) LLMs. For foundational LLMs, we select GPT-3.5-Turbo as the representative closed-source model, recognized for its notable performance. Among the open-source foundational LLMs, we focus on the LLaMA-2 series including LLaMA2-7B, LLaMA2-13B, and LLaMA2-70B, as well as the Mistral series, which spans from"}, {"title": "4.4 Implement Details", "content": "We implement FRONT with different sizes of foundational models (LLaMA-2-7B and LLaMA-2-13B) to evaluate its effectiveness. During the evaluation, FRONT utilize the same retrieval settings as those outlined by Gao et al. (2023b). Additional details of training and evaluation settings can be found in Appendix D."}, {"title": "5 Results and Analysis", "content": "5.1 Overall Results\nSimply supervised fine-tuning can boost citation quality. As shown in Table 1, teaching LLMs to generate responses with citations via supervised fine-tuning significantly enhances citation quality, demonstrating substantial improvements over both prompt-based and post-hoc retrieval baselines across all datasets. Specifically, with LLaMA-2-7B, VANILLA-SFT led to substantial gains in citation F1 scores over prompting: ASQA (17.55 \u2192 65.61), ELI5 (4.54 \u2192 41.15), and QAMPARI (6.19 \u2192 21.35). These gains highlight the effectiveness of our training data generation pipeline.\nFRONT achieves significant performance gains and surpasses ChatGPT. While VANILLA-SFT demonstrates strong performance, it still shows notable discrepancies compared to leading open-source LLMs, such as Mixtral-8\u00d77B-Instruct (e.g., 41.15 vs. 45.74) and ChatGPT (e.g., 41.15 vs. 48.22) on the ELI5 dataset. FRONT not only bridges these gaps but also establishes significant leads across all datasets. Specifically, using LLaMA-2-7B, FRONT comprehensively outperforms ChatGPT, achieving increases of 3.32%, 18.04%, and 21.28% in citation quality on the ASQA, ELI5, and QAMPARI datasets respectively. This performance underscores the effectiveness of FRONT in enhancing attribution capabilities.\nFRONT exhibits scalability with model size. As illustrated at the bottom of Table 1, the performance of FRONT in terms of citation quality shows notable improvements when scaling from 7B to 13B. Specifically, we observe improvements of 3.23% in ASQA, 4.97% in ELI5, and 1.33% in QAMPARI. This upward trend underscores the scalability of FRONT with increasing model size, demonstrating the potential of FRONT in leveraging the increased capabilities of larger LLMs for further performance gains."}, {"title": "5.2 Ablation Study", "content": "We conduct ablation studies to verify the effectiveness of different components proposed in FRONT.\nEffects of Data Generation Pipeline. As illustrated in \u00a75.1, simply SFT achieves strong performance, underscoring the high quality of our training data. Furthermore, data filtering, a crucial component of our data generation pipeline, plays a pivotal role in ensuring the quality of the generated data by filtering out queries that yield non-informative answers or fail to meet attribution criteria. To validate the effectiveness of our data filtering strategies, we conducted experiments comparing models fine-tuned on both pre-filtered and post-filtered data. The results, depicted in Figure 4, confirm that models trained on filtered data exhibit a notable improvement in citation quality over those trained on unfiltered data, achieving superior attribution performance with reduced data volume."}, {"title": "6 Human Evaluation", "content": "Given the significant impact of the quality of grounded quotes on fine-grained verification for users, we conducted a human evaluation to assess the quality of grounded quotes at different stages of our framework: (1) Quotes extracted by ChatGPT from 50 sampled data points during the G\u00b3 stage. (2) Quotes generated by FRONT-7B across three datasets, with 50 data points sampled from each.\nWe engaged four annotators, each with relevant expertise and holding at least a bachelor's degree. The quality of quotes was evaluated on two dimensions: authenticity and helpfulness. Authenticity (a binary scale of 0/1) refers to whether the quotes genuinely originate from the corresponding documents (quotes that are hallucinated or mismatched with the corresponding document ID are considered inauthentic). Helpfulness (5-point Likert scale) refers to the degree to which the quotes are beneficial in addressing the query. The results in Table 3 represent the average scores for all quotes within each model response, with two annotators evaluating each response to ensure reliability.\nFurthermore, to evaluate the consistency of quote quality annotations, we computed the inter-annotator agreement using Fleiss' Kappa coefficient. The obtained Kappa coefficient of 0.82 indicates a high level of agreement among annotators."}, {"title": "7 Conclusion", "content": "In this work, we present FRONT, a two-stage training framework designed to equip LLMs with fine-grained attribution capabilities. FRONT enables LLMs to initially select supporting quotes, which then guide the generation process. By further enhancing the consistency between the grounding and generation process via preference optimization, these supporting quotes can serve as fine-grained citations. Through comprehensive experiments, FRONT has demonstrated its ability to generate superior grounded responses and highly supportive citations. Further analysis shows that FRONT significantly reduces hallucinations and benefits user verification."}, {"title": "8 Limitation", "content": "Our study presents several limitations worth noting. Firstly, the validation of our framework is predominantly conducted on models of sizes 7B and 13B, leaving the exploration of larger models, such as LLaMA-2 70B due to computational constraints. Secondly, our framework relies on a prior retrieval process, wherein relevant documents are retrieved at one time. The incorporation of adaptive retrieval, enabling more dynamic interactions with LLMs, could potentially enhance performance. We leave it for future research. Lastly, evaluating the correctness of long-form question answering presents inherent challenges, leading our framework to primarily enhance citation quality, with modest advancements in correctness. Therefore, we advocate for the development of more robust metrics capable of accurately assessing the correctness of long-form QA responses, paving the way for future work."}, {"title": "A.1 Data Statistic", "content": "# Questions\n8,098\n# Long Answer\n5667\n# Short Answer\n2431\nAvg. Words per Answer\n50.48\nAvg. Words per Long Answer\n69.15\nAvg. Words per Short Answer\n6.94\nAvg. Citation per Answer\n4.40\nAvg. Citation per Long Answer\n4.68\nAvg. Citation per Short Answer\n3.77"}, {"title": "A.2 Details of Data Filtering", "content": "We trained our Attributed Discriminator using the manually annotated data provided by Liu et al. (2023), which is sampled from real generative search engines. Each statement and its cited document have been meticulously annotated for attribution, categorized into three types: complete support, partial support, and no support. For training, we utilized a dataset of 8,834 instances, comprising 6,415 instances of complete support, 1,552 of partial support, and 867 of no support. The discriminator initialized with LLaMA-2-7B, was trained with a maximum sequence length of 512. We trained it for 3 epochs, with a total batch size of 128, and a peak learning rate of 2e-5, incorporating 3% warmup steps, followed by a linear decay.\nDuring the data filtering stage, we first break down the automatically generated attributed answers into statement form and use the trained discriminator to annotate the attribution between each statement and its cited documents. Specifically, we assign different attribution scores to each statement s based on its attribution relationship with cited documents d, as shown in Equation 7. Consequently, for each attributed answer, we can calculate its average attribution score. Attributed answers with an average attribution score below 0.8 are filtered out. The threshold of 0.8 was determined through preliminary testing on the development set, for which we manually annotated 100 samples to ensure the effectiveness of our filtering criteria.\nr(s) = {1, Dis(s,d) = complete support\\0.5, Dis(s,d) = partial support\\0, Dis(s, d) = no support      (7"}, {"title": "B Details of Evaluation Metrics", "content": "In addition to evaluating citation quality and correctness, the ALCE benchmark includes a broader set of dimensions, such as fluency, ROUGE-L, and generation length."}, {"title": "C Prompts", "content": "C.1 Prompts for Prompting-based Methods\nFollowing Gao et al. (2023b), we adopt the vanilla prompting strategy for its simplicity and effectiveness. Specifically, the prompts vary according to the type of data within the ALCE benchmark. For long-form QA datasets such as ASQA and ELI5, the prompt format is detailed in Table 5. For the short-form QA dataset QAMPARI, the format is outlined in Table 6.\nC.2 Instructions for FRONT\nDuring the training process, we follow the instruction format of Alpaca6. Specifically, we employ varied instructions for different question types, as delineated in Table 7 for long-form questions and Table 8 for short-form questions."}, {"title": "D.1 Training Details of FRONT", "content": "The training of all models is executed on 4 Nvidia A100 GPUs, each with 80GB of memory, leveraging the Deepspeed (Rasley et al., 2020) and HuggingFace Accelerate libraries (Gugger et al., 2022) to conduct multi-GPU distributed training. Given the long nature of the inputs, the maximum token length is set to 2,048 tokens.\nDuring the grounding guide generation stage, models are trained for 5 epochs with a total batch size of 128, a peak learning rate of 2e-5 with 3% warmup steps followed by a linear decay. During the contrastive alignment stage, we set the  \\beta  to 0.1 and continued training for two additional epochs.\nE.1 The Effect of Training Data Scale.\nWe examine how model performance varies with changes in data scale, as depicted in Figure7. The upper part of the figure illustrates the impact of the training data scale on citation quality during the Grounding Guided Generation training stage, with datasets ASQA, ELI5, and QAMPARI represented from left to right. Similarly, the lower part of the figure describes the influence during the Consistency-Aware Alignment training stage.\nE.2 The Generalization Across Model Architectures.\nFRONT demonstrates exceptional generalization capabilities across various foundational model architectures. Specifically, transitioning the foundational model from LLaMA-2-7B to the stronger foundational model, Mistral-7B, results in even greater performance enhancements as shown in Figure 8. This further underscores the broad applicability and generalizability of FRONT.\nE.3 The effect of \u03b2 in Consistency-Aware Alignment Training Stage\nIn the Consistency-Aware Alignment Training Stage, the \u03b2 parameter in Direct Preference Optimization (DPO) controls the strength of the Kullback-Leibler penalty, typically set within the range of 0.1 to 0.5. A higher \u03b2 value indicates a preference for the policy model's training process to remain closer to the initially referenced model. In extreme cases, as \u03b2 \u2192 0, we ignore the constraints imposed by the reference model. This setting aims to balance the model's ability to adapt to new training signals while maintaining the stability of the learned behaviors from the reference model."}]}