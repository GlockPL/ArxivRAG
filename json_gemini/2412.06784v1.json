{"title": "P3-PO: Prescriptive Point Priors for Visuo-Spatial Generalization of Robot Policies", "authors": ["Mara Levy", "Siddhant Haldar", "Lerrel Pinto", "Abhinav Shirivastava"], "abstract": "Developing generalizable robot policies that can robustly handle varied environmental conditions and object instances remains a fundamental challenge in robot learning. While considerable efforts have focused on collecting large robot datasets and developing policy architectures to learn from such data, na\u00efvely learning from visual inputs often results in brittle policies that fail to transfer beyond the training data. This work presents Prescriptive Point Priors for Policies or P3-PO, a novel framework that constructs a unique state representation of the environment leveraging recent advances in computer vision and robot learning to achieve improved out-of-distribution generalization for robot manipulation. This representation is obtained through two steps. First, a human annotator prescribes a set of semantically meaningful points on a single demonstration frame. These points are then propagated through the dataset using off-the-shelf vision models. The derived points serve as an input to state-of-the-art policy architectures for policy learning. Our experiments across four real-world tasks demonstrate an overall 43% absolute improvement over prior methods when evaluated in identical settings as training. Further, P3-PO exhibits 58% and 80% gains across tasks for new object instances and more cluttered environments respectively. Videos illustrating the robot's performance are best viewed at point-priors.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "A long standing goal in robotics has been to develop robot policies that are robust to environmental changes and can operate across variations in spatial configurations and object instances. While significant advances have been made in this direction for computer vision [1, 2] and natural language processing [3, 4, 5], the majority of robot policies remain confined to controlled laboratory environments with carefully designed settings. Robotic policies struggle to generalize to real-world scenarios because of the challenges and high costs associated with gathering diverse, high-quality robotic data.\nRecent efforts aim to address this data problem by ei-ther aggregating existing robot datasets under a common framework [6] or collecting extensive real-world datasets through easy-to-use teleoperation tools [7, 8, 9, 10]. However, aggregated datasets suffer from inconsistencies across actions recorded for different projects [6], while large teleoperated collections, though useful, are often specific to a single robot type [8, 7] and it is unclear whether these approaches would scale for different robot morphologies. Consequently, developing generalized robot models still largely depends on collecting more expert demonstrations.\nOne way to get around this data problem is to use strong representation priors that transfer across scenarios and feed these representations as input into existing policy architectures. While priors such as object proposals [11, 12, 13] and pose estimation [14, 15] have been used in prior work, they often lose information which makes policy learning harder or require accurate modeling of the object poses to make the policy work. In this work, we explore if there exists a representation that is flexible, serves as a strong prior, and can provide the object-centric abstraction of a scene to enhance generalization. Compared to segmentation and object models, a point-based representation retains fine-grained spatial information without requiring accurate modeling of object boundaries or poses. By representing objects and scenes as a set of unstructured points, these representations extract only the essential geometric relationships between relevant elements in the scene. This allows the policy to focus exclusively on the key spatial interactions.\nWe present Prescriptive Point Priors for Policies or P3-PO, a novel framework that leverages the generalization capa-bilities of state-of-the-art computer vision models alongside state-of-the-art robot policy architectures. Through P3-PO, we demonstrate improved spatial generalization, the ability to"}, {"title": "II. RELATED WORKS", "content": "Imitation Learning (IL) [21] refers to training policies with expert demonstrations, without requiring a predefined reward function. In the context of reinforcement learning (RL), this is often referred to as inverse RL [22, 23], where the reward function is derived from the demonstrations and used to train a policy [24, 25, 26, 27, 28]. While these methods reduce the need for extensive human demonstrations, they still suffer from significant sample inefficiency. As a result of this inefficiency in deploying RL policies in the real world, behavior cloning (BC) [29, 30, 31, 32] has become increasingly popular in robotics. Recent advances in BC have demonstrated success in learning policies for both long-horizon tasks [33, 34, 35] and multi-task scenarios [18, 36, 6, 37]. However, most of these approaches rely on image-based representations [38, 18, 39, 36, 6, 40], which limits their ability to generalize to new objects and function effectively outside of controlled lab environments. In this work, we propose P3-PO, which attempts to address this reliance on image representations by directly using points priors as an input to the policy instead of raw images. Through extensive experiments, we observe that such an abstraction helps learn robust policies that generalize across varying scenarios.\nObject-centric representation learning aims to create struc-tured representations for individual components within a scene, rather than treating the scene as a whole. Common techniques in this area include segmenting scenes into"}, {"title": "III. BACKGROUND", "content": "Behavior cloning [48, 49] aims to learn a behavior policy $\\pi_{\\theta}$ given access to either the expert policy $\\pi_e$ or trajectories derived from the expert policy $T_e$. This work operates in the setting where the agent only has access to observation-based trajectories, i.e. $T_e = \\{(o_t, a_t)\\}_{t=0}^{N=0}$. Here N and T denote the number of demonstrations and episode timesteps respectively. We choose this specific setting since obtaining observations and actions from expert or near-expert demonstrators is feasible in real-world settings [7, 50] and falls in line with recent work in this area [18, 51, 52, 7, 39].\nFinding corresponding points across multiple images of the same scene is a well-established problem in computer vi-sion [53, 54]. Correspondence is essential for solving a range of larger challenges, including 3D reconstruction [55, 56], motion tracking [17, 57, 58, 59], image registration [54], and object recognition [60]. In contrast, semantic correspondence focuses on matching points between a source image and an image of a different scene (e.g., identifying the left eye of a cat in relation to the left eye of a dog). Traditional correspondence methods [54, 53] often struggle with semantic correspondence due to the substantial differences in features between the images. Recent advancements in semantic correspondence utilize deep learning and dense correspondence techniques to enhance robustness [61, 62, 63] across variations in background, lighting, and camera perspectives. In this work, we adopt a diffusion-based point correspondence model, DIFT [16], to establish correspondences between a reference and an observed image, which is illustrated in Figure 3.\nPoint tracking across videos is a problem in computer vision, where a set of reference points are given in the first frame of the video, and the task is to track these points"}, {"title": "IV. PRESCRIPTIVE POINT PRIORS FOR POLICIES (P3-PO)", "content": "Given demonstrations for robot manipulation tasks that cover a small set of possible object configurations and types, we seek to learn a generalizable robot policy that is robust to significant environmental variations and applicable to diverse object locations and types. To achieve this, we introduce P3-PO, an algorithm that decouples perception and planning to promote generalization. P3-PO operates in two phases. First, given a small set of robot demonstrations, the user annotates a single task frame with a set of semantically meaningful points. These reference points are propagated to the rest of the dataset using a combination of semantic correspondence and point tracking. The points obtained are fed into a transformer-based policy model for action prediction. An overview of our method has been provided in Figure 2. Below, we describe each component in detail.\nOur method begins by collecting robot demonstrations for a task through robot teleoperation [50]. The user then randomly selects one demonstration and annotates semanti-cally meaningful points on the first frame that are relevant to performing the task, such as points on the robot and the"}, {"title": "V. EXPERIMENTS", "content": "Our experiments are designed to answer the following questions: (1) How well does P3-PO work for policy learning? (2) How well does P3-PO work for novel object instances? (3) Can P3-PO handle background distractors? (4) How does P3-PO perform with estimated depth? (5) Can P3-PO be improved with stronger priors?\nOur experiments are performed on a Ufactory xArm 7 robot with an xArm Gripper in a kitchen environment. The policies are trained with RGB-D images from a third-person camera view and robot proprioception as input. The action space is comprised of the robot end effector pose and the gripper state. We collect a total of 160 demonstrations across 4 real-world tasks with varied object positions and types. The demonstrations are collected using a VR-based teleoperation system [50] at a 30Hz frequency, which are then subsampled to 5Hz. The learned policies are deployed at 5Hz.\nWe experiment with four manipulation tasks that exhibit significant variability in object position, type, and background context. Figure 6 provides rollouts of the tasks performed in our real-world setup. For each task, we collect expert demon-strations across a variety of object sizes and appearances. We refer to objects and environments seen in our collected data as in-domain. During evaluations, we add novel object instances that are unseen in the training data. The variations in positions and object instances for each task are depicted in Figure 4 and Figure 5 respectively. We provide a brief description of each task below.\na) Pick mug: The robot arm picks up a mug placed on the kitchen counter. The position of the mug is varied for each evaluation. We collect 15 demonstrations for 4 different mugs, resulting in a total of 60 demonstrations for the task. During evaluation, we introduce 3 novel mugs."}, {"title": "VI. CONCLUSION AND LIMITATIONS", "content": "In this work, we presented Prescriptive Point Priors for Policies (P3-PO), a simple yet effective framework that leverages human-provided semantic key points to enable more robust policy learning. P3-PO demonstrates improved generalization to spatial variations, novel objects, and distract-ing backgrounds compared to prior state-of-the-art methods. We recognize a few limitations in this work: (a) P3-PO's reliance on existing vision models makes it susceptible to their failures. For instance, point tracking failures under occlusion hurt policy performance. However, we believe that continued advances in computer vision will serve to further strengthen performance of P3-PO. (b) While point abstractions facilitate better generalization, they lose information about scene context that could be important for navigation amid obstacles or clutter. Future work developing algorithms to retain sparse contextual cues while maintaining P3-PO's object-centric representation may help address this. (c) In this work, we primarily study the single task performance of point prior policies. Extending the framework to multitask learning would be an interesting research direction. Overall, we believe P3-PO takes an important step toward developing general, data-"}]}