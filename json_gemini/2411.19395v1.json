{"title": "CONCEPT-DRIVEN OFF POLICY EVALUATION", "authors": ["Ritam Majumdar", "Jack Teversham", "Sonali Parbhoo"], "abstract": "Evaluating off-policy decisions using batch data poses significant challenges due\nto limited sample sizes leading to high variance. To improve Off-Policy Evaluation\n(OPE), we must identify and address the sources of this variance. Recent research\non Concept Bottleneck Models (CBMs) shows that using human-explainable con-\ncepts can improve predictions and provide better understanding. We propose\nincorporating concepts into OPE to reduce variance. Our work introduces a family\nof concept-based OPE estimators, proving that they remain unbiased and reduce\nvariance when concepts are known and predefined. Since real-world applications\noften lack predefined concepts, we further develop an end-to-end algorithm to\nlearn interpretable, concise, and diverse parameterized concepts optimized for\nvariance reduction. Our experiments with synthetic and real-world datasets show\nthat both known and learned concept-based estimators significantly improve OPE\nperformance. Crucially, we show that, unlike other OPE methods, concept-based\nestimators are easily interpretable and allow for targeted interventions on specific\nconcepts, further enhancing the quality of these estimators.", "sections": [{"title": "1 INTRODUCTION", "content": "In domains like healthcare, education, and public policy, where interacting with the environment can\nbe risky, prohibitively expensive, or unethical (Sutton & Barto, 2018; Murphy et al., 2001; Mandel\net al., 2014), estimating the value of a policy from batch data before deployment is essential for the\npractical application of RL. OPE aims to estimate the effectiveness of a specific policy, known as the\nevaluation or target policy, using offline data collected beforehand from a different policy, known\nas the behavior policy (e.g., Komorowski et al. (2018a); Precup et al. (2000); Thomas & Brunskill\n(2016); Jiang & Li (2016)).\nImportance sampling (IS) methods are a popular class of methods for OPE which adjust for distri-\nbutional mismatches between behavior and target policies by reweighting historical data, yielding\ngenerally unbiased and consistent estimates (Precup et al., 2000). Despite their desirable properties\n(Thomas & Brunskill, 2016; Jiang & Li, 2016; Farajtabar et al., 2018), IS methods often face high\nvariance, especially with limited overlap between behavioral samples and evaluation targets or in\ndata-scarce conditions. Evaluation policies may outperform behavior policies for specific individuals\nor subgroups (Keramati et al., 2021b), making it misleading to rely solely on aggregate policy value\nestimates. In practice however, these groups are often unknown, prompting the need for methods to\nlearn interpretable characterizations of the circumstances where the evaluation policy benefits certain\nindividuals over others.\nIn this paper, we propose performing OPE using interpretable concepts (Koh et al., 2020; Madeira\net al., 2023) instead of relying solely on state and action information. We demonstrate that this\napproach offers significant practical benefits for evaluation. These concepts can capture critical\naspects in historical data, such as key transitions in a patient's treatment or features affecting short-\nterm outcomes that serve as proxies for long-term results. By learning interpretable concepts from\ndata, we introduce a new family of concept-based IS estimators that provide more accurate value\nestimates and stronger statistical guarantees. Additionally, these estimators allow us to identify which\nconcepts contribute most to variance in evaluation. When the evaluation is unreliable, we can modify,\nintervene on, or remove these high-variance concepts to assess how the resulting evaluation improves\n(Marcinkevi\u010ds et al., 2024; Madeira et al., 2023)."}, {"title": "2 RELATED WORK", "content": "Off-Policy Evaluation. There is a long history of methods for performing OPE, broadly categorized\ninto model-based or model-free (Sutton & Barto, 2018). Model-based methods, such as the Direct\nMethod (DM), learn a model of the environment to simulate trajectories and estimate the policy value\n(Paduraru, 2013; Chow et al., 2015; Hanna et al., 2017; Fonteneau et al., 2013; Liu et al., 2018b).\nThese methods often rely on strong assumptions about the parametric model for statistical guarantees.\nModel-free methods, like IS, correct sampling bias in off-policy data through reweighting to obtain\nunbiased estimates (e.g., Precup et al. (2000); Horvitz & Thompson (1952); Thomas & Brunskill\n(2016)). Doubly robust (DR) estimators (e.g., Jiang & Li (2016); Farajtabar et al. (2018)) combine\nmodel-based DM and model-free IS for OPE but may fail to reduce variance when both DM and IS\nhave high variance. Various methods have been developed to refine estimation accuracy in IS, such\nas truncating importance weights and estimating weights from steady-state visitation distributions\n(Liu et al., 2018a; Xie et al., 2019; Doroudi et al., 2017; Bossens & Thomas, 2024).\nOff-Policy Evaluation based on Subgroups. Keramati et al. (2021b) extend OPE to estimate\ntreatment effects for subgroups and provide actionable insights on which subgroups may benefit\nfrom specific treatments, assuming subgroups are known or identified using regression trees. Unlike\nregression trees, which are limited in scalability, our approach employs CBMs to learn interpretable\nconcepts that directly characterize individuals, enabling a new family of IS estimators based on these\nconcepts. Similarly, Shen et al. (2021) propose reducing variance by omitting likelihood ratios for\ncertain states. Our work complements this by summarizing relevant trajectory information using\nconcepts, rather than omitting states irrelevant to the return. The advantage of using concepts as\nopposed to states is that we can easily interpret and intervene on these concepts unlike the state\ninformation.\nMarginalized Importance Sampling (MIS) estimators (Uehara et al., 2020; Liu et al., 2018a; Nachum\net al., 2019; Zhang et al., 2020b;a) mitigate the high variance of traditional IS by reweighting data\ntuples using density ratios computed from state visitation at each time step. These estimators enhance\nrobustness by focusing on states with high visitation density ratios, thereby marginalizing out less\nvisited states. However, MIS has its challenges: computing density ratios can introduce high variance,\nparticularly in complex state spaces, and it obscures which aspects of the state space contribute\ndirectly to variance. Some studies, such as Katdare et al. (2023) and Fujimoto et al. (2023), improve"}, {"title": "3 PRELIMINARIES", "content": "Concept Bottleneck Models Conventional CBMs learn a mapping from some input features\n$x \\in \\mathbb{R}^d$ to targets y via some interpretable concepts $c \\in \\mathbb{R}^k$ based on training data of the form\n$\\left\\{x_{n}, c_{n}, y_{n}\\right\\}_{n=1}^{N}$. This mapping is a composition of a mapping from inputs to concepts, $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{k}$,\nand a mapping from concepts to targets, $g: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}$. These may be trained via independent,\nsequential or joint training (Marcinkevi\u010ds et al., 2024). Variations which consider learning concepts\nin a greedy fashion or in a semisupervised way include Wu et al. (2022); Havasi et al. (2022).\nMarkov Decision Processes (MDP). An MDP is defined by a tuple $M=\\left(\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma, T\\right)$.\n$\\mathcal{S}$ and $\\mathcal{A}$ are the state and action spaces, $\\mathcal{P}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ and $\\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathbb{R})$ are\nthe transition and reward functions, $\\gamma \\in[0,1]$ is the discount factor, $T \\in \\mathbb{Z}_{+}$ is the fixed time\nhorizon. A policy $\\pi: \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})$ is a mapping from each state to a probability distribution over\nactions in $\\mathcal{A}$. A $T$-step trajectory following policy $\\pi$ is denoted by $\\tau=\\left[\\left(s_{t}, a_{t}, r_{t}, s_{t+1}\\right)\\right]_{t=1}^{T}$ where\n$s_{1} \\sim d_{0}, a_{t} \\sim \\pi(s_{t}), r_{t} \\sim \\mathcal{r}(s_{t}, a_{t}), s_{t+1} \\sim \\mathcal{p}(s_{t}, a_{t})$. The value function of policy $\\pi$, denoted by\n$V_{\\pi}: \\mathcal{S} \\rightarrow \\mathbb{R}$, maps each state to the expected discounted sum of rewards starting from that state\nfollowing policy $\\pi$. That is, $V_{\\pi}(s)=\\mathbb{E}_{\\pi}[\\sum_{t=1}^{T} \\gamma^{t-1} r_{t} | s_{1}=s]$.\nOff-Policy Evaluation. In OPE, we have a dataset of $T$-step trajectories $\\mathcal{D}=\\left{\\tau^{(n)}\\right}_{n=1}^{N}$ inde-\npendently generated by a behaviour policy $\\pi_{b}$. Our goal is to estimate the value function of another\nevaluation policy, $\\pi_{e}$. We aim to use $\\mathcal{D}$ to produce an estimator, $\\hat{V}_{\\pi_{e}}$, that has low mean squared\nerror, $\\operatorname{MSE}\\left(\\hat{V}_{\\pi_{e}}, V_{\\pi_{e}}\\right)=\\mathbb{E}_{\\mathcal{D} \\sim P_{\\tau},\\left[\\left(\\hat{V}_{\\pi_{e}}-V_{\\pi_{e}}\\right)^{2}\\right]$. Here, $P_{\\tau}$, denotes the distribution of trajectories\n$\\tau$, under $\\pi_{b}$, from which $\\mathcal{D}$ is sampled."}, {"title": "4 CONCEPT-BASED OFF-POLICY EVALUATION", "content": "In this section, we formally define the mathematical definition of the concept, outline their desiderata,\nand present the corresponding OPE estimators. In the following sections, we divide our Concept-OPE\nstudies into two parts. Section 5 covers scenarios where concepts are known from domain knowledge,\nwhile Section 6 addresses cases where concepts are unknown and must be learned by optimizing a\nparameterized representation."}, {"title": "4.1 FORMAL DEFINITION OF THE CONCEPT", "content": "Given a dataset $\\mathcal{D}=\\left{\\tau^{(n)}\\right}_{n=1}^{N}$ of $n$ $T$-step trajectories, let $\\phi: \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R} \\times \\mathcal{S} \\rightarrow \\mathcal{C} \\in \\mathbb{R}^{d}$\ndenote a function that maps trajectory histories $h_t$ to interpretable concepts in $d$-dimensional concept"}, {"title": "4.2 CONCEPT-BASED ESTIMATORS FOR OPE.", "content": "We introduce a new class of concept-based OPE estimators to formalize the application of concepts in\nOPE. These estimators are adapted versions of their original non-concept-based counterparts. Here,\nwe present the results specifically for per-decision IS and standard IS estimators, as these serve as the\nfoundation for several other estimators. We also demonstrate in Appendix C how these methods can\nbe extended to other estimators."}, {"title": "5 CONCEPT-BASED OPE UNDER KNOWN CONCEPTS", "content": "We first consider the scenario where the concepts are known apriori using domain knowledge and\nhuman expertise. These concepts automatically satisfy the desiderata defined in Appendix A."}, {"title": "5.1 THEORETICAL ANALYSIS OF KNOWN CONCEPTS", "content": "In this subsection, we discuss the theoretical guarantees of OPE under known concepts. We make\nthe completeness assumption where every action of a particular state has a non-zero probability of\nappearing in the batch data. When this assumption is satisfied, we obtain unbiasedness and lower\nvariance when compared with traditional estimators. Proofs follow in Appendix D."}, {"title": "5.2 EXPERIMENTAL SETUP AND METRICS", "content": "Environments: We consider a synthetic domain: WindyGridworld and the real world MIMIC-III\ndataset for acutely hypotensive ICU patients as our experiment domains for the rest of the paper.\nWindyGridworld: We (as human experts) define the concept $c_{t}=\\phi(\\$distance to target, wind) as a\nfunction of the distance to the target and the wind acting on the agent at a given state. This concept\ncan take 25 unique values, ranging from 0 to 24. For example: $c_{t}=0$ when distance to target $\\in$\n[15, 19] $\\times$ [15, 19] and wind = [0, 0]. The first and second co-ordinates represent the horizontal and\nvertical features respectively. Detailed description of known concepts in Appendix G.\nMIMIC: The concept $c_{t} \\in \\mathbb{Z}^{15}$ represents a function of 15 different vital signs (interpretable features)\nof a patient at a given timestep. The vital signs considered are: Creatinine, FiO2, Lactate, Partial\nPressure of Oxygen (PaO2), Partial Pressure of CO2, Urine Output, GCS score, and electrolytes such\nas Calcium, Chloride, Glucose, HCO3, Magnesium, Potassium, Sodium, and SpO2. Each vital sign\nis binned into 10 discrete levels, ranging from 0 (very low) to 9 (very high).\nPolicy descriptions: In the case of WindyGridworld, we run a PPO Schulman et al. (2017) algorithm\nfor 10k epochs and consider the evaluation policy $\\pi_{e}$ as the policy at epoch 10k, while the behavior\npolicy $\\pi_{b}$ is taken as the policy at epoch 5k. For the MIMIC case, we generate the behavior\npolicy $\\pi_{b}$ by running an Approximate Nearest Neighbors algorithm with 200 neighbors, using\nManhattan distance as the distance metric. The evaluation policy $\\pi_{e}$ involves a more aggressive use\nof vasopressors (10% more) compared to the behavior policy. See Appendix F for further details.\nMetrics: In the case of the synthetic domain, we measure bias, variance, mean squared error, and\nthe effective sample size (ESS) to assess the quality of our concept-based OPE estimates. The ESS\nis defined as $N \\times \\frac{\\mathbb{V}_{\\pi_{e}}\\left[V_{\\pi_{e}}^{\\text {on-policy }}\\right]}{\\mathbb{V}_{\\pi_{b}}\\left[V_{\\pi_{e}}\\right]}$ where $N$ is the number of trajectories in the off-policy data, and\n$\\mathbb{V}_{\\pi_{e}}^{\\text {on-policy }}$ and $V_{\\pi_{e}}$ are the on-policy and OPE estimates of the value function, respectively. For\nMIMIC, where the true on-policy estimate is unknown due to the unknown transition dynamics\nand environment model, we only consider variance as the metric. Additionally, we compare the\nInverse Propensity scores (IPS) under concepts and states to better underscore the reasons for variance\nreduction: Figures 3,10."}, {"title": "5.3 RESULTS AND DISCUSSION", "content": "Known concept-based estimators demonstrate reduced variance, improved ESS, and lower\nMSE compared to traditional estimators, although they come with slightly higher bias. Figure 2\ncompares known-concept and traditional OPE estimators. We observe a consistent reduction in\nvariance and an increase in ESS across all sample sizes for the concept-based estimators. Although\nour theoretical analysis suggests that known-concept estimators are unbiased, practical results indicate\nsome bias. While unbiased estimates are generally preferred, they can lead to higher errors when\nthe behavior policy does not cover all states. This issue is especially pronounced in limited data\nsettings, which are common in medical applications. Despite this bias-variance trade-off, the MSE\nfor concept-based OPE estimators shows a 1-2 order of magnitude improvement over traditional\nestimators due to significant variance reduction. In the real-world MIMIC example, concept-based\nestimators exhibit a variance reduction of one order of magnitude compared to traditional OPE\nestimators. This demonstrates that categorizing diverse states such as varying gridworld positions or\npatient vital signs into shared concepts based on common attributes improves OPE characterization.\nThe Inverse Propensity Scores (IPS) are more left-skewed under concepts as compared to\nstates. Figure 3 compares the IPS scores under concept and state estimators. We observe, the\nfrequency of lower IPS scores is higher under concepts as opposed to states. This indicates the\nsource of variance reduction in Concept-based OPE lies in the lowering of the IPS scores, which\nis also backed theoretically in Theorem 5.4 when the rewards $r_{t}$ are fixed to 1. Similar result for\n$N=\\{100,300,500,1500,2000\\}$ can be found in the Appendix I."}, {"title": "6 CONCEPT-BASED OPE UNDER UNKNOWN CONCEPTS", "content": "While domain knowledge and predefined concepts can enhance OPE, in real-world situations concepts\nare typically unknown. In this section, we address cases where concepts are unknown and must be\nestimated. We use a parametric representation of concepts via CBMs, which initially may not meet\nthe required desiderata. This section introduces a methodology to optimize parameterized concepts\nto meet these desiderata, alongside improving OPE metrics like variance."}, {"title": "6.1 METHODOLOGY", "content": "Algorithm 1 outlines the training methodology. We split the batch trajectories $\\mathcal{D}$ into training\ntrajectories $\\mathcal{T}_{train}$ and evaluation trajectories $\\mathcal{T}_{OPE}$, with the evaluation policy $\\pi_{e}$, the behavior policy\n$\\pi_{b}$, and an OPE estimator (eg: CIS/CPDIS) known beforehand. We aim to learn our concepts using"}, {"title": "6.2 THEORETICAL ANALYSIS OF UNKNOWN CONCEPTS", "content": "The theoretical implications mainly differ in the bias, consequently MSE and their Confidence bounds\non moving from known to unknown concepts, as analyzed below. Proofs are listed in Appendix E."}, {"title": "6.3 EXPERIMENTAL SETUP", "content": "Environments, Policy descriptions, Metrics: Same as those in known concepts section.\nConcept representation: In both examples, we use a 4-dimensional concept $c_{t} \\in \\mathbb{R}^{4}$, where each\nsub-concept is a linear weighted function of human-interpretable features $f$, i.e., $c_{i}=w \\cdot f\\left(s_{t}\\right)$, with\n$w$ optimized as previously discussed. Detailed descriptions of the features and optimized concepts\nafter CBM training are provided in Appendix I. For MIMIC, features $f$ are normalized vital signs, as\nthreshold information for discretization is unavailable. In brevity of space, we move the training and\nhyperparameter details to Appendix G."}, {"title": "6.4 RESULTS AND DISCUSSION", "content": "Optimized concepts using Algorithm 1 yield improvements across all metrics except bias\ncompared to traditional OPE estimators. Significant improvements in variance, MSE, and ESS\nare observed for the WindyGridworld and MIMIC datasets, with gains of 1-2 and 2-3 orders of\nmagnitude, respectively. This improvement is due to our algorithm's ability to identify concepts\nthat satisfy the desiderata, including achieving variance reduction as specified in line 12 of the\nalgorithm. However, like known concepts, optimized concepts show a higher bias than traditional\nestimators. This is because, unlike variance, bias cannot be optimized in the loss function without the\ntrue on-policy estimate, which is typically unavailable in real-world settings. As a result, external\ninformation may be essential for further bias reduction.\nOptimized concepts yield improvements across all metrics besides bias over\nknown concept estimators. Our methodology achieves 1-2 orders of magnitude improve-\nment in variance, MSE, and ESS compared to known concepts. This suggests that our algorithm\ncan learn concepts that surpass human-defined ones in improving OPE metrics. This is particularly\nvaluable in cases with imperfect experts or highly complex real-world scenarios where perfect\nexpertise is unfeasible. However, these optimized concepts introduce higher bias, primarily because\nthe training algorithm prioritized variance reduction over bias minimization. This bias could be\nreduced by incorporating variance regularization into the training process.\nOptimized concepts are interpretable, show conciseness and diversity. We list the optimized\nconcepts in Appendix I. These concepts exhibit sparse weights, enhancing their conciseness, with\nsignificant variation in weights across different dimensions of the concepts, reflecting diversity.\nThis work focuses on linearly varying concepts, but more complex concepts, such as symbolic\nrepresentations (Majumdar et al., 2023), could better model intricate environments."}, {"title": "7 INTERVENTIONS ON CONCEPTS FOR INSIGHTS ON EVALUATION", "content": "Concepts provide interpretations, allowing practitioners to identify sources of variance an advantage\nover traditional state abstractions like Pavse & Hanna (2022a). Concepts also clarify reasons behind\nOPE characteristics, such as high variance, enabling corrective interventions based on domain\nknowledge or human evaluation. We outline the details of performing interventions next."}, {"title": "7.1 METHODOLOGY", "content": "Given trajectory history $h_t$ and concept $c_t$, we define $c_{int}$ as the intervention (alternative) concept\nan expert proposes at time $t$. We define criteria $\\kappa: (h_{t}, c_{t}) \\rightarrow \\{0,1\\}$ as a function constructed from\ndomain expertise that takes in $(h_{t}, c_{t})$ as input and outputs a boolean value. This criteria function\ndetermines whether an intervention needs to be conducted over the current concept $c_{t}$ or not. For e.g.,\nif a practitioner has access to true on-policy values, he/she can estimate which concepts suffer from\nbias. If a concept doesn't suffer from bias, the criteria $\\kappa(h_{t}, c_{t}) = 1$ is satisfied and the concept is not\nintervened upon, else $\\kappa(h_{t}, c_{t}) = 0$ and the intervened concept $c_{int}$ is used instead. The final concept\n$\\check{c}_{t}$ is then defined as: $\\check{c}_{t} = \\kappa(h_{t}, c_{t}) \\cdot c_{t} + (1 - \\kappa(h_{t}, c_{t})) c_{int}$. Under the absence of true on-policy\nvalues, the practitioner may chose to intervene using a different criteria instead.\nWe define criteria $\\kappa$ for our experiments as follows. In Windygridworld, we assume access to oracle\nconcepts, listed in Appendix G. When the learned concept $c_t$ matches the true concept, $\\kappa(h_{t}, c_{t}) = 1$,\notherwise 0. In MIMIC, the interventions are based on a patient's urine output at a specific timestep\nwith $\\kappa(h_{t}, c_{t}) = 1$ when urine output $> 30$ ml/hr, and 0 otherwise. Performing interventions based\non urine output enables us to assess the role of kidney function in hypotension management. In this\nwork, we consider 3 possible intervention strategies either based on state representations or based on\ndomain knowledge."}, {"title": "7.2 RESULTS AND INTERPRETATIONS FROM INTERVENTIONS ON LEARNED CONCEPTS", "content": "We interpret the optimized concepts in Fig. 5. In the WindyGridworld environment, we compare\nthe ground-truth concepts with the optimized ones and observe two additional concepts predicted\nin the bottom-right region. This likely stems from overfitting to reduce variance in the OPE loss,\nsuggesting a need for inspection and possible intervention. Additionally, we compare our clusters\nwith state-abstraction baseline (clustering in the state-space), and observe the clusters to be widely\ndifferent from the learnt concepts. For MIMIC, prior studies indicate that patients with urine output\nabove 30 ml/hr are less susceptible to hypotension than those with lower output Kellum & Prowle\n(2018); Singer et al. (2016); Vincent & De Backer (2013). Using this knowledge, we analyze patient\ntrajectories and find that lower urine output correlates with higher variance, while higher output\ncorresponds to lower variance. This insight helps identify patients who may benefit from targeted\ninterventions.\nInterpretable concepts allow for targeted interventions that further enhance OPE estimates.\nIn the WindyGridworld environment, we observe a reduction in bias. This occurs because replacing\nerroneous concepts with oracle concepts introduces information about the on-policy estimates that"}, {"title": "8 CONCLUSIONS, LIMITATIONS AND FUTURE WORK", "content": "We introduced a new family of concept-based OPE estimators, demonstrating that known-concept\nestimators can outperform traditional ones with greater accuracy and theoretical guarantees. For\nunknown concepts, we proposed an algorithm to learn interpretable concepts that improve OPE\nevaluations by identifying performance issues and enabling targeted interventions to reduce variance.\nThese advancements benefit safety-critical fields like healthcare, education, and public policy by\nsupporting reliable, interpretable policy evaluations. By reducing variance and providing policy\ninsights, this approach enhances informed decision-making, facilitates personalized interventions,\nand refines policies before deployment for greater real-world effectiveness. A limitation of our work\nis trajectory distribution mismatch when learning unknown concepts, particularly in low-sample\nsettings, which can lead to high-variance OPE. Targeted interventions help mitigate this issue. We\nalso did not address hidden confounding variables or potential CBM concept leakage, focusing\ninstead on evaluation. Future work will address these challenges and extend our approach to more\ngeneral, partially observable environments."}, {"title": "A CONCEPT DESIDERATA", "content": "Explainability: Explainability ensures that the concept function $\\phi$ is composed of human-\ninterpretable functions $f_{1}, f_{2},..., f_{n}$. Each interpretable function $f_{i}$ depends on the current state,\npast actions, rewards, and states, i.e., $s_{t}, a_{0: t-1}, r_{0: t-1}, s_{0: t-1}$. Mathematically:\n$\\begin{aligned}c_{t} &=\\phi\\left(s_{t}, a_{0: t-1}, r_{0: t-1}, s_{0: t-1}\\right) \\\\&=\\psi\\left(f_{1}\\left(s_{t}, a_{0: t-1}, r_{0: t-1}, s_{0: t-1}\\right), ..., f_{n}\\left(s_{t}, a_{0: t-1}, r_{0: t-1}, s_{0: t-1}\\right)\\right)\\end{aligned}$\nHere, $\\psi$ maps the human-interpretable functions $f_{i}$ to the concept $c_{t}$, and both $\\phi$ and $\\psi$ share the\nsame co-domain space $\\mathcal{C}$. In essence, $\\phi$ can be defined using a single interpretable function or a\ncombination of multiple interpretable functions.\nAs a running example in this paper (applicable across domains), the concept function $\\phi(s_{t})$ for\ndiagnosing hypertension can be expressed using human-interpretable features:\n$\\begin{aligned}c_{t} &=\\phi\\left(s_{t}\\right) \\\\&=\\phi(\\text { SBP, DBP, HR, Glucose levels, GCS, Age, Weight }) \\\\&=\\psi\\left(f_{1}(\\text { SBP }), f_{2}(\\text { DBP }), f_{3}(\\text { HR }), f_{4}(\\text { Glucose levels }), f_{5}(\\text { GCS }), f_{6}(\\text { Age, Weight })\\right)\\end{aligned}$\nWhere:\n$\\bullet$ $f_{1}(\\text { SBP })$ maps Systolic Blood Pressure to a category (e.g., Low, Normal, High).\n$\\bullet$ $f_{2}(\\text { DBP })$ maps Diastolic Blood Pressure to a category (e.g., Low, Normal, High).\n$\\bullet$ $f_{3}(\\text { HR })$ maps Heart Rate to a category (e.g., Low, Normal, High).\n$\\bullet$ $f_{4}(\\text { Glucose levels })$ maps blood glucose levels to a category (e.g., Low, Normal, High).\n$\\bullet$ $f_{5}(\\text { GCS })$ maps GCS scores to a category.\n$\\bullet$ $f_{6}(\\text { Age, Weight })$ maps age and weight to Body Mass Index (BMI).\nThis ensures that the concept $\\phi(s_{t})$ for diagnosing hypertension is built from human-interpretable\nfeatures, making the diagnostic process explainable. Each function $f_{i}$ translates raw medical data\ninto intuitive categories that are meaningful to medical practitioners.\nConciseness: Conciseness ensures that the concept function $\\phi$ represents the minimal mapping of\ninterpretable functions $f_{1}, f_{2},..., f_{n}$ to the concept $c_{t}$. If multiple mappings $\\psi_{1}, \\psi_{2}, ..., \\psi_{m}$ satisfy\n$\\psi$, we choose the mapping $\\psi$ that provides the simplest composition of $f_{i}$ to describe $c_{t}$.\nE.g. Obesity can be represented by different combinations of human-interpretable functions. We\nselect the least complex representation that remains interpretable. The two possible representations\nare:\n$\\begin{aligned}c_{t} &=\\psi_{1}\\left(f_{1}(\\text {height}), f_{2}(\\text {weight}), f_{3}(\\text {SBP }), f_{4}(\\text {DBP })\\right) \\\\c_{t} &=\\psi_{2}\\left(f_{5}(\\text {BMI}), f_{3}(\\text {SBP })\\right)\\end{aligned}$"}, {"title": "B CHOICE OF CONCEPT TYPES", "content": "Concepts capturing subgroups with short-term benefits. If $\\phi$ maps state $s_{t}$ and action $a_{t}$ to immediate\nreward $r_{t}$, the resulting concepts can identify subgroups with similar short-term benefits, facilitating\nmore personalized OPE, as seen in Keramati et al. (2021a). Unlike Keramati et al. (2021b), we do\nnot limit $\\phi$ to a regression tree.\nConcepts capturing high-variance transitions. If $\\phi$ highlights changes in state $s_{t}$ and action $a_{t}$ that\ncause significant shifts in value estimates, it can capture influential transitions or dynamics from\nhistorical data, similar to Gottesman et al. (2020).\nConcepts capturing least influential states. If $\\phi$ identifies the least (or most) influential states $s_{t}$, it\ncan help focus more on critical states, reducing variance by only applying IS ratios to those states\nBossens & Thomas (2024).\nConcepts capturing state-density information. If $\\phi$ extracts information from histories to predict\nstate-action visitation counts, concept-based OPE with $\\phi$ functions similarly to Marginalized OPE\nestimators, like Xie et al. (2019), which reweight trajectories based on state-visitation distributions.\nHowever, density-based concepts may be less interpretable and harder to intervene in the context of\nOPE."}, {"title": "C GENERALIZED CONCEPT-BASED OPE ESTIMATORS", "content": "Building on the OPE estimators discussed in the main paper, we extend the integration of concepts\ninto other popular OPE estimators. Without making any additional assumptions about the estimators'\ndefinitions, concepts can be seamlessly incorporated into the original formulations of these estimators."}, {"title": "D KNOWN CONCEPT-BASED OPE ESTIMATORS: THEORETICAL PROOFS", "content": "In this section, we provide the detailed proofs for the known concept scenario.\nTheorem. For any arbitary function f, $\\mathbb{E}_{c \\sim d_{c}}f(c)=\\mathbb{E}_{s \\sim d_{s}}f(\\phi(s))$\nProof: See Pavse & Hanna (2022b)."}, {"title": "E UNKNOWN CONCEPT-BASED OPE ESTIMATORS: THEORETICAL PROOFS", "content": "In this section, we provide the theoretical proofs of the unknown concept scenarios."}, {"title": "F ENVIRONMENTS", "content": "WindyGridworld Figure 7 illustrates the Windy Gridworld environment, a 20x20 grid divided into\nregions with varying wind directions and penalties. The agent's goal is to navigate from a randomly\nchosen starting point to a fixed goal in the top-right corner. Off-diagonal winds increase in strength\nnear non-windy regions, affecting the agent's movement. Each of the four available actions moves\nthe agent four steps in the chosen direction. Reaching the goal earns a +5 reward while moving away\nresults in a -0.2 penalty. Additional negative rewards are based on regional penalties within the grid.\nEach episode ends after 200 steps.\nThe grid is split into 25 blocks, each measuring 4x4 units with each region having a penalty based\non the wind-strength. Blocks affected by wind display the direction and strength (e.g., '$\\leftarrow \\uparrow(-2,+2)$'\nindicates northward and westward winds with a strength of 2 units each). This setup encourages the\nagent to navigate through non-penalty areas for optimal rewards.\nMIMIC-III We use the publicly available MIMIC-III database (Johnson et al., 2016) from Phys-\nioNet (Goldberger et al., 2000"}]}