{"title": "A Note on Shumailov et al. (2024): 'AI Models Collapse When Trained on Recursively Generated Data'", "authors": ["Ali Borji"], "abstract": "The study conducted by Shumailov et al. (2024) demonstrates that repeatedly training a generative model on synthetic data leads to model collapse. This finding has generated considerable interest and debate, particularly given that current models have nearly exhausted the available data. In this work, we investigate the effects of fitting a distribution (through Kernel Density Estimation, or KDE) or a model to the data, followed by repeated sampling from it. Our objective is to develop a theoretical understanding of the phenomenon observed by Shumailov et al. (2024). Our results indicate that the outcomes reported are a statistical phenomenon and may be unavoidable.", "sections": [{"title": "Introduction", "content": "The paper by Shumailov et al. 2024 [1] discusses how AI models collapse when trained on recursively generated data (i.e., data created by other models rather than real-world sources). It highlights that as AI-generated data proliferates, models trained on such data experience significant performance degradation (See Figure 1). This is due to feedback loops where models increasingly rely on lower-quality synthetic data, causing errors to compound over time. The research warns that this issue could compromise the reliability of AI systems, especially as synthetic data becomes more prevalent in training datasets.\nIn this work, we carry out a series of experiments to explore the generalization of these findings from a theoretical standpoint, focusing specifically on statistical sampling and distribution fitting."}, {"title": "Experiments and results", "content": "We seek to understand the underlying causes of this phenomenon and have considered whether a theoretical approach exists to study it systematically. The results indicate a collapse in the final distribution, with the magnitude of the effect varying according to the distance metric used between distributions. We present these findings with the hope of encouraging further exploration in this area.\nFirst, samples are drawn from a distribution composed of two normal distributions and one uniform distribution. A distribution is subsequently fitted to this data using KDE with a Gaussian kernel. Figure 2 illustrates the data distribution, the fitted KDE, and the distribution of samples drawn from the KDE. The code in the appendix demonstrates the process.\nNext, we repeat the procedure and compute the KL divergence and Wasserstein distance between the data sampled from the fitted KDE and the original data.\nAfter several iterations, the distribution progressively converges to something resembling a normal distribution, as illustrated in Figure 3. This collapse occurs as the repeated sampling and refitting smooths out the original structure of the data, gradually losing its distinctive features and leading to a more uniform, bell-shaped curve (Figure 3). With each successive iteration, both the KL divergence and Wasserstein distance (WSD) progressively increase. This suggests that the fitted distribution drifts further from the original data, indicating a growing discrepancy between the two as the process continues.\nWe also considered two other mixture of distributions. In the first one, we created a distribution composed of four distributions: three Gaussians and one uniform (See the left column in Figure 5). In the second one, we created distribution composed of a Gamma distribution, two Gaussians and one uniform (See the left column in Figure 5). In both of these cases, the distributions collapse to a uni-modal Gaussian-looking distribution.\nThe results of these simulations reveal that the KL divergence increases during the initial iterations but then stabilizes within a certain range (in some cases, it keeps rising). In contrast, WSD continuously grows throughout the iterations. Thus, the conclusions drawn can vary depending on the choice of distance metric."}, {"title": "Conclusion", "content": "Overall, these findings raise important questions about whether large language models (LLMs) and other generative models are truly capable of effectively learning and capturing the underlying distributions of data. One of the key challenges identified is the difficulty in accurately modeling the tails of these distributions, which often contain critical information but are prone to being poorly represented by generative models.\nMeasuring the distances between distributions is a particularly complex issue, as there are numerous metrics available, each with its own implications and limitations. In this paper, two such metrics are explored in detail.\nThe results presented in this paper seem to reflect a broader and more pervasive phenomenon, underscoring a significant concern in the field of generative AI. Specifically, it calls attention to the potential limitations of these models in faithfully reproducing data distributions, particularly when recursively generated data is involved, which could have profound implications for their long-term reliability and robustness.\nFuture research in this area should investigate whether 1) our results are applicable to all types of distributions, potentially from a more theoretical perspective; 2) repeated sampling and fitting results in a uni-modal Gaussian distribution; and 3) this phenomenon is inevitable, suggesting that there may be no viable remedy."}]}