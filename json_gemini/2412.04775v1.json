{"title": "A TEMPORALLY CORRELATED LATENT EXPLORATION FOR REINFORCEMENT LEARNING", "authors": ["SuMin Oh", "WanSoo Kim", "HyunJin Kim"], "abstract": "Efficient exploration remains one of the longstanding problems of deep reinforce-\nment learning. Instead of depending solely on extrinsic rewards from the environ-\nments, existing methods use intrinsic rewards to enhance exploration. However,\nwe demonstrate that these methods are vulnerable to Noisy TV and stochastic-\nity. To tackle this problem, we propose Temporally Correlated Latent Explo-\nration (TeCLE), which is a novel intrinsic reward formulation that employs an\naction-conditioned latent space and temporal correlation. The action-conditioned\nlatent space estimates the probability distribution of states, thereby avoiding the\nassignment of excessive intrinsic rewards to unpredictable states and effectively\naddressing both problems. Whereas previous works inject temporal correlation\nfor action selection, the proposed method injects it for intrinsic reward compu-\ntation. We find that the injected temporal correlation determines the exploratory\nbehaviors of agents. Various experiments show that the environment where the\nagent performs well depends on the amount of temporal correlation. To the best\nof our knowledge, the proposed TeCLE is the first approach to consider the action-\nconditioned latent space and temporal correlation for curiosity-driven exploration.\nWe prove that the proposed TeCLE can be robust to the Noisy TV and stochasticity\nin benchmark environments, including Minigrid and Stochastic Atari.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement learning (RL) agents learn how to act to maximize the expected return of a policy.\nHowever, in real-world environments where rewards are sparse, agents do not have access to contin-\nuous rewards, which makes learning difficult. Inspired by human beings, numerous studies address\nthis issue through intrinsic motivation, which uses so-called bonus or intrinsic reward to encour-\nage agents to learn environments when extrinsic rewards are rarely provided (Schmidhuber, 1991b;\nOudeyer & Kaplan, 2007a; Schmidhuber, 2010).\nA notable intrinsic motivation is the curiosity-driven exploration method that adopts prediction er-\nror as intrinsic rewards (Oudeyer & Kaplan, 2007b; Pathak et al., 2017). For instance, Pathak et al.\n(2017) uses the difference between predicted states from the forward dynamics model and actual\nstates as intrinsic rewards. Besides, the difference between the output of the fixed randomly initial-\nized target network and the prediction network is adopted as intrinsic rewards (Burda et al., 2018b).\nSince the above methods encourage the exploration of rarely visited states, they can be useful in\nsparse reward environments such as Montezuma's Revenge (Mnih et al., 2015). However, curios-\nity agents can be trapped if the state prediction is inherently impossible or difficult. The problem\nof trapped agents could be caused by noise sources such as the Noisy TV or stochasticity in en-\nvironments (Burda et al., 2018b; Pathak et al., 2019; Mavor-Parker et al., 2022). Therefore, it is\nchallenging for curiosity agents to learn environments where noise sources exist.\nTo overcome the limitation, this paper proposes Temporally Correlated Latent Explo-\nration (TeCLE), a novel curiosity-driven exploration method that employs an action-conditioned\nlatent space and temporal correlation. Firstly, this paper formulates intrinsic reward from the dif-\nference between the reconstructed states and actual states. Secondly, we introduce the conditioned"}, {"title": "2 RELATED WORKS", "content": "latent spaces for exploration. Whereas previous studies (Oh et al., 2015; Kim et al., 2018) use ac-\ntion as a condition for prediction problems, the proposed TeCLE uses action as a condition variable\nto learn a conditioned latent space, which is referred to as action-conditioned latent space. In the\nproposed method, the state is embedded as a state representation, which is then encoded into an\naction-conditioned latent space. This enables the action-conditioned latent space to learn the dis-\ntribution of the state representation, allowing agents to effectively avoid noise sources. Whereas\nprevious works have used conditioned latent spaces to alleviate the out-of-distribution (OOD) prob-\nlem in offline RL (Zhou et al., 2021; Rezaeifar et al., 2022), this paper employs the conditioned\nlatent space for curiosity-driven exploration methods. On the other hand, temporal correlation using\ncolored noise was successfully applied to the action selection for RL agents (Eberhard et al., 2023;\nHollenstein et al., 2024). Different from the above works, our proposed method injects temporal cor-\nrelation into the action-conditioned latent space. As far as we know, this paper is the first approach\nto inject temporal correlation for intrinsic motivation. To prove the effectiveness, we evaluate our\nproposed TeCLE on Minigrid and Stochastic Atari, comparing its performance with baselines. Fur-\nthermore, the generalization ability of TeCLE is demonstrated through experimental results with no\nextrinsic reward setting. For a more qualitative analysis, we discuss the performance that depends\non the amount of temporal correlation (i.e., colored noise) and propose an optimal colored noise\naccording to the properties of the noise source and the environment. The contributions of our study\nare summarized as follows:\n\u2022 Defining Intrinsic Rewards via Action-Conditioned Latent Spaces: Since the action-\nconditioned latent space reconstructs states by learning the distribution of states, it avoids\nbeing trapped in noise sources where the state prediction is inherently impossible. There-\nfore, we formulate intrinsic rewards using action-conditioned latent spaces for exploration.\n\u2022 Introducing Temporal Correlation for Intrinsic Motivation: By injecting colored noise\ninto the action-conditioned latent space, we further introduce temporal correlation into the\ncomputation of intrinsic reward. Furthermore, we find that different colors of noise encour-\nage agents to have different exploratory behaviors.\n\u2022 Benchmarking the Performance: To evaluate the effectiveness of the proposed TeCLE,\nwe conduct extensive experiments on the Minigrid and Stochastic Atari environments.\nCompared to several strong baselines, TeCLE achieves good performance not only on dif-\nficult exploration tasks but also on environments where noise sources exist."}, {"title": "2.1 EXPLORATION WITH INTRINSIC MOTIVATION", "content": "The bonus or intrinsic reward in RL refers to an additional reward often used to encourage explo-\nration of less frequently visited states. In the count-based exploration method, state-action visita-\ntion was directly used to compute intrinsic reward (Strehl & Littman, 2008). To reduce compu-\ntational efforts and generalize intrinsic rewards to a large state-space, numerous works have been\nstudied (Bellemare et al., 2016; Martin et al., 2017; Ostrovski et al., 2017; Tang et al., 2017; Choshen\net al., 2018; Choi et al., 2018; Machado et al., 2020). However, the above count-based methods can\nbe less effective in sparse reward environments and break down when the number of novel states is\nlarger than their approximation (Raileanu & Rockt\u00e4schel, 2020; Mavor-Parker et al., 2022).\nOn the other hand, curiosity-based exploration method proposed to predict the dynamics of the\nenvironment to compute intrinsic reward (Schmidhuber, 1991a;b; Oudeyer & Kaplan, 2007a; Stadie\net al., 2015). Using a self-supervised manner, the curiosity can be quantified as the prediction error\nor uncertainty of a consequence of the actions (Pathak et al., 2017; Burda et al., 2018a; Pathak\net al., 2019; Raileanu & Rockt\u00e4schel, 2020). Moreover, Burda et al. (2018b) introduced a novel\nframework where the prediction problem is randomly generated. Whereas the above curiosity-driven\nexploration methods were effective on several sparse reward environments in Atari (Mnih et al.,\n2015), Noisy TV or stochasticity can misdirect the curiosity of the curiosity agent (Raileanu &\nRockt\u00e4schel, 2020; Mavor-Parker et al., 2022)."}, {"title": "2.2 TEMPORALLY CORRELATED NOISE AS ACTION NOISE", "content": "A common exploration technique in RL is to add noise such as Ornstein-Uhlenbeck (OU) noise (Uh-\nlenbeck & Ornstein, 1930) or Gaussian noise to an action sampled from the policy. Recently, several\nstudies introduced different types of action noise. Eberhard et al. (2023) studied the effects of the\ntemporally correlated noise as action noise for off-policy algorithms in continuous control envi-\nronments. Besides, the amount of the temporal correlation, which depends on the color parameter\n\u03b2, was described as colored noise. The evaluation of different kinds and colors of noise shows\nthat pink noise (\u03b2 = 1.0), which has the intermediate amount of Gaussian noise (\u03b2 = 0) and OU\nnoise (\u03b2 \u2248 2), can be the optimal noise in action selection. Furthermore, Hollenstein et al. (2024)\nstudied the effects of the temporally correlated noise for on-policy algorithms, where an intermedi-\nate amount of temporal correlation between Gaussian noise and pink noise with \u03b2 = 0.5 achieved\nthe best performance. However, there is no attempt to introduce temporal correlations to intrinsic\nmotivation, in contrast to the action selection."}, {"title": "2.3 CONDITIONAL VARIATIONAL AUTO-ENCODER (CVAE) FOR EXPLORATION", "content": "CVAE (Sohn et al., 2015) was introduced to learn the unlabeled dataset efficiently. Since input\nvariables are encoded as probability distributions into the conditioned-latent spaces, the policy of\nRL agents can be efficiently modeled. Thus, several studies adopted CVAE to mitigate the OOD\nproblem in offline-RL. Zhou et al. (2021) employed CVAE to model the behavior policies of agents\nfor a dataset or pre-collected experiences. The policy network was trained from the latent behavior\nspace, and its decoder was used to output actions from the behavior space of the environment. Since\nthe latent space after training was fit for the dataset distribution, the OOD problem of generating\nunpredictable actions could be mitigated. Besides, Rezaeifar et al. (2022) computed intrinsic reward\nfor anti-exploration using the L2-norm between the predicted action by a decoder and actual action.\nUnlike previous studies (Klissarov et al., 2019; Kubov\u010d\u00edk et al., 2023; Yan et al., 2024) that adopted\nVAE for intrinsic motivation, numerous studies adopted CVAE to model the policy networks."}, {"title": "3 BACKGROUND", "content": "In this paper, we use the Markov Decision Process (MDP) of a single RL agent represented as\na tuple M = (S, A, P, r, \u03b3). The tuple includes a set of states S, a set of actions A, and the\ntransition function P : S \u00d7 A \u00d7 S \u2192 [0,1] that provides the distribution P(s'|s, a) over the next\npossible successor state s' given a current state s and action a. The agent chooses an action from\na deterministic policy \u03c0 : S \u2192 A and receives a reward r:S \u00d7 A \u2192 R at each time step.\nThe goal of the agent is to learn the policy that maximizes the discounted expected return Rt =\n\u0395[\u03a3k=0 \u03b3krt+k+1] at a time step t, where \u03b3 \u2208 [0, 1] is the discount factor and rt is the sum of the\nextrinsic reward re and the intrinsic reward ri, respectively.\nPathak et al. (2017) proposed Intrinsic Curiosity Module (ICM) to formulate future prediction er-\nrors as the intrinsic reward. Since making predictions from the raw states is undesirable, ICM\nuses an embedding network fe that takes the state representation \u03c6(st) = fo(st) by training the\nlearnable parameters \u03b8 using two submodules as: firstly, the inverse dynamics model ge in the first\nsubmodule takes \u03c6(st) and \u03c6(st+1) as its inputs. The inverse dynamics model ge predicts the ac-\ntion of agents \u00e2t, which is equated as at = g(\u03c6(st), \u03c6(st+1)). Model ge is trained to minimize\nL\u2081 = CrossEntropy(\u00e2t, at) denoting the loss from the error between \u00e2t and at. The forward\ndynamics model h in the second submodule takes \u03c6(st) and at as its inputs. The forward dynamics\nmodel h predicts the next state representation \u03c6(st+1), which is equated as \u03c6(st+1) = h(\u03c6(st), at).\nModel ge is trained to minimize LF = ||\u03c6(st+1) \u2013 \u03c6(st+1)||2 denoting the loss from the error\nbetween \u03c6(st+1) and \u03c6(st+1)."}, {"title": "4 TECLE: TEMPORALLY CORRELATED LATENT EXPLORATION", "content": "Although the existing curiosity-driven methods improved exploration, they can be vulnerable to\nNoisy TV problems or stochasticity of environments (Raileanu & Rockt\u00e4schel, 2020; Mavor-Parker\net al., 2022). TeCLE started with the assumption that this is caused by predicting the noise sources,"}, {"title": "A. FEATURE EMBEDDING", "content": "which is inherently impossible, and the predictions themselves must contain noise to solve this\nproblem. In the following paragraphs, we describe the role and effect of each part. Consequently, in\npart C. Colored noise, we prove that these effects ultimately help the agents deal with the Noisy TV\nproblem. As shown in Figure 1, TeCLE consists of three parts, and the intrinsic reward is computed\nseparately from the policy networks. Similar to other curiosity-driven exploration methods, the\nintrinsic reward is computed separately from the policy networks.\nIt has been proven that predicting feature space leads to better generalization compared with pre-\ndicting raw pixel space (Burda et al., 2018a). Furthermore, since predicting the raw pixel is chal-\nlenging (Pathak et al., 2017), we use the embedding network and inverse network to learn the state\nrepresentation. In our formulation, embedding network fe that shares the parameters takes states st\nand st+1 as inputs. To optimize fe, state representation \u03c6(st) and future state representation \u03c6(st+1)\nare used as input of the inverse network ge as:\n\u00e2t = go(\u03c6(st), \u03c6(st+1)),\nwhere at denotes the predicted action. The loss function L\u2081 is equated as:\nL\u2081 = CrossEntropy(\u00e2t, at).\nBy learning state representations through embedding networks, the agent extracts important infor-\nmation from the environment, such as things that agents can control (e.g., steering wheel) and things\nthat agents cannot control but can be affected (e.g., passing vehicles). Detailed explanations of the\nstate representation and inverse network are provided in Section 3."}, {"title": "B. ACTION-CONDITIONED LATENT EXPLORATION", "content": "Several existing studies use the \u03c6(st+1) and the predicted future state representation \u03c6(st+1) in the\ncomputation of the intrinsic reward (Pathak et al., 2017; Burda et al., 2018a; Pathak et al., 2019).\nUnlike the above approaches, intrinsic reward of the proposed TeCLE is computed by using the\nreconstructed \u03c6(st+1) from the action-conditioned latent space. Firstly, \u03c6(st+1) and action at are\ntaken as inputs of an encoder qx as denoted in Eq.(3). Each corresponds to an input variable x and a\ncondition variable y of CVAE.\nqx(zt+1|x, y) := qx(zt+1|\u03c6(st+1), at), zt+1 ~ N(\u03bct+1, \u03c3t+1),"}, {"title": "C. COLORED NOISE", "content": "where latent representation zt+1 is sampled using the \u03bct+1 and \u03c32t+1 from output of the encoder qx.\nThen, zt+1 and at are taken as inputs to the decoder py, which outputs \u03c6(st+1) as:\npy(\u03c6(st+1)/zt+1,at).\nConsequently, the intrinsic reward ri is computed using L2-norm of the difference between \u03c6(st+1)\nand \u03c6(st+1) as follows:\nri = ||\u03c6(st+1) \u2013 \u03c6(st+1)||2.\nThe intuition for how TeCLE can encourage better exploration while avoiding noise sources is as\nfollows: py reconstructs the \u03c6(st+1) based on the probabilities of the previously visited states.\nBesides, at is used as a condition variable of the qx and py for self-supervised learning. Therefore,\nthe proposed TeCLE can encourages agents to explore by assigning larger intrinsic rewards to rarely\nvisited states in a self-supervised manner, while avoiding noise sources based on state visitation\nprobabilities. The training loss is the sum of reconstruction loss Lrecon and KL divergence LKL,\nwhere each loss function is formulated as:\nLrecon = BinaryCrossEntropy(\u03c6(st+1), \u03c6(st+1)).\nLKL = KL(qx(zt+1|\u03c6(st+1), at)||p\u03c8(zt+1|\u03c6(st+1))).\nThe detailed formulation and explanation of optimization are described in Appendix A.1.\nIt has been demonstrated that temporally correlated noise for action selection enhances exploration\nin both on-policy and off-policy RL (Eberhard et al., 2023; Hollenstein et al., 2024). However, as\nfar as we know, there have been no attempts to apply temporal correlation to intrinsic motivation.\nTherefore, we consider the utilization of temporally correlated noise when computing the intrinsic\nreward. To explain the temporally correlated noise, we revisit zt+1 ~ N(\u03bct+1, \u03c3t+1) in Eq.(3).\nUsing a reparameterization trick, it can be re-written as:\nzt+1 = \u03bct+1 + \u03b5t+1\u03c3t+1,\nwhere \u03b5t+1 is the injected noise. If \u03b5(1:t) = (\u03b51,\u00b7\u00b7\u00b7, \u03b5i,\u00b7\u00b7\u00b7,\u03b5t) is sampled from the Gaussian dis-\ntribution at every timestep, any \u03b5i, \u03b5j \u2208 \u2208(1:t) can be expressed as temporally uncorrelated. Besides,\ntemporally uncorrelated noise (i.e., white noise) corresponds to color parameter \u03b2 = 0. In terms\nof signal processing, |\u03b5 (1:t) (f)2| and \u03b5(1:t) (f) is converted as the Power Spectral Density (PSD) of\n\u03b5(1:t) and the Fourier transform of \u03b5(1:t), where \u03b2 has the properties of |\u03b5 (1:t) (f)2| \u221d f-\u03b2 (Timmer\n& Koenig, 1995; Eberhard et al., 2023). Therefore, it can be concluded that \u03b2 controls the amount\nof temporal correlation in the \u03b5(1:t). In other words, the noise with \u03b2 > 0 produces a temporal\ncorrelation between any \u03b5i, \u03b5j \u2208 \u2208(1:t) at different time steps. On the other hand, the noise with\n\u03b2 < 0 produces a temporal anti-correlation between any \u03b5i,\u03b5j \u2208 \u2208(1:t), causing high variation of\nnoises between time steps. A more detailed explanation of colored noise sequences is described in\nAppendix A.2.\nIn our intrinsic formulation, the generated \u03b5t+1 is used to sample the latent representation zt+1,\nand the \u03c6(st+1) is reconstructed from py using zt+1 and at. Therefore, it can be considered that\nsequence $(\u03c6(1:t)) = (\u03c6(s1),\u2026\u2026, \u03c6(st)) has an amount of temporal correlation, depending on \u03b2.\nWe hypothesize that the temporal correlation and anti-correlation (\u03b2 \u2260 0) in the generated noise\nsequence determine the exploratory behavior of the agent. When temporally anti-correlated noise\nwith \u03b2 < 0 is injected, noise sequences with constantly fluctuating magnitude can dynamically\nproduce the reconstructed state sequence. Thus, agents can be less sensitive to novel states, making\nthem more robust to Noisy TV by assigning smaller intrinsic rewards than when \u03b2 \u2265 0. Besides, in\nthe injection of temporally correlated noise with \u03b2 > 0, the noise sequence with smooth changing\nmagnitude generates a larger intrinsic reward in the novel states than when \u03b2 \u2264 0. To be more\nspecific, temporally anti-correlated noise with \u03b2 < 0 can make the proposed TeCLE continue to have\na perturbation of subsequent intrinsic rewards. On the other hand, the smooth change of temporally\ncorrelated noise with \u03b2 > 0 makes the change of subsequent intrinsic rewards stable. Therefore,"}, {"title": "5 EXPERIMENTAL RESULTS AND ANALYSIS", "content": "In the experiments, we analyzed the performance of TeCLE by varying \u03b2 of generated noise se-\nquence \u03b5(1:t). Also, we proved the effectiveness of TeCLE by comparing it with baselines in the\nMinigrid and Stochastic Atari environments. Further experiments, including the hard exploration\ntasks, can be found in the Appendix."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Baseline: For all our experiments, we adopted Proximal Policy Optimization (PPO) (Schulman\net al., 2017) as the base RL algorithm and Adam (Kingma, 2014) as the optimizer. In the experi-\nmental results, term ICM refers to the Intrinsic Curiosity Module, which uses the forward dynamics-\nbased prediction error as the intrinsic reward (Pathak et al., 2017). Term RND refers to the Random\nNetwork Distillation, which uses the fixed randomly initialized network-based prediction error as\nthe intrinsic reward (Burda et al., 2018b). Besides, terms TeCLE (-1.0) and TeCLE (2.0) refer to our\nproposed TeCLE with blue (\u03b2 = \u22121.0) and red (\u03b2 = 2.0) noises, respectively. All models used the\nsame base RL algorithm and neural network architecture for both the policy and value functions.\nThe only difference among them was in how intrinsic rewards were defined. Details on the hyper-\nparameters and neural network architectures can be found in Appendix C. For the comparison, we\nadopted average return during training as the performance metric. In the experimental results, solid\nlines and shade regions of training results denote the mean and variance, respectively.\nEnvironments: Since we focused on the exploration ability of agents, we not only used rewards\nbut also directly measured the state coverage (Raileanu & Rockt\u00e4schel, 2020; Kim et al., 2023)\nfor evaluation. In the Minigrid experiments, the world is partially observable (Chevalier-Boisvert\net al., 2018). Also, N \u00d7 N in the environment name refers to the size of a map, and SXRY\nrefers to a map of size X with rows of Y. Besides, SXNY refers to X size map with Y number\nof valid crossings across lava or walls from the starting position to the goal. Additionally, Noisy\nTV experiments were implemented by adding action-dependent noise when the agent selects done\naction in environments (Raileanu & Rockt\u00e4schel, 2020). In the Stochastic Atari experiments, we\nadopted sticky actions (i.e., randomly repeating the previous action (Burda et al., 2018b)), which\nwere proposed by Machado et al. (2018)."}, {"title": "5.2 DISCUSSION AND ANALYSIS OF EFFECTS OF DIFFERENT COLORED NOISE", "content": "In this subsection, we performed experiments in environments with and without Noisy TV\nto show the exploratory behaviors of the TeCLE with various colored noises. To ana-\nlyze the effects when temporally correlated noise is injected into action-conditioned latent\nspace and find the optimal \u03b2 for each environment, experiments were performed with \u03b2 \u2208\n{-1.0 (blue noise), 0 (white noise), 0.5, 1.0 (pink noise), 2.0 (red noise)} in \u03b5(1:t) on the Minigrid\nand Stochastic Atari environments. Besides, the normalized average return (Hollenstein et al., 2024)\nwas chosen as the performance metric.\nIn Table 1, when the blue noise (\u03b2 = -1.0) was applied to the environments with Noisy TV, the\nnormalized average returns in four environments had the highest values. Notably, compared with\nthe cases applying the white noise (\u03b2 = 0), the experiments for DoorKey environments significantly\nincreased the normalized average returns. Additionally, the experiments with the red noise (\u03b2 = 2.0)\nshowed good normalized average returns. Overall, when averaging the normalized average returns"}, {"title": "5.3 EXPERIMENTS ON MINIGRID ENVIRONMENTS", "content": "To prove the effectiveness of the proposed TeCLE, we compared the experimental results with the\nbaseline PPO, ICM, and RND in the Minigrid with and without Noisy TV. Considering notable\noutputs in Table 1 and Figure 2, we adopted red (\u03b2 = 2.0) and blue (\u03b2 = -1.0) noises as the default\ncolored noise for TeCLE. The policy network is updated every 128 steps.\nTo demonstrate the exploratory behavior of TeCLE and compare its effectiveness with baselines,\nwe measured the number of state visits by the agent (i.e., state coverage) (Raileanu & Rockt\u00e4schel,\n2020; Kim et al., 2023). State coverage was measured by clipping when visitation exceeded 10k\nduring training. It was then normalized to a range between 1 and 100. Figure 3 shows the state\ncoverage in DoorKey16\u00d716 and Empty16\u00d716 environments. As shown in DoorKey16\u00d716, whereas\nother baselines failed to open the door below and enter the other room, only TeCLE with red (\u03b2 ="}, {"title": "5.4 EXPERIMENTS ON STOCHASTIC ATARI ENVIRONMENTS", "content": "2.0) and blue (\u03b2 = -1.0) noises succeeded in solving the tasks and learned the optimal policy\nfor exploration. Additionally, it seems that blue noise (\u03b2 = -1.0) encourages agents to exploit\nmore than explore compared to red noise. In other words, red noise (\u03b2 = 2.0) encourages agents\nto explore more than exploit compared to blue noise (\u03b2 = -1.0). These exploratory behaviors\ndepending on different colored noise also can be seen in Empty16 \u00d7 16. While TeCLE with red\nnoise (\u03b2 = 2.0) showed global exploration, blue noise (\u03b2 = -1.0) showed local exploration.\nMoreover, the experimental results for all \u03b2 in Appendix D.2 show that as \u03b2 increases, TeCLE\nencourages the agent to explore more than exploit. This phenomenon is similar to the previous\nstudies (Eberhard et al., 2023; Hollenstein et al., 2024) that adjusted exploratory behaviors of agents\nby applying colored noise for action selection. Thus, we concluded that the amount of temporal\ncorrelation is closely related to the exploratory behaviors as well as robustness to the Noisy TV.\nFigure 4 shows the experimental results in the Minigrid environments with Noisy TV. In DoorKey8 \u00d7\n8, it is shown that only TeCLE can effectively learn the environments where Noisy TV exists,\nwhereas other methods failed. In particular, TeCLE with blue noise (\u03b2 = -1.0) showed faster\nconvergence than the red noise (\u03b2 = 2.0) in both DoorKey8 \u00d7 8 and DoorKey16 \u00d7 16 environments.\nThis means that the improved exploitation from the temporal anti-correlation could be suitable for\nsparse reward environments with Noisy TV. On the other hand, in DynamicObstacles (denoted as\nDO), TeCLE with red noise (\u03b2 = 2.0) showed the faster convergence. As in DoorKey environments,\nthe other methods failed to learn the optimal policy and avoid being trapped by Noisy TV. Notably,\nalthough the convergence of the TeCLE with blue noise (\u03b2 = \u22121.0) was slightly slower than red\nnoise (\u03b2 = 2.0) due to the improved exploitation, it eventually converged to the highest average\nreturn. Furthermore, it seems that in easy environments such as Empty8 \u00d7 8, all methods converged\nto a high average return. However, in difficult environments such as Empty16 \u00d7 16, the conver-\ngence was slow for all methods except TeCLE. This is because the rewards become sparse as the\nstate-space expands, and agents using other methods tend to lose curiosity about the environment."}, {"title": "5.5 ABLATION STUDY I: EFFECTS OF ACTION AS A CONDITION", "content": "To further investigate whether TeCLE can be robust to stochasticity or not, we evaluated it in the\nStochastic Atari environments (Burda et al., 2018b; Pathak et al., 2019) and compared it with other\nbaselines. As in the previous Minigrid experiments, we adopted red (\u03b2 = 2.0) and blue (\u03b2 = \u22121.0)\nnoises as the colored noise for TeCLE.\nFigure 6 shows the experimental results in several Stochastic Atari environments. Whereas SpaceIn-\nvaders and Enduro are known as easy and dense reward environments, BankHeist and Solaris are\nknown as hard and sparse reward environments (Ostrovski et al., 2017). In SpaceInvaders and En-\nduro, TeCLE outperformed other baselines, showing no significant difference between red (\u03b2 = 2.0)\nand blue (\u03b2 = -1.0) noises. It also showed that all methods except for RND can handle stochastic-\nity in dense reward environments. However, whereas other baselines failed to learn the BankHeist\nand Solaris, only TeCLE learned the environments while avoiding being trapped by stochasticity.\nTherefore, experimental results of the Stochastic Atari environments confirmed that the proposed\nTeCLE is the most effective in handling stochasticity in both dense and sparse reward environments."}, {"title": "5.6 ABLATION STUDY II: EXPLORATION WITHOUT EXTRINSIC REWARD", "content": "To demonstrate the effects of action as a condition for the action-conditioned latent space of TeCLE,\nwe experimented with an ablation study. Figure 7 shows the experimental results for analyzing the\neffects of action as a condition. The term TeCLE refers to the TeCLE using action as a condition,\nwhile the term w/o TeCLE refers to the TeCLE without using the condition.\nSince DoorKey 8 \u00d7 8 has a small state-space, the effects of action were not significant. However,\nin DoorKey 16 \u00d7 16, TeCLE without using action as a condition failed to learn the optimal policy.\nTherefore, it seems that the effects of the action were significant in terms of self-supervised learning.\nAlso, it is shown that when an environment has a large state-space, the action-conditioned latent\nspace can make better state reconstructions, helping find the optimal policy.\nTo prove whether TeCLE can be robust in the absence of any extrinsic rewards, we additionally\nexperimented with an ablation study. For experiments, we set the coefficient of extrinsic reward to\nzero and compared the average return of TeCLE with the baselines. Note that only intrinsic rewards\nare used to update the policy network of agents. Thus, extrinsic rewards are not used except for\nperformance measurements. Since PPO does not use intrinsic rewards, it was not compared.\nFigure 8 shows the experimental results in the Stochastic Atari environments when extrinsic re-\nwards were absent, demonstrating that only TeCLE can learn the environments. Experiments were\nconducted on SpaceInvaders and BankHeist, which are dense and sparse reward environments, re-\nspectively. However, since the agent does not receive any extrinsic rewards, it was expected that the\nagent could not learn the environment. Surprisingly, the experimental results of both environments\nshow that TeCLE can learn the environments without using extrinsic rewards. Most of all, TeCLE\nin BankHeist shows a similar average return to when extrinsic rewards are present, as shown in\nFigure 6 (c). Although RND is known to perform well in sparse reward environments and hard ex-\nploration tasks, the above experimental results show that TeCLE outperformed RND. In conclusion,\nthe above ablation study shows that the effects of the intrinsic reward from the proposed TeCLE were\nconsiderable in the absence of extrinsic reward. Therefore, we expect that the proposed TeCLE can\nbe more effective than other methods in real-world scenarios where rewards are sparse or absence."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "This paper proposes TeCLE, representing a novel curiosity-driven exploration method that defines\nintrinsic rewards through states reconstructed from an action-conditioned latent space. Extensive\nexperiments on benchmark environments show that the proposed method outperforms popular ex-\nploration methods such as ICM and RND and avoids being trapped by Noisy TV and stochasticity\nin the environments. Most of all, we find that the amount of temporal correlation is closely related"}, {"title": "A PRELIMINARIES", "content": "to the exploratory behaviors of agents. Therefore, we recommend that the blue and red noise, which\nshow notable performance among various colored noises, be the default settings for TeCLE in en-\nvironments where noise sources exist and rewards are sparse, respectively. As far as we know, our\nstudy is the first approach to introduce temporal correlation and temporal anti-correlation to intrinsic\nmotivation. Therefore, future studies are needed to verify that temporal correlation is effective in\nvarious intrinsic motivation methods, such as count-based exploration methods.\nIn this section, we compared the intrinsic rewards of TeCLE and those of baselines to explain how\nTeCLE can be robust to noise sources while outperforming baselines. Figure 15 shows that only\nTeCLE can learn the optimal policy network in Minigrid DoorKey 8 \u00d7 8 and 16 \u00d7 16 environments.\nThe intrinsic rewards measured during training of the policy networks are shown in Figure 16. While\nthe intrinsic reward of the baselines shows a small value near zero, the intrinsic reward of TeCLE\nmaintains a relatively large value. As we hypothesized in Section 4, the reason for the difference in\ntraining and intrinsic reward between baselines and TeCLE is that CVAE in the TeCLE continuously\ninjects noise when reconstructing state representation. Therefore, unlike baselines that maintain\nsmaller intrinsic rewards since they minimize the prediction error of the state representation, TeCLE\nmaintains a large intrinsic reward since it contains noise regardless of whether it is sufficiently\nexplored. As a result, this tendency of intrinsic reward from TeCLE helps agents prevent being\ntrapped in environments that contain inherently unpredictable noise sources."}, {"title": "A.1 OPTIMIZATION OF CVAE", "content": "The goal of a VAE is to output \u00ee that has a similar distribution to the input data x. The VAE\nconsists of an encoder qx and a decoder py, where"}]}