{"title": "Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models", "authors": ["Qitan Lv", "Jie Wang", "Hanzhu Chen", "Bin Li", "Yongdong Zhang", "Feng Wu"], "abstract": "Generation of plausible but incorrect factual information, often termed hallucination, has attracted significant research interest. Retrieval-augmented language model (RALM)\u2014which enhances models with up-to-date knowledge-emerges as a promising method to reduce hallucination. However, existing RALMs may instead exacerbate hallucination when retrieving lengthy contexts. To address this challenge, we propose COFT, a novel Coarse-to-Fine highlighTing method to focus on different granularity-level key texts, thereby avoiding getting lost in lengthy contexts. Specifically, COFT consists of three components: recaller, scorer, and selector. First, recaller applies a knowledge graph to extract potential key entities in a given context. Second, scorer measures the importance of each entity by calculating its contextual weight. Finally, selector selects high contextual weight entities with a dynamic threshold algorithm and highlights the corresponding paragraphs, sentences, or words in a coarse-to-fine manner. Extensive experiments on the knowledge hallucination benchmark demonstrate the effectiveness of COFT, leading to a superior performance over 30% in the F1 score metric. Moreover, COFT also exhibits remarkable versatility across various long-form tasks, such as reading comprehension and question answering.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have exhibited remarkable power and impressive generalization capabilities across a wide range of domains (Brown et al., 2020; El-Kassas et al., 2021). However, even the currently most capable LLM still exhibits knowledge hallucination issues, i.e., GPT4 (OpenAI, 2023) may also generate plausible yet incorrect factual information (Zhang et al., 2023b). Moreover, in long-"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Retrieval-Augmented Language Models", "content": "Retrieval-Augmented Language Models (RALMs) that enhance models with up-to-date knowledge by external knowledge sources, extend the knowledge boundaries of LLMs (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022). These models first retrieve an external evidence corpus, such as Wikipedia, to pinpoint documents relevant to the query as reference texts (Karpukhin et al., 2020a; Sachan et al., 2023). Then, a reader component analyzes these documents and provides a response (Izacard & Grave, 2020a; Yu et al., 2021a). This approach effectively retrieves reference texts related to the query, thereby enhancing the credibility of generated questions (Gao et al., 2023b; Jiang et al., 2023b). The evolution also leads to the emergence and popularity of retrieval-augmented products, such as ChatGPT plugins, Langchain, and New Bing."}, {"title": "2.2. Chain-of-X Approaches in LLMs", "content": "LLMs are capable of decomposing complex problems into a series of intermediate steps and generate internal thoughts or reasoning steps before responding, known as Chain-of-Thought (CoT) prompting (Wei et al., 2022). The CoT approach mirrors human problem-solving by breaking complex issues into smaller components, helping LLMs focus on each segment, reducing errors, and enhancing logic in reasoning (Wang et al., 2022b). Following-up works effectively extend CoT to other chain-of-X methods, including chain-of-explanation (Huang et al., 2023) and chain-of-knowledge (Wang et al., 2023b). More recently, chain-of-verification (Dhuliawala et al., 2023) prompts LLMs to draft initial responses, plan verification questions, answer questions, and generate the final verified response, which reduces the likelihood of LLMs to misunderstand a specific concept. Chain-of-note (Yu et al., 2023a) enables LLMs to annotate retrieved documents and incorporates them in formulating the response to enhance the robustness of LLMs."}, {"title": "2.3. Knowledge Hallucination", "content": "Hallucination is a general problem in LLMs, affecting various natural language processing tasks, such as reading comprehension (Maynez et al., 2020), open-domain question answering (Roller et al., 2020), and remains unresolved by simply enlarging training data or model size (Zhang et al., 2023a). We mainly discuss generation-time and retrieval-augmented methods to reduce knowledge hallucination, which are most relevant to our COFT."}, {"title": "3. Preliminaries", "content": null}, {"title": "3.1. Notations", "content": "We denote an input prompt for LLM as x = (xins, xque, xrefs), where xins denotes the instructions for downstream tasks, xque denotes the queries, and xrefs denotes reference contexts. Let S = [S1, S2, S3, ...] denote the sentence list of xrefs, where si denote the i-th sentence and E = [e1, e2, e3, . . .] denote the candidate key entity list, where ek denote the k-th candidate. For a given entity ek, we denote fek,si and fek,s as the number of times ek appears in si and S. Let |si| and |S| denote the number of words within sentence si and the reference context S. Let t\u2081 denote the i-th token in xrefs, P(t\u2081) denote its output probability by a small language model Ms, and I (t\u2081) denote the self-information of token ti. Let  denote the concatenation of two texts."}, {"title": "3.2. Self-Information", "content": "Self-information is a fundamental concept in information theory, which quantifies the amount of information contained in a random event (Shannon, 1948). In natural language processing, an event can be regarded as a generation step (i.e., a token), and the distribution corresponds to its output distribution. We can obtain self-information of a token ti by the follow equation:\nI(t) = - log2 P (ti | t1, t2,..., t-1)\nwhere I(t) denotes the self-information of token ti and P(t) denotes its output probability."}, {"title": "4. Method", "content": "We propose a COarse-to-Fine highlighTing method (COFT) that promotes LLMs to focus on key lexical units, which preserves complete contextual semantics and avoids getting lost in long contexts. COFT can highlight different granularity-level lexical units in a coarse-to-fine manner, such as paragraphs, sentences, and words. COFT organically integrates three modules in a unified framework."}, {"title": "4.1. Recaller", "content": "Recaller first generates candidate key entities extracted from the query and then retains the candidates occurred in the reference contexts. Specifically, for a given query and reference context, the workflow of recaller is as follows:\n(i) Recaller first conducts named entity recognition on the query to extract named entities that represent keywords within the query. These entities include some specific terms and important nouns such as people, places, organizations, etc.\n(ii) After obtaining named entities, recaller leverages them to search one-hop neighbor entities in wikidata to enrich candidate entities. The named entities and corresponding one-hop neighbors are combined to form candidate entities for the query.\n(iii) Recaller finally retains candidate entities that are also present in the reference context, forming the final candidate key entities list.\nAs shown in the left part of Figure 2, given a query such as \"Which country or city has the maximum number of nuclear power plants?\u201d, recaller first performs named entity recognition to identify entities like \"country\", \"city\", and \"nuclear power plants\". Then, recaller extracts one-hop neighboring entities from wikidata for each named entity, such as \"United States\" and \"France\". Finally, based on these named entities and neighboring entities, recaller retains entities that are present in the reference context as the final candidate key entities list. For example, \"France\u201d will not be retained because it is not in the reference context."}, {"title": "4.2. Scorer", "content": "After obtaining candidate key entities, scorer proceeds to assess their importance. With this desiderata, scorer proposes an entity-level iterative algorithm based on a small language model, Llama 7B (Touvron et al., 2023) to calculate the contextual weight of each entity in the context. Algorithm 1 outlines the overall procedure.\nSpecifically, we first segment reference contexts xrefs into sentence list S = [S1, S2, S3, . . .]. Drawing upon the TF-IDF (Term Frequency-Inverse Document Frequency) algorithm (Sparck Jones, 1972), a well-suited text relevance assessment and text mining approach that enables the exclusion of the majority of common entities while preserving important entities. We introduce the TF-ISF algorithm, which involves considering the TF-IDF algorithm at the Sentence level. For a given entity ek in sentence si, the corresponding TF-ISF calculation function is as follows:\nTF-ISF(ek) = \\frac{fek,Si}{|Si|} \u00d7 log2 \\frac{|S|}{fek,s +1} (2)\nwhere fek,s; and feks denote the number of times ek appears in s\u2081 and S. |s| and |S| denote the number of words within sentence si and reference contexts S.\nTF-ISF evaluates the importance of entities in reference context based on word frequency and effectively distinguishes common but unimportant entities. Higher TF-ISF suggests that the entity plays a more important role in understanding the sentence semantics, and vice versa.\nWe further concatenate the query and reference context to measure the importance of each token in the reference context based on self-information. Given the input xque xrefs, the self-information calculation function is as follows:\nI(t) = log2 P (ti|xque, t1, t2,..., ti\u22121) (3)\nwhere ti denotes the i-th token within the reference context xrefs, P(ti|xque, t1, t2, ..., ti\u22121) denotes its output probability by the small language model Ms, and I (ti) denotes the self-information of token ti. We can further leverage the additivity property of self-information in Equation 1 to merge tokens into entity e, thereby obtaining the self-information of each individual key candidate entity I(e).\nTo comprehensively consider both the TF-ISF and self-information, we propose contextual weights to indicate the importance of each key candidate entity in the reference context. A higher contextual weight suggests greater importance of the entity to answer the query. The contextual weight calculation function is as follows:\nw(ek) = TF-ISF(ek) \u00d7 I(ek) (4)\nwhere TF-ISF(ek) and I(ek) denote the TF-ISF and self-information of a key candidate entity ek, respectively. Other combination methods for TF-ISF and self-information scores are also feasible, and we leave it as a future work."}, {"title": "4.3. Selector", "content": "After obtaining candidate key entities and their contextual weights, selector highlights the final lexical units for the query. Specifically, selector first sorts entities based on contextual weights, and proposes a dynamic threshold algorithm to filter a dynamic proportion of key entities. The dynamic thresholds can be defined as T = 0.5\u00d7 (Tlen+Tinfo), where Tlen and Tinfo denote the min-max normalized value of the length and informativeness for each reference context. T varies with the length and informativeness of the reference context, as longer and more informative reference context requires more highlights. Then, selector highlights the reference context according to the granularity of selected lexical units. This highlighting process is as follows:\n(i) Split the reference context according to the granularity of selected lexical units.\n(ii) Calculate the contextual weight of the split lexical units by summing the contextual weight of candidate key entities occurred in the split.\n(iii) Sort these lexical units in descending order by their contextual weight, and select the lexical units with contextual weights in the top \u315c \u00d7 100% for highlighting.\nAfter selecting the highlighted lexical units, selector inserts special symbols around these lexical units. Considering the rich diversity of formatting found in publicly accessible web data, which forms a part of the pre-training corpus for LLMs, we adopt markdown syntax, particularly the bold syntax (**) as an example, to highlight important lexical units. This approach aligns with the natural occurrence of formatted text in online sources, thereby enabling the LLMs to more accurately interpret and process textual emphasis as it appears in real-world scenarios. Take word-level granularity highlighting as an example. If the selected highlighted entities are \"nuclear power plants\" and \"United States\", then the sentence \"The nuclear power plants in the United States play a crucial role in providing ...\" will be highlighted as \"The **nuclear power plants** in the **United States** play a crucial role in providing...\" as input for LLM inference. Other highlighting methods, such as HTML bold symbols or different markdown syntax are also viable options and we leave the exploration as a future work."}, {"title": "5. Experiments", "content": "We design experiments to evaluate the effectiveness of COFT for reducing knowledge hallucination and demonstrate the versatility of COFT on a variety of tasks. With this desiderata, we divide the experiments into four parts:\n(i) To evaluate the effectiveness of COFT, we compare COFT with existing state-of-the-art methods for reducing knowledge hallucination.\n(ii) To demonstrate the versatility of COFT, we conduct experiments on reading comprehension and question-answering benchmarks.\n(iii) To investigate the contribution of each component within COFT, we conduct the ablation study.\n(iv) To provide more insight into COFT, we conduct the visualization study."}, {"title": "5.1. Experiment Setups", "content": "Experiment Setups. We apply LLMs including Vicuna2 (vicuna-33B-v1.3) (Zheng et al., 2023), ChatGPT3 (gpt-3.5-turbo) and GPT4 (gpt-4) (OpenAI, 2023) as backbone models. To guarantee stable and reproducible results, we utilize greedy decoding and set the temperature parameter as 0 in all experiments. For knowledge halluci-"}, {"title": "5.2. Knowledge Hallucination Results", "content": "In this section, we conduct experiments on the knowledge hallucination benchmark. As shown in Table 1, we observe that COFT significantly and consistently outperforms existing methods on the hallucination benchmark. Specifically, for all three backbone models, COFT achieves average improvements of 34.5%; 33.1%; 28.7% in the F1 score metric, 16.3%; 22.6%; 11.6% in precision metric, and 30.9%; 35.9%; 28.7% in recall metric for WK (world knowledge, a wide domain including movies, countries, places, and so on), Sci/Tech (Science and Technology spanning various academic disciplines such as physics, chemistry, and biology), and Wri/Rec (Writing and Recommendation, including details of some books and movies) domains.\nWhile methods such as CoT and CoN do not consistently enhance the performance of Vicuna-33B and ChatGPT across various datasets, COFT consistently demonstrates a superior performance over vanilla models. Notably, in the science and technology domain, COFT achieves a maximum performance enhancement of over 60% in the F1 score metric, which effectively underscores the importance of capturing key information in the entire context. The universality of three backbone models also suggests that COFT possesses the potential across various LLMs."}, {"title": "5.3. Reading Comprehension Results", "content": "Reading comprehension task necessitates that LLMs answer certain questions based on the entire content, requiring the model to retain a comprehensive understanding of the complete contextual semantics. Through the reading compre-"}, {"title": "5.4. Question Answering Results", "content": "Question answering task requires the LLM to effectively focus on keywords and phrases within a question. Following CoN (Yu et al., 2023a), we conduct experiments on question-answering tasks to evaluate the robustness of COFT under scenarios where reference texts contain both relevant and noisy documents. These noise documents are retrieved based on their semantic similarity to the input questions, which often contain similar but misleading information. We employ the noise ratio to represent the extent of noisy interference under varying degrees of noise. For instance, if the top-k documents are retrieved for LLMs, then k x r represents the number of noisy documents, while k \u00d7 (1 - r) indicates the number of relevant documents. For example, with a 20% noise ratio and a requirement for the top-5 documents, 4 would be relevant documents, and 1 would be a noisy document. We concatenate relevant and noisy documents randomly, to mitigate position bias (Zheng et al., 2023). This requires LLMs to identify the most relevant information under lengthy and noisy conditions."}, {"title": "5.5. Ablation Study", "content": "To further investigate the contribution of each component within COFT, we conduct a series of ablation experiments on the entire framework. We select a word-level version, COFTw to conduct the ablation study. Other granularity versions of COFT including sentences or paragraphs follow a similar way. For simplicity, we denote COFTw as COFT in this section. Specifically, we denote COFT without recaller extracting candidate key entities as COFTw/o recaller, COFT without the TF-ISF score as COFTw/\u03bf TF-ISF, COFT without the self-information score as COFTw/o SI, COFT without scorer calculating the contextual weight as COFTw/o scorer, and COFT without dynamic threshold selecting key candidate entities as COFTw/o selector, respectively. We set the threshold 7 to 0.5 for COFTw/o selector as an example. More detailed results are in Appendix E.1.\nWe present ablation results of COFT using ChatGPT as the backbone model. More Results using backbone models including Vicuna-33B and GPT4 are in Appendix E.2. As shown in Table 3, the absence of any component within COFT results in a performance degradation of the entire framework. Notably, recaller and scorer have more significant impacts on the performance of COFT, which demonstrates the importance of extracting candidate key lexical units from the reference text and ranking them based on contextual weight to reduce knowledge hallucination."}, {"title": "5.6. Visualization Study", "content": "To provide more insight into COFT, we conduct a visualization study. As mentioned above, COFT promotes LLMs to focus on key texts in the entire context. We employ attention scores to trace the information flow in the reference context based on Vicuna-33B, both before and after highlighting (Wang et al., 2023d). As shown in Figure 4, the highlighted key lexical units possess higher attention scores and exhibit stronger interactions with other words. This suggests that LLMs better focus on these highlighted key lexical units during inference."}, {"title": "6. Conclusions", "content": "In this paper, we propose a novel COarse-to-Fine highlighTing method to effectively reduce knowledge hallucination. Specifically, we propose recaller, scorer, and selector to form a general framework for LLMs to focus on key texts and avoid getting lost in long contexts. Extensive experiments on the knowledge hallucination task demonstrate the effectiveness of COFT with an average improvement of 32.1% in the F1 score metric. This superior performance over existing state-of-the-art methods demonstrates the effectiveness of COFT in reducing knowledge hallucination in LLMs. COFT also serves as a plug-and-play framework for many long-form tasks that achieves an average improvement of 4.6% in the precision metric for reading comprehension tasks and a maximum improvement of 10.5% in the F1 score metric for question-answering tasks."}, {"title": "A. More Related Works", "content": null}, {"title": "A.1. Language Models", "content": "Language models such as GPT (Radford et al., 2018), BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and Megatron-LM (Shoeybi et al., 2019) have led to a learning paradigm shift in natural language processing (NLP). Models are first pre-trained on extensive volumes of unlabeled text corpora with language modeling objectives, and then fine-tuned on downstream tasks. Recently, large language models (LLMs) including ChatGPT, PaLM (Chowdhery et al., 2022), and Gemini (Team, 2023) have shown great performance in both few-shot and even zero-shot scenarios (Brown et al., 2020)."}, {"title": "A.2. Knowledge Hallucination", "content": "Besides the methods mentioned in Section 2.3 to address knowledge hallucinations during the generation time or through the RALM framework, these are some methods that address hallucinations during training time. These interventions during the training stage of LLMs to tackle the issue of model hallucinations are termed training-time correction. For training-time correction, efforts are made to enhance the raw left-to-right outputs of either an encoder-decoder or a decoder-only language model. This enhancement involves training or suitably adjusting the model's weights to reduce the likelihood of hallucinated content. This includes using reinforcement learning (Roit et al., 2023; Wu et al., 2023c) as well as contrastive learning methods (Chern et al., 2023b; Sun et al., 2023). For training-time correction methods, models designed to resolve knowledge hallucinations during the training phase typically require the use of open-source LLMs and substantial computational resources. Our COFT effectively reduces the hallucination issue in LLMs without the finetuning process. Moreover, LLMS after the training-time generation method can also be integrated as a part of our COFT pipeline."}, {"title": "A.3. Context Compression", "content": "One significant challenge in the computation of self-attention mechanisms is the computational complexity O(L2), which exhibits a quadratic scaling in relation to the length of the input sequence L. Numerous variations of the Transformer architecture have been introduced, aiming to modify the conventional attention mechanism into more efficient alternatives specifically designed for tasks involving very long context (Zaheer et al., 2020; Katharopoulos et al., 2020). Extensive endeavors also focus on context compression by compressing the context into fewer soft tokens. This includes substitutes with summary tokens (Chevalier et al., 2023), leveraging additional auto-encoder schemes (Ge et al., 2023a), and semantic compression (Fei et al., 2023). Sparse attention (Anagnostidis et al., 2023) adopts a methodology predicated on learning to dynamically excise uninformative context tokens for each individual token. Several efforts also select contexts to compress the input prompt (Li et al., 2023; Jiang et al., 2023a;b). However, due to the incomplete context, these methods may confront inevitable losses of information in real-world scenarios characterized by more complex distributions of attention."}, {"title": "B. More Details of Datasets and Experiment Setups", "content": "We present more details of datasets and experiment setups in this section.\nFor more details of experiment setup, in this paper, we use ChatGPT and GPT4 as the representatives of the current closed-source LLMs, both of which can be get access via OpenAI4. We apply Vicuna-33B (Zheng et al., 2023) as a representative of open-source LLMs. All experiments were performed on four Nvidia A100 GPUs (80GB). We implement our approach based on PyTorch 1.13.05 and Huggingface's Transformers."}, {"title": "C. More Results of Reading Comprehension", "content": "As we mentioned above, COFT can be effectively implemented across various NLP tasks for LLM long-form inference. In this section, we present more results of COFT with Vicuna-33B and GPT-4 as backbone models on the reading comprehension task to serve as a supplement to Section 5.3, where ChatGPT is employed as the backbone model. We observe from Table 7 that COFT consistently enhances performance across various LLM backbones in both RACE-H and RACE-M benchmarks. Specifically, COFT obtains superior performances of 11.6% and 3.1% in RACE-H and RACE-M for the Vicuna-33B model and 1.2% and 1.3% in RACE-H and RACE-M for GPT4 model, respectively. These results effectively demonstrate that COFT shows versatility under multiple LLMs as backbone models in the reading comprehension task, which also suggests that COFT effectively promotes LLMs to retain a comprehensive understanding of the long contextual semantics and to focus on keywords and phrases relevant to the question. Furthermore, we observe that COFT achieves more performance enhancement on the more challenging and complex dataset, RACE-H. This also suggests that COFT possesses potential for application in more complex real-world scenarios. Notably, when utilizing Vicuna-33B as the backbone model, COFT achieves 11.6% superior performance in precision metric on RACE-H over the suboptimal approaches. This also indicates the potential of COFT to better assist relatively \u201csmall\u201d models in more effectively maintaining complete context semantics, focusing on key lexical units, and avoiding getting lost in the lengthy context. These findings also demonstrate the efficacy of COFT applicable in reading comprehension tasks, where complete contextual semantics are necessary."}, {"title": "D. More Results of Question Answering", "content": "COFT also exhibits robustness against noise texts present in the reference contexts. We provide more question answering results illustrated in Figures 5, 6, 7, 8, and 9. We also provide detailed data tables as a numerical complement to the visual results in Section 5.4. As illustrated in Tables 11, 12, and 13, we observe that COFT is capable of maintaining relative robustness compared to other methods under conditions of severe noisy scenarios. This also demonstrates the effectiveness of mining key lexical and phrases relevant to the query. We further observe that as the noise ratio increases, that is, a greater proportion of irrelevant text in the reference context, COFT demonstrates enhanced robustness compared to other methods, thereby yielding relatively superior results. COFT achieves improvements or comparable results to baseline methods across nearly all conditions of noise ratio. Notably, under conditions where the noise ratio is 80%, COFT achieves a maximum improvement of 6.5% in EM metric and 10.5% in the F1 score metric when utilizing ChatGPT as the backbone model, which also demonstrates the noise robustness under similar nosy documents and the capability to focus on the highlighted key lexical units to the given query of our COFT.\nSpecifically, when ChatGPT serves as the backbone model and the noise ratio goes from 0% to 80%, on the Natural Questions dataset, COFT achieves average improvements of 4.3% in the EM metric and 3.4% in the F1 score metric. On the TriviaQA dataset, our COFT achieves average improvements of 2.7% in the EM metric, alongside 2.0% in the F1 score metric. Furthermore, on the WebQ dataset, COFT achieves average improvements of 2.3% in the EM metric and 4.7% in the F1 score metric. These results underscore the efficacy of the COFT approach in enhancing the performance of ChatGPT.\nWhen Vicuna-33B serves as the backbone model, COFT has demonstrated notable improvements across different evaluation metrics. On the Natural Questions dataset, COFT achieves average improvements of 1.6% in the EM metric and 3.0% in the F1 score metric. On the TriviaQA dataset, COFT achieves average improvements of 2.0% in the EM metric, along with 2.5% in the F1 score metric. Furthermore, on the WebQ dataset, COFT achieves average improvements of 1.4% in the EM metric and 1.8% in the F1 score metric. These results underscore the efficacy of the COFT approach in enhancing the performance of Vicuna-33B.\nWhen GPT4 serves as the backbone model, our COFT exhibits enhancements across various evaluation metrics as well."}, {"title": "E. More Results of Ablation Study", "content": null}, {"title": "E.1. Detailed Ablation Results of Selector", "content": "In Section 5.5, we conduct the ablation study on selector by setting the threshold to a fixed value of 0.5, utilizing ChatGPT as the backbone model. In this section, we conduct a more detailed ablation study of the selector. We experiment with the threshold T for selector, ranging from 0.1 to 1.0, and report the ablated results of Vicuna-33B, ChatGPT, and GPT4, respectively to provide more insight into our dynamic threshold algorithm.\nAs shown in Tables 14, 15, and 16. We still observe that our dynamic threshold algorithm achieves consistently superior and robust results against all other fixed thresholds. This effectively demonstrates the necessity of considering both the length and the amount of information of a given input reference context when setting the filtering thresholds to key lexical units. Moreover, the proposed dynamic threshold algorithm may potentially be beneficial to consider additional factors or optimize the combination method of context length and the amount of information to get improved results and we leave the exploration as a future work."}, {"title": "E.2. Ablation Results for Vicuna-33B and GPT4", "content": "In Section 5.5, we report the results of the ablation study using ChatGPT as the backbone model. In this section, we will further present the results using Vicuna-33B and GPT4 as backbone models to obtain more insights into the individual components constituting COFT across various backbone models. As illustrated in Tables 8 and 9, we still observe that the absence of each component within COFT invariably leads to a decline in performance across diverse domains for Vicuna-33B and GPT4 in the FELM benchmark, which demonstrates that COFT organically integrates these components into a unified framework as well.\nRemarkably, we observe that in the absence of a scorer, i.e., selector randomly retains the top 7 \u00d7 100% of key candidates obtained by the recaller using a dynamic threshold algorithm, rather than preserving them in descending order based on contextual weight, leads to a more significant decline in performance. This underscores the critical importance of effectively measuring the candidates' significance and highlights these candidates in reducing the issue of knowledge hallucination within LLMs as well.\nThese results underscore the organic integration of the three core components of COFT, recaller, scorer, and selector. This"}, {"title": "F. Inference Time Comparisons", "content": "We note that COFT requires an additional process of highlighting the input text before feeding it into the LLM for reasoning. Compared to the vanilla model, this process could potentially introduce extra inference time. Hence, in this section, we record and compare the average inference time per sample of different methods including vanilla, CoT, RALM, CON, CoVe, and COFT on the knowledge hallucination benchmark, FELM, to explore the influence of additional inference time and provide more insight of our COFT. We report the word-level granularity COFT as an example, as the inference times for COFT at three different granularity levels (paragraph level, sentence level, and word level) are nearly identical.\nWe report the average inference time per sample as a metric, as shown in Table 10. We observe from the table that although the incorporation of COFT as a preprocessing module for LLM introduces additional inference time costs, this impact is marginal. On average, the increase in inference time cost per sample due to the introduction of COFT, compared to the vanilla model, is 12%. Notably, when utilizing accelerated APIs such as GPT, this additional inference time is less than one second, yet it offers an average improvement of 33.2% and a maximum of 60.5% in the F1 score for existing LLMs to reduce the issue of knowledge hallucination. Furthermore, COFT exhibits higher inference efficiency compared to methods"}, {"title": "G. More Results of Smaller Self-information Calculator", "content": "In Section 5.2, we utilize Llama 7B as the self-information calculator due to its great performance across a wide range of downstream tasks (Touvron et al., 2023). To further demonstrate the generalization and versatility of COFT across models of smaller scales, we conduct additional experiments using GPT-2 small (124M), GPT-2 medium (355M), GPT-2 large (744M), and GPT-2 XL (1.5B) to calculate the self-information, respectively (Radford et al., 2019). As shown in Tables 17, 18, 19 and 20, COFT consistently exhibits superior performance across all baseline methods, which demonstrates the effectiveness and potentially broad applications to smaller models."}, {"title": "H. More In-depth Analysis of COFT", "content": null}, {"title": "H.1. Comparison Results of Adding Special Prompt", "content": "We conduct experiments of the baseline methods with the addition of the prompt \"Please pay close attention to the most relevant content in the text\" on the FELM benchmark for the knowledge hallucination task. As shown in Table 21, we"}, {"title": "H.2. Two-hop Neighborhood Results of COFT", "content": "COFT exhibits excellent scalability and can be extended to multi-hop neighbor situations. COFT initially focuses on one-hop neighbors of candidate entities within the KG due to their intrinsic relevance and close association. And leveraging only a single hop from the neighbors results in an average increase of 30% and a maximum improvement of 60.5% in the F1 score on the knowledge hallucination task.\nWe conduct additional experiments incorporating two-hop neighbor information of COFT on the knowledge hallucination benchmark. As shown in Table 22, compared to the one-hop version of COFT, integrating two-hop neighbor information further enriches the input provided to the LLMs, leading to a moderate performance improvement over the one-hop scenario.\nDespite the primary focus on one-hop neighbors, COFT maintains great performance. This demonstrates the effectiveness of extracting one-hop neighbors. When incorporating two-hop information, COFT further achieves a better result, which also demonstrates the flexibility and scalability of COFT. Therefore, for more complex question scenarios, there are also potential benefits of incorporating two-hop or even multi-hop neighbors to further increase the performance of COFT."}, {"title": "H.3. Only Input Highlights for LLM Inference", "content": "We incorporate specific symbols to highlight these units within the context to preserve the complete contextual semantics. The absence of complete contextual semantics may face inevitable information loss in real scenarios with more complex attention distributions (Miao et al., 2023b).\nWe further conduct experiments on the knowledge hallucination dataset, where we only use the highlighted lexical units as input to LLMs. As shown in Table 23, we find that even when only the highlighted lexical units are provided as reference context, the model achieves notable improvements over the baseline methods. This outcome demonstrates the efficacy of our COFT approach in accurately identifying and leveraging support facts within the reference text, thereby enhancing the inference performance. Meanwhile, compared to Table 1, we find that COFT uses only the highlighted lexical units as input is less competitive than the original version of CO"}]}