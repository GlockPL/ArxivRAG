{"title": "Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in Federated Learning", "authors": ["Runhua Xu", "Shiqi Gao", "Chao Li", "James Joshi", "Jianxin Li"], "abstract": "Federated learning (FL) is inherently susceptible to privacy breaches and poisoning attacks. To tackle these challenges, researchers have separately devised secure aggregation mechanisms to protect data privacy and robust aggregation methods that withstand poisoning attacks. However, simultaneously addressing both concerns is challenging; secure aggregation facilitates poisoning attacks as most anomaly detection techniques require access to unencrypted local model updates, which are obscured by secure aggregation. Few recent efforts to simultaneously tackle both challenges offen depend on impractical assumption of non-colluding two-server setups that disrupt FL's topology, or three-party computation which introduces scalability issues, complicating deployment and application. To overcome this dilemma, this paper introduce a Dual Defense Federated learning (DDFed) framework. DDFed simultaneously boosts privacy protection and mitigates poisoning attacks, without introducing new participant roles or disrupting the existing FL topology. DDFed initially leverages cutting-edge fully homomorphic encryption (FHE) to securely aggregate model updates, without the impractical requirement for non-colluding two-server setups and ensures strong privacy protection. Additionally, we proposes a unique two-phase anomaly detection mechanism for encrypted model updates, featuring secure similarity computation and feedback-driven collaborative selection, with additional measures to prevent potential privacy breaches from Byzantine clients incorporated into the detection process. We conducted extensive experiments on various model poisoning attacks and FL scenarios, including both cross-device and cross-silo FL. Experiments on publicly available datasets demonstrate that DDFed successfully protects model privacy and effectively defends against model poisoning threats.", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL)[18] is gaining popularity as a collaborative model training paradigm that provides primary privacy protection by eliminating the need of sharing private training data. Based on the participants' scale, FL is typically divided into two categories: cross-silo FL and cross-device FL[17]. Cross-device FL typically involves numerous similar devices, while cross-silo FL usually includes fewer participants like organizations. Recent studies show that FL mainly confronts two types of threats: privacy risks from curious adversaries attempting to compromise data privacy through methods like membership inference and model inversion attacks, and security risks from Byzantine adversaries looking to damage the final model's integrity with backdoors or by lowering its accuracy [2, 24, 14, 11, 3, 1, 34].\nTo mitigate privacy risks in FL, researchers have developed a range of techniques to bolster privacy. These encompass differential privacy-based aggregation [32], as well as secure aggregation approaches using homomorphic encryption[41], functional encryption[35], and secure multi-party computation[6, 43]. Aside from privacy concerns, many studies have proposed strategies to identify and mitigate potentially harmful updates during the model aggregation phase, thereby safeguarding the global model against adversarial attacks. Notable Byzantine-resistant aggregation mechanisms encompass the Krum fusion method[5], cosine defense aggregation mechanism[29, 38], and median/mean-based strategies like clipping median and trimmed mean strategies [40]. Research in these two areas has been conducted separately, and addressing both issues at once continues to be challenging. This difficulty arises because secure aggregation makes it easier for adversarial attacks to occur, as most anomaly detection methods need access to \u201cunencrypted\" local model updates that secure aggregation protects.\nFew recent efforts [39, 13, 16, 43, 15, 23, 9, 20] to tackle both challenges simultaneously often depend on differential privacy techniques [39, 13, 16, 22, 12], which can degrade model performance due to added noise, or rely on impractical non-colluded two-server assumption that disrupts FL's topology[43, 15, 23, 9, 20], complicating its deployment and application. In light of these limitations, a critical yet overlooked question is how to create a straightforward dual defense strategy that simultaneously strengthens privacy protection and mitigates poisoning attacks without introducing new participant roles or altering the single-server multiple-clients structure?\nTo address this dilemma, this paper proposes a Dual Defense approach that simultaneously enhances privacy protection and combats poisoning attacks in Federated learning (DDFed), without changing the structure of current FL frameworks. DDFed initially leverages cutting-edge cryptographic technology, specifically fully homomorphic encryption (FHE), to securely aggregate model updates without the impractical assumption of non-colluding two-server setups and ensures strong privacy protection by permitting only the aggregation server to perform secure aggregation in the dark. To tackle the challenge of detecting malicious models within encrypted model updates, DDFed introduces a novel two-phase anomaly detection mechanism. This approach enables cosine similarity computation over encrypted models and incorporates a feedback-driven collaborative selection process, with additional measures to prevent potential privacy breaches from Byzantine clients incorporated into the detection mechanism. Our main contributions are summarized as follows:\n\u2022 We introduce a dual defense strategy that simultaneously boosts privacy and combats poisoning attacks in federated learning. This is achieved by integrating FHE-based secure aggregation with a mechanism for detecting malicious encrypted models based on similarity.\n\u2022 To effectively detect malicious models in encrypted updates, we propose a novel two-phase anomaly detection mechanism with extra safeguards against potential privacy breaches by Byzantine clients during the detection process. Additionally, we introduce a clipping technique to bolster defenses against diverse poisoning attacks.\n\u2022 We carried out comprehensive experiments on multiple model poisoning attacks and federated learning scenarios, covering both cross-device FL and cross-silo FL. Our experiments with publicly accessible datasets demonstrate DDFed's effectiveness in safeguarding model privacy and robustly defending against model poisoning threats."}, {"title": "2 Related Works", "content": "Privacy Risks and Countermeasures in FL The fundamental design of FL ensures that all training data stays with its owner, offering basic privacy. However, it still exposes vulnerabilities to inference attacks, which allow adversaries to extract information about the training data used by each party [24, 27, 2, 24, 14, 11]. In some cases, the risk of private information leakage may be unacceptable. Therefore, several defenses have been suggested to mitigate these risks, including differential privacy (DP) and secure aggregation (SA), based on various cryptographic primitives such as (partial) homomorphic encryption [21, 41], threshold Paillier [30], functional encryption [36], and pairwise masking protocols [6].\nPoisoning Risks and Countermeasures in FL. Besides privacy inference attacks, FL is also susceptible to poisoning attacks, where adversaries can compromise certain clients and manipulate their data or models to intentionally worsen the global model's performance by introducing corrupted updates during training. This paper focuses on untargeted model attacks, whose goal is to significantly diminish the effectiveness of the global model through methods such as Inner Product Manipulation (IPM) attack [34], scaling attack[1], and \u201ca little is enough\" (ALIE) attack [3]. Several strategies have been developed to counteract the impact of attacks, ensuring they don't compromise model performance. These strategies fall into two categories: client-side and server-side defenses. Client-side defenses adjust the local training algorithm with a focus on secure client updates[28], whereas server-side defenses [5, 29, 38, 40] either reduce the influence of updates from malicious clients through adjusted aggregation weights or use clustering techniques to aggregate updates from trustworthy clients only. However, these defense strategies typically operate under the assumption that model updates are not encrypted, which contradicts the objectives of privacy-focused secure aggregation defense strategies.\nPrivate and Robust Federated Learning. In privacy-preserving FL, identifying poisoning attacks is harder because of the need to balance local model privacy with the detection of harmful models. Only a few existing studies like those mentioned in [39, 13, 16, 15] employ Byzantine-resilient aggregation through differential-privacy techniques. This approach necessitates a compromise between privacy and model accuracy. Additionally, recent initiatives have been launched to address this problem through diverse methods by using various secure computation technologies. These include 3PC[9], which faces scalability limitations; an oblivious random grouping method constrained by its design for partial parameter disclosure[43]; and both additive secret sharing[20] and two-trapdoor homomorphic encryption[23], which depend on the impractical assumption of non-colluding dual servers."}, {"title": "3 Dual Defense Federated Learning Framework", "content": "3.1 Formulation and Assumption\nFormulation. A typical FL framework involves m clients, $C_1, ..., C_m$, and a single aggregation server A. Each client $C_i$ possesses its own dataset $D_i$. The overarching goal in FL across these m clients is to minimize the global objective function:\n$\\min_{W_1,...,W_m} \\frac{1}{m} \\sum_{i=1}^{m} \\frac{1}{|D_i|} L_i (W_i; D_i).$ \nHere, $L_i$ represents the local loss function for each client's data, and $W_i$ are the local model parameters specific to client $C_i$. The term $D_i$ refers to the private dataset of client i, with $|D_i|$ indicating its size in terms of sample count. In short, the goal of general FL is to learn an optimal global model $W_G$ across m clients. This is achieved by periodically synchronizing the model parameters from all clients using specified fusion algorithms like FedAvg and its variants, with the aggregation server A over several training rounds.\nDue to various malicious activities, including inference attacks that aim to steal private information from legitimate clients and poisoning attacks designed to undermine model integrity by degrading its performance, existing privacy-preserving FL often relies on a secure aggregation mechanism[21, 41, 30, 36, 6]. Typically, without loss of generality, during the t-th federated learning training round, each client $C_i$ secures its local model update $W_i^{(t)}$ - referred to as $[W_i^{(t)}]$ throughout this paper- before transmitting it to the aggregation server. This is achieved by using various privacy-enhancing technologies such as homomorphic encryption and secure multi-party computation.\nThreat Assumption. DDFed tolerates an adversary, capable of corrupting any subset of local clients at a specified ratio $P_{ATTACK}, s.t., r_{attack} < 0.5$, to carry out model poisoning attacks that degrade the global model's performance. Additionally, we assume the aggregation server A is semi-honest (honest-but-curious), meaning it adheres to the protocol but seeks to glean as much private information as possible. Similarly, the compromised clients $C_{ADV}$ can conduct privacy inference attacks like those performed by A. In summary, regarding privacy preservation, both the inquisitive A and the corrupted client subset aim to extract private information from benign clients; however, only the corrupted client subset will also initiate model poisoning attacks to undermine the global model."}, {"title": "3.2 Framework Details", "content": "Objective of DDFed. DDFed is designed to bolster privacy protection and mitigate model poisoning attacks seamlessly within the existing FL framework. Unlike existing private and robust approaches [39, 13, 16, 43, 15, 23, 9, 20] that add new participant roles or depend on differential privacy, which may compromise model performance, DDFed maintains effectiveness efficiently. DDFed introduces a dual defense strategy that combines fully homomorphic encryption (FHE) for secure data aggregation with an optimized similarity-based mechanism to detect malicious models, ensuring unparalleled privacy protection and security against model poisoning attacks.\nSimilarity-based methods are commonly used in existing studies for anomaly detection models [29, 38]. Specifically, it computes the cosine similarity between each local model update of training round t and the global model from the previous round t \u2013 1:\n$cos(a_i) = \\frac{<W_i^{(t)}, W_G^{(t-1)}>}{||W_i^{(t)}||_2 \\cdot ||W_G^{(t-1)}||_2}$ \nwhere $a_i$ denotes the angle between global model weights $W_G^{(t-1)}$ and local model update $W_i^{(t)}$ of client $C_i$. However, existing similarity-based mechanisms [29, 38] offer no privacy protection for local model updates, and integrating FHE into them poses significant challenges. These challenges arise from FHE's limitations in performing division and comparison operations, which are essential for identifying benign clients in these methods.\nFramework Overview and Training Process. Figure 1 provides an overview of DDFed framework, which includes several clients $C_1,..., C_m$ and a single aggregation server A, consistent with the architecture of most existing FL frameworks. In the following section, we demonstrate the DDFed training process. Due to space limitations, the formal algorithm pseudocode is provided solely in Appendix A.1.\nBefore the FL training begins, each client $C_i$ is equipped with an FHE key pair (PK, SK). During the FL training phase, let's assume that in the t-th round, each client $C_i$ trains a local model $W_i^{(t)}$ and performs the normalization and encryption as $[W_i^{(t)}] = FHE.ENC_{PK}(W_i^{(t)})$ with public key PK (\u2461). Upon receiving encrypted local models, $\\{[W_i^{(t)}]\\}_{i\\in[1,...,m]}$, A starts to detect anomaly model updates over all encrypted local models. Specifically, A first extracts the last layer, denoted as $\\{[W_i'^{(t)}]\\}_{i\\in[1,...,m]}$, which remains encrypted (\u2462), and adds a perturbation $\\triangle^{(t)}$ to safeguard against potential privacy attacks by malicious clients. Next, it retrieves the last layer of the encrypted global model from the previous training round $([W_G'^{(t-1)}])$. The method for adding perturbations will be discussed in Section 3.2. Then, A performs secure inner-product between each perturbed $[W_i'^{(t)}] + \\triangle^{(t)}$ and $[W_G'^{(t-1)}]$ to derive encrypted similarity score, denoted as $[s'^{(t)}] = ([s_1^{(t)}], ..., [s_m^{(t)}])$, and query each client (\u2463). After receiving $[s'^{(t)}]$, each client $C_i$ decrypts it to obtain the plaintext scores $s_i^{(t)}$. Subsequently, each client submits their list of similarity scores (\u2464). It's important to note that at this stage, malicious clients may tamper with their similarity scores in an attempt to prevent detection of their compromised models. Since a benign client will honestly and accurately decrypt and select trustworthy clients group via threshold-based filter, and hence their results should be consistent. Therefore, A uses a majority voting strategy to acquire the final client score list, i.e., the voted $s^{(t)}$ (\u2465). Next, A normalizes $s^{(t)}$ and generates the fusion weight (\u2466). Here, DDFed employs FedAvg's approach by weighting the aggregation according to dataset size proportions in current training round (\u2467). Finally, each client $C_i$ receives the aggregated global model $[W_G^{(t)}]$, decrypts it, and initiates the (t + 1)-th round of DDFed training (\u2468).\nPrivate and Robust Malicious Model Detection. As observed in [38], the distribution of local data labels can be more effectively represented in the weights of the last layer than in other layers. Consequently, DDFed employs a similar approach to enhance the efficiency of detecting anomalies, as it requires performing similarity computation on encrypted model updates. Given that FHE supports only basic mathematical operations, and the similarity-based anomaly model detection mechanism needs complex operations like division (as shown in equation 2), comparison and sorting operations, DDFed breaks it down into two stages: secure similarity computation and feedback-driven collaborative selection. In the rest of the paper and during our experimental evaluation, we adhere to the layer section settings described in [38]. However, DDFed can be easily extended to support strategies for detecting malicious models using full layers. Additional experiments are detailed in Appendix A.2.4 to demonstrate the impact of layer sections on the DDFed framework.\nSecure Similarity Computation. To circumvent division operations, DDFed necessitates that all clients pre-process their inputs for normalization and shifts the task of comparing similarity scores to the client side. This is because clients possess the FHE private key, allowing them to obtain the similarity score in plaintext. Formally, we have the following:\n$[cos(a_i)] = \\frac{<[W_i^{(t)}], [W_G^{(t-1)}]>}{||[W_i^{(t)}]||_2 \\cdot ||[W_G^{(t-1)}]||_2} = {<[\\frac{W_i^{(t)}}{||W_i^{(t)}||_2}], [\\frac{W_G^{(t-1)}}{||W_G^{(t-1)}||_2}]>}$ \nwhere each client $C_i$ prepares the $\\frac{W_i^{(t)}}{||W_i^{(t)}||_2}$ and $\\frac{W_G^{(t-1)}}{||W_G^{(t-1)}||_2}$ in advance, and then encrypts them using FHE encryption algorithm. Next, the aggregation server S verifies received $[\\{\\frac{W_G^{(t-1)}}{||W_G^{(t-1)}||_2}\\}}]$ and perturbs local inputs and conducts secure inner-product computation as follows:\n$[s'^{(t)}] = <[\\frac{W_i^{(t)}}{||W_i^{(t)}||_2}] + \\triangle^{(t)}, [\\frac{W_G^{(t-1)}}{||W_G^{(t-1)}||_2}]>.$ \nMotivation of Similarity Score Perturbation. DDFed aims to simultaneously address privacy and poisoning risks. This means it not only considers model poisoning attacks but also prevents adversarial clients from inferring private information from other benign clients by exploiting decrypted similarity scores and previous global models. To mitigate this privacy risk, DDFed improves secure inner-product computation by introducing perturbations into each normalized and encrypted model update. Specifically, DDFed uses (\u03b5, \u03b4)-differential privacy with a Gaussian mechanism as its method of perturbation, $\\triangle^{(t)} = N(0, \u03c3^2), \u03c3 = A_f\\sqrt{2\\ln(1.25/\u03b4)}/\u03b5$, where (\u03b5, \u03b4) represents the parameters of the DP mechanism and $A_f$ denotes sensitivity.\nIt's important to note that our perturbation affects only the anomaly detection phase and does not change the encrypted model updates that are to be aggregated. Consequently, the final aggregated model retains its accuracy, just as it would with a standard aggregation mechanism. Furthermore, our experiments indicate that the perturbation noise does not affect the effectiveness of anomaly detection."}, {"title": "3.3 Analysis on Privacy and Robustness", "content": "Even at \u025b = 0.01, which offers strong privacy protection, DDFed still performs well and delivers good model performance.\nFeedback-driven Collaborative Selection. As shown in the threat model, DDFed tolerates less than 50% malicious clients, indicating that over half of the clients are benign and will execute the steps honestly and correctly as designed. DDFed employs a feedback-driven collaborative selection approach to filter out potentially malicious models. Specifically, upon receiving the encrypted $[[s'^{(t)}]]$, each client $C_i$ first decrypts to acquire $s_i^{(t)}$ using the FHE private key sk. Next, each client $C_i$ independently decrypts the similarity scores, sorts them, and selects trustworthy clients $s_i^{(t)}$ for the current training round based on a threshold. DDFed uses only the mean value of similarity scores as its filtering threshold. Subsequent experiments have demonstrated its effectiveness. Additionally, DDFed is open and compatible with alternative methods for setting thresholds. After each client returns their decision on the group of benign clients ($(s_i^{(t)})$), the aggregation server uses a majority of vote strategy to decide the final aggregation group ($s^{(t)}$) for the current training round. Next, similar to FedAvg, DDFed applies a data size-based fusion weight strategy to calculate each client's fusion weight $fw_i^{(t)}$ in the aggregation group, where $fw_i^{(t)} = \\frac{D_j}{\\sum_{i \\in s^{(t)}} D_j}$ .\nFHE-based Secure Aggregation with Clipping. DDFed's secure aggregation leverages the FHE cryptosystem, specifically the CKKS instance[8], which excels in arithmetic operations on encrypted real or complex numbers and stands as one of the most efficient methods for computing with encrypted data. Formally, the aggregation server performs secure aggregation as $[W_G^{(t)}] = [\\sum_i([W_i^{(t)}], fw_i^{(t)})]$.\nOnce receiving the aggregated global model $[W_G^{(t)}]$, each client $C_i$ uses their private key to decrypt it, obtaining the final global model $W_G^{(t)}$ in plaintext via the FHE decryption algorithm. In contrast to current approaches in private and robust FL, DDFed uniquely enables each benign client to execute a clipping operation before the next training round. This enhancement is designed to counteract more sophisticated model poisoning attacks that conventional similarity-based methods [29, 38] fail to address, as will be shown in the experiments section.\nBased on the threat model discussed earlier, DDFed prevents an honest-but-curious aggregation server from potentially inferring private information from accessible model updates. Additionally, it also withstands a subset of local clients, compromised by an adversary, to launch model poisoning attacks and attempt to infer private information from other benign clients during the anomaly model detection phase.\nIn terms of privacy risks, DDFed utilizes FHE primitives to ensure cryptographic-level privacy protection. This means the aggregation server processes each operation without any insight into the model update (in the dark), eliminating any chance of inferring private information from local model updates. Furthermore, to counter potential inferences by corrupted clients exploiting decrypted similarity scores, DDFed incorporates a perturbation method where DP noise is added during the secure similarity computation phase. Due to space limitations, the formal DP-enhanced perturbation analysis is provided solely in Appendix A.3.\nRegarding the risk of poisoning attacks, DDFed adopts similarity-based anomaly detection technologies with additional optimizations such as perturbation-based similarity computation and post-aggregation clipping. These enhancements bolster the robustness of its aggregation mechanism. Our experiments demonstrate that DDFed effectively resists a range of continuous poisoning attacks, including IPM, SCALING, and ALIE attacks, which will be elaborated in Section 4."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets and Implementation. We assessed our proposed DDFed framework using publicly available benchmark datasets: MNIST[19], a collection of handwritten digits, and Fashion-MNIST (FMNIST)[33], which includes images of various clothing items, offering a more challenging and diverse dataset for federated learning tasks. We create non-iid partitions for all datasets based on previous research [38, 43], using a default q value of 0.5, where a higher q reflects greater degrees of non-iid. We assess the framework's performance using a nine-layer CNN model with 225k parameters, secured by the FHE cryptosystem in each training round. This secure aggregation is implemented through TenSEAL library [4]. The experimental DDFed is available on the GitHub repository.\nBaselines and Default Setting. We compare our proposed method DDFed with well-known FL fusion algorithms and robust aggregation methods, including Krum [5], Cos Defense [38], and median/mean-based approaches like Median, Clipping Median, and Trimmed Mean strategies[40]. We exclude baselines such as FLTrust[7] or RFFL[37] because they require server-side validation data or are incompatible with client sampling, making them impractical for real-world applications. Additionally, we omit secure robust approaches[9, 20, 23, 43] that depend on complex secure aggregation techniques due to their requirement for additional non-colluding participants, which alters the original structure of the federated learning framework. Note that the core contribution of this paper is not to propose new model poisoning defense approaches, but to enhance existing popular defenses with privacy features-specifically, server-side similarity-based defenses. Therefore, the experiments aim to evaluate how these privacy-preserving features affect the original defense methods, rather than defending against recent attack techniques and strategies as shown in works like [26, 31, 42, 10, 25].\nTo assess defense performance, we evaluated the proposed work against popular model poisoning attacks: Inner Product Manipulation (IPM) attack [34], scaling attack[1], and the \"a little is enough\" (ALIE) attack[3]. Unless otherwise mentioned, we assume a default attacker ratio of 0.3 among all participants as malicious clients. The attacks commence at the 50th round and persist until training ends. The default FL training involves 10 clients randomly chosen from 100 for each communication round. Furthermore, we employ a batch size of 64 with each client conducting local training over three epochs per round using an SGD optimizer with a momentum of 0.9 and a learning rate of 0.01. Our DDFed implementation's default epsilon (\u03b5) value is set to 0.01 unless specified differently."}, {"title": "4.2 Performance Evaluation", "content": "Performance of Defense Effectiveness under Various Attacks. Figure 2 demonstrates the effectiveness of our DDFed method compared to baseline methods in countering three prevalent model poisoning attacks, with an attacker ratio set at 0.3. The attack commences at the 50th round and continues until training concludes. Under the IPM attack scenario, aside from FedAvg, Trimmed Mean, and Clipping Median mechanisms, our approach along with other defense strategies performs well (nearly as model accuracy as without any model poisoning attack) in defending against the IPM attack. The same conclusion also holds true in the ALIE attack. However, only DDFed and Clip Median successfully withstand SCALING attacks with minor and acceptable losses in model performance. Note that DDFed remains robust even when attackers target the system from the start of training. Due to space constraints, we present the defense effectiveness against various cold-start attacks in Appendix A.2.3. In summary, our DDFed method achieves the best comprehensive defense performance.\nImpact of Attacker Ratio. To further investigate the impact of attacker ratio in the DDFed framework, we conducted experiments with various attacker ratio settings. It's important to note that DDFed operates under the security assumption that at least half of the participants must be benign (i.e., $r_{attacker} < 0.5$), therefore, in our experiments, the attacker ratio setting is ranged from 0.1 to 0.4. As shown in Figure 3, the proportion of attackers among all clients does not significantly affect our proposed DDFed method. This suggests that it can effectively counter three types of model poisoning attacks. Additionally, we observed that under an ALIE attack scenario, our method may require approximately 10-20 training rounds to recover from the continuous attack, depending on the dataset evaluated.\nCompatibility with Cross-device and Cross-silo FL Scenarios. To explore how the number of clients affects our DDFed framework and to confirm its compatibility with two common federated learning scenarios, i.e., cross-device and cross-silo, we conducted multiple experiments. These experiments had an attacker ratio fixed at 0.3, with client counts varying from 10 to 100. In cross-silo FL, client numbers are typically small, often ranging from a few to several dozen; however, for simulating the cross-device FL scenario in our study, we used 100 clients due to their generally larger population. As illustrated in Figure 4, our DDFed framework effectively defends against all three attacks across various client number settings. This suggests that the performance of DDFed is not significantly affected by the number of clients, indicating its suitability for both cross-silo and cross-device FL scenarios. Furthermore, a higher number of client settings may result in relatively large fluctuations during training rounds immediately following the attack; however, the model training ultimately converges steadily, unaffected by the continuous attack."}, {"title": "4.3 Discussion and Limitation", "content": "To the best of our knowledge, DDFed offers a dual defense strategy that simultaneously boosts privacy protection and fights against poisoning attacks in FL, without altering the existing FL framework's architecture. DDFed utilizes FHE for top-notch privacy, enabling the aggregation server to perform similarity calculations and aggregation without directly accessing model updates. Additionally, DDFed introduces perturbation techniques to block attempts by malicious clients to infer information from similarity scores. It further employs similarity-based anomaly detection, enhanced with strategies like perturbation and post-aggregation clipping, to protect against various types of poisoning attacks. However, DDFed has not fully explored two related questions: how can we relax the attacker ratio restriction (i.e., $r_{ATTACK} < 0.5$) while still ensuring effective dual defense? And how can we adapt DDFed to more complex FL scenarios, such as dropout and dynamic participant groups? We leave these questions open for future research. Currently, DDFed only enhances existing popular defenses, such as similarity-based strategies with privacy features. Extending DDFed to support other or more recent defense strategies remains an open question."}, {"title": "5 Conclusion", "content": "To tackle the dual challenges of privacy risks and model poisoning in federated learning, we introduce DDFed, a comprehensive approach that strengthens privacy protections and counters model poisoning attacks. DDFed enhances privacy by using an FHE-based secure aggregation mechanism and addresses encrypted poisoned model detection through an innovative secure similarity-based anomaly filtering method. This method includes secure similarity computation with perturbation and feedback-driven selection process to distinguish safe model updates from potentially harmful ones. Our approach has been rigorously tested against well-known attacks on diverse datasets, demonstrating its effectiveness. We believe our work sets a solid foundation for future advancements in secure and robust federated learning."}, {"title": "A.1 DDFed Algorithm", "content": ""}, {"title": "A.2 Additional Experimental Results", "content": "A.2.1 Impact of Epsilon with 100 Clients\nFigure 6 presents the experimental findings on how different & values affect perturbations during the secure similarity computation phase, with experiments focusing on an IPM attack scenario and involving 100 clients. These tests were carried out using the MNIST and Fashion-MNIST (FMNIST) datasets. For both datasets, we explored a range of epsilon values from 0.01 to 0.1, noting that lower epsilon values indicate enhanced privacy through increased noise addition. Initially, all configurations demonstrated high accuracy levels; however, performance fluctuations became evident following the attack. Specifically, the MNIST dataset exhibited a notable decrease in accuracy at certain epsilon settings, while the FMNIST dataset showed more moderate variations in performance. Ultimately, both datasets achieved relatively stable model accuracy. Determining the optimal & setting is task-dependent and remains an area for future investigation.\nA.2.2 Time Cost of Secure Aggregation on Scaling and ALIE attacks.\nTable 2 and Table 3 report additional results on the time cost of each training round taken for various defense strategies against SCALING and ALIE attacks on the MNIST and FMNIST datasets, respectively. Consistent with the findings presented in Section 4.2, our DDFed approach adds only 2 seconds to the usual 10-second training round across multiple experiments, datasets, and attack scenarios, resulting in a 20% increase in time per round. Despite this slight increase, DDFed successfully defends against model poisoning attacks while ensuring robust privacy protection.\nA.2.3 Performance of DDFed Against Cold-Start Model Poisoning Attacks.\nThe primary purpose that we initiated the attack at round 50 is to demonstrate the effectiveness of defense mechanisms and clearly show the comparative effects of different defense methods before and after an attack. This setup can also illustrate how various defensive measures impact training convergence and model quality, even without attacks.\nDDFed is resilient to poisoning attacks from the beginning of training. Our design is not constrained by the attack's initiation round. Supplementary experimental results as reported in Table 4 on the FMNIST dataset with 100 clients in a non-iid setting support this claim.\nA.2.4 Impact of Selected Layer Count on Poisoning Model Detection in DDFed.\nIn the main body of the paper, we use only the last layer for similarity computation because our primary goal is to integrate privacy-preserving functionality into existing poisoning defense strategies rather than optimizing these mechanisms. Our exploration shows that similarity-based methods and their variants provide comprehensive defense effectiveness, robust against various threat scenarios such as server reliance on validation data, types of model poisoning attacks, and the number of compromised clients. Therefore, we selected a typical similarity-based defense strategy (Cosine Defense) as a starting point to enhance privacy-preserving features. Our approach can easily extend to other similarity-based detection variants using full layers for secure similarity computation. As"}, {"title": "A.3 Differential Privacy", "content": "A.3.1 Differential Privacy\nDifferential privacy is a mathematical framework designed to provide privacy guarantees for individ-uals in a dataset. The standard definition of differential privacy is as follows:\nA randomized algorithm M is said to be (\u03b5, \u03b4)-differentially private if, for any two adjacent datasets D and D' (i.e., datasets differing by only one element), and for any subset of outputs SC Range(M), the following inequality holds:\n$Pr[M(D) \\in S] < e^{\\epsilon} Pr[M(D') \\in S] + \u03b4$ \nwhere &epsiolon is the privacy budget parameter, which controls the trade-off between privacy and utility. A smaller &epsiolon indicates stronger privacy. \u03b4 (delta) is a small probability that accounts for the possibility of the privacy guarantee being violated.\nThe Gaussian mechanism is a specific method to achieve differential privacy by adding Gaussian noise to the output of a function. The definition of the Gaussian mechanism is as follows:\nGiven a function f and any two adjacent datasets D and D', the sensitivity of f is defined as:\n$A_f = \\max_{D,D'} ||f(D) \u2212 f(D')||_2$"}, {"title": "A.3.2 Privacy Analysis of Differentially Private Similarity Computation in DDFed", "content": "The DDFed framework aims to enhance privacy protection and mitigate poisoning attacks within federated learning systems by integrating FHE and a similarity-based anomaly detection system. To further bolster privacy, DDFed incorporates DP during the similarity score computation process. This section provides a theoretical analysis of the differential privacy levels maintained by each participant in the DDFed framework, specifically focusing on clients during similarity score computation and feedback stages, as well as the aggregation server during model aggregation and similarity score processing.\nIn the similarity score computation phase, each client normalizes its local model updates before submitting them. To ensure DP, Gaussian noise is added to these normalized updates. By adding Gaussian noise, each client's similarity score computation adheres to (\u03b5, \u03b4)-differential privacy, ensuring that the privacy of the client's data is preserved even in the presence of adversaries.\nDuring the feedback phase, clients decrypt the similarity scores and submit their results. Since these scores have already been DP due to the added Gaussian noise, the privacy level remains at (\u03b5, \u03b4)-differential privacy. This ensures that even when clients provide feedback, their privacy is not compromised.\nIn the model aggregation phase, the aggregation server receives encrypted model updates from clients. While FHE inherently provides a high level of security for these crucial parameters, the aggregation server further ensures privacy by applying DP during the similarity score calculation. The server aggregates the encrypted updates without accessing the plaintext data, thereby maintaining the privacy of the individual model updates.\nFor the similarity score processing phase, the aggregation server handles the scores submitted by clients, which have already been protected using differential privacy. Consequently, the server does not need to apply additional privacy mechanisms during this phase. The DP guarantees provided during the similarity score computation phase by clients are sufficient to protect the overall process.\nBased on the analysis, the privacy levels for each client in DDFed framework can be summarized as follows. During the similarity score computation phase, clients achieve (\u03b5, \u03b4)-differential privacy by adding Gaussian noise to their normalized model updates. During the feedback phase, clients maintain (\u03b5, \u03b4)-differential privacy as the similarity scores they submit have already been differential private.\nBy thoughtfully designing and selecting parameters, the DDFed framework can provide robust privacy protection and maintain high model performance. The use of FHE for critical parameters and differential privacy for similarity scores ensures a balanced and comprehensive approach to privacy protection, addressing both security and utility needs effectively."}, {"title": "A.3.3 Impact of DP on FHE-based Similarity Computation in DDFed", "content": "Generally, the reader may concern about whether [[x"}]}