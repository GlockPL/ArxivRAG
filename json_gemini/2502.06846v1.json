{"title": "Prot2Chat: Protein LLM with Early Fusion of Sequence and Structure", "authors": ["Zhicong Wang", "Zicheng Ma", "Ziqiang Cao", "Changlong Zhou", "Jun Zhang", "Yiqin Gao"], "abstract": "Proteins play a pivotal role in living organisms, yet understanding their functions presents significant challenges, including the limited flexibility of classification-based methods, the inability to effectively leverage spatial structural information, and the lack of systematic evaluation metrics for protein Q&A systems. To address these limitations, we propose Prot2Chat, a novel framework that integrates multimodal protein representations with natural language through a unified module, enabling large language model (LLM)-driven answer generation. Our model incorporates a modified Protein-MPNN encoder, which encodes protein sequence and structural information in a unified manner, a protein-text adapter with cross-attention mechanisms, and a LLaMA3 decoder. To optimize training efficiency, we freeze the encoder and employ LORA techniques for the decoder. We conducted experiments on two datasets, both automated metrics and expert evaluations demonstrate the superior performance of our model. Furthermore, zero-shot prediction results highlight its strong generalization capabilities. This framework offers a promising solution for bridging protein domain knowledge with natural language understanding, paving the way for transformative advancements in protein-related research.", "sections": [{"title": "1 Introduction", "content": "With the continuous advancement of biotechnology, gaining a deeper understanding of protein sequences, structures, and their functions has become increasingly important. To elucidate the complex mechanisms underlying protein-mediated physiological processes, researchers have focused on studying and predicting the physicochemical properties of proteins. The widespread availability of protein sequences and structures, enabled by novel high-throughput sequencing technologies and advanced structure prediction tools [Jumper et al., 2021; Abramson et al., 2024], has greatly facilitated the exploration of intricate relationships between protein sequences, structures, and functions. In this context, protein Q&A systems have emerged as a research hotspot, aiming to help scientists and researchers efficiently access information about protein properties and functions [Zhou et al., 2023]. These systems are of considerable significance in fields such as drug discovery and disease research. Besides, just as some proteins mentioned in AF-Cluster [Wayment-Steele et al., 2024], it indicates that even if the protein sequences are the same, they may present different 3D structures due to different environmental conditions, binding partners, or point mutations, thereby performing different biological functions.\nTraditional approaches, such as those based on Gene Ontology (GO) classifications, predict protein functions by leveraging sequence information through convolutional models or sequence similarity-based computations, as exemplified by methods like DeepGOPlus [Kulmanov and Hoehndorf, 2020]. Such models primarily utilize protein sequence information and are limited to classification tasks, which oversimplifies the complexity of protein functions. The recent rise of large language models (LLMs) has propelled protein Q&A systems into a cutting-edge research domain, allowing scientists to efficiently access and interpret critical information about protein properties and functions [Zhou et al., 2023]. These systems hold immense potential for transformative applications in areas such as drug discovery and disease research. To support this growing field, several datasets, including Mol-Instructions [Fang et al., 2023] and UniProtQA [Luo et al., 2023], have been developed, providing essential resources for advancing protein Q&A research.\nAn increasing number of models have fine-tuned LLMs using protein-specific Q&A datasets to generate contextually relevant text or answer questions related to protein structure and function. However, as highlighted in Table 1, existing approaches often face significant limitations, including a lack of multimodal information integration, limited flexibility, and high computational costs. These shortcomings restrict their ability to provide precise and contextually appropriate responses. For instance, FAPM [Xiang et al., 2024] and InstructProtein [Wang et al., 2023], despite incorporating generative Q&A functionalities, remain heavily influenced by classification-based frameworks. This often results in the generation of extraneous or irrelevant information, deviating from the intended scope of the answers and limiting their utility in addressing complex protein-related queries. BiomedGPT [Luo et al., 2023] introduces a generative framework tailored for protein Q&A tasks but relies solely on sequence information, which significantly limits its predictive capabilities.\nWe compared the generation effects of large language models trained with protein structures and sequences, respectively, on different types of questions, as shown in Figure 1. The results show that when answering questions related to protein subcellular localization and domains or motifs, the generation effect of the model jointly trained with protein structures and sequences is significantly better than that trained only with protein sequences. Some following works such as ProtChatGPT [Wang et al., 2024] and Evola [Zhou et al., 2025] have tried to employ two separate modules for encoding sequence and structural information. However, such practice makes it difficult for protein sequence and structural features to interact, and significantly increases computational complexity. Moreover, no existing model has systematically examined the relative contributions of sequence and structural information to the performance of protein Q&A systems. This gap underscores the critical need for future research to evaluate how integrating both sequence and structural data can enhance the accuracy, robustness, and interpretability of protein-related Q&A.\nAs the sequence and structural features of proteins are one-to-one correspondence, we propose a novel framework called Prot2Chat that seamlessly incorporates the spatial structure and sequence information into LLMs. This approach seeks to enhance both the performance and applicability of protein Q&A systems. As illustrated in Figure 2, the overall architecture of our model comprises three primary components: the protein encoder, the protein-to-text adapter, and the large language model. Specifically, we extended the structure encoder ProteinMPNN [Dauparas et al., 2022] to fuse sequence information during node initialization. The sequence features we introduced are inherent to the model, thus eliminating the need for additional training parameters. Then following the methodology of InstructPLM [Qiu et al., 2024], a lightweight cross-attention layer was incorporated to align the protein encoder with a LoRA [Hu et al., 2022] fine-tuned LLM. This approach preserves the primary sequence and structural information of proteins during the compression process. The processed protein structure and sequence data are provided as a soft prompt to the LLM. Leveraging the flexible text generation capabilities of the LLM, our model effectively follows residue-level protein instructions, significantly enhancing its utility in protein-related tasks.\nTo evaluate the model's performance, we conducted comparative experiments on the Mol-Instructions and UniprotQA datasets. And we conducted zero-shot experiments using a part of the UniprotQA dataset to verify the generalization ability of the model. The generated answers were assessed using common metrics such as BLEU and ROUGE, as well as online KIMI [Qin et al., 2024] scoring metrics and expert evaluations, ensuring the reliability and relevance of the outputs. Experimental results demonstrate that the model employing a unified encoder to integrate protein sequence and structural information outperforms those using sequence-only pretraining or ESM-based encoders [Lin et al., 2022]. Furthermore, the high consistency between expert evaluations and online KIMI assessments confirms the robustness of our model.\nIn summary, our contributions are as follows:\n1.  We extended the existing structure encoder Protein-MPNN to realize structure and sequence early fusion without the need for training.\n2.  Based on the protein encoder, we achieved a lightweight and effective protein large language model with only 93M training parameters.\n3.  We performed various systematic assessments across various evaluation datasets to validate our model's generative and generalization capabilities. These included traditional metrics, large model evaluations online, and manual expert evaluations."}, {"title": "2 Related work", "content": "Protein Representation Learning\nProteins are fundamental components of cells, essential for their biological activities and diverse functions. Previous studies on protein characterization have explored various methods to learn protein representations based on different forms of protein information. Protein sequences, often referred to as the \"language of life,\u201d have been extensively studied using advanced natural language processing techniques. For instance, Tranception [Notin et al., 2022] employs the Transformer [Vaswani, 2017] model to encode amino acid sequences, capturing relationships between residues and autoregressively reconstructing protein sequences from large-scale databases. Similarly, sequential modeling approaches such as ESM [Rives et al., 2021; Lin et al., 2023; Hayes et al., 2024] leverage masked language modeling (MLM) to develop attention patterns that correspond to residue-residue contact maps, enhancing sequence-based protein representations. On the other hand, structure-based approaches [Gligorijevi\u0107 et al., 2021; Achiam et al., 2023] directly indicates protein function and encodes geometric information of the protein for topology-sensitive tasks such as protein property prediction. Foldseek [van Kempen et al., 2022] introduces the concept of a structural alphabet, encoding protein structures into a discrete representation space. Similarly, Saprot [Su et al., 2023] introduces a structure-aware vocabulary, embedding structural information into model inputs to enhance representational capacity, achieving significant success in protein function prediction tasks.\nMulti-Modal Alignment\nEnabling large language models (LLMs) to understand additional modalities has been a rapidly evolving research direction, with notable examples including image-text models [Tsimpoukelli et al., 2021; Li et al., 2023], video-text models like VideoLlama [Zhang et al., 2023], audio-text models such as Macaw-LLM [Lyu et al., 2023], and molecular-text models, including ReLM [Shi et al., 2023], and MolTC [Fang et al., 2024]. This line of research was pioneered by advancements in visual language modeling (VLM), which has been successfully applied to tasks such as few-shot image classification, image captioning, and image Q&A [Li et al., 2023; Alayrac et al., 2022]. To enable LLMs to understand images, leading VLM methods adopt different strategies. Some, like BLIP-2 [Li et al., 2023], employ nonlinear and expressive cross-modal projectors, while others, such as PaLM-E [Driess et al., 2023], utilize visual encoders and fine-tune LLMs on multimodal datasets. These approaches have also been increasingly applied to protein analysis and understanding. For instance, Galactica [Taylor et al., 2022] leverages scientific literature to model protein sequences and SMILES representations, enabling the model to interpret sequence properties. ProtNote [Char et al., 2024], utilize two encoders to encode protein sequence information and text information, respectively, and Multilayer Perceptron (MLP) to fuse these inputs, achieving the goal of using text to achieve supervised and zero-shot protein functional prediction. ProteinChat [Guo et al., 2023] further uses the protein structure and the corresponding description to model for the protein Q&A tasks. ProtChatGPT [Wang et al., 2024] uses two independent modules to encode sequence and structural information respectively, and aligns them with natural language to explore protein functions. InstructBioMol [Zhuang et al., 2024] integrates multimodal biomolecular inputs through a feature extraction module, allowing researchers to articulate design goals in natural language. InstructProtein [Wang et al., 2023] employs a supervised training framework based on instruction generation using knowledge graphs, enabling bidirectional generation between natural language and protein language. This model can predict protein functional descriptions and generate protein sequences based on natural language prompts. Prot2Text [Abdine et al., 2024] combines graph neural networks (GNNs) with LLMs to predict protein functions in free-text form. Similarly, ProtT3 [Liu et al., 2024] provides an efficient protein-to-text generation framework by integrating protein language models (PLMs) with LLMs. FAPM [Xiang et al., 2024] utilizes a contrastive learning framework to implement a generative-like Q&A model, while BiomedGPT [Luo et al., 2023] bridges the gap between biological and human natural language through a multimodal generative pre-trained transformer (GPT). This allows users to \"communicate\" with biological modalities using free-text queries, facilitating interactions with biological data in natural language."}, {"title": "3 Method", "content": "3.1 Overview of Prot2Chat\nAs illustrated in Figure 2, our model leverages the 3D coordinate structure information of protein residue atoms along with the protein sequence. The protein encoder processes this information to generate a protein embedding representation, which is then transformed into a text-aligned protein prompt via the protein-text adapter. This prompt, combined with the input question, serves as the input to the large language model (LLM), ultimately producing the output answer.\n3.2 Sequence and Structure Fused Protein Encoder\nWe modified the structure encoder ProteinMPNN to realize the sequence and structure early fusion. ProteinMPNN is originally dedicated to design protein sequences based on backbone structures. The input is structural information E, namely the 3D coordinates of protein residue atoms (N, Ca, C, O). Protein features $h_E$ are obtained through the encoder layer and the decoder layer.\n$h_E = Linear(E)$\n$h_v = Zeros(n, D_c)$\n$h_v, h_E = Encoderlayers(h_v, h_E)$\n$h_v = Decoderlayers(h_v, h_E)$\nwhere Linear, Encoderlayers and Decoderlayers are all modules in ProteinMPNN, E is the structual features and n is the number of protein residues and $D_c$ is the embedding dimension of the protein encoder.\nWe find that a variant version of ProteinMPNN additionally contains the Embedding module for protein sequences S, and the Decoderlayer is sequence-aware, which changes Equation 4 to:\n$h_v = Decoderlayers(h_v, h_E, Embedding(S))$\nTo be specific, the Decoderlayer combines the node embeddings, the output features of the encoder, and the sequence embeddings to construct the context features for decoding through neighborhood feature aggregation. Using the message passing mechanism, the hidden state $h_v$ is continuously updated through multi-layer stacking to simulate the dynamic generation process of the decoder.\nOn this basis, we further initialized $h_v$ with the off-the-shelf sequence embedding to make the sequence and structural information early fused. Specifically, Equation 2 is replaced by:\n$h_v = Embedding(S)$\nIn particular, all of the model weights we used are from ProteinMPNN, with no need for additional training. The final protein node vector $h_v$ is used as the protein feature for the protein-text adapter. Inspired by InstructPLM [Qiu et al., 2024], we concatenate the protein representations encoded by nine released ProteinMPNN models. As a result, $D_c = 128 * 9 = 1152$.\n3.3 Protein-Text Adapter\nWe implemented a protein-text adapter to semantically align the information obtained by the protein encoder with natural language, and provided both types of information as input to the large model simultaneously. The feature $h_v$ obtained from the modified ProteinMPNN model is output through the linear projection layer, positional encoding and the cross-attention mechanism. Specifically, the input protein feature first passes through a linear projection layer $W_{proj}$ to transform the input features to the target output dimension $D_0$:\n$X_{proj} = W_{proj}h_v + PE$\nwhere PE is the Dynamic Positional Encoding [Vaswani, 2017] used to capture the positional information of amino acids.\nThe complete protein encoding is too long. We adopt the idea of BLIP-2 to extract the important semantic features from it. In particular, we introduce $n_q$ learnable queries $Q \\in R^{n_q \\times D_0}$ and also apply PE to them. Then the multi-head attention layer is adapted to capture key protein information based on queries:\n$A_k = softmax(\\frac{Q^kK^{kT}}{\\sqrt{D_k}})V^k$\nwhere $Q^k = QW^Q, K^k = X_{proj}W^K$ and $V^k = X_{proj}W^V$ stand for queries, keys and values. Among them, $W^Q, W^K, W^V$ are related parameters, k is the head index. The multi-head attention outputs are then concatenated across heads and linearly transformed to the final protein prompt:\n$X_{protein} = Concat(A^1, A^2,\u2026\u2026\u2026, A^M)W^{out}$\n3.4 LLM Decoder\nWe combine $X_{protein}$ representing protein information with the text question and input it into the existing LLM to obtain the response.\nresponse = LLM($X_{protein}$, question)\nTo improve domain adaptability, we fine-tuned the LLM with LoRA while training the adapter. The number of adapter training parameters is 89,702,400, while the number of LLaMA3 training parameters is 3,407,872. The total number of training parameters of the Prot2Chat is 93M. Meanwhile, the number of training parameters of BiomedGPT is 3B, that of ProtT3 is approximately 2B, the range of trainable parameters of Prot2Text is from 256M to 898M, and that of FAPM is 188M. This also shows the advantage of our model in significantly reducing the computational cost. Besides, we adpot the CrossEntropyLoss function as common."}, {"title": "4 Experiment", "content": "4.1 Datasets\nWe evaluated the performance of our model using the Mol-Instructions [Fang et al., 2023] and UniProtQA [Luo et al., 2023] datasets. Mol-Instructions is a comprehensive instruction dataset specifically designed for the biomolecular domain, comprising both human-constructed and machine-generated data. For our experiments, we utilized the protein-oriented instruction subset, which is primarily derived from entries in the UniProtKB/Swiss-Prot database [UniProt Consortium, 2018]. This subset encompasses tasks such as predicting protein domains, functions, and activities. Our model was predominantly trained on this dataset.\nAdditionally, we employed a portion of the UniProtQA dataset, introduced by BiomedGPT [Luo et al., 2023], to assess the generalization capability of our model through zero-shot evaluation. Fine-tuning was also performed on this dataset to further validate performance. UniProtQA consists of textual descriptions of proteins, including their functions and properties. It was curated by UniProt [UniProt Consortium, 2018] and includes four types of questions related to protein function, formal name, protein family, and subcellular location. The detailed sizes of our dataset splits are provided in Table 2.\nTo incorporate structural and sequence information, we retrieved the corresponding PDB files for proteins listed in Swiss-Prot. These PDB files, combined with the associated questions, serve as the input to Prot2Chat.\n4.2 Baselines\nWe introduced the following baselines that represent protein information using sequence.\n\u2022  LLaMA3: LLaMA3 [Dubey et al., 2024] series has attracted a lot of research attention due to its outstanding capabilities in the general domain. We use LLaMA3-8B-Instruct and input the protein sequence and question to achieve zero-shot.\n\u2022  LLAMA3-FT: We also fine-tuned LLaMA3-8B-Instruct using textual protein sequence information and used this model as a reference model.\n\u2022  BioMedGPT-10B: BioMedGPT is a domain-specific LLM trained on a large selection of corpora of human scientific knowledge. The model encodes the protein sequence with the ESM-3B [Lin et al., 2022] and uses BioMedGPT-LM-7B as the decoder to generate the response."}, {"title": "4.3 Model Setting", "content": "We jointly trained the protein adapter and LLaMA3 on the Mol-Instructions dataset, employing full training for the adapter and LoRA (Low-Rank Adaptation) fine-tuning for LLaMA3. Inspired by BLIP-2 [Li et al., 2023], we configured the adapter with 256 queries. For the LoRA setup, we set the rank r to 8, LORA alpha to 16, and targeted the \"q_proj\" and \"v_proj\" modules, with a LoRA dropout rate of 0.1. The adapter comprises 89,702,400 trainable parameters, while LLaMA3 has 3,407,872 trainable parameters.\nWe utilized the Adam optimizer and implemented gradient accumulation to optimize the training process. The initial learning rate was set to 10-4, with a batch size of 2 and a maximum context length of 1024 tokens, which includes both the question and answer text. The model was fine-tuned for 2 epochs, with each training session requiring approximately 1600 hours on an NVIDIA RTX 3090 GPU."}, {"title": "4.4 Evaluation Metrics", "content": "To evaluate the effectiveness of the model's text generation, we employed performance metrics including BLEU [Papineni et al., 2002] and ROUGE [Lin, 2004]. Additionally, we utilized the prompt templates shown in Table 3 to simultaneously input the target text and the text generated by different models into the online KIMI model. This allowed us to determine which generated answer was closer to the target text, thereby assisting in the evaluation of the model's output quality. The results, as shown in Table 6, demonstrate that our model outperforms other comparative models overall. Furthermore, we conducted expert manual evaluations, where professional biology PhDs ranked the responses of different models based on their alignment with the target text. The results of these evaluations (Table 7) are consistent with those obtained using KIMI."}, {"title": "4.5 Main Results", "content": "The results of the assessment are shown in Table 4 and Table 5. The results show that there is a significant modal gap between the protein sequence and the natural language, and it is incomprehensible to directly input the protein sequence as text and problem into the large language model, resulting in the disordered and meaningless response of the model. Aligning protein language with human language is an effective solution to solve this problem, and we have observed that the performance of the model trained with the addition of structural information is significantly better than that of other models. This suggests that we can directly use the structure and sequence information of proteins to help language models better understand and generate technical terms through feature space alignment. Establishing a link between proteins and natural language can provide guidance for researchers in their research on unknown proteins. The results of our model's prediction on the UniprotQA [Luo et al., 2023] dataset verify the generalization performance of our model. Further illustrating the role of our model in protein Q&A tasks."}, {"title": "4.6 Ablation Experiments", "content": "As shown in Table 4, we conducted a comparative analysis to evaluate the impact of incorporating protein structure during training, thereby highlighting the importance of structural information for protein understanding. We also examined the effect of fine-tuning LLMs, revealing that training adapters alone was insufficient for achieving optimal performance. Furthermore, by comparing the results obtained from training with sequence text alone versus training with both sequence and structural information, we observed that LLMS struggle to effectively interpret protein sequence information presented in textual form. These findings underscore the necessity of integrating protein structural information alongside a unified multimodal alignment module, which enables LLMs to better understand and generate domain-specific technical terms."}, {"title": "4.7 Case Study", "content": "As shown in Table 8, the results we obtained using the protein Q&A test dataset indicate that the model trained with the addition of protein structure information generates responses significantly better than the model trained with protein sequence. Further prove that it is difficult for LLM to discover the corresponding protein functions and effects only through the text information of protein sequences, and thus draw the conclusion that the protein structure is vital for the comprehensive understanding of proteins."}, {"title": "5 Conclusion", "content": "This paper introduces a novel protein Q&A model that addresses the limitations of traditional protein Q&A tasks by leveraging the modified ProteinMPNN and an adapter module to integrate protein structure and sequence information with natural language text. By harnessing the flexible generation capabilities of large language models (LLMs), our approach effectively bridges the gap between protein data and textual understanding. Experimental results demonstrate that the model trained with structural information significantly outperforms other baseline models, underscoring the critical role of protein structure in protein understanding and analysis. Furthermore, the results validate the effectiveness and strong generalization ability of our model in protein Q&A tasks."}]}