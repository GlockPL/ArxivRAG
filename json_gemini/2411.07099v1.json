{"title": "Bounded Rationality Equilibrium Learning in Mean Field Games", "authors": ["Yannick Eich", "Christian Fabian", "Kai Cui", "Heinz Koeppl"], "abstract": "Mean field games (MFGs) tractably model behavior in large agent populations. The literature on learning MFG equilibria typically focuses on finding Nash equilibria (NE), which assume perfectly rational agents and are hence implausible in many realistic situations. To overcome these limitations, we incorporate bounded rationality into MFGs by leveraging the well-known concept of quantal response equilibria (QRE). Two novel types of MFG QRE enable the modeling of large agent populations where individuals only noisily estimate the true objective. We also introduce a second source of bounded rationality to MFGs by restricting agents' planning horizon. The resulting novel receding horizon (RH) MFGs are combined with QRE and existing approaches to model different aspects of bounded rationality in MFGs. We formally define MFG QRE and RH MFGs and compare them to existing equilibrium concepts such as entropy-regularized NE. Subsequently, we design generalized fixed point iteration and fictitious play algorithms to learn QRE and RH equilibria. After a theoretical analysis, we give different examples to evaluate the capabilities of our learning algorithms and outline practical differences between the equilibrium concepts.", "sections": [{"title": "1. Introduction", "content": "Learning equilibria in multi-agent games is of great practical interest but hard to scale to many agents (Daskalakis et al., 2009; Deng et al., 2023). Mean field games (MFGs) allow scaling to arbitrarily many exchangeable agents at fixed complexity. MFGs are of recent interest as a tractable method to learn approximate equilibria of rational, selfish agents (Guo et al., 2019; Cui & Koeppl, 2021; Xie et al., 2021; Lauri\u00e8re et al., 2022; Anahtarci et al., 2023). Thus, MFGs are applied in various settings ranging from finance to engineering (Djehiche et al., 2017; Achdou et al., 2020; Carmona, 2020).\nA common solution concept in the realm of multi-agent learning, and consequently in the context of MFGs, is the Nash equilibrium (NE). In a NE, each player's strategy is considered optimal, given the strategies of the other agents, resulting in an equilibrium where no agent has an incentive to change their strategies. The optimality notion inherent in NE assumes full rationality of the individual agents.\nHowever, in many real-world situations individuals may not behave perfectly rational due to limited information processing capabilities, psychological factors, social considerations or other factors. Deviations from perfect rationality are described by the fundamental concept of bounded rationality (Simon, 1955; 1979; Kahneman & Tversky, 1982; Selten, 1990; Gigerenzer & Selten, 2002; Kahneman, 2013). Bounded rationality implies that for many real-world scenarios NE are insufficient due to their rigorous perfect rationality assumption. Instead of NE, we require a more realistic equilibrium concept accounting for partially irrational agents.\nA popular game-theoretic approach to modeling bounded rationality of agents are quantal response equilibria (QRE) (McKelvey & Palfrey, 1995; 1998) which are used, e.g. in economics (Breitmoser et al., 2010), robust RL (Reddi et al., 2024) and for efficient NE approximation (Gemp et al., 2024). Intuitively, in a QRE agents perceive rewards perturbed by noise and act optimally with respect to these perturbed rewards. In our work, we extend QRE to the domain of MFGs to model the behavior of a large number of agents who deviate from perfect rationality.\nMeanwhile on the control-theoretic side, a common approximately optimal control method is model predictive control (MPC) (Kouvaritakis & Cannon, 2016), also known as receding horizon control. To further enhance modeling of bounded rationality in MFGs, we incorporate a receding horizon method, where agents make decisions based on a limited future time horizon, reflecting more realistic decision-making processes. In contrast to MPC-based variants of MFGs such as (Inoue et al., 2021), we analyze the resulting novel receding horizon equilibria and instead focus on learning such equilibria, in a discrete-time setting.\nBeyond realism, introducing bounded rationality yields possible tractability advantages. NE computation for MFGs can be hard, motivating the search for alternative equilibrium no-"}, {"title": "2. Equilibria in MFGs", "content": "In this section, we first give a description of finite games in discrete time and their corresponding MFGs. We then define common and new equilibrium notions as solution concepts and desired results of multi-agent equilibrium learning algorithms, which are introduced thereafter. For space reasons, some proofs are in the appendix.\nNotation: Denote by $\\mathcal{P}(X)$ the space of probability measures on finite set $X$, equipped with the $L_1$ norm $||\\cdot||$ unless noted otherwise. Equip products of metric spaces with the sup metric. Further, let $[N] := \\{1, ..., N\\}$ for $N\\in\\mathbb{N}$.", "subsections": [{"title": "2.1. Finite Agent Games", "content": "For the finite $N$-agent game of practical interest, consider agents $i \\in [N]$ endowed with random states $x$ and actions $u$ at all times $t \\in \\mathcal{T} := \\{0, 1, . . ., T-1\\}$ up to time horizon $T\\in \\mathbb{N}$. Let $\\mathcal{X}$ and $\\mathcal{U}$ be the finite state and action spaces for agents, respectively. The empirical mean field (MF) $\\mu_t := \\sum_{i=1}^{N}\\delta_{x_t^i}$ can be understood as a histogram of agent states. Each agent $i$ implements stochastic Markovian policies $\\pi^i \\in \\Pi = \\mathcal{P}(\\mathcal{U})^{\\mathcal{X}\\times\\mathcal{T}}$ depending on the current time and local agent state. For some initial state distribution $\\mu_0$ with $x_0^i \\sim \\mu_0$, for all agents $i$ define state-action dynamics\n$$ u_t^i \\sim \\pi_t^i(u_t^i | x_t^i), x_{t+1}^i \\sim p_t(x_{t+1}^i | x_t^i, u_t^i, \\mu_t) $$"}, {"title": "2.2. Mean Field Games", "content": "MFGs are the limit of finite $N$-agent games with $N \\rightarrow \\infty$ and approximate many-agent finite games well. By a law of large numbers, the empirical MF is essentially replaced by its deterministic limiting MF. The idea of MFGs is to find approximate (symmetric) equilibria, which are otherwise hard to find in finite games with many agents (Deng et al., 2023). MFGs assume all agents to symmetrically play the same policy $\\pi^* \\in \\Pi$ \u2013 the equilibrium solution. Whenever agent $i$ deviates from $\\pi^*$ and instead uses some policy $\\pi$, this corresponds to the policy tuple $(\\pi, \\pi^{-i})$, where $\\pi^{-i} = (\\pi^*, ..., \\pi^*)$ denotes all but the $i$-th policy in the finite game. Hence, in the limit as $N \\rightarrow \\infty$ we have\n$$ u_t \\sim \\pi_t(u_t | x_t), x_{t+1} \\sim p_t(x_{t+1} | x_t, u_t, \\mu_t) $$\nfor the representative deviating agent. Here, the empirical MFs $\\mu$ are replaced by the deterministic limiting MF $\\mu := (\\mu_t)_{t\\in\\mathcal{T}} \\in \\mathcal{M} \\subseteq \\mathcal{P}(\\mathcal{X})^\\mathcal{T}$, given by the probability law $\\mu$ of any other agent playing the assumed equilibrium policy $\\pi^*$. Further, $\\mathcal{M}$ is the space of all obtainable MFs. We write $\\mu = \\Gamma_{\\mathcal{M}}(\\pi^*)$, defined by fixed initial $\\mu_0$ and the recursion\n$$ \\mu_{t+1}(x') = \\sum_{x \\in \\mathcal{X}} \\mu_t(x) \\sum_{u \\in \\mathcal{U}} \\pi_t^*(u | x)p_t(x' | x, u, \\mu_t). $$"}, {"title": "2.3. Notions of Non-Cooperative Equilibria", "content": "As discussed, there are many equilibrium notions. Here, we focus on non-cooperative equilibria where agents optimize over independent policies to maximize their own objective."}, {"title": "2.3.1. NASH EQUILIBRIA.", "content": "First, we have the standard objective of any agent $i$ given as\n$$ \\mathcal{J}^i(\\pi^i, \\pi^{-i}) = \\mathbb{E} \\bigg[\\sum_{t\\in\\mathcal{T}} r_t(x_t^i, u_t^i, \\mu_t) \\bigg], $$\nwhich, for $\\pi = (\\pi^i, \\times_{j\\neq i}\\pi^j)$, in the limiting MFG yields\n$$ \\mathcal{J}(\\pi, \\pi^*) = \\mathbb{E} \\bigg[\\sum_{t\\in\\mathcal{T}} r_t(x_t, u_t, \\mu_t) \\bigg], \\quad \\mu = \\Gamma_{\\mu}(\\pi^*) $$\nwhere only agent $i$ deviates from policy $\\pi^*$ to $\\pi$. If agents are rational and anticipate other agents' decisions, all agents should use policies such that none can improve their objective by deviating from the equilibrium. This leads to the well-known Nash equilibrium."}, {"title": "2.3.2. REGULARIZED EQUILIBRIA", "content": "A common alternative to MFNE is to use regularized control (Geist et al., 2019; Belousov & Peters, 2019) (typically entropy regularization). The idea is to replace the objective in Eq. (3) by an entropy-regularized one, which maximizes the entropy $H(\\pi_t(\\cdot | x_t)) := - \\sum_{u \\in \\mathcal{U}} \\pi_t(u | x_t) \\log \\hat{\\pi}_t(u | x_t)$ of policies in encountered states $x_t$,\n$$ \\mathcal{J}_{RE}(\\pi, \\pi^*) := \\mathbb{E} \\bigg[ \\sum_{t \\in \\mathcal{T}} r_t(x_t, u_t, \\mu_t) + \\alpha H(\\pi_t(\\cdot | x_t)) \\bigg] $$\nwith $\\mu = \\Gamma_{\\mu}(\\pi^*)$, and temperature $\\alpha > 0$. Accordingly, we define regularized equilibria (RE) similar to MFNE."}, {"title": "2.3.3. QUANTAL RESPONSE EQUILIBRIA.", "content": "To incorporate bounded rationality, we assume agents act suboptimally and do not exactly optimize an objective. Building on economics literature (Breitmoser et al., 2010; Eibelsh\u00e4user & Poensgen, 2019), we introduce Markov QRE as an MFG equilibrium notion where agents only noisily estimate the state-action value function. Depending on the noise, agents act independently according to their own estimates, to the best of their knowledge. In the mentioned literature, the QRE definition is based on the state-action values $Q = Q_{\\mu,\\pi} = \\Gamma_{\\rho_{\\pi}} (\\mu, \\pi)$ of a policy $\\pi$ under current MF $\\mu$, given by the recursion\n$$ Q_t^{\\pi}(x, u) = r(x, u, \\mu_t) + \\sum_{x' \\in \\mathcal{X}} p_t(x' | x, u, \\mu_t) \\bigg[ \\sum_{u' \\in \\mathcal{U}} \\pi_t(u' | x')Q_{t+1}^{\\pi}(x', u') \\bigg], $$\nwith $Q_{T-1}^{\\pi}(x, u) = r(x, u, \\mu_{T-1})$. We extend this to the optimal state-action value function $Q^*$ defined in Eq. (4) and denote the resulting equilibria as Q\"RE and Q*RE, respectively.\nFor Q\"RE, the noisy state-action value function is\n$$ Q_t^{\\pi}(x, u) = Q_t^{\\pi}(x, u) + \\epsilon_t(x, u), $$"}, {"title": "2.3.4. OTHER MFG EQUILIBRIUM CONCEPTS.", "content": "The literature contains many equilibrium concepts for MFGs. One example are (coarse) correlated equilibria"}, {"title": "2.3.5. RECEDING HORIZON EQUILIBRIA.", "content": "Next, we describe a second method to model bounded rationality that can be combined with the previously defined equilibrium concepts. We introduce receding horizon (RH) equilibria to model limited lookahead capacity of agents by considering a shorter horizon for the underlying Bellman recursion. They describe the behaviour of agents with a model predictive control (MPC) (Kouvaritakis & Cannon, 2016), where decisions are based on a shortened future horizon, and therefore allows for more realistic or practical MFG models.\nThe previously defined objectives $\\mathcal{J}$, e.g. for NE or RE, are sums over the whole time horizon T. In the RH scenario, however, agents plan ahead only the next $H$ time steps beyond the current time $t \\in \\mathcal{T}$. Like in MPC, we assume that agents apply the first action of the resulting policy and then repeat the optimization for the next time step. A RH equilibrium is thus an ensemble of sequential MFG equilibria.\nFor RH MFG, define the respective RH NE as follows. For an agent at time t, the RH objective given the MF policy $\\pi$ is\n$$ \\mathcal{J}_t^H(\\hat{\\pi}, \\pi^*) := \\mathbb{E} \\bigg[\\sum_{t'=t}^{\\min(T,t+H)} r_{t'}(x_{t'}, u_{t'}, \\mu_{t'}) \\bigg], $$\nwith $\\mu^H = \\Gamma_{\\mu,t}(\\pi)$, defined by initial $\\mu_t^0$ and the recursion\n$$ \\mu_{t+1}^H(x') = \\sum_{x \\in \\mathcal{X}} \\mu_t^H(x) \\sum_{u \\in \\mathcal{U}} \\pi_t(u | x)p_t(x' | x, u, \\mu_t). $$"}, {"title": "2.4. Connections between Equilibrium Notions.", "content": "In the following, we compare the notions of equilibria introduced in the prequel. In all mentioned equilibria, $\\pi^*$ can be written down using the equivalent fixed-point equations\n$$(NE)\\quad \\pi^* = \\Gamma(\\Gamma_{Q^*}(\\Gamma_{\\mathcal{M}}(\\pi^*))), $$\n$$ (QRE)\\quad \\pi^* = \\Gamma_{\\lambda}^a(\\Gamma_{\\rho_{\\pi}}(\\Gamma_{\\mathcal{M}}(\\pi^*), \\pi^*)), $$\n$$ (Q^*RE)\\quad \\pi^* = \\Gamma_{\\lambda}^a(\\Gamma_{Q^*}(\\Gamma_{\\mathcal{M}}(\\pi^*))), $$\n$$ (RE)\\quad \\pi^* = \\Gamma(\\Gamma_Q(\\Gamma_{\\mathcal{M}}(\\pi^*))). $$\nThe decomposed definition enables a comparison between different equilibria. If we choose $a = 1/\\lambda$, the QRE, Q*RE and RE policies are all of a-softmax form. Therefore,"}]}, {"title": "3. Learning Non-Cooperative MF Equilibria", "content": "There is a variety of methods for computing or learning NE in MFGs, each with its own limitations. Standard fixed-point iteration (FPI) is not guaranteed to converge, as it cannot be Lipschitz even in simple standard finite MFGs (Cui & Koeppl, 2021, Thm. 2). Meanwhile, other algorithms such as fictitious play (FP) and Online Mirror Descent require monotonicity and their theory is currently limited to dynamics independent of the mean field (Perrin et al., 2020; P\u00e9rolat et al., 2022). Other recent ideas include an optimization-based approach (Guo et al., 2024) and a regularization approach (Guo et al., 2019; Cui & Koeppl, 2021), for which convergence via FPI is guaranteed for strong enough regularization. In the following, we generalize FPI and FP to our equilibrium concepts of interest."}, {"title": "4. Experiments", "content": "In this section we evaluate our algorithms and analyze the different equilibria for several MFGs. We analyze"}]}