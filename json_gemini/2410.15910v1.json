{"title": "DIVERSE POLICIES RECOVERING VIA POINTWISE\nMUTUAL INFORMATION WEIGHTED IMITATION\nLEARNING", "authors": ["Hanlin Yang", "Jian Yao", "Weiming Liu", "Qing Wang", "Hanmin Qin", "Hansheng Kong", "Kirk Tang", "Jiechao Xiong", "Chao Yu", "Kai Li", "Junliang Xing", "Hongwu Chen", "Juchao Zhuo", "Qiang Fu", "Yang Wei", "Haobo Fu"], "abstract": "Recovering a spectrum of diverse policies from a set of expert trajectories is an\nimportant research topic in imitation learning. After determining a latent style for\na trajectory, previous diverse policies recovering methods usually employ a vanilla\nbehavioral cloning learning objective conditioned on the latent style, treating each\nstate-action pair in the trajectory with equal importance. Based on an observation\nthat in many scenarios, behavioral styles are often highly relevant with only a sub-\nset of state-action pairs, this paper presents a new principled method in diverse\npolices recovery. In particular, after inferring or assigning a latent style for a tra-\njectory, we enhance the vanilla behavioral cloning by incorporating a weighting\nmechanism based on pointwise mutual information. This additional weighting re-\nflects the significance of each state-action pair's contribution to learning the style,\nthus allowing our method to focus on state-action pairs most representative of\nthat style. We provide theoretical justifications for our new objective, and exten-\nsive empirical evaluations confirm the effectiveness of our method in recovering\ndiverse policies from expert data.", "sections": [{"title": "1 INTRODUCTION", "content": "Imitation Learning (IL) is about observing expert demonstrations in performing a task and learn-\ning to mimic those actions (Hussein et al., 2017; Osa et al., 2018). Vanilla behavioral cloning\n(BC) (Pomerleau, 1991) learns a mapping from state to actions using expert state-action pairs via\nsupervised learning, which is simple to implement but may have the issue of compounding errors.\nGenerative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016) mitigates the issue via\nlearning both a discriminator and a policy. Despite their wide applications, these methods lack\nmechanisms to generate diverse policies, which may be essential in certain tasks.\nThere has been an increase in recent research addressing policy diversity in imitation learning (Li\net al., 2017; Wang et al., 2017; Zhan et al., 2020; Shafiullah et al., 2022; Mao et al., 2023), which can\ngenerally divided into two categories. In one category, the latent style z of a trajectory is inferred\nin an unsupervised manner, for instance, by an expectation maximization (EM) procedure. In the\nother category, the style z of a trajectory is determined by a user-specified function, for instance, a\nprogrammatic labeling function. No matter in which category, samples that are used to train a style-\nconditioned policy \u03c0(\u03b1|s, z) are treated with equal importance in those methods. However, in many\ncases, we observe that the relevance of different state-action pairs to the trajectory style can vary\nsignificantly. For example, in autonomous driving tasks, the diversity of overtaking policies (from\nthe left or right) is primarily relevant to the overtaking period in the trajectory. In other words, the\npreceding normal driving period is less relevant to the overtaking diversity. Again, in a basketball\ngame, the diversity of shooting position behaviors is primarily related to the part leading up to the"}, {"title": "2 RELATED WORK", "content": "Imitation Learning Imitation learning (IL) methods are designed to mimic the behaviors of ex-\nperts. Behavioral Cloning (BC) (Pomerleau, 1991), a well-known IL algorithm, learns a policy by\ndirectly minimizing the discrepancy between the agent and the expert in the demonstration data.\nHowever, offline learning methods like BC suffer from compounding errors and the inability to han-\ndle distributional shifts during evaluation (Ross et al., 2011; Fujimoto et al., 2019; Wu et al., 2019;\nPeng et al., 2019; Kostrikov et al., 2021). Inverse Reinforcement Learning (IRL) (Ng et al., 2000;\nArora & Doshi, 2021), another type of IL, learns a reward function that explains the expert behavior\nand then uses this reward function to guide the agent's learning process. Popular IRL approaches\nlike Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016) and Adversarial In-\nverse Reinforcement Learning (AIRL) (Fu et al., 2017) use adversarial training to learn a policy that\nis similar to the expert policy while being robust to distributional shift. However, these methods are\nlimited to imitating a single policy and do not address the issue of promoting diverse policies. When\nimitating diverse policies, BC approaches using supervised learning tend to learn an average policy,\nwhich does not fully capture the range of diverse behaviors (Codevilla et al., 2018). GAIL tends\nto learn a policy that captures only a subset of the expert's control behaviors, which can be viewed\nas modes of distribution. Consequently, the learned policy fails to cover all styles of the expert's\ndiverse behaviors Wang et al. (2017).\nPolicy Diversity Diversity is crucial in imitation learning algorithms, especially in practical con-\ntrol tasks and multi-player games (Zhu et al., 2018), as diverse policies in control tasks can enhance\nthe robustness of adapting to various environments. In contrast, AI with diverse policies can max-\nimize the player's experience and the ornamental value in games and competitions (Yannakakis &\nTogelius, 2018). Some approaches utilize information-theoretic methods to address this issue and\nlearn the behavioral styles of policies. InfoGAIL (Li et al., 2017) and Intention-GAN (Hausman\net al., 2017) augment the objective of GAIL with the mutual information between generated trajec-\ntories and the corresponding latent codes. Wang et al. (2017) use a variational autoencoder (VAE)\nmodule to encode expert trajectories into a continuous latent variable. Eysenbach et al. (2018) pro-\npose a general method called Diversity Is All You Need (DIAYN) for learning diverse skills without\nexplicit reward functions. DIAYN focuses on discovering skills in online reinforcement learning\ntasks (Campos et al., 2020; Sharma et al., 2019; Achiam et al., 2018), whereas our method specifi-\ncally considers pure offline imitation learning scenarios. More recently, Mao et al. (2023) introduced"}, {"title": "3 PRELIMINARY", "content": "We consider the standard Markov Decision Process (MDP) (Sutton & Barto, 2018) as the mathe-\nmatical framework for modeling sequential decision-making problems, which is defined by a tuple\n(S, A, P, r, do, T), where S is a finite set of states, A is a finite set of actions, P : S \u00d7 A \u00d7 S \u2192 R\nis the transition probability function, r : S \u2192 R is the reward function, do is the initial distribution,\nand T is an episode horizon. A policy \u03c0 : S \u00d7 A \u2192 [0, 1] maps from state to distribution over ac-\ntions. Let dr and d\u2122\n\u03c0. The vanilla BC loss function is:\n\nLBC = E [ - log \u03c0(als)].\n(s,a)~De\n\nwhich aims to maximize the probability of selecting action a for a given policy under the state s.\nBuilding upon the basic setup, we focus on an assumption where the expert demonstrations De,\nwhich consist of many diverse trajectories \u03c4, are collected by stylized expert policies denoted as\n{\u03c0\u20ac1), \u03c0(2), ..., \u03c0.\u039a)}. Let z \u2208 Z denotes the variable indicating which stylized policy T belongs\nto, and p(r|z = i) denotes the probability of 7 sampled under policy \u03c0\u03ad). Our objective is to\nlearn a conditioned policy \u03c0(als, z) such that trajectories generated by \u03c0(\u03b1|s, z) closely match the\ndemonstrations in De that exhibit the corresponding style z.", "subsections": [{"title": "3.1 PROBLEM SETTING AND VANILLA BEHAVIOR CLONING", "content": "We consider the standard Markov Decision Process (MDP) (Sutton & Barto, 2018) as the mathe-\nmatical framework for modeling sequential decision-making problems, which is defined by a tuple\n(S, A, P, r, do, T), where S is a finite set of states, A is a finite set of actions, P : S \u00d7 A \u00d7 S \u2192 R\nis the transition probability function, r : S \u2192 R is the reward function, do is the initial distribution,\nand T is an episode horizon. A policy \u03c0 : S \u00d7 A \u2192 [0, 1] maps from state to distribution over ac-\ntions. Let dr and d\u2122\n\nLBC = E [ - log \u03c0(als)]. \\qquad (1)\n(s,a)~De\n\nwhich aims to maximize the probability of selecting action a for a given policy under the state s.\nBuilding upon the basic setup, we focus on an assumption where the expert demonstrations De,\nwhich consist of many diverse trajectories \u03c4, are collected by stylized expert policies denoted as\n{\u03c0\u20ac1), \u03c0(2), ..., \u03c0.\u039a)}. Let z \u2208 Z denotes the variable indicating which stylized policy T belongs\nto, and p(r|z = i) denotes the probability of 7 sampled under policy \u03c0\u03ad). Our objective is to\nlearn a conditioned policy \u03c0(als, z) such that trajectories generated by \u03c0(\u03b1|s, z) closely match the\ndemonstrations in De that exhibit the corresponding style z."}, {"title": "3.2 MUTUAL INFORMATION NEURAL ESTIMATION", "content": "Mutual Information Neural Estimation (MINE) is a powerful technique for estimating the mutual\ninformation between two random variables using neural networks (Belghazi et al., 2018). It has\nbeen widely used in various domains, including representation learning (Hjelm et al., 2018), gener-\native modeling (Chen et al., 2016), and imitation learning (Eysenbach et al., 2018). The key idea\nbehind MINE is to formulate the estimation of mutual information as an optimization problem.\nGiven two random variables X and Y, the mutual information I(X; Y) can be expressed as the\nKullback-Leibler (KL) divergence between the joint distribution Pxy and the product of marginal\ndistributions Px Py:\n\nI(X; Y) = DKL(PXY||PX & Py). \\qquad (2)\nMINE approximates this KL divergence using a lower bound based on the Donsker-Varadhan rep-\nresentation (Donsker & Varadhan, 1983):\n\nI(X; Y) \u2265 sup EPxy [To] \u2013 log(EPx\u00aePy [eTo]),\\qquad (3)\nTOEF\nwhere F is a class of functions T : X \u00d7 Y \u2192 R. In MINE, this function class is parameterized by a\nneural network Te, which takes as input samples from the joint distribution Pxy and the product of\nmarginal distributions Px Py. The network is trained to maximize the lower bound, equivalent\nto estimating the mutual information."}]}, {"title": "4 BEHAVIORAL CLONING WITH POINTWISE MUTUAL INFORMATION\nWEIGHTING", "content": "As discussed in the introduction, in many real-world applications, the impact of different state-action\npairs on the style can vary greatly, and often only a part of the trajectory is highly relevant to the"}, {"title": "4.1 THEORETICAL ANALYSIS", "content": "We give some theoretical justifications for our BC-MI objective in Eq.(6). Considering an extreme\ncase where the mutual information between (s, a) and style z approaches zero, which means policies\nwith different styles have nearly the same state-action distribution. In this case, the BC-PMI objec-\ntive degenerates into vanilla BC, which means there is no difference in training style conditioned\npolicies and an average unconditioned policy. In the opposite extreme case, where all (s, a) pairs\nexhibit significant style differences, which means trajectories of different styles have minimal over-\nlap, we find that the BC-PMI objective degenerates into behavior cloning on each style. Formally,\nwe have the following proposition:\nProposition 1. (a). When the mutual information I(Z; S, A) equals to 0, it indicates that there is\nno distinction in the trajectory style corresponding to all the state-action pairs. In this case, the\nBC-PMI objective degenerates to the vanilla behavior cloning objective:\n\narg min LBC-PMI(0) = arg min LBC(0). \\qquad (7)\n\u03b8\n\u03b8\n(b). When the conditional entropy H(Z|S, A) equals 0, it indicates that there is a significant dis-\ntinction in the trajectory style corresponding to all the state-action pairs. In this case, the BC-PMI\nobjective degenerates to the behavior cloning on each style:\n\nLBC-PMI(0) = \u2211(0), \\qquad (8)\ni=1\nBC\nwhere L(0) LBC is the behavior cloning loss on the subset of data with style label i.\nProof. Refer to Appendix A.1."}, {"title": "4.2 PRACTICAL IMPLEMENTATION", "content": "To practically estimate the PMI values in Eq.(6), we employ the Mutual Information Neural Esti-\nmation (MINE) (Belghazi et al., 2018) method. MINE is a neural network-based approach that can\neffectively estimate mutual information between high-dimensional random variables. By leveraging\nthe Donsker-Varadhan representation (Donsker & Varadhan, 1983) of the Kullback-Leibler (KL)\ndivergence, MINE allows for the estimation of mutual information using neural networks.\nLet T : S \u00d7 A \u00d7 Z \u2192 R be a neural network parameterized by 4. The mutual information between\n(s, a) and z can be estimated as:\n\nI(s, a; z) \u2265 E [To(s, a, z)] - log E [(9)\n(s,a,z)~De\n[(s,a)~De,z~p(z)\n[eTo(8,1,2)]]\nThe neural network To is trained to maximize the lower bound in Eq.(9), which is equivalent to\nminimizing the KL divergence between the joint distribution p(s, a, z) and the product of marginals\np(s, a)p(z). According to the proof of Theorem 1 in MINE (Belghazi et al., 2018), the optimal\nsolution for To is:\n\nT(s, a, z) = log = log . \\qquad (10)\np(s, a)p(z)\np(z)\nwhich is exactly the PMI we aim to estimate.\nIn practice, we can train the MINE network To using samples from the expert demonstrations De\nand the style distribution p(z). The training objective for To is:\n\nmax\nE [T(s, a, z)] - log\nE [(11)\n[(s,a)~De,z~p(z)\n(s,a,z)\nBy optimizing Eq.(11), we obtain an approximation of the PMI values, which can be used as the\nweights in the behavioral cloning objective in Eq.(6). The weights can be denoted as:\n\n\u03c3(s, a, z) = exp[T(s, a, z)].\\qquad (12)\nFurthermore, in order to reduce the variance of the gradient in optimizing Eq.(6), we can subtract an\noptimal baseline from the weight, like what has been done in A3C (Mnih et al., 2016). This turns\nEq.(6) to the following objective:\n\nmin E [ - log (als, z) \u2022 [exp(T (s, a, z))-6]], \\qquad (13)\n\u03c0\n(s,a,z)~De\nwhere b = E [exp(T(s, a, z))].\n(s,a,z)~De\nIn practice, we can estimate b using the moving average.\nThe pseudo-code for the BC-PMI algorithm is shown in Algorithm 1."}, {"title": "5 EXPERIMENTS", "content": "In this experimental section, we aim to address the following questions:\nQ1. Can our method recover diverse and controllable policies from diverse style trajectories?\nQ2. Does the PMI weight have interpretability and improve policy diversity and style calibration to\nthe algorithm?\nQ3. Can our method cope with complex, real-world tasks, particularly those that involve extensive\ndatasets derived from human participants?"}, {"title": "5.1 STYLES AND BASELINES", "content": "The dimension of the style needs to be specified if the style needs to be controllable. Our method can\nhandle both style-labeled data and data without style labels. For the latter, getting the style labels\ncan be expensive when relying on manual annotations and uncontrollable when using unsupervised\napproaches. Instead, the programmable labeling functions (Ratner et al., 2016; Zhan et al., 2020)\ncan be used to automatically generate style labels.\nWe compare PMI-BC to the following baselines: (1) BC, which directly imitates expert actions\nacross all styles; (2) CBC, which separates trajectories of different styles and uses BC to imitate\neach style separately; (3) CTVAE, which is the conditional version of TVAEs (Wang et al., 2017);\n(4) InfoGAIL (Li et al., 2017), which infers the latent style of trajectories by maximizing the mu-\ntual information between the latent codes and trajectories; (5) SORL (Mao et al., 2023)*, which\nuse Expectation-Maximization (EM) algorithm to classify the trajectories from the heterogeneous\ndataset into clusters where each represents a distinct and dominant motion style. Specifically, we\nfirst introduce a toy example called Circle 2D to provide a simple visualization and analysis. Then\nin Atari games, detailed analysis and validation were provided regarding the PMI weights. Lastly,\nwe evaluate all these baseline methods in Section 5.4 to illustrate the effectiveness of BC-PMI."}, {"title": "5.2 CIRCLE 2D", "content": "The Circle 2D environment is a 2D plane where an agent can freely move at a constant velocity by\nselecting its direction, denoted as pt, at time step t. For the agent, the observation at time step t\nincludes the state information from time step t - 4 to t. The offline expert trajectories consist of four\ndifferent styles, each generated by a random expert policy. The expert policy generates trajectories\nthat resemble circular patterns after a period of translation (75 time steps). This design aims to\nintroduce partial diversity in the trajectories. In this environment, each episode consists of 300 time\nsteps. If the first loop around the circle is completed before reaching 300 steps, the agent continues\ncircling until the end of the episode. Hence, in this scenario, there is minimal difference in the\noffline trajectories during the first 75 steps, and the trajectory differences vary as the agent's position\nprogresses after 75 steps. During the imitation learning training process, the expert trajectories used\nare noisy, meaning there is randomness introduced in both the sampled actions and the environment.\nThis task is a toy example that can help the readers intuitively understand our motivation. In this\ntask, the agent's behavioral diversity is highly related to the curvature of the circle moving after 75"}, {"title": "5.3 ATARI GAMES", "content": "In this experiment, we concentrate on three widely recognized Atari games: Alien, MsPacman and\nSpaceInvaders. The datasets utilized in this study are sourced from Atari-Head (Zhang et al., 2018;\n2020), an extensive collection of human game-play data. Atari-Head are meticulously recorded in a\nsemi-frame-by-frame manner, ensuring high data quality and granularity, which facilitates in-depth\nanalysis and robust evaluation of our proposed method.\nThis experiment consists of three parts. Firstly, we demonstrate the convergence of the lower bound\nof mutual information in Eq.(9), which indicates the relevance between state-action pairs and styles.\nSecondly, we provide interpretability of PMI weights by assessing the extent to which they appropri-\nately reflect the influence of the current (s, a) pair on the style. Lastly, we evaluate the controllability\nof the BC-PMI policy, which refers to the ability of the policy to act according to a given style once\nit is specified.\nIn the Alien and MsPacman game, we employ two styles: the area style and the range style. The\nformer divides the map into four distinct areas, distinguishing the agent's preferences for moving\ntoward each area. The latter models the agent's displacement trajectory on the map as a Gaussian\ndistribution, differentiating the variance of the distribution. A higher variance indicates a tendency\nfor the agent to move across areas, while a lower variance indicates a preference for movement"}, {"title": "5.4 PROFESSIONAL BASKETBALL PLAYER DATASET", "content": "In this experiment, we validate our method on the dataset of a collection of professional basketball\nplayer trajectories (Zhan et al., 2020) with the goal of recovering policies that can generate trajec-\ntories with diverse player-movement styles. The basketball trajectories are collected from tracking\nreal players in the NBA. We primarily focus on two movement styles: (1) The Destination, which\nis the distance from the final position to a fixed destination on the court (e.g. the basket), and (2)\nThe Curvature, which measures the player's propensity to change directions."}, {"title": "6 DISCUSSION", "content": "In this paper, we investigate how to recover diverse policies from a set of expert trajectories. We\npropose a new diverse policy recovering method by leveraging the relevance of the state-action\npair with the trajectory styles. The highlight of our method lies in our approach to the problem of\npolicy diversity from a different perspective, which involves the introduction of Pointwise Mutual\nInformation to model the relevance between each (s, a) pair and the style. By utilizing a unique and\nstraightforward approach, we achieved results that surpassed previous state-of-the-art methods."}, {"title": "6.1 CONCLUSION", "content": "In this paper, we investigate how to recover diverse policies from a set of expert trajectories. We\npropose a new diverse policy recovering method by leveraging the relevance of the state-action\npair with the trajectory styles. The highlight of our method lies in our approach to the problem of\npolicy diversity from a different perspective, which involves the introduction of Pointwise Mutual\nInformation to model the relevance between each (s, a) pair and the style. By utilizing a unique and\nstraightforward approach, we achieved results that surpassed previous state-of-the-art methods."}, {"title": "6.2 LIMITATIONS AND FUTURE WORK", "content": "In our setting, our goal is to recover policies from diverse offline data, assuming that the data within\nthe trajectories already meet the performance requirements. If the policy that generates the offline\ntrajectories performs poorly, our method cannot further enhance the performance of the learned\npolicy. Hence, we consider the combination of PMI and RL as future research, in which we believe\nthe PMI weight can be associated with the offline RL method to promote diversity while improving\nperformance."}]}