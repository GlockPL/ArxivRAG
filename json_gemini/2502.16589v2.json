{"title": "Co-MTP: A Cooperative Trajectory Prediction Framework\nwith Multi-Temporal Fusion for Autonomous Driving", "authors": ["Xinyu Zhang", "Zewei Zhou", "Zhaoyi Wang", "Yangjie Ji", "Yanjun Huang", "Hong Chen"], "abstract": "Abstract-Vehicle-to-everything technologies (V2X) have be-\ncome an ideal paradigm to extend the perception range and\nsee through the occlusion. Exiting efforts focus on single-\nframe cooperative perception, however, how to capture the\ntemporal cue between frames with V2X to facilitate the pre-\ndiction task even the planning task is still underexplored. In\nthis paper, we introduce the Co-MTP, a general cooperative\ntrajectory prediction framework with multi-temporal fusion\nfor autonomous driving, which leverages the V2X system to\nfully capture the interaction among agents in both history and\nfuture domains to benefit the planning. In the history domain,\nV2X can complement the incomplete history trajectory in\nsingle-vehicle perception, and we design a heterogeneous graph\ntransformer to learn the fusion of the history feature from\nmultiple agents and capture the history interaction. Moreover,\nthe goal of prediction is to support future planning. Thus, in\nthe future domain, V2X can provide the prediction results\nof surrounding objects, and we further extend the graph\ntransformer to capture the future interaction among the ego\nplanning and the other vehicles' intentions and obtain the\nfinal future scenario state under a certain planning action.\nWe evaluate the Co-MTP framework on the real-world dataset\nV2X-Seq, and the results show that Co-MTP achieves state-of-\nthe-art performance and that both history and future fusion\ncan greatly benefit prediction. Our code is available on our\nproject website: https://xiaomiaozhang.github.io/Co-MTP/", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous vehicles (AVs) should interact with complex\nand dynamic environments to ensure safe and efficient oper-\nation [1]\u2013[3]. Thus, the trajectory prediction has attracted the\ncommunity's attention [4]\u2013[6]. The accurate prediction relies\non the comprehensive perception information, especially in\ncomplex and dense scenarios [7]. The limited perception\nrange and occlusion always lead to catastrophic results [8]-\n10], however, most existing works assume that all sur-\nrounding objects are fully visible and have complete history\ntrajectories [11]\u2013[13], which is unreal in real vehicle onboard\nsensors. Thus, vehicle-to-everything (V2X) technology has\nbeen an ideal paradigm for sharing information among agents\nand seeing through occlusion. Nevertheless, existing V2X\nefforts focus on the single-frame cooperative perception\n[14]\u2013[16]. Issues such as How to capture the temporal cue\nbetween frames with V2X to facilitate the prediction task\nand even the planning task, and how to fully employ the\nV2X information in real-world environments with occlusion\nare still underexplored and present deployment challenges.\nIn the real world, each agent's sensor differs in capability,\naccuracy, and coverage [17], [18], and the positioning of\nthese sensors can lead to varied levels of perception and\nocclusion of the same object. The simplest method of trajec-\ntory fusion is association and stitching [19], but it cannot\nhandle the diverse errors from different sensors and still\nfaces the incomplete problem when multiple agents lose the\nsame timestamp in dense scenarios. Thus, it is necessary to\nexplore how cooperative prediction integrates and leverages\nincomplete history trajectories from multiple agents to obtain\nmore comprehensive history information.\nFurthermore, the goal of prediction is not merely to\npredict the motion of surrounding objects but to support the\nego vehicle's planning [1], [20]. Each planning candidate\nrepresents a driving intent, and the vehicle should evaluate\neach candidate by reasoning the future because the reaction\nof the environment varies depending on the ego's actions.\nIf the AV reasons the surroundings solely based on the\nisolated prediction, it will become too afraid to drive and\nwill always consistently yield to other road users [21], [22].\nOn the contrary, proactive planning and prediction make AV\nconsider their intention first and the reason for the other\nreaction. However, the ego planning trajectory is the only\nfuture feature in the future domain, leading to aggressive\ndriving behaviors and danger. Thus, for planning-oriented\nprediction, it is necessary not only to incorporate history\ninteractions but also to consider future interactions among\nsurrounding objects.\nTo address these challenges, we present Co-MTP, a\nCooperative Multi-Temporal fusion Prediction framework.\nFirst, the infrastructure, with elevated and stationary perspec-\ntives, can provide perception and prediction support to each\nagent in the scenario, e.g. AV, human driver, and pedestrian.\nSecond, when AV drives into the infrastructure area, it can\nreceive the infrastructure's history and prediction informa-\ntion, and fuse them in both history and future domains with\nthe planning information of the ego vehicle to facilitate\nthe planning-oriented prediction. Our contributions can be\nsummarized as follows:\n\u2022\n\u2022\nWe propose Co-MTP, a general cooperative trajectory\nprediction framework with multi-temporal fusion across\nboth history and future domains. Co-MTP is the first\nframework to fully fuse and exploit comprehensive\ntemporal information in prediction with V2X.\nFor the issue of incomplete trajectory in the history\ndomain, we designed a heterogeneous graph to learn\nthe fusion of the history feature from multiple agents\nwith transformers."}, {"title": "II. RELATED WORK", "content": "Cooperative Prediction. The existing effort of V2X re-\nsearch focuses on cooperative perception, and cooperative\nprediction which considers the temporal information with\nV2X is still in its infancy [23]\u2013[25]. V2VNet [26] is the\nfirst work to explore this field, which proposed an end-to-\nend perception and prediction model. However, its prediction\nhead is simple and ignores the map information and complex\ninteraction features. Yu, et al. [19] presented a open-source\nV2X sequence dataset for cooperative prediction, V2X-Seq.\nV2XPnP [25] provides an open-source general framework\nand V2X sequential dataset for end-to-end cooperative per-\nception and prediction. Furthermore, the V2X-Graph [18] is\nproposed based on V2X-Seq, which provides a graph neural\nnetwork (GNN) to learn the cooperative trajectory represen-\ntations. However, how to fully employ the V2X temporal\ninformation, such as the different temporal domains, and fuse\nthe cooperative trajectory still needs to be explored.\nPlanning-informed Prediction. The couple of prediction\nand planning is the focus for safe driving in interaction\nscenarios [1], [27]. PiP model [22] directly incorporated"}, {"title": "III. METHODOLOGY", "content": "Fig. 1 illustrates the Co-MTP framework. Both AVs and\ninfrastructures can operate prediction individually to support\nthe AV and scenario object separately. The input of the\nvehicle-side prediction model is not limited to the history\ndomain but also includes the prediction results from the\ninfrastructure-side prediction model in the future domain.\nA. Problem Definition\nThe input comprises two parts: history trajectories, AV's\nfuture ground truth and map data. Consider a scenario where\nmultiple viewpoints jointly observe a set of Z objects \u03a9 =\n{\ud835\udc4e\ud835\udc56|\ud835\udc56 = 1,2,...,\ud835\udc4d} from time step \ud835\udc610 to \ud835\udc61\ud835\udc5d. Among them,\nthe AV perceives M objects \u03a9\ud835\udc4e\ud835\udc63 = {\ud835\udc4f\ud835\udc56|\ud835\udc56 = 1,2,...,\ud835\udc40} and\n\ud835\udc4f\ud835\udc56 \u2208 \u03a9, while the infrastructure perceives N agents \u03a9\ud835\udc56\ud835\udc5b\ud835\udc53\ud835\udc5f\ud835\udc4e =\n{\ud835\udc50\ud835\udc56|\ud835\udc56 = 1,2,...,\ud835\udc41} and \ud835\udc50\ud835\udc56 \u2208 \u03a9. All objects' features are\ngiven, including their bounding boxes (length, width, height)\nand types (pedestrian, bicycle, vehicle). Define the object\ntrajectory \ud835\udc56 as \ud835\udc4b\ud835\udc56 = {\ud835\udc65\ud835\udc610,\ud835\udc65\ud835\udc611,...,\ud835\udc65\ud835\udc61\u210e} in a certain frequency,\nwhere \ud835\udc65\ud835\udc61 includes coordinates, heading, velocity, etc. Due\nto occlusion and other reasons, the observation could be\nmissing in some frames. Thus, each object's trajectory has\na corresponding mask vector \ud835\udc5a\ud835\udc56 = {\ud835\udc5a\ud835\udc610,\ud835\udc5a\ud835\udc611,...,\ud835\udc5a\ud835\udc61\u210e}, which\nindicates the data validity. If the data exists at time step\nt, then \ud835\udc5a\ud835\udc61 = 1; otherwise, the value is 0. In addition, to\nsimulate the planning trajectory from the real-world dataset,\nwe follow the setting of [22], and use the quintic curve to\ninterpolate the future ground truth to follow the planning\nmission goal. The map data includes roads and signals, and\nall roads are represented as polylines or polygons, such as\nroad lines, crosswalks, and stop-lines.\nThe prediction goal is to use the aforementioned infor-\nmation to predict the future trajectories of the surrounding\nobjects. In other words, the task is to learn a model \ud835\udc53(.) that\n\ud835\udc61\u210e+\ud835\udc61\ud835\udc53\noutputs \ud835\udc4c\ud835\udc56 = {\ud835\udc66\ud835\udc61\u210e+1,\ud835\udc66\ud835\udc61\u210e+2,...,\ud835\udc66\ud835\udc61\u210e+\ud835\udc61\ud835\udc53}. Additionally, to cap-\nture the uncertainty, the predicted output could be multiple\nmodes, each associated with a corresponding probability.\nB. Scene Representing with Graph\nHeterogeneous Graph. The driving scenario has a lot of\nheterogeneous elements, such as map, trajectories, different\nobject types and traffic signals. To leverage the powerful\nscenario-representation capabilities, we build a heteroge-\nneous graph for prediction scenario \ud835\udc3a = {\ud835\udc41, \ud835\udc38}, where\n\ud835\udc41 and \ud835\udc38 present the set of nodes and edges separately.\nObjects and the map are represented as graph nodes, and the\nrelationships between them are represented as edges between\nthe nodes. To represent heterogeneity, we construct a type\nmapping function \ud835\udcaf for the nodes and define the type of\neach edge based on its originating node.\nTrajectory Data Preprocess. In Co-MTP framework, ob-\njects' history trajectories come from both the AV and in-\nfrastructure perspectives, along with the corresponding mask\nmatrices. Before learning and fusing the information from\nthese perspectives, we follow the stitch method CBMOT\n[36] in V2X-Seq [19] to use the high-detection-score to\ninterpolate the vehicle history trajectory with infrastructure\nhistory, and the masks also are reorganized. We found the\nprestitch of the incomplete trajectory can complement the\ntemporal index and accelerate the learning of the following\ntrajectory fusion. This prestitch method does not mean that\nthe infrastructure information not involved in the interpola-\ntion is discarded. All the original objects' history trajectories\nfrom the infrastructure are encoded in the graph nodes. In the\nfuture domain, in addition to the AV's planning trajectory,\nthe infrastructure's prediction results of the other objects\nare also included. Since our research does not focus on the\ninfrastructure's prediction task itself, we use the our Co-MTP\nmodel with only infrastructure information to output other\nagents' predicted trajectories.\nTo reduce the computational burden, the number of edges\nfor each node has the upper boundary. Whether the node\n\ud835\udc5b can establish an edge with the node \ud835\udc60 depends on the\ndistance between them. For object nodes, we use the position\nobserved at the last time step \ud835\udc61\u210e to calculate the distance to\nothers. For map nodes, the shortest distance from other nodes\nto the polyline or polygon is selected.\nC. Relative Spatial-Temporal Encoding\nLocal Coordinate System. To facilitate interaction learning\nin a normalized form, each node requires a local coordinate\nsystem (\ud835\udc65\ud835\udc5f\ud835\udc52\ud835\udc53, \ud835\udc66\ud835\udc5f\ud835\udc52\ud835\udc53, \ud835\udc42\ud835\udc5f\ud835\udc52\ud835\udc53) to provide a unique motion representa-\ntion for each object, regardless of its global coordinates. All\nsubsequent computations related to the node will be carried\nout within this local coordinate system. The position and\nheading of each object at the final time step \ud835\udc61\u210e are taken as\n(\ud835\udc65\ud835\udc5f\ud835\udc52\ud835\udc53, \ud835\udc66\ud835\udc5f\ud835\udc52\ud835\udc53) and \ud835\udc42\ud835\udc5f\ud835\udc52\ud835\udc53. For map data, the center point coordinates\nare used as (\ud835\udc65\ud835\udc5f\ud835\udc52\ud835\udc53, \ud835\udc66\ud835\udc5f\ud835\udc52\ud835\udc53), and the direction of the longest line\nsegment within the polyline or polygon is treated as \u0398\ud835\udc5f\ud835\udc52\ud835\udc53.\nGraph Node Encoding. All the positions of objects from\nboth AV and infrastructure are encoded with a shared-\nparameter MLP. Then, the position features are concatenated\nwith other information (e.g. bounding box and speed) input\ninto two distinct MLPs-one designed for the AV and the\nother for the infrastructure with different parameters. We\nemploy two self-attention layers to capture the temporal\nfeatures of each object in both history and future domains.\nSince the history trajectory may be incomplete, the mask\nvector \ud835\udc5a\ud835\udc56 is also input to the temporal encoding. For map\ndata, we combine the MLP used for coordinate encoding with\nan improved PointNet to form the map position encoding\nmethod. Additionally, we use a set of Linear layers and\nGRUs to encode traffic signals. Thus, all nodes have been\nencoded into a set {\ud835\udc5b\ud835\udc56}.\nGraph Edge Encoding. All edges are first input into a\nshared-parameter MLP to encode the coordinate transforma-\ntion \u0393\ud835\udc60\ud835\udc5b. According to the type mapping function \ud835\udcaf(.), all\nedges are connected to their starting nodes and the combined\ninput is fed into the corresponding type-specific MLP to\ngenerate the initial edge features.\n\ud835\udc52\ud835\udc60\ud835\udc5b = \ud835\udc40\ud835\udc3f\ud835\udc43\ud835\udc60(\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61[\ud835\udc60\ud835\udc5c, \ud835\udc40\ud835\udc3f\ud835\udc43(\u0393\ud835\udc60\ud835\udc5b)])\n(1)"}, {"title": "D. Cross-Temporal and Cross-Agent (CTCA) Fusion", "content": "To capture the complex trajectory features from multiple\nagents, we propose a cross-temporal graph with the spatial-\ntemporal fusion association (STFA) module. A K-layer\nTransformer is employed to aggregate and update features\nfor nodes and edges.\nSpacial-Temporal Fusion Association. Fig. 2 illustrates the\nSTFA module. It aims to fully integrate information from\ndifferent views and temporal domains. AV and infrastructure\ncan observe the same object in the same timestamp and\nhave separate history information record, thus, it is neces-\nsary to establish a connection between these two agents'\ninformation. We employ an implicit approach to extract\nthe multi-view features across multi-temporal domain. When\ngathers information, the vehicle-side node \ud835\udc63, after trajectory\ninterpolation, is continuously influenced by the infrastructure\nnode \ud835\udc56. This not only reuses the infrastructure information\nthat was not utilized during the interpolation but also learn\nthe relationship between the two views. Furthermore, when\naggregating agent node, future information should be consid-\nered. Similarly to the above approach, future nodes provide\nthe agents with the necessary information for aggregation.\nTransformer Layer. We utilize the multi-head attention\nlayer (MHA) to capture the feature from various types of\nin-edges for each node, followed by an MLP to combine the\ninformation. During the aggregation phase, the node features\nare calculated as the query (\ud835\udc44), and the in-edge features are\nused as the key (\ud835\udc3e) and the value (\ud835\udc49).\n\ud835\udc41\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52\ud835\udc56\ud835\udc5b = \ud835\udc40\ud835\udc3b\ud835\udc34\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52\ud835\udc52\ud835\udc5b(\ud835\udc5b\ud835\udc58\u22121,\ud835\udc52\ud835\udc58\ud835\udc60\u2192\ud835\udc5b)\n= \ud835\udc40\ud835\udc3f\ud835\udc43(\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61({\ud835\udcaf\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52\ud835\udc52\ud835\udc5b | \ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52\ud835\udc52\ud835\udc5b = 1,...,\ud835\udc5b}))) (2)\nwhere \ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52\ud835\udc52\ud835\udc5b means the type of \ud835\udc52\ud835\udc58\ud835\udc60\u2192\ud835\udc5b, and \ud835\udc5b, is number of\nthe in-edges' types for node \ud835\udc5b. After that, the features of\neach type of node are updated with a feed-forward network\n(FFN).\n\ud835\udc5b\ud835\udc58\ud835\udc5b = \ud835\udc39\ud835\udc39\ud835\udc41\ud835\udc5b(\ud835\udc5b\ud835\udc58\u22121,\ud835\udc41\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52\ud835\udc56\ud835\udc5b) (4)\nwhere \ud835\udc5b\ud835\udc58\ud835\udc5b is the hidden feature of the \ud835\udc58\ud835\udc61\u210e Transformer. \ud835\udc39\ud835\udc39\ud835\udc41\ud835\udc5b\nmeans the parameters depend on the type of node \ud835\udc5b."}, {"title": "E. Multimodal Decoder", "content": "The motion pattern of an agent is inherently multi-modal,\nwhere multiple future trajectory possibilities coexist.The\nhidden features output by the final layer of the transformer\nlayer typically consist of a single set of feature embeddings.\nIn this paper, we first replicate these single embeddings\nand their corresponding in-edge features \ud835\udc3e times. Then,\nwe refine the node information through a cross-attention\nmechanism with processed in-edge features to extract richer\nfeature representations and output \ud835\udc3e modes via a feed-\nforward network (FFN).\nTraditionally, the \ud835\udc3e trajectory modes are treated as in-\ndependent of one another. However, the modes of an object\nexhibit a competitive relationship, where the rise of one mode\nleads to the decline of others. Therefore, we input the refined\nfeatures into a self-attention layer to capture the relationships\nbetween the \ud835\udc3e modes of the same object.\nAfter this refinement process, we employ both regression\nand classification techniques to decode the agent's trajectory.\nSpecifically, a multi-layer perceptron (MLP) classifies the\n\ud835\udc3e modalities, assigning each a probability. However, simply\nclassifying the trajectory can lead to issues such as trajectory\ndrift, which may violate the agent's history tend. To mitigate\nthis, we reintroduce the original node features \ud835\udc5b0 and impose\ndynamic constraints on the regression results. These refined\nfeatures are concatenated with the regression output and\npassed through an MLP, with temporal aspects handled by a\nseries of refined convolutional neural networks (CNNs). For\na single agent, the final predicted output trajectory can be\nexpressed as follows:\n\ud835\udc4c\ud835\udc50\ud835\udc59\ud835\udc60 = {\ud835\udc50\ud835\udc58}, \ud835\udc58 \u2208 [1,\ud835\udc3e] (6)\n\ud835\udc4c\ud835\udc5f\ud835\udc52\ud835\udc54 = {(\ud835\udc65\ud835\udc58\ud835\udc61,\ud835\udc66\ud835\udc58\ud835\udc61,...,\ud835\udc65\ud835\udc58\ud835\udc61,\ud835\udc66\ud835\udc58\ud835\udc61)}, \ud835\udc58 \u2208 [1,\ud835\udc3e] (7)\nwhere \ud835\udc50\ud835\udc58 is probability of the \ud835\udc58-\ud835\udc61\u210e mode, \ud835\udc66\ud835\udc61\ud835\udc58 is the position\nof the agent at the timestamp \ud835\udc61, the shape of tensor \ud835\udc4c\ud835\udc50\ud835\udc59\ud835\udc60 and\n\ud835\udc4c\ud835\udc5f\ud835\udc52\ud835\udc54 are (\ud835\udc41\u00d7\ud835\udc3e) and (\ud835\udc41 \u00d7 \ud835\udc3e \u00d7 \ud835\udc47 \u00d7 2). \ud835\udc41 is the number of\ntarget agents, 2 means the 2-D coordinate (\ud835\udc65,\ud835\udc66).\nDuring the training, the total loss is calculated by the\nregression head and the classification head.\n\ud835\udc3f = \ud835\udf061\ud835\udc3f\ud835\udc50\ud835\udc59\ud835\udc60 + \ud835\udf062\ud835\udc3f\ud835\udc5f\ud835\udc52\ud835\udc54\n1 \ud835\udc41\n=\ud835\udf061\u2211 \ud835\udc36\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc38\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc66({\ud835\udc50\ud835\udc58}\ud835\udc5b,\ud835\udc3c(\ud835\udc58\ud835\udc5b))\n\ud835\udc41\n\ud835\udc5b=1\n1 \ud835\udc47\n+\ud835\udf062 \u2211 \u2211 [\ud835\udc3c(\ud835\udc58\ud835\udc5b) + \ud835\udc591(\ud835\udc66\ud835\udc61\ud835\udc5b,\ud835\udc58, \ud835\udc66\ud835\udc61)]\n2\ud835\udc41\ud835\udc47\n\ud835\udc5b=1 \ud835\udc61=1 (8)\nwhere \ud835\udf061 and \ud835\udf062 are the weights, which are set to 0.1 and\n10. \ud835\udc3c(\ud835\udc58\ud835\udc5b) is a hot vector, where the \ud835\udc58\ud835\udc5b-\ud835\udc61\u210e element is 1 and\nFor edge feature aggregation and updating, we retrieve\nthe outputs \ud835\udc60\ud835\udc58\u22121 and \u0393\ud835\udc60\u2192\ud835\udc5b from the edge encoding stage\nand combine them with the edge features from the previous\nlayer, \ud835\udc52\ud835\udc58\ud835\udc60\ud835\udc5b. These combined features are then input into an\nMLP to obtain the updated edge features.\n\ud835\udc52\ud835\udc58\ud835\udc60\ud835\udc5b = \ud835\udc40\ud835\udc3f\ud835\udc43\ud835\udc60\u2192\ud835\udc5b(\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61[\ud835\udc60\ud835\udc58\u22121, \u0393\ud835\udc58\ud835\udc60\u2192\ud835\udc5b], \ud835\udc52\ud835\udc58\ud835\udc60\ud835\udc5b)) (5)"}, {"title": "IV. EXPERIMENTS", "content": "Dataset and metrics. We evaluate the Co-MTP framework\non the real-world V2X dataset, V2X-Seq, which comprises\nover 50,000 trajectory fragments of vehicle-infrastructure\ncooperation. Each fragment lasts for 10s, with a recorded\nfrequency of 10 Hz. The dataset labels the type (e.g. vehicle,\nbicycle, pedestrian, etc.) of each agent. To increase the\nsample size, we slice each original dataset fragment into 20\nsamples. Each sample contains 3s of history trajectories and\n5s of future trajectories. According to standard evaluation\nprotocol, we utilize metrics including minimum Average Dis-\nplacement Error (minADE), minimum Final Displacement\nError (minFDE), and Missing Rate (MR) for evaluation.\nImplementation Details. All the trajectories are downsam-\npled to 5Hz to accelerate the training. The map polylines and\npolygons are segmented, with each segment represented by\n21 points. We select a map within 250m of the evaluation\narea and limit the number of edges for each node in the\nGNN to a maximum of 32. The prediction mode is set to 6\nfor fair comparison. 6 layers of Transformer are stacked for\nthe encoder module. The dimension of the hidden feature is\nset to 256, and the number of heads in all MHA is 8. The\nlearning rate is set to 2.5 \u00d7 10\u22123 initially and decreases over\nthe training epochs. The AdamW optimizer is adopted with a\nweight decay of 1 \u00d7 10\u22126. We train the model for 45 epochs\nwith a batch size of 24 on a server with 4 NVIDIA 4090s.\nB. Compared Methods\nWe consider the existing methods on the V2X-Seq dataset\nand a baselines we proposed for performance comparison.\n\u2022\n\u2022\n\u2022\nPP-VIC stitch the AV and infrastructure trajectory\nthrough CBMOT, and then test it with the SOAT pre-\ndiction models TNT [13] and HiVT [33].\nV2X-Graph constructs a graph to fuse the history\ntrajectories from AV and infrastructure, however, it just\nfocuses on the history domain fusion.\nCo-HTTP is the baseline model, simplified from our\nmodel. It uses the CBMOT [36] method to\nCo-MTP\nstitch history trajectories from AV and infrastructure.\nIn the Table I, we can observe that our cooperative\nframework Co-MTP ranks first across minADE/minFDE/MR\nin the benchmark of V2X-Seq dataset, with a 35% reduc-\ntion in minADE compared to V2X-Graph. Moreover, the\nperformance of our proposed baseline is also noteworthy,\nwhich preliminarily indicates that the heterogeneous graph\nnetwork and cross-temporal fusion strategies are beneficial\nfor prediction in cooperative scenarios."}, {"title": "V. CONCLUSION", "content": "To fully explore the fusion and exploit comprehensive\ntemporal information in prediction with V2X, in this paper,"}]}