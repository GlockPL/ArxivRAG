{"title": "BF-GAN: Development of an AI-driven Bubbly Flow Image Generation Model Using Generative Adversarial Networks", "authors": ["Wen Zhou", "Shuichiro Miwa", "Yang Liu", "Koji Okamoto"], "abstract": "In recent years, image processing methods for gas-liquid two-phase flow, including conventional computer vision techniques, bubble detection, segmentation, and tracking algorithms, have seen significant development due to their high efficiency and accuracy. Nevertheless, obtaining extensive, high-quality two-phase flow images continues to be a time-intensive and costly process. To address this issue, a generative AI architecture called bubbly flow generative adversarial networks (BF-GAN) is developed, designed to generate realistic and high-quality bubbly flow images through physically conditioned inputs, namely superficial gas ($j_g$) and liquid ($j_f$) velocities.\nInitially, 105 sets of two-phase flow experiments under varying conditions are conducted to collect 278,000 bubbly flow images with physical labels of $j_g$ and $j_f$ as training data. A multi-scale loss function of GAN is then developed, incorporating mismatch loss and feature loss to further enhance the generative performance of BF-GAN. The BF-GAN's results indicate that it has surpassed conventional GAN in all generative AI indicators, establishing for the first time a quantitative benchmark in the domain of bubbly flow. In terms of image correspondence, BF-GAN and the experimental images exhibit good agreement. Key physical parameters of bubbly flow images generated by the BF-GAN, including void fraction, aspect ratio, Sauter mean diameter, and interfacial area concentration, are extracted and compared with those from experimental images. This comparison validates the accuracy of BF-GAN's two-phase flow parameters with errors ranging between 2.3% and 16.6%. The comparative analysis demonstrates that the BF-GAN is capable of generating realistic and high-quality bubbly flow images for any given $j_g$ and $j_f$ within the research scope, and these images align with physical properties.\nBF-GAN offers a generative Al solution for two-phase flow research, substantially lowering the time and cost required to obtain high-quality data. In addition, it can function as a benchmark dataset generator for bubbly flow detection and segmentation algorithms, enhancing overall productivity in this research domain. The BF-GAN model is available online (https://github.com/zhouzhouwen/BF-GAN).", "sections": [{"title": "1 Introduction", "content": "In modern engineering, solving complex physical problems typically depends on conventional simulation and experimental approaches, and it is essential to obtain extensive experimental data to progressively develop mechanistic models and empirical correlations. The emergence of artificial intelligence (AI) has introduced a new approach driven by AI, so called AI-driven methodology. This methodology uses datasets collected through conventional simulations and experiments, along with developed physical models, as training data and loss function for AI models. This has led to the development of data-driven or physics-informed Al models, such as those for parameters prediction, classification, clustering, object detection, segmentation, tracking, and physics-informed neural networks (PINNs) that incorporate empirical correlations or mechanistic models. The methodology mentioned above is depicted in Fig. 1.\nHowever, both conventional methods and AI-based models come with high costs and substantial time investment, especially for acquiring large volumes of high-quality data. As the complexity of the problem increases, data acquisition becomes even more challenging. Consequently, a key limitation of the AI-driven methodology is its reliance on experimental and simulation-based methods for data generation, resulting in a lack of a critical component in the data development process. In this context, the emergence of generative AI addresses this gap by providing a vital solution for AI-driven data generation, as illustrated in Fig. 1. By producing large amounts of high-quality synthetic data, generative AI enhances data availability, significantly accelerating the problem-solving process and providing a powerful AI-based methodology for addressing complex engineering challenges.\nThe integration of generative AI, with conventional engineering techniques open new possibilities for tackling multiphase flow problems. One such example is bubbly flow, which is a type of gas-liquid two-phase flow characterized by the dispersion of gas bubbles within a continuous liquid phase. The bubbly flow regime is distinguished by the presence of numerous bubbles that vary in size and distribution, moving through the liquid medium. The behavior of these bubbles, including their formation, coalescence, break-up, and interaction with the liquid, significantly influences the mass transfer, heat transfer, and mixing efficiency within various industrial applications [1-5]. In the chemical industry, the distribution and size of bubbles in catalytic reactors can affect the surface area available for reactions, thus impacting the overall chemical processes of reaction rates, heat transfer, and mass transfer [6-8]. In nuclear reactors coolant systems often involve gas-liquid mixtures, where the presence of bubbles can affect heat transfer rates and system stability [9-13]. Similarly, in the development of next-generation energy technologies, such as hydrogen production via electrolysis, the formation and detachment of gas bubbles at electrode surfaces are critical factors that determine the efficiency of the process [14-17]. Understanding the complex interactions and dynamics of bubbles within these systems is therefore essential for driving innovation and achieving optimal performance in various applications.\nImage processing methods, shown in Fig. 2, encompass advanced AI-based bubble detection and tracking algorithms [18], segmentation techniques [19], or conventional computer vision technologies [20], have emerged as pivotal research tools for non-invasive detection of bubbly flow characteristics. These methods have significantly enhanced the efficiency and accuracy of detecting and analyzing bubbly flows.\nDespite these advancements, a major challenge remains: the need for large quantities of high-quality bubbly flow images as benchmark datasets. Capturing these images requires meticulous experimental setups, including the construction of specialized test loops and the utilization of high-speed cameras capable of recording the transient behaviors of bubbles in bubbly flows. These steps are not only time-consuming but also financially demanding, as it involves repeated experiments to ensure the accuracy and reliability of the collected data.\nTo further enhance the methods for obtaining large quantities of high-quality bubbly flow images, current research primarily focuses on two approaches:\nThe first approach utilizes conventional image processing techniques based on numerical computation, specifically the circular concentric approximation (CCA) method [21, 22]. This method assumes that bubble edge intensities follow a concentric circular/elliptical arrangement. Bubbly flow images are synthesized based on predefined bubble shapes and certain distribution information. However, the method fails to simulate the intricate structures of bubble shape and intensity variations, limiting them to generating simple bubble shapes, such as spherical or elliptical bubbles. Synthetic bubbles of different sizes may exhibit similar intensity distributions. Consequently, these synthetic images perform significantly differently when comparing with real bubbly flow images.\nThe second approach involves the single bubble generation model BubGAN [23]. While the generation of individual bubbles is improved, the results fail to produce realistic images of bubbly flow where multiple bubbles exist in a single image frame. Additionally, BubGAN cannot directly generate images from physical parameters such as superficial gas and liquid velocities. Generating an entire bubbly flow image requires stitching together each individual bubble, which is highly time-consuming. Although this method effectively enhances the accuracy of local bubble morphology, it is limited in capturing global features.\nThe significant advancements in text-to-image generative AI have provided valuable solutions for generating high-fidelity bubbly flow images. Prominent text-to-image AI models like DALL-E 2 [24], Stable Diffusion [25], and MidJourney [26] have made significant strides in generating high-fidelity images from textual descriptions. Fig. 3 is generated from the text prompt: \"A student in a University of Tokyo classroom is reading Energy.\" However, while these models are capable of generating a wide range of images and artwork, they have certain limitations when it comes to producing bubbly flow images. First, the interpretation of text prompts can be inconsistent, which may result in variations in image details such as lighting, contrast, and texture. Second, precise control over bubble size, shape, and location can be very challenging with these models. Achieving the desired images may require highly detailed and specific prompts. Additionally, the process of translating text into images is not fully transparent, which makes it difficult to interpret two-phase flow behavior in the generated images.\nIn the present study, a generative AI architecture termed Bubbly Flow Generative Adversarial Networks (BF-GAN) is developed, which is designed to generate realistic and high-quality bubbly flow images from physically conditioned inputs, namely, superficial gas velocity $j_g$ and superficial liquid velocity $j_f$. To train the BF-GAN, 105 sets of bubbly flow experiments with varying $j_g$ and $j_f$ are conducted, resulting in a dataset of 278,000 images. A generator developed by NVIDIA is employed to learn the features of bubbly flows. Additionally, a multi-scale loss, encompassing mismatch loss and feature loss, is incorporated into the BF-GAN to further enhance its generative performance. The generative capability of the BF-GAN is subsequently validated, demonstrating comprehensive superiority over conventional GANs in AI indicator and providing, for the first time, quantifiable benchmarks in the domain of bubbly flow generative AI. Regarding the image correspondence, BF-GAN and the experimental images exhibit good agreement. In terms of physical indicators, void fraction, bubble aspect ratio, Sauter mean diameter, and interfacial area concentration are extracted and compared with experimental images, further validating the BF-GAN's physical performance. The BF-GAN is open-sourced and available in the GitHub repository, accompanied by detailed installation and usage instructions. (https://github.com/zhouzhouwen/BF-GAN)"}, {"title": "2 Method", "content": null}, {"title": "2.1 Experimental setup", "content": "The experimental setup consists of a vertical upward two-phase flow loop, depicted in Fig. 4, designed to operate at room temperature and atmospheric pressure. Water is drawn from a 1 m\u00b3 tank by a centrifugal pump, and its flow rate is recorded using a magnetic flowmeter. Air is supplied from a buffer tank, maintained at a pressure of 0.7 MPa by an air compressor, with its mass flow rate measured by airflow sensors, pressure transducers, and a K-type thermocouple. The flow rates of both air and water are regulated by control valves and introduced into the test section via a two-phase mixer made of polyvinyl chloride and porous materials. This mixture travels through a section of clear acrylic pipe (25.4 mm internal diameter and 1 m length), enabling the observation of flow regimes. High-speed imaging is conducted 140 cm above the mixer outlet, corresponding to a length-to-diameter ratio of 55.1. After passing through the test section, the air-water mixture returns to the water tank, where it is naturally separated by gravity."}, {"title": "2.2 BF-GAN", "content": "Conditional generative adversarial network (CGAN) is a class of generative Al deep learning frameworks [27], which is designed to generate new data samples that are indistinguishable from real data. CGAN consist of two main components: the generator and the discriminator, which engage in a two-player-zero-sum game for Nash equilibrium.\nThe generator is a neural network tasked with producing data samples that resemble the training data. It takes random noise and feature matrix as input and transforms it into a data sample through a series of nonlinear transformations. Typically, the generator is composed of a deep neural network, such as a feedforward network or a convolutional neural network (CNN) [28]. The primary goal of the generator is to cheat the discriminator into believing that the generated data is real. A schematic diagram of a generator network is shown in Fig. 5. This generator follows an encoder-decoder framework, utilizing convolutional and deconvolutional layers to effectively process and transform the input into realistic images. The encoding begins with a convolutional layer that transforms the input into a 512 \u00d7 512 \u00d7 64 tensor. Subsequent layers further downsample the spatial dimensions while increasing the depth of the feature maps. The architecture employs Leaky ReLU (LReLU) activations and Batch Normalization (BN) after each convolutional layer to enhance the learning stability and convergence. The final encoding is achieved through a series of 1 \u00d7 1 convolutions, which compress the information into a 1\u00d71\u00d7 n vector, where n represents the latent space dimensions. The decoding phase involves upsampling the encoded representation back to the original image dimensions through deconvolutional layers. This process is designed to reconstruct high-resolution images that retain the characteristics defined by the input feature matrix.\nThe discriminator, on the other hand, is a neural network designed to distinguish between real data and fake data generated by the generator. It is often implemented as a deep neural network, specifically a CNN due to its efficacy in handling image data. The discriminator receives both real and fake data samples and outputs a probability indicating whether the sample is real or fake (generated). The objective of the discriminator is to correctly classify the input data, thereby improving its ability to detect fake samples. A schematic diagram of a generator network is shown in Fig. 6. The discriminator employs several convolutional layers, each followed by activation functions and normalization techniques, to extract and process the features from the input images. The convolutional layers progressively reduce the spatial dimensions of the input while increasing the depth of the feature maps, enabling the network to capture features. The final layer of the discriminator is a convolutional layer with a Sigmoid activation function, which outputs a single-channel image with dimensions 512 \u00d7 512 \u00d7 1. The Sigmoid function is used to produce a probability map, where each pixel value ranges between 0 and 1, indicating the likelihood of the corresponding input region being real or fake.\nThe training of CGAN involves a simultaneous optimization process where both networks are trained together in a zero-sum game. The generator aims to minimize the following loss function:\n$Loss_g = -E_{z~p_z(z)}[log (D(G(z,c)))] \t(1)$\nwhere G(z, c) is the output of the generator given input noise z and feature matrix c, and D(G(z,c)) is the probability assigned by the discriminator to the generated sample being real.\nConversely, the discriminator aims to maximize its classification accuracy using the following loss function:\n$LOSSD = -E_{x~p_{data}(x)}[log (D(x,c))] \u2013 E_{z~p_z(z)} [log (1 \u2013 D(G(z,c)))] \t(2)$\nhere, D(x) represents the probability that the discriminator assigns to a real data sample x with corresponding feature matrix c being real, and D(G(z,c)) is the probability assigned to a generated sample being real.\nThe overall objective of the CGAN can be expressed as a minimax optimization problem:\n$minmax Loss(D, G) = E_{x~p_{data}(x)}[log D(x, c)] + E_{z~p_z(z)} [log (1 - D(G(z,c)))] \t(3)$\nThis formulation illustrates the adversarial nature of CGAN, where the generator and discriminator are in constant competition. As training progresses, the generator becomes increasingly adept at producing realistic data, while the discriminator becomes better at identifying generated data. The objective of training is to develop an effective generator network for future image generation. Essentially, generative AI models are probabilistic models in a multidimensional space. Fig. 7 visualizes the training process of a CGAN model, depicted as a blue point cloud in three-dimensional space. The red points represent a true distribution with a mean of [0, 0, 0] and a covariance of [1, 1, 1]. As training progresses, the probability distribution of the CGAN gradually approximates the real distribution.\nConventional CGAN face several significant challenges, particularly related to the effectiveness of their generators [29] and the inherent limitations of the original CGAN loss functions [30]. One major issue is the generator's difficulty in producing high-fidelity, realistic images, often resulting in outputs that lack sharpness and exhibit noticeable artifacts. This problem is partly due to the instability commonly observed during training, where the generator learns to produce a limited variety of outputs rather than capturing the full diversity of the data distribution. Furthermore, the original GAN loss function, especially when the discriminator with conditional input becomes too strong, provides little useful feedback to the generator. This imbalance disrupts the learning process of generator.\nTo address these limitations and further enhance the generative performance of BF-GAN, the generator architecture developed by NVIDIA [31], as depicted in Fig. 8, has been utilized. This generator transforms the noise into an intermediate latent space W, which enables more stable and controllable manipulations of the generated images. The generator also employs Fourier features and 1\u00d71 convolution layers to facilitate better integration of the input latent space and spatial features, enhancing the quality of the generated images. Each layer in the network, denoted as L0-L13, is designed to progressively refine the image resolution, ensuring fine-grained details are captured accurately. The incorporation of exponential moving averages (EMA) in the weight updates further stabilizes the training process by smoothing out the parameter updates. Additionally, the custom CUDA kernel enhances computational efficiency, enabling the generator to perform complex transformations such as upsampling, downsampling, and cropping with high performance.\nMoreover, a multi-scale loss has been developed to improve the generative performance. This includes the incorporation of mismatch loss, which distinguishes between correct and incorrect matching conditions, thereby ensuring that the generator produces images that are not only realistic but also contextually accurate according to the given conditions:\n$minmax Loss(D, G) = E_{x~p_{data}(x)} [log D(x, c_t) + log (1 \u2013 D(x, c_f))] + E_{z~p_z(z)} [log (1 - D(G(z, c_t))) + log (1 \u2013 D(G(z, c_f)))] \t(4)$\nHere, $c_t$ indicates the true condition and $c_f$ represents the false condition, $c_f$ are randomly generated during the training process.\nAdditionally, feature loss has been integrated, leveraging the L1 and L2 distances to penalize average discrepancies in the features extracted by a pre-trained VGG network [32]. The VGG network is a convolutional neural network architecture recognized for its deep structure of up to 19 layers. It employs sequences of convolutional layers with small receptive fields of 3x3, followed by max pooling layers, which collectively enable the network to effectively capture complex features at multiple scales. This approach ensures that the differences are measured in a more meaningful feature space, capturing perceptual discrepancies that are more aligned with human visual perception. The L1 and L2 loss are defined as:\n$L1(G) = E_{x~p_{data}(x),z~p_z(z)} [||F(x) \u2013 F(G(z, c_t))||_1] \t(5)$\n$L2(G) = E_{x~p_{data}(x),z~p_z(z)} [||F(x) - F(G(z, c_t))||_2] \t(6)$\nwhere F() represents the feature extraction function of the pre-trained VGG network."}, {"title": "2.3 Bubble detection model based on YOLO", "content": "To quantify and validate the physical properties of images generated by the BF-GAN, a bubble detection model based on You Only Look Once (YOLO) has been developed in the previous study [33].\nYOLO is a state-of-the-art, real-time object detection AI framework designed for speed and accuracy in detecting objects within an image [34]. It predicts the locations and categories of objects through a single forward pass of the network, significantly improving processing speed. Unlike conventional object detection methods, YOLO frames detection as a single regression problem, directly mapping from image pixels to bounding box coordinates and class probabilities. This integrated approach allows YOLO to achieve high bubble detection accuracies while maintaining real-time processing speeds.\nIn previous research, approximately 600 bubbles were manually annotated using the Labelme software to create a training dataset. By training with YOLO, a bubble detection model based on YOLO has been developed and validated. The performance of bubble detection is shown in Fig. 10.\nUsing this bubble detection model, each bubble in a bubbly flow image can be detected well. The bounding box coordinates of the detected bubbles are extracted and converted to real-world dimensions. This enables the extraction of four key bubbly flow parameters from a bubbly image: void fraction, aspect ratio, Sauter mean diameter, and interfacial area concentration.\nThe development workflow of the BF-GAN is illustrated in Fig. 11.\nStep 1: Initially, the research flow pattern was identified as the bubbly flow region within the Mishima-Ishii flow regime map.\nStep 2: Videos of the bubbly flow under each condition were recorded and segmented into individual frames.\nStep 3: In the dataset, labels were assigned to all images under each specific $j_g$ and $j_f$ condition, and each was assigned a unique address.\nStep 4: The BF-GAN was trained using the prepared dataset.\nStep 5: Manual optimization of the BF-GAN parameters was performed to achieve the optimal model. Upon inputting the $j_g$ and $j_f$ conditions, the BF-GAN generates the corresponding bubbly flow images.\nStep 6: The authenticity of the images generated by the BF-GAN was verified through AI indicator, image correspondence, and two-phase flow parameters."}, {"title": "3 Results and discussions", "content": null}, {"title": "3.1 Collection of datasets", "content": "In the present study, 105 sets of experiments under varying $j_g$ and $j_\u0192$ conditions were conducted within the bubbly flow region, as categorized according to the Mishima-Ishii flow regime map and depicted in Fig. 12. Each experimental session was recorded for 200 seconds at a rate of either 10 or 20 frames per second, resulting in a total of 278,000 different images of bubbly flow. The original resolution of these images was 968 x 968 pixels. Considering the training duration for the generative AI model, BF-GAN, images resized to 512 x 512 pixels were selected for the training dataset. This resolution represents a balance of efficiency for the current study, as images at 1024 pixels would entail approximately three to four times the amount of training data compared to 512 pixels. Specific training durations and configurations will be discussed in Section 3.2."}, {"title": "3.2 Quantitative assessment of BF-GAN from the perspective of generative AI", "content": "The workstations used in this study were based on Ubuntu 22.04, equipped with an NVIDIA RTX A6000 ADA GPU featuring 48 GB of memory and an Intel i9-13900 CPU with 32 cores. GPU memory consumption during training ranged from approximately 30-40 GB. The training period extended roughly 12 to 13 days for each training. Including the time for parameter tuning, the entire training phase lasted about 100 days. For inference, despite the extensive training period, the GPU consumption was markedly reduced to approximately 2-3 GB, with each bubbly flow image being processed in less than 0.1 second."}, {"title": "3.3 Quantitative assessment of BF-GAN from the perspective of image correspondence", "content": "In this section, the image correspondence was evaluated between 525,000 bubbly flow images generated by BF-GAN and 278,000 experimental images. For each comparison, 5000 images were generated by BF-GAN, resulting in a total of 105 comparisons. Five image correspondence indicators\u2014luminance, contrast, magnitude, homogeneity, and correlation\u2014were utilized to comprehensively compare the images from real experiments with those generated by BF-GAN."}, {"title": "3.3.1 Luminance", "content": "Luminance refers to the average light intensity of an image, which can be expressed as the mean value of pixel intensities. In the evaluation of bubbly flow images, luminance aids in understanding the visibility of the bubbles and the background lighting. The formula for calculating luminance is as follows:\n$Luminance = \\frac{1}{n}\\sum_{i=1}^{n} imgi \t(7)$\nwhere n is the total number of pixels in the image, and imgi represents the intensity value of the ith pixel. Table 3 presents the comparative luminance results of these bubbly flow images. The mean of the absolute mean relative error (MAMRE) of luminance in 105 experiments is 2.22%. It indicates that the luminance of the generated images closely approximates that of the experimental images."}, {"title": "3.3.2 Contrast", "content": "Contrast refers to the degree of difference between its brightest and darkest areas, and is typically estimated by calculating the standard deviation of pixel intensities within the image. High contrast indicates a clearer outline between the bubbles and background. The formula for calculating contrast is as follows:\n$Contrast = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (imgi \u2013 \u03bc)^2} \t(8)$\nwhere, u represent the mean value of all pixel intensities in the image.  presents the comparative results for contrast. The MAMRE of contrast in 105 experiments is 2.93%. Luminance and contrast are directly related to the way light is reflected and refracted on bubble surfaces."}, {"title": "3.3.3 Magnitude", "content": "The magnitude of the gradient is commonly used to measure the intensity of edges within an image [39]. In the present study, edge detection utilizing the Sobel operator was employed to assess the magnitude of images. Comparing magnitude assists in determining whether the edges of bubbles in images generated by BF-GAN resemble those in experimental images. The calculation of magnitude includes the bubbles breakup and coalescence. The formula for calculating magnitude is as follows:\n$Magnitude = \\sqrt{sobel(x)^2 + sobel(y)^2} \t(9)$\nhere, sobel () denotes the result of applying the Sobel operator to the image in the x or y direction. Table 5 presents the comparative results for the magnitude of these bubbly flow images. The MAMRE of magnitude in 105 experiments is 24.74%."}, {"title": "3.3.4 Homogeneity", "content": "Homogeneity describes the consistency or smoothness within local regions of an image [40]. Within the context of the Gray Level Co-occurrence Matrix (GLCM), it measures the intensity near the diagonal of the matrix, reflecting the proximity of similar gray levels in the image. In bubbly flow images, high homogeneity indicates a uniform distribution of bubbles without excessive noise. This indicator is utilized to evaluate the visual smoothness and realism of bubbly flow images generated by BF-GAN. The formula for calculating homogeneity is presented as follows:\n$Homogeneity = \\sum_{i,j} \\frac{1}{1+(i-j)^2} glcmi,j \t(10)$\nhere, i and jare the pixel intensity values, glcmij is the frequency of occurrence of pixel with intensity i adjacent to a pixel with intensity j. Table 6 lists the comparative results for homogeneity. The MAMRE in all 105 experiments is 21.34%."}, {"title": "3.3.5 Correlation", "content": "Correlation quantifies the linear relationships between different gray levels in the image [41]. This indicator reflects the structure and texture patterns of bubbly flow, including the spatial distribution and position of bubbles under transitional flow regimes. The formula for correlation is given as:\n$Correlation = \\sum_{i,j} \\frac{(i-\u03bc_i)(j-\u03bc_j)glcmi,j}{\u03c3_i\u03c3_j} \t(11)$\nwhere, \u03bc\u2081 and \u03bc; are the mean values of the gray levels i and j, respectively. \u03c3\u1f31 and of are the standard deviations of gray levels i and j, respectively.\nTable 7 presents the comparative results for correlation. The MAMRE in all 105 experiments is 13.59%."}, {"title": "3.4 Quantitative assessment of BF-GAN from the perspective of two-phase flow parameters", "content": "In this section, the two-phase flow parameters of the bubbly flow images generated by BF-GAN were extracted using a bubble detection model and then compared with experimental images to validate the accuracy of the generated images' two-phase flow parameters. Considering that the bubble detection model was developed specifically for areas with low void fractions, only the 38 red points shown in Fig. 21 were utilized for the comparison."}, {"title": "3.4.1 Void fraction", "content": "The void fraction represents the volumetric fraction of the gas phase within a flow channel. It is dimensionless and varies between 0 and 1, directly influencing the flow regime and heat transfer characteristics. Mathematically, the void fraction a can be expressed:\n$\\alpha = \\frac{V_{gas}}{V_{total}} \t(13)$\nwhere, $V_{gas}$ represents the gas phase volume, and $V_{total}$ is the total pipe volume.\nThe extraction of void fraction by BF-GAN is defined as:\n$\\alpha = \\frac{V_{bubble}}{V_{total}} = \\frac{\\Sigma v_i}{\\pi D^2L/4} \t(14)$\n$V = \\frac{4}{3}\\pi abc \t(15)$\nhere, $V_{total}$ represents the total volume of the pipeline, where D is the diameter of the pipe and L is the length of the pipe. V denotes the volume of each individual bubble.\nThe actual void fraction values were obtained based on the experimental images."}, {"title": "3.4.2 Aspect ratio", "content": "Aspect ratio refers to the ratio of characteristic lengths perpendicular to the flow direction to those along the flow direction. It is dimensionless and affects the phase distribution and pressure drop, thereby impacting the thermal performance and operational efficiency of heat exchangers and micro-reactors. Its mathematical definition can be given as follows:\n$E = \\frac{Minor AxisLength}{MajorAxisLength} = \\frac{2a}{2b} \t(16)$\nThrough the bubble detection model, a and b were readily obtained, as shown in Fig. 22(b). Table 10 displays the comparative results for the aspect ratios, with a MAMRE of 2.31%."}, {"title": "3.4.3 Sauter mean diameter", "content": "Sauter mean diameter (SMD) represents the diameter of a sphere that has the same volume/surface area ratio as the average of the droplets or bubbles in the dispersion. SMD is used in the optimization of spray processes, as it influences the surface area available for mass and heat transfer processes between phases. Mathematically, the SMD ($D_{SM}$) is calculated:\n$D_{SM} = \\frac{6 \\Sigma_{i=1}^{n} V_n}{\\eta \\Sigma_{i=1}^{n} A_n} \t(17)$\n$A \u2248 4\u03c0 \\sqrt[3]{a^pb^p+a^pc^p+b^pc^p} \t(18)$\nwhere, A represents the estimated surface area of an ellipsoid bubble, calculated using Knud Thomsen's approximation. The surface area value derived from this approximation exhibits the lowest relative error when p=1.6075 [42, 43]."}, {"title": "3.4.4 IAC", "content": "Interfacial area concentration (IAC) quantifies the total interfacial area per unit volume between two phases. High IAC values typically indicate enhanced transfer rates between the two phases. Its mathematical definition can be given as follows:\n$\\alpha_i = \\frac{\\Sigma_{i=1}^{n} Ai}{Vtotal} \t(19)$\nTable 12 provides the comparative analysis of the IAC, with a MAMRE of 8.03%."}, {"title": "4 Conclusion and ongoing work", "content": "A generative AI architecture, BF-GAN, has been developed for generating high-quality bubbly flow images. A dataset comprising 278,000 images was collected from 105 different experiments with varying $j_g$ and $j_f$, serving as the training set. An NVIDIA generator and a multi-scale loss function were developed to enhance the performance of BF-GAN in generating bubbly flow images. The efficacy of BF-GAN was validated across generative AI indicators, image correspondence, and two-phase flow parameters, confirming its capability to generate high-fidelity bubbly flow images under various $j_g$ and $j_f$ flow conditions. By integrating a YOLO-based bubble detection model with conditional BF-GAN, automated generation and parameter extraction of bubbly flow images have been achieved.\nOngoing work is divided into two main areas. First, to more comprehensively and clearly generate gas-liquid two-phase flow images, three flow patterns-slug, churn, and annular-will also be included within the scope of the generative AI. A diffusion model will be employed for training generative AI across all $j_g$ and $j_f$ conditions, extending to 1024 pixels resolution. Second, inspired by the latest advancements in text-to-video and image-to-video AI generative technologies, the generation of two-phase flow videos from textual or image inputs will be explored. These videos will not only facilitate the extraction of static parameters but will also allow for the extraction of time-dependent information such as bubble velocity and interface changes. Related works will be reported in the near future."}, {"title": "Appendix 2: Comparison of the BF-GAN and experimental measurement, empirical correlations.", "content": "To further validate the authenticity of bubbly flow images generated by BF-GAN, the void fraction of the BF-GAN-generated bubbly flow images was compared with a wire mesh sensor measurement. Additionally, the aspect ratio, Sauter mean diameter, and interfacial area concentration were compared with empirical correlations."}, {"title": "1. Void fraction", "content": "Table 14 presents the comparative results of the void fraction between the BF-GAN and a wire mesh sensor, with a MAMRE for the 39 comparisons being 30.62%."}, {"title": "2. Aspect ratio", "content": "In the present study, Besagni's correlation [44] was utilized to evaluate the aspect ratio of bubbly flow images generated by BF-GAN, as presented in Eq. (20).\n$E = \\frac{1}{[1+0.45(EoRe)]^{0.08}} \t(20)$\n$Eo = \\frac{g(PL-PG)deg}{\u03c3} \t(21)$\n$Re = \\frac{JfPID}{\u03bc_l} \t(22)$\nwhere, Eo is E\u00f6tv\u00f6s number, Re is Reynolds number. $P_l$, $P_g$, g, $d_{eq}$, \u03c3, and $\u03bc_l$, denote the density of the liquid, the density of the gas, the gravitational acceleration, the equivalent diameter of the bubble, the surface tension, and the dynamic viscosity of the liquid, respectively.\nTable 15 displays the comparative results for the aspect ratios, with a MAMRE of 18.39%."}, {"title": "3. Sauter mean diameter", "content": "Hibiki's empirical correlation [45] for SMD was employed to evaluate the performance of BF-GAN.  presents the comparative results for SMD, with a MAMRE of 21.81%.\n$DSMD = 1.63 (\\frac{\u03c3}{g(\u03c1_l-\u03c1_g)})^{-0.335} \u03b1^{0.170} N_{Rep}^{0.239}(\\frac{\u03c1_g}{\u03c1_l})^{0.138} L_o \t(23)$\nhere, Lo, a, and $N_{Rep}$ are the Laplace length, the void fraction, and the bubble Reynolds number, respectively."}, {"title": "4. IAC", "content": "In the present study, Zeitoun's empirical correlation [46] was utilized to assess the IAC of images generated by BF-GAN, as outlined in Eq. (24). Table 17 provides the comparative analysis of the IAC, with a MAMRE of 17.52%.\n$\\alpha_i = 3.24 \\alpha^{0.757}(\\frac{g d_{eq} d}{\\sigma})^{0.55} (\\frac{\\mu_l}{l p_i})^{0.1} \t(24)$"}]}