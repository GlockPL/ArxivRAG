{"title": "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning", "authors": ["Jiachun Li", "Pengfei Cao", "Chenhao Wang", "Zhuoran Jin", "Yubo Chen", "Kang Liu", "Xiaojian Jiang", "Jiexin Xu", "Jun Zhao"], "abstract": "Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named eliciting, filtering and iNtegrating Knowledge in large languagE model (LINKED). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on four complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to 9.0% improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks.", "sections": [{"title": "1 Introduction", "content": "Commonsense reasoning is one of the key abilities for models to reach artificial general intelligence (AGI). To measure it, researchers designed commonsense reasoning tasks (Talmor et al., 2019; Zellers et al., 2019; Sakaguchi et al., 2020; Liu et al., 2024), which require models to answer questions based on commonsense knowledge (see Figure 1 for examples). In recent works, large language models (LLMs) (e.g. PaLM2 (Anil et al., 2023), GPT-4 (OpenAI, 2023), Llama2 (Touvron et al., 2023)) have improved performances in this task compared to small models. Nevertheless, there is still a considerable gap between them and humans. For instance, on WinoGrande (Sakaguchi et al., 2020), the accuracy of Llama2-70B is 80.2%, lagging more than ten points behind the 94.1% accuracy of humans (Touvron et al., 2023).\nTo further improve LLM's commonsense reasoning abilities, a series of works are proposed (Wang et al., 2023a; Wu et al., 2023; Li et al., 2024), which can be mainly divided into two different lines: (1) Retrieval augmentation. As shown in Case 1 of Figure 1, these methods retrieve knowledge corresponding to the question from knowledge graphs (KGs), then integrate it into the model's input as supplementary information (Chen et al., 2023; Wang et al., 2023a). (2) Self-enhancement. As illustrated in Case 2 and 3 of Figure 1, these methods employ a chain-of-thought (CoT) like prompting technique, empowering LLMs to generate the knowledge required for reasoning in the form of a rationale (Wei et al., 2022; Wang et al., 2023c; Li et al., 2023b). For the former method, considering the limited coverage of commonsense knowledge by KGs and the fact that the retriever can only capture the semantic similarity of entities, it struggles to recall effective information in complex commonsense reasoning scenarios (e.g. event-based reasoning). As shown in Case 1 of Figure 1, for the question in WinoGrande, models need commonsense knowledge that describes the relation between \u201cbe a better surgeon\" and \u201cget the easier cases\", but the most relevant knowledge \u201c(PersonX gets stitches, yEffect, PersonY will gets more medical experience)\" from ATOMIC-2020 (Hwang et al., 2021) is still far from what is required. Hence, the self-enhancement method becomes the dominant method for LLM augmentation in commonsense reasoning.\nOur work follows the self-enhancement approach. Although these methods have made some progress, they still suffer from two main challenging problems: (1) Noisy knowledge: Some works have pointed out that the rationale generated by the LLM itself may contain severe noise (Zhao et al., 2023; Gao et al., 2023; Trivedi et al., 2023) that is harmful to reasoning. For example, in Case 2 of Figure 1, the generated knowledge indicates \u201cTo wake up late means wake up later\", which is a piece of noisy information and leads to LLM's incorrect response \u201cAnswer: Hunter\u201d. (2) Invalid reasoning: Sometimes, even if reasonable knowledge is provided to the LLM, it may still result in incorrect answers (Kojima et al., 2022; Lyu et al., 2023; Lanham et al., 2023). We define this situation as the 'invalid reasoning' issue. As illustrated in Case 3 of Figure 1, while the rationale \u201cThe grave is not large enough to fully accommodate the body\" is correct for the question, LLMs still fail to draw the correct conclusions based on it. In our pilot experiments, the noisy knowledge issue accounts for 34% in all of the failure cases and the invalid reasoning issue accounts for 28%\u00b9. Hence, these two issues are not negligible for further improving the LLM's commonsense reasoning abilities.\nIn this paper, we propose a novel method named LINKED (eLiciting, filtering and iNtegrating Knowledge in large languagE moDel) to enhance the commonsense reasoning abilities of LLMs with effective knowledge. Firstly, we design the reward model to filter out the noisy knowledge generated by LLMs. We define the confidence level of knowledge based on its contribution to question-answering and use it as a supervision signal for training the reward model. Then, we propose the marginal consistent reasoning module to reduce invalid reasoning. Given a rationale, the traditional CoT-like methods only perform the reasoning process once, which may lead to wrong outputs when the probability distribution of candidate answers is relatively uniform. To avoid it, we use one effective rationale, execute multiple rounds of reasoning based on it and select the answer with the highest marginal probability.\nWe evaluate our method on extensive commonsense reasoning benchmarks. Since the traditional metric accuracy can not measure how much noisy knowledge the enhancement method brings, we propose a new metric named effectiveness-preservation score (EPS) to mitigate this gap. This metric measures both the positive and negative impact a knowledge augmentation method has on the model's reasoning. Experimental results show that our method brings significant improvements over baselines.\nWe summarize the contribution of this paper as follows:\n(1) We propose a novel method LINKED to enhance the performance of LLMs in commonsense reasoning tasks. Additionally, we introduce a novel metric EPS to evaluate both the effectiveness and harmfulness of knowledge augmentation methods.\n(2) In our method, we not only train a reward model to mitigate noisy knowledge in LLM's generations, but also devise the marginal consistent reasoning module to solve invalid reasoning issues.\n(3) We conduct extensive experiments on two benchmarks, demonstrating that our method outperforms SOTA methods. Impressively, we observe up to 9.0% accuracy improvement and 12.5% EPS improvement. Furthermore, we get several meaningful conclusions about LLM's commonsense reasoning based on the experimental results. Our code is available at: https://github.com/BugMakerzzz/linked_code"}, {"title": "2 Related Work", "content": "Commonsense reasoning is a crucial capability that language models must master to progress toward AGI. However, since commonsense knowledge is rarely explicitly expressed in texts, models perform poorly on these tasks and require additional enhancement (Talmor et al., 2019; Sakaguchi et al., 2020; Wang et al., 2022, 2024). Traditional works usually fine-tune the model on synthetic commonsense datasets, but they incur high costs, and the models trained are difficult to apply to new commonsense reasoning tasks directly (Hwang et al., 2021; Khashabi et al., 2020; Lourie et al., 2021). Recently, the excellent in-context learning (ICL) capabilities of LLMs allow us to enhance their commonsense reasoning abilities without extra training. Specifically, we can supplement the additional commonsense knowledge through retrieval augmentation (Yu et al., 2022; Chen et al., 2023; Wang et al., 2023a) or self-enhancement methods (Wei et al., 2022; Wu et al., 2023; Li et al., 2023b). Our work follows self-enhancement methods, while addressing the issues of noisy knowledge and invalid reasoning in previous methods."}, {"title": "2.2 Knowledge Enhancement for LLMs", "content": "LLMs have suffered from serious hallucination issues (Sun et al., 2024; Wen et al., 2023; Li et al., 2023a). To solve the problem, researchers retrieve related knowledge to enhance the models (Wen et al., 2023; Lu et al., 2023; Wang et al., 2023b). Firstly, several works get knowledge through search engines, they finetune models to imitate human's searching actions (Nakano et al., 2021) or use in-context learning to let the model generate API calls (Gao et al., 2023; Trivedi et al., 2023; Lu et al., 2023). Secondly, other works use KGs (such as ConceptNet (Speer et al., 2017)) as knowledge resources, they train a retriever, use it to get sub-graphs or triples from the KG and embed this extra information into the input prompt of models (Yasunaga et al., 2021; Baek et al., 2023; Chen et al., 2023). At last, researchers also elicit the knowledge inside LLMs to enhance themselves. They design new structures for the mid steps of reasoning (Yao et al., 2023a; Besta et al., 2023; Li et al., 2024) or generate higher quality rationales by referring to external knowledge sources or tools (Wang et al., 2023b; Yao et al., 2023b; Zhao et al., 2023). Our work aims to get high-quality commonsense knowledge from LLMs to further enhance their commonsense reasoning performances."}, {"title": "3 Methodology", "content": "Figure 2 demonstrates the main architecture of our LINKED method, which is divided into two phases. In the training phase, we aim to train a reward model to address the issue of noisy knowledge. To this end, we first prepare the training data and define the confidence level of the knowledge to distinguish knowledge of different quality (\u00a7 3.1). Then, we train the reward model using a ranking task based on the annotated data (\u00a7 3.2). As for mitigating the invalid reasoning issue, we propose the marginal consistent reasoning module in the inference phase. We prompt LLMs to conduct multiple reasoning processes on one effective rationale and choose the final answer based on the marginal majority vote (\u00a7 3.3)."}, {"title": "3.1 Knowledge Pool Construction", "content": "Previous studies have demonstrated that LLMs inherently contain a vast amount of commonsense knowledge (Wang et al., 2022; Liu et al., 2022; Yuan et al., 2023). Thus, here we use LLM itself as the knowledge source. When provided with a question q in the training data, we use in-context learning to prompt the model and generate multiple pieces of related knowledge, denoted as Kg. Then we instruct LLMs to predict answers to q, considering two scenarios: with access to k in Kq and without it:\n$r(q) = M(q, P_d)$\n$r(q, k) = M(q, P_k, k)$\nHere, Pd is the prompt for LLMs to generate direct answer r(q), while Pk is the prompt for LLMs to generate the answer r(q, k) based on the provided knowledge k. M represents output of LLMs. Therefore, for each knowledge piece k, we can classify it into four different confidence levels according to the correctness of r(q) and r(q, k), which is defined as follows:\n\u2022 Useful (Level 0): $r(q) \\neq a^* \\land r(q,k) = a^*$\n\u2022 Harmless (Level 1): $r(q) = a^* \\land r(q,k) = a^*$\n\u2022 Useless (Level 2): $r(q) \\neq a^* \\land r(q,k) \\neq a^*$\n\u2022 Harmful (Level 3): $r(q) = a^* \\land r(q,k) \\neq a^*$\nHere a* is the correct answer. Notably, for a pair <q, k>, the effectiveness of knowledge k in enabling the model to answer the question q correctly decreases from level 0 to level 3. Level 0 knowledge can enhance LLMs to answer questions correctly that they couldn't initially handle. In contrast, level 3 knowledge leads to incorrect responses to commonsense questions that LLMs typically answer correctly. Hence, the knowledge level can gauge its effectiveness and harmfulness, offering supervised learning signals to train a reward model."}, {"title": "3.2 Reward Model Design", "content": "In this section, we focus on training a reward model to filter out noisy knowledge.\nTraining Data We collect a set of <q,k> pairs and the corresponding knowledge level through the knowledge pooling module. To prepare training data, we need to further classify them into positive and negative examples with the label l. Considering the contribution of knowledge to answering questions, here a piece of knowledge k is defined as positive to the query q when its level is 0 or 1, otherwise, it is negative. We remove questions that related to only positive or negative knowledge during implementation.\nTraining Objective Here we encourage the reward model to give effective knowledge a higher score than the noisy one through the following objective function L(0):\n$L(0) = -y\\log(f(q, k; \\theta)) \u2013 (1 \u2013 y)\\log(1 \u2212 f(q, k; \\theta))$\nwhere y represents the knowledge label and f(\u00b7; 0) is the score predicted by the reward model. We use the Deberta (He et al., 2023) model as a CrossEncoder to encode both q and k simultaneously, then produce a confidence score f between 0 and 1. More training details and performances about our reward model are presented in Appendix A."}, {"title": "3.3 Marginal Consistent Reasoning", "content": "According to Wang et al. (2023c)'s work, the randomness in the model's output sampling may cause the invalid reasoning issue. As shown in the CoT case of Figure 3, even with a reasonable rationale, if we only sample the answer once, there remains a significant possibility of generating an incorrect option. From this perspective, to mitigate the problem, we need to adopt a more stable approach when sampling the answer.\nIn previous CoT-like works (Wang et al., 2023c; Zhao et al., 2023; Yao et al., 2023a), self-consistency is a critical method to make the final output more stable by exploring a large set of rationales. The key idea behind it can be expressed using the following formula:\n$\\arg \\max_{a} P(a|q) = \\arg \\max_{a} \\sum_{k} P(a, k|q)$\n$\\sum_{k} P(a, k|q) \\approx \\frac{\\sum_{k} P(a, k|q)}{n} \\times \\text{frequency}(a)$\nwhere a is the answer to question q, k is the generated rationale, and n is the sampling count. Based on it, we can choose the answer that receives the majority vote as the final prediction because of its highest frequency. However, when addressing difficult questions, the quality of each rationale is relatively random, leading to unstable answer distributions across different samplings based on them. Therefore, we cannot guarantee the \u2018\u2248' in the above equation to hold within a limited number of samplings. Like the Self-Consistency case in Figure 3, it is easy to select the wrong option when the probability distribution of different answers is relatively uniform (see Rationale 2 in the case).\nTo mitigate the above problem, we implement the marginal consistent reasoning module. The principle behind it is as below:\n$\\arg \\max_{a} P(a|q) \\approx \\arg \\max_{a} P(a|k^*, q)$\n$P(a|k^*, q) \\approx \\frac{\\text{frequency}(a)}{n} \\times \\text{frequency}(a)$\nSince it is unstable to continue to generate answers based on k in an auto-regressive manner, we use an effective rationale k* as the condition to shift the calculation goal from joint probability P(a, kq) to marginal probability P(a|k*,q). Hence, the search space for generating answers becomes smaller, which makes the sampling more stable. Besides, we also perform multi-round samplings for the answers. Through it, we can further decrease uncertainty during the sampling process. To make our method effective, we require a piece of k* that supports the correct answer's generation, holding the first '\u2248' in the equation. This is precisely the problem that is addressed in \u00a73.2.\nSpecifically, the process of this module is illustrated in Figure 3. For each question, we utilize the reward model to rate the generated knowledge, select the top-k pieces of it and concatenate them to create an effective rationale k*. Then we integrate it into the input and prompt the LLM to conduct multi-round reasoning. The final output is determined by taking the majority vote on the answers. Through this module, we can mitigate the invalid reasoning issue by enhancing the stability of the LLM's reasoning process."}, {"title": "4 Experiments", "content": "Datasets We conduct experiments on four representative commonsense reasoning datasets: WinoGrande (Wino) (Sakaguchi et al., 2020), HellaSwag (Hella) (Zellers et al., 2019), SocialIQA (SIQA) (Sap et al., 2019) and PIQA (Bisk et al., 2020). For each dataset, we use 500 samples from the development set as our testing set. We present more details and discussions in Appendix B.1.\nBaselines We include the following baselines in our experiments:\nFew-shot. We prompt the LLM to directly answer questions in the test set through ICL.\nFine-tuning. We fine-tune the Roberta-large model (Liu et al., 2019) on the training data and use it to predict answers. Besides, we also apply two traditional SOTA methods: UnifiedQA (Khashabi et al., 2020) and Unicorn (Lourie et al., 2021).\nRetrieval augmentation. For retrieval augmentation methods, we implement two baselines, BM25 and dense passage retrieval (DPR) (Karpukhin et al., 2020), to retrieve additional commonsense knowledge from knowledge sources.\nSelf-enhancement. We implement several self-augmentation methods, including: CoT (Wei et al., 2022), CoT-SC (SC) (Wang et al., 2023c), Self-Refine (SR) (Madaan et al., 2023), Least-to-Most (LtM) (Zhou et al., 2023).\nWe illustrate the details and prompts when implementing these baselines in Appendix B.2.\nMetrics In traditional reasoning tasks, accuracy is almost the only metric. Nevertheless, it can not measure how much benefit or harm the knowledge-enhancement method brings. For example, suppose a method produces three pieces of level 1 knowledge and two pieces of level 3 knowledge, it performs as well as another method producing three pieces of level 0 knowledge and two level 2 knowledge in accuracy. But in practice, the latter performs better since it does not harm the model's original reasoning performance. Therefore, a more detailed metric is needed to measure how many wrong answers are corrected by the method (effectiveness) and how many correct answers are made incorrect (harmfulness). To make up for the issue, we design a novel metric called effectiveness-preservation score (EPS) as follows:\n$ES = \\frac{|{q|r(q,k) = a^* \\land q \\in Q_{\\text{false}}}|}{|Q_{\\text{false}}|}$\n$PS = 1 - \\frac{|{q|r(q,k) \\neq a^* \\land q \\in Q_{\\text{true}}}|}{|Q_{\\text{true}}|}$\n$EPS = \\frac{2* ES * PS}{ES + PS}$\nwhere Qtrue and Qfalse represent sets of correct and incorrect cases of the model directly answering questions under few-shot settings. The ES quantifies the method's effectiveness in improving the model's performance on previously unanswered questions, while the PS measures the method's detrimental impact on questions the model initially answered correctly. Our EPS metric provides a measurement of the impact on both aspects.\nImplementation Details In this work, we utilize gpt-3.5-turbo-0613 provided by OpenAI as the LLM and Deberta-v3-large as the backbone of our reward model. For generation parameters, we set the temperature to 1.3 and the sample count to 5 when generating knowledge. As for the reasoning step, we set the temperature to 0.7 and the sampling count to 3. All experiments are conducted using 4 NVIDIA GeForce RTX 3090 GPUs."}, {"title": "4.2 Main Results", "content": "The main result of our experiments is presented in Table 2, from which we can obtain two key conclusions: (1) Our method effectively enhances the LLM's commonsense reasoning performance. For different datasets, our work significantly surpasses most existing SOTA methods. Impressively, on WinoGrande, our method exhibits a significant 9.0% improvement in accuracy. (2) Our method maintains a good balance between effectiveness and harmfulness. On average, we improve EPS by 5.4%, demonstrating that our method can introduce effective knowledge while avoiding damage to the LLM's original reasoning capabilities. We validate the robustness and generalizability of the results in Appendix C."}, {"title": "4.3 Ablation Study", "content": "To verify the effectiveness of the different components in our method, we conduct ablation experiments (see Table 3). The following conclusions can be drawn from the experimental results: (1) Both modules are effective. After we remove any of the two modules, the accuracy decreases, which indicates both the RM and MCR can successfully improve commonsense reasoning performance. (2) The reward model plays important roles. In most cases, removing the reward model results in the greatest performance decline. This indicates that high-quality knowledge assumes a prominent role in LLMs' commonsense reasoning."}, {"title": "4.4 Human Evaluation", "content": "In this section, we explore whether our method effectively solves the two issues found in previous work and whether our metric is effective through manual evaluation.\nMethod Evaluation We manually verify whether our method truly resolves the two issues mentioned in \u00a71. Firstly, for the noisy knowledge issue, we conduct the case study, comparing the first and last knowledge ranked by the reward model (see Table 4). As we can see, the knowledge ranked 1st contains the key evidence that leads to the correct answer, while the knowledge ranked 5th contains the wrong statement without any evidence to support it. Therefore, our method can effectively mitigate noisy knowledge by assigning it a lower score. Secondly, for the invalid reasoning issue, we manually annotate and compute the occurrence rates of the issue under different methods (see Table 5). It demonstrates that our method can reduce the rate across different datasets, mitigating this issue.\nMetrics Evaluation We compare the correlations of the ES and PS with the human evaluation scores separately. The intuition is that a good evaluation metric should assign a good score to a good method (i.e. effective or harmless). Thus, we manually evaluate the effectiveness and harmfulness of the injected knowledge generated by different methods (DPR, CoT, Ours), calculating Pearson's correlations under different cases (see results in Table 6). In most cases, our metrics show a high positive correlation with human evaluations (\u2265 0.80), indicating the effectiveness of these two scores. Since the EPS metric is the average of them, we can further prove its validity and reliability.\nWe present additional evaluation results and detailed experimental setups in Appendix D."}, {"title": "4.5 Experimental Factors Analysis", "content": "In our experiments, various factors can influence the performance, here we aim to draw general conclusions by observing the effects of them.\nTop-k Knowledge The top-k knowledge is selected to construct the final rational in the inference time, we change this value and compare their difference, whose results are shown in Figure 4a. We find that the optimal value for top-k is no more than 2. Compared to the introduction of a large volume of relevant knowledge, the filtration of knowledge is more crucial for LLMs.\nSampling Counts We change the numbers of generated knowledge to figure out whether more sampling counts make it more likely to bring effective knowledge. As illustrated in Figure 4b, the number of effective knowledge produced by a model does not directly correlate with the sampling count. LLMs exhibit significant quality fluctuations between multiple rounds of generation.\nSampling Strategy In our MCR module, we only construct one rationale and sample multiple answers. Here, we explore the performance of integrating other sampling strategies. Concretely, we compare the accuracy under four settings: one rationale + one answer (OO), one rationale + multi-answer (OM), multi-rationale + one answer (MO), and multi-rationale + multi-answer (MM). We set the top-k value to 3 and the sampling count to 3. As we can get from 4c, OM and MM perform the best among all, but considering the higher cost of the latter, our MCR module adopts the former."}, {"title": "4.6 Effects Analysis of Different Methods", "content": "We evaluate the effect of different methods on the model's performance using ES and PS scores. The results are shown in Figure 5, from which we get the following findings: (1) Retrieval augmentation methods have low harmfulness but also low effectiveness. From the results, we can see that the BM25 and DPR methods get higher PS and lower ES among all the methods, proving that these methods struggle to retrieve effective information. (2) Self-enhancement method can cause significant harm to the model's commonsense reasoning. As for self-enhancement methods (i.e. CoT, SC, SR, LtM), they have a relatively higher ES but lower PS as well, highlighting the serious noisy knowledge issues in these methods. Our method performs well in both effectiveness and harmfulness (high ES and high PS)."}, {"title": "4.7 Cost Analysis", "content": "To demonstrate the efficiency and practicality of our method, we calculate its average token cost per example and compare it with other methods (see Table 7). As we can see, compared to other self-enhancement methods (e.g. SR, LtM), our method uses significantly fewer tokens, averaging only twice the number used by the basic CoT method. This indicates that our method can achieve high performance in commonsense reasoning with fewer computational resources during downstream inference. Our method is also cost-efficient when training, which we discuss in Appendix A.3."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel method named LINKED to enhance the LLM's performance on commonsense reasoning tasks. Specifically, we train a reward model to filter out noisy knowledge in LLM's generation and take the marginal consistent reasoning module to reduce invalid reasoning. Besides, we design a new metric named EPS to evaluate both the effectiveness and harmfulness of different knowledge enhancement methods, which the former metric can not. We conduct comprehensive experiments on four representative commonsense reasoning benchmarks, and experimental results demonstrate that our method significantly outperforms previous baselines."}, {"title": "Limitations", "content": "While our method significantly improves LLM's performance in commonsense reasoning tasks, it has two primary limitations: (1) The black-box nature of the LLM we study hinders our ability to delve deeper into the model and explain why the filtered knowledge is effective. (2) Due to time and resource constraints, we were unable to conduct extensive prompt design work, which could have further improved our method's performance. We leave these limitations as our future work to explore."}]}