{"title": "Do Automatic Factuality Metrics Measure Factuality? A Critical Evaluation", "authors": ["Sanjana Ramprasad", "Byron C. Wallace"], "abstract": "Modern LLMs can now produce highly readable abstractive summaries, to the point where traditional automated metrics for evaluating summary quality, such as ROUGE, have become saturated. However, LLMs still sometimes introduce unwanted content into summaries, i.e., information inconsistent with or unsupported by their source. Measuring the occurrence of these often subtle \u201challucinations\" automatically has proved to be challenging. This in turn has motivated development of a variety of metrics intended to measure the factual consistency of generated summaries against their source. But are these approaches measuring what they purport to do? In this work, we stress-test automatic factuality metrics. Specifically, we investigate whether and to what degree superficial attributes of summary texts suffice to predict \u201cfactuality\u201d, finding that a (supervised) model using only such shallow features is reasonably competitive with SOTA factuality scoring methods. We then evaluate how factuality metrics respond to factual corrections in inconsistent summaries and find that only a few show meaningful improvements. In contrast, some metrics are more sensitive to benign, non-factual edits. Motivated by these insights, we show that one can \u201cgame\u201d (most) automatic factuality metrics, i.e., reliably inflate \"factuality\" scores by appending innocuous sentences to generated summaries. Taken together, our results raise questions about the degree to which we should rely on existing automated factuality metrics and what exactly we want \"factuality metrics\" to measure.", "sections": [{"title": "1 Introduction", "content": "LLMs are generally strong abstractive summarizers (Goyal et al., 2022; Zhang et al., 2024). Still, they are not infallible. Even the largest, most capable models sometimes introduce subtle \u201challucinations\" (or \"confabulations\u201d) into summaries that are unsupported by or in contradiction to the corresponding input document (Zhang et al., 2024; Tang et al., 2024b; Ramprasad et al., 2024b). Such behaviour is especially problematic in domains such as medicine or law, where inaccurate information could translate into meaningfully negative consequences for individuals.\nHowever, manually evaluating model outputs' faithfulness with respect to their source is expensive, time-consuming, and impractical to scale. This has motivated the development of automated methods that score generated summaries for faithfulness with respect to reference documents. Such metrics have been operationalized using a range of underlying techniques, including entailment (Laban et al., 2022; Scir\u00e8 et al., 2024), QA models (Scialom et al., 2021; Fabbri et al., 2021b), specialized models explicitly trained to score source-summary pairs (Zhong et al., 2022; Zha et al., 2023; Tang et al., 2024a), and more recently, prompt-based methods that rely on LLM calls (Luo et al., 2023; Wang et al., 2023a).\nMost of these metrics have been evaluated against human benchmark assessments (binary labels or Likert scores) of factual consistency (Maynez et al., 2020; Fabbri et al., 2021a; Laban et al., 2022; Honovich et al., 2022; Wang et al., 2022; Gao et al., 2023; Tang et al., 2024a). These assessments have established that automated factuality metrics correlate with human evaluations to varying degrees. But are such approaches actually attuned to subtle question of factual consistency between inputs and outputs, or are they merely relying on shallow heuristic signals\u2014such as lexical overlap, entity repetition, or the presence of specific phrases\u2014to make their judgments? For instance, Kamoi et al. (2023b) assessed the reliability of QA-based metrics and found that while they can predict summary-level factuality, they fail to identify correct error spans, and are outperformed by simple exact match baselines."}, {"title": "2 Experimental Setup", "content": "Automatic factuality metrics for summarization have been evaluated using various benchmarks, predominantly based in news sources (Hermann et al., 2015; Narayan et al., 2018). The AggreFact dataset (Tang et al., 2022) consolidates several benchmarks on fine-tuned model-generated summaries from such sources (Maynez et al., 2020; Wang et al., 2020; Pagnoni et al., 2021; Fabbri et al., 2021a; Honovich et al., 2022; Laban et al., 2022).\nSimilarly, datasets like FacEval (Wang et al., 2022) and ReferenceMatters (Gao et al., 2023) benchmark dialogue summarization of fine-tuned models on sources from SAMSum (Gliwa et al., 2019a) and DialogSum (Chen et al., 2021b).\nHowever, these datasets primarily focus on summaries generated by fine-tuned models that predate modern LLMs, which have demonstrated superior zero-shot summarization capabilities (Goyal et al., 2022). As a result, the performance of factuality metrics on these benchmarks may not accurately reflect their effectiveness on outputs from SOTA LLMs, which are prone to different types of errors (Tang et al., 2022). To bridge this gap, Tang et al. (2024a) introduced the LLM-AggreFact dataset, which includes factuality labels for summaries generated by recent LLMs across a variety of domains, including Wikipedia, interviews, news, dialogues, science, and healthcare. Similarly, Krishna et al. (2024) released the GenAudit dataset with factuality annotations for LLM summaries in news, Reddit, and clinical settings, while Ramprasad et al. (2024a) focus on LLM generated dialogue summaries in the LLM-dialogue dataset.\nFor our analysis we use all of the above benchmarks to capture a wide range of error types. For fine-tuned model summaries, we use AggreFact for news and FacEval for dialogues. For LLM-generated summaries, we rely on LLM-AggreFact, GenAudit, and LLM-dialogue across varied domains. We note that each benchmark consolidates multiple datasets and to ensure clean separation of distributions, we avoid overlapping datasets between our test and development splits. We achieve this by randomly selecting datasets from each benchmark for the development set (we include only the dev split), while the remaining datasets are reserved for testing (using their respective test split). Given its smaller size, the GenAudit dataset is included exclusively in the test set. A detailed breakdown of the dev and test splits is provided in Appendix A.1."}, {"title": "2.2 Automatic Factuality Metrics for Summarization", "content": "We group SOTA factuality metrics into four broad methodological categories. This includes metrics based on: Question Answering (QA), Natural Language Inference (NLI), fine-tuned (specialized) models and LLM prompting methods; see Table 1. For QA-based metrics we use QuestEval (Scialom et al., 2021). As an NLI-based metric we use SummaC-Conv (Laban et al., 2022). We use three specialized models for evaluation: UniEval, AlignScore and MiniCheck. UniEval (Zhong et al., 2022) reframes NLG evaluation as a Boolean QA task and uses T5 (Raffel et al., 2020) to score different dimensions. AlignScore (Zha et al., 2023) evaluates summaries by combining an alignment function\u2014a RoBERTa model (Liu, 2019) fine-tuned on diverse tasks\u2014with a splitting and aggregation strategy. MiniCheck(Tang et al., 2024a) uses a Flan-T5 model (Chung et al., 2022) fine-tuned on a synthetic dataset created by the authors.\nWe also use GPT-4 to score the factual consistency of summaries based on a direct assessment (DA) prompt template from Wang et al. (2023a)."}, {"title": "2.3 Heuristic Indicators of \"Factuality\"", "content": "The methods for automatically scoring summary factuality reviewed above often rely on complex models, such as QA or NLI-based approaches, or specialized models. Could simpler heuristics provide comparable signal? Here we explore a set of simple features for predicting \u201cfaithfulness\" to evaluate if they are sufficient for this task.\nOne of the simplest features we use is lexical overlap between a summary and its source. We measure this using ROUGE-2 F1 which matches word pairs or bigrams between the summary and the source.\nWe also include entity overlap, i.e., the proportion of entities in the summary that are present in the corresponding source text.\nAnother predictor we use is semantic similarity. Specifically, embed summary and source sentences via BERT (Devlin, 2018), then score each summary sentence based on which source sentence it is most similar to (using cosine similarity). We average all summary sentence scores to derive a final score.\nWe include word and sentence novelty ratio features, which measure the proportion of words and sentences in the summary that do not appear in the source, respectively.\nFinally, we measure text conciseness by calculating a conciseness ratio, or the ratio of the length (number of words) of the source document to the length of the summary. This measures the extent to which a summary has condensed the original text."}, {"title": "3 Are Superficial Features Sufficient to Infer \"Factuality\"?", "content": "The degree to which a summary is faithful to the text it summarizes is a subtle question that demands understanding the content in both texts to determine if they are consistent. We would therefore expect that metrics capable of measuring factual consistency in general would need to capitalize on information beyond the superficial features of source and summary texts just discussed.\nTo assess if this is in fact the case we first evaluate the extent to which a model using superficial features alone can be trained to infer factuality. We train an MLP to predict (human) factuality labels as a function of shallow input features. We compare the results achieved-specifically, AUC scores\u2014using this method to a suite of more sophisticated approaches that have been previously proposed.\nThe shallow model has been explicitly supervised. To replicate the scenario of metrics trained on different datasets, we ensure that our training set is entirely separate from the benchmark datasets in the test set; this makes the approach akin to the supervised, \"specialized\" models for measuring factual consistency considered.\nWe emphasize that the purpose of this exercise is not to propose a practical \u201cnew\u201d approach to score summaries for factuality. Rather, our point here is to determine the extent to which existing metrics improve performance beyond what can be achieved using simple, superficial attributes of text."}, {"title": "4 What do automatic factuality scores measure?", "content": "We next attempt to characterize what factuality metrics measure. First, we examine the correlations between shallow features and the scores produced by these metrics (4.1). Then (4.2), we evaluate the sensitivity of the metrics to controlled changes in summary faithfulness, measuring how effectively they respond to corrections of factual inaccuracies. This allows us to assess their ability to detect and reflect improvements in factual consistency."}, {"title": "4.1 Predicting Automatic Factuality Measures with Shallow Features", "content": "We aim to evaluate the extent to which metrics rely on shallow features. To this end we first assess whether we can replicate the scores assigned to summaries by SOTA factuality consistency metrics using shallow predictors only.\nTo explore this, we again train an MLP model that takes these shallow features as inputs. However, instead of predicting factual inconsistency labels directly (as in the preceding section), we train the model to predict the corresponding scores assigned by the SOTA factuality metrics. In other words, we ask whether such metrics could, in principle, be a function solely of heuristic indicators. Figure 4.1 reports Spearman rank correlations between the predicted scores from the MLP and the metric scores on the test set.\nThese results show a moderate-to-strong correlation between the predicted scores of the MLP model, trained on shallow features, and the actual metric scores. Note, UniEval and ChatGPT exhibit the weakest correlation among the metrics. These correlations alone do not provide definitive evidence that the metrics necessarily over-rely on shallow indicators; however, they do suggest that such reliance may be a possibility. To try and establish a causal link (i.e., whether such metrics are sensitive to heuristic indicators rather than underlying changes in factual inconsistencies) we next perform a series of controlled manipulations to summaries and measure their responsiveness to these."}, {"title": "4.2 Measuring Metric Sensitivities to Controlled Manipulations", "content": "Here we examine the sensitivity of automatic factuality metrics to factual corrections and benign modifications. We aim to clarify whether metrics are responsive to changes in factual consistency, and/or if they are unduly influenced by superficial adjustments with no bearing on factual consistency.\nFor this analysis, we focus on a subset of summaries manually labelled as inconsistent by human annotators. For each of these, we have a paired, minimally edited version that has been corrected for consistency. These inconsistent/corrected summary pairs were released with the Genaudit dataset (Krishna et al., 2024). Using these, we can measure metric sensitivity to factual consistency (the actual quality of interest).\nWe are also interested in score variation that owes to superfluous factors. To this end, we prompt GPT-4 to create versions of summaries modified to vary in targeted ways that should be independent of factual consistency. For this we use several prompts requesting different transformations to summaries: Adding least relevant sentences from the source to extend summaries (added source text); Reducing vocabulary diversity (less diverse); Introducing negation words in a meaning-preserving way (negated); Paraphrasing; Shuffling sentence order (shuffled), and; Simplifying text (simplified) or replacing random words in the summary with a synonym (synonym replacement). We provide prompts for summary edits in Appendix C.1.\nIdeally, automatic factuality metrics should show a positive score change for corrected summaries, reflecting improved factual consistency. Conversely, generated summary rewrites\u2014which do not alter factual content-should exhibit minimal score changes. Any significant score differences in these rewrites likely indicate that the metric is sensitive to artifacts incidental to factual consistency."}, {"title": "5 Can we game factuality metrics?", "content": "Given the preceding observations\u2014which suggest that superficial cues may influence automated factuality metrics, we turn to a practical question that would exploit this behavior: Are these metrics gameable? In other words, can we systematically manipulate summaries to induce higher factuality scores? If one can reliably do this, it may raise concerns about the use of these metrics for tasks like leaderboard rankings. Independent of practical considerations, establishing the gameability of factuality metrics would provide additional evidence that they may not be measuring what we think.\nAs a \"gaming\u201d strategy, we introduce innocuous phrases containing no factual content, testing their effect on metric scores as standalone inputs and as additions to existing summaries. These phrases, when appended, should not alter the factual accuracy of the summaries they accompany. Our goal here is to assess whether such neutral modifications can nonetheless inflate factuality scores."}, {"title": "Strings for summary manipulation", "content": "We attempt to identify a set of strings that artificially inflate metric scores. To do so, we first identify the top 20th percentile of summaries ranked by each factuality metric, reasoning that these high-scoring summaries may contain patterns that raise scores, independent of context. To this end, we compute TF-IDF scores for all bigrams in these summaries, identifying those disproportionately present in summaries deemed \u201cfactual\u201d by each metric.\nWe select from these the top 100 bigrams, aggregating results across metrics. This set includes, e.g., \"the document\u201d and \u201cdocument discusses\u201d. We adopt the constant phrase \"the document discusses\" (top phrase) into all documents and observe whether metrics are responsive to this. We consider one additional phrase: such as \"The summary entails information in the document.\" (assertion phrase) which explicitly asserts factual consistency. The wording of this phrase varies slightly across metrics to align with their specific methods for evaluating factual consistency. The complete list of these phrases for each metric is provided in the Appendix D.1. Finally, we append both phrases to the corresponding summaries summ (+top) and summ (+assertion).\nOur goal here is to determine whether the appended text unduly influences metrics, potentially resulting in inflated scores that do not accurately reflect the factual quality of the original content.\nWe report the average pairwise difference in metric scores between the gamed versions of summaries and their original versions when evaluated for consistency with the source. Examples of these manipulated summaries and their corresponding scores are provided in Table 2."}, {"title": "Results and Discussion", "content": "Figure 5.1 reports the effects of our gaming strategies on metric scores. Notably, the constant phrases boosts scores by \u22650.2 points (absolute) for NLI-based SummaC-Conv, as well as specialized models UniEval, Align-Score, and MiniCheck. This is surprising, as these phrases are not valid summaries and do not contain any factual content (note that top phrase is an incomplete sentence). Adding constant phrases as suffixes to summaries increased scores by 0.1-0.15 points; this is comparable to the gains realized following factual corrections, as reported in Section 4.2. In fact, SummaC-Conv shows no score increase for corrected summaries, suggesting an under-sensitivity to actual changes in factuality. To further contextualize these numbers, we analyze metric score differences between summaries generated by larger models (e.g., GPT-4, Gemini) and smaller models (e.g., Llama-7B, Mistral-7B, Falcon-7B, BART). Larger models generally produce more consistent summaries (Tang et al., 2022; Goyal et al., 2022), providing a baseline for assessing gaming strategies. A natural question, then, is whether \"gaming\" achieves score improvements comparable to those that have resulted from advances in model size and complexity.\nWe analyze a subset of 115 document-summary pairs from GenAudit (Krishna et al., 2024) and Aggrefact-old (Tang et al., 2022), where model information is available. Our findings reveal that gaming with constant phrases leads to metric score improvements 3x to 8x greater than those achieved through genuine improvements in generated summaries, particularly for NLI and specialized metrics. For instance, gaming increases SummaC scores by nearly 0.2 points, while model improvements have minimal impact. Similarly, adding suffixes to summaries achieves gains comparable to those won with better summarization models.\nQuestEval is mostly unaffected by these manipulations. ChatGPT-DA shows a negative trend: Scores drop when summaries are \"gamed\". Further analysis suggests that the QA-based metric generates questions based on innocuous phrases and entity repetitions that introduce noise, leading to unchanged or lower scores. ChatGPT-DA's performance decline likely results from its strict adherence to the prompts. A qualitative analysis, where the model was prompted to explain its scoring, revealed that in several instances, it explicitly identified the gaming phrase as misleading. However, we observe that ChatGPT-DA's scores can be inflated by replacing summaries with a disclaimer acknowledging potential errors in interpreting the source (see Appendix D.2).\nOverall, these results suggest that adding fixed phrases to summaries can boost metric scores-for most finetuned model metrics\u2014at levels comparable to, or even exceeding, presumably genuine improvements due to advances in summarization models themselves.\nThe innocuous, non-factual phrases used in our experiments above constitute \"filler\" phrases, which introduce no substantive content. Since most factuality metrics primarily assess factual precision, we hypothesize that they may struggle to differentiate between informative content and irrelevant filler text leading to a weakness that could be exploited. Our experiments above provide some evidence of this and we explore it further in this section. First, in addition to \"gaming prefixes\u201d we consider two other types of constant strings that are neutral phrases: They are independent of our analysis of top-scoring bigrams and do not allude to the consistency of summaries with respect to the document. We measure the pairwise difference in metric scores when these phrases are appended as a point of reference.\nWe consider two neutral phrase types: a \u201cbaseline\" phrase, consisting of two generic phrases that don't mention the summary or source, and a \"qualifier\" phrase, suggesting the summary may have multiple interpretations (see Table 6 for phrases). This contrasts with our \u201cassertion\" phrase, which directly affirms the summary's faithfulness. These filler phrases should ideally not affect consistency scores, as they don't alter factual content. We also compare how metrics respond to these phrases versus \"gaming\" phrases to determine if only targeted fillers increase scores.\nAs shown in Appendix D.2, all metrics assign lower scores to summaries with a baseline phrase, indicating that not all irrelevant sentences inflate scores. However, qualifier phrases have similar effects to gaming prefixes on MiniCheck and UniEval, suggesting that neutral phrases referencing the summary may boost scores for these metrics. For SummaC and AlignScore, only specific gaming prefixes increase scores, while other phrases reduce them. Interestingly, ChatGPT, which usually shows a negative trend, exhibits inflated scores when the summary is replaced with a neutral qualifier phrase acknowledging potential errors in interpreting the source.\nAlthough human evaluators may accept such neutral phrases as consistent, they likely assess models more comprehensively, identifying deeper inconsistencies. In contrast, metrics are not sophisticated enough to capture these nuances and can be influenced by irrelevant information, inflating scores independent of factual consistency."}, {"title": "6 Discussion", "content": "Distilling salient information from large volumes of unstructured text-i.e., summarization is a compelling practical use case for LLMs. As they have grown increasingly competent at this task, it has become concomitantly difficult to evaluate their performance, which requires assessing often subtle but potentially critical factual inconsistencies between inputs and generated summaries. Safe deployment of, and continued progress on, such approaches to summarization requires automatic metrics to assess factual consistency automatically.\nMany such approaches have been proposed in the literature, but there has been little effort to stress-test them, which is the goal of the present work.\nWe found first that a simple MLP fine-tuned over superficial features and summaries can realize performance competitive with SOTA factual consistency metrics. We then showed that while such metrics are generally responsive to manual changes affecting factual consistency, they are also often sensitive to spurious transformations which should not affect scores. Motivated by these findings, we then showed that factual consistency metrics are often gameable, i.e., in many cases we can add fixed phrases as suffixes into output summaries or use them standalone to reliably inflate \u201cfactuality\" scores beyond gains observed via improvements in model size and complexity."}, {"title": "Limitation and Ethics", "content": "This work has several important limitations that should be taken into account when interpreting our results.\nFirst, in Section 3, we demonstrated that a model trained solely on heuristic indicators of factual consistency can achieve performance on par with state-of-the-art (SOTA) approaches, though this was a supervised model specifically for consistency detection. Despite our training data being sourced from entirely different datasets and domains than those used for evaluation, it is important to highlight that the dataset was manually annotated. However, we cannot yet conclude how a shallow model would perform if trained on heuristic-based predictors of synthetically generated summaries\u2014this would be most similar to the data that other (specialized) metrics have been trained with. We emphasize, however, that our goal with these experiments was not to introduce a new metric, but rather to investigate how much current metrics improve upon basic textual attributes.\nAnother limitation here is that one could quibble with just how \u201cshallow\u201d these predictors are. Our view is that even the more involved features we use for this\u2014e.g., semantic similarity based on BERT embeddings and entity overlap-should not be taken as reliable measures of factual consistency, but empirically they do correlate with manual annotations of factual consistency (see Appendix B). We can only speculate regarding the underlying causal structure based on these results.\nOur subsequent interventions in Section 4.2 established that in some cases metrics are sensitive to extraneous transformations (e.g., re-phrasings). However this was not observed uniformly, and we performed such transformations using LLMs, which may have introduced legitimate factual inconsistencies (though we believe this to be rare). A limitation to our \"gaming\" experiment is that it involved a manual process; however, this does not affect the main result, which is that there exist strings that can be appended to outputs in such a way that consistently boosts scores.\nA further limitation to this (and all manipulation experiments) is that they produce outputs which may be viewed as \u201cout of distribution\u201d; but generally the purpose of summary factuality metrics is to be able to apply them to arbitrary model outputs and corresponding references, so it is not entirely clear what \"in distribution\" means under this assumption.\nDespite these limitations, we believe that-taken together our results suggest that one should be careful about their interpretation of automatic factual consistency metrics. And we hope these results inform research on summarization moving forward."}]}