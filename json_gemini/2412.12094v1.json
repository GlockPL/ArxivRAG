{"title": "SepLLM: Accelerating Large Language Models by Compressing One Segment into One Separator", "authors": ["Guoxuan Chen", "Han Shi", "Jiawei Li", "Yihang Gao", "Xiaozhe Ren", "Yimeng Chen", "Xin Jiang", "Zhenguo Li", "Weiyang Liu", "Chao Huang"], "abstract": "Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity. In this work, we have identified a key pattern: certain seemingly meaningless special tokens (i.e., separators) contribute disproportionately to attention scores compared to semantically meaningful tokens. This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss. Guided by this insight, we introduce SepLLM, a plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens. Additionally, we implement efficient kernels for training acceleration. Experimental results across training-free, training-from-scratch, and post-training settings demonstrate SepLLM's effectiveness. Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities.", "sections": [{"title": "1. Introduction", "content": "Transformer-based models (Vaswani et al., 2017) have exhibited exceptional performance across a wide range of tasks, including natural language processing (Zhang et al., 2020; Raffel et al., 2020), computer vision (Dosovitskiy et al., 2020), and scientific machine learning (Geneva & Zabaras, 2022). However, vanilla Transformers that rely on next-token prediction face significant computational challenges, particularly when scaling to larger models and longer contexts. These computational inefficiencies significantly impact both inference speed and training time.\nThe core challenge underlying these efficiency issues is the self-attention module, which exhibits quadratic complexity with respect to the number of input tokens. Research on efficient Transformers in LLMs primarily follows two major directions. The first approach focuses on linear attention (Katharopoulos et al., 2020; Schlag et al., 2021), replacing the vanilla self-attention module with alternatives that achieve linear complexity. However, these architectural modifications are significantly different from traditional self-attention, preventing direct utilization of powerful pre-trained Transformer models. The second approach emphasizes KV cache optimization (Xiao et al., 2024a; Zhu et al., 2024; Xiao et al., 2024b; Li et al., 2024b), aiming to eliminate redundant KV cache to accommodate longer input contexts. For example, Xiao et al. (2024a) introduced an adaptive mechanism that selectively retains essential tokens and their key values based on cumulative attention scores. Similarly, Zhu et al. (2024) proposed a token selection strategy with controlled sparsity, achieving near-lossless acceleration. While promising, these training-free methods adapt poorly to the training stage, resulting in discrepancies between training and inference performance. StreamingLLM (Xiao"}, {"title": "2. Related Work", "content": "KV Cache Compression. Recent research has focused on overcoming LLMs' limitations in processing extensive contextual inputs. FastGen (Ge et al., 2024) proposes an adaptive KV cache management method, optimizing memory usage by customizing retention strategies for different attention heads. SnapKV (Li et al., 2024b) enhances efficiency through KV cache compression, utilizing attention scores to select and cluster significant positions. H2O (Zhang et al., 2024b) implements a dynamic token retention policy, balancing recent and historically important information to optimize memory use. StreamingLLM (Xiao et al., 2024b) expands LLMs' capabilities to handle infinite sequence lengths without fine-tuning, by reserving attention sinks and local tokens. QuickLLaMA (Li et al., 2024a) proposes to evict the query-aware KV caches for inference acceleration. PyramidInfer (Yang et al., 2024) and PyramidKV (Zhang et al., 2024a) modify the KV cache capacity throughout various layers, with a larger allocation in the lower sections and a reduced amount in the upper portions. However, most works in this category cannot be applied into training phase.\nSparse Attention. Sparse attention involves creating sparse attention matrices by limiting attention to predefined patterns, such as local windows or fixed-stride block patterns. Beltagy et al. (2020) combine dilated local window attention with task-specific global attention. BigBird (Zaheer et al., 2020) proposes a linear complexity attention alternative using global tokens, local sliding window attention, and random attention. In comparison, SparseBERT (Shi et al., 2021) proposes a differentiable attention mask algorithm to learn the attention mask in an end-to-end manner. Note"}, {"title": "3. Method", "content": "3.1. Fundamental Design\nFrom Figure 2, we can observe that within a given input context, seemingly \u201cmeaningless\" separator tokens receive higher attention scores compared to tokens with actual semantic meanings. Therefore, we propose a novel Transformer architecture where, for a certain layer of the transformer (i.e., a self-attention layer), each token in the input can only see a portion (not all) of the hidden states of tokens preceding the current token, outputted by the previous transformer layer. This subset of tokens includes the initial few words (i.e., attention sinks (Xiao et al., 2024b)), all separator tokens before the current token, and the closest n tokens to the current token. Details are as follows.\nInitial Tokens. When using the sliding window mechanism (Beltagy et al., 2020) for generation, removing the key-value (KV) pairs corresponding to the initial tokens in the KV Cache results in a noticeable increase in the perplexity of generated tokens, a phenomenon mentioned by Xiao et al. (2024b). The initial few tokens are also referred to as attention sinks. We retained this setup and further validated the role of initial tokens in subsequent experiments. Usually, a initial tokens are kept.\nSeparator Tokens. From Figure 2, we can observe that within a given input context, seemingly \u201cmeaningless\" separator tokens (such as commas, periods, exclamation marks, semicolons, etc.) that segment sequences receive higher attention scores compared to semantically meaningful tokens (such as nouns or verbs). Therefore, we hypothesize that these separators may compress the information of the text segments naturally segmented by them, such that when the Transformer generates new tokens, it only needs to reference the information contained in these separators to extract the information pertaining to those text segments. Hence, in a training-free scenario, we employed this strategy and achieved similar results to the original model based on full attention across many tasks. Furthermore, to reinforce the effect of using separators to compress information within their respective segments, we employed training-from-scratch and post-training approaches to compel the model during training to restrict the current token from accessing all information from distant preceding text, i.e., in each segment, only the separator representing its segment is visible to the current token (with other tokens being masked, see Figure 3). After training in this manner, the information within segments is forced to be condensed into the separators, leading the Transformer's probability distribution for predicting"}, {"title": "3.2. Overall Pipeline", "content": "We split the overall pipeline of our proposed SepLLM into training/pre-filling stage and generating stage.\nTraining/Pre-filling. During the training/pre-filling stage of SepLLM architecture, we do not need to multiply all query vectors corresponding to tokens in the input context with all the key vectors. It is sufficient to just multiply the vectors of the query-key pairs corresponding to the highlighted elements in the mask matrix shown in Figure 3. The formulation can be illustrated in the following.\n$A = Softmax(\\Lambda), \\Lambda = \\frac{Mul(Q, K^T)}{\\sqrt{d_k}}$\n$O = A.V$ (1)\nwhere $Q \\in R^{m \\times d_k}, K \\in R^{m \\times d_k}$ are the matrices of query and key for one attention layer, in which each row vector $Q_i, K_j$ correspond to the query of i-th token and the key of j-th token in the input context with sequence length m. $d_k$ denotes the dimension for key and query vectors. $\\Lambda, A \\in R^{m \\times m}$ are the raw and final attention maps, respectively. $V \\in R^{m \\times d_v}$ is value matrix of dimension $d_v$"}, {"title": "3.3. Tailored Streaming Design", "content": "In real-world scenarios, there are numerous streaming applications such as multi-round dialogues, where long interactions are expected (Xiao et al., 2024b). Hence, we expect SepLLM to handle infinite input without significantly sacrificing efficiency and performance, especially for the streaming applications. As discussed in Fundamental design (Section 3.1), SepLLM can save a substantial amount of KV cache by retaining only the KV for separators, neighboring and initial tokens. However, as the number of input tokens increases, the number of separators in KV cache will also accumulate endlessly, which is not feasible for streaming settings. Therefore, we propose Tailored Streaming Design for streaming scenarios.\nFramework. Figure 4 illustrates the SepLLM's processing architecture for streaming inference applications. The diagram depicts multiple iterations, with each row representing a distinct processing cycle. The system simultaneously maintains four specialized cache blocks: Initial Cache, Separator Cache, Past Window Cache, and Local Window Cache. Specifically, Initial Cache captures the attention sink proposed by Xiao et al. (2024b). Local Window and Past Window Caches store the KVs for consecutive tokens, with Past Window Cache serving as an overflow buffer for the Local Window Cache. Separator Cache retains the KVs for separators which contain condensed segment information.\nTo describe the cache management policies, we denote the runtime usage of the four caches as $Size_{init}, Size_{sep}, Size_{past\\_w},$ and $Size_{local\\_w},$ respectively. The runtime usage across all KV caches is defined as $Size_{run} := Size_{init} + Size_{sep} + Size_{past\\_w} + Size_{local\\_w},$ which satisfies $Size_{run} < c$. The number of contiguous Neighboring tokens is defined as $n := Size_{past\\_w}+Size_{local\\_w}$. Notably, n is a function of the input sequence length m rather than a fixed hyperparameter for streaming setting. For clarity, we detail the preset hyperparameters of this caching system as follows.\n\u2022\n\u2022 c: The maximum capacity of the entire KV caches.\n\u2022 a: The maximum capacity of Initial Cache.\n\u2022 s: The maximum capacity of Separator Cache.\n\u2022 w: The maximum capacity of Local Window Cache. Notably, w is also the minimum value of n after the runtime KV caches usage $Size_{run}$ reaches c."}, {"title": "4. Experiments and Results", "content": "4.1. Experimental Settings\nWe evaluate our proposed SepLLM on the following tasks, i.e., training-free, training from scratch, posting-training and streaming applications.\nModel. Two popular model families, i.e., Pythia (Biderman et al., 2023) and Llama-3 (Dubey et al., 2024), are employed for evaluation. Specifically, Pythia-160m-deduped is used in the training from scratch tasks since the model, data, configurations, and checkpoints are all open-source and the training results are reproducible. As for post-training settings, we take Pythia-1.4B-deduped as our base model. Even though Llama-3 exhibits powerful performance on various downstream task, these training experimental details are not available. Therefore, we only use Llama-3 for training-free and streaming tasks.\nTraining Datasets. In the training from scratch and post-training tasks, the deduplicated Pile (Gao et al., 2020) is utilized for training, which contains about 207B tokens. And all other configurations are the same as the corresponding settings as Pythia (Biderman et al., 2023). Specifically, the training epoch is set to 1.5 epoch (143000 steps with"}, {"title": "4.2. Training-free", "content": "We evaluate the proposed SepLLM architecture in the training-free tasks based on the popular Llama-3-8B-Instruct model (Dubey et al., 2024).\nBenchmarks. The representative and commonly-used GSM8K-COT (Cobbe et al., 2021) and MMLU (Hendrycks et al., 2021)) are adopted. GSM8K-CoT (Cobbe et al., 2021) tests a model's ability to solve mathematical problems by evaluating its reasoning and step-by-step problem-solving skills. CoT (Wei et al., 2022) means the ability to simulate a reasoning process by breaking down complex problems into a series of logical steps. And the commonly-used 8 shots are adopted. MMLU (Hendrycks et al., 2021) assesses a model's general knowledge and reasoning across a wide range of subjects, such as history, science, mathematics and so on. The common 5-shot setting is used for MMLU.\nResults. The experimental results for training-free are shown in Table 1. \"Vanilla\" represents the original Llama-3 model with full attention, while \u201cStrmLLM\" represents StreamingLLM (Xiao et al., 2024b). n means the number of KV for neighboring tokens we retain. For SepLLM, all the KV for Special Tokens are kept and for the setting SepLLM (n=256), we find that SepLLM exhibits comparable performance in both multi-step mathematical CoT task and multidisciplinary knowledge reasoning tasks, when compared to the full-attention Llama-3. SepLLM achieves this using only 47.36% of KV utilized by the original Llama-3 for reasoning, indicating SepLLM's capability of modeling contexts requiring both multi-step logical analysis and those involving multi-domain knowledge reasoning while retaining only 50% original KV.\nStrmLLM (n=256) setting corresponds to removing all separators' KV from SepLLM (n=256) setting, except for those in Neighboring and Initial tokens. We observe a noticeable decrease in both mathematical analysis and multidisciplinary knowledge reasoning abilities of StrmLLM (n=256). StrmLLM (n=256) utilizes only 26.00% and 37.73% of the KV for the GSM8K and MMLU tasks, respectively, which are less than SepLLM (n=256) (47.36% and 44.61% re-"}, {"title": "4.3. Training from Scratch", "content": "We train the original Pythia-160m-deduped model as well as the Pythia-160m-deduped model modified with the SepLLM (and StreamingLLM) architecture on the Pile dataset for 143,000 steps using a global batch size of 1024 (involving approximately 300B tokens in total for training). All training configurations are consistent with Pythia (Biderman et al., 2023) (see Section 4.1). And following Pythia (Biderman et al., 2023), we conduct testing on the following downstream tasks: ARC-Challenge and ARC-Easy (Clark et al.,"}, {"title": "4.4. Post-training", "content": "Since training from scratch is time-consuming, we also conduct post-training experiment using 93000-step Pythia-1.4B-deduped checkpoint officially released by Pythia (see Section 4.1 for details). Figure 6 displays the loss curves for post-training, where SepLLM (n=64, larger lr) denotes that we employ an entire cosine learning rate scheduler (including a warm-up process starting from 0) identical to that of original Pythia-1.4B-deduped from step 0. SepLLM (n=64) and SepLLM (n=128) utilize a cosine learning rate scheduler that continues to decay from the 93000th step. From Figure 6, it is evident that increasing n and appropriately raising the learning rate both facilitate the decrease in loss. Moreover, this also illustrates that SepLLM for a swift transformation from a full-attention Transformer checkpoint to a model that aligns with the requirements of the SepLLM architecture's embedding distribution through post-training."}, {"title": "4.5. Streaming Applications", "content": "SepLLM can also adapt well to streaming applications, where infinite-length interactions may occur. Here, we follow StreamingLLM (Xiao et al., 2024b) to validate scenarios of infinite-length interactions using our Tailored Streaming Design on the commonly used PG19 dataset (Rae et al., 2020), which comprises 100 extensive literary works. The results are shown in Table 4. We can observe that for the same KV cache capacity c, the perplexity of predicting the"}, {"title": "4.6. Ablation Study", "content": "We conduct various ablation experiments specifically for long streaming applications. This includes a detailed study of the impact of various hyperparameters across different text lengths (5K to 20K). The experimental results about s and (w,c) pair are illustrated in Table 6 and Table 7 respectively. The conclusions are:\n\u2022 s: From Table 6, the capacity of Separator Cache affects the perplexity of long-text inference, as we find that in-"}, {"title": "4.7. Generalization and Information Retrieval", "content": "To verify the generalization of our SepLLM, we adapt our method to models with different architectures and different scales. The results in Appendix D can validate the generalization of our proposed framework. Specifically, we adapt our proposed SepLLM to different models in-"}, {"title": "A. Visualization of Attention Scores", "content": "We take Llama-3-8B-instruct (Dubey et al., 2024) as the model for visualization. The input sentence is \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Answer Natalia sold 48 clips in April. In May, she sold half as many clips as she did in April, so she sold 48/2=24 clips in May. Therefore, Natalia sold a total of 48+24=72 clips in April and May. The answer is 72.\" and the visualization of different attention maps are shown in Figure 12,13,14."}, {"title": "B. The Evolution of KV Caches", "content": "To explain the dynamic design for streaming setting better, we illustrate the detailed evolution of KV caches in Figure 7. As can be seen, n and Sizerun are both periodic functions after mo tokens. And the average KV caches used is much less than the maximum capacity c."}, {"title": "C. Training Acceleration", "content": "We list the detailed wall-clock time per iteration and throughput in Table 9 and the speed-up ratio is 1.53."}, {"title": "D. The Performance of Different Models", "content": "Different Architectures. Concerning different decoder-only models, we test our SepLLM on Llama3 and Pythia backbones on PG19 test dataset (generating 64K tokens). The results are shown in Table 10 (For SepLLM, a = 4, s = 64, w = 256, \u0441 = 800).\nDifferent Scales. To learn the generalization to different scales, we test our SepLLM on Pythia-6.9B and Pythia-12B backbones on PG19 test dataset (generating 20K tokens). The results are illustrated in Table 11.\nCompared to Pythia-12B, the smaller model Pythia-6.9B will have a higher perplexity if the capacity of the KV caches is the same (c=800). Therefore, it is necessary to increase c to achieve a lower perplexity close to that of Pythia-12B (c=800). On the other hand, the larger models will have lower perplexity but require a longer inference time.\nLarger Model Falcon-40B (Almazrouei et al., 2023) is an another larger architecture we adapted for evaluation the scalability of our proposed SepLLM. The experiment results are shown in Table 12, where we set a=4,s=64,w=512/720, c=800/1024 for SepLLM, and a=4, c=800/1024 for StreamingLLM. And the conclusions are similar with previous parts.\nBase or Instruct. In general, whether it is the base model or the instruction-tuned model, we can condense the seg-"}, {"title": "E. Needle In A Haystack", "content": "To evaluate the long-context ability of our proposed SepLLM, we take Needle In A Haystack as the benchmark and compare the performance of SepLLM and StreamingLLM. The results are shown in Figure 8,9,10,11 and SepLLM can achieve more scores compared with StreamingLLM."}, {"title": "F. Discussions on Separators", "content": "We provide the following assumptions and discussions on why keeping the KV caches corresponding to separators can maintain the performance of the original model."}]}