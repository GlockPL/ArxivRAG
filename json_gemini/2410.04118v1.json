{"title": "Riemann Sum Optimization for Accurate Integrated Gradients Computation", "authors": ["Swadesh Swain", "Shree Singhi"], "abstract": "Integrated Gradients (IG) is a widely used algorithm for attributing the outputs of a deep neural network to its input features. Due to the absence of closed-form integrals for deep learning models, inaccurate Riemann Sum approximations are used to calculate IG. This often introduces undesirable errors in the form of high levels of noise, leading to false insights in the model's decision-making process. We introduce a framework, RIEMANNOPT, that minimizes these errors by optimizing the sample point selection for the Riemann Sum. Our algorithm is highly versatile and applicable to IG as well as its derivatives like Blur IG and Guided IG. RIEMANNOPT achieves up to 20% improvement in Insertion Scores. Additionally, it enables its users to curtail computational costs by up to four folds, thereby making it highly functional for constrained environments.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Network (DNN) classifiers for computer vision are increasingly being utilized in critical fields such as healthcare [4] and autonomous driving [3]. Hence, it has become increasingly important to understand the decision-making process for these models. This has led to a growing body of research focused on understanding how the predictions of these deep networks can be attributed to specific regions of the image. An attribution method attempts to explain which inputs the model considers to be most important for its outputs. Several gradient-based [20, 18, 19, 9, 17, 12] and gradient-free [11, 14, 23, 5, 7, 30, 22, 29] attribution methods have been developed for deep learning models. Integrated Gradient methods [27, 10, 24] are a specific class of gradient-based attribution methods that compute a line integral of the gradients of the model over a path defined from a baseline image to the given input.\n\nThe complex functional space of deep learning models is often considered as a source of noise for many gradient-based attribution methods, resulting in undesirable high attribution to some background regions. For Integrated Gradients, Kapishnikov et al. [2021] claim that the source of noise is large gradients in the model surface while Smilkov et al. [2017] argue that the source of error is the rapid fluctuation of the gradients of deep learning models.\n\nDeep learning models do not have closed-form integrals, so their integrals are approximated by the Riemann Sums [24]. This approximation involves the sampling of a number of points along the path and approximating the integral using interpolation between these points. Using more points to approximate the Riemann sum naturally results in cleaner saliency maps. However, most applications of Integrated Gradients require a high number of steps for the Riemann Sum [24, 16], generally between 20 to 1000, rendering Integrated Gradients' usage practically unfeasible in real-time applications. On the other hand, using lesser number of samples severely impacts the quality of the saliency map. This results in a trade-off between speed and performance."}, {"title": "2 Background", "content": "In this section, we review the mathematical definition of IG [24], BlurIG [27], and GIG [10]."}, {"title": "2.1 Integrated Gradients", "content": "Sundararajan et al. [2017] utilized the idea of a path function. $\\gamma : [0, 1] \\rightarrow \\mathbb{R}^n$ is a smooth function that denotes a path within $\\mathbb{R}^n$ from $x'$ to $x$, satisfying $\\gamma(0) = x'$ and $\\gamma(1) = x$. Further, they defined path integrated gradients along the ith dimension for an input $x$, given a baseline $x'$, obtained by integrating the gradients along the path $\\gamma(\\alpha)$ for $\\alpha \\in [0, 1]$ as:\n\n$I_i(x) = \\int_{0}^{1} \\frac{\\partial f(\\gamma(\\alpha))}{\\partial \\gamma_i(\\alpha)} \\frac{\\partial \\gamma_i(\\alpha)}{\\partial \\alpha} d\\alpha,$"}, {"title": "2.2 Blur Integrated Gradients", "content": "Xu et al. [2020] introduced Blur Integrated Gradients: For a given function $f : \\mathbb{R}^{m\\times n} \\rightarrow [0, 1]$ representing a classifier, let $z(x, y)$ be the 2D input. Blur IG's path is defined by a Gaussian filter that progressively blurs the input. Formally:\n\n$\\gamma^{BlurIG}(x, y, \\alpha) = \\sum_{m=-\\infty}^{\\infty} \\sum_{n=-\\infty}^{\\infty} \\frac{1}{\\pi \\alpha} e^{-\\frac{x^2+y^2}{\\alpha}} z(x-m, y-n)$\n\nThe final BlurIG computation is as follows:\n\n$[I^{BlurIG}(x, y)]_c = \\int_{0}^{1} \\frac{\\partial f_c(\\gamma^{BlurIG}(x, y, \\alpha))}{\\partial \\gamma^{BlurIG}(x, y, \\alpha)} \\frac{\\partial \\gamma^{BlurIG}(x, y, \\alpha)}{\\partial \\alpha} d\\alpha$"}, {"title": "2.3 Guided Integrated Gradients", "content": "Guided IG [10] (GIG) follows an adaptive integration path $\\gamma^{GIG}(\\alpha), \\alpha \\in [0, 1]$ to avoid high gradient regions. An adaptive path is one that depends on the model being used:\n\n$\\gamma^{GIG} = \\underset{\\Upsilon \\in \\Gamma}{\\text{argmin}} \\sum_{i=1}^{N} \\int_{0}^{1} |\\frac{\\partial f(\\gamma(\\alpha))}{\\partial \\gamma_i(\\alpha)}| \\frac{\\partial \\gamma_i(\\alpha)}{\\partial \\alpha} d\\alpha,$\n\nAfter finding the optimal path $\\gamma^{GIG}$, GIG computes the attribution values similar to IG,\n\n$I^{GIG}_i(x) = \\int_{0}^{1} \\frac{\\partial f(\\gamma^{GIG}(\\alpha))}{\\partial \\gamma_i^{GIG}(\\alpha)} \\frac{\\partial \\gamma_i^{GIG}(\\alpha)}{\\partial \\alpha} d\\alpha.$"}, {"title": "3 Methodology", "content": "In this section, we present simple derivation to determine an upper bound on the error introduced by approximating a one-dimensional integral using a Riemann Sum. We then extend the definition for multi-dimensional line integrals and define the algorithm RIEMANNOPT uses to schedule samples to minimize this upper bound."}, {"title": "3.1 Error Minimization of Riemann Sums in 1D", "content": "We now present the derivation to estimate the error introduced due to the left Riemann Sum ap-proximation of a standard 1D integral $\\int_{a_0}^{a_k} g(\\alpha) d\\alpha$ where $\\{a_i\\}_{i=0}^{k}$ is the set of points at which the integrand, $g(\\alpha)$, is evaluated.\n\nThe standard way to calculate the left Riemann Sum is:\n\n$R = \\sum_{i=0}^{n-1} g(a_i)(a_{i+1} - a_i)$\n\nThe integral can be broken down as:\n\n$I = \\sum_{i=0}^{n-1} \\int_{a_i}^{a_{i+1}} g(\\alpha) d\\alpha$\n\nBy applying the Taylor Series approximation around $a_i$ in (5):\n\n$I \\approx \\sum_{i=0}^{n-1} \\int_{a_i}^{a_{i+1}} g(a_i) + (\\alpha - a_i)g'(a_i) d\\alpha$\n\n$\\approx \\sum_{i=0}^{n-1} g(a_i)(a_{i+1} - a_i) + g'(a_i) \\frac{(a_{i+1} - a_i)^2}{2}$"}, {"title": "3.2 Algorithm", "content": "IG computes d integrals (attributions) per image, one for each pixel. We treat each integral independently and use the derivation above to estimate the average error over all integrals. The input to the function, g, would be multidimensional, resulting in a different Taylor Series expansion. However, the approximation would still be mathematically sound since the integral corresponding to the $i^{th}$ feature is only dependant on the gradient along that component, i.e. error corresponding to the $i^{th}$ dimension of gradient only contributes to the $i^{th}$ integral. We use this observation in conjunction with the finite distance approximation of the derivative to determine the optimal points for sampling a Riemann Sum for the dataset. The primary idea behind the algorithm is to approximate the average $|g'(x)|$ for all input features on a small subset of images, ~ 1% of the validation dataset, then compute the optimal sampling points and use them for the entire dataset.\n\nThe following tensors are used in Algorithm 1 where d is the dimensionality of the input:\n\n$\\bullet$ $I_{k \\times d}$: Samples evaluated at k equispaced points along the path.\n\n$\\bullet$ $C_{(k-1) \\times d}$: Finite difference estimate of the derivative of I for all input features.\n\n$\\bullet$ $A_{k-1}$: Absolute derivative estimate of I, corresponding to $|g'(\\alpha)|$"}, {"title": "4 Experimental Setup and Metrics", "content": "In this section, we discuss the details of the implementation, dataset, model, and metrics used."}, {"title": "4.1 Experimental Setup", "content": "We use the original implementations with default parameters in the authors' code for IG, GIG, and BlurIG and implement RIEMANNOPT as a pre-computation step that links with the original implementations. We present our results using InceptionV3 for 16, 32, 64 and 128 sample points on the correctly classified images of the ImageNet validation dataset, ~ 40K. To estimate $|g'(\\alpha)|$, we apply Algorithm 1 to a set of 200 randomly correctly classified images from the ImageNet validation dataset for 128 samples. Then, we use Powell's method [15] to determine the optimal set of sampling points. This roughly has the same computational cost as computing the saliency map for the set of 200 images. Using RIEMANNOPT is still cost-effective since we only use a small number of images to calculate sample points but are able to use these points for the entire dataset."}, {"title": "4.2 Metrics", "content": "Previous works use the Insertion Score and Normalized Insertion Score to compare different attribution methods [27, 9, 10, 28, 14, 13]. It is critical to note that the purpose of the Insertion Score is to measure the efficacy of a saliency map, i.e. it is not designed to measure how close the Riemann Sum is to the actual integral. However, it is reasonable to assume that the true saliency map would generally achieve better Insertion Scores than an inaccurate approximation since inaccurate estimates introduce noise. Hence, we report the Insertion Scores and, additionally, employ the Axiom of Completeness [24] to define a new metric that measures the quality of the saliency maps without the need for this hypothesis.\n\nAccording to the Axiom of Completeness, the sum of all feature attributions, determined by any Integrated Gradients method, must ideally add up to the difference between the output of $f$ at $x$ and $x'$. However, there is always an error due to inaccurate Riemann Sum estimates. Furthermore, Sundararajan et al. [2017] advise the developer to ensure that all feature attributions add up to $f(x) - f(x')$ (within 5%) and suggest increasing the number of samples if the error is greater.\n\nSince the ground truth is unavailable, it is non-trivial to determine the numerical accuracy of a computed saliency map. We use the relative error between the sum of feature attributions and $f(x) - f(x')$ to estimate the error. This metric is not infallible since the features' positive and negative errors partially offset each other during the summation. Using the Triangle Inequality, it can be easily shown that this metric is a lower bound on the true error. Nevertheless, it serves as a helpful proxy since near-perfect saliency maps will have near-zero error, and highly erroneous maps will, on average, have high error even after the errors partial offset."}, {"title": "5 Results and Discussion", "content": "In this section, we compare the sampling points chosen by RIEMANNOPT to the linear schedules, followed by qualitative and quantitative evaluation against the baselines. In the case of BlurIG, the sample points chosen by RIEMANNOPT highly differ from the linearly spaced samples, as depicted in Figure 2. Every path starts with an information-less baseline image, $x'$, and gradually gains perceptible features as it moves towards the input image, $x$. Along the path, when the image becomes perceptible, the gradients rapidly change, resulting in large values of $|g'(\\alpha)|$. For BlurIG, the image features become perceptible at the end of the path, when most of the sharpening occurs. For IG and GIG, the image becomes perceptible as soon as its brightness crosses a certain threshold, $\\alpha \\approx 0.1$."}, {"title": "6 Conclusion", "content": "In this paper, we present RIEMANNOPT, a highly efficient framework designed to optimize sample points in Riemann Sums for the computation of Integrated Gradients. Both qualitative and quantitative results demonstrate that RIEMANNOPT effectively minimizes numerical errors in saliency maps and achieves improved Insertion Scores by up to 20%, thereby enhancing the accuracy and reliability of attribution maps. RIEMANNOPT is adaptable, extending its applicability to any multi-dimensional line integral computation, including derivatives of Integrated Gradients such as BlurIG and GIG. Additionally, it enables users to curtail computational costs by up to fourfold, significantly boosting efficiency. Opportunities for future work include extending RIEMANNOPT to further improve its suitability for Integrated Gradient methods that employ adaptive paths."}]}