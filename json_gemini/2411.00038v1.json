{"title": "Topic-Conversation Relevance (TCR) Dataset and Benchmarks", "authors": ["Yaran Fan", "Jamie Pool", "Senja Filipi", "Ross Cutler"], "abstract": "Workplace meetings are vital to organizational collaboration, yet a large percentage of meetings are rated as ineffective [1]. To help improve meeting effectiveness by understanding if the conversation is on topic, we create a comprehensive Topic-Conversation Relevance (TCR) dataset that covers a variety of domains and meeting styles. The TCR dataset includes 1,500 unique meetings, 22 million words in transcripts, and over 15,000 meeting topics, sourced from both newly collected Speech Interruption Meeting (SIM) data and existing public datasets. Along with the text data, we also open source scripts to generate synthetic meetings or create augmented meetings from the TCR dataset to enhance data diversity. For each data source, benchmarks are created using GPT-4\u00b9 to evaluate the model accuracy in understanding transcription-topic relevance.", "sections": [{"title": "Introduction", "content": "Since the 2020 COVID-19 pandemic, an increasing share of meetings have shifted from in-person to online. The Gartner 2021 Digital Worker Experience Survey reports that the number of in-person meetings dropped from 63% in 2019 to 33% in 2021 [2]. The same survey predicted that in 2024, only 25% of the meetings will happen in person.\nTogether with the growing number of online meetings, there are ongoing complaints about ineffective meetings due to a lack of focused discussions or focused tasks [3, 4, 5, 6]. Having a meeting facilitator to keep the discussions focused is one of the meeting design characteristics enabling more effective meetings [6, 7].\nMeasuring how relevant a conversation transcript is to an intended topic is crucial to quantifying how focused the communication is, and to creating tools that behave as a virtual meeting moderator by keeping the discussion on-track. A very low rating on the relevance of the conversation to the topic meant for discussion would be an indicator of a non-focused discussion. In practice this translates to the problem of keeping discussions focused on a predefined meeting agenda.\nWhile there is existing work about the importance of topics serving as input to text summarizing models [8, 9], we could not find references about work studying the relevance of a topic to a particular body of text it didn't originate from. One of our intuitions for why this field has had little exploration is because of technological limitations before the recent Generative AI advancements."}, {"title": "Related Work", "content": "In this paper, we refer to \"topics\" as the key points to be discussed during a meeting and such topics would have been put in the meeting agenda by the organizer before the meeting starts. To the best of the authors knowledge there is no research related to the task of measuring conversation relevance to pre-meeting agenda topics. However, the related topic of meeting summarization, or minuting has been well studied.\nTwo challenges (AutoMin) in the field of meeting summarization have been held where teams participated in order to progress the field [15, 16]. The first challenge had teams using BART-based models achieving the best performance [17, 18]. With the improvements in generative AI and the growing adoption of Large Language Models (LLMs), a second challenge was done recently. In this challenge, the participants [19, 20, 21] achieved good results with different large models, such as, Llama-based Vicuna [22], Dolly [23], and GPT-3's text-davinci-003 [24]. The challenge organizers used GPT-4 for benchmarking as well and it demonstrated good performance for the meeting summarization task. The organizers also used GPT-4 to evaluate submissions along with human evaluation results, but found that it was unreliable for this task. The challenge organizers also called out the need to answer research questions related to transcript summary relevance, to better understand content and coverage from different annotators.\nThere are multiple datasets for the task of benchmarking the meeting summarization task. Most of such individual datasets often contains only one type of meeting. The AMI dataset [25] is a collection of meetings transcripts and summaries that cover the topic of product design in an scenario setting and a small amount of non-scenario meetings. The topic annotation is very brief and limited. The ICSI [26] Corpus contains 75 project meetings and discussions in an academic environment. It has high-level human-annotated topics that are very brief. MeetingBank [27] is a dataset of 1,250 city council meetings from multiple US counties. Detailed meeting minutes for each meeting subsection are documented in this dataset. The QMSum [28] dataset aggregates three public data sources (ICSI, AMI, and Parliament meetings from Welsh and Canada) and generates minutes for the text summarization tasks. The paper further shows that for a BART model that training a model on data from one of the datasets and testing it on the other one leads to poor performance. By training on all datasets they were able to build a more robust model. To further expand on data for the automatic minute task Nedoluzhko [29] put together the ELITR data corpus. This data consists of meetings in both Czech and English, with transcripts and meeting minuting being taken by different annotators. In order to align the transcripts with the minutes the tool ALIGNMEET [30] was used."}, {"title": "Topic-Conversation Relevance (TCR) Dataset", "content": "We create the TCR Dataset that covers a variety of meeting topics and styles. The dataset consists of both meeting data collected by the author team and existing publicly available datasets.\nOverall, the TCR dataset contains 1,506 unique meetings, 22 million words in transcripts and more than 15,000 meeting topics. Table 1 provides high-level statistics of the dataset.\nThe pre-selected MeetingBank data is large comparing with other data sources. To balance the meeting styles and create representative benchmark results, we also create a subset of 30 randomly selected meetings denoted as MeetingBank_rnd30. The subset is available separately from the complete MeetingBank data in the TCR dataset. A summary of the balanced subsets is presented in Table 2.\nWe also provide exploratory analysis of per meeting metrics in Table 3. The full exploratory analysis with standard deviations is provided in the Appendix 5 . The dataset and related scripts are available in the topic_conversation GitHub repository\u00b2."}, {"title": "Data Schema", "content": "All data files in the TCR dataset are in JSON format. An example schema is presented in Figure 1.\nData from different sources are split into separate files. In each file, data is grouped by meeting. For each meeting, we provide meeting level metadata and detailed topic level information. The topics"}, {"title": "New Data Collection: Speech Interruption Meetings (SIM)", "content": "In a previous study done by the team regarding speech interruptions and meeting inclusiveness [31], we collect multi-party online meetings in which participants actively interact with each other to debate a topic. This Speech Interruption Meetings (SIM) dataset is released for the first time as part of the TCR dataset. We also create 100 synthetic meetings on top of these raw meetings. Both the original meetings and synthetics meetings are included in the TCR dataset."}, {"title": "Raw Data", "content": "In total, we include 84 raw meetings (48.6 hours) in the TCR dataset. The meetings cover 14 different topics and there are about 530,000 words in the transcripts. In total, 149 unique speakers\u00b3 participated in this batch of data collection. Speaker distribution details can be found in Appendix Table 6.\nThe SIM data captures natural online meeting dynamics. To collect the data, we invite 4 participants to join a remote conference call on Microsoft Teams. Each meeting has a single dedicated topic that can elicit debate. The participants discuss the topic for about 30 minutes. Natural interactions between participants are strongly encouraged. We collect separate audio channels and machine-generated transcripts for each meeting. In the transcripts, the participants are marked as speaker_1,2,3,4 randomly within each meeting. We only include transcripts data in the TCR dataset at this stage as audio is not directly related to the the benchmark task."}, {"title": "Synthetic Meetings", "content": "Given the raw meetings from the SIM dataset has only one dedicated topic per meeting, we also generate 100 synthetic meetings with multiple topics by randomly combining meetings snippets from different topics together.\nThe workflow to generate such synthetic meetings involves the following steps. First, we remove the first and last 5 minutes of the transcripts, to eliminate potential meeting setup contents, greetings, and icebreaker talk. These trimmed meetings are the candidate meetings. Second, for each new synthetic"}, {"title": "Public Data Sources", "content": "To make the TCR dataset cover a wide range of meeting styles and domains, we integrate another 5 publicly available data sources. In this section, we describe the pre-processing procedures we apply to each of them."}, {"title": "ICSI Corpus", "content": "We use all 75 meetings from the the ICSI Corpus [26]. Starting from the word-level transcripts from the original corpus, we exclude the tags for non-verbal events and keep only the transcribed words. This is because for real-time machine-generated transcripts, such events are not marked as tags, but either transcribed as part of the contents, or omitted. For long utterances from the same speaker, we assign a line break in the transcript either when an end-of-sentence tag occurs, or there is a gap that is at least 0.5 second long between two words. We assign timestamps for each sentence based on the original word-level timestamps from the data source.\nWe make minor adjustments to the topic annotations if there is an identify-mismatch problem between the topics and the speaker IDs. The speaker IDs for each meeting are assigned as speaker_A,"}, {"title": "Selected QMSum Dataset", "content": "The QMSum [28] data has 3 different input data sources and we treat them separately. For QM-Sum_ICSI data, we use the pre-processed transcripts and timestamps from the original ICSI corpus. We use the QMSum annotations as the new topics. Given the topic styles and the section breaks are very different between QMSum_ICSI and the original ICSI Corpus, we decide to keep both sets of meetings and create benchmark results for them separately. For QMSum_AMI and QM-Sum_Parliament data, we remove non-verbal tags from the transcripts. As timestamps are not available in QMSum, we create estimated timestamps by the fixed 150 words per minute rate for these two data sources.\nThe annotations are done as meeting minutes in the QMSum dataset. In cases where the transcripts are not included in the minuting, we fill the empty values by the following logic. If the missing topic is at the very beginning of the meeting, we assign a topic of \"Beginning_no_topic\"; if the missing topic is at the very end of the meeting, we assign a topic of \"Ending_no_topic\". If the lack of annotation happens between two topics, we assume the previous topic continues and fill the empty value by taking the previous topic. In the QMSum annotation, it is also possible that one line of transcripts belong to multiple topics. We use the first annotation based on the timestamps. In any given meeting, if more than 15% of the transcripts have missing annotations or overlapping annotations issues, we exclude the meeting due to undesirable annotation quality. Overall, we keep 168 out of the original 232 meetings."}, {"title": "Selected MeetingBank Dataset", "content": "We use the timestamps in the metadata from the original data source to exclude meetings that are shorter than 15 minutes. In total, 1,100 out of the 1,250 MeetingBank [27] meetings are included in our dataset. We remove unicode from both the transcripts and annotations. Though some of the original timestamps do not start from 0, we keep the original timestamps as it is necessary to locate the corresponding audio contents if needed. In the TCR data, it is very easy to align the beginning to 0 by removing the start timestamp documented in the meeting metadata.\nIn the TCR dataset, we provide two sets of annotations for the MeetingBank data:\nOriginal Annotations We take the \"summary\" field from the MeetingBank metadata as the topic annotations. These annotations are in meeting minutes styles and often are long and very detailed. If in the original data source one sentence belong to multiple summaries, we keep only the first occurrence.\nRe-annotated Topics The original meeting summaries contain not only the topic for a section but often the outcomes. To have pre-meeting style topics, we need to remove outcomes that would not have been known before the meeting happens. Additionally some of the meeting minutes are excerpts from the transcripts, so modifying the annotations would give a more accurate representation of the topic-conversation relevance benchmark. In order to rewrite a summary to a pre-meeting agenda type of topic, a GPT-4 prompt is developed. An example of the input and output is shown below:\n\u2022 Original summary: A bill for an ordinance changing the zoning classification for 5611 East Iowa Avenue in Virginia Village. Approves an official map amendment to rezone property located at 5611 East Iowa Avenue from S-SU-D to S-RH-2.5 (suburban, single-unit to suburban, rowhouse) in Council District 6. The Committee approved filing this item at its meeting on 7-10-18.\n\u2022 Re-annotated topic: Zoning Change for 5611 East Iowa Avenue in Virginia Village.\nTo guarantee high quality of the re-annotated topics, we randomly selected 100 samples and collected Mean Opinion Score (MOS) scores of 1-5 from 3 project members. A score of 5 means that the re-annotated topic is in a proper pre-meeting agenda style and it captures the key information from"}, {"title": "Selected NCPC Meetings", "content": "The National Capital Planning Commission (NCPC) [32] is a government agency that meets once a month to discuss projects for in the united states capitol region. The meeting agendas and transcripts are publicly available. To the best of our knowledge, the TCR dataset is the first work to add this data source to a structured dataset. We randomly select 20 NCPC meetings where agenda is available. Both meeting transcripts and agenda topics are documented in the same PDF file for each meeting. In order to convert the data to a structured format, the PDFs are converted to text files, and the body of the text is extracted, along with the topic titles. As the PDFs do not share the same structure, additional manual adjustments are applied to guarantee a high conversion accuracy. The original transcripts do not have time information, hence the timestamps are estimated with the fixed rate of 150 words per minute."}, {"title": "Selected ELITR Dataset", "content": "The ELITR [29] data is a corpus of meetings in Czech and English containing transcripts along with minutes written by multiple annotators. As our work focuses on English only at this stage, we keep just the English meetings. Among the English meetings, 49 have meeting minutes that can be aligned with the corresponding transcripts. We further reduce the size of this dataset to address the following challenges: First, with multiple annotations available from up to 11 different annotators per meeting, we need to keep only one annotation per meeting. Second, the meeting minutes can contain too many detailed items that are not suitable to be considered as topics. Third, the annotations do not necessarily point to a consecutive chunk of transcripts, but jump back and forth. To account for these issues, we keep only meetings with an annotation of at most 10 topics, and the annotations are not interspersed. With all the filters, we include 11 meetings into the TCR dataset. If there is no annotation for some parts of the transcripts, we follow the same logic described in Section 3.3.2 to fill the empty values. The original transcripts do not have timestamps, so we estimate the time information with the fixed rate of 150 words per minute."}, {"title": "Data Augmentation", "content": "All data sources described above provide ground truth topics for subsections of transcripts. However, the list of topics in the annotation only reflect the topic that are discussed. Real meetings do not always follow the planned agenda. Participants sometimes go off topic and have to skip some pre-arranged topics due to time limits. The TCR dataset schema is designed to test and evaluate such scenarios by incorporating the \"variations\" section. To reflect such scenarios, we also provide a script to either (1) add topics that are not discussed to a meeting as a planned topic or (2) remove topics and corresponding contents from a meeting. This can help enrich the TCR dataset to include a varied range of meeting styles. Implementation details can be found in the project repository.\nFor the augmented meetings, we keep the type of variation and the changed topic list in the \"variations\" field in the metadata for each meeting. Figure 2 shows an example of the change in metadata for an augmented meeting. If a topic is planned, but not discussed in a meeting, the topic is added to the \"variation_addTopics\" and the corresponding empty contents are also added to the \"topic\" section. Users can easily extend this by adding topics with non-empty contents to expand the simulation further. If we want to remove a topic and its corresponding contents together from a meeting, the changes are reflected in the \"variation_removeTopics\" list as well as the \"topic\" contents. The timestamps of the remaining contents are also changed accordingly. With this structure, we can test the relevance between the transcripts and topics that could have been planned but are not part of the actual conversation."}, {"title": "Topic-Conversation Relevance Benchmarks", "content": "We generate Topic-Conversation Relevance benchmarks on a selected subset of the TCR dataset. Given the significant difference in meeting styles and structures, the benchmarks are reported for each data source separately."}, {"title": "Methodology", "content": "We use GPT-4 to create the benchmark results. For each meeting, we cut the transcripts into snippets with equal length based on timestamps. We conduct the experiments in duration length of 5 minutes, 10 minutes and 15 minutes. Then the prompt takes the snippet of transcripts and the full topic list from the meeting as inputs, and asks for an evaluation of the transcript's relevance to each topic in the list. The relevance score is represented by 4 levels: 0 means Not Relevant, 1 means Somewhat Relevant, 2 means Mostly Relevant, and 3 means Very Relevant. The detailed definitions of the relevance levels are given as a multiple-choice question in the prompt. In the development stage, we try different output requests, such as floats, integers, binaries and multiple choices. We present the final benchmark results all in the multiple choices style as it has been giving the most robust results across all data sources.\nIn the evaluation stage, we treat the Topic-Conversation Relevance problem as a binary classification. If based on the ground truth label, a topic is discussed for more than 30 seconds in the transcripts, then we mark it as \"Discussed\", otherwise \"Not Discussed\". For the GPT-4 responses, we treat \"0 Not Relevant\" as \"Not Discussed\", and everything else as \"Discussed\". In the results presented in Section4.2, we specifically focus on scenarios where the discussion is off-topic, so the \"Not Discussed\" topics are treated as positive cases. We use Precision (\u201cLLM detects a topic is not being discussed and it is true\") and Recall (\"A topic is not being discussed and LLM detects it\") as the main metrics. The full results treating \"Discussed\" and \"Not Discussed\" as positive cases respectively are shown in the Appendix A.3."}, {"title": "Results", "content": "The benchmark results focusing on the \"Not Discussed\" category are shown in Table 4. We split the results by data source and transcripts length.\nFor the highly structured meetings (MeetingBank, NCPC), the benchmark results show very high precision and recall. Most of these meetings follow the pre-defined agenda topics strictly and often state the topic to-be-discussed at the beginning of the section. Different annotations do not impact the results much. The other less structured meetings, such as project meetings (ICSI, ELITR) and brainstorming discussions (SIM), are more challenging. Most of these meetings do not have clear statements separating different topics and related sub-topics are often discussed back and forth. Different topic annotations can impact the results significantly.\nWe also notice that if there are multiple topics included in the same snippet of transcripts (8), it is even more challenging to correctly predict the relevance comparing with single-topic transcript (9). This could be due to the fact that transitions between topics are not always clear in the less structured meetings. Results split by topic counts are also included in Appendix A.3."}, {"title": "Future Work", "content": "The dataset can be further improved by including more types of meetings in different domains. However, it is particularly hard to obtain real day-to-day meetings in a working environment as most of such meetings consist sensitive business information. Hence the project team is working on inviting domain experts (e.g., legal, healthcare, finance, etc.) to create meeting agendas for different types of meetings in their industry, and conducting domain-specific meetings based on the agendas. We are currently in the data collection stage using the same method as the SIM dataset, with additional requirements on participants' professional experience.\nIn addition to English, we are also working on integrating other languages into the dataset. One of the efforts is to translate the current data sources into other languages with reliable translation services and test the performance on the same tasks.\nA challenge of evaluating topic-conversation relevance is the blurred boundaries between topics. At a meeting structure level, a certain chunk of transcripts can be marked as belonging to a topic, but it is very likely that some parts of the conversation are actually not directly related to the topic or even belong to another listed topic. It would be desirable to create sub-labels at sentence or group of sentences level to capture relevance scores at a lower granularity.\nAdditionally, we believe it would be beneficial to include audio data in the TCR dataset along with transcripts. We will work on aggregating audio data for multiple data sources (SIM data and other public data) into the dataset."}, {"title": "Appendix", "content": "Exploratory Analysis per Meeting by Data Source"}, {"title": "SIM Dataset Unique Speaker Metadata", "content": ""}, {"title": "Complete Evaluation Results", "content": "Complete benchmark results by positive classes, topic counts and snippet sizes are reported in Table 7 to Table 12."}]}