{"title": "Exploring the Role of Transliteration in In-Context Learning for Low-resource Languages Written in Non-Latin Scripts", "authors": ["Chunlan Ma", "Yihong Liu", "Haotian Ye", "Hinrich Sch\u00fctze"], "abstract": "Decoder-only large language models (LLMs) excel in high-resource languages across various tasks through few-shot or even zero-shot in-context learning (ICL). However, their performance often does not transfer well to low-resource languages, especially those written in non-Latin scripts. Inspired by recent work that leverages transliteration in encoder-only models, we investigate whether transliteration\u00b9 is also effective in improving LLMs' performance for low-resource languages written in non-Latin scripts. To this end, we propose three prompt templates, where the target-language text is represented in (1) its original script (SCRIPT {Orig}), (2) Latin script (SCRIPT{Latn}), or (3) both (SCRIPT{Combined}). We apply these methods to several representative LLMs of different sizes on various tasks including text classification and sequential labeling. Our findings show that the effectiveness of transliteration varies by task type and model size. For instance, all models benefit from transliterations for sequential labeling (with increases of up to 25%). We make our code publicly available.", "sections": [{"title": "Introduction", "content": "Decoder-only LLMs, such as LLaMA (Touvron et al., 2023), Mixtral (Jiang et al., 2024), XGLM (Lin et al., 2022), and BLOOM (Scao et al., 2023), have shown impressive capability across a wide range of tasks for high-resource languages, particularly through few-shot ICL (Brown et al., 2020). However, they often underperform in low-resource languages, especially those written in underrepresented scripts. Multiple reasons exist, such as the scarcity of low-resource languages in the training data (Team et al., 2022; \u00dcst\u00fcn et al., 2024), insufficient crosslingual alignment during pretraining (H\u00e4mmerl et al., 2024), as well as English being the only language in the instruction tuning phase"}, {"title": "Experimental Settings", "content": "Models. We experiment with six models: LLaMA2-7B (Touvron et al., 2023), Mistral-7B-Instruct (Jiang et al., 2024), and the 7B, 3B, 1B, and 560M variants of the BLOOM model (Scao et al., 2023). LLaMA2 is a model trained on 28 languages and 5 scripts (Cyrillic, Latin, Hang, Hani and Japanese). Mistral is an English-centric model trained on five languages in Latin script, while BLOOM is a multilingual LLM covering a wide range of languages in 11 scripts. We select these models to compare the effectiveness of transliteration-augmented ICL on model type (English-centric vs multilingual models) and model size (different variants of BLOOM).\nMethods. To investigate how transliteration impacts the ICL performance for low-resource languages in non-Latin scripts, we propose three prompt methods: (1) SCRIPT{Orig}, where we feed the model with text in its original script, (2) SCRIPT{Latn}, where we first transliterate the text into Latin script and only feed the transliteration into the model, and (3) SCRIPT{Combined}, where we combine the text in its original script and its"}, {"title": "Results and Discussion", "content": "We report the average performance across all languages in Table 1 (per-language performance is in \u00a7C). In addition, we show the performance on NER averaged by script group in Table 3.\nTransliteration benefits sequential labeling. Across all models, we can observe that either SCRIPT{Latn} or SCRIPT{Combined} outperforms SCRIPT{Orig} on NER. For instance, SCRIPT{Combined} increases by 12.4 compared to SCRIPT{Orig} on BLOOM-1B, which is more than 24% improvement. This demonstrates that models can make better predictions by leveraging the knowledge encoded in the Latin-script transliterations. This can be explained by the fact that NER data contains many (proper) nouns shared across languages. Transliteration enables the model to better exploit such shared vocabularies for inference.\nThe impact of transliteration on text classification varies across models. SCRIPT{Latn} almost always performs the worst across all models compared with its counterparts, indicating that the transliteration alone is not enough for the model to understand the sentence-level semantics. Besides, SCRIPT{Combined} performs suboptimal compared to SCRIPT{Orig} on the English-centric (Mistral) model and models trained on many multilingual"}, {"title": "Related Work", "content": "Positive effects of transliterating data into a common script have been demonstrated in various recent works for encoder-only models (Dhamecha et al., 2021; Purkayastha et al., 2023; Moosa et al., 2023; Liu et al., 2024b). Additionally, leveraging transliteration as an auxiliary input at fine-tuning stage improves the cross-script performance (Liu et al., 2024a). To improve ICL performance for low-resource languages, demonstrations play an important role. One line of approaches replaces the target-language texts with English translations (Artetxe et al., 2023; Shi et al., 2023; Etxaniz et al., 2023). Another type of research augments the ICL demonstrations, e.g., by retrieving the most similar English texts to the target-language text (Nie et al., 2023; Li et al., 2023; Wang et al., 2023)"}, {"title": "Conclusion", "content": "This study explores the effectiveness of transliteration in enhancing the ICL performance of decoder-only LLMs, focusing on low-resource languages written in non-Latin scripts. By proposing three prompt templates \u2013 using original script, Latin script, and a combination of both we evaluate their impact across various tasks on several representative LLMs. Our findings indicate that transliteration is particularly effective for sequential labeling but its benefits for text classification tasks are less consistent. We also observe a mixed effect of transliteration related to the model type and model size. Our results highlight the potential of transliteration as a possible way to enhance LLMs' performance for low-resource languages."}, {"title": "Limitations", "content": "There are mainly two limitations in our work. First, we only consider models with up to 7 billion parameters due to constraints in our computing resources. Second, the evaluation data is limited in terms of the types of tasks. The major reason is the limited availability of evaluation datasets containing a variety of scripts. Nevertheless, as a pioneer study in exploring the effectiveness of transliteration for ICL involving low-resource languages in non-Latin scripts, we hope future research can leverage larger models and more datasets to explore this direction."}, {"title": "Task Data Information", "content": "The basic information of each task dataset is shown in Table 3. The number of languages of script groups for each downstream task is shown in Table 2. We introduce the detailed hyperparameters settings for each task in the following.\nFor Named Entity Recognition (NER), we employ a 3-shot prompting strategy. Given that each sentence comprises multiple tokens requiring prediction, we have determined that three randomly selected demonstrations typically encompass the majority of NER categories. For SIB200, we do a 7-shot prompt. The 7 demonstrations are manually selected to cover the 7 classes of the task. For Taxi1500, we use a 3-shot prompt and adhere"}, {"title": "B Prompt Templates", "content": "We follow the prompt templates in (Lin et al., 2024) for SCRIPT{Orig}, where the demonstrations and the query are in the original script of the target language. We employ Uroman (Hermjakob et al., 2018) to transliterate the target-language demonstrations and the target-language query into Latin script. SCRIPT{Latn} only uses the transliteration, while SCRIPT{Combined} leverage both the original script and its Latin transliteration."}, {"title": "C Full Results for All Scripts/ Languages", "content": "We report the complete results for all tasks and language-scripts in Table 4 and Table 5 (NER), Table 6 and Table 7 (SIB200), and Table 8 and Table 9 (Taxi1500)."}]}