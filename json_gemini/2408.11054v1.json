{"title": "NeCo: Improving DINOv2's spatial representations in 19 GPU hours with Patch Neighbor Consistency", "authors": ["Valentinos Pariza", "Mohammadreza Salehi", "Gertjan Burghouts", "Francesco Locatello", "Yuki M. Asano"], "abstract": "We propose sorting patch representations across views as a novel self-supervised\nlearning signal to improve pretrained representations. To this end, we introduce\nNeCo: Patch Neighbor Consistency, a novel training loss that enforces patch-\nlevel nearest neighbor consistency across a student and teacher model, relative to\nreference batches. Our method leverages a differentiable sorting method applied\non top of pretrained representations, such as DINOv2-registers to bootstrap the\nlearning signal and further improve upon them. This dense post-pretraining leads\nto superior performance across various models and datasets, despite requiring only\n19 hours on a single GPU. We demonstrate that this method generates high-quality\ndense feature encoders and establish several new state-of-the-art results: +5.5\n% and + 6% for non-parametric in-context semantic segmentation on ADE20k\nand Pascal VOC, and +7.2% and +5.7% for linear segmentation evaluations on\nCOCO-Things and -Stuff.", "sections": [{"title": "Introduction", "content": "Dense self-supervised learning trains feature extractors to produce representations for every pixel\nor patch of an image without supervision. This field has seen substantial advancements in recent\nyears, notably improving unsupervised semantic segmentation [1-5], object-centric representation\nlearning [6], and other dense downstream tasks like object tracking and object detection [7-9, 2].\nOne particularly interesting use-case of densely pretrained encoders was developed by Balazevic et.\nal. [10]. They propose to solve semantic segmentation by posing it as a nearest-neighbor retrieval\nproblem utilizing the features of the spatial patches. This non-parametric method not only mirrors in-\ncontext learning in large language models (LLMs) [11] but also delivers rapid and robust performance,\nespecially with limited data.\nInspired by this idea, we turn this evaluation approach on its head and propose to use nearest-neighbor\nretrieval as a training mechanism for encoders. By integrating nearest-neighbor relationships into\nthe training phase, we aim to train models that can naturally cluster and distinguish intricate visual\nsimilarities. This method promises to yield models with deeply semantic spatial features specifically\ntailored for in-context tasks, enhancing their adaptability and robustness. This approach, while\npromising, presents two main challenges.\nThe first is the source of supervision. In the case of evaluation, ground-truth labels are used, yet\nwe are interested in obtaining better self-supervised representations. While previous works [10, 9]\naddress this by essentially converting dense learning to image-level learning via learnable pooling of\npatches, we offer a more practical and versatile solution. We simply start from already image-level\npretrained models and adapt them further. We term this stage dense post-pretraining and demonstrate"}, {"title": "Related Works", "content": "Dense Self-supervised Learning. Dense self-supervised learning methods aim to generate cat-\negorizable representations at the pixel or patch level, rather than the image level. This field has\ngained significant attention [2, 1, 3, 4, 9, 10, 17, 18, 8, 19, 8, 7] due to the observation that image-\nlevel self-supervised methods [20-28, 15, 13, 29-34] do not necessarily produce expressive dense\nrepresentations [10, 8, 35, 36].\nCroC [4] is a recent method that has proposed a dense self-supervised loss to address the issue. It\napplies joint clustering between different views, ensuring that cluster centers capturing the same\nobject are similar. Leopart [1] improves dense representations by applying a dense clustering loss\nto pretrained models. Extending this concept, Timetuning [2] has demonstrated that finetuning\npretrained backbones over the temporal dimension of unlabeled videos enhances dense reasoning\ncapabilities. Recently, Hummingbird [10] has proposed a dense loss that leverages attention within\nand across images during training, showing strong in-context scene understanding during evaluation.\nCrIBo [9] takes this further by explicitly enforcing cross-image nearest neighbor consistency between\nimage objects, achieving state-of-the-art results.\nWe similarly adopt nearest neighbor consistency due to its promising results in in-context scene\nunderstanding but with two major differences: (1) instead of using pooled versions of patches at\neither the image or object level, we directly apply it to patch features; (2) in addition to nearest\nneighbor consistency, we ensure that the order of neighbors for the same patches from different views\nis similar. These changes result in more semantic patch-level features, directly enhancing in-context\nscene understanding and stabilizing training, as there is no need to infer object-level features through\nclustering methods, which can be unstable during training.\nUnsupervised Object Segmentation. Several works specifically target unsupervised object segmen-\ntation [37-40, 6, 41-44]. The goal of these works is not to learn semantic patch-level representations;\ninstead, they often utilize the existing information in frozen pretrained backbones and train another\nmodel to explicitly solve semantic segmentation. For instance, Seitzer et al. [37] train a slot-attention\nencoder and decoder module [45] to reconstruct DINO [13] pretrained features with a few slots for\neach input image. This process enables the creation of per-image object cluster maps, where each\nslot represents a distinct object or part of an object within the image, based on the features it predicts.\nIn contrast, our approach learns distinct features for various objects and employs dense representations\nas intermediaries for dense tasks like semantic segmentation. These features can then be used to\ndevelop per-dataset cluster maps for semantic segmentation use cases."}, {"title": "Patch Neighbor Consistency", "content": "The goal is to develop a feature space in which, for a given input, patches representing the same\nobject exhibit similar features, whereas patches representing different objects show distinct features.\nA key challenge for a self-supervised method in this process is defining the similarity between image\npatches. Although patches from the same object (e.g. a cat) are expected to be more similar to each\nother than to those from different objects (e.g. a dog), they may still depict different parts of the object\n(e.g. a cat's tail and legs). Both part- and whole-object representations have important applications.\nTherefore, to generate a feature space that can reliably represent both, merely establishing a single\nclustering per object is not enough. Instead, this requires accurately ordering similarities, such that\npatches representing the same object and its parts are more similar to one another and less to those of\nother objects. To this end, our method works by extracting dense features of the inputs, finding their\npair-wise distances, and forcing a consistency between the order of nearest neighbors within a batch\nacross two views.\nFeature Extraction and Alignment. Given an input image $I$, two augmentations specified by\nparameters $\\tau_1$ and $\\tau_2$ are applied to create two different views $V_1$ and $V_2$. These views are then\ndivided into $N = \\lfloor \\frac{H}{P} \\rfloor \\times \\lfloor \\frac{W}{P} \\rfloor$ separate patches, where $H$ and $W$ represent the height and width of\nthe input image, and $P$ represents the patch size. The patches are represented as $V_1 = [v^1_1, ..., v^N_1]$\nand $V_2 = [v^1_2, ..., v^N_2]$, which are fed to the feature extractor.\nWe utilize the Vision Transformer (ViT) architecture [46] as the backbone and employ a teacher-\nstudent framework, where the student and teacher models are denoted by $\\phi_s$ and $\\phi_t$, respectively.\nThe teacher's weights are updated using the exponential moving average of the student's weights.\nAs the generated views cover different parts of the input, the extracted features do not necessarily\ncorrespond to the same objects. To address this, we align the features by applying ROI-Align [47],\nadjusted according to the crop augmentation parameters. This process creates spatially aligned dense\nfeatures for the teacher and student networks, represented by $F_s \\in \\mathbb{R}^{N' \\times d}$ and $F_t \\in \\mathbb{R}^{N' \\times d}$. These\nfeatures are then forced to maintain a consistent order of nearest neighbors, ensuring more robust and\nmeaningful feature representations, as explained next.\nPairwise Distance Computation. To identify the nearest neighbors of the patches, it is necessary\nto extract features from other images in the batch and compute their distances with respect to $F_s$\nand $F_t$. To achieve this, all batch images are fed through the student network, $\\phi_s$, to obtain the\nbatch features $F_B \\in \\mathbb{R}^{BN \\times d}$. We sample a random fraction $f \\ll 1$ of these patches to obtain the\n$R = fBN$ reference patches $F_r \\in \\mathbb{R}^{r \\times d}$ which we use to compare the nearest neighbors of our $F_s$"}, {"title": "Differentiable Sorting of Distances", "content": "Next, these distance matrices are sorted in a differentiable manner to produce a loss that enforces a\nsimilar sorting across the two views.\nDifferentiable Sorting of Distances. To determine the order of nearest neighbors from distance\nmatrices, sorting is necessary. However, traditional sorting algorithms cannot propagate gradients\nbecause they use non-differentiable operations such as $d'_i \\leftarrow \\min(d_a, d_b)$ and $d'_a \\leftarrow \\max(d_a, d_b)$\nto facilitate element swapping in the sequence for an ordering $a < b$. Given a sequence $s =$\n$(d_1,..., d_R)$, where $R$ is the length of the sequence, We use relaxed, differentiable versions of these\noperations by defining their soft versions following recent work [17], as follows:\n$\\begin{aligned}\nd'_a &= \\text{softmin}(d_a, d_b) := d_a f(d_b - d_a) + d_b f(d_a \u2013 d_b), \\\\\nd'_b &= \\text{softmax}(d_a, d_b) := d_a f(d_a \u2013 d_b) + d_b f(d_b - d_a),\n\\end{aligned}$\nwhere the function $f(x) = \\frac{1}{\\beta}(\\arctan(\\beta x) + 0.5)$, and $\\beta > 0$ is an inverse temperature parameter,\nspecifying the steepness of the operator. This function is sigmoid-shaped and centered around $x = 0$.\nAs $\\beta$ approaches infinity, the relaxation converges to the discrete swap operation. This operation\ncan be defined in an approximate permutation matrix $P_{\\text{swap}}(d_i, d_j) \\in \\mathbb{R}^{L \\times L}$, which is essentially an\nidentity matrix except for the entries $P_{ii}, P_{ij}, P_{ji}$, and $P_{jj}$ defined as\n$P_{ii} = P_{jj} = f(d_j \u2013 d_i), \\quad P_{ij} = P_{ji} = f(d_i \u2013 d_j),$\nsuch that one step of swapping the pair $(d_i, d_j)$ in the sequence is equivalent to multiplying $P$ with\nthat sequence. The final permutation matrix for the entire sequence is determined by the sorting\nalgorithm employed. For example, in the odd-even sorting algorithm, the permutation matrix $P_t$ for a\nstep $t$ is defined as:\n$P_t = \\prod_{i \\in M} P_{\\text{swap}}(d_i, d_{i+1}),$\nwhere $M$ is the set of odd indices if $t$ is odd and the set of even indices if $t$ is even. The overall\npermutation matrix $Q$ is obtained by multiplying the permutation matrices from all steps of the\nsorting algorithm, $Q = \\prod_{t=1}^L P_t$. As shown by [12], $L = R$ of such steps are sufficient for efficient\nsorting. In the discrete case, for each column $i$, the permutation matrix has exactly one entry of 1,\nindicating the sequence element that should be placed in the $i$-th column. In the relaxed version,\ncolumn values represent a distribution over possible sequence elements. In our case, a row $i$ of the\ndistance matrix $D_s$ shows the distance of the $i$-th student feature to all the reference features.\nWith its sorting matrix $Q_i$, the $(r, k)$ element of this matrix can be viewed as the probability of\na reference feature $r$ being the $k$-th nearest neighbor for the $i$-th feature. Hence, to maintain the\norder of nearest neighbors for every ROI-aligned patch feature, we compute $Q_i$ for every row of\n$D_s$ and $D_t$ and force these to be similar. This results in final matrices $Q^s = [Q_1, \\dots, Q_{N'}]$ and\n$Q^t = [Q_1, \\dots, Q_{N'}]$, which are used in the training loss."}, {"title": "Training Loss", "content": "Training Loss. After computing permutation matrices, we enforce similarity on the order of nearest\nneighbors for each of the aligned patch features using the cross-entropy loss. The loss for the\npermutation matrix of patch $i$ is defined as:\n$\\mathcal{L}_{CE}(Q^s_i, Q^t_i) = - \\sum_{j,k} Q^t_i(j, k) \\log(Q^s_i(j, k)),$\nTo ensure robust nearest neighbor consistency, we compute the cross-entropy loss in both directions\nand sum the losses. The final training loss, incorporating both directions, for all the patches is:\n$\\mathcal{L}_{NeCo} = \\sum_{i=1}^{N'} \\mathcal{L}_{CE}(Q^s_i, Q^t_i) + \\mathcal{L}_{CE}(Q^t_i, Q^s_i)$\nThis ensures, in a differentiable manner, that the order of nearest neighbors is consistent between the\nstudent and teacher features in both directions."}, {"title": "Experiments", "content": "Benchmarked Methods. We compare our method against state-of-the-art dense self-supervised\nlearning methods, including CrIBo [9], Hummingbird [10], TimeT [2], and Leopart [1]. To provide a\nmore comprehensive evaluation, We also include the performance of DINOv2 enhanced with registers,\nreferred to as DINOv2R [15, 16], as it has demonstrated strong dense capabilities. Additionally,\nwe benchmark our method against leading unsupervised semantic segmentation approaches such as\nCOMUS [44].\nTraining. We run our experiments on ViT-Small and ViT-Base with a patch size of 14. We start\nfrom various pretrained backbones, and use DINOv2 with registers' unless otherwise noted. We\npost-pretrain these models for 25 COCO epochs on a single NVIDIA RTX A6000-46GB GPU, taking\naround 19 hours. For other training details, we refer readers to the Appendix A.\nEvaluation. In all our evaluations, we discard the projection head, following previous works [13, 1,\n2], and directly use the spatial tokens from the Vision Transformer backbone. Scores in all experiments\nare reported as mean intersection over union (mIoU). We conduct four types of evaluations: linear\nsegmentation fine-tuning with a 1 \u00d7 1 convolution, end-to-end segmentation with the Segmenter\nhead [48], clustering and overclustering semantic segmentation [1, 2], and dense nearest neighbor\nretrieval [10]. For clustering and overclustering, we apply K-Means to spatial tokens, setting K to\nthe number of ground truth objects and to high values like 300 and 500, as previously used [1, 2]. We\nthen extract object cluster maps and match them using Hungarian matching [49]. For dense nearest\nneighbor retrieval, we follow the protocol from [10], implemented in [50].\nDatasets. We train our model on ImageNet-100 [51], Pascal VOC12 [52], and COCO [53] for\nablations and use COCO as our primary training dataset for all state-of-the-art comparisons. For\nevaluations, we use the validation sets of Pascal VOC12 [52], COCO [53], ADE20k [54], and Pascal\nContext [55]. For finetuning and feature transferability evaluations on COCO [56], we train using a\n10% split of the training set, while we use the full training splits of the other datasets."}, {"title": "Comparison to State-of-the-Art", "content": "In this section, we first compare the quality of frozen features learned through NeCo with state-\nof-the-art methods in in-context learning via nearest neighbor retrieval and unsupervised semantic\nsegmentation tasks. Next, we demonstrate that NeCo's versatility by applying it to five different\npretraining models and show it improves their dense features consistently. Finally, we evaluate the\ntransferability of our learned dense representations to other datasets by using linear head semantic\nsegmentation and end-to-end fine-tuning with Segmentor [48].\nVisual In-Context Learning Evaluation. We compare our approach to a recently proposed bench-\nmark [10] that evaluates in-context reasoning in vision models. Unlike traditional linear segmentation\nmethods, this evaluation does not require fine-tuning or end-to-end training. Instead, it creates\nvalidation segmentation maps by matching patch-level, feature-wise nearest neighbor similarities\nbetween validation images (queries) and training samples (keys). This method, inspired by NLP"}, {"title": "Ablation Studies", "content": "Here, we examine the essential parameters of our method by training NeCo on Pascal VOC12 and\nADE20k. We assess its ability to perform linear segmentation and in-context scene understanding\nusing the frozen representations learned with each set of parameters. For in-context scene under-\nstanding evaluations, we use $\\frac{1}{128}$ fraction of the training data and reduce the spatial dimension to\n$448^2$. The number of training epochs for linear segmentation evaluations is set to 20 epochs. For\nmore ablations refer to Appendix B.\nPatch Selection Approach. We demonstrate the effect of selecting patches from the foreground,\nbackground, or both in Table 5a. Foreground patches are selected using the attention map averaged\nacross heads. Our results indicate that selecting patches from the foreground gives 1% better results\ncompared to the background selection in 3 out of 4 metrics. However, the performance peaks when\nwe select patches from both locations. This improvement can be attributed to the use of scene-centric\nimages for training, where the background often contains meaningful objects that contribute to\nenhanced performance.\nUtilizing a Teacher. We ablate the role of teacher-student architecture in Table 5b. As shown,\nemploying a teacher network updated by exponential moving average can significantly improve the"}, {"title": "Discussion", "content": "Broader impact. By leveraging pretrained models, our method has implications for reducing the\ncarbon footprint of model training. Similar to how once an LLMs is trained, it can be re-used and\ntuned with very little compute, we demonstrate that this is possible for visual foundation models. Our\nfinetuning requires only 19 GPU-h, yet yields vastly better models for dense image understanding.\nWe believe this work spurs further research into post-pretraining methods.\nConclusion. In this work, we propose Patch Nearest Neighbor Consistency as a new method\nfor dense post-pretraining of self-supervised backbones. By applying our method to the many\nbackbones including the DINOv2-registers model, we improve upon these models by a large margin\nfor frozen clustering, semantic segmentation and full finetuning, setting several new state-of-the-art\nperformances."}, {"title": "Experimental Setup", "content": "Dense Post-Pretraining\nImplementation Framework Our model is implemented in Python, using Torch [59] and PyTorch\nLightning [60].\nDatasets Our pretraining datasets consist of COCO [56] and ImageNet-100 subset of the original\nImagenet [61]. COCO contains approximately 118,000 scene-centric images, whereas ImageNet-\n100k includes around 100k object-centric images.\nData Augmentations Our Data augmentations are the same as in [1]. More specifically, we use:\nrandom color-jitter, Gaussian blur, grayscale and multi-crop augmentations. Similarly, the global\ncrop's resolution is 224x224 and the local crop's resolution is 96x96, for the all the experiments\nexcept when working with Dinov2 where we use 98x98 for local crops. Furthermore, our generated\nglobal and local crops have the constraint that they intersect at least by 1% of the original image size.\nNetwork Architecture For our backbone, we employ vision transformers. More specifically, we\ntrain on ViT-Small and ViT-Base [46]. Moreover, following [13, 29], we use a student-teacher setup\nwhere the teacher weights are updated by the exponential moving average of the student weights.\nFollowing [13], the projection head consists of three linear layers with hidden\ndimensionality of 2048, a Gaussian error linear units as activation function [62], and an output\ndimensionality of 256.\nDense Image Representation Alignment of Crops Following [1], due to the distinction between\nglobal and local crops, after projecting the dense spatial output to a lower space, the alignment step is\napplied on the dense image representations to bring them to a fixed spatial resolution of size of 7x7\nduring training. This ensures that the local and global crop feature maps have the same size and that\nthey correspond to each other. The alignment is done using region of interest alignment (roi align)\n[47].\nOptimization We train both network sizes with a cosine learning rate schedule going down to 0\nover 25 training epochs, except for the ablation studies where we use 10 epochs. The initial projection\nhead learning rate is 1e-4 for all the experiments, whereas the backbone's learning rate is 1e-5,\nwith the exception of being 1e-6 when applying our method on Dinov2. The exponential moving\naverage for updating the teacher's weights is adapted with a cosine schedule starting at 0.9995 and\ngoing up to 1. We use Adam Optimizer [63] with a cosine weight decay schedule.\nDifferentiable Sorting Networks By default we use the Bitonic Differentiable Sorting Networks\n[12] and the steepnesses (i.e., inverse temperatures) used for the network are 100 for the Student and\n100 for the teacher. All the other parameters remain as the default ones; i.e., we use the logistic\nfunction with a $\\lambda = 0.25$ for the interpolation of numbers in the differentiable sorting algorithms."}, {"title": "Evaluation Setup", "content": "Visual In-Context Learning The Dense Nearest Neighbor Retrieval Evaluation is a retrieval-based\nscene understanding evaluation introduced by [10]. Its goal is to assess the scene understanding\ncapabilities of a dense image encoder. It works as follows:\n1. Memory Bank Construction: Using a dataset of images and their dense annotations, two\nmemory banks are created. One memory bank stores image patch features extracted from\nthe spatial output of a dense encoder applied to the training images. The other memory bank\nstores the corresponding patch labels from the dataset annotations.\n2. Query Processing: For an image from the validation split, the spatial output of the dense\nimage encoder is processed. For each patch representation in this output, the $k$ nearest\nneighbors are identified from the memory bank of features. The labels of these nearest\nneighbors are then combined to construct the query's label.\n3. Comparison: After constructing the annotation for the entire image, it is compared with the\nground truth annotation.\nDue to the unavailability of the original implementation by [10], we use the open implementation\nfrom [50]. This implementation aligns with the original authors' description and details, including"}, {"title": "Full Visual In-Context Learning Tables", "content": "We show the results shown by Figure 2 in Table 8. To provide a more comprehensive comparison,\nwe also evaluate our method against SelfPatch [69], a finetuning approach with a similar aim of\nenhancing dense representations.\nIn-context scene understanding benchmark. Dense nearest neighbor retrieval performance\nis reported across various training data proportions on two scene-centric datasets, ADE20k and Pascal\nVOC. The retrieved cluster maps are compared with the ground truth using Hungarian matching [49],\nand their mIoU score is reported.\nSorting Steepness. In Table 9c, we vary the sorting steepness, denoted by $\\beta$, for both teacher\nand student networks to evaluate the influence of hard or soft nearest neighbor assignments. The\nperformance improves when the teacher's steepness is higher or equal to the student's, consistent\nwith previous findings [13]. Our best results are achieved when both networks have equal steepness.\nHowever, extreme steepness values (e.g., 1000) harm performance. This is because sorting patch\nsimilarities lacks clear boundaries, and formulating it as a hard assignment can force incorrect\norderings, negatively impacting performance.\nBatch Size. Table 9b examines the impact of batch size on performance. We maintain the same\nexperimental setup as reported in the paper for all ablations. As anticipated, smaller batch sizes result\nin marginal improvements due to the weaker supervisory signal. However, a moderately large batch\nsize, such as the 64 used in the paper, shows significant improvements over the baseline. Furthermore,\nthe trend suggests that increasing batch sizes could lead to even greater performance gains.\nNumber of Neighbors. As mentioned in the method section, compute and sort the distances\nbetween each patch and all other patches using a differentiable sorting algorithm. Ultimately, The\nsorted distances for the same patches in different views should be similar. Here, we conduct an\nablation study, selecting only the top K distances for each patch instead of all distances. The\nresults are reported in Table 9a. As shown, incorporating more neighbours in NeCo loss enhances\nperformance. However, after reaching a threshold (e.g., 32 neighbors), adding more becomes\nless effective. This indicates our method's robustness against this hyperparameter, not relying on\ncomputing distances for all batch patches to improve the baseline.\nTraining Epochs. We show the performance across different training epochs in Table 9d. As the\ntable shows, even after just one epoch of training, DINOv2R improves by 1% to 3% across various\nmetrics. The performance continues to increase with more training epochs, but the improvements\nbecome smaller after 25 epochs, which is the number used in the paper."}, {"title": "Computational Analysis", "content": "We provide a detailed runtime analysis for DINO, CrIBo, TimeT, and NeCoin Table 10. All experi-\nments are conducted on 8 NVIDIA RTX A6000-46GB GPUs. As shown in Table 3, starting from\nalready existing pretrained networks, NeCo only requires 2.5 GPU hours to enhance the performance\nof the state-of-the-art method, CrIBo, by 3.7% in linear segmentation. Furthermore, when NeCo is\napplied to TimeT, starting from DINO initialization, it can surpass CrIBO with nearly 30% less total\ntraining time (20h + 1.6h + 2.5h vs. 70h). This trend is also observed in clustering and overclustering\nnumbers, showing up to a 6% improvement over CrIBO when starting from TimeT, as detailed in\nTable 3. These results validate the computational efficiency of the proposed method."}, {"title": "Additional Visualizations", "content": "Visualization of nearest patch retrieval. In Figure 4, we take one patch from an image in Pascal\nVOC as the query and retrieve its seven nearest patches across the dataset. We compare NeCo against\nDINOv2R. As illustrated, the nearest neighbors retrieved by NeCo are not only more relevant\ncompared to DINOv2R but also more precise, successfully finding nearest patches not only within\nthe same object but also within object parts."}, {"title": "Dataset Details", "content": "Pascal VOC 2012 [65] This dataset, the latest split version of trainaug, features 10,582 images\nand their annotations distributed across 21 classes, with one referring to the background class. The\nvalidation set consists of 1,449 images. Following [19] we ignore unlabelled objects as well as the\nboundary class. Moreover, for hyper-parameter tuning of the fully unsupervised segmentation method\n[1] that we apply on our method, we use the PVOC12 train split with 1464 images. Figure 3 shows\nthe dataset images overlaid by the annotations.\nPascal Context [52] This scene-centric dataset includes 4,998 training images covering 60 semantic\nclasses, including the background. The validation set consists of 5,105 images. We use this dataset\nfor the Linear Segmentation and Segmenter experiments, via the MMSegmentation Library [68].\nCOCO-Stuff 164K [56] This scene-understanding dataset includes labels across 91 \"stuff\" categories\nand 80 \"things\" categories. The training set comprises 118,000 images, and the validation set contains\n5,000 images. We follow the same setup as [1] and thus we use the COCO benchmark in two ways to\nisolate further the given object definitions.\nConcisely, we begin by extracting stuff annotations, which refer to objects without clear boundaries\nand often found in the background, using the COCO-Stuff annotations [56]. Then, we consolidate\nthe 91 detailed labels into 15 broader labels, as described in [67] and we assign the general label\n\"other\" to non-stuff objects, as this label lacks specific semantic meaning. Non-Stuff objects are\nignored during training and evaluation. We indicate this version of the dataset within our work as\nCOCO-Stuff used in Overclusterring and Linear Segmentation in Appendix A.2.\nNext, we extract foreground annotations utilizing the panoptic labels from [70]. We combine the\ninstance-level annotations into object categories using a script provided by the authors. Additionally,\nwe consolidate the 80 detailed categories into 12 broad object classes. The background class is ignored\nduring training and evaluation. This leads as to the COCO-Thing version of the dataset which we use\nfor the Overclusterring and our Linear Segmentation in Appendix A.2.\nADE20K [54] The dataset is a collection of images used for semantic segmentation tasks, featuring\nfinely detailed labels across 150 unique semantic categories. Some of the categories include stuffs\nlike sky and grass, as well as distinguishable objects like person, and a car. Overall, it includes a\nwide variety of scenes, with 20,210 images in the training set and 2,000 images in the validation set,\nmaking it one of the most challenging and diverse datasets for scene understanding. We use the full\ndataset in our experiments. In our experiments, we ignore the others label of the dataset.\nImagenet [61] The dataset, is a large-scale visual database designed for use in visual object recogni-\ntion research. It contains over 1.3 million images categorized into 1,000 object classes. Each image\nis labeled with detailed annotations, making it a critical resource for training and evaluating machine\nlearning models, particularly in the field of computer vision. In our work, we also explore training\non part of the Imagenet, the Imagenet100k that consists 100K images across 100 classes, from the\noriginal dataset."}]}