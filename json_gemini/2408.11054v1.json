{"title": "NeCo: Improving DINOv2's spatial representations in 19 GPU hours with Patch Neighbor Consistency", "authors": ["Valentinos Pariza", "Mohammadreza Salehi", "Gertjan Burghouts", "Francesco Locatello", "Yuki M. Asano"], "abstract": "We propose sorting patch representations across views as a novel self-supervised learning signal to improve pretrained representations. To this end, we introduce NeCo: Patch Neighbor Consistency, a novel training loss that enforces patch-level nearest neighbor consistency across a student and teacher model, relative to reference batches. Our method leverages a differentiable sorting method applied on top of pretrained representations, such as DINOv2-registers to bootstrap the learning signal and further improve upon them. This dense post-pretraining leads to superior performance across various models and datasets, despite requiring only 19 hours on a single GPU. We demonstrate that this method generates high-quality dense feature encoders and establish several new state-of-the-art results: +5.5 % and + 6% for non-parametric in-context semantic segmentation on ADE20k and Pascal VOC, and +7.2% and +5.7% for linear segmentation evaluations on COCO-Things and -Stuff.", "sections": [{"title": "1 Introduction", "content": "Dense self-supervised learning trains feature extractors to produce representations for every pixel or patch of an image without supervision. This field has seen substantial advancements in recent years, notably improving unsupervised semantic segmentation [1-5], object-centric representation learning [6], and other dense downstream tasks like object tracking and object detection [7-9, 2].\nOne particularly interesting use-case of densely pretrained encoders was developed by Balazevic et. al. [10]. They propose to solve semantic segmentation by posing it as a nearest-neighbor retrieval problem utilizing the features of the spatial patches. This non-parametric method not only mirrors in-context learning in large language models (LLMs) [11] but also delivers rapid and robust performance, especially with limited data.\nInspired by this idea, we turn this evaluation approach on its head and propose to use nearest-neighbor retrieval as a training mechanism for encoders. By integrating nearest-neighbor relationships into the training phase, we aim to train models that can naturally cluster and distinguish intricate visual similarities. This method promises to yield models with deeply semantic spatial features specifically tailored for in-context tasks, enhancing their adaptability and robustness. This approach, while promising, presents two main challenges.\nThe first is the source of supervision. In the case of evaluation, ground-truth labels are used, yet we are interested in obtaining better self-supervised representations. While previous works [10, 9] address this by essentially converting dense learning to image-level learning via learnable pooling of patches, we offer a more practical and versatile solution. We simply start from already image-level pretrained models and adapt them further. We term this stage dense post-pretraining and demonstrate"}, {"title": "2 Related Works", "content": "Dense Self-supervised Learning. Dense self-supervised learning methods aim to generate categorizable representations at the pixel or patch level, rather than the image level. This field has gained significant attention [2, 1, 3, 4, 9, 10, 17, 18, 8, 19, 8, 7] due to the observation that image-level self-supervised methods [20-28, 15, 13, 29-34] do not necessarily produce expressive dense representations [10, 8, 35, 36].\nCroC [4] is a recent method that has proposed a dense self-supervised loss to address the issue. It applies joint clustering between different views, ensuring that cluster centers capturing the same object are similar. Leopart [1] improves dense representations by applying a dense clustering loss to pretrained models. Extending this concept, Timetuning [2] has demonstrated that finetuning pretrained backbones over the temporal dimension of unlabeled videos enhances dense reasoning capabilities. Recently, Hummingbird [10] has proposed a dense loss that leverages attention within and across images during training, showing strong in-context scene understanding during evaluation. CrIBo [9] takes this further by explicitly enforcing cross-image nearest neighbor consistency between image objects, achieving state-of-the-art results.\nWe similarly adopt nearest neighbor consistency due to its promising results in in-context scene understanding but with two major differences: (1) instead of using pooled versions of patches at either the image or object level, we directly apply it to patch features; (2) in addition to nearest neighbor consistency, we ensure that the order of neighbors for the same patches from different views is similar. These changes result in more semantic patch-level features, directly enhancing in-context scene understanding and stabilizing training, as there is no need to infer object-level features through clustering methods, which can be unstable during training.\nUnsupervised Object Segmentation. Several works specifically target unsupervised object segmentation [37-40, 6, 41-44]. The goal of these works is not to learn semantic patch-level representations; instead, they often utilize the existing information in frozen pretrained backbones and train another model to explicitly solve semantic segmentation. For instance, Seitzer et al. [37] train a slot-attention encoder and decoder module [45] to reconstruct DINO [13] pretrained features with a few slots for each input image. This process enables the creation of per-image object cluster maps, where each slot represents a distinct object or part of an object within the image, based on the features it predicts.\nIn contrast, our approach learns distinct features for various objects and employs dense representations as intermediaries for dense tasks like semantic segmentation. These features can then be used to develop per-dataset cluster maps for semantic segmentation use cases."}, {"title": "3 Patch Neighbor Consistency", "content": "The goal is to develop a feature space in which, for a given input, patches representing the same object exhibit similar features, whereas patches representing different objects show distinct features. A key challenge for a self-supervised method in this process is defining the similarity between image patches. Although patches from the same object (e.g. a cat) are expected to be more similar to each other than to those from different objects (e.g. a dog), they may still depict different parts of the object (e.g. a cat's tail and legs). Both part- and whole-object representations have important applications. Therefore, to generate a feature space that can reliably represent both, merely establishing a single clustering per object is not enough. Instead, this requires accurately ordering similarities, such that patches representing the same object and its parts are more similar to one another and less to those of other objects. To this end, our method works by extracting dense features of the inputs, finding their pair-wise distances, and forcing a consistency between the order of nearest neighbors within a batch across two views. Figure 1 provides an overview of the method, which we describe in detail below.\nFeature Extraction and Alignment. Given an input image I, two augmentations specified by parameters \\tau_1 and \\tau_2 are applied to create two different views V_1 and V_2. These views are then divided into N = \\lceil \\frac{H}{P} \\rceil \\times \\lceil \\frac{W}{P} \\rceil separate patches, where H and W represent the height and width of the input image, and P represents the patch size. The patches are represented as V_1 = [v_1^1, ..., v_1^N] and V_2 = [v_2^1, ..., v_2^N], which are fed to the feature extractor.\nWe utilize the Vision Transformer (ViT) architecture [46] as the backbone and employ a teacher-student framework, where the student and teacher models are denoted by \\phi_s and \\phi_t, respectively. The teacher's weights are updated using the exponential moving average of the student's weights.\nAs the generated views cover different parts of the input, the extracted features do not necessarily correspond to the same objects. To address this, we align the features by applying ROI-Align [47], adjusted according to the crop augmentation parameters. This process creates spatially aligned dense features for the teacher and student networks, represented by F_s \\in \\mathbb{R}^{N'\\times d} and F_t \\in \\mathbb{R}^{N'\\times d}. These features are then forced to maintain a consistent order of nearest neighbors, ensuring more robust and meaningful feature representations, as explained next.\nPairwise Distance Computation. To identify the nearest neighbors of the patches, it is necessary to extract features from other images in the batch and compute their distances with respect to F_s and F_t. To achieve this, all batch images are fed through the student network, \\phi_s, to obtain the batch features F_B \\in \\mathbb{R}^{BN \\times d}. We sample a random fraction f \\ll 1 of these patches to obtain the R = fBN reference patches F_r \\in \\mathbb{R}^{r \\times d} which we use to compare the nearest neighbors of our F_s"}, {"title": "4 Experiments", "content": "4.1 Setup\nBenchmarked Methods. We compare our method against state-of-the-art dense self-supervised learning methods, including CrIBo [9], Hummingbird [10], TimeT [2], and Leopart [1]. To provide a more comprehensive evaluation, We also include the performance of DINOv2 enhanced with registers, referred to as DINOv2R [15, 16], as it has demonstrated strong dense capabilities. Additionally, we benchmark our method against leading unsupervised semantic segmentation approaches such as COMUS [44].\nTraining. We run our experiments on ViT-Small and ViT-Base with a patch size of 14. We start from various pretrained backbones, and use DINOv2 with registers' unless otherwise noted. We post-pretrain these models for 25 COCO epochs on a single NVIDIA RTX A6000-46GB GPU, taking around 19 hours. For other training details, we refer readers to the Appendix A.\nEvaluation. In all our evaluations, we discard the projection head, following previous works [13, 1, 2], and directly use the spatial tokens from the Vision Transformer backbone. Scores in all experiments are reported as mean intersection over union (mIoU). We conduct four types of evaluations: linear segmentation fine-tuning with a 1 \u00d7 1 convolution, end-to-end segmentation with the Segmenter head [48], clustering and overclustering semantic segmentation [1, 2], and dense nearest neighbor retrieval [10]. For clustering and overclustering, we apply K-Means to spatial tokens, setting K to the number of ground truth objects and to high values like 300 and 500, as previously used [1, 2]. We then extract object cluster maps and match them using Hungarian matching [49]. For dense nearest neighbor retrieval, we follow the protocol from [10], implemented in [50].\nDatasets. We train our model on ImageNet-100 [51], Pascal VOC12 [52], and COCO [53] for ablations and use COCO as our primary training dataset for all state-of-the-art comparisons. For evaluations, we use the validation sets of Pascal VOC12 [52], COCO [53], ADE20k [54], and Pascal Context [55]. For finetuning and feature transferability evaluations on COCO [56], we train using a 10% split of the training set, while we use the full training splits of the other datasets."}, {"title": "4.2 Comparison to State-of-the-Art", "content": "In this section, we first compare the quality of frozen features learned through NeCo with state-of-the-art methods in in-context learning via nearest neighbor retrieval and unsupervised semantic segmentation tasks. Next, we demonstrate that NeCo's versatility by applying it to five different pretraining models and show it improves their dense features consistently. Finally, we evaluate the transferability of our learned dense representations to other datasets by using linear head semantic segmentation and end-to-end fine-tuning with Segmentor [48].\nVisual In-Context Learning Evaluation. We compare our approach to a recently proposed benchmark [10] that evaluates in-context reasoning in vision models. Unlike traditional linear segmentation methods, this evaluation does not require fine-tuning or end-to-end training. Instead, it creates validation segmentation maps by matching patch-level, feature-wise nearest neighbor similarities between validation images (queries) and training samples (keys). This method, inspired by NLP"}, {"title": "5 Discussion", "content": "Broader impact. By leveraging pretrained models, our method has implications for reducing the carbon footprint of model training. Similar to how once an LLMs is trained, it can be re-used and tuned with very little compute, we demonstrate that this is possible for visual foundation models. Our finetuning requires only 19 GPU-h, yet yields vastly better models for dense image understanding. We believe this work spurs further research into post-pretraining methods.\nConclusion. In this work, we propose Patch Nearest Neighbor Consistency as a new method for dense post-pretraining of self-supervised backbones. By applying our method to the many backbones including the DINOv2-registers model, we improve upon these models by a large margin for frozen clustering, semantic segmentation and full finetuning, setting several new state-of-the-art performances."}, {"title": "A Experimental Setup", "content": "A.1 Dense Post-Pretraining\nImplementation Framework Our model is implemented in Python, using Torch [59] and PyTorch Lightning [60].\nDatasets Our pretraining datasets consist of COCO [56] and ImageNet-100 subset of the original Imagenet [61]. COCO contains approximately 118,000 scene-centric images, whereas ImageNet-100k includes around 100k object-centric images.\nData Augmentations Our Data augmentations are the same as in [1]. More specifically, we use: random color-jitter, Gaussian blur, grayscale and multi-crop augmentations. Similarly, the global crop's resolution is 224x224 and the local crop's resolution is 96x96, for the all the experiments except when working with Dinov2 where we use 98x98 for local crops. Furthermore, our generated global and local crops have the constraint that they intersect at least by 1% of the original image size.\nNetwork Architecture For our backbone, we employ vision transformers. More specifically, we train on ViT-Small and ViT-Base [46]. Moreover, following [13, 29], we use a student-teacher setup where the teacher weights are updated by the exponential moving average of the student weights.\nProjection Head Following [13], the projection head consists of three linear layers with hidden dimensionality of 2048, a Gaussian error linear units as activation function [62], and an output dimensionality of 256.\nDense Image Representation Alignment of Crops Following [1], due to the distinction between global and local crops, after projecting the dense spatial output to a lower space, the alignment step is applied on the dense image representations to bring them to a fixed spatial resolution of size of 7x7 during training. This ensures that the local and global crop feature maps have the same size and that they correspond to each other. The alignment is done using region of interest alignment (roi align) [47].\nOptimization We train both network sizes with a cosine learning rate schedule going down to 0 over 25 training epochs, except for the ablation studies where we use 10 epochs. The initial projection head learning rate is 1e-4 for all the experiments, whereas the backbone's learning rate is 1e-5, with the exception of being le-6 when applying our method on Dinov2. The exponential moving average for updating the teacher's weights is adapted with a cosine schedule starting at 0.9995 and going up to 1. We use Adam Optimizer [63] with a cosine weight decay schedule.\nDifferentiable Sorting Networks By default we use the Bitonic Differentiable Sorting Networks [12] and the steepnesses (i.e., inverse temperatures) used for the network are 100 for the Student and 100 for the teacher. All the other parameters remain as the default ones; i.e., we use the logistic function with a \\lambda = 0.25 for the interpolation of numbers in the differentiable sorting algorithms."}, {"title": "A.2 Evaluation Setup", "content": "Visual In-Context Learning The Dense Nearest Neighbor Retrieval Evaluation is a retrieval-based scene understanding evaluation introduced by [10]. Its goal is to assess the scene understanding capabilities of a dense image encoder. It works as follows:\nMemory Bank Construction: Using a dataset of images and their dense annotations, two memory banks are created. One memory bank stores image patch features extracted from the spatial output of a dense encoder applied to the training images. The other memory bank stores the corresponding patch labels from the dataset annotations.\nQuery Processing: For an image from the validation split, the spatial output of the dense image encoder is processed. For each patch representation in this output, the k nearest neighbors are identified from the memory bank of features. The labels of these nearest neighbors are then combined to construct the query's label.\nComparison: After constructing the annotation for the entire image, it is compared with the ground truth annotation.\nDue to the unavailability of the original implementation by [10], we use the open implementation from [50]. This implementation aligns with the original authors' description and details, including"}]}