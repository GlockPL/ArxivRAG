{"title": "Ensemble architecture in polyp segmentation", "authors": ["Hao-Yun Hsu", "Yi-Ching Cheng", "Guan-Hua Huang"], "abstract": "In this research, we revisit the architecture of semantic segmentation and evaluate the models excelling in polyp segmentation. We introduce an integrated framework that harnesses the advantages of different models to attain an optimal outcome. More specifically, we fuse the learned features from convolutional and transformer models for prediction, and we view this approach as an ensemble technique to enhance model performance. Our experiments on polyp segmentation reveal that the proposed architecture surpasses other top models, exhibiting improved learning capacity and resilience. The code is available at https://github.com/HuangDLab/EnFormer.", "sections": [{"title": "Introduction", "content": "Polyp segmentation is a critical area within the field of medical image analysis, particularly in the context of gastrointestinal health. Polyps are abnormal tissue growths that can develop on the lining of the colon or rectum. While many polyps are benign, some can evolve into colorectal cancer. Therefore, early detection and removal are crucial for preventing cancer progression.\nIn recent years, convolutional neural networks (CNNs) and transformer architecture have demonstrated significant promise in the field of medical image segmentation. These models can learn complex features from training data, enabling them to generalize well to new, unseen images. Moreover, the integration of CNNs and transformer architecture has been widely applied for semantic segmentation. This is because CNNs and transformer has different assumptions for guiding the learning process. CNNs incorporates two inductive biases, namely locality and translation equivariance. The locality assumption posits that pixels in close spatial proximity are more likely to be related compared to those that are further apart, and translation equivariance pertains to the characteristic that a shift in the input image causes a corresponding shift in the feature maps generated by the convolutional layers. These attributes allow convolutional layers to focus on capturing a small portion of the input image rather than the whole image and maintain CNNs as a leading approach for vision tasks. In contrast, the transformer architecture does not possess these inherent inductive biases. Consequently, the transformer must acquire such biases from large-scale datasets by leveraging the attention mechanism to capture long-range dependencies. Thus, transformers tend to excel with large-scale datasets, whereas CNNs typically yield superior results with smaller datasets.\nIn summary, CNNs excel at capturing local structures, whereas transformers are adept at modeling global information. To create a model that comprehensively understands both local and global patterns, a hybrid architecture combining CNNs and transformers has been suggested to leverage the strengths of both architecture. In particular, some studies combine a CNN framework to capture the edge and corner features of images with a transformer framework to capture the long-distance relationships. We viewed this approach as an ensemble approach which integrate different architecture's strength to determine the final solution. These architectures achieve a great success in the filed of computer vision, especially in polyp segmentation. In the field of polyp segmentation, the traditional method is based on Unet-like CNN architectures, which act as the baseline model. However, with the increasing number of transformer architectures being introduced, research has shifted towards exploring the limitations of using transformers as the core component. This shift has led to significant improvements compared to CNN architectures."}, {"title": "Related Work", "content": "Semantic segmentation models typically comprise two main components: an encoder and a decoder. The encoder's role is to extract semantic details at various scales from the input image, while the decoder uses this information to produce the final output. For encoder design, ResNet architecture is the most commonly used convolution module for feature extraction, by passing the input through one or more layers to help mitigate the issue of gradient vanishing. In the context of transformer backbones, Pyramid Vision Transformer (PVT) introduced a progressively shrinking pyramid and spatial-reduction attention layer to create high-resolution, multi-scale feature maps, enabling Vision Transformer (ViT) models to perform effectively on dense predictions. PVTv2 further enhances the original PVT by adding three modules: a linear complexity attention layer, overlapping patch embedding, and a convolutional feed-forward network. CoaT , another transformer backbone, integrates co-scale and conv-attentional mechanisms, demonstrating superior performance compared to PVT.\nIn designing semantic segmentation models, the decoder component seeks to restore the original image dimensions using multi-scale feature maps. Typically, the decoder employs upsampling techniques and a progression of matrix operations. Traditional Unet architectures use skip connections to combine feature maps from the encoder into the decoder, leading to successful results. UNet++ redesigned these skip connections and introduced deep supervision to average the feature maps created by each segmentation branch. Moreover, alternative decoder designs, such as those used by DeepLab, have also achieved impressive performance on benchmark datasets like MS COCO and VOC. For instance, DeepLab uses the Atrous Spatial Pyramid Pooling (ASPP) block to better capture multi-level information for both small and large objects by dilating the receptive fields, leading to state-of-the-art performance. Building on Unet, MA-Net utilizes a self-attention mechanism to adaptively combine local features with their global contexts. LinkNet adheres to the standard encoder-decoder architecture commonly used in segmentation tasks and introduces a new method for connecting each encoder with its corresponding decoder. Feature Pyramid Network (FPN) leverages attention mechanisms in conjunction with spatial pyramids to achieve precise dense feature extraction for pixel-wise classification, avoiding the complexity of dilated convolutions and manually designed decoder networks. Pyramid Scene Parsing Network (PSPNet) introduces a pyramid pooling module to aggregate context from different regions. The Pyramid Attention Network (PAN) improves semantic segmentation by merging attention mechanisms with spatial pyramids to accurately extract dense features for pixel labeling. This network introduces a Feature Pyramid Attention module for better feature representation and a Global Attention Upsample module to assist low-level feature localization. Other architectures highlight various strategies for setting up encoders and decoders, as well as approaches to integrate features from the encoder into the decoder."}, {"title": "Polyp Segmentation", "content": "In the context of polyp segmentation, a Unet-like architecture is utilized as a baseline model to evaluate model performance. To enhance this performance, researchers have focused on devising a more precise and efficient method for decoding the feature maps created by the input image. PraNet leverages a parallel partial decoder to generate the high-level semantic global map and employs multiple reverse attention modules for accurate polyp segmentation. Polyp-PVT , which uses a transformer encoder"}, {"title": "Architecture in FCBFormer", "content": "We consider FCBFormer as a stacking approach in the ensemble technique that combines the outputs from both the convolution branch and the transformer branch. Specifically, the convolutional branch in FCBFormer employs a combination of Residual Blocks (RBs) with both downsampling and upsampling components. The link between the encoder and decoder functions similarly to a Unet structure. We named the encoder block and decoder block of the convolution branch as CBE and CBD respectively. For the input x, the definition of RB is as follows:\n$GSC(x) = Conv(SiLU(GN(x)))$\n$RB(x) = x + GSC(GSC(x))$\nwhere GN denotes group normalization, SiLU acts as a non-linearity activation function, and Conv represents a standard convolution layer. In the transformer branch, FCBFormer employs PVTv2-B3 as the encoder to derive feature maps from the transformer architecture. These extracted features are subsequently processed by a local emphasis (LE) module to improve local feature representations, followed by the integration of multi-scale features through a stepwise feature aggregation (SFA) module. The definitions of LE and SFA are as follows:\n$RB^2(x) = RB(RB(x))$\n$LE(x) = Up_4(RB^2(x))$\n$D(x, y) = RB^2(concat(x, y))$\n$SFA(e^1, e^2, e^3, e^4) = D(D(D(e^4, e^3), e^2), e^1)$\nHere, RB2 denotes double RBs, and $Up_4$ stands for an upsampling block that takes input feature maps and upsamples them to (H, W). The D refers to a decoder block that concatenates the output of the preceding decoder block with the skip connection from the transformer encoder. We use $e^j$ to signify the LE-improved extracted feature map from the jth stage of the transformer encoder. The entire SFA and LE processes are collectively termed as the Improved Progressive Locality Decoder (PLD+) to enhance the transformer's capability in extracting local features. Finally, the prediction head (PH) generates the final predicted probabilities:\n$PH(x) = \\sigma(Conv(RB^2(Up_4(x))))) $\nThe input to PH is formed by combining the outputs of the convolution and transformer branches. This combined output is then resized to match the original image dimensions, passed through RB2 to refine the decoding feature, and ultimately processed by a convolution layer with a kernel size of 1 then a sigmoid nonlinear transformation ($\\sigma$) for the final prediction."}, {"title": "Material", "content": "Semantic segmentation consists of three main sequential stages: encoding, decoding, and prediction. The encoding stage, represented by E, processes an input image to extract multi-scale information. Specifically, the jth block of the encoder E is denoted as $E_j$. Then, the decoding stage, denoted as D, uses the feature maps generated by the encoding stage to derive more precise features. Finally, the output of D is fed into a segmentation head S to segment objects in the image. For notation simplicity, these stages are combined into a composite function H =S0D0E.\nIn a segmentation task, we consider an image X with dimensions $X \\in R^{I \\times H \\times W}$, where I denotes the number of input channels. The associated label is $Y \\in R^{H \\times W}$, with $y_{h,w} \\in \\{0,1,...,C\\}$, where C indicates the number of segmentation classes. The feature maps produced after processing through the encoder block Ei are represented as $e^j \\in R^{I^j \\times H^j \\times W^j}$, where I, H, and Wi denote the number of output channels, height, and width of feature maps from Ei, respectively. The decoder D takes inputs from E and generates the decoding features d = D(E(X)), and predicted probabilities denoted as P = H(X) = S(d), where $P \\in R^{C \\times H \\times W}$, and $P_{c,h,w}$ signifies the probability of belonging to class c at pixel (h, w).\nIn an encoder-decoder-based architecture, selecting the encoder and decoding strategy is essential. For example, ResNet can be used to extract image features, and the Unet decoding strategy can be used to process these multi-scale features. Here, E = ResNet, $E_{i, j} = 1,\u2026, 4$ denotes the feature maps obtained from various stages of ResNet, D = Unet, and S refers to a series of convolutional layers followed by an upsampling layer. This notation can be applied to any encoder-decoder-based architecture.\nTo extend these notations into ensemble-base approach, let's say we have K encoders $E_1,\u00b7\u00b7\u00b7, E_K$. We use $\\&_i$ to represent the jth block of ith encoder, and $D_i$ to represent the ith decoder w.r.t. Ei, and we denote the encoding features as $e_{i,j}$ and decoding features as di. We also defined a module named fusion decoder (FD), denoted as F, to fuse the features generated by different encoders, which is, the input of F is $e_{i,j}$'s.\nThere are various ensemble methods. One method is the stacking approach, which concatenates the outputs of multiple decoders $D_1,\u2026,D_K$ and then constructs a segmentation head S to integrate the information from these decoders for the final prediction. Another method involves fusing feature maps produced in the early stages; the features $e^j$ are fused by fusion decoder F, and the output of F serves as the input for the segmentation head S. For instance, in FCBFormer, K = 2, where $E_1$ = CBE serves as the encoder for the convolution branch, and $D_1$ = CBD acts as the decoder. Additionally, $E_2$ =PVTv2-B3 represents the encoder for the transformer branch, and $D_2$ = PLD+ is its decoder. The final predicted probabilities are computed as P = H(X) = S(d1,d2), with S = PH. TransFuse, integrates ResNet and DeiT using a BiFusion architecture . Since ResNet and DeiT lack decoders, in this scenario, $E_1$ = ResNet, $E_2$ = DeiT, F = BiFusion and the prediction is given by P = H(X) = S(F([$e^j$]))."}, {"title": "Proposed method", "content": "Our methodology draws considerable inspiration from FCBFormer. We assert that FCBFormer's model architecture is deficient in facilitating information exchange during backpropagation. To mitigate this, we suggest incorporating a fuse decoder (FD) F into FCBFormer to enhance information transfer between encoders. Furthermore, constrained by hardware and computational efficiency considerations, our paper utilizes only two encoders (K = 2), integrating convolution and transformer modules in our technique to more effectively capture both local and global contexts. Our FD is characterized as follows:\n$MLP(x) = Up_4(ReLU(BN(Conv(x))))$\n$F_i(e_1^j, e_2^j) = MLP(concat(RB^2(e_1^j), RB^2(e_2^j)))$\n$F([e]) = MLP(concat(F_i(e_1^j, e_2^j) j = 1, 2, 3, 4))$\nwhere BN denotes batch normalization and ReLU non-linearity activation. The architecture of F is fairly simple. First, features are extracted using the convolution encoder $E_1$ and the transformer encoder $E_2$. These features are then concatenated and processed through a basic convolution module, MLP, to merge them. The fused features are then fed into the fuse decoder F.\nWithin the context of the fuse decoder configuration, we solely made use of the input features generated by the encoders. To derive the ultimate predicted probabilities, we proposed two different methods. The first method preserves the decoding strategy for each branch, resulting in the final prediction P = H(X) = S(d1,d2, F([$e^j$])) where S = PH. This approach is referred to as Ensemble Transformer and"}, {"title": "Experiment", "content": "To assess the performance of various models, we utilize five datasets: Kvasir , CVC-ClinicDB, CVC-300, CVC-ColonDB, and ETIS-LaribPolypDB . We adopt the data split from ."}, {"title": "Dataset", "content": "which supplies the training and testing sets for these datasets. The number of images in these datasets is displayed in Table 2. However, since  does not provide a validation set for hyperparameter tuning, we further divide the training set in a 9:1 ratio to avoid overfitting the testing set. We employ the training sets of the first two datasets for model training. During the testing phase, the testing sets of Kvasir and CVC-ClinicDB will be used to evaluate the model's learning ability, while those of the remaining three datasets will be used to evaluate the model's generalization capability.\nThe dataset is intended for polyp segmentation tasks, with an emphasis on outlining the polyp areas within the images. As a result, there is only one class (C = 1). To accelerate the model's training, we resize the original images to 352\u00d7352 pixels. To increase dataset diversity, we employ various data augmentation techniques, drawing primarily from [15, 16]. For geometric augmentations, we apply multiple transformations to both images and masks, such as horizontal and vertical flips, affine transformations, and grid distortions (dividing the image into a grid of cells and randomly shifting the grid intersections to create localized distortions). Additionally, exclusively for images, we use pixel-level color variations such as color jitter and unsharp (using a Gaussian kernel for image filtering). We also normalize the images using the standard ImageNet mean and standard deviation. Each augmentation is applied independently with a probability of 50%."}, {"title": "Training configuration", "content": "We used the AdamW  optimizer with a learning rate of 0.0001 and no weight decay. Additionally, we adopted the OneCycleLR schedule to regulate the learning rate. To ensure a fair evaluation of the different models, each was trained for a total of 200 epochs with a batch size of 16. Moreover, the models' encoder parameters were initialized with pretrained weights from ImageNet. The entire training dataset comprised 1305 examples, and the best model was saved based on the highest dice coefficient on a validation set of 145 images. The loss function for updating the parameters was the average of dice loss and binary cross entropy loss. All experiments were conducted using 4 Tesla V100-SXM2-16GB GPUs. In this paper, we divided the experiment into three segments. First, we examined the performance of the encoder-decoder-based model, utilizing nine different decoding strategies with ResNet50 as the encoder. At this point, all models are constructed using the Python package segmentation_models_pytorch [45]. Second, we explored the ensemble-based approach of our proposed EnFormer and EnFormer-Lite models. Lastly, we compared our models with FCBFormer, PraNet, and Polyp-PVT. Each or our proposed models was trained using our specified parameters, whereas PraNet and Polyp-PVT were trained with their released codes, and the best model was chosen based on our validation set partition."}, {"title": "Evaluation metrics", "content": "To assess the performance on the test set, different studies employ different methods. FCBFormer evaluates images at a resolution of 352 \u00d7 352 with a 0.5 threshold (i.e., probabilities > 0.5 are classified as polyp areas). In contrast, other studies assess images at their original sizes using varying thresholds, and then compute the average metrics across all thresholds. Following [36] approach, we resize the predicted probabilities to each test image's original size and compute the metrics over 256 thresholds, evenly distributed within [0,1], for each image in the test set. We then average these metrics across all images in the test set. Six metrics are employed for model evaluation: dice coefficient, intersection over union (IoU), mean absolute error (MAE), weighted F-measure (F\u03b2) , S-measure (Sa) , and E-measure (E\u03c6). Dice coefficient and IoU measure the shape similarity between the predicted mask and the ground truth, while MAE represents the L1 distance between the predicted probabilities and the ground truth. F\u03b2 is a weighted average of recall and precision, extending the F-measure (F1 score) to evaluate pixel-wise accuracy. Sa assesses structural similarity, and E\u03c6 evaluates both local and global similarity."}, {"title": "Result analysis", "content": "Our experimental results for five datasets are presented in Table 3, 4, 5, 6, and 7. The first two datasets are utilized to assess the model performance, as their training images are included in the training phase, and the validation images are employed to choose the best model for evaluation on the test set. Conversely, the last three datasets have no images included in the training phase and are therefore used to evaluate the model's generalization ability. From the first two datasets, our EnFormer-Lite Large achieves the highest dice coefficient on Kvasir and EnFormer achieves the highest dice coefficient on CVC-ClinicDB. Nevertheless, FCBFormer also performs excellently on both datasets. Additionally, the encoder-decoder-based model yields impressive results and is competitive with the ensemble-based model. In contrast, for the last three datasets, the encoder-decoder-based model shows a significant gap compared to the ensemble-based model, indicating that the ensemble-based model has superior robustness. In our experiment, our model demonstrates better robustness compared to FCBFormer, revealing that integrating the fuse decoder can enhance robustness without compromising learning ability."}, {"title": "Visualization", "content": "To explore the impact and implications of the transformer and convolution branches on model outcomes, we utilize visual representations of the feature maps generated by each branch. We standardize the feature maps of each module (i.e., transformer module vs. CNN module) to the interval [0,1], followed by their summation to emphasize the particular regions of the image detected by each branch. Additionally, to determine the image regions that the model relies on for its predictions, we apply the basic Grad-CAM  visualization method across multiple datasets, and the visual representation for EnFormer is illustrated in Figure 3.\nThe visualizations convincingly show that the two encoders capture distinct textures within the images. In the decoding part, the transformer branch and the fuse decoder play key roles in computing the majority of the gradients. At times, the convolution branch complements the transformer branch (e.g., in CVC-ColonDB). Furthermore, the segmentation head S adeptly merges feature maps from various branches. Even though convolution branches may occasionally capture incorrect features (e.g., in CVC-300), the fuse decoder manages to discern which branches accurately detect polyp regions."}, {"title": "Conclusion", "content": "In this paper, we explore various decoding strategies and ensemble-based models for polyp segmentation. Our experiments indicate that while encoder-decoder-based models effectively learn the polyp regions in the training set, they struggle to generalize to unseen images. In contrast, ensemble-based models significantly outperform the encoder-decoder models on unseen images, demonstrating higher robustness. Furthermore, we found that selecting an appropriate encoder within the ensemble-based approach is more beneficial than devising an advanced decoding strategy for the features learned by the encoders. For example, our proposed EnFormer-Lite, which includes a simple and lightweight decoder, achieves superior performance in terms of dice coefficient across five different datasets."}]}