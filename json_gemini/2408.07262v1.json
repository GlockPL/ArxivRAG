{"title": "Ensemble architecture in polyp segmentation", "authors": ["Hao-Yun Hsu", "Yi-Ching Cheng", "Guan-Hua Huang"], "abstract": "In this research, we revisit the architecture of semantic segmentation and evaluate the models\nexcelling in polyp segmentation. We introduce an integrated framework that harnesses the advantages\nof different models to attain an optimal outcome. More specifically, we fuse the learned features from\nconvolutional and transformer models for prediction, and we view this approach as an ensemble\ntechnique to enhance model performance. Our experiments on polyp segmentation reveal that the\nproposed architecture surpasses other top models, exhibiting improved learning capacity and resilience.\nThe code is available at https://github.com/HuangDLab/EnFormer.", "sections": [{"title": "1 Introduction", "content": "Polyp segmentation is a critical area within the field of medical image analysis, particularly in the context\nof gastrointestinal health. Polyps are abnormal tissue growths that can develop on the lining of the\ncolon or rectum. While many polyps are benign, some can evolve into colorectal cancer. Therefore, early\ndetection and removal are crucial for preventing cancer progression.\nIn recent years, convolutional neural networks (CNNs) and transformer architecture have demonstrated significant promise in the field of medical image segmentation. These models\ncan learn complex features from training data, enabling them to generalize well to new, unseen images.\nMoreover, the integration of CNNs and transformer architecture has been widely applied for\nsemantic segmentation. This is because CNNs and transformer has different assumptions for guiding the\nlearning process. CNNs incorporates two inductive biases, namely locality and translation equivariance.\nThe locality assumption posits that pixels in close spatial proximity are more likely to be related compared\nto those that are further apart, and translation equivariance pertains to the characteristic that a shift in the\ninput image causes a corresponding shift in the feature maps generated by the convolutional layers. These\nattributes allow convolutional layers to focus on capturing a small portion of the input image rather than\nthe whole image and maintain CNNs as a leading approach for vision tasks. In contrast, the transformer\narchitecture does not possess these inherent inductive biases. Consequently, the transformer must acquire\nsuch biases from large-scale datasets by leveraging the attention mechanism to capture long-range\ndependencies. Thus, transformers tend to excel with large-scale datasets, whereas CNNs typically yield\nsuperior results with smaller datasets.\nIn summary, CNNs excel at capturing local structures, whereas transformers are adept at modeling\nglobal information. To create a model that comprehensively understands both local and global patterns,\na hybrid architecture combining CNNs and transformers has been suggested to leverage the strengths of\nboth architecture. In particular, some studies combine a CNN framework to capture the edge\nand corner features of images with a transformer framework to capture the long-distance relationships.\nWe viewed this approach as an ensemble approach which integrate different architecture's strength to\ndetermine the final solution. These architectures achieve a great success in the filed of computer vision,\nespecially in polyp segmentation. In the field of polyp segmentation, the traditional method is based on\nUnet-like CNN architectures, which act as the baseline model. However, with the increasing number\nof transformer architectures being introduced, research has shifted towards exploring the limitations of\nusing transformers as the core component. This shift has led to significant improvements compared to\nCNN architectures."}, {"title": "2 Related Work", "content": "2.1 Semantic Segmentation\nSemantic segmentation models typically comprise two main components: an encoder and a decoder. The\nencoder's role is to extract semantic details at various scales from the input image, while the decoder\nuses this information to produce the final output. For encoder design, ResNet architecture is\nthe most commonly used convolution module for feature extraction, by passing the input through one or\nmore layers to help mitigate the issue of gradient vanishing. In the context of transformer backbones,\nPyramid Vision Transformer (PVT) introduced a progressively shrinking pyramid and spatial-reduction\nattention layer to create high-resolution, multi-scale feature maps, enabling Vision Transformer (ViT)\nmodels to perform effectively on dense predictions. PVTv2 further enhances the original PVT by adding\nthree modules: a linear complexity attention layer, overlapping patch embedding, and a convolutional\nfeed-forward network. CoaT, another transformer backbone, integrates co-scale and conv-attentional\nmechanisms, demonstrating superior performance compared to PVT.\nIn designing semantic segmentation models, the decoder component seeks to restore the original image\ndimensions using multi-scale feature maps. Typically, the decoder employs upsampling techniques and\na progression of matrix operations. Traditional Unet architectures use skip connections to combine\nfeature maps from the encoder into the decoder, leading to successful results. UNet++ redesigned\nthese skip connections and introduced deep supervision to average the feature maps created by each seg-\nmentation branch. Moreover, alternative decoder designs, such as those used by DeepLab, have\nalso achieved impressive performance on benchmark datasets like MS COCO and VOC. For\ninstance, DeepLab uses the Atrous Spatial Pyramid Pooling (ASPP) block to better capture multi-level\ninformation for both small and large objects by dilating the receptive fields, leading to state-of-the-art\nperformance. Building on Unet, MA-Net utilizes a self-attention mechanism to adaptively combine\nlocal features with their global contexts. LinkNet adheres to the standard encoder-decoder archi-\ntecture commonly used in segmentation tasks and introduces a new method for connecting each encoder\nwith its corresponding decoder. Feature Pyramid Network (FPN) leverages attention mechanisms in\nconjunction with spatial pyramids to achieve precise dense feature extraction for pixel-wise classification,\navoiding the complexity of dilated convolutions and manually designed decoder networks. Pyramid Scene\nParsing Network (PSPNet) introduces a pyramid pooling module to aggregate context from differ-\nent regions. The Pyramid Attention Network (PAN) improves semantic segmentation by merging\nattention mechanisms with spatial pyramids to accurately extract dense features for pixel labeling. This\nnetwork introduces a Feature Pyramid Attention module for better feature representation and a Global\nAttention Upsample module to assist low-level feature localization. Other architectures highlight vari-\nous strategies for setting up encoders and decoders, as well as approaches to integrate features from the\nencoder into the decoder.\n2.2 Polyp Segmentation\nIn the context of polyp segmentation, a Unet-like architecture is utilized as a baseline model to evaluate\nmodel performance. To enhance this performance, researchers have focused on devising a more precise\nand efficient method for decoding the feature maps created by the input image. PraNet leverages\na parallel partial decoder to generate the high-level semantic global map and employs multiple reverse\nattention modules for accurate polyp segmentation. Polyp-PVT, which uses a transformer encoder"}, {"title": "2.3 Architecture in FCBFormer", "content": "We consider FCBFormer as a stacking approach in the ensemble technique that combines the outputs\nfrom both the convolution branch and the transformer branch. Specifically, the convolutional branch in\nFCBFormer employs a combination of Residual Blocks (RBs) with both downsampling and upsampling\ncomponents. The link between the encoder and decoder functions similarly to a Unet structure. We\nnamed the encoder block and decoder block of the convolution branch as $CBE$ and $CBD$ respectively. For\nthe input $x$, the definition of RB is as follows:\n$GSC(x) = Conv(SiLU(GN(x)))$\n$RB(x) = x + GSC(GSC(x))$\nwhere $GN$ denotes group normalization, $SiLU$ acts as a non-linearity activation function, and $Conv$ rep-\nresents a standard convolution layer. In the transformer branch, FCBFormer employs PVTv2-B3 as\nthe encoder to derive feature maps from the transformer architecture. These extracted features are sub-\nsequently processed by a local emphasis (LE) module to improve local feature representations, followed\nby the integration of multi-scale features through a stepwise feature aggregation (SFA) module. The\ndefinitions of LE and SFA are as follows:\n$RB^{2}(x) = RB(RB(x))$\n$LE(x) = Up_{4}(RB^{2}(x))$\n$D(x, y) = RB^{2} (concat(x, y))$\n$SFA(e^{1}, e^{2}, e^{3}, e^{4}) = D(D(D(e^{4}, e^{3}), e^{2}), e^{1})$\nHere, $RB^{2}$ denotes double RBs, and $Up_{4}$ stands for an upsampling block that takes input feature maps\nand upsamples them to $(\\frac{H}{4}, \\frac{W}{4})$. The $D$ refers to a decoder block that concatenates the output of the\npreceding decoder block with the skip connection from the transformer encoder. We use $e^{j}$ to signify the\nLE-improved extracted feature map from the jth stage of the transformer encoder. The entire SFA and\nLE processes are collectively termed as the Improved Progressive Locality Decoder (PLD+) to enhance\nthe transformer's capability in extracting local features. Finally, the prediction head (PH) generates the\nfinal predicted probabilities:\n$PH(x) = \\sigma(Conv(RB^{2}(Up_{4}(x))))$\nThe input to PH is formed by combining the outputs of the convolution and transformer branches. This\ncombined output is then resized to match the original image dimensions, passed through $RB^{2}$ to refine the\ndecoding feature, and ultimately processed by a convolution layer with a kernel size of 1 then a sigmoid\nnonlinear transformation ($\\sigma$) for the final prediction."}, {"title": "3 Material", "content": "Semantic segmentation consists of three main sequential stages: encoding, decoding, and prediction. The\nencoding stage, represented by $E$, processes an input image to extract multi-scale information. Specifically,\nthe jth block of the encoder $E$ is denoted as $E_{j}$. Then, the decoding stage, denoted as $D$, uses the feature\nmaps generated by the encoding stage to derive more precise features. Finally, the output of $D$ is fed\ninto a segmentation head $S$ to segment objects in the image. For notation simplicity, these stages are\ncombined into a composite function $H = S \\circ D \\circ E$.\nIn a segmentation task, we consider an image $X$ with dimensions $X \\in \\mathbb{R}^{I \\times H \\times W}$, where $I$ denotes\nthe number of input channels. The associated label is $Y \\in \\mathbb{R}^{H \\times W}$, with $y_{h,w} \\in {0,1,...,C}$, where $C$\nindicates the number of segmentation classes. The feature maps produced after processing through the\nencoder block $E_{j}$ are represented as $e^{j} \\in \\mathbb{R}^{I^{j} \\times H^{j} \\times W^{j}}$, where $I^{j}$, $H^{j}$, and $W^{j}$ denote the number of output\nchannels, height, and width of feature maps from $E_{j}$, respectively. The decoder $D$ takes inputs from $E$ and\ngenerates the decoding features $d = D(E(X))$, and predicted probabilities denoted as $P = H(X) = S(d)$,\nwhere $P \\in \\mathbb{R}^{C \\times H \\times W}$, and $P_{c,h,w}$ signifies the probability of belonging to class $c$ at pixel $(h, w)$.\nIn an encoder-decoder-based architecture, selecting the encoder and decoding strategy is essential. For\nexample, ResNet can be used to extract image features, and the Unet decoding strategy can be used to\nprocess these multi-scale features. Here, $E = ResNet$, $E_{j}, j = 1,..., 4$ denotes the feature maps obtained\nfrom various stages of ResNet, $D = Unet$, and $S$ refers to a series of convolutional layers followed by an\nupsampling layer. This notation can be applied to any encoder-decoder-based architecture.\nTo extend these notations into ensemble-base approach, let's say we have $K$ encoders $E_{1}, \\cdots, E_{K}$. We\nuse $E_{i,j}$ to represent the jth block of ith encoder, and $D_{i}$ to represent the ith decoder w.r.t. $E_{i}$, and we\ndenote the encoding features as $e_{i}$ and decoding features as $d_{i}$. We also defined a module named fusion\ndecoder (FD), denoted as $F$, to fuse the features generated by different encoders, which is, the input of\n$F$ is $e_{i}$'s.\nThere are various ensemble methods. One method is the stacking approach, which concatenates the\noutputs of multiple decoders $D_{1},...,D_{K}$ and then constructs a segmentation head $S$ to integrate the\ninformation from these decoders for the final prediction. Another method involves fusing feature maps\nproduced in the early stages; the features $e_{i}$ are fused by fusion decoder $F$, and the output of $F$ serves as\nthe input for the segmentation head $S$. For instance, in FCBFormer, $K = 2$, where $E_{1} = CBE$ serves as\nthe encoder for the convolution branch, and $D_{1} = CBD$ acts as the decoder. Additionally, $E_{2} =PVTv2-B3\nrepresents the encoder for the transformer branch, and $D_{2} = PLD+$ is its decoder. The final predicted\nprobabilities are computed as $P = H(X) = S(d_{1},d_{2})$, with $S = PH$. TransFuse, integrates ResNet\nand DeiT using a BiFusion architecture. Since ResNet and DeiT lack decoders, in this scenario,\n$E_{1} = ResNet$, $E_{2} = DeiT$, $F = BiFusion$ and the prediction is given by $P = H(X) = S(F([e_{i}]))$.\n3.1 Proposed method\nOur methodology draws considerable inspiration from FCBFormer. We assert that FCBFormer's model\narchitecture is deficient in facilitating information exchange during backpropagation. To mitigate this, we\nsuggest incorporating a fuse decoder (FD) $F$ into FCBFormer to enhance information transfer between\nencoders. Furthermore, constrained by hardware and computational efficiency considerations, our paper\nutilizes only two encoders ($K = 2$), integrating convolution and transformer modules in our technique to\nmore effectively capture both local and global contexts. Our FD is characterized as follows:\n$MLP(x) = Up_{4}(ReLU(BN(Conv(x))))$\n$F_{i}(e^{i}_{1}, e^{i}_{2}) = MLP(concat(RB^{2}(e^{i}_{1}), RB^{2}(e^{i}_{2})))$\n$F([e_{i}]) = MLP(concat(F_{i} (e^{i}_{j}, e^{2}_{j}) j = 1, 2, 3, 4))$\nwhere $BN$ denotes batch normalization and $ReLU$ non-linearity activation. The architecture of\n$F$ is fairly simple. First, features are extracted using the convolution encoder $E_{1}$ and the transformer\nencoder $E_{2}$. These features are then concatenated and processed through a basic convolution module,\nMLP, to merge them. The fused features are then fed into the fuse decoder $F$.\nWithin the context of the fuse decoder configuration, we solely made use of the input features generated\nby the encoders. To derive the ultimate predicted probabilities, we proposed two different methods.\nThe first method preserves the decoding strategy for each branch, resulting in the final prediction $P =$\n$H(X) = S(d_{1},d_{2}, F([e_{i}]))$ where $S = PH$. This approach is referred to as Ensemble Transformer and"}, {"title": "4 Experiment", "content": "4.1 Dataset\nTo assess the performance of various models, we utilize five datasets: Kvasir, CVC-ClinicDB,\nCVC-300, CVC-ColonDB, and ETIS-LaribPolypDB. We adopt the data split from"}, {"title": "4.2 Training configuration", "content": "We used the AdamW optimizer with a learning rate of 0.0001 and no weight decay. Additionally,\nwe adopted the OneCycleLR schedule to regulate the learning rate. To ensure a fair evaluation of\nthe different models, each was trained for a total of 200 epochs with a batch size of 16. Moreover, the\nmodels' encoder parameters were initialized with pretrained weights from ImageNet. The entire training\ndataset comprised 1305 examples, and the best model was saved based on the highest dice coefficient on a\nvalidation set of 145 images. The loss function for updating the parameters was the average of dice loss and\nbinary cross entropy loss. All experiments were conducted using 4 Tesla V100-SXM2-16GB GPUs. In this\npaper, we divided the experiment into three segments. First, we examined the performance of the encoder-\ndecoder-based model, utilizing nine different decoding strategies with ResNet50 as the encoder. At this\npoint, all models are constructed using the Python package segmentation_models_pytorch. Second,\nwe explored the ensemble-based approach of our proposed EnFormer and EnFormer-Lite models. Lastly,\nwe compared our models with FCBFormer, PraNet, and Polyp-PVT. Each or our proposed models was\ntrained using our specified parameters, whereas PraNet and Polyp-PVT were trained with their released\ncodes, and the best model was chosen based on our validation set partition."}, {"title": "4.3 Evaluation metrics", "content": "To assess the performance on the test set, different studies employ different methods. FCBFormer eval-\nuates images at a resolution of 352 \u00d7 352 with a 0.5 threshold (i.e., probabilities > 0.5 are classified\nas polyp areas). In contrast, other studies assess images at their original sizes using varying\nthresholds, and then compute the average metrics across all thresholds. Following approach, we resize\nthe predicted probabilities to each test image's original size and compute the metrics over 256 thresholds,\nevenly distributed within [0,1], for each image in the test set. We then average these metrics across\nall images in the test set. Six metrics are employed for model evaluation: dice coefficient, intersection\nover union (IoU), mean absolute error (MAE), weighted F-measure (F$\\beta$ ), S-measure ($S_{a}$ ), and\nE-measure ($E_{\\xi}$ ). Dice coefficient and IoU measure the shape similarity between the predicted mask\nand the ground truth, while MAE represents the L1 distance between the predicted probabilities and the\nground truth. For F is a weighted average of recall and precision, extending the F-measure (F1 score) to\n$\\beta$\nevaluate pixel-wise accuracy. $S_{a}$ assesses structural similarity, and $E_{\\xi}$ evaluates both local and global\nsimilarity."}, {"title": "4.4 Result analysis", "content": "Our experimental results for five datasets are presented in Table 3, 4, 5, 6, and 7. The first two datasets are\nutilized to assess the model performance, as their training images are included in the training phase, and\nthe validation images are employed to choose the best model for evaluation on the test set. Conversely, the\nlast three datasets have no images included in the training phase and are therefore used to evaluate the\nmodel's generalization ability. From the first two datasets, our EnFormer-Lite Large achieves the highest\ndice coefficient on Kvasir and EnFormer achieves the highest dice coefficient on CVC-ClinicDB. Never-\ntheless, FCBFormer also performs excellently on both datasets. Additionally, the encoder-decoder-based\nmodel yields impressive results and is competitive with the ensemble-based model. In contrast, for the last\nthree datasets, the encoder-decoder-based model shows a significant gap compared to the ensemble-based\nmodel, indicating that the ensemble-based model has superior robustness. In our experiment, our model\ndemonstrates better robustness compared to FCBFormer, revealing that integrating the fuse decoder can\nenhance robustness without compromising learning ability."}, {"title": "4.5 Visualization", "content": "To explore the impact and implications of the transformer and convolution branches on model outcomes,\nwe utilize visual representations of the feature maps generated by each branch. We standardize the feature\nmaps of each module (i.e., transformer module vs. CNN module) to the interval [0,1], followed by their\nsummation to emphasize the particular regions of the image detected by each branch. Additionally, to\ndetermine the image regions that the model relies on for its predictions, we apply the basic Grad-CAM\nvisualization method across multiple datasets, and the visual representation for EnFormer is illustrated\nin Figure 3.\nThe visualizations convincingly show that the two encoders capture distinct textures within the images.\nIn the decoding part, the transformer branch and the fuse decoder play key roles in computing the majority\nof the gradients. At times, the convolution branch complements the transformer branch (e.g., in CVC-\nColonDB). Furthermore, the segmentation head S adeptly merges feature maps from various branches.\nEven though convolution branches may occasionally capture incorrect features (e.g., in CVC-300), the\nfuse decoder manages to discern which branches accurately detect polyp regions."}, {"title": "5 Conclusion", "content": "In this paper, we explore various decoding strategies and ensemble-based models for polyp segmentation.\nOur experiments indicate that while encoder-decoder-based models effectively learn the polyp regions\nin the training set, they struggle to generalize to unseen images. In contrast, ensemble-based models\nsignificantly outperform the encoder-decoder models on unseen images, demonstrating higher robustness.\nFurthermore, we found that selecting an appropriate encoder within the ensemble-based approach is more\nbeneficial than devising an advanced decoding strategy for the features learned by the encoders. For\nexample, our proposed EnFormer-Lite, which includes a simple and lightweight decoder, achieves superior\nperformance in terms of dice coefficient across five different datasets."}]}