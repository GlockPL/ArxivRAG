{"title": "World-Grounded Human Motion Recovery via Gravity-View Coordinates", "authors": ["ZEHONG SHEN", "HUAIJIN PI", "YAN XIA", "ZHI CEN", "SIDA PENG", "ZECHEN HU", "HUJUN BAO", "RUIZHEN HU", "XIAOWEI ZHOU"], "abstract": "We present a novel method for recovering world-grounded human motion from monocular video. The main challenge lies in the ambiguity of defining the world coordinate system, which varies between sequences. Previous approaches attempt to alleviate this issue by predicting relative motion in an autoregressive manner, but are prone to accumulating errors. Instead, we propose estimating human poses in a novel Gravity-View (GV) coordinate system, which is defined by the world gravity and the camera view direction. The proposed GV system is naturally gravity-aligned and uniquely defined for each video frame, largely reducing the ambiguity of learning image-pose mapping. The estimated poses can be transformed back to the world coordinate system using camera rotations, forming a global motion sequence. Additionally, the per-frame estimation avoids error accumulation in the autoregressive methods. Experiments on in-the-wild benchmarks demonstrate that our method recovers more realistic motion in both the camera space and world-grounded settings, outperforming state-of-the-art methods in both accuracy and speed.", "sections": [{"title": "1 INTRODUCTION", "content": "World-Grounded Human Motion Recovery (HMR) aims to reconstruct continuous 3D human motion within a gravity-aware world coordinate system. Unlike conventional motion captured in the camera frame [Kanazawa et al. 2018], world-grounded motion is inherently suitable as foundational data for generative and physical models, such as text-to-motion generation [Guo et al. 2022; Tevet et al. 2023] and humanoid robot imitation learning [He et al. 2024]. In these applications, motion sequences must be high-quality and consistent in a gravity-aware world coordinate system.\nMost existing HMR methods can recover promising camera-space human motion from videos [Kocabas et al. 2020; Shen et al. 2023; Wei et al. 2022]. To recover the global motion, a straightforward approach is to use camera poses [Teed et al. 2024] to transform camera-space motion to world-space. However, the results are not guaranteed to be gravity-aligned, and errors in translations and poses can accumulate over time, resulting in implausible global motion. Recent work, WHAM [Shin et al. 2024], attempts to recover global motion by autoregressively predicting relative global poses with RNN. While this method achieves significant improvements, it requires a good initialization and suffers from accumulated errors over long sequences, making it challenging to maintain consistency in the gravity direction. We believe the inherent challenge stems from the ambiguity in defining the world coordinate system. Given the world coordinate axes, any rotation around the gravity axis defines a valid gravity-aware world coordinate system.\nIn this work, we propose GVHMR to estimate gravity-aware human poses for each frame and then compose them with gravity constraints to avoid accumulated errors in the gravity direction. This design is motivated by the observation that, for a person in any image, we humans are able to easily infer the gravity-aware human pose. Additionally, given two consecutive frames, it is intuitively easier to estimate the 1-degree-of-freedom rotation around the gravity direction, compared to the full 3-degree-of-freedom rotation. Therefore, we propose a novel Gravity-View (GV) coordinate system, defined by the gravity and camera view directions. Using the GV system, we develop a network that predicts the gravity-aware human orientation. We also propose a recovery algorithm to estimate the relative rotation between GV systems, enabling us to align all frames into a consistent gravity-aware world coordinate system.\nThanks to the GV coordinates, we can process human rotations in parallel over time. We propose a transformer [Vaswani et al. 2017] model enhanced with Rotary Positional Embedding (RoPE) [Su et al. 2024] to directly regress the entire motion sequence. Compared to the commonly used absolute position encoding, RoPE better captures the relative relationships between video frames and handles long sequences more effectively. During inference, we introduce a mask to limit each frame's receptive field, avoiding the complex sliding windows and enabling parallel inference for infinitely long sequences. Additionally, we predict stationary labels for hands and feet, which are used to refine foot sliding and global trajectories.\nIn summary, our contributions are threefold: 1. We propose a novel Gravity-View coordinate system and the global orientation recovery method to reduce the cumulative errors in the gravity direction. 2. We develop a Transformer model enhanced by RoPE to generalize to long sequences and improve motion estimation. 3. We demonstrate the effectiveness of our approach through extensive experiments, showing that it outperforms previous methods in both in-camera and world-grounded accuracy."}, {"title": "2 RELATED WORKS", "content": "Camera-Space Human Motion Recovery. Recent studies in 3D human recovery predominantly use parametric human models such as SMPL [Loper et al. 2023; Pavlakos et al. 2019]. Given a single image or video, the target is to align the human mesh precisely with the 2D images. Early methods [Bogo et al. 2016; Pavlakos et al. 2019] employ optimization-based approaches by minimizing the repro-jection error. Recently, regression-based methods [Goel et al. 2023; Kanazawa et al. 2018] trained on a large amount of data predict the SMPL parameters from the input image directly. Many efforts have been made to improve the accuracy by specialized design architectures [Li et al. 2023; Zhang et al. 2023], part-based reasoning [Kocabas et al. 2021a; Li et al. 2021], and incorporating camera parameters [Kocabas et al. 2021b; Li et al. 2022b]. HMR2.0 [Goel et al. 2023] designs a ViT architecture [Vaswani et al. 2017] and outperforms the previous methods. To utilize temporal cues, [Shi et al. 2020] uses deep networks to predict skeleton pose sequence di-rectly from videos. To recover the human mesh, most methods build upon the HMR pipeline. [Kanazawa et al. 2019] adopts a convolu-tional encoder. [Choi et al. 2021; Kocabas et al. 2020; Luo et al. 2020] apply RNN successfully. [Sun et al. 2019] introduces self-attention to CNN. [Shen et al. 2023; Wan et al. 2021] employ a transformer encoder to extract temporal information.\nAlthough these methods can accurately estimate human pose, their predictions are all in the camera-space. Consequently, when the camera moves, the human motion becomes physically implausible.\nWorld-Grounded Human Motion Recovery. Traditionally, estimating human motion in a gravity-aware world coordinate system requires additional floor plane calibration or gravity sensors. In multi camera capture systems [Huang et al. 2022; Ionescu et al. 2014], calibration boards are placed on the ground to reconstruct the ground plane and global scale. IMU-based methods [Kaufmann et al. 2023; von Marcard et al. 2018; Yi et al. 2021] use gyroscopes and accelerometers to estimate the gravity direction and then project human motion onto the gravity direction. Recently, researchers put efforts to estimate global human motion from a monocular video. [Yu et al. 2021] reconstructs human motion using physics law but requires a provided scene. Methods like [Li et al. 2022a; Yuan et al. 2022] predicts the global trajectory from locomotion cues. However, the camera motion and human motion are coupled, which make the results noisy. SLAHMR [Ye et al. 2023] and PACE [Kocabas et al. 2024] further integrate SLAM [Teed and Deng 2021; Teed et al. 2024] and pre-learned human motion priors [Rempe et al. 2021] in an optimization framework. Although these methods achieve promising results, the optimization process is time-consuming and faces convergence issues with long video sequences. Furthermore, these methods do not obtain gravity-aligned human motion.\nThe most relevant work is WHAM [Shin et al. 2024], which di-rectly regresses per-frame pose and translation in an autoregressive manner. However, their method relies on a good initialization and the performance drops in long-term motion recovery due to error accumulation."}, {"title": "3 METHOD", "content": "Given a monocular video ${I_t}_{t=0}^T$, we formulate the task as predicting: (1) the local body poses ${\\theta_t \\in \\mathbb{R}^{21\\times 3}}_{t=0}^T$ and shape coefficients $\\beta \\in \\mathbb{R}^{10}$ of SMPL-X, (2) the human trajectory from SMPL space to the camera space, including the orientation ${\\Gamma_t^c \\in \\mathbb{R}^{3}}_{t=0}^T$ and translation ${\\tau_t^c \\in \\mathbb{R}^{3}}_{t=0}^T$, (3) the trajectory to the world space, including the orientation ${\\Gamma_t^w \\in \\mathbb{R}^{3}}_{t=0}^T$ and translation ${\\tau_t^w \\in \\mathbb{R}^{3}}_{t=0}^T$.\nAn overview of the proposed pipeline is shown in Fig. 3. In Sec. 3.1, we first introduce the global trajectory representation and discuss its advantages over previous trajectory representations. Then, Sec. 3.2 describes a specially designed network architecture as well as post-process techniques for predicting the targets. Finally, implementation details are presented in Sec. 3.3."}, {"title": "3.1 Global Trajectory Representation", "content": "Global human trajectory {$\\Gamma, \\tau$} refers to the transformation from SMPL space to the gravity-aware world space W. However, the definition of W varies, as any rotation of W around the gravity direction is valid, leading to different $I_w$ and $\\tau_w$. We propose to first recover a gravity-aware human pose for each image, then transform these poses to a consistent global trajectory. This approach is inspired by the observation that humans can easily infer the orientation and gravity direction of a person in an image. And for consecutive frames, estimating the relative rotation around the gravity direction is intuitively easier and more robust.\nSpecifically, for each image, we use the world gravity direction and the camera's view direction (i.e., the normal vector of the image plane) to define Gravity-View (GV) Coordinates. The proposed new GV coordinate system is mainly used to resolve the rotation ambiguity, so we only predict the per-frame human orientation $\\Gamma^{GV}_t$ relative to the GV system. When the camera moves, we compute the relative rotation between the GV systems of two adjacent frames with relative camera rotations $R_t^c$, thus transforming all $\\Gamma^{GV}_t$ to a consistent gravity-aware global space. For global translation, following [Rempe et al. 2021; Shin et al. 2024], we predict the human displacement in the SMPL coordinate system from time t to t + 1, and finally roll out in the aforementioned world reference frame.\nGravity-View Coordinate System. As illustrated in Fig. 4, (a) given a person with orientation $\\Gamma^c$ and a gravity direction $\\vec{g}$ both described in the camera space: (b) the y-axis of the GV coordinate system aligns with the gravity direction $\\vec{g}$, i.e., $\\hat{y} = \\vec{g}$; (c) the x-axis is perpendicular to both the camera view direction $view = [0, 0, 1]^T$ and $\\hat{y}$ by cross-product, i.e., $\\hat{x} = \\hat{y} \\times view$; (d) finally, the z-axis is calculated by the right-hand rule, i.e., $\\hat{z} = \\hat{x} \\times \\hat{y}$. After obtaining these axes, we can re-calculate the person's orientation in the GV coordinate system as our learning target: $\\Gamma^{GV} = R_{c2GV} \\cdot \\Gamma^c = [\\hat{x}, \\hat{y}, \\hat{z}]^T \\cdot \\Gamma$.\nRecovering Global Trajectory. It is noteworthy that an independent GV exists for each input frame t, where we predict the person's orientation $\\Gamma^{GV}_t$. To recover a consistent global trajectory {$\\Gamma, \\tau$}, all orientations must be transformed to a common reference system. In practice, we use GV as the world reference system W.\nTo begin with, in the special case of a static camera, the $GV_t$ systems are identical across all frames. Therefore, the human global orientation {$\\Gamma_t^w$} is equivalent to {$\\Gamma^{GV}_t$}. The translation {$\\tau_t^w$} is obtained by transforming all predicted local velocities $v_{root}$ into the world coordinate system using the orientations {$\\Gamma_t^w$} and then performing a cumulative sum:\n$\\tau_0^w$ = $[0,0,0]^T$,\n$\\tau_t^w = \\sum_{t=0}^{t} \\Gamma_{t-1}^{w}v_{root}  \\quad \\quad t > 0.  \\qquad(1)$\nFor a moving camera, we first compute the rotation $R_{GV}$ between the GV coordinate systems of frame t to frame t - 1 by leveraging the input camera relative rotations $R^c_t$, the predicted human orientations $\\Gamma^{GV}_t$ and $\\Gamma^{GV}_{t-1}$. As illustrated in Fig. 5, we first calculate the rotation from camera to GV coordinate system at frame t: $R_{c2gv} = \\Gamma^{GV} \\cdot (\\Gamma^c)^{-1}$. Then, the camera view direction $view = [0, 0, 1]^T$ is transformed to the GV coordinate system as $view_t^{GV} = R_{c2gv} view$.\nWe use the camera's relative transformation to rotate this view direction to frame t \u2212 1, i.e., $view_{t-1} = (R_t^c)^{-1} \\cdot view_t$. Since the rotation between the GV systems is always around the gravity vector, we can calculate the rotation matrix $R_{GV}$ by projecting the view directions $view_{t-1}$ and $view_t$ onto the xz-plane and computing the angle between them. After obtaining {$R_{GV}$} of the entire input sequences, we can roll out to the first frame's GV coordinate system for all frames:\n$\\Gamma_0^{GV} \\qquad t = 0$,\n$\\Gamma_t^w = \\prod_{i=1}^{t} R_{GV}  \\Gamma^{GV}_t \\qquad t > 0.  \\qquad(2)$\nThis formulation also applies to static cameras, as the transformation $R_{GV}$ is the identity transformation in this case. Finally, the translation is obtained using the same method as described in Eq. 1.\nThe human orientation in the GV coordinate system is well-suited for deep network learning, given that the establishment of the GV coordinate system is determined from the input images. It also ensures that the learned global orientation is naturally gravity-aware. We have also found this approach beneficial for learning local pose and shape, as demonstrated in the ablation study Tab. 3. In the rotation recovery algorithm between GV systems, we utilize the consistency of the y-axis in the GV system to systematically avoid cumulative errors in the gravity direction. This also mitigates potential errors in camera rotation estimation, resulting in our method achieving similar results under both GT Gyro and DPVO estimated relative camera rotations, as shown in Tab. 1. Compared to WHAM, our method does not require initialization and can predict in parallel without the need for autoregressive prediction."}, {"title": "3.2 Network Design", "content": "Input and preprocessing. The network design is shown in Fig. 6. Inspired by WHAM [Shin et al. 2024], we first preprocess the input video into four types of features: bounding boxes [Jocher et al. 2023; Li et al. 2022b], 2D keypoints [Xu et al. 2022], image features [Goel et al. 2023], and relative camera rotations [Teed et al. 2024]. Then, in the early-fusion module, we use individual MLPs to map these features to the same dimension. These vectors are then element-wise added to obtain per-frame tokens {$f_{token} \\in \\mathbb{R}^{512}$}. These tokens are processed by a Relative Transformer, where we introduce rotary positional encoding (RoPE) [Su et al. 2024] to enable the network to focus on relative position features. Additionally, we implement a receptive-field-limited attention mask to improve the network's generalization ability when testing on long sequences.\nRotary positional embedding. Absolute positional embedding is a common approach for transformer architectures in human motion modeling. However, this implicitly reduces the model's ability to generalize to long sequences because the model is not trained on positional encodings beyond the training length. We argue that the absolute position of human motions is ambiguous (e.g., the start of a motion sequence can be arbitrary). In contrast, the relative position is well-defined and can be easily learned.\nHere we introduce rotary positional embedding to inject relative features into temporal tokens, where the output of of the t-th token after the self-attention layer is calculated via:\n$o_t =  \\sum_{s \\in T} Softmax(a_{ts}) W_o f_{token}   \\qquad(3)$\n$a_{ts} = (W_q f_{token})^T R(p^t - p^s) (W_k f_{token})   \\qquad(4)$\nwhere $W_q$, $W_k$, $W_v$ are the projection matrix, $R(\\cdot) \\in \\mathbb{R}^{512 \\times 512}$ is the rotary encoding of the relative position between two tokens, and $p^t$ indicates the temporal index of the t-th token. Following the definition in RoPE, we divide the 512-dimensional space into 256 subspaces and combine them using the linearity of the inner product. R() is defined as:\n$R(\\rho) = \\begin{bmatrix} \\hat{R} (a_1\\rho) & & 0 \\\\ & \\ddots & \\\\ 0 & & \\hat{R} (a_{256}\\rho)  \\end{bmatrix}  \\qquad(5)$\n$\\hat{R}(0) =  \\begin{pmatrix} cos \\theta & -sin \\theta \\\\ sin & cos \\theta  \\end{pmatrix} ,$\nwhere $a_i$ is pre-defined frequency parameters.\nAt inference time, we further introduce an attention mask [Press et al. 2022] and the self-attention becomes:\n$o_t =  \\sum_{s \\in T} Softmax(a_{ts} + m_{ts}) W_o f_{token}   \\qquad(6)$\n$m_{ts} =  \\begin{cases}  0, & if -L < t - s < L, \\\\ -\\infty, & otherwise.  \\end{cases}  \\qquad(7)$\nwhere L is the maximum training length. The token t attends only to tokens within L relative positions. Consequently, the model can generalize to arbitrarily long sequences without needing autoregressive inference techniques, such as sliding-window."}, {"title": "4 EXPERIMENTS", "content": "Evaluation datasets. Following WHAM [Shin et al. 2024], we eval-uate our method on three in-the-wild benchmarks: 3DPW [von Marcard et al. 2018], RICH [Huang et al. 2022], EMDB [Kaufmann et al. 2023]. We use RICH and EMDB-2 split to evaluate the global performance. The RICH test set contains 191 videos captured with static cameras, totaling 59.1 minutes with accurate global human motion annotations. The EMDB-2 is captured with moving cameras and contains 25 sequences totaling 24.0 minutes. Additionally,"}, {"title": "4.2 Comparison on Global Motion Recovery", "content": "We compare our method with several state-of-the-art methods that recover global motion and a straightforward baseline method that combines the state-of-the-art camera-space method HMR2.0 [Goel et al. 2023] with a SLAM method (DPVO [Teed et al. 2024]). The GT gyro indicates the ground-truth camera rotation data in the EMDB dataset provided by the ARKit. For the static camera in the RICH dataset, we set the camera transformation to an identity matrix.\nAs illustrated in Tab. 1, our method achieves the best performance on all metrics. Compared to WHAM, we can better handle errors in relative camera rotation estimation. On the EMDB dataset with dy-namic camera inputs, using DPVO instead of Gyro results in only a 1.6mm/0.1% drop in the W-MPJPE100/RTE metrics, while WHAM experiences a drop of 19.5mm/1.9%. Compared to optimization-based algorithms like GLAMR and SLAHMR, our method also achieves better smoothness metrics. Although these methods incorporate a smoothness loss, they may struggle due to the high difficulty of the actions in the dataset. Compared to regression methods like TRACE, our algorithm generalizes better to new datasets and achieves superior results. An important baseline is HMR2.0+DPVO. We found that, although HMR2.0 performs well in camera-space Tab. 2, it performs poorly in global motion recovery. Particularly on the RICH dataset, the camera transformation is identity, indicating that camera-space estimation of human pose struggles to recover correct and consistent translation and scale. Additionally, such methods cannot achieve gravity-aligned results. In contrast, our algorithm naturally provides gravity-aligned results."}, {"title": "4.3 Comparison on Camera Space Motion Recovery", "content": "We compare our method with state-of-the-art motion recovery methods that predict camera-space results. The results are shown in Tab. 2, where our method achieves the best performance on most of the metrics with a clear margin, demonstrating the effectiveness of our method in camera-space motion recovery. We attribute this to the multitask learning strategy that enables our model to use global motion information to improve the camera-space motion estimation, especially the shape and smoothness of the motion. Our PA-MPJPE performance is slightly behind WHAM by 0.3 mm on the 3DPW dataset. This may be due to the fact that we do not directly predict the SMPL parameters, but rather the SMPLX parameters, which might introduce some errors. Nevertheless, the numbers are still competitive."}, {"title": "4.4 Understanding GVHMR", "content": "Ablation Studies. To understand the impact of each component in our method, we evaluate seven variants of GVHMR using the same training and evaluation protocol on the RICH dataset. The results are shown in Tab. 3: (1) w/o GV: when predicting human motion solely in the camera coordinate system, the metrics drop slightly. This suggests that gravity alignment improves camera-space human motion estimation accuracy. For this variant, we can further recover a non-gravity-aligned global motion, which performs poorly in global metrics. (2) w/o IGV: when predicting the relative global orientation from frame to frame, the world-coordinate metrics drop substantially, indicating that the model suffers from error accumulation in this configuration. (3) w/o Transformer: adopting a convolutional architecture yields poor performance, highlighting that our transformer architecture is more effective. (4) w/o Transformer*: when applying a convolutional architecture with a sliding window inference strategy, the performance remains similarly poor, further validating the superiority of our transformer approach. (5) w/o RoPE: substituting RoPE with absolute positional encoding leads to very poor results. This is primarily because absolute positional embedding struggles to generalize well in long sequences. (6) w/o RoPE*: even when using absolute positional embedding with a sliding window inference strategy, the results are still worse than our approach, confirming the inadequacy of this embedding strategy. (7) w/o Post-Processing: Omitting the postprocessing step causes a significant increase in global metrics, demonstrating that our post-processing strategy substantially enhances global accuracy."}, {"title": "5 CONCLUSIONS", "content": "We introduce GVHMR, a novel approach for regressing world-grounded human motion from monocular videos. GVHMR defines a Gravity-View (GV) coordinate system to leverage gravity priors and constraints, avoiding error accumulation along the gravity axis. By incorporating a relative transformer with RoPE, GVHMR handles sequences of arbitrary length during inference, without the need for sliding-window. Extensive experiments demonstrate that GVHMR outperforms existing methods across various benchmarks, achieving state-of-the-art accuracy and motion plausibility in both camera-space and world-grounded metrics."}]}