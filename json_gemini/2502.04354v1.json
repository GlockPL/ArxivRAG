{"title": "Reviving The Classics: Active Reward Modeling in Large Language Model Alignment", "authors": ["Yunyi Shen", "Hao Sun", "Jean-Fran\u00e7ois Tons"], "abstract": "Building neural reward models from human preferences is a pivotal component in reinforcement learning from human feedback (RLHF) and large language model alignment research. Given the scarcity and high cost of human annotation, how to select the most informative pairs to annotate is an essential yet challenging open problem. In this work, we highlight the insight that an ideal comparison dataset for reward modeling should balance exploration of the representation space and make informative comparisons between pairs with moderate reward differences. Technically, challenges arise in quantifying the two objectives and efficiently prioritizing the comparisons to be annotated. To address this, we propose the Fisher information-based selection strategies, adapt theories from the classical experimental design literature, and apply them to the final linear layer of the deep neural network-based reward modeling tasks. Empirically, our method demonstrates remarkable performance, high computational efficiency, and stability compared to other selection methods from deep learning and classical statistical literature across multiple open-source LLMs and datasets. Further ablation studies reveal that incorporating cross-prompt comparisons in active reward modeling significantly enhances labeling efficiency, shedding light on the potential for improved annotation strategies in RLHF.", "sections": [{"title": "1 Introduction", "content": "The safe and successful deployment of Large Language Models (LLMs) across various application domains requires alignment with human values. Current research working on LLM alignment mainly focuses on reinforcement learning from human feedback (RLHF) which rely on preference-based annotations provided by human annotators. However, obtaining human feedback can be expensive, and the noisy and binary nature of such data often limits its information density, posing a challenge for effective reward modeling.\nActive learning, where the model queries most informative labels based on its current state, offers a potential solution. It typically involves three key components: an initial model, a query strategy often in the form of maximizing a scoring function over unlabeled data, and a pool of unlabeled data. The model selects a subset of data for labeling and retrains iteratively until a stopping criterion is met.\nIn this work, we study the problem of active data acquisition in reward modeling. Technically, we introduce various scoring rules inspired by both classical experimental design and recent deep learning-based advancements. We adapt those methods to the learning of Bradley-Terry (BT) reward models, which have been successfully applied in large-scale alignment practices and proven to be theoretically sound.\nWe benchmark 8 scoring algorithms using 2 datasets and 3 LLMs, ranging in size from 2B to 8B, and evaluate a wide range of active learning setups. Our results show that two classical experimental design methods applied to the final linear feature layer of deep models achieve state-of-the-art performance and strong stability across different setups, model architectures, and datasets.\nOur main contributions can be summarized as follows:\n\u2022 Formally, we characterize the problem of optimal preference label annotation using embedding space BT regression framework and establish connections between active learning and classical experimental design literature under the BT context.\n\u2022 Methodologically, we introduce a set of algorithms inspired by classical experimental design literature, adapt them for deep BT regression models, and develop an efficient gradient approximation for the associated combinatorial optimization challenge in large-scale alignment problems.\n\u2022 Empirically, we evaluate different methods for preference label annotation across diverse setups, datasets, and base models. Our results suggest that applying classical experimental design techniques to the final layer of a deep neural network yields strong performance and stability."}, {"title": "2 Background and setup", "content": "Reward modeling in alignment. Reinforcement learning is a key technique for aligning LLMs to ensure their safe and effective deployment. The most prevailing approach, RLHF, relies on reward models as a fundamental mechanism for quantifying content quality and scaling the reinforcement learning. During fine-tuning and deployment, reward models serve as proxies for human evaluators, assessing how well LLM outputs align with human intent. Despite significant progress, reward modeling remains challenging due to the scarcity and inaccuracy of annotations. Prior research has attempted to mitigate these challenges through different aspects when learning from a fixed set of annotations. While demonstrate that online annotations are more efficient in RLHF, the topic of online annotation prioritization strategy remain under-explored except for heuristic designs.\nBradley-Terry model for reward modeling. A canonical model used for reward modeling from binary preference data is the Bradley-Terry (BT) model, or more precisely, its regression variant. In the most general setting, which allows for cross-prompt comparisons, a human annotator is presented with two pairs of prompts and responses, (xi,1, yi,1) and (xi,2, yi,2). The annotator then provides a preference, hi = 1{(xi,1,yi,1)>(xi,2,yi,2)} indicating whether the first pair is preferred over the second.\nOften, both responses correspond to the same prompt, i.e., xi,1 = xi,2 however, Bradley-Terry regression can operate without this assumption. The model regresses these annotations onto an embedding \u03a8(xi,1, yi,1). This is a mild assumption since these embeddings can be,"}, {"title": null, "content": "for example, a concatenation of word embeddings, the output of tokenizers, or the output embedding of an LLM.\nWhen there is no risk of confusion, we denote the embeddings of pair i as \u03a8i,1 \u2208 RD and \u03a8i,2 \u2208 RD, with a reward function r \u2208 RD \u2192 R. The goal is to learn this function from annotations. In the BT model, we assume that\nhi \u223c Bernoulli (\u03c3[r(\u03a8i,1) \u2212 r(\u03a8i,2)])  (1)\nwith \u03c3 being the sigmoid function.\nActive learning. In a typical active learning setting, we have a labeled dataset, Ds = (xi,1, yi,1, xi,2, yi,2, hi)Ji=1 at step s, and a typically large pool of unlabeled data to be chosen from, Ps = {(xj,1, yj,1, xj,2, yj,2)}Jsj=1. The goal is to select a small subset Cs \u2282 Ps, subject to certain constraints, for labeling. Once labeled (denoted as Cs), this subset is added to the labeled dataset to train the next iteration of the model.\nWe also consider this process in the embedding space, where the labeled and unlabeled sets are given by Ds = {(\u03a8i,1, \u03a8i,2, hi)}Ji=1 and Ps = {(\u03a8j,1, \u03a8j,2)}Jsj=1. A typical model-based active learning procedure is outlined in algorithm 1. In this work, we focus on identifying the best-performing scoring rules.\nAlgorithm 1 Model-based active learning\nRequire: initial labeled dataset D0, pool set P0, model M0, a scoring rule S, budget constraint c, and number of rounds n\n1: RETURN Last trained model Mn\n2: for s \u2190 1 to n do\n3: generate pool Ps\n4: Cs \u2190 argmaxC\u2282Ps\u22121,|C|\u2264cS(Ms\u22121, C, Ds\u22121)\n5: get labeled dataset Cs\n6: Ds \u2190 Cs \u222a Ds\u22121\n7: train model Ms using Ds\n8: end for\n9: return Mn\nRelated work. considered active learning and proposed a strategy that combines entropy with model certainty (which is equivalent to the maxdiff strategy in our notation). For non-binary data, suggested maximizing the determinant of the feature matrix. BatchBALD is a general-purpose"}, {"title": "3 Designing of comparisons", "content": "active learning algorithm that requires a Bayesian model. The scoring in this method aims to maximize the expected entropy reduction by selecting the most informative data points. Experimental design for generalized linear models has been extensively studied in the classical statistical literature, with logistic regression serving as a key example. Under the assumption of a linear reward function, the Bradley-Terry (BT) model simplifies to logistic regression."}, {"title": "3.1 Linear BT Regression.", "content": "Consider a simplified case where the true reward function is linear with respect to some intermediate embedding, r(\u03a6i,1) = \u03a6Ti\u03b2\u22121, for weight vector \u03b21. We use \u03a6 instead of \u03a8 because the reward may not be linear with respect to the original embedding \u03a8 used in reward modeling, and we wish to avoid confusion. The subscript \u22121 in \u03b21 reflects how we will apply these results in practice: \u03a6 represents the output before the final linear layer, and B\u22121 corresponds to the weight of this last layer. For now, we assume that this linear feature \u03a6 is known to us. Note that there is no bias term because linear BT is identified only up to translation.\nUnder this simplified setting the preference generating process of ith pair hi can be simplified to\nhi \u223c Bernoulli [\u03c3[(\u03a6i,1 \u2212 \u03a6i,2)T\u03b2\u22121]] (2)\nIt can be observed that this corresponds to a logistic regression, where the covariates are the difference \u03a6i,1 \u2212 \u03a6i,2.\nBy applying the theory from generalized linear models, we know that the maximum likelihood estimate \u03b21 is asymptotically Gaussian distributed, with mean \u03b21 and covariance matrix I\u22121, where I denotes the Fisher information (FI) matrix. For the linear Bradley-Terry model, the FI is\nI = \u2211i=1(\u03a6i,1 \u2212 \u03a6i,2)(\u03a6i,1 \u2212 \u03a6i,2)Tpi(1 \u2212 pi) (3)\nWhere pi = \u03c3[(\u03a6i,1 \u2212 \u03a6i,2)T\u03b2\u22121], it can be observed that pi(1 \u2212 pi) represents the variance of a Bernoulli random variable."}, {"title": null, "content": "The Fisher information matrix can be interpreted as the metric tensor in a Riemannian manifold of distributions, where the distance between them is given by the symmetrized KL divergence. FI quantifies the amount of information in the dataset for estimating the parameters \u03b21. From a Bayesian perspective, the Bernstein-von Mises theorem states that I\u22121 is also the asymptotic covariance matrix of the posterior distribution of \u03b2\u22121, assuming mild regularity conditions on the prior.\nThe FI can be viewed as a sum over all independent data points' contribution. For each data point, there are two terms multiplied together: the empirical covariance of embedding differences (\u03a6i,1 \u2212 \u03a6i,2)(\u03a6i,1 \u2212 \u03a6i,2)T, and pi(1 \u2212 pi), the variance of the comparison results. suggested that improving the variance of comparisons can be interpreted as improving annotation quality which can also be seen from FI.\nTo make the FI large eq. (3) an ideal comparison should exhibit both a large variance in the embedding difference (thus (\u03a6i,1 \u2212 \u03a6i,2)(\u03a6i,1 \u2212 \u03a6i,2)T having large eigenvalues) and a high variance in the comparison outcomes (thus pi(1 \u2212 pi) large). This implies that the embedding space should be diverse, such that \u03a6i,1 \u2212 \u03a6i,2 captures a wide range of differences, and each comparison should be informative\u2014not too close to 0 or 1. The former encourages exploration within the embedding space, leading to a better regression model, while the latter ensures that comparisons are not trivial, improving sample efficiency. An everyday analogy for comparing non-obvious pairs would be that comparing a world champion to a newbie in chess offers little insight into the abilities of either player.\nThe FI plays a crucial role in the classical theory of experimental design, both in frequentist and Bayesian frameworks, as highlighted by the Bernstein-von Mises theorem. This leads to a family of design strategies known as alphabetical designs.\n(Bayesian) D-optimality. The alphabetical designs focus on the (co)variance of either estimating weights \u03b21 or making predictions under new embeddings, typically summarized through the covariance matrix. For example, the D-optimal design minimizes the determinant of the (asymptotic) covariance matrix of the last layer weights, B.1. Since |I\u22121| = 1/|I|, this is equivalent to maximizing the determinant of the FI.\nThe Bayesian variant of D-optimal involves having prior contribution, such as maximizing"}, {"title": null, "content": "|I+I/\u03c32|, where I is the identity matrix, to avoid a determinant of zero. This corresponds to the inverse covariance matrix of the Laplace approximation of the posterior of \u03b2\u22121, assuming a normal prior with variance \u03c32.\nA plug-in estimator of pi, p\u02c6i, using the current best model, can be used to estimate the FI. In this approach, the scoring rule is the determinant of the Fisher Information matrix.\nSdopt(C) = |\u2211i\u2208C(\u03a6i,1 \u2212 \u03a6i,2)(\u03a6Ti,1 \u2212 \u03a6i,2)pi(1 \u2212 pi)| (4)\nIn experiments, we refer to this strategy as D-opt. Other forms of optimality also exist, each targeting different summaries of the Fisher Information (FI), such as A-optimality, which focuses on minimizing the trace of I\u22121. When the prediction of a new, known embedding is the primary concern, G-optimality aims to minimize the variance of predictions on new embeddings.\nIn this work, we suggest using D-optimality because it avoids the need to invert the FI, as required in A-optimality, and doesn't require specifying which samples to predict, as in G-optimality. For readers interested in further details, we refer to. (Ch. 9).\nThe D-optimality strategy can be made a past-aware version by incorporating previously collected data. The asymptotic covariance of the full data-conditioned posterior is then (Ipast + I)\u22121, where Ipast is computed using prior data and eq. (3). This approach relates to Bayesian methods like Bayesian active learning by disagreement (BALD), which minimizes posterior entropy. Since Gaussian entropy is proportional to the log-determinant of its covariance. In our experiments, we refer to this variant as PA D-opt.\nNext, we review some other strategies that can be applied to BT models.\nEntropy sampling. This strategy aims to select samples about which the current model is most uncertain. In the context of binary preference modeling, this corresponds to choosing data whose predictions p\u02c6i are closest to 0.5, effectively exploring the level set of the reward. This is similar to a binary classification problem where the goal is to explore the decision boundary. This approach was also proposed by as maximizing predictive entropy. The scoring rule"}, {"title": null, "content": "is then,\nSentropy(C) = \u2211i\u2208C[\u2212p\u02c6i log(p\u02c6i) \u2212 (1 \u2212 p\u02c6i) log(1 \u2212 p\u02c6i)] (5)\nSince the entropy of a Bernoulli distribution reaches its maximum when p = 0.5, this ap- proach is equivalent to selecting the top c pairs where the predicted probability is closest to 0.5. In our experiments, we refer to this method as Entropy.\nMaximum difference. Contrasting with entropy sampling, this strategy focuses on comparing samples that the current reward model predicts to be the best and the worst, corresponding to probabilities close to 0 or 1. This approach was used by to measure model certainties. The scoring rule to be maximized can thus be interpreted as difference in estimated reward |ri,1 \u2212 ri,2|.\nSmaxdiff(C) = \u2211i\u2208C|ri,1 \u2212 ri,2| (6)\nThis strategy encourages exploration in the reward space rather than the embedding space. It is sometimes used in active learning when the goal is to identify positive examples rather than the best classification. This justifies its use in reward modeling, where the goal is to obtain responses that yield better rewards in downstream tasks. In our experiments, we refer to this method as Maxdiff.\nOptimizing design matrix. This strategy focuses on find- ing the best collection of embeddings, or the design matrix in statistics terms \u03a6i,1 \u2212 \u03a6i,2, without looking at model predictions. A common objective is to optimize the covariance matrix of the designs, \u03a3 = \u2211Ci=1(\u03a6i,1 \u2212 \u03a6i,2)(\u03a6Ti,1 \u2212 \u03a6i,2). One approach is to maximize the determinant of \u03a3, |\u03a3|, which encourages exploration over a large space of embedding differences. In fact, if we assume a linear regression model with additive Gaussian noise instead of logistic regression, this covariance matrix corresponds to the Fisher Information matrix of the regression coefficients, and this strategy aligns with the D-optimal design. The scoring rule is\nSxtx(C) = |\u2211i\u2208C(\u03a6i,1 \u2212 \u03a6i,2)(\u03a6Ti,1 \u2212 \u03a6i,2)| (7)\nused a similar strategy for a different type of preference data that is not purely binary. In our experiments, we refer to this method as det(XtX), for the determinant of XTX."}, {"title": null, "content": "Coreset. Instead of minimizing uncer- tainty in parameter estimations, the Coreset strategy aims to find a small subset of samples such that the trained model closely approximates the one trained on the full dataset, effec- tively transforming the problem into a sparse approximation task on weighting data points. The Coreset method for logistic regression has been studied recently by and in both frequentist and Bayesian settings. In our exper- iment, we adopted the method of . The scoring rule does not have a simple closed-form solution, so we refer interested readers to and denote it as Scoreset. In our experiments, we refer to this method as Coreset.\nBALD and batchBALD. When transition- ing from frequentist to Bayesian framework, BALD and BatchBALD select data with high mutual information between the candidate batch's prediction and model parameters, making the data more informative. showed that this approach maximizes expected posterior entropy reduction. This strategy applies to preference learning but requires a Bayesian model. We de- note the corresponding scoring rule as SBALD. In our experiments, we refer to this method as BatchBald. We used implementation in batchbald_redux.\nThis strategy relates to Bayesian D-optimality; when posterior entropy is tractable, it can be minimized directly instead of relying on approximations from . If the posterior is Gaussian, entropy is proportional to the log-determinant of its covariance, leading to D-optimality."}, {"title": "3.2 Gradient Approximation for Combinatorial Optimization.", "content": "In some strategies, we select a data subset to maximize information criteria like the determi- nant of FI or the design matrix. These often lead to intractable combinatorial optimization problems. To address this, we use the sensitivity approach from the coreset and robustness literature. When the informa- tion criteria are expressed as a nonlinear function over sum of data point contributions, i.e., S = f(\u2211i ci), where each data point contributes ci, we introduce weights wi, allowing the score to be rewritten as S(w) = f(\u2211i wici). For instance, the D-optimal score expresses the determinant of FI of a subset C as a weighted sum.\nSdopt(w) = |\u2211i wi(\u03a6i,1 \u2212 \u03a6i,2)(\u03a6Ti,1 \u2212 \u03a6i,2)pi(1 \u2212 pi)| (8)\nEach candidate pair is assigned a weight wi = 1i\u2208C. Selecting a subset C to maximize Sdopt is equivalent to finding a sparse 0-1 weight vector w that maximizes Sdopt(w).\nTo approximate the optimization, we treat w as continuous and perform a Taylor expansion around w = 1, the all 1 vector, i.e., all data points are included.\nS(w) \u2248 S(1) \u2212 (1 \u2212 w)T\u2207wS(w)|w=1 (9)\nThe approximated optimization problem becomes\nargmax S(w) \u2248 argmax wT\u2207wS(w)|w=1 (10)\nA sparse 0-1 valued vector w that optimizes the right-hand side of eq. (9) can be obtained by selecting the data points with the largest gradient, \u2207wS(w)|w=1. A probabilistic ap- proach, when all gradients are positive, involves sampling according to the weights given by \u2207wS(w)|w=1"}, {"title": "3.3 Handling nonlinear model using last layer features.", "content": "For nonlinear reward models in eq. (2), the dependencies on embeddings become more com- plex. Strategies like maximum difference and entropy sampling, which depend only on model predictions, remain unaffected by the architecture, while batchBALD is designed for (Bayesian) deep models. Feature-based methods like coreset or D-optimal need adaptation. A heuristic from the Bayesian last layer and computer vision literature suggests using the last layer before the linear output as a feature, applying linear strategies to it.\nr(\u03a8) = F\u03b8(\u03a8)\u03b2.1 (11)\nFor some nonlinear function F\u03b8 parameterized by \u03b8, e.g., an MLP and \u03a6 := F\u03b8(\u03a8). We apply methods in linear settings with features F\u03b8(\u03a8). We then train \u03b8 and \u03b2.1 together once data are labeled. In particular, in the nonlinear function F\u03b8 is a CNN and they took a coreset approach. Here we apply this strategy to the coreset, optimal design matrix and D-optimal setting."}, {"title": "4 Illustrative Examples in Dimension Two", "content": "Experiment Setups In this experiment, we provide a two-dimensional example of the com- parisons made by each strategy. The ground truth reward was defined as the log probability of a mixture of two Gaussians, centered at (-2.5, -2.5) and (2.5,2.5) with a variance of 0.25. Preference data was simulated using the BT model, and we attempted to learn the reward function with a 3-layer MLP with 16 hidden units. For each round, 1000 points were sampled from a standard normal distribution, and 200 comparisons were selected using different strategies. 4 rounds are shown in fig. 1.\nWhat were compared in dimension two? We observed that D-optimal selects diverse samples with many anchoring points, often comparing multiple points to a single one, spread- ing out the level set in the original space. Entropy sampling, similar to random sampling, focuses on points near reward values, effectively traversing the reward function's level set. Coreset also selects diverse comparisons, though not always among points with similar reward values. The best design matrix method behaves similarly to coreset, emphasizing diversity in comparisons. In contrast, the max difference method tends to compare extreme values with many others, promoting exploration but potentially yielding less informative compar- isons. BatchBALD also selects diverse comparisons, though without a clear pattern. These"}, {"title": "5 Experiments with LLMs", "content": "5.1 Overall Setup\nIn this section, we test different design strategies in LLM applications. We start with dis- cussions of general experiment setups and the evaluation metrics we used in experiments.\nObjective and Evaluation Metrics. We assess the data efficiency of various comparison selection methods. The main metrics are 1- Spearman's rank correlation and best-of-N test- time reward, as reward modeling aims to order responses correctly and select the best one during test time. The golden reward models from serve as the surrogate for ground truth. Specifically, we consider\n\u2022 Batched Spearman's correlations: we measure the ranking correlation within each test prompt across 500 generations. We took 1- Spearman's correlations as a test set metric.\n\u2022 Best-of-N Reward: we evaluate the best-of-N (N=500) reward on test prompts.\nA method is considered superior if it achieves a smaller 1- Spearman's correlation, a larger"}, {"title": null, "content": "Best-of-N reward, or the same performance with fewer annotations.\nBase Models, Annotations, and Golden Reward Models. We conducted experiments using three open-source LLMs: Gemma2b, Gemma7b, and LLaMA3-8b. To ensure affordability, we followed methods from to use open-source golden reward models as annotators. We used the Anthropic Harmless and Helpful datasets that has been widely studied in reward modeling, and golden reward models are available. The dataset includes 40k prompts with 10 responses each for training, and 2k prompts with 500 generations each for testing.\nReward modeling. To separate representation learning from reward modeling, we train our reward model using joint embeddings of prompts and responses. An MLP with three hidden layers and BT loss was used. Since the BT model is not identified up to a translation, we exclude bias in the final linear layer. Our ablation studies show that the size of hidden units does not significantly affect the results. For more details, see appendix A.4.\nOnline Annotation Pipeline. We train our model sequentially, increasing the sample size at each step. At the beginning of each step, we randomly draw 500 prompts. For each of the 500 prompts, we randomly select 2 out of 10 responses for in-prompt comparisons, yielding 500 \u00d7 45 = 22500 potential comparisons. For cross-prompt annotations, there are approximately 25 million potential comparisons. We randomly sample a fix-sized subset 20000 out of those potential comparisons for different algorithms to choose from, see fig. 2.\nAt each online iteration, strategies that require model predictions use the reward model from the previous iteration. We test different annotation batch sizes, an important hyperpa- rameter to tune, ranging from 125, 250, 500, 1000 to understand performance across various settings. After annotation, we retrain the entire model and evaluate it after each re-training."}, {"title": "5.2 Comparing Annotation Efficiency", "content": "Figure 3 presents results on the Harmless dataset (see Appendix A.1, Figure 6 for Helpful results). D-opt and Past-Aware D-opt outperform other methods, demonstrating both superior performance and greater stability. In contrast, alternative approaches exhibit train- ing instability and significantly higher variance during online learning."}, {"title": "5.3 Comparing Annotation Batch Sizes", "content": "In this section, we evaluate different methods under varying annotation batch size setups, ranging from 125 to 1000. Notably, our proposed methods are computationally efficient: since"}, {"title": null, "content": "the reward model operates on embeddings, re-training a 3-layer MLP with 10k annotations takes only a few minutes on a GPU server\u2014while human annotation is significantly more time-consuming.\nFigure 4 presents results for the Gemma2B model (results for the other two base models are in Appendix A.2 due to space constraints). Overall, D-opt and Past-Aware D-opt consistently outperform other methods across different annotation batch sizes. Additionally, we observe performance improvements when using smaller batch sizes, corresponding to a more online setup. Given the low computational cost, this suggests a practical strategy: using small annotation batches with frequent model retraining to enhance annotation efficiency in reward model development."}, {"title": "5.4 Results with Cross-Prompt Annotations", "content": "Cross-prompt annotation has been proposed as an alternative to in-prompt comparison, demonstrating superior performance in LLM alignment tasks. To assess the generality of our methods, we extend our exper- iments to cross-prompt setups and compare different approaches.\nFigure 5 shows the results under cross-prompt annotation. D-opt and Past-Aware D-opt"}, {"title": null, "content": "achieve significantly better performance in both annotation efficiency and alignment across tasks and base models.\nComparing Figure 5 with Figure 3, we observe efficiency gains across all methods, with the entropy-based approach exhibiting the most substantial improvement. Appendix A.3 provides a direct comparison between in-prompt and cross-prompt annotations for interested readers."}, {"title": "5.5 Hyper-Parameter Sensitivity Analysis", "content": "To examine the sensitivity of different methods to hyper-parameter choices and provide insights for real-world applications, we varied two key factors: Candidate Number and Hidden Dimension of Reward Model MLPs across different active reward modeling designs.\nOur sensitivity analysis reveals that all algorithms remain robust and are largely insensitive to specific hyper-parameter choices in our embeddings-as-inputs setup. Detailed results are provided in Appendix A.4."}, {"title": "6 Discussion", "content": "Designing comparisons. Our experiments show that applying the classic method to the last-layer features yields strong performance and stability. The D-opt method is also highly efficient, as its information criteria and optimization procedure are largely analytical, en- abling real-time pair selection. This is valuable when collecting user preferences in a user in- terface without introducing significant latency. Additionally, this approach might be adapted to other model architectures, including e.g., vision-language models.\nAn Empirical Bayes View and Stability of Last-Layer Design. The connection be- tween the last-layer D-optimal method and BALD can be seen by considering previous layers as a transformation of the Gaussian prior for the last layer's weights. These previous layer weights act as hyperparameters of the prior, which are fitted using maximum likelihood, akin to an empirical Bayes procedure. By minimizing posterior entropy, we perform D-optimal design followed by Gaussian approximation after the trans- formation. Empirical Bayes helps reduce the subjectivity and uncertainty in selecting priors, potentially explaining the robustness of our method compared to full Bayesian approaches"}, {"title": null, "content": "like batchBALD, which involve hyper-priors on these hyperparameters.\nClassic Experimental Design Methods in the Foundation Model Era. We conjec- ture that the success of using the last layer in classical experimental design stems from the fact that the embeddings are already close to linear features. Given the extensive study of experimental design in generalized linear models we believe it is a general strategy to apply these methods to the last layer of deep learning models, particularly when leveraging learned representations from foundation mod- els.\nLimitations and future directions. Our proposed active learning scheme relies on a well-trained embedding model. The effectiveness of selecting comparisons based on last- layer features depends on these features being informative, which might in turn requires signal-rich embeddings with low noise as input of the reward model (which is MLP in our experiment). An interesting question is whether embeddings that better capture human values (and thus improve reward modeling) differ fundamentally from those optimized for generation. A related consideration is whether reward modeling in LLMs should start from embedding or ealier."}, {"title": "Impact Statement", "content": "Our work advances the efficiency of aligning LLMs with human values by optimizing the way human preferences are queried. Since human feedback is costly and time-consuming, our approach can potentially reduce wasted effort on uninformative comparisons, maximizing the value of each annotation. By improving the efficiency of learning from human preferences, this research has the potential to accelerate the development of safer and more helpful AI systems."}, {"title": "B Further discussions", "content": "B.1 Cross-Prompt Annotations.\nCross-Prompt annotations were explored as a way to increase annotation quality by Sun et al. (2024) and empirically verified in Yin et al. (2024). A natural question is whether this is possible in practice. If one is willing to assume there exists a scalar value reward function, and human comparisons are based on that function, then cross-prompt is possible because each prompt-response pairs are assigned a real value that are comparable to each other. A single-word change in the prompt without changing its meaning likely will not change what responses are helpful or harmful and make these pairs, even if cross-prompt, comparable. It is possible however, that the reward function is very rough in changing prompts making the reward function for one prompt not transferable to the other and hard to get a better response for one prompt using a reward function learned from other prompts. Even though, if one is willing to believe that prompts live in some lower dimensional manifold and the reward function acquires some regularity in that space, Cross-Prompt annotations might help better learn these dependencies."}]}