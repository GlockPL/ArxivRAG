{"title": "FlashVTG: Feature Layering and Adaptive Score Handling Network for Video Temporal Grounding", "authors": ["Zhuo Cao", "Bingqing Zhang", "Heming Du", "Xin Yu", "Xue Li", "Sen Wang"], "abstract": "Text-guided Video Temporal Grounding (VTG) aims to localize relevant segments in untrimmed videos based on textual descriptions, encompassing two subtasks: Moment Retrieval (MR) and Highlight Detection (HD). Although previous typical methods have achieved commendable re- sults, it is still challenging to retrieve short video moments. This is primarily due to the reliance on sparse and limited decoder queries, which significantly constrain the accu- racy of predictions. Furthermore, suboptimal outcomes of- ten arise because previous methods rank predictions based on isolated predictions, neglecting the broader video con- text. To tackle these issues, we introduce FlashVTG, a framework featuring a Temporal Feature Layering (TFL) module and an Adaptive Score Refinement (ASR) module. The TFL module replaces the traditional decoder structure to capture nuanced video content variations across multi- ple temporal scales, while the ASR module improves pre- diction ranking by integrating context from adjacent mo- ments and multi-temporal-scale features. Extensive exper- iments demonstrate that FlashVTG achieves state-of-the- art performance on four widely adopted datasets in both MR and HD. Specifically, on the QVHighlights dataset, it boosts mAP by 5.8% for MR and 3.3% for HD. For short- moment retrieval, FlashVTG increases mAP to 125% of pre- vious SOTA performance. All these improvements are made without adding training burdens, underscoring its effective- ness. Our code is available at https://github.com/Zhuo- Cao/FlashVTG.", "sections": [{"title": "1. Introduction", "content": "The increasing prevalence of video content across var- ious platforms has amplified the need for advanced video analysis techniques, particularly in the context of Video Temporal Grounding (VTG). The task of VTG involves ac- curately identifying specific video segments that correspond to given natural language descriptions, a capability that is crucial for applications such as Moment Retrieval (MR), event detection, and Highlight Detection (HD). Addressing these challenges is critical as it directly impacts the perfor- mance and usability of systems that rely on video under- standing. For instance, the ability to precisely localize and retrieve short yet significant moments within videos can en- hance user experiences in applications ranging from video editing to automated video surveillance. Moreover, improv- ing the ranking of the predictions ensures that the first re- trieved moment is more accurate and contextually relevant, thereby reducing errors in downstream tasks.\nDespite advancements in video temporal grounding, cur- rent methods remain limited, particularly in short mo- ment retrieval in complex, densely packed video sequences. DETR [3]-based models [16, 33, 42], though effective, un- derperform in short moment retrieval due to their reliance on sparse and limited decoder queries, which tend to over- look short moments. However, simply increasing the num- ber of queries significantly increases the computational complexity of the methods. Moreover, relying solely on isolated predicted moments for comparison and ranking can lead to suboptimal results, especially when fine-grained dis- tinctions are required.\nIn this paper, we introduce a Temporal Feature layer- ing architecture (FlashVTG) to solve VTG tasks, including MR and HD. We first develop a Temporal Feature Layering module to extract and integrate video features across mul- tiple temporal scales, enabling a more nuanced and com- prehensive representation of video content. Subsequently, we introduce an Adaptive Score Refinement Module to rank predicted moments, enhancing the confidence scores of mo- ments by integrating context from adjacent moments and multi-temporal-scale features. Together, these components enhance the ability to accurately predict moments of vary- ing durations, particularly short moments (see Fig 1), which are often problematic for previous methods.\nSignificant performance discrepancies are observed in DETR-based methods [15, 32], with mAP scores around 50% for moments longer than 10 seconds and a drastic drop to approximately 10% for shorter moments. These findings motivate us to develop a Temporal Feature Layering module for moment localization across various moment durations. We discard the conventional decoder structure and instead utilizes the Temporal Feature Layering module. By doing so, it addresses the inherent issue of sparse decoder queries in DETR-based methods, significantly improving the ac- curacy of moment retrieval without introducing additional training complexity.\nFor each predicted moment, a corresponding confidence score helps the model select the best prediction. However, we found that previous methods typically generate the con- fidence score only based on the current predicted moment, making it difficult to distinguish between closely adjacent moments when the video content is very similar. There- fore, we design the Adaptive Score Refinement Module. It enhances the model's ability to generate accurate confi- dence scores by leveraging both intra-scale and inter-scale features. This adaptive mechanism evaluates predicted mo- ments not only across different feature scales but also within adjacent predicted moments on the same scale. Addition- ally, to further enhance accuracy for short moments, we introduced a novel Clip-Aware Score Loss, which applies labels from the Highlight Detection task to the Moment Re- trieval task, offering fine-grained supervision that was pre- viously unexplored.\nTo validate the effectiveness of FlashVTG, we con- ducted extensive experiments on widely adopted VTG benchmarks. Integrating FlashVTG with InternVideo2 [48], SlowFast [6], and CLIP [36] yield remarkable results for moment retrieval and highlight detection.\nThese results not only underscore the robustness of FlashVTG but also demonstrate its superiority over state- of-the-art methods, with performance improvements of 2.7%, 2.2%, and 11.9% respectively in MR, positioning FlashVTG as a leading approach in VTG tasks. Our con- tributions are as follows:\n\u2022 We propose FlashVTG, a novel architecture that sig- nificantly enhances Video Temporal Grounding by em- ploying a strategic integration of Temporal Feature Layering and Adaptive Score Refinement.\n\u2022 We design a Temporal Feature Layering (TFL) Module that replaces the traditional decoder. TFL is designed to overcome sparse query limitations and improve re- trieval without additional training.\n\u2022 We introduce an Adaptive Score Refinement (ASR) Module that selects predicted moments using both intra-scale and inter-scale features, enhancing first- moment accuracy and highlight detection."}, {"title": "2. Related Work", "content": "Video Temporal Grounding (VTG). In the field of text- guided Video Temporal Grounding [15, 16, 20], the objec- tive is to identify specific temporal segments within a video based on given natural language descriptions. Essentially, this task requires models to discern and quantify the associ- ations between the content of the video and natural language descriptions across different time stamps. Subsequently, we will provide a detailed exposition of two specific VTG tasks: moment retrieval and highlight detection.\nMoment Retrieval (MR). The goal of this task is to identify the start and end points of a video segment based on a natural language query. Given the limitations of cur- rent datasets, it is typically assumed that a single ground truth (GT) segment exists. If multiple GT segments are available, the one with the highest Intersection Over Union (IOU) with the predicted moment is selected as the GT. Existing MR methods primarily fall into two categories:"}, {"title": "3. Methodology", "content": "3.1. Problem Formulation\nIn Video Temporal Grounding (VTG) tasks, we repre- sent the input video and the extracted features as Vand V = {vi}1, respectively, where each vi \u2208 RDv. Here, L denotes the number of video clips and Dv represents the feature dimension of each clip. For the natural language query Q, we represent it as a set Q = {qi}1Lq, where each qi \u2208 RDq is the i-th word token in the query. Here, Lq de-"}, {"title": "3.2. Overall Framework", "content": "As illustrated in Figure 2, the input video and query are initially encoded into video and query features using frozen Feature Encoders. In addition to extracting the original video and text features, FlashVTG incorporates a dummy token to supplement semantic information beyond the query, enhancing alignment between the video and text.\nFollowing the encoding process, the video and query features are integrated within the Feature Fusion Module, enabling precise alignment of the two modalities. The fused features are subsequently processed in two separate streams. For Highlight Detection, the HD head transforms the fused features into saliency scores, while the Temporal Feature Layering Module expands the features across mul- tiple granularities.\nThe Adaptive Score Refinement Module then evalu- ates these multi-scale features to generate a confidence Concurrently, the Moment Retrieval predic- score, s. tion head outputs predictions across different scales, pro- ducing a series of tuples (bs, be). Finally, the pre- dicted moments and confidence scores are combined into (bs,i, be,i, Ci) | i = 1, 2, . . ., n, which specify the predicted start and end points of moments, along with their associated confidence scores."}, {"title": "3.3. Feature Extraction and Fusion Module", "content": "Feature Extraction. Consistent with previous method- ologies, we employ the CLIP [36] image encoder and Slow- Fast [6] to extract clip-level video features V, and use the CLIP text encoder along with GloVe [35] to derive word- level text features Q. Specifically, raw videos are segmented into clips at predetermined FPS, such as 0.5 or 1, using frozen pretrained models as feature extractors. This process transforms each clip into a distinct video feature vi. Simi- larly, each word in the query is encoded into corresponding text features qi.\nFeature Fusion. To align video and query features for subsequent processing, we project them into the same di- mensional space d using two MLPs before fusion. We adopt the Adaptive Cross Attention (ACA) module from CG-DETR [32], which extends the traditional Cross At- tention by incorporating the ability to supplement query information. A known limitation of standard Cross At- tention, particularly with the softmax operation, is that it may not perfectly align all video clips with the query, as the query cannot fully encapsulate the semantic scope of an untrimmed video. In ACA module, we use learnable dummy tokens, D = [D1, ..., DLd], where Ld is a hyperpa- rameter. These tokens capture semantic information beyond the original query, complementing its content. The encoded dummy tokens D are concatenated with the query features Qto form the Key, with the video features V as the Query and the query features as the Value, as shown in Equa- tion (1).\nQuery = [PQ(v1),...,PQ(vLv)]\nKey = [PK(q1),...,PK(qLq),PK(\u010e1),\u2026\u2026\u2026,PK(DLd)]\nValue = [pV (q1),...,pV (qLq)]\n(1)\nHere, PQ(\u00b7), PK(\u00b7), and pv(\u00b7) denote the projection functions used to transform inputs into the Query, Key, and Value formats, respectively. These Query, Key, Value are input into the ACA module to obtain the fused features F\u2208 RLv\u00d7d, as shown in Equations (2) and (3).\nF = ACA(vi) = \u2211 Wij Vj;\nj=1\nWij = exp \nQiKj\n\u221ad\n\u2211 exp (QiKj)\nLq+Ld\nk=1\n\u221ad\n(2)\n(3)\nwhere F denotes the fused feature and stands for the dot product. While Q, K, and V represent the Query, Key, and Value, respectively. Through this adaptive cross-attention mechanism, the model more effectively aligns the query with the relevant video segments.\nFollowing the ACA module, the fused features F are passed through a Transformer Encoder to further refine the multi-modal representations. This additional processing step enables the model to enhance the interaction between the video and query features, allowing for a more compre- hensive understanding of the temporal and semantic rela- tionships across the entire sequence.\nHighlight Detection Head. The refined features output from the Transformer Encoder are further processed to gen- erate clip-level saliency scores s for highlight detection. We aggregate the fused features to form a global contextual rep- resentation, which is then combined with the original fused"}, {"title": "3.4. Temporal Feature Layering Module", "content": "Temporal Feature Layering. This module transforms the fused features into a feature pyramid to enhance the model's capability to process features at various granular- ities. The feature pyramid is a commonly used structure in computer vision for extracting and utilizing vision fea- tures across multiple scales [9, 47]. This architecture em- ulates the hierarchical processing of the human visual sys- tem, while enabling the network to effectively perceive and process video information of varying lengths.\nBuilding on this insight, we incorporate layered fea- ture implementations into DETR-based models to over- come their limitations in short moment retrieval. This mod- ule separates moments of varying lengths, enabling the sub- sequent ASR module (Sec. 3.5) to provide more detailed supervision for short moments prediction. Specifically, we apply 1D convolution operations with various strides to the fused features F, constructing a feature pyramid composed of features at different granularities. Predictions of moment locations are then made at these various granularities using a uniform prediction head.\nLet F \u2208 RLv\u00d7d represent the input fused feature, where Lv is the length of the features and d is the dimensionality of the features. The process of temporal feature layering can be represented as:\nF, if k = 1,\nFk =\nConv Conv1Dk\u22121(F, stride = 2), if k = 2, 3, . . ., K.\nBy applying the temporal feature layering, we obtain a sets of features at different granularities, which can be rep- resented as:\nLu\nFk \u2208 R21\u00d7d, k = 1, 2, ..., K.\nHere, {Fk|k = 1,2,..., K} represent the feature pyra- mid obtained from the original fused feature F through mul- tiple convolution operations. This multi-scale processing approach enables the model to capture and process infor- mation at different temporal resolutions, thereby adapting to scene changes at various scales. As referenced in Sec. 4.5, this module significantly improves the model's ability to re- trieve moments of varying lengths. This operation paves the way for enhanced supervision of short moment retrieval in subsequent steps.\nMoment Prediction Head. This module primarily ad- justs and reduces the feature dimension to 2, and through a series of transformations, it yields the predicted start and end points of moments. The specific process can be ex- pressed as:\nBkT = (\u03c3 (Conv1D (\u03c3 (Conv1D(F)))))\u00d7Ck.\n(5)\nLv\nHere, Bk \u2208 R 2\u00d72 represents a set of boundaries at scale k, \u03c3(\u00b7) denotes the ReLU activation function. The term Ck refers to a learnable parameter corresponding to each scale, which is used to adjust the influence of different scales on boundary prediction."}, {"title": "3.5. Adaptive Score Refinement Module", "content": "Adaptive Score Refinemen Module used to assign a con- fidence score c\u2208 [0,1] to each predicted moment in MR. This score indicates the extent to which the given query is relevant to the predicted boundary. Compared with the pre- vious method of generating scores on a single scale feature, we leverage both intra-scale and inter-scale scores to im- prove the final predictions.\nFor each level of the feature pyramid Fk, intra-scale scores are first generated through a score head, produc- ing outputs corresponding to the varying dimensions of the pyramid levels. These outputs are then concatenated into a unified tensor along the length dimension. These steps are shown in Equation (6) and Equation (7), respectively.\nCk = ScoreHead\u2081(Fk) \u2208 R21\u00d7\u00b9, k = 1, 2, ..., K.\n(6)\nCintra = Concat(C1, C2, ..., CK).\n(7)\nOur score head utilizes a 2D convolutional network with a kernel size of 1 \u00d7 5, which is effectively equivalent to a 1D convolution. Simultaneously, as shown in Equation (8), inter-scale scores are computed by concatenating the fea- tures from all pyramid levels and passing them through an- other score head, resulting in a tensor that matches the di- mensions of the intra-scale output.\nCinter = ScoreHead2(Concat(F1, F2,..., FK)).\n(8)\nThe final prediction is obtained by a weighted combina- tion of the intra-scale and inter-scale scores:\nCfinal = x. Cintra + (1 \u2212 x) \u00b7 Cinter.\n(9)\nHere, the learnable weighting factor x enables adaptive adjustment between Cintra and Cinter, thereby resulting in a more comprehensive score prediction."}, {"title": "3.6. Training Objectives", "content": "In FlashVTG, we employ a series of loss functions to ensure the model converges towards the desired objectives. For MR, we use Focal Loss [21], L1 Loss, and Clip- Aware Score Loss to respectively optimize the classifica- tion labels, boundaries, and clip-level confidence scores of the predicted moments. For HD, we utilize SampledNCE Loss [29] and Saliency Loss to optimize the saliency scores for each clip. The overall loss can be expressed as:\nLoverall = RegLL1 + AClsLFocal + ACASLCAS\n+ ASNEC LSNCE + Sal LSal,\nwhere each A represents the corresponding weight for each loss component. Due to the space limitation, we will focus on the Clip-Aware Score Loss, further details on the other loss functions can be found in the supplementary materials.\nClip-Aware Score Loss. This Loss is designed to align the predicted confidence scores with the target saliency la- bels. Given the predicted clip-wise moment confidence score Cfinal and the target saliency scores sgt, we first nor- malize both sets of scores using min-max normalization to get final and Sgt. The loss is then computed as the mean squared error between \u0109final and \u015dgt:\nLCAS = MSE(\u0108final, $gt).\n(10)\nThis loss encourages the model to produce confidence scores that not only match the target labels but also ad- here to the relative distribution of saliency across clip- level, thereby specifically enhancing the model's perfor- mance when predicting short moments."}, {"title": "4. Experiments", "content": "4.1. Datasets\nWe evaluated our model on five VTG task datasets, including QVHighlights, TACOS, Charades-STA, TVSum, and YouTube-HL.\nQVHighlights [16] is the most widely used dataset for MR and HD tasks, as it provides annotations for both tasks. This dataset marked the beginning of a trend where these two tasks are increasingly studied together. It includes more than 10,000 daily vlogs and news videos with text queries. Our main experiments were conducted on this dataset, and we provide comprehensive comparisons with other meth- ods on it. Charades-STA [7] and TACOS [37] were used to evaluate the model's performance on MR, containing daily"}, {"title": "4.2. Evaluation Metrics", "content": "We follow previous works [15, 16, 29] and adopt con- sistent evaluation metrics: R1@X, mAP, and mIoU for MR, and mAP and Hit@1 for HD. Specifically, R1@X stands for \"Recall 1 at X\", which refers to selecting the predicted moment with the highest confidence score and checking whether its IoU with any ground truth moment exceeds the threshold X. If it does, the prediction is con- sidered positive, and R@X is then calculated at thresholds \u03a7\u2208 {0.3,0.5,0.7}. For MR, mAP serves as the primary metric, representing the mean of the average precision (AP) across all queries at thresholds [0.5:0.05:0.95]. We also use mean Intersection over Union (mIoU) to evaluate the aver- age overlap between predicted moments and ground truth segments. For HD, mAP remains the key evaluation met- ric, with Hit@1 used to assess the hit ratio for the highest- scored clip."}, {"title": "4.3. Implementation Details", "content": "As in previous methods [16, 30, 33], we primarily used video and text features extracted by CLIP [36] and Slow- Fast [6] across all five datasets for a fair comparison. To further verify the generalizability of our model, we incorpo-"}, {"title": "4.4. Comparison Results", "content": "The comparison of MR experimental results is shown in Tables 1, 2, 3, 7, where FlashVTG achieved state-of-the- art (SOTA) performance on nearly all metrics. Table 1, 7 presents the MR results on the QVHighlights [16]. Table 1 demonstrates that combining FlashVTG with a newly se- lected backbone [48] resulted in a significant performance improvement, evidenced by a 5.8% increase in mAP on the test set. Even when using the same backbone, mAP im- proved by 1.4%. Table 7 shows that FlashVTG improved the mAP of short moment retrieval to 125% of the previ- ous SOTA method. These performance gains on one of the most widely used datasets demonstrate the effectiveness of FlashVTG compared to contemporaneous approaches.\nTables 2 and 3 present experimental results on two other MR datasets, TaCos and Charades-STA. Similarly, FlashVTG significantly outperformed previous methods on almost all metrics, achieving either SOTA or the second- highest performance. On Charades-STA, performance im- provements were observed across all three different back- bones, further validating the robustness of FlashVTG.\nThe comparison of HD experimental results is shown"}, {"title": "4.5. Ablation Study", "content": "We conducted an ablation study on the QVHighlights validation set to verify the effectiveness of FlashVTG. This dataset is currently the most widely used VTG benchmark and supports both MR and HD tasks, making it the most suitable for our research. We used the original single-layer feature and a method that generates confidence scores based solely on single predicted moments as our baseline.\nEffect of different components. As shown in Table 8, we studied the effects of the Temporal Feature Layering and Adaptive Score Refinement modules. It can be observed that adding the TFL module to the baseline improved per- formance in both the MR and HD tasks, with the mAP for MR increasing by nearly 6%. Building on this, we fur- ther incorporated the ASR module to refine the confidence scores of the predicted moments. The improvements in MR-mAP, R@0.5, and R@0.7 indicate that the overall pre- cision was maintained while enhancing Recall at 1, mean- ing the recall for the first predicted moment improved. Ad- ditionally, using the saliency score label from the HD task as a supervision signal indirectly boosted HD performance, with mAP increasing by 0.5% and Hit@1 improving by 1%."}, {"title": "5. Conclusion", "content": "This paper introduced FlashVTG, a novel architecture for Video Temporal Grounding tasks. The proposed Tem- poral Feature Layering and Adaptive Score Refinement modules improve the retrieval and selection of more ac- curate moment predictions across varying lengths. Exten- sive experiments on five VTG benchmarks demonstrate that FlashVTG consistently outperforms state-of-the-art meth- ods in both Moment Retrieval and Highlight Detection, achieving substantial improvements. These results validate the robustness and effectiveness of our approach, establish- ing FlashVTG as a leading solution for VTG tasks."}]}