{"title": "LEARNING TO ACHIEVE GOALS WITH BELIEF STATE TRANSFORMERS", "authors": ["Edward S. Hu", "Kwangjun Ahn", "Qinghua Liu", "Haoran Xu", "Manan Tomar", "Ada Langford", "Dinesh Jayaraman", "Alex Lamb", "John Langford"], "abstract": "We introduce the \u201cBelief State Transformer\u201d, a next-token predictor that takes both a prefix and suffix as inputs, with a novel objective of predicting both the next token for the prefix and the previous token for the suffix. The Belief State Transformer effectively learns to solve challenging problems that conventional forward-only transformers struggle with, in a domain-independent fashion. Key to this success is learning a compact belief state that captures all relevant information necessary for accurate predictions. Empirical ablations show that each component of the model is essential in difficult scenarios where standard Transformers fall short. For the task of story writing with known prefixes and suffixes, our approach outperforms the Fill-in-the-Middle method for reaching known goals and demonstrates improved performance even when the goals are unknown. Altogether, the Belief State Transformer enables more efficient goal-conditioned decoding, better test-time inference, and high-quality text representations on small scale problems.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformer models (Vaswani et al., 2017) have created a revolution in language modeling (Achiam et al., 2023) with the capability to generate language with many emergent properties at large scale. Examining these models for flaws in the pursuit of further progress, it's notable that they struggle with planning-heavy problems (Bubeck et al., 2023). How can we modify the architecture, objectives, and algorithms to create a model more capable of reaching goals?\nTo make progress, we propose the new Belief State Transformer architecture and objective in Section 2. Informally, a belief state is a sufficient amount of information from the past to predict the outcomes of all experiments in the future, which can be expressed as either a distribution over underlying world states or a distribution over future outcomes. The Belief State Transformer is similar to a standard decoder-only Transformer (e.g., GPT2), except that it has encodings that run both forward and backward. Both of these encodings are fed into output heads which predict not only the next token after the prefix but also the previous token before the suffix as shown in Figure 1.\nIn Section 3 we then study in depth how the Belief State Transformer performs on a known-hard problem, the star graph (Bachmann & Nagarajan, 2024) which is an elegantly simple sequential prediction problem known to confound next token prediction approaches. It's easy to show that transformers can represent star graph solutions using known results (e.g., (Sanford et al., 2024)), so the problem here is one of optimization. In particular, we discover that parity problems can be embedded within star graph problems, with parity known as difficult for gradient-based optimizers. Shockingly, despite throwing away the backward encoder for inference, the Belief State Transformer solves even relatively difficult instances of star graphs with experiments detailed in Section 3.4. Analyzing this discovery, the Belief State Transformer benefits from extra gradients, enabling avoidance of the parity-by-gradient problem systematically. We also show that data augmentation approaches and ablations of the Belief State Transformer cannot solve the star graph problem systematically.\nBuilding on this discovery, Section 4 proves this is a general phenomenon: ideal Belief State Transformers recover the full belief state in a compact representation for the output head. In contrast, a forward-only transformer and even modifications which predict every future token do not. This result implies that the Belief State Transformer learns maximal information from a sequence-there is no other objective/representation which pulls more relevant information into a compact belief state."}, {"title": "2 THE BELIEF STATE TRANSFORMER", "content": "This section introduces the Belief State Transformer. We start by introducing the architecture and the training objective then discuss how to utilize the model for inference.\n2.1 ARCHITECTURE AND OBJECTIVE\nLet 11:T be shorthand for the sequence x1,..., X. First, we set up the following networks:\nForward encoder F(x1:t) \u25b7 Encodes prefix\nBackward encoder B(Xt+k:T) \u25b7 Encodes suffix\nNext decoder $Xt+1 \\sim T_n(\\cdot | F(x_{1:t}), B(x_{t+k:T}))$ \u25b7 Predicts next token\n(1)\nPrev decoder $Xt+k-1 \\sim T_p(\\cdot | F(x_{1:t}), B(x_{t+k:T}))$ \u25b7 Predicts previous token\nThe forward encoder aggregates the prefix into a latent F(x1:t), and the backward encoder aggregates the suffix into a latent B (Xt+k:T). We use GPT2-style encoders throughout our experiments, including baselines. The output heads Tn and Tp then predict their respective tokens. In our experiments, the parameters of Tn and Tp are tied with only the last layer differing. See Figure 1 for an illustration.\nThe Belief State Transformer objective is the straightforward sum of the objectives of forward and backward Transformers conditioned on the prefix and suffix.\n$\\frac{1}{E_{t,k}:1\\leq t,k \\leq T-t} \\log T_n(x_{t+1} | F(x_{1:t}), B(x_{t+k:T})) + \\frac{1}{E_{t,k}} \\log T_p(x_{t+k-1} | F(x_{1:t}), B(x_{t+k:T})_$\n(2)\nAn obvious alternative (called \"Fill in the Middle\" (Bavarian et al., 2022)) when both a prefix and suffix are available is simply putting them together and then using a forward encoder. Information- theoretically, Fill in the Middle works, but we'll see that the Belief State Transformer has several advantages: it causes the system to coalesce a compact belief state which has many benefits explored here. This approach also extracts O(n\u00b2) gradients from sequences enabling a gradient based optimizer to solve new problems as discussed in the next section. Pseudocode for training is in Appendix D.\nTraining on all prefix-suffix pairs is surprisingly efficient. First, we cache all forward fo:T = {Vi \u2208 [0: T]: F(x1:1)} and backwards b1:T+1 = {\u2200i \u2208 [1,T + 1] : B(xi:T)} latents. Then the loss"}, {"title": "2.2 BELIEF STATE INFERENCE", "content": "During inference time, the forward model Tn(F(x1:t), B(\u00d8)) is given a prefix X1:t and an empty suffix \u00d8. The default inference technique is autoregressive sampling (ARS), where we sample the next token \u00ee from the next token decoder, add it to the prefix, and repeat. Note that since B(\u00d8) can be precomputed, this approach requires no more parameters at inference time than a standard forward-only Transformer. Later in Section 5, we study more complex inference approaches."}, {"title": "3 TESTING PLANNING ABILITIES WITH STAR GRAPHS", "content": "Bachmann & Nagarajan (2024) propose the star graph problem as an elegantly simple yet challenging task for forward Transformers to solve. In Figure 2, we reproduce their results while adding a data augmentation baseline and the new Belief State Transformer results. Notably, the Belief State Transformer performs exceptionally well without relying on domain-specific adaptations. In the following sections, we explain the star graph problem, present a new theory to account for these results, provide a detailed discussion of our experiments, and ablate key design choices.\n3.1 THE STAR GRAPH PROBLEM\nA star graph (depicted in Figure 3) G(d, l) is a graph with d paths of length I emanating out from the start node. To construct a graph, nodes ni are sampled uniformly from {1, ..., N}. A training example is formatted as a sequence containing the edge list &, the start and end nodes, and a path of length I from start to end: [En\u2081, \u03b9 | N1, N2, N3, ...]. Despite its simplicity, modern next token prediction models fail to solve it.\nThis task captures a core challenge found in practical planning tasks like story writing, where creating a coherent narrative requires the author to keep the story's resolution and backstory in mind while progressing through each plot point.\n3.2 WHY DO FORWARD-ONLY APPROACHES FAIL?\nAs shown by (Bachmann & Nagarajan, 2024, Appendix F.2) through extensive experiments, next- token predictors quickly learn a \"flawed cheat\" strategy: soon after training begins, forward-only transformers learn to arbitrarily select a neighbor of the current node since, aside from the start node,"}, {"title": "3.3 HOW DOES THE BELIEF STATE TRANSFORMER SUCCEED?", "content": "The Belief State Transformer is trained to predict the previous token for every suffix, which prevents the problem from collapsing into the parity problem described in Theorem 1. At a high level, for our approach to reduce to the parity problem, a large number of gradients must approach zero. However, the Belief State Transformer ensures that the information necessary to construct path suffixes (e.g., n2:1) is present in the input to the output head. From this information, predicting n2 is straightforward, allowing the model to solve the problem effectively.\nThis is an instance of a more general phenomenon: the Belief State Transformer naturally converges toward extracting a compact, complete belief state as discussed further in Section 4.1."}, {"title": "3.4 EXPERIMENTAL RESULTS ON STAR GRAPH", "content": "Here, we provide details on the Stargraph results in Figure 2. We run experiments on three types of graphs: G(2,5), G(5,5), G(2, 20). In each experiment, we choose one graph topology and generate many example graph sequences, with all methods receiving the same amount of data and every baseline receiving at least as much computation as the Belief State Transformer uses. For evaluation, the models are conditioned on the edge list, start, goal, and current path, with the task of next node prediction: p(ni | E, n1, n\u0131, N1:i-1). We report the path accuracy, which is the percentage of correct path generations during the test time, over 10,000 evaluation graphs.\nEmpty suffix at inference. The Belief State Transformer trains an additional encoder B for suffixes. However, as discussed in Section 2.2, we remove the dependency on B during inference time by pre-computing the backward latent b\u00f8 = B(\u00d8) with an empty input and proceed with auto-regressive sampling of the Next decoder Tn(\u00b7 | F(E, n\u2081, \u043f\u0442, \u043f\u2081,...ni), b\u00f8). The model is still able to produce goal-conditioned behavior since the goal node ny is present in the prefix input to the forward encoder.\nBaselines. We compare against several baselines.\n\u2022 Forward-only next-token prediction: This baseline follows the conventional strategy of training a Transformer with the next token prediction objective and teacher forcing, i.e., the model is trained by feeding the correct previous token as input.\n\u2022 Data augmentation: A common strategy to improve performance is to employ some form of data augmentation, although this requires some domain expertise to perform (Lee et al., 2024). This baseline augments the training data by replacing the goal with subgoals to potentially improve the learning of goal-conditioned behaviors. Specifically, the goal node, usually the terminal node in the path, is replaced with an intermediate node in the path. Then, a Transformer is trained with the next-token objective on this augmented dataset.\n\u2022 Teacherless: Proposed by Bachmann & Nagarajan (2024), the teacherless approach refers to a Transformer trained to predict the entire path $\u041f_{i=1}^{l-1} P(n_i | E, n_1, n_T)$ in one forward pass without providing access to tokens from a partial path."}, {"title": "3.5 RESULTS", "content": "As seen in Figure 2, the Belief State Transformer successfully learns to solve all the graphs. Since the Belief State Transformer uses an empty suffix, the parameter counts at inference time are very similar with minor variations driven by variations in output heads.\nThe baselines have varying degrees of success. The forward only baseline achieves at most a 1/d success rate, as it learns to output valid paths at random during inference time.\nThe data augmentation baseline also performs poorly. We suspect that despite having intermediate sub- goals, the gradients which the model is exposed to are still inadequate to encourage the development of appropriate representations due to the easy availability of shortcut solutions for most subgoals.\nThe teacherless baseline, while successful in some smaller graphs like G(2, 5), fails to solve more complicated graphs with more arms (G(5, 5)) or with longer path lengths (G(2, 20)), reproducing prior results of Bachmann & Nagarajan (2024)."}, {"title": "3.6 BELIEF STATE TRANSFORMER VARIANTS", "content": "Next, we ablate the Belief State Transformer to characterize its performance. The Belief w/o Prev ablation removes the previous token decoder and its objective from the training. The Belief w/o Backward ablation removes the backward encoder B from the training and inference process."}, {"title": "4 BELIEF STATE TRANSFORMER ANALYSIS", "content": "In this section, we show that the Belief State Transformer discovers a compact belief state, and that the forward-only and teacherless approaches do not\u00b9. First, we formally define a belief state.\nDefinition 1 (Belief State). For any probability distribution over a set of sequences P(x1:T), for any partial sequence s = x1:t, a vector vs is a belief state for s if there exists a randomized function g(vs) which can sample from the conditional distribution P(xt+1:TX1:t)."}, {"title": "4.1 BELIEF STATE DISCOVERY", "content": "We first show that successfully optimizing a Belief State Transformer results in a compact belief state.\nTheorem 2. Let D = P(x1:T) represent any given probability distribution over a set of sequences. Consider an ideal Belief State Transformer that satisfies the following conditions for all prefixes X1:t and suffixes Xt+k+1:T:\n$T_n(x_{t+1} | F(x_{1:t}), B(x_{t+k+1:T})) = P(x_{t+1} | x_{1:t}, x_{t+k+1:T}),$\n(3)\n$T_p(x_{t+k} | F(X_{1:t}), B(X_{t+k+1:T})) = P(x_{t+k} | x_{1:t}, x_{t+k+1:T}).$\n(4)\nThen, for any partial sequence x1:t supported by D, the forward encoding of the ideal Belief State Transformer, F(x1:t), is a belief state for x1:t.\nProof. Let st = X1:t denote the prefix. To prove that ft := F(st) is a belief state for st, we must show that, given ft, one can sample from the conditional distribution P(xt+1:T | St).\nThe key observation is that the conditional distribution can be decomposed as follows:\nP(Xt+1:T | St) = P(xT | St)P(XT\u22121 | St,XT)\uff65\uff65\uff65P(Xt+1 | St, Xt+2:T).\nFor an ideal Belief State Transformer, this decomposition can be rewritten as:\n= Tp(xT | ft, B(0)) \u00b7 Tp(xT\u22121 | ft, B(xT))\u2026Tp(Xt+1 | ft, B(xt+2:T)),\nThus, using the forward encoding ft, one can generate the remaining sequence Xt+1:T by sampling in reverse order-first sampling XT, then XT-1 conditioned on xr, and so on, until xt+1. Each step involves using the Prev decoder and updating the backward encoder with the newly generated token.\nSince the forward encoding ft enables sampling from the conditional distribution P(xt+1:T | st) in this way, it follows that ft is a belief state for St."}, {"title": "4.2 NEXT-TOKEN PREDICTION DOES NOT GUARANTEE BELIEF STATES", "content": "Theorem 2 establishes that an ideal Belief State Transformer learns correct belief states. In contrast, the following two theorems demonstrate a fundamental limitation of standard next-token predictors and their variants when viewed from the belief state perspective.\nTheorem 3. Consider a standard next-token predictor with a forward encoder F and output head T such that\nT(F(x1:t)) = P(Xt+1 | X1:t).\n(5)\nThere exists a distribution P(x1:t) over sequences and a next-token predictor of the form Equation (5) such that the input to the output head is not a belief state.\nThe proof is provided in Section A.2. Next, we analyze the case of teacherless training (see Sec- tion 3.4), where the model is asked to predict multiple future tokens at once.\nTheorem 4. Consider the teacherless setting where, for predicting H tokens into the future, the model is of the form\nTj(F(x1:t)) = P(Xt+j|x1:t) for j = 1, 2, . . ., \u0397.\n(6)\nThere exists a distribution D = P(x1:t) over sequences and a teacherless model satisfying Equa- tion (6) such that the input to the output head is not a belief state.\nThe proof is provided in Section A.3. In summary, the analysis in this section underscores the inherent limitations of next-token predictors: the input to the output head fails to capture the belief state."}, {"title": "5 EXPERIMENTING WITH THE BELIEF STATE TRANSFORMER", "content": "Given a model jointly trained on prefixes and suffixes, the representation is useful in new ways which are not available to simple forward Transformers. Here we detail two different forms of search based on forward and backward probabilities (respectively) as well as belief state embedding extractions.\nSetup. We use TinyStories (Eldan & Li, 2023), a dataset consisting of synthetic short stories. TinyStories aims to represent key challenges in text generation while keeping training tractable for small to medium scale models. We tokenize the dataset into a vocabulary space of size 1000, and discard stories greater than 256 tokens long resulting in a dataset consisting of 2.7 million stories.\nDuring evaluation, the models generate text using prefix-suffix snippets from an evaluation set of 100 unseen stories. Given stories from two competing models, GPT4 is then asked to output an analysis of each story examining multiple factors (e.g. grammar, flow, cohesiveness, creativity) before outputting a final recommendation. We follow best practices in evaluating with multiple trials and shuffling choice order. We report the winrate and confidence interval (CI) for each model. See Appendix C.2 for more details and examples of the GPT4 judge outputs and scoring.\n5.1 GOAL-CONDITIONED TEXT GENERATION\nIn the goal-conditioned setting, the user provides the model with a prefix and suffix, and the model infills the text in between. See below for an example of the prefix and suffix. We describe a goal- conditioned planning procedure with the Belief State Transformer for text generation in Algorithm 1.\nMethod. The algorithm performs n roll-outs.\nIn each roll-out (starting at line 3), a candi- date trajectory is generated that differs from the greedy trajectory but maintains high prob- ability. The key features of the process are as follows:\n1. Greedy generation: Starting with a se- quence s popped from the priority queue Q, a candidate trajectory is extended up to a max- imum length k using argmax greedy selection (line 6). The completed trajectory is appended to the set of candidates C (line 11).\n2. Priority queue update for future generation: Simultaneously, the priority queue Q is up- dated to track alternative candidate sequences. This is done by appending non-greedy tokens to the queue, with their priority set by the rela- tive suboptimality of each token. Specifically, lines 8 and 9 add alternative tokens with pri- ority equal to the current sequence priority multiplied by the ratio of the alternative token's probability to that of the greedy token. Since this ratio is \u2264 1, priorities decreases with suboptimality. Also, this ensures that partial sequences of different lengths remain comparable, as suboptimality is independent of sequence length. This encourages branching at ambiguous points.\n3. Scoring: Once candidate trajectories are generated, they are scored by evaluating the consistency of generated tokens Xt+1:t+k-1 with the goal Xt+k:T using the next-head probability:\n$T\n\\prod_{i=t+k} T_n(X_i | F(X_{1:i-1}), B(x_{i+1:T}))$\n(7)\nWe then return the highest-scoring trajectory based on Equation (7).\nBaseline. We select the Fill-in-the-middle (FIM) (Bavarian et al., 2022) approach as a natural goal-conditioned baseline. FIM trains a single forward-only transformer where the input is the"}, {"title": "5.2 UNCONDITIONAL TEXT GENERATION", "content": "Method. Next, we investigate the unconditional setting, or when the goal is unknown. We propose to largely reuse the logic in Algorithm 1, where we set the goal to the empty input z\u00fd := {0}. Rather than scoring the sequences with the next head Tn In we propose to use the previous head Tp p over a fixed amount of suffix tokens k.\n$\\prod_{i=T-k}^{T} T_p(X_i | F(x_{1:i-1}), B(x_{i+1:T}))$\n(8)\nWe choose to use the previous head rather than the next head as a semi-independent evaluator of next-generated tokens to reduce the bias associated with self-evaluation. By scoring over the last k tokens, this selects for trajectories whose endings are more likely.\nResults. As seen in Table 2, the Belief State transformer outperforms the FIM model. The Belief State Transformer outputs consistently have correct grammar, whereas the FIM models often abruptly end in the middle of sentences. This grammatical flaw is consistently picked up by the GPT4 judge, and results in the FIM's lower winrate."}, {"title": "5.3 ABLATIONS", "content": "Finally, we conduct a few ablations to verify our design choices. First, the Belief Beamsearch ablation uses beam- search to generate the candidate set rather than the priority queue scheme in Algorithm 1. Next, in the unconditioned setting, the Belief Next Score ablation uses the next head Tn to score the suffix instead of Tp.\nTable 3 shows the performance drops in both cases. The Belief Beamsearch outputs is more prone to repetition than Algorithm 1. Next, the Belief Next Score ablation frequently outputs stories that end abruptly and incorrectly, similar to the FIM baseline in the unconditional setting."}, {"title": "5.4 UNDERSTANDING BELIEF STATES", "content": "We investigated the representations learned by the Belief State Transformer to provide insight into what the states capture and how they are learned. First, we verified that the representations are indeed belief states, by training small MLP networks from the first hidden state on the G(2,5) Stargraph to predict the future token on a specific timestep. We found that the Belief State Transformer has the information to predict all future tokens, while the information contained within the state learned using the Forward-Only objective is incomplete"}, {"title": "6 OTHER RELATED WORK", "content": "Several papers explore non-left-to-right generation approaches. Gu et al. (2019) model the generation order as latent variables and performs beam search over the generation order itself to produce the final text. This is beneficial because in general, the best ordering for decoding is task-dependent. On the other hand, Welleck et al. (2019) explore generating text in a binary tree order, but their model struggles to outperform traditional left-to-right autoregressive generation in tasks like language modeling, machine translation, sentence completion, and word reordering. In Nguyen et al. (2024), they propose training two separate transformers and implement several strategies to \u201cmeet in the middle\" during decoding. In contrast, our approach involves jointly training a transformer capable of both forward and backward decoding, which has important implications for creating compact belief states. A general study of the quality of backward prediction is done in Papadopoulos et al. (2024) discovering that backward prediction is possible but generally slightly worse than forwards prediction. Somewhat further afield, combined forward/backward decoding approaches with RNNs have proved useful (Mou et al., 2015; Serdyuk et al., 2018; Mangal et al., 2019) and similarly for neural machine translation (Liu et al., 2016; Zhang et al., 2018).\nThe concept of employing multiple decoding blocks has been explored in other works. For example, MEDUSA (Cai et al., 2024) leverages post-training on multiple decoders to accelerate decoding, achieving up to a twofold increase in speed. Similarly, multi-head learning (Gloeckle et al., 2024) focuses on training models to predict multiple next tokens simultaneously. Although these approaches are focused on computational performance, they align with our theoretical insights, as predicting multiple tokens fosters the creation of more robust representations of future contexts.\nShai et al. (2024) show that transformers create a non-compact representation of belief states within their residual stream. Compact belief states are accessible in state-space models (Hasani et al., 2020; Gu & Dao, 2023), although this comes with different trade-offs compared to transformer-based approaches. For example, the Mamba training process is known to fail on star graphs (Bachmann & Nagarajan, 2024)."}, {"title": "7 CONCLUSION", "content": "The Belief State Transformer advances goal-conditioned next-token predictors, effectively addressing the limitations of traditional forward-only transformers. The ability of our model to learn a compact belief state provides a maximal solution: since the belief state contains all the information useful in predicting the future, no more complex objective can elicit more information. Through experiments with both star graphs and story writing tasks, we have demonstrated the necessity of each component of our model, particularly in challenging scenarios where standard transformers struggle.\nLooking ahead, while our results demonstrate the superior performance of the Belief State Transformer in small scale story writing, exploring its application to other goal-conditioned tasks would be valuable. Our current experiments serve as proof-of-concept. Further investigation into the scalability of our approach to larger practical scenarios is essential."}, {"title": "A.1 PROOF OF THEOREM 1", "content": "In this section, we provide a formal statement and a proof of Theorem 1.\nAs discussed in the main text, the starting point of our argument is the observation by (Bachmann & Nagarajan, 2024, Appendix F.2) that the model quickly learns the aforementioned \u201cflawed cheat\" solution, which prevents it from learning the true solution. Specifically, next-token predictors rapidly adopt this flawed shortcut, but make little progress in correctly predicting the first vertex on the path. Below, we provide formal evidence that learning the flawed cheat indeed hinders the model from learning the true solution.\nIn essence, once the flawed cheat solution is perfected, the model receives supervision only from the prediction of the first vertex after the starting node. This effectively reduces the star graph task to the following simplified task:\nTask 1. Given a stargraph and a goal node, predict the correct neighbor of the starting node.\nThe formal statement of Theorem 1 is that Task 1 is at least as difficult as learning the full parity function (i.e., predicting whether the sum of the elements in a binary string is even or odd).\nTheorem 5. For every full parity problem, there exists a stargraph such that solving Task 1 provides a solution to the parity problem.\nNotably, empirical evidence shows that gradient-based methods struggle to learn the full parity function in standard training setups, especially in high dimensions (e.g., Shalev-Shwartz et al., 2017; Abbe & Sandon, 2023). It has been also conjectured that learning the full parity function requires a number of samples and computations exponential in the input dimension (e.g., Abbe & Boix-Adsera, 2022). Consequently, Task 1 inherits this difficulty, as it encapsulates learning the full parity function as a special case."}, {"title": "A.2 PROOF OF THEOREM 3", "content": "We prove this theorem by constructing a counterexample. Consider the distribution D that is uniform over the sequences {ACA, BCB}. Our goal is to show that there exists an ideal next-token predictor, in the sense of Equation (5), that does not learn a belief state.\nFirst, note that an ideal next-token predictor achieves a log loss of 1 bit on the first token and 0 log loss on all subsequent tokens. Define the forward encoder F(x1:t) to output a two-dimensional vector for any partial sequence X1:t. Specifically, we define the encoding as follows:\nF(0) = (-1,-1),\nF(A) = F(B) = (\u22121, 1),\nF(AC) = (1, -1),\nF(BC) = (1, 1).\nNext, we define the output head T for the next-token predictions:\nT(-1,-1) = uniform(A, B),\nT(-1,1) = C,\nT(1,-1) = A,\nT(1, 1) = B.\nIt can be verified that this setup achieves the optimal performance: a 1-bit log loss on the first token (since A and B are equally likely), and 0 log loss on subsequent tokens (since the continuation of the sequence is fully deterministic). However, the key observation is that F(A) = F(B) = (\u22121, 1) is not a belief state for the remainder of the sequence. This is because any function g(-1, 1) applied to this encoding outputs a distribution that is independent of whether the initial token was A or B. Thus, the encoding fails to capture the information necessary to distinguish between the two possible continuations of the sequence, violating the definition of a belief state. Therefore, this next-token predictor does not output a belief state, completing the proof."}, {"title": "A.3 PROOF OF THEOREM 4", "content": "We again use a counterexample construction to prove this theorem. Consider the distribution D, which is uniform over the four sequences {DAA, DBB, SAB, SBA}. Here, the initial tokens D and S determine whether the next symbol is doubled or not. Our goal is to demonstrate that a teacherless model, in the sense of Equation (6), does not learn a belief state.\nFirst, we construct an ideal teacherless model that suffers a log loss of 2 bits on each token except for the last token. Note that this is the minimal log loss achievable. Define the forward encoder F(x1:t) to output a two-dimensional vector for any partial sequence X1:t. Specifically, we define the encoding as follows:\nF(0) = (-1,-1),\nF(D) = F(S) = (\u22121, 1),\nF(DA) = F(SB) = (1, \u22121),\nF(DB) = F(SA) = (1, 1).\nNow, define the output head for predicting the next tokens at different time steps as follows:\nT\u2081((-1,-1)) = uniform(S, D),\nT2((-1,-1)) = uniform(A, B),\nT3((-1,-1)) = uniform(A, B),\nT1((-1,1)) = uniform(A, B),\nT2((-1,1)) = uniform(A, B),\nT1((1,-1)) = A,\nT1((1,1)) = B.\nThis teacherless model clearly achieves the minimal log loss. However, the key observation is that F(S) = F(D) = (-1, 1) is not a belief state for the remainder of the sequence. The reason is that any"}, {"title": "B STAR GRAPH DETAILS", "content": "In this section, we provide detailed descriptions of the star graph experiments from Section 3.4.\n\u2022 Data Generation. We use the data generation code from the official codebase. To generate the star graph structure, each node ni is sampled uniformly from the set {1, . . ., N}. For all experiments, we set N = 50, and we generate 8M examples for each data set.\n\u2022 Data Tokenization. We follow the same tokenization settings as in (Bachmann & Nagarajan, 2024, Section G.1).\n\u2022 Architectures. Both the forward and backward encoders consist of nlayers = 6 layers with an embedding dimension of 768, nhead = 8 attention heads, and an MLP expansion factor of 1. The baselines use the same configuration.\n\u2022 Training Details. In all cases, we use the AdamW optimizer with a weight decay strength of 0.1. For G(2,5), the learning rate is set to n = 3\u00b7 10\u22124, while for G(5,5) and G(2, 20), a smaller learning rate of n = 1 \u00b7 10\u20134 is used. We run all experiments for 100 epochs to ensure convergence. All models run quickly, finishing 100 epochs in less than 2 hours. The belief state transformer reaches \u2248 100% accuracy in a few minutes on the easier graphs. Each model is trained on a single A100/H100 GPU with 80GB memory.\n\u2022 Evaluation. We evaluate the models on 10,000 unseen graphs. To succeed, the model must output the entire path correctly, and we report the success rate."}, {"title": "B.1 BASELINE DETAILS", "content": "We use the official codebase from Bachmann & Nagarajan (2024) to instantiate and train the baselines.\n\u2022 Forward-only. This baseline trains a transformer to do next-token prediction over the sequence data. Teacher forcing is used during training, so the model gets ground truth intermediate sequences as input.\n\u2022 Data Augmentation. Here, our data augmentation process entails simply modifying the data using some domain knowledge, before feeding it into the standard forward-only transformer training process.\nGiven the original training sequences [E | n1, n\u0131 | N1, N2, N3, ... n\u0131], we propose to replace the task specification n\u2081, nm with n\u2081, n' where n' ~ {n2... ni-1} is an intermediate node in the path. The resulting training sequence would then look like: [E | n\u2081, n' | n1, n2, 13, . ni]. The idea here is that replacing the long horizon goal with a subgoal closer to the start would make learning the goal conditioning easier.\n\u2022 Teacherless. This baseline changes the objective from next-token prediction, to multiple token prediction. Given the graph description, start and goal, [E | n1, n\u0131] the model needs to predict the path [n1, n2, n3, . . . n\u0131] in a single forward pass."}, {"title": "C TINYSTORIES EXPERIMENTS", "content": "We train all models on a single A100/H100 GPU with 80GB memory.\nC.1 TRAINING\nThe Belief State Transformer's encoders have the following settings: nlayers = 8", "settings": "nlayers = 12, blocks with embedding dimension edim = 76"}]}