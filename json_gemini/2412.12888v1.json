{"title": "ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction", "authors": ["Zhongjie Duan", "Qianyi Zhao", "Cen Chen", "Daoyuan Chen", "Wenmeng Zhou", "Yaliang Li", "Yingda Chen"], "abstract": "The emergence of diffusion models has significantly advanced image synthesis. The recent studies of model interaction and self-corrective reasoning approach in large language models offer new insights for enhancing text-to-image models. Inspired by these studies, we propose a novel method called ArtAug for enhancing text-to-image models in this paper. To the best of our knowledge, ArtAug is the first one that improves image synthesis models via model interactions with understanding models. In the interactions, we leverage human preferences implicitly learned by image understanding models to provide fine-grained suggestions for image synthesis models. The interactions can modify the image content to make it aesthetically pleasing, such as adjusting exposure, changing shooting angles, and adding atmospheric effects. The enhancements brought by the interaction are iteratively fused into the synthesis model itself through an additional enhancement module. This enables the synthesis model to directly produce aesthetically pleasing images without any extra computational cost. In the experiments, we train the ArtAug enhancement module on existing text-to-image models. Various evaluation metrics consistently demonstrate that Art Aug enhances the generative capabilities of text-to-image models without incurring additional computational costs. The source code and models will be released publicly.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have been extensively studied in recent years. With the development of large-scale text-image datasets (Schuhmann et al., 2022; Gu et al., 2022), pre-trained large text-to-image models (Rombach et al., 2022; Chen et al., 2023; Saharia et al., 2022) have fully developed and demonstrated strong application potential. Downstream tasks such as interactive creation (Liu et al., 2024c), controllable image generation (Zhang et al., 2023), and consistent story generation (Zhou et al., 2024) all require the generated content to align with human preferences. However, due to the training datasets being filled with a large number of low-quality images, pre-trained text-to-image models cannot generate high-quality images without guidance. This has become a challenge in the application of diffusion models.\nTo guide image generation models in producing high-quality images, current research primarily focuses on three aspects: 1) Data refinement (Chen et al., 2024a; Schuhmann et al., 2022) are employed to eliminate low-quality images from large training datasets, thereby preventing them from negatively impacting the model's performance. 2) Prompt engineering (Wang et al., 2024b; Cao et al., 2023) aims to craft detailed prompts to guide the model in producing superior-quality images. 3) Alignment training (Wallace et al., 2024; Fan et al., 2024) focuses on aligning the model's generative inclinations with human preferences via training. However, these methods all have certain limitations. Data refinement can only be used for coarse filtering. Directly filtering out low-quality images requires meticulous efforts and potentially leads to overfitting due to the insufficient amount of data. Prompt engineering based on language models might result in generated images containing content that is inconsistent with the user-provided prompts, thereby compromising the text-image correlation. Alignment training is currently the key method for improving image quality. The mainstream alignment training methods, including Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2024), require a large amount of manually annotated data, leading to extremely high costs.\nOn the other hand, recent studies of model interaction and self-corrective reasoning provide us with new insights for enhancing the capabilities of image generation models. Particularly, GPT-01 (OpenAI, 2024) significantly enhances the capabilities of LLMs (Large Language Models) (Brown, 2020) through self-corrective reasoning via the model itself, at the expense of longer computation time. LLMs are trained on human-generated data and potentially understand human interpretations and preferences for aesthetics. Recent studies have preliminarily demonstrated the feasibility of guiding image generation models through interactive conversations using language models (Huang et al., 2024). Some multimodal models (Wang et al., 2024a; Chen et al., 2024b; Liu et al., 2024b) are capable of understanding image content and expressing it through natural language, motivating us to explore the deeper assistive roles of LLMs in relation to image generation models.\nTo address the current challenges faced by image generation models, inspired by the model interaction and self-corrective reasoning approaches, we propose a novel text-to-image generation model enhancement approach called ArtAug. As shown in Figure 1, ArtAug can significantly improve the image quality, aligning the generated image content with human preference.\nThe framework Art Aug is presented in Figure 2. There are three modules in ArtAug, including a generation module for text-to-image generation, an understanding module for analyzing and refining the image content, and an enhancement module for improving the generation module. Firstly, we design an interaction algorithm, utilizing the understanding module to provide fine-grained modification suggestions for the generation module, and subsequently generate enhanced images. Secondly, we generate a large batch of image pairs using the interaction algorithm and filter them. Thirdly, we propose differential training, aimed at training the enhancement module to learn the differences before and after image enhancement. Fourthly, the enhancement module is then fused to the generation module, imbuing the generation module with the enhancement capability brought by interactions without requiring additional computational resources. By repeating the aforementioned process, we iteratively improve the generation module. In our experiments, we train the enhancement module for the state-of-the-art text-to-image model. ArtAug can notably enhance the performance of text-to-image models, resulting in the generation of aesthetically pleasing images. This improvement is consistently evidenced by various evaluation metrics. We will release the source codes\u00b9 and the improved model\u00b2. Overall, the contributions of this paper include:\n\u2022 We design an interaction algorithm between a generation module and an understanding model in image synthesis, demonstrating that current multimodal LLMs can guide text-to-image models to generate high-quality images aligned with human preferences.\n\u2022 We propose ArtAug, a framework for improving text-to-image models. By learning the differences between images before and after interaction, we iteratively enhance the capabilities of the text-to-image model."}, {"title": "2. Related Work", "content": "2.1. Large Image Synthesis Models\nIn recent years, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have achieved significant breakthroughs in the field of image synthesis. Since the introduction of Latent Diffusion (Rombach et al., 2022), models pre-trained on large-scale text-image datasets (Schuhmann et al., 2022; Lin et al., 2014; Gu et al., 2022) have made considerable advancements. The generative capabilities of these models have been steadily improved, including both UNet-based models (Ronneberger et al., 2015; Rombach et al., 2022; Podell et al., 2023; Sauer et al., 2025) and the more recent DiT-based models (Li et al., 2024; Chen et al., 2023; Esser et al., 2024; Labs, 2024). Notably, DiT (Diffusion Transformer) (Peebles & Xie, 2023) has considerably enhanced both the convergence speed and the generalization ability of image generation models, establishing itself as one of the most popular architectures in the realm of image synthesis. To further enhance image quality in terms of text-image alignment and aesthetic appeal, various approaches, such as data refinement (Chen et al., 2024a; Schuhmann et al., 2022), prompt engineering (Wang et al., 2024b; Cao et al., 2023), and alignment training (Wallace et al., 2024; Fan et al., 2024), have been extensively investigated. Based on these studies, we propose a new approach to enhance image quality in this paper.\n2.2. Aligning Models with Human Preferences\nText-to-image models pre-trained on extensive text-image datasets have demonstrated rudimentary image generation abilities, but these models often produce suboptimal quality images without fine-tuning (Liu et al., 2024a). Currently, alignment training stands as the principal method for improving image quality by aligning generated content with human preferences. Alignment training is initially investigated in large language models (Ouyang et al., 2022; Rafailov et al., 2024), and has recently been applied to diffusion models. For example, based on reinforcement learning, approaches like DPOK (Fan et al., 2024) and DDPO (Black et al., 2023) gather human preferences on model-generated outputs for fine-tuning text-to-image models. Similarly, Diffusion-DPO (Wallace et al., 2024) and SPO (Liang et al., 2024) employ auxiliary models to model human preferences, using DPO (Rafailov et al., 2024) to fine-tune diffusion models accordingly. However, because human preferences are difficult to quantify, these alignment training methodologies necessitate extensive manually labeled datasets, which are prohibitively expensive to produce. Inspired by these studies, we explore the possibility of using multimodal LLMs to replace manual annotation, aiming to obtain a large amount of training data at a lower cost for alignment training."}, {"title": "3. Methodology", "content": "The framework of ArtAug is presented in Figure 2. ArtAug consists of three key components: the interaction algorithm, data generation and filtering, and differential training. In this section, we provide a detailed description of each module.\n3.1. Interaction Algorithm\nEmpirically, we observe that text-to-image models tend to generate simple content when given simple prompts. Prompt engineering is generally essential for generating high-quality images, but crafting high-quality prompts manually poses certain challenges. Therefore, a single image synthesis module (a text-to-image model) struggles to generate detailed and aesthetically pleasing images. To address this challenge, we propose an interactive algorithm and utilize an additional understanding module to aid the synthesis module.\nThe interactive algorithm includes three steps: generation, understanding, and refinement. First, we use the original generation pipeline of the text-to-image model to generate an image X. Second, we employ the understanding module u to analyze the image content and generate modification suggestions. The understanding module is implemented based on multimodal LLMs due to their significant image understanding and grounding capabilities. The modification suggestions provided by the understanding module are in the form of n pairs of bounding box and prompt, which are formulated as $u(X) = \\{(P_i, M_i)\\}_{i=1}^n$, where the bounding box $M_i \\in \\{0,1\\}^{H \\times W}$ represents the location and the prompt $P_i$ describes the corresponding modified content. To improve computational efficiency, we directly generate all bounding boxes and prompts through a single-turn dialogue. Third, we use the synthesis module to regenerate the image according to the suggestions for image modifications. To finely control the content of images and ensure that each prompt affects the corresponding area, we design a partitioned image generation method based on previous studies (Li et al., 2023; Bar-Tal et al., 2023). Assuming that the original model output is $\\epsilon_{\\theta}(P, t, h)$, where P is the original prompt, t is the timestep of the denoising process, and $h\\in \\mathbb{R}^{H\\times W}$ is the latent representation of the image, we use\n$\\epsilon_{\\theta}(P,t,h|\\{(P_i, M_i)\\}_{i=1}^n) = \\epsilon_0(P,t,h) + \\frac{\\Sigma_{i=1}^n \\epsilon_0(P_i, t, h) \\cdot M_i}{1 + \\Sigma_{i=1} M_i}$\nto replace the original model output, where \\cdot denotes the element-wise multiplication. Intuitively, the interaction algorithm employs the model to infer according to different prompts and then performs a weighted average of the results based on the location information.\n3.2. Data Generation and Filtering\nAlthough the procedures described in the last subsection can improve the quality of generated images without training, this algorithm has two drawbacks. 1) Slow computational speed. Since fine-grained prompts require independent forward inferences through the model, the overall computation time increases to n + 1 times the original. This prolongs the generation process to several minutes. 2) Risk of bad cases. Due to the lack of end-to-end training in the modified image generation pipeline, there is a potential for producing unrealistic images. To overcome these drawbacks, we aim to consolidate the enhancements produced through interaction into the text-to-image model itself via post-training. Specifically, we need to generate a large batch of image pairs (X, X') through interaction, where X represents images generated by the original text-to-image pipeline and X' represents the interactively refined images. After filtering, a smaller dataset is obtained, which is then used for training.\nIn the data filtering process, we apply stringent filtering criteria. Fundamentally, we expect that images enhanced through interaction should be more aesthetically pleasing than the originals. Therefore, we only retain image pairs with increased aesthetic scores (Schuhmann et al., 2022). Additionally, we need to ensure that the enhanced images are consistent with the semantic meaning of the text prompts. To achieve this, we use the CLIP model (Radford et al., 2021) to filter out all image pairs where the text-image similarity decreases. These two filtering steps can effectively eliminate a significant portion of the data unsuitable for training. However, given that the prompts are collected from real users and exhibit complex and varied content, we conduct further meticulous manual reviews. During the manual review, we remove the content related to pornography, violence, politics, and racial discrimination, and ensure that the enhanced images in the image pairs show a clear improvement in quality. Appendix A shows several categories of the final retained training data, where the image pairs exhibit significant improvements in various aspects.\n3.3. Differential Training\nAfter filtering and review, approximately 1% to 2% images are retained. The amount of data is relatively small, and directly using these data to fine-tune the model leads to overfitting. As shown in our preliminary experiments. Based on the observation, we propose a differential training approach to learn the differences between images, rather than directly learning the images enhanced by interactions. The model structure of the enhancement module is LORA (Hu et al., 2021). Assume that the parameters of fully connected layers in the original model are formulated as\n$\\Theta = \\{W_i\\}_1^I$\nFor each parameter matrix $W_i \\in \\mathbb{R}^{d_1 \\times d_2}$, the parameter matrix after adding LoRA becomes $W_i + B_iA_i$, where $A_i \\in \\mathbb{R}^{r \\times d_2}$ and $B_i \\in \\mathbb{R}^{d_1r}$ are two low-rank matrices. The LoRA rank r is a hyperparameter. We use $\\Phi = \\{(A_i, B_i)\\}_{i=1}^I$ to denote all LoRA parameters and use \"$\\oplus$ \" to denote the adding LoRA operator, i.e.,\n$\\Theta \\oplus \\Phi = \\{W_i + B_iA_i\\}_1^I$\nFor a diffusion model based on flow matching (Esser et al., 2024; Labs, 2024), the LoRA training problem using a single image X with its latent representation x is formulated as\n$\\Phi(\\Theta, X) = \\underset{\\Phi}{\\text{arg min }} E_{t\\sim T,\\epsilon\\sim N(0,1)} [L(P, t, h, \\epsilon)]$,\nwhere\n$L(P,t,h, \\epsilon) = w_t||\\epsilon_{\\theta \\oplus \\Phi}(P,t,h_t) - \\epsilon + h||_2$,\n$h_t = (1 - \\sigma_t)h + \\sigma_t \\epsilon$.\nFor a denoising diffusion probabilistic model (Rombach et al., 2022; Podell et al., 2023), the LoRA training problem can be similarly formulated. We only need to replace the loss function as\n$L(P,t,h, \\epsilon) = ||\\epsilon_{\\theta \\oplus \\Phi}(P, t, h_t) - \\epsilon||^2$,\n$h_t = \\sqrt{1 - \\alpha_t}h + \\sqrt{\\alpha_t} \\epsilon$.\nwhere\nThe parameters $w_t, \\sigma_t, \\alpha_t$, and the probabilistic distribution T are determined by the flow matching theory and the probabilistic diffusion process. We recommend readers to read the corresponding original papers (Liu et al., 2022; Ho et al., 2020) for more details. Our training approach is compatible with both two kinds of diffusion models.\nGiven an image pair (X, X') generated by the interaction algorithm. We train the first LoRA model $\\Phi_1 = \\Phi(\\Theta, X)$ using the single image generated by the original model so that it always generates image X when inputting the corresponding prompt. Then, based on this, we train the second LoRA model $\\Phi_2 = \\Phi(\\Theta \\oplus \\Phi_1, X') = \\Phi(\\Theta \\oplus \\Phi(\\Theta, X), X')$ so that it always generates image X' when inputting the corresponding prompt. Thus, the second LoRA $\\Phi_2$ can be used to represent the differences between the two images. We drop the first LoRA and call the second LoRA differential LoRA. We train a corresponding differential LORA model for each image pair. The training process for each image pair is independent, which makes our training process distributed and scalable.\n3.4. Iterative Improvement\nThrough differential training, we obtain a LORA model $\\Phi$ that can enhance the generative capabilities of the text-to-image model. This LoRA model can be fused into the base model, i.e., let\n$\\Theta \\leftarrow \\Theta \\alpha\\Phi (\\Theta \\oplus \\Phi(\\Theta, X), X')$, where \\alpha is the weight of the LoRA. To enhance the stability of the model's preferences, we average the LoRA parameters across multiple image pairs. Based on this iterative formula, we make it possible to continue generating data through the interaction processes. Consequently, the data generation and the differential training process can be iteratively repeated until the interactive algorithm can no longer significantly improve the quality of the generated images. Ultimately, we obtain a series of stacked LoRA models $\\{\\Phi^{[1]}, \\Phi^{[2]}, ... \\}$. We merge them into a single LoRA model by concatenating the corresponding matrices. The use of the entire enhancement module is consistent with that of a standard LoRA model and maintains compatibility with other LORA models. Additionally, users can adjust the influence of the enhancement module on the text-to-image model by tuning the weight of the merged LoRA model, thereby achieving controllable generation.\nFrom another perspective, this iterative enhancement process involves updating the model parameters at each iteration, akin to a gradient descent step. We provide a detailed analysis of the iterations in the experiments in Section 4.2. The trainable LoRA parameters correspond to the gradient. The parameter $\\alpha$ corresponds to the learning rate in gradient descent. A smaller $\\alpha$ can make the training process more stable, but it will slow down the convergence speed. The number of averaged LoRA models corresponds to the batch size. In this manner, we can employ human preference, an inherently non-differentiable training objective, for the training of the model implicitly via data synthesis."}, {"title": "4. Experiments", "content": "We conduct extensive experiments to demonstrate ArtAug's effectiveness, including improving off-the-shelf models and thoroughly investigating each component.\n4.1. Improving Off-the-Shelf Models\n4.1.1. EXPERIMENTAL SETTINGS\nWe train the ArtAug LoRA based on the state-of-the-art text-to-image model \u201cFLUX.1[dev]\u201d (Labs, 2024). In the interaction algorithm, the understanding module is implemented based on Qwen2-VL-72B (Wang et al., 2024a) due to its sufficiently accurate visual grounding capabilities, which enable the generation of fine-grained bounding boxes and prompts. The prompt used in Qwen2-VL-72B is presented in Appendix B. We provide detailed discussions about the selection of the multimodal LLM in Appendix C, including a comparative analysis between six SOTA multimodal LLMs. Our experiments do not require a text-image dataset; we only use a dataset of prompts. In each training iteration, we randomly sample approximately 3k prompts from the DiffusionDB dataset (Wang et al., 2022). Considering that these prompts are collected from users on the internet and some may contain ambiguous semantics, we refine the prompts using Qwen2-VL-72B before generating images. After filtering and reviewing as described in Section 3.2, approximately 30 to 100 image pairs remain. We trained a differential LoRA model for each image pair. The learning rate is set to 1 \u00d7 10\u22124, with a batch size of 1, and the LORA model is trained for 400 steps. The LoRA rank is manually adjusted to 4, 8, or 16 to ensure convergence on the training image. The loss function is consistent with the flow match theory, and other training hyperparameters are consistent with those of the FLUX.1[dev] model itself.\n4.1.2. QUANTITATIVE COMPARISON\nAfter training, we randomly sample 10k prompts in DiffusionDB (Wang et al., 2022) to evaluate the quality of the generated images. These prompts are not used in the data generation. The evaluation metrics include:\n\u2022 Aesthetic (Schuhmann et al., 2022): This metric utilizes an aesthetic evaluation model trained on human-labeled LAION image data. The model assesses the visual appeal of the images and is widely used in data processing.\n\u2022 PickScore (Kirstain et al., 2023): A model trained on the Pick-a-Pic dataset based on human preferences for model-generated images. This model can compare two images and estimate the probability that humans consider one image to be of higher quality.\n\u2022 MPS (Zhang et al., 2024): A multi-dimensional human preference evaluation model. This model can comprehensively assess image-text consistency, aesthetics, and details, providing a score that represents the overall quality of the image.\n\u2022 CLIP (Radford et al., 2021): A multimodal model trained using contrastive learning, utilized in our experiments to calculate the similarity between text and image, which is a measure of text-image consistency.\nThe quantitative results are presented in Table 1. In the three aesthetic quality metrics, the model trained with ArtAug demonstrates a significant improvement. This clearly indicates the efficacy of ArtAug in enhancing image quality. Furthermore, the CLIP text-image similarity metric does not exhibit a noticeable decline, suggesting that ArtAug does not compromise the original text comprehension ability of the base model. Overall, Art Aug serves to enhance the fundamental capabilities of the text-to-image model, without sacrificing their linguistic interpretative abilities.\n4.1.3. HUMAN EVALUATION\nSome studies (Podell et al., 2023; Jiang et al., 2024) have highlighted the limitations of automatically computed evaluation metrics, prompting us to conduct an additional double-blind human evaluation. We invite 20 participants to take part in this evaluation. In each round, participants are shown two images: one generated by the original text-to-image model and the other generated by the ArtAug-enhanced model. Similar to GenAI-Arena (Jiang et al., 2024), the positions of the two images are randomized. Each participant is asked to select the image with better visual quality or to choose \"tie\". We record the percentage of user votes, as shown in Table 1. ArtAug achieves a winning rate of 45.93%, demonstrating the effectiveness of ArtAug in enhancing visual quality.\n4.1.4. ETHICS CONSIDERATION\nAlthough ArtAug can enhance the capabilities of image generation models, this also introduces potential ethical risks. We have observed that user-provided prompts collected from the internet contain descriptions of harmful content such as pornography and violence. Pre-trained text-to-image models have the ability to generate such harmful content. Additionally, pre-trained image understanding models potentially learn biases towards pornographic content, resulting in suggestive content appearing in the images generated by the interaction algorithm. This necessitates additional human resources to review the generated datasets during the iterative training process of ArtAug, otherwise, these biases would be learned and propagated. We have tried to prevent the model from being influenced by these biases. To ensure that ArtAug does not increase the propensity of the model to generate harmful content, we conduct additional experiments. We use NudeNet (Bedapudi, 2019) to calculate the harmful scores of the images generated in the aforementioned experiments. The results, presented in the last column of Table 1, show that Art Aug does not increase the propensity of the model to generate harmful content.\n4.2. Impact of Iteration Step\nTo better understand the changes in the model's capabilities throughout the iterative training process, we analyzed the data of image pairs generated in each iteration. Some statistical indicators are presented in Figure 4. In each iteration, our primary focus is on the enhancement of image aesthetics by the interaction algorithm. It can be observed from the figure that the aesthetic scores consistently improve after interaction. This enhancement capability is ingrained into the model during training and carries over to the next iteration, enabling continuous improvement. We also calculated the correlation between images and prompts before and after interaction using the CLIP model. It should be noted that these prompts are refined by the language model, so the CLIP scores appear slightly higher than those in Table 1. Although the interaction algorithm may sometimes alter the image content away from its original semantics, especially when the prompt contains terms like ugly, dirty, or bloody, our rigorous data filtering process eliminates such data to prevent compromising the model's original capabilities. Additionally, we calculated the cosine similarity of images before and after interaction using the vision encoder component (Dosovitskiy, 2020) of the CLIP model. As iterations progress, the enhancement effect of the interaction algorithm on image quality diminishes. In the eighth iteration, we are unable to obtain sufficient image pairs for training after filtering, and thus, we stop the training process.\n4.3. Ablation Studies\n4.3.1. INTERACTION ALGORITHM\nWe compared the interaction algorithm outlined in Section 3.1 with the naive prompt refinement approach, as demonstrated in Figure 5. In the naive prompt refinement approach, we leverage the multimodal LLM to directly generate a detailed prompt for the image generation model. This example reveals that regenerating images using only refined prompts typically results in a complete alteration of the scene's overall composition. Conversely, our interaction algorithm is capable of enhancing the details while preserving the fundamental composition, exemplified by the flowers and light in the image. This suggests that our interactive algorithm effectively utilizes the capabilities of the image understanding model to generate fine-grained prompts ensuring consistency in image content. Therefore, the image pairs generated using the interaction algorithm are better suited for the subsequent differential training process, which is aimed at learning the differences between two images.\n4.3.2. DIFFERENTIAL TRAINING\nWe also investigate the effectiveness of the differential LORA training mentioned in Section 3.3. We compare it with a LoRA model naively trained using the filtered data. We evaluate the two training methods in the first iteration of FLUX.1[dev]. By employing the same learning rate and number of training steps, we calculate the four evaluation metrics of the LORA models. As shown in Table 2, it is evident that naive LoRA training leads to significant overfitting, resulting in a noticeable decline in text-image alignment, thereby compromising the model's original generative capabilities. Conversely, differential LoRA training better captures the difference in image pairs and avoids overfitting.\n4.4. Limitations\nThis study explores a novel approach to improve text-to-image models, distinct from RLHF and DPO. We aim to replace costly human resources with multimodal LLMs. However, to ensure the controllability of training outcomes, we have retained manual selection and review steps. Due to budget constraints, we employed only two graduate-level computer science students to complete this process, making our approach more lightweight compared to previous studies (Black et al., 2023) that required hundreds of individuals for manual annotation. Similar to the pre-training of large models, we believe this process is scalable. By utilizing a larger cluster for data generation and hiring more personnel for selection and review, we could further enhance the model's performance, which remains an avenue for future work."}, {"title": "5. Conclusion", "content": "In this paper, we explore a method to enhance text-to-image models. To guide these models in generating high-quality images that align with human preferences, we introduce ArtAug. By employing a synthesis-understanding interaction algorithm, we improve the image quality, albeit at the expense of increased inference time. This interaction algorithm enables the creation of a dataset specifically for training, thereby facilitating a differential training approach to learn the enhancement. By iterating this process, we progressively refine the text-to-image model. Based on state-of-the-art text-to-image models, we trained Art Aug modules in the form of LoRA. Experimental results highlight the substantial improvements achieved through ArtAug. Our approach effectively attains alignment training with minimal human resource costs."}, {"title": "A. Examples of interactions", "content": "Some image pairs generated by our proposed interaction algorithm are displayed in Figures 6 and 7. These images clearly demonstrate that the multimodal LLM can enhance image quality across various fundamental aesthetic aspects. The improvements in the basic aesthetic aspects include:\n\u2022 Lighting: Optimizes the effects of natural and artificial light, ensuring a balance of highlights and shadows.\n\u2022 Detail: Enhances subtle yet crucial elements of objects in the image, boosting realism and visual appeal.\n\u2022 Composition: Adjusts the relative positions of objects within the image, enhancing compositional effects and achieving balanced spatial aesthetics.\n\u2022 Ambiance: Optimizes the background and atmosphere of the image, creating an environment and mood that matches the theme.\n\u2022 Clarity: Improves overall clarity, reducing noise and blur.\n\u2022 Color: Adjusts temperature, saturation, and more, resulting in vibrant, harmonious colors while retaining the original scene's atmosphere.\nBeyond these fundamental aesthetic enhancements, our algorithm achieves more advanced effects, including but not limited to:\n\u2022 Particle Effects: Introduces dynamic or special effects, such as particle effects, to images.\n\u2022 Shooting Angle: Alters camera angles for a richer visual experience.\n\u2022 Exposure Compensation: Simulates realistic scenarios like a galaxy appearing with increased exposure.\n\u2022 Style Adjustment: Converts images to specific artistic styles to make them aesthetically pleasing.\n\u2022 Background Blur: Highlights main subjects while ensuring natural transitions in the background.\n\u2022 Color Gradient: Employs color gradients to smoothly transition between colors, resulting in a softer and more harmonious image.\nThese improvements highlight how well multimodal LLMs can enhance image aesthetics and adapt content and style to suit human preferences. The interactive algorithm effectively transfers the multimodal LLMs' understanding of aesthetics to the text-to-image model, thereby guiding the image generation process."}, {"title": "B. Prompt of Multimodal LLMs", "content": "The prompt used in Qwen2-VL-72B has undergone several iterations and extensive testing to ensure its effectiveness in guiding the model to generate enriched and aesthetically pleasing details in the image. This prompt is detailed as follows, where \"__prompt__\" denotes the original prompt of the text-to-image model.\nYou are a helpful assistant. Given the image please analyze the following image and complete the following tasks:\n1. Add more details to this image. For example, beautiful light and shadow, exquisite decorations, gorgeous clothing, beautiful natural landscapes, etc. Caution:\nThe added details should be consistent with the original description: _ prompt__\n2. Mark the locations where these details can be added. Caution:\nEach entity should have only a bounding box in the format (x1, y1, x2, y2] represented using absolute pixel coordinates."}, {"title": "C. Comparison of Different Multimodal LLMs", "content": "In the interactive algorithms", "follows": "nQwen2-VL-7B:\n1. The couple is beautifully illuminated by the warm glow of the full moon", "scene.\nLLaMa3.2-vision-90b-instruct": "n1. The couple's embrace is surrounded by a halo of soft", "VL2-26B": "n1. A luminous full moon with a detailed cratered surface, radiating a warm glow, casting a golden light across the scene. The stars twinkle softly in the night sky.\n2. Mountains with jagged peaks, their dark outlines contrasting against the twilight sky, casting deep blue shadows that blend into the cool night.\n3. A serene lake reflecting the full moon, mirroring the silhouettes of the mountains and adding a touch of tranquility with its still waters.\n4. A field of vibrant red flowers, their petals rich and lush, with leaves that add green highlights, creating a striking contrast to the dark silhouettes of the couple.\n5. A dense cluster of coniferous trees, their dark, spiky branches reaching upwards, standing still in the night breeze.\n6. The couple's outlines are refined with fluid lines, their forms bathed in the moonlight, emphasizing the romantic"}]}