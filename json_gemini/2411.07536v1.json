{"title": "Model Stealing for Any Low-Rank Language Model", "authors": ["Allen Liu", "Ankur Moitra"], "abstract": "Model stealing, where a learner tries to recover an unknown model via carefully chosen queries, is a critical problem in machine learning, as it threatens the security of proprietary models and the privacy of data they are trained on. In recent years, there has been particular interest in stealing large language models (LLMs). In this paper, we aim to build a theoretical understanding of stealing language models by studying a simple and mathematically tractable setting. We study model stealing for Hidden Markov Models (HMMs), and more generally low-rank language models.\nWe assume that the learner works in the conditional query model, introduced by Kakade, Krishnamurthy, Mahajan and Zhang [KKMZ24]. Our main result is an efficient algorithm in the conditional query model, for learning any low-rank distribution. In other words, our algorithm succeeds at stealing any language model whose output distribution is low-rank. This improves upon the result in [KKMZ24] which also requires the unknown distribution to have high \"fidelity\" a property that holds only in restricted cases. There are two key insights behind our algorithm: First, we represent the conditional distributions at each timestep by constructing barycentric spanners among a collection of vectors of exponentially large dimension. Second, for sampling from our representation, we iteratively solve a sequence of convex optimization problems that involve projection in relative entropy to prevent compounding of errors over the length of the sequence. This is an interesting example where, at least theoretically, allowing a machine learning model to solve more complex problems at inference time can lead to drastic improvements in its performance.", "sections": [{"title": "Introduction", "content": "Proprietary machine learning models are often highly confidential. Not only are their weights not publicly released, but even their architecture and hyperparameters used in training are kept a closely guarded secret. And yet these models are often deployed as a service, allowing users to make queries to the model and receive answers. These answers can take the form of labels or completions of prompts, and sometimes a model will even report additional information such as its confidence scores. This raises a natural question:\nQuestion. Are these black-box models actually secure, or is it possible to reverse engineer their parameters or replicate their functionality just from query access to them?\nThis task is called model stealing and it threatens the security of proprietary models and the privacy of data they are trained on. Beyond nefarious reasons, it can also be used in model distillation [MCH+21], where we have trained a large and highly complex model and we want to transfer its knowledge to a much smaller model. It can also be a useful tool for identifying vulnerabilities, as those are often inherited by stolen models.\nIn any case, model stealing continues to be a very active area of research. The influential work in [TZJ+16] showed that there are simple and efficient attacks on popular models like lo- gistic regression, decision trees and deep neural networks that often work in practice. Since then, many new attacks and defenses have been formulated [HLXS21, HJB+21, RST19, WG18, JSMA19, OMR23, OSF19, WXGD20]. There are also approaches based on embedding watermarks [JCCCP21, ZWL23, LZJ+22] that make it possible to detect when one model has been stolen from another. In recent years, there has been particular interest in stealing large language models (LLMs). Various works have shown how to steal isolated components of a language model such as the decoding algorithm [NKIH23], prompts used to fine-tune the model [SZ24], and even the entire weight matrix of the last layer (the embedding projection layer) [CPD+24].\nIn this work, our main interest will be in theoretical foundations for stealing language models. As is all too familiar, proving rigorous end-to-end guarantees when working with modern machine learning models with all their bells and whistles seems to be an extremely difficult task. For example, while we can understand the training dynamics on multilayer neural networks in terms of gradient flow in the Wasserstein space of probability distributions [MMM19, NP23], it has turned out to be quite difficult to analyze these dynamics except in simplified settings with a high degree of symmetry. Even worse, there are strong lower bounds for learning deep neural networks [KS09] even with respect to nice input distributions [GGJ+20, DKKZ20, GGK20]. The task of reasoning about modern language models seems no easier, as they are built on transformers [VSP+17] with many building blocks such as word embeddings, positional encodings, queries, keys, values, attention, masking, feed-forward neural networks and layer normalization.\nNevertheless there are often simplified models that abstract important features of more complex models and give us a sandbox in which to try to find theoretical explanations of empirical phenom- ena. For example, analyzing the dynamics of gradient descent when training a deep neural network is notoriously difficult. But in an appropriate scaling limit, and when the network is wide enough, it can be approximated through the neural tangent kernel [JGH18]. For recurrent neural networks, a popular approach is to analyze gradient descent on linear dynamical systems instead [HMR18]. Likewise for language models, it is natural to work with Hidden Markov Models (HMMs), which are in some sense the original language model, dating back to the work of Claude Shannon in 1951 [Sha51] and were the basis of other early natural language processing systems including the IBM"}, {"title": "Main Results", "content": "Formally, we view a language model as a distribution H over OT for some alphabet O and sequence length T. For simplicity, we treat the sequence length as fixed. Following Kakade, Krishnamurthy, Mahajan and Zhang [KKMZ24], the rank of the distribution generated by a language model is defined as follows:\nDefinition 1.1. [Low Rank Distribution] A distribution H over OT for alphabet O of size O and sequence length T is rank S if for all t < T, the OT\u2212t \u00d7 Ot matrix, M(t), with entries equal to $\\text{Pr}_H[f|h]$ for $f \\in O^{T-t}$ and $h \\in O^t$ has rank at most S.\nIn other words, a distribution is rank-S if for any t < T, the information in the prefix of length t can be embedded in an S-dimensional space such that the distribution of the future tokens can be represented as a linear function of this embedding. We note that low-rank distributions are expressive and encompass distributions generated by a Hidden Markov Model (HMM) with S hidden states (see Fact 2.2).\nNext, we formalize the setup for studying model stealing. We allow the learner to make con- ditional queries - that is, the learner can specify a history of observations, and then receives a random sample from the conditional distribution on the future observations. Formally, we have the following definition:\nDefinition 1.2 (Conditional Query). The learner may make conditional queries to a distribution H by querying a string $h \\in O^t$ where $0 < t < T$. Upon making this query, the learner obtains a string f of length $T - t$ drawn from the distribution $\\text{Pr}_H[f|h]$.\nIn this model, our goal is to design an algorithm that makes a total number of queries that is polynomial in S, O, T and learns an efficiently samplable distribution that is e-close in total variation distance to H. This conditional query model was recently introduced by Kakade, Krishnamurthy, Mahajan and Zhang [KKMZ24]. Their motivation was two-fold: First, while learning an HMM from random samples is known to be computationally hard [MR05], in principle one can circumvent these barriers if we are allowed conditional samples. Second, a solution to their problem would generalize Angluin's classic L* algorithm which learns deterministic finite automata from membership queries [Ang87]. In terms of results, Kakade, Krishnamurthy, Mahajan and Zhang [KKMZ24] introduced a notion that they called fidelity and gave a polynomial time algorithm to learn any low-rank distribution (and thus any HMM) which has high fidelity through conditional queries. However this property does not always hold. Thus, their main question still remains: Is it possible to learn arbitrary low-rank distributions through conditional queries? Here we resolve this question in the affirmative. We show:"}, {"title": "Discussion", "content": "Theorem 1.3 shows that we can efficiently learn any low-rank distribution via conditional queries. Thus, we can view our results as showing that in some sense, the rank of a distribution can be a useful proxy for understanding the complexity of model stealing, similar to how complexity measures, such as Bellman rank [JKA+17] and its relatives [FKQR21], are useful for understanding the statistical complexity of learning a near optimal policy in reinforcement learning.\nThere is a key conceptual insight driving our algorithm. One of the challenges in learning sequential distributions is that the error can grow exponentially in the sequence length T. In particular if we imagine sampling from H one token at a time, the low-rank structure ensures that we only need to keep track of an S-dimensional hidden state at each step. However, each step involves multiplication by a change-of-basis matrix and these repeated multiplications cause the error to grow exponentially. The key to mitigating this error blowup is that we combine each change-of-basis with a projection step, where we solve a convex optimization problem that performs a projection with respect to KL divergence. Crucially, projection in KL divergence has a contractive property (see Fact 3.9) that does not hold for other natural measures of distance between distributions, such as TV distance. We give a more detailed overview of our algorithm in Section 3. This is an interesting example where allowing a machine learning model to solve more complex problems at inference time can lead to drastic improvements in its performance. Of course, phrased in general terms, this is a driving philosophy behind OpenAI's ol model. But so far we have little theoretical understanding of the provable benefits of allowing more compute at inference time."}, {"title": "Related Work", "content": "There has been a long line of work on learning HMMs from random samples. Mossel and Roch [MR05] gave the first polynomial time algorithms that work when under appropriate full rankness conditions on the transition and observation matrices. Other works gave spectral [HKZ12] and method of moments based approaches [AHK12]. Learning HMMs can also be thought of as a special case of learning phylogenetic trees [CGG01, MR05]. Other approaches assume the output distributions belong to a parametric family [KNW13], or study quasi-HMMs [HGKD15].\nThe main computational obstruction in this area is that HMMs, without full rankness condi- tions, can encode noisy parities [MR05], which are believed to be computationally hard to learn. In the overcomplete setting, where the hidden state space is allowed to be larger than the observa- tion space, there are ways around these lower bounds. First, one can aim to predict the sequence of observations, rather than learning the parameters [SKLV18]. Under a natural condition called multi-step observability one can get quasi-polynomial time algorithms [GMR23]. Second, one can"}, {"title": "Basic Setup", "content": "Recall that we have conditional query access to some unknown distribution H over OT for some alphabet O. We also assume that H has rank S (recall Definition 1.1). Our goal will be to learn a description of H via conditional queries. Let us first formally define Hidden Markov Models (HMMs):\nDefinition 2.1 (Hidden Markov Model). An HMM H with state space S, |S| = S, and observation space 0, |0| = O, is specified by an initial distribution \u03bc over the states, a transition matrix T\u2208 RS\u00d7S, and an emission matrix O \u2208 RO\u00d7S. For a given sequence length T, the probability of generating a given sequence x1,...,xT with each xi \u2208 O is\n$\\Pr[x_1,...,x_T] = \\Sigma_{s_1,...,s_{T+1} \\in S^{T+1}} \\mu(s_1) \\Pi_{t=1}^T O_{x_t,s_t} T_{s_{t+1},s_t}.$\nNext we give a basic observation (see [KKMZ24]) that the class of rank S distributions contains the set of all distributions generated by an HMM with S hidden states.\nFact 2.2. [KKMZ24] Let H be a distribution over OT generated by an HMM with S hidden states. Then H is a rank at most S distribution.\nNotation Throughout this paper, we will use the same global notation for the unknown dis- tribution H with rank S, observation space O of size O and sequence length T. We also write $\\text{Pr}_H[f|h]$ for sequences f of length less than T \u2013 t. This will be used to denote the probability that conditioned on the prefix h, the observations immediately following h are those in f.\nWe use the following notation for prefixes and conditional probabilities. For a distribution H on OT and t < T, we use H[: t] to denote the distribution induced by H on t-character prefixes. For any prefix h\u2208 Ot, we use $\\text{Pr}_H[\\cdot|h]$ to denote the distribution on futures $f \\in O^{T-t}$ given by"}, {"title": "Technical Overview", "content": "Our algorithm is composed of two main parts: We work with a certain representation of the distribution and show how to estimate it from conditional samples. Then we give an algorithm that takes this learned representation and can generate samples. In most learning problems, estimating the parameters is the challenging part and sampling is often trivial. But in our case, designing the sampling algorithm, which involves solving a sequence of convex optimization problems, is one of the key ingredients."}, {"title": "Idealized Representation", "content": "In this section, we introduce the representation that will be central to our learning algorithm. First we consider an idealized setting where we ignore issues like sampling noise and the fact that we cannot afford to work with exponentially large vectors directly. We include this subsection for pedagogical reasons as many of the ideas are already in [KKMZ24]. Nevertheless, it will be useful setup for explaining our new contributions in the following subsections.\nWe slightly abuse notation and for h \u2208 Ot, let $\\text{Pr}_H[\\cdot|h]$ denote the vector whose entries are $\\text{Pr}_H[f|h]$ as f ranges over all elements of OT\u2212t. Now Definition 1.1 tells us that as h ranges over Ot, all of the vectors $\\text{Pr}_H[\\cdot|h]$ are contained in an S-dimensional subspace. Thus, we only need to store S of these vectors to \"span\" the whole space.\nBarycentric Spanners This leads us to the notion of a barycentric spanner, which is the key building block in the representation we will use:\nDefinition 3.1 (Barycentric Spanner). For a collection of vectors $z_1,..., z_n \\in \\mathbb{R}^d$, we say another set of vectors $v_1,..., v_s \\in \\mathbb{R}^d$ is a (C, \u03b3)-spanner for them if for all j \u2208 [n], there are coefficients $c_1,...,c_s$ with $|c_i| < C$ such that\n$|| z_j - (c_1v_1 + \\ldots + c_sv_s)||_1 \\leq \\gamma.$\nCrucially, for any set of vectors that are exactly contained in a S-dimensional subspace, there is always a subset of them that form a (1,0) barycentric spanner for the full collection.\nFact 3.2. Let $v_1,..., v_n$ be arbitrary vectors in Rd, Then there exists a subset S \u2286 [n] with |S| \u2264 d such that {vi}i\u2208S form a (1,0) spanner for the collection (v1, ..., vn).\nThus ideally, for each 0 < t < T, we could store a subset $H^{(t)} \\subseteq O^t$ with $|H^{(t)}| \\leq S$ and such that {PrH[\u00b7|h]}h\u2208H(t) is a (1,0)-barycentric spanner for {PrH[\u00b7|h]}h\u2208Ot.\nChange-of-Basis In addition to these spanning subsets, we will need to store some additional information about sampling probabilities and \"change-of-basis\" between these spanning sets. In particular we claim that if, in addition to the barycentric spanners, we also had the following information, then it would be enough to sample:\n\u2022 The next character probabilities (under H) for all of the elements of H(t)"}, {"title": "Technical Overview", "content": "As in the idealized sampling algorithm in Section 3.1, we sample a string x one character at a time. At each step t < T, we maintain a linear combination {a[t]}h\u2208H(t) of the strings h\u2208H(t) as in (2), except with conditional distributions with respect to H. We then sample a character and re-normalize as in (3). The main difficulty now is the change-of-basis step.\nNaive Attempt: Linear Change-of-Basis Naively, for each h\u2208 H(t) and o\u2208 O, we could simply pre-compute coefficients {\\beta_h^{h^{\\prime} \\vee o}}}_{h^{\\prime} \\in H^{(t+1)}}$ such that\n$\\mu hVo \\approx \\sum_{h^{\\prime} \\in H^{(t+1)}} \\beta_h^{h^{\\prime} \\vee o} \\mu h^{\\prime}$\nand thus (since the vectors un are representative for the distribution $\\text{Pr}_H[\\cdot|h]$),\n$\\Pr[h \\vee o] \\approx \\sum_{h^{\\prime} \\in H^{(t+1)}} \\beta_h^{h^{\\prime} \\vee o} \\text{Pr}_H[h^{\\prime}].$\nWe can then substitute this into the re-normalized relation (see (3)) and get\n$\\Pr[x \\vee q] \\approx \\sum_{h^{\\prime} \\in H^{(t+1)}} \\alpha^{h \\vee q} \\text{Pr}[\\cdot|h] , \\text{ where } \\alpha_h^{h \\vee q} = \\sum_{h \\in H^{(t)}} \\alpha^{h^{\\prime} \\vee o} \\beta_h^{h \\vee q} \\frac{\\Pr(o|h)}{\\Pr(o|x)}.$\nThis gives us an expression for $\\text{Pr}_H[\\cdot|x \\vee q]$ as a linear combination of the vectors $\\text{Pr}_H[\\cdot|h\u2032]$ for h\u2032 \u2208 H(t+1). However, the issue with this approach is that the coefficients and approximation error may grow exponentially with t.\nNeed for Projection The fact that the coefficients may grow motivates the need for a \"projec- tion\" step, where we reduce the magnitudes of the coefficients. One natural attempt is to take the coefficients {\u03b1xh \\vee o }h\u2032\u2208H(t+1) in (5) and \u201cproject\u201d them by finding an equivalent set of coefficients of bounded magnitude, say {\u03b1xh \\vee o }h\u2032\u2208H(t+1), such that"}]}