{"title": "Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation", "authors": ["Xiaoyu Wang", "Ningyuan Xi", "Teng Chen", "Qingqing Gu", "Yue Zhao", "Xiaokai Chen", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "abstract": "Large Language Models (LLM) are usually\nfine-tuned to participate in dyadic or two-party\ndialogues, which can not adapt well to multi-\nparty dialogues (MPD), which hinders their\napplications in such scenarios including multi-\npersonal meetings, discussions and daily com-\nmunication. Previous LLM-based researches\nmainly focus on the multi-agent framework,\nwhile their base LLMs are still pairwisely fine-\ntuned. In this work, we design a multi-party\nfine tuning framework (MuPaS) for LLMs on\nthe multi-party dialogue datasets, and prove\nsuch a straightforward framework can let the\nLLM align with the multi-party conversation\nstyle efficiently and effectively. We also de-\nsign two training strategies which can convert\nMuPaS into the MPD simulator. Substantial ex-\nperiments show that MuPaS can achieve state-\nof-the-art multi-party response, higher accu-\nracy of the-next-speaker prediction, higher hu-\nman and automatic evaluated utterance quali-\nties, and can even generate reasonably with out-\nof-distribution scene, topic and role descrip-\ntions. The MuPaS framework bridges the LLM\ntraining with more complicated multi-party ap-\nplications, such as conversation generation, vir-\ntual rehearsal or meta-universe.", "sections": [{"title": "Introduction", "content": "In recent years, large language model (LLM)\nhas demonstrated significant advancements in\ndyadic conversational contexts, such as question-\nanswering systems and chatbot companions. Such\napplications are primarily structured around binary\ndialogue attendants (typically \u2018human' and \u2018assis-\ntant'), which are supported by widespread open-\nsource models and datasets. However, many real-\nworld scenarios instead encompass the Multi-Party\nDialogues (MPD) 1, such as team meetings, class-\nroom discussions, court or academic debates, or\nsimply daily conversions with multiple humans in-\nvolved (Mahajan and Shaikh, 2021; Ganesh et al.,\n2023). Instead of responding to a single user's\nquery, in such case the dialog system needs to un-\nderstand conversation contexts from multiple users,\ndetermine whether to speak or not, and reasonably\nparticipate in potential multiple concurrent topics.\nNovel modeling technique is therefore required to\nadapted with this different dialogue paradigm.\nPrevious researches have sought to address the\nunique challenges of MPD modeling, such as\nMIDS (Yang et al., 2019), ChatMDG (Li et al.,\n2024), ReDE (Shen et al., 2023), SDMPED (Zhu\net al., 2022) and MPC-BERT (Gu et al., 2021).\nHowever, these works are mostly RNN, Bert or"}, {"title": "Problem Formulation", "content": "Naturally, an MPD sample is consisting of multiple\nroles and utterances. We further assume a scene de-\nscription can be constructed for an arbitrary MPD\nsample, which contains information on participat-\ning roles, the conversation topic, location or other\ncontexts2. Utterances appear in an interleaved man-\nner and belong to different roles. For simplicity, we\njust assume the adjacent utterances can not belong\nto the same role.\nAs the prerequisite of methodology derivation,\nhere we first propose some variable definitions, to\nformulate the MPD problem. Given a MPD sample,\nthere are maximally L roles and T utterances; we\nfurther assume s denotes the scene description, ut\ndenotes the content of the t-th utterance, while\nrt denotes the role index that the i-th utterance\nbelongs to:\n$r_t = r(u_t) \\in [0, \\dots, L - 1], t \\in [0, \\dots, T \u2013 1]$\nFor abbreviation, we use the following shortcut\nvariable to indicate the utterance sequence:\n${u}_{0:t} := \\{u_t, t \\in [0,\\dots,t]\\}$ (1)"}, {"title": "Method", "content": "In this section, we propose a straightforward but\neffective approach to employ the LLM to solve the\nMPD problem. We demonstrate the training and the\ninference details respectively, then provide further\nstrategies to convert the model to a MPD simulator.\nFigure 2 indicates our methodology details."}, {"title": "Training", "content": "Figure 2 (Left) visualizes the training methodology.\nSimilar to the conventional LLM training, logit of\nthe MPD textual input is obtained by a forward pass\nof LLM. For each role of the sample, we calculate\nits Supervised Fine-Tuning (SFT) loss by masking\nout the tokens' corresponding utterances of the\nsystem and all other roles 3. We average each role's\nloss to obtain the entire training loss:\n$L = \\frac{1}{L} \\sum_{i=1}^{L} \\sum_{t=1}^{T} log [P((\\{u\\}_{0:T}^{r=i}) |s, \\{u\\}_{0:T}^{r \\neq i})] $ (2)\nin which log [P(\u00b7)] indicates the log probability cal-\nculated by the current LLM, $\\{u\\}_{0:T}^{r=i}$ and $\\{u\\}_{0:T}^{r \\neq i}$ are\nabbreviations of the utterance sequence whether be-\nlongs to and not to the i-th role:\n${\\{u\\}}_{0:T}^{r=i} := \\{u_t, t \\in [0,\\dots,T] | r(u_t) = i\\}$ (3)\n${\\{u\\}}_{0:T}^{r \\neq i} := \\{u_t, t \\in [0,\\dots,T] | r(u_t) \\neq i\\}$ (4)"}, {"title": "Inference", "content": "During the inference stage, MuPaS is first assigned\nwith the current role, then generates its utterance\ngrounded by the system prompt and previous utter-\nances:\n$u_t \\leftarrow LLM(s, \\{u\\}_{0:t-1},r_t)$ (5)\nIf the active role has utterance1, it might be better to\nalso mask this part; however, here we just omit this detail for\ndemonstration clarity."}, {"title": "The MPD Simulator", "content": "A more interesting and intriguing application might\nbe the MPD simulation, where a series of speaking\nroles and their utterances are needed to generate se-\nquentially, with some pretended scene description\nand utterances. Such a simulator can be applied in\ndebate rehearsal, show script auto-writing, or meta-\nuniverse creation. Note this situation is different\nand more complicated than the inference stage in-\ntroduced in Subsection 3.2, where the speaking role\nis foreknown. To build a MPD Simulator, the next-\nspeaker prediction or recognition is also needed;\nand it is also important that the model can adapt\nwith some specific role description and portrays\ndifferent characteristics or personas.\nWe integrate the above tasks into a comprehen-\nsive task and find that the LLM fine-tuning frame-\nwork can handle it efficiently, with only minor\nmethodological revisions. Motivated by the differ-\nence between centralization and decentralization\narchitectures, we propose the Speaker Predictor\nand Silence Switcher strategies respectively, which\nare demonstrated in the following subsections."}, {"title": "Speaker Predictor", "content": "We re-paraphrase the next speaker role ($r_t$) as part\nof generation during inference, and correspond-\ningly unmask its loss during training. By such a"}, {"title": "Silence Switcher", "content": "In this strategy, the LLM is still grounded with\nthe current role but also allowed to possibly generate\n'<s>', a special token representing the 'silence'.\nThe simulator then becomes a multi-agent frame-\nwork where different LLMs (or one LLM with dy-\nnamically switching role prompts) portray different\nroles.\nUpon each utterance generation, we allow each\nLLM to speculate its possibility of 'silence', and\nchoose the one with the minimum likelihood as the\ncurrent speaker:\n$r_t = arg min log [P(<s> | s, \\{u\\}_{0:t\u22121}, r_t = i)]$ (7)\nThen the LLM is called again to generate the ut-\nterance content ut based on Equation 5, and the\ndialogue continues incrementally until the maxi-\nmum turn number is reached. We further summa-\nrize and compare details of the above two strategies\nby Algorithm 1 in Appendix B.1."}, {"title": "Experiments", "content": "In this section, we first provide the experimental\nsettings, then exhibit the training results, including\ndialogue generation and speaker prediction."}, {"title": "Settings", "content": "We collect substantial MPD datasets most of which\nbelong to two main categories: the show scripts\nand debates records. One can refer to Appendix\nA.1 for completed details. Among these, we di-\nvide the 'Friends' dataset into the training and test\ntest with the same split fraction as (Yang et al.,\n2019), such that some of their experimental results\ncan be directly compared. We also use the entire\n'Game of Thrones' dataset as the test test, to test\nthe zero-shot ability. We further illustrate the ex-\nperimental details to test different aspects of model\ncapabilities:\n\u2022 Test: We select the scene description and the\nfirst utterance of each sample of the Friends\ntest test, and let the model to extend the MPD\nby generating more utterances.\n\u2022 Generalization: We manually write the scene\ndescription and the first utterance according to\nthe Friends scenario; since the model already\nlearn the roles' characteristic and talking cor-\npus through the training dataset, this approach\ntest the model completion ability given arbi-\ntrary scene and previous utterances.\n\u2022 Zero-Shot: we select the beginning utterances\n(maybe 2~3) of the Game of Thrones (GOT)\nsamples (not covered by the training set) and\nmanually write descriptive scenes. This ap-\nproach test the model's zero-shot ability given\nunseen role definitions and utterances."}, {"title": "Baselines", "content": "\u2022 Previous non-LLM based works on MPD,\nsuch as MIDS (Yang et al., 2019), SI-RNN\n(Zhang et al., 2017) and Static/Dynamic-ADR\n(Ouchi and Tsuboi, 2016).\n\u2022 The prompt-based approach. We achieve so\nby converting the MPD problem in to a single-\nturn instruction following task, in which we\nconcatenate historical utterances into a single\nuser query, and write an extra instruction to\nlet LLM to generate MPD response grounded\nby multi-party history.\n\u2022 The vanilla SFT method (VanillaSFT) on\nLLM which also concatenates historical utter-\nances as the query, and labels the ground-truth\nutterance as the target text.\nFor LLM-based baselines, we examine Qwen2\nInstruct (Qwen Team, 2024), Llama3 Instruct\n(AI@Meta, 2024), Deepseek-v2 (DeepSeek-AI,\n2024) and GPT-4 (Team, 2024). Qwen2-7B-\nInstruct and Llama3-8B-Instruct are also set as our\nbase model. We experiment with MuPaS-Speaker\nand MuPaS-Switcher corresponding to the Speaker\nPredictor and the Silence Switcher strategies pro-\nposed in Section 3.3."}, {"title": "Results", "content": "Figure 3 presents the loss curves for the Speaker\nPredictor and Silence Switcher methods in MuPaS.\nInitially, both approaches exhibit high loss as the\ninstruction-based LLM transitions from a two-party"}, {"title": "Quantitative Results", "content": "Table 1 list the quality assessment results of MPD\nresponses, by the manners of LLM auto-evaluation\nand human annotation.In this evaluation, the au-\ntomatic assessment of the model utilizes the ad-\nvanced GPT-4, which assigns scores ranging from\n0 to 10 based on the fluency of the dialogue and\nadherence to the character traits established in\nFriends. The prompt used for this evaluation is\nprovided in the Appendix C.4. Additionally, the\nmanual assessment is conducted by trained pro-\nfessionals and individuals with prior experience\nwatching Friends. They evaluate the dialogue on\nthree criteria: Fluency, Consistency, and Interest-\ningness, with each criterion having a maximum\nscore of 10. Detailed scoring criteria are also in-\ncluded in the Appendix C.5.\nOur model outperforms other baselines on both\nassessment approaches. While the automatic evalu-\nation by GPT-4 considers its own dialogue genera-\ntion quality to be the best, our method still achieves\nthe second-highest score. In contrast, human evalu-"}, {"title": "Results of Multi-Party Simulation", "content": "We employ the trained model to build a MPD sim-\nulator. We examine the performance of MuPaS\nacross different configurations proposed in Sub-\nsection 4.1, and observe reasonable performance.\nTable 2 exhibits both typical test-set and general-\nization cases. MuPaS can produce fluent, consis-\ntent and high-quality utterances, no matter that the\nscene description and previous utterances are sam-\npled from test set directly or written manually, in-\ndicating its high robustness. Furthermore, it can be\nobserved that each utterance's style matching the\nrole's characteristics. For example, Ross is more\nprone to talk about academic topic while Monica\ncares about the fairness.\nWe observe astonishing results for zero-shot ex-\nperiments, in which the entire set of story contexts\nand role styles have not been studied by the model\nfine-tuning 4. In this case, we manually input more\ncontents of scene description which including the\nrole introductions, their location, topics and other\ncontexts ('Jon Snow are preparing to tell Daen-\nerys his true identity'). MuPaS generates fluent\nand interesting dialogues between 'Jon Snow' and\n'Daenerys' even it does not know well about them\nbefore prompted. Table 9 provides another case\nwhich include more than two roles (\u2018Tywin Lannis-\nter, Tyrion Lannister, Varys and Petyr Baelish are\nhaving a council meeting') in the Appendix D."}, {"title": "Speaker Prediction Accuracy", "content": "Table 4 shows the speaker prediction accuracy on\nthe Friends test set. It can be observed that meth-\nods relying on LLM prompting generally achieve\nrelatively low accuracy, ranging from 61.49% for\nDeepseek-v2 to 72.47% for GPT-4. After applying\nVanilla Supervised Fine-Tuning, there is a notice-\nable improvement in accuracy. In addition, tradi-\ntional approaches that rely on multi-party dialogue\nmodeling tend to perform better in this task surpris-\ningly, as they are specifically designed and trained\nto handle the final round of dialogue. Nevertheless,"}, {"title": "Ablation Study", "content": "To investigate the impact of different model com-\nponents on overall performance, this section ex-\nplores the effects of modifying the conditions for\nthe speaker and silence models. The following\napproaches are employed:\n\u2022 One role/sample: For each data instance, only\none speaker's utterances are randomly se-\nlected for training, allowing for an analysis\nof how different MPD learning strategies af-\nfect the training process.\n\u2022 Without scene: The system prompt descrip-"}, {"title": "Related Work", "content": "In this section, we review previous works and the\nrecent progress of LLM on multi-party dialogues."}, {"title": "Modeling on Multi-Party Dialogue", "content": "Recent research has sought to address the unique\nchallenges of MPD modeling. For instance, the\nMIDS (Yang et al., 2019) framework captures\nspeaker roles and content information through a\nrole-defining encoder and an attention-enhanced\nencoder. Other models, such as ChatMDG (Li et al.,\n2024), leverage graph neural networks to model the\ninteractions, while ReDE (Shen et al., 2023) uses\nrelative dependency encoding to better capture the"}, {"title": "Multiple LLM-Agent Conversations", "content": "There are also some efforts to apply LLM on\nmulti-agent conversations. Such approach employs\nLLM in a traditional user-assistant (or instruction-\nresponse) manner and aim to solve other tasks.\nFor example, LLM-debate (Yung-Sung Chuang,\n2023) and Agent4Detate (Zhang et al., 2024) let"}, {"title": "Conclusion", "content": "In this paper, we propose a novel LLM-based train-\ning paradigm called MuPaS, to encompass the\nmulti-party dialogue generation. The paradigm\nis straightforward and easy to understand, but is\nproved to be effective and efficient to allow LLM\nprovide reasonable response grounded by context\nof multiple roles, instead of the traditional user-\nassistant chatting scenario. Our methodology out-\nperforms LLM-based baselines or previous multi-\nparty chatting models on the response quality, and\nalso has higher next-speaker prediction accuracy.\nWe validate MuPaS can also be a good basis of\nmulti-party dialogue simulator with substantial typ-\nical cases provided."}, {"title": "Limitation", "content": "MuPaS is trained with general MPD datasets, and\ncurrently do not cover multimodal or multi-thread\ntopics. The MPD scenario can be classified into\nscripted (such as show, movie scripts) and un-\nscripted (such as daily and open-domain conver-\nsations), while we provide a general training-based\nand data-driven solution, while do not study these\nscenario differences.\nOur work propose an academic solution to gen-\nerate MPD while it does have the possibility that\nour MuPaS can be used to create fake and fraud\nstories. Such application should be prohibited."}, {"title": "Datasets", "content": "We collect substantial MPD datasets which can\nbe classified into two main categories: the show\nscripts and debates records. To align with the ex-\nperimental settings of (Yang et al., 2019), we also\ndivide the 'Friends' dataset into the training and\ntest test with the same split fraction. We also use\nthe entire 'Game of Thrones' dataset as the test test.\nAll other datasets are part of training set. We sum-\nmarize the statistics and configuration of training\ndatasets in Table 6. We limit each sample con-\ntains mostly 10 utterances and divide the clip into\nmultiple parts which is longer than that."}, {"title": "Scene Description", "content": "Our Default scene description can be as follows:\n'You are participating in a multi-role conversation\ncomposed of A, B, C.... 'which is applied when\nthere is no special annotation or extra information\nin the original dataset."}, {"title": "Training Data Format", "content": "Starting from the OpenAI ChatCompletion prompt,\nwe re-define the original roles (system, user, re-\nsponse) with the list of MPD roles. Below is the\nresulting prompt format:"}, {"title": "Extra Details in Approaches", "content": "Algorithm 1 summarizes more details about our\nsimulation strategies."}, {"title": "Prompt Template of zero-shot Baseline", "content": "You are participating in a multi-role conversation\ncomposed of\nYou are playing the role of\nAccording to the dialogue content, predict what\nthe role should say. The output shouldn't contain\nthe role's name."}, {"title": "Prompt Template of Fine-Tuning Baseline", "content": "You are participating in a multi-role conversation\ncomposed of\nPlease provide an appropriate response of"}, {"title": "Extra Implementation Details", "content": "The learning rate is 5.0e - 6, the training batch size\nis 32 and the sequence window length is 2048. The\ntraining epoch is set to 2. We perform the training\nexperiment in LlamaFactory (Zheng et al., 2024),\nrunning by 8 A100 GPUs. We use the AdamW\noptimizer with the cosine scheduler of learning rate\nand decay of 0.01. We first train the model with\nsome open-domain dialogue and reasoning datasets"}]}