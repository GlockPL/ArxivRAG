{"title": "Lessons Learned from a Unifying Empirical Study of Parameter-Efficient Transfer Learning (PETL) in Visual Recognition", "authors": ["Zheda Mai", "Ping Zhang", "Cheng-Hao Tu", "Hong-You Chen", "Li Zhang", "Wei-Lun Chao"], "abstract": "Parameter-efficient transfer learning (PETL) has attracted significant attention\nlately, due to the increasing size of pre-trained models and the need to fine-tune\nthem for superior downstream performance. This community-wide enthusiasm\nhas sparked a plethora of new approaches. Nevertheless, a systematic study to\nunderstand their performance and suitable application scenarios is lacking, leav-\ning questions like \u201cwhen to apply PETL\" and \"which approach to use\u201d largely\nunanswered, especially in visual recognition. In this paper, we conduct a unifying\nempirical study of representative PETL approaches in the context of Vision Trans-\nformers (ViT). We systematically tune their hyper-parameters to fairly compare\ntheir accuracy on downstream tasks. Our study not only offers a valuable user\nguide but also unveils several new insights. First, if tuned carefully, different\nPETL approaches can obtain quite similar accuracy in the low-shot benchmark\nVTAB-1K. This includes simple approaches like fine-tuning the bias terms that\nwere reported inferior. Second, though with similar accuracy, we find that PETL\napproaches make different mistakes and high-confidence predictions, likely due\nto their different inductive biases. Such an inconsistency (or complementariness)\nopens up the opportunity for ensemble methods, and we make preliminary attempts\nat this. Third, going beyond the commonly used low-shot tasks, we find that PETL\nis also useful in many-shot regimes \u2014 it achieves comparable and sometimes better\naccuracy than full fine-tuning, using much fewer learnable parameters. Last but not\nleast, we investigate PETL's ability to preserve a pre-trained model's robustness\nto distribution shifts (e.g., a CLIP backbone). Perhaps not surprisingly, PETL ap-\nproaches outperform full fine-tuning alone. However, with weight-space ensembles,\nthe fully fine-tuned model can achieve a better balance between downstream and\nout-of-distribution performance, suggesting a future research direction for PETL\u00b9.", "sections": [{"title": "1 Introduction", "content": "Pre-training and then fine-tuning has become the standard practice to tackle visual recognition\nproblems [7]. The community-wide enthusiasm for open-sourcing has made it possible to access large,\npowerful pre-trained models learned from a gigantic amount of data (e.g., ImageNet-21K [13, 77] or\nLAION-5B [79]). More research focus has thus been on how to fine-tune such large models [15, 96].\nAmong existing efforts, parameter-efficient transfer learning (PETL), a.k.a parameter-efficient fine-\ntuning (PEFT), has attracted increasing attention lately [25, 15]. Instead of fine-tuning the whole\nmodel (i.e., full fine-tuning) or the last fully connected layer (i.e., linear probing), PETL approaches\nseek to update or insert a relatively small number of parameters to the pre-trained model [94].\nDoing so has several noticeable advantages. First, as named, PETL is parameter-efficient. For one"}, {"title": "2 Background", "content": "2.1 Large pre-trained models\nPre-trained models have become an indispensable part of modern AI development [7]. Building upon\nneural networks with millions if not billions of parameters and gigantic amounts of training data, these\nlarge pre-trained models have led to groundbreaking results in various downstream tasks [54, 63, 68]\nand shown several emerging capabilities not observed previously [44, 51, 7]. For example, in\ncomputer vision, a Vision Transformer (ViT) [17] trained with ImageNet-21K (around 14M images)\nleads to consistent gains v.s. a ViT trained with ImageNet-1K (around 1.3M images) [17]. ViTs\npre-trained with millions of image-text pairs via a contrastive objective function (e.g., a CLIP-ViT\nmodel) [73, 11] show an unprecedented zero-shot capability and robustness to distribution shifts [73].\nIn this paper, we focus on the ImageNet-21K-ViT and use the CLIP-ViT in the robustness study.\nVision Transformer (ViT). We briefly review ViTs [17], which are adapted from the Transformer-\nbased models [87] in NLP [91]. ViTs divide an image $I$ into a sequence of $N$ fixed-sized patches\n${I^{(n)}}_{n=1}^{N}$ and treat them like NLP tokens. Each patch is first embedded into a $D$-dimensional vector\n$x^{(n)}$ with positional encoding. The sequence of vectors is then prepended with a \u201cCLS\u201d vector\nto generate the input $Z_0 = [x^{(Class)}, x^{(1)},...,x^{(N)}] \\in \\mathbb{R}^{D\\times(1+N)}$ to a ViT, composed of\n$M$ Transformer layers. We use super-/sub-script to index token/layer. The output vector $x_M^{(Class)}$\ncorresponding to the \u201cCLS\u201d token is used as the image representation. For classification tasks, $x_M^{(Class)}$\nis fed into a fully connected (FC) layer for the final prediction.\nEach of the ViT's $M$ Transformer layers consists of a multi-head self-attention (MSA) block, a\nmulti-layer perceptron (MLP) block, two Layer Normalization (LN) blocks [3], and two residual\nlinks. The $m$-th Transformer layer can be formulated as"}, {"title": "2.2 Parameter efficient transfer learning (PETL)", "content": "Fine-tuning is arguably the most common way to tailor a pre-trained model for downstream tasks. As\nthe size of pre-trained models gets larger, copying and updating all the parameters for one downstream\ntask becomes inefficient. PETL has thus emerged as a promising paradigm.\nPETL was originally developed in NLP [26, 49, 30, 66, 82, 98, 2, 88, 56, 81, 103] and has attracted in-\ncreasing attention in vision [39, 9, 40, 102, 57, 53]. Existing approaches can generally be categorized\ninto four groups: prompt-based, adapter-based, direct selective parameter tuning, and efficient selec-\ntive parameter tuning. We focus on visual recognition and compare representative PETL approaches\napplicable to ViTs. During fine-tuning, all approaches learn a new FC layer for prediction.\nPrompt-based approaches. Prompt-based learning emerged in NLP [55, 52]. The core concept is to\naugment the input data with task-specific hints (prompts). Visual Prompt Tuning (VPT) [39] adapts\nsuch an idea to ViTs. Specifically, its deep version (VPT-Deep) prepends a set of soft prompts to\nthe input tokens of each Transformer layer (i.e., ${Z_m}_{m=0}^{M-1}$) and only optimizes the prompts during\nfine-tuning. Other representative works in this category include [97, 86, 24].\nAdapter-based approaches. This category typically introduces additional trainable parameters (e.g.,\nan MLP block) to the frozen pre-trained model [52]. It was initially developed for multi-domain\nadaptation [74, 75] and continual learning [78, 64], and was subsequently extended to the NLP and\nvision domains to adapt Transformer-based models [33, 97].\nWe consider five popular adapter-based methods. Houl. Adapter [33] is the first adapter-based\nPETL approach. It inserts two Adapters a two-layer bottleneck-structured MLP with a residual\nlink into each Transformer layer, one after the MSA block and the other after the MLP block.\nPfeif. Adapter [72] inserts the Adapter solely after the MLP block, a strategy shown effective in\nrecent studies [34]. AdaptFormer [9] inserts the Adapter in parallel with the original MLP block\nin a Transformer layer, different from the sequential design of Houl. and Pfeif. Adapter. One can\nview it as an ensemble, summing the task-specific features (by the Adapter) and the task-agnostic\nfeatures (by the original MLP) to form $Z_m$ in Equation 2. ConvPass [40] introduces a convolutional-\nbased bottleneck module (without a skip link) that explicitly encodes visual inductive biases: the\n2D convolution is performed over tokens of nearby patches. The module is inserted in parallel\nwith the MSA and/or MLP block. RepAdapter [61] introduces a linear Adapter with group-wise\ntransformations [62] and sequentially inserts two such modules after both MSA and MLP blocks.\nDirect selective parameter tuning. This category selectively updates a subset of parameters of\nthe pre-trained model, seen as a trade-off between full fine-tuning and linear probing. We consider\nthree approaches. BitFit [98] updates the bias terms, including those in the Q/K/V projections, the\nMLP blocks, the LN blocks, and the projection for patch embeddings. LayerNorm [5] updates\nthe trainable parameters of the LN blocks in each Transformer layer. DiffFit [93] updates both the\nbias terms and the LN blocks and inserts learnable factors to scale the features after the MSA and\nthe MLP blocks. Instead of updating parameters, SSF [53] linearly adapts intermediate features,\nmotivated by feature modulation [36, 71]. For an intermediate feature $Z \\in \\mathbb{R}^{D\\times(N+1)}$, SSF learns a\n$D$-dimensional scaling vector and a $D$-dimensional additive vector broadcasting to the tokens.\nEfficient selective parameter tuning. Unlike the above category which directly updates parameters,\nthis category learns additive residuals (e.g., $ \\triangle W$) to the original parameters (e.g., $W$). By injecting\na low-rank constraint to the residuals, this category effectively reduces the learnable parameters,\nsuitable for updating parameter-heavy components like projection matrices. LoRA [34] is arguably"}, {"title": "3 Results on Low-Shot Regimes", "content": "Pre-trained models are meant to ease downstream applications. One representative scenario is low-\nshot learning: supervised fine-tuning with a small number of examples per class. Indeed, low-shot\nlearning has been widely used to evaluate PETL performance.\nDataset. VTAB-1K [99] consists of 19 image classification tasks from three groups. The Natural\ngroup comprises natural images captured with standard cameras. The Specialized group contains\nimages captured by specialist equipment for remote sensing and medical purposes. The Structured\ngroup evaluates the scene structure comprehension, such as object counting and 3D depth estimation.\nFollowing [99], we perform an 80/20 split on the 1000 training images in each task for hyperparameter\nsearching. The reported result (top-1 classification accuracy) is obtained by training on the 1000\ntraining images and evaluating on the original test set. We provide more detail in the Appendix.\nMethods. We consider linear probing, full fine-tuning, and 13 PETL methods including 1 prompt-\nbased [39], 5 adapter-based [33, 72, 9, 40, 61], 4 Direct selective [98, 5, 93, 53], and 3 Efficient\nselective [34, 41]. Please refer to subsection 2.2 for details.\nSetup. We employ the ViT-B/16 model [17] pre-trained on ImageNet-21K [13] as the backbone.\nThe prediction head is randomly initialized for each dataset. Images are resized to $224 \\times 224$. We\nperform no data augmentation and normalization during training, following [99]. (The exception is\nCIFAR-100 [46], on which normalization is necessary.) We systematically tune 1) learning rate, 2)\nweight decay, and 3) approach-specifics like the size of PETL parameters which are often left intact.\nWe set a cap for 3), $ \\leq $ 1.5% of ViT-B/16. We also turn the drop path rate on (e.g., 0.1) or off (i.e., 0).\nA detailed hyper-parameter search grid and additional training details are provided in the Appendix."}, {"title": "4 PETL Approaches Offer Complementary Information", "content": "The previous section shows all PETL methods achieve similar accuracy in various domains. Does\nit mean all PETL methods learn similar knowledge from the downstream data? As different PETL"}, {"title": "5 Results on Many-Shot Regimes", "content": "Recent works in NLP [8] have indicated that\nPETL methods may not perform as competi-\ntively as full fine-tuning when data is abundant.\nWe thus aim to investigate PETL's performance\nin many-shot regimes by addressing the follow-\ning questions: (1) Should we use PETL or full\nfine-tuning when data is sufficient? (2) How\nshould we adjust the number of trainable param-\neters for PETL methods in many-shot regimes?\nDataset. We select one representative dataset\nfrom each of the natural, specialized, and struc-"}, {"title": "6 Why Do PETL Methods Work?", "content": "Within 19 VTAB-1K tasks, we see two cases: (1) Full fine-tuning outperforms linear probing. Since\nlinear probing reflects the pre-trained feature quality for downstream tasks, case (1) suggests the\nnecessity to update the backbone to address the gap between pre-trained and downstream domains.\n(2) Linear probing surpasses full fine-tuning, suggesting the pre-trained features are good enough\n(at least in a low-shot scenario). Recklessly updating them with limited data may risk over-fitting.\nFigure 7 (a-b) summarizes the low-shot accuracy comparison based on the categorization above.\nSpecifically, each line corresponds to one task. Linear probing, PETL, and fine-tuning are located in\norder, from left to right, to reflect their tunable parameter sizes. PETL's superior performance in both\ncases showcases its capacity to learn and its regularization role to prevent over-fitting.\nWe also draw the many-shot accuracy in Figure 7 (c-d) based on the same categorization: RESISC and\nClevr in case (1), and CIFAR-100 in case (2) . In the many-shot setting, full fine-tuning consistently\noutperforms linear probing, which seems to suggest no more risk of over-fitting. However, on CIFAR-"}, {"title": "7 Empirical Results on Robustness to Distribution Shifts", "content": "Large pre-trained models such as CLIP [73] and ALIGN [38] have demonstrated unprecedented\naccuracy across a range of data distributions when performing zero-shot inference. However, recent\nstudies [92, 73] have shown that fine-tuning on downstream data, while significantly boosting\nperformance on the target distribution, often compromises the model's robustness to distribution\nshifts. Given that PETL only updates a limited number of parameters in the model, we investigate\nwhether PETL can offer a more robust alternative to full fine-tuning for pre-trained models.\nDataset. We use 100-shot ImageNet-1K as our target distribution, with each class containing 100\nimages. Following [92], we consider 4 natural distribution shifts from ImageNet: ImageNet-V2 [76],\na new ImageNet test set collected with the original labeling protocol; ImageNet-R [31], renditions for\n200 ImageNet classes; ImageNet-S [22], sketch images for 1K ImageNet classes; ImageNet-A [32],\na test set of natural images misclassified by a ResNet-50 [27] for 200 ImageNet classes.\nSetup. We focus on the CLIP ViT-B/16 model, which comprises a visual encoder and a text encoder,\npre-trained via contrastive learning on image-text pairs. Following [92], we add an FC layer as the\nprediction head with zero-initialized bias and initialize weights using the class label text embedded\nby the text encoder. Subsequently, we discard the text encoder and apply PETL methods to the visual\nencoder, fine-tuning only the PETL modules and the head.\nResults. As shown in Table 2, while some PETL methods may not surpass full fine-tuning on the\ntarget distribution, they consistently demonstrate more robust performance on OOD data. This is\nlikely because PETL updates only a small fraction of the parameters, thus preserving the robust\nfeatures of the foundation models. Given the similar target distribution performance, should we\nblindly use PETL methods for more robustness?\nWeight-space ensembles (WiSE) for PETL. WiSE [92], which linearly interpolates the full fine-\ntuned and original models, is a popular fine-tuning approach to enhance robustness. We explore\nwhether WiSE can enhance the robustness of PETL. To apply WiSE to PETL, we first linearly\ninterpolate the prediction head with a mixing coefficient $ \\alpha $. For direct selective tuning methods\n(e.g.BitFit), we directly interpolate with the original model. Since most Adapter-based methods\nhave residual connections, we can multiply the adapter modules with $ \\alpha $ to control their strengths.\nA similar approach can be applied to efficient selective methods (e.g.LoRA) as they learn additive\nresiduals to the original parameters. As shown in Figure 1b, WiSE improves both the robustness\nand the in-distribution performance of PETL methods. Interestingly, even though full fine-tuning is\ngenerally less robust than PETL methods, applying WiSE allows it to achieve better performance in\nboth in-distribution and OOD data, which suggests a promising research direction for robust PETL."}, {"title": "Appendix", "content": "We provide details that are omitted from the main paper.\n\u2022 Appendix A: Experiment and dataset details\n\u2022 Appendix B: Detailed descriptions of ViT and compared methods.\n\u2022 Appendix C: Additional results not presented in main paper\n\u2022 Appendix D: Discussion about further impacts of this work"}, {"title": "A Experiment and Dataset Details", "content": "A.1 Experiment Details\nThroughout all experiments, we employ AdamW optimizer [60] with a batch size of 64 and utilize\nthe cosine decay learning rate scheduler. We train all methods with 100 epochs for VTAB-1K and\n40 epochs for many-shot experiments. For the low-shot experiments, the learning rate is tuned from\n[1e-3, 1e-2] and weight decay from [1e-4, 1e-3]. The method-specific hyperparameter tuning grip is\nshown in Table 3 with the tunable parameter range in millions. For the many-shot experiment, the\nlearning rate is tuned from [5e-4, 1e-3] and weight decay keeps the same range of [1e-4, 1e-3]. For\nthe robustness experiment, following [92], we set a small learning rate as 3e-5 and weight decay as\n5e-3. We use a strong data augmentation following [102].\nWe used a workstation with eight NVIDIA RTX 6000 Ada GPUs, two AMD EPYC 9554 64-Core\nProcessors, and 800GB of RAM.\nA.2 Dataset Details\nVTAB-1K The processed VTAB-1K can be downloaded from our official code base to ensure\nreproducibility.\nMany-shot Datasets We perform 90/10 train-val split for CIFAR-100, RESISC and Clevr-Distance.\nThe split details are provided in our code base for reproducibility. We apply horizontal flipping for\nCIFAR100, horizontal and vertical flipping for Resisc, and no augmentation for Clevr. All data are\nnormalized by ImageNet mean and standard deviation."}, {"title": "B Background", "content": "B.1 Vision Transformer\nOverview of ViT. Inspired by the recent success of Transformer-based models [87] in NLP [91],\nVision Transformer (ViT) [17] has become widely used in computer vision. To handle 2D images,\nViT divides an image $I \\in \\mathbb{R}^{H\\times W \\times C}$ into $N$ non-overlapping patches ${I^{(n)} \\in \\mathbb{R}^{P^2\\times C}}_{n=1}^{N}$, where\n$(H, W)$ is the resolution of the input image, $C$ is the number of channels, $N = HW/P^2$ and $(P, P)$ is\nthe resolution of each patch. Each patch $I^{(n)}$ is flattened and embedded into a $D$-dimensional vector\n$x^{(n)}$ with a trainable linear projection. Incorporating the BERT design approach [43], a \"Class\u201d token\n$x^{(Class)}$ is prepended to the sequence of embedded patches, whose output state at the last Transformer\nlayer is utilized as the image representation. Finally, position embeddings $E_{pos} \\in \\mathbb{R}^{D\\times(1+N)}$ are\nadded to preserve positional information and form the input $Z_0 \\in \\mathbb{R}^{D\\times(1+N)}$ to the ViT, which can\nbe formulated by:\nB.3 Prompt-based Methods\nPrompt-based learning emerged in NLP as an effective approach to adapt pre-trained models for\ndownstream tasks [55, 52]. The core concept involves augmenting the model input with task-specific\nhints (prompts), which aid the pre-trained model in addressing novel tasks with its existing knowledge.\nHard prompts are human-interpretable natural language hints, encompassing task instructions, in-\ncontext examples, or supporting information. Alternatively, soft prompts are continuous vector hints\nthat are incorporated into the input embeddings of the input layers or hidden states of other layers.\nSoft prompts are updated during the fine-tuning process using gradient-based methods, guided by\nthe downstream task-specific loss functions, while the pre-trained model itself remains fixed. The\nsplendent success of prompts in NLP has sparked a growing interest in adopting it in computer\nvision [97, 86] and multi-modal domains [24].\nIn this paper, we investigate a prominent and strong prompt-based method called Visual Prompt\nTuning (VPT) [39], which represents one of the early endeavours in introducing prompts to computer\nvision. Specifically, VPT-Shallow adds $l$ prompts $P_0 \\in \\mathbb{R}^{l\\times D}$ to the input of the first Transformer\nlayer $Z_0$ and the output $P_0$ of $P_0$ serves as the input for the next layer as depicted in Equation 11.\nVPT-Shallow can be perceived as the addition of learnable pixels to the original images. On the other\nhand, VPT-Deep inserts $l$ prompts ${P_m \\in \\mathbb{R}^{l\\times D}}_{m=0}^{M}$ to the input of every Transformer layer $Z_m$\nbut their outputs are discarded at the end of the layer as illustrated in Equation 12.\nB.4 Adapter-based Methods\nAdapter-based methods typically introduce additional trainable parameters into a frozen pre-trained\nmodel to facilitate learning of downstream tasks [52]. Initially developed for multi-domain adap-\ntation [74, 75] and continual learning [78, 64], the idea of Adapters is subsequently embraced by\nHoulsby et al. [33] in the NLP domain to adapt Transformer-based networks for downstream tasks,\nand it also has garnered increasing interest in the computer vision field [97]. In this comparative\nanalysis, we concentrate on five popular Adapter-based methods, encompassing the original Adapter,\nalong with variants focusing on adjusting the positions of Adapters [9, 72], introducing visual in-\nductive biases [40], as well as employing re-parameterization to reduce the number of trainable\nparameters and inference latency [61].\nHoul. Adapter [33] inserts two lightweight bottleneck-structured modules into each Transformer\nlayer: one after the MSA block and the other after the MLP block. As depicted in Figure 9a, the\nAdapter is composed of a down-projection layer with $W_{down} \\in \\mathbb{R}^{r\\times D}$, a nonlinear activation function\n$ \\sigma $, an up-projection layer with $W_{up} \\in \\mathbb{R}^{D\\times r}$, a scaling factor $s$ and a skip-connection. To limit the\nB.5 Selective Parameter Tuning Methods\nThe methods falling within this category aim to selectively update the parameters of a pre-trained\nmodel for downstream tasks. Within transfer learning, two prominent strategies, namely full fine-\ntuning and linear probing [45, 105], represent the two extremes of this category. Full fine-tuning\nupdates all the model parameters end-to-end based on the new dataset while linear probing treats\nthe pre-trained model as a feature extractor and only updates the prediction heads while keeping\nthe backbone frozen. Although full fine-tuning generally exhibits superior performance compared\nto linear probing [99], it possesses certain limitations that may hinder its practicality in real-world"}]}