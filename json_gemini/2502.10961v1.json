{"title": "Graders should cheat: privileged information enables expert-level automated evaluations", "authors": ["Jin Peng Zhou", "S\u00e9bastien M. R. Arnold", "Nan Ding", "Kilian Q. Weinberger", "Nan Hua", "Fei Sha"], "abstract": "Auto-evaluating language models (LMs), i.e., using a grader LM to evaluate the candidate LM, is an appealing way to accelerate the evaluation process and the cost associated with it. But this presents a paradox: how can we trust the grader LM, which is presumably weaker than the candidate LM, to assess problems that are beyond the frontier of the capabilities of either model or both? For instance, today's LMs struggle on graduate-level physics and Olympiad-level math, making them unreliable graders in these domains.\nWe show that providing privileged information \u2013 such as ground-truth solutions or problem-specific guidelines \u2013 improves automated evaluations on such frontier problems. This approach offers two key advantages. First, it expands the range of problems where LMs graders apply. Specifically, weaker models can now rate the predictions of stronger models. Second, privileged information can be used to devise easier variations of challenging problems which improves the separability of different LMs on tasks where their performance is generally low. With this approach, general-purpose LM graders match the state of the art performance on RewardBench, surpassing almost all the specially-tuned models. LM graders also outperform individual human raters on Vibe-Eval, and approach human expert graders on Olympiad-level math problems.", "sections": [{"title": "1. Introduction", "content": "Automated evaluation metrics [Papineni et al., 2001, Zheng et al., 2023, Vu et al., 2024] have become a cornerstone of natural language processing, serving as a cost-effective substitute for human evaluations. The underlying idea is simple: replace the human grader with a language model (LM) and ask it to score the predictions of the candidate LMs. While these metrics are crucial for tasks where human judgment is unavailable or impractical, they often fall short of matching the nuanced assessments of human experts, particularly on tasks that fall beyond the frontier of today's LM ability. This discrepancy stems from a chicken-and-egg issue:\nHow can we trust LMs to grade themselves on tasks they don't master yet?\nThese frontier tasks, like Olympiad-level or graduate-level STEM benchmarks, are not only inspiring but also serve as frontier for the development of LMs. [Rein et al., 2023, Fang et al., 2024, Trinh et al., 2024, OpenAI, 2024] Therefore resolving this issue is paramount, as inaccurate evaluations hinder our ability to precisely gauge progress, particularly when the models are iteratively improved.\nWe propose a novel approach to address these challenges: equip automatic graders with privileged information (PI) \u2013 information only available to the grader and designed to ease the evaluation task. Some examples of privileged information include worked-out ground-truth solutions (e.g., for math prompts), prompt-specific rating guidelines (e.g., for cooking prompts), and detailed image description (e.g., for visual commonsense reasoning prompts). We borrow the concept of privileged information from Vapnik's work, where it refers to additional information for the learner to learn well, for example, rationales to solutions offered by a student to help students to learn better [Vapnik, 1982, Postscript].\nWhile privileged information can be used on any evaluation task, it is particularly impactful for frontier problems which suffer from two main impediments. First, as noted, LMs are by definition incapable of grading frontier problems by themselves. Providing them with privileged information enables the grader to specialize to the task at hand; thanks to privileged information, the grader has become an expert on the given prompt and is now capable of judging candidate responses.\nA second challenge arises for the most difficult frontier benchmarks where a majority of prompts are too difficult for today's LMs, resulting in evaluations dominated by noise. In those cases, we automatically devise simpler variations of the same problems by providing the candidate LMs with hints synthesized from privileged information. These hints enable finer grained analyses and also come at no additional human labour cost once the privileged information is collected. More importantly, they allow directly hill-climbing on the frontier benchmark of interest, instead of relying on simplified proxy problems.\nConcretely, our work highlights the value of privileged information in automated evaluations. We detail several kinds of privileged information in Section 2, as well as their use cases for both graders and candidate LMs. Section 3 builds towards Table 3 where we leverage privileged information and show how to fully automate evaluations on MathOdyssey [Fang et al., 2024], a frontier benchmark of Olympiad-level math problems. We demonstrate the effectiveness of the privileged information in Section 3.1: on RewardBench [Lambert et al., 2024], the graders closely match state-of-the-art and improve upon graders without privileged information by more than 6% accuracy points (Table 1); on Vibe-Eval [Padlewski et al., 2024], they surpass individual human graders and improve correlation with the average human rating by more than 0.35 points (Figure 1); and on MathOdyssey, they exceed 0.7 correlation thus approaching expert human graders (Figure 5). Finally, our analysis in Section 3.2 shows that hints derived from privileged information help separate models (Figure 3) and uncover unknown trends w.r.t. problem difficulty (Figure 4)."}, {"title": "2. Privileged Information for Evaluation", "content": "This section introduces privileged information and shows how we use it with automatic graders to evaluate language models.\nThe typical automatic evaluation setting has two types of language models interacting. The first are the candidate models. The candidate models are given a prompt, such as the equation \u201c$\\int ln(x) dx = ?$\u201d or \u201cWhat is the wifi password?\u201d. Their task is to respond as effectively as they can by carefully trading conflicting criteria such as conciseness, clarity, and completeness. The second are grader models, which see the prompt and assign a grade to each model's response. In our experiments, we mostly consider the pairwise setting where two candidate models answer the same prompt, and the grader assigns a single grade to both predictions: response A is preferred, response B is preferred, or tie. This process is illustrated in the left part of Figure 2.\nOur main proposal is to augment the inputs of the grader with privileged information. Here privileged information refers to information that eases the job of grader; it is \u201cprivileged\" insofar as it is only available to the grader and not the candidates. Privileged information can take many forms; for example, in Figure 2 it is pictured as the right box and provides ground-truth solution to the integral together with the integration by parts explanation for how to obtain it. Here are a few more examples of privileged information.\n\u2022 Ground-truth solutions (or gold-reference responses) help to grade close-ended prompts with a strong correctness component. These include prompts focused on factuality (\u201cBarack Obama's wife is Michelle Obama.\u201d), information-seeking (\u201cBeat eggs, cook, add fillings, fold.\u201d), or translation (\u201c\u00a1Ser, o no ser, es la cuesti\u00f3n!\u201d). With ground-truths the grader doesn't need to solve"}, {"title": "2.1. The How's and Why's of Privileged Information", "content": "We now briefly discuss how to source and use privileged information, before presenting our empirical results in Section 3.\nWe explore two approaches to collect privileged information. First, humans can manually handcraft privileged information for each prompt if the prompt set is small enough. This is particularly useful when the grading function is unintuitive to the LM while also easily specified by text. One such example are the adversarial prompts in the Chat Hard and Reasoning splits of RewardBench- more in Section 3.\nIf human annotations are too labor-intensive, we can resort to automatically synthesized privileged information. For example, in Figure 1 we aggregate all human ratings for each Vibe-Eval prompt and ask an LM to synthesize rating guidelines out of them. Both approaches can also be combined. In Vibe-Eval we first generate image descriptions by asking an LM to describe the image in details, and manually edit them for accuracy."}, {"title": "2.2. Privileged Information for Frontier Problems", "content": "For frontier problems, we introduce a second approach to leverage privileged information. On these challenging tasks LMs fail to solve most prompts making the aggregated evaluation metrics largely dictated by noise. We propose to extract problem hints out of the grader's privileged information with an LM. Hints are then provided to the candidates at evaluation-time, hopefully simplifying the problem enough that it can be solved. Adding more hints simplifies the problem further, and lets us construct difficulty tiers by including more or less hints. Figure 9 in Appendix showcases templates to extract hints from the grader's privileged information.\nGenerating hints from privileged information yields several benefits. First, it enables tracking progress directly on the frontier problems of interest rather than proxies to the frontier problems. Second, as showcased in Section 3, it naturally lets us analyze competing models on different difficulty tiers. Finally, it significantly cheapens the data collection effort since the same prompts and ground-truth solutions can be reused to. This is particularly valuable for frontier problems where expensive experts need to come up with prompts and ground-truth solutions."}, {"title": "3. Experiments", "content": "Our experiments show how privileged information can help improve automated evaluations. We ask the following questions:\n\u2022 How much do graders improve when given access to privileged information?\n\u2022 Can privileged information help ease problem difficulty and thus improve separability?\n\u2022 How to build expert-level evaluations with privileged information?\nUnless otherwise specified, we refer to Gemini 1.5 Flash, Gemini 1.5 Pro, and Claude 3.5 Sonnet as Gemini Flash (gemini-1.5-flash-001), Gemini Pro (gemini-1.5-pro-001), and Sonnet (claude-3-5-sonnet-20240620), respectively."}, {"title": "3.1. Better automatic graders with privileged information", "content": "We first study how the grading performance of various automatic graders change when given access to privileged information.\nDatasets. We study the performance of graders with privileged information on two well-established benchmarks: RewardBench [Lambert et al., 2024] and Vibe-Eval [Padlewski et al., 2024]. RewardBench has four categories: Chat, Chat Hard, Safety and Reasoning in total 2985 prompts. For each prompt, the grader is given two responses and asked to decide which one is more preferred by human. If the grader chooses the same response as the human label, it is considered correct. The benchmark also actively maintains a leaderboard for the average grading accuracy for the best graders\u00b9. Vibe-Eval is a challenging visual question answering dataset consisting of 269 prompts, with 169 categorized as normal difficulty and the remaining as hard. Each prompt also comes with a golden reference\nPI generation. On RewardBench, we obtain rating guidelines by distilling it from rated responses. Specifically, for each subset in Chat and Safety, we use 20 rated responses and ask Gemini Pro to synthesize generally applicable rating guidelines from them. The guidelines are then used to rate all prompts in the subset. Manually inspection show that these guidelines are generic enough. We obtain 10 subset-specific prompts (5 Chat and 5 Safety). For chat hard and reasoning subset, we manually craft one rating guideline and use it to grade all prompts in the two subsets. On Vibe-Eval, we leverage three types of privileged information: reference answer, rating guideline and image caption. The reference answers are directly taken from the dataset and rating guidelines are explicitly written to focus on the correctness of the response rather than the verbosity. Finally, image captions are synthesized from Gemini Pro by asking the model to provide a description for the image. Examples of rating templates with privileged information can be found in Appendix A.2.\nMetrics. On RewardBench, we use the standard rating accuracy to evaluate the graders. On Vibe-Eval, since we not only know which response is preferred by human but also the extent of the preference, we use Spearman correlation between automatic graders and human graders as our evaluation metric. To reduce rating variance and position bias, each response pair is graded eight times, alternating the order in which the two responses are presented.\nResults. In Table 1, we compare the rating accuracy of Gemini Flash and Pro as graders, with and without PI. The top 5 models on the leaderboard as of February 13th, 2025 is also shown for reference. In Figure 1, we show the performance of graders as well as human performance. We further analyze the effect of privileged information on both human and LM graders along with different subsets.\nGraders with privileged information outperform almost all specialized models and human graders. As seen in both Table 1 and Figure 1 (left), providing privileged information significantly improve the rating accuracy by more than 6% on RewardBench and more than double the Spearman correlation on Vibe-Eval. Moreover, the improvement is large enough on RewardBench to closely match the SOTA result on the leaderboard for Gemini Pro. On Vibe-Eval, privileged information enables both Gemini Flash and Pro to outperform human graders. This is particularly encouraging from a cost perspective. LM graders not only have the potential to support and partly substitute human graders"}, {"title": "3.2. Simplifying frontier problems with privileged information", "content": "We now show how to address the second challenge that frontier benchmarks pose: they are hard enough that we don't get meaningful signal to evaluate our models.\nDatasets and metrics. We use two widely-recognized reasoning datasets, MATH [Hendrycks et al., 2021] and GPQA [Rein et al., 2023], to evaluate model performance. The MATH dataset contains 5,000 open-ended problems from high school curricula and competitions spanning seven mathematical topics. Since most MATH problems are easily solved, we adversarially select problems that both Gemini 1.5 Flash and Pro solve less than 10% of the time and call this subset MATH-Adv. For GPQA, we use all 448 questions across biology, chemistry, and physics. These graduate-level problems are challenging: even human experts solve only 65% of the time. Both datasets provide step-by-step ground truth solutions created by human experts, which we use as privileged information. For these studies, we measure accuracy against a known final answer to reduce variability and control for confounders due to an automatic grader. We also sample eight model responses per problem and bootstrapping to compute 95% confidence intervals.\nFrom privileged information to hints. We use the privileged information, here step-by-step solutions, to generate hints with a goal to simplify frontier problems. Given the ground truth solution of a target problem, we ask Claude 3.5 Sonnet to breakdown the solution into three standalone hints. We explicitly instruct not to reveal the final answer so that providing hints one-by-one incrementally eases the target problem. Example hint generation template can be found in Figure 9.\nResults summary. Figure 3 shows that using our PI-generated hints is crucial to robustly separate models which would otherwise be indistinguishable on MATH-Adv and GPQA. Moreover these hints also uncover new insights in Figure 4, namely that GPT-40 shines on harder problems while Gemini models can better take advantage of problem hints.\nHints from privileged information ease problems and improve model separability. First, we see that performance monotonically increases when we provide more hints for both Figures 3 and 4. These hints significantly improve performance, e.g. boosting from almost 0% to 80% on MATH-Adv. Second, in Figure 3, we observe that both candidate models yield very similar performance on the original problems (i.e., hints = 0), with confidence intervals largely overlapping. This is particularly true for MATH-Adv since the problems are deliberately selected to be difficult to both candidates. Third, the gap between the two candidates first increases and then decreases, almost overlapping again when all the hints are provided to the models. This illustrates the existing of an evaluation sweet spot and echoes the \u201cGoldilock zone\u201d message from Padlewski et al. [2024]. We observe similar trends regardless of the model used to generate hints and regardless of the number of hints generated"}, {"title": "3.3. Expert-level evaluations with privileged information", "content": "In this section, we propose to use privileged information for both improving the reliability of automatic grader and easing problem difficulties for better performance separability on frontier problems.\nDataset. We identify a recent and particularly challenging math reasoning dataset MathOdyssey -Olympiad, a subset of MathOdyssey [Fang et al., 2024]. This subset has 148 very challenging high school competition level problems featuring both open-ended and multiple-choice types. Fang et al. [2024] evaluate GPT-4 Turbo on this subset and the accuracy based on final answer is only 10.14%. The dataset also has huamn written reference solutions and answers.\nBecause close to 90% of the problems cannot be solved by frontier models, accuracy based evaluation provides very limited information for assessing different frontier model performance. To this end, we propose to first leverage pairwise comparison between model solutions using automatic graders. Pairwise comparison can provide performance signals on problems where both models solve incorrectly. Then, since the original problems are still very hard for frontier LMs, we leverage the insights from Section 3.2 to ease the problem difficulty levels for a more fine-grained evaluation.\nPI generation. The privileged information we leverage is the ground-truth solution. For grading model responses, the entire ground-truth solution is provided. For easing problem difficulties, we use the same technique from 3.2 where we convert each ground-truth solution to three hints.\nLM grader correlation with human ratings. Before applying LM graders to these challenging math"}, {"title": "4. Related Works", "content": "Evaluation metrics on open-ended outputs from language models. Significant effort has been dedicated to creating effective evaluation metrics to measure the quality of open-ended outputs from language models. Early methods like BLEU [Papineni et al., 2001] and ROUGE [Lin, 2004] rely on rule-based approaches that focus on lexical overlap to gauge similarity between generated responses and references. However, these methods may fail to capture the deeper semantic meaning of the text. This limitation led to research exploring the use of language model embeddings [Zhang et al., 2019, Sellam et al., 2020, Yuan et al., 2021] for evaluating generations. More recently, language models (LMs) have also been leveraged to score text. Broadly speaking, there are two types of approaches: training and training free. Specifically, training based approaches trains or finetunes LMs directly on ground truth scores [Juraska et al., 2023, Wang et al., 2024, Kim et al., 2024, Vu et al., 2024] or performs RLHF to align with human preferences [Ouyang et al., 2022, Sun et al., 2023, Li et al., 2023, Yuan et al., 2024, Zhang et al., 2024a, Shankar et al., 2024]. Training-free approach, however, directly leverages the instruction following capability of LMs and prompts the model to evaluate outputs via chain of thought [Wei et al., 2022]. Besides vanilla prompting LMs on text and other modalities [Zheng et al., 2023, Yu et al., 2023], aggregating ratings from a variety of LMs [Verga et al., 2024, Ning et al., 2024], generating reference answers [Zeng et al., 2023], grounding quantitative reasoning [Zhou et al., 2024] and simulating debates among LMs [Khan et al., 2024] have been shown to further improve evaluation effectiveness. In this work, we do not train or finetune any models; instead, we show that privileged information improves automatic evaluations such that they outperform the best finetuned LMs and match expert human graders.\nProviding LM graders with additional information. When asking LM-based grader to rate text, additional context can be provided to align with human. One prominent example is Constitutional AI [Bai et al., 2022] where human oversight are written in the form of rules or principles. The principles provided is a general set of principles without any variation for different queries. Others [Vu et al., 2023, Zeng et al., 2023, Yu et al., 2023, Bai et al., 2023, Padlewski et al., 2024] have explored generating or using reference answers to automatic graders for better decision making. Finkelstein et al. [2024] constructs few-shot prompting examples from prior ratings while Cook et al. [2024], Zhang et al. [2024b] use grading checklist or criteria as additional information. In this paper, we"}, {"title": "5. Limitations and Discussion", "content": "Although LM graders can outperform humans on many tasks, they still exhibit unreliability and biases. For instance, as shown in Table 2 and the findings of Panickssery et al., LM graders tend to favor their own generations. Additionally, due to the inherent biases in human-annotated data, LM graders may raise concerns about fairness. Moreover, the reliability of automatic metrics in general must be questioned [Doostmohammadi et al., 2024, Boubdir et al., 2023]. For these reasons, we caution against the uncritical replacement of human judgment with LM graders. Instead, there should be a concerted focus on refining and improving their performance and reliability. In this paper, we study the use of privileged information to improve the reliability of evaluation."}, {"title": "6. Conclusion", "content": "In conclusion, our research emphasizes the importance of PI in enhancing automated evaluations, particularly for challenging frontier problems. By incorporating PI, we have demonstrated significant improvements in the performance of automatic graders across various benchmarks, including Reward-Bench, Vibe-Eval, and MathOdyssey. Furthermore, our analysis reveals that hints derived from PI can effectively differentiate model capabilities and uncover trends related to problem difficulty. We believe that this methodology offers a promising avenue for developing reliable automated evaluations that push the boundaries of our most advanced models."}, {"title": "A. Appendix", "content": "A.1. Additional details on Vibe-Eval human ratings\nWe crowdsource human raters, instructing them to evaluate each pairwise comparison based on the fulfillment, groundedness, and presentation quality of the responses. The raters are also provided with ground truth references from Vibe-Eval to guide their assessments. For each comparison, the raters select a rating from 7 categories: {\u22123, -2, -1, 0, 1, 2, 3}, where 1, 2, and 3 indicate that one response is slightly better, better, or significantly better than the other, and 0 indicates that both responses are of similar quality. Each comparison receives approximately five human ratings, and the final score is determined by averaging these ratings.\nA.2. Rating guidelines and templates examples\nExample rating templates for RewardBench with category-specific rating guidelines as privileged information are shown in Figure 6 and 7. Rating template for Vibe-Eval is included in Figure 8.\nA.3. Additional results on Vibe-Eval\nIn Table 4, we study the rating performance of Gemini Flash and Gemini Pro when given different combinations of privileged information. The results how that more privileged information generally helps improve rating and reference answer is the most beneficial privileged information."}]}