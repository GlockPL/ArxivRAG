{"title": "Hierarchical Mixture of Experts: Generalizable Learning for High-Level Synthesis", "authors": ["Weikai Li", "Ding Wang", "Zijian Ding", "Atefeh Sohrabizadeh", "Zongyue Qin", "Jason Cong", "Yizhou Sun"], "abstract": "High-level synthesis (HLS) is a widely used tool in designing Field Programmable Gate Array (FPGA). HLS enables FPGA design with software programming languages by compiling the source code into an FPGA circuit. The source code includes a program (called \"kernel\") and several pragmas that instruct hardware synthesis, such as parallelization, pipeline, etc. While it is relatively easy for software developers to design the program, it heavily relies on hardware knowledge to design the pragmas, posing a big challenge for software developers. Recently, different machine learning algorithms, such as GNNs, have been proposed to automate the pragma design via performance prediction. However, when applying the trained model on new kernels, the significant domain shift often leads to unsatisfactory performance. We propose a more domain-generalizable model structure: a two-level hierarchical Mixture of Experts (MoE), that can be flexibly adapted to any GNN model. Different expert networks can learn to deal with different regions in the representation space, and they can utilize similar patterns between the old kernels and new kernels. In the low-level MoE, we apply MoE on three natural granularities of a program: node, basic block, and graph. The high-level MoE learns to aggregate the three granularities for the final decision. To stably train the hierarchical MoE, we further propose a two-stage training method. Extensive experiments verify the effectiveness of the hierarchical MoE.", "sections": [{"title": "Introduction", "content": "With the heated demand for domain-specific accelerators, field-programmable gate arrays (FPGA) are widely used. It is, however, labor-intensive to write register-transfer-level hardware description languages, such as VHLD and Verilog. High-level synthesis (HLS) provides a much easier solution by compiling a source code written in a software programming language, such as C, MatLab, etc., into an FPGA circuit (Cong et al. 2011, 2022; Schafer and Wang 2019).\nThe source code consists of a program (also called \"kernel\") that describes the FPGA's functional behaviors, and several pragmas inserted in the program that instruct the hardware synthesis process, such as parallelization, pipeline, etc., as illustrated in the code snippets in Fig. 1. While it is relatively easy to design the program, it heavily requires hardware knowledge to design the pragmas, and different pragma designs can lead to significantly different FPGA performances, posing a great challenge for software developers (Sohrabizadeh et al. 2022). Recent works have automated pragma design for programs, employing heuristics methods or machine learning methods. The heuristic methods use bottleneck analysis (Sohrabizadeh et al. 2021) or non-linear programming based on lower-bound objective function (Pouget, Pouchet, and Cong 2024a,b). The machine learning methods train a surrogate model to predict the FPGA's performance from the source code, so that we can rely on the model prediction to find the best pragmas without running time-consuming HLS. Since HLS data is very scarce, it is difficult to train a large language model, and graph neural networks (GNNs) that are based on the control data flow graph (Sohrabizadeh et al. 2022; Bai et al. 2022; Sohrabizadeh et al. 2023; Ustun et al. 2020; Wu, Xie, and Hao 2023; Wu et al. 2022a) are widely utilized. Some very recent work (Qin et al. 2024) explores using both GNN and lightweight language models.\nWhile machine learning models have better learning ability than the heuristic methods, they do not generalize well to unseen kernels. In real-world applications, we often encounter new circuit design requirements. The model trained on existing kernels usually fails to perform well on new kernels. This can be viewed as a domain generalization (Bai et al. 2022; Kwon and Carloni 2020) problem, where each kernel is a domain. Note it is very time-consuming to run HLS to acquire the labels on new kernels, with each run taking minutes to hours (Sohrabizadeh et al. 2023), thus data-efficient fine-tuning is required for new kernels. Domain generalization methods for GNN usually employ adversarial training to align the representation space between the source domain and target domain (Dai et al. 2019; Zhang et al. 2019; Wu et al. 2020; Shen et al. 2023), or do data augmentation to learn invariant representation under risk extrapolation (Wu et al. 2022b; Liu et al. 2023a). However, these approaches are not applicable to our application, as the differences between kernels are informative for the prediction, so forcing them to the same distribution does not work.\nNonetheless, a unique opportunity of the HLS prediction task is that kernels usually share some similar substructures, as shown in Fig. 1. It could be beneficial if we can leverage such similarities. We thus propose a two-level hierarchical mixture of experts (MoE) to address this issue. MoE (Ja-"}, {"title": "Preliminary", "content": "HLS prediction is formalized as a graph regression task. Following previous works (Sohrabizadeh et al. 2022, 2023), we utilize ProGraML (Cummins et al. 2021) graph to represent a source code, which is a control data flow graph. Nodes represent instructions, variables, and constant values, and edges represent the control flow and data flow. A GNN model is trained to predict the FPGA's latency and the utilization of four resources: LUT, FF, DSP, and BRAM. In the domain generalization setting, we train the model on N source kernels $D^{(train)} = {D_1, D_2, ..., D_N}$, where $D_i = {(X_i, T_{i1}, Y_{i1}), (X_i, T_{i2}, Y_{i2}), ..., (X_i, T_{in}, Y_{in})}$ is the i-th kernel containing in samples, and each sample consists of a program X, a pragma design T, and a label Y.\nFor a new kernel $D_{test}$, we mimic the data-scarce situation where we only use k samples from the dataset $D^{(k)} \\subset D_{test}, |D^{(k)}| = k$, to fine-tune the model. After fine-tuning, there are two ways of evaluation: offline evaluation and online evaluation. In the offline evaluation, we evaluate the MSE on the left-out test samples: $D_{test} \\backslash D^{(k)}$. In the online evaluation, we run a DFS searching algorithm used by previous works (Sohrabizadeh et al. 2022, 2023) to search for good pragma designs. Based on the fine-tuned model's prediction, We select the top-10 pragma designs, run HLS for them, and evaluate the FPGA's speedup compared to AutoDSE."}, {"title": "HARP Model", "content": "The proposed hierarchical MoE can be adapted to any GNN model. We use one of the SOTA GNN models for HLS prediction, HARP (Sohrabizadeh et al. 2023), as the base model. Since the original ProGraML graph is sparse and not good at modeling long-term dependency, HARP creates a \"pseudo node\" for each basic block in the program and connects it with every node within that basic block. Defined by the compiler community, a basic block is a sequence of instructions that has a single entry point and a single exit point, whose terminator instruction can be branch, return, etc. We illustrate HARP's model structure in the appendix. We use V to denote the set of all nodes and VB to denote the set of pseudo nodes. HARP consists of five components: (1) GNN encoder: it consists of 6 GNN layers and learns node representation; (2) Pragma MLP: it uses an MLP for each pragma type to transform the representation of pseudo nodes that are modified by that pragma type; (3) Another GNN layer after pragma MLP: it spreads the information of pragmas from pseudo nodes to normal nodes and to pseudo nodes that do not have pragmas; (4) Graph pooling: it does graph pooling on pseudo nodes VB to form a graph representation and on normal nodes $V \\backslash V_B$ to form another graph representation, then it concatenates them; (5) Output MLP: it makes the prediction based on the graph representation."}, {"title": "Methods", "content": "A unique opportunity for our task is that the input graph has three granularities: normal nodes that represent data and instructions, pseudo nodes that represent basic blocks, and the whole graph that represents a source code file. If a data point from the target kernel shares a similar statement (node), basic block, or even the whole code (graph) with a data point from the training kernels, MoE could be helpful. Thus, we consider employing MoE on the three granularities: MoE on all nodes including normal nodes and pseudo node (node MoE), MoE only on pseudo nodes (block MoE), and MoE on the graph (graph MoE). Based on our pre-exploration, the best practice is to apply MoE in the components after the pragma MLP, since the experts need to share the same encoder and pragma MLPs. Our design of the three low-level MoE models is illustrated in Figure 2.\nNode MoE. For the MoE that operates on all nodes, we apply it on the GNN layer after the pragma MLP. We train"}, {"title": "Low-Level Mixture of Experts"}, {"title": "High-Level Mixture of Experts", "content": "Now we have three MoE models operating on different granularities. A simple approach to combining them is to use all of them together in one model. However, our ablation study verifies that the performance will be worse. This demonstrates that we only need MoE on one granularity in one model. However, the best granularity greatly varies for different kernels, and it is difficult to discover a pattern. Therefore, we propose a high-level MoE to aggregate them. It calculates the weighted sum of the outputs of the three low-level MoE models.\nWe propose two designs of the high-level gating network, as illustrated in Figure 2. The first design is to do graph pooling on the input node features to form a graph representation as the input to the high-level gating network. We use the self-attention graph pooling. Denoting the input feature of node $v_i$ as $x_i$, the graph pooling is formalized as:\n$x_G = \\sum_{i \\in V} softmax(MLP(x_i)) \\cdot x_i$ (5)\nwhere $x_G$ is the aggregated input feature, and V is the set of all nodes. The second design is to concatenate the graph representation in the three low-level MoE models. The second design performs better in our experiments, since the hidden representations are more expressive than the input features. Nonetheless, when we use a sparse MoE where only one or two experts are selected, the first design will be more memory efficient. It can determine the expert assignment before the computation of three expert models, thus reducing unnecessary computation. However, the best-performing method is to utilize all the experts, and in this case, the two designs are similar in efficiency. We utilize all the experts in our experiments. We also apply the regularization term for the high-level gating network."}, {"title": "Two-Stage Training and Constant Initialization", "content": "Optimizing the hierarchical MoE is challenging. Different from previous MoE studies where expert networks have the same or similar structures, our three low-level MoEs are very different and thus have different convergence speeds. The graph MoE model converges the fastest, since its MoE operates on the graph representation and has the least computation. Thus, the high-level gating network suffers severely from expert polarization, leaning towards the graph MoE model. If we simply increase the weight of the regularization term, the high-level gating network can effectively learn to assign about weight to each expert, but the graph MoE model still converges much faster than the other two models. In the first few epochs, the output of node MoE and block MoE models will be noisy and thus they are discouraged from making predictions. As a result, the graph MoE model will learn to output three times the label, while node and block MoE models will learn to output zero.\nTo address this unique challenge, we design a two-stage training strategy to encourage every expert model to perform well individually. In the first stage containing T epochs (warmup), we train the three expert models individually. In the second stage, we take turns training the whole model end-to-end and the three expert models individually. If we denote the label as Y, the prediction made by the i-th expert model as $\\hat{Y_i}$, and the MSE loss function as $L(Y, Y_i)$, then we define the loss function at epoch t as:\n$L =\\begin{cases}L(Y, \\hat{Y_1}) + L(Y, \\hat{Y_2}) + L(Y, \\hat{Y_3}) + \\alpha L_R, & \\text{if } t < T \\text{ or } 2 \\nmid t \\\\L(Y,\\sum_{i=1}^{3} g_i \\cdot \\hat{Y_i}) + \\alpha L_R + \\beta L_{Rh}, & \\text{otherwise}.\\end{cases}$ (6)\nHere, $L_R = (L_{R1} + L_{R2} + L_{R3})$, where $L_{Ri}$ is the regularization term of the i-th low-level MoE. $L_{Rh}$ is the regularization term of the high-level MoE. $g_i$ is the weight assigned by the high-level gating network. When we fine-tune our model on target kernels, we directly use the end-to-end joint training, since all the experts can already perform well during fine-tuning and there is no risk of expert polarization.\nIn common situations where the expert networks have similar or the same structures, the gating network's parameters should be randomly initialized, because we need the randomness to diversify the experts. Instead, we initialize the high-level gating network to assign the same weights to the expert models, which can further prevent expert polarization. For low-level MoEs, we use random initialization."}, {"title": "Experiments", "content": "Datasets. We use the most comprehensive benchmark dataset, HLSyn (Bai et al. 2023). It consists of 42 kernels covering various categories: linear algebra on vectors and matrices, data mining, stencil operations, etc. We utilize the AMD/Xilinx HLS tool, Vitis 2021.1 (AMD/Xilinx 2020), to run HLS targeting the Xilinx Alveo U200 FPGA with a working frequency of 250MHz. We select 6 kernels as the target kernels that span representative categories including linear algebra computation, data mining, and stencil computation, and we use the other kernels as source kernels. Table 2 shows the dataset statistics. We introduce the kernels' details in the appendix. The HLSyn dataset was generated by running the bottleneck-based heuristics method, AutoDSE (Sohrabizadeh et al. 2021), for 24 hours. Many designs explored by AutoDSE are unavailable in HLS, so we collect this information to train a HARP classification model. Since its accuracy already exceeds 95%, there is no need to employ MoE on the classification model. We use the available designs in the dataset to train the regression model, and we employ MoE on the regression model.\nModels. Based on our pre-exploration, we use 4 experts in the low-level MoEs, and we set the weight of the regularization term of both low-level and high-level MoE, a and"}, {"title": "Experiment Results", "content": "Table 1 shows the main results. In the main experiments, we use the second design of the high-level gating network as it performs better. Neither MAML nor ProgSG performs well in our setting. It might be because we have many more source kernels compared to the previous paper that utilizes MAML on this task (Bai et al. 2022). Different kernels might result in different directions of the meta gradient, leading to unstable meta-learning. ProgSG which combines GNN and language models has a strong ability on the HLSyn dataset when the training data is sufficient (Qin et al. 2024), but since the language model needs a large dataset to fine-tune, it severely overfits in our data-scarce setting. During fine-tuning, the training loss is lower than 0.05, but the test loss is high. Comparatively, the hierarchical MoE is a more generalizable structure. In the online evaluation, on most kernels, the best of three single MoE models outperforms HARP, but different kernels favor different low-level MoEs. For example, the block MoE model performs better than the node MoE and the graph MoE for \"Fd\", while graph MoE is the best for \"Gemv\u201d, \u201cJa\u201d, and \u201cTr\". Since a single MoE model could not perform well on all kernels, its overall MSE and the average speedup might not outperform HARP. By aggregating them together, the hierarchical MoE performs the best or close to the best on almost every kernel. Its geometric mean speedup achieves about 26.6% higher than HARP.\nVarious model structures. To verify the effectiveness of our elaborate model design, we run extensive experiments on various model structures. First, instead of aggregating the three low-level MoEs, can we apply MoE on the three granularities together in a single model? We specify the detailed structure design in the appendix. Table 3 shows the results."}, {"title": "Conclusion", "content": "Domain generalization is a big challenge for HLS prediction models. Based on the unique challenges and opportunities of this task, we propose the hierarchical MoE structure, where different expert networks can specialize in different regions, and it can be flexibly adapted to any GNN model. In the low-level MoE, we apply MoE on the three natural granularities of the graph: nodes, basic blocks, and the graph. In the high-level MoE, we aggregate the three low-level MoE models, so that different data points can flexibly decide which granularity to utilize MoE. To address the severe expert polarization, we propose two-stage training via deactivating the high-level gating network for some epochs. Extensive experiments have verified the effectiveness of hierarchical MoE. We focus on the generalizability of the HLS prediction task, but our model might also potentially improve the generalizability in other applications of various domains."}, {"title": "Additional Background Knowledge of HLS", "content": "As the HLS prediction task is quite new to the AI community, we would like to illustrate this task at the beginning of the appendix. Taking the \"syr2k\" kernel as an example, Figure 3 shows its source code. As explained in the Introduction section, the source code consists of the program that describes the FPGA's function, which is similar to any normal C program, and several pragmas. Now the pragmas are represented by the placeholders \u201cauto{pragma index}\". We utilize the machine learning model to find the best pragmas to insert in these placeholders.\nThere are three types of pragmas: parallel, pipeline, and loop tiling. For the parallel pragma placeholders, we should insert an integer as the parallel factor. For the pipeline pragma placeholders, we should choose between the fine-grained pipeline, coarse-grained pipeline, or not using the pipeline. For the loop tiling pragma placeholders, we should insert an integer as the loop tiling factor. Table 8 lists the introduction of the pragmas.\nDesigning the pragmas is labor-intensive and heavily relies on hardware knowledge, which motivates the automated pragma design research. Different pragmas can lead to significantly different FPGA performance. Table 9 shows the latency (clock cycle) of each target kernel's best pragma design and worst pragma design in our dataset. The shorter latency the better. We also list the mean and standard deviation of latency in our dataset, and the latency of not inserting any pragma. We can see that the gap between the best design and worst design is very big, and the worst design is sometimes even worse than not inserting any pragma. This is because some pragmas might use too many resources and occupy the resources of other modules. The standard deviation is also very large, and sometimes it is even larger than the mean. Therefore, the pragma design is very difficult."}, {"title": "Additional Problem Analysis", "content": "Domain shift brings a lot of challenges to our task. First, as shown in Table 9, the mean latency of different kernels is very different, and we hope the model can learn this distribution, so we cannot align the representation space of different kernels, as did in many previous domain generalization papers. We extract the graph representation of each design point from the HARP model and visualize it in Figure 4. In the figures, points belonging to the same kernel have the same color. Due to the limited number of colors, different kernels might use the same color. We can see that points from the same kernels are clustered together, while points from different kernels are located in different places. The visualization further verifies that the domain difference between different kernels is very big. Such a big difference is informative to the model, so we cannot align the representation space of different kernels.\nSecond, kernels share some similar substructions, where both their similarities and differences should be carefully noticed. The similarity could benefit the MoE model by utilizing the knowledge learned from other kernels. Nonetheless, the difference is also very important. An example is the \"syr2k\" kernel and the \"syrk\u201d kernel. They only differ in one line inside a nested loop, shown in Figure 5 and Figure 3. However, their best pragma designs are very different. In the best pragma design of \u201csyrk\u201d, the tiling factor of \"__TILE__L2\" is 60, but it is 1 for \"syr2k\"; the parallel factor of \"__PARA_L1\" and \"__PARA__L2\" are 20 and 1 for \"syrk\", but they are 8 and 4 for \"syr2k\"; \"syrk\" uses coarse-grained pipeline for \u201c__PIPE__L2", "syr2k": "oes not use pipeline there. Therefore, domain generalization is difficult for HLS prediction models. The MoE structure can utilize similar experts to deal with similarities, but it can also discern the differences and use different experts to deal with them."}, {"title": "HARP Model Illustration", "content": "Figure 6 illustrates the HARP model using the same diagram style as our model's diagram (Figre 2 in the main paper). It shows the five components of the HARP model. Since we focus more on the MoE structure, the diagram ignores some details of the HARP model. For more details, please refer to the HARP paper (Sohrabizadeh et al. 2023)."}, {"title": "Detailed Experiment Settings", "content": "Datasets. The HLSyn benchmark dataset (Bai et al. 2023) consists of 42 kernels selected from the MachSuite dataset (Reagen et al. 2014) and the Polyhedral dataset (PolyBench) (Pouchet 2012). Since the FPGA's latency is too diverse, it is difficult for the model to directly learn. Thus, we normalize the FPGA's latency by 0.5*log2(latency) as the normalized performance. It is the lower the better for the original latency, but it is the higher the better for the normalized performance. Our HLS targets the Xilinx Alveo U200 FPGA with a working frequency of 250MHz. We also normalize the resource utilization by dividing it by the total available resources of the targeting FPGA. We mainly care about four types of resources: LUT, FF, DSP, and BRAM, so we have five prediction targets including the normalized performance.\nOur resource constraint is that each resource type should be used no more than 80%, because we need to leave some spaces for downstream tasks or other additional modules. We hope the model can make accurate predictions of the performance and resource utilization, so we can search for the best pragma design that satisfies the resource constraints and has the best normalized performance (lowest latency). We rely on the classification model to help us wipe out the invalid designs, but we rely on the regression model to help us wipe out the valid designs that exceed our resource constraints.\nEvaluation. We use 80% of data points in the source kernels as the training set to pre-train the model, 10% for validation, and 10% for testing. We use five random seeds (1, 2, 3, 4, 5) to randomly split the source kernel's dataset. The MSE on the source kernels is unimportant, so currently the test set on the training kernels is not used. We use the validation set for early stopping. We pretrain the model for 1000 epochs, then we finetune it for 500 epochs on the target kernels. During fine-tuning, we use the same random seed as"}, {"title": "Study of Model Variants", "content": "As analyzed in the Experiments section in the main paper, one concern is whether the hierarchical MoE model simply benefits from having more parameters. To address this concern, we enlarge the hidden size of HARP from 64 to 128 and 256, and we evaluate their performance. The results are listed in Table 10. As we enlarge the hidden size, the MSE also increases. Even if the parameter size is larger than hierarchical MoE, its performance is still worse than hierarchical MoE. In the original best hyper-parameter setting of HARP, 64 is already the best-performing hidden size. We also list the parameter size of single-level MoE models in Table 11. If we do not utilize the hierarchical structure, only using MoE on one granularity does not significantly increase the parameter size. It verifies that parameter size is not the key reason for MoE's success.\nAnother concern is about the number of experts. In the hierarchical MoE, if we only utilize MoE on the node granularity or graph granularity, its performance is slightly worse than utilizing MoE on three granularities, but it could still perform well (Table 4 in the main paper). In this case, the three expert models for the high-level MoE have the same structure. Each of the low-level MoE models has 4 experts, so in total we have 12 experts for that granularity and 3 experts for other layers that are not equipped with low-level MoE. Does its success come from a large number of expert models (12 experts for that granularity), or the hierarchical structure (12 experts for that granularity + 3 experts for other layers)? To answer this question, we run experiments on the single-level MoE structure with different numbers of experts. In addition to our original setting using 4 experts, we experiment with 2, 8, and 12 experts. The results are listed in Table 12. Increasing the number of experts does not necessarily improve the performance. For all three MoEs, the lowest MSE is achieved when we use 4 experts. Therefore, the performance gain is probably because of the hierarchical structure (12 experts for that granularity and 3 experts for other layers).\nAs discussed in the main paper, when we were designing the block MoE's structure, there were many design choices, from which we chose the current one. We also conduct experiments on other variants of the single-level block MoE model. In our design, we remove the GNN layer after the"}, {"title": "Analysis of Expert Assignment", "content": "Due to the limited space, we only analyze the gating network of the high-level MoE and the block MoE in the main paper. Here we analyze the gating network of the node MoE and the graph MoE. We still pick the best-performing hierarchical MoE model from the five repeated experiments to analyze. For the node MoE, we show the averaged assigned weights for each type of node in Table 14. There are three non-overlapping types of nodes: normal nodes that represent the data and instructions, pseudo nodes that represent basic blocks, and pragma nodes that represent pragmas. A special type of normal node is icmp node, which represents the \"icmp\" instruction of a loop. If a pragma modifies a loop, that pragma node will be connected to both the icmp node and the pseudo node corresponding to the loop, so the icmp nodes are very meaningful. The normal nodes and pseudo nodes are relatively equally distributed to the experts. The icmp nodes are mainly handled by the third expert, and the pragma nodes are mainly handled by the fourth expert. Similar to our analysis of the block MoE in the main paper, the node MoE's experts also diversify their roles.\nFor graph MoE, based on the expert assignment weights of each data point, we calculate the average expert assignment weights of each kernel. Then, we can calculate the cosine similarity between two kernels using the weights. Table 15 lists the most similar source kernel of each target kernel. \"Gemver-medium\" and its most similar kernel, \u201cgesummv-medium", "syr2k": "nd its most similar kernel,", "trmm-opt\" and its most similar kernel, \u201csymm\", are both matrix multiplications. This indicates that the expert weights assigned by the graph MoE are partially explainable.\"\n    },\n    {\n      \"title\": \"Other Detailed Information\",\n      \"content\": \"Hyper-parameters. We basically follow the hyper-parameter settings of HARP. We use the Adam optimizer to train the model for 1000 epochs and fine-tune it for 500 epochs. The learning rate is 1e-3 and the weight decay is 1e-4. We use the cosine annealing learning rate schedule (Loshchilov and Hutter 2017) with a minimum learning rate of 1e-5, and the unturned linear warmup (Ma and Yarats 2021). We set the hidden size as 64 and the dropout as 0.1. The regression models use the MSE loss to train, while the classification models use the cross entropy loss. In the first design of the high-level gating network, we use the graph pooling of input features (Equation 5 in the main paper). Its MLP has two layers and one ReLU function between them, where the first layer has the same hidden size as the input dimension and the second one shrinks the output dimension to 1.\nFor hyper-parameters related to MoE, we try the hyper-parameters in a reasonable scope and use the best one with the lowest test MSE on the target kernels. The low-level MoE uses the weighted sum of 4 experts, and the high-level MoE calculates the weighted sum of three models (the node MoE model, the block MoE model, and the graph MoE model). The gating network is a linear layer and a softmax function. We set the weight of the regularization term of MoE, both a and \u03b2, as 5e-3. In the two-stage training of the high-level MoE, the first stage (warmup) takes the first 500 epochs, and the second stage takes the later 500 epochs. In the model variants experiments, we only modify the different parts as introduced in the experiments. Table 16 lists the scope of hyper-parameters we have tried.\nReproducibility. We have submitted our data and codes in the supplementary material. The readme file contains instructions on how to run the codes. We have introduced the important hyper-parameters in this appendix, while other hyper-parameters can be found in the \\\"src/config.py\\\", We use NVIDIA L40S (40GB) to train our model. It takes about 5 hours to train the model on the source kernels and about 1 hour to fine-tune on the target kernels. Training costs about\"\n    },\n    {\n      \"title\": \"Limitations and Future Work\",\n      \"content\": \"Though we have made much progress on domain generalization for HLS prediction, we also face many limitations. First, the hierarchical MoE model does not have a huge advantage over HARP on every kernel. As shown in Table 1 in the main paper, it is significantly better than HARP on \u201cFd\" and \"Gemv\", slightly better on \u201cGemm\" and \"Tr\", but it is the same as HARP or worse than HARP on \"Sy\" and \"Ja\". This might be due to the inherent difficulty of this problem. Future research is needed to develop even stronger models.\nBesides, the hierarchical MoE structure is mainly useful in quick domain adaption using a small amount of data to fine-tune. However, when the computing resource is abundant enough to run a lot of HLS on the target kernels, we will have more data points for tine-tuning, and in this case, MoE would perform similarly to HARP. The proposed model should be used in suitable applications. It also remains an open question of how to enhance the model's general ability when the data is abundant.\"\n    }": ""}]}