{"title": "Data Augmentation via Diffusion Model to Enhance AI Fairness", "authors": ["Christina Hastings Blow", "Lijun Qian", "Camille Gibson", "Pamela Obiomon", "Xishuang Dong"], "abstract": "AI fairness seeks to improve the transparency and explainability of AI systems by ensuring that their outcomes genuinely reflect the best interests of users. Data augmentation, which involves generating synthetic data from existing datasets, has gained significant attention as a solution to data scarcity. In particular, diffusion models have become a powerful technique for generating synthetic data, especially in fields like computer vision. This paper explores the potential of diffusion models to generate synthetic tabular data to improve AI fairness. The Tabular Denoising Diffusion Probabilistic Model (Tab-DDPM), a diffusion model adaptable to any tabular dataset and capable of handling various feature types, was utilized with different amounts of generated data for data augmentation. Additionally, reweighting samples from AIF360 was employed to further enhance AI fairness. Five traditional machine learning models-Decision Tree (DT), Gaussian Naive Bayes (GNB), K-Nearest Neighbors (KNN), Logistic Regression (LR), and Random Forest (RF) were used to validate the proposed approach. Experimental results demonstrate that the synthetic data generated by Tab-DDPM improves fairness in binary classification.", "sections": [{"title": "I. INTRODUCTION", "content": "In our rapidly evolving society, artificial intelligence (AI) has become a ubiquitous presence, influencing everyday activities like online banking and digital assistants. However, how can we ensure the fairness of AI-generated outcomes? AI fairness seeks to enhance the transparency and explainability of AI systems [1]. It scrutinizes the results to determine if they genuinely consider the users' best interests. Additionally, guidelines are being established to ensure the safety of both corporations and consumers. Various fairness tools have been developed to address the growing need to mitigate AI biases [2]. For example, AIF360 [3] offers a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to reduce bias in datasets and models concerning protected attributes such as sex and race. Data augmentation [4] aims to generate synthetic data from existing datasets to enlarge the training data to enhance the machine learning performance [5]. This technique increases both the quantity and variety of data available for training and testing models, eliminating the need for new data collection.\nData augmentation can be achieved by either learning a generator, such as through GAN networks [6], to create data from scratch, or by learning a set of transformations to apply to existing training set samples [7]. Both approaches enhance the performance of deep learning models by providing a more diverse and abundant dataset.\nIn recent years, diffusion models [8] have emerged as a powerful technique for generating synthetic data to address data scarcity. For example, Villaiz\u00e1n-Vallelado et. al proposed a diffusion model for generating synthetic tabular data with three key enhancements: a conditioning attention mechanism, an encoder-decoder transformer as the denoising network, and dynamic masking [9]. Nguyen et. al introduced a novel method for generating pixel-level semantic segmentation labels using the text-to-image generative model Stable Diffusion (SD) [10], which incorporates uncertainty regions into the segmentation to account for imperfections in the pseudo-labels. Additionally, Hu et. al developed a novel diffusion GNN model called Syngand, capable of generating ligand and pharmacokinetic data end-to-end, providing a methodology for sampling pharmacokinetic data for existing ligands using this model [11].\nThis paper aims to investigate whether diffusion models can generate synthetic data to enhance AI fairness as well as machine learning performance. Tabular Denoising Diffusion Probabilistic Model (Tab-DDPM) [12] is a diffusion model that can be universally applied to any tabular dataset, handling all feature types. It uses multinomial diffusion for categorical and binary features, and Gaussian diffusion for numerical ones. Tab-DDPM effectively manages mixed data types and consistently generates high-quality synthetic data. It is used to conduct different increments of generated data samples, specifically 20,000, 100,000, and 150,000 samples. To further mitigate bias, reweighting samples was employed to recalibrate the data. This involves adjusting the significance or contribution of individual samples within the training dataset, making it possible to remove discrimination concerning sensitive attributes without altering existing labels [13]. We used techniques from AIF360 [3] to determine these weights, based on the frequency counts associated with the sensitive attribute. To validate the proposed method, five traditional machine learning models were applied: Decision Tree (DT), Gaussian Naive Bayes (GNB), K Nearest Neighbor (KNN), Logistic Regression (LR), and Random Forest (RF). Experimental results indicate that the synthetic data generated by Tab-DDPM enhances the fairness of binary classification. For instance, both RF performance in binary classification and fairness evaluated by five evaluation metrics has been improved when"}, {"title": "II. METHODOLOGY", "content": "This paper aims to examine the effectiveness of Tab-DDPM and sample reweighting in enhancing the fairness of traditional machine learning algorithms on classification tasks, focusing on two key AI techniques: diffusion models and sample reweighting."}, {"title": "A. Tab-DDPM", "content": "Tab-DDPM [12] is a generative model for tabular data, an area of active research. Tabular datasets are often limited in size due to privacy concerns during data collection. Generative AI, like Tab-DDPM, can create new synthetic data without these privacy issues. It is a newly developed model capable of effectively generating new data from tabular datasets. In detail, the DDPM process consists of three main components: the forward process, the backward process, and the sampling procedure [14]. The forward process adds noise to the training data. The reverse process trains denoising networks to iteratively remove noise, differing from generative adversarial networks (GANs) by removing noise over two timesteps instead of one. The sampling procedure uses the optimized denoising network to generate novel data. It uses a Gaussian diffusion model for numerical data and a Multinomial diffusion model for categorical and binary features. Hyperparameters play a crucial role in Tab-DDPM, significantly influencing the model's effectiveness."}, {"title": "B. Reweighting Samples", "content": "Reweighting samples is a preprocessing technique that adjusts the significance or contribution of samples within a training dataset. Weights are strategically assigned making it possible to render datasets free from discrimination pertaining to sensitive attributes without altering existing labels. One such approach is by based on the frequency counts associated with the sensitive attribute [13].\nThis paper utilized the reweighting sample technique from the AIF360 toolbox for reweighting during the preprocessing phase. The contribution of the reweighting process comprises the training dataset with generated data of different increments with these samples containing attributes (including a sensitive attribute) and labels along with the specification of the sensitive attribute. The result being a transformed dataset where sample weights are adjusted for the sensitive attributes, mitigating potential classification bias. Throughout the reweighting process, an analysis of the allocation of the sensitive attributes within various groups is conducted. This analysis informs the calculation of reweighting coefficients, which, in turn, amends the sample weights to encourage a more uniform distribution across groups [15]. For instance, given a sensitive (protected) attribute, the privileged group of these samples includes the samples with the positive sensitive attribute while the unprivileged group of samples includes the samples with the negative sensitive attribute."}, {"title": "C. Proposed Method", "content": "The flow of the proposed method is depicted in Figure 2. The process begins with the random sampling of data, which serves as input to Tab-DDPM to generate synthetic tabular data. This synthetic data is then combined with the original training data to create a comprehensive dataset for training the ML model. In addition, reweighting samples from AIF360 is employ to adjust weights of different categories of samples to enhance fairness. Finally, the trained ML model is evaluated using test data, with performance assessed through multiple evaluation metrics, including various fairness metrics.\nTabDDPM processes numerical and categorical features using Gaussian diffusion and Multinomial diffusion, respectively. For instance, a tabular data sample $x = \\langle X_{num_1}, ..., X_{num_N}, X_{C_1}, ..., X_{C_N} \\rangle$ consists of $num_n$ numerical features and $C_N$ categorical features. Specifically, TabDDPM applies a Gaussian quantile transformation to process each categorical feature through a separate forward diffusion process, where noise components for all features are sampled"}, {"title": "III. EXPERIMENT", "content": "This study utilized the Adult Income dataset and applied Tab-DDPM to generate new synthetic data, aiming to assess the combined effectiveness of data augmentation and sample reweighting in mitigating fairness issues.\nAdult Income Dataset: The dataset consists of 48,842 samples with 14 attributes, designed to predict whether an individual's income exceeds $50K/year based on census data [16]. It was divided into training (28,048 samples), testing (16,281 samples), and validation (6,513 samples) sets. The attributes were categorized into eight categorical and six numerical features.\nSynthetic Dataset: Tab-DDPM was employed to generate synthetic samples to implement data augmentation, enhancing both AI fairness and classification performance. Synthetic data was added to the training set in varying sample sizes: 20, 000, 100,000, and 150,000 samples. Figure 3 compares the attribute distributions between the original Adult Income dataset and the synthetic datasets. The distributions of synthetic data closely resemble the original data across all sample sizes. Furthermore, the synthetic data is free of missing values, improving overall data quality. These observations suggest that synthetic data is promising for data augmentation, particularly in terms of maintaining data quality."}, {"title": "B. Evaluation Metrics", "content": "This paper utilized five evaluation metrics to determine the effectiveness of reweighting samples for mitigating bias.\nDisparate Impact (DI) refers to the unintentional bias that can occur when predictions result in varying error rates or outcomes across different demographic groups, where certain attributes like race, sex, religion, and age are considered protected. This bias may arise from training models on biased data or from the model itself being discriminatory. In this study, Disparate Impact is defined as the differential effects on prediction outcomes.\n$DI = \\frac{p_{pup}}{p_{pp}}$\nwhere $p_{pup}$ presents the prediction probability for unprivileged samples with positive predictions, while $p_{pp}$ denotes the prediction probability for privileged samples with positive predictions. A disparate impact value approaching 0 indicates bias in favor of the privileged group, while a value greater than 1 indicates bias in favor of the unprivileged group. A value of 1 reflects perfect fairness in the predictions [17].\nAverage odds difference (AOD) measures the average difference in false positive rates (FPR) and true positive rates (TPR) between unprivileged and privileged groups. It is calculated as:\n$AOD = \\frac{(FPR_{up} - FPR_{p}) + (TPR_{up} \u2013 TPR_{p})}{2}$\nwhere $FPR_{up}$ and $FPR_{p}$ represent the False Positive Rates for unprivileged and privileged samples, respectively, while $TPR_{up}$ and $TPR_{p}$ represent the True Positive Rates for unprivileged and privileged samples. An AOD value of 0 indicates perfect fairness. A positive AOD suggests bias in favor of the unprivileged group, while a negative AOD indicates bias in favor of the privileged group.\nStatistical parity difference (SPD) is to calculate the difference between the ratio of favorable outcomes in unprivileged and privileged groups. It is defined by\n$SPD = p_{pup} - p_{pp}$\nA score below 0 suggests benefits for the unprivileged group, while a score above 0 implies benefits for the privileged group. A score of 0 indicates that both groups receive equal benefits.\nEqual opportunity difference (EOD) assesses whether all groups have an equal chance of benefiting from predictions. EOD focuses on the True Positive Rate (TPR), which reflects"}, {"title": "C. Results and Discussion", "content": "To comprehensively validate the proposed method, we conduct extensive experiments in that regard of two protect attributes, namely Race and Sex, to examine the effectiveness of bias mitigation.\nRace: Table II presents a performance comparison across all outputs before reweighting samples, using one classification metric, BA, and five fairness metrics\u2014SPD, AOD, DI, EOD, and TI-on the Adult Income dataset, focusing on the protected attribute of Race. Before reweighting samples to mitigate bias, augmenting the training data does not appear to effectively reduce bias. For example, the SPD value for GNB increases with the addition of synthetic samples, indicating that synthetic data may actually exacerbate bias. Additionally, the performance of LR in terms of classification and bias mitigation remains relatively unchanged, as reflected in the BA, AOD, and TI values.\nHowever, after reweighting the samples, bias is mitigated for most ML models, as indicated by improvements in SPD, AOD, and TI values. For instance, the bias in LR significantly decreases across all fairness metrics\u2014SPD, AOD, DI, EOD, and TI. Similar trends are observed for other models like DT, GNB, and RF. Moreover, when more synthetic samples are added to the original training data, bias is further reduced, particularly in the case of the (150,000) sample size, as shown in the improved performance of LR and RF.\nSex: Table III presents a performance comparison of all outputs before reweighting samples, using one classification metric (BA) and five fairness metrics (SPD, AOD, DI, EOD, and TI) on the Adult Income dataset, focusing on the protected attribute Sex. Before reweighting samples to address bias, similar patterns are observed: augmenting the training data does not effectively reduce bias, as shown consistently across ML models like DT and LR, particularly in fairness metrics such as SPD and AOD. Additionally, LR performance in both classification and bias mitigation remains largely unchanged.\nHowever, after reweighting the samples, bias is reduced in most ML models, as indicated by improvements in SPD, AOD, and TI. For example, significant bias reductions are observed across all fairness metrics-SPD, AOD, DI, EOD, and TI-for models like LR, GNB, DT, and RF. Moreover, when additional synthetic samples are added to the original training data, bias is further mitigated, particularly in the case of GNB at the (150,000) sample size.\nAdditionally, we examine the correlation between classification performance (Balanced Accuracy, BA) and fairness performance (AOD), aiming for an ideal scenario where both are improved simultaneously. Figures 4 and 5 illustrate two cases where training data is augmented with AI-generated samples\u2014150,000 for LR and 100,000 for RF. Comparing subfigures (a) and (b) in Figure 4, it is observed that after reweighting the samples, at a classification threshold around 0.23, the AOD value for LR improves from -0.17 to 0.04, indicating enhanced fairness according to the AOD metric. However, the BA value slightly decreases from 0.75 to 0.74. Similar trends are observed in Figure 5, where at the same threshold, the AOD value for RF improves from -0.35 to 0.025, but the BA value drops from 0.75 to 0.72. These results suggest that improving fairness through augmented training with synthetic data may come at the cost of a slight reduction in classification performance."}, {"title": "IV. RELATED WORK", "content": ""}, {"title": "A. Generative Models", "content": "Generative models have a rich history in artificial intelligence, starting in the 1950s with the development of Hidden Markov Models (HMMs) [18] and Gaussian Mixture Models (GMMs) [19], which were used to generate sequential data. However, significant advancements in generative models occurred with the rise of deep learning. In natural language processing (NLP), traditional methods for sentence generation involved learning word distributions using N-gram language models [20] and then searching for the best sequence. To handle longer sentences, recurrent neural networks (RNNs) [21] were introduced for language modeling tasks, allowing for the modeling of relatively long dependencies, a capability enhanced by Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), which use gating mechanisms to control memory during training. These methods can effectively attend to approximately 200 tokens in a sample manner [22], marking a substantial improvement over N-gram models. In computer vision (CV), Generative Adversarial Networks (GANs) [23] have achieved remarkable results across various applications. Additionally, Variational Autoencoders (VAEs) [24] and diffusion models [25] have been developed to provide more fine-grained control over the image generation process, enabling the creation of high-quality images."}, {"title": "B. Diffusion Models", "content": "Diffusion models are powerful tools for generating synthetic data. The Denoising Diffusion Probabilistic Model (DDPM) is a type of latent variable model inspired by nonequilibrium thermodynamics, using a Gaussian distribution for data generation [26]. These models are not only simple to define but also efficient to train, and they can be integrated with non-autoregressive text generation methods to improve text generation quality [27]. Song et al. [28] introduced a stochastic differential equation (SDE) that gradually transforms a complex data distribution into a known prior distribution by adding noise, and a reverse-time SDE that reconstructs the data distribution from the prior by gradually removing the noise. The reverse-time SDE relies solely on the time-dependent gradient field of the perturbed data distribution. Vahdat et al. [29] proposed the Latent Score-based Generative Model (LSGM), a new method that trains Score-based Generative Models (SGMs) in a latent space within the framework of variational autoencoders for image generation."}, {"title": "C. Reweighting Samples for AI Fairness", "content": "AI fairness has emerged as one of the most critical challenges of the decade [30]. Although machine learning models are designed to intelligently avoid errors and biases in decision-making, they can sometimes unintentionally perpetuate bias and discrimination within society. Concerns have been raised about various forms of unfairness in ML, including racial biases in criminal justice, disparities in employment, and biases in loan approvals [31]. The entire lifecycle of an"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "Understanding the impact of generative modeling is crucial to preventing unintended bias when augmenting training data. This study explores data augmentation via diffusion models, aiming to reduce bias and improve overall performance. It involved evaluating model performance with the generated data added in various increments to the original dataset, and comparing the results to the original outputs using metrics including balanced accuracy and fairness metrics. Experimental results indicated the effectiveness of synthetic data generated by diffusion models for data augmentation. Future work will build on this exploration by incorporating additional datasets and comparing the effects of varying data increments. Additionally, different tools from AI Fairness 360 (AIF360) will be tested to further mitigate bias."}]}