{"title": "HAWKBENCH: Investigating Resilience of RAG Methods on Stratified\nInformation-Seeking Tasks", "authors": ["Hongjin Qian", "Zheng Liu", "Chao Gao", "Yankai Wang", "Defu Lian", "Zhicheng Dou"], "abstract": "In real-world information-seeking scenarios,\nusers have dynamic and diverse needs, requir-\ning RAG systems to demonstrate adaptable re-\nsilience. To comprehensively evaluate the re-\nsilience of current RAG methods, we introduce\nHawkBench, a human-labeled, multi-domain\nbenchmark designed to rigorously assess RAG\nperformance across categorized task types. By\nstratifying tasks based on information-seeking\nbehaviors, HawkBench provides a systematic\nevaluation of how well RAG systems adapt to\ndiverse user needs.\nUnlike existing benchmarks, which focus pri-\nmarily on specific task types (mostly factoid\nqueries) and rely on varying knowledge bases,\nHawkBench offers: (1) systematic task strati-\nfication to cover a broad range of query types,\nincluding both factoid and rationale queries,\n(2) integration of multi-domain corpora across\nall task types to mitigate corpus bias, and (3)\nrigorous annotation for high-quality evaluation.\nHawkBench includes 1,600 high-quality test\nsamples, evenly distributed across domains and\ntask types. Using this benchmark, we eval-\nuate representative RAG methods, analyzing\ntheir performance in terms of answer quality\nand response latency. Our findings highlight\nthe need for dynamic task strategies that in-\ntegrate decision-making, query interpretation,\nand global knowledge understanding to im-\nprove RAG generalizability. We believe Hawk-\nBench serves as a pivotal benchmark for ad-\nvancing the resilience of RAG methods and\ntheir ability to achieve general-purpose infor-\nmation seeking. We will release our codes and\ndata in this repository.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) excel in general\nreasoning and knowledge-based tasks but often\nstruggle with timeliness and knowledge coverage"}, {"title": "2 HawkBench", "content": "Preliminary\nRecent advancements in large language models\n(LLMs) have popularized the Retrieval-Augmented\nGeneration (RAG) approach, which leverages exter-\nnal knowledge to perform specific tasks. In RAG,\na generation model 0(\u00b7) and a retrieval model y(\u00b7)\ncollaborate to produce a final response Y. Formally,\nthe process is expressed as:\nY = 0(q, Z), Z = \u03b3(q, X),\n(1)\nwhere q denotes the input query, X represents the\nexternal knowledge base, Z is the retrieved relevant\ninformation, and Y is the generated answer.\nThis RAG framework can be viewed as an\ninformation-refinement process following the\nMarkov chain: X \u2192 Z \u2192 Y. As informa-\ntion passes through each stage, it is progressively\ndistilled, leading to the inequality I(X,Z) >\nI(Y, Z), where I(\u00b7) denotes mutual information.\nIdeally, the retrieval step should extract a Z that\nis both sufficient\u2014containing all the information\nnecessary to generate Y\u2014and minimal\u2014excluding\nirrelevant details from X. In fact, the condition\nI(X,Z) = I(Y, Z) would hold if and only if an\noptimal retrieval output Z* exists that perfectly bal-\nances these two criteria. Achieving such an optimal\nZ* is challenging due to estimation biases in both\nthe retrieval and generation processes. To better un-\nderstand these challenges, it is essential to consider\ntwo interrelated dimensions:\nReferencing The retrieval process must deter-\nmine not only which pieces of information in X are\nrelevant to the query q but also how much informa-\ntion is required. The referencing is straightforward\nwhen q explicitly states its intent, as the semantic\nconnections between q and the relevant content in\nX are easier to measure. However, for implicit\nqueries-where the intent is not clearly stated-\nidentifying the necessary evidence becomes more\ncomplex. Thus, the referencing dimension mea-\nsures how to access the relevant knowledge, cap-\nturing both the volume of information needed and\nits accessibility within the knowledge base."}, {"title": "2.2 Query Stratification", "content": "In Figure 1 (middle), we illustrate our query strati-\nfication, presenting the four query types below.\nLevel 1: Explicit Factoid Query Level 1 queries\nexhibit an explicitly stated information-seeking in-"}, {"title": "2.3 Comparison of the Four Query Levels", "content": "In Figure 1 (right), we compare the four query lev-\nels across two aspects: Reference and Reasoning.\nFirst, in terms of Reference, the amount of rel-\nevant knowledge required increases from Level 1\nto Level 4 queries, reflected in the mutual infor-\nmation between the knowledge base and retrieved\nknowledge, I(X, Z). Level 1 queries require mini-\nmal knowledge, as answers are directly extractable\nfrom a few text chunks. In contrast, higher-level\nqueries, such as Level 3 and Level 4, require syn-\nthesizing information from a broader range of texts.\nSecond, in terms of Reasoning, complexity in-\ncreases across levels due to the growing semantic\ngap between retrieved knowledge and the final an-\nswer. For Level 1 queries, reasoning is minimal,\nbut for Level 3 and Level 4 queries, more reasoning\nis needed to connect multiple, loosely connected\npieces of information. This is reflected in the de-\ncreasing mutual information I(Z, Y) as redundant\ninformation is filtered out during refinement.\nThese varying requirements for referencing and\nreasoning present significant challenges for current\nRAG systems, which struggle to adapt to the di-\nversity of information-seeking tasks. There is no\none-size-fits-all solution, as each task demands dis-\ntinct capabilities. This underscores the necessity\nof benchmarking current RAG methods across a\nbroad range of tasks to better assess their resilience."}, {"title": "2.4 Construction", "content": "Corpus Collection While most current LLMs\nare proficient in general world knowledge due to\ntheir training on large-scale corpora, they often\nlack coverage in specialized, domain-specific ar-\neas. To address this gap, HawkBench incorporates\n229 domain-specific texts into its knowledge base.\nThese texts cover a wide range of domains, in-\ncluding professional textbooks (manually labeled\ninto categories such as technology, humanities, art,\nand science), as well as financial reports, legal\ncontracts, novels, and academic papers. This di-\nverse and comprehensive collection ensures that\nHawkBench can thoroughly evaluate the domain"}, {"title": "3 Experiment", "content": "To investigate the resilience of RAG methods on\nHawkBench, we select the following representa-\ntive baseline methods: Vanilla RAG: This method\nretrieves the top passages as context. Enhanced\nRAG Methods: HyDE (Gao et al., 2023) generates\na hypothetical document to enhance query retrieval.\nRQRAG (Chan et al., 2024) rewrites the input query\ninto sub-queries to refine retrieval. Global RAG:\nThese methods index the knowledge base into an\nintermediate form to enhance global awareness.\nThis includes memory-based methods such as Mem-\nORAG (Qian et al., 2024b) and graph-based meth-\nods like GraphRAG (Edge et al., 2024).\nAdditionally, we explore the application of long\nLLMs in HawkBench, including vanilla LLMs, the\nprompt compression method Lingua-2 (Pan et al.,\n2024), and long-context acceleration methods such\nas MInference (Jiang et al., 2024). All baselines in\nthe main experiments use Qwen2.5-7B-instruct as\nthe generator (Qwen et al., 2025), with BGE-M3 as\nthe retriever (Chen et al., 2023) and the top-k set\nto 5 for all RAG methods.\nFor Level-1 and Level-2 tasks, which focus on\nfactoid queries, we use Rouge-L and lexical F1-\nscore as evaluation metrics. For Level-3 and Level-\n4 tasks, which involve rationale queries, we intro-"}, {"title": "3.1 Baselines and Metrics", "content": "duce a new evaluation metric, S-F1, defined as:\nS-F1(A, A*) = {1 \\over 2n} \\sum_{i=1}^n 1{LLM(s_i, A^*)=True} (2)\n+ {1 \\over 2n} \\sum_{i=1}^n 1{LLM(s_i,A)=True}, (3)\nwhere A* represents the ground-truth answer, and\nA denotes the predicted answer. This metric evalu-\nates: 1) The proportion of sentences in the ground-\ntruth answer s\u2208 A* that can be supported by the\npredicted answer. 2) The proportion of sentences in\nthe predicted answer si \u2208 A that correctly reflect\nthe meaning of the ground-truth answer.\nThe indicator function 1 condition returns 1 if the\ncondition holds, and 0 otherwise.\nCompared to lexical F1-score, S-F1 evaluates\nsentence-level semantic equivalence between the\nground-truth and predicted answers, making it a\nmore robust metric for rationale-based tasks as lex-\nical overlapping cannot infer the rationale equality.\nIn addition to S-F1, we also employ Rouge-L to\nevaluate Level-3 and Level-4 tasks."}, {"title": "3.2 Main Results", "content": "We conduct comprehensive experiments across all\nbaselines, with the full results presented in Table 5.\nTo provide a more detailed analysis, we examine\nthe results from multiple perspectives, offering a"}, {"title": "deep understanding of performance across differ-\nent dimensions.\nResilience across Levels", "content": "Table 2 presents the\nperformance of all baselines across the four task\nlevels, averaged by domain. From these results, we\ndraw several key insights:\n(1) Standard RAG and Enhanced RAG meth-\nods perform well on factoid queries (Level-1 and\nLevel-2), suggesting that these queries often rely\non specific text spans that can be easily located\nwith minimal reasoning or simple enhancements.\n(2) Global RAG methods underperform on\nLevel-1 and Level-2 tasks but excel on Level-3\nand Level-4 tasks. This indicates that global rea-\nsoning is not beneficial for factoid queries and may\neven hinder performance. However, for rationale\nqueries, which require synthesizing information\nfrom a broad range of text, global awareness helps\ngather more comprehensive evidence, leading to\nimproved performance.\n(3) Directly applying long LLMs to process the\nentire knowledge base is feasible but underper-\nforms on factoid queries due to over-referencing\nand redundant noise. However, for rationale\nqueries, long LLMs outperform vanilla RAG meth-\nods due to their strong reasoning ability over long\ncontexts. Efficient long-context methods, such\nas accelerated pre-filling or prompt compression,"}, {"title": "3.3 Key Insights", "content": "Current RAG methods Lack Resilience Cur-\nrent RAG methods tend to be optimized for spe-\ncific types of information-seeking tasks (e.g., fact\nretrieval or rationale generation). However, this\nspecialization leads to a lack of overall resilience\nacross a broader range of tasks. While empirical\nanalyses provide heuristics to guide method selec-\ntion for particular tasks, we still lack a systematic,\nadaptable solution that can handle diverse tasks\nwith varying requirements. This gap emphasizes\nthe need for developing RAG systems that can dy-\nnamically adjust to different information-seeking\nchallenges, moving beyond task-specific optimiza-\ntions toward a more generalized framework.\nGlobal Awareness: Construction and Utiliza-\ntion Challenges Global awareness is essential\nfor tasks that require the integration of information\nfrom multiple sources. However, current global\nRAG methods struggle with efficiently building\nand fully leveraging this awareness. While methods\nsuch as GraphRAG (which uses graph construction)"}, {"title": "Dynamic Task Understanding and Adaptive\nQuery Interpretation", "content": "As information-seeking\ntasks become more complex, the need for dynamic\ntask understanding and adaptive query interpreta-\ntion becomes increasingly important. A one-size-\nfits-all solution is not feasible; instead, RAG sys-\ntems must integrate decision-making mechanisms\nthat allow them to dynamically adjust how they\naccess (referencing) and utilize (reasoning) knowl-\nedge. By understanding the task context and adapt-\ning the retrieval strategy accordingly, RAG sys-\ntems can more effectively address a wider range of\nqueries. This adaptability would significantly en-\nhance the robustness and efficiency of RAG meth-\nods, enabling them to handle varying complexities\nand task types more effectively."}, {"title": "The Potential of Agentic Information-Seeking\nSystems", "content": "Looking ahead, agentic information-\nseeking systems-capable of autonomously navi-\ngating knowledge acquisition\u2014represent a promis-\ning frontier. These systems could perform complex\ntasks, such as writing papers or conducting liter-\nature surveys, by integrating retrieval, reasoning,\nand synthesis. Recent advancements, such as Ope-\nnAI's Deep Research, highlight the potential for\nthese agentic systems to become next-generation\nsolutions for a wide range of tasks. With the abil-\nity to autonomously manage complex information-\nseeking tasks, these systems could transform how\nwe approach knowledge-intensive tasks, making\nthem an exciting area for future exploration."}, {"title": "4 Related Work", "content": "RAG Methods RAG was introduced by Lewis\net al. (2020) to enhance language models' ability to\nhandle knowledge-intensive tasks by providing rel-\nevant context through retrieval. Research in RAG\nhas focused on two main areas: (1) improving re-"}, {"title": "5 Conclusion", "content": "In this paper, we introduce HawkBench, a\ncomprehensive framework designed to evaluate\nthe resilience of RAG systems across diverse\ninformation-seeking tasks. HawkBench is distin-"}, {"title": "Limitations", "content": "This paper focuses on constructing a benchmark,\nHawkBench, to evaluate the resilience of RAG\nmethods across stratified tasks. While the bench-\nmark provides a comprehensive framework, there\nare several limitations to consider. First, dataset\nbias may arise during the curation process, as the\nraw data are collected from multiple domains. This\ndiversity, while beneficial, may inadvertently intro-\nduce biases that could affect the generalizability\nof the results. Additionally, during the annotation\nprocess, both the assisting LLMs and human an-\nnotators may introduce errors, which could impact\nthe overall evaluation quality. Although we strive\nfor thoroughness in evaluating task and domain di-\nversity, HawkBench's size, while reasonable, may\nnot cover all professional knowledge-intensive do-\nmains or task types.\nFurthermore, while we conduct comprehensive\nexperiments using HawkBench, it is not feasible to\ntest all available RAG methods, alternative retriev-\ners, or LLMs on this benchmark. We selected rep-\nresentative methods and models that are expected\nto provide generalizable findings, but this selec-\ntion does not encompass the full range of possi-\nble approaches. Additionally, we did not evaluate\ncommercial RAG solutions in this study, as these\nsystems are typically closed-sourced and subject\nto changes over time, making them challenging to\nincorporate into a static benchmark evaluation."}]}