{"title": "DMin: Scalable Training Data Influence Estimation for Diffusion Models", "authors": ["Huawei Lin", "Yingjie Lao", "Weijie Zhao"], "abstract": "Identifying the training data samples that most influence a\ngenerated image is a critical task in understanding diffusion\nmodels, yet existing influence estimation methods are con-\nstrained to small-scale or LoRA-tuned models due to com-\nputational limitations. As diffusion models scale up, these\nmethods become impractical. To address this challenge,\nwe propose DMin (Diffusion Model influence), a scalable\nframework for estimating the influence of each training data\nsample on a given generated image. By leveraging effi-\ncient gradient compression and retrieval techniques, DMin\nreduces storage requirements from 339.39 TB to only 726\nMB and retrieves the top-k most influential training sam-\nples in under 1 second, all while maintaining performance.\nOur empirical results demonstrate DMin is both effective\nin identifying influential training samples and efficient in\nterms of computational and storage requirements.", "sections": [{"title": "1. Introduction", "content": "Diffusion models have emerged as powerful generative\nmodels, capable of producing high-quality images and me-\ndia across various applications [8, 25, 42, 44]. Despite their\nimpressive performance, the extremely large scale and com-\nplexity of the datasets used for training are often sourced\nbroadly from the internet [23, 37, 38, 40]. This vast dataset\ndiversity allows diffusion models to generate an extensive\nrange of content, enhancing their versatility and adaptabil-\nity across multiple domains [5, 19]. However, it also means\nthat these models may inadvertently generate unexpected\nor even harmful content, reflecting biases or inaccuracies\npresent in the training data.\nThis raises an important question: given a generated im-\nage, can we estimate the influence of each training data\nsample for a diffusion model? Such an estimation is crucial\nfor various applications, such as understanding potential bi-\nases [17, 26] and improving model transparency by tracing\nthe origins of specific generated outputs [7, 13, 16].\nRecently, many studies have explored influence estima-\ntion in diffusion models [11, 18, 28\u201330]. These methods\nassign an influence score to each training data sample rel-\native to a generated image, quantifying the extent to which\neach sample impacts the generation process. For instance,\nDataInf [18] and K-FAC [28] are influence approximation\ntechniques tailored for diffusion models. However, they are\nboth second-order methods that require the inversion of the\nHessian matrix. To approximate this inversion, they must\nload all the gradients of training data samples across sev-\neral predefined timesteps. Notably, in the case of the full-\nprecision Stable Diffusion 3 medium model [10], the gra-\ndient of entire model requires approximately 8 GB of stor-\nage. Collecting gradients for one training sample over 10\ntimesteps would consume 8 \u00d7 10 = 80 GB. Scaling this\nrequirement to a training dataset of 10,000 samples results\nin a storage demand of around 800 TB \u2013 far exceeding the\ncapacity of typical memory or even hard drives. Given that\ndiffusion models are often trained on datasets with millions\nof samples, this storage demand becomes entirely imprac-\ntical. Consequently, these methods are limited to LoRA\nadapters and small diffusion models such as Denoising Dif-\nfusion Probabilistic Models (DDPM) [15] or Latent Diffu-\nsion Models (LDM) [35], restricting their scalability.\nAlternatively, Journey-TRAK [11] and D-TRAK [30]\nare first-order methods for influence estimation on diffusion\nmodels, which are extended from TRAK [31] on deep learn-\ning models. Both approaches utilize random projection to\nreduce the dimensionality of gradients. However, for large\ndiffusion models, such as the full-precision Stable Diffusion\n3 Medium model, the gradient dimensionality exceeds 2 bil-\nlion parameters. Using the suggested projection dimension\nof 32, 768 in D-TRAK, store a such 2B \u00d7 32, 768 projection\nmatrix requires more than 238 TB of storage. Even projec-\ntion matrix is dynamically generated during computation,\nthe scale of these operations substantially slows down the\noverall process. As a result, they are only feasible for small\nmodels or adapter-tuned models.\nChallenges. Although these approaches have demon-\nstrated superior performance on certain diffusion models,\nthere are several key challenges remain: (1) Scalability\non Model Size: Existing methods either require comput-\ning second-order Hessian inversion or handling a massive"}, {"title": "2. Influence Estimation for Diffusion Models", "content": "For a latent diffusion model, data xo is first encoded into a\nlatent representation zo using an encoder E by zo = E(xo).\nThe model then operates on 20 through a diffusion process\nto introduce Gaussian noise and iteratively denoise it. The\nobjective is to learn to reconstruct 20 from a noisy latent zt\nat any timestept \u2208 {1,2,..., T} in the diffusion process,\nwhere T is the number of diffusion steps. Let et ~ N(0, I)\ndenotes the Gaussian noise added at timestep t. We define\nthe training objective at each timestep t as follow:\n$\\theta^* = \\arg \\min_{ \\theta} E_{z_0, \\epsilon_t} [L(f_{\\theta} (z_t, t), \\epsilon_t)]$\n(1)\nwhere \u03b8 represents the model parameters, zt is the noisy\nlatent representation of zo at timestep t, f\u03b8(zt, t) represent\nthe model's predicted noise at timestep t for the noisy latent\nzt. L(\u00b7) is the loss function between the predicted noise and\nactual Gaussian noise.\nGiven a test generation x, where xs is generated by a\nwell-trained diffusion model with parameters \u03b8\u2217, the goal"}, {"title": "3. DMin: Scalable Influence Estimation", "content": "For a given generated image xs and the corresponding\nprompt ps, the objective of DMin is to estimate an in-\nfluence score Io(XS, X\u00b2) for each training pair Xi =\n(p\u00b2, x\u00b2), where X\u00b3 = (p\u00b3, x\u00b3). Based on Equation 8, the\nI\u0189(X, X\u00b2) can be expressed as the summation of the in-\nner product between the loss gradients of the training sam-\nple and the generated image, computed with respect to the\nsame noise e across timesteps t\u2208 {1,2,\u2026\u2026,T}. Since\nthe training dataset is fixed and remains unchanged after\ntraining, a straightforward approach is to cache or store the\ngradients of each training sample across timesteps. When\nestimating the influence for a given query generated image,\nwe only need to compute the gradient for the generated im-\nage and perform inner product with the cached gradients of\neach training sample.\nHowever, as the size of diffusion models and training\ndatasets grows, simply caching the gradients becomes in-\nfeasible due to the immense storage requirements. For in-\nstance, for a diffusion model with 2B parameters and 1,000\ntimesteps, caching the loss gradient of a single training sam-\nple would require over 7,450 GB of storage, making the\napproach impractical when scaled to large datasets.\nIn this section, we explain how we reduce the storage re-\nquirements for caching such large gradients from gigabytes\nto kilobytes (Gradient Computation) and how we perform\ninfluence estimation for a given generated image on the fly\n(Influence Estimation), as shown in Figure 2. We use sta-\nble diffusion with text-to-image task as an example in this\nsection; similar procedures can be applied to other models."}, {"title": "3.1. Gradient Computation", "content": "Since the training dataset remains fixed after training, we\ncan cache the loss gradient of each training data sample,\nas illustrated in Figure 2(a). For a given training pair\nXi = (p\u00b2, x\u00b2), and a timestep t, the training data is pro-\ncessed through the diffusion model in the same way as dur-\ning training, and a loss is computed between the model-\npredicted noise and a predefined Gaussian noise \u20ac. Back-\npropagation is then performed to obtain the gradient g\u00e5 for\nthe training data pair X\u00b2 at timestep t. Once all gradients\n{91,92,\u2026,9} for X\u00b2 at all timesteps are obtained, we\napply a compression technique to these gradients and cache\nthe compressed versions for influence estimation. Further-\nmore, for tasks where only the top-k most influential sam-\nples are required, we can construct a KNN index on the\ncompressed gradients to enable efficient querying.\nForward and Backward Passes. In forwarding, follow-"}]}