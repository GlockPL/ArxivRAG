{"title": "A Comprehensive Survey and Classification of Evaluation Criteria for Trustworthy Artificial Intelligence", "authors": ["Louise McCormack", "Malika Bendechache"], "abstract": "This paper presents a systematic review of the literature on evaluation criteria for Trustworthy Artificial Intelligence (TAI), with a focus on the seven EU principles of TAI. This systematic literature review identifies and analyses current evaluation criteria, maps them to the EU TAI principles and proposes a new classification system for each principle. The findings reveal both a need for and significant barriers to standardising criteria for TAI evaluation. The proposed classification contributes to the development, selection and standardization of evaluation criteria for TAI governance.", "sections": [{"title": "1 Introduction", "content": "AI has become prominently used for decision-making across various sectors, including finance, healthcare, judiciary, and insurance[1]. 72% of businesses are now using AI in at least one business function, with a spike in generative AI adoption which doubled in ten months to a 65% adoption rate[2]. Additionally, black box algorithms are increasingly being commercially employed to make critical safety decisions in areas such as"}, {"title": "2 Background", "content": "The following section discusses the fundamental concepts of Trustworthy AI (TAI) and provides a background to TAI assessment.\nTAI refers to ethical considerations that must be considered when building and using AI systems[23]. AI4People published their core principles for AI in their Ethical Framework for a Good AI Society in 2018[24], foundational for the subsequent work undertaken in this area, including the work by the EU. Their five critical metrics for trustworthy AI were fairness and non-discrimination, technical robustness and safety, transparency and explainability, accountability and human oversight. They offered vital advice for trustworthy AI, including what they counselled as the most crucial recommendation, which was to design an explicit process for assessing AI risk and mitigating it for each application of AI, explicitly calling out the need for the development of agreed-upon Trustworthy AI metrics to enable user-driven benchmarking. Some participants later challenged the success of the AI4People programme, highlighting a lack of diversity of experts involved in the media industry[6]. The European Commission's independent High-Level Expert Group for Artificial Intelligence (AI HLEG) published Ethics Guidelines for Trustworthy AI[7] in 2019, which included seven principles that AI systems should adhere to. In July 2020, following a piloting scheme, the AI HLEG subsequently developed The Assessment List for Trustworthy Artificial Intelligence (ALTAI)[8], an assessment booklet and some proposed questions that aim to assist in actioning the seven TAI principles. It advised that TAI guidelines be implemented in all stages of the AI system. The seven TAI principles are human agency and oversight, technical robustness and safety, privacy and data governance, transparency, diversity, non-discrimination and fairness, societal and environmental well-being, and accountability [8]. Building on this work, the EU drafted a proposal for the AI act. It was partly criticized because it did not provide an assessment criterion and left businesses responsible for self-assessment[6]. ISO/IEC 42001[14] offers a high-level specification for Artificial Intelligence Management Systems and is considered a key document to consider concerning conformity with the EU AI Act[9].\nThere is a clear need for a governance framework to measure trustworthy AI[11][12][13]. Haupt et al. (2021) [25] called for more standardized metrics for trustworthy AI, noting that while some standards existed for evaluating various AI trust metrics in their use case of ML in weather and climate, there was a lack of standardization around the metrics used. Standard identification and management is also outlined as a key step in the AI tool-supported audit process outlined by Ojewale et al.[26]. Their research into AI Audit Tooling also raised the need for metrics and benchmarks for AI evaluation.\nWe found that the high-level TAI governance frameworks we have outlined in this section were either influenced by or heavily overlapped with the seven EU TAI principles. For this reason, we chose to use these seven principles as a classification for summarising and commenting on assessment approaches and evaluation criteria for TAI.\nWhen we refer to Trustworthy AI (TAI) throughout this paper, we do this through the lens of these seven EU principles. Trust metrics are metrics used to score AI within these seven TAI principles."}, {"title": "2.1 Seven EU Principles for Trustworthy AI", "content": "This section gives an overview of the seven ethical principles for developing and using AI systems proposed by the High-Level Expert Group informing European strategy on AI 2019[7], which were developed further in 2020 in the assessment list for trustworthy AI (ALTAI) [8], and subsequently incorporated into the design of the EU AI Act[27].\nWe also include a table of specific terms and outline the specific context for those terms as we refer to them throughout this paper.\n\u2022 Human agency and oversight: The first of the seven ethical AI principles is human agency and oversight. AI systems should be designed not to replace human autonomy but to enhance and support human decision-making. It advises that humans should be able to intervene in AI decisions when required.\n\u2022 Technical robustness and safety: AI systems should be dependable. This TAI principle stipulates that from a security perspective, they should be resilient to attack. They should be designed to consider possible threats, and risks should be defined and mitigated as much as possible. The data used to train the AI should be accurate. There should be fallback plans in place. In cases where the AI is designed for continuous self-learning, fallback plans and constant security, robustness and safety reviews are essential to consider.\n\u2022 Privacy and data governance: This TAI principle considers the area of privacy and data governance. AI systems should be in line with privacy and data protection laws. The most relevant of these in the EU is the General Data Protection Regulation (GDPR). Where data is collected, it should be transparent about which data they collect, how it is used, and who can access that data.\n\u2022 Transparency: Transparency, or explainable AI (XAI), is a crucial principle of trustworthy AI. AI systems should be explainable and understandable. Users should be able to understand the actions or decisions made by the AI quickly. Decision-making processes by the AI should be documented to allow for traceability.\n\u2022 Diversity, Non-discrimination, and Fairness: This TAI principle advises that training and decision-making should consider diversity, non-discrimination, and fairness. AI systems should be designed to avoid bias and discrimination in the decision design and how the AI gets trained. The system should also be designed to be inclusive and accessible to all, regardless of any personal protected classes, such as race or gender. They should incorporate elements of accessibility and universal design. Systems should be tested for bias and mitigated to avoid bias and discrimination.\n\u2022 Societal and environmental well-being: Broader societies, both human and other sentient beings, should be considered in the design of AI systems. This TAI principle of trustworthy AI advises that AI systems should be designed to benefit society and the environment. One key aspect of this is to align the AI design with the global sustainable development goals (SDGs) and promote social responsibility. When AI is introduced in the workplace, specifically when it impacts or replaces human roles, the team members should be considered and consulted in designing the AI system.\n\u2022 Accountability: The principle of accountability states that AI systems should be designed in a way that they can be audited and, therefore, held accountable for"}, {"title": "2.2 EU AI Act", "content": "The EU AI Act[27] requires high-risk AI systems to come with detailed explanations of their design and build, including documentation on their metrics and thresholds for accuracy, robustness, as well as potential discriminatory impacts. They must quantify the degrees of accuracy for specific persons or groups, as well as the accuracy in relation to the specific purpose of the AI system. They must also provide a detailed description of the appropriateness of the performance metrics for the specific AI system. These systems must also include documentation showing compliance with other requirements in the Act. These include establishing foreseeable risks to health and safety or other unintended outcomes, and the AI system's cybersecurity capabilities against things such as data poisoning and adversarial attacks. The act also stipulates that appropriate degrees of transparency and compliance must be reached. The act also calls for the inclusion of the ethics guidelines for trustworthy AI[7], when designing and using AI systems.\nThe EU AI Act stipulates the importance of regulatory oversight and the integration of appropriate safeguards. The act stipulates the importance of the development of standards AI systems must adhere to, which would have to include a balanced representation of interests and involve relevant stakeholders. They also propose the development of national or international regulatory sandboxes to facilitate the testing of AI systems. The Act seeks to enhance existing regulations that exist at a sector level, such as legislation for credit institutes or the medical sector, that do not include AI-specific legislation."}, {"title": "3 Methodology", "content": "This section details the methodology of our study, which employed a systematic literature survey technique. The study was conducted in three phases: (i) actively planning, (ii) conducting and reporting the review results, and (iii) exploration of research challenges. The systematic survey described in this paper followed the widely accepted guidelines and process outlined accepted guidelines and process outlined in Pai et al.[28] and Kitchenham et al.[29]. The remainder of this section details the research questions, the process for identifying research, and the data extraction process."}, {"title": "3.1 Research Questions", "content": "The following are the identified research questions (RQs) for this review:\n\u2022 RQ1: What are the main initiatives to establish standards and best practices for determining the trustworthiness of an AI system?\n\u2022 RQ2: What metrics or criteria are currently used to evaluate AI systems for trustworthiness?\n\u2022 RQ3: Are there differences in how each principle of Trustworthy AI is currently being evaluated and scored?\n\u2022 RQ4: Are there works focusing on scoring all the TAI principles?\n\u2022 RQ5: What are the issues and challenges that hinder the development of a scoring and assessment system for the trustworthiness of an AI system?"}, {"title": "3.2 Identification of Research (Search Strategy)", "content": "To conduct this survey, a search string for Google Scholar was designed to capture papers discussing topics in the machine learning, trust and scoring arenas.  shows the string used.\n(\"Machine Learning\" OR \"Artificial Intelligence\" OR \"Supervised\nLearning\" OR \"NLP\")\nAND\n(\"Framework\" OR \"Scoring\" OR \"Algorithm\" OR \"Rating\")\nAND\n(\"Bias\" OR \"Fairness\" OR \"Trust\" OR \"Explainability\" OR \"Ethics\" OR\n\"Governance\" OR \"Transparency\").\nThe initial search returned 971 papers selected for review related to the research topic. A set of inclusion and exclusion criteria, shown in Table 1, were defined to enable the selection of papers to include in this study to be carried out in a systematic manner. Two researchers independently screened titles in line with Kitchenham et al.[29]. During the subsequent phase, the full text of the selected papers was thoroughly reviewed. Throughout all three stages, any disagreements regarding the inclusion or exclusion of a paper were resolved through discussions until a consensus was reached.\nThe inclusion and exclusion criteria for papers that contributed to the evaluation criteria classification are detailed in table 1. To ensure all literature review papers covering AI trustworthiness were captured, a second search string term, 'Review of the literature trustworthy AI', was used. A further 11 papers were reviewed to enhance the background, but they did not contribute directly to the paper's main findings. An additional 14 documents were added through citation tracing (snowballing) of some of the critical academic, ISO, and EU publications, culminating in 63 publications contributing to the core findings."}, {"title": "3.3 Data Extraction", "content": "The papers were manually reviewed by two authors independently. For each one of the 63 returned papers, the following information was compiled: bibliographic data, the contribution towards the domain of TAI evaluation, the TAI evaluation criteria, the use case and the type of validation used (if applicable). The data was then compared and aligned, with discussions taking place if any inconsistencies were found."}, {"title": "4 Review Results", "content": "In this section, we examine the papers acquired through our systematic review. We summarize their bibliographic details, including the number of publications per year and the publication venues. Subsequently, we dive into a more detailed analysis where we explore the paper content, including the classification of evaluation criteria by TAI principle."}, {"title": "4.1 Preliminary Results", "content": "There has been a notable increase in publications in TAI from 2020 onwards as illustrated in Figure 3. This aligns with heightened interest from regulatory bodies such as the European Commission.\nThe diversity of publication platforms, including several ISO standards and EU TAI publications, reflects the vast array of stakeholders engaged in the field of TAI, spanning regulatory bodies, industry, and academia. This diversity is detailed in Figure 4."}, {"title": "4.2 Analysis", "content": "We identified the papers that included scoring or evaluation of the seven TAI principles. We include a subsection for each principle and a subsection focused on papers that sought to score for multiple TAI principles. Although we include considerations from the more recent and detailed EU documents such as ISO42001[14] and the AI Act[27] throughout this paper, our primary classification is high-level and informed"}, {"title": "4.3 Evaluation Criteria for Trustworthy AI", "content": "In this section, we use the evaluation criteria in the literature to propose a classification of criteria to evaluate individual TAI principles, including specific metrics where available. We found that most papers returned involved the evaluation of individual principles. In addition to this classification, this section includes a discussion scoring for multiple TAI principles. Our proposed classification is detailed in . Additionally,  seeks to provide further clarity and guidance around key evaluation terminology. These definitions are grounded in the definitions provided in the EU\nAI Act[27], with additional context provided in relation to their usage as evaluation criteria based on the findings from this paper."}, {"title": "4.3.1 Evaluation Criteria for Diversity, Non-discrimination, and Fairness", "content": "The evaluation of fairness is one of the most researched TAI principles, with many metrics already developed [1]. The maturity of this TAI principle is also reflected in its governance. The EU AI Act [27] requires certain AI systems to produce performance metrics, including accuracy metrics for specific people or groups, which is a way to quantify fairness. This section builds on the literature, proposing a high-level classification for the commonly used fairness metrics and several more recently developed fairness metrics. We extracted fairness metrics from the reviewed papers and classed them based on their underlying principles. Additional fairness evaluation considerations, such as accessibility, which are outlined in the ALTAI and have no established evaluation metrics, are also included. Caton and Haas [31] classified metrics into either group fairness or individual and counterfactual fairness metrics. This section expands on this existing classification and proposes three additional high-level classifications: Intersectional Fairness Metrics, Complex Fairness Metrics, and Inclusive Design and Participation Metrics, outlined in Figure 5.\nGroup Fairness and Individual and Counterfactual Fairness\nThere are many established metrics for fairness evaluation, with two key papers, Zhou et al. [18] and Pagano et al. [1], offering summaries of the usage of several standard"}, {"title": "Intersectional Fairness", "content": "The approaches to fairness evaluation introduced by Zhang et al. [32], Lee et al. [33], and Ferry et al. [34] provide a case for a new high-level \"Intersectional Fairness Metrics\" classification. Zhang et al. [32] introduced a semi-supervised learning technique to improve fairness, which scored the dependency of a model on a set of protected attributes for their use cases in the areas of health and finance. This combination of techniques does not fit the original classification proposed in Caton and Hass (2020).\nSimilarly, Lee et al. [33] developed a fairness metric by measuring the dependency or impact of protected classes to assess their technical solution to increase fairness for their use case in crime rate prediction. Their maximal correlation framework sought to improve this fairness metric in ML algorithms by again reducing dependencies between variables, which included protected metrics like race or gender, thus getting a better balance between accuracy and fairness, again showing the need for a new class which involved more complex relationship to multiple pieces of sensitive data."}, {"title": "Inclusive Design and Participation", "content": "The ALTAI includes the additional fairness considerations of accessible universal design and stakeholder participation[8]. Both fairness considerations are also considered in the EU AI Act and ISO/IEC 42001[14][8]. In the literature, assessing for stakeholder participation is also included in questionnaires that seek to evaluate AI Systems for Trustworthiness[36][37][38]. The questionnaire published by Landers and Behrend [36] also proposed an inclusive design assessment. Due to the inclusion of these fairness considerations in the literature, we suggest including a new classification for fairness evaluation, which assesses both Inclusive Design and Stakeholder Participation.\nAlthough Universal Design does not have established metrics, the ALTAI suggests the ISO standardization for Universal design as an evaluation approach for this metric[8]. Landers and Behrend [36] also proposed evaluating for conformity with the Universal Guidelines and along with other ethical standards.\nFor stakeholder participation, the ALTAI offers one high-level generic question asking if stakeholders were used in the AI systems design. This question is extended in ISO/IEC 42001[14], which asks the organization to ensure relevant experts and human resources are available during the entire AI lifecycle. It also refers to stakeholder considerations such as the requirements of regulatory or other third-party bodies and the impact of the AI system on different stakeholders. Evaluation of stakeholder participation is also included in the latest literature in the area; for example, Chaudery et al.[37] and Fehr et al.[38] both propose questions about the inclusion of experts during the AI design and operation stages. Landers and Behrend [36] also propose questions about community participation during the design process and the impact of the AI system on stakeholders, including assessing the reaction of those impacted. This area would require further study to develop a more comprehensive quantifiable evaluation approach.\nAlthough there are some technical metrics currently being used to evaluate fairness in ML, there are issues with these, such as a lack of available standardization and benchmarks. Additionally, these metrics focus primarily on assessing ML outcomes for groups or individuals without considering the broader concept of fairness. For a comprehensive fairness evaluation, these wider considerations need to be evaluated."}, {"title": "Complex Fairness Evaluation", "content": "In this section, we include several complex purpose-built metrics created to assess technical solutions proposed to improve fairness. These are primarily created in papers where researchers developed, adapted and combined factors to create fairness metrics to suit their use cases. Due to the complexity of these fairness metrics, they do not fit under the other primary classifications. Thus, we propose a new classification of Complex Fairness Metrics.\nMany of these metrics were combined with other metrics, with trade-offs already defined for the specific use case. Lee[39] considered commonly utilized fairness metrics such as parity and equalized opportunity flawed. Instead, they measured the aggregate benefit for their use case of loan approval by scoring for increased financial inclusion and measured inequity by scoring the negative impact on minority borrowers. Singh et al. [40] also developed a custom metric to score fairness in loan approval and expanded on this by providing a benchmark for acceptable fairness. Their universal fairness metric is titled Alternate World Index (AWI) and includes proposed levels for what they considered a balanced trade-off of accuracy and AWI. The proposed standardization of evaluation metrics and the inclusion of suggested levels for acceptable fairness make this fairness metric available for adoption by industry. Khalili et al. [41] developed a formula to create a new fairness metric that expands on the equalized opportunity fairness concept, adapting it to include group fairness. Although this was a custom-developed formula to score for fairness, it could be also considered a demographic parity fairness metric. It was tested for multiple use cases, including loan approval and job application fairness. Krasanakis and Papadopoulos [42] propose a Phython library called Fairbench, which computes multiple fairness metrics including representation bias, group and individual fairness and parity metrics to create a standardized approach for metrics to assess fairness.\nA number of complex fairness metrics are proposed in the area of generative AI. Due to models being trained on large amounts of uncurated data, there are significant fairness concerns in this area[43]. Barza[44] developed an exciting metric to assess fairness in natural language processing (NLP). They developed a sentence-based evaluation technique to score for fairness using sentence likelihood difference (SLD). The author's NLP bias metric and technique assessed the three classes of gender bias they identified. This process involved creating a Triple Gender Bias dataset, which is used to score for bias with SLD. Platek[45] prepared metrics to evaluate Task-oriented Dialogue (ToD) systems and Text-to-Speech Synthesis (TTS). Their open-ended dialog metrics were based on SelfSupervised Learning (SSL) Models, and were enhanced by contrastive losses. When evaluating language, researchers noted complexity due to difficulty in reproducibility[46], along with challenges including pragmatics, semantics and grammar[45]. Teo et al. [47] reviewed existing fairness evaluators and proposed a"}, {"title": "4.3.2 Evaluation Criteria for Transparency", "content": "In contrast to fairness, there is limited research into evaluation criteria for scoring transparency, including a lack of standardization[15]. However, notable categorizations provide a starting point for evaluating AI transparency [15][3]. Bommasani et al.[48] proposed the Foundation Model Transparency Index (FMTI), which includes a comprehensive set of indicators for transparency evaluation relating to multiple TAI principles, including accountability and traceability. Fehr et al.[49] also proposed evaluating five key areas: intended use, algorithmic development, ethical considerations, technical validation and quality assessment, and deployment caveats. When considering evaluating transparency, both Bommasani et al. and Fehr et al. proposed scoring the criteria based on public availability and accessibility. However, there are wider considerations needed, such as the variation in transparency requirements by stakeholder, with research[50] finding that stakeholders such as engineers and legal experts require different types of transparency.\nThis paper considers the evaluation of auditability and traceability within the TAI principle of accountability rather than in this transparency section. Additionally, although model performance metrics such as accuracy and reliability are correlated with XAI[22], these do not evaluate or score the transparency itself; thus, they are also outside the scope of this section.\nThis section proposes three high-level classifications for criteria to evaluate the transparency of an AI system: Data Transparency, Model Transparency, and Outcome Transparency."}, {"title": "Data Transparency", "content": "Researchers have proposed evaluation considerations around data transparency with a focus on data collection transparency, including transparency of assumptions made, consent and sensitive data collection [51], [38], [37], [36]. Data processing transparency, including evaluating if there is disclosure of how data was cleaned and engineered into model features and classes and if any assumption testing was undertaken was also considered [51], [38], [37], [36]. ISO/IEC 420001[14] includes a classification for data in AI Systems. This aims to ensure data transparency to enable the organization to understand the role and impact of data on the AI system. Their four proposed areas for assessment are as follows: data for the development and enhancement of AI systems, acquisition of data, quality of data for AI systems, and data provenance and data preparation. Evaluation of the transparency of data quality throughout the lifecycle enables control over critical steps like data collection, annotation, feature engineering, safeguarding against biases and ensuring the reliability and representation of datasets"}, {"title": "Model Transparency", "content": "Model behaviour, model selection and model explanation are three key areas for transparency. We included model transparency as a high-level classification for transparency due to the importance placed on three aspects of model transparency in the literature and related ISO standards detailed in this section.\nModel behaviour transparency is a critical part of the understandability of an AI system and includes the interrelated areas of model selection, development, and explanation [15]. Model selection, development, and explainability are critical aspects of AI transparency due to their essential role in how AI operates, as highlighted by the literature discussed in this section. This section explains the importance of transparency for model selection, development, and explanation.\nModel selection and development is a crucial area for AI transparency as decisions around model selection directly relate to the trade-offs between trust principles, for example, accuracy and interpretability[39]. Transparency around model selection and development includes considerations such as appropriateness for intended use, sufficient stakeholder consultation, model selection, and model testing[38][37],[36]. Model selection should include assessing if alternative architectural designs were evaluated, such as using sampling, experimental design, variable choices, analysis, and interpretation, and assessing if these impacted the validity of conclusions[36]. ISO/IEC 42001 requires organizations to maintain documentation around model transparency, including the algorithm type, model training, model evaluation and model refinement [14].\nModel explanation is another vital part of model transparency and helps people understand how a model works to make decisions or predictions[48]. Although this is a well-researched area, with evaluation criteria for rule-extraction techniques being discussed as early as 1995[53], there is still no standardization or agreement on what a model explanation is for a black box algorithm[3]. Despite this lack of standardization, there are many commonly used model explanation techniques, such as SHAP[54] and LIME[55], albeit with some concerns over their limitations[56] [57] [58]. ISO/IEC 25059 [59] suggests using systems documents, logs or introspection tools and data files to improve transparency and states that low transparency systems are ones where internal workings are challenging to inspect externally."}, {"title": "Outcome Transparency\u0443", "content": "The AI legislation, ISO standards and the literature discussed in this section highlight the importance of evaluating the transparency of outcomes of the AI system. This includes transparency around the potential impact on society, outcome explainability, and the ability to challenge the outcome. While there are no metrics developed specifically to quantify AI outcome transparency, this section explains the criteria to be considered for evaluation.\nThe potentially dangerous impact of AI on society is heavily criticized[60]. ISO/IEC 42001 aims to help organizations manage their AI systems responsibly and includes impact assessment sub-sections in both the planning and operation sections of the document[14]. It offers comprehensive considerations for evaluation, including the impact on culture, values, norms, ethics, legal, and contractual or regulatory obligations. It recommends the consideration of these impacts be documented for transparency. The impact of AI systems was also a focus for researchers looking to evaluate AI systems for Trustworthiness[36], [37]. FMTI, the transparency index proposed by Bommasani et al.[48] evaluated the downstream impact of the model on market sectors, individuals, and geographies, as well as future applications. Their results found that downstream impact transparency was virtually non-existent, highlighting this as an area for further research. Both the AI outcome and the ability to challenge the outcome are critical questions for evaluating AI systems for transparency [51]. Commonly used tools like SHAP and LIME can be enhanced through tools like PLENARY, which uses SHAP outputs to explain better the model's decision[15]. Outcome transparency could be scored by checking for the presence of commonly used tools such as those detailed in the taxonomy of interpretability model outputs proposed by Mittelstadt[61] or the explainability methods detailed in Llorca et al. [62]. Another common way to assess model outcome transparency is through user trust, which researchers frequently use as an assessment criterion to measure the success of introducing improvements in XAI, consistently demonstrating a correlation between enhanced explainability and increased user trust[63] [64], [65]. This metric is generally assessed by asking the human user of the AI system how much trust they have in the results or decisions provided by the AI system. Jia[66] also proposed a method of clinical validation involving an evaluation of user satisfaction, and Feher et al. (2022) [38] included user trust as a metric in their questionnaire designed to score transparency. While user trust is correlated, technology, context, social, user-related, and organizational factors contribute to user trust in AI[67].\nThe transparency assessment frameworks proposed by Bommasani et al. [48] and Fehr et al. [49] provide a starting point for evaluating AI transparency; however, they both primarily offer high-level questions to assess the transparency of the system, in conjunction with various aspects of Trustworthy AI. There is a need for more quantifiable metrics, standardization, and benchmarks to assess transparency. A challenge arises due to the varying levels of transparency and explanations required at different stages of AI development and operation, which are demanded by different types of stakeholders [66]. Further research would need to be done to develop the existing literature around evaluation criteria and classifications for transparency into a practical scoring mechanism."}, {"title": "4.3.3 Evaluation Criteria for Human Agency and Oversight", "content": "Although specific scoring metrics for this TAI principle have not been heavily researched, criteria to evaluate human agency and oversight as part of TAI evaluations are proposed in the literature. In this section, we propose four key evaluation areas for this TAI principle, which are informed by the ALTAI[8], ISO/IEC 42001[14], and by research papers in this area[38][37][36]."}, {"title": "Human Control", "content": "The importance of human control is highlighted through the focus given to this area within the ALTAI. The ALTAI proposes both a high-level assessment of societal dependence on a system, as well as individual risks of loss of independence or addiction. Areas to assess for human control include the levels of user control, perceived feedback quality and perceived feedback [65]). Another consideration for determining human control is establishing if the human has been adequately trained to use the AI system [37], ISO/IEC 42001[14]. The focus on human control in the literature and this ISO standard show the need to evaluate the area of human control.\nAn AI system's ability to stop is another element of human control and includes considerations such as whether there are limits identified for where a human is required to intervene and for the AI to stop completely [37]. There is a requirement for an AI to have the ability to be stopped to limit negative impacts when things go wrong, in particular for autonomous systems impacting humans such as autonomous vehicles[68]."}, {"title": "Human-AI Relationship", "content": "The literature highlights that human involvement in AI systems is a crucial area for this TAI principle. Considerations for evaluating the human-AI relationship include evaluating the level of human-in-the-loop involvement[37], and the level of human oversight by an expert in the field[38]. Additionally, training for AI oversight is important for the Human-AI relationship[14].\nGuo[65] proposed evaluating user satisfaction, including scoring for perceived system trust and system satisfaction. Evaluating user satisfaction, user trust, and user understandability of the system are considerations for evaluating the human-AI relationship. This overlaps with the area of transparency, which also correlates with user trust (section 4.2.3). The ALTAI also proposes evaluating the TAI principle of Human Agency and Oversight, which overlap with transparency, including assessing the ability of humans to understand the decision made by an AI system and how the system reached its conclusion. We see an additional overlap with transparency in the literature through a positive correlation of user trust metrics with increased transparency and increased human control [39] [65]. Although user satisfaction overlaps with other TAI principles, these points also highlight it as a critical area for assessing the TAI principle of Human Agency and Oversight.\nThere is limited research on quantifiable metrics for the assessment of the TAI principle of Human Agency and Oversight, however, the literature in this area highlights a need for evaluating specific aspects in the areas of human control, and the human-AI relationship."}, {"title": "4.3.4 Evaluation Criteria for Privacy and Data Governance", "content": "Privacy and data governance for AI and non-AI systems intersect, so existing policies can be utilized or extended for AI systems[14]. Evaluating these areas for AI is not a widely researched area. The papers returned from the search string for this paper. For this reason, we did not recommend any change to the high-level classifications proposed in the ALTAI and discussed the evaluation criteria in the literature in this area."}, {"title": "Privacy", "content": "In this section, we include privacy metrics found in the literature that can be used to assess privacy in AI systems.\nDifferential privacy, as a statistical notion of privacy, aims to provide a way of maximising accuracy while minimising risk and is widely researched and deployed[41]. Khalili et al.[41] proposed utilising differential privacy to quantify and improve privacy through the introduction of an exponential mechanism which introduces randomness in decision-making. This level of randomness can be controlled through an epsilon ($\\epsilon$) score. In this instance, the Epsilon is a parameter used to represent privacy by controlling randomness. A lower $\\epsilon$ value means a higher level of privacy due to introducing more randomness, and a higher $\\epsilon$ indicates a lower level of privacy and a higher level of accuracy through introducing less randomness. This allows the $\\epsilon$ to decide the trade between privacy and accuracy in an ML model.\nLee [33] and Ferry et al. [34] developed metrics to score the impact of sensitive attributes on a model; while their goal was fairness evaluation, this technique could also be used to evaluate privacy. This was undertaken by Li[69] who developed a privacy evaluation and improvement process which was based on evaluating the usage of sensitive data in their model. Other areas which are important for privacy include how data is aggregated, network compression and cryptography [17]), making these areas important for evaluation.\nEvaluating the level of data leakage from a model was also researched by Van der Valk and Picek[70], and Murakonda et al. [71] who both looked at assessing the level of data leakage from a model. Van der Valek et at. evaluated how well a malicious agent could learn a secret key from the model. Murakonda et al. built on established algorithms like OpenDP and TensorFlow Privacy, to develop their ML Privacy Meter which quantifies privacy risks to training data for ML models."}, {"title": "Data Governance", "content": "AI has the potential to significantly change organizational objectives, meaning existing governance may no longer be fit for purpose, and the governing bodies accountable for all activities in an organization may need to change policies to incorporate AI, including data governance (ISO/IEC 38507) [72]. This section discusses evaluation criteria discussed in the literature in the areas of data collection, data processing, data compliance and data consistency.\nData collection and processing are key considerations when assessing AI systems for Trustworthiness [51] [38], [37], [36]. Section 4.3.2 of this paper explained the importance"}, {"title": "4.3.5 Evaluation Criteria for Technical Robustness and Safety", "content": "The TAI principle of technical robustness and safety is widely researched in ML. In this context, we specify evaluation considerations for this principle, including resilience to attack and security, general safety, accuracy, reliability, fall-back plans and reproducibility."}, {"title": "Safety and risk", "content": "Safety and risk evaluation are well-researched areas, with a number of proposed evaluation metrics in this area. These topics can be considered in line with standard information security evaluation criteria such as those in ISO/IEC 27001[77", "Abramson[78": "measured safety against adversarial attacks to assess the success of their proposed ML privacy-preserving trust framework. The authors quantified this by measuring the success of requests made by their malicious agents, which were designed to attempt to create self-signed credentials to request access to their model. Papadopoulos et al. [79", "Picek[70": "scored the safety of an AI system by using ML to guess classified information by reviewing how data can be leaked unintentionally. The tool they created evaluated how well the model learned a secret key. The researchers scored and analyse the performance of ML-based"}]}