{"title": "Multimodal contrastive learning of urban space representations from POI data", "authors": ["Xinglei Wang", "Tao Cheng", "Stephen Law", "Zichao Zeng", "Lu Yin", "Junyuan Liu"], "abstract": "Existing methods for learning urban space representations from Point-of-Interest (POI) data face several limitations, including issues with geographical delineation, inadequate spatial information modelling, underutilisation of POI semantic attributes, and computational inefficiencies. To address these issues, we propose CaLLiPer (Contrastive Language-Location Pre-training), a novel representation learning model that directly embeds continuous urban spaces into vector representations that can capture the spatial and semantic distribution of urban environment. This model leverages a multimodal contrastive learning objective, aligning location embeddings with textual POI descriptions, thereby bypassing the need for complex training corpus construction and negative sampling. We validate CaLLiPer's effectiveness by applying it to learning urban space representations in London, UK, where it demonstrates 5 - 15% improvement in predictive performance for land use classification and socioeconomic distribution mapping tasks compared to state-of-the-art methods. Visualisations of the learned representations further illustrate our model's advantages in capturing spatial variations in urban semantics with high accuracy and fine resolution. Additionally, CaLLiPer achieves reduced training time, showcasing its efficiency and scalability. This work provides a promising pathway for scalable, semantically rich urban space representation learning that can support the development of geospatial foundation models. The implementation code is available at https://github.com/xlwang233/CaLLiPer.", "sections": [{"title": "1. Introduction", "content": "Accurate and comprehensive characterisation of urban spaces help us better understand how cities develop and function (Carmona, 2021; Gill et al., 2008), upon which we can further renovate and improve existing urban environment or future planning processes in order to tackle pressing challenges such as spatial inequalities (Nijman and Wei, 2020) and sustainability (Puchol-Salort et al., 2021). Point of interest (POI) data have been widely used for characterising urban spaces, as they provide crucial information about both \"where\" (spatial) and \u201cwhat\" (platial) aspects of cities (Goodchild, 2020), capturing fine-grained and up-to-date details depicting the land use composition, urban facilities distribution, and socioeconomic fabric of cities (Liu et al., 2020).\nIn today's booming age of big data and artificial intelligence, urban studies increasingly rely on computational methods to model, simulate, and analyse urban environments. Consequently, there has been extensive research focused on deep representation learning of urban spaces from POI data. Such representation learning are performed on various geographical scales, ranging from locations (Hong et al., 2023), neighbourhoods (Huang et al., 2021; Wang et al., 2020), to regions (Huang et al., 2022; Niu and Silva, 2021) and city level (Huang et al., 2023). And the learned representations have found widespread applications in urban functional distribution mapping (Huang et al., 2022, 2023), land use classification (Jean et al., 2019) socioeconomic indicator estimation (Jean et al., 2016), footfall prediction (Feng et al., 2017) and individual's next location prediction (Hong et al., 2023).\nExisting urban space representation learning approaches are largely unsupervised or self-supervised, aiming at learning general purpose representations that can generalise to various downstream tasks. These methods propose to"}, {"title": "2. Related work", "content": "Our work builds on best practices from a range of studies. We adopt location encoding in our urban space encoder and utilise well-trained text encoders from the NLP domain to extract rich semantic information from textual descriptions of POIs. Additionally, we employ a contrastive multimodal representation learning process to encode this semantic information into the location encoder. In the following subsections, we review not only existing methods for urban space representation learning but also the relevant techniques incorporated into our model."}, {"title": "2.1. Learning urban space representations with POIs", "content": "There has been extensive research on learning numerical representations of urban spaces. Early studies utilised topic models to derive these representations, with TF-IDF (Sparck Jones, 1972) and LDA (Blei et al., 2003) being the most commonly applied. These models have been used for region function discovery (Yuan et al., 2012) and semantic enrichment of places (Cheng and Shen, 2018; Shen, 2018). Later, following the success of Word2Vec models in modelling the distributed representation of words (Mikolov et al., 2013), numerous studies adapted this approach to learn distributed representations of POIs and aggregate the POI embeddings to various urban spaces. The general workflow of these methods consists of two steps: first, learning POI category embeddings, and second, deriving the urban space representations by aggregating the POI category embeddings within specific areas.\nLearning POI category embeddings requires a POI co-occurrence corpus that can constructed in various ways. Yao et al. (2017) utilised shortest path algorithm to traverse all POIs in a given area and extract the co-occurrence corpus from the resulting POI sequences. Yan et al. (2017) proposed the Place2Vec model, which identifies context POIs using a K-nearest-neighbor (KNN) method. Niu and Silva (2021) constructed the corpus along road networks while Huang et al. (2022) built a POI network through Delaunay triangulation and captured the spatial co-occurrence patterns through random walks on this network.\nFor the aggregation of POI embeddings into urban space representations, most studies have employed an average pooling function. Niu and Silva (2021) used the Doc2Vec model to obtain urban region embeddings as a by-product of the POI embeddings. Huang et al. (2022) used a long short-term memory (LSTM) model and attention mechanisms to aggregate POI embeddings to urban region embedding through a supervised training objective. More recently, Huang et al. (2023) employed multi-head attention and graph neural networks (GNNs) in their aggregation process and introduced hard negative sampling strategy and a contrastive training objective to facilitate fully unsupervised learning.\nWe argue that the POI category embedding aggregation-based paradigm for learning urban space representations have several limitations. First, it emphasises learning the representation of POI category rather than directly addressing the primary challenge of capturing spatial variations in POI distribution, which is central to urban space representation learning. Additionally, these methods focus on learning POI semantic embedding via modelling the spatial context of different POI categories, failing to fully utilise the textual description of POIs. Furthermore, the second step of the learning paradigm \u2013 aggregating POI embeddings within specific areas - relies on the geographical delineation of urban spaces. The problem is that deciding the appropriate spatial resolution for these areas is challenging, and slight changes in delineation can significantly alter the resulting representations.\nIn terms of computational efficiency, probabilistic topic models (e.g., TF-IDF and LDA) are highly efficient as they do not require training. However, deep representation learning approaches involve constructing co-occurrence corpora and executing sophisticated training processes, both of which increase computation time.\nAddressing these limitations, our method directly learns the spatial-semantic representation of urban spaces by integrating location encoding with semantic encoding from POIs through contrastive learning, thereby improving both modelling accuracy and efficiency."}, {"title": "2.2. Location encoding", "content": "Location encoding refers to the process of encoding point locations into an embedding space so that these location embeddings can be readily used in downstream neural network modules (Mai et al., 2022), with various aims like geographic prior modelling (Chu et al., 2019; Klemmer et al., 2023; Mac Aodha et al., 2019) or spatial context modelling (Mai et al., 2020b), etc.\nThe motivation of such an idea was first comprehensively articulated in (Mai et al., 2022), in which the authors provided a general conceptual framework that unifies the formulation of location encoding methods. Following their formulation, location encoding methods generally take the form of $y = NN(PE(\\lambda, \\phi))$, where a geographical or projected coordinate $(\\lambda, \\phi)$ is processed through a parametric positional encoding (PE) and a neural network (NN).\nDepending on specific PE functions, location encoding can be performed on either planar or spherical geometries at different spatial scales. Over recent years, a series of PE methods have been proposed, including Wrap (Mac Aodha et al., 2019), Grid and Theory (Mai et al., 2020b), Sphere* (Mai et al., 2023), and Spherical Harmonics (SH) (Ru\u00dfwurm et al., 2024), etc.\nTheoretically, location encoding can embed continuous urban spaces into an embedding space, enabling represen- tations at extremely fine scales (e.g., metre or even sub-metre level). Therefore, it has great potential for addressing the ecological fallacy that can arise under coarse spatial scales. Learning a continuous encoding function instead of a discrete embedding matrix also makes the model highly inductive and generalisable.\nPractice of location encoding has been found in a variety of downstream applications, including geo-aware image classification (Mac Aodha et al., 2019), POI classification (Mai et al., 2020b), geographic question answering (Mai et al., 2020a), etc. However, it has not yet been explored for learning urban space representations with POIs in a multimodal setting, which is the one of the research gaps we are addressing in this paper."}, {"title": "2.3. Text embedding", "content": "Text embedding models encode the semantic content of natural language into vector representations, facilitating various natural language processing (NLP) tasks such as semantic textual similarity, information retrieval, question answering, and clustering (BehnamGhader et al., 2024). Early approaches often employed weighted averages of pre-trained word embeddings to measure semantic similarity (Arora et al., 2017; Pennington et al., 2014). With the development of pretrained language models, like BERT (Devlin et al., 2019), more advanced methods, such as Sentence-BERT (Nils and Iryna, 2019) and SimCSE (Gao et al., 2021), have been developed to fine-tune BERT on natural language inference datasets. These models use encoder-only or encoder-decoder architectures, while decoder- only language models have also shown strong performance in producing high-quality text embeddings (Muennighoff, 2022). Recently, the rise of large language models (LLMs) (Touvron et al., 2023a,b; Zhao et al., 2023) has encouraged researchers to apply them to text embedding tasks (BehnamGhader et al., 2024; Ma et al., 2024; Wang et al., 2023), achieving new state-of-the-art performance on benchmark datasets.\nOur proposed CaLLiPer model leverages text embedding models to embed the textual descriptions of POIs into high-quality vector representations, taking advantages of their advanced capacity to capture the semantics of text."}, {"title": "2.4. Contrastive learning and multimodal learning", "content": "Contrastive learning is a self-supervised learning technique used to learn representations by contrasting positive and negative pairs of examples. It aims to bring similar (positive) examples closer in the representation space and push dissimilar (negative) examples farther apart (Chen et al., 2020). This technique has been widely applied in various research domains, including computer vision (Chen et al., 2020; He et al., 2020), NLP (Gao et al., 2021; Gunel et al., 2021), recommendation systems (Chen et al., 2022) and more. This technique has also been applied in representation learning of urban spaces. In a pioneering study on POI-based urban region representation learning, Liu et al. (2018) treated each region as an \u201cimage\u201d where pixels are filled with POIs. For each anchor region, positive samples were generated by random removal, addition, and shifting of POIs, while negative samples are non-overlapping regions or augmentations with larger perturbations (hard negative samples). More recently, Huang et al. (2023) proposed generating city-region and region-POI negative sample pairs to learn POI embeddings and region embeddings through maximising the mutual information among the POI-region-city hierarchy. These single modality learning methods require dedicated positive and negative samples to enable contrastive learning within the same modality.\nThe development of jointly learning representations for multiple modalities have opened new avenue for contrastive learning. One pioneering and groundbreaking model for multimodal representation learning is Contrastive Language- Image Pre-training (CLIP) model (Radford et al., 2021). CLIP is trained on large datasets of images and their"}, {"title": "3. Notations and Problem Statement", "content": "A set of POIs is denoted as $P = \\{p_1, p_2, ..., p_i, ..., p_M \\}$, containing a total of M POIs. Each POI $p_i = (x_i^L, x_i^S)$ includes location $x_i^L$ and semantic properties $x_i^S$. The location of a POI is typically a pair of coordinates $x_i^L = (\\lambda_i, \\Phi_i)$ while the semantic properties $x_i^S$ are generally labels that indicate the POI's functions or attributes. For example, a POI could be represented as ((536366.19, 190390.01), Retail). Existing approaches treat the POI semantic labels as purely categorical values, i.e., $x_i^S = \\tilde{x}_i^S$. However, in our proposed model, these labels are treated as textual descriptions, i.e., $x_i^S = \\hat{x}_i^S$.\nA certain urban space is denoted as u. Depending on specific scales, urban spaces can range from individual buildings, neighbourhoods to districts and regions. The goal of urban space representation learning is to develop a model F that can embed an arbitrary urban space u into a meaningful representation $e_u = F(u)$, which encodes the rich semantic features of the local urban environment.\nFor methods based on POI category embeddings, the learning target is an embedding matrix of the POI categories, denoted as E. And the representation of a space u is then expressed as an aggregation of the POIs therein:\n$e_u = Agg_{p \\epsilon P_u}(E(x_i^S))$"}, {"title": "4. Method", "content": ""}, {"title": "4.1. Model overview", "content": "Figure 1 shows the overall framework of the proposed CaLLiPer model (left pane) and the workflow of downstream applications (right pane). The working mechanism of our method comprises two stages: pre-training of the CaLLiPer model and applications in downstream tasks."}, {"title": "4.2. Location encoder", "content": "As introduced in Section 2.2, all location encoding approaches take the form of\n$NN(PE(\\lambda, \\phi))$\nwhere a parametric positional encoding (PE) projects the raw coordinates $(\\lambda, \\phi)$ into a higher-dimensional encoding space, followed by a neural network (NN) that further encodes the target feature distributions.\nIn this work, we employ Grid (Mai et al., 2020b) as the PE, which is motivated by several considerations. First, our goal is to embed city-wide coordinates into a high dimensional space, and thus, we use projected coordinates on 2D planes rather than geographical coordinates that work on a spherical surface. Secondly, we aim for the location encoder to capture multi-scale spatial information while remaining computationally efficient. Grid operates effectively on projected coordinates, resolves multiple scales, and is more efficient than alternative methods like Theory (Mai et al., 2020b) without sacrificing the performance. Therefore, we choose it as the PE method. As for the design of NN, we adopt a fully connected residual network named FC-Net (Figure 2), as it is a performant and widely used neural architecture in location encoding (Ru\u00dfwurm et al., 2024).\nThe mathematical formulation of Grid (Mai et al., 2020b) is as follows:\n$PE(\\lambda, \\phi) = \\bigcup_{s=0}^{S-1} (cos\\frac{\\lambda}{a_s}, sin\\frac{\\lambda}{a_s}, cos\\frac{\\phi}{a_s}, sin\\frac{\\phi}{a_s})$"}, {"title": "4.3. Text encoder", "content": "In order to obtain expressive textual representations that effectively encode the semantic features of POIs, we propose using off-the-shelf language models that have been pre-trained on massive amounts of high-quality textual data as the text encoder in CaLLiPer. Specifically, we utilise Sentence-BERT (Nils and Iryna, 2019) (also known as SBERT or Sentence Transformers) and Llama3-8B (Dubey et al., 2024), which are encoder-only and decoder-only Transformer architectures, respectively. It is worth noting that Llama3-8B, one of the most recent and performant LLMs, is significantly larger in size compared to Sentence-BERT.\nInspired by (Radford et al., 2021), we design prompt templates to further enhance the text encoding process to better capture the semantics of POIs. Generally, we use \u201ca place of {label}", "retail\" type may also include specific goods information, and in such cases, we use \u201ca place that sells {goods_type}\". Additionally, some POI data might include a hierarchy of labels and the prompt template can be adjusted accordingly, such as \u201ca place of {label_1}, a type of {label_2}": "More details regarding prompt templates are described in Appendix A.1."}, {"title": "4.4. Projection layer", "content": "The projection layer is essentially a linear layer that projects the text embeddings (output of the text encoder) to the same dimension as the location embeddings, facilitating the construction of the contrastive learning objective between these two modalities. This kind of projection layer is commonly used for jointly embedding multiple modalities in contrastive pre-training (Radford et al., 2021) and for aligning two modalities in fine-tuning processes (Jin et al., 2024; Liu et al., 2024)."}, {"title": "4.5. Applying to downstream tasks", "content": "After the pre-training of CaLLiPer is completed, the pre-trained location encoder can be applied to various downstream tasks that are influenced by the spatial-semantic distribution information. As shown in the right pane of Figure 1, the representations of arbitrary urban spaces in downstream tasks are generated by inputting their point locations into the pre-trained location encoder. These representations then serve as the input (or part of the input) to a downstream model $f^D$, which undergoes a supervised leaning process to predict target variables. Note that the pre-trained location encoder is frozen in this stage, with only the downstream model being trained. This process is formulated in Equation 6."}, {"title": "5. Experimental setup", "content": "We choose London as the study area to evaluate the proposed model. London is the capital and largest city in the UK, and an important international city in the world. It not only has a diverse urban spatial configuration, but also a diverse sociodemographic population, making it an ideal testbed to conduct our experiments.\nWe evaluate the learned urban space representations using two downstream tasks: land use classification (LUC) and socioeconomic status distribution mapping (SDM). LUC is widely recognised as a standard task for verifying urban representation learning, especially in capturing urban functional distributions (Huang et al., 2022, 2023; Zhai et al., 2019). SDM, on the other hand, is particularly relevant because socioeconomic status (SES) and the urban environment are strongly intertwined, with SES affecting access to essential urban resources such as education and healthcare (McMaughan et al., 2020). In turn, the distribution of these resources shapes living conditions and reinforces inequality and segregation (Useche et al., 2024).\nWe introduce the data used in the downstream tasks testing, comparison methods, implementation details in the following subsections."}, {"title": "5.1. Data", "content": "POI data. The POI data, comprising 339,956 POIs in the Greater London Area, was obtained from Ordnance Survey through Digimap using an educational licence. The dataset is maintained and updated regularly; we used the March 2022 version in the experiment. The POI classification scheme follows a three-tier hierarchy with 9 groups, 52 categories and 616 classes.\nLand use dataset. The land use dataset was obtained from EDINA Verisk Digimap Service, with high-level classification proposed in the National Land-Use Database. To construct a dataset that is suitable for the LUC task, we sampled a number of points with 200-metre radius buffer and further adjusted the number of samples for certain land use types to account for the data imbalance. The resulting dataset contains 6697 samples, with the counts of different land use types shown in Table 1.\nNational Statistics Socio-economic Classification (NS-SeC) dataset. The NS-SeC dataset categorises a person's socio-economic position based on their occupations and other job characteristics (see Table 2 for the detailed classification scheme). This dataset is part of the 2021 Census and was acquired from the Office for National Statistics (ONS). We use data organised by Lower-layer Super Output Areas (LSOAs), which are a kind of lower-level geographical units for census statistics, each comprising 400 to 1200 households and a resident population of 1000 to 3000 people. The NS-SeC dataset used in our experiment consists of 4994 samples, corresponding to the 4994 LSOAS in London."}, {"title": "5.2. Comparison methods", "content": "We compare our model with a range of unsupervised learning models, including a baseline method (Random);\ntopic modelling-based approaches (TF-IDF and LDA); approaches that first learn POI category embeddings and then aggregate them within areas (Place2Vec, Doc2Vec, SPPE, HGI); and a location encoding-based method (Space2Vec).\nThese methods are introduced as follows:\n\u2022 Random: This method randomly initialises embeddings for urban spaces, serving as the lower bound perfor-\nmance.\n\u2022 TF-IDF (Sparck Jones, 1972): A statistical measure used in text mining to evaluate the importance of a word in a\ndocument relative to a collection of documents. It has been adopted in many studies to characterise the semantics\nof urban spaces (Niu and Silva, 2021; Shen, 2018; Zhai et al., 2019).\n\u2022 LDA (Blei et al., 2003): A generative probabilistic model used in topic modelling that identifies underlying topics\nin a corpus, which has also been used in many studies.\n\u2022 Place2vec (Yan et al., 2017): This method incorporates spatial co-occurrence information using a KNN sampling\nstrategy and distance decay to learn POI category embeddings, subsequently creating region embeddings by\naveraging the POI category embeddings within each region.\n\u2022 Doc2Vec (Niu and Silva, 2021): This approach constructs POI category co-occurrences using a KNN strategy,\ntreating each POI as a \u201cword\u201d and each urban region as a \"document\" to jointly train embeddings for both POI\ncategories and regions. The aggregation of POIs into regions occurs simultaneously during the model training\nprocess.\n\u2022 SPPE (Semantics preserved POI embedding) (Huang et al., 2022): This method integrates both spatial co-\noccurrence information and categorical semantics, creating POI category embeddings using random walks"}, {"title": "5.3. Implementation details", "content": ""}, {"title": "5.3.1. Computing environment", "content": "All the experiments were conducted on a server equipped with an AMD Ryzen Threadripper PRO 5975WX and one Nvidia RTX A6000 GPU, running Ubuntu 22.04.4 LTS."}, {"title": "5.3.2. Pre-training models", "content": "To ensure a fair comparison, we set the dimension for urban space representation as 128 across all methods. For CaLLiPer, we tuned the hyperparameters via grid search, resulting in the following final setttings: $A_{min} = 100$, $A_{max} = 10000$, $S = 32$, and a hidden dimension of 256 for FC-Net. We implemented two versions of CaLLiPer model, each using a different text encoder: CaLLiPer-SenTrans, which employs Sentence Transformer, and CaLLiPer-Llama, which adopts Llama3-8B as its text encoder.\nTF-IDF, LDA and Doc2Vec were implemented using the Gensim library. All the deep learning-based models were implemented using PyTorch 2.2.2 (Paszke et al., 2019). The corresponding open-source code for SPPE, HGI, and Space2Vec were used in the implementation. For methods that require pre-defined spatial division of the urban space (i.e., Doc2Vec and HGI), we used LSOAs as the division. For Place2Vec and SPPE, which aggregate POI category embeddings, we used mean pooling as the aggregation approach. More implementation details regarding SPPE, HGI and Space2Vec can be found in Appendix A.2 and A.3."}, {"title": "5.3.3. Downstream training and evaluation metrics", "content": "Two downstream models were used to evaluate the pre-trained urban space representations. The first is a linear model, essentially a one-layer neural network for linear probing of the pre-trained representations. The second is a non-linear model, specifically a multi-layer perceptron (MLP) with a single hidden layer.\nFor the LUC task, which is a multi-class classification task, we used cross-entropy loss to train the downstream models. The evaluation metrics are standard for classification tasks: Precision, Recall, and F1 score, macro-averaged over classes. For these metrics, higher values indicate better performance.\nFor the SDM task, a regression task, mean squared error loss was applied for the downstream training. For evaluation metrics, we follow prior studies (Huang et al., 2022, 2023) and treat this task as a label distribution learning problem (Geng, 2016), selecting three representative measures: (1) L1 distance: $\\sum_{k=1}^{m} |y_{i}^{s_{k}}-y_{i}^{*s_{k}}|$; (2) Chebyshev distance: $max_{k}|y_{i}^{s_{k}}-y_{i}^{*s_{k}}|$ (3) KL divergence: $\\sum_{k=1}^{m} y_{i}^{*s_{k}} log(y_{i}^{*s_{k}} /y_{i}^{s_{k}})$, where $y_{i}^{s_{k}}$ is the estimated proportion of the socioeconomic status $s_k$ that the i-th LSOA bears, and $y_{i}^{*s_{k}}$ is the corresponding ground truth. For these metrics, lower values indicate better performance."}, {"title": "6. Results and analyses", "content": ""}, {"title": "6.1. Performance on downstream tasks", "content": ""}, {"title": "6.1.1. Land use classification", "content": "The results of the LUC task are presented in Table 3. We can observe that the embeddings produced by CaLLiPer (both variants) surpass other competitive methods across all evaluation metrics in both downstream models (i.e., Linear and MLP). This indicates that our method captures the spatial distributions of urban functions implied by POIs more effectively, and this information significantly aids in land use classification. Moreover, our method outperforms other baselines by a large margin in the linear probing setting, further proving that the pre-trained representations are highly robust and thus do not require complex downstream models or extensive supervised training.\nAs for the performance of the other compared methods, the probabilistic topic modelling models, i.e., TF-IDF and Doc2Vec, show limited improvement when using a more complex non-linear model, indicating that these models may not capture sufficient spatial or contextual nuances from the POI data, limiting their ability to benefit from non- linear transformations. In contrast, deep representation learning models (Place2Vec, Doc2Vec, SPPE) that learn POI embeddings first and then aggregate, achieved significantly better performance with the MLP model compared to the linear probing setting, suggesting that they perform more effectively when combined with non-linear models. HGI, which uses a pre-defined delineation of the urban space as LSOA, performed worse than SPPE (the model it is based on) with MLP as the downstream model, indicating that relying on rigid spatial divisions limits its ability to generalise across varying urban contexts and capture finer spatial patterns. As for Space2Vec, despite its location encoding capability, it did not perform well in this task. We argue that this is mainly because the encoder-decoder architecture falls short in learning the geographic prior as effectively as our proposed model."}, {"title": "6.1.2. Socioeconomic status distribution mapping", "content": "Table 4 shows the performance on the SDM task. CaLLiPer models consistently achieved the best performance across all metrics. This indicates that the distributions estimated using the CaLLiPer representations have the smallest distance and the greatest similarity to real distributions. It is worth noting that CaLLiPer-Llama outperformed CaLLiPer-SenTrans on the SDM task but performed worse on the LUC task (see Table 3). This suggests that using a larger text encoder may not always provide additional benefits. Moreover, the discrepancy between these two model variants is not significant. Therefore, in real-world applications where efficiency is prioritised, more lightweight text encoders are preferable."}, {"title": "6.2. Qualitative results and analyses", "content": "We conduct qualitative analysis of the learned representations in the following sections. Unless stated otherwise,\nthe representations produced by CaLLiPer-SenTrans are used throughout."}, {"title": "6.2.1. Clustering of the learned representations", "content": "To further investigate the effectiveness of the compared methods, we conducted clustering analysis and presented the results in Figure 3. Specifically, we generated the representations for each hexagonal area in London hexagonal grid using six of the compared methods. We adopted basic K-Means clustering for this analysis, following the practice of (Niu and Silva, 2021), although other clustering methods like hierarchical clustering could also be applied (Mai et al., 2020b). From the visualisation, we can observe that there are no clear spatial distribution patterns for Place2Vec and SPPE. While some spatial patterns are visible in TF-IDF and Space2Vec results, they appear more dispersed and random compared to CaLLiPer's. The clusters produced from HGI representations are more spatially contiguous and less noisy but have coarser spatial resolution because of the larger spatial units (i.e., LSOAs) originally used for training the model.\nTo help interpret the results, we further visualised the clustering results of CaLLiPer representations and labelled the prominent local areas in Figure 4. As shown in the figure, the 4 clusters can be interpreted as \u201cGreenspace\", \u201cInfrastructures and Industrial Areas\u201d, \u201cResidential Areas\" and"}, {"title": "6.2.2. Zoom into local areas", "content": "In this subsection, we visualise a low-dimensional projection of the latent representations of London's output areas (OAs), which are subdivisions of LSOAs. For better comparison, we focus on a specific local area\u2014LSOA Waltham Forest 002C-as a case study. Specifically, we projected the 128-dimensional representation vectors into 3-dimensional vectors using t-SNE and used these for RGB-colouring of the OAs in Figure 5. From the figure, we can observe that OA"}, {"title": "6.3. Computational efficiency", "content": "We present the computation time for all neural network-based models in Table 5 to compare their computational efficiency. To ensure a fair comparison, the batch size is set to 1024 for all methods except for SPPE, which uses a batch size of 32768. This is because using a batch size of 1024 for SPPE would be nearly intractable due to the significantly longer training time required (refer to Appendix A.2 for more details).\nIt is evident that simpler models, like the word2vec-based Place2Vec, are much faster to train. SPPE has the highest per-epoch training time, primarily because it involves constructing a training corpus through random walks on a weighted graph generated by Delaunay Triangulation of tens of thousands of POIs, leading to a much larger training set compared to the other methods. HGI has the second highest per-epoch training time due to the sophisticated negative sampling strategy it uses in its contrastive learning process. Additionally, HGI requires many training epochs, which results in an extremely long total training time. Our model demonstrates relatively high training efficiency, with each epoch taking about 20 seconds and the entire training process completed within half an hour. This makes it both efficient and accurate, making it a more practical choice for real-world applications."}, {"title": "7. Conclusions and discussion", "content": "In this paper, we presented a novel approach, CaLLiPer, for learning urban space representations based on POIs using a contrastive location-language pre-training objective. The proposed method offers a unique perspective by directly embedding the continuous geographical space into a higher-dimensional vector space to capture the spatial distribution of the semantics of the urban environment. We combined location encoding with contrastive learning to learn the geographic prior of the entire study area, effectively capturing the spatial variations in POI semantic distributions. We used pre-trained text encoders from the NLP domain to fully capture the semantic meaning of POIs, intricately aligning the location embedding space with the language embedding space. Comprehensive experiments on two downstream tasks demonstrated the effectiveness of our method, confirming its effectiveness and potential for a wide range of urban studies. In the following subsections, we delve into a more in-depth discussion to interpret the findings and explore their broad implications."}, {"title": "7.1. Capturing spatial variations", "content": "The extensive experiments demonstrated the superior performance of our model. But why is our model so successful? We argue that it is because our model addresses the core challenge in learning representations for computational urban studies. Many past studies have emphasised that the core of learning urban space representations lies in capturing the spatial variations of POI distributions (Zhai et al., 2019; Huang et al., 2023). Our model aligns location embeddings with their corresponding semantic representations derived from POIs, which closely matches with this objective, and thus enabling the improved performance.\nIn comparison, existing models might be inherently suboptimal for learning urban space representations. For deep representation learning methods that first learn POI category embedding and"}]}