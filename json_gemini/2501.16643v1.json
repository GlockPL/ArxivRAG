{"title": "An LLM Benchmark for Addressee Recognition in Multi-modal Multi-party Dialogue", "authors": ["Koji Inoue", "Divesh Lala", "Mikey Elmers", "Keiko Ochi", "Tatsuya Kawahara"], "abstract": "Handling multi-party dialogues represents a significant step for advancing spoken dialogue systems, necessitating the development of tasks specific to multi-party interactions. To address this challenge, we are constructing a multi-modal multi-party dialogue corpus of triadic (three-participant) discussions. This paper focuses on the task of addressee recognition, identifying who is being addressed to take the next turn, a critical component unique to multi-party dialogue systems. A subset of the corpus was annotated with addressee information, revealing that explicit addressees are indicated in approximately 20% of conversational turns. To evaluate the task's complexity, we benchmarked the performance of a large language model (GPT-40) on addressee recognition. The results showed that GPT-40 achieved an accuracy only marginally above chance, underscoring the challenges of addressee recognition in multi-party dialogue. These findings highlight the need for further research to enhance the capabilities of large language models in understanding and navigating the intricacies of multi-party conversational dynamics.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in dialogue systems, fueled by the emergence of large language models (LLMs) capable of generating human-like text and engaging in natural conversations, have been largely confined to the realm of dyadic interactions. While these systems have demonstrated remarkable progress, they fail to capture the complexities inherent in multi-party dialogues, involving three or more participants. These dialogues are characterized by intricate information flow, dynamic participant roles, and nuanced social cues, posing significant challenges for system development.\nPrevious research has explored specific aspects of multi-party dialogues, including turn-taking (Lee and Deng, 2024; Auer, 2018; Skantze et al., 2015), addressee recognition (Le et al., 2019; Li and Zhao, 2023; Tan et al., 2023), and dialog act recognition (Qamar et al., 2023). However, existing benchmarks are limited by their reliance on text-based or acted dialogue data, failing to reflect the spontaneity and multi-modality inherent in natural human interactions.\nTo address this crucial gap, this paper introduces a novel, spontaneous, and multi-modal multi-party dialogue corpus specifically designed to facilitate research on triadic (three-participant) dialogue systems. This research further focuses on the critical, yet under-explored, task of addressee recognition the identification of the intended recipient of a turn which is foundational for enabling dialogue systems to navigate and participate effectively in multi-party settings. Unlike dyadic interactions where the addressee is implicitly defined, turn-taking in multi-party settings is far more complex. The intended recipient might be a specific participant or the group as a whole, and behavioral signals are often subtle and inconsistent (Auer, 2018; Skantze et al., 2015)."}, {"title": "2 TEIDAN Corpus", "content": "We first briefly describe the TEIDAN multi-party corpus. The corpus is of free discussion, as opposed to other works where a specific setting was used such as a meeting (Carletta, 2007; Mostefa et al., 2007) or the participants were involved in a task (Kontogiorgos et al., 2018; Nihei et al., 2014) or game (Stefanov and Beskow, 2016; Litman et al., 2016; Hung and Chittaranjan, 2010). Other multiparty corpora also exist for online discussions (Reverdy et al., 2022).\nDiscussions consisted of participants in a triad (three people). Each participant was seated in a circle with a table in the center, as shown in Figure 1. Cameras captured the face of each participant while separate pin microphones were attached to each of them.\nWe had three general topics of conversation: which city would be best for the alternative capital of Japan, which items would be necessary to bring to a desert island, and where they would like to travel on the weekend. Each triad conducted the discussions three times, corresponding to the above topics.\nTriads conversed for approximately 5-10 minutes per session, with no requirement to reach a conclusion. Data was collected from 10 sets of triads, resulting in a total of 30 discussion sessions. Note that this corpus is in the Japanese language."}, {"title": "3 Annotation of Addressee", "content": "We annotated a subset of the TEIDAN corpus for addressee information. The annotation process consisted of the following steps:\n(1) Initially, turns and the current speaker were annotated. Since the original TEIDAN corpus contains only IPU utterance segments, turn segments within the dialogue were annotated by removing certain utterances, including backchannels. This ensured that only one speaker could hold the floor at any given time. Minimal overlap was permitted during turn transitions.\n(2) Following turn annotation, we labeled the addressee information to indicate whether the next speaker was explicitly addressed. If addressed, the label corresponded to one of the participant IDs (e.g., A, B, or C). Otherwise, it was labeled as 'O', signifying that no specific individual was addressed and any participant could take the turn.\nThis labeling process considered both textual and visual cues, such as gaze behavior. Initially, a single session was annotated and discussed to ensure inter-rater agreement by the authors. Subsequently, the remaining four sessions were annotated by one of the authors.\nWe have so far annotated five sessions from the TEIDAN corpus. The analysis revealed that only approximately 20% (80 / 322) of turns explicitly specify an addressee. The 'O' label, indicating no specific addressee, was prevalent, particularly in discussions involving multiple opinions (statements). This result implies that a multi-party dialogue system that participates in this type of discussion and disregards addressee information may potentially interrupt the dialogue in 20% of turn-taking instances, assuming that they can always correctly recognize the end of the turn of a human participant."}, {"title": "4 Benchmark for Addressee Recognition", "content": "To evaluate the task's complexity, we tested the performance of a multimodal large language model (GPT-40) on addressee recognition. The model was given a prompt as follows:\nIn the following conversation among A, B, and C, please infer who is addressed as the next speaker in the last utterance. Answer with one of the following: \u201cA, B, C, or O\u201d. \u201cA, B, and C\" represent the participants, and \u201cO\u201d represents the case where no one is addressed, and anyone can take the turn next. The output should only contain the label \u201cA, B, C, or O\" and should not include any other characters.\nThis was followed by five context turn utterances with the current utterance, and also it contained the name of the discussion topic and designated identifier of the participants. Note that the utterances were manually transcribed.\nThe GPT-4o achieved an accuracy of 80.9%, which is only marginally above chance level (80.1%). This indicates that the model struggles to identify the addressee in multi-party dialogues.\nThe output by the LLM is summarized in Table 2 which shows that the model tends to output 'O', indicating that it often fails to recognize when an utterance is directed at a specific participant.\nWe then analyzed samples to examine how GPT-40 deals with addressee recognition, as illustrated below:\n(1) Explicit Question (Correct) An example in Table 3 shows a case where GPT-40 correctly identified the addressee as C because the final utterance, a question, was explicitly directed to that individual. Although current LLMs effectively handle such explicit cases, the corpus contains many instances that are not as straightforward.\n(2) False Negative In both examples presented in Table 4 and Table 5, the GPT-40's output indicated no specific addressee (O), while the reference labels were A and B, respectively. This kind of false-negative instance represented the majority of errors in this experiment. In the Table 4 example, the final speaker, C, was looking at person A, suggesting that gaze information is crucial for this task.\nIn the Table 5 example, the final speaker inquires about Nagoya, a city in Japan. Within the context of this discussion, B was about to recommend this city. Therefore, this task also necessitates the consideration of such prior information."}, {"title": "5 Adding Gaze Features", "content": "To see the effect of gaze information in the current task, we processed the video of each participant (shown at the bottom of Figure 1) and automatically annotated their gaze throughout the discussion. OpenFace 2.0 was used to estimate the eye gaze vector (Baltrusaitis et al., 2018; Wood et al., 2015). We could then generate a gaze vector 30 times a second.\nFor every gaze timestamp, we then estimated whether the gaze of the participant who had the turn (speaker) was directed at either one of the other participants or at nobody in particular. As each participant was seated in an approximately equilateral triangle, we used a simple heuristic to test if the speaker was looking at another participant. The y (up-down) portion of the gaze vector must be within a certain range (0.2), and the x (left-right) gaze vector had to be out of a certain range (-0.2 to 0.2). If this heuristic was met, then the gaze timepoint was labeled as the speaker looking at the relevant participant, else the gaze timestamp was labeled as O (no participant).\nWe also labeled the turn of a speaker as opposed to continuous timestamps. We based our approach on previous research which found that end-of-turn gaze was important (Kawahara et al., 2016; Degutyte and Astell, 2021) and labeled the majority gaze in the turn's final second, specifically the gaze label which was present in over 50% of the timestamps, or O if this was not reached.\nThis information was added to the previous prompt to assess if adding gaze information in this way could improve the result. However, as shown in Table 6, adding this information did not improve accuracy, as the accuracy score (75.2%) went down under the chance level. While future work necessitates manual annotation of gaze information, the current results indicate that existing LLMs also struggle to incorporate such additional modalities within the context of multi-party dialogues."}, {"title": "6 Benchmark for Next Speaker Prediction", "content": "We are also interested in predicting the actual next speaker in the multi-party scenarios (Lee and Deng, 2024; Lee et al., 2023). This is distinguished from addressee annotation, where subsequent information on who took the turn is unknown, and there is no 'O' label as somebody must take the turn. It is possible that the addressee and the actual next speaker differ because of interruptions during the turn.\nWe then evaluated GPT-40's performance on this next speaker prediction task. Using a prompt similar to that used for addressee recognition, the model was tasked with predicting the actual next speaker. The output label was limited to A, B, or C, with a chance-level accuracy of 50%, as either of the other two participants could take the turn. As a result, GPT-40 attained an accuracy of 46.0% on this task, performing below the chance level. This outcome further suggests that the model struggles to effectively capture the dynamics of turn-taking in spontaneous multi-party dialogues."}, {"title": "7 Conclusion", "content": "This study investigated the challenges of addressee and next speaker prediction in multi-party dialogues. We introduced a new multi-party dialogue corpus and analyzed the performance of an LLM (GPT-40) on these tasks. The findings revealed that LLMs struggle with the complexities of multi-party interactions. They perform only marginally above the chance level in addressee recognition and below the chance level in the next speaker prediction task. Although the LLM was given the simple gaze feature, it did not improve the performance.\nThese results underscore the need for further research to improve LLMs' understanding of multi-party conversational dynamics. Future work should explore more sophisticated methods for incorporating contextual information, including gaze and other non-verbal cues, and develop new models that can better capture the intricate interplay between participants in multi-party conversations."}]}