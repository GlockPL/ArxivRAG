{"title": "Accelerating Proximal Policy Optimization Learning Using Task Prediction for Solving Games with Delayed Rewards", "authors": ["Ahmad Ahmad", "Mehdi Kermanshah", "Kevin Leahy", "Zachary Serlin", "Ho Chit Siu", "Makai Mann", "Cristian-Ioan Vasile", "Roberto Tron", "Calin Belta"], "abstract": "In this paper, we tackle the challenging problem of delayed rewards in reinforcement learning (RL). While Proximal Policy Optimization (PPO) has emerged as a leading Policy Gradient method, its performance can degrade under delayed rewards. We introduce two key enhancements to PPO: a hybrid policy architecture that combines an offline policy (trained on expert demonstrations) with an online PPO policy, and a reward shaping mechanism using Time Window Temporal Logic (TWTL). The hybrid architecture leverages offline data throughout training while maintaining PPO's theoretical guarantees. Building on the monotonic improvement framework of Trust Region Policy Optimization (TRPO), we prove that our approach ensures improvement over both the offline policy and previous iterations, with a bounded performance gap of $(2\\varsigma\\gamma a^2)/(1-\\gamma)^2$, where a is the mixing parameter, y is the discount factor, and s bounds the expected advantage. Additionally, we prove that our TWTL-based reward shaping preserves the optimal policy of the original problem. TWTL enables formal translation of temporal objectives into immediate feedback signals that guide learning. We demonstrate the effectiveness of our approach through extensive experiments on an inverted pendulum and a lunar lander environments, showing improvements in both learning speed and final performance compared to standard PPO and offline-only approaches.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL) in environments with delayed rewards presents a fundamental challenge: actions that lead to successful outcomes may not receive immediate positive feedback, making it difficult for learning algorithms to identify and reinforce beneficial behaviors. This challenge is particularly acute in complex games like soccer, where the value of tactical decisions (e.g., passing plays) may only become apparent after multiple time steps later through their impact on strategic outcomes (e.g., scoring opportunities). Policy Gradient (PG) methods, particularly Proximal Policy Optimization (PPO) Schulman et al. (2017), have demonstrated remarkable success in RL tasks. PPO's effectiveness stems from its optimization of a surrogate objective that provides stable policy updates while maximizing expected rewards. However, in settings with delayed rewards, even PPO's carefully constructed optimization landscape becomes difficult to navigate, as the temporal gap between actions and their consequences creates a sparse and uninformative gradient signal.\nOur work addresses these challenges through three main contributions. First, we introduce a hybrid policy architecture that combines an offline policy (trained on expert demonstrations) with an online PPO policy. Unlike previous approaches that use offline data merely for initialization Nair et al. (2020); Ross and Bagnell (2012); Schmitt et al. (2018); Chen et al. (2021); Kumar et al. (2020), our method maintains the offline policy as an active guide throughout training. We prove that this architecture guarantees monotonic improvement over both the offline policy and previous iterations. Second, we develop a reward-shaping mechanism using Time Window Temporal Logic (TWTL) Vasile et al. (2017) that provides immediate, semantically meaningful feedback while preserving the optimal policy of the original problem. This approach bridges the temporal gap in the reward signal by formally encoding desired temporal behaviors. Third, we extend existing convergence proofs for actor-critic methods to handle our hybrid architecture and reward shaping mechanism, providing theoretical foundations for our approach. Our analysis demonstrates that the proposed method maintains PPO's convergence properties, while accelerating learning in delayed-reward settings.\nRelated work. Our work builds upon three research directions: policy optimization, temporal logics in RL, and offline reinforcement learning.\nIn policy optimization methods, Trust Region Policy Optimization (TRPO) Schulman et al. (2015a) and PPO Schulman et al. (2017) provides stability guarantees. However, these methods, while effective for standard RL tasks, do not specifically address the challenges of delayed rewards. Our work extends PPO by incorporating temporal logic guidance and offline data while preserving its theoretical guarantees.\nIn using TL in RL, several works have investigated temporal logic (TL) in RL primarily focuses on task specification Asarkaya et al. (2021); Cai et al. (2023); Icarte et al. (2022); Xu et al. (2020); Neider et al. (2021); Alshiekh et al. (2018); Balakrishnan and Deshmukh (2019); Li et al. (2017); Aksaray et al. (2016). TL formalizes high-level tasks into propositional and temporal constraints Baier and Katoen (2008), with some, like Time Window Temporal Logic (TWTL) Vasile et al. (2017), handling delayed rewards through temporal constraints Aksaray et al. (2016); Balakrishnan and Deshmukh (2019). TWTL's clear syntax efficiently expresses sequential tasks. In Asarkaya et al. (2021), a Q-learning algorithm learns an optimal policy for TWTL-modeled tasks while maximizing external rewards. Our approach differs by actively using TWTL for reward shaping while maintaining optimality guarantees. Recent works like Cai et al. (2023) and Icarte et al. (2022) have"}, {"title": "2. Preliminaries", "content": "Finite-horizon Markov Decision Process (MDP)\nDefinition 1 A finite horizon MDP is a tuple $(X,U,p(\\cdot|\\cdot, \\cdot), r(\\cdot, \\cdot), l(.), O)$, where $X, U$ and $O$ are the state, control, and output spaces, respectively; $p(\\cdot|\\cdot,\\cdot)$ is the state-action pair transition probability; $r : X \\times U \\leftrightarrow [0, 1]$ is the reward function; and $l : X \\rightarrow O$ is a labeling function that maps the state to an output observation.\nWe denote a state trajectory of the MDP as $x_{i,i+N} := x_ix_{i+1}...x_{i+N}$, where $x_i \\in X$ and $i \\in N$. $x_{i,i+N}$ generates a word, $o_{i,i+N} = o_io_{i+1}...o_{i+N}$, where $o_i = l(x_i)$, where $o_r \\in O$. Let $\\pi : X \\rightarrow U$ be a stochastic policy. In an episodic RL setting Sutton and Barto (2018), for $K$ learning episodes the state value function and the state-action value, at iteration $t$ of episode $k$, are defined, respectively, as follows Schulman et al. (2017):\n$V_t^{\\pi,k}(x) := E\\Big[\\sum_{i=t}^N r(x_i, w_i) \\Big| x_t = x\\Big]$\n$Q_t^{\\pi,k}(x, u) := E\\Big[\\sum_{i=t}^N r(x_i, w_i) \\Big| x_t = x, u_t = u\\Big]$\nwhere $E_\\pi$ is the expectation over the stochastic policy $\\pi$. Correspondingly, the advantage function at iteration $t$ of episode $k$:\n$A_t^{\\pi,k}(x, u) := Q_t^{\\pi,k}(x, u) - V_t^{\\pi,k}(x)$\nquantifies how much better (or worse) an action $u$ is compared to the average action that would be taken by policy $\\pi$ in state $x$. A positive advantage indicates that action $u$ is better than $\\pi$'s average action, while a negative advantage suggests it is worse. This function plays a crucial role in policy gradient methods by identifying which actions to encourage or discourage during policy updates."}, {"title": "Time Window Temporal Logic", "content": "We use TWTL to define tasks. TWTL allows us to specify sequences of tasks, including their order and time constraints. This helps us create reward functions that guide the agent towards specific goals.\nWe use atomic propositions to build these tasks. An atomic proposition, AP \u2208 II, II is the set of atomic propositions, is true (T) if $o := l(x)$ satisfies $h_{AP}(o) > \\sigma$, where $h : O \\rightarrow R^d$ is a predicate function and $\\sigma \\in R^d$. Otherwise, it's false (\u22a5).\nThe syntax of TWTL is defined, inductively, as follows Vasile et al. (2017).\n$\\phi ::= H_d s | \\phi_1 \\cdot \\phi_2 | \\phi_1 \\lor \\phi_2 | [\\phi][a,b]$\nwhere s is either true, T, or an atomic proposition in II ;$H^d$ is the hold operator where d \u2208 N; \u00b7 is the concatenation operator; $\\lor$ is the Boolean conjunction operator; $[\\cdot][a,b]$ is the within operator, where d, a, b \u2208 Z\u22650 and a > b; and $\\phi_1$ and $\\phi_2$ are TWTL formulae.\nThe Boolean semantics over an observation word $O_{t_1,t_2}, t_1, t_2 \\in N$ and $t_1 < t_2$, are defined recursively as follows:\n$O_{t_1,t_2} \\models H^d s \\Leftrightarrow s \\in o_t, \\forall t \\in [t_1, t_2] \\land (t_2-t_1 \\geq d)$\n$O_{t_1,t_2} \\models \\phi_1 \\cdot \\phi_2 \\Leftrightarrow \\exists t = \\arg\\min_{t \\in [t_1,t_2]} O_{t_1,t} \\models \\phi_1 \\land (O_{t+1,t_2} \\models \\phi_2)$\n$O_{t_1,t_2} \\models \\phi_1 \\lor \\phi_2 \\Leftrightarrow (O_{t_1,t_2} \\models \\phi_1) \\lor (O_{t_1,t_2} \\models \\phi_2)$\n$O_{t_1,t_2} \\models [\\phi][a,b] \\Leftrightarrow \\exists t > t_1 + a s.t. O_{t,t_1+b} \\models \\phi \\land (t_2-t_1 \\geq b)$\nIn the context of soccer games, the task of passing the ball can be defined using TWTL as $\\phi = [H^1 P_a \\cdot H^{T_p-2} P_f \\cdot H^1 P_a]_{t_0,t_0+T_p}$. This formula means: \u201cWithin time $t_0$ and time $t_0 + T_p$, the ball must be possessed by team \u201ca\u201d for at least one time step, hence the subformula $H^1 P_a$. Then, the ball must be unpossessed for $T_p \u2013 2$ time steps, hence the subformula $H^{T_p-2} P_f$. Finally, the ball must again be possessed by team \u201ca\u201d for at least one time step, hence the subformula $H^1 P_a.\u201d\nThe time horizon of $\\phi$ is defined as follows.\n$||\\phi|| := max\\{ max(||\\phi_1||, ||\\phi_2||) \\cdot 1_{\\phi \\equiv \\phi_1 \\lor \\phi_2}, (||\\phi_1||+||\\phi_2||+1) \\cdot 1_{\\phi = \\phi_1 \\cdot \\phi_2}, d \\cdot 1_{\\phi = H^d s}, b \\cdot 1_{\\phi = [\\phi_1][a,b]} \\}$\nwhere 1 is the indicator function.\nLet $\\Gamma$ be the set of feasible TWTL formulas. A formula is feasible if its within operators have time windows longer than the enclosed sub-formula (Definition IV.1 in Vasile et al. (2017)).\nIn the following, we define a TWTL robustness which measures how close or far a sequence of observations, $o_{t_1,t_2}$, is from satisfying the TWTL task. A positive value indicates task satisfaction, with higher values signifying greater robustness. Conversely, a negative value indicates task violation Ahmad et al. (2023)."}, {"title": "3. Problem Formulation", "content": "We consider a model-free RL setting, in which an agent interacts with a complex environment or opponent. A prominent challenge is delayed rewards, meaning the agent might not get immediate feedback for its actions. Before stating the problem, we introduce a concrete-time reward function which is based on TWTL specifications.\nDefinition 3 (Concrete Time Reward) Let $\\phi \\in \\Phi$ be a TWTL formula. For a finite-horizon MDP with a trajectory $x_{t,t+||\\phi||}$ produced by applying $u_{t-1,t+||\\phi||-1} := u_{t-1}u_t... u_{N-t+||\\phi||-1}$, we define an episodic concrete-time reward, over the generated observation word $o_{t,t+||\\phi||}$, at time $t$, as follows.\n$r_{\\phi,t}(o_{t,t+||\\phi||}) :=\\begin{cases}1 & \\text{if } o_{t,t+||\\phi||} \\models \\phi \\\\\\theta & \\text{if } o_{t,t+||\\phi||} \\not\\models \\phi\\end{cases}$\nwhere k denotes the training episode, and t denotes the time instance at which we observe the reward.\nProblem 1 Given a finite horizon MDP with an episodic concrete-time reward $r_{\\phi,t}$, compute the optimal parameter $\\theta^*$ of the stochastic policy, $\\pi_\\theta$, that maximizes the total expected episodic reward. I.e.,\n$\\pi_{\\theta^*} = \\arg\\max_{\\theta \\in \\Theta} E_{\\pi_\\theta}\\Big[\\sum_{t=1}^N r_{\\phi,t}^k\\Big]$"}, {"title": "4. Accelerated Proximal Policy Optimization", "content": "We enhance PPO with an offline policy and a reward-shaping function based on TWTL. The offline policy improves performance, while the reward shaping guides learning towards desired temporal goals.\n4.1. Proximal Policy Optimization\nPPO is a policy optimization algorithm that uses the policy gradient to optimize a parameterized policy. PPO provides stability to the learning process. At each iteration, the algorithm aims to find"}, {"title": "4.2. Temporal Logic Reward-Shapping", "content": "Consider an episodic RL framework where tasks are formulated using TWTL formulae. To simplify the analysis, we consider a single task $\\phi$, with time Horizon $||\\phi|| < N$, where N is the number of steps in the training episode.\nEquation (8) defines the concrete time reward. Computing this reward requires complete MDP trajectories that span the entire task horizon, $||\\phi||$. However, obtaining these full trajectories during real-world implementation might be impractical. To overcome this challenge, we need a method to complete or extend partially observed trajectories. Existing approaches for trajectory completion include trajectory prediction Salzmann et al. (2020), or runtime monitoring techniques Ahmad et al. (2023).\nWe propose using an observation predictor. This predictor takes an MDP state and generates a sequence of predicted observations spanning the entire task horizon ($||\\phi||$). Details on the CNN-based task predictor are in Section 5.1. For simplicity, assume we have a pre-defined predictor function, $Pred : X \\rightarrow O$, mapping states to observation sequences.\n$Pred(x_t) := \\hat{o}_t\\hat{o}_{t+1}... \\hat{o}_{t+||\\phi||}$"}, {"title": "4.3. Online-Offline Policy Architecture", "content": "In this section, we propose a novel policy architecture to accelerate PPO for episodic learning. The core idea is to combine an offline policy, pre-trained on expert demonstrations using, for example, BC, with an online policy continuously optimized by PPO. Initially, the offline policy guides the online policy's learning. Over time, a mixing parameter gradually reduces the offline policy's influence, allowing the online policy to take over action selection.\nLet $D_u$ be the set of distributions over $U$. We introduce a deep policy architecture, $\\pi_\\theta \\in D_u$, that we constitute using two parallel deep policies $\\pi_\\rho, \\pi_\\beta \\in D_u$ and combine them using a fully connect layer (FCL), see Figure 1. Let $v := \\sum_{u' \\in U} [(1 - \\alpha) \\cdot \\pi_\\rho(u'|x) + \\alpha \\cdot \\pi_\\beta(u'|x)]$ be a normalization factor, then we compute $\\pi_\\theta$ as follows.\n$\\pi_\\theta(u/x) := (1 - \\alpha) \\cdot \\frac{\\pi_\\rho(u/x)}{v} + \\frac{\\alpha}{v} \\cdot \\pi_\\beta(u/x)$\nwhere a is mixing parameters which are the weights of the FCL of the architecture."}, {"title": "4.4. The Algorithm", "content": "Following the formulation of Cai et al. (2020) we introduce APPO in Algorithm 1. Given the offline policy, $\\pi_\\rho$, we initialize for episode 1 a series of the policy, $\\pi_\\theta$, with an initial parameter $\\theta_0$, Line 4.4. Then at every training episode k we observe the state after applying the 0-th step policy, $\\pi_{\\theta,0}$, Line 4.4. Then at every iteration i of the episode k, the parameter $\\theta_i$ of the policy is optimized, Line 4.4-4.4.\nGiven a concrete-time reward (Def 3), we introduce $J^k(\\cdot, \\cdot, \\cdot, \\cdot)$, a variant of the clipped objective (11) defined based on $A_{\\phi,t}^k$ and is given as $J^k(x, u, \\theta, \\theta_i) := min \\Big(\\frac{\\pi_\\theta(u|x)}{\\pi_{\\theta_i}(u|x)}, A_{\\phi,\\theta_i,t}^k(u, x), g(\\epsilon, A_{\\phi,\\theta_i,t}^k(u, x))\\Big)$.\nConsequently, the parameterized part of the policy update is computed according to $\\theta_{i+1} \\leftarrow \\arg \\max_{\\theta \\in \\Theta} E[J(x, u, \\theta, \\theta_i)]$."}, {"title": "5. Case Study", "content": "To illustrate the effectiveness of our proposed APPO approach with reward shaping, we present a case study. This section focuses on training an agent to play soccer within a simplified environment.\nWe will compare the performance of two training methods. The first method utilizes the standard PPO algorithm with a basic reward function derived from Equation (8). The second method employs our APPO algorithm, leveraging both the basic reward and an additional shaped reward function defined in Equation (15). By comparing the performance of these two approaches as well as running the game using an offline policy, we aim to assess the impact of reward shaping on the agent's ability to learn an optimal soccer-playing strategy faster.\nWe evaluate the proposed framework on a simplified soccer game using Google Research Football (GRF) Kurach et al. (2020). Our game setting is defined over concrete-time rewards whereby the right-side team aims to maximize the number of successful passes while avoiding interception by the opposing team on the left side (see Figure ??).\nThe game state captures both teams player positions and the ball position: $x := [q_1,...,q_{22}, q_b]$. Each player's position $q_i, i = 1,..., 22$, and the ball position $q_b$ is within the field boundaries ($F \\subset R^2$). We assume full observability of the state (i.e., the agent sees everything). We train a task predictor (see Section 5.1) $Obs_prd(x_t) := \\hat{o}_t\\hat{o}_{t+1} \u00b7 \u00b7 \u00b7 \\hat{o}_{t+d}$, where d is the deadline for performing a pass and $\\hat{o}_t$ is the prediction probability of event that is taking place at time t is a pass. We define the passing task using TWTLs for the concrete time reward (see Eq. 8) as follows.\nWe use a trained predictor (details in Section 5.1) to estimate future events. This predictor, denoted by $Obs_prd(x_t)$ takes the current game state and predicts a sequence of events $(\\hat{o}_t\\hat{o}_{t+1} \u00b7 \u00b7 \u00b7 \\hat{o}_{t+d})$. Here, $\\hat{o}_t$ represents the predicted chance of a successful pass happening at time t. The length of this sequence, d, corresponds to the maximum time steps allowed to complete a successful pass. This predicted sequence, along with TWTLs, will be used to define the passing task for the concrete-time and the shaped rewards, (8) and (15), respectively.\n$\\phi_{pass} := [H^2AP_{pass} \\lor...\\lor H^dAP_{pass}]_{0,d}$\nwhere $AP_{pass} \\begin{cases}T & \\text{if } \\hat{o} \\geq 0.7 \\\\\\\\theta & \\text{if } \\hat{o} < 0.7\\end{cases}$. Formula $\\phi_{pass}$ reads that a pass is successful if there are at least two consecutive time steps within the next d steps where the predicted observation indicates a successful pass event."}, {"title": "5.1. Task Predictor for Reward Shaping and Behaviour Cloning for Offline Policy", "content": "This section describes our framework for training a CNN to predict future events in the game. This CNN acts as our task predictor, $Obs_prd(x_t)$. Additionally, using the same CNN, we introduce a method for selecting an action, $u \\in U$. This selection process represents an offline policy, denoted by $\\pi_\\rho$, which guides the agent's behavior without directly interacting with the environment.\nWe introduce a novel image abstraction method by which trajectories of the game, which may include trajectories of agents and other entities of the environment, are encoded in a single RGB image. The abstracted game trajectory image is fixed in memory and allows including the whole history of the game, as will be clear in the following.\nTo predict future tasks, we engineer the dataset as follows. Given the abstracted images, we create, at every time step of the game, a vector valid label of labels that includes the labels of the future task.\nLet $l \\in N$ and $w \\in N$ be the length and width of the image, respectively. An instantaneous image of the game at time t, $C_t \\in I$, where $I := N^{255} \\times N^{255} \\times N^{255} \\times N^{l \\times w}$, is defined as $C_t := RGB_gameAbst(x)$. $RGB_gameAbst : X \\rightarrow I$ is a function that maps the state of the game into an RGB image.\nWe compute a history-dependent image of the game at time t, $Im_t$, as the following:\n$Im_t \\leftarrow C_t + \\epsilon Im_{t-1}; 0 < \\epsilon < 1$\nwhere $\\epsilon$ is a discount factor. See Figure ?? where we show an abstracted image of a soccer game.\nWe train our model using data recorded from an expert playing the game. This data consists of sequences of moments (time steps) where each moment is labeled with two things: Task: Which specific action was happening at that moment (identified by a number from 1 to $N_T$ in the set $C$; $N_T$ is the total number of tasks; and Action: What the expert player did at that moment (identified by a number from 1 to $N_A$ in the set $C$; $N_A$ is the total number of actions.\nTo account for the duration of tasks, we create a special label, concrete-time label denoted by $L_{t;j} = \\{l_t l_{t+1} ... l_{t+d, uf}\\}$. The specific architecture of the Convolutional Neural Network (CNN) used in this work, along with the details of its training process, are presented in Appendix ??."}, {"title": "5.2. Testing APPO", "content": "Given the soccer game setting and the definition the pass task $\\phi_{pass}$ in (32), and $Obs_prd$ and the offline policy $\\pi_\\rho$ (which are detailed in the previous section), we consider the corresponding concrete-time reward (8) and shaped reward function (15) in testing the proposed algorithm.\nWe tested the policy mixing variant for the inverted pendulum gym environment. Figure 2 depicts the fact that the proposed mixing algorithm episodic return surpasses the vanilla PPO."}]}