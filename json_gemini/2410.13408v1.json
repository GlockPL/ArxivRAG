{"title": "MoR: Mixture of Ranks for Low-Rank Adaptation Tuning", "authors": ["Chuanyu Tang", "Yilong Chen", "Zhenyu Zhang", "Junyuan Shang", "Wenyuan Zhang", "Yong Huang", "Tingwen Liu"], "abstract": "Low-Rank Adaptation (LoRA) drives research\nto align its performance with full fine-tuning.\nHowever, significant challenges remain: (1)\nSimply increasing the rank size of LoRA does\nnot effectively capture high-rank information,\nwhich leads to a performance bottleneck. (2)\nMoE-style LoRA methods substantially in-\ncrease parameters and inference latency, con-\ntradicting the goals of efficient fine-tuning and\nease of application. To address these chal-\nlenges, we introduce Mixture of Ranks (MoR),\nwhich learns rank-specific information for dif-\nferent tasks based on input and efficiently inte-\ngrates multi-rank information. We firstly pro-\npose a new framework that equates the integra-\ntion of multiple LoRAs to expanding the rank\nof LoRA. Moreover, we hypothesize that low-\nrank LoRA already captures sufficient intrinsic\ninformation, and MoR can derive high-rank\ninformation through mathematical transforma-\ntions of the low-rank components. Thus, MoR\ncan reduces the learning difficulty of LoRA\nand enhances its multi-task capabilities. MoR\nachieves impressive results, with MoR deliver-\ning a 1.31% performance improvement while\nusing only 93.93% of the parameters compared\nto baseline methods.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (OpenAI, 2023;\nAnthropic, 2023) based on the Transformer archi-\ntecture have made significant breakthroughs in var-\nious natural language tasks due to their powerful\nunderstanding and generation capabilities. When\nfine-tuned on downstream datasets, these models\ndemonstrate outstanding performance across vari-\nous tasks (Touvron et al., 2023; Jiang et al., 2023).\nHowever, their ever-increasing size requires sub-\nstantial computational resources for comprehensive\nHow can we enhance the model's multi-task capa-\nbilities with high-level parameter efficiency?"}, {"title": "2 Preliminary", "content": "In this section, we briefly introduce the background\nand principles of MoE-style LoRA and demon-\nstrate how MoE-style LoRA leverages sparse acti-\nvation to construct an effective high-rank structure\nfrom low-rank LORA modules."}, {"title": "When MoE adapted to LoRA", "content": "The traditional\nMoE structure is incorporated into model fine-\ntuning by employing multiple expert modules,\nwhich can be expressed as:\n$Wx = \\sum_{i=1}^{N}Gi(x)Ei(x)$ (1)\nwhere $W \\in R^{dout\\times din}$ is the pre-trained model\noriginal parameter. After converting the network\ninto MoE structure, the entire module is replaced\nwith multiple isomorphic or heterogeneous sub-\nexpert network modules $E_i$. The gated routing $G_i$\nis used to calculate expert weights and allocate\nrouting for different input tokens.\nConsidering that the MoE networks have a large\nnumber of parameters and require extensive re-\nsources to maintain model resilience and continue\npre-training, it is impractical to use MoE approach\nfor instruction supervision fine-tuning. A feasible\napproach is using LoRA module to implement the\nMoE model. Specifically, each MoE expert $E_i$ will\nbe replaced by LoRA matrices $A_i$ and $B_i$, and it\ncan be expressed as $E_i = B_iA_i$. Therefore, the\nMoE-style LoRA network plugin module can be\ndenoted as:\n$o = Wx + \\Delta Wx$\n$= Wx + \\sum_{i=1}^{N} G_i(x) B_i A_i x$ (2)\nwhere $B\\in R^{d\\times r}$ and $A \\in R^{r\\times h}$ denotes the LORA\nmatrice, where $r$ denotes the rank of the matrix, and\n$N$ denotes the number of LoRA experts."}, {"title": "MoE-style LoRA plugin is a High-Rank LORA\nwith Sparse Activation", "content": "We begin by consid-\nering a high-rank LoRA, where the updated ma-\ntrix $\\Delta W = BA$ captures information across $r$\ndimensions. To decompose this high-rank struc-\nture, we introduce $n$ low-rank LoRA modules,\neach with a rank of $\\frac{r}{n}$, such that the update ma-\ntrix for the i-th module is given by $\\Delta W_i = B_iA_i$,\nwhere $B_i\\in R^{d \\times \\frac{r}{n}}$ and $A_i \\in R^{\\frac{r}{n} \\times h}, \nB = [\\begin{smallmatrix} B_1\\\\ B_2\\\\ ... \\\\ B_n \\end{smallmatrix}],\nA = [A_1,...,A_n].$ These\nmatrices $B_i$ and $A_i$ represent sub-blocks of the\noriginal matrices $B$ and $A$, respectively.\nThe original high-rank matrix $\\Delta W$ can then be\nreconstructed by summing over the contributions\nof all low-rank modules as:\n$\\Delta W = BA = \\sum_{i=1}^{n}B_iA_i$ (3)\nThis decomposition shows that the high-rank ma-\ntrix is, in fact, a combination of several lower-rank\nmatrices. In the context of MoE-LORA, this decom-\nposition is further enhanced by introducing sparse\nactivation. MoE activates only a subset of experts\nfor any given input, and the output is the weighted\nsum of these activated experts. For $k$ activated\nexperts, the update matrix becomes:\n$\\Delta W_{MOE} = \\sum_{i=1}^{k} G_i(x) B_iA_i$ (4)\nwhere $G_i$ represents the gating function's weight for\nthe i-th expert. By selectively activating experts,\nMoE effectively constructs a higher-rank represen-\ntation from a subset of low-rank modules. This\nsparse activation ensures computational efficiency\nwhile maintaining the expressive power of a high-rank matrix."}, {"title": "Advantages and Challenges of increasing LoRA\nrank", "content": "High-rank LoRA retains a more complete\nrepresentation of the original model's weight ma-\ntrix $W$. From the perspective of singular value\ndecomposition (SVD) (Wall et al., 2003), $W$ can\nbe expressed as $W = U\\Sigma V^T$, where $\\Sigma$ contains\nthe singular values. In LoRA with rank $r$, truncates\nsmaller singular values, potentially losing valuable\ninformation. By increasing the rank $r$, high-rank\nLORA retains more singular values, thus minimiz-\ning the information loss, which can be summarized\nas:\n$||\\Delta W - AU_r \\Sigma_r V_r^T ||_F \\to 0$\nas $r \\to min(d, h)$ (5)\nwhere $|| \\cdot ||_F$ is the Frobenius norm. This higher\nfidelity approximation allows the model to capture\nmore intricate task-specific features, improving per-\nformance in complex tasks by preserving a broader\nspectrum of information in the parameter space.\nHowever, the increased rank also brings chal-\nlenges. High-rank LoRA introduces more param-\neters, which raises the risk of overfitting, partic-\nularly when the additional capacity exceeds the\ntask's complexity (Yang et al., 2024). This in-\ncreased parameter space can obscure task-relevant\ninformation and make learning more difficult (Zi\net al., 2024). To address this, it is possible to lever-\nage transformations to map low-rank LoRA into\na higher-rank space. By applying such transfor-\nmations, we can effectively represent multi-task\ninformation in a high-rank space while maintain-\ning the computational efficiency of low-rank mod-\nels (Wang et al., 2024). This approach allows us\nto exploit the expressiveness of high-rank LoRA\nwithout the drawbacks of increased complexity:\n$B' A' = \\Lambda_B B \\Lambda \\Lambda A \\Lambda_A$ (6)\nwhere $\\Lambda$ is a transformations matrix that expands\nthe low-rank matrix into a higher-dimensional sub-\nspace, facilitating the learning of multi-task repre-\nsentations while controlling over-fitting."}, {"title": "3 Method", "content": "In this section, we introduce MoR for efficient\nmulti-task low-rank parameter fine-tuning. MoR\nenhances the efficiency of parameter tuning across\nvarious domains by leveraging multi-dimensional\nscaling transformations on the LoRA matrix.\nBy transforming the LoRA matrix, MoR can\ncapture task-specific information while preserving\nshared knowledge in the standard LoRA parame-\nters. This design enhances the model's capacity\nto learn high-rank spatial representations, thereby\nimproving the performance of LLMs in multi-task\nlearning scenarios."}, {"title": "3.1 Transform of LoRA", "content": "In this section, We introduce a LoRA transforma-\ntion method by applying intrinsic scaling transfor-\nmations to the exact subspace of the LoRA matrix.\nApplying specific transformations to the LoRA\nmatrix enables the model to capture more in-\ntricate information (Renduchintala et al., 2023;\nWang et al., 2024). To emphasize directional and\nmagnitude-specific changes in LoRA, we introduce\ntask-related, learnable vectors, $\\Lambda_A$ and $\\Lambda_B$, to trans-\nform the LORA matrices $A$ and $B$. We concrete\nthe transformation operations on LoRA matrices,\nwhich is expressed in Equation 6. In order to re-\nduce computation, we have performed parameter\nabsorption on $\\Lambda$ and $\\Lambda_A$, and adopted $\\Lambda$ into\nrouter, as follows:\n$\\hat{A} = \\Lambda A \\Lambda_A, \\hat{B} = \\Lambda_B B$ (7)"}, {"title": "3.2 MoR Architecture", "content": "In the MoE-LORA structured model, each expert\nconsists of projection matrices $A_i$ and $B_i$, leading\nto significant parameter redundancy (Dou et al.,\n2024; Liu et al., 2023). Additionally, the division\nof parameters across different experts can result in\nlosing coupling information.\nTo alleviate these challenges, we propose a\nparameter-sharing method called MoR, which\nleverages multiple transformation vectors to scale\nthe shared LoRA matrices $A$ and $B$ within a spe-\ncific parameter subspace. A space importance ad-\njustment router $G$ is introduced to weigh the effect\nof different vectors, enabling efficient parameter\nfine-tuning across multiple task-specific subspaces.\nThe overall structure is illustrated in Figure 2.\nFor a single direction $D_i$, the transformation is\ndefined as:\n$D_i = \\alpha \\hat{B} B \\Lambda \\Lambda \\hat{A} A$ (10)\nwhere $B\\in R^{dout \\times r}$ and $A \\in R^{r \\times din}$ represent\nthe shared LoRA matrices, and $\\Lambda_A$ and $\\Lambda_B$ are\nthe transformation matrices of the LoRA projec-\ntions, described in Section 3.1. $\\alpha$ is the scaling\nhyperparameter.\nTo ensure that the shared LORA matrices scale\nacross multiple directions without favoring any sin-\ngle component excessively (which could result in\nparameter updates along only one direction and\nhinder task adaptability), we introduce a transfor-\nmation component normalization matrix $G$:\n$G_i(x) = \\frac{exp(h(x)_i)}{\\sum_{i=1}^{N}exp(h(x)_i)}$ (11)\nwhere $h(x)$ denotes the router layer matrix, calcu-\nlated as $h(x) = W_r x$.\nFor a maximum of $N$ directions, the entire MOR\nmodule is expressed as:\n$o = Wx + \\Delta Wx$\n$\\approx Wx + \\sum_{i=1}^{N}G_i(x) D_i x$ (12)\nwhere $N$ is the number of transformation spaces."}, {"title": "3.3 Implementation in LLAMA", "content": "In our implementation, we replace only the Feed-\nForward Neural Network with the MoR plug-in.\nWe denote the output of the FFN as $o$, and the\nforward pass during training can be expressed as:\n$o = Wx + \\Delta Wx$ (13)\nwhere $W\\in R^{dout\\times din}$ represents the pre-trained\nmodel parameters, and $\\Delta W \\in R^{dout \\times din}$ represents\nthe parameters updated during training.\nThe MoR plug-in is used to replace $\\Delta W$, keep-\ning all parameters of the base model frozen during\ntraining, and only the MoR parameters within the\nFFN are trained.\nTo ensure more efficient model training, we im-\nplement the MoR method in parallel on the Llama\nmodel, as shown in Figure 3. For the trainable pa-\nrameters $\\Lambda_A$ and $\\Lambda_B$, we stack them as matrices,\nwhich can be expressed as:\n$\\Omega_A \\leftarrow \\Lambda_A, \\Omega_B \\leftarrow \\Lambda_B$ (14)\nwhere $\\Omega_A \\in R^{N \\times r}, \\Omega_B \\in R^{N \\times dout}$, and $N$ is the\nnumber of specified expert hyperparameters.\nDuring the forward computation, for $N$ Rank\nexperts, the transformation vector can be matrixed\nusing $\\Omega_A$ and $\\Omega_B$ to accelerate training. The for-\nward computation of Llama, can be expressed as:\n$o = Wx + \\Delta Wx$\n$= Wx + \\sum_{i=1}^{N} G_i(x) \\Omega_B \\cdot B(\\Omega_A \\cdot (Ax))$ (15)"}, {"title": "4 Experiments", "content": "Datasets To demonstrate the effectiveness of our\nmethod in multi-task learning, we train the model\non a series of well-curated datasets, enabling it to\nexcel in multiple downstream tasks simultaneously.\nDetails of the datasets are provided in Appendix A."}, {"title": "Training", "content": "Our experimental framework utilizes\nthe implementation on the PEFT package (Man-\ngrulkar et al., 2022) and is executed on 8 NVIDIA\nA800 GPUs (80GB). Our MoR plugin is integrated\ninto the LLaMA feedforward neural network,\nspecifically within the 'up_proj', 'down_proj', and\n'gate_proj' layers. Each layer is initialized with\nmultiple sets of transformation vectors designed to\ncapture diverse aspects of information across tasks.\nMeanwhile, task-shared information is stored in\nthe shared LoRA mapping matrices, denoted as As\nand Bs.\nFor the shared LoRA parameters, we set the hy-\nperparameters $\\alpha$ and r to 32 and 8, respectively.\nDropout rate is applied at 0.05, and the learning\nrate is set to 2$\\times 10^{-4}$. During the model training\nphase, we freeze all base model parameters; only\nthe transformation vectors in MoR, shared LORA\nparameters, and direction constraint matrices are\nfine-tuned. To ensure fairness, the global batch size\nis consistently set to 8, and the input max length is\nset to 1024 across all experiments."}, {"title": "Evaluation", "content": "We employ the OpenCompass evalu-\nation framework (Contributors, 2023) to conduct a\nthorough assessment of the model's performance,\nfocusing on three primary tasks. The details about\nthe evaluation are shown in Appendix B."}, {"title": "4.2 Baseline", "content": "Supervised (Full) Fine-tuning We fine-tune all\nmodel parameters with the training set. Full param-\neter fine-tuning generally achieves the best perfor-\nmance in instruction-tuning tasks.\nLow-Rank Adaptation We apply both vanilla\nLORA and its improved version, DoRA (Hu et al.,\n2021; Liu et al., 2024b), for model fine-tuning.\nSpecifically, We apply LoRA and DoRA to all lin-\near layers in feed-forward network, and fine-tune\nthe output, up, gate, and down projection weights\nin these layers. The scaling factor $\\alpha$ and dropout\nrate are set to 32 and 0.05, respectively.\nMulti-LoRA Tuning We also conduct exper-\niments with a method that integrates multi-\nple LoRA modules with the MoE mechanism,\nMOELORA (Liu et al., 2023). Specifically, we ap-\nply two LoRA experts with a rank of 8 and a router\nlinear layer to all linear layers in the feed-forward\nnetwork, fine-tuning the output, up, gate, and down\nprojection weights. The LoRA scaling factor $\\alpha$ and\ndropout rate are set to 32 and 0.05, respectively."}, {"title": "4.3 Result", "content": "Enhancement of Performance The results pre-\nsented in Table 1 demonstrate the effectiveness of\nour proposed MoR method, which simultaneously\nenhances various specialized capabilities by per-\nforming intrinsic transformations on LoRA. Specif-\nically, MoR improves the average performance\nby 7.4% over LoRA and by 7.21% over DoRA.\nFurthermore, when compared with the MoELORA\nmethod, which involves multiple LoRA ensem-\nbles, our method achieves a 1.31% performance\nimprovement over the simple ensemble approach.\nThis improvement is attributed to the intrinsic trans-\nformation of LoRA and the use of scaling vec-\ntors to store specific knowledge while sharing the\nLORA matrix. Our method achieves the best results\non four of the six tasks in the Commonsense &\nReading Comprehension benchmark, and the Open-\nbookQA and MMLU tasks. However, performance\ndecreases on Lambda tasks that were not included\nin the training data, and NQ task. This indicates\nthat, while our approach greatly enhances learning\ncapabilities, it also results in limited performance\non out-of-distribution data."}, {"title": "Parameter Efficiency", "content": "Table 1 and Figure 4 il-\nlustrate that when our model employs a single vec-\ntor expert group, its performance matches that of\nLORA and DORA. This is because when the experts\nnumber comes to 1, our method is fundamentally\nequivalent to the DoRA approach. When increas-\ning the number of vector expert groups to four\nleads to a substantial improvement in model per-\nformance without a significant increase in the num-\nber of parameters compared to LoRA and DORA.\nFurthermore, when the number of vector expert\ngroups is expanded to eight, our method surpasses"}, {"title": "4.4 Analysis", "content": "We conducted a series of\nanalyses on the impact of the number of transfor-\nmation vectors on model performance, including\ncases where the number of scaling vector experts\nis 1, 2, 4, 8, and 12, as shown in Table 3.\nAs is shown in Figure 6(a) When the number\nof experts is less than 8, the model's performance\ngradually improves as the number of experts in-\ncreases, and reaches its peak at 8 experts. When\nthe number of experts increases to 12, the model's\nperformance decreases. This indicates that increas-\ning the number of model experts can improve the\nperformance within a certain range, but when it ex-"}, {"title": "5 Related Work", "content": "LORA and its variants Low-Rank Adapta-\ntion (Hu et al., 2021) is motivated by the discovery\nthat a low intrinsic dimension within large param-\neters plays a crucial role in Large Language Mod-\nels (Aghajanyan et al., 2020). It introduces two\ntrainable low-rank matrices into each layer and\nreduces storage and task-switching overhead by\nsharing pre-trained models across different tasks.\nSubsequently, numerous methods have been pro-\nposed to enhance the performance and efficiency of\nLORA and enlarge the learnable rank. Inspired by\nrandom projections, VeRA (Kopiczko et al., 2023)\nshares two frozen random matrices across all lay-\ners and only trains a set of vectors for every VeRA\ncomponent. Additionally, Tied-LoRA (Renduchin-\ntala et al., 2023) enhances parameter efficiency by\nsharing trainable LoRA matrices across all layers\nand introducing the trainable vectors to improve the\nperformance of LoRA. Moreover, DoRA (Liu et al.,\n2024b) enhances the model performance by decom-\nposing LoRA into direction and amplitude with\nonly one learnable vector added. However, these\nmethods just transform a single group of LoRA in a\nsingle direction, resulting in poor learning ability of\nthe model on multiple tasks. Our proposed method\ncan more effectively learn multiple task data by\nperforming intrinsic changes on the LoRA matrix\nat various angles and finally integrating them.\nMoE-style PEFT The Mixture of Experts frame-\nwork replaces the model with sparsely activated ex-\npert modules, boosting model capacity and through-\nput speed (Jacobs et al., 1991). Token-level MoE\narchitectures are now widely used in pre-trained\nlanguage and vision models(Shazeer et al., 2017;\nLiu et al., 2024a; Wu et al., 2024).\nTo make the model better learn these multi-task\ndatasets, MoELORA (Liu et al., 2023) leverages\nLORA as a plugin-type method for task-specific\nand multi-task tuning, particularly in medical appli-\ncations. LoRAMOE (Dou et al., 2024) introduces\nan MoE-style plugin and a Localized Balancing\nConstraint to mitigate world knowledge forgetting\nin LLMs while enhancing the model's multi-task\nlearning abilities. Nevertheless, these methods ex-\nhibit significant parameter redundancy across dif-\nferent LoRAs. To achieve parameter efficiency,\nour proposed method substantially improves the\nmodel's multi-task learning capabilities by perform-\ning multi-dimensional intrinsic transformations on\nshared LORA."}, {"title": "6 Conclusion", "content": "In this work, we first analyze the advantages and\nchallenges associated with expanding the rank of\nLORA and introduce a framework that equates the\nintegration of multiple LoRAs to an expansion\nof LoRA rank. Building on this foundation, we\npropose the MoR framework. MoR learns LORA\ntransformations specific to different tasks based on\nvarying inputs and efficiently integrates multi-rank\ninformation. By fully leveraging the foundational\nknowledge captured by LoRA, MoR reduces the\ncomplexity of learning and enhances multi-task ca-\npabilities. Compared to baseline methods, MoR\nachieves an absolute performance improvement of\n1.31% with only 93.93% of the parameter count,\nestablishing itself as a state-of-the-art LoRA ap-\nproach that balances efficiency and performance."}, {"title": "Limitations", "content": "In this section, we discuss the potential limitations\nof our proposed method MoR. Firstly, although we\nhave demonstrated our advantage while enhancing\nthe downstream ability of the LLMs with multi-\ntask datasets, we limit the model size to 7B due to\nresource and time constraints. Further work will\nbe conducted on the larger LLMs to understand\nthe influence of large-scale SFT on these LLMs\nand to boost their multi-task abilities. Secondly,\nOur method is similar to other MoE-style LoRA\nmethods that can't merge the plugin parameters\nwith a pre-trained model like vanilla LoRA, which\nwill lead to latency during the inference period.\nFuture work will be conducted on the rule-guided\nrouter to solve the problem."}]}