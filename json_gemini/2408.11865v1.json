{"title": "How Susceptible are LLMs to Influence in Prompts?", "authors": ["Sotiris Anagnostidis", "Jannis Bulian"], "abstract": "Large Language Models (LLMs) are highly sensitive to prompts, including\nadditional context provided therein. \u0100s LLMs grow in capability, under-\nstanding their prompt-sensitivity becomes increasingly crucial for ensuring\nreliable and robust performance, particularly since evaluating these mod-\nels becomes more challenging. In this work, we investigate how current\nmodels (Llama2, Mixtral, Falcon) respond when presented with additional\ninput from another model, mimicking a scenario where a more capable\nmodel \u2013 or a system with access to more external information \u2013 provides\nsupplementary information to the target model. Across a diverse spec-\ntrum of question-answering tasks, we study how an LLM's response to\nmultiple-choice questions changes when the prompt includes a prediction\nand explanation from another model. Specifically, we explore the influence\nof the presence of an explanation, the stated authoritativeness of the source,\nand the stated confidence of the supplementary input. Our findings reveal\nthat models are strongly influenced, and when explanations are provided\nthey are swayed irrespective of the quality of the explanation. The models\nare more likely to be swayed if the input is presented as being authoritative\nor confident, but the effect is small in size. This study underscores the\nsignificant prompt-sensitivity of LLMs and highlights the potential risks\nof incorporating outputs from external sources without thorough scrutiny\nand further validation. As LLMs continue to advance, understanding and\nmitigating such sensitivities will be crucial for their reliable and trustworthy\ndeployment.", "sections": [{"title": "1 Introduction", "content": "There are many settings where input to Large\nLanguage Models (LLMs) is augmented by out-\nput from other models or external sources. This\nincludes for example self-critique and oversight\n(Bai et al., 2022), retrieval-augmented genera-\ntion (RAG) (Lewis et al., 2020) and collabora-\ntive multi-agent systems where LLMs interact\nwith each other or with humans to solve com-\nplex tasks. As LLMs become increasingly inte-\ngrated into real-world applications, understand-\ning how they respond to and incorporate infor-\nmation from external sources becomes crucial.\nHowever, LLMs are known to show sycophan-\ntic behaviour (Perez et al., 2022; Sharma et al., 2023). Specifically, models tend to agree\nwith the interacting users' views, even if these are different from their own, manifesting\na Clever Hans effect\u00b9 in LLMs. This behaviour is not generally desirable when interacting"}, {"title": "2 Methodology", "content": "We investigate the setup where an LLM judge is tasked with evaluating answers to a\ngiven question. The input may be augmented by another model acting as the advocate,\nrecommending a particular answer to the query. Both judges and advocates stem from\npre-trained language models LLM(w), that define a distribution across the set of finite-\nlength strings W*, where W signifies the alphabet (i.e. model vocabulary that includes an\nend-of-sequence token). These models operate in an auto-regressive manner, meaning that\nthey model the conditional distribution LLM(\u00b7 | w<t) and compute\n\\(LLM(w) = \\prod_{t=1}^{T+1} LLM(w_t | w_{<t}).\\)"}, {"title": "3 Experiments", "content": "Effect of inputs on LLM\u2081's predictions. We define influence as the likelihood of a judge to\nadhere to the guidance of the advocate, irrespective of its own unbiased prediction, i.e.\n\\(Influence = E_{i=1,2,...,n} \\sum_{j=1,2,...,|Y_i|} [1(\\hat{y}_{ij} = \\text{argmax}_{y \\in Y_i}LLM_J(y|x_i, e_{ij}))].\\)\nWe will often group results depending if the provided explanation corresponds to a correct\nor not choice, i.e. \u0177ij = yi or \u0177ij \u2260 yi. This allows us to investigate how the influence\nof the advocate varies based on the quality and accuracy of the provided explanations.\nThroughout, we test two distinct scenarios, where the advocate promotes one specific\nanswer with and without providing an explanation, as per Eq. 2. Results in Fig. 3 reveal that\nthe level of influence under this setting is generally very high. We do note that for datasets\nthat models exhibited higher unbiased performance (see Fig. 2), models are able partially\nsuppress the influence provided. Still, though all models are highly susceptible to anchoring\nopinions and argumentation. This result, validates reported sycophantic behavior across\nLLMs (Perez et al., 2022; Sharma et al., 2023). Additionally, we investigate the impact of\ncomprehensive argumentation on endorsing a specific answer. Surprisingly, we discover\nthat more extended explanations involving argumentation result in a less pronounced\nimpact. This suggests that models may be able to identify flawed argumentation when\npresented with the reasoning behind a conclusion."}, {"title": "3.1 Mitigation", "content": "Our examination has revealed the pervasive nature of influence across language models\nand tasks, even in the absence of adequate explanation. Naturally, the question whether\nthis negative bias can be mitigated arises. Conventional techniques for tuning and align-\ning LLMs, without resorting to additional fine-tuning, resolve around prompting. These\nprompting strategies can take various forms, including (a) instructing the model to adhere\nto specific values (Si et al., 2022), (b) chain-of-thought mechanisms (Wei et al., 2022) or (c)\nutilizing few-shot prompting techniques (Brown et al., 2020; Nori et al., 2023). To implement\nthese strategies we first (a) instruct the model to adopt a highly critical stance towards\nadditional provided opinions and explanations. Furthermore, (b) we prompt the assistant\nto begin its reply by explicitly disregarding previously provided opinions. Lastly, (c) we\npresent a number of few-shot examples (5 throughout our experiments) of questions and\ncorresponding explanations put forth by advocates. In these few-shot examples, regardless\nof the advocate's opinions, the judge replies correctly, thus potentially ignoring the advo-\ncate's suggestion. Surprisingly, our findings indicate that influence largely persists across\nthe various settings for the models tested. This underscores the challenges associated with\neffectively countering the negative bias introduced. Notably, the Mixtral and Llama2 models\nonly exhibit significant degree of influence mitigation under appropriate chain-of-thought\nprompting."}, {"title": "4 Related Work", "content": "Sycophancy and LLM bias.\nRecent research has begun to underscore the sycophantic\ntendencies exhibited by LLMs (Perez et al., 2022; Sharma et al., 2023). Notably, the presenta-\ntion of information (Wan et al., 2024; Turpin et al., 2024) causes significant influence, but\nthe mechanism differs in princliple between LLMs and humans (Slovic, 2020; Tjuatja et al.,\n2023). Shedding more light into this phenomenon and investigating mitigation is important,\ngiven the potential use of automated oversight as a means of alignment (Bai et al., 2022;\nBengio et al., 2023; Yang et al., 2023; Ji et al., 2023; Kenton et al., 2024). We expect this trend\nto continue as these models become more powerful (Radhakrishnan, 2023; Burns et al., 2023)\nand as evaluation becomes an increasingly difficult task (Xu et al., 2023; Bulian et al., 2023).\nCritiques. To tackle the potential compounding errors due to auto-regressive inference\nof current state-of-the-art language models (Bachmann & Nagarajan, 2024), and moti-\nvated by the premise that for some tasks, validation is inherently an easier task than gener-\nation (Qiao et al., 2022), critiques have been established as a suitable technique to refine\noriginal model predictions. Critiques, contingent upon their source, manifest in various\nforms; self-critique (Saunders et al., 2022) when originating from the agent itself, or resem-\nbing a form of debate (Michael et al., 2023; Khan et al., 2024) if these are coming from\nexternal agents, with potential access to additional context. These critiques serve to rectify\nerrors (Fernandes et al., 2023) and enhance the model's understanding and decision-making\ncapabilities, thus fortifying its overall performance and reliability (Leike et al., 2018). In that\nspirit, Bowman et al. (2022) use oversight as a mean to reinforce non expert human raters,\nand Saunders et al. (2022) teach models to generate critiques with the same goal."}, {"title": "5 Discussion", "content": "In this work, we investigate the influence that inputs augmented by model-generated\npredictions and explanations can have on the decision-making process of a language model\nacting as a judge. We hypothesize that these predictions and explanations serve as an\nanchoring mechanism, leading LLM judges to place excessive reliance on the opinions and\ninformation presented therein (Kahneman & Tversky, 1982; Baumeister et al., 2001).\nOur findings reveal that the current LLMs analysed in this study are heavily influenced by\nthe provided context across a wide range of question-answering tasks, regardless of the\nadequacy of the explanations provided. This raises concerns about proposals to use LLMs\nas a substitute for human raters (Gilardi et al., 2023; Chiang & Lee, 2023). While humans\nwill likely remain an essential part of oversight (Bulian et al., 2023; Michael et al., 2023), the\nsusceptibility of human judges to be influenced by LLMs should also be examined. We do\nnot expect our results to extend directly to humans, as the effect of influence of human judges\nby persuasive LLMs may manifest in different ways. LLMs can generate flawed arguments\nwith high fluency and may possess the ability to obfuscate these flaws, potentially making\nhumans vulnerable to poor judgment. Our results underscore the need for special care\nto be taken to mitigate the influence of model-generated predictions and explanations on\nLLM judges. We provide evidence suggesting that prompting strategies alone may not be\nsufficient to address this issue. The findings indicate that the critical reasoning abilities of\nthe models are inadequate for distinguishing between good and bad arguments. Therefore,\nprogress in reasoning is necessary to make the use of model-generated predictions and\nexplanations more beneficial.\nHowever, we have also demonstrated that predictions and explanations presented with\nhigher levels of authority or confidence exert a stronger influence. Furthermore, we have\nshown that disclosing confidence for a specific explanation or providing multiple explana-\ntions can significantly enhance the performance of judges. While these findings are generally\ndesirable, they may also open up avenues for \u201cjailbreaking\u201d a model, potentially leading to\nunintended consequences."}, {"title": "6 Ethics Statement", "content": "Large language models have emerged as a high-impact technology with the potential for\nboth productive and destructive use cases. In this study, we take steps towards better\nunderstanding the impact of misleading argumentation or information on the reasoning\ncapabilities of LLMs. We find that the current LLMs that we analysed are susceptible to\nbeing mislead and hope that our framework prompts future work aimed towards better\nmitigation of such behaviors."}, {"title": "7 Reproducibility Statement", "content": "We have taken multiple steps to ensure reproducibility of our work. We provide in the main\ntext and in the appendix the full text of all prompts used. We use publicly available models\nand datasets, as described in detail in App. A."}, {"title": "A Experimental Details", "content": "In the following, we provide more information on the experimental setup.\nPrompt Formatting. We modify the chat template format for the Mixtral and Falcon models\nto enable multi-turn discussions with optional system prompts. These are provided in\nDataset Details. We provide more details on the datasets used, which we access through\nhttps://huggingface.co/.\nPIQA. This is developed with the aim of exploring the limits of commonsense reasoning,\ndelving into the comprehension of physical knowledge within existing models.\nSIQA. It's a benchmark designed for evaluating social commonsense intelligence through\nquestion answering.\nCommonsenseQA. A multiple-choice question answering dataset requiring different types\nof commonsense knowledge to predict the correct answers.\nOpenBookQA. Encourages exploration in question-answering, fostering deeper insights\ninto topics by presenting them as open books alongside datasets. It challenges participants\nwith questions demanding multi-step reasoning, leveraging common and commonsense\nknowledge, and adept text comprehension.\nWikiQA. Offers a collection of question and sentence pairs, designed for research on open-\ndomain question answering. Each question is associated with a Wikipedia page containing\npotential answers.\nGPQA. A challenging dataset of multiple-choice questions written by domain experts in\nbiology, physics, and chemistry. The questions are high-quality and extremely difficult.\nQuALITY. Multiple-choice questions designed for comprehensive understanding of\nlengthy documents. Questions are crafted and verified by contributors who have thor-\noughly read the entire passage, rather than relying on summaries or excerpts."}, {"title": "B More Experiments", "content": "We present supplementary results to the experiments presented in the main text.\nChange in predictions. We present results missing in Section 3 for the Llama2 and Fal-\ncon models. In Figures 10 and 11 we provide the influence decomposed by question type as\nwe did in Figure 4."}]}