{"title": "How Susceptible are LLMs to Influence in Prompts?", "authors": ["Sotiris Anagnostidis", "Jannis Bulian"], "abstract": "Large Language Models (LLMs) are highly sensitive to prompts, including additional context provided therein. \u0100s LLMs grow in capability, understanding their prompt-sensitivity becomes increasingly crucial for ensuring reliable and robust performance, particularly since evaluating these models becomes more challenging. In this work, we investigate how current models (Llama2, Mixtral, Falcon) respond when presented with additional input from another model, mimicking a scenario where a more capable model or a system with access to more external information \u2013 provides supplementary information to the target model. Across a diverse spectrum of question-answering tasks, we study how an LLM's response to multiple-choice questions changes when the prompt includes a prediction and explanation from another model. Specifically, we explore the influence of the presence of an explanation, the stated authoritativeness of the source, and the stated confidence of the supplementary input. Our findings reveal that models are strongly influenced, and when explanations are provided they are swayed irrespective of the quality of the explanation. The models are more likely to be swayed if the input is presented as being authoritative or confident, but the effect is small in size. This study underscores the significant prompt-sensitivity of LLMs and highlights the potential risks of incorporating outputs from external sources without thorough scrutiny and further validation. As LLMs continue to advance, understanding and mitigating such sensitivities will be crucial for their reliable and trustworthy deployment.", "sections": [{"title": "1 Introduction", "content": "There are many settings where input to Large Language Models (LLMs) is augmented by output from other models or external sources. This includes for example self-critique and oversight (Bai et al., 2022), retrieval-augmented generation (RAG) (Lewis et al., 2020) and collaborative multi-agent systems where LLMs interact with each other or with humans to solve complex tasks. As LLMs become increasingly integrated into real-world applications, understanding how they respond to and incorporate information from external sources becomes crucial. However, LLMs are known to show sycophantic behaviour (Perez et al., 2022; Sharma et al., 2023). Specifically, models tend to agree with the interacting users' views, even if these are different from their own, manifesting a Clever Hans effect\u00b9 in LLMs. This behaviour is not generally desirable when interacting"}, {"title": "2 Methodology", "content": "We investigate the setup where an LLM judge is tasked with evaluating answers to a given question. The input may be augmented by another model acting as the advocate, recommending a particular answer to the query. Both judges and advocates stem from pre-trained language models LLM(w), that define a distribution across the set of finite-length strings W*, where W signifies the alphabet (i.e. model vocabulary that includes an end-of-sequence token). These models operate in an auto-regressive manner, meaning that they model the conditional distribution LLM(\u00b7 | w<t) and compute\nLLM(w) = \\prod_{t=1}^{T+1} LLM(w_t | w_{<t}).\nWe focus on instruction-tuned conversational agents (Longpre et al., 2023), that we either use to generate new text, as an advocate for a specific answer option (LLMA) or to evaluate the model's behaviors, i.e. general knowledge, specific tendencies or other biases. We refer to the LLM used in the latter case as a judge (LLMJ). Our setting resembles a situation where a models input is augmented with additional information to improve the response (seel also Fig. 1).\nWe experiment with powerful current open-source models, namely Llama2 (Touvron et al., 2023), Mixtral (Jiang et al., 2024) and Falcon (Almazrouei et al., 2023)2. We evaluate on a range of datasets, namely:\n\u2022 Commonsense Reasoning. We evaluate PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), CommonsenseQA (Talmor et al., 2018), OpenBookQA (Mihaylov et al., 2018).\n\u2022 World Knowledge. We report results for WikiQA (Yang et al., 2015) and GPQA (Rein et al., 2023).\n\u2022 Reading Comprehension. We conduct experiments on QuALITY (Pang et al., 2021) and BoolQ (Clark et al., 2019).\nThese datasets pose questions or goals along with multiple options to choose from, along with optional explanation for the correct choice. We format individual samples in a multiple-choice format, as illustrated in Table 1 (further details in App. A). Instead of sampling from the model outputs, we directly assess the probability assigned to individual multiple-choice answers. In other words, given evaluations consisting of pairs (x1, y1), (X2,Y2), ... (Xn, Yn), we measure the performance of our language model as\n\\mathbb{E}_{i=1,2,...,n}.[1(y_i = \\underset{y \\in \\mathcal{Y}_i}{argmax} LLM_J(y|x_i))].\nHere 1 denotes the indicator function and Y; C W, the restricted set of accepted choices for the sample i, e.g. \"A\", \"B\" and \"C\" for multiple choice questions with three options\u00b3. We also randomly shuffle the order of the multiple choice options within x\u2081, to mitigate position bias (Liu et al., 2024).\nWe report initial performance of our conversational agents in Fig. 2 across the different datasets. We will refer to these results as the unbiased models' performance, as no external signal is provided to anchor their predictions. The chosen datasets exemplify various levels of difficulty across the spectrum. For some of them (e.g. PIQA, SIQA, CommonsenseQA, WikiQA), we expect the model to have a strong bias and thus be strongly grounded and less easily susceptible to external opinions provided. For other ones (e.g. GPQA, QuALITY), where the correct answer is likely unknown, we expect the model to be less certain and thus more easily manipulated (we provide more details on the datasets used in App. A)."}, {"title": "3 Experiments", "content": "Effect of inputs on LLM\u2081's predictions. We define influence as the likelihood of a judge to adhere to the guidance of the advocate, irrespective of its own unbiased prediction, i.e.\n\\mathbb{E}_{i=1,2,...,n} \\mathbb{E}_{j=1,2,...,|\\mathcal{Y}_i|} [1(\\hat{y}_{ij} = \\underset{y \\in \\mathcal{Y}_i}{argmax} LLM_J(y|x_i, e_{ij}))].\nWe will often group results depending if the provided explanation corresponds to a correct or not choice, i.e. \u0177ij = yi or \u0177ij \u2260 yi. This allows us to investigate how the influence of the advocate varies based on the quality and accuracy of the provided explanations. Throughout, we test two distinct scenarios, where the advocate promotes one specific answer with and without providing an explanation, as per Eq. 2. Results in Fig. 3 reveal that the level of influence under this setting is generally very high. We do note that for datasets that models exhibited higher unbiased performance (see Fig. 2), models are able partially suppress the influence provided. Still, though all models are highly susceptible to anchoring opinions and argumentation. This result, validates reported sycophantic behavior across LLMs (Perez et al., 2022; Sharma et al., 2023). Additionally, we investigate the impact of comprehensive argumentation on endorsing a specific answer. Surprisingly, we discover that more extended explanations involving argumentation result in a less pronounced impact. This suggests that models may be able to identify flawed argumentation when presented with the reasoning behind a conclusion."}, {"title": "3.1 Mitigation", "content": "Our examination has revealed the pervasive nature of influence across language models and tasks, even in the absence of adequate explanation. Naturally, the question whether this negative bias can be mitigated arises. Conventional techniques for tuning and aligning LLMs, without resorting to additional fine-tuning, resolve around prompting. These prompting strategies can take various forms, including (a) instructing the model to adhere to specific values (Si et al., 2022), (b) chain-of-thought mechanisms (Wei et al., 2022) or (c) utilizing few-shot prompting techniques (Brown et al., 2020; Nori et al., 2023). To implement these strategies we first (a) instruct the model to adopt a highly critical stance towards additional provided opinions and explanations. Furthermore, (b) we prompt the assistant to begin its reply by explicitly disregarding previously provided opinions. Lastly, (c) we present a number of few-shot examples (5 throughout our experiments) of questions and corresponding explanations put forth by advocates. In these few-shot examples, regardless of the advocate's opinions, the judge replies correctly, thus potentially ignoring the advocate's suggestion. Surprisingly, our findings indicate that influence largely persists across the various settings for the models tested. This underscores the challenges associated with effectively countering the negative bias introduced. Notably, the Mixtral and Llama2 models only exhibit significant degree of influence mitigation under appropriate chain-of-thought prompting."}, {"title": "4 Related Work", "content": "Sycophancy and LLM bias. Recent research has begun to underscore the sycophantic tendencies exhibited by LLMs (Perez et al., 2022; Sharma et al., 2023). Notably, the presentation of information (Wan et al., 2024; Turpin et al., 2024) causes significant influence, but the mechanism differs in princliple between LLMs and humans (Slovic, 2020; Tjuatja et al., 2023). Shedding more light into this phenomenon and investigating mitigation is important, given the potential use of automated oversight as a means of alignment (Bai et al., 2022; Bengio et al., 2023; Yang et al., 2023; Ji et al., 2023; Kenton et al., 2024). We expect this trend to continue as these models become more powerful (Radhakrishnan, 2023; Burns et al., 2023) and as evaluation becomes an increasingly difficult task (Xu et al., 2023; Bulian et al., 2023).\nCritiques. To tackle the potential compounding errors due to auto-regressive inference of current state-of-the-art language models (Bachmann & Nagarajan, 2024), and motivated by the premise that for some tasks, validation is inherently an easier task than generation (Qiao et al., 2022), critiques have been established as a suitable technique to refine original model predictions. Critiques, contingent upon their source, manifest in various forms; self-critique (Saunders et al., 2022) when originating from the agent itself, or resembling a form of debate (Michael et al., 2023; Khan et al., 2024) if these are coming from external agents, with potential access to additional context. These critiques serve to rectify errors (Fernandes et al., 2023) and enhance the model's understanding and decision-making capabilities, thus fortifying its overall performance and reliability (Leike et al., 2018). In that spirit, Bowman et al. (2022) use oversight as a mean to reinforce non expert human raters, and Saunders et al. (2022) teach models to generate critiques with the same goal."}, {"title": "5 Discussion", "content": "In this work, we investigate the influence that inputs augmented by model-generated predictions and explanations can have on the decision-making process of a language model acting as a judge. We hypothesize that these predictions and explanations serve as an anchoring mechanism, leading LLM judges to place excessive reliance on the opinions and information presented therein (Kahneman & Tversky, 1982; Baumeister et al., 2001).\nOur findings reveal that the current LLMs analysed in this study are heavily influenced by the provided context across a wide range of question-answering tasks, regardless of the adequacy of the explanations provided. This raises concerns about proposals to use LLMs as a substitute for human raters (Gilardi et al., 2023; Chiang & Lee, 2023). While humans will likely remain an essential part of oversight (Bulian et al., 2023; Michael et al., 2023), the susceptibility of human judges to be influenced by LLMs should also be examined. We do not expect our results to extend directly to humans, as the effect of influence of human judges by persuasive LLMs may manifest in different ways. LLMs can generate flawed arguments with high fluency and may possess the ability to obfuscate these flaws, potentially making humans vulnerable to poor judgment. Our results underscore the need for special care to be taken to mitigate the influence of model-generated predictions and explanations on LLM judges. We provide evidence suggesting that prompting strategies alone may not be sufficient to address this issue. The findings indicate that the critical reasoning abilities of the models are inadequate for distinguishing between good and bad arguments. Therefore, progress in reasoning is necessary to make the use of model-generated predictions and explanations more beneficial.\nHowever, we have also demonstrated that predictions and explanations presented with higher levels of authority or confidence exert a stronger influence. Furthermore, we have shown that disclosing confidence for a specific explanation or providing multiple explanations can significantly enhance the performance of judges. While these findings are generally desirable, they may also open up avenues for \u201cjailbreaking\u201d a model, potentially leading to unintended consequences."}, {"title": "6 Ethics Statement", "content": "Large language models have emerged as a high-impact technology with the potential for both productive and destructive use cases. In this study, we take steps towards better understanding the impact of misleading argumentation or information on the reasoning capabilities of LLMs. We find that the current LLMs that we analysed are susceptible to being mislead and hope that our framework prompts future work aimed towards better mitigation of such behaviors."}, {"title": "7 Reproducibility Statement", "content": "We have taken multiple steps to ensure reproducibility of our work. We provide in the main text and in the appendix the full text of all prompts used. We use publicly available models and datasets, as described in detail in App. A."}, {"title": "A Experimental Details", "content": "In the following, we provide more information on the experimental setup.\nPrompt Formatting. We modify the chat template format for the Mixtral and Falcon models to enable multi-turn discussions with optional system prompts. These are provided in Table 6.\nDataset Details. We provide more details on the datasets used, which we access through https://huggingface.co/.\nPIQA. This is developed with the aim of exploring the limits of commonsense reasoning, delving into the comprehension of physical knowledge within existing models.\nSIQA. It's a benchmark designed for evaluating social commonsense intelligence through question answering.\nCommonsenseQA. A multiple-choice question answering dataset requiring different types of commonsense knowledge to predict the correct answers.\nOpenBookQA. Encourages exploration in question-answering, fostering deeper insights into topics by presenting them as open books alongside datasets. It challenges participants with questions demanding multi-step reasoning, leveraging common and commonsense knowledge, and adept text comprehension.\nWikiQA. Offers a collection of question and sentence pairs, designed for research on open-domain question answering. Each question is associated with a Wikipedia page containing potential answers.\nGPQA. A challenging dataset of multiple-choice questions written by domain experts in biology, physics, and chemistry. The questions are high-quality and extremely difficult.\nQuALITY. Multiple-choice questions designed for comprehensive understanding of lengthy documents. Questions are crafted and verified by contributors who have thoroughly read the entire passage, rather than relying on summaries or excerpts."}, {"title": "B More Experiments", "content": "We present supplementary results to the experiments presented in the main text.\nChange in predictions. We present results missing in Section 3 for the Llama2 and Falcon models. In Figures 10 and 11 we provide the influence decomposed by question type as we did in Figure 4."}]}