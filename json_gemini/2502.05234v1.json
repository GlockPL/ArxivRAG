{"title": "Optimizing Temperature for Language Models with Multi-Sample Inference", "authors": ["Weihua Du", "Yiming Yang", "Sean Welleck"], "abstract": "Multi-sample aggregation strategies, such as majority voting and best-of-N sampling, are widely used in contemporary large language models (LLMs) to enhance predictive accuracy across various tasks. A key challenge in this process is temperature selection, which significantly impacts model performance. Existing approaches either rely on a fixed default temperature or require labeled validation data for tuning, which are often scarce and difficult to obtain. This paper addresses the challenge of automatically identifying the (near)-optimal temperature for different LLMs using multi-sample aggregation strategies, without relying on task-specific validation data. We provide a comprehensive analysis of temperature's role in performance optimization, considering variations in model architectures, datasets, task types, model sizes, and predictive accuracy. Furthermore, we propose a novel entropy-based metric for automated temperature optimization, which consistently outperforms fixed-temperature baselines. Additionally, we incorporate a stochastic process model to enhance interpretability, offering deeper insights into the relationship between temperature and model performance.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities across various domains, including question answering (Kamalloo et al., 2023), intelligent agents (Wang et al., 2024b; Zhang et al., 2023), scientific discovery (Ma et al., 2024; Romera-Paredes et al., 2024), and mathematical reasoning (Ahn et al., 2024; Sun et al., 2024; Lin et al., 2024; Wu et al., 2024). A fundamental research question in generative models is how to effectively sample solutions from a learned distribution and perform inference-time reasoning.\nRecently, multi-sample aggregation strategies have gained increasing attention. These strategies involve generating multiple solutions from the underlying distribution and aggregating them into a final prediction (Wei et al., 2022; Yao et al., 2024). Common aggregation techniques, such as majority voting, weighted majority voting, and best-of-N selection, have demonstrated significant performance improvements in benchmark evaluations of LLMs (Welleck et al., 2024; Wang et al., 2024a).\nDespite the promising success of multi-sample aggregation strategies, there remains a lack of deep understanding regarding how to optimize the sampling process to enhance LLM performance under different conditions, including variations in training datasets, task types, and model sizes. A crucial open question is how to tune temperature, a key hyperparameter that controls the smoothness of the system-learned distribution. Intuitively, increasing the temperature leads to a smoother distribution, enhancing the diversity of sampled outputs. However, excessively high temperatures can introduce many low-quality samples, making aggregation more challenging (Holtzman et al., 2019; Renze & Guven, 2024). Conversely, lowering the temperature results in a highly concentrated distribution, reducing diversity and potentially omitting high-quality samples. Striking the right balance between over-sampling and under-sampling is therefore essential for optimizing LLM performance.\nA common practice in prior evaluations is to use the same temperature across all methods despite variations in training datasets, task types, model sizes, and aggregation strategies. This is clearly suboptimal. An alternative approach is to empirically tune the temperature using labeled validation data for each task, dataset, model size, and aggregation strategy (Zhang et al., 2024a; Dhuliawala et al., 2024). However, such a process is tedious and time-consuming and heavily dependent on the availability of labeled validation data, limiting its applicability when such data are scarce.\nIn this paper, we present the first systematic investigation of how temperature affects LLM performance under multi-sample aggregation strategies across various conditions. Furthermore, we propose a principled algorithmic solution for automated temperature optimization without requiring labeled validation data. Our key idea is as follows:\n1.  We use the confidence score of each model as a self-assessment measure.\n2.  If this self-assessment measure is highly correlated with model accuracy on test data, it can serve as a surrogate metric for tuning temperature in the absence of labeled validation data.\nA surprising finding from our temperature tuning experiments is the discovery of a phenomenon we term the entropy turning point (EntP) in the self-assessed performance curve. As illustrated in Figure 1(a), the token-level entropy (y-axis) of an LLM varies with temperature values (x-axis), shown by the blue curve, while its log-scale representation appears as the red curve. Notably, there is a transition point (EntP) where the red curve shifts from concave to convex. Figure 1(b) shows that the accuracy scores at EntP for a set of LLMs are strongly correlated with their highest accuracy scores obtained through grid-based temperature tuning. This finding supports our intuition that EntP can be leveraged to automatically determine the optimal temperature for each LLM using multi-sample aggregation strategies. We introduce TURN, our proposed approach for automated temperature optimization. Through extensive experiments, TURN has demonstrated strong generalizability across diverse tasks (e.g., mathematical problem-solving, code generation), model sizes, and aggregation strategies (e.g., majority voting, best-of-N). It consistently outperforms baseline methods using a fixed temperature, yielding significant performance improvements. Additionally, our approach enhances the interpretability of temperature's role in model performance by analyzing EntP. Moreover, our analysis explores how the optimal temperature is influenced by the divergence or similarity between model training and tasks (Section 3).\nIn summary, TURN provides a novel, efficient, and principled method for optimizing temperature in LLM inference with multi-sample aggregation. It eliminates the need for labeled validation data and significantly improves performance across a wide range of applications."}, {"title": "2. Preliminary & Related Work", "content": "Before moving to our main contributions, we first review how language models typically generate samples and an introduction to multi-sample aggregation strategies.\nLanguage Model Sampling Language models typically generate outputs for generative tasks by autoregressively sampling from the conditional probability distribution over the next token, given both the input context and previously generated tokens. Formally, for an input X and an output sequence Y = (Y_1, Y_2,\\cdots, Y_N), the probability of producing Y is given by:\n$$P(Y | X) = \\prod_{i=1}^N P(Y_i | Y_{<i}, X).$$\nTo compute the probability distribution, the model obtains a set of logits $z_i$ and then divides them by a temperature hyperparameter $\\tau$ before applying the softmax function and a regularization function $F$:\n$$P(Y_i | Y_{<i}, X) = F\\left(\\text{softmax}\\left(\\frac{z_i}{\\tau}\\right)\\right),$$\nwhere $z_i$ is the logit corresponding to token $y_i$. The temperature $\\tau$ controls how peaked or flat the resulting probability distribution will be. The regularization function $F$ is used to reschedule the sampling process (e.g., Top-k (Kool et al., 2019), Top-p (Holtzman et al., 2019), Min-p (Nguyen et al., 2024) and Locally Typical Sampling (Meister et al., 2023)).\nMulti-Sample Aggregation Strategy Since different random seeds can produce varying outcomes, a common approach to mitigate sampling variance is to draw multiple samples and aggregate their results. In practice, it leads to substantial performance improvements and has been widely adopted to achieve state-of-the-art performance in math reasoning (Sun et al., 2024; Jaech et al., 2024), code generation (Wang et al., 2024a), and many other domains.\nSpecifically, a set of candidate outputs $Y = \\{Y_1, ..., Y_N \\}$ is generated and then aggregated into a final answer. Two standard aggregation methods are typically employed:"}, {"title": "3. Correlation Between Model Training and Optimal Temperature", "content": "Multi-sample aggregation strategies\u2014commonly used in problem-solving, code generation, and related domains\u2014leverage information from multiple samples, which helps escape local minima and improve robustness. In these settings, sample diversity becomes crucial: a diverse set of candidate samples increases the likelihood that the correct solution appears in the pool, rather than repeating the same mistake. The temperature parameter is a primary lever for controlling this diversity.\nWe hypothesize that how a model is trained impacts the optimal temperature for multi-sample inference strategies. In particular, a more specialized or fine-tuned model can safely explore higher temperatures without drifting into low-quality outputs. In contrast, a general-purpose model typically benefits from a lower temperature to remain focused on relevant content.\nWe investigate this in two steps: In Section 3.1, we show that the optimal temperature varies for a base, instruction-tuned, and fine-tuned model. Then in Section 3.2, we establish a general relationship between a model's proximity to the target task and its corresponding optimal temperature. Our key insight is that token-level entropy is a proxy of distance from a task, which motivates our entropy-based method for automatic temperature selection in Section 4."}, {"title": "3.1. Optimal Temperature Range Varies", "content": "We first demonstrate that the optimal sampling temperature varies by model type. We test three Mistral-7B variants: the pretrained base model, the instruction-finetuned version (Mistral-7B-Instruct), and a task-finetuned model for MATH\u00b9 (Wang et al., 2024c). Each model is evaluated using multi-sample aggregation across different temperatures. Figure 2(a) presents the accuracy heatmap for the Mistral-7B-Instruct model on the MATH dataset. At smaller sample sizes, lower temperatures tend to produce better accuracy. However, higher temperatures can yield better results as the sample size increases. For a fixed sample size, the accuracy curve follows a single-peak pattern: it rises as temperature increases and peaks, and then gradually declines, staying relatively steady near the peak.\nSince the single-peak behavior, we define the e-optimal temperature range. This range encompasses temperatures T where the accuracy A(T) is no less than A(T*) \u2013 \u0454, with A(T*) representing the peak accuracy. Given the curve's single-peak nature, this range forms an interval around T*. For our analysis, we set \u0454 = 0.02, effectively capturing the temperatures close to the peak where the accuracy remains relatively high.\nWe then plot the midpoint of this optimal temperature range for each model variant and various sample sizes (Figure 2(b)). We observe that the pretrained model has the lowest midpoint, the instruction-finetuned model has a higher midpoint, and the task-finetuned model has the highest. Another observation is that optimal temperature ranges change slowly once beyond a sample size of 32. Therefore, we choose a sample size of 128 in our following experiments to ensure stable performance in the rich-sample setting.\nFrom these observations, we hypothesize a general relationship between how closely a model is tuned to a particular task and the temperature that yields the best accuracy. We discuss this hypothesis further in the next section."}, {"title": "3.2. Correlation Between Training-Task Similarity and Optimal Temperature", "content": "Our goal is to establish a general relationship between a model's learned distribution and its optimal temperature for a task. Our key intuition is that token-level entropy can serve as a surrogate for a model's \u2018distance' from a target task and that this distance helps identify the optimal temperature.\nSpecifically, we define a distance metric that measures how similar a model's training data is to a given task. Let T = {X_1, ..., X_k} be the task with k problem instances. We define this distance D(M, T) as the average of token-level entropy H(.) of the language model M when generating the answers A = {Y_1, ..., Y_k} for the problems in T:\n$$D(M,T) = \\frac{1}{k} \\sum_{i=1}^k \\frac{1}{|Y_i|} \\sum_{j=1}^{|Y_i|} H(p_M(Y_j | X_i, Y_{<j})),$$\nwhere\n$$H(p) = -\\sum_{v \\in V} p(v) \\log p(v).$$\nTo avoid bias toward ground-truth references, we use model-generated sequences {Y} instead of official gold solutions. Meanwhile, the distance is measured at a low temperature $\\tau = 0.5$ to ensure the generation stability.\nWe evaluated several language models on the MATH and MBPP datasets, including pretrained, instruction-finetuned, and task-finetuned models. Figure 3 plots the midpoint of the optimal temperature range against our distance metric, demonstrating a strong negative correlation. Specifically, across our model set, the correlation on MATH is -0.895, while on MBPP it is -0.777.\nIn practice, this suggests using a higher temperature (e.g., T = 0.9 ~ 1.1) for task-finetuned models and a lower temperature (e.g., T = 0.5 ~ 0.7) for more general-purpose models (pretrained or instruction-finetuned)."}, {"title": "4. Entropy-Based Automatic Temperature Selection", "content": "Determining an optimal sampling temperature is crucial in multi-sample aggregation strategies, yet existing approaches often rely on labeled data or tuning on a validation set. This reliance becomes problematic when no such data are available. In this section, we show how to leverage token-level entropy as an intrinsic property to pinpoint a suitable temperature without labeled data. We first demonstrate a spike on token-level entropy as a signal of quality collapse in Section 4.1. Then develop a method that automatically selects temperature using an entropy turning point (EntP) derived from the spike in Section 4.2. Finally, we applied a stochastic process model to explain the mechanism of our algorithm in Section 4.3."}, {"title": "4.1. Entropy Spike as an Indicator of Quality Collapse", "content": "First, we discover a surprising phenomenon that we call the entropy spike. Specifically, increasing the temperature smoothly increases the model's entropy, until a dramatic spike where the entropy rapidly increases. We believe the spike is a good indicator of sample quality collapse.\nAs illustrated in Figure 4(a), we calculate the token-level entropy at different temperature levels (solid blue line). To reduce computational overhead, we compute the entropy only over the top-K tokens (with the highest probabilities) at each step, setting K = 1000 in all subsequent experiments. The entropy curve remains stable for lower temperatures but then shows a sudden rise. One might attribute this behavior to temperature's role in flattening the distribution (Equation 1). However, the following analysis indicates that this spike reflects a substantial change in the model's next-token distribution.\nSpecifically, we constrain the model to generate the same outputs produced by greedy decoding while evaluating entropy under a higher temperature (dotted blue line). If temperature alone were responsible for the entropy spike, these fixed outputs would yield a similarly high entropy. However, as shown in Figure 4(a), we observe a significant gap between these two entropy curves, indicating that the actual sampling distribution undergoes a large shift.\nThus, we infer that the sudden rise in the entropy curve implies a substantial drop in sample quality. Setting the temperature around this sudden rise can balance sufficient diversity without a large quality drop, which is suitable for multi-sample aggregation strategies."}, {"title": "4.2. Turning Point Temperature Selection (TURN)", "content": "Given the token-level entropy curve of a language model on a specific task, how can we identify a suitable temperature for multi-sample aggregation strategies? Inspired by the difference in the shapes of the entropy curve: When the temperature remains low, the entropy increases flatly. However, when the sampling temperature is near the spike, the entropy increases (super)-exponentially, implying a quality drop in samples. Therefore, after taking the logarithm of the entropy curve (shown in Figure 4(a), red line), the flat part becomes concave while the exponentially-increase part becomes convex. We define the entropy turning point (EntP) as the temperature where the log entropy curve becomes convex. Figure 4(b) tests the llemma-7b base model and its task-finetuned variant\u00b2 (Sun et al., 2024), and EntP matches the position with the highest accuracy and varies between different models. Based on EntP, we develop a new method for automatic temperature prediction in multi-sample aggregation strategies, called Turning Point Temperature Selection (TURN).\nThe optimal temperature should be around EntP to achieve both sample quality and diversity. At the same time, we found that some aggregation methods may be more tolerant of quality drops (e.g., for best-of-N, only one sample is enough to be correct). So we added a small adaptation factor \\beta based on the aggregation function, and it is set to 0 and +0.1 for majority voting and best-of-N, respectively. The aggregation adaptation for best-of-N is calculated in the MATH dataset but can be directly applied to other tasks. Refer to Appendix C for details.\nSpecifically, given a language model M, a task T = {X_1,..., X_k} with k input instances, and an aggregation method A. To estimate the token-level entropy, we random sample N times. In each time, we randomly choose an input instance X_i, and generate one sample by M under each candidate temperature t_j = j \\cdot \\tau (with \\tau being the temperature interval and j = 0, 1, ..., J, where J = \\lfloor t_{\\text{max}}/\\tau \\rfloor). These entropies are then aggregated to calculate the average entropy H(t_j) at each temperature t_j. By taking the logarithm, we obtain l(t_j) = \\log H(t_j).\nNext, we identify EntP index j^*, where the second derivative of l changes from negative to positive and select its corresponding temperature j^* \\cdot \\tau. Then we add the aggregation adaptation factor \\beta to form the final prediction. The pseudocode for our algorithm is listed in Algo. 1."}, {"title": "4.3. A Stochastic Process Model", "content": "We applied a stochastic process model to explain why the entropy curve exhibits a sudden spike and what that spike signifies.\nBecause inference is sequential, when the language model makes an error (for example, by sampling an improper token), it increases the likelihood of further mistakes. Meanwhile, the model may occasionally recover and return to a correct trajectory.\nTo simulate this process, we adopt a stochastic process model with K steps in sequential, generating a token in each step. At the start, the model has an initial error rate p = P_{\\text{init}}, representing the probability of selecting an improper token. At each step, if the model selects an improper token, the likelihood of further errors increases to 1 \u2013 (1 \u2212 p)^\\alpha, where \\alpha > 1 is called the noise tolerance rate. Conversely, if the model selects a proper token, the error probability decreases to p^{\\alpha} (but cannot be smaller than p_{\\text{init}}).\nTo build a bridge between the temperature T and the initial error rate P_{\\text{init}}, we propose an estimation. All tokens are labeled proper or improper irrelevant to contexts, and the number of improper tokens (N_1) is much larger than that of proper tokens (N_0). In the beginning, proper tokens have high logits L_0 with a variance N(0,\\sigma_0^2) to reflect the nature that there may be several proper next tokens with similar logits. Improper tokens have uniformly low logits L_1. Then, the initial error rate P_{\\text{init}} is determined as the probability of selecting an improper token based on the logits and temperature. During inference, all improper tokens equally share the error rate p, while proper tokens account for the remaining probability based on their logits.\nUsing this setup, we can estimate the token-level entropy. As shown in Figure 5(a), the simulated entropy curve (blue line) aligns well with the observed entropy curves of a real language model (Figure 4(1) solid blue line). Meanwhile, Figure 5(b) shows the relationship between the temperature and the percentage of improper tokens, which rises quickly after EntP. This observation suggests that, before EntP, increasing the temperature can help explore the proper tokens. However, after EntP, the increase in the percentage of improper tokens makes the model uncertain and creates errors, implying a quick drop in sample quality. The behavior of the stochastic process model is consistent with our observations of language models, proving that token-level entropy is a good indicator of sample quality. Detailed formulas and experiments can be found in Appendix B."}, {"title": "5. Evaluating TURN", "content": "Table 1. The prediction from our algorithm TURN, the optimal temperature ranges from grid search, and the performance drop (PD) for various models tested in the MATH and MBPP datasets. TURN achieved hit rates of 12/13 and 11/13, average temperature gaps of 0.023 and 0.015, and average performance drop of 0.32% and 0.59%.\nWe want to answer the following research questions about our approach TURN for selecting the optimal temperature:\n\u2022  RQ1: How is the accuracy of TURN in automatic temperature prediction?\n\u2022  RQ2: How efficient is TURN regarding the number of samples (the parameter N in Algo. 1)?\nThrough experiments, TURN proves effective across models, aggregation strategies, and tasks while remaining efficient, requiring only a few samples for temperature prediction."}, {"title": "5.1. Experiment Setup", "content": "We evaluate our methods in two scenarios where sampling-based inference is widely used: Math Problem Solving with Majority Voting and Code Generation with Best-of-N. The datasets and models are as follows:\nMath Problem Solving: We assess language models' reasoning abilities using the MATH dataset (Hendrycks et al., 2021), which consists of competition-level math problems. To accommodate multiple models, we randomly select 200 test problems (40 per difficulty level). Accuracy is measured based on majority voting. We test general-purpose models (Llama (Dubey et al., 2024), Mistral (Jiang et al., 2023)), domain-specific models (Llemma (Azerbayev et al., 2023), OpenMath2 (Toshniwal et al., 2024), Deepseek-Math (Shao et al., 2024)), and fine-tuned models (Math-Shepherd (Wang et al., 2024c), Easy-to-Hard (Sun et al., 2024)).\nCode Generation: For code generation, we use the MBPP dataset (Austin et al., 2021), selecting the first 100 programming problems. Accuracy is measured using pass@K, where correctness is determined by passing provided unit tests. We regard the unit tests as the best-of-N strategy with a perfect reward model to rank answers. Besides general-purpose models, we evaluate code-specific models, including Deepseek-Coder (Guo et al., 2024), CodeLlama (Roziere et al., 2023), Qwen2.5-Coder (Hui et al., 2024), and Yi-coder (01.AI, 2024).\nImplement Details: For both tasks, we sample 256 times per question at each temperature level and compute accuracy across different sampling sizes. For temperature prediction in TURN, we use an interval of \\tau = 0.1 and set N = 8 \u00d7 dataset size (an excessive sample size, see Section 5.4 for discussion). Additional inference configurations are detailed in Appendix A."}, {"title": "5.2. Evaluation Metrics", "content": "To assess the performance of our algorithm for automatically selecting the optimal sampling temperature, we define the following key metrics (all the metrics are calculated under a large sample size of 128, refer to Section 3.1 for discussion):\nMetrics: We use the following metrics to evaluate the accuracy and reliability of our temperature prediction algorithm:\n\u2022  Hit Rate (HR): The frequency with which TURN selects a temperature within the e-optimal range\u00b3, indicating practical reliability.\n\u2022  Temperature Gap (TG): The absolute difference between the predicted temperature and the nearest boundary of the e-optimal temperature range.\n\u2022  Performance Drop (PD): The accuracy loss compared to the best temperature found via grid search."}, {"title": "5.3. Baseline", "content": "As no existing method automatically adjusts temperatures in multi-sample aggregation strategies, we compare against a fixed temperature baseline. We search over {0.1, 0.3, 0.5, 0.7, 0.9, 1.1} and select the temperature that maximizes overall accuracy. This mimics a common yet suboptimal practice where developers apply a single temperature across all models, disregarding variations in model behavior and task requirements."}, {"title": "5.4. Results", "content": "We evaluated 13 models on two tasks\u2014MATH (with majority voting) and MBPP (with Best-of-N)\u2014and present the results in Table 1. Recall Figure 1(b), the correlation coefficient between the accuracy of the predicted temperature and the best accuracy from grid search is 0.9998 for MATH (and 0.9913 for MBPP). TURN achieves a Hit Rate of 12/13 on MATH and 11/13 on MBPP, indicating strong performance across most models. The Temperature Gap remains minimal even when the predicted temperature falls outside the e-optimal range (0.023 for MATH and 0.015 for MBPP). Compared to the best temperatures found via grid search, TURN incurs only a small average performance drop (0.32% and 0.59%, respectively). Full per-model results and predicted turning points are provided in Appendix D.\nComparison with Fixed Temperatures: We next compare TURN to a fixed temperature baseline. Specifically, we sample temperatures from 0.1 to 1.1 at intervals of 0.2 and report the Temperature Gap (TG) and Performance Drop (PD) in Table 2. Our method outperforms the best of fixed temperatures by 0.5% on MATH and 0.4% on MBPP in average accuracy. When both tasks are combined, the margin increases to 0.75%, highlighting the benefit of adaptive temperature selection over a uniform fixed temperature.\nNumber of Samples for Temperature Estimation: Finally, we assess the efficiency of TURN by examining the prediction variance under different sample sizes for Llama-3.1-8B-Instruct on MATH. As shown in Table 3, we report the average variance of the entropy curve across all choices of T, the variance of predicted temperature, and the average performance drop. We find out that even with a moderate sample size (e.g. 40 samples), the variance remains low and the performance drop is tiny (0.2%), suggesting that a small sampling budget is sufficient for accurate temperature estimation and thus proves the efficiency of our algorithm."}, {"title": "6. Conclusion", "content": "In this paper, we investigated the critical role of temperature in multi-sample aggregation strategies. We observed that the optimal temperature varies significantly across models due to differences in training strategies and data distributions. By analyzing the relationship between training-testing distribution similarity and the optimal temperature range, we identify a strong correlation that provides valuable insights into model behavior. Furthermore, we proposed the first method for automatically predicting optimal temperatures across diverse tasks, achieving this without labeled data. Our findings contribute to a deeper understanding of temperature's impact on language model performance and offer a practical approach for optimizing inference settings."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Inference Configuration", "content": "A.1. Software\nOur experiments build upon two open-source projects: Easy-to-Hard Generalization (Sun et al., 2024) for the MATH dataset and bigcode-evaluation-harness (Ben Allal et al., 2022) for the MBPP dataset. We employ vLLM (Kwon et al., 2023) to accelerate inference. All experiments can be reproduced on a single L40S or A6000 GPU."}, {"title": "A.2. Sampling Parameters", "content": "We use zero-shot inference for models fine-tuned specifically for each dataset. For general-purpose models, we use four in-context examples (few-shot inference) to ensure correct output formatting. The maximum output length is set to 1024 tokens for all tasks. For the MATH dataset, we use top-k sampling with k = 20. No additional sampling constraints are imposed for the MBPP dataset."}, {"title": "A.3. Metric Calculation", "content": "To compute majority-vote results for the MATH dataset, we consider two samples to have the same answer if they match after normalization. For the pass @K metric, we follow the definition in Chen et al. (2021). Let N be the total number of samples and C be the number of correct samples. Then pass@K is defined as:\n$$\\text{pass@K} = 1 - \\frac{\\binom{N-C}{K}}{\\binom{N}{K}}$$"}, {"title": "B. Details of the Stochastic Process Model", "content": "We introduce a stochastic process model to explain that (1) the token-level entropy increases steadily at the beginning but rises rapidly when the sampling temperature reaches a certain threshold. (2) The optimal temperature is near the turning point when using multi-sample aggregation strategies.\nThe stochastic process model has two underlying assumptions: (1) Every token can be labeled as 'proper' or 'improper' at each decoding step. Generally, proper tokens have relatively higher logits than improper tokens, while the number of improper tokens is much higher than that of proper tokens. (2) When an improper token is generated, improper tokens have a higher generation probability in the next step, and vice versa.\nUnder these two assumptions, we can calculate the token-level entropy under different sampling temperatures, and the temperature-entropy curve fits that of real language models. Meanwhile, the percentage of improper tokens quickly increases after the turning point, implying a quick drop in sample quality in real language models."}, {"title": "B.1. Model Setup", "content": "B.1.1. INITIAL CONDITIONS\nWe consider a discrete-time process {x_t}_{t=0} where each x_t \\in [0,1] represents the model's probability of producing an improper token at time step t. We start with an initial error rate:\nx_0 = x_{\\text{init}} \\in [0, 1].\nConceptually, x_{\\text{init}} corresponds to the model's baseline 'error propensity' at the start. This value is related to the sampling temperature T of the language model: higher T typically yields a flatter probability distribution over tokens, increasing the chance of selecting an improper token and thus increasing x_{\\text{init}}. (See Section B.1.4 for a heuristic link between temperature and initial error rate.)"}, {"title": "B.1.2. INTERPRETING THE ERROR RATE", "content": "At each step t, the language model chooses a single token, and each token is classified as proper or improper. Although in practice, the correctness of a token depends on the context and is not truly binary, we approximate this by treating correctness as a Bernoulli trial:"}, {"title": "B.1.3. ERROR RATE UPDATE RULES", "content": "After each step, the error rate x_{t+1} is updated based on whether the token at time t was improper or proper:\nIf an error occurs (E_t = 1): The error rate is increased. Intuitively, making a mistake can make the model more likely to continue making errors. Formally, we update:\nx_{t+1} = 1 - (1 - x_t)^\\alpha.\nFor x_t \\in [0, 1] and \\alpha > 1 (\\alpha is a hyperparameter), it can be seen that x_{t+1} \\ge x_t. Here \\alpha can be regarded as the noise tolerance rate, measuring how stable the model is when suffering from unexpected noises, and we will try different \\alpha in experiments.\nIf a proper token is produced (E_t = 0): The error rate is reduced, reflecting a 'reinforcement' of correct behavior. We do this by:\nx_{t+1} = \\frac{x_t - \\text{max}(x_t^\\alpha, x_{\\text{init}})}{1 - \\text{max}(x_t^\\alpha, x_{\\text{init}})}.\nIt generally makes it smaller, so this step lowers the error rate. However, we do not allow the error rate to drop below the initial baseline x_{\\text{init}}."}, {"title": "B.1.4. LINKING INITIAL ERROR RATE AND TEMPERATURE", "content": "At step t, the model's token probability mass is divided into:\n\u2022  Improper tokens with total probability P_{1,\\text{improper}} = x_t.\n\u2022  Proper tokens with total probability P_{0,\\text{proper}} = 1 - x_t.\nTherefore, by definition:\nx_{\\text{init}} = P_{1, \\text{improper}}.\nUnder higher temperatures T, the softmax distribution flattens, increasing P_{1,\\text{improper}} because the number of improper tokens is large but their logits are low. Thus, x_{\\text{init}} increases as T increases."}, {"title": "\u0392.1.5. \u03a4\u03a5PE OF TOKENS DURING DECODING", "content": "The probability of tokens when decoding is usually multi-peak (i.e., except for the token with the highest logit, some other tokens have reasonably high logits and are also acceptable during decoding), so it is natural to consider a scenario with three categories of tokens:\n\u2022  Proper tokens: A small number of tokens with high logits. Let N_0 be the number of proper tokens. To capture the multi-peak behavior, the logits of proper tokens l_{0,1}, ..., l_{0, N_0} are sampled from the Gaussian distribution N(L_0, \\sigma_0).\n\u2022  Low Probability Improper tokens: Many low-logit tokens where language models seldom choose them. Let N_1 be the number of tokens in this type and their logits are set to L_1 for simplicity.\n\u2022  High Probability Improper tokens: Due to insufficient training or errors in training data, some tokens may have exceptionally high logits but are logically improper in (e.g., the token 3 in 1 + 1 = 3). Since different language models behave differently and this type of token will be selected regardless of temperature, we only consider decoding without high-probability improper tokens in our discussion."}, {"title": "\u0392.1.6. \u03a4\u039fKEN PROBABILITY DURING SAMPLING", "content": "At each step t, the probability of producing improper tokens is x_t. Let p_{t,\\text{proper/improper},j} be the probability of the j-th proper/improper tokens at step t. For the improper tokens, we have:\np_{t,\\text{improper},j} = \\frac{x_t}{N_1}, \\forall j \\in [1, N_1].\nFor the proper tokens, we allocate the remaining probability 1 - x_t according to their relative logits:\np_{t,\\text{proper},j} = (1 - x_t) \\text{softmax}(l_{0,1}, ..., l_{0, N_0})_j, \\forall j \\in [1, N_0].\nThis ensures that the relative ordering of probabilities for the proper tokens remains determined by their logits, while the total mass allocated to improper tokens is x_t."}, {"title": "B.1."}]}