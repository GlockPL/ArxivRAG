{"title": "Online inductive learning from answer sets for efficient reinforcement learning exploration", "authors": ["Celeste Veronese", "Daniele Meli", "Alessandro Farinelli"], "abstract": "This paper presents a novel approach combining inductive logic programming with reinforcement learning to improve training performance and explainability. We exploit inductive learning of answer set programs from noisy examples to learn a set of logical rules representing an explainable approximation of the agent's policy at each batch of experience. We then perform answer set reasoning on the learned rules to guide the exploration of the learning agent at the next batch, without requiring inefficient reward shaping and preserving optimality with soft bias. The entire procedure is conducted during the online execution of the reinforcement learning algorithm. We preliminarily validate the efficacy of our approach by integrating it into the Q-learning algorithm for the Pac-Man scenario in two maps of increasing complexity. Our methodology produces a significant boost in the discounted return achieved by the agent, even in the first batches of training. Moreover, inductive learning does not compromise the computational time required by Q-learning and learned rules quickly converge to an explanation of the agent's policy.", "sections": [{"title": "1 Introduction", "content": "The intersection of symbolic, logic-based reasoning and Reinforcement Learning (RL) represents a trending topic in Artificial Intelligence (AI) research, aiming to harness the strengths of both domains and address their individual limitations [1]. Current state-of-the-art approaches in model-free RL, in fact, predominantly rely on extensive data or pre-defined environmental models, facing significant challenges in terms of efficiency and scalability. Furthermore, the decision process underlying policy generation is not transparent to other agents and humans, hindering certifiability and trustworthiness. On the other hand, symbolic AI works well in the small data regime but does not perform well on non-symbolic data and is not noise tolerant [29]. Incorporating symbolic and logical formalisms into RL systems, as highlighted by [10], can significantly boost their interpretability, thereby fostering wider acceptance and diffusion while also playing a crucial role in improving the policy and the training phase. However, symbolic learning"}, {"title": "2 Related Work", "content": "Recent works demonstrate that incorporating existing knowledge into RL and Markov Decision Processes (MDPs) can greatly enhance the development of effective policies [1]. The integration of such knowledge through logical formalisms has significantly improved policy computation. For instance, the REBA framework by [27] employs ASP to define spatial relations in a domestic setting, guiding a robotic agent to select specific rooms for inspection while addressing more straightforward MDP tasks locally. Similarly, DARLING [14] uses ASP to restrict MDP exploration in simulated grid environments and real-world robotics applications. Furthermore, [4] and [13] utilize linear temporal logic to direct exploration in MDPs. Logical constraints also play a crucial role in avoiding undesirable behaviours, particularly in safety-sensitive scenarios [18,17]. However, since logical heuristics usually express policy-related knowledge (unknown to the agent), they need to be defined by expert users. In the alternative, a huge amount of high-quality training examples are needed to learn them, such as in [3], in which temporal logic specifications are learnt as finite automata from good example traces and used to shape the reward signal. For this reason, attempts have been made to combine neural and symbolic learning. For instance, in [16] Neural Markov Logic Networks are used to learn relational representations from structured examples. However, the approach is computationally inefficient out of very simple domains, involving few variables and predicates. In [8] a deep relational reinforcement learning approach to learn generalizable policies is proposed. Nonetheless, the agent only adopts the logical policy at training convergence, thus not fully exploiting the synergy with neural methods, which are inherently more noise-robust and can help refine the agent's performance. Furthermore, even the solution presented in [8] suffers from poor scalability, requiring task-specific constraints on the available search space for logical policies.\nIn this paper, we combine deep RL with an ILP framework under the ASP semantics, which offers great expressiveness for structured task representation and reasoning in a fragment of first-order logic [22]. ILP under the ASP semantics has been proven successful in numerous scenarios, e.g., in enhancing the explainability of black-box models [30,25] and gaining task knowledge [26,20]. Among different available implementations [9,2], we adopt the popular FastLAS system [11], which can scale to very large search spaces. In this way, we can efficiently learn ASP policy approximations online while training the RL agent, solving the computational limitations posed by [16,8].\nOur work is close to the one in [5], where ASP rules are learned to define reward machines in RL settings. However, our solution is more scalable thanks to the FastLAS approach. Furthermore, we do not use rules to shape the reward signal of the RL agent, which may be inefficient in effectively boosting the performance of RL training since still many interactions with the environment may be required to learn the value of action [1]. Instead, we take inspiration from [21] and perform ASP reasoning over the learned policy heuristics to bias the exploration of an RL agent with a higher probability towards heuristic-suggested actions."}, {"title": "3 Background", "content": "We now introduce some basic notions about the approximate Q-Learning algorithm, ASP and ILP, and describe the benchmark domain we used to test our methodology."}, {"title": "3.1 Approximate Q-Learning", "content": "Q-learning is a model-free reinforcement learning algorithm used to find an optimal action-selection policy [28]. The goal of the agent is to learn the optimal policy that maximizes the expected cumulative reward over time. The core concept in Q-learning is the action-value function, $Q(s, a)$, which estimates the expected future rewards for taking action $a$ in state $s$. The Q-learning update rule is given by:\n$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') \u2013 Q(s_t, a_t))$\nwhere $s_t$ is the current state, $a_t$ is the action taken in that state, $r_{t+1}$ is the reward received after taking action $a_t$, and $s_{t+1}$ is the next state. The parameter $\\alpha \\in [0, 1]$ is the learning rate, which controls how much new information overrides old information, and $\\gamma\\in [0,1]$ is the discount factor, which determines the importance of future rewards. The algorithm converges to the optimal action-value function $Q^* (s, a)$ under certain conditions, such as a decaying learning rate and sufficient exploration of all state-action pairs. The optimal policy $\\pi^*(s)$ is then derived by choosing the action that maximizes the Q-value in each state:\n$\\pi^*(s) = arg \\max_a Q^*(s, a)$.\nApproximate Q-learning addresses scalability issues inherent in traditional Q-learning, particularly in environments with large or continuous state spaces [28]. The key idea is to approximate the Q-function using a combination of features instead of the full state. That is, instead of recording everything in detail, we think about what is most important to know, and model that."}, {"title": "3.2 Answer Set Programming", "content": "Answer Set Programming (ASP) defines a domain as a set of logical statements (axioms) articulating the logical relationships between entities represented as predicates and variables (atoms) [6]. Axioms considered in this work are normal rules $h :- b_1,..., b_n$, which define the body of the rule (i.e. the logical conjunction of literals $\\bigwedge_{i=1} b_i$) as a precondition for the head $h$. We say that a variable is grounded when assigned a particular value. Consequently, an atom is grounded when all its variables are grounded. Given an ASP program P, its Herbrand base $H(P)$ defines the set of ground atoms which can be generated from it. From an ASP domain definition, an ASP solver computes the answer sets, i.e., the minimal set of literals that satisfies the given logic program according to the stable model semantics. This work assumes that body atoms represent environmental features describing S in the MDP, while head atoms are actions. Answer sets will then contain feasible actions for the RL agent."}, {"title": "3.3 Inductive Logic Programming", "content": "A generic ILP task under a logical formalism F [23] is defined as a tuple T = (B, SM, E), consisting of background knowledge B expressed in a logic formalism F, a search space SM containing the set of possible axioms to be learned (defined, e.g., via a mode declaration M [24]), and a set of examples E, all expressed in the syntax of F. The goal is to find a hypothesis (i.e. an axiom) $H \\subseteq SM$ covering E. Under the ASP semantics, we consider the generic case where examples are weighted context dependent partial interpretations (WCDPI's) [12]. A partial interpretation $epi$ is a pair of sets of ground atoms ($e_{inc}$, $e_{exc}$). An interpretation (i.e., a set of ground atoms) I extends $e$ iff $e_{inc} \\subseteq I$ and $e_{exc} \\cap I = \\emptyset$. A WCDPI is then a tuple $e = (e_{id}, e_{pen}, e_{pi}, e_{ctx})$, where $e_{id}$ is an identifier for $e$, $e_{pen}$ is either a positive integer or $\\infty$, called a penalty, $e_{pi}$ is a partial interpretation, and $e_{ctx}$ is an ASP program called the context. A WCDPI $e$ is accepted by a program P iff there is an answer set of $P \\cup e_{ctx}$ that extends $e_{pi}$. Following the definition by [12], the goal of ILP is then to find a hypothesis $H \\subseteq SM$ with minimal length (i.e., number of atoms) and $\\sum e_{pen}$, for all examples $e$ not accepted by H $\\cup$ B. Since we are interested in discovering normal rules matching actions to environmental features, $e_{inc}, e_{exc}$ represent executed and not executed actions, respectively, while $e_{ctx}$ is the set of ground environmental features. We employ FastLAS learner by [11] for fast computation from acquired examples gathered from RL batches of experience."}, {"title": "3.4 The Pac-Man domain", "content": "In the Pac-Man domain, an agent (Pac-Man) needs to navigate a maze-like environment to collect food pellets (each one gives a +10 reward) while avoiding enemies (ghosts). The G ghosts present in the environment generally move randomly but may start chasing Pac-Man (with probability $p_g$) if they are close (we conducted our experiments with $p_g = 0.8$). Given the $N \u00d7 M$ grid, the agent can move in the four cardinal directions or stay still (hence |A| = 5), receiving a time penalty of -1 for each time step spent in the maze. The episode ends with a +500 reward if Pac-Man eats all food pellets or a -500 penalty if one of the ghosts chases Pac-Man. The environment also contains P power capsules (big yellow dots in Figure 1) that Pac-Man can eat to gain the ability to scare and eat ghosts. Catching a scared ghost results in a +200 reward. The state S contains the position of each wall, food pellet, power pill, ghost in the map, and the Pac-Man position; each is expressed as (x,y) coordinates. The environment is fully deterministic. The challenge Pac-Man presents derives from the vast dimensions of the state space and the extended planning horizon: to complete the level, the agent may have to explore the entire environment to find all the pellets, and non-trivial movements are often required to escape ghosts."}, {"title": "4 Methodology", "content": "This section describes our methodology for integrating approximate Q-learning, symbolic learning, and reasoning in ASP. We exemplify it in the context of the standard RL Pac-Man domain used for empirical evaluation."}, {"title": "4.1 ASP representation of the domain", "content": "We start from the representation of the domain in ASP syntax. This requires defining environmental features F and actions A. For the Pac-Man domain, F contains: wall(Dir), which denotes the presence of a wall in front of the agent (i.e., one cell away) in that direction, food(Dir, Dist), ghost(Dir, Dist), and capsule (Dir, Dist), representing Manhattan distance $Dist \\in [0,10]$ between PacMan and a cell containing food (or ghost, or capsule) in direction Dir \u2208 {north, south, east, west}. Finally, we need to introduce upper and lower bounds on Dist to obtain more informative policy heuristics. To this aim, we define atoms in the form X_dist_Y(Dir, Dist, D), where X is either ghost, food or caps, and Y is either geq or leq, defined as follows:\nX_dist_geq(Dir, Dist, D) :- X(Dir, Dist), Dist >= D, d_const(D).\nX_dist_leq(Dir, Dist, D) :- X(Dir, Dist), Dist <= D, d_const(D).\nwhere d_const(0.4) limits the set of possible values that D can take in the rule. Action atoms are constructed from A as move(Dir), denoting the movement of the agent in direction Dir. The agent also has the option to perform a 'stop' action. However, we chose not to include it in the learning phase as it was rarely performed in the examples collected during our tests. Once F and A are defined, we need a feature map $F_F : S \\rightarrow H(F)$ and an action map $F_A : A \\rightarrow H(A)$ to ground atoms from collected batches of RL. The only information needed to build F is the positions of the agent, food, ghosts and capsules in the environment, all of which are available in S."}, {"title": "4.2 Definition of the learning task", "content": "Given the ASP formalization of the task, we need to generate the ILP task T = (B, SM, E) for FastLAS, starting from RL episodes. Given an episode consisting of a sequence of N state-action pairs $(\\bar{a}_i, \\bar{s}_i)$, i = 1, . . . N, we can build a WCDPI of the following form:\n$e_i = (id_i, W_i, (\\{\\bar{a}_i\\}, a \\in F_A \\backslash F_A(\\bar{a}_i)), F_F(\\bar{s}_i))$,\nwhere $id_i$ is a unique identifier and $w_i$ represents the penalty of the WCDPI, set as the reward obtained by the agent in that episode and assigning $w_i = 0$ to those episodes that obtained a negative reward. This gives more relevance to the agent's behaviour, resulting in a higher reward for generating rules that can lead to at least the same performance. We populate $e_{inc}$ with the agent's chosen action and $e_{exc}$ with the grounding for all unobserved actions. For example, considering the Pac-Man scenario depicted in Figure 1, if at time t the agent moves left within an episode with return 50, we generate the following WCDPI:\n$e_t =(idt, 50, (move(east),{move(west),move(north),move(south)}),\n{wall(north), wall(south),food(east, 1),food(west, 1),...}), (1)$\nWe have omitted ground context atoms with a distance higher than 1 for simplicity. The background knowledge of the task only contains the definition of the ASP variables and ranges, and the mode declaration is defined in order only to have rule's heads in the form move(Dir), and bodies containing atoms from F."}, {"title": "4.3 Neurosymbolic Q-learning", "content": "Algorithm 1 shows our Neurosymbolic Q-Learning methodology. At the end of each batch of experience acquired by Approximate Q-Learning, we store at most \u03c3 highest-return episodes (Line 22). We then follow the procedure described in Section 4.2 to generate WCDPIs and learn policy heuristics H (Line 23). These heuristics, together with the background knowledge B defined in Section 4.2 and the grounding of state variables $F_F(s)$, are used to perform ASP reasoning and compute a set of suggested actions $A_h$ at each step of the Q-learner (Line 7). Specifically, in the exploration phase of the agent (with probability \u20ac), it may choose either an action from $A_h$ or A \\ $A_h$ (i.e., actions which H does not suggest), with probability $p$ and $1-p$ respectively (Line 10). In this way, we preserve the asymptotic optimality of the Q-learning algorithm [28], following a soft bias approach as in existing literature [21]. We empirically choose $p$ as the average discounted return over the best-saved episodes $E_r$, normalized by the average return of the whole training. In order not to start with $A_h = \\emptyset$ until the first batch of episodes is gathered, we use the very first episode of training"}, {"title": "5 Empirical Evaluation", "content": "Experimental results on the methodology outlined in the previous section are now presented. We tested our approach on two different scenarios of the Pac-Man domain: a smaller map, with 18 \u00d7 9 grid dimensions, P = 2 power capsules and G = 2 ghosts, and a way more challenging map, with 25\u00d726 grid dimensions, G = 4 ghosts and P = 4 power capsules. All experiments were performed on a computer equipped with 5.1GHz, a 13th Gen Intel i5 processor and 64GB RAM.\nWe evaluate 2 different performance measures:\ntraining performance: it measures the RL discounted return and computational efficiency;\npolicy heuristic convergence: it measures the explainability and robustness of the symbolic component of our methodology, evaluating how the set of policy heuristics reaches a fixed point as RL training progresses.\nFor all metrics, we compare our methodology (NeuroQ) against classical Approximate Q-Learning (ApproxQ). We report results as mean and standard deviation over a set of 5 random seeds for statistical relevance. For each seed, we choose a maximum number of episodes E = 20000 and a batch size $S_t$ = 100 (resulting in 200 training batches). Q-Learning parameters for both evaluated algorithms are set as follows: learning rate \u03b1 = 0.2, \u03b3 = 0.8, \u20ac = 0.05. We empirically choose to keep track of \u03c3 = 5 best episodes on the smaller map, while on the bigger map (in which longer episodes are generated), we set \u03c3 = 3. In this way, we balance RL performance (discounted return) and the computational cost of symbolic learning."}, {"title": "5.1 Training performance", "content": "Figure 2 shows the return obtained by the execution of ApproxQ and NeuroQ in the small map (2a) and in the more challenging map (2b). We note that, in both scenarios, the introduction of policy heuristics significantly increases the return obtained by the agent, leading to a more efficient training process. In addition to assessing the trade-off between performance and computation, we investigated the computational impact of generating policy heuristics and calculating the suggested actions. The execution times, shown in Tables 1-2 for the small and large maps, respectively, demonstrate that the additional time required by FastLAS learning and ASP reasoning (Lines 7 and 23 in Algorithm 1) is acceptable, given the substantial increase of performance. In particular, in the large map, NeuroQ requires \u2248 25% more time per batch, but the average discounted return is double (Figure 2b)."}, {"title": "5.2 Policy heuristics convergence", "content": "At each batch iteration of Algorithm 1, the ILP task solved by FastLAS consists of \u2248 650 WCDPIs (on average) for both the small and large maps. The search space SM remains constant during the procedure and consists of 226 unique rules for both environments. As the first training episode is acquired (Line 21 of Algorithm 1), we learn a first set of policy heuristics to bias the agent's exploration towards more convenient actions immediately. On the 18 \u00d7 9 map, for example, the following heuristic is generated:\nmove(Dir) :- food_dist_leq(Dir, Dist,1).\nIn other words, the agent immediately learns to move in the direction where the food is close. In the 25 \u00d7 26 map, FastLAS learns a similar heuristic at the first iteration, with slight adjustments depending on the specific seed (e.g., not wall(Dir) is included in the body, too)."}, {"title": "6 Conclusion and Future Work", "content": "The research presented in this paper demonstrates how to effectively integrate neurosymbolic learning and reasoning to improve the efficiency and explainability of RL agents. We leverage the expressive ASP semantics to represent structured task knowledge in a fragment of first-order logic and reason over the most convenient actions in the RL exploration phase. ASP policy heuristics are learned and refined online via a scalable ILP algorithm, gathering examples at each batch of RL training. Moreover, a probabilistic soft bias approach in ASP guidance preserves the convergence guarantees of RL. We validated our algorithm in the Pac-Man benchmark RL scenario in two maps of increasing complexity involving long planning horizons, large state space and contrastive agents (ghosts). Our neurosymbolic RL algorithm extends Q-Learning, but it could be applied to any RL algorithm involving a random exploration phase. Our methodology achieves a significantly higher discounted return than traditional RL (almost double in the largest map), at the cost of \u2248 25% increase in the computational cost. In addition, learned ASP rules converge within \u2248< 70/200 RL batches, providing an interpretable logical explanation of the black-box RL policy and training process. As a future work, we plan to test the scalability of these techniques to even more complex and varied environments, also investigating the integration with other RL algorithms and more complex forms of neurosymbolic integration [1]."}]}