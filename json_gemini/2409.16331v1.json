{"title": "Exploring the traditional NMT model and Large Language Model for chat translation", "authors": ["Jinlong Yang", "Hengchao Shang", "Daimeng Wei", "Jiaxin Guo", "Zongyao Li", "Zhanglin Wu", "Zhiqiang Rao", "Shaojun Li", "Yuhao Xie", "Yuanchang Luo", "Jiawei Zheng", "Bin Wei", "Hao Yang"], "abstract": "This paper describes the submissions of Huawei Translation Services Center(HW-TSC) to WMT24 chat translation shared task on English Germany (en-de) bidirection. The experiments involved fine-tuning models using chat data and exploring various strategies, including Minimum Bayesian Risk (MBR) decoding and self-training. The results show significant performance improvements in certain directions, with the MBR self-training method achieving the best results. The Large Language Model also discusses the challenges and potential avenues for further research in the field of chat translation.", "sections": [{"title": "1 Introduction", "content": "Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Wu et al., 2023) has made substantial progress in recent years, largely due to the adoption of the transformer (Vaswani et al., 2017) architecture. NMT has demonstrated promising translation results across various scenarios. However, research in the field of chat translation remains limited, primarily due to the scarcity of chat data. In prior chat-related tasks, we utilized data from related domains, such as spoken dialogue and subtitles, to augment our translation models, but the outcomes were only mediocre.\nLike the preceding two chat shared tasks, the WMT24 chat shard task concentrates on translating conversations between consumers and servers in different languages. We participated in the en-de bidirectional translation task. The en-de bidirectional models we submitted to the WMT22 chat task (Yang et al., 2022) function as our baseline models, leveraging the deep transformer (Dou et al., 2018) architecture. Building on this foundation, we employed the Minimum Bayesian Risk (MBR) strategy to select the optimal translation outcomes, and iterative self-training yielded the best results on the development set.\nBeyond traditional NMT models, the emergence of large language model(LLM) has introduced a new paradigm to translation tasks(Wang et al.; Moslem et al., 2023; Guo et al., 2024). Due to its extensive context length and powerful language modeling capabilities, large language models significantly outperform NMT in the translation of lengthy texts and the fluency of translation results. We input the translation output from the NMT model into the LLM as a prompt, allowing the LLM to combine the reference translatio from traditional NMT model to produce an improved translation. However, the comet metric of the LLM's output did not surpass the optimal results of the NMT model.\nRecognizing that chat translation is a context-aware task, we conducted a series of context-aware experiments(Wu et al., 2024) using LLMs with WMT and IWSLT document data. We fine-tuned the LLM by constructing streamed translations and contextualized translation data, and translated the development set in the same format. Unfortunately, the results were unsatisfactory.\nThe structure of this paper is as follows: Section 2 describes our data volume and format for fine-tuning the LLM. The model structure and key methods utilized are presented in Section 3. Section 4 outlines our experiment setting. Results and analysis are presented in Section 5, and we conclude our work in Section 6."}, {"title": "2 Data", "content": null}, {"title": "2.1 Data Size", "content": "All experiments conducted for this task are based on the model developed by our team, as participated in the WMT22 chat shared task. For details on the training data and strategies used for this model, please refer to the system report Yang et al. (2022); Wei et al. (2021)."}, {"title": "2.2 Data pre-processing", "content": "Since the domain-specific data listed in Table 1 is limited, no special treatment was applied to this portion of the data; it was simply tokenized and input into the NMT model. For the document data in Table 2, we constructed the two formats shown in Table 3 by considering the characteristics of chat tasks, and used them to fine-tune the LLM, separately validating the impact of only preceding information and both preceding and context information on chat translation quality.\nIn the format of streamlined translation, during each translation session, only preceding information is visible. The LLM generates results based on this preceding information and the previews translation output, resulting in a translation that leans more towards the style of the reference.\nIn the context-aware translation format, during each translation session, preceding and following N sentences are provided along with the output of the NMT model, guiding the LLM to combine context information to produce a more natural translation."}, {"title": "3 System Overview", "content": null}, {"title": "3.1 Model", "content": "The baseline models for WMT24 chat task use the Transformer-Big architecture. Deep transformer is an improvement of Transformer, which increases the number of encoder layers and uses pre-layer-normalization to further improve model performance. Therefore, in this task, we adopt the following model architecture:\n\u2022 Deep 25-6 large Model: This model features 25-layer encoder, 6-layer decoder, 1024 dimensions of word vector, 4096 domensions of FFN, 16-head self-attention, and pre-layer-normalization.\nFor experiments related to large language model, we choose llama2-8b as the base."}, {"title": "3.2 MBR Decoding", "content": "Minimum Bayesian Risk (MBR) decoding was initially introduced during the era of statistical machine translation(Kumar and Byrne, 2004; Jinnai et al., 2024). This strategy calculates the output with the minimum expected error among multiple candidates, rather than simply selecting the result with the highest probability during the decoding process. In our experimental approach, we utilize the outputs of 10 distinct models as candidates. These candidates are then used to score each other's comet, and the candidate with the highest average comet is chosen as the final output. Algorithm 1 show the detail."}, {"title": "3.3 Regularized Dropout", "content": "Regularized Dropout (R-Drop) (Liang et al., 2021) presents a simple yet more effective approach to regulate the training inconsistency caused by dropout (Srivastava et al., 2014). Specifically, during each mini-batch training, each data sample is processed twice through the forward pass, with each pass utilizing a distinct sub-model and randomly dropping out some hidden units. R-Drop minimizes the bidirectional Kullback-Leibler (KL) divergence (van Erven and Harremos, 2014) between the two distributions outputted by the two sub-models for the same data sample, thereby regulating the outputs of two sub-models randomly sampled from dropout for each data sample in training. This method effectively alleviates the inconsistency between the training and inference stages."}, {"title": "3.4 Self-Training", "content": "Self-Training(ST) (Imamura and Sumita, 2018), also known as forward translation (FT) (Wu et al., 2019), typically involves utilizing a forward NMT"}, {"title": "3.5 Back Translation", "content": "Back-translation (Edunov et al., 2018; Wei et al., 2023) is acknowledged as a highly effective data augmentation strategy to boost NMT model performance. Unlike forward translation, back-translation converts target-side monolinguals into source-side text, thereby producing synthetic parallel corpora. Numerous back-translation techniques have been explored, with sampling (Gra\u00e7a et al., 2019), noise (Edunov et al., 2018), and tagged back-translation (Caswell et al.) demonstrating superior results. In our experimental setup, we opted for sampling back-translation."}, {"title": "3.6 Model Averaging", "content": "Model averaging (Dormann et al., 2018) is a widely utilized technique to enhance translation quality. Typically, models (in our experiment, 5 models) that exhibit the highest performance on the development set are chosen for parameter averaging, which leads to substantial improvements."}, {"title": "3.7 LLM Few-shot Prompting", "content": "Although large language models exhibit impressive zero-shot capabilities, they still struggle with more complex tasks in the zero-shot setting. To address this, few-shot prompting can be employed as a technique for in-context learning, where demonstrations are provided in the prompt to guide the model towards enhanced performance. In our approach, we provide 5 reference translations to assist the large language model in producing superior results."}, {"title": "3.8 LLM SFT with LoRA", "content": "LLM SFT (Supervised Fine-Tuning) is a technique for fine-tuning large language models using specific datasets, which effectively enhances the performance of large language models on tasks such as text generation, machine translation, or sentiment analysis. LoRA (Low-Rank Adaptation)(Hu et al., 2022) is a technique that reduces the computational burden during large language model training by decreasing the number of model parameters through matrix decomposition. This technique maintains performance while lowering computational and memory requirements. By applying LoRA, large language models can perform better under limited computational resources, reducing training costs and resource consumption."}, {"title": "4 Experiment Setting", "content": "During the NMT model training phase, we use Pytorch-based Fairseq (Ott et al., 2019) open-source framework as our benchmark system. Each model is trained using 8 GPUs with a batch size of 2048. The update frequency is 4 and the learning rate is 5e-4. The label smoothing rate is set to 0.1, the warm-up steps to 4000, and the dropout to 0.3. Adam optimizer (Kingma and Ba, 2015) with B1=0.9 and B2=0.98 is also used. Beyond that, we have configured the hyper parameter reg-alpha of the R-Drop technique to a value of 5. In the evaluation phase, We employ the official automatic evaluation scripts and primarily base our model and result selection on the comet metric(Rei et al., 2022).\nIn the experiments related to large models, we utilize the open-source model llama2_8b_instruct from Meta and the training scripts from HF to train our models, setting the max_seq_length to 1024."}, {"title": "5 Result and Analysis", "content": "Table 4 displays the results of the official test set, ranked according to the comet-22 score, where our system achieved the top position in comet-22, chrF, and BLEU metrics.\nThe primary results we submit are obtained by translating the source text of the test set with multiple NMT models, selecting the optimal output using MBR strategy, then training on the best models from the validation set using self-training method. The models are averaged over 5 epochs before being used to translate the test set to yield the final results."}, {"title": "5.1 Sentence-level NMT", "content": "In the previous chat tasks, we have tried various strategies to optimize the model, and the results from the validation set indicate that the baseline model from 2022 was already sufficiently powerful. On this basis, we combined this year's training set, the 2022 validation and test sets, and conducted BT and ST reinforcement strategies, only in the direction of translation from English to German has there been a noticeable improvement. The results shown in Table 5.\nTo further improve the results, we attempted the MBR decoding strategy, generating 10 alternative outputs for the validation set using different NMT models in previous steps. These outputs were scored using comet, and the output with the lowest Bayesian risk was selected as the final result. The results in Table 5 indicate that improvement was only seen in the en\u2192de direction. Further, we utilized the MBR results to perform another ST on each direction, ultimately achieving the best results in both directions in the validation set. The reason for the improvement we observed is that the MBR algorithm can integrate the capabilities of multiple models. When performing self training, it essentially utilizes the optimal results of multiple models for a round of knowledge distillation."}, {"title": "5.2 Document-level MT with LLM", "content": "According to the test results shown in Table 6, on the chat task valid set, the results of LLM (Large Language Model) are significantly worse than sentence-level under both comet or doc-comet metrics. The few-shot capabilities of LLM is indeed far better than zero-shot, but it still falls short of sentence-level results. After using the document-level data for LLM SFT, the results became even worse. We analyzed that the reason is the large domain shift, as the IWSLT and WMT datasets we used are far from the domain of the chat task.\nTo validate the capability of LLM in translating document-level content, we tested the results on the iwslt2017 en-de document-level test set. The results in the right half of Table 6 demonstrate that LLM's few-shot capability surpassed that of the chat task's sentence-level model on this test set. Further, by fine-tuning the large model with document-level data, we obtained better results.\nComparing the results of stream translation and context-aware translation, we originally expected context-aware format data to yield better results because the model could refer to contextual information during translation. However, we analyzed that stream translation sees the previous step's translation result each time, which is more consistent with the translation style of large model. On the contrary, context-aware requires input of the reference MT result from sentence-level model in one go, which is less consistent with the style of large model, causing the model to fail to effectively utilize these information."}, {"title": "6 Conclusion", "content": "This paper presents the submissions of HW-TSC to the WMT 2024 Chat Translation Shared Task. For both direction in en de translation task, we perform experiments with a series of training strategies. The results show that MBR self-training achieves the best results. In the future, we will continue to explore the applicability of MBR strategy mentioned in this paper.\nBeyond that, due to time constraints, further fine-tuning of large language models using chat task data was not conducted to assess its performance. Additionally, there is room for continued exploration of the translation capabilities of large language models."}]}