{"title": "Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn In-context by Multi-step Gradient Descent", "authors": ["Bo Chen", "Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "In-context learning has been recognized as a key factor in the success of Large Language Models (LLMs). It refers to the model's ability to learn patterns on the fly from provided in-context examples in the prompt during inference. Previous studies have demonstrated that the Transformer architecture used in LLMs can implement a single-step gradient descent update by processing in-context examples in a single forward pass. Recent work has further shown that, during in-context learning, a looped Transformer can implement multi-step gradient descent updates in forward passes. However, their theoretical results require an exponential number of in-context examples, $n = exp(\u03a9(\u03a4))$, where T is the number of loops or passes, to achieve a reasonably low error. In this paper, we study linear looped Transformers in-context learning on linear vector generation tasks. We show that linear looped Transformers can implement multi-step gradient descent efficiently for in-context learning. Our results demonstrate that as long as the input data has a constant condition number, e.g., $n = O(d)$, the linear looped Transformers can achieve a small error by multi-step gradient descent during in-context learning. Furthermore, our preliminary experiments validate our theoretical analysis. Our findings reveal that the Transformer architecture possesses a stronger in-context learning capability than previously understood, offering new insights into the mechanisms behind LLMs and potentially guiding the better design of efficient inference algorithms for LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have gained immense success and have been widely used in our daily lives, e.g., GPT4 [Ope23], Claude 3.5 [Ant24], and so on, based on its Transformer architecture [VSP+17]. One core emergent ability of LLMs is In-Context Learning (ICL) [BMR+20]. During ICL, the user provides an input sequence (prompts) containing some question-answer pairs as in-context examples and the goal query that the user cares about, where the examples and query are drawn from an unknown task. The LLMs can in-context learn from these examples and generate the correct answer for the goal query without any parameter update. Notably, these unknown tasks may never seen by LLMs during their pre-training and post-training, e.g., synthetic task [WHL+23a]. Thus, many believe that the in-context learning mechanism is different from supervised learning or unsupervised learning, where the latter may focus on feature learning, while the ICL may perform some algorithm to learn. For instance, the Transformer can implement algorithm selection [BCW+24] or gradient descent [DLD+22, VONR+23] by in-context learning forward pass.\nMany works have tried to understand how Transformers perform single-step gradient descent [ZFB23, ACDS24, MHM23, CCS23, BCW+24, HCL23, LWL+23, GHM+24, WZC+24]. They study one-layer linear Transformer in-context learning on linear regression tasks and show that the one-layer linear Transformer can perform single gradient updates based on the in-context examples during a forward pass. Recent work [GSR+24] has further shown that a linear looped Transformer can implement multi-step gradient descent updates with multiple forward passes, meaning that Transformers can express multi-step algorithms during in-context learning. However, their theoretical results require an exponential number of in-context examples, exp(\u03a9(\u03a4)), where T is the number of loops or passes, to achieve a reasonably low error for linear regression tasks. This violates the intuition that more gradient descent updates lead to better performance.\nThus, it is natural to ask the following question:\nIs it necessary to use an exponential number of examples for Transformers to implement multi-step gradient descent during in-context learning?\nIn this work, we study linear looped Transformers (Definition 3.6) in-context learning on linear vector generation tasks (Definition 3.4), which is as hard as linear regression. We show that the linear looped Transformer can efficiently perform multi-step gradient descent as long as in-context examples are well-conditioned. We present our main result in the following theorem."}, {"title": "2 RELATED WORK", "content": "This section briefly reviews the related research work on Large Language Models (LLM), In-Context Learning (ICL), and looped transformers. These topics have a close connection to our work.\nLarge Language Models. Neural networks based on the Transformer architecture [VSP+17] have swiftly become the dominant paradigm in machine learning for natural language process-ing applications. Expansive Transformer models, trained on diverse and extensive datasets and comprising billions of parameters, are called large language models (LLM) or foundation mod-els [BHA+21]. Examples include BERT [DCLT19], PaLM [CND+22], Llama [TLI+23], Chat-GPT [Ope24], GPT4 [Ope23], among others. These LLMs have demonstrated remarkable general intelligence capabilities [BCE+23] across various downstream tasks.\nResearchers have developed numerous adaptation techniques to optimize LLM performance for specific applications. These include methods such as adapters [HysW+22, ZHZ+23, GHZ+23, SCL+23], calibration approaches [ZWF+21, ZLX+23], multitask fine-tuning strategies [GFC21a, XSW+23, VONR+23, XSW+24], prompt tuning techniques [GFC21b, LARC21], scratchpad ap-proaches [NAA+21], instruction tuning methodologies [LL21, CHL+22, MKBH22], symbol tun-ing [WHL+23b], black-box tuning [SSQ+22], reinforcement learning from the human feedback (RLHF) [OWJ+22], chain-of-thought reasoning [WWS+22, KSL+22, YYZ+23, ZMC+24] and vari-ous other strategies. And here are more related works aiming at enhancing model efficiency without compromising performance, such as [CLS+24, LSS+24b, LLS+24c, HYW+23, WHL+24, HLSL24, XHH+24, WHHL24, LSSY24, HCL+24, HWL24, HCW+24, LLSS24b, SYZ23, SMN+24].\nIn-context Learning. A significant capability that has emerged from Large Language Models (LLMs) is in-context learning (ICL) [BMR+20, WTB+22, SWXL24]. This feature allows LLMs"}, {"title": "3 PRELIMINARY", "content": "In this section, we present some preliminary concepts and definitions of our paper. In Section 3.1, we introduce some basic notations used in our paper. In Section 3.2, we defined some important variables to set up our problem."}, {"title": "3.1 Notations", "content": "For any integer n, we define $[n] = {1, 2, ..., n}$. For two vectors $x \u2208 R^n$ and $y \u2208 R^n$, we use $\\langle x, y \\rangle$ to denote the inner product between $x, y$, i.e., $\\langle x, y \\rangle = \\sum_{i=1}^n X_i y_i$. For each $a, b \u2208 R^n$, we use $a ob \u2208 R^n$ to denote the Hardamard product, i.e. the $i$-th entry of $(aob)$ is $a_i b_i$ for all $i \u2208 [n]$. We use $1_n$ to denote a length-$n$ vector where all the entries are ones. We use $||x||_p$ to denote the $l_p$ norm of a vector $x\u2208 R^n$, i.e. $||x||_1 := \\sum_{x=1}^n |x_i|$, $||x||_2 := (\\sum_{i=1}^n x_i^2)^{1/2}$, and $||x||_\u221e := \\max_{i\u2208[n]} |x_i|$. For $n > d$, for any matrix $A \u2208 R^{n\u00d7d}$, we use $||A||$ to denote the spectral norm of $A$, i.e. $||A|| := \\sup_{x\u2208R^d} || Ax ||_2 / ||x||_2$.\nFor a matrix A, we use $\u03bb_{min}(A)$ and $\u03bb_{max}(A)$ to denote the minimum and maximum eigenvalue of A, respectively."}, {"title": "3.2 In-context Learning", "content": "First, we introduce some definitions of in-context examples and their labels.\nDefinition 3.1 (In-context prompt data). Let $X \u2208 R^{n\u00d7d}$ be the input data with n tokens and each token with feature dimension d, i.e, $X = [x_1, x_2,...,x_n]^T$, where $x_i \u2208 R^d$ for any $i \u2208 [n]$."}, {"title": "3.3 Linear Looped Transformer", "content": "In line with recent work by [GSR+24] and [ACDS24], we consider a linear self-attention model, formally defined as follows:\nDefinition 3.5 (Linear attention). Let $Q, P \u2208 R^{d\u00d7d}$ be the query-key matrix and the value-output matrix. Let $Z \u2208 R^{n\u00d7d}$ be input data. The linear attention is defined as\n$Attn(Z; Q, P) := (M\u00b0 (ZQZ^T ))ZP$,\nwhere $M = \\begin{bmatrix} 0_{n\u00d7n} & 0_{n\u00d71} \\\\ 1_{1\u00d7n} & 0 \\end{bmatrix}$ is a casual attention mask for text generation.\nIn Definition 3.5, we combine the query matrix and key matrix as Q and combine the value matrix and output matrix as P for simplicity following previous works [ZFB23, GSR+24].\nBuilding upon this, we introduce the concept of a Linear Looped Transformer [GSR+24]:\nDefinition 3.6 (Linear looped transformer). Let T be the loop number. Let $\u03b7(t) > 0$ for any $t \u2208 {0, 1, . . ., T \u2013 1}$. The linear looped transformer TF(Z(0); Q, P) is defined as\n$Z^{(t)} := Z^{(t-1)} - \u03b7^{(t-1)} Attn(Z^{(t-1)}; Q, P), \u2200t \u2208 [T]$\n$TF(Z^{(0)}; Q, P) := -\u03b7 Z^{(T)}_{n+1,1:d}^T$\nThe Looped Transformer is to simulate a real multiple-layer Transformer with residual connec-tions [HZRS16], where \u03b7 represents the weights of the residual components.\nRemark 3.7. Our settings are more practical than [GSR+24] in the following sense."}, {"title": "3.4 Linear Regression with Gradient Descent", "content": "In this section, we introduce some key concept of linear regression with gradient descent.\nDefinition 3.8 (Linear regression). Given $X \u2208 R^{n\u00d7d}, y \u2208 R^n$, and $\u03b8 \u2208 R^d$, the linear regression loss function is defined as\n$l(\u03b8) := 0.5||y \u2013 X\u03b8||_2^2$.\nThen, the gradient formulation is\n$\u2207_\u03b8l(\u03b8) = X^T X\u03b8 \u2013 X^T y \u2208 R^d.  (1)$\nFor any $t \u2208 N^+$, let $\u03b7(t) > 0$ and by Gradient Descent, we have\n$\u03b8^{(t)} := \u03b8^{(t-1)} \u2013 \u03b7^{(t-1)} (X^T X\u03b8^{(t\u22121)} \u2013 X^T y). (2)$\nWe define the optimizer below.\nDefinition 3.9. Let $\u03b8^* = (XX)^{-1} X^T y$ be the optimizer of $l(\u03b8)$.\nIn this work, we consider $n > d$, assuming $X^T X$ is inevitable. Then, we have $\u03b8^* = \u03b8$.\nLemma 3.10 (Forklore). In the realizable setting (no noise term), if $n > d$, then $\u03b8^* = \u03b8$.\nFinally, we define the condition number, which will be used in our final convergence bound.\nDefinition 3.11. We define the condition number of input data as $\u03ba := \\frac{\u03bb_{max}(X^TX)}{\u03bb_{min}(X^TX)}.$"}, {"title": "4 GRADIENT COMPUTATION IN LOOPED TRANSFORMER", "content": "In this section, we present a comprehensive analysis of the gradient computation process within the Looped Transformer architecture. Our investigation begins with an examination of computations in individual layers and subsequently extends to the full looped structure. This approach allows us to build a nuanced understanding of the Looped Transformer's behavior, starting from its fundamental components.\nWe commence our analysis by establishing a crucial result regarding the output of a single layer in our Looped Transformer model. This foundational lemma serves as a cornerstone for our subsequent derivations and provides valuable insights into the model's inner workings.\nLemma 4.1 (Single layer output). Let $Z^{(0)}$ be defined in Definition 3.4. Let $Q = I_{d+1,d+1}$. Let $P = \\begin{bmatrix} I_{d\u00d7d} & 0_{d\u00d71} \\\\ 0_{1\u00d7d} & 0 \\end{bmatrix}$. Let causal attention mask be $M = \\begin{bmatrix} 0_{n\u00d7n} & 0_{n\u00d71} \\\\ 1_{1\u00d7n} & 0 \\end{bmatrix}$. Then, we have\n$Attn(Z^{(0)}; Q, P) = \\begin{bmatrix} (q^{(0)})^T X^TX + \u03b1y^T X \\\\ 0_{n\u00d7d} \\end{bmatrix} \\begin{bmatrix} 0_{d\u00d71} \\\\ \u03b1 \\end{bmatrix} $"}, {"title": "5 ERROR CONVERGENCE", "content": "In this section, we explore the convergence properties of looped transformers, focusing on their behavior under conditions of strong convexity and smoothness. We begin by defining these key concepts and then proceed to establish their implications."}, {"title": "5.1 Convexity and Smoothness Analysis", "content": "We first introduce some crucial definitions.\nDefinition 5.1 (Strong convexity). Let $f : R^d \u2192 RU{+\u221e}$ and $\u03bc > 0$. We say that f is p-strongly convex if, for every x, y \u2208 Rd, and every t \u2208 (0,1) we have that\n$\\frac{\u03b3(1 \u2013 \u03b3)}{2} \u03bc ||x \u2212 y||^2 + f(\u03b3x + (1 \u2212 \u03b3)y) \\leq \u03b3f(x) + (1 \u2212 \u03b3)f(y)$.\nWe say that u is the strong convexity constant of f.\nDefinition 5.2 (L-smooth). Let $f : R^d \u2192 R$ and $L > 0$. We say that f is L-smooth if it is differentiable and if \u2207 f : Rd \u2192 Rd is L-Lipschitz for all x, y \u2208 Rd\n$||\u2207f(x) \u2013 \u2207 f(y)||_2 \u2264 L||x \u2212 y||_2$.\nTo quantitatively analyze the parameter dynamics in our linear vector generation task, we first derive the Lipschitz and convexity constants for the model introduced in Definition 3.8.\nLemma 5.3. Given $X \u2208 R^{n\u00d7d}, y \u2208 R^n$, and $\u03b8\u2208 R^d$ in Definition 3.8, we have\n$L = ||X^T X||$\nwhere L is the Lipschitz constant defined in Definition 5.2."}, {"title": "5.2 Main Result", "content": "We first commence with a statement of Lemma 5.6, which furnishes a convergence rate for gradient descent on strongly convex and smooth functions.\nLemma 5.6 (Theorem 3.6 in [GG23]). Let l : Rd \u2192 R be a differentiable function and assume l is p-strongly convex and L-smooth. Let {0(t) }ten be the sequence generated by the gradient descent algorithm, with a stepsize \u03b7 \u2208 (0,1]. Then for 0* = arg mine l(0) and for all t \u2208 N, we have\n$||0^{(t)} \u2013 0^*||_2^2 \u2264 (1 \u2013 \u03b7\u03bc)^t||0^{(0)} \u2013 0^*||^2_3$.\nFact 5.7 (Folklore). For any n,T \u2208 N+, we have\n$(1-\\frac{1}{n})^T \u2264 e^{-T/n}$.\nWe now present a rigorous upper bound on the error magnitude of the gradient descent al-gorithm's output after T iterations, elucidating the convergence properties of this optimization method in the context of linear vector generation.\nTheorem 5.8. If the following holds:\n\u2022 Let T be the loop number.\n\u2022 Let $X \u2208 R^{n\u00d7d}, y \u2208 R^n$, and $\u03b8 \u2208 R^d$ be defined in Definition 3.8.\n\u2022 Let the condition number $\u03ba = \\frac{A_{max}(X^TX)}{A_{min}(X^TX)}.$  \n\u2022 The step size n = 1.\n\u2022 Let \u00b5 and L be defined in Definition 5.1 and Definition 5.2.\n\u2022 For l(0) defined in Definition 3.8, we have l(0) is L-smooth and \u00b5 strong convex, where L = $||X^T X||$ and \u00b5 = $A_{min}(X^TX)$.\n\u2022 The initial point (0(0) satisfies $||0^{(0)} \u2013 0^*||_2 \u2264 R$.\nThen, we have\n$||0^* \u2013 0^{(T)} ||_3 < \u0435^{-T/\u03ba}R^2$.\nProof. First, we have\n$||0^{(t)} \u2212 0^* ||_3 \\leq (1 \u2013 \u03b7\u03bc)^t ||0^{(0)} \u2212 0^* ||_2$\n$\\leq (1-\\frac{\u03bc}{L})^t ||0^{(0)} - 0^*||_2$"}, {"content": "where the first step follows from Lemma 5.6, the second step follows from we choose n(t) = 1. Then consider the term $\\frac{\u03bc}{L}$, we have\n$\\frac{\u03bc}{L} = \\frac{A_{min}(X^TX)}{||X^T X||} = \\frac{1}{\u03ba}$\nwhere the first step follows from Lemma 5.3 and Lemma 5.5, the second step follows the definition of condition number \u043a= $\\frac{A_{max}(X^TX)}{A_{min}(X^TX)}$."}, {"title": "6 EXPERIMENTS", "content": "In this section, we aim to verify our theory by evaluating the convergence behavior of gradient descent for linear vector generation. We designed our experiment to examine the impact of varying sample sizes on convergence rates while keeping the feature dimension fixed. Our results demon-strate that empirical convergence rates consistently outperform theoretical upper bounds across all sample sizes, with significant improvement in convergence speed as the condition number decreases, validating our theoretical predictions.\nExperiment Setup. In this experiment, we aimed to investigate the convergence behavior of multi-step gradient descent for Linear Looped Transformer (Definition 3.6) in-context learning the linear vector generation task (Definition 3.3). We randomly draw each entry of $X \u2208 R^{n\u00d7d}$ from standard Gaussian distribution, N(0, 1), and response variables y = X0*, where 0* \u2208 Rd was randomly chosen. Our experiments focused on scenarios with d fixed at 4 and n varying in {16, 32, 64, 128}. For each (n,d) combination, we implemented gradient descent with T = 200 iterations and learning rate \u03b7 = 1/L, where L = ||XX||. To ensure statistical robustness, we conducted 10 independent trials for each configuration. Convergence was measured by tracking $||0^{(t)} - 0^*||_2$, where 0* is the optimal least squares solution. To facilitate comparison across different problem sizes, we normalized error plots by the initial error, plotting $log(||0^{(t)} \u2013 0^*||^3/||0^{(0)} \u2013 0^*||_2)$ against t. This normalization enabled a clear comparison of relative decay rates, irrespective of initial error magnitudes, thus providing insights into the impact of condition number on gradient descent convergence in the linear vector generation task.\nResult Interpretation. Our experiment investigates the convergence behavior of gradient de-scent for linear vector generation with varying sample sizes n and a fixed feature dimension d = 4."}, {"title": "7 CONCLUSION", "content": "In this work, we have demonstrated that linear looped Transformers can efficiently implement multi-step gradient descent for in-context learning, requiring only a reasonable number of examples when input data is well-conditioned. This finding relieves the previous assumptions of an expo-nential number of in-context examples and offers new insights into the capabilities of Transformer architectures. Our theoretical analysis and preliminary experiments pave the way for more efficient inference algorithms in large language models and open avenues for future research in this domain."}]}