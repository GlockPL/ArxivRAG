{"title": "SATVISION-TOA: A GEOSPATIAL FOUNDATION MODEL FOR\nCOARSE-RESOLUTION ALL-SKY REMOTE SENSING IMAGERY", "authors": ["Caleb S. Spradlin", "Jordan A. Caraballo-Vega", "Jian Li", "Mark L. Carroll", "Jie Gong", "Paul M. Montesano"], "abstract": "Foundation models have the potential to transform the landscape of remote sensing (RS) data analysis\nby enabling large computer vision models to be pre-trained on vast amounts of remote sensing data.\nThese models can then be fine-tuned with small amounts of labeled training and applied to a variety\nof mapping and monitoring applications. Most existing foundation models are designed for high\nspatial resolution, cloud-free satellite imagery or photos, limiting their applicability in scenarios\nthat require frequent temporal monitoring or broad spectral profiles. As a result, foundation models\ntrained solely on cloud-free images have limited utility for applications that involve atmospheric\nvariables (e.g., cloud or aerosol) or require atmospheric corrections. We introduce SatVision-TOA, a\nnovel foundation model pre-trained on 14-band MODIS L1B Top-Of-Atmosphere (TOA) radiance\nimageries, addressing the need for models pre-trained to handle moderate- and coarse-resolution all-\nsky remote sensing data. The SatVision-TOA model is pre-trained using a Masked-Image-Modeling\n(MIM) framework and the SwinV2 architecture, and learns detailed contextual representations through\nself-supervised learning without the need for labels. It is a 3 billion parameter model that is trained on\n100 million images. To our knowledge this is the largest foundation model trained solely on satellite\nremote sensing imagery. Initial results indicate that SatVision-TOA achieves superior performance\nover baseline methods when applied to downstream tasks such as 3D cloud retrieval. Notably, the\nmodel achieves a mean intersection over union (mIOU) of 0.4638, a substantial improvement over\nthe baseline mIOU of 0.218. Additionally, the rate of false negative results in the fine-tuning task\nwere reduced by over 50% compared to the baseline. This improvement in mIOU and sensitivity\ndemonstrates the model's enhanced capability to delineate segmented regions effectively for this\nspecific task. Our work advances pre-trained vision modeling for multispectral remote sensing by\nlearning from a variety of atmospheric and aerosol conditions to improve cloud and land surface\nmonitoring.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 The potential of foundation models in remote sensing", "content": "Foundation models have emerged as powerful tools in remote sensing data analysis, providing the ability to pre-train\nlarge computer vision models on vast datasets for use in diverse downstream tasks. However, most of these models have\nfocused on high spatial resolution satellite imagery, optimizing for tasks like fine-grained object detection, segmentation,"}, {"title": "1.2 Leveraging frequent multispectral spaceborne observations", "content": "Currently, a suite of spaceborne multispectral sensors offer frequent (daily) coverage along a broad spectral range at a\nmoderate resolution. MODIS is a passive spectroradiometer on board the Terra and Aqua satellites. Flying at different\nequator crossing times, Terra-MODIS (morning orbit) and Aqua-MODIS (afternoon orbit) together revisit any place on\nEarth every day, acquiring data in 36 spectral bands ranging from visible to thermal-infrared (TIR) wavelengths with\n250 m spatial resolution for Band 1 and 2 at nadir to 1 km spatial resolution for thermal bands. These observations help\nin monitoring environmental changes, atmospheric phenomena and other diverse applications. The MODIS successors\ninclude the Visible Infrared Radiometer Suite (VIIRS) onboard NOAA's polar-orbiting JPSS satellite series, and SEVIRI\nonboard European's polar-orbiting Meteosat series. In addition, there are spectrometers on geostationary platforms such\nas the Advanced Baseline Imager (ABI) onboard NOAA's GOES geostationary satellite series, and the Atmospheric\nHumidity Imager (AHI) onboard Japan's Himawari geostationary platforms. Because of the data rate consideration\n(images every 10 \u2013 20 minutes), ABI and AHI have only 16 channels, 14 of which can find similar wavelengths with\n14 MODIS bands as described in Table 1, but they can provide a much higher temporal sampling rate which enables\nweather monitoring. ABI data have been extensively cross compared with MODIS for similar bands to demonstrate\nconsistency [4, 5]."}, {"title": "1.3 Vision transformers can improve with more frequent observations", "content": "Machine learning has been used in many forms to analyze satellite remote sensing data for decades. Until recently,\nconvolutional neural networks (CNN) [6, 7, 8] and fully convolutional networks (FCN) [9, 10] have been the de-facto\nmodel of choice for many computer vision tasks over the past decade [11, 12]. Recently, following the remarkable\nsuccess achieved using Transformer models in natural language processing (NLP) [13], the development of Vision\nTransformers (ViTs) was adapted to computer vision tasks such as image classification, semantic segmentation and\nobject detection. Dosovitskiy et al. [14] applied a standard Transformer directly to images by splitting the image\ninto a sequence of 16\u00d716 subsets referred to as patches, not focusing on pixels, then input to the Transformer the\nsequence of embeddings for those patches. The image patches were treated as tokens in NLP applications. These\nmodels led to state-of-the-art results on the ImageNet dataset. The ViT model architecture is highly scalable and benefits\nfrom large amounts of data, effectively utilizing extensive datasets while maintaining a low propensity for over-fitting.\nThis scalability allows ViTs to improve performance as more data becomes available. Vision transformers can take\nadvantage of frequent multispectral observations. When trained on a large volume of data (millions of images), ViT\nshows superior accuracy, compared with state-of-the-art CNNs [14]. The main driving force behind the ViT is the\nmulti-head self-attention mechanism. It helps ViT to derive long-range contextual dependencies between pixels in\nimages [15]."}, {"title": "1.4 Remote sensing foundation ViT models capture spatial patterns", "content": "Foundation models (FMs) are typically large-scale pre-trained models which can be used as a starting point to fine-tune\nthe model for a downstream task. These foundation models are trained on large amounts of remote sensing data, such\nas satellite imagery, using techniques such as self-supervised learning (SSL). Through the capture of important spatial\nand spectral information in data, FMs can be fine-tuned for specific tasks, including land cover classification and cloud\ndetection. The core idea is that with extensive pre-training, the foundation model learns the underlying patterns and\ncharacteristics of the input data, even in the absence of explicit labels or a defined mapping task. This approach enables\ndownstream users to fine-tune the pre-trained model with a limited number of labeled examples, hence reducing the\ncomputational cost required to address specific scientific questions. Users can apply their own labels to fine-tune the\nfoundation model for their specific tasks, and, in theory, achieve greater computational efficiency since the model has\nalready learned the intrinsic relationships in the input data; and only needs to adapt to the task-specific label assignment.\nRemote sensing FMs have been fueled by the ability to leverage large amounts of unlabeled data through SSL techniques.\nThese techniques impart learning through robust grouped representations based on similarities within the data without"}, {"title": "2 Objectives", "content": "In this paper we introduce SatVision-TOA, a foundation model pre-trained on 14-band MODIS TOA imagery [17].\nSatVision-TOA leverages the spectral diversity of moderate-resolution TOA imagery, using all-sky data that captures\nnatural atmospheric variability, including cloud cover, without relying on cloud-cleared images. This approach expands\nthe scope of foundation models in remote sensing to tasks requiring rich spatial and spectral knowledge. By pre-training\non MODIS TOA data, which includes 14 spectral channels spanning visible, near-infrared, and thermal-infrared\nwavelengths as presented in Table 1, the model develops robust contextual representations needed for atmospheric\nscience and environmental monitoring. The work on the pre-trained SatVision-TOA model has been in the spirit of\nopen-science to enhance the use of machine learning within the expanding remote sensing community."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Developing a remote sensing pre-training dataset with MODIS TOA", "content": "We have developed an extensive pre-training dataset comprised of TERRA-MODIS Top-Of-Atmosphere (TOA) image\nchips to facilitate the pre-training of SatVision-TOA model. Each chip is a 128\u00d7128 pixel image subset of a global daily\ncomposite made from MODIS TOA all-sky swaths. Composites were generated for approximately 270 days per year\nfrom 10 different years of the MODIS record, March 2000 to present. This provides a data record that spans all seasons\nacross multiple decades, which allows us to create a robust sample spatially and temporally to use in model training."}, {"title": "3.1.1 Preparing daily TOA composites from swath data", "content": "MODIS data comes in three spatial resolutions 250 m, 500 m and 1 km where bands 1 and 2 are natively 250 m, bands\n3 \u2013 7 are natively 500 m and bands 8 \u2013 36 are natively 1 km. For this work all bands need to be at the same spatial\nresolution, so the finer resolution bands 1 \u2013 7 have been aggregated to 1 km resolution by the Elliptical Weighted\nAveraging (EWA) method [18, 19]. With MODIS instrument geometry the image distortion and footprint size increase\nat off-nadir views, hence wherever there were overlapping observations we selected the observation with the lowest\nview zenith angle to maximize the fidelity of the observations. Additionally, we only retained observations with solar\nzenith angles lower than 72\u00b0 to avoid pixels with low solar illumination conditions which can lead to erroneous model\npredictions.\nThe MODIS TOA data originates from MODIS MOD02 Level 1B swaths, these contain calibrated and geolocated\nradiances for 36 bands. The first step in preparing the dataset involved compositing the 5-minute MODIS swaths into\ndaily global composites at 1 km spatial resolution. This process aggregates the continuous swath data into a consistent\ndaily grid. Next, the digital numbers (DNs) from the MODIS swaths were converted into TOA reflectance for visible\nand NIR bands.\n$Reflectance = (DN \u2013 Reflectance Offsets) \u00d7 Reflectance Scales \u00d7 100$ \n(1)\nThe calibration of brightness temperature (BT) values for TIR bands is a more complex process. It requires methods\noutlined in the MODIS Level 1B product user guide [20] and is implemented through the SatPy Python package [21].\nThis calibration step converts Digital Number values into physical meaning and units. Specifically, for TIR bands, the\nprocess involves using physical constants and the effective central wave-number (WN) to accurately derive brightness\ntemperature.\n$Radiance = (DN \u2013 Radiance Offsets) \u00d7 Radiance Scales$ (2)\n$BT = \\frac{C_2}{WN \u00d7 In(\\frac{C_1}{Radiance\u00d7WN^5} +1)}$ (3)\n$BT = \\frac{(BT - tci)}{tcs}$ (4)\n$C_1$ and $C_2$ are derived constants based on the Planck constant h, the speed of light c, and the Boltzmann constant k. tcs\nis the temperature correction slope, and tci is the temperature correction intercept.\nFor both the reflectance and the brightness temperature, the values were scaled to a data range of 0-1. This is essential\nto ensure that the values fall within a range suitable for machine learning (ML) models, improving convergence and\nstability during training. For reflectance, values were scaled by a factor of 0.01, effectively transforming the data range\nfrom 0-100 to 0-1. For brightness temperature, values were min-max scaled to fit within a 0 to 1 range. This was done\nby identifying the global minimum and maximum brightness temperature in the dataset and applying the standard\nmin-max scaling formula.\n$Scaled Value = \\frac{Original Value \u2013 Min}{Max - Min}$ (5)\nThis pre-processing step ensured that both the reflectance and temperature features were normalized into a similar range,\nwhich is crucial for stable performance of deep learning models [22]."}, {"title": "3.1.2 Devising a sampling strategy for characterizing land- and cloud-cover diversity", "content": "To ensure a geographically diverse training data set, the image chip selection was stratified across each quadrant of\nglobal composites generated from the MODIS L1B TOA swaths (NW, NE, SW, SE). We sampled from a broad range\nof global cover types, including ocean extents, the full suite of terrestrial land cover, as well as cloud and not-cloud\nsamples. Because only daytime TERRA-MODIS images are used for training this model, it is expected to have some"}, {"title": "3.2 Configuring a model architecture and pre-training", "content": "We utilized the Masked-Image-Modeling (MIM) framework [26] for pre-training SatVision-TOA. MIM, inspired by\nMasked Language Modeling [27] from Natural Language Processing, involves randomly masking patches of the input\nsatellite imagery chip and training the model to predict missing regions. This approach allows the model to learn\ndetailed contextual representations through self-supervision, which is particularly beneficial when labeled data is scarce.\nThe MIM framework was chosen for its ability to capture both local and global context [28] of the imagery and its\nfeatures by reconstructing masked portions of the image chips. This process enhances SatVision-TOA's understanding\nof visual patterns and structures serving as a robust foundation for remote sensing downstream tasks.\nWe implemented this framework using the Swin-Version 2 [29] model architecture, leveraging its hierarchical approach\nto portion the image chips into smaller overlapping windows and its efficient attention mechanisms. During the\npre-training phase, 8\u00d78 patches were randomly masked from the image chips and the model was trained to predict the\noriginal pixel values of the masked regions. This pre-training, conducted on a 100 million set of image chips, enabled\nthe model to learn the complex relationships between the spectral bands in the image chips."}, {"title": "3.3 Assessing model performance", "content": "To validate the model we focused on two key applications: image reconstruction performance, which evaluates the\nmasked-image-modeling task on unseen data, and 3D cloud retrieval, a downstream task targeting the prediction of\nvertical cloud structure.\nFor image reconstruction, the primary performance validation metric used in our experiment is the Structural Similarity\nIndex Measure (SSIM) [30]. Unlike traditional metrics such as Mean Squared Error (MSE), which focuses on pixel-wise\nabsolute error, SSIM evaluates image quality by quantifying perceived changes in structural information. This makes\nSSIM more suitable for measuring performance in tasks where the goal is not only to reconstruct pixel intensity but\nalso to preserve edges and textures.\nFor 3D cloud retrieval, we validate the model's performance using both the mIOU metric and pixel-wise accuracy.\nAccuracy assesses how closely the model's predictions align with the ground truth on a pixel-by-pixel basis, while\nmIOU evaluates the model's ability to delineate segmented regions and accurately capture cloud structures. Relying\nsolely on pixel-wise accuracy can be problematic for imbalanced datasets, such is the case with the 3D cloud retrieval\ntask, where non-cloud pixels outnumber cloud pixels. To address this limitation, mIOU provides a more robust measure\nby calculating the overlap between predicted and ground-truth vertical cloud masks, averaged across the dataset.\nA summary of the findings from the experiments associated with these two applications that use the best SatVision-TOA\nconfiguration is presented in the Results section. Additionally, we examined model scalability to explore the effects of\nincreasing model complexity and pre-training dataset size. The detailed results of model scalability are provided in\nAppendix Tables 4, 5, 6."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Image Reconstruction", "content": "To validate the reconstruction performance we measure how well SatVision-TOA could reconstruct masked inputs, a\ntask consistent with the pre-training objective of masked-image modeling. Specifically, we evaluate how accurately the\npre-trained model predicts masked regions of the input data based on the learned representations. The goal is to evaluate\nhow well the model can reconstruct complicated and heterogeneous features such as different cloud- and land-cover\ntypes."}, {"title": "4.2 3D Cloud Retrieval Downstream Task", "content": "Cloud 3D structures dominate the impact to cloud radiative feedback at the top of the atmosphere [31, 32] and influence\nprecipitation initiation time, intensity and duration at the surface [33, 34]. As such, the international GEWEX cloud\nassessment program recommends that cloud retrieval algorithms should focus on the vertical structure of clouds"}, {"title": "5 Discussion", "content": "SatVision-TOA represents a groundbreaking advancement in the use of deep learning for remote sensing applications.\nTo our knowledge, SatVision-TOA is the largest (in terms of model parameters and number of training images) vision\ntransformer-based foundation model developed exclusively with remotely sensed data. Trained on an unprecedented\n100 million MODIS image chips spanning more than a decade and leveraging over 100 TB of data, SatVision-TOA sets\na new benchmark for large-scale, moderate- to coarse-resolution deep learning models. Its design uniquely captures\nglobal-scale spatial relationships and temporal patterns while maintaining high data efficiency and adaptability across\ndiverse downstream tasks. We evaluated SatVision-TOA by examining its performance in both image reconstruction\nand a downstream task (3D cloud retrieval), establishing the baseline for countless data-driven science applications\npowered by foundation models."}, {"title": "5.1 Reconstruction Insights: Learning Generalizable Representations", "content": "The image reconstruction experiment not only highlights the model's ability to reproduce input data but also underscores\nits capacity to learn complex spatial patterns and transitional regions critical for tasks like land and cloud modeling.\nAchieving an exceptionally high SSIM score of 0.9289 on the test dataset, SatVision-TOA demonstrates its ability to\nreconstruct fine-grained details and maintain structural coherence, even across masked regions. Figure 1 demonstrates\nthree example images at different levels of complexity and their reconstruction and reports the SSIM score for each\nimage. Visual results (Figure 1) reveal how the model effectively preserves the continuity of features spanning\nmultiple patches, offering strong evidence of its ability to generalize across diverse geophysical conditions. The results\ndemonstrate that SatVision-TOA pre-trained on 100 million image chips achieves a remarkably high SSIM score of\n0.9289. These results clearly indicate that the model can generalize and reconstruct complex masked regions with\nstrong fidelity to the original image chip while preserving fine-grained details and structural coherence. More so,\nSatVision-TOA is capable of seamlessly reconstructing regions with complex cloud dynamics which exemplifies the\nstrong capabilities of this model across different types of landscape and atmospheric conditions. This capacity is critical\nto its extensibility in fine-tuning for specialized applications."}, {"title": "5.2 Downstream Performance: Excelling in 3D Cloud Retrieval", "content": "The primary value of foundation models is their ability to be \"fine-tuned\" for specific tasks (aka downstream tasks) with\na minimal amount of labeled training data and compute time). In the downstream 3D cloud retrieval task, SatVision-TOA\ndramatically outperforms a baseline model, such as FCN trained from scratch, with \u00bf50% higher true positive cloud"}, {"title": "5.3 Versatility and Transferability: Bridging Modalities", "content": "SatVision-TOA's adaptability extends beyond MODIS data. By fine-tuning on GOES-ABI data with distinct spatial\nresolutions and spectral profiles, the model maintains high performance with minimal retraining. This transferability\nwas evidenced by fine-tuning SatVision-TOA for a specific science question such as 3D cloud retrieval, achieving\nremarkable performance compared to its baseline FCN counterpart, without requiring extensive task-specific retraining.\nThis adaptability, enabled by its vision transformer architecture, underscores its efficiency in transferring learned\nrepresentations to new domains and data modalities. The efficiency of SatVision-TOA in fine-tuning on small datasets\ndemonstrates the scalability and versatility of foundation models for computer vision science applications, paving\nthe way for rapid deployment in diverse Earth observation tasks, including operational atmospheric monitoring and\nenvironmental research, largely reducing the costs of training data development and compute time."}, {"title": "5.4 Redefining \"All-Sky\" Modeling", "content": "A key innovation of SatVision-TOA lies in its inclusion of \"all-sky\" conditions during pre-training, incorporating\na vast range of cloud conditions often excluded in traditional models. This intentional design allows the model to\ncapture the structural and spectral variability of atmospheric layers, enabling it to tackle cloud-rich tasks critical to\natmospheric sciences. Unlike models optimized solely for cloud-cleared data, SatVision-TOA addresses the challenges\nof natural atmospheric variability, unlocking new potential for applications such as cloud lifecycle studies and global\nclimate monitoring. This approach redefines how deep learning models approach atmospheric phenomena, positioning\nSatVision-TOA as a foundational tool for addressing previously overlooked challenges in atmospheric research. This\ncapability enables scientists at different skill levels and from different backgrounds to easily and in a very short order\ndevelop deep learning data-driven applications to answer some of the most complex questions using remote sensing\ndata."}, {"title": "6 Conclusion", "content": "The full potential of deep learning-driven science applications leveraging moderate- to coarse-resolution data remains\nuntapped. To address this, SatVision-TOA, a large-scale, open-source vision transformer model with 3 billion parameters,\nwas trained on an unprecedented 100-million-image dataset derived from NASA MODIS TOA data, encompassing over\n100 TB of data across a decade. By utilizing MODIS's high temporal frequency and preserving spatial relationships"}, {"title": "A Hardware and Software", "content": "Our pre-training and data generation utilized NASA GSFC's NCCS Discover Supercomputer and the Frontier Super-\ncomputer at the Oak Ridge Leadership Facility. The SwinV2 Huge and SwinV2 Giant models pre-trained with 2 million\nand 26 million image chips utilized the Discover supercomputer. The SwinV2 Giant pre-trained with 100 million image\nchips utilized the Frontier supercomputer. The SCU17 partition of the Discover supercomputer has a 40-core AMD\nRome CPU and four NVIDIA A100-40GB GPU accelerators. Each Frontier node has a 64-core AMD EPYC CPU with\nfour AMD Instinct MI250X GPU accelerators. We used Pytorch [41] and DeepSpeed [42] as the stack for performing\npre-training.\nEach fine-tuning experiment was conducted on 4 NVIDIA A100 GPUs with an input size of 128\u00d7128\u00d714. An effective\nbatch size of 8 was utilized. We employed the AdamW optimizer, couples with a linear warm up learning rate scheduler\nwith a starting rate of 5e \u2013 7, eventually peaking at 3e - 4.\nTotal compute spent pre-training the various model experiments was 68,329 GPU hours, or 7.8 GPU years."}, {"title": "B Pre-Training", "content": "We use the fused AdamW [43] optimizer with $\u03b2_1$ = 0.9, and $\u03b2_2$ = 0.99, with the OneCycle [44] cosine learning\nrate scheduler, with the maximum learning rate variable between experiments due to scaling pre-training between\nthe different dataset sizes. We utilized the SwinV2 Huge (658 Million parameters) and the SwinV2 Giant (3 Billion\nparameters) configurations of the SwinV2 model. For each pre-training experiment, we trained the models for 50\nepochs. The input image size of the MODIS TOA image chips is 128\u00d7128 with a total of 14 bands. The patch size for\nMIM pre-training was 8\u00d78 with a window size of 12.\nWe employed bfloat16 precision [45] during pre-training, which optimized memory usage for both the model and the\ndata. Bfloat16, a compact 16-bit floating-point format, retains the dynamic range of the standard 32-bit float (float32)\nbut with reduced precision. This approach enabled more efficient use of hardware resources, allowing for the training of\nlarger models or processing of larger datasets without compromising the model's ability to represent a wide range of\nvalues. By utilizing bfloat16, we effectively balanced computational efficiency with the need to maintain a sufficiently\nbroad numerical range, ensuring robust model performance during pre-training. The bfloat16 format retains all the\nprecision of the original remote sensing data."}, {"title": "C Model and Dataset Scaling Details", "content": "In pre-training with the SwinV2 Giant and SwinV2 Huge models, we leveraged DeepSpeed's Zero Redundancy\nOptimizer (ZERO) distributed strategy techniques which were instrumental in managing the increased computational\ndemands of the 658 million and 3 billion parameter vision models.\nIn addition to scaling model size, we conducted experiments to scale dataset (batch) size. We observed that increasing\nthe batch size while maintaining model performance was key to reducing the time for pre-training. We identified\noptimal batch sizes that balanced GPU memory usage without decreasing model performance. Scaling the learning rate\nto account for larger batch sizes allowed for continued model performance stability. We utilized the following scaling\nmethod. For the SwinV2 Giant pre-trained with 100 million chips, we found that scaling the learning rate to have a\nmaximum of 0.00159727 allowed us to reach a maximum effective batch size of 16,384, spread across 256 GPUs on\nthe Frontier supercomputer."}, {"title": "D Model Configuration Comparison", "content": "A key focus of our study is the relationship between both model and dataset scale and model performance on downstream\ntasks. We designed different experiments to test the robustness of model performance against input sample size and\nnumber of parameters. To explore this, we pre-trained SwinV2 Giant models on three datasets of varying sizes- 2\nmillion, 26 million and 100 million MODIS TOA image chips (subsets of larger images). This systematic comparison\nallows us to evaluate how increasing the volume of pre-training data impacts the model's ability to generalize to specific\ntasks. By examining the gains and diminishing returns associated with larger datasets, we provide insights into the\noptimal scale of data needed for effective model training. Additionally, this approach aids in identifying the balance\nbetween computational resources used and model performance, which guides future efforts in visual foundation model\ndevelopment and deployment for remote sensing applications."}]}