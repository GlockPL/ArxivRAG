{"title": "Retrieval Augmented Generation Integrated Large Language Models in Smart Contract Vulnerability Detection", "authors": ["Jeffy Yu"], "abstract": "The rapid growth of Decentralized Finance (DeFi) has been accompanied by substantial financial losses due to smart contract vulnerabilities, underscoring the critical need for effective security auditing. With attacks becoming more frequent, the necessity and demand for auditing services has escalated. This especially creates a financial burden for independent developers and small businesses, who often have limited available funding for these services. Our study builds upon existing frameworks by integrating Retrieval-Augmented Generation (RAG) with large language models (LLMs), specifically employing GPT-4-1106 for its 128k token context window. We construct a vector store of 830 known vulnerable contracts, leveraging Pinecone for vector storage, OpenAI's text-embedding-ada-002 for embeddings, and LangChain to construct the RAG-LLM pipeline. Prompts were designed to provide a binary answer for vulnerability detection. We first test 52 smart contracts 40 times each against a provided vulnerability type, verifying the replicability and consistency of the RAG-LLM. Encouraging results were observed, with a 62.7% success rate in guided detection of vulnerabilities. Second, we challenge the model under a \"blind\" audit setup, without the vulnerability type provided in the prompt, wherein 219 contracts undergo 40 tests each. This setup evaluates the general vulnerability detection capabilities without hinted context assistance. Under these conditions, a 60.71% success rate was observed. While the results are promising, we still emphasize the need for human auditing at this time. We provide this study as a proof of concept for a cost-effective smart contract auditing process, moving towards democratic access to security.", "sections": [{"title": "1. Introduction", "content": "The rapid expansion of blockchain technology and decentralized finance (DeFi) has transformed the financial landscape, offering novel opportunities for peer-to-peer transactions and automated financial services. At the heart of this transformation are smart contracts-self-executing contracts that facilitate, verify, and enforce the terms of an agreement [1]. Despite their potential to streamline and secure transactions, smart contracts are not without risks. Vulnerabilities within these contracts can lead to significant financial losses and undermine trust in the DeFi ecosystem [12].\nThe critical need for effective smart contract auditing has never been more apparent. Traditional auditing methods, which rely on manual code reviews by security experts, are thorough but costly and time-intensive [2]. This poses a significant barrier for independent developers and small businesses that may lack the resources to afford such services. Consequently, there is a pressing demand for scalable, cost-effective solutions that can democratize access to smart contract security auditing, ensuring that all participants in the blockchain space can safeguard their assets and operations [15].\nIn response to this need, our study explores the integration of Retrieval-Augmented Generation (RAG) with large language models (LLMs) to enhance the detection of smart contract vulnerabilities. By leveraging the advanced capabilities of models such as GPT-4-1106, combined with a comprehensive vector store of known vulnerabilities, we aim to develop a robust auditing tool that is both accessible and reliable [3]. Our research evaluates the performance of this RAG-LLM system under both guided and blind conditions, providing insights into its potential and identifying areas for further refinement. Through this approach, we seek to contribute to the democratization of smart contract security, fostering a more inclusive and secure DeFi ecosystem."}, {"title": "2. Background", "content": "The explosive growth of decentralized finance (DeFi) has introduced new paradigms in financial transactions, leveraging blockchain technology to enable peer-to-peer lending, trading, and other financial services without traditional intermediaries. Central to these operations are smart contracts\u2014self-executing contracts with the terms of the agreement directly written into code. While smart contracts offer numerous advantages, including automation, transparency, and reduced costs, they also pose significant security risks.\nSmart contract vulnerabilities can lead to catastrophic financial losses, as evidenced by numerous high-profile attacks in the DeFi space. Common vulnerabilities include reentrancy attacks, integer overflows, and unchecked external calls [18]. These vulnerabilities exploit the logic and structure of smart contract code, often resulting in the unauthorized transfer of funds or other malicious outcomes.\nTo mitigate these risks, robust smart contract auditing is essential. Traditional auditing involves manual code reviews by security experts, which, although effective, are time-consuming and expensive [14, 16]. This creates a bottleneck for independent developers and small businesses, who may not have the resources to access high-quality auditing services.\nThe emergence of large language models (LLMs), such as OpenAI's GPT-4, has opened new avenues for automating parts of the auditing process. LLMs have demonstrated remarkable capabilities in understanding and generating human-like text, making them suitable for tasks that involve code analysis and pattern recognition. However, LLMs on their own are limited by their training data and may lack the specificity required for thorough smart contract auditing.\nTo address these limitations, the concept of Retrieval-Augmented Generation (RAG) has been introduced. RAG combines the generative abilities of LLMs with retrieval mechanisms that access a curated database of relevant information. This hybrid approach enables the model to generate more accurate and contextually relevant responses by referencing external sources of knowledge.\nIn the context of smart contract auditing, a RAG-LLM system can retrieve examples of known vulnerabilities from a vector store, enhancing its ability to identify similar issues in new contracts. By integrating this technology, we aim to develop a scalable and cost-effective solution that democratizes access to smart contract security auditing.\nOur study builds upon this framework, employing GPT-4-1106 with a 128k token context window, Pinecone for vector storage, and OpenAI's text-embedding-ada-002 for generating embeddings. We construct a vector store of 830 known vulnerable contracts and design prompts to test the model's ability to detect vulnerabilities under both guided and blind conditions.\nThe experimental results from our study show promising success rates in vulnerability detection, demonstrating the potential of RAG-LLMs to provide reliable and accessible smart contract auditing services. However, challenges remain, particularly in ensuring the unbiased evaluation of model performance and addressing the variability in detection accuracy across different types of vulnerabilities."}, {"title": "3. Democratic Access", "content": "The advent of blockchain technology and decentralized finance (DeFi) has revolutionized the way financial transactions are conducted, offering unprecedented transparency, security, and decentralization. However, the increasing complexity and proliferation of smart contracts have introduced significant security challenges [13]. Vulnerabilities in smart contracts can lead to substantial financial losses, undermining trust in these systems and hindering widespread adoption [17]. This underscores the critical need for accessible and reliable smart contract auditing services.\nTraditional smart contract auditing is typically conducted by specialized firms that offer thorough but expensive services. This creates a significant barrier for independent developers and small businesses that may lack the financial resources to afford these audits. Consequently, the democratization of smart contract security auditing is essential to ensure equitable access to these vital services, fostering a more inclusive and secure DeFi ecosystem.\nDemocratic access to security auditing means making robust and reliable auditing tools available to all participants in the blockchain space, regardless of their size or resources. By leveraging advanced technologies such as Retrieval-Augmented Generation (RAG) integrated with large language models (LLMs), our project aims to provide a scalable, cost-effective solution that democratizes access to smart contract security.\nRAG-LLMs enhance the auditing process by combining the generative capabilities of LLMs with the precision of retrieval mechanisms, allowing for the identification and analysis of vulnerabilities in smart contracts. This integration not only improves the accuracy of vulnerability detection but also makes these tools more accessible to a broader audience. Independent developers and small enterprises can leverage these advanced auditing capabilities without the prohibitive costs associated with traditional auditing services.\nThe motivation behind this project is to empower all stakeholders in the blockchain ecosystem with the tools necessary to ensure the security of their smart contracts. By providing decentralized, trustless auditing capabilities, we aim to foster greater trust in blockchain technologies. This trust is crucial for the widespread adoption and integration of blockchain into various sectors, from finance to supply chain management.\nIn a trustless environment, where transactions and operations are executed without the need for intermediaries, the integrity and security of smart contracts become paramount [19]. Democratic access to auditing tools ensures that all participants can independently verify the security of their smart contracts, promoting a more resilient and trustworthy blockchain infrastructure.\nMoreover, as the DeFi space continues to evolve, the scalability of our approach allows it to keep pace with the growing number of smart contracts and their increasing complexity. By continually enhancing the capabilities of RAG-LLMs and making these tools widely available, we support the sustainable growth of a secure and inclusive DeFi ecosystem."}, {"title": "4. RAG-LLM Pipeline", "content": "The integration of Retrieval-Augmented Generation (RAG) with large language models (LLMs) extends their capacity to generate responses by incorporating external knowledge sources. This paradigm enhances model performance on queries involving information beyond its initial training data. RAG operates by enriching the model's context with relevant content from a vector store, expanding the data reservoir accessible during inference. This dual retrieval mechanism distinguishes RAG-LLMs from traditional LLMs, elevating their ability to synthesize and contextualize responses by concurrently utilizing the model's internal knowledge and external databases.\nOur system capitalizes on integrating RAG with LLMs to apply their combined strengths for the domain of smart contract security. We employ GPT-4-1106 due to its extensive 128k token context window that enables the processing of lengthy prompts combined with information retrieval context [4]. The system is setup with a vector store containing embeddings for 830 known vulnerable smart contracts. We utilize Pinecone for vector indexing and storage due to its efficient scaling and retrieval capabilities, coupled with OpenAI's text-embedding-ada-002 to generate the embeddings [4,5]. The OpenAI text-embedding-ada-002 model is capable of generating embeddings for mixed content, encompassing both natural language and code, which is ideal for our dataset that includes Solidity smart contracts interlaced with natural language comments [4]. LangChain, a framework optimized for constructing RAG-LLM pipelines, facilitates our retrieval-generative processes [6].\nThe system is designed to enhance the quality of responses by retrieving pertinent-vulnerability patterns from the vector store of known vulnerable smart contracts. Upon receiving a user prompt potentially indicative of insecure coding practices, our setup identifies similarities within the vector store. A match suggests parallels between the user's code and pre-identified vulnerabilities, flagging a likely security issue."}, {"title": "4.1 Vulnerable Contract Dataset", "content": "In order to construct an expansive and robust dataset for our RAG-LLM pipeline, we collected a total of 830 vulnerable smart contracts. These contracts were sourced from public Github repositories known to house collections of compromised or flawed smart contract code. The repositories utilized include \"(Not So) Smart Contracts\" by Trail of Bits, \"DeFiHackLabs\" and \"DeFiVulnLabs\" by SunWeb3Sec, and \"Smart Contract VulnDB\" by tintinweb [7,8,9].\nThe dataset comprises a wide spectrum of vulnerability types which are essential to develop a comprehensive understanding of the security pitfalls within smart contracts. Some of the vulnerabilities included are:\n\u2022 Bad randomness\n\u2022 Denial of service\n\u2022 Forced ether reception\n\u2022 Honeypots\n\u2022 Incorrect interface implementation\n\u2022 Integer overflow\n\u2022 Race condition occurrences\n\u2022 Reentrancy attacks\n\u2022 Unchecked external calls\n\u2022 Unprotected functions\n\u2022 Variable shadowing\n\u2022 Misnamed constructors\nThis list is not exhaustive.\nThis variation in the dataset aims to ensure that the system is well-equipped to recognize and provide context for a diverse array of security weaknesses.\nThe selection criteria for contract inclusion was dictated by the requirement to have a diverse set of vulnerabilities rather than by volume. While a greater number of contracts per vulnerability type may have contributed to a more substantial dataset, the prevalence of vulnerable contract datasets is limited, offering a pragmatic cap to the dataset size.\nMoreover, we reserved a significant portion of gathered contracts exclusively for validation purposes to avoid direct data overlap between the RAG vectorstore and subsequent testing. While we had to sacrifice data for vectorstore creation, this measure is taken to ensure the integrity of our experimental evaluation, preventing the model from simply regurgitating exact code matches without genuine understanding."}, {"title": "4.2 Data Processing and Embedding", "content": "We initiated the data processing by loading .sol files-designating them as unstructured text documents. We employed the LangChain DirectoryLoader to retrieve Solidity smart contracts from a specified directory. Following loading, the documents underwent fragmentation into smaller text chunks. To accommodate the maximum context window of our selected model, each chunk was composed of 1024 tokens, a size that was large enough to prevent truncation of functions while reasonably fitting in the context window of the GPT-4 model.\nLangChain's TokenTextSplitter, utilizing the tokenisation method from TikToken, was selected as the tool to divide the contracts into token-sized chunks. TikToken provided two key benefits for processing our dataset:\n\u2022 Reversibility - The tokenization process is designed to be reversible, ensuring that the transformed tokens can be converted back into the original text without any loss of information, which is imperative for the integrity and fidelity of the dataset.\n\u2022 Generalization Capability - TikToken is engineered to generalize beyond its training corpus, enabling it to tokenize unfamiliar text sequences effectively. This characteristic is particularly beneficial for parsing Solidity code, as its syntax may deviate considerably from the data TikToken was originally trained on.\nSubsequently, embeddings were created for the tokenized chunks using the OpenAI API with the \"text-embedding-ada-002\" model. This model was specifically selected due to its proficiency in encoding a blend of natural language and programming code, which makes it particularly suited to the mixed content of smart contracts that incorporates both Solidity code and natural language code annotations.\nFor the vector store, we configured Pinecone with a 'p2' index option, which was chosen for its rapid search capabilities. Upon passing the embeddings to Pinecone, we created a vector database with 939 vectors, each with the embeddings (of 1536 dimensions) and corresponding metadata for each code chunk from the smart contracts."}, {"title": "4.3 RAG with LangChain", "content": "Our system incorporates Retrieval-Augmented Generation (RAG) technology within the LangChain framework to analyze smart contracts for potential vulnerabilities. LangChain is a versatile toolkit that facilitates the creation and deployment of chains a series of operations involving language models and other processing steps.\nOur LLM, specifically ChatOpenAI configured with the GPT-4 model \"gpt-4-1106-preview,\" operates with set parameters such as a temperature of 0.7 for response variability and a maximum token allowance. We chose this model due to the expanded 128,000 token window, which allows fitting the full user smart contract along with all retrieved code from vector search in a single prompt. For our experiment, we set a maximum token response of 5, as our prompts instructed a binary YES or NO response. When using this system for open ended analysis and identification of potential fixes, a larger maximum token response can be set up to 4095 tokens.\nWe engage Pinecone's services to retrieve relevant code from our established index, populated with our embeddings. This index functions as the RAG vector store from which our retriever conducts searches. The searcher is set to return the top 'k' results, in this case.\nWe elected to return the top 5 most relevant documents for a given query. We chose this based on the reasoning that having a larger or unbounded quantity may dilute the prompt due to the sheer amount of data returned. Additionally, we believe the focus on the prompt would be diluted, on both the content for in-context learning and the user prompt, if too much context is provided. Therefore, we limit retrieval to 5 similar documents.\nThe core of our RAG system is powered by a prompt template which scripts the interaction between the user's input and the LLM for auditing smart contracts. This template guides the LLM to focus solely on identifying any vulnerabilities that may exist within the user's question, which comprises the source code of a smart contract. The template disallows the analysis of vulnerabilities detailed in the retrieved contextual information, ensuring that the LLM's response adheres strictly to the information provided in the user's query."}, {"title": "5. Experiment Design", "content": "Our experiment design is aligned with evaluating the efficacy of the RAG-LLM system in identifying smart contract vulnerabilities. The testing methodology bifurcates into two distinct phases: guided detection and blind detection. Guided detection involves an explicit indication of the vulnerability type and a corresponding description provided within the prompt.\nBlind detection, conversely, relies solely on the RAG retrieved context and the smart contract code in question, without explicit prompt instructions regarding the vulnerability type."}, {"title": "5.1 Guided vs. Blind Audit", "content": "The dichotomy in our experiment design, specifically the demarcation of guided versus blind audit phases, is structured to test the RAG-LLM system's capabilities under contrasting conditions.\nIn the guided audit phase, the prompts are engineered to include explicit mention of the vulnerability type accompanied by its description. This phase evaluates the system's precision when the language model is provided with a clear framework of the vulnerability expected to be found within the smart contract. The benefits of this method include testing the LLM's comprehension of specific vulnerability descriptors and its ability to correlate those with patterns detected in the code. Moreover, we assess whether the presence of guided information aids or biases the decision-making accuracy of the model. The reasoning behind incorporating a guided audit is to measure the added value or possible detraction of closely directed context in enabling the model to pinpoint exact vulnerabilities.\nIn contrast, the blind audit phase assesses the language model's unsupervised analytical proficiency. It eliminates any direct guidance toward a particular vulnerability, instead relying solely on the system's RAG retrieval mechanism to provide context. This method tests the model's ability to generalize from the vectorized corpus of smart contract vulnerabilities and its intrinsic understanding derived from initial training. It pushes the boundaries of the system's autonomous functionality by requiring it to discern and identify vulnerabilities purely based on the contextual relevance of the presented code. This approach also provides insights into potential model limitations and uncovers areas where in-context learning might need to be refined for accurate unsupervised decision-making.\nBoth phases are critical for a comprehensive analysis of the system design. The guided audits act as a controlled environment to monitor the system's performance with clear indicators, similar to assessing its capability to perform targeted code analysis. The blind audits test the system's generalization, replication of logical reasoning, and its independent inferential abilities, mirroring a more practical real-world scenario where explicit guidance is not always available."}, {"title": "5.2 Testing Dataset Curation", "content": "The curation of our testing dataset was methodically approached to ensure rigorous assessment in both phases of our experiment. For the initial phase focused on guided detection, we utilized the set of 52 smart contracts catalogued in Table 1 of the study by David et al [11, Table 1]. This particular collection was initially sampled and presented by Zhou et al. in their work on decentralized finance (DeFi) attacks [12]. We have replicated this enumerated dataset in Table 1 of our paper, and the vulnerability types along with their descriptions were directly sourced from Table 6 in David et al.'s publication [number, Table. 6]. Adopting this dataset enables us to undertake a level of replication from David et al.'s experimental design while adapting the study to evaluate our RAG system's specific parameters.\nFor the second phase of blind detection, our experiment expands the evaluation scope to include the broadly composed dataset of 219 smart contracts, as detailed in the comprehensive \"Systematization of Knowledge\" (SoK) by Zhou et al. The use of this extensive dataset allows for an exploration into the generative capabilities of our RAG system across a diverse and realistic range of smart contracts, derived from actual DeFi incident analyses. This phase is critical in assessing the model's performance in a more autonomous scenario without prior indication of potential vulnerabilities."}, {"title": "5.3 Prompt Engineering", "content": "Prompt engineering plays a critical role in the effectiveness of language model interactions, particularly when specialized tasks such as smart contract audits are involved. In our methodology, we designed the prompts to elicit specific responses from the model regarding potential vulnerabilities in smart contract code.\nIn the initial stages of our study, we considered replicating the experimental design and prompt engineering methods employed by David et al., with the intention of conducting a comparative analysis. David et al. conducted tests on 52 smart contracts, each against 38 different vulnerability types; a process that involved evaluating a smart contract with a known vulnerability using a corresponding prompt explaining that vulnerability, and then separately using prompts for the 37 other vulnerability types to assess the language model's susceptibility to false positives and negatives.\nHowever, upon further consideration, we determined that our experimental setup, which inherently includes a RAG component that retrieves contextually relevant code snippets, was not conducive to the same methodology. In their framework, each vulnerability type is evaluated in isolation to test the model's identification accuracy. In contrast, our RAG system is designed to pull examples of code similar to the user input implying that, for a contract known to be susceptible to reentrancy attacks, the retrieved code would primarily illustrate reentrancy vulnerabilities.\nThis retrieval specificity introduces a complication; when the RAG component fetches documents that align with the primary vulnerability, the in-context learning becomes biased towards that vulnerability type. As a result, evaluating the contract against other vulnerability categories, such as integer overflow, might lead to misleading outcomes since the retrieved context would not pertain to the vulnerability in question. Therefore, our RAG-enhanced setup naturally skews the language model towards a correct identification of the dominant vulnerability, thereby undermining the validity of a testing approach designed around the identification of different potential vulnerabilities.\nConsequently, we elected not to directly replicate David et al.'s experimental framework, but rather to retain certain elements from their study, including the use of analogous prompts and datasets. This strategic adjustment allows us to test for the replicability and consistency of our RAG-enhanced system in identifying vulnerabilities within smart contracts. We aim to measure the repeatable efficiency of our RAG integration and gain insights into the variability and potential non-determinism within large language model responses.\nThis approach not only preserves the integrity of our experimental setup, but also contributes to the assessment of our system's precision and the exploration of the intrinsic randomness that may affect the outcomes of language model-based analyses.\nThe prompt for phase one is shown in Figure 2. The placeholders within the prompt are replaced with specific information for each iteration of the model's use:\n1. {context} - Content retrieved from the vector store (Pinecone), which is semantically relevant to the user's question, replaces this placeholder. These documents typically hold patterns, code snippets, or commentaries pertaining to recognized vulnerabilities in smart contracts.\n2. {question} - Into this placeholder, the actual code snippet or smart contract furnished by the user for analysis is inserted. It represents the text that the model is tasked with examining to ascertain the existence or non-existence of vulnerabilities.\n3. {vulnerability_description}, {vulnerability_type} - These placeholders are filled with the specific type and definition of a vulnerability from a pre-curated list. This specific information directs the model to assess the query with regard to a singular vulnerability type, testing guided detection.\nThe design choice to solicit binary responses\u2014'YES' or 'NO'-from the language model is underpinned by methodological considerations that enhance the reliability and objectivity of the analysis. By constraining the language model's output in this manner, the need for human interpretation of more verbose responses is eliminated, thereby reducing the potential introduction of subjective bias into the evaluation process. A binary decision format ensures deterministic results that are straightforward to classify, categorize, and analyze statistically."}, {"title": "6. Phase One", "content": "Presented here are the outcomes of the first phase of our experimentation, which sought to evaluate the precision of our RAG-LLM system in a guided setting. The results derived from this phase provide insight into the system's capability to correctly identify specific vulnerabilities when provided with contextual cues in the form of vulnerability types and descriptions."}, {"title": "6.1 Results", "content": "The experiment involved the application of the RAG-LLM system to 52 smart contracts, each evaluated against a multitude of vulnerability types explicitly mentioned within the prompts. The tabulated data in Table 2 compiles the aggregated results from these individual assessments.\nAs depicted, the system achieved a total of 1303 successful identifications, representing a success rate of approximately 62.7%. Conversely, the number of instances where the system failed to correctly identify a vulnerability\u2014or incorrectly identified the presence of one that was not present\u2014stood at 777, accounting for 37.3% of the evaluations. The overall success rate of 0.6270 attests to the system's capability to correctly identify vulnerabilities with modest reliability when provided with specific, indicative context."}, {"title": "6.2 Discussion", "content": "In comparing our findings with those presented by David et al., we observe an improvement in the success rate of vulnerability detection. Their study, employing a GPT-4 model with a 32k token window, achieved a success rate of approximately 59.76%, calculated by aggregating 32 true positives (TP) and 1128 true negatives (TN) over the total number of data points (1950), with the failure rate registering around 40.24% derived from 740 false positives (FP) and 41 false negatives (FN). Our usage of GPT-4-1106-preview yielded a slightly higher success rate of 62.7% and a correspondingly lower failure rate of 37.3%, likely attributed to the expanded token windows and retrieved context. We note that these results may not be directly comparable due to the differences in experiment design.\nThe dataset analyzed revealed no discernible pattern in the performance of the RAG-LLM system across different vulnerability types. Variability in the success rates was evident, with specific contracts having the same vulnerability type demonstrating significant deviations in how effectively they were audited. This lack of consistency suggests that the model's performance is influenced by more nuanced factors than simply the category of vulnerability being scrutinized.\nMoreover, the results demonstrated the absence of uniformity in classification accuracy, as very few smart contracts were consistently identified with 0% or 100% efficacy. This variability implies an element of randomness in the model's classification process, whereby the same input does not guarantee identical results across multiple audits. To construct a metric of classification reliability, we tested each smart contract 40 times against a single specified vulnerability. The derived percentages display the inherent volatility in the model's decision-making, providing quantifiable insights into the repeatability of results. The practical implication of this finding suggests that multiple runs may be needed to corroborate the model's conclusions reliably when utilizing the RAG-LLM system for smart contract audits.\nIn the retrieval process, the vectorstore was queried using the code provided from smart contract being tested without incorporating the vulnerability type or description. Our system only fetched results relying solely on the semantic similarity inherent in the smart contract code and the vectorstore. Future iterations of testing could explore \"guided retrieval,\" incorporating the vulnerability type and description into the retrieval query, to scrutinize the impact of this information on the retrieval process and performance of vulnerability identification.\nIt is crucial to acknowledge a limitation in our current design; the binary YES/NO response mechanism does not differentiate between false positives and false negatives. As such, any positive (YES) identification cannot be conclusively verified within the scope of our analysis due to the absence of an underlying ground truth for validation. This binary outcome design focused on simplicity and elimination of subjective biases implies that while we can quantify success and failure rates, we lack granular insight into the nature of inaccuracies that arise from the model's processing. Future studies may address this by incorporating a mechanism to assess responses against a verifiable ground truth, enabling distinction between false positives and negatives which would enhance the depth of system evaluation."}, {"title": "7. Phase Two", "content": "Building on the methodologies established in the guided audit phase, Phase Two introduces a more stringent testing environment for evaluating the Retrieval-Augmented Generation (RAG) system's capabilities in detecting smart contract vulnerabilities. In this phase, the prompts provided to the language model exclude explicit vulnerability types, challenging the system to rely solely on its retrieval mechanism and intrinsic understanding of smart contract security.\nThe blind audit phase is designed to test the model's unsupervised analytical proficiency. By removing direct guidance towards specific vulnerabilities, this phase emphasizes the model's ability to generalize from the vectorized corpus of known vulnerabilities and apply this knowledge autonomously."}, {"title": "7.1 Results", "content": "For brevity, the full results for phase two are listed in the appendix, within Table 4.\nIn Phase Two, the RAG-LLM system was tested under a blind audit setup, evaluating its ability to detect vulnerabilities without explicit prompt instructions regarding the vulnerability type. The experiment involved 219 smart contracts, each undergoing 40 tests. The aggregated results are summarized in Table 2, showing a success rate of approximately 60.71%."}, {"title": "7.2 Discussion", "content": "The results from Phase Two indicate that the RAG-LLM system maintains a robust performance in a more challenging blind audit scenario. The overall success rate of 60.71% is encouraging, showcasing the model's potential to generalize and detect vulnerabilities autonomously.\nComparison To Phase One The guided detection phase yielded a higher success rate of 62.7%, benefiting from explicit vulnerability type cues.\nThe slight drop in success rate during the blind audit phase (60.71%) suggests that the model's performance is influenced by the absence of direct guidance, highlighting areas for improvement in its unsupervised analytical capabilities.\nVariability The performance variability observed across different smart contracts and vulnerability types indicates that certain vulnerabilities are more challenging for the model to detect without explicit context. For instance, smart contracts with reentrancy vulnerabilities showed a significant deviation in detection accuracy, with success rates ranging from 0% to 97.5%. This variability underscores the model's dependency on the quality and relevance of retrieved contextual information. It suggests that while the model can generalize, its effectiveness is enhanced when relevant examples are retrieved and presented. For instance:\nOn-chain Oracle Manipulation: Success rates varied significantly, with some contracts achieving near-perfect detection accuracy (e.g., 97.5% for contract 172) and others much lower (e.g., 30% for contract 174).\nAbsence of Code Logic or Sanity Check: This vulnerability type also showed a wide range of success rates, from as low as 0% (contract 7) to as high as 100% (contracts 109 and 110).\nReentrancy: Detection accuracy for reentrancy vulnerabilities ranged from 0% for some contracts (e.g., contract 47) to 97.5% for others (e.g., contract 14), indicating inconsistency in the model's ability to detect this vulnerability type without explicit guidance.\nBackdoor / Honeypot: This category showed relatively high detection rates, with most contracts achieving success rates above 70%, suggesting that the model is generally proficient in identifying these vulnerabilities even in a blind audit scenario.\nThe data also highlights specific contracts where the model struggled, such as those involving \"Deployment Mistakes\" and \"Function Visibility Errors,\" which showed lower and more variable success rates. These findings suggest that the model's retrieval mechanism and its contextual understanding are critical factors influencing its performance. Contracts like those with \"Unfair Slippage Protection\" and \"Improper Asset Locks\" had particularly low success rates, pointing to areas where the model's detection capabilities could be enhanced with better contextual information and retrieval techniques .\nIn conclusion, while the RAG-LLM system demonstrates substantial promise in autonomous vulnerability detection, the results from Phase Two highlight the importance of context and guidance in maximizing its effectiveness. Continued enhancements in retrieval strategies and model training will be critical in advancing the system's capabilities for practical deployment in smart contract security auditing."}, {"title": "8. Considerations", "content": "In reflecting on the comprehensive scope and findings of this project, several key considerations emerge that warrant attention for future research and practical application in the field of smart contract auditing. These considerations encompass limitations, potential improvements, and broader implications of our study."}, {"title": "8.1 Training Data Integrity", "content": "The language models (LLMs) utilized in this study, such as GPT-4-1106, operate as black boxes with undisclosed training datasets. Consequently, there exists a possibility that these models have been exposed to some of the vulnerabilities and smart contract codes we tested. This exposure could introduce biases, potentially inflating the models' success rates. Future work should aim to develop and test models on completely novel datasets to ensure unbiased evaluation of their capabilities."}, {"title": "8.2 Binary Classification Compliance", "content": "One observed limitation is the occasional non-compliance of LLMs with binary classification prompts. Instances where the models provided verbose explanations rather than simple \"YES\" or \"NO\" responses necessitated manual verification to maintain metric coherence. Future research should explore refining prompt engineering techniques or model adjustments to enhance compliance with binary classification tasks."}, {"title": "8.3 False Positives and Ground Truth Ambiguity", "content": "The ground truth for the set of 52 vulnerable DeFi contracts used in our study is not definitive. These contracts might harbor additional, unreported vulnerabilities, which could affect the accuracy of our false positive metrics. Future studies should incorporate a mechanism for more thorough verification of contract vulnerabilities, possibly involving expert human auditors or cross-referencing multiple vulnerability databases."}, {"title": "8.4 Context Limitations and Truncation Strategies", "content": "We encountered context length limitations with the GPT-4-1106 model, particularly with lengthy smart contract codes. Our approach involved naive truncation to fit the model's context window, which may have resulted in loss of critical information. Future work should investigate more sophisticated methods, such as dynamic context compression or multi-pass querying, to handle extensive source codes without truncation."}, {"title": "8.5 Impact of Extended Context on Model Performance", "content": "Qualitative observations suggest that providing longer contexts might reduce the likelihood of models adhering to binary classification tasks. Understanding the internal mechanisms of LLMs, especially those capable of handling extended contexts, could shed light on this phenomenon. Future research should quantify the impact of longer contexts on model performance and classification accuracy, potentially leading to improved prompt structuring."}, {"title": "8.6 Security Implications and Ethical Considerations", "content": "While our study demonstrates the potential of RAG-LLMs in smart contract auditing, it also raises security and ethical considerations. The reliance on automated systems for vulnerability detection could inadvertently introduce new risks if these systems are not rigorously validated. Ethical considerations regarding the transparency and accountability of AI-driven audits should be addressed, ensuring that human oversight remains integral to the auditing process."}, {"title": "8.7 Scalability and Real-World Application", "content": "The scalability of our RAG-LLM approach for large-scale, real-world applications presents another critical consideration. While our experiments show promising results in controlled settings, deploying this system in diverse, dynamic environments such as decentralized finance ecosystems will require extensive testing and optimization. Future research should focus on scalability, robustness, and integration with existing security frameworks.\nIn summary, while our study marks significant progress in the field of smart contract auditing, these considerations highlight areas for further research and development. Addressing these aspects will be crucial in advancing the efficacy, reliability, and ethical deployment of Al-driven vulnerability detection systems."}, {"title": "9. Conclusion", "content": "The rapid evolution of Decentralized Finance (DeFi) necessitates robust and scalable security auditing mechanisms to mitigate the risks posed by smart contract vulnerabilities. Our study explores the integration of Retrieval-Augmented Generation (RAG) with large language models (LLMs) to enhance the detection of these vulnerabilities, marking a significant step towards democratizing access to smart contract security.\nOur approach leverages the extensive context windows of GPT-4-1106, combined with a meticulously curated dataset of known vulnerabilities, to construct a powerful RAG-LLM pipeline. Through a two-phased experimental design, we evaluated the system's performance under guided and blind audit conditions, yielding promising results that underscore the potential of RAG-LLMs in this domain.\nIn the guided audit phase, the explicit provision of vulnerability types facilitated a higher success rate of 62.7%, demonstrating the system's ability to effectively utilize contextual cues. Conversely, the blind audit phase, which challenged the model to detect vulnerabilities without explicit guidance, resulted in a slightly lower success rate of 60.71%. This indicates the model's robust generalization capabilities, albeit with room for improvement in unsupervised contexts.\nThe variability observed in detection accuracy across different smart contracts and vulnerability types highlights the nuanced nature of smart contract auditing. Certain vulnerabilities, such as reentrancy, proved more challenging to detect consistently, pointing to the critical role of contextually relevant retrieval in enhancing model performance"}]}