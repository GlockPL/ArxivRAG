{"title": "Retrieval Augmented Generation Integrated Large Language Models in Smart Contract Vulnerability Detection", "authors": ["Jeffy Yu"], "abstract": "The rapid growth of Decentralized Finance (DeFi) has been accompanied by substantial financial losses due to smart contract vulnerabilities, underscoring the critical need for effective security auditing. With attacks becoming more frequent, the necessity and demand for auditing services has escalated. This especially creates a financial burden for independent developers and small businesses, who often have limited available funding for these services. Our study builds upon existing frameworks by integrating Retrieval-Augmented Generation (RAG) with large language models (LLMs), specifically employing GPT-4-1106 for its 128k token context window. We construct a vector store of 830 known vulnerable contracts, leveraging Pinecone for vector storage, OpenAI's text-embedding-ada-002 for embeddings, and LangChain to construct the RAG-LLM pipeline. Prompts were designed to provide a binary answer for vulnerability detection. We first test 52 smart contracts 40 times each against a provided vulnerability type, verifying the replicability and consistency of the RAG-LLM. Encouraging results were observed, with a 62.7% success rate in guided detection of vulnerabilities. Second, we challenge the model under a \"blind\" audit setup, without the vulnerability type provided in the prompt, wherein 219 contracts undergo 40 tests each. This setup evaluates the general vulnerability detection capabilities without hinted context assistance. Under these conditions, a 60.71% success rate was observed. While the results are promising, we still emphasize the need for human auditing at this time. We provide this study as a proof of concept for a cost-effective smart contract auditing process, moving towards democratic access to security.", "sections": [{"title": "1. Introduction", "content": "The rapid expansion of blockchain technology and de-centralized finance (DeFi) has transformed the financial landscape, offering novel opportunities for peer-to-peer transactions and automated financial services. At the heart of this transformation are smart contracts-self-executing contracts that facilitate, verify, and enforce the terms of an agreement [1]. Despite their potential to streamline and secure transactions, smart contracts are not without risks. Vulnerabilities within these contracts can lead to significant financial losses and undermine trust in the DeFi ecosystem [12].\nThe critical need for effective smart contract auditing has never been more apparent. Traditional auditing methods, which rely on manual code reviews by secu-rity experts, are thorough but costly and time-intensive [2]. This poses a significant barrier for independent developers and small businesses that may lack the re-sources to afford such services. Consequently, there is a pressing demand for scalable, cost-effective solu-tions that can democratize access to smart contract security auditing, ensuring that all participants in the blockchain space can safeguard their assets and opera-tions [15].\nIn response to this need, our study explores the in-tegration of Retrieval-Augmented Generation (RAG) with large language models (LLMs) to enhance the de-tection of smart contract vulnerabilities. By leveraging the advanced capabilities of models such as GPT-4-1106, combined with a comprehensive vector store of known vulnerabilities, we aim to develop a robust au-diting tool that is both accessible and reliable [3]. Our research evaluates the performance of this RAG-LLM system under both guided and blind conditions, pro-viding insights into its potential and identifying areas for further refinement. Through this approach, we seek to contribute to the democratization of smart contract security, fostering a more inclusive and secure DeFi ecosystem."}, {"title": "2. Background", "content": "The explosive growth of decentralized finance (DeFi) has introduced new paradigms in financial transactions, leveraging blockchain technology to enable peer-to-peer lending, trading, and other financial services with-out traditional intermediaries. Central to these opera-tions are smart contracts\u2014self-executing contracts with the terms of the agreement directly written into code. While smart contracts offer numerous advantages, in-cluding automation, transparency, and reduced costs, they also pose significant security risks.\nSmart contract vulnerabilities can lead to catas-trophic financial losses, as evidenced by numerous high-profile attacks in the DeFi space. Common vulner-abilities include reentrancy attacks, integer overflows, and unchecked external calls [18]. These vulnerabilities exploit the logic and structure of smart contract code, often resulting in the unauthorized transfer of funds or other malicious outcomes.\nTo mitigate these risks, robust smart contract audit-ing is essential. Traditional auditing involves manual code reviews by security experts, which, although effec-tive, are time-consuming and expensive [14, 16]. This creates a bottleneck for independent developers and small businesses, who may not have the resources to access high-quality auditing services.\nThe emergence of large language models (LLMs), such as OpenAI's GPT-4, has opened new avenues for automating parts of the auditing process. LLMs have demonstrated remarkable capabilities in understanding and generating human-like text, making them suitable for tasks that involve code analysis and pattern recogni-tion. However, LLMs on their own are limited by their training data and may lack the specificity required for thorough smart contract auditing.\nTo address these limitations, the concept of Retrieval-Augmented Generation (RAG) has been introduced. RAG combines the generative abilities of LLMs with retrieval mechanisms that access a curated database of relevant information. This hybrid approach enables the model to generate more accurate and contextually relevant responses by referencing external sources of knowledge.\nIn the context of smart contract auditing, a RAG-LLM system can retrieve examples of known vulner-abilities from a vector store, enhancing its ability to identify similar issues in new contracts. By integrat-ing this technology, we aim to develop a scalable and cost-effective solution that democratizes access to smart contract security auditing.\nOur study builds upon this framework, employ-ing GPT-4-1106 with a 128k token context win-dow, Pinecone for vector storage, and OpenAI's text-embedding-ada-002 for generating embeddings. We construct a vector store of 830 known vulnerable con-tracts and design prompts to test the model's ability to detect vulnerabilities under both guided and blind conditions.\nThe experimental results from our study show promising success rates in vulnerability detection, demonstrating the potential of RAG-LLMs to provide reliable and accessible smart contract auditing services. However, challenges remain, particularly in ensuring the unbiased evaluation of model performance and addressing the variability in detection accuracy across different types of vulnerabilities."}, {"title": "3. Democratic Access", "content": "The advent of blockchain technology and decentralized finance (DeFi) has revolutionized the way financial transactions are conducted, offering unprecedented transparency, security, and decentralization. How-ever, the increasing complexity and proliferation of smart contracts have introduced significant security challenges [13]. Vulnerabilities in smart contracts can lead to substantial financial losses, undermining trust in these systems and hindering widespread adoption [17]. This underscores the critical need for accessible and reliable smart contract auditing services.\nTraditional smart contract auditing is typically con-ducted by specialized firms that offer thorough but expensive services. This creates a significant barrier for independent developers and small businesses that may lack the financial resources to afford these audits. Consequently, the democratization of smart contract security auditing is essential to ensure equitable access to these vital services, fostering a more inclusive and secure DeFi ecosystem.\nDemocratic access to security auditing means mak-ing robust and reliable auditing tools available to all participants in the blockchain space, regardless of their size or resources. By leveraging advanced technologies such as Retrieval-Augmented Generation (RAG) inte-grated with large language models (LLMs), our project aims to provide a scalable, cost-effective solution that democratizes access to smart contract security.\nRAG-LLMs enhance the auditing process by combin-ing the generative capabilities of LLMs with the preci-sion of retrieval mechanisms, allowing for the identifi-cation and analysis of vulnerabilities in smart contracts. This integration not only improves the accuracy of vul-nerability detection but also makes these tools more accessible to a broader audience. Independent develop-ers and small enterprises can leverage these advanced auditing capabilities without the prohibitive costs asso-ciated with traditional auditing services.\nThe motivation behind this project is to empower all stakeholders in the blockchain ecosystem with the tools necessary to ensure the security of their smart con-tracts. By providing decentralized, trustless auditing capabilities, we aim to foster greater trust in blockchain technologies. This trust is crucial for the widespread adoption and integration of blockchain into various sectors, from finance to supply chain management.\nIn a trustless environment, where transactions and operations are executed without the need for interme-diaries, the integrity and security of smart contracts become paramount [19]. Democratic access to auditing tools ensures that all participants can independently verify the security of their smart contracts, promoting a more resilient and trustworthy blockchain infrastruc-ture.\nMoreover, as the DeFi space continues to evolve, the scalability of our approach allows it to keep pace with the growing number of smart contracts and their increasing complexity. By continually enhancing the ca-pabilities of RAG-LLMs and making these tools widely available, we support the sustainable growth of a se-cure and inclusive DeFi ecosystem."}, {"title": "4. RAG-LLM Pipeline", "content": "The integration of Retrieval-Augmented Generation (RAG) with large language models (LLMs) extends their capacity to generate responses by incorporating external knowledge sources. This paradigm enhances model performance on queries involving information beyond its initial training data. RAG operates by en-riching the model's context with relevant content from a vector store, expanding the data reservoir accessible during inference. This dual retrieval mechanism distin-guishes RAG-LLMs from traditional LLMs, elevating their ability to synthesize and contextualize responses by concurrently utilizing the model's internal knowl-edge and external databases.\nOur system capitalizes on integrating RAG with LLMs to apply their combined strengths for the domain of smart contract security. We employ GPT-4-1106 due to its extensive 128k token context window that enables the processing of lengthy prompts combined with infor-mation retrieval context [4]. The system is setup with a vector store containing embeddings for 830 known vulnerable smart contracts. We utilize Pinecone for vector indexing and storage due to its efficient scal-ing and retrieval capabilities, coupled with OpenAI's text-embedding-ada-002 to generate the embeddings [4,5]. The OpenAI text-embedding-ada-002 model is capable of generating embeddings for mixed content, encompassing both natural language and code, which is ideal for our dataset that includes Solidity smart con-tracts interlaced with natural language comments [4]. LangChain, a framework optimized for constructing RAG-LLM pipelines, facilitates our retrieval-generative processes [6].\nThe system is designed to enhance the quality of re-sponses by retrieving pertinent-vulnerability patterns from the vector store of known vulnerable smart con-tracts. Upon receiving a user prompt potentially indica-tive of insecure coding practices, our setup identifies similarities within the vector store. A match suggests parallels between the user's code and pre-identified vulnerabilities, flagging a likely security issue."}, {"title": "4.1 Vulnerable Contract Dataset", "content": "In order to construct an expansive and robust dataset for our RAG-LLM pipeline, we collected a total of 830 vulnerable smart contracts. These contracts were sourced from public Github repositories known to house collections of compromised or flawed smart con-tract code. The repositories utilized include \"(Not So) Smart Contracts\" by Trail of Bits, \"DeFiHackLabs\" and \"DeFiVulnLabs\" by SunWeb3Sec, and \"Smart Contract VulnDB\" by tintinweb [7,8,9].\nThe dataset comprises a wide spectrum of vulnera-bility types which are essential to develop a compre-hensive understanding of the security pitfalls within smart contracts. Some of the vulnerabilities included are:\n\u2022 Bad randomness\n\u2022 Denial of service\n\u2022 Forced ether reception\n\u2022 Honeypots\n\u2022 Incorrect interface implementation\n\u2022 Integer overflow\n\u2022 Race condition occurrences\n\u2022 Reentrancy attacks\n\u2022 Unchecked external calls\n\u2022 Unprotected functions\n\u2022 Variable shadowing\n\u2022 Misnamed constructors\nThis list is not exhaustive.\nThis variation in the dataset aims to ensure that the system is well-equipped to recognize and provide context for a diverse array of security weaknesses.\nThe selection criteria for contract inclusion was dic-tated by the requirement to have a diverse set of vul-nerabilities rather than by volume. While a greater number of contracts per vulnerability type may have contributed to a more substantial dataset, the preva-lence of vulnerable contract datasets is limited, offering a pragmatic cap to the dataset size.\nMoreover, we reserved a significant portion of gath-ered contracts exclusively for validation purposes to avoid direct data overlap between the RAG vectorstore and subsequent testing. While we had to sacrifice data for vectorstore creation, this measure is taken to ensure the integrity of our experimental evaluation, prevent-ing the model from simply regurgitating exact code matches without genuine understanding."}, {"title": "4.2 Data Processing and Embedding", "content": "We initiated the data processing by loading .sol files-designating them as unstructured text docu-ments. We employed the LangChain DirectoryLoader to retrieve Solidity smart contracts from a specified di-rectory. Following loading, the documents underwent fragmentation into smaller text chunks. To accommo-date the maximum context window of our selected model, each chunk was composed of 1024 tokens, a size that was large enough to prevent truncation of functions while reasonably fitting in the context win-dow of the GPT-4 model.\nLangChain's TokenTextSplitter, utilizing the tokeni-sation method from TikToken, was selected as the tool to divide the contracts into token-sized chunks. Tik-Token provided two key benefits for processing our dataset:\n\u2022 Reversibility The tokenization process is de-signed to be reversible, ensuring that the trans-formed tokens can be converted back into the orig-inal text without any loss of information, which is imperative for the integrity and fidelity of the dataset.\n\u2022 Generalization Capability TikToken is engi-neered to generalize beyond its training corpus, enabling it to tokenize unfamiliar text sequences effectively. This characteristic is particularly bene-ficial for parsing Solidity code, as its syntax may deviate considerably from the data TikToken was originally trained on.\nSubsequently, embeddings were created for the to-kenized chunks using the OpenAI API with the \"text-embedding-ada-002\" model. This model was specifi-cally selected due to its proficiency in encoding a blend of natural language and programming code, which makes it particularly suited to the mixed content of smart contracts that incorporates both Solidity code and natural language code annotations.\nFor the vector store, we configured Pinecone with a 'p2' index option, which was chosen for its rapid search capabilities. Upon passing the embeddings to Pinecone, we created a vector database with 939 vectors, each with the embeddings (of 1536 dimensions) and corresponding metadata for each code chunk from the smart contracts."}, {"title": "4.3 RAG with LangChain", "content": "Our system incorporates Retrieval-Augmented Gener-ation (RAG) technology within the LangChain frame-work to analyze smart contracts for potential vulnera-bilities. LangChain is a versatile toolkit that facilitates the creation and deployment of chains a series of operations involving language models and other pro-cessing steps.\nOur LLM, specifically ChatOpenAI configured with the GPT-4 model \"gpt-4-1106-preview,\" operates with set parameters such as a temperature of 0.7 for response variability and a maximum token allowance. We chose this model due to the expanded 128,000 token window, which allows fitting the full user smart contract along with all retrieved code from vector search in a single prompt. For our experiment, we set a maximum token response of 5, as our prompts instructed a binary YES or 'NO'-from the language model is underpinned by methodological considerations that enhance the relia-bility and objectivity of the analysis. By constraining the language model's output in this manner, the need for human interpretation of more verbose responses is eliminated, thereby reducing the potential introduction of subjective bias into the evaluation process. A bi-nary decision format ensures deterministic results that are straightforward to classify, categorize, and analyze statistically."}, {"title": "5. Experiment Design", "content": "Our experiment design is aligned with evaluating the efficacy of the RAG-LLM system in identifying smart contract vulnerabilities. The testing methodology bi-furcates into two distinct phases: guided detection and blind detection. Guided detection involves an explicit indication of the vulnerability type and a correspond-ing description provided within the prompt.\nBlind detection, conversely, relies solely on the RAG retrieved context and the smart contract code in ques-tion, without explicit prompt instructions regarding the vulnerability type."}, {"title": "5.1 Guided vs. Blind Audit", "content": "The dichotomy in our experiment design, specifically the demarcation of guided versus blind audit phases, is structured to test the RAG-LLM system's capabilities under contrasting conditions.\nIn the guided audit phase, the prompts are engi-neered to include explicit mention of the vulnerability type accompanied by its description. This phase evalu-ates the system's precision when the language model is provided with a clear framework of the vulnerability expected to be found within the smart contract. The benefits of this method include testing the LLM's com-prehension of specific vulnerability descriptors and its ability to correlate those with patterns detected in the code. Moreover, we assess whether the presence of guided information aids or biases the decision-making accuracy of the model. The reasoning behind incorpo-rating a guided audit is to measure the added value or possible detraction of closely directed context in enabling the model to pinpoint exact vulnerabilities.\nIn contrast, the blind audit phase assesses the lan-guage model's unsupervised analytical proficiency. It eliminates any direct guidance toward a particular vul-nerability, instead relying solely on the system's RAG retrieval mechanism to provide context. This method tests the model's ability to generalize from the vector-ized corpus of smart contract vulnerabilities and its intrinsic understanding derived from initial training. It pushes the boundaries of the system's autonomous functionality by requiring it to discern and identify vul-nerabilities purely based on the contextual relevance of the presented code. This approach also provides insights into potential model limitations and uncov-ers areas where in-context learning might need to be refined for accurate unsupervised decision-making.\nBoth phases are critical for a comprehensive analy-sis of the system design. The guided audits act as a controlled environment to monitor the system's per-formance with clear indicators, similar to assessing its capability to perform targeted code analysis. The blind audits test the system's generalization, replication of logical reasoning, and its independent inferential abil-ities, mirroring a more practical real-world scenario where explicit guidance is not always available."}, {"title": "5.2 Testing Dataset Curation", "content": "The curation of our testing dataset was methodically ap-proached to ensure rigorous assessment in both phases of our experiment. For the initial phase focused on guided detection, we utilized the set of 52 smart con-tracts catalogued in Table 1 of the study by David et al [11, Table 1]. This particular collection was initially sampled and presented by Zhou et al. in their work on decentralized finance (DeFi) attacks [12]. We have replicated this enumerated dataset in Table 1 of our paper, and the vulnerability types along with their de-scriptions were directly sourced from Table 6 in David et al.'s publication [number, Table. 6]. Adopting this dataset enables us to undertake a level of replication from David et al.'s experimental design while adapt-ing the study to evaluate our RAG system's specific parameters.\nFor the second phase of blind detection, our exper-iment expands the evaluation scope to include the broadly composed dataset of 219 smart contracts, as de-tailed in the comprehensive \"Systematization of Knowl-edge\" (SoK) by Zhou et al. The use of this extensive dataset allows for an exploration into the generative capabilities of our RAG system across a diverse and realistic range of smart contracts, derived from actual DeFi incident analyses. This phase is critical in assess-ing the model's performance in a more autonomous scenario without prior indication of potential vulnera-bilities."}, {"title": "5.3 Prompt Engineering", "content": "Prompt engineering plays a critical role in the effective-ness of language model interactions, particularly when specialized tasks such as smart contract audits are in-volved. In our methodology, we designed the prompts to elicit specific responses from the model regarding potential vulnerabilities in smart contract code.\nIn the initial stages of our study, we considered repli-cating the experimental design and prompt engineering methods employed by David et al., with the intention of conducting a comparative analysis. David et al. con-ducted tests on 52 smart contracts, each against 38 different vulnerability types; a process that involved evaluating a smart contract with a known vulnera-bility using a corresponding prompt explaining that vulnerability, and then separately using prompts for the 37 other vulnerability types to assess the language model's susceptibility to false positives and negatives.\nHowever, upon further consideration, we determined that our experimental setup, which inherently includes a RAG component that retrieves contextually relevant code snippets, was not conducive to the same method-ology. In their framework, each vulnerability type is evaluated in isolation to test the model's identification accuracy. In contrast, our RAG system is designed to pull examples of code similar to the user input im-plying that, for a contract known to be susceptible to reentrancy attacks, the retrieved code would primarily illustrate reentrancy vulnerabilities.\nThis retrieval specificity introduces a complication; when the RAG component fetches documents that align with the primary vulnerability, the in-context learning becomes biased towards that vulnerability type. As a result, evaluating the contract against other vulnerability categories, such as integer overflow, might lead to misleading outcomes since the retrieved context would not pertain to the vulnerability in ques-tion. Therefore, our RAG-enhanced setup naturally skews the language model towards a correct identifica-tion of the dominant vulnerability, thereby undermin-ing the validity of a testing approach designed around the identification of different potential vulnerabilities.\nConsequently, we elected not to directly replicate David et al.'s experimental framework, but rather to retain certain elements from their study, including the use of analogous prompts and datasets. This strategic adjustment allows us to test for the replicability and consistency of our RAG-enhanced system in identi-fying vulnerabilities within smart contracts. We aim to measure the repeatable efficiency of our RAG inte-gration and gain insights into the variability and po-tential non-determinism within large language model responses.\nThis approach not only preserves the integrity of our experimental setup, but also contributes to the assess-ment of our system's precision and the exploration of the intrinsic randomness that may affect the outcomes of language model-based analyses.\nThe prompt for phase one is shown in Figure 2. The placeholders within the prompt are replaced with specific information for each iteration of the model's use:\n1. {context} - Content retrieved from the vector store (Pinecone), which is semantically relevant to the user's question, replaces this placeholder. These documents typically hold patterns, code snippets, or commentaries pertaining to recog-nized vulnerabilities in smart contracts.\n2. {question} - Into this placeholder, the actual code snippet or smart contract furnished by the user for analysis is inserted. It represents the text that the model is tasked with examining to ascertain the existence or non-existence of vulnerabilities.\n3. {vulnerability_description}, {vulnerability_type} These placeholders are filled with the specific type and definition of a vulnerability from a pre-curated list. This specific information directs the model to assess the query with regard to a singular vulnerability type, testing guided detection.\nThe design choice to solicit binary responses\u2014'YES' or 'NO'-from the language model is underpinned by methodological considerations that enhance the relia-bility and objectivity of the analysis. By constraining the language model's output in this manner, the need for human interpretation of more verbose responses is eliminated, thereby reducing the potential introduction of subjective bias into the evaluation process. A bi-nary decision format ensures deterministic results that are straightforward to classify, categorize, and analyze statistically."}, {"title": "6. Phase One", "content": "Presented here are the outcomes of the first phase of our experimentation, which sought to evaluate the precision of our RAG-LLM system in a guided setting. The results derived from this phase provide insight into the system's capability to correctly identify specific vulnerabilities when provided with contextual cues in the form of vulnerability types and descriptions."}, {"title": "6.1 Results", "content": "The experiment involved the application of the RAG-LLM system to 52 smart contracts, each evaluated against a multitude of vulnerability types explicitly mentioned within the prompts. The tabulated data in Table 2 compiles the aggregated results from these individual assessments.\nAs depicted, the system achieved a total of 1303 successful identifications, representing a success rate of approximately 62.7%. Conversely, the number of instances where the system failed to correctly identify a vulnerability\u2014or incorrectly identified the presence of one that was not present\u2014stood at 777, accounting for 37.3% of the evaluations. The overall success rate of 0.6270 attests to the system's capability to correctly identify vulnerabilities with modest reliability when provided with specific, indicative context."}, {"title": "6.2 Discussion", "content": "In comparing our findings with those presented by David et al., we observe an improvement in the success rate of vulnerability detection. Their study, employing a GPT-4 model with a 32k token window, achieved a success rate of approximately 59.76%, calculated by ag-gregating 32 true positives (TP) and 1128 true negatives (TN) over the total number of data points (1950), with the failure rate registering around 40.24% derived from 740 false positives (FP) and 41 false negatives (FN). Our usage of GPT-4-1106-preview yielded a slightly higher success rate of 62.7% and a correspondingly lower fail-ure rate of 37.3%, likely attributed to the expanded token windows and retrieved context. We note that these results may not be directly comparable due to the differences in experiment design.\nThe dataset analyzed revealed no discernible pattern in the performance of the RAG-LLM system across different vulnerability types. Variability in the success rates was evident, with specific contracts having the same vulnerability type demonstrating significant de-viations in how effectively they were audited. This lack of consistency suggests that the model's performance is influenced by more nuanced factors than simply the category of vulnerability being scrutinized.\nMoreover, the results demonstrated the absence of uniformity in classification accuracy, as very few smart contracts were consistently identified with 0% or 100% efficacy. This variability implies an element of ran-domness in the model's classification process, whereby the same input does not guarantee identical results across multiple audits. To construct a metric of clas-sification reliability, we tested each smart contract 40 times against a single specified vulnerability. The de-rived percentages display the inherent volatility in the model's decision-making, providing quantifiable in-sights into the repeatability of results. The practical implication of this finding suggests that multiple runs may be needed to corroborate the model's conclusions reliably when utilizing the RAG-LLM system for smart contract audits.\nIn the retrieval process, the vectorstore was queried using the code provided from smart contract being tested without incorporating the vulnerability type or description. Our system only fetched results relying solely on the semantic similarity inherent in the smart contract code and the vectorstore. Future iterations of testing could explore \"guided retrieval,\" incorporating the vulnerability type and description into the retrieval query, to scrutinize the impact of this information on the retrieval process and performance of vulnerability identification.\nIt is crucial to acknowledge a limitation in our cur-rent design; the binary YES/NO response mechanism does not differentiate between false positives and false negatives. As such, any positive (YES) identification cannot be conclusively verified within the scope of our analysis due to the absence of an underlying ground truth for validation. This binary outcome design fo-cused on simplicity and elimination of subjective biases implies that while we can quantify success and failure rates, we lack granular insight into the nature of inac-curacies that arise from the model's processing. Future studies may address this by incorporating a mecha-nism to assess responses against a verifiable ground truth, enabling distinction between false positives and negatives which would enhance the depth of system evaluation."}, {"title": "7. Phase Two", "content": "Building on the methodologies established in the guided audit phase, Phase Two introduces a more strin-gent testing environment for evaluating the Retrieval-Augmented Generation (RAG) system's capabilities in detecting smart contract vulnerabilities. In this phase, the prompts provided to the language model exclude explicit vulnerability types, challenging the system to rely solely on its retrieval mechanism and intrinsic understanding of smart contract security.\nThe blind audit phase is designed to test the model's unsupervised analytical proficiency. By removing direct guidance towards specific vulnerabilities, this phase emphasizes the model's ability to generalize from the vectorized corpus of known vulnerabilities and apply this knowledge autonomously."}, {"title": "7.1 Results", "content": "For brevity, the full results for phase two are listed in the appendix, within Table 4.\nIn Phase Two, the RAG-LLM system was tested un-der a blind audit setup, evaluating its ability to detect vulnerabilities without explicit prompt instructions re-garding the vulnerability type. The experiment in-volved 219 smart contracts, each undergoing 40 tests. The aggregated results are summarized in Table 2, showing a success rate of approximately 60.71%."}, {"title": "7.2 Discussion", "content": "The results from Phase Two indicate that the RAG-LLM system maintains a robust performance in a more challenging blind audit scenario. The overall success rate of 60.71% is encouraging, showcasing the model's potential to generalize and detect vulnerabilities au-tonomously.\nComparison To Phase One The guided detection phase yielded a higher success rate of 62.7%, benefiting from explicit vulnerability type cues.\nThe slight drop in success rate during the blind audit phase (60.71%) suggests that the model's performance is influenced by the absence of direct guidance, high-lighting areas for improvement in its unsupervised analytical capabilities.\nVariability The performance variability observed across different smart contracts and vulnerability types indicates that certain vulnerabilities are more challeng-ing for the model to detect without explicit context. For instance, smart contracts with reentrancy vulner-abilities showed a significant deviation in detection accuracy, with success rates ranging from 0% to 97.5%. This variability underscores the model's dependency on the quality and relevance of retrieved contextual information. It suggests that while the model can gen-eralize, its effectiveness is enhanced when relevant examples are retrieved and presented. For instance:\nOn-chain Oracle Manipulation: Success rates var-ied significantly, with some contracts achieving near-perfect detection accuracy (e.g., 97.5% for contract 172) and others much lower (e.g., 30% for contract 174).\nAbsence of Code Logic or Sanity Check: This vul-nerability type also showed a wide range of success rates, from as low as 0% (contract 7) to as high as 100% (contracts 109 and 110).\nReentrancy: Detection accuracy for reentrancy vul-nerabilities ranged from 0% for some contracts (e.g., contract 47) to 97.5% for others (e.g., contract 14), in-dicating inconsistency in the model's ability to detect this vulnerability type without explicit guidance.\nBackdoor / Honeypot: This category showed rela-tively high detection rates, with most contracts achiev-ing success rates above 70%, suggesting that the model is generally proficient in identifying these vulnerabili-ties even in a blind audit scenario.\nThe data also highlights specific contracts where the model struggled, such as those involving \"De-ployment Mistakes\" and \"Function Visibility Errors,\" which showed lower and more variable success rates. These findings suggest that the model's retrieval mech-anism and its contextual understanding are critical fac-tors influencing its performance. Contracts like those with \"Unfair Slippage Protection\" and \"Improper Asset Locks\" had particularly low success rates, pointing to areas where the model's detection capabilities could be enhanced with better contextual information and retrieval techniques .\nIn conclusion, while the RAG-LLM system demon-strates substantial promise in autonomous vulnerabil-ity detection, the results from Phase Two highlight the importance of context and guidance in maximizing its effectiveness. Continued enhancements in retrieval strategies and model training will be critical in advanc-ing the system's capabilities for practical deployment in smart contract security auditing."}, {"title": "8. Considerations", "content": "In reflecting on the comprehensive scope and find-ings of this project, several key considerations emerge that warrant attention for future research and practi-cal application in the field of smart contract auditing. These considerations encompass limitations, potential improvements, and broader implications of our study."}, {"title": "8.1 Training Data Integrity", "content": "The language models (LLMs) utilized in this study, such as GPT-4-1106, operate as black boxes with undis-closed training datasets. Consequently, there exists a possibility that these models have been exposed to some of the vulnerabilities and smart contract codes we tested. This exposure could introduce biases, poten-tially inflating the models' success rates. Future work should aim to develop and test models on completely novel datasets to ensure unbiased evaluation of their capabilities."}, {"title": "8.2 Binary Classification Compliance", "content": "One observed limitation is the occasional non-compliance of LLMs with binary classification prompts. Instances where the models provided verbose expla-nations rather than simple \"YES\" or \"NO\" responses necessitated manual verification to maintain metric coherence. Future research should explore refining prompt engineering techniques or model adjustments to enhance compliance with binary classification tasks."}, {"title": "8.3 False Positives and Ground Truth Ambiguity", "content": "The ground truth for the set of 52 vulnerable DeFi contracts used in our study is not definitive. These contracts might harbor additional, unreported vulnera-bilities, which could affect the accuracy of our false pos-itive metrics. Future studies should incorporate a mech-anism for more thorough verification of contract vul-nerabilities, possibly involving expert human auditors or cross-referencing multiple vulnerability databases."}, {"title": "8.4 Context Limitations and Truncation Strategies", "content": "We encountered context length limitations with the GPT-4-1106 model, particularly with lengthy smart contract codes. Our approach involved naive trunca-tion to fit the model's context window, which may have resulted in loss of critical information. Future work should investigate more sophisticated methods, such as dynamic context compression or multi-pass querying, to handle extensive source codes without truncation."}, {"title": "8.5 Impact of Extended Context on Model Performance", "content": "Qualitative observations suggest that providing longer contexts might reduce the likelihood of models adher-ing to binary classification tasks. Understanding the internal mechanisms of LLMs, especially those capa-ble of handling extended contexts, could shed light on this phenomenon. Future research should quantify the impact of longer contexts on model performance and classification accuracy, potentially leading to improved prompt structuring."}, {"title": "8.6 Security Implications and Ethical Considerations", "content": "While our study demonstrates the potential of RAG-LLMs in smart contract auditing, it also raises security and ethical considerations. The reliance on automated systems for vulnerability detection could inadvertently introduce new risks if these systems are not rigorously validated. Ethical considerations regarding the trans-parency and accountability of AI-driven audits should be addressed, ensuring that human oversight remains integral to the auditing process."}, {"title": "8.7 Scalability and Real-World Application", "content": "The scalability of our RAG-LLM approach for large-scale, real-world applications presents another critical consideration. While our experiments show promising results in controlled settings, deploying this system in diverse, dynamic environments such as decentralized finance ecosystems will require extensive testing and optimization. Future research should focus on scalabil-ity, robustness, and integration with existing security frameworks.\nIn summary, while our study marks significant progress in the field of smart contract auditing, these considerations highlight areas for further research and development. Addressing these aspects will be crucial in advancing the efficacy, reliability, and ethical deploy-ment of Al-driven vulnerability detection systems."}, {"title": "9. Conclusion", "content": "The rapid evolution of Decentralized Finance (DeFi) necessitates robust and scalable security auditing mech-anisms to mitigate the risks posed by smart contract vulnerabilities. Our study explores the integration of Retrieval-Augmented Generation (RAG) with large lan-guage models (LLMs) to enhance the detection of these vulnerabilities, marking asignificant step towards de-mocratizing access to smart contract security.\nOur approach leverages the extensive context win-dows of GPT-4-1106, combined with a meticulously curated dataset of known vulnerabilities, to construct a powerful RAG-LLM pipeline. Through a two-phased experimental design, we evaluated the system's per-formance under guided and blind audit conditions, yielding promising results that underscore the poten-tial of RAG-LLMs in this domain.\nIn the guided audit phase, the explicit provision of vulnerability types facilitated a higher success rate of 62.7%, demonstrating the system's ability to effectively utilize contextual cues. Conversely, the blind audit phase, which challenged the model to detect vulnera-bilities without explicit guidance, resulted in a slightly lower success rate of 60.71%. This indicates the model's robust generalization capabilities, albeit with room for improvement in unsupervised contexts.\nThe variability observed in detection accuracy across different smart contracts and vulnerability types high-lights the nuanced nature of smart contract auditing. Certain vulnerabilities, such as reentrancy, proved more challenging to detect consistently, pointing to the critical role of contextually relevant retrieval in en-hancing model performance. These findings suggest that while the RAG-LLM system is a powerful tool, its efficacy can be significantly influenced by the quality of the retrieved contextual information.\nDespite the promising results, our study also reveals several limitations. The potential biases introduced by the unknown training datasets of the LLMs, the occasional non-compliance with binary classification prompts, and the challenges posed by context length limitations all underscore the need for further refine-ment and validation. Moreover, the ethical implications of relying on automated systems for vulnerability de-tection warrant careful consideration, ensuring that human oversight remains a cornerstone of the auditing process.\nOverall, our study demonstrates the feasibility and potential of using RAG-LLMs for smart contract vulner-ability detection, offering a cost-effective and scalable solution that could significantly enhance the security landscape of DeFi ecosystems. However, realizing the full potential of this approach will require addressing the identified limitations and exploring new avenues for improvement."}, {"title": "9.1 Future Work", "content": "Future research should build on our findings, address-ing the following key areas:\n\u2022 Training Data Integrity: Develop and evaluate models on novel, unbiased datasets to mitigate potential training data biases.\n\u2022 Binary Classification Compliance: Refine prompt engineering techniques and model adjustments to ensure consistent compliance with binary classifi-cation tasks.\n\u2022 False Positive Verification: Incorporate mecha-nisms for thorough verification of smart contract vulnerabilities, involving expert human auditors or cross-referencing multiple databases.\n\u2022 Context Handling: Investigate sophisticated meth-ods for handling lengthy smart contract codes, such as dynamic context compression or multi-pass querying.\n\u2022 Impact of Extended Contexts: Quantify the im-pact of extended contexts on model performance and classification accuracy to inform improved prompt structuring.\n\u2022 Ethical Considerations: Address ethical issues re-lated to the transparency and accountability of AI-driven audits, ensuring robust human oversight.\n\u2022 Scalability: Focus on the scalability and robust-ness of the RAG-LLM system for deployment in diverse, real-world DeFi environments.\n\u2022 Integration with Security Frameworks: Explore the integration of RAG-LLM systems with existing security frameworks to enhance overall effective-ness.\n\u2022 Long-term Performance Monitoring: Establish mechanisms for continuous monitoring and eval-uation of the system's performance in real-world applications to ensure sustained efficacy.\nBy addressing these areas, future research can fur-ther advance the capabilities of RAG-LLMs, making them an indispensable tool in the ongoing effort to secure the rapidly evolving landscape of decentralized finance."}]}