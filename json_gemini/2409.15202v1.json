{"title": "ASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction", "authors": ["Iwo Naglik", "Mateusz Lango"], "abstract": "Aspect-Sentiment Triplet Extraction (ASTE) is a recently proposed task of aspect-based sentiment analysis that consists in extracting (aspect phrase, opinion phrase, sentiment polarity) triples from a given sentence. Recent state-of-the-art methods approach this task by first extracting all possible text spans from a given text, then filtering the potential aspect and opinion phrases with a classifier, and finally considering all their pairs with another classifier that additionally assigns sentiment polarity to them. Although several variations of the above scheme have been proposed, the common feature is that the final result is constructed by a sequence of independent classifier decisions. This hinders the exploitation of dependencies between extracted phrases and prevents the use of knowledge about the interrelationships between classifier predictions to improve performance. In this paper, we propose a new ASTE approach consisting of three transformer-inspired layers, which enables the modelling of dependencies both between phrases and between the final classifier decisions. Experimental results show that the method achieves higher performance in terms of F1 measure than other methods studied on popular benchmarks. In addition, we show that a simple pre-training technique further improves the performance of the model.", "sections": [{"title": "1 Introduction", "content": "Aspect-Sentiment Triplet Extraction (ASTE, Peng et al., 2020) is a recent task in aspect-based sentiment analysis that involves the extraction of (aspect phrase, opinion phrase, sentiment polarity) triples from text. The aspect phrase denotes features or attributes of the described object, towards which the sentiment is expressed in the opinion phrase. The categorisation of this sentiment into typically three classes (positive/negative/neutral) is the final element of the triple. For example, in the sentence \"The hotel was very good\" there is only one ASTE triple (hotel, very good, positive). See Fig 1 for more examples.\nSince ASTE provides an answer to what? (aspect phrase), how? (sentiment), and why? (opinion phrase) questions regarding sentiment, it is sometimes referred to as a \"near complete solution\" to sentiment analysis (Peng et al., 2020) and has attracted considerable research attention. Several types of methods have been proposed, including sequence prediction (Xu et al., 2020), cascade processing (Li et al., 2021), prompting approaches (Zhang et al., 2021) or predicting a special word-by-word matrix (Wu et al., 2020). However, the approaches that currently achieve the best predictive performance are span-level approaches (Li et al., 2023b; Naglik and Lango, 2023), which are the focus of this work.\nSpan-level methods (Xu et al., 2021) typically begin by extracting all possible text spans up to a predefined length from a given text. Each span undergoes evaluation by a classifier to determine whether it contains an aspect phrase, an opinion phrase, or whether it should be excluded from further processing. The method then considers all possible pairs of identified opinion and aspect phrases, with a secondary classifier examining these pairs to discard false matches and assign sentiment polarity to the valid ones.\nAlthough several modifications of this scheme have been proposed (Chen et al., 2022; Li et al., 2023a; Liang et al., 2023), they share the same property: the prediction of ASTE triples consists of dozens of independent classifier decisions, and the dependencies between decisions regarding the analysed spans are not modelled. This limits the predictive performance of these techniques, as some task-related knowledge simply cannot be learned. Such unexploited properties of structured output include both deterministic rules and probabilistic patterns, some examples of which are given below (see more in App. G).\n\u2022 An opinion phrase assigned to multiple aspects, typically assign them the same sentiment polarity (see the 2nd example in Fig 1).\n\u2022 Two opinion phrases linked with a contrastive conjunction (like \"but\") and attached to one aspect phrase should have different sentiment polarities. A similar rule applies to opinions linked with correlative conjunctions (\"and\").\n\u2022 Although one-to-many relations between aspect and opinion phrases are possible, the general probabilistic property is that constructing an increasing number of triples with a given phrase should be less and less likely.\n\u2022 An aspect phrase should only be extracted if it is associated with an opinion phrase. For instance, consider the word \"room\" in \"The room was fine\" and \"I was given a single room\". In the first sentence, this word should be extracted because it forms part of a triple (see Fig. 1), whereas in the second sentence, there is no associated opinion phrase.\nNote that learning these properties requires joint modelling of the decisions to extract or link particular phrases. This is impossible to achieve in the current span-based ASTE frameworks, which often adapt end2end training but make a strong independence assumption and perform multiple independent classifier predictions.\nIn this paper, we address the challenge of modelling dependencies between the extracted phrases and between constructed triples by introducing ASTE-Transformer, a novel architecture for ASTE. Unlike conventional span-based ASTE approaches, our method does not perform multiple independent classifications to categorize extracted text spans into aspect/opinion/invalid phrases. Instead, aspect-opinion pairs are formed through a search in a specialized embedding space induced by modified self-attention mechanizm, where each span is represented twice: once as a potential aspect and once as a potential opinion phrase. Furthermore, our approach does not construct final triples through independent classifications without considering other candidate triples; instead, for each candidate triple, it produces a representation that depends on all other triples.\nThe contributions of this paper are as follows:\n\u2022 We propose ASTE-Transformer, a new architecture composed of three types of transformer-inspired layers, that enables modelling the dependencies between the extracted phrases and between the constructed aspect-opinion pairs.\n\u2022 To address the additional difficulty of training transformer models on relatively small ASTE datasets, we propose the simple idea of using pre-training on noisy supervised data that can be artificially generated from datasets for the more popular sentiment classification task.\n\u2022 We carry out a fairly extensive experimental evaluation of the newly proposed method on four standard English benchmarks and two datasets for a more under-resourced language: Polish. Ablation study and error analysis are also performed.\nThe experimental results demonstrated the superior predictive performance of ASTE-Transformer compared to other methods under study. Furthermore, the ablation study highlighted the importance of modelling dependencies, which accounted for improvements of up to 5 ppt on F1 score. Lastly, the proposed pre-training technique yielded additional, statistically significant performance improvements over previous state-of-the-art ASTE approaches."}, {"title": "2 ASTE-Transformer", "content": "The proposed method involves several processing steps, realised by three types of transformer-inspired layers: 1) standard transformer layers, 2) an aspect-opinion construction layer, and 3) a triple construction layer.\nFirst, the input sentence is processed by a masked language model (MLM) composed of standard transformer layers that produce an embedding representation for each token. Second, in line with span-based approaches, all text spans up to a certain length are extracted, and their corresponding embedding representations are constructed. Next, the spans are analysed by an aspect-opinion pair construction layer, which searches for corresponding aspect-opinion phrases. Finally, all candidate aspect-opinion pairs are processed by the triplet construction layer. This layer first computes a representation for each candidate pair, then a classifier assigns them a sentiment polarity or filters them out. Importantly, the constructed representation of an aspect-opinion pair depends on all the other candidate pairs.\nAll the above-mentioned steps are realized by a single neural architecture, consisting of three types of transformer-like layers. The network is trained in an end-to-end fashion, by optimizing a loss function measuring the quality of constructed triplets and loss functions of additional intermediary tasks. An overview of the proposed neural network is shown in Fig. 2, and each of its parts is described in the following sections.\nProblem\nformulation Given an input sentence \\(w_1, w_2, ..., w_n\\), construct a set of ASTE triples \\(\\{(a_i, o_i, y_i)\\}i=1,..,m\\) where \\(a_i = \\{w_j, w_{j+1}, ..., w_{j+|a_i|}\\}\\), \\(o_i = \\{w_k, w_{k+1}, ..., w_{k+|o_i|}\\}\\) are aspect and opinion phrases consisting of one continuous text span, and \\(y_i \\in \\{Positive, Negative, Neutral\\}\\) is the polarity of sentiment expressed by \\(o_i\\) towards the aspect mentioned in \\(a_i\\).\n2.1 Contextualized representation\nIn the first part of our architecture, a distributed representation for each word in the input sentence is constructed by a transformer-based masked language model (MLM). Recall that in the transformer layer, a key \\(k_i\\), value \\(v_i\\) and query \\(q_i\\) representations are constructed by fully-connected layers for each input word.\n\\(k_i = W_K w_i\\)\n\\(v_i = W_V w_i\\)\n\\(q_i = W_Q w_i\\)\nThen, according to the similarities between keys and queries (measured by the vector inner-product), a new word embedding \\(e_i\\) representation is constructed by a weighted sum of value vectors.\n\\(e_i = \\sigma(Q K^T) V\\)\nwhere \\(\u03c3\\) is the softmax function, K and V are matrices containing \\(k_i\\) and \\(v_i\\) vectors for all input words, respectively. Note, that the word representation \\(e_i\\) is dependent on all the input words \\(w_1, w_2, ..., w_n\\).\n2.2 Span constructor\nIn line with other span-based approaches, the next processing step is the extraction of all text spans up to a certain maximum length. For instance, with the maximum length of 3, the following spans would be extracted: {\\(w_1\\)}, {\\(w_1,w_2\\)}, {\\(w_1,w_2, w_3\\)}, {\\(w_2\\)}, {\\(w_2, w_3\\)}, etc. The representation of each extracted span \\(s_i\\) is constructed by max-pooling embeddings of words constituting it.\n\\(s_i = max\\text{-}pooling(e_j, e_{j+1}, ..., e_k)\\)\nwhere \\(s_i\\) is the representation of span starting from j-th word and ending at k-th word.\n2.3 Aspect-opinion pair construction layer\nTo match spans containing corresponding aspect-opinion phrases, we introduce a special transformer layer featuring a modified the attention mechanism that performs pair matching. This layer computes the distributed representations of each input span \\(s_i\\) as potential aspect phrase \\(a_i\\) and potential opinion phrase \\(o_i\\) through fully-connected layers:\n\\(A_i = W_A s_i\\)\n\\(O_i = W_O s_i\\)\nThese representations are subsequently used to align opinions with their corresponding aspects through a search process, which involves computing similarities between aspect and opinion vectors and matching those with similarities exceeding a predefined threshold \u03c4.\n\\(p_i = [a_i; o_j] = \\Phi_\\tau(A O^T)\\)\nwhere A, O are matrices containing vectors \\(a_i\\), \\(o_i\\) for all constructed spans and \\(\\Phi_\\tau()\\) is a thresholding operation, similar to attention masking, that from a given similarity matrix \\(S = A O^T\\) extracts the indices (i, j) of all the values \\(S_{i,j} > \\tau\\) above a threshold \\(\u03c4\\). The output of this layer is a set of extracted aspect-opinion pairs \\(p_i\\), represented as a concatenation of aspect and opinion spans.\nNote, that during the aspect-opinion pair construction, aspect and opinion phrases are considered jointly. This step lets us avoid initial categorization of spans into aspect and opinions phrases by a separate classifier applied multiple times."}, {"title": "2.4 Triplet construction layer", "content": "During the final processing step, each extracted pair \\(p_i\\) is either assigned a sentiment (positive, negative, or neutral) to create a triple or is dismissed as invalid. However, using a 4-class classification head for each pair \\(p_i\\) on its own does not account for the interdependencies among the aspect-opinion triples while making predictions. For example, as mentioned earlier, all triples with a given opinion phrase tend to have the same sentiment polarity. On the other hand, aspects with two opinions linked with contrasting conjunctions (like \"but\") will likely have opposite sentiments. It is not possible to model these and similar dependencies if the classification is performed completely independently.\nTherefore, the extracted aspect-opinion pairs \\(p_i\\) are processed jointly by an additional bidirectional transformer layer, which proved to be effective in modelling dependencies between the inputs for many tasks (Devlin et al., 2019). The input to this layer consists of representations of all extracted pairs \\(p_1, p_2, ..., p_N\\) without typically added positional encoding, since the prediction should not vary on the arbitrary order of extracted pairs. A classification head is then applied on top of the transformer layer with 4 classes: invalid, positive, negative, and neutral. Predicting one of the last three classes results in the construction of a (aspect, opinion, sentiment) triple.\nMore formally, for each input aspect-opinion pair \\(p_i = [a_i, o_j]\\), a new aspect-opinion embedding representation \\(e_i^{(p)}\\) is constructed:\n\\(k_i = W_K p_i\\)\n\\(v_i = W_V p_i\\)\n\\(q_i = W_Q p_i\\)\n\\(e_i^{(p)} = \\sigma(Q K^T) V\\)\nwhere \\(\u03c3\\) is the softmax function, K and V are matrices containing \\(k_i\\) and \\(v_i\\) vectors for all input aspect-opinion pairs, respectively. A softmax classifier then computes the final prediction using the constructed aspect-opinion embedding \\(e_i^{(p)}\\).\n\\(y_i = \\sigma(W e_i^{(p)})\\)\nNote that the aspect-opinion pair representation \\(e_i^{(p)}\\) depends on all input pairs \\(p_1, p_2, ..., p_n\\) returned by the aspect-opinion pair construction layer."}, {"title": "3 Training procedure", "content": "ASTE-Transformer model is trained via standard backpropagation in an end-to-end fashion. The training involves minimizing a composite loss function comprising the final ASTE loss, assessing the correctness of the constructed triples, along with two intermediate losses: the span selection loss and the aspect-opinion matching loss.\n\\(L = L_{ASTE} + L_{SpanSel} + L_{AO}\\)\nwhere \\(L_{ASTE}\\) is the final ASTE loss, \\(L_{SpanSel}\\) is the span selection loss, and \\(L_{AO}\\) is the aspect-opinion matching loss.\nSpan selection loss To facilitate the matching of correct aspect and opinion phrases, we added an intermediary task of predicting whether the text span \\(s_i\\) contains a valid aspect/opinion phrase. This is implemented as a simple binary task with valid/invalid outputs. Since the task suffers from heavy class imbalance, we applied Dice loss (Li et al., 2020) instead of standard cross-entropy:\n\\(z_i = \\sigma(w s_i + b)\\)\n\\(L_{SpanSel} = \\sum_{s_i \\in Spans(W_{1..n})} \\frac{2(1 - z_i)z_i \\delta_i + \\gamma}{(1 - z_i)^2 + z_i^2 + \\gamma}\\)\nwhere \\(Spans(W_{1..n})\\) generates all considered text spans, \\(z_i\\) is the estimated probability of the span \\(s_i\\) being valid, w, b are additional weights, \u03b1 = 0.7 is a scaling hyperparameter and \u03b3 = 1 is introduced for smoothing. The span representation \\(s_i\\) is constructed in the span constructor (see Sec. 2.2).\nAspect-opinion matching loss To promote the construction of a search space where correct aspect phrases are close to their corresponding opinion phrases, we apply a contrastive loss.\n\\(L_{AO} = \\sum_{a_i \\in A} \\frac{exp(a_o a_i)}{\\sum_{a_o \\in NegOpinions(a_i)} exp(a_o)} + \\sum_{o_i \\in O} \\frac{exp(o_a o_i)}{\\sum_{a_o \\in NegAspects(o_i)} exp(o a)}\\)\nwhere A, O are sets of all considered aspect and opinion phrases, \\(o_{a_i}\\) is the representation of a correct opinion phrase for \\(a_i\\), similarly \\(a_{o_i}\\) is the correct aspect phrase for \\(o_i\\). The negative examples for a given aspect/opinion \\(NegOpinions(a_i)\\) (\\(NegAspects(o_i)\\)) are constructed using hard mining, i.e. four closest incorrect phrases are selected.\nSince the aspect-opinion pair construction layer (Sec. 2.3) processes also incorrect aspect/opinion phrases, we intend to push them away from all the phrases of opposite type. In this case, we also hard mine negative examples for the denominator of the loss function but in the nominator we put a constant instead of an inner-product with a corresponding correct phrase (as such does not exist).\nASTE loss The construction of correct ASTE triples is enforced with the classification loss on the final \\(y_i\\) with four possible outputs: positive, negative, neutral and invalid. Due to the class imbalance of this task, the Focal loss (Lin et al., 2017) is applied instead of standard cross-entropy\u00b9.\n\\(L_{ASTE} = -(1 - y_i)^\\gamma ln(y_i)\\)\nwhere \\(y_i\\) is the probability of the correct class and \u03b3 = 2 is a scaling hyperparameter. During training, all correct triples (even if not selected by previous layers) are passed to the final transformer classifier to fully utilize the learning information."}, {"title": "4 Pretraining for ASTE", "content": "Transformer-based architectures are known to benefit from previous pretraining, especially when dealing with limited supervised data. In the proposed ASTE-Transformer, only the first part (MLM) is pre-trained using standard methods, while all subsequent layers are randomly initialized. Therefore, we propose a simple idea of generating abundant noisy ASTE data from sentiment classification (SC) datasets and employing them for pretraining purposes. Note that SC datasets are often much larger than ASTE datasets because they can be automatically collected from e-commerce platforms, where the consumer's overall product rating can be used as a proxy for opinion sentiment (Ni et al., 2019).\nThe first step of our method is to train ASTE-Transformer model on the original ASTE dataset and apply it to texts from the SC dataset to produce artificial annotations. As predicting incorrect sentiment polarity is a factor negatively affecting the performance of ASTE models (Yu et al., 2023), we substitute the sentiment polarity in the generated triples with the gold standard sentiment of the whole sentence as provided in SC dataset. Finally, we train a new ASTE-Transformer from scratch, starting from pre-training it on the set of pseudo-labelled data, and then combining it with gold standard ASTE data. The last few training epochs are performed on gold standard ASTE data only."}, {"title": "5 Experimental evaluation", "content": "5.1 Experimental setup\nDatasets To evaluate the predictive performance of ASTE-Transformer, we conducted computational experiments on four ASTE datasets commonly used in related work: 14res, 14lap, 15res,"}, {"title": "5.3 Ablation study", "content": "To verify the effectiveness of using a triplet representation that takes into account the dependencies between all candidate triples, we performed an ablation study where our triplet construction layer was replaced with a standard fully-connected layer. Both the results with and without pretraining are reported in Table 3."}, {"title": "5.4 Evaluation on other languages", "content": "In contrast to most related work, which only runs experiments on English, we also ran evaluations on two recent ASTE datasets for Polish (Lango et al., 2024). In all methods, MLM was replaced by Polish TrelBERT (Szmyd et al., 2023).\nThe results presented in Table 5 show that the ASTE Transformer obtained the highest F1 score on both datasets. As the texts in the Polish datasets contain on average more triples and a higher number of more difficult one-to-many relations (see App. A), we also report the results of our method without the final transformer layer modelling dependencies. As expected, we observe even more significant improvements compared to the ablation experiment on English."}, {"title": "5.5 Visualization of the search space", "content": "To better understand how the search mechanism works in the proposed aspect-opinion pair construction layer, we visualized the induced embedding space using PCA for randomly selected test instances. An example of such a visualization is shown in Fig. 3 and further examples can be found in App. D. In most of the visualizations, we observe that the representations of correct aspect and opinion phrases are close to each other. Moreover, the groups of invalid aspect and opinion phrases are clearly separated, placed at a considerable distance."}, {"title": "5.6 Error analysis", "content": "We investigate how specific components of our model affect the outcome by measuring several intermediate metrics: 1) the performance of an additional binary classifier trained to determine the validity of a span based on its representation si (Span binary), 2) the effectiveness of extracting correct aspect-opinion pairs in the aspect-opinion pair construction layer (A-O pair layer), 3) the classification performance of the final sentiment classifier for assigning sentiment to triples or filtering them (Final 4-class), and 4) the classification performance of the same classifier when tasked with recognizing valid/invalid triples in a binary manner (Final binary). The results are showcased in Tab. 6 and in App. F.\nWe observe that the span representation si computed in the span constructor layer already encodes the information whether the span is a valid aspect or opinion phrase, as the simple binary classifier was able to distinguish them with 84% F1 score. The pairs constructed by the aspect-opinion pair construction layer have high recall and only slightly lower precision. The final classifier has a high performance in discriminating between valid and invalid triples, but assigning sentiment polarity to the triples seems to be more challenging."}, {"title": "6 Related works", "content": "Since the introduction of Aspect Sentiment Triplet Extraction (ASTE) by Peng et al. (2020), various approaches were proposed for this task.\nJET (Xu et al., 2020) converts the problem into a sequence labelling task using enhanced BIOES tagging schema. Similarly, PBF (Li et al., 2021) uses three sequential tagging predictors to construct triplets. A different approach is to encode ASTE triples in a word-by-word matrix with Grid Tagging Scheme (GTS, Wu et al., 2020). Each element of this matrix is predicted by an independent classifier.\nIn contrast to GTS which relies on word-to-word interactions, Span-ASTE (Xu et al., 2021) considers all possible text spans from the input sentence, performing multiple independent classifications to construct the output. SBC (Chen et al., 2022) uses span representations constructed with a special separation loss and has bidirectional structure to generate aspect-opinion pairs. FTOP (Huang et al., 2021) divides the input sentence into opinion/aspect phrases using sequence prediction and considers all aspect-opinion pairings by a classifier. Recently, EPISA (Naglik and Lango, 2023) explored the possibility of making the predictions dependent while constructing ASTE triples. In contrast to our method, EPISA uses a 2-dimensional CRF over a decision matrix, which is intractable without making additional assumptions about pair-wise decision independence and approximating potential functions with Gaussian kernels.\nAnother group of approaches combines span-based and matrix prediction approaches by predicting a matrix with span-based tags. Such approaches include STAGE (Liang et al., 2023) and SimSTAR (Li et al., 2023a). Both these approaches predict matrices by applying softmax classifiers independently. Finally, Zhang et al. (2021) presented a generative approach called GAS, which uses prompting of the T5 language model. Some of the generative methods also use contrastive learning to learn better representations (Yang et al., 2023), but they do not use search to pair aspects with opinions.\nA data augmentation technique for ASTE was proposed in (Zhang et al., 2023), but in comparison to our simple pretraining idea, it is rather complex as it employs reinforcement learning and trains additional generator and discriminator models."}, {"title": "7 Summary", "content": "In this paper, we have demonstrated the potential of exploiting dependencies between constructed triples in span-based ASTE approaches. The proposed ASTE-Transformer method showed superior predictive performance on both English and Polish benchmarks. Additionally, a simple pre-training scheme proved to further improve the performance."}, {"title": "G Characteristics of ASTE problems requiring dependency modeling", "content": "The following is an extended, but still non-exhaustive, list of properties unexplored by previous span-level ASTE approaches. All these properties have a common feature: they cannot be exploited due to the strong independence assumption made by span-level ASTE approaches when constructing the results. The proposed method, ASTE-Transformer, addresses this by relaxing this assumption and modelling the dependencies between the output triples.\n\u2022 A phrase should be of the same type in all triples. For instance, if a given phrase is an aspect phrase then in other triples it can not be an opinion phrase.\n\u2022 An opinion phrase assigned to multiple aspects, typically assign them the same sentiment polarity (see the 2nd example in Fig 1).\n\u2022 Two opinion phrases linked with a contrastive conjunction (like \"but\") and attached to one aspect phrase should have different sentiment polarities. A similar rule applies to opinions linked with correlative conjunctions (\"and\").\n\u2022 Construction of a triple with a given aspect/opinion phrase should invalidate triples with overlapping phrases. For example, if the model constructed the correct triple with \"extremely pricy\" when processing the second sentence in Fig. 1, then all candidate triples with \"pricy\" should be discarded. The same is true for multi-word aspect phrases.\n\u2022 Although one-to-many relations between aspect and opinion phrases are possible, the general probabilistic property is that constructing an increasing number of triples with a given phrase should be less and less likely.\n\u2022 A potential aspect phrase should only be extracted if there is an opinion phrase associated with it. For example, consider the aspect word \"room\" in \"The room was fine...\" and \"I was given a single room\". In the first sentence this word should be extracted because it is part of a triple (see Fig 1), but in the second sentence there is no opinion phrase attached to it."}]}