{"title": "COLLAGE: Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models", "authors": ["Divyanshu Daiya", "Damon Conover", "Aniket Bera"], "abstract": "We propose a novel framework COLLAGE for generating collaborative agent-object-agent interactions by leveraging large language models (LLMs) and hierarchical motion-specific vector-quantized variational autoencoders (VQ-VAEs). Our model addresses the lack of rich datasets in this domain by incorporating the knowledge and reasoning abilities of LLMs to guide a generative diffusion model. The hierarchical VQ-VAE architecture captures different motion-specific characteristics at multiple levels of abstraction, avoiding redundant concepts and enabling efficient multi-resolution representation. We introduce a diffusion model that operates in the latent space and incorporates LLM-generated motion planning cues to guide the denoising process, resulting in prompt-specific motion generation with greater control and diversity. Experimental results on the CORE-4D, and InterHuman datasets demonstrate the effectiveness of our approach in generating realistic and diverse collaborative human-object-human interactions, outperforming state-of-the-art methods. Our work opens up new possibilities for modeling complex interactions in various domains, such as robotics, graphics and computer vision.", "sections": [{"title": "I. INTRODUCTION", "content": "Modeling human-like agent-object interactions is fundamental in the vision community, enabling applications in gaming, embodied AI, robotics, and VR/AR. While recent works have explored single-person and multi-human object interactions in non-collaborative settings [1]-[6], generating collaborative human-object-human interactions remains largely unexplored. This task requires a complex understanding of human actions and object interactions, as guiding individual agents along with the task involves extensive planning. Given the lack of rich datasets, training a generalized model is challenging. To address this, we propose incorporating the knowledge and reasoning abilities of large language models (LLMs) to guide a generative diffusion latent diffusion model for multi-human-object motion generation in collaborative settings. In the remainder of this paper, we will use the terms 'human' and 'agent' interchangeably, with the specific application determining the appropriate usage. For robotics applications, 'agent' may refer to either a real human or a robotic, human-like entity such as a humanoid.\nPre-trained LLMs, such as GPT-4 [7] and Llama 2 [8], have demonstrated emergent capabilities in reasoning, planning, and motion planning [9]\u2013[12]. We hypothesize that LLMs could provide a general and domain-independent approach to modeling and planning interactive multi-human object and human-object-human task collaboration, given proper learning approaches. Learning to plan without a dataset can help with motion planning in outdoor settings, where currently, no dataset exists with extensive motion capture data. Utilizing humanoid robots in such settings is a significant hurdle, and effective use of planning via LLMs for fine-grained motion generation could help with humanoid-based motion and interaction in outdoor environments\u00b9 (Fig. 1). To capture the complex motion dynamics in collaborative settings, we propose a hierarchical motion-specific vector-quantized variational autoencoder (VQ-VAE) architecture that explicitly captures different motion-specific characteristics at different levels of abstraction, addressing the limitations of previous VQ-VAE models [13]\u2013[15]. We incorporate a diffusion model [14], [16], [17] for learning human motion in the latent space and propose a novel architecture to incorporate multi-human object interactions. We augment textual planning cues from LLM with codebook-based associations learned via VQ-VAE training, helping the diffusion model better learn to model according to the task description, given the complex interaction setting and"}, {"title": "II. RELATED WORK", "content": "Text-Conditioned Human Motion Generation. Generating human motions based on textual descriptions has been a recent research focus. Early approaches generated motions based on action categories [20]\u2013[23], past motions [24]\u2013[29], trajectories [30]\u2013[34], and scene context [35]\u2013[45]. Recent works have enabled direct generation of human motions from textual inputs [14], [46]\u2013[66], extending to multi-person [67]\u2013[69] and human-scene interactions [41], [70], [71]. However, generating collaborative human-object-human interactions remains largely unexplored.\nHuman-Object Interaction Generation. Modeling realistic human-object interactions is challenging due to the complexity of capturing both human motions and object dynamics. Prior research has addressed hand-object interactions [72]\u2013[76], single-frame human-object interactions [77]\u2013[82], and zero-shot settings [83]\u2013[85]. Recent studies have explored whole-body dynamic interaction generation through kinematic-based [4], [86]\u2013[102] and physics-based methods [103]\u2013[114], but often suffer from limitations such as a narrow scope of actions, static objects, or lack of comprehensive whole-body motion representation.\nCollaborative Multi-Human Interaction Modeling. Collaborative human-object-human interactions remain largely unexplored, despite the study of multi-human interactions in non-collaborative contexts [6]. The complexity arises from modeling intricate coordination between multiple humans and objects, requiring advanced planning and understanding of collective actions. Recent datasets and baselines, such as CORE-4D [18], have begun to address this gap, but further research is needed to develop models capable of handling such complex interactions.\nUtilizing LLMs in Motion Generation. Large language models (LLMs) have demonstrated remarkable abilities in reasoning [9], planning [10], and task execution [11]. In the realm of digital humans, LLMs have been employed to guide motion generation [51], [115]\u2013[118]. Our approach extends this line of work by utilizing LLMs to guide the generation of collaborative human-object-human interactions.\nHierarchical VQ-VAE and Diffusion Models in Motion Generation. Vector-Quantized Variational Autoencoders (VQ-VAEs) have been used to create quantized motion latent spaces [13], [14], [117], but may struggle with complex and diverse motion generation due to limitations like small codebook, as increasing the codebook size for incorporating the complex dataset leads to issue of codebook collapse, as we observed while generalizing [14] for multi human setting, further smaller codebooks even in single human setting results in less diverse motion. Hierarchical architectures have been proposed to enhance motion modeling [15]. Diffusion models have also been employed for learning human motion in latent spaces [14], [16], [17]. Our approach incorporates a hierarchical motion-specific VQ-VAE architecture and a diffusion model guided by LLM-generated plans to effectively generate collaborative human-object-human interactions."}, {"title": "III. METHODOLOGY", "content": "Modeling complex human-object interactions necessitates capturing motion dynamics at multiple levels of abstraction, from high-level trajectories and interaction types to low-level limb movements and object manipulations. To achieve this, we propose a hierarchical Vector Quantized Variational Autoencoder (VQ-VAE) that incorporates description cues provided by a Language Model (LLM) at each level of abstraction. This architecture enables the model to learn disentangled motion representations corresponding to different semantic concepts guided by hierarchical textual cues.\nOur hierarchical VQ-VAE architecture captures motion dynamics at multiple levels of abstraction, as shown in 2. The encoders at each level map the inputs to latent representations, which are then quantized using codebooks. The decoders reconstruct the original data from the quantized latent representations. At each level $l$, the encoder for human $i$ computes $Z_H^{(l)} = E_H^{(l)}(Z_H^{(l-1)};\\theta^{(l)})$, where $E_H^{(l)}$ is a neural network with parameters $\\theta^{(l)}$, and $Z_H^{(0)} = X_i$ is the input sequence for human $i$. Similarly, the object encoder computes $Z_O^{(l)} = E_O^{(l)}(Z_O^{(l-1)};\\theta^{(l)})$, with $Z_O^{(0)} = Y$. We incorporate description cues $e^{(l)}$ provided by an LLM at each level $l$, which are integrated into the encoder by augmenting the latent representations: $\\tilde{Z}_H^{(l)} = Concat(Z_H^{(l)}, e_H^{(l)})$, $\\tilde{Z}_O^{(l)} = Concat(Z_O^{(l)}, e_O^{(l)})$, where $e_H^{(l)}$ and $e_O^{(l)}$ are the description embeddings for humans and objects at level $l$, respectively."}, {"title": "IV. EXPERIMENTATION AND RESULTS", "content": "a) Implementation Details: Our model consists of a hierarchical VQ-VAE with $L = 6$ levels, each with a codebook size of 512 x 512 (latent dimension 512) and two Conv1D blocks per level per entity (kernel size 3, residual connections), similar to T2M-GPT [13]. Vector quantization is performed using the straight-through estimator, and hierarchical planning cues are generated via GPT-4 [7] and embedded using CLIP ViT-B/32 [127], associated with the VQ-VAE codebooks through contrastive learning (temperature $\\tau = 0.07$, top $u$ = 8 latent codes per level). The latent diffusion model is based on a U-Net architecture with $M = 4$ Motion Modeling Blocks (MM-Blocks), each consisting of Temporal Convolutional Networks (TCNs) with kernel sizes {3,5,7} [121] and Graph Attention Networks (GATs) with 8 attention heads [122], capturing spatio-temporal dependencies.For training, we use the Adam optimizer [128] for VQ-VAE with a learning rate of $1 \\times 10^{-4}$ and AdamW [129] for the diffusion model with a learning rate of $2 \\times 10^{-4}$, both with cosine annealing, gradient clipping (max norm 1.0), and weight decay of $1 \\times 10^{-5}$. For CORE-4D [18], we train for 50K iterations with a learning rate of $2 \\times 10^{-4}$ and an additional 30K iterations with a reduced learning rate of $1 \\times 10^{-5}$. For InterHuman [19], we train for 200K iterations at $2 \\times 10^{-4}$ and 100K iterations at $1 \\times 10^{-5}$. We use a batch size of 256 for both datasets and apply the Adam optimizer with $[\\beta_1,\\beta_2] = [0.9,0.99]$ and an exponential moving constant $\\lambda = 0.99$. Loss terms include $\\lambda_{recon} = 1.0$, $\\lambda_{commit} = 0.25$ per level, $\\lambda_{codebook}^{(l)} = 0.25$ per level, $\\lambda_{align}^{(l)} = 0.5$ per level, $\\lambda_{smooth} = 0.1$ [51], $\\lambda_{penetration} = 10.0$ [72], and $\\lambda_{contact} = 5.0$. The hierarchical disentanglement loss is weighted by $\\lambda_{disent} = 1.0$. The diffusion model uses 1000 diffusion steps and we test for 5, 15, 55, 100 DDIM [130] sampling steps during inference. The hierarchical cue modulation function applies exponential decay for high-level cues and increasing influence for low-level cues across diffusion steps\nWe train COLLAGE on the CORE-4D dataset [18], which contains 998 motion sequences of human-object-human interactions spanning 5 object categories. We annotates the motion sequences with textual descriptions, the annotated text-motion dataset has an average length of 8.54 words, totaling 8,542 words. We split the dataset into training, validation, and test sets with a ratio of 0.8, 0.05, and 0.15, respectively. We also evaluate our model on the InterHuman dataset [19] for multi-human generation, which includes 6,022 motions with 16,756 unique descriptions. We use same train/test formulation as [19]. We additionally also train our model for single human motion generation on KIT-ML [131] and HumanML3D [132] Dataset, the visualisations and comparisons are available in attached video.\nb) Evaluation Metrics: For text-conditioned generation on CORE-4D and InterHuman, we adopt the metrics from InterGen [19]: (1) FID, (2) R-Precision, (3) Diversity, (4) Multimodality (MModality), and (5) MM Dist. For additional tasks on CORE-4D, we follow their own metrics [18]: (1) RR.Je, (2) RR.Ve, and (3) Cacc. All evaluations are run 20 times (except MModality, 5 times) with average results reported with a 95% confidence interval. For detailed descriptions of these metrics, we refer readers to [18], [19].\nc) Baselines: For text-conditioned generation on the CORE-4D dataset, we compare COLLAGE against state-of-the-art methods, including TEMOS [48], T2M [124], MDM [125], MDM-GRU [125], [133], ComMDM [126], and InterGen [19]. We modify these models to handle two-person interactions and train them on the CORE-4D dataset. For the additional tasks on the CORE-4D dataset, we compare against MDM [125], a one-stage motion diffusion model, and OMOMO [18], a two-stage approach for object-conditioned human motion generation."}, {"title": "A. Results", "content": "1) Text-Conditioned Generation:\na) Results on CORE-4D: Table I (left) presents the results of text-conditioned generation on the CORE-4D dataset. COLLAGE outperforms all baselines across most metrics, achieving the highest R-Precision scores, lowest FID, and best diversity. The hierarchical VQ-VAE effectively captures multi-scale motion dynamics, while the LLM-guided diffusion model generates motions that align well with the textual descriptions. The incorporation of hierarchical planning cues enables COLLAGE to generate more coherent and diverse interactions compared to the baselines.\nb) Results on InterHuman: We further evaluate COLLAGE on the InterHuman dataset for multi-human generation. Table I (right) shows the comparison with state-of-the-art methods. COLLAGE achieves superior performance across nearly all metrics, demonstrating its effectiveness in generating diverse and realistic multi-human interactions. The hierarchical modeling of motion dynamics and the incorporation of LLM-guided planning enable COLLAGE to better capture the complexities of human-human interactions compared to the baselines.\n2) Object-Conditioned Generation on CORE-4D: We evaluate COLLAGE on the task of object-conditioned human motion generation on the CORE-4D dataset. Given an object geometry sequence, the goal is to generate two-person collaboration motions using the SMPL-X model [134]. Table II presents the quantitative results, comparing COLLAGE with MDM [125] and OMOMO [18]. COLLAGE achieves the lowest joint and vertex position errors, highest contact accuracy, and best motion quality (FID) on both test sets (S1 and S2). The hierarchical modeling and LLM guidance enable COLLAGE to generate more precise and realistic human-object interactions compared to the baselines.\n3) Ablation Studies: We conduct ablation studies on the CORE-4D dataset to validate the effectiveness of the proposed components in COLLAGE. Table I (bottom) presents the results. Removing the hierarchical structure in the VQ-VAE (w/o Hierarchy) significantly drops performance across all metrics, highlighting the importance of modeling motion dynamics at multiple scales. Removing LLM guidance (w/o LLM) also decreases performance, demonstrating the effectiveness of incorporating hierarchical planning cues. Replacing time-dependent modulation with a fixed weighting scheme (w/o Time Modulation) degrades performance, indicating the benefit of adaptively adjusting the influence of planning cues over diffusion steps. These studies confirm that the hierarchical VQ-VAE, LLM guidance, and time-dependent"}, {"title": "V. DISCUSSION AND LIMITATIONS", "content": "The experimental results demonstrate the effectiveness of COLLAGE in generating realistic and diverse collaborative human-object-human interactions. The hierarchical VQ-VAE architecture captures motion dynamics at multiple scales, while the LLM-guided diffusion model generates motions that align well with textual descriptions and planning cues. The incorporation of hierarchical planning cues from the LLM allows for more coherent and controllable generation, as evidenced by COLLAGE's superior performance across various metrics and datasets.\nHowever, there are some limitations to our approach. First, COLLAGE does not explicitly model the physical interactions between humans and objects, relying on learned motion priors to generate plausible interactions. Incorporating explicit physics modeling could further improve the realism and consistency of the generated motions. Second, the current approach generates motions from scratch based on input text and object geometry, but does not allow for fine-grained editing or control over specific aspects of the motion. Extending the model to support motion editing and user-guided refinement could enhance its practical utility.\nDespite these limitations, COLLAGE represents a significant step towards generating realistic and diverse collaborative human-object-human interactions. Our approach can be seamlessly extended to generate collaborative interactions between humanoid robots and objects. By training our model on motion data from humanoid robots, we can generate realistic and diverse interactions that mimic human-like behaviors. This extension has significant implications for the deployment of humanoid robots in various real-world scenarios, where they are expected to collaborate with objects and other agents in a human-like manner.\nThe proposed approach opens up new possibilities for applications in robotics, virtual reality, and computer graphics, where generating plausible and coordinated multi-agent interactions is crucial. Future work could explore incorporating explicit physics modeling, supporting motion editing and user control, and extending the approach to handle a larger variety of objects and interaction scenarios."}]}