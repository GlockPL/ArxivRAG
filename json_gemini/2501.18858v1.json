{"title": "BRITE: Bootstrapping Reinforced Thinking Process to Enhance\nLanguage Model Reasoning", "authors": ["Han Zhong", "Yutong Yin", "Shenao Zhang", "Xiaojun Xu", "Yuanxin Liu", "Yifei Zuo", "Zhihan Liu", "Boyi Liu", "Sirui Zheng", "Hongyi Guo", "Liwei Wang", "Mingyi Hong", "Zhaoran Wang"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes LLM reasoning through a novel graphical model incorporating latent thinking processes and evaluation signals. Within this framework, we introduce the Bootstrapping Reinforced Thinking Process (BRITE) algorithm, which works in two steps. First, it generates high-quality rationales by approximating the optimal thinking process through reinforcement learning, using a novel reward shaping mechanism. Second, it enhances the base LLM by maximizing the joint probability of rationale generation with respect to the model's parameters. Theoretically, we demonstrate BRITE's convergence at a rate of 1/T with T representing the number of iterations. Empirical evaluations on math and coding benchmarks demonstrate that our approach consistently improves performance across different base models without requiring human-annotated thinking processes. In addition, BRITE demonstrates superior performance compared to existing algorithms that bootstrap thinking processes use alternative methods such as rejection sampling, and can even match or exceed the results achieved through supervised fine-tuning with human-annotated data.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMS; OpenAI, 2023; Anthropic, 2023; Team et al., 2024a), have emerged as a breakthrough in artificial intelligence, demonstrating unprecedented capabilities in natural language process-ing and generation. The training pipeline of these state-of-the-art models consists of two critical phases: pre-training and post-training. During the pre-training phase, LLMs learn from vast datasets to predict subsequent tokens in sequences, enabling them to learn extensive linguistic patterns, contextual understand-ing, and general world knowledge. The post-training phase further refines these models through two stages: Supervised Fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022). Recent research (OpenAI, 2024) has shown that by scaling the inference time, these models demonstrate sophisticated reasoning capabilities, particularly in domains such as mathematics and programming.\nUnlocking LLM reasoning abilities typically relies on structured prompting methods that break down problems into step-by-step solutions, known as chain-of-thought (CoT) reasoning (Wei et al., 2022). While this approach has shown promise and inspired various extensions (Wang et al., 2022; Yao et al., 2024), the fundamental challenge of reasoning reliability remains unresolved. Generated rationales often lack logical completeness or validity, with their quality heavily dependent on task-specific prompting strategies. Recent developments in inference-time scaling techniques (Snell et al., 2024) have shown potential improvements."}, {"title": "1.1 Related Works", "content": "Reasoning in LLMs. Prior work has explored various prompting techniques to enhance language model reasoning capabilities. CoT prompting (Wei et al., 2022) has emerged as a particularly effective approach, encouraging models to break down complex problems into intermediate steps by demonstrating step-by-step reasoning paths. The following works, such as Zhou et al. (2022); Yao et al. (2024), design more prompting techniques to enhance the model's capacity. However, these methods typically rely on manually crafted (COT) prompts to elicit reasoning processes, which are then used to guide the model's generation process. While such approaches have shown promising results in improving model performance across various reasoning tasks, they remain dependent on human-designed prompting templates and may not fully capture the natural reasoning patterns that emerge during model inference. To this end, a line of works aims to boost the latent reasoning process quality or even achieve automatic reasoning process generation, where the latter one is our"}, {"title": "1.2 Notations", "content": "For any space X, we denote \u2206(X) as the set of distributions over X. For any positive integer h, we denote\nthe sequence {a\u2081,\u2026\u2026,ah} by a1:h. We use 1{\u00b7} to denote the indicator function."}, {"title": "2 Preliminaries", "content": "In this section, we present the single-step bandit formulation and the multi-step Markov decision process (MDP) formulation for LLMs.\nBandit Formulation of LLMs. A simple way to understand LLMs is through the bandit formulation. In this context, the prompt and the response are represented as x \u2208 X and y \u2208 V, respectively. Here, X refers to the set of prompts, while y represents the set of responses. The LLM corresponds to the policy in this bandit framework, where \u03c0(y | x) indicates the probability of generating the response y given the prompt x.\nMDP Formulation of LLMs. Following the notations in Zhong et al. (2024), we consider an MDP M = (S,A,P, r, p, H). In this framework, S and A represent the state and action spaces, respectively. The transition kernel is denoted by P: S \u00d7 A \u2192 \u2206(S), while r indicates the reward function. The initial distribution is defined by p\u2208 \u0394(S), and H specifies the horizon length. A policy \u03c0: S \u2192 A(A) maps a state to a distribution over the action space. Initially, an initial state is sampled using s\u2081 ~ p. At the h-th step, the agent receives the state sh and chooses the action ah ~ \u03c0(\u00b7 | Sh). This interaction continues until a specified ending condition is met, which will occur within H steps.\nIn the context of generating large language models (LLMs), let s1 ~ prepresent the prompt x ~ p. At each step h, the state sh = ($1,A1:h\u22121) consists of the prompt x and all tokens generated up to that point. The LLM acts as a policy that maps sh to a distribution over the action ah ~ \u03c0(\u00b7 | sh), where the action signifies a token (or a series of consecutive tokens). The transition process is deterministic; it simply concatenates sh = ($1,01:h-1) and an to create a new state sh+1 = (81,01:h). The generation process concludes with a special end-of-sentence token Eos, which will be generated within H steps. For simplicity, we consider the length-H trajectories {(sh,ah)}=1, noting that this does not lose generality since we can pad the Eos token to the text to reach length H. With this notation and recognizing the autoregressive nature of LLMs, for any realizable trajectory {(sh, ah)}=1, the generation probability is given by \u03c0(\u03b11:H | 81) = \u03a0_{h=1}^{H} \u03c0(ah | 81, A1:h\u22121).\nRegularized Value Functions. For a policy \u03c0, its entropy-regularized value function is defined as\nV^\u03c0 (s; r) = E_\u03c0 [\u2211_{h=1}^{H}(r(sh, ah) \u2013 \u03b2\u00b7 log \u03c0(an |sh) ]\nS1 = S  (2.1)\nwhere \u03b2 > 0 is a regularization parameter. The regularized Q-function Q^\u03c0 of a policy \u03c0 is related to the regularized value function V^\u03c0 as\nQ^\u03c0 (s, a; r) = r(s, a) + E_{s'\u223cP(\u00b7|s,a)} [V^\u03c0 (s';r)], V^\u03c0 (s; r) = E_{a\u223c\u03c0(\u00b7|s)}[\u2212Blog \u03c0(a|s) + Q^\u03c0 (s, a; r)],  (2.2)\nThe regularized optimal policy \u03c0* is the policy that maximizes the regularized value function defined in (2.1), and its corresponding optimal Q-function and value function are denoted as Q* and V*, respectively. By (2.2), it can be shown that\nV^* (s; r) = log \u2211_{\u03b1\u2208A} exp (Q^*(s, a;r)), \u03c0^*(a | s) = exp{(Q^*(s, a; r) \u2013 V^*(s;r))/\u03b2} x exp (Q^*(s, a;r)). (2.3)"}, {"title": "3 Unified Framework and Generic Algorithm", "content": "In this section, we present a new framework for LLM reasoning and our generic algorithm within this frame-work.\n3.1 LLM as A Probabilistic Graphical Model\nWe consider four different spaces: X represents the prompt space, Z denotes the latent space that captures the intrinsic thought process (CoT), y signifies the response space, and O stands for the evaluation signal space, which reflects the optimality of the prompt-latent-response tuple. Furthermore, for any (x, z, y, o) \u2208 X\u00d7Z \u00d7 Y \u00d7 O, we describe the generation process using the probabilistic graphical model illustrated in Figure 1. This indicates that\nP(z, y, o|x, \u03b8) = P(z, y|x, \u03b8) \u00b7 P(o | x, z, y) = P(z|x, \u03b8) \u00b7 P(y|x, z, \u03b8) \u00b7 P(o | x, z, y), (3.1)\nwhere \u03b8 is the parameter of the LLM that guides the generation process. First, a latent variable z is generated from the distribution P(\u00b7|x,\u03b8), and then the response y ~ P(\u00b7|x,z,\u03b8) is produced based on both the prompt x and the latent variable z. Importantly, the probability P(o|x,z,y) is independent of the LLM parameterized by \u03b8, as we assume there exists a ground-truth judgment for the triplet (x, z, y), such as a ground-truth/human reward function. Unlike traditional LLM frameworks that only consider the prompt space X and output space y, our framework incorporates both a latent thinking process space and an observation space. These additional components are crucial for mathematically understanding how to improve the quality of thinking processes using evaluation signals.\nUnder this probabilistic graphical modeling of LLMs, our learning objective is to maximize\nL(\u03b8) = log P(z \u2208 Z, y \u2208 Y, o \u2208 O|x, \u03b8), (3.2)\nwhere Z \u2286 Z, Y \u2286 Y, and O \u2286 O denote the subsets of spaces representing the latent thinking process, response, and evaluation signals, respectively. In Section 3.2, we develop a general optimization algorithm that works with any choice of these spaces (X, Y, O). Subsequently, in Section 3.3, we show how this frame-work unifies existing learning approaches by demonstrating how different choices of these spaces (X,Y, O) correspond to various established learning paradigms and algorithms."}, {"title": "3.2 Bootstrapping Reinforced Thinking Process", "content": "We propose the algorithm, Bootstrapping Reinforced Thinking Process (BRiTE), to maximize the objective (3.2) within the framework proposed in the previous subsection. Since this objective may be difficult to optimize directly, we rewrite it as\nL(\u03b8) = log P(z \u2208 Z, y \u2208 Y, o \u2208 O|x, \u03b8)\n= log \u03a3_{(z,y,o) \u2208X\u00d7Y\u00d7 O}  P(z, y, o|x, \u03b8)\n= max_{Q(\u00b7,\u00b7,\u00b7|x,\u03c8)} {\u03a3_{(z,y,o) \u2208X\u00d7Y\u00d7 O} log P(z, y, o|x, \u03b8) \u00b7 Q(z, y, o|x, \u03c8) \u2013 \u03a3_{(z,y,o) \u2208X\u00d7Y\u00d7 O} log Q(x, y, o|x, \u03c8) \u00b7 Q(z, y, o|x, \u03c8) }, (3.3)\nL_\u03c8(\u03b8)\nwhere Q(\u00b7,\u00b7,\u00b7|x,\u03c8) can be regarded as another LM parametrized by \u03c8 and the last equality follows the following lemma:\nLemma 3.1. For any set W and non-negative numbers {Pw \u2265 0}w\u2208w, it holds\nlog \u03a3_{w\u2208W} Pw = max_{\u03a6\u2208\u0394 (W)}  \u03a3_{w\u2208W} [log Pw - log q(w)].\nThe maximum is achieved when q(w) = Pw/(\u03a3_{w'\u2208w} Pw')."}, {"title": "3.3 Connections to Existing Learning Paradigms and Algorithms", "content": "Example 3.4 (Pre-training, SFT and Conditional SFT). We first set aside the latent space and observation space, meaning that Z = O = \u2205. In this case, our learning objective (3.2) encompasses two key processes: (I) pre-training, where we let x represent the prompt and Y the next token; and (II) supervised fine-tuning (SFT), where we define Y = {y*(x)} as the expert response corresponding to the prompt x. Moreover, for the prompt-response pair (x,y) and singleton space Y = {y}, we consider Z = R+ and X = {R(x,y)} be the reward corresponding to the prompt-response pair, then our objective recovers the conditional SFT (Lu et al., 2022; Dong et al., 2023b; Yang et al., 2024).\nExample 3.5 (RLHF: PPO and DPO). We choose Y = Y and Z = Z as the complete response space and latent space, respectively. Additionally, we set O = {0,1}, where 1 indicates optimality and 0 indicates non-optimality, respectively. Since our goal is to maximize the probability of observing the signal of optimality, we focus on O = {1}. We also assume that P(o = 1 | x, z, y) = exp(R(x, z, y)/\u03b2) for some reward function R and \u03b2 > 0. With these choices, we have\nL_\u03c8(\u03b8) = \u2211_{(z,y)\u2208ZxY} log P(z, y, 1|x, \u03b8). Q(x, y, 1|x, \u03c8) \u2013 \u2211_{(z,y)\u2208ZxY} log Q(x, y, 1 | x, y) \u00b7 Q(z, y, 1|x, \u03c8)\n=[E_{(x,y)~Q(\u00b7,\u00b7|x;\u03c8)}R(x, z,y)- \u03b2 log (P(z, y|x, \u03b8) / Q(z, y | x, \u03c8) ]\nwhere Q(z,y|x, \u03c8) = \u2211_{o\u2208o}Q(z, y, o|x,y) = Q(z,y,1|x,y). This expression recovers the proximal policy optimization (PPO; Schulman et al., 2017) for RLHF (Christiano et al., 2017; Ouyang et al., 2022), where \u03c8 represents the LLM being optimized, and \u03b8 stands for the reference policy. Assuming that the preference data {(x,z+,y+,z-,y-)} is drawn from the Bradley-Terry (BT) model (Bradley and Terry, 1952), with (z+,y+) denoting preferred data and (z-,y-) indicating dispreferred data, one can follow Rafailov et al. (2024) to derive the latent direct preference optimization (latent DPO) objective:\nL_{latent-DPO} = \u03c3(log Q(z+, y+|x, \u03c8) / P(z+, y+|x, \u03b8) - Blog Q(z-, y-|x, \u03c8) / P(z-, y-|x, \u03b8) ) , (3.6)\nwhere \u03c3 is the sigmoid function. In contrast to the standard DPO objective in Rafailov et al. (2024), the objective in (3.6) additionally incorporates the latent variable z.\nExample 3.6 (Rejection Sampling EM Methods). Let Y = Y represent the complete response space, and define O = {0,1}, where 1 indicates the optimal outcome and 0 indicates otherwise. We focus on the optimal"}, {"title": "3.4 Practical Implementation: The Power of Reinforcement Learning", "content": "We have demonstrated that our generic algorithm encompasses a wide range of existing learning paradigms and algorithms, with a provable convergence guarantee. In this section, we carefully examine the practicality of our algorithm.\nFirst, the \u03b8-updating step is relatively straightforward to implement, as this step simply maximizes the predicted probability of the next tokens after we sample (z, y, o) from Q(z, y, o|x, Vt+1). In contrast, the \u03c8-updating step can be challenging in certain contexts. For instance, when Y = {y} represents the response corresponding to x, O = {1} indicates the optimality signal of interest, and Z = Z, we have Q(z, y, 1|x, \u03b8) x P(x, y, 1|x, \u03b8) = P(z | x, y, 1, \u03b8)\u2014the posterior of the latent variable z. Intuitively, obtaining this distribution requires us to identify the ideal latent (CoT) based on the pair (x, y), which represents an intractable posterior.\nTo achieve this goal, we aim to use RL to train an LLM that characterizes the distribution Q(z, y, o|x, Vt+1) or P(z, y, o|x, \u03b8t) in (3.4). Since an LLM acts as the policy of an MDP, our approach involves two main steps: (i) constructing an MDP whose optimal policy matches the distribution that we need to learn; and (ii) applying RL algorithms to solve this MDP and identify the optimal policy to learn the desired distribution. These two steps convert the challenging sampling problem to a more amenable RL optimization problem. Notably, the main challenge we face is reward shaping, which involves designing appropriate reward functions to ensure that the optimal policy aligns with the intended LLM that accurately represents the posterior. Our approach to reward shaping is based on the following proposition, which characterizes the optimal policy for deterministic entropy-regularized MDPs.\nProposition 3.7. Assuming the transition dynamic of entropy-regularized MDP is deterministic, then for any trajectories {(si, ai)}^{H}_{h} satisfying aH = EOS, we have\n\u03c0^*(ah U {(Si, ai)}^{H}_{i=h+1} | Sh) = [\u03c0^* (ai | Si) x exp{\u2211_{i=h}^{H}r(Si, Ai)} ]"}, {"title": "4 Experiments", "content": "In this section, we systematically demonstrate that our unified algorithm enhances the reasoning capability of LLMs.\n4.1 Experimental Setups\nTasks and Datasets. To evaluate mathematical reasoning capabilities, we conduct experiments on two prominent benchmarks: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). GSM8K contains 1,319 high-quality grade school math problems requiring multi-step reasoning, ranging from basic arithmetic to elementary algebra. The MATH dataset comprises 5,000 competition-level problems covering advanced topics such as algebra, geometry, number theory, probability, and calculus. These problems require more sophisticated problem-solving strategies and formal mathematical reasoning. The training sets of GSM8K and MATH datasets each contain approximately 7,500 data points. Each data point includes a question x, along with human-annotated rationale z* and the correct final answer y*.\nBase Models. We use several open-source instruction-tuned LLMs as base models, including Gemma-2-9b-it (Team et al., 2024b), Gemma-1.1-7B-it (Team et al., 2024a), Mixtral-7B-Instruct-v0.2 (Jiang et al., 2023) and Llama-3-8B-Instruct (Touvron et al., 2023).\nBaselines. Our first baseline is rejection sampling (RS) methods (Neal and Hinton, 1998; Dong et al., 2023a; Yuan et al., 2023; Zelikman et al., 2022). For each problem x, we generate N = 30 candidate rationales and select one (zrs) containing the correct answer y*. The model is fine-tuned on these problem-rationale-answer tuples {(x, zrs, y*)}. Importantly, this method does not use human-annotated rationales z*, making it directly comparable to our approach. We also test the performance of supervised fine-tuning (SFT) on datasets with human-annotated rationales {(x, z*, y*)}. As a baseline learning from preference data, we implement the iterative DPO (Xiong et al., 2024; Pang et al., 2024), initialized with instruction-tuned models. The training process consists of 3 iterations. In each iteration, we: (1) use the current model to generate 30 CoT and responses {(z,y)} per prompt; (2) select the best and worst response pairs (z+,y+,z-,y-) based on the correctness of the y; and (3) apply DPO training (see Rafailov et al. (2024) or (3.6)) on these preference tuples {(x,z\u207a,y+,z-,y-)}.\nImplementations of BRITE. To systematically demonstrate the effectiveness of our algorithm, we im-plemented BRITE in three distinct configurations:\n1. We implement the \u03c8-updating step of BRITE to obtain Q using (3.4), examining how the generated thinking process aligns with the desired reasoning path to enhance LLM reasoning capabilities. Fol-lowing our theoretical framework in Section 3.4, we optimize the entropy-regularized MDP with a re-ward function defined as log(z, y, o|x) = log P(z, y*(x)|x), where y* represents the correct answer for question x. This equality holds because we focus solely on correct questions, using proper choices of 0 = {answer verified to be correct} and Y = {y*}. We employ PPO (Schulman et al., 2017) to optimize this MDP using the GSM8K and MATH datasets respectively."}, {"title": "5 Conclusion", "content": "In this work, we explore methods for enhancing language model reasoning through the automated gener-ation of high-quality thinking processes. We present a unified probabilistic framework that characterizes both the reasoning process and evaluation signals. Within this framework, we develop the Bootstrapping Reinforced Thinking Process (BRITE) algorithm, which advances automated reasoning generation through reinforcement learning during inference and incorporates improved reasoning processes into post-training phases. Furthermore, we demonstrate that BRITE possesses a provable convergence property and unifies various existing learning paradigms and algorithms. Extensive empirical results on mathematics tasks show that our approach surpasses traditional chain-of-thought prompting while enhancing existing supervised/re-jection sampling fine-tuning and reinforcement learning from human feedback methods. Our work opens several promising research directions, including the application of our framework to broader reasoning tasks and the investigation of its potential for developing more robust and reliable AI systems."}, {"title": "A Missing Proofs in the Main Paper", "content": "A.1 Proof of Lemma 3.1\nProof of Lemma 3.1. For any q(\u00b7) \u2208 \u2206(W), we have\nE_{w~g(\u00b7)} [log Pw - log q(w)] \u2013 log P\u2082 = E_{w~g(\u00b7)} [log Pw/\u03a3_{w'\u2208w} Pw\u2019 - log q(w)] = -KL(q||p) < 0,\nwhere p is the distribution defined as p(w) = Pw/(\u03a3_{w'\u2208w} Pw\u2019), and the equality is achieved when q = p.\nHence, we have finished the proof of Lemma 3.1.\nA.2 Proof of Proposition 3.7\nProof of Proposition 3.7. The first equation follows from the deterministic transition. We will now focus on proving the second propositional relationship. According to equation (2.3), we have:\n\u03c0^*(ai | si) = exp{(Q^*(si, ai) \u2013 V^*(si))/\u03b2}, Wh\u2264 i \u2264 H,\nwhich implies that\n\u03a3_{i=h}^{H}\u03b2log \u03c0^*(ai | Si) = \u2211_{i=h}^{H} (Q^*(Si, ai) - V^*(Si))\n= \u2211_{i=h}^{H-1} (r(si, ai) + V^*(Si+1) \u2013 V^*(Si)) + (r(s\u043d, \u0430\u043d) \u2013 V^*(SH))\n= \u2211_{i=h}^{H}r(si, ai) - V^*(sh),\nwhere the second equality uses the fact that \u0430\u043d = EOS. Hence, we have\n\u03a0_{i=h}^{H}\u03c0^*(\u03b1i | Si) = exp{\u2211_{i=h}^{H}r(si, ai)} / exp(V^*(sh)/\u03b2)\nx exp{\u2211_{i=h}^{H} r(Si, ai)},\nwhere the last step is obtained by the fact that sh is a fixed state, independent of an \u222a {(si, ai)}^{H}_{i=h+1}."}, {"title": "B Convergence Results", "content": "B.1 Proof of Theorem 3.3\nBefore starting the proof of Theorem 3.3, we present two technical lemmas.\nLemma B.1. For any (\u03b8,\u03b8\u2019) and fixed x \u2208 X, we have\nKL(P(\u00b7, \u00b7 | x, 0) ||P(\u00b7,\u00b7|x, 0\u2019)) = A(x, \u03b8\u2019) \u2013 A(x, 0) + \u27e8\u2207A(x, 0), fo \u2013 for\u27e9.\nProof of Lemma B.1. By the definition of KL-divergence, we have\nKL(P(\u00b7,\u00b7|x, 0) ||P(\u00b7, \u00b7 | x, 0\u2019)) = \u2211_{(z,y)\u2208ZxY} P(z, y|x, 0). log P(z, y|x, 0) / P(z, y|x, 0\u2019)\n= \u2211_{(z,y)\u2208ZxY}P(z, y|x, 0). [fo(x,z,y) \u2013 fo\u2019 (x, z, y) + A(x, \u03b8\u2019) \u2013 A(x, \u03b8)]\n= A(x, \u03b8\u2019) \u2013 A(x, 0) + \u27e8E_{(z,y)~P(\u00b7,\u00b7|x,0)} [K((x, z, y), \u00b7)], fo \u2013 for\u27e9,"}]}