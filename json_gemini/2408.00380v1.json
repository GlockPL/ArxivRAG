{"title": "ENHANCING WHOLE SLIDE PATHOLOGY FOUNDATION MODELS THROUGH STAIN NORMALIZATION", "authors": ["Juseung Yun", "Yi Hu", "Jinhyung Kim", "Jongseong Jang", "Soonyoung Lee"], "abstract": "Recent advancements in digital pathology have led to the development of numer- ous foundational models that utilize self-supervised learning on patches extracted from gigapixel whole slide images (WSIs). While this approach leverages vast amounts of unlabeled data, we have discovered a significant issue: features ex- tracted from these self-supervised models tend to cluster by individual WSIs, a phenomenon we term WSI-specific feature collapse. This problem can potentially limit the model's generalization ability and performance on various downstream tasks. To address this issue, we introduce EXAPath, a novel foundational model trained on patches that have undergone stain normalization. Stain normalization helps reduce color variability arising from different laboratories and scanners, enabling the model to learn more consistent features. EXAPath is trained using 285,153,903 patches extracted from a total of 34,795 WSIs, combining data from The Cancer Genome Atlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments demonstrate that EXAPath significantly mitigates the feature collapse problem, indicating that the model has learned more general- ized features rather than overfitting to individual WSI characteristics. We com- pared EXAPath with state-of-the-art models across six downstream task datasets, and our results show that EXAPath achieves excellent performance relative to the number of WSIs used and the model's parameter count. This suggests that the ap- plication of stain normalization has substantially improved the model's efficiency and generalization capabilities.", "sections": [{"title": "INTRODUCTION", "content": "In modern pathology, the analysis of digital pathology images plays a crucial role in cancer sub- typing (Lu et al., 2021; Shao et al., 2021), prognosis prediction (Mobadersany et al., 2018; Chen et al., 2022b; Shmatko et al., 2022), and quantifying the tissue microenvironment (Schapiro et al., 2017; Mahmood et al., 2018; Saltz et al., 2018a; Graham et al., 2019). Whole Slide Images (WSIs), in particular, are high-resolution digitized tissue slides that often measure tens of thousands by tens of thousands of pixels in size. Due to the vast dimensions of these WSIs, many researchers have adopted the approach of dividing the images into smaller patches to train their models. A single WSI can be divided into thousands or even tens of thousands of patches, but accurately labeling each of these patches is a significant burden in terms of time and cost. To overcome this limitation, recent approaches have widely adopted self-supervised learning techniques to first train a foundation model, which is then fine-tuned for various downstream tasks (Kang et al., 2023; Chen et al., 2024; Vorontsov et al., 2023; Dippel et al., 2024). This approach allows for the effective utilization of large amounts of unlabeled data while achieving high performance with limited labeled data.\nIn this paper, we first introduce our novel observation that the features of models trained through self- supervised learning exhibit an unexpectedly high degree of overlap. Specifically, we divide whole slide images into patches and create a foundation model using the DINO (self-DIstillation with NO labels) (Caron et al., 2021) self-supervised learning method. During this process, as the whole slides are divided into small patches for training, the model has no information about which whole slide each specific patch originated from. Subsequently, we extract features of each patch and visualize them using t-SNE (van der Maaten & Hinton, 2008). Surprisingly, we observe that the patch fea-"}, {"title": "RELATED WORK", "content": "In this section, we briefly overview some relevant literature.\nPathology Image Foundation Models. With the significant advancements in self-supervised learn- ing (SSL) methods, there is a notable trend in applying these methods, commonly used in natural image processing, to histopathology to develop robust foundation models for whole slide images (WSIs) (Gildenblat & Klaiman, 2019; Ciga et al., 2022; Yang et al., 2022; Kang et al., 2023). Given the gigapixel resolution of WSIs, creating foundation models by dividing images into patch units has become a prevalent approach. For example, REMEDIS (Azizi et al., 2023) employs SimCLR (Chen et al., 2020a) to train on various medical domain images, including pathology. Similarly, Ciga et al. (2022) utilizes SimCLR for training on histopathology images. Kang et al. (2023) applies four SSL methods-MoCo v2 (Chen et al., 2020b), SwAV (Caron et al., 2020), Barlow Twins (Zbontar et al., 2021), and DINO (Caron et al., 2021)\u2014to pathology data and evaluates their performance on image classification and nuclei instance segmentation tasks. Campanella et al. (2023) compares the Masked Autoencoder (MAE) (He et al., 2022) and DINO (Caron et al., 2021) algorithms using the largest pathology dataset to date, which comprises over three billion images. Their evaluation across six clinical tasks demonstrates that pre-training on pathology data is more effective than pre-training on natural images, with the DINO algorithm showing superior generalization performance across all tasks. HIPT (Chen et al., 2022a) also implements DINO within a hierarchical structure. Phikon (Filiot et al., 2023) employs iBOT (Zhou et al., 2022), a self-supervised learning method based on masked image modeling (MIM), for histology images. The ViT-Base model using iBOT exhibits excellent performance across 17 downstream tasks on 7 cancer types, suggesting its potential for developing a foundation model for histopathology. CTransPath (Wang et al., 2022) introduces a new self-supervised learning strategy called Semantically-Relevant Contrastive Learning (SRCL), an en- hancement of MoCo v3 (Chen* et al., 2021). Additionally, UNI (Chen et al., 2024), RudolfV (Dippel et al., 2024), Virchow (Vorontsov et al., 2023), and Prov-GigaPath (Xu et al., 2024) utilize DINO v2 for training pathology foundation models. Prov-GigaPath (Xu et al., 2024) applies the LongNet (Ding et al., 2023) architecture to learn slide-level representations.\nTo our knowledge, none of these studies utilizing self-supervised learning for developing pathology foundation models employ stain normalization on the training data. In contrast, we incorporate stain normalization in our training process, which we believe offers significant advantages for learning robust foundation models.\nStain Normalization. Stain normalization is a critical process in histopathology designed to address the inconsistencies in color that arise from staining protocols, device properties, scanner settings, and tissue preparation methods. These variations can affect the accuracy of automated diagnostic systems. Therefore, robust methods to standardize the appearance of histopathological images are essential to ensure reliable and reproducible analyses.\nEarly methods focused on histogram transformation, such as Reinhard normalization (Reinhard et al., 2001), which employs statistical techniques to transfer the color characteristics from one image to another by converting them to the Lab color space. This method is widely recognized for its simplicity and effectiveness in standardizing color distributions across different images. Another notable approach is Macenko normalization (Macenko et al., 2009), which transforms images into an optical density (OD) space. PCA is then performed on the OD space to identify the principal components corresponding to the Hematoxylin and Eosin stains, thus enabling effective stain sepa- ration and normalization. Inspired by the biological fact about stain binding, Vahadane et al. (2016) extended the use of nonnegative matrix factorization (NMF) (Rabinovich et al., 2003) by introduc- ing sparsity constraints, which enhance the robustness of stain separation by limiting the number of non-zero components, thereby aligning with the actual biological composition of histological samples."}, {"title": "SELF-SUPERVISED PRE-TRAINING FOR PATHOLOGY", "content": "This section introduces the 'WSI-specific feature collapse' that occurs when pathology founda- tion models are trained using self-supervised learning, and discusses methods to reduce this phe- nomenon. In Section 3.1, we discuss the 'WSI-specific feature collapse' problem. This refers to the phenomenon where features from the same WSI tend to cluster together, even though each patch is learned without information about which WSI it comes from during the training process. Section 3.2 demonstrates that applying stain normalization before feature extraction does not significantly reduce the collapse. In Section 3.3, we introduce EXAPath, which applies stain normalization to the training data from the beginning of foundation model training to further reduce 'WSI-specific feature collapse'."}, {"title": "MOTIVATION: WSI SPECIFIC FEATURE COLLAPSE", "content": "DINO (Caron et al., 2021) involves applying different augmentations to an image and inputting them into both teacher and student models, training them to produce identical features. Color jittering is a data augmentation technique commonly used in self-supervised learning, including DINO. It in- volves randomly altering the colors of an image to create different variations, helping the model learn robust features invariant to color changes. However, despite the use of color jittering augmen- tation, when visualizing patch features using t-SNE (van der Maaten & Hinton, 2008), we observe that features still tend to collapse according to their source WSI. Specifically, we trained a foun- dation model using DINO on 2562-sized patches extracted from gigapixel WSIs. It is important to"}, {"title": "STAIN NORMALIZATION", "content": "We hypothesized that the WSI-specific feature collapse occurs due to variations in staining intensity across different WSIs, which result in color differences. This phenomenon arises from the staining process of WSIs, which depends on the types of stains used and the degree of staining. Additionally, differences in slide scanner settings or performance can affect color variations in digitized WSIS. Therefore, to address these color differences, we employed Macenko stain normalization (Macenko et al., 2009) before extracting features. We used the same pretrained model and patches as described in Section 3.1. Figure 2b shows the results. Compared to Figure 2a, where features are completely separated by WSI, Figure 2b shows that features from different WSIs are now slightly mixed, but they still tend to cluster by WSI. This indicates that while Macenko normalization partially mit- igates color differences between WSIs, it is not sufficient to fully resolve the issue, as features from the same WSI still exhibit noticeable clustering and the collapse problem remains significant. Furthermore, it suggests that applying Macenko normalization only during inference has limited ef- fectiveness because the pretrained model has already learned to extract features biased toward color characteristics."}, {"title": "STAIN NORMALIZED WHOLE SLIDE PATHOLOGY IMAGE FOUNDATION MODEL", "content": "We use DINO (Caron et al., 2021) self-supervised learning to train a WSI foundation model. We refer to this model as EXAPath, which is trained with stain-normalized data using DINO. DINO involves applying different augmentations to an image and inputting them into both teacher and student models, training them to produce identical features. The teacher model, updated with an exponential moving average (EMA) of the student parameters, guides the output of the student model. The student model is trained using cross-entropy loss to measure their similarity. By inputting differently augmented images into the teacher and student models, the features produced by both models are compared and aligned. In the original DINO paper, different augmentation methods are applied to two 2562 global views and several 962 local views of each image.\nCombining Macenko normalization with the existing DINO augmentation is a design choice. We ap- ply Macenko normalization to all images with 100% probability before applying DINO augmenta- tion. This approach is chosen because applying Macenko normalization to each global view and local view individually is computationally intensive, creating a bottleneck in the data loader during train- ing. Figure 2c visualizes the features of EXAPath trained with DINO using Macenko-normalized images. Macenko normalization is applied not only during training but also when extracting fea- tures for each patch in the visualization process. Compared to Figures 2a and 2b, we observe that patch features from various WSIs are much more intermingled. This indicates that the model learns more generalized features and is less dependent on the source of the WSIs. This allows the model to focus on more histologically relevant features. We still observe that some clusters of features are grouped by WSI. We believe this occurs for two reasons: first, while Macenko normalization signif- icantly reduces color differences, it may not completely eliminate them; second, patches from the same WSI likely share clinically relevant features beyond just color, due to variations in tissue type, cancer subtype, and patient characteristics within each WSI."}, {"title": "EXPERIMENTS", "content": "In this section, we describe the experimental settings and results. Section 4.1 provides a detailed explanation of the data used for pretraining the whole slide image foundation model. Section 4.2 offers comprehensive information on the pretraining methodology. Section 4.3 describes the data for the downstream tasks we aim to evaluate. Section 4.4 explains in detail the methods used to training the downstream tasks. Finally, Section 4.5 presents the results of the linear evaluation on the downstream tasks."}, {"title": "PRETRAINING DATA", "content": "Table 1 shows the number of whole slides and patches per data source used for pretraining. We first collected 11,677 Formalin-Fixed, Paraffin-Embedded (FFPE) whole slides from The Cancer Genome Atlas (TCGA) (Weinstein et al., 2013) and 23,118 FFPE whole slides from the Genotype- Tissue Expression (GTEx) project (Lonsdale et al., 2013). Both datasets consist of Hematoxylin and Eosin (H&E) stained WSIs. The TCGA dataset contains WSIs for various cancer types, while the GTEx dataset contains WSIs of normal tissues. Both the TCGA and GTEx datasets are publicly available. Following CLAM (Lu et al., 2021), we divide the WSIs into non-overlapping patches, focusing only on the tissue-containing regions. Given that each WSI has a different micron per pixel (MPP), we adjust the patch sizes so that they correspond to 0.5 MPP when resized to 2562. For instance, for WSIs with 0.25 MPP, we extract patches at 5122 dimensions, and for WSIs with 0.5 MPP, we extract patches at 2562 dimensions. However, for training the EXAPath model, the patches are not resized to achieve a uniform 0.5 MPP. Instead, we retain the original patch sizes during training because the DINO algorithm's input image transform includes random resized cropping. Even if we resize the patches to 0.5 MPP, the model's input would still undergo changes in MPP due to this random resized crop operation. Therefore, maintaining the originally extracted patch sizes allows us to preserve the maximum amount of information from each WSI while relying on DINO's augmentations to introduce the necessary variability in scale and perspective. Figure 3a shows the distribution of MPP values for the patches extracted from the TCGA dataset, and Figure 3b shows the distribution of MPP values for the patches extracted from the GTEx dataset. Figure 3c presents the overall distribution of MPP values for patches extracted from both the TCGA and GTEx datasets."}, {"title": "PRETRAINING DETAILS", "content": "We train EXAPath using DINO (Caron et al., 2021) with a ViT-B model (Dosovitskiy et al., 2021), which has a patch size of 16, initializing the weights with those pretrained on the ImageNet dataset (Russakovsky et al., 2015). Training is conducted on 10 machines, each equipped with 8 A100 GPUs. We use a batch size of 5,120 and a learning rate of 0.005, training the model for 10 epochs. The learning rate is warmed up over the first 1,000 iterations. We employ bfloat16 precision training. The output layer is not fixed; instead, it is trained from scratch. Additionally, we set the number of local crops to 8. Other hyperparameters follow the settings from the original DINO paper for training the ViT-B model."}, {"title": "DOWNSTREAM TASK DATA", "content": "We evaluate EXAPath on six publicly available patch-level datasets for classification tasks: PCAM (Veeling et al., 2018; Bejnordi et al., 2017), MHIST (Wei et al., 2021), CRC-100K (Kather et al., 2018), TIL Detection (Abousamra et al., 2022; Saltz et al., 2018b; Kaczmarzyk et al.), MSI CRC (Kather, 2019), and MSI STAD (Kather, 2019).\nPCAM (Veeling et al., 2018; Bejnordi et al., 2017) consists of 327,680 color images, each of 962 pixels, extracted from histopathologic scans of lymph node sections, and each annotated with a binary label indicating the presence of metastatic tissue. The dataset is divided into a training set of 262,144 examples and validation and test sets of 32,768 examples each, with no overlap between WSIs across splits. All splits maintain a 50/50 balance between positive and negative examples. A positive label indicates the presence of at least one pixel of tumor tissue within the central 322 pixel region of a patch, while tumor tissue in the outer region does not affect the label. PCAM is derived from the Camelyon16 Challenge, which includes 400 H&E-stained WSIs of sentinel lymph node sections, digitized at 40x magnification (0.243 MPP) and undersampled to 10x for a larger field of view. The dataset follows the train/test split from Camelyon16, with 20% of the training WSIs held out for validation.\nMHIST (Wei et al., 2021) consists of 3,152 H&E-stained FFPE images, each 2242 pixels in size, of colorectal polyps, collected from the Department of Pathology and Laboratory Medicine at Dartmouth-Hitchcock Medical Center (DHMC). Each image is scanned at 40x magnification and resized to 8x magnification and is labeled based on the consensus of seven pathologists at DHMC, categorizing the type of colorectal polyps. The MHIST dataset focuses on a binary classification task, distinguishing between Hyperplastic Polyps (HP) and Sessile Serrated Adenomas (SSA). The dataset is divided into training and test sets, with the training set further split into training and vali- dation sets using an 80:20 ratio. This results in 1,740 training samples, 435 validation samples, and 977 test samples.\nCRC-100K (Kather et al., 2018) consists of 100,000 non-overlapping image patches (2242 pix- els, 0.5 MPP) from H&E-stained histological images of human colorectal cancer (CRC) and nor- mal tissue. The dataset includes both color-normalized images using Macenko's method, and non- normalized images, with the latter exhibiting slight variations in staining intensity and color. We utilize the non-normalized images for our work, as we intend to apply Macenko normalization us- ing our Macenko target image. Tissue classes include Adipose (ADI), Background (BACK), De- bris (DEB), Lymphocytes (LYM), Mucus (MUC), Smooth Muscle (MUS), Normal Colon Mucosa (NORM), Cancer-associated Stroma (STR), and Colorectal Adenocarcinoma Epithelium (TUM). These patches are manually extracted from 86 H&E-stained FFPE samples from the NCT Biobank and the UMM pathology archive. An additional set, CRC-VAL-HE-7K, consists of 7,180 patches from 50 patients with CRC and serves as a validation set with no overlap with the original dataset. Since there is no separate test set provided, we use CRC-VAL-HE-7K as our test set. For training and validation, we split the original training set into an 80:20 ratio to create training and validation sets.\nTIL Detection (Abousamra et al., 2022; Saltz et al., 2018b; Kaczmarzyk et al.) dataset consists of 304,097 H&E-stained images, each 1002 pixels at 0.5 MPP, extracted from FFPE sections covering"}, {"title": "DOWNSTREAM TASK TRAINING DETAILS", "content": "We use linear evaluation, a common practice in previous SSL research for evaluation (Chen et al., 2020a; Zbontar et al., 2021; He et al., 2020). Training is performed using the SGD optimizer with a learning rate of 0.1, without weight decay, and a momentum of 0.9, with a batch size of 128 for 12,500 iterations. The images are first resized to 2562 pixels and then center-cropped to 2242 pixels. No additional data augmentation is applied. Macenko normalization is used in both the training and evaluation processes. Even datasets that are already Macenko-normalized are re-normalized using the same Macenko target images employed during EXAPath training."}, {"title": "DOWNSTREAM TASK RESULTS", "content": "Table 2 presents the classification accuracy results obtained from the linear evaluation. EXAPath demonstrated competitive performance, achieving an average accuracy of 0.861 across the six down- stream tasks. Notably, EXAPath achieves the highest performance among all models on the MSI CRC dataset, with an accuracy of 0.756, and on the MSI STAD dataset, with an accuracy of 0.804. Figure la compares the number of model parameters with the average accuracy across the six down- stream tasks. Models positioned in the top left have fewer parameters and higher accuracy, indicating better efficiency. EXAPath exhibits parameter efficiency compared to other models. Figure 1b shows the average accuracy across six downstream tasks in relation to the number of WSIs used for model training. Models in the top left exhibit higher accuracy with fewer WSIs used for training. Train- ing with stain-normalized images effectively mitigates the WSI-specific feature collapse problem, allowing for strong performance with fewer data and relatively smaller models."}, {"title": "CONCLUSION", "content": "In this paper, we have made a notable discovery during the visualization of features from patches ex- tracted from models trained with self-supervised learning. We have termed this phenomenon 'WSI- specific feature collapse', where features from patches extracted from the same Whole Slide Image (WSI) tend to cluster together. This suggests inefficient learning, as it leads to the extraction of sim- ilar features when thousands of patches are derived from a single WSI. We hypothesized that this phenomenon arose from the substantial color variations between different WSIs. To address this issue, we utilized Macenko normalization on the training data to minimize color differences and developed EXAPath. As a result, EXAPath significantly reduced the WSI-specific feature collapse and achieved comparable or superior performance to existing models, even though those models use more data and parameters. However, despite the considerable improvement in mitigating WSI- specific feature collapse through EXAPath, this phenomenon appears to persist to some extent. This suggests the need for further research. Specifically, developing more effective and computationally efficient stain normalization techniques, suitable for integration into data loaders, could be a promis- ing direction for future research. Additionally, exploring new learning methods or model architec- tures that can more effectively prevent feature collapse is also necessary. In conclusion, EXAPath represents significant progress in WSI analysis, laying the groundwork for more efficient and accu- rate pathological image analysis."}]}