{"title": "Little Giants: Synthesizing High-Quality Embedding Data at Scale", "authors": ["Haonan Chen", "Liang Wang", "Nan Yang", "Yutao Zhu", "Ziliang Zhao", "Furu Wei", "Zhicheng Dou"], "abstract": "Synthetic data generation has become an increasingly popular way of training models without the need for large, manually labeled datasets. For tasks like text embedding, synthetic data offers diverse and scalable training examples, significantly reducing the cost of human annotation. However, most current approaches rely heavily on proprietary models like GPT-4, which are expensive and inefficient for generating large-scale embedding data. In this paper, we introduce SPEED, a framework that aligns open-source small models (8B) to efficiently generate large-scale synthetic embedding data. Through supervised fine-tuning, preference optimization, and self-improvement, SPEED enables small open-source models to produce high-quality data. Remarkably, SPEED uses only less than 1/10 of the GPT API calls, outperforming the state-of-the-art embedding model E5mistral when both are trained solely on their synthetic data. Using this efficient generator, we conduct a comprehensive study on how various factors within the alignment pipeline impact data quality and reveal the scaling law for synthetic embedding data.", "sections": [{"title": "Introduction", "content": "Text embedding models encode natural language texts into latent vectors. They are widely used in downstream tasks such as classification, clustering, retrieval, and summarization. Many researchers have trained general embedding models that can support various tasks (Reimers and Gurevych, 2019; Wang et al., 2022; Xiao et al., 2024). Most of these models require large-scale weakly-supervised data and high-quality labeled data for multi-stage training, which requires careful data curation and costly human effort. Thanks to the powerful language modeling ability and vast knowledge of large language models (LLMs), some works attempt to utilize LLMs to generate synthetic data for training embedding models (Jeronymo et al., 2023; Wang et al., 2024; Lee et al., 2024).\nHowever, most of these works solely use proprietary LLM like GPT-4 for data synthesis (Wang et al., 2024; Lee et al., 2024). For example, E5mistral generates triplets of (query, positive document, hard negative document) for various embedding tasks from scratch. While synthesizing embedding data without relying on existing corpora can yield more diverse examples, using black-box models can be extremely costly, especially given that this data often includes long documents. A straightforward approach to reduce costs is to utilize small open-source models, which have proven effective for tasks such as mathematical reasoning (Zhou et al., 2024b; Bansal et al., 2024; Chen et al., 2024b). However, synthesizing embedding data often requires the generation of hard negatives documents that are similar to positive ones and are essential for learning nuanced embedding representations. These hard negatives are challenging for small models to synthesize, as they are difficult for language models to distinguish. An early work explores the ability of small models for synthesizing embedding data (Jeronymo et al., 2023), but it uses small models to generate data directly without special tailoring for data synthesis, resulting in poor performance.\nIn this work, we propose to align open-source small models (8B) to synthesize large-scale high-quality embedding data. Compared to existing methods that rely solely on expensive GPT-4, our approach can generate more data at a much lower cost. Our primary goal is to study the alignment of small models for synthesizing embedding data, which has been neglected by existing works. Specifically, we aim to address the following research questions in this paper:\nRQ1: How to align small models for synthesizing high-quality embedding data at scale?\nRQ2: How do factors within the alignment framework affect the quality of synthetic data?\nRQ3: Synthetic data is theoretically infinite. What is the scaling law for synthetic embedding data?\nTo shed light on RQ1, we design an alignment framework that trains small LLMs to efficiently Synthesize large-scale suPErior Embedding Data (SPEED). As illustrated in Figure 1, our framework consists of three key models: a junior generator for initial data synthesis, a senior generator for advanced data generation, and a data revisor for self-improvement. The goal is to distill knowledge from GPT-4 into these smaller models. We first use GPT-4 to brainstorm task descriptions. However, since GPT-4 often generates hallucinations and data of specific domains (e.g., climate change) (Chang, 2023), we sample topics from the Open Directory Project to ensure diverse and balanced tasks.\nBased on these tasks, GPT-4 produces a small set of seed data, which we use to finetune the junior generator via supervised fine-tuning (SFT). The junior generator produces root data, which is further evaluated by GPT-4 to produce signals that guide the preference optimization process, resulting in a senior generator. The root data is also revised by GPT-4 to produce revision signals for training a data revisor. Inspired by the idea of scaling inference compute for LLMs (Brown et al., 2024), the revisor refines the synthetic data with minimal additional inference cost, enabling self-improvement.\nAs for RQ2, with these low-cost yet powerful data synthesis models ready, we are able to conduct extensive experiments to study the factors affecting the alignment. We find that settings such as the base model used for alignment, the diversity of tasks, and the number of training samples can influence the quality of synthetic data. For RQ3, we generate large-scale data using the efficient generators to reveal the scaling law. We observe a log-linear relationship between the performance of the embedding model and the size of synthetic embedding data.\nIn summary, our contributions are as follows:\n\u2022 We design a framework to fine-tune small LMs (8B) for synthesizing large-scale data, achieving superior embedding performance with less than 1/10 of the GPT API calls required by E5mistral.\n\u2022 We comprehensively study how the factors within the alignment framework influence the quality of synthetic data.\n\u2022 We investigate the scaling law of synthetic embedding data and reveal that the embedding model's performance follows a log-linear relationship with the data size."}, {"title": "Related Work", "content": "Text Embedding Text embedding models have gained much attention in the era of deep learning. Some existing models, such as SBERT (Reimers and Gurevych, 2019), E5 (Wang et al., 2022), and BGE (Xiao et al., 2024), attempt to produce general text embeddings for various tasks. However, most of them require lots of labeled data. In this work, we attempt to train a model with synthetic data.\nLarge Language Models Though proprietary LLMS (OpenAI, 2023; Anthropic, 2024) are very powerful, invoking their APIs can be quite expensive and unaffordable for common usage. Many open-source LLMs have been released for more efficient language modeling, such as LLaMA (Dubey et al., 2024) and Mistral (Jiang et al., 2023). Some works attempt to improve the ability of LLMs for text embedding tasks, such as ad-hoc retrieval (Ma et al., 2024), conversational retrieval (Chen et al., 2024a), and multilingual text embedding (Wang et al., 2024). Our work aims to use synthetic data to improve the LLM's ability of text embedding.\nSynthetic Data The generation of synthetic data have been studied by many researchers for various embedding tasks. In early times, they have been used to produce pseudo labels and query/document expansions (Nogueira et al., 2019; Wang et al., 2023; Dai et al., 2023). Using the ability of LLMs, synthetic data have been used for code generation (Gunasekar et al., 2023; Hui et al., 2024), mathematical reasoning (Chan et al., 2024;"}, {"title": "Methodology: SPEED", "content": "In this section, we aim to answer RQ1 using our alignment framework, SPEED. As shown in Figure 2, SPEED consists of four stages: (1) GPT-4 is first used to generate diverse task descriptions based on multi-grained topics sampled from the ODP. A junior generator then distills knowledge from GPT-4 by training on a small set of seed data. (2) The junior generator synthesizes root data, which GPT-4 uses to produce preference signals. These signals are used to train a senior generator through preference optimization. (3) The root data is also evaluated by GPT-4 to produce revised data for finetuning a data revisor. (4) Finally, the senior generator synthesizes large-scale embedding data, and the revisor refines them into high-quality data for training the embedding model."}, {"title": "Preliminaries", "content": "Many works have tried to generate synthetic data using modern LLMs for downstream tasks finetuning. Following E5mistral (Wang et al., 2024), in order to synthesize data for training an embedding model, we generate data for four kinds of tasks: classification (long-short match), semantic textual similarity (STS), retrieval (short-long match), and text matching (short-short and long-long match).\nFor simplicity, we will denote the data synthesis prompts as a set $P$ without distinction. We use GPT-4 to brainstorm a pool of candidate tasks $T$ as instructions. With a prompt $p \\in P$ and a task instruction $t \\in T$, an LLM $\\pi_{\\theta}$ can synthesize an embedding data sample $d \\sim \\pi_{\\theta}(d | p,t)$. Each data example is a triplet of (query, positive document, hard negative document). For example, for a classification task, the query is a long text and documents are short labels. More information on the structure of these data can be found in Appendix D."}, {"title": "Aligning Small Models for Synthesizing Embedding Data", "content": "Most existing approaches that synthesize embedding data suffer from the high cost of heavily relying on proprietary LLMs. We aim to align small models that can generate large-scale embedding data effectively and efficiently."}, {"title": "Task Brainstorming", "content": "Synthesizing embedding data from scratch can be quite challenging since these data are often long and complex. We first generate a pool of candidate tasks as instructions for LLMs to further generate concrete data. Since these task descriptions are very short (about 10 words) and need to be high-quality, we use GPT-4 to brainstorm them. Furthermore, we sample multi-grained topics from open directory project (ODP) and specify one topic for each brainstorming prompt to mitigate the hallucination and extract more diverse knowledge from GPT-4 (Chang, 2023). For example, we prompt GPT-4 as \"Brainstorm a list of potentially useful"}, {"title": "Training a Junior Generator", "content": "Proprietary LLMs such as GPT-4 have been proven to generate high-quality embedding data (Wang et al., 2024; Lee et al., 2024). However, it can be expensive if we generate large-scale embedding data solely using GPT-4. Our goal is to distill the data synthesis capability of GPT-4 into small models that can synthesize large-scale data at low cost. We first use GPT-4 to generate a small set of seed data $D_{seed} \\sim GPT-4(D_{seed} | P,T)$. The constructed training data for SFT is $D_{SFT} = {P_i, t_i, d_i}_{i=1}^N$. To distill knowledge from GPT-4, we apply a standard Supervised Fine-tuning (SFT) objective to initialize our junior generator $\\pi_{\\Jr}$:\n$L(\\theta^{Jr}) = - \\sum_{(P_i,t_i,d_i) \\in D_{SFT}} log P_{\\theta}(d_i | P_i, t_i),$ (1)\nwhere $\\theta^{Jr}$ denotes the parameters of our junior generator. We aim to train a small model with basic capability of synthesizing embedding data given various prompt templates and task instructions."}, {"title": "Further Training Using Preference Optimization", "content": "Although our junior generator can already generate embedding data of decent quality, we still want to boost its ability. Preference optimization (Schulman et al., 2017) is a popular way to be performed on a model for further training after SFT (Dong et al., 2024; Yu et al., 2024). Since our goal is to perform optimization on $\\pi_{\\Jr}$, we use GPT-4 to produce preference signals based on the data generated by $\\pi_{\\Jr}$ itself.\nSpecifically, $\\pi_{\\Jr}$ generates a list of embedding data given each prompt, formatting a set of root data $D_{root} \\sim \\pi_{\\Jr} (D_{root} | P,T)$. As illustrated in Figure 2, GPT-4 evaluates the best and the worst data in each data list and constructs preference pairs accordingly. We prompt GPT-4 as: \"Your mission is to judge which data this language model generates fits the prompt most and which fits worst, and explain your judgment.\". In this work, we perform Direct Preference Optimization (DPO) (Rafailov et al., 2023) because it is a popular and low-cost method. The formatted training set for DPO is\n$D_{DPO} = {p,t,d_w, d_l}$, where $d_w$ and $d_l$ are the winning and losing one, respectively. Then, we apply the standard DPO on our junior generator:\n$L_{DPO}(\\pi; \\pi_{ref}) = - E_{(p,t,d_w,d_l)\\sim D}  log \\sigma(\\beta log \\frac{\\pi(d_w| x)}{\\pi_{ref}(d_w| x)} - log \\frac{\\pi(d_l| x)}{\\pi_{ref}(d_l| x)}),$ (2)\nwhere $\\pi_{ref}$ is the reference model set as $\\pi_{\\Jr}$ in the beginning and remains frozen, $\\sigma$ is the sigmoid function, and $\\beta$ controls how much DPO focus on $\\pi_{ref}$. After this, we manage to obtain a senior generator $\\pi_{\\Sr}$ that can synthesize higher-quality data since it has learned about how to make better choices given a data synthesis prompt."}, {"title": "Training a Data Revisor", "content": "Scaling the inference compute of LLMs has been a popular way to boost the LLM's performance from the inference side (Brown et al., 2024). Inspired by this, we employ another small model to refine our synthetic data. This allows us to further improve data quality with only a small increase in inference cost, as the revisor model is also small. Specifically, we train an additional LLM to serve as the data revisor, identifying and refining potential flaws in the synthetic data.\nSpecifically, to boost the efficiency of the alignment process, we reuse $D_{root}$ to produce revised data. This allows us to train both $\\pi_{\\Sr}$ and the revisor $\\pi_{\\theta^{Re}}$ simultaneously. GPT-4 produces data revision signals by evaluating the root data from three key aspects: (1) its relevance to the task, (2) its completeness based on the requirements in the prompt, (3) the accuracy of its factual content. The revised data is $D_{root} \\sim GPT-4(D_{root} | P, T, D_{root})$ and the data for SFT is $D_{SFT} = {P_j, t_j, d_{root}^j, d_{r}^j}_{j=1}^M$. Similarly, a standard SFT approach is performed on an unaligned small LM:\n$L(\\theta^{Re}) = - \\sum_{j=1}^M log P_{\\theta}(d_r^j | x_j), x_j = (p_j, t_j, d_{root}^j),$ (3)\nwhere $\\theta^{Re}$ denotes the parameters of our revisor."}, {"title": "Finetuning Embedding Model Using Synthetic Data", "content": "With our aligned senior generator $\\pi_{\\Sr}$ and revisor $\\pi_{\\theta^{Re}}$ ready, we are able to generate high-quality synthetic embedding data at scale. Specifically, $\\pi_{\\Sr}$ first generates a large set of synthetic data $D_{syn} \\sim \\pi_{\\Sr}(D_{syn} | P,T)$. Then we revises them into high-quality data $D_{ryn} \\sim \\pi_{\\theta^{Re}}(D_{ryn} | P, T, D_{syn})$. For efficiency, we avoid iterative improvements and perform the revision in a single pass.\nFollowing the common approach of task-specific fine-tuning (Xiao et al., 2024; Wang et al., 2024), an instruction template is applied on each query within $D_{ryn}$ as: $q' = Instruct:{t} \\n Query:{q}$, where $q'$ is the original query $q$ with task description. We do not apply this template on the document side for pre-building the index. We append an [EOS] token to each $q'$ and document $d$. Each output of the last layer [EOS] is taken as the representation $q'$ and $d$. To train the embedding model, we apply a standard contrastive learning objective:\n$L_{CL} = - log \\frac{\\phi(q', d+)}{\\sum_{d \\in N} \\phi(q', d)},$ (4)\nwhere $N$ represents negative documents, $\\phi(\\cdot) = exp(cos(\\cdot)/\\tau)$, $cos(\\cdot)$ denotes cosine similarity, and $\\tau$ is a temperature hyperparameter."}, {"title": "Experiments", "content": "SPEED synthesizes 920K embedding data samples in total for training after MinHash deduplication. The proprietary LLM used for knowledge distillation is GPT-4o-2024-05-13. The base model we use to train our generators is LLaMA-3-8B (Meta, 2024). We test our finetuned embedding model on the MTEB benchmark (Muennighoff et al., 2023). This benchmark contains 7 kinds of 56 English embedding tasks: classification (12), clustering (11), pair classification (3), reranking (4), retrieval (15), semantic textual similarity (10) and summarization (1). The synthetic data proportion of our four embedding task types, i.e., classification, STS, retrieval, and text matching is 7:7:7:2. For fair comparisons to E5mistral, we train Mistral-7B-v0.1 (Jiang et al., 2023) as our embedding model and use the same labeled data for \u201cSupervised Models\" setting. We use LoRA (Hu et al., 2022) to finetune our embedding model.\nIn addition to existing baselines that consists of OpenAI's text-embedding-3, GTR (Ni et al,"}, {"title": "Experimental Setup", "content": "SPEED synthesizes 920K embedding data samples in total for training after MinHash deduplication. The proprietary LLM used for knowledge distillation is GPT-4o-2024-05-13. The base model we use to train our generators is LLaMA-3-8B (Meta, 2024). We test our finetuned embedding model on the MTEB benchmark (Muennighoff et al., 2023). This benchmark contains 7 kinds of 56 English embedding tasks: classification (12), clustering (11), pair classification (3), reranking (4), retrieval (15), semantic textual similarity (10) and summarization (1). The synthetic data proportion of our four embedding task types, i.e., classification, STS, retrieval, and text matching is 7:7:7:2. For fair comparisons to E5mistral, we train Mistral-7B-v0.1 (Jiang et al., 2023) as our embedding model and use the same labeled data for \u201cSupervised Models\" setting. We use LoRA (Hu et al., 2022) to finetune our embedding model.\nIn addition to existing baselines that consists of OpenAI's text-embedding-3, GTR (Ni et al"}, {"title": "Main Results", "content": "The results are presented in Table 1. SPEED achieves the best performance in the zero-shot setting and the second-best performance in the supervised setting. This demonstrates the effectiveness of our framework, as SPEED can generate large-scale high-quality data using the smallest language model. These results address RQ1, confirming that SPEED is an effective way to align small models for synthesizing large-scale embedding data. Furthermore, we can make these observations: (1) Comparing to Mistralllama3, SPEED improves its performance greatly. This demonstrates that our alignment framework enables a base small model to synthesize higher-quality data than its instruct-tuned version. Additionally, as shown in Table 2, SPEED with just 230K data examples also outperforms Mistralllama3. (2) Intriguingly, SPEED outperforms E5mistral-7b in the zero-shot setting but slightly underperforms in the full-data setting. We attribute this to the fact that, while our synthetic data is more diverse and covers a broader range of scenarios, E5mistral-7b's data is structurally closer to labeled data, as it is generated by the powerful but costly GPT. (3) Gecko performs well on some certain types of embedding tasks. We believe this is because Gecko uses a black-box model to generate a large set of synthetic data (6.6M), potentially covering more task types than both SPEED and E5mistral-7b."}, {"title": "RQ2. Alignment Analysis", "content": "In this section, we will look deeper into SPEED and provide comprehensive analysis of how each factor influences the synthetic data. For efficient analysis, we synthesize 230K embedding data using the same data proportion of SPEED for each model and perform zero-shot evaluation on MTEB."}, {"title": "Ablation Study", "content": "To evaluate each component of SPEED, we first conduct ablation experiments on our alignment framework. The results are presented in Table 2. We can make the following observations: (1) $\\pi_{\\Jr}$ itself can already synthesize embedding data of decent quality (62.6), which demonstrates the effectiveness of our aligned junior generator. (2) \u201cSPEED w/o. DPO\u201d, i.e., only $\\pi_{\\Jr}$ and $\\pi_{\\theta^{Re}}$ causes performance decreasing. This demonstrates our DPO training process can further enhance the synthesis ability of $\\pi_{\\Sr}$. (3) The performance drops after discarding $\\pi_{\\theta^{Re}}$. This shows revising the synthetic data with our data revisor can enhance the data quality by introducing a little more inference compute."}, {"title": "Task Brainstorming", "content": "To mitigate hallucination and introduce diversity to LLMs, we propose to use GPT-4 to brainstorm a candidate pool of task descriptions with multi-grained topics before we synthesize specific data. To study the influence of topic diversity and coverage, we perform experiments from two aspects and present the results in Table 3: (1) The number of tasks per topic. For each topic sampled from ODP, we generate 1, 3, and 5 tasks. We find that the performance of $\\pi_{\\Jr}$ drops greatly when we generate more tasks per topic. This demonstrates that the diversity of tasks is important for the quality of synthetic data. (2) The granularity of topics. The sampled topics are multi-grained and we truncate those extremely specific topics to a maximum depth of 4. Without truncation, those topics will produce tasks harming the generalization of SPEED."}, {"title": "Junior Generator $\\pi_{\\Jr}$", "content": "In this section, we will look into our SFT process and discuss the factors that may influence $\\pi_{\\Jr}$: Base LLM. The base model that we train into our synthesis LLM is directly related to the data quality. To study this, we apply our SFT pipeline on several other base LLMs. From the results in Table 4, we can observe that all LLMs can synthesize embedding data of decent quality with our SFT pipeline. This shows the effectiveness and applicability of our designed alignment process again. Besides, $\\pi_{\\Jr}$ trained on LLaMA-3-8B achieves the best performance, which is consistent with its superior language modeling ability. This means we can easily boost the quality of synthetic data by applying SPEED on more advanced open-source LLMs.\nThe generation temperature. Temperature is a crucial hyperparameter that controls the randomness of the text generation process. We set the generation temperature of $\\pi_{\\Jr}$ in the range of [0.2, 1.5], and present the performances on MTEB in the left part of Figure 3. Due to space limitations, we only show results for five values (this policy will be followed in the subsequent displays). We can observe that the performance of $\\pi_{\\Jr}$ first increases then drops. This phenomenon indicates a trade-off: If the temperature is too low, the synthetic data will lack diversity. However, the LLM may generate data that do not follow the required structure and guidelines if the temperature is too high.\nThe number of training samples. In our training process of $\\pi_{\\Jr}$, we use GPT-4 to produce signals for knowledge distillation. This raises a question: how many samples should we use for finetuning the generator? Is it the more the better? We study this question by set the number of training samples of $\\pi_{\\Jr}$ in the range of [5K, 100K]. As shown in the middle left part of Figure 3, a small set of training samples can already train a decent generator using our SFT pipeline, which validates its effectiveness again. However, too many training samples will harm the language modeling ability of the LLM."}, {"title": "Senior Generator $\\pi_{\\Sr}$", "content": "We propose to further train the junior generator with DPO into a more powerful synthesis model $\\pi_{\\Sr}$. In this part, we will look into this process from these aspects:\nThe hyperparameter $\\beta$. When performing DPO on $\\pi_{\\Jr}$, we aim to improve its performance by directly optimizing for preference signals produced by GPT-4. $\\beta$ is the hyperparameter used to control the trade-off between aligning the model to preference signals and avoiding over-optimization that may degrade performance on the original task. To study it empirically, we set $\\beta$ in the range of [0.05, 0.3]. As presented in the middle part of Figure 3, SPEED's performance increases to an optimal value when $\\beta$ = 0.1 then drops. This validates the trade-off: A high $\\beta$ controls $\\pi_{\\Sr}$ to stay close to the reference model ($\\pi_{\\Jr}$), ensuring it doesn't drift too much, while a low $\\beta$ encourages stronger adaptation to the preference signals, but at the risk of overfitting.\nThe number of training samples. Similar to the SFT process, we can raise a question: how many preference data pairs we should use to align $\\pi_{\\Sr}$? We study this question by setting the number of training samples for $\\pi_{\\Sr}$ in the range of [5K, 15K]. From the results in the middle right part of Figure 3, we can observe that finetuning $\\pi_{\\Sr}$ using DPO needs fewer data that the SFT process. This is consistent with previous studies that pairwise signals of outputs (preferences) are more informative per instance than standard supervised data. We also notice that the performance drops when we use too many preference signals. This indicates that overfitting the junior generator will harm its ability of following basic guidelines and instructions."}, {"title": "Data Revisor $\\pi_{\\theta^{Re}}$", "content": "The number of training samples. SPEED further enhances the quality of synthetic embedding data using a data revisor. GPT-4 evaluates the root data synthesized by $\\pi_{\\Sr}$ from multi-grained aspects and produces data revision signals to finetune $\\pi_{\\theta^{Re}}$. $\\pi_{\\theta^{Re}}$ revises the synthetic data generated by $\\pi_{\\Sr}$ to take a reflection at them and boost their quality. To study the influence of the number of the revision signals used for aligning the revisor, we set it in the range of [5K, 50K]. As shown in the right part of Figure 3, we can observe a similar pattern as the training of $\\pi_{\\Jr}$. This is consistent with their training protocol that they are both aligned by SFT. However, it takes fewer training data to finetune $\\pi_{\\theta^{Re}}$ than $\\pi_{\\Jr}$. This is because that it is easier to revise a data sample of decent quality than synthesize one from scratch."}, {"title": "RQ3. Scaling Synthetic Embedding Data", "content": "In the era of LLMs, models are often trained on billions or even trillions of data points. This raises a key question: does increasing training data always lead to better performance? Some existing works has explored this through scaling laws in areas like language modeling (Kaplan et al., 2020) and dense retrieval (Fang et al., 2024). However, these works primarily focus on scaling the labeled data or existing corpora.\nSynthetic data, which are theoretically unlimited, remains an underexplored area for scaling laws (Liu et al., 2024). This is a non-trivial problem because: (1) The distribution of synthetic data differs from that of labeled data (Yu et al., 2023). (2) Generating large-scale synthetic data with black-box LLMs to study scaling laws can be costly. With the efficient data synthesis capabilities of SPEED, we are able to generate large-scale embedding data and analyze the corresponding scaling law. As shown in Figure 4, we observe a log-linear relationship between the embedding model's performance and the size of the synthetic data. This scaling law offers key insights for future works: (1) The log-linear trend enables researchers to predict performance improvements from synthesizing more data. (2) It guides trade-offs by showing diminishing returns\u2014beyond a certain point, additional data yields marginal improvement, making further investment in data synthesis less valuable."}, {"title": "Cost Analysis", "content": "In this section, we analyze the cost of our alignment framework, SPEED. The cost is reported from two aspects: GPT API calls (the number of invoking times) and GPT token usage. We omit the task brainstorming process, as the task descriptions are very short compared to the embedding data, and we also neglect the cost of deploying the aligned generators since they are very small.\nSpecifically, SPEED costs 25K (SFT $\\pi_{\\Jr}$) + 10K (DPO $\\pi_{\\Sr}$) + 10K (SFT $\\pi_{\\theta^{Re}}$) = 45K GPT API calls. As for GPT token usage it costs 10M (SFT $\\pi_{\\Jr}$) + 12M (DPO $\\pi_{\\Sr}$) + 10M (SFT $\\pi_{\\theta^{Re}}$) = 32M.\nFor a more staightforward understanding, we compare these costs with the synthesis process of E5mistral, which solely uses GPT to synthesize data. It requires 500K API calls and consumes 180M GPT tokens (Wang et al., 2024). The comparison, shown in Table 5, highlights that SPEED is significantly more efficient, requiring only less than 1/10 of the GPT-4 API calls and about 1/6 of the tokens to align small open-source models for synthesizing large-scale data efficiently and effectively."}, {"title": "Conclusion", "content": "In this work, we propose a framework SPEED that aligns small models for the efficient and effective synthesis of embedding data. Through supervised finetuning, preference optimization, and self-improvement, small models can also synthesize high-quality embedding data at scale. Additionally, we comprehensively investigate how various factors within the alignment pipeline influence data quality. We reveal the scaling law of synthetic embedding data, demonstrating a log-linear relationship between the performance of the embedding model and the size of the synthetic data."}, {"title": "Limitations", "content": "Our work still have several limitations that we plan to address in future works:\n1. The training signals we produce may be improved in the future. Although GPT-4o is already a very powerful LLM, it still can not perfectly interpret the guidelines and requirements in our prompts. For example, some of the long hard negative documents are too close to the positive ones.\n2. Our senior generator is trained by DPO. More advanced preference optimization approaches such as step-DPO will be utilized.\n3. The base models used for data synthesis and embedding model can be improved. For fair comparisons to baselines, we train Mistral-7B-v0.1 as our embedding model. In future works, we plan to use more advanced LLMs to boost our model's performance.\n4. We do not fit a function for the scaling law we reveal for synthetic embedding data. In future work, we will explore a power-law function that can represent the scaling relationship we find in this paper."}, {"title": "Appendix", "content": "Details about Synthetic Data\nSynthetic Task Type\nImplementation Details\nIn this part, we delve into the details about the implementation of SPEED. Specifically, we finetune LLaMA-3-8B as data synthesis models and Mistral-7B-v0.1 as our embedding model. For the SFT process of $\\pi_{\\Jr}$, the learning rate is 1e-4 and the batch size is 16. As for the DPO process of $\\pi_{\\Sr}$, the learning rate is 1e-5, beta $\\beta$ is set as 0.1, and the batch size is 16. For the SFT process of $\\pi_{\\theta^{Re}}$, the learning rate is 5e-6 and the batch size is 24.\nFor the data generation, we set the temperature as 1.0 for all data synthesis except 0.0 for producing the preference signal. The top_p is set as 1.0.\nFor the training of our embedding model, we use LORA with rank 16 and DeepSpeed ZeRO-3. We set the batch size as 1,536 using 16 40G A100 and fp16. For the training data, we use a combination of synthetic data and a collection of 13 public datasets. These labeled datasets used for finetuning are the same as those in E5mistral.\nFor the instructions we used for the training and evaluation datasets (MTEB), please refer to the original paper of E5mistral (Wang et al., 2024)."}, {"title": "Prompts", "content": "The prompts we used in our work can be categorized into two kinds: prompts used for generating synthetic data and aligning data generators."}, {"title": "Data Generation", "content": "Since our work focuses on the alignment of small models for synthesizing large-scale embedding data", "for the topic": {}}]}