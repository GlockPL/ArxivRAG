{"title": "DEEP LEARNING-BASED CLASSIFICATION OF BREAST CANCER\nMOLECULAR SUBTYPES FROM H&E WHOLE-SLIDE IMAGES", "authors": ["Masoud Tafavvoghi", "Anders Sildnes", "Mehrdad Rakaee", "Nikita Shvetsov", "Lars Ailo Bongo", "Lill-Tove Rasmussen Busund", "Kajsa M\u00f8llersen"], "abstract": "Classifying breast cancer molecular subtypes is crucial for tailoring treatment strategies. While\nimmunohistochemistry (IHC) and gene expression profiling are standard methods for molecular subtyping,\nIHC can be subjective, and gene profiling is costly and not widely accessible in many regions. Previous\napproaches have highlighted the potential application of deep learning models on H&E-stained whole slide\nimages (WSI) for molecular subtyping, but these efforts vary in their methods, datasets, and reported\nperformance. In this work, we investigated whether H&E-stained WSIs could be solely leveraged to predict\nbreast cancer molecular subtypes (luminal A, B, HER2-enriched, and Basal). We used 1,433 WSIs of breast\ncancer in a two-step pipeline: first, classifying tumor and non-tumor tiles to use only the tumor regions for\nmolecular subtyping; and second, employing a One-vs-Rest (OvR) strategy to train four binary OvR\nclassifiers and aggregating their results using an eXtreme Gradient Boosting (XGBoost) model. The pipeline\nwas tested on 221 hold-out WSIs, achieving an overall macro F1 score of 0.95 for tumor detection and 0.73\nfor molecular subtyping. Our findings suggest that, with further validation, supervised deep learning models\ncould serve as supportive tools for molecular subtyping in breast cancer. Our codes are made available to\nfacilitate ongoing research and development.", "sections": [{"title": "Introduction", "content": "Breast cancer accounts for 12.5% of all diagnosed cancer types globally, with around 2.3 million new cases\nand 685,000 fatalities annually, and is expected to grow to 3 million newly diagnosed cases and 1 million\ndeaths by 2040 [1]. Breast cancer is a heterogeneous disease, and its outcome depends on patients'\ndemographic factors and tumor characteristics, including the crucial distinction among molecular subtypes,\nwhich play a significant role in determining treatment strategies. Broadly, breast cancer has four molecular\nsubtypes: luminal A (LumA), luminal B (LumB), HER2-enriched (HER2), and basal-like (BL). Normally, BL\ntumors exhibit higher rates of recurrence during the initial five years following detection and treatment, but\nthey show higher response to chemotherapy. On the other hand, luminal cancers, accounting for 60-70% of all\nbreast cancers [2], respond poorly to chemotherapy, and LumA tumors have lower early recurrence compared\nto other breast cancer molecular subtypes [3, 4]. Therefore, identifying the molecular subtypes of breast\ncancer is crucial for treatment decisions.\nCurrently, gene expression profiling serves as a new technology for breast cancer molecular subtyping, which\nis substantially more expensive and not available in all healthcare systems [5]. As a result,\nimmunohistochemistry (IHC) staining is still widely used to classify the subtypes in clinical practice. IHC\nstaining involves using specific antibodies to detect and visualize specific proteins' presence, localization, and\nabundance within breast cancer tissue samples. The IHC staining is typically performed for four key"}, {"title": "Methods", "content": "The procedure of classification of breast cancer molecular subtypes in this paper consists of two main parts. In\nthe first part, we trained a deep learning model for classifying tumor and non-tumor tiles in a WSI, intending\nto utilize only tumor regions for the classification of molecular subtypes. In the second part, we trained\nseparate classifiers for breast cancer molecular subtyping. In the following subsections, we delve into\ndescriptions of utilized datasets, preprocessing steps, and training of our models. The CNN classifier codes"}, {"title": "Datasets", "content": "In this study, we used two publicly available sets of data, one for classifying tumor and non-tumor tiles in\nH&E-stained WSIs and the other set for the classification of breast cancer molecular subtypes."}, {"title": "Classification of tumor and non-tumor regions", "content": "To\ndevelop and test a binary deep learning model for tumor and non-tumor classification of image tiles, we used\n195 WSIs of TCGA-BRCA [19] and 129 WSIs of BRACS [20] datasets. The BRACS dataset has annotated\nregions of interest that can be used to extract tumor tiles from WSIs. The TCGA-BRCA dataset has no\nofficially published annotations of regions in WSIs. However, there are 195 WSIs from TCGA-BRCA in the\nDRYAD [21] dataset that have annotated tumor regions. Since both images and annotations in the DRYAD\ndataset are downsized to a 1/10 scale, we replicated the annotated regions for the full-size WSIs and then\nextracted image tiles from those areas."}, {"title": "Classification of breast cancer molecular subtypes", "content": "The selection of datasets for classifying breast cancer molecular subtypes was based on the availability of\npertinent labels for WSIs. According to a review paper we published earlier on publicly available datasets of\nbreast cancer histopathology images [22], there were only two datasets featuring these specific labels: TCGA-\nBRCA and CPTAC-BRCA [23]. From 1,134 available WSIs in the TCGA-BRCA dataset, we acquired 980\nWSIs labeled with molecular subtypes and excluded 154 WSIs that either lacked labels or were categorized as\n\"Normal-like\" tumors. In the CPTAC-BRCA dataset, there are 640 WSIs of breast tissue. Of these, 382 WSIs\nare labeled with molecular subtypes, and the remaining 258 -either unlabeled or labeled as \"Normal-like\"-\nwere excluded from this study. In both TCGA-BRCA and CPTAC-BRCA datasets, the major and minor\nclasses were LumA and HER2, with an overall share of 50.1% and 8.6%, respectively, which can cause\nsignificant dataset imbalance. To mitigate this imbalanced distribution of classes, we added the HER2-\nWarwick dataset [24], which has 86 H&E-stained WSIs of invasive breast carcinomas from 86 patients, 71 of\nthose with positive HER2 expression scores that were used in our study"}, {"title": "Preprocessing", "content": "Typically, WSIs are extremely large, containing billions of pixels, and cannot be directly fed into any deep\nlearning model. Therefore, regions of interest within the WSIs are divided into tiles to be compatible with\nthese algorithms."}, {"title": "Classification of tumor and non-tumor regions", "content": "For the classification of tumor and non-tumor tiles, we used QuPath [25] software to make\nand extract non-overlapping tiles with size 512\u00d7512 at 0.5 \u00b5m/pixel magnification from the annotated tumor\nregions in TCGA-BRCA and BRACS WSIs. This yielded 38,392 tumor tiles from the TCGA-BRCA and\nBRACS datasets, sufficient to serve as the tumor class for training a binary classifier. Additionally, we\nextracted 37,407 tiles from the non-tumor areas in the same WSIs to have roughly balanced tumor and non-\ntumor classes. The non-tumor class comprised normal tissues, folded tissue areas, marker signs, and white\nareas on the slides to prevent extra steps to remove low-quality and white tiles for the classification of breast\nmolecular subtypes in the subsequent steps. All 75,799 tiles were then split into 70%, 15%, and 15%\nfor training, validation, and testing, respectively, ensuring that tiles from each WSI were assigned exclusively\nto one of these sets. This split also maintained the balance of classes in each set."}, {"title": "Classification of breast cancer molecular subtypes", "content": "Data preprocessing for the classification of breast cancer molecular subtypes was more comprehensive. In the\nfirst step, we used QuPath to detect the tissue areas in 1,433 WSIs. Detected areas were then divided into tiles\nwith size 512\u00d7512 at 0.5 \u00b5m/pixel magnification in TIFF format without compressing the image data. For\nLumA, LumB, and BL classes, we extracted the tiles without overlapping. However, to increase the number of\nextracted tiles (instances) for the minority class, we set an overlap of 64 pixels for the HER2 WSIs. This is\ndue to the fact that WSIs in the HER2-Warwick dataset originate from biopsies rather than tissue resections,\nresulting in fewer image tiles compared to surgical resections.\nSince normal areas and artifacts in the image do not contribute to our classification task, we chose to use only\ntumor tiles. However, most of the WSIs in the TCGA-BRCA dataset and all WSIs of the CPTAC-BRCA and\nHER2-Warwick datasets lacked annotations of tumor areas. Therefore, to take only tumor tiles, we fed all\n3,571,651 extracted tiles to the earlier trained binary tumor/non-tumor classifier to determine the likelihood of\neach tile belonging to the tumor class. Following that, to create a balanced dataset with a nearly equal number\nof tiles in each of the four classes of breast cancer molecular subtypes, namely LumA, LumB, HER2, and BL,\nwe used the minor class (HER2) as the reference class with 278,675 tumor tiles and balanced the four breast\ncancer classes based on that. Since the number of WSIs in each class was different, we took 441, 1180, and\n1410 random tumor tiles per WSI from LumA, LumB, and BL classes, respectively, to have a roughly equal\ndistribution of tiles among each class. It is\nimportant to note that the actual counts of selected tumor tiles for model development differ from the expected\nvalues. This discrepancy arose because many WSIs contained small tumor regions, resulting in fewer tumor\ntiles than specified for each class."}, {"title": "Model training", "content": "To train the tumor/non-tumor classifier, we used the Inception_V3 [28] architecture with pre-trained weights,\nimplemented in PyTorch (version 1.7.1 + cu110). Inception_V3 is known for its efficiency in capturing\ncomplex hierarchical features through the use of Inception modules, which perform convolutions of various\nsizes (1x1, 3x3, 5x5) within the same layer, allowing the network to capture multi-scale features effectively.\nThe architecture also incorporates auxiliary classifiers at intermediate layers to help propagate gradients and\nimprove convergence during training. Inception_V3's modular approach allows it to capture intricate patterns\nand features within histopathology images [29, 30], making it a suitable choice for this classification task. The\nauxiliary classifiers embedded within the network also aid in preventing gradient vanishing issues, which can\nbe prevalent in deep networks.\nThe training was performed using an RTX-3090 GPU with 24 GB of VRAM. The hyperparameters were set\nas follows: a batch size of 64, a learning rate of le-5, and a dropout rate of 0.33 to prevent overfitting. We\nused the ADAM optimizer, known for its robustness to sparse gradients, and the cross-entropy loss function,\nwhich is suitable for binary classification."}, {"title": "Deep convolutional neural networks", "content": "Classification of tumor and non-tumor regions"}, {"title": "Classification of breast cancer molecular subtypes", "content": "For classifying breast cancer molecular subtypes from H&E-stained images, we trained four separate binary\nclassifiers using the One-vs-Rest (OvR) strategy to simplify a complex multi-class classification into binary\ntasks. For training and validation, 70% of the selected tumor tiles were allocated (the gray part of the pie chart\nin Fig. 2), with 80% (56% of the total data in light gray) for training and 20% (14% of the total data in dark\ngray) for validation (model fine-tuning). Each classifier was trained using the ResNet-18 architecture [31],\nwith pre-trained weights, incorporating all tiles from the target subtype and one-third from each of the other\nthree subtypes as the rest class, ensuring balanced binary classes (Fig. 2 and Fig. S3) for training the models.\nThis architecture was chosen due to its proven effectiveness in various image classification tasks, offering a\ngood balance between depth and computational efficiency [32, 33]. Moreover, ResNet-18's relatively shallow\ndepth compared to deeper variants ensures faster training and inference times without significant loss in\naccuracy, which is crucial when training four deep classifiers with large-scale datasets. The training was\nperformed using the same hardware with the following hyperparameters: a batch size of 128, a learning rate of\n5e-6, a dropout rate of 0.33, the ADAM optimizer, and the cross-entropy loss function."}, {"title": "Thresholding", "content": "The output of a binary classifier for the target class is a single score between 0 and 1 for each image tile. To\naggregate these scores into a definitive classification for each WSI, we set a threshold for each classifier,\nwhich determines the predicted class of each tile. Using a fixed threshold of 0.5 for assigning classes to\nimages can be sub-optimal [34, 35]. Therefore, we employed Precision-Recall (PR) analysis to establish the\noptimum decision thresholds for each classifier. PR analysis aids in identifying a threshold that balances\nprecision (positive predictive value) and recall (sensitivity) effectively. The PR curve plots precision against\nrecall, enabling the selection of a threshold that maximizes the classifier's performance. Although our classes\nare balanced, this approach is particularly relevant as we train four separate binary OvR classifiers, where the\ntarget class in each classifier is treated as the positive class. This allows us to fine-tune each OvR classifier for\noptimal performance in distinguishing the target class from all others. We used only the validation set, which\nconstitutes 20% of the classification data (equivalent to 14% of the total tumor tiles), to adjust the threshold of\nOvR classifiers. The optimum thresholds for LumA, LumB, HER2, and BL classifiers were determined to be\n0.434, 0.415, 0.481, and 0.424, respectively. These thresholds were applied in subsequent stages of our study."}, {"title": "XGBoost", "content": "By feeding the tumor tiles into the four binary OvR classifiers, we obtained eight scores for each tile, two\nfrom each classifier. Each pair of scores represents the classifier's confidence in the images belonging to\neither the target class or the rest class. To aggregate the tile-wise predictions of four OvR CNNs to classify the\nmolecular subtype of WSIs, we quantified the number of tiles within a WSI that had scores exceeding the\noptimal threshold for each class. This process involved counting the tiles classified as LumA, \u00acLumA (not-\nLumA), LumB, \u00abLumB, HER2, \u00abHER2, BL, and \u00acBL. Subsequently, these eight values were used as new\nfeatures to train an XGBoost model, which predicts the molecular subtype of the WSIs (Fig. 3). XGBoost, an\nimplementation of gradient-boosted decision trees, is a highly flexible and versatile machine learning tool,\nproven effective in a wide range of supervised learning tasks, demonstrating its ability to learn complex\npatterns in large volumes of data [27, 36].\nTo train and fine-tune the XGBoost model, we allocated 15% of the entire dataset (the yellow part of the pie\nchart in Fig. 2), which was further divided randomly into 80% for training and 20% for validation. We used\nthe thresholds established in the earlier classification step to count the tiles predicted as either the target class\nor the rest class for all four breast cancer molecular subtypes in each WSI."}, {"title": "Results", "content": ""}, {"title": "Classification of tumor and non-tumor tiles", "content": "The model trained on 75,799 tiles labeled as 'tumor' or 'non-tumor' from TCGA-BRCA and BRACS datasets\nwas evaluated using the F1 score, achieving a value of 0.954. The F1 score provides a balance between\nprecision and recall (sensitivity), making it a suitable metric for our classification task. Such a high F1 score\nindicates that the model is reliable and performs very well in distinguishing between tumor and non-tumor\ntiles."}, {"title": "Classification of breast cancer molecular subtypes", "content": "We evaluated the entire pipeline on a hold-out test set (15% of the entire data, shown as the red part in the pie\nchart of Fig. 2) consisting of 221 H&E WSIs that had not been used in any part of the workflow. Table 4\npresents the classification results for breast cancer molecular subtypes within the test set at the WSI level,\nobtained using an XGBoost model that aggregates predictions from four binary OvR models.\nThe performance of our model varies significantly across different breast cancer subtypes. We present the F1\nscore as our primary metric, commonly used in multi-class classification problems to balance precision and\nsensitivity, thereby providing a more comprehensive view of model performance. For the LumA subtype, the\nmodel achieved an F1 score of 0.922, indicating strong performance in correctly identifying cases of this"}, {"title": "Discussion", "content": "In this study, we developed a supervised deep learning model to investigate whether H&E-stained\nhistopathological images contain sufficient information for classifying breast cancer molecular subtypes. IHC\nstaining is fundamental for molecular subtyping of breast cancer, offering greater precision than H&E staining\nbut at a higher cost and longer processing time. However, it is susceptible to inter-observer variability, which\ncan lead to diagnostic discrepancies. IHC-based classification may not always correspond with gene\nexpression profiles, with discrepancies up to 31% [37", "6": "."}]}