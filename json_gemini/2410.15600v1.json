{"title": "Patrol Security Game: Defending Against Adversary with Freedom in Attack Timing, Location, and Duration", "authors": ["HAO-TSUNG YANG", "TING-KAI WENG", "TING-YU CHANG", "KIN SUM LIU", "SHAN LIN", "JIE GAO", "SHIH-YU TSAI"], "abstract": "We explored the Patrol Security Game (PSG), a robotic patrolling problem modeled as an extensive-form Stackelberg game, where the attacker determines the timing, location, and duration of their attack. Our objective is to devise a patrolling schedule with an infinite time horizon that minimizes the attacker's payoff. We demonstrated that PSG can be transformed into a combinatorial minimax problem with a closed-form objective function. By constraining the defender's strategy to a time-homogeneous first-order Markov chain (i.e., the patroller's next move depends solely on their current location), we proved that the optimal solution in cases of zero penalty involves either minimizing the expected hitting time or return time, depending on the attacker model, and that these solutions can be computed efficiently. Additionally, we observed that increasing the randomness in the patrol schedule reduces the attacker's expected payoff in high-penalty cases. However, the minimax problem becomes non-convex in other scenarios. To address this, we formulated a bi-criteria optimization problem incorporating two objectives: expected maximum reward and entropy. We proposed three graph-based algorithms and one deep reinforcement learning model, designed to efficiently balance the trade-off between these two objectives. Notably, the third algorithm can identify the optimal deterministic patrol schedule, though its runtime grows exponentially with the number of patrol spots.\nExperimental results validate the effectiveness and scalability of our solutions, demonstrating that our approaches outperform state-of-the-art baselines on both synthetic and real-world crime datasets.", "sections": [{"title": "1 Introduction", "content": "Public safety is crucial for ensuring a thriving and harmonious society. In responding to criminal activities, it is essential to account for game-theoretic models and strategic behaviors, a core concept of Stackelberg security games (see [76] for further detail). In this framework, the problem is modeled as a Stackelberg game, wherein a defender, with a limited set of resources, protects a set of targets, and an attacker plans attacks after observing the defender's strategy. The goal is to compute a Stackelberg equilibrium-a mixed strategy for the defender that maximizes their utility, taking into account that the attacker is aware of this strategy and will respond optimally. This approach extends to cyber-physical systems, including the deployment of mobile robots for autonomous security enforcement [70]. This domain is often referred to as patrolling security games or adversarial patrolling games [5, 15, 17, 20, 23, 36, 82]. These games are modeled as two-player, multi-stage interactions over an infinite time horizon, in which the defender controls a patroller moving between vertices on a graph to protect targets, while the attacker chooses when and where to launch an attack.\nA common approach to analyzing or solving patrolling security games is to formulate them as mixed-integer linear programming problems and compute approximate optimal strategies for the defender. However, given the infinite time horizon in these games, the number of pure strategies is infinite. To address this, additional constraints are often imposed to reduce the strategy space. For instance, time constraints may be simplified by ignoring the time it takes for a patroller to move between locations, assuming that movement time is negligible compared to time spent guarding [17, 23, 34, 68, 75]. Other works adopt specific attacker models, such as attackers that require a fixed period to execute an attack [15], or models that introduce an exponential discount factor on the attacker's utility [82]. Despite these constraints, which limit the number of pure strategies, scalability remains a significant challenge due to the exponential growth of the strategy space [64].\nProblem Statement We consider a generalization of zero-sum patrolling security game (PSG), in which the attacker is given not only the freedom to decide when and where to launch the attack but also the duration of the attack in order to maximize the expected payoff. The attacker's payoff is the acquired utilities of the attack minus a penalty if the attacker is caught by the defender in patrol. To the best of our knowledge, this is the first work considering varying attack duration in the patrolling game. We consider three different attacker models which affects how much information that the attacker can possibly gain by observing the patrol routes. The game is converted to a minimax problem with geometric properties. One main challenge is the exponentially increased size in the solution space due to varying attack durations. Furthermore, for general utility functions, the problem of finding optimal defender strategy is not convex in general.\nOur Contribution To tackle the problem, we first focus on a subset of the defender strategy which is restrcited as a time-homogeneous first-order Markov chain. Finding the optimal defender strategy under this subset can be formulated as a closed-form minimax problem. In special cases with the zero penalties, the optimal solutions can be linked to minimizing the expected pairwise/ average hitting time or return time, depending on the visibility model of the attacker. In a scenario of high penalties, increasing the entropy of visiting time for each site helps to reduce the attacker's expected payoff, since the attacker would pay a high price if he is getting caught, even with a small chance. Thus, a randomness patrol schedule with high entropy of visiting time is beneficial to decrease the attacker's payoff. By the"}, {"title": "2 Related Work", "content": "2.1 Surveillance and Security Game\nPatrolling and surveillance problems have been extensively studied in the fields of robotics (see the detailed survey by Basilico et al. [14]) and operations research [72]. In non-strategic settings, algorithms are designed for traversing a specified region using centralized optimization to achieve specific objectives [33, 43, 59, 69, 78]. For example, Alamdari et al. [6] address the problem of minimizing the maximum duration between consecutive visits to a particular location, providing a log n-approximation algorithm for general graphs. Subsequent research has expanded on these results for specific graph structures, such as chains and trees [65]. Recently, this objective has been extended to multi-agent scenarios [2, 3].\nIn strategic settings, patrol strategies are designed to defend against intelligent intruders who seek to avoid detection. Consequently, many studies model patroller movements as Markov chains or random walks to introduce unpredictability into patrol routes [11, 25, 31, 38, 66], focusing on metrics such as efficiency or entropy. For instance, Patel et al. [66] investigate minimizing the first-passage time, while Duan et al. [31] examine maximizing the entropy of return times. Salih et al. [24] combine game theory with these approaches to estimate expected return times. These objectives can also be discussed in more advanced random walk settings [21, 30], corresponding to different defender models.\nA more advanced strategic settings which explicitly define the interaction between defenders and attackers, called Stackelberg security games. In this field, Kiekintveld et al. [50] introduced a general framework for security resource allocation, which has since been widely applied in various security domains with differing scenarios [1, 4, 16, 58, 81]. Notable applications include the deployment of randomized checkpoints and canine patrol routes at airports [68], deployment scheduling for U.S. Federal Air Marshals [46, 79], and the patrol schedules for U.S. Coast Guard operations [34, 74] and wildlife protected rangers [53, 86]. On the other hand, various adaptations of the security game"}, {"title": "2.2 TSP", "content": "The problem of planning patrol routes is related to the general family of vehicle routing problems (VRPs) and traveling salesman problems (TSPs) with constraints [12, 56, 67, 87]. This is a huge literature thus we only introduce the most relevant papers.\nTSP is a well-known NP-complete problem in combinatorial optimization and has been discussed in operation research [8, 26, 51]. Christofides algorithm [28] provides a tour whose length is less or equal to 1.5 times of the minimum possible. Additionally, there are two independent papers that provide polynomial-time approximation scheme (PTAS) for Euclidean TSP by Mitchell and Arora [9, 62]. There are many variations of TSP that consider multiple objectives [13, 19]. In this work, one objective is to increase the randomness between generated tours. A close-related objective called \"diversity\" has been discussed recently with other combinatorial problems such as diverse vertex cover [39] or diverse spanning trees [35]. However, to the best of our knowledge, TSP had not been studied in the terms of diversity or tours with high randomness. The other objective is related to minimize the maximal weighted latency among sites of the tour, which has been discussed in some works [2, 3, 6, 61]. One difference is that this work generalizes the \"weight latency\" as functions rather than constant weights.\nIn recent years, the integration of Deep learning into combinatorial optimization problems has seen significant advances, including the TSP problem A pivotal development in this domain was introduced by Vinyals et al., who developed the Pointer Network (PN), a model that utilizes an attention mechanism to output a permutation of the input and trains to solve TSP [80]. Building on this, Bello et al. enhanced the PN by employing an Actor-Critic algorithm trained through reinforcement learning, further refining the network's ability to optimize combinatorial structures [18]. The application of PN was further extended by Kool et al., who incorporated a transformer-like structure to tackle not only TSP but also the Vehicle Routing Problem (VRP), the Orienteering Problem (OP), and a stochastic variant of the Prize Collecting TSP (PCTSP). [55]. In addressing challenges associated with large graph sizes, Ma et al. [60]"}, {"title": "3 Problem Definition", "content": "The patrol game is structured as a Stackelberg zero-sum game. That is, the defender executes a strategy first and the attacker chooses the best strategy based on the defender's executed strategy. The attacker's objective is to choose a strategy that maximizes his (expected) payoff and the defender's objective is to choose a strategy that minimizes the attacker's maximum expected payoff.\nMathematically, given a tuple (G, H, M), where G = (V, E, W) is a weighted graph with vertices V = {1, 2,\u2026\u2026n}, edge set E, and edge-weight matrix W representing the traveling costs. M is the penalty cost (M > 0) and each vertex j has a utility function hj \u2208 H. Time is discretized into time slots. The attacker can launch one attack and can decide where (j), when (t) and how long (T) the attack lasts. During the attack, at the (t + t')-th time slot the attacker collects a utility $h_j (t)$, where $1 \u2264 t \u2264 T$. Note that the utility function can be node dependent. We assume that $h_j (t) \u2265 0$ always. If the attacker is caught by the defender at the (t + t')-th time slot, the attacker would pay a penalty M and be forced to stop the attack. Thus, the total collected utilities of the attacker is $\\sum_{t'=1}^T h_j (t) \u2013 M$. Otherwise, the total collected utilities is $\\sum_{t'=1}^T h_j (t)$ if the attacker is not caught.\nNotice that in the adversarial patrolling games, it is possible that the attacker waits for a long time and acquires additional information such as when the patroller passes by. In the literature, there are different models which specify how much information the attacker can collect.\n\u2022 Full visibility: The attacker has a probe in each site such that it would notify the position of the patroller when he arrives any site during the game. This model is used in Patrolling Security Games [17, 82].\n\u2022 Local visibility: The attacker would have to choose a site j first and would launch an attack right after the patroller leaves site j [11].\n\u2022 No visibility: The attacker cannot know the patroller's positions during the whole game. This is a common assumption in [7, 68].\nIn general assumption, the attacker knows the strategy used by the defender before the game starts in any attacker models."}, {"title": "4 Strategy with First-order Markov Chain", "content": "To tackle the problem, the defender's strategy is restricted as a time-homogeneous first-order Markov chain (only in this section). That is, the patroller movement is modeled as a Markov process over graph G with a transition matrix P, which is known by the attacker. Notice that any high-order Markov chain can be \"flatten\" into the first order one by some standard methods (which takes time exponential on the order of the Markov chain) [17].\nTo calculate the attacker's payoff we use the notation of first visit matrix F [11], where each element represents the visit probability distribution from a site i to another site j. In detail, given graph G and transition matrix P, the probability of taking k slots for the patroller, starting at i to reach j for the first time is given by"}, {"title": "4.1 Attacker has full visibility", "content": "In the model of full visibility, the attacker knows the exact position of the patroller among all sites. Denote $Z_{i,j,T}$ as the expected payoff if the attacker launches an attack at j with the attack period T when the patroller is at i. In any time slot t during the attack, where $1 \u2264 t \u2264 T$, there are only 3 possible events: the patroller comes to site j (after visiting i) in the period of time 1 to t - 1, the patroller comes exactly at time t, or the patroller comes after time t. In the first case, the attacker cannot collect utility at time t since the attack is enforced to stop at t', where $t' < t$ (the penalty is also paid at time t' too). In the second case, the attack is caught at time t thus there is a penalty M substrated from the attacker's payoff. In the third case, the attacker collects utility $h_j (t)$. Thus, the expected payoff at time t, $1 \u2264 t \u2264 T$, can be expressed as a closed form associated with F.\n$Z_{i,j}(t) = (h_j(t) \u2013 M) \\cdot F_t (i, j) + h_j (t) (\\sum_{k=t+1}^\\infty F_k (i, j)).$  (1)\nThe total (expected) payoff during the whole attack period is $Z_{i,j,T} = \\sum_{t=1}^T z_{i,j}(t)$, which is called as the payoff matrix. The attacker chooses an element of Z with the highest payoff, which describes his strategy of when, where, and how long the attack lasts.\nFor the defender, the problem of choosing a best strategy can be formulated as a minimax problem:\n$\\min f(P)$, where $f(P) = \\max Z_{i,j,T}$.\nFor general utility function hj and penalty M, the Hessian matrix of f is not guaranteed to be semi-definite thus f (P) is not convex in general. However, in special cases f (P) has strong connection with the expected hitting time matrix A. If M = 0 and the utility functions are all constant functions, then f (P) is either \u221e or the maximum weighted expected hitting time of all pairs (i, j), with the weight for (i, j) as the constant of the utility function hj.\nPROOF. If the transition matrix P is reducible, i.e, there exists a pair of vertices i, j such that the patroller starting at i would never visit site j, then the attacker can choose to attack j for infinitely long. In this case $Z_{i,j,\\infty} = \\infty$.\nNow, assume that the transition matrix is irreducible. Denote by $h_j$ the constant of the utility function at site j. Given an attack period T, M = 0, from Equation 1, $Z_{i,j,T}$ can be simplified as\n$Z_{i,j,T} = h_j \\cdot \\sum_{k=1}^T k \\cdot F_k (i, j) + h_j \\cdot T \\cdot \\sum_{k=T+1}^\\infty F_k (i, j)$ (2)\nSince $z_{i,j} (t) \u2265 0$ for any t. Thus, taking T = \u221e period maximizes his payoff. That is,\n$f(P) = \\max Z_{i,j,\\infty} = \\max h_j\\sum_{k=1}^\\infty F_k (i, j) k = \\max h_j. a_{i,j}$ (3)"}, {"title": "4.2 Attacker has local visibility", "content": "In this model, assume the attacker's strategy is to attack site j with the attack period T. Denote $z'_j(t)$ as the utility he collects for every time t where $1 \u2264 t \u2264 T$,\n$z_j(t) = z_{j,j} (t)$ (4)\nBy a similar discussion in Observation 4.1, one can infer that the best strategy for the attacker is to attack the site with the longest expected (weighted) return time if the utility functions are all constants and the penalty is zero. If all edges have weight one, the optimal defender strategy can be derived by constructing an ergodic Markov chain with stationary distribution $\u03c0^*$, where $\u03c0_j^* = \\frac{h_j}{\\kappa}$, since the expected return time of a site j is 1/\u03c0 [73]."}, {"title": "4.3 Attacker has no visibility", "content": "In this case, the attacker has no information of the patroller's trace thus it is meaningless for the attacker to choose when to launch an attack; instead, the payoff of attacking site j is the expected payoff when the patroller is either at a random site i or travels on a random edge (i, j). For the following analysis, we only consider the attacks that starting at the time when the patroller is at exactly one of the sites. For general cases, it would underestimate the attacker's expected payoff at most $max_{i,j} (\\sum_{t=1}^{j,T} h_j(t))$ utilities.\nDenote $Z_{j,T}$ as the cumulative expected payoff for attacking j with period T and $z'_j (t)$ is the expected payoff at time t. Assume the attack is launched at a random time slot, $z'_j (t)$ is\n$2'(t) = \\sum_{i=1}^n \\pi_i\\cdot Z_{i,j} (t)$\nwhere \u03c0 is the stationary distribution with transition matrix P. Thus, the cumulative expected payoff is\n$Z'_T = \\sum_{t=1}^T z'(t) = \\sum_{t=1}^T (\\sum_{i=1}^n \\pi_i\\cdot Z_{i,j}(t) ) = \\sum_{i=1}^n \\pi_iZ_{i,j,T}.$  (5)\nDenote \u03bai as the Kemeny constant [48], the expected hitting time when the walk starts at i, $k_i = \\sum'_{j=1} a_{i,j}\u03c0_j$. It is known that the Kemeny constant is independent of the start node [49]. Thus, the Kemeny constant can be written as another formation\n$K = \\sum_{i=1}^n \\sum_{j=1}^n \\pi_ia_{i, j}\u03c0_j.$ (6)\nEquation 6 can be written as an expression with matrix A.\n$\u03ba = \u03c0^T A\u03c0.$ (7)"}, {"title": "4.4 High penalty scenarios", "content": "When M > hj(t) for all sites j and all time t, Equation 1 can be simplified as\n$z_{i,j}(t) = h_j(t)(\\sum_{k=t+1}^\\infty F_k (i, j)) \u2013 M \\cdot F_t (i, j)$.\nAssume that the attacker has full visibility and all utility functions are constants.\n$f(P) = \\max (h_j.T \u2013 (M + 1) \\cdot \\sum_{t=1}^T Ft (i, j))$. (11)\nAt the defender side, it is beneficial to increase $\\sum_{t=1}^T Ft (i, j)$ for all (i, j) pairs. Thus, having a schedule which is more random could help in this case. This observation also works in other two attacker models."}, {"title": "5 Graph-based Algorithmic Strategy", "content": "In the previous section, we show that in special cases (e.g. When the attacker has no visibility, the penalty is zero, and utility functions are all constants) the minimax problem of the zero-sum game is possibly solvable. In general, the optimization problem is not convex. Our solution for general cases is motivated by two observations. First, when the penalty is zero, the optimal schedule is to minimize the expected (pairwise/ average) hitting time or return time. Secondly, if the penalty is significant, it would be better to increase the randomness of the patrol schedule to \"scare\u201d the attacker away. In fact, there are prior works emphasizing each one as the objective for the patrol mission [14, 31, 32, 37, 66] but, to the best of our knowledge, this is the first work to incorporate both objectives at the same time."}, {"title": "5.1 TSP-based solution", "content": "The Algorithm TSP-based solution (TSP-b) is perturbing the optimal (or approximately optimal) deterministic EMR solutions by a parameter \u03b1. Adjusting this skipping parameter \u03b1 will balance the two criteria. Roughly speaking, the main idea is to traverse on a deterministic tour but each vertex is only visited with probability \u03b1 (i.e., with probability 1 - \u03b1 it is skipped). Obviously, Algorithm TSP-b generates a randomized schedule. Also, since the algorithm works with a metric (with triangular inequality), the total travel distance after one round along the tour is bounded by the original tour length. Hence, the expected reward can be bounded.\nThe following is the analysis of EMR and entropy rate for TSP-b when the utility functions are polynomial functions with the maximum degree d."}, {"title": "5.1.1 Analysis of TSP-b with the uniform utility functions", "content": "When the utility functions among all sites are the same, Algorithm TSP-b firstly generates a approximated-TSP tour Q = {q1, q2,\u00b7\u00b7\u00b7 qn}, qi \u2208 {1, 2,... n} by, for example, a PTAS algorithm [10, 62]. Denote Y as the randomized schedule perturbed by \u03b1. Now, assume the site of an arbitrary index k in the schedule is i, i.e., Yk = i, without loss of generality, the tour Q is shifted such that q1 = i. Thus, the probability of"}, {"title": "5.1.2 Analysis of TSP-b with non-uniform utility functions", "content": "In the case of non-uniform utility functions, TSP-b firstly generates the deterministic schedule by Bamboo garden trimming (BGT) algorithm [61] and then perturb it into a randomized schedule with \u03b1. One can describe BGT as a vertex-weighted version of TSP. The objective is to output schedule such that the maximal weighted visited time among all sites is minimized. For the input, the graph is set up as G and each vertex j has a weight $l_j$, which is the coefficient of degree d in $h_j$, where d is the maximum degree among all sites. BGT divides sites into groups such that the weight of each group is less than 2. Then, the patroller visits one"}, {"title": "5.2 Biased Random Walk", "content": "Algorithm Biased Random Walk (Bwalk) uses a biased random walk to decide the patrol schedule. Define matrix $W' = (w' (i, j)) \u2208 Z^{nxn}$. For each pair (i, j),\n$w' (i, j) = 1/a^{w(i,j)}$, \u03b1 > 1,\nwhere \u03b1 is an input parameter. Define stochastic matrix P' as\n$P' (i, j) = \\begin{cases} \\frac{w' (i,j)}{\\sum_{(i,j')\u2208E} W' (i,j')} & if (i, j) is an edge\\\\ 0 & otherwise. \\end{cases}$"}, {"title": "5.2.1 Analysis of Bwalk with same utility functions", "content": "In this case, Bwalk repeatedly generates a set of randomized tours {S1, S2,...}. Each tour St is an Euler-tour traversing on a randomized spanning tree It, where It is generated by the biased random walk with transition probability P'.\nLet $(B_k; k \u2265 0)$ be the biased walk on G with $B_0$ arbitrary. For each site i, let $v_i$ be the first hitting time:\n$v_i = \\min{k \u2265 0 : B_k = i}$.\nFrom $(B_k; k \u2265 0)$, a randomized spanning tree $\u0393$ can be constructed, which consists of these n - 1 edges,\n$(B_{vi-1}, B_{v_i}); i \u2260 B_0$.\nNotice that the probability of generating a specific tree $\u0393$ is proportional to the product of $w' (i, j)$, for all edge (i, j) \u2208 \u0393 [63]. Thus, by controlling the input parameter \u03b1, the two criteria can be balanced.\nDenote the schedule generated by Bwalk as $Y_\u03b2$. If \u03b1 = 1 and assume that graph G is a complete graph, St is actually a random permutation of n sites, which has the entropy $log n + log(n - 1) + 1 = log(n!) = O(nlogn)$. Thus, the entropy rate of $Y_\u03b2$ is\n$H_{Y} (Y_\u03b2) = lim \\frac{\\sum_{l=1}^m H(S_l)}{m} = lim \\frac{mnlogn}{m} = O(log n)$.\nOn the other hand, the expected reward is bounded by the expected time of traversal on the uniform random spanning tree. Since each edge is traversed at most twice, the length of the tour is less than $2\u03b7n$, where \u03b7 is the maximum distance among all edges. Thus, the maximum payoff of the attacker is actually $max_j \\sum_{t=1}^{\u03b7n} h_j(t) = O(nd+1)$, if the utility function is polynomial with maximum degree d.\nIn other cases that \u03b1 > 1, the generated spanning tree is more likely a low-weight tree. Thus, the traversing distance is lower which makes EMR lower. However, the entropy would also become lower due to the probability distribution among all generated spanning tree is more \"biased\"."}, {"title": "5.3 Walk on State Graph", "content": "Algorithm Walk on the State Graph (SG) with a parameter \u03b1 generates the schedule by a state machine with the transition process as another random walk."}, {"title": "5.3.1 Deterministic SG", "content": "One characteristic of deterministic SG is that it generates the optimal deterministic schedule for any utility functions and has the running time exponential in the number of sites.\nDefine D is a state machine and each state x is a (n+1)-dimension vector x = (x1, x2, \u00b7\u00b7\u00b7, xn, kx), where xj \u2208 R, xj \u2265 0 and kx \u2208 {1,..., n}. xi represents the maximum utility the attacker could have collected since the last time the defender leaves site i. The last variable represents the defender's current position.\nState x, y is said to have an arc from x to y if y = (Y1, Y2, \u2026, Yn, ky), where\n$Y_i = \\begin{cases} h_i(x_i + d(k_x, k_y)), & if i \u2260 k_y\\\\ 0, & otherwise. \\end{cases}$\nd(kx, ky) represents the time needed to travel from kx to ky. An arc represents the change of state from x to y when the defender moves from kx to ky.\nClearly, any periodic R schedule of the defender can be represented as a cycle on the state machine defined above. Further, the state diagram captures all the information needed to decide on the next stop. Although there could be infinitely many states as defined above, only a finite number of them is needed. Basically, let's take a periodic schedule S with the kernel as some traveling salesman tour C. Suppose the maximum utility of this schedule is Z. Z is finite and is an upper bound of the optimal value. Thus, all states x that have any current utility of xj greater than Z can be removed. This will reduce the size of the state machine to be at most O(Zn).\nNow we attach with each edge (x, y) a weight as the maximum payoff among all variables within state x, y. That is,\nw(x, y) = max{x1, xn, Y1, Yn}.\nFor any cycle/path in this state machine, define bottleneck weight as the highest weight on edges of the cycle/path. The optimal deterministic schedule is actually the cycle of this state machine with the minimum bottleneck weight. To find this cycle, the first step is to find the minimum bottleneck path from any state u to any state v by Floyd-Marshall algorithm. The total running time takes time O(|V|\u00b3), where |V| is the number of vertices (states) in the state machine. The optimal tour is obtained by taking the cycle $u \u2192 v \u2192 u$ with the minimum bottleneck value for all possible u, v. The total running time is still bounded by O(|V|\u00b3)."}, {"title": "5.3.2 Non-deterministic SG", "content": "Since the state graph records the utility that would be collected at each site from the historical trace at each state, we run a random walk on the state graph with a probability dependent on the utility of the state."}, {"title": "6 Reinforcement Learning Strategy", "content": "Following the discussion in Section 4, the patrol game is formulated as a Markov decision process. It is natural to consider solving this game using deep reinforcement learning (DRL), as explored in works such as [71]. However, applying DRL directly to our scenario presents several challenges.\n(1) The existence of an infinite number of patrol strategies.\n(2) A lack of a closed-form solution for the attacker's optimal strategy.\n(3) Delayed feedback on the attacker's strategies.\nThe second challenge implies the necessity of an additional heuristic method to effectively model attacker behavior, complicating the framework (e.g., incorporating a GAN structure). These challenges also increase convergence diffi- culties [27, 57], and, more critically, may result in the model becoming trapped in local minima, yielding suboptimal solutions. To address these challenges, we proposed Graph pointer network-based (GPN-b) model that draws on the intuitions outlined in Section 5, focusing on two criteria: Expected Maximal Reward (EMR) and entropy. GPN-b seeks to solve the traveling salesman problem while incorporating randomness, using a hyper-parameter \u03b1 to balance these criteria.\nThe learning process of GPN-b integrates several advanced deep learning techniques applied recently to NP-hard routing problems such as the TSP. The framework adopts a transformer-like architecture and utilizes a \"rollout baseline\" to smooth the training process [52, 55]. Since the model is a MDP, one can calculate the distribution among the policy in each state and then derive the entropy of the generated tours. We add an additional loss term of the randomness with the derived entropy in the training phrase such that the model is able to learn the way of generating efficient tours and increase the entropy of its policy at the same time. On the architecture part, the model is based on the work of graph pointer network [60], which is an autoencoder design with a graph convolutional network (GCN) as the encoder, augmented by an LSTM. The decoder employs an attention mechanism, enabling adaptability to various graph structures [80]. In the following sections, we give the details of our model design, the training methodology, and preliminary experimental results."}, {"title": "6.1 Model and Training", "content": "Given a graph, the tour of the graph can be treated as a rearrangement or permutation n of the input nodes. We can formulate it as the Markov decision process(MDP).\n$\u03c0 = (\u03c0_0, ..., \u03c0_{N\u22121}), where \u03c0_t \u2208 (0, ..., N \u2013 1) and \u03c0_i \u2260 \u03c0_j iff i \u2260 j$ (13)\nAt each time step t, the state consists of the sequence of action made from step 0 to t - 1, while the action space at time step t is the remaining unvisited nodes. Our objective is to minimize the length of the action sequence made by MDP, so we define the negative tour length as our cumulative reward R for each solution sequence. Now we can define the policy p(\u03c0|g) which can generate the solution of instance g parameterized by parameter 0 as below:\n$p(\u03c0|g) = \\prod_{t=0}^{N-1} P_\u03b8(\u03c0_t|g, \u03c0_{1:t\u22121})$ (14)\nwhere $p_\u03b8(\u03c0_t|g, \u03c0_{1:t-1})$ provide action $a_t$ at each time step t on instance g. Overall, the model takes the input of the coordinates among all sites X and put into the encoder. Then, the decoder starts to output the desired policy (i.e., a site) step by step recurrently till the sequence includes all the sites. The model structure of the encoder and decoder is shown at fig. 1 which is base on the work of Ma et al [60]."}, {"title": "6.1.1 Preliminary experiment", "content": "Before applying our model to the patrol problem, we conducted preliminary experiment to observe the model's edge usage and path length statistics in graphs(fig. 2). For each setting of \u03b1,\nIn these tests, all graphs were generated by sampling 10 sites from a uniform distribution within a unit square and all graphs are complete graph. Each model with different parameters of \u03b1 is trained via these graphs by 20 epochs and 512 batch size. Each epoch includes 2500 steps which takes around 4 minutes on NVIDIA RTX 4090 GPU. Thus, the total running time for training a model takes around 80 minutes.\nFor the edge usage test, we generated single graph and ran the model with different \u03b1 1,000 times, recording the usage frequency of each edge. For the path length statistics, we generated 1,000 graphs as instance for models and recording the resulting path lengths. In this experiment, we can see the trade-off between total length and randomness at various values of \u03b1. Figure 2 (a) shows that when \u03b1=1, the distribution of used edges is highly skewed, and as \u03b1 increases, the edge usage distribution becomes more uniform. Concurrently, Figure 2 (b) illustrates that the path length tends to increase with rising \u03b1 values. Based on these observations, we can confidently assert that the model effectively adjusts the two conflicting criteria according to the settings of \u03b1."}, {"title": "6.2 Incorporating BGT", "content": "For cases involving uniform utility weights, where the utility functions are consistent across all sites, directly applying the aforementioned model with an appropriate hyperparameter \u03b1 yields the desired tours that effectively balance Expected Maximal Reward (EMR) and entropy. In scenarios with non-uniform utility weights, it is necessary to integrate the BGT algorithm [61"}]}