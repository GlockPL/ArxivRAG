{"title": "Honest Al: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG", "authors": ["Xinxi Chen", "Li Wang", "Wei Wu", "Qi Tang", "Yiyao Liu"], "abstract": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune \"small\" language models to say \"I don't know\" to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question\u00b9. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark [27]. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), as a type of foundation models with general language capabilities, have eclipsed traditional Natural Language Processing (NLP) models that focus on specific tasks in majority of NLP applications since the inception of GPT [1]. Supervised fine tuning (SFT) using labeled data and reinforcement learning from human feedback (RLHF) using preference data have proven effective in further enhancing LLMs' performance and alignment (e.g., ChatGPT [17] for question answering applications). However, LLMs suffer from hallucination, which hinders their application in accuracy-sensitive scenarios, such as the enterprise applications.\nTo alleviate the hallucinations of LLMs, several approaches have been proposed, including retrieval-augmented generations (RAGs) [12] and fine-tuning with domain-specific knowledge. In this paper, we summarize the approaches we tried in the 2024 Meta KDD Cup competition with Comprehensive RAG Benchmark (CRAG) data [27]. Since the CRAG benchmark focuses on difficult problems for LLMs, vanilla LLMs fail to perform well right out of the box. It turns out that RAG alone is not enough to alleviate hallucination in the benchmark and fine-tuning is needed to achieve higher accuracy. Our results show that the hybrid approach using both RAG and fine tuning performs best in CRAG. With those approaches combined, our team, Team Future, ranked high in each task of the competition and won first place in the false premise question in Task 2 (Fig. 1). The review of related work is summarized in section 2, followed by a description of methodologies in section 3. The results are shown in section 4, with conclusions and future works discussed in section 5 and 6."}, {"title": "2 Related Work", "content": "LLMs are very good at memorizing the content used for their pre-training. However, there are many drawbacks to using the memorization capability directly for Question Answering (QA) tasks. For example, depending on the size of the model, the quality of the pretraining data, and the type of questions, LLMs' memorization capability can be very limited and difficult to control. LLMs are also challenging to update except through retraining or fine-tuning, so they cannot handle questions on recent events if deployed. Arguably, the most problematic drawback of using LLMs for QA is that they can hallucinate, especially when the models are unsure if the context contains the information needed to answer a question.\nFine-tuning is a straightforward way to update the knowledge of LLMs when new information becomes available. However, due to the scarcity of GPUs and limited access to high-quality data, it is not feasible for many use cases. Furthermore, the behavior of fine-tuning language models is not well studied, and fine-tuning with poor data or practices might decrease the model's capabilities, causing modality collapse unless more complex methods such as RLHF are used [17].\nRetrieval-Augmented Generation (RAG) [8, 12] is a popular method to address the shortcomings of LLMs, by augmenting them with non-parametric data sources, and leveraging LLMs' powerful in-context learning [2] capability. [8] grouped the approaches of using RAG for LLMs into three categories: Naive RAG, Advanced RAG [14, 31], and Modular RAG [22, 29]. Naive and Advanced RAG approaches are widely used in practice due their simplicity and low development cost. These approaches generally consist of three parts: curating non-parametric databases, retrieving relevant snippets from the databases given the query, and generating responses using LLMs through in-context learning and prompt engineering with the related snippets.\nWhile the research on combining LLMs and RAG for QA mainly focuses on text, there is also research exploring the use of resources beyond text, such as images [4], audio [30], video [26], and code [16] to enhance the capabilities of language models.\nThere are various efforts to create RAG benchmarks and proposing appropriate evaluation metrics in recent years. [27] is one of the recent ones, which forms the foundation of this paper. [27] created a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) searches. It also proposes an evaluation mechanism that distinguishes between hallucinations and missing answers, and assigns a higher penalty to hallucinations. [3] created a RAG evaluation benchmark in both English and Chinese, and analyzed different LLMs from 4 aspects: noise robustness, negative rejection, information integration, and counterfactual. They found that LLMs demonstrate a certain degree of noise robustness, but struggle significantly in other aspects.\nApart from specific RAG datasets, there are many existing QA datasets that include context passages for each question. These datasets can also be used for RAG experiments, and cover a wide range of questions such as multiple-choice QA [5, 18, 23], single-hop QA [10, 11, 19], multi-hop QA [9, 24, 28], and domain-specific QA [6, 15, 25]."}, {"title": "3 Methodologies", "content": "In this section, we will first introduce the CRAG dataset and then describe the main approaches we have tried, including native RAG, fine-tuning, and hybrid approaches."}, {"title": "3.1 Dataset", "content": "The CRAG dataset includes five question domains (e.g. movie, sports, and etc.) with varying levels of complexity, ranging from straightforward facts to those requiring reasoning (e.g. false premise and multi-hop reasoning) [27]. It also considers facts with different levels of timeliness (e.g. real-time, fast-changing, slow-changing, and stable). Figure 2 illustrates the distribution of question types across different domains and timeliness categories. Notably, the characteristics of question distributions in the movie and finance domains differ significantly. Real-time and fast-changing questions necessitate access to relevant, up-to-date data sources and effective RAG implementations. In contrast, the movie domain contains more static questions than the finance domain, which may be easier to address."}, {"title": "3.2 Scoring", "content": "The overall score is a macro-average across all domains. The scoring system awards scores based on the quality of the response, and penalizes hallucination [27]. If the response is perfect, it receives"}, {"title": "3.3 RAG", "content": "RAG [8, 12] is a popular approach to alleviate the hallucinations of LLMs. There are various architectures for RAG in real applications, including advanced RAG techniques. In this paper, we explored several naive RAG approaches. Generally, naive RAG selects the highest cosine similarity results from a vector database and supplies the context as inputs for LLMs. There are multiple ways to find the relevant information to implement RAG. We tried using naive cosine similarity and using LLMs as retrieval and ranking models to find relevant information for each web page.\nFurthermore, as an extreme case, we also tried using the state-of-the-art Gemini 1.5 pro model with a 1 million token context window [20] to supply all the retrieved web pages to LLMs. These models with long context windows are promising because they eliminate the need to truncate information from retrieved results. However, with vanilla Gemini 1.5 pro and raw retrieved web pages, we achieved similar results like vanilla RAG with severe hallucination, which is disappointing. Therefore, we didn't further investigate more advanced RAG techniques. An example of hallucination is shown in Appendix A.2.\nThese results for RAG are summarized in Section 4."}, {"title": "3.4 Fine-tuning", "content": "Our initial investigation revealed that answers in most categories are very challenging, and the evaluation metric heavily penalizes hallucinations. Therefore, it is better for the model to be \"honest\" about its limits by replying \"i don't know\" in cases of uncertainty, rather than providing wrong answers. However, it is non-trivial to assess LLMs' confidence level reliably [13]. Instead, we hypothesize that by explicitly teaching LLMs to reply \"i don't know\" to challenging questions while providing real answers for easy questions, LLMs may be able to learn the ability to distinguish between challenging and easy questions.\nTo test this hypothesis, we decided to use the QLoRA [7] technique to fine-tune the Llama-2-7b-chat in 4-bit precision and optimize VRAM usage, due to limited GPU resources. More specifically, as it is shown in Fig. 3, we used the training data provided by the organizer, reserved 250 instances for testing, and made a few modifications to the rest of the data. If the question_type is comparison or false_premise, or the answer is \"yes\", \"no\", \"true\" or \"false\", we do not modify the answers to the questions; otherwise, we replace the original answer with \"i don't know\". Then we used the modified data to fine-tune the model.\nWe use Alpha=16, r=64, and Dropout=0.1 for QLoRA. Additionally, we use a batch size of 8, a learning rate of 0.0002, a weight decay of 0.001, and fine-tune the model for 5 epochs. We evaluate the fine-tuned model on the 250 withheld questions, using the offline evaluation script provided by the organizer.\nWe also experimented with Meta-Llama-3-8B-Instruct, which performed consistently worse."}, {"title": "3.5 Hybrid Approach", "content": "With the fine-tuned model mentioned in the above section, we achieved high rankings in the competition, and first place for the false premise question type in Task 2: Knowledge Graph and Web Retrieval. However, the method did not fully utilize the additional knowledge from the web search results and the knowledge graph. To improve the results, we developed the following hybrid approach to better utilize the knowledge in certain domains.\nThe hybrid approach leveraged the benefits of both the RAG model and the Fine-tuned Question Type model (Fig. 4). The hybrid approach first utilized the vanilla Meta-Llama-3-8B-Instruct to serve as the RAG model to generate results that include domain and answer information. The next step is to determine if the domain belongs to the movie domain, which has a lower level of hallucination based on the CRAG benchmark results [27]. If it is in the movie domain and the answer is valid, the answer is used as the final answer. If the answer is invalid, the question is sent to the Fine-tuned Question Type Model, which is good at answering false premise questions, and more modest by answering \"i don't know\" for other types of questions. Specifically, an answer is considered \"invalid\" in two scenarios: when the response is \"invalid question,\" or when there is a JSON processing error.\nFor the input of the RAG model, we also applied a Pruner to extract the top k sentences from the web search results. This pruner computes the cosine similarity of each sentence $W_{ij}$ in the document with the question $Q_i$, after converting them to Sentence-BERT embeddings [21]. For each top sentence, the following m sentences in the paragraph are appended to the context to enrich the information. To ensure content quality, an additional cosine similarity threshold n is applied, and the answer is used only if this threshold is met for top k sentences.\n$\\text{cosine\\_similarity}(Q_i, W_{ij}) = \\frac{Q_i \\cdot W_{ij}}{|| Q_i || ||W_{ij} ||}$ (1)\nBased on offline evaluation, this approach improved the total score from 0.073 to 0.86 using results from 300 samples, compared with the Fine-tuned Question Type model. Since our Fine-tuned Question Type model achieved a score of 0.0960 in the online evaluation for Task 3 (without the holdout test), this hybrid approach is expected to achieve a higher score. It has not been evaluated online yet, because the online evaluation system was closed after phase 2. In addition, the prompt in Appendix A.1 is used to get the domain name of the question in a JSON output, which can be used for easy processing."}, {"title": "4 Results", "content": "We firstly tested the Llama3 8b pretrained model with or without RAG and prompt tuning in the domain of movie vs. finance and types of questions of simple vs. post-processing and multi-hop. The results of which are shown in Table 1. Each test case ran 100 samples out of the 2.7k samples in CRAG. Surprisingly, we observed that adding more detailed instructions in the prompt actually dropped the overall performance significantly on the Llama3 8b pretrained model. One hypothesis is that the model's performance is highly sensitive to the format of prompting and needs to be properly configured and fine-tuned.\nGiven that the retrieval might not be correct, we also tried Gemini 1.5 pro with a 1 million token context window as a long context window models to see if feeding all the retrieved information to the LLMs would perform better than any RAG approaches. The results show no improvement, and we didn't further investigate on this.\nTable 2 shows the overall results from our fine-tuned model, which achieved 0.096 with 323 samples from the online judging system. With this model, we achieved the highest score in Task 2 for the false premise problems (Fig. 5). Finally, our hybrid approach results with cosine similarity threshold of 0.75 show that the score improved by 0.013 from 0.073 to 0.086 (Table 3) with fine-tuning and RAG combined. The accuracy increased by 0.026 and there was a slight increase in hallucination by 0.013.\nFurthermore, 4 shows some key examples of differences in predictions comparing the fine-tuned only model and the hybrid approach. For the first false premise question \"when did hamburg become the biggest city of germany\", both models provide the answer \"invalid question\", since the largest city in Germany is Berlin. For the second question which is a comparison question, both models also provide the same correct answer. For the third question related to movie domain, the fine-tuned model responds with \"i don't know\", while the hybrid approach provides \"inception\" which is a correct answer. For the forth question related to movie domain, the fine-tuned model provides \"i don't know again\", while the hybrid approach provide a relevant answer including who are professionals relevant to the"}, {"title": "5 Discussion", "content": "We tried various approaches to alleviate the hallucinations of LLMs, including Retrieval-Augmented Generations (RAGs) and fine-tuning with domain-specific knowledge, while participating in the 2024 Meta KDD Cup with Comprehensive RAG Benchmark (CRAG) data. It turns out that RAG alone is not enough to perform well in the benchmark, and fine-tuning is needed to achieve a higher score. Our results show that the hybrid approach using both RAG and fine-tuning performs best in CRAG.\nFor the basic RAG approach, we are uncertain whether the model's accurate answers stem from the pretrained model's prior knowledge or the retrieved data from the reference. At first glance, it appears that basic RAG does not significantly improve the final score. One possible reason is that the retrieved content, while relevant (cosine similarity score > 0.7), might not be useful because it is too basic, general, or vague. Additionally, improper prompting significantly reduces accuracy compared to using no prompts, likely because the pretrained model is misled by the instruction and loses its own prior knowledge to answer the question. More advanced RAG and prompt design should be tested to draw definitive conclusions.\nWith further improvement in the hybrid RAG approach, focusing on the movie domain, it is clear that better quality of retrieved content helps improve the RAG results. In the Table 5, the second question shows that if the birth date is correct in the retrieved content, the prediction of Hybrid RAG Approach is correct. There is also another offline example that there are two different birth date search results for Woody Allen: November 30, 1935 and December 1, 1935, and RAG can generate a wrong answer depending on which search results it retrieved.\nThe CRAG question dataset also contains many questions like \"In year x, which type y movie was recognized with the best type y movie in Oscar\". a common hallucination type occurs in this way: it provides a movie which was released in year x, but it won Oscar in year x+1. In the table 5, it shows an example that \"the incredibles\" was provided by RAG as the best animated feature film in Oscar 2004. However, the ground truth is \"finding nemo\", since \"Finding Nemo\" won Oscar in 2004, and \"The Incredibles\" which was released in 2004 won Oscar in 2005."}, {"title": "6 Future Work", "content": "With hallucination being one of the biggest challenges in applying LLMs to real-world applications, addressing different types of hallucination continues to require deeper and more extensive research and efforts. Based on our experimental results that the hybrid approach provides the best outcome, we believe in the direction of an adaptive methodology using divide-and-conquer: splitting the solution into two high-level parts: hallucination detection and hallucination correction, and adopting tailored hallucination correction methods based on the detected hallucination types and probabilities.\nIn terms of detection, questions with low probabilities of hallucination can be questions in common domains, requiring simple logic, and/or about a past fact that doesn't change; questions with high probabilities of hallucination can often be in specialized domains, requiring complex reasoning and multi-hop steps, and/or"}, {"title": "A Research Methods", "content": ""}, {"title": "A.1 Prompt of Generating Answers with Domain in JSON format", "content": "You are an agent that only outputs JSON. You are given a Query and References. Do the following:\n1. Determine the domain the query is about. The domain should be one of the following: \"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". If none of the domains apply, use \"other\". Use \"domain\" as the key in the result json.\n2. Answer the question in as few words as possible. Please follow these guidelines when formulating your answer. If the question contains a false premise or assumption, answer"}, {"title": "A.2 An example of hallucination", "content": "For example, a simple movie question \"can you tell me the date that Randall Wallace was born?\"(Ground truth: 1949-07-28), the model answers the same question repetitively:\n>>According to the provided information, Randall Wallace was born on July 28, 1949;\n>>According to my sources, Randall Wallace, the American screenwriter and director, was born on July 28, 1949;\n>>I can confirm it now: Randall Wallace was born on November 7, 1948.\n>>Randall Wallace's birthday is September 1, 1948.\n>>I've got it nailed down now! Randall Wallace was indeed born on November 7, 1949\n>>I finally have it exact: Randall Wallace was born on July 1, 1948 (no, wait!), I mean... September 1, 1949 (no), actually it is November 7, 1946. Yes, that's correct!"}]}