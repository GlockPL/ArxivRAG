{"title": "Data Quality Antipatterns for Software Analytics: A Case Study of Software Defect Prediction", "authors": ["Aaditya Bhatia", "Dayi Lin", "Gopi Krishnan Rajbahadur", "Bram Adams", "Ahmed E. Hassan"], "abstract": "Data quality is crucial in software analytics, particularly for machine learning (ML) applications like software defect prediction (SDP). Despite the widespread use of ML models in software engineering, the impact of data quality antipatterns on these models remains underexplored.  This study aims to develop a comprehensive taxonomy of data quality antipatterns specific to ML and assess their impact on the performance and interpretation of software analytics models.  Through a literature review, we identified eight types and 14 sub-types of ML-specific data quality antipatterns. We then conducted a series of experiments to determine the prevalence of data quality antipatterns in SDP data (RQ1), assess the impact of the order of cleaning antipatterns on model performance (RQ2), evaluate the effects of antipattern removal on model performance (RQ3), and examine the consistency of interpretation results from models built with different antipatterns (RQ4).  In our SDP case study, we identified the prevalence of nine antipatterns from our taxonomy. Notably, we observed that more than 90% of the antipatterns identified from our taxonomy overlapped at both the row and column levels within the structured SDP dataset. This overlap can be a major issue as it can complicate the prioritization of cleaning processes as it may obscure critical antipatterns and lead to excessive data removal or improper cleaning. Next, the order of cleaning antipatterns significantly impacts the performance of ML models, with learners like neural networks being more resilient to changes in cleaning order than simpler models like logistic regression. Additionally, antipatterns such as Tailed Distributions and Class Overlap have a statistically significant correlation with performance metrics when other antipatterns are cleaned. Models built from data with different antipatterns showed moderate consistency in interpretation results.  Our results indicate that the order of cleaning different antipatterns impact the performance of ML models. Five of the studied antipatterns from our taxonomy have a statistically significant correlation with model performance in a setting where the remaining antipatterns have been cleaned out. Finally, apart from model performance, model interpretation results are also moderately impacted by different data quality antipatterns.", "sections": [{"title": "1 Introduction", "content": "Machine Learning (ML) has been widely adopted in various domains of software analytics, including software defect prediction, technical debt identification, and effort estimation [1,2,3,4,5]. These applications leverage ML to make key decisions, such as predicting future bugs, identifying problematic code, or estimating the required effort for software projects. In the domain of software analytics, data for models typically originates from software project repositories, issue tracking systems, and code commit histories, providing real-time and historical insights into software development practices. The users of these ML models expect not only high precision (for identification of specific issues) and recall (for ensuring no potential problems are overlooked), but also rely on these models for critical decision-making in software development and maintenance [6,7].  Apart from the choice of learning algorithm, hyperparameter tuning or embedding, the reliability of these decisions heavily depends on the quality of the data used to train the ML models [8,9,10]. Indeed, even if the code building the ML model is of high quality and is highly optimized, if the underlying data is flawed, the resulting model will be ineffective and potentially misleading. Unlike traditional software, where requirements are embedded as business logic encapsulated in software code, the requirements of ML models are embedded within the data itself. For example, to train a classifier distinguishing between cats and dogs, the training data must consist of accurate images of cats and dogs. If the dataset includes mislabeled images (e.g., an image of a rabbit labeled as a dog), the model will be compromised. Thus, the requirements of ML models are inherently tied to the quality and correctness of the data, making data an essential asset that determines the quality of the trained ML model [11]."}, {"title": "2 Literature Review", "content": "While Section 3 analyzes literature on data quality antipatterns to develop a taxonomy, this section discusses research on 1) data quality issues in general domains, 2) data quality issues in software engineering and SDP, and 3) data quality tools."}, {"title": "2.1 Data quality issues in general.", "content": "The challenges in data quality for ML were identified as early as 1993 [41] for different domains. For instance, Gupta et al. [42] analyzed data quality issues like inactive customers in the observation window and anomalous transaction behaviors that led to sub-optimal ML models used in banking systems. Kerr et al. [43] presented a case study of data quality and its critical nature in the healthcare industry. For environment monitoring domain, Okafor et al. [44] developed an ML framework to monitor the quality of data for low-cost IoT Sensors. Our research builds upon these works by investigating data quality specifically in the SDP domain, allowing us to uniquely identify the challenges and opportunities in addressing data quality attributes for software engineering applications, thereby contributing novel insights to the broader data quality literature.  Along with characterizing data quality issues in different domains, several studies have proposed tools for monitoring the quality of data. For example, Grosso et al. [45] developed an ML-based tool for real-time monitoring of particle detector data, and Ehrlinger et al. [46] proposed a DaQl (DAta Quality Library) as a generally-applicable tool to monitor the quality of data to increase the prediction accuracy of ML models. Similarly, Dai et al. [47] presented a framework built on deep learning and statistical models to identify data quality, and Shrivastava et al.[48] presented a toolkit for detecting, correcting, and explaining data quality for structured data. Our research aids such toolsmiths by providing a comprehensive taxonomy of data quality issues, helping them identify missed opportunities in the identification and mitigation of antipatterns in their domains."}, {"title": "2.2 Data Quality in Software Analytics.", "content": "In the realm of defect prediction, several research works have meticulously curated and refined predictive features, underscoring the importance of data quality within this domain. Seminal works by Kamei et al. [4] in 2013 initiated a shift towards identifying defect-prone (\"risky\") software changes just-in-time, rather than focusing on broader entities like files or packages. Concurrently, Zimmermann et al. [3] pioneered the concept of Cross-Project Defect Prediction (CPDP), leveraging defect labels from one project to improve predictions in another, and outlined crucial features and machine learning models suitable for CPDP. Such seminal studies have spurred extensive subsequent research in the field, which are encapsulated in comprehensive survey papers such as those by Punitha et al. [49], Zhao et al. [50], and Thota et al. [51].  For general software engineering, research around data quality has increasingly focused on data management and operational best practices. For instance, in 2020, Jain et al. [52] presented an overview of data quality for machine learning applications at IBM, highlighting the importance of intelligently designed metrics in upholding data quality. The authors examined quality for structured data, unstructured text, and human in the loop validation by subject matter experts. Renggli et al. [53] from Microsoft Research presented a data quality view of MLOps, specifically providing guidelines and perspectives on designing an efficient MLOps pipeline. Abdallah et al. [54] uncovered data quality issues for big data, presenting factors from different perspectives such as management, user, processing, service pipeline, and data perspective. Moran et al. [55] assessed the impact of data quality by performing specific data preprocessings, and found that the significance of selecting a classifier decreases after applying an appropriate preprocessing step. Such aspects of data quality have been studied in research; however, our research is the first to empirically analyze the impact of multiple data quality antipatterns in the domain of SDP. Moreover, to the best of our knowledge, no software analytics study has yet focused on the impact of antipatterns on model interpretation.  Different software engineering domains can exhibit different ML engineering characteristics. For example, Ouatiti et al. [56] found that log level prediction models demonstrate very weak correlation in interpretation ranks with even contradictory model interpretations, a phenomenon not observed in SDP [29]). As such, software engineering researchers should evaluate antipatterns within the specific context of their respective domains."}, {"title": "3 Taxonomy of ML-specific Data Quality Antipatterns", "content": "Although previous research has demonstrated the prevalence and impact of various data quality antipatterns, there is currently no comprehensive taxonomy of antipatterns in general literature. In this section, we perform a literature review and summarize data quality into a set of eight broad antipattern types, i.e., Packaging Antipatterns, Schema Violations, Data Miscoding, Feature-Row Imbalance, Data Distribution Antipatterns, Inconsistent Representation, Correlation & Redundancy, and Label Antipatterns, which comprise of 14 sub-types. As the first step, this paper focuses on structured data quality antipatterns, in the context of ML. Future work should examine antipatterns for other data types (e.g., images, audio or video). Our taxonomy of antipatterns and strategies to address them will help researchers and practitioners in effective dealing of these antipatterns for their use cases.  Approach. Given the absence of comprehensive studies listing these antipatterns, we focused on both tool-based solutions along with a traditional academic survey. Overall, we derived our taxonomy of antipatterns, by targeting three aspects:  Data Cleaning Tools We began our search by looking for tools and identified specific antipatterns targeted by each tool. Particularly, we looked for data cleansing, profiling, and validation tools offering detection and mitigation solutions for different antipatterns. In Subsection 3.2, we present and cite various data profiling tools along with our findings on their detection and mitigation strategies.  Leveraging academic publication accompanying open source tools for an initial round of snowballing. Often, open source tools are accompanied by a corresponding academic publication, which we used for snowballing (i.e., looking at the cited publications from that paper). For instance, tools like data linter\u00b9 are accompanied by a scholarly paper [39], served as a foundational reference for further academic exploration. The data linter tool cleans 1) miscoding errors, 2) outliers and scaling issues, and 3) packaging errors, all three of which were directly incorporated into our taxonomy. Andrew NG2's [57] emphasis on data-centered innovations and development in ML 3 is centered around the inconsistent labeling antipattern. Akin to the data linter tool, the TFDV (Tensorflow Data Validation) tool is accompanied by a publication by Breck et al. [16] and also served as a foundation reference for our academic exploration. TFDV detects antipattens like \"schema validation\", \"training serving skew\", and \"package errors\", which we directly adopted in our taxonomy. Tools like DataPrep.ai validate and clean entities (like Zip codes, phone numbers, and country names), a manifestation of Data miscoding. For each tool-academic based research, the second author employed a lightweight snowballing methodology, utilizing each tool's publication's references to expand our study.  General academic search. After the above two steps, the initial candidate list of publications was further refined and expanded upon by incorporating a systematic search using Google Scholar, employing terms such as \"data quality\", \"data issues machine learning\", and \"data quality artificial intelligence\". This search yielded at least 50 relevant candidate publications along with their referenced publications via snowballing. Antipatterns discussed in these sources were cross-referenced with our existing taxonomy, and any new antipatterns were integrated accordingly. Notable additions in this phase included sub-types like class imbalance, feature-row imbalance, and concept drift.  Finally, the first two authors categorized the identified antipatterns into a taxonomy using a card sorting method. To ensure the robustness and coherence of the expanded taxonomy, the first two authors engaged in four collaborative sessions, each lasting approximately one hour, spread over several days. The categorization process was straightforward, with antipatterns logically grouped based on their characteristics. For instance, schema violations are clearly distinguished from label antipatterns, and the obvious placement of class overlap lies in the label antipatterns sub-type. The clarity of the antipattern characteristics facilitated a unanimous agreement on their categorization in the taxonomy.  In the subsections below, we discuss the 1) resulting taxonomy and 2) detection and mitigation strategies for each of the antipatterns from the resulting taxonomy."}, {"title": "3.1 Resulting Taxonomy", "content": "We identified a total of eight high-level types of antipatterns comprising a total of 14 sub-types. Figure 1 presents a taxonomy of the eight high-level data quality antipattern types along with their 14 sub-types.  Schema Violations. A data schema identifies a blueprint for the organization of a dataset based on a set of rules. Violations of the rules that explain the logic of the objects/associations/entities described by the data are schema violations.  For relational databases, integrity constraints are enforced to ensure the correctness of data within a defined schema. However, violations can occur if such rules are not enforced or are undefined. Schema violations are quality antipatterns as data that violates its schema is likely to be inaccurate and therefore should introduce noise to the pattern that ML models are trying to learn.  Schema rules have been proposed and used for data cleaning in the literature. For instance, in the context of defect prediction, Shepperd et al. [10] $number-lines (total~lines~of~in~a~source~code~file) > number\\_blanklines (lines~which~have~no~code~or~comments,and~are~left~blank)$ is a schema rule.  Data Miscoding. Data miscoding refers to the incorrect assignment of data types to features, such as treating numerical data as strings. ML models are trained based on the incorrect data type, leading to suboptimal performance if, for instance, a numerical column is incorrectly treated as a string. Data miscoding encompasses the use of incorrect data types while performing ML operations during the training, validation, or serving phases of machine learning processes [58]. For instance, in the training phase, models trained from an incorrect data type may learn improper information from the miscoded features."}, {"title": "5 Results", "content": "In this section, we delve into the motivation and results of each of our research questions."}, {"title": "5.1 RQ1: What is the prevalence of data quality antipatterns in software defect prediction data?", "content": "Despite the widespread use of SDP data in software engineering research [35, 65,123], researchers utilizing SDP datasets lack an empirical understanding of the presence of data quality antipatterns within the software defect prediction datasets. Singular data quality antipatterns have been found in Software Engineering datasets ([10,21,39,124]), however, to the best of our knowledge, no research yet checked the prevalence of multiple data quality antipatterns in the context of software engineering. Understanding the prevalence of these antipatterns is important for both researchers and practitioners, as it can provide valuable insights into the factors that impact the performance and interpretation of machine learning models used for defect prediction. Hence, in this RQ, we aim to check the prevalence of data quality antipatterns in SDP data.  Results. Out of the 19, i.e., five top-level types28 and 14 sub-types of antipatterns in our taxonomy 1, we found nine antipatterns to commonly occur in the studied SDP dataset. The distribution based antipatterns (tailed and unnormalized metrics) along with the label antipatterns, class overlap and mislabel are prevalent in all SDP project versions, while the other antipatterns occur in 2-50% of the projects versions.  We did not detect the presence of four antipatterns (i.e., Data Miscoding, Inconsistent Representation, Missing Values and Data Drift) in the studied datasets. Since Data Miscoding is a dynamic issue, an ML pipeline (or non-ML data-driven program) should be able to validate the correct data type; nevertheless, we are unable to identify this as an antipattern in the static dataset from Yatish et al [26]. Furthermore, there are neither missing values nor inconsistent representations in the studied SDP data. Perhaps, Yatish et al. [26] cleaned missing values during the data procurement phase of conducting their study, while we do not suspect Yatish et al's work to encounter inconsistent representation, as this antipattern would most likely be prevalent in text-like features.  Below, we present each of the antipatterns prevalent in the SDP data:"}, {"title": "5.1.1 Row Feature Imbalance", "content": "The studied SDP data lacks any row feature imbalance. As illustrated in Figure 2, the ratio of the number of rows to the number of columns is greater than a threshold of 10, as demarcated by the red line in the Figure. The threshold of 10 is commonly used as a rule-of-thumb value in the industry2930. With more than 10 times the rows than the features, we consider there to be no row feature imbalance. Notably, the outliers in Figure 2 include the project version Camel-2.9.0 having the maximum (7,120) number of rows, and the project version Jruby-1.1 having the minimum (731) number of rows. Each row in Yatish et al.'s [26] dataset represents a java file, and these outliers are due to the project versions with the least or the maximum number of files in their codebase."}, {"title": "5.1.2 Schema violations", "content": "2% of the studied SDP dataset has schema violations. Figure 3 shows the distribution of violations for different project versions of the studied SDP data. We observed violations for five (i.e., R8, R14, R15, R17, and R19 out of 20 rules defined in Table 3. A total of 30, 12, 7, 6, and 6 files had violations against rules R14, R8, R14, R15, and R17. Overall, the maximum number of violations were observed for project hive version 0.10.0 with 7% data having schema violations.  We suspect that the calculation of complex metrics like complexity could lead to higher chances of violations either due to errors in calculations or due to more complex code structures not adhering to the rules R14, R15, R17. Rules without violations might be simpler and more direct in their measurements (like R1, R2, R3). For R14, in deeply nested functions, calculation of the average number of lines might become skewed, with large disparity in size and complexity of nested functions which may have led to an erroneous calculation. Further work may be required to pinpoint the exact cause of such violations."}, {"title": "5.1.3 Data Distribution Anti-patterns", "content": "Tailed Distribution. All project versions have tailed columns. The number of projects having tailed distributions is shown in Figure 4 (left). Overall, the 5 point summary of tailed projects is: 11 (min), 15.25 (25th), 18 (median), 21 (27th 24 (max) indicating that the studied project versions have at least a median of 18 tailed metrics. The metrics CountDeclMethod, COMM and MaxNesting_Min are tailed across all projects.  Unnormalized Distributions. Unnormalized distributions are prevalent in all project versions of the studied SDP dataset. Particularly, 17% of metrics are consistently unnormalized, while 37% are inconsistently unnormalized. The 19 metrics detected as unnormalized are illustrated in Figure 4 (right)."}, {"title": "5.1.4 Packaging Errors: Duplicated Data", "content": "In the studied SDP dataset, a median of 2.6% data is duplicated. Figure 5 depicts the percentage of duplicated rows across different project versions of the studied SDP data. The project ActiveMQ-5.8.0 contains the maximum, i.e., 17.4% duplicates, whereas the project Hbase-0.94.0 contains no duplicates."}, {"title": "5.1.5 Label Antipatterns", "content": "Class Overlap. A median SDP project version has 17% rows having overlapping classes. The percentage of overlapping instances detected on a per-project version basis is shown in red in Figure 6. The project Derby, version 10.2.2.6, had the highest amount of overlapped instances (i.e., 654 out of 1,963). According to prior studies, such outcomes are to be expected in defect prediction [21,18].  Class Imbalance. A median of 11% class imbalance is prevalent in the studied SDP dataset. Figure 6 shows the ratio of minority classes (buggy instances) with respect to the total data size per project-version-basis. Class Imbalance is depicted by the blue color in Figure 6. As shown in the Figure, project-versions having high class imbalance are also likely to have class overlap. Such findings are not uncommon as prior research has indicated the relation between the two antipatterns [22,23,24]. Below the threshold of 10% as demarcated by the gray line in Figure 6, 43% of the project-versions have class imbalance. While a specific threshold for \"high\" class imbalance is not provided in literature, we use a threshold of 10% to indicate the prevalence of class imbalance in different project-versions of the studied SDP dataset.  Mislabels. All project versions of the studied SDP dataset have mislabeling, as demonstrated by Yatish et al. [26]. The inconsistencies observed between the two labeling schemes are depicted in green color in Figure 6. A median of 15% of labels had conflicting labels produced from the two (i.e., Heuristic or Realistic) labeling strategies."}, {"title": "5.1.6 Correlation & Redundancy", "content": "Within the studied dataset, 38-50% columns are correlated & redundant. Figure 7 shows the distribution of the percentage of correlated & redundant features within different project versions of the studied SDP dataset. A median of 43.3% metrics are correlated & redundant. This is accompanied by a high variance of 8%, which indicates that the quantity of correlated and redundant metrics varies across different project versions within the studied SDP dataset, indicating that each project has a unique distribution of such metrics."}, {"title": "5.2 Overlap of Antipatterns", "content": "In the previous subsection, we studied the prevalence of different antipatterns in our studied SDP dataset. In this subsection, we present the extent of overlap of these antipatterns. We bifurcate the results into two categories: row-level antipatterns (including schema violations, mislabels, class overlapped rows, and duplicates) and column-level antipatterns (encompassing tailed, unnormalized, constant, and correlated & redundant features). We focus on the overlap within the same category, i.e., either row-level or column-level.  Row level overlap. The majority (67.9%) of rows impacted by antipatterns do not exhibit any overlap. As shown in Figure 8 only 31% rows having antipatterns have two antipatterns, out of which 94% cases are overlapped between Mislabels and Class Overlap. The remaining 1% cases having row-based antipatterns had three antipatterns affecting the same row.  Column level overlap. Most (71.6%) columns having antipatterns do not have any overlap with another antipattern as shown in Fig 8. 90% of the remaining 28% cases with two column-based antipatterns comprised correlation & redundancy along with tailed features. A trivial amount of columns (0.4%) had three antipatterns."}, {"title": "5.3 RQ2: Does the order of cleaning data quality antipatterns impact the model performance?", "content": "The data pre-processing phase in ML pipelines involves preparing the data for training the ML model, as noted by Amershi et al. [58]. This includes cleaning, performing transformations, and more, a topic that has been widely studied in literature and is outlined in our Section 3. However, despite the recognition of the importance of cleaning data quality antipatterns, the optimal order for addressing multiple antipatterns remains unknown. Understanding the influence of the order in which antipatterns are cleaned is crucial, as it can affect the size of the training data and, thereby, the performance of the ML model. As demonstrated in our previous research question, 32.1% (100-67.9) rows and 28.4% (100-71.6) columns with antipatterns overlap, indicating that data quality antipatterns can often coexist in practice, exacerbating this need.  The overlap of these antipatterns may potentially lead to compounded errors and biases in the dataset, and addressing them in an improper sequence may inadvertently amplify these issues, or negate the benefits of cleaning performed in previous steps. For instance, normalizing data before addressing missing values might lead to skewed distributions, impacting the accuracy of imputation techniques. Conversely, handling class overlapped rows prior to normalization could remove valuable data points that are actually significant once the data is scaled appropriately. Therefore, establishing an effective order for cleaning operations is essential to ensure that each step enhances the quality of the data without introducing new issues or magnifying existing ones.  Approach. We remove antipatters in different orders from the training data and build ML models using the studied learners. Figure 9 provides an overview of our approach. A systematic description is provided below.  Cleaning Types. We categorize the cleaning into four cleaning types: Filtering (Fi), Transformations (Tr), Class Overlap (Ov), and Mislabeling (Mi). We use the abbreviations for each type henceforth. Classification into these types is based on the type of cleaning required for each antipattern as shown in Table 5. For instance, the Fi category involves filtering schema violations, duplicated data, constant features, and correlated and redundant features, as data is removed (filtered) from rows or columns. The Tr category entails performing log-transformation and z-scoring to transform tailed and unnormalized features respectively. We keep Class Overlap separate from other filtering-based cleanings because Mislabeling explicitly impacts Class Overlap, while having no effect on other cleaning techniques, such as schema violations or constant features.  Creating cleaning orders. Performing different order of cleaning entails performing one type of cleaning first, and other types later. For example, an order FiTrMiOv would entail performing cleaning in the following order Fi\u2192Tr Mi\u2192Ov. Similarly, from the four cleaning groups Fi, Tr, Mi and \u039f\u03c5 lead to 4!, i.e., 24 cleaning permutations (e.g., FiTrMiOv, FiTrOvMi, TrFiMiOv, etc.). We call these permutations as \"orders\" henceforth.  Culling redundant orders. From all the permutations, we exclude those that lead to the same order. In particular, the positioning of Mi at any step before Ov does not matter, as both Mi and Ov are based on the response (y-label) and do not impact the explanatory variables. For instance, in the case of FiTrMiOv, mislabel positioning before overlap (i.e., FiMiTrOv, or MiFiTrOv) yields the same data. Similarly, mislabel correction after class overlapping (i.e., OvFiTrMi, OvFiMiTr, OvMiFiTr) yields the same data as well. Removing such redundant orders leaves us with 12 orders. These cleaning orders are: FiMiOvTr, FiOvMiTr, FiTrMi\u039f\u03c5, FiTrOvMi, MiOvFiTr, \u039f\u03c5MiFiTr, \u039c\u03af\u039f\u03c5TrFi, OvMiTrFi, TrFiMiO\u03c5, Tr- FiOvMi, TrMiOvFi, and TrOvMiFi.  Building ML models from each cleaning order. We clean the training data based on each of these orders, and build an ML model using the model-building process outlined in Section 4. Using the unseen test set, we obtain model metrics, Precision, Recall, F1, MCC, AU-ROC, and AU-PRC from each model. To enable fairness, we compare the performance of models built using different cleaning orders of the same training data and evaluate the performance on the same unseen test set.  Creation of sub-sequences. Our objective is to evaluate whether one cleaning type outperforms another cleaning type. To study this, we repeatedly divide the 12 orders into two groups (sub-sequences) based on the occurrence of an antipattern pair: one group containing all orders in which an antipattern precedes another, and the other group with all orders in which the antipattern follows another. For instance, to check whether Fi should be performed before or after performing Tr, we form the sub-sequence Fi\u2192 Tr along with its inverse sub-sequence Tr \u2192 Fi. The sub-sequence Fi\u2192 Tr has six orders in which Filtering is performed before Transformation (i.e., FiMiOvTr, FiOvMiTr, FiTrMiOv, FiTrOv\u039c\u03af, MiOvFiTr, \u039f\u03c5MiFiTr), while its inverse sub-sequence, Tr \u2192 Fi has the remaining six orders. Similarly, from the 12 orders, six belong to one sub-sequence and the remaining six belong to the inverse sub-sequence. In total, we have four sub-sequences, i.e., MiOv, FiTr, Trov, and FiOv; and naturally their inverse sub-sequences would be OvMi, TrFi, OvTr, and OvFi respectively.  Evaluation of sub-sequence impact. We use the odds ratio (OR) [125] to show the likelihood of one sequence leading to better performance over its inverse (e.g., Fi\u2192Tr versus Tr\u2192Fi). The null hypothesis is that a sequence ab (a\u00dfb) is in the top-performing sequences for a given set of training data. The alternative hypothesis is that a sequence ab is not in the top 50% performing sequences for the same training data. Hence, we calculate the odds ratio as follows for all models in a learner: $Odds for AB = \\frac{AB\\_in\\_top}{AB\\_not\\_in\\_top}$.  Similarly, $Odds for BA = \\frac{BA\\_in\\_top}{BA\\_not\\_in\\_top}$.  To calculate the odds ratio, we use: $OR = \\frac{Odds for AB}{Odds for BA}$.  An odds ratio of 1 indicates that the odds of AB and BA are the same. To interpret the significance of the odds ratios, we set thresholds where an OR in the range of 0.64 to 1.5 is considered unimportant [126,127]. This implies that variations within this range do not significantly affect the performance difference between the sequences. Conversely, an OR less than 0.64 or greater than 1.5 is deemed important, signifying a substantial difference in performance. Overall, the experiment setup is given in Figure 9.  Results. Different learners (RF, XGB, LR, DNN) respond uniquely to the order of cleaning antipatterns. Figure 10 illustrates the odds ratios for different orders of cleaning antipatterns. Notably, the odds ratios for orders Mi\u2192Ov, Fi\u2192Tr, and Tr\u2192Ov vary across learners, highlighting the learner-dependent impact of cleaning orders. DNN, having higher degrees of freedom is the least impacted by the cleaning order. Below, we explain the rationale for this variation for each pair of cleaning orders, along with the differences in the learners below."}, {"title": "5.3.1 \u039c\u03af\u039f\u03c5 \u03c5\u03c2 \u039f\u03c5\u039c\u03af", "content": "Correcting mislabeled instances in the dataset primarily improves the quality of the target variable (y-label). If mislabels are not corrected first, overlap removal might incorrectly remove or retain instances based on inaccurate labels.  As indicated by the blue circles in Figure 10, logistic Regression is highly sensitive to the quality of class labels. Correcting mislabels first ensures that the subsequent removal of overlapping classes is based on accurate labels, which is crucial for LR as it directly models the probability of the target variable. This sequence improves model performance, resulting in a higher odds ratio for MiOv compared to OvMi. Random Forest (RF) and XGBoost (XGB), on the contrary, are more robust to noise and label errors due to their aggregative nature. Being ensemble based methods, RF and XGB can mitigate the impact of mislabels through averaging and boosting mechanisms, perhaps making the order of MiOv vs OvMi less critical. This robustness explains the odds ratios closer to 1, indicating less sensitivity to the sequence of these operations. Finally, DNNs have a high degree of freedom and can model complex patterns. They can adapt to initial mislabels or overlaps, making them less sensitive to the order of Mi and Ov."}, {"title": "5.3.2 FiTr vs TrFi", "content": "Since the Fi aims to remove extreme values or incorrect data points, performing Fi before Tr would help in reducing the impact of these outliers, which might still influence the transformed data if done post-transformation (TF). As shown in the Green rectangles in Figure 10, RF, XGB and DNN are less sensitive to the order of Tr and Fi cleanings. XGB is known for its robustness to different feature scales due to its grading boosting framework and can handle missing values and outliers more effectively making it less sensitive to the order of Fi and Tr. Similarly, DNNs are highly flexible for learning complex data patterns, given their higher degrees of freedom. Out of these three learners, RF is the least resilient to the order of Fi and Tr given the its odds ratio at the border-line significance level in Figure 10. Perhaps, transforming later (FiTr) can remove the variability needed by the decision trees within the random forest, whereas, due to their robustness (given RF is an ensemble-based technique), the order has a less pronounced impact, with the odds ratios closer to 1.  In fact, filtering after transformation would 1) lead to incorrect filtering since the rule based checks would not work on transformed data; and 2) invalidate transformations (e.g., filtering data after log transformation would change the distribution of the logged data). However, Tr first improves the performance of LR and RF (slightly), as shown by the lower odds ratio for FT compared to TF. This can be attributed to the reduced influence of outliers. Post-transformation, the effects of outliers on the model's learning process are reduced, because transformations like the logarithm reduce the variability caused by extreme values. Hence, even though these outliers are still present in the dataset during the training phase, their influence is diminished, allowing the models to focus on more general patterns. LR having the lowest DOF amongst the other learners makes the impact of the FiTr order the most pronounced."}, {"title": "5.3.3 TrO\u03c5 \u03c5\u03c2 \u039f\u03c5\u03a4r", "content": "Removing overlapped datapoints before transformation boosts the performance for all learners. The odds ratios significantly less than 1 indicates that performing overlap removal first (OT) results in comparatively better performance than performing transformation first (TO). Class overlap removal entails removing a significant number of data points (as shown in RQ1), which, if done after the transformation, makes the transformations no longer meaningful. For instance, once a feature's log transformation is done, followed by the removal of 17% data due to class overlap, the distribution of that feature is no longer a logged distribution. Conversely, from the perspective of class overlap removal on model performance, the data distribution available for class overlap for an accurate removal, should be similar to that of the final data used to train the model. When transformations are done later, the distribution based on which class overlap removal was done, and the distributions based on which a model is trained are dissimilar to each other, which could also explain the lower performance for OvTr as compared to TrOv.  Similar to the case of TrFi, the effects are less pronounced for DNN, given its ability to be less resilient to noise."}, {"title": "5.3.4 FiO\u03c5 \u03c5\u03c2 OvFi", "content": "RF, XGB, and LR have odds ratios significantly less than 1, suggesting that performing overlap removal before filtering tends to result in better performance compared to the reverse order, whereas DNN, with an odds ratio close to 1, indicate that the sequence of these operations does not significantly impact performance. We suspect data dependency and model selectivity to influence the performance of the Filtering"}]}