{"title": "Data Quality Antipatterns for Software Analytics", "authors": ["Aaditya Bhatia", "Dayi Lin", "Gopi Krishnan Rajbahadur", "Bram Adams", "Ahmed E. Hassan"], "abstract": "Background: Data quality is crucial in software analytics, particularly for machine learning (ML) applications like software defect prediction (SDP). Despite the widespread use of ML models in software engineering, the impact of data quality antipatterns on these models remains underexplored.\nObjective: This study aims to develop a comprehensive taxonomy of data quality antipatterns specific to ML and assess their impact on the performance and interpretation of software analytics models.\nMethods: Through a literature review, we identified eight types and 14 subtypes of ML-specific data quality antipatterns. We then conducted a series of experiments to determine the prevalence of data quality antipatterns in SDP data (RQ1), assess the impact of the order of cleaning antipatterns on model performance (RQ2), evaluate the effects of antipattern removal on model performance (RQ3), and examine the consistency of interpretation results from models built with different antipatterns (RQ4).\nResults: In our SDP case study, we identified the prevalence of nine antipatterns from our taxonomy. Notably, we observed that more than 90% of the antipatterns identified from our taxonomy overlapped at both the row and column levels within the structured SDP dataset. This overlap can be", "sections": [{"title": "1 Introduction", "content": "Machine Learning (ML) has been widely adopted in various domains of software analytics, including software defect prediction, technical debt identification, and effort estimation [1,2,3,4,5]. These applications leverage ML to make key decisions, such as predicting future bugs, identifying problematic code, or estimating the required effort for software projects. In the domain of software analytics, data for models typically originates from software project repositories, issue tracking systems, and code commit histories, providing real-time and historical insights into software development practices. The users of these ML models expect not only high precision (for identification of specific issues) and recall (for ensuring no potential problems are overlooked), but also rely on these models for critical decision-making in software development and maintenance [6,7].\nApart from the choice of learning algorithm, hyperparameter tuning or embedding, the reliability of these decisions heavily depends on the quality of the data used to train the ML models [8,9,10]. Indeed, even if the code building the ML model is of high quality and is highly optimized, if the underlying data is flawed, the resulting model will be ineffective and potentially misleading. Unlike traditional software, where requirements are embedded as business logic encapsulated in software code, the requirements of ML models are embedded within the data itself. For example, to train a classifier distinguishing between cats and dogs, the training data must consist of accurate images of cats and dogs. If the dataset includes mislabeled images (e.g., an image of a rabbit labeled as a dog), the model will be compromised. Thus, the requirements of ML models are inherently tied to the quality and correctness of the data, making data an essential asset that determines the quality of the trained ML model [11]."}, {"title": "2 Literature Review", "content": "While Section 3 analyzes literature on data quality antipatterns to develop a taxonomy, this section discusses research on 1) data quality issues in general domains, 2) data quality issues in software engineering and SDP, and 3) data quality tools."}, {"title": "2.1 Data quality issues in general.", "content": "The challenges in data quality for ML were identified as early as 1993 [41] for different domains. For instance, Gupta et al. [42] analyzed data quality issues like inactive customers in the observation window and anomalous transaction behaviors that led to sub-optimal ML models used in banking systems. Kerr et al. [43] presented a case study of data quality and its critical nature"}, {"title": "2.2 Data Quality in Software Analytics.", "content": "In the realm of defect prediction, several research works have meticulously curated and refined predictive features, underscoring the importance of data quality within this domain. Seminal works by Kamei et al. [4] in 2013 initiated a shift towards identifying defect-prone (\"risky\") software changes just-in-time, rather than focusing on broader entities like files or packages. Concurrently, Zimmermann et al. [3] pioneered the concept of Cross-Project Defect Prediction (CPDP), leveraging defect labels from one project to improve predictions in another, and outlined crucial features and machine learning models suitable for CPDP. Such seminal studies have spurred extensive subsequent research in the field, which are encapsulated in comprehensive survey papers such as those by Punitha et al. [49], Zhao et al. [50], and Thota et al. [51].\nFor general software engineering, research around data quality has increasingly focused on data management and operational best practices. For instance, in 2020, Jain et al. [52] presented an overview of data quality for machine learning applications at IBM, highlighting the importance of intelligently designed metrics in upholding data quality. The authors examined quality for structured data, unstructured text, and human in the loop validation by subject matter experts. Renggli et al. [53] from Microsoft Research presented a data quality view of MLOps, specifically providing guidelines and perspectives on designing an efficient MLOps pipeline. Abdallah et al. [54] uncovered data quality issues for big data, presenting factors from different perspectives such as management, user, processing, service pipeline, and data perspective. Moran et al. [55] assessed the impact of data quality by performing specific"}, {"title": "3 Taxonomy of ML-specific Data Quality Antipatterns", "content": "Although previous research has demonstrated the prevalence and impact of various data quality antipatterns, there is currently no comprehensive taxonomy of antipatterns in general literature. In this section, we perform a literature review and summarize data quality into a set of eight broad antipattern types, i.e., Packaging Antipatterns, Schema Violations, Data Miscoding, Feature-Row Imbalance, Data Distribution Antipatterns, Inconsistent Representation, Correlation & Redundancy, and Label Antipatterns, which comprise of 14 sub-types. As the first step, this paper focuses on structured data quality antipatterns, in the context of ML. Future work should examine antipatterns for other data types (e.g., images, audio or video). Our taxonomy of antipatterns and strategies to address them will help researchers and practitioners in effective dealing of these antipatterns for their use cases.\nApproach. Given the absence of comprehensive studies listing these antipatterns, we focused on both tool-based solutions along with a traditional academic survey. Overall, we derived our taxonomy of antipatterns, by targeting three aspects:\nData Cleaning Tools We began our search by looking for tools and identified specific antipatterns targeted by each tool. Particularly, we looked for data cleansing, profiling, and validation tools offering detection and mitigation solutions for different antipatterns. In Subsection 3.2, we present and cite various data profiling tools along with our findings on their detection and mitigation strategies.\nLeveraging academic publication accompanying open source tools for an initial round of snowballing. Often, open source tools are accompanied by a corresponding academic publication, which we used for snowballing (i.e., looking at the cited publications from that paper). For instance, tools like data linter are accompanied by a scholarly paper [39], served as a foundational reference for further academic exploration. The"}, {"title": "3.1 Resulting Taxonomy", "content": "We identified a total of eight high-level types of antipatterns comprising a total of 14 sub-types. Figure 1 presents a taxonomy of the eight high-level data quality antipattern types along with their 14 sub-types.\nSchema Violations. A data schema identifies a blueprint for the organization of a dataset based on a set of rules. Violations of the rules that explain the logic of the objects/associations/entities described by the data are schema violations.\nFor relational databases, integrity constraints are enforced to ensure the correctness of data within a defined schema. However, violations can occur if such rules are not enforced or are undefined. Schema violations are quality antipatterns as data that violates its schema is likely to be inaccurate and therefore should introduce noise to the pattern that ML models are trying to learn.\nSchema rules have been proposed and used for data cleaning in the literature. For instance, in the context of defect prediction, Shepperd et al. [10] number-lines (total lines of in a source code file) > number_blank_lines (lines which have no code or comments, and are left blank) is a schema rule.\nData Miscoding. Data miscoding refers to the incorrect assignment of data types to features, such as treating numerical data as strings. ML models are trained based on the incorrect data type, leading to suboptimal performance if, for instance, a numerical column is incorrectly treated as a string. Data miscoding encompasses the use of incorrect data types while performing ML operations during the training, validation, or serving phases of machine learning processes [58]. For instance, in the training phase, models trained from an incorrect data type may learn improper information from the miscoded features."}, {"title": "3.2 Antipattern Detection and Mitigation Strategies", "content": "In this sub-section, we describe the detection approach and mitigation process for each of these eight top level and 16 sub-categories of antipatterns discussed."}, {"title": "3.3 Packaging Antipatterns", "content": "1. Missing values\nDetection Strategies: Missing values can be detected by checking for null data.\nMitigation Strategies: Dropping or imputing the missing values. Typical imputation strategies include Hot Deck, Cold Deck, Mean Substitution, Non-negative matrix factorization, and regression [62].\n2. Duplicates\nDetection Strategies:\nDetecting exact duplicates across rows.\nUsing distance metrics for detecting numeric duplicates [62].\nString duplicates can be checked using a) character-based b) token-based, and c) phonetics-based similarity metrics [62].\nMitigation Strategies:\nDropping duplicates.\nClustering duplicates and record fusion. Hassanzadeh et al. [73] provide a framework for evaluating clustering algorithms for data duplication. Clustering is followed by fusing records, which can be done by probabilistic methods [62] or other conflict resolution strategies provided by [74]."}, {"title": "3.4 Schema Violations", "content": "Detection Strategies: Rule based unit testing (i.e., checking the data values against a set of pre-defined rules). Tools like [80] can be used to encode schema rules and check for violations.\nMitigation Strategies: Either removing or imputing the violations. Imputation strategies are discussed in the previous subsection."}, {"title": "3.5 Data Miscoding", "content": "Detection Strategies. Explicit type checking9.\nMitigation Strategies. In the scripts for data preprocessing for model training and serving, explicit type casting10 can help prevent this antipattern."}, {"title": "3.6 Row Feature Imbalance", "content": "Detection Strategies. Checking the ratio of the number of rows with respect to the number of columns.\nMitigation Strategies include downsampling or upsampling. Downsampling methods include:\nFeature selection can be done by wrapper methods [82] or filter methods [83].\nDimensionality reduction can be done by Principal Component Analysis (PCA), Linear Distriminant Analysis (LDA), etc. algorithms [62].\nClustering can be done by K-Means, Hierarchical Clustering, etc. algorithms [62].\nUpsampling or data augmentation can be done via algorithms like Synthetic Minority Over-Sampling technique (SMOTE), Adaptive Synthetic Sampling (ADASYN), etc. [62]"}, {"title": "3.7 Data Distribution Antipatterns", "content": "3.7.1 Distribution Shift\nTraining-serving skew\nDetection: Checking data drift using L-infinity distance for categorical features, and Jensen-Shannon divergence for numeric features11.\nMitigation strategies: Reshuffling or taking alternative batches of training and serving data.\nConcept Drift\nDetection Strategies: such as 1) Sequential analysis [84,85], 2) Control charts [86,87], and 3) Contextual analysis [88,89]. Gama et al. [90] provide a comprehensive survey explaining these three strategies.\nMitigation Strategies: Online learning, periodically retraining the data, feature dropping12."}, {"title": "3.7.2 Outliers", "content": "Detection Strategies:\nML-based detection. ML outlier detection algorithms include Seq2Seq, Isolation Forest, and Variational Auto-Encoders13.\nDistance-based detection. [62] Global or local methods depending on the reference population used when determining whether a point is an outlier.\nStatistics-based. [62] Hypothesis testing methods like Grubbs Test [91] and Tietjen-Moore Test [92].\nFitting distribution methods: Fitting a distribution or inferring a probability density function based on the observed data, for example, local distance-based outlier detection method. [93]\nMitigation Strategies: Dropping the outliers [94], or transforming the feature having outliers."}, {"title": "3.7.3 Skewness", "content": "1. Categorical variable skewness\nDetection Strategies: Evaluating the frequency distribution (same as class imbalance but for explanatory variables).\nMitigation Strategies: Using over-sampling or under-sampling algorithms to balance the affected variables [95].\nOversampling techniques like random oversampling, SMOTE, ADASYN, and augmentation."}, {"title": "3.8 Unnormalized Features", "content": "Detection Strategies: Inspecting feature distributions using tools like [80, 81,39].\nMitigation Strategies: Transformations like min-max scaling, Z-scoring [39] robust scaling, etc."}, {"title": "3.9 Inconsistent representation", "content": "Defection Strategies: Rule based testing of inconsistencies. This can be done by: 1) Using regex for verifying patterns14.2) Validating categorical values to belong within a predetermined set15.\nMitigation Strategies 1) Removing inconsistencies. 2) Rule-Based replacing inconsistencies can be done by unifying representations programmatically or using external tools. 16"}, {"title": "3.10 Label Antipatterns", "content": "3.10.1 Inconsistent labeling\nRefer to Subsection 3.9 for detection and mitigation strategies."}, {"title": "3.10.2 Mislabels", "content": "Mitigation strategies:\nHuman Verification:\nLabel verification using crowdsourcing [97] can correct labeling errors."}, {"title": "3.10.3 Class Overlap", "content": "Detection Strategies: Detection strategies for Class overlap have been discussed in Section 4.\nMitigation Strategies: Separating, Merging or Discarding [99] class overlapping rows."}, {"title": "3.10.4 Class Imbalance", "content": "Detection Strategies: Detection strategies include checking the ratio of the different classes in the training data, as discussed in Section 4.\nMitigation strategies\nData-sampling methods like Random Over-Sampling (ROS), Random Under Sampling (RUS), synthetic minority over sampling technique (SMOTE) [100].\nFeature selection methods can identify key features that distinguish between classes, improving classifier accuracy in uneven datasets [101]. A survey by Leevy et al [102] provides a detailed explanation different feature selection strategies.\nAlgorithm-level methods like Chi-FRBCS-BigDataCS algorithm [103], a Fuzzy Rule-Based Classification System (FRBCS), linguistic cost-sensitive FRBCS [104] are designed to manage the challenges involved with class imbalance [102]."}, {"title": "3.11 Correlation & Redundancy", "content": "Detection Strategies:\nCorrelation. For detection of correlation between 1) numerical variables, Pearson or Spearman correlation metrics can be used; 2) Numerical and categorical using ANOVA or Kruskal-Wallis H test; 3) Categorical variables using Chi-Squared test.\nRedundancy analysis can be done by 1) building parametric additive model which determines how well a variable is predicted from the remaining explanatory variables, and dropping the most predictable variable in a step wise fashion 20"}, {"title": "4 Case Study Setup", "content": "In this section, we describe the case study setup that is used to answer the research questions."}, {"title": "4.1 Studied Dataset", "content": "Software defect prediction (SDP) is the process of using software-related metrics as explanatory and response variables towards building ML or statistical models for identifying and forecasting potential defects in software applications before they occur. Such models help to identify patterns and correlations that can predict future software bugs, improving the quality of future releases.\nIn this study, we use the SDP dataset from Yatish et al. [26]. The metrics used in this dataset are code related (i.e., metrics that define the relationship between code properties and software quality), owner related (i.e., metrics that describe the relationship between module ownership and software quality) and process related (i.e., metrics that define relationship between development activities and software quality) [26]. Table 1 presents the 65 metrics adapted from Yatish et al. used in this study. Yatish et al.'s study delved into the characteristics of the response label in SDP, can be interpreted in our study context as an effort towards \"mislabel correction\". The dataset comprises nine popular open source libraries and their 30 project versions, as aggregated in Table 2."}, {"title": "4.2 Antipattern Detection Techniques", "content": "In this sub-section, we outline the procedure for checking the different data quality antipatterns within our studied SDP dataset. We describe the approach to detect each of these data quality antipatterns below."}, {"title": "4.2.1 Schema Violations", "content": "A schema refers to predefined rules and structures that dictate the acceptable values and relationships for data. These rules help identify and rectify implausible or erroneous data points. Since there is no formal definition of SDP schema rules in academia, we first formulate a schema for our studied SDP data, then check for deviance from the logic defined within these rules. Our approach for finding schema rules is two-phased. Firstly, we referred to"}, {"title": "4.2.2 Distribution Antipatterns", "content": "We use Hynes et al.'s [39] implementation to identify non-normalized features, and tailed features.\nConstant Features. We identify constant features by checking whether the variance of a feature is zero. Such an approach can only be adopted for numerical features. We do not have any categorical features within our studied SDP data, and hence, have not adopted any technique to remove any constant categorical feature.\nData Drift. For each project in our studied SDP dataset, we detect drift between its consecutive versions. For instance, to detect data drift for the project Derby across versions, 10.2.1.6, 10.3.1.4, and 10.5.1.1, we study drift between i) 10.2.1.6 and 10.3.1.4, ii) 10.3.1.4 and 10.5.1.1. Similarly, we compared all consecutive version data for each of the nine projects in our dataset. Notably, our approach employs TFDV's (tensorflow data validation tools) implementation to detect this consecutive drift.\nUnnormalized Distributions. This calculation involves trimmed means (the mean calculated after removing the top and bottom 10% of data points, which may include extreme low or high values) and standard deviations for these features, thereby discounting extreme values at both ends of the spectrum. This approach focuses the analysis on more representative central data points. For each numeric feature, the detector assesses the deviation of its mean and standard deviation from these trimmed statistics using z-score calculations. Features whose mean or standard deviation deviances exceed a certain threshold23 are considered unnormalized.\nTailed Distributions. Along with deriving this antipattern from Hynes et al. [39], we derived the implementation from their research. Notably, the implementation relies on checking whether the deviation of a feature mean is greater than that of its trimmed mean. The implementation detects custom missing/placeholder values (e.g., -999), along with finding obvious statistical outliers."}, {"title": "4.2.3 Package Antipatterns", "content": "Empty Values. We use pandas' isnull24 function to check for empty values.\nDuplicates. In the studied SDP dataset, a row represents a source code file within a specific project version. If that file has identical feature values as another file, then such files (rows) are duplicated. We detect duplicates on a row level using the duplicated() function25 from the Pandas package."}, {"title": "4.2.4 Label Antipatterns", "content": "Yatish et al.'s [26] dataset consists of labels (i.e., whether a file is buggy) assigned based on two different labeling strategies. At the core of data preparation for SDP is the identification of post-release defects, where modules are modified to fix defects post-release. The traditional approach for identifying post-release defects involves extracting specific keywords and issue IDs from commit logs in the version control system using regular expressions [4,105]. Labels derived from this heuristic-based approach are used to train ML models to predict the bugginess of future code.\nYatish et al. refined this traditional approach following Da Costa et al. [63], by considering affected releases in an issue-tracking system to identify defect-introducing commits. This realistic method incorporates release data when finding bug-introducing commits, unlike the traditional \"heuristic\" approach. We use the feature names \"RealBug\" and \"HeuBug\" from Yatish's dataset to label correct and mislabeled tuples.\nClass Imbalance. We determine the presence of class imbalance by evaluating the ratio of the minority class with respect to the overall size of the data for each project-version of the SDP dataset.\nClass Overlap. We use Gong et al.'s [21] improved K-Means clustering cleaning approach (IKMCCA) to detect rows with overlapping classes for each project version of the SDP data. This approach solves both class overlap and class imbalance problems by combining the K-Means Clustering Cleaning Approach (KMCCA) [106] and the Neighbourhood cleaning technique (NCL) [18]. Gong et al. [21] inculcated class imbalance into the KMCCA and empirically demonstrated model performance boosts from the IKMCCA over the KMCCA approach. After performing K-means on the explanatory variables, IKMCCA checks if the percentage of defective instances in the ith cluster lies below a threshold p%, then the cluster's defective instances are removed, otherwise, the non-defective instances for that cluster are removed.\nSince class overlap depends on the labels, our studied dataset contains the potentially mislabeled variable (\u201cHeuBug\u201d) and correctly labeled variable (\"RealBug\"), we use the corrected labels in RQ1, and use the correct or"}, {"title": "4.2.5 Feature-Row Imbalance", "content": "We check the ratio of the number of features (columns) and samples (rows) for each project version of our studied SDP dataset. The threshold of 10 is commonly used as a rule-of-thumb value in the industry2627."}, {"title": "4.2.6 Correlation & Redundancy", "content": "We consider both correlation & redundancy as one group, since prior research [107] identified that these two have an analagous impact on model interpretation, while causing slight reduction to the model performance. While, typically, a data scientist/engineer would use domain knowledge to cherry pick one important feature from a group of the correlated and redundant ones, we use Jiarpakdee et al.'s [14] tool,, RNalytica [108], specifically designed for picking the important SDP features from a cluster of correlated & redundant variables. Moreover, this tool is commonly adopted by many research publications on software engineering [26,66, 109, 110]."}, {"title": "4.3 Constructing Defect Models", "content": "In this subsection, we discuss the different aspects of constructing ML models for RQ2 and RQ3."}, {"title": "4.3.1 Studied Learners", "content": "For RQ2 and RQ3, we construct ML models using four learners, namely, Neural Network (NN), Logistic Regression (LR), Random Forest (RF), and XG-Boost (XGB). These learners are chosen as they represent the prominent families of learners [111] used in defect prediction. Specifically, these learners were selected from the Bagging, Boosting, Regression, and Neural Network families respectively. From Tantithamthavorn et. al [111], we excluded other families such as Naive Bayes, Nearest Neighbors, Discrimination analysis, Rule-based, and SVM due to them being known for a lack of performance in defect prediction [112,113]. Since we already use the random forest classifier, which is an aggregation of decision trees, we exclude the decision tree family. Lastly, the Partial Least Square family is a dimensionality reduction technique, and is not directly applicable to our classification task. Within the Neural Network family, we use a plain neural network classifier with one to five layers, as opposed to RNN or CNN architectures given that deep learning was shown to not be necessarily better for the use case of SDP [114]."}, {"title": "4.3.2 Hyper-parameter tuning", "content": "Each model built for evaluating RQ2 and RQ3, was subjected to hyper-parameter tuning for optimal model performance. We performed parameter tuning, keeping the #iterations = 10 (i.e., 10 different sets of hyper-parameters were evaluated) using the Random Searching algorithm, which is more efficient than its alternative, grid search in estimating the optimal hyper-parameters [115]. We kept the value of k = 3 in k-fold cross-validation parameter (i.e., training data is split into three smaller folds and the process is repeated three times). Overall, the evaluation of one model's hyper-parameter with #iterations = 10 and k = 3 resulted in 30 model training runs.\nWhile software analytics research (e.g., Agarwal et al. [116]) has commonly used the default values of k, i.e., k = 5, the massive number of models trained in our research, i.e., a total of 0.78 Million models for RQ2 and RQ3, made this value of k infeasible, forcing us to reduce to k = 3.\nWe obtain our hyper-parameter grid (see Table 4) by adopting values from popular software analytics research. Specifically, the hyper-parameters for LR and RF learners were adopted from Aggarwal et al. [117], XGB hyper-parameters from Alazba et al. [118], and NN parameters from Bhaweres et al. [119]."}, {"title": "4.3.3 Model Validation", "content": "We split 80% of the data into training and 20% into testing using stratified sampling to ensure that there are instances of buggy and non-buggy classes in both sets. This split is done 10 times, with ML models built on each split to obtain the performance metrics. Although bootstrapping is considered a robust validation strategy [66], sampling with replacement leads to duplication of data in the training set. This does not allow us to assess the impact of Duplication as a data quality antipattern, restricting our use of bootstrapping as a validation technique."}, {"title": "4.3.4 Performance Estimation", "content": "For each of the ML models built in RQ2 and RQ3, we evaluate the performance of the SDP models using the two threshold dependent and four threshold independent metrics given below. A threshold classifies a probabilistic outcome as either \"defective\" or \"non-defective\".\nAU-ROC or Area Under the Receiver Operator Curve is a threshold-independent metric. AU-ROC measures the area under the curve at all thresholds of the receiver operator curve. This curve plots the true positive rate (sensitivity) against the false positive rate (1 - Specificity).\nAU-PRC is another threshold-independent metric that calculates the Area Under the Precision-Recall Curve at different thresholds. AU-PRC is a better estimate for datasets suffering from the class imbalance antipattern [120].\nPr or Precision estimates the observations correctly predicted out of all predictions. It is calculated as True Positives / (True Positives + False Positives).\nRe or Recall estimates the observations correctly predicted over the total number of correct observations. It is calculated as True Positives / (True Positives + False Negatives).\nF1 Score strikes a balance between Precision and Recall by calculating the harmonic mean between the two. Its calculated as 2(Pr * Re)/(Pr + Re)\nMCC or Mathews Correlation Coefficient (MCC) is considered as a reliable metric [121] as it produces good scores only if the prediction returns good rates for true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). MCC is calculated as\n(TP *TN-FP * FN)/\u221a(TP + FP)(TP + FN)(TN + FP)(TN + FN)\nWe use a threshold of 0.5 in the confusion matrix to estimate our threshold dependent metrics (Pr, Re, F1, and MCC). Although prior research suggests that threshold independent metrics (AU-ROC and AU-PRC) are more accurate estimators of model performance [6,122], we report both to provide practitioners with more insights on how data quality antipatterns affect their specific use case. For example, in some cases, identification of True-Positives using Recall may be more important than AU-ROC or Precision."}, {"title": "5 Results", "content": "In this section, we delve into the motivation and results of each of our research questions."}, {"title": "5.1 RQ1: What is the prevalence of data quality antipatterns in software defect prediction data?", "content": "Motivation.\nDespite the widespread use of SDP data in software engineering research [35, 65,123], researchers utilizing SDP datasets lack an empirical understanding of the presence of data quality antipatterns within the software defect prediction datasets. Singular data quality antipatterns have been found in Software Engineering datasets ([10,21,39,124]), however, to the best of our knowledge, no research yet checked the prevalence of multiple data quality antipatterns in the context of software engineering. Understanding the prevalence of these antipatterns is important for both researchers and practitioners, as it can provide valuable insights into the factors that impact the performance and interpretation of machine learning models used for defect prediction. Hence, in this RQ, we aim to check the prevalence of data quality antipatterns in SDP data.\nResults. Out of the 19, i.e., five top-level types 28 and 14 sub-types of antipatterns in our taxonomy 1, we found nine antipatterns to commonly occur in the studied SDP dataset. The distribution based antipatterns (tailed and unnormalized metrics) along with the label antipatterns, class overlap and mislabel are prevalent in all SDP project versions, while the other antipatterns occur in 2-50% of the projects versions.\nWe did not detect the presence of four antipatterns (i.e., Data Miscoding, Inconsistent Representation, Missing Values and Data Drift) in the studied datasets. Since Data Miscoding is a dynamic issue, an ML pipeline (or non-ML data-driven program) should be able to validate the correct data type; nevertheless, we are unable to identify this as an antipattern in the static dataset from Yatish et al [26]. Furthermore, there are neither missing values nor inconsistent representations in the studied SDP data. Perhaps, Yatish et al. [26] cleaned missing values during the data procurement phase of conducting their study, while we do not suspect Yatish et al's work to encounter inconsistent representation, as this antipattern would most likely be prevalent in text-like features.\nBelow, we present each of the antipatterns prevalent in the SDP data:"}, {"title": "5.1.1 Row Feature Imbalance", "content": "The studied SDP data lacks any row feature imbalance. As illustrated in Figure 2, the ratio of the number of rows to the number of columns is greater than a threshold of 10, as demarcated by the red line in the Figure. The threshold of 10 is commonly used as a rule-of-thumb value in the industry2930. With more than 10 times the rows than the features, we consider there to be no row feature imbalance. Notably, the outliers in Figure 2 include the project version Camel-2.9.0 having the maximum (7,120) number of rows, and the project version Jruby-1.1 having the minimum (731) number of rows. Each row in Yatish et al.'s [26] dataset represents a java file, and these outliers are due to the project versions with the least or the maximum number of files in their codebase."}, {"title": "5.1.2 Schema violations", "content": "2% of the studied SDP dataset has schema violations. Figure 3 shows the distribution of violations for different project versions of the studied SDP data. We observed violations for five (i.e., R8, R14, R15, R17, and R19 out of 20 rules defined in Table 3. A total of 30, 12, 7, 6, and 6 files had violations against rules R14, R8, R14, R15, and R17. Overall, the maximum number of violations were observed for project hive version 0.10.0 with 7% data having schema violations.\nWe suspect that the calculation of complex metrics like complexity could lead to higher chances of violations either due to errors in calculations or due to more complex code structures not adhering to the rules R14, R15, R17. Rules without violations might be simpler and more direct in their measurements (like R1, R2, R3). For R14, in deeply nested functions, calculation of the average number of lines might become skewed, with large disparity in size and complexity of nested functions which may have led to an erroneous calculation. Further work may be required to pinpoint the exact cause of such violations."}, {"title": "5.1.3 Data Distribution Anti-patterns", "content": "Tailed Distribution. All project versions have tailed columns. The number of projects having tailed distributions is shown in Figure 4 (left). Overall, the 5 point summary of tailed projects is: 11 (min), 15.25 (25th), 18 (median), 21 (27th 24 (max) indicating that the studied project versions have at least a median of 18 tailed metrics. The metrics CountDeclMethod, COMM and MaxNesting_Min are tailed across all projects.\nUnnormalized Distributions. Unnormalized distributions are prevalent in all project versions of the studied SDP dataset. Particularly, 17% of metrics are consistently unnormalized, while 37% are inconsistently unnormalized. The 19 metrics detected as unnormalized are illustrated in Figure 4 (right)."}, {"title": "5.1.4 Packaging Errors: Duplicated Data", "content": "In the studied SDP dataset, a median of 2."}]}