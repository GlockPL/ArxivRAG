{"title": "Single Ground Truth Is Not Enough:\nAdd Linguistic Variability to Aspect-based Sentiment Analysis Evaluation", "authors": ["Soyoung Yang", "Hojun Cho", "Jiyoung Lee", "Sohee Yoon", "Edward Choi", "Jaegul Choo", "Won Ik Cho"], "abstract": "Aspect-based sentiment analysis (ABSA) is\nthe challenging task of extracting sentiment\nalong with its corresponding aspects and opin-\nions from human language. Due to the inher-\nent variability of natural language, aspect and\nopinion terms can be expressed in various sur-\nface forms, making their accurate identification\ncomplex. Current evaluation methods for this\ntask often restrict answers to a single ground\ntruth, penalizing semantically equivalent pre-\ndictions that differ in surface form. To address\nthis limitation, we propose a novel, fully auto-\nmated pipeline that augments existing test sets\nwith alternative valid responses for aspect and\nopinion terms. This approach enables a fairer\nassessment of language models by accommo-\ndating linguistic diversity, resulting in higher\nhuman agreement than single-answer test sets\n(up to 10%p improvement in Kendall's Tau\nscore). Our experimental results demonstrate\nthat Large Language Models (LLMs) show sub-\nstantial performance improvements over T5\nmodels when evaluated using our augmented\ntest set, suggesting that LLMs' capabilities in\nABSA tasks may have been underestimated.\nThis work contributes to a more comprehensive\nevaluation framework for ABSA, potentially\nleading to more accurate assessments of model\nperformance in information extraction tasks,\nparticularly those involving span extraction.", "sections": [{"title": "1 Introduction", "content": "Aspect-based sentiment analysis (ABSA) is a so-\nphisticated natural language processing task that\naims to extract fine-grained sentiment information\nfrom text. This method provides nuanced insights\ninto opinions about specific attributes of products\nor services, enabling organizations to precisely\nidentify consumer preferences and criticisms. The\ngranularity of ABSA facilitates targeted improve-\nments and data-driven decision-making across di-\nverse sectors, including product development, cus-\ntomer service, political analysis, and e-commerce."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 LLM in Information Extraction", "content": "Along with the advent of proprietary GPT (Brown\net al., 2020), Gemini (GeminiTeam, 2024) and\nthe open-sourced disclosures such as Llama-like\nLLMS (Touvron et al., 2023; Taori et al., 2023;\nChiang et al., 2023) and others (Jiang et al., 2023,\n2024; Team, 2024). LLMs are commonly adopted\nin recent Natural Language Processing (NLP) (Liu\net al., 2022; Min et al., 2022; Yoo et al., 2022). For\ninformation extraction (IE) tasks, diverse works\nin named entity recognition (Xie et al., 2023; Wu\net al., 2024), relation extraction (Wan et al., 2023;\nPang et al., 2023a; Li et al., 2023a,b; Xu et al.,\n2023; Ma et al., 2024), and event extraction (Pang\net al., 2023b; Wang et al., 2023b).\nData augmentation with LLMs is studied ac-\ntively (Xu et al., 2023; Wang et al., 2023a). While"}, {"title": "3 Method", "content": "To encompass the diversity of span extraction in\nthe ABSA task, our pipeline aims to extract diverse\nexpressions of ground truth (GT) aspect and opin-\nion terms. Based on Figure 2, we introduce the\nalgorithm of our pipeline, ZOOM IN-N-OUT, and\ndescribe how the exact match measure is calculated"}, {"title": "3.1 ZOOM IN-N-OUT", "content": "Let test set be $D = \\{(x_k, Y_k)\\}\\_{data}$ where $N\\_{data}$\nis the total number of test examples. Let $d =$\n$(x, Y) \u2208 D$ that includes a ground truth (GT) $Y$\nwhich consists of one or more quadruples, that is,\n$Y = \\{y_i\\}\\_{i=1}^{N}$ for $N$ > 1. Each GT quadruple\n$y \u2208 Y$ is derived from $x$ and refers to (a, c, s, o),\nnamely aspect, category, sentiment, and opinion.\nAs shown in Figure 2, our pipeline consists of three\nsteps that are separately applied to aspect and opin-\nion terms before merging the results. From now on,\nwe will explain each step, focusing on the case of\nthe opinion term.\nZOOM-IN Starting from the original GT opinion\nterm $o$ in $y$ and the given sentence $x$, the LLM\n(hereafter 'the model') investigates the inside of\n$o$, either by reshaping the given span or extracting\na meaningful part(s) of it. In Figure 2, from the\n\"n't worth,\" the model generates (i) \u201cnot worth\u201d by\nresolving the contraction from \"n't\" to \"not\" and\n(ii) \"worth\" as a partial span.\nZOOM-OUT After focusing on the inside of $o$,\nwe expand our probe to the entire sentence $x$ to find\nand integrate nearby words. In the case of Figure 2,\nsince \"waiting\u201d follows \"n't worth\", the model gen-\nerates combinations such as \"n't worth waiting\",\n\"not worth waiting\u201d, and \u201cworth waiting\u201d.\nJUDGE and Filter However, newly generated\nterms might not always be appropriate due to the\nhallucination. To exclude the inappropriately gen-\nerated terms, we construct four criteria as follows:\n(i) relevance to aspect and category, (ii) consis-\ntency with opinion and sentiment, (iii) extractabil-\nity from $x$, and (iv) independence from other terms,\nwhich is based on the SemEval annotation guide-\nline (Pontiki et al., 2016). Following the LLM-as-a-\njudge (Zheng et al., 2023; Madaan et al., 2023), the\nmodel verifies if a new term meets all four criteria.\nIf any of them are not met, the term is excluded.\nIn Figure 2, \"worth\" and \"worth waiting\" are ex-\ncluded because they fail to maintain the negative\nsentiment of original GT \"n't worth,\" violating the\nsecond consistency criteria with original opinion\nand sentiment.\nAfter conducting the same procedure on the\naspect term, we have an aspect term set $A =$\n$\\{a_n\\}\\_{n=1}^{N_a}$ and an opinion term set $O = \\{o_m\\}\\_{m=1}^{M_o}$,\nwhere $N_a$ is the number of aspect terms and $M_o$"}, {"title": "3.2 Measurement", "content": "For a given task $T$, here ABSA (especially ASQP\ntask), let a sentence $x$ be an input of a language\nmodel $f$ that predicts one or more quadruples. For\neach $x$ given principles of $T$, there exists ground\ntruth $(Y)$, a set of quadruples $\\{y_i\\}$ where each $y_i$\nconsists of (a, c, s, o). Let a prediction be $\\hat{Y} =$"}, {"title": "4 Experiments", "content": "In this section, we first assess the validity of our\nexpanded GTs by having humans evaluate our new\nGTs. We show that evaluations using ours align\nmore closely with human assessments than the orig-\ninal and analyze diverse baseline models with the\noriginal and our GT sets.\nOur GT set We expanded the GT quadruples\nof the ACOS and ASQP test sets with ZOOM IN-\nN-OUT. The pipeline is executed for each aspect\nand opinion term independently and merges the\nresults at last. All the expansion processes utilize\ngpt-4o-2024-05-1. The average cost to build one\nextended dataset was $75. ZOOM-IN and ZOOM-\nOUT steps collect possible quadruples via 5-shot\nICL (examples in Table 15). To collect as many\ndiverse terms as possible, we repeat generation\nthree times with a temperature of 0.3. JUDGE had\nthe LLM verify each quadruple via a 5-shot CoT\nprompting (examples in Table 18). Overall prompts\nused in each stage can be found in the Appendix D.\nDatasets ASQP (Zhang et al., 2021) and\nACOS (Cai et al., 2021) datasets are utilized. For\nACOS-Laptop, the total number of categories is\n121. Since the category consists of two levels: en-\ntity (e.g., \"laptop\") and attribute (e.g., \u201cprice\u201d), we\nonly consider the 23 entity-level categories.\nBase models To compare the original GT set $Y$\nand ours $Y\\_{new}$ extended by ZOOM IN-N-OUT, we\nevaluate both T5 and LLM-based ABSA models in\na few-shot setting. All experiments utilize the same\n20 examples for each dataset, and the considera-\ntions for the selection process can be found in Ap-\npendix C.3. Representative T5-based fine-tuning\nmethodologies for ABSA are Paraphase (Zhang"}, {"title": "4.1 Dataset Validity", "content": "To assess the validity of our expanded GT sets, we\nconducted human evaluations on our newly gener-\nated GTs. Specifically, we randomly sample 80 ex-\namples from each dataset and then ask three human\nevaluators to determine whether our expanded GTs\nare valid considering the given sentence. Table 2\nshows the number of quadruples for prediction and\nGT sets. Before the evaluation, three human evalu-\nators were educated about the task and had a full\nunderstanding of the ABSA task. A majority vote\ndetermines the final human validity result. Details\nabout the interface are in Appendix A.\nFrom Table 3, our human evaluation shows a\nhigh level of human validity percentage, with all\npercentages exceeding 90%. We also calculate the"}, {"title": "4.2 Alignment with Human Evaluation", "content": "To prove that evaluations with our expanded GTs\nare more closely aligned with humans than those on\nthe original GTs, we conduct human evaluations\non the model-predicted output and compare the\nalignment with our and the original GT sets.\nFrom each of the four datasets, we randomly\nsampled 80 test examples and then extracted pre-\ndictions for two models: MvP and GPT-3.5-Turbo,\nwhich is the same as with the previous validity\nexperiment. Three human annotators are given\ntask descriptions and 20 examples identical to\nthose given to the models. Then, annotators judge\nwhether each prediction is appropriate or inappro-\npriate for a given sentence and task. A majority\nvote determines the final human decision. Inter-\nannotator agreement (IAA) is measured between\nhuman evaluators with the two GT sets, i.e., Origin\nvs. Humans and Ours vs. Humans. We utilize\nCohen's Kappa (\u03ba) (Cohen, 1960) and Kendall Tau\n(\u03c4) (Kendall, 1938) as metrics for IAA evaluation.\nPearson correlation is omitted because it had the\nsame score as Kendall Tau due to the limited sam-\nple size. Details of human evaluation, including the"}, {"title": "4.3 Re-evaluate Model Performance", "content": "To re-evaluate the models in our new GT set, we\ncompare diverse ABSA models, including five\nLLMs on four ASQP datasets. The scores are F1\nusing the exact match measurement. Since the pre-\ndiction order of the elements of each quadruple\nhighly matters in accuracy (Hu et al., 2022; Gou\net al., 2023), all models predict 24 different or-\nders. Naive scores are summarized in their average\nvalues. We report the results using an ensemble\nalongside the naively predicted results. The pre-\ndiction in the ensemble is chosen if the prediction\nappears at least three times in top-5 order or seed.\nThe scores for all orders are in Appendix E.\nAs shown in Table 6, our experimental results\nreveal significant performance improvements in\nABSA tasks when evaluating LLMs using our aug-\nmented GT sets. LLM-based models demonstrate\nan average F1 score improvement of 9.8 percentage\npoints, compared to 2.3 for T5-based models. This\nsubstantial disparity suggests that conventional GT\nsets may have underestimated LLM performance\nin ABSA tasks due to their limited consideration\nof expression diversity. Notably, Llama-3.1 ad-\nvances from second to first place in three out of\nfour datasets under our new evaluation method, sur-\npassing the initially top-performing Gemini."}, {"title": "5 Analysis", "content": ""}, {"title": "5.1 Statistics of Our Dataset", "content": "Changes in each step Table 7 shows the added\nor removed number of terms at each step of our\npipeline for each aspect (A) and opinion (O) term.\nOur final datasets have more 638 aspect and 967\nopinion terms than the original GT set, on aver-\nage. ZOOM-OUT generates more terms in both\naspect and opinion than ZOOM-IN. This is natural\nbecause ZOOM-OUT can extract terms anywhere"}, {"title": "5.2 Ablation Study", "content": "We validate the role of the ZOOM-IN, ZOOM-OUT,\nand JUDGE steps in expanding the existing GT set.\nFollowing the experimental setup in Section 4.2,\nwe analyze the agreement with human evaluation\nfor each step.\nTable 8 shows the number of expanded quadru-\nples for the sampled 80 examples with human rat-\nings, with the changes in IAA for each step. The\nZOOM-IN and ZOOM-OUT steps show a general\nimprovement in IAA, with the exception of the\nZOOM-OUT step for ACOS-Laptop. This result im-\nplies that each of these processes helps transform\nthe existing GT to correspond closely to human\nevaluation criteria. On the other hand, the drop in\nIAA in the ZOOM-OUT process of ACOS-Laptop\nwas analyzed to be caused by an issue with categor-\nical values, which our extension does not handle.\nThis can be understood as a limitation since our\nmodel does not consider category and sentiment.\nIn the JUDGE process, the number of quadruples\ndecreases by about 20% on average but the IAA"}, {"title": "6 Conclusion", "content": "We present ZOOM IN-N-OUT, a novel approach\nfor the Aspect-Based Sentiment Analysis (ABSA)\ntask that addresses the limitations of traditional\nevaluation methods by accounting for the diversity\nof expressions in aspect and opinion terms. By ex-\npanding the ground truth sets to include valid vari-\nations that preserve original meanings, our method\naligns more closely with human judgments. Exper-\nimental results show that Large Language Models\n(LLMs) significantly outperform T5 models under\nthis improved evaluation, suggesting that LLMs\u2019\ncapabilities in span extraction tasks have been un-\nderestimated due to restrictive evaluation practices."}, {"title": "Limitation", "content": "Our study performed the quadruple prediction task\nbetween the various tasks of ABSA, which in-\ncluded tuple or triplet predictions. However, such\ntasks exist as part of quadruples, so there is plenty\nof room for our approach to be utilized. Also, only\nexplicit mention in the sentence is covered in our\nexperiments. This is because the implicit cases\ntagged in \"null\" make it difficult to extract differ-\nent representations where the meaning of \u201cnull\u201d is\nhidden in the sentence."}, {"title": "Ethics Statement", "content": "All human evaluators voluntarily agreed to partic-\nipate in this study. We informed them that their\nresponses would be anonymized and securely pro-\ntected. Evaluators were free to stop the survey at\nany time. Compensation was more than adequate.\nAs our survey and dataset do not contain any sen-\nsitive or harmful content, we do not anticipate any\nnegative ethical impacts from our research."}, {"title": "Broader Impacts", "content": "We expect that our work will contribute consider-\nably to improving ABSA's use of LLMs. Our re-\nsearch will inspire further studies, fostering a more\naccurate and diverse understanding of ABSA. Ow-\ning to the flexibility of our framework, we strongly\nencourage researchers in NLP tasks other than\nABSA to apply our method to their tasks. However,\nit is important to note that since each task requires\ndifferent characteristics, we recommend that re-\nsearchers make necessary changes to the tasks at\nhand."}, {"title": "Appendix", "content": ""}, {"title": "A Details of Human Evaluation", "content": "In this section, we describe the full results and the\nexperimental details of Section 4.1 and Section 4.2,\nwhich include human annotations. Three authors\nvolunteered to participate in this study. We used\nthe four ASQP datasets and randomly selected 80\nsamples in test sets where the seed was 42."}, {"title": "A.1 Dataset Validity", "content": "In this experiment, we asked human annotators\nto validate whether the new GT quadruple is cor-\nrect with the given sentence. The user interface is\nshown in Figure 4."}, {"title": "A.2 Human evaluation", "content": "Human annotators evaluated the predicted quadru-\nples from three models: MvP, Llama-2-70B, and\nGPT-3.5-Turbo. The annotators were provided with\na sentence and a predicted quadruple and asked to\ntag whether the predicted quadruple was correct or\nnot. The user interface is shown in Figure 5."}]}