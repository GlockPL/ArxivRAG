{"title": "Building an Ethical and Trustworthy Biomedical Al Ecosystem for the Translational and Clinical Integration of Foundational Models", "authors": ["Baradwaj Simha Sankar", "Destiny Gilliland", "Jack Rincon", "Henning Hermjakob", "Yu Yan", "Irsyad Adam", "Gwyneth Lemaster", "Dean Wang", "Karol Watson", "Alex Bui", "Wei Wang", "Peipei Ping"], "abstract": "Foundational Models (FMs) are emerging as the cornerstone of the biomedical Al ecosystem due to their ability to represent and contextualize multimodal biomedical data. These capabilities allow FMs to be adapted for various tasks, including biomedical reasoning, hypothesis generation, and clinical decision-making. This review paper examines the foundational components of an ethical and trustworthy AI (ETAI) biomedical ecosystem centered on FMs, highlighting key challenges and solutions. The ETAI biomedical ecosystem is defined by seven key components which collectively integrate FMs into clinical settings: Data Lifecycle Management, Data Processing, Model Development, Model Evaluation, Clinical Translation, Al Governance and Regulation, and Stakeholder Engagement. While the potential of biomedical Al is immense, it requires heightened ethical vigilance and responsibility. For instance, biases can arise from data, algorithms, and user interactions, necessitating techniques to assess and mitigate bias prior to, during, and after model development. Moreover, interpretability, explainability, and accountability are key to ensuring the trustworthiness of Al systems, while workflow transparency in training, testing, and evaluation is crucial for reproducibility. Safeguarding patient privacy and security involves addressing challenges in data access, cloud data privacy, patient re-identification, membership inference attacks, and data memorization. Additionally, Al governance and regulation are essential for ethical Al use in biomedicine, guided by global standards. Furthermore, stakeholder engagement is essential at every stage of the Al pipeline and lifecycle for clinical translation. By adhering to these principles, we can harness the transformative potential of Al and develop an ETAI ecosystem.", "sections": [{"title": "Introduction", "content": "A corollary to the rise of \"Big Data\u201d is the development of large scale machine learning models that have the capacity to learn from large datasets (Reference). Foundational models, developed by Stanford's Institute for Human-Centered Artificial Intelligence, are increasingly recognized as a preferred component in the workflow for large-scale Al development, leveraging millions to billions of parameters through self-supervised or semi-supervised learning techniques (Reference). These techniques allow FMs to learn patterns, structures, and context within the data without the need for tedious manual annotation. Ultimately, FMs function as pre-trained backbones, providing foundational architectures for further adaptation and fine-tuning across diverse tasks, from predictive analytics to generative applications (Reference, Reference). For example, well-known FMs such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) are pretrained on extensive text corpora to develop a comprehensive understanding of language, significantly enhancing natural language processing capabilities in tasks such as translation (Reference).\nThe broad applicability of these models streamlines the process of model development, leading to cost-effective and scalable Al solutions across many fields (Reference). Model Hubs (such as Hugging Face's Model Hub, PyTorch Hub and TensorFlow Hub) provide a repository of pre-trained models that facilitate their integration into task specific Al workflows. This allows for the efficient development of new Al applications by adapting existing architectures, rather than necessitating the training of unique models from scratch. However, in the realm of biomedical informatics we have seen the development of domain-specific foundational models from scratch\u2014such as BioLinkBert, HeartBEiT, and scFoundation (Reference, Reference, Reference). Despite the significant overhead in terms of computational power and data handling, typically involving the orchestration of hundreds of GPUs to process and learn billions of parameters, the strategic investment is warranted. Custom-built foundational models, engineered from the ground up, are pivotal for leveraging the full spectrum of Al's capabilities in biomedicine, resulting in enhanced reasoning and understanding for specialized tasks (Reference, Reference).\nBiomedical Al technologies that were initially centered on supervised models showed promising capabilities to diagnose, predict, and recommend treatments across a variety of medical modalities and data types, such as electronic health records (EHRs), chest X-rays, and electrocardiograms (Reference). However, there has been a notable shift towards employing FMs in biomedical Al, initiating an era where their full potential can be leveraged to cultivate groundbreaking clinician-Al interactions that significantly enhance medical practice (Reference). For instance, clinical language models are able to encode clinical and biomedical language [Reference, Reference]. Similarly, FMs trained on electronic medical records (EMRs) comprehensively encode the timelines of patients' disease course [Reference]. FMs developed using single-cell and histopathology images from patient tissues, when correlated with clinical outcomes, facilitate the identification of biomarker signatures and functional states in both health and disease [Reference, Reference, Reference]. Biomedical FMs, with the ability to interpret various biomedical data modalities - encompassing multi-omics, (EMRs), radiology, histology, medical knowledge bases, and more - are envisioned to produce sophisticated biomedical insights that enhance clinical decision-making, enable precision medicine, and enhance public health (Reference, Reference).\nThe emergence of big multimodal data and foundational models shapes our perspective of the evolving Al ecosystem in biomedicine. While the emergence of a biomedical Al ecosystem holds remarkable potential for breakthroughs in disease understanding and treatment, it also necessitates a significant increase in our ethical vigilance and responsibility. It is critical that we adopt an ethically governed, co-designed approach that is built on evidence-based principles and prioritizes the needs of individuals and communities impacted by biomedical Al. This approach is essential to ensure that Al innovations contribute positively to healthcare and public health, reinforcing rather than compromising the necessary safeguards and ethical standards.\nThe main contributions of this paper are as follows:\n1. We outline the essential components of an Al Ecosystem for integrating foundational models (FMs) into clinical and/or translational settings.\n2. We examine the current landscape of ethical considerations and ethical practices specific to the development and implementation of FMs, highlighting critical challenges and existing mitigation strategies across three key areas:\nA. Mitigating Bias and Enhancing Fairness\nB. Ensuring Trustworthiness and Reproducibility\nC. Safeguarding Patient Privacy and Security\n3. We examine pivotal government and scholarly publications that chart the present guidelines and future directions for Al stewardship, emphasizing two main components:\nA. Al Governance and Regulation\nB. Stakeholder Engagement\n4. Finally, we discuss a unified perspective of how the principles of ethical and trustworthy Al & stewardship in Al integrate into the ecosystem."}, {"title": "II. Al Ecosystem in Biomedicine.", "content": "Al ecosystem is a concept that defines the complex interdependent patterns that connect developers, manufacturers, and users of Al. It provides a structure to develop an ethical and regulatory framework that promotes fairness, transparency, and accountability in the development and use of AI/ML systems (Reference).\nThe Al ecosystem can best be defined via 7 key components and is presented in Figure 1 (Reference, Reference, Reference)."}, {"title": "II. Ethical Considerations in the Al Pipeline for Foundational Models.", "content": "II.A. Mitigating Bias and Enhancing Fairness.\nBiases can arise at any stage of Al pipelines and can perpetuate throughout the system. Foundational models (FMs) require the ingestion of massive amounts of data to train billions of parameters in order to perform effectively. This scale and complexity introduces significant challenges in minimizing undesirable biases. As a result, biases can perpetuate outdated claims, lead to inaccurate insights, compromise the quality of care for marginalized groups, and/or exacerbate disparities. In this section, we explore the continuum of bias induction into Al, and explore metrics and mitigation strategies to guard against their detrimental effects. In Sections 2.A.1. to 2.A.2., we examine the sources of bias from data, algorithms, and users. Sections 2.A.3. to 2.A.5. focus on social biases identified in generative Al and the alignment methods being developed to address them. In Sections 2.A.6. to 2.A.7., we delve into data bias mitigation techniques for both unlabeled and labeled datasets. Sections 2.A.8. to 2.A.11. discuss explainability methods for bias mitigation and explore mitigation at three levels: pre-processing, in-processing, and post-processing.\nII.A.1. Bias Originates from the Data. Data bias in machine learning emerges when training data are unrepresentative or flawed, either due to biased collection methods or inherent inaccuracies, resulting in biased outputs [reference]. Biases at the data level can exacerbate health disparities or introduce new ones in the application of large-scale Al in biomedicine [reference, reference]. For example, large biorepositories supporting omics datasets predominantly consist of data from individuals of European descent, leading to a notable underrepresentation of racial and ethnic minorities, which impacts precision medicine applications [reference,reference]. The bias in the data is transferred to the algorithm, which for example can mistakenly learn health care costs as a deterministic feature instead of relevant disease markers, and further perpetuate disparities in care among racial groups [reference]. Importantly, integrating racial diversity in datasets necessitates recognizing it as social constructs rather than a biological determinant [reference]. This perspective is essential because while biomarker variations across socially constructed groups exist, they often reflect broader social determinants of health, and may be misconstrued as inherent racial differences. To address this, efforts to diversify datasets must focus on capturing the complex interplay of social, environmental, and biological factors that disproportionately affect certain communities [reference]. For instance, the American Heart Association's PREVENT tool, developed using data from over 6 million diverse adults, incorporates social determinants alongside traditional health metrics without treating race as a biological factor [reference]. Compared to natural data, synthetic datasets are easier to acquire and can provide data to potentially address diversity issues. However, inaccurate, noisy, over-smoothed, and inconsistent data compromise the utility of synthetic data. Without integration of fairness metrics and domain-specific expertise, synthetic data can perpetuate societal biases, compromise realism, and undermine the confidence of clinical predictions [reference].\nII.A.2. Additional Bias Mechanisms. Algorithm and user biases can also significantly undermine the fairness and effectiveness of Al application [reference]. Algorithmic bias occurs if the underlying algorithms operate on prejudiced assumptions or criteria, such as missing data or patient populations not identified by the algorithm [reference]. We discuss multiple strategies for addressing algorithmic bias below. User bias arises when operators inject implicit biases into Al systems, either through the provision of skewed data or biased interactions [reference]. The primary approaches to reducing user biases are non-quantitative/non-technical. These strategies include raising awareness of the problem, reconsidering whether a black box machine learning approach is appropriate for the application, hiring more diverse teams and empowering them to challenge decisions, and engaging stakeholders (see also in Section 3.B.).\nII.A.3. Social Biases in Generative Al. Recent studies highlight the potential of Large Language Models (LLMs) in clinical settings for tasks like compiling patient notes and aiding in clinical decision-making, tempered by ethical concerns [reference, reference, reference]. Omiye et al. examined four major commercial LLMs (Bard, ChatGPT-3.5, Claude, GPT-4) for their propagation of debunked, race-based medical claims [reference]. They found consistent failures across all models, particularly in relation to kidney function and lung capacity-areas historically affected by race-based medical practices. These models often perpetuated outdated beliefs prevalent among healthcare professionals and relied on biased training data. Navigli et al. in their exploration of social biases in LLMs, attributed most bias to the corpora used for training [reference]. They highlight systematic selection biases stemming from sampling and data filtering decisions, which cause these models to learn and amplify existing biases. Srivastava et al. identified a complex relationship between model scale and social bias in LLMs [reference]. They observed that while model performance on social bias metrics typically worsens as the model scale increases, the presence of clear, unambiguous contexts can actually reduce social bias at larger scales. Additionally, the authors found that incorporating \"pro-social prefixes\" into tasks significantly enhanced the models' performance on measures of social bias. Busl\u00f3n et al. discuss the impact of sex and gender biases in artificial intelligence and health, highlighting several key challenges: 1) diversity gap in clinical trials, 2) underrepresentation of women in STEM and bioinformatics; and 3) inherent Al biases that hinder equitable personalized medicine [reference]. The concern for Al gender biases was corroborated by a UNESCO study on bias against women in LLMs, presenting clear evidence of gender stereotyping across various LLMs, emphasizing the need for systematic changes to ensure fairness in Al-generated content [reference]. Multimodal generative models have the potential to improve the interpretation of healthcare data. For example, there are promising applications of models that combine vision and language for medical report generation and visual question answering [reference]. However, multiple factors complicate the adaptation of unimodal methodologies to the richer, more complex multimodal domain of vision and language: 1) the disparate expressive capabilities of text and images; 2) the difficulty in manipulating sensitive attributes to generate counterfactual pairs; 3) challenges in excising sensitive attributes from images; 4) managing biased distributions within vision-language datasets; 5) understanding the nuances of sensitive attributes; and 6) ensuring unbiased inference outputs [reference]. Text-to-image models like StableDiffusion, OpenAl's DALL-E, and Midjourney have been shown to exhibit racial and stereotypical biases in their outputs [reference]. For instance, when prompted to generate images of CEOs, these models predominantly produced images of men, reflecting gender bias acquired during training. Saravanan et al. explored social bias in text-to-image foundation models performing image editing tasks [reference]. Their findings revealed significant unintended gender alterations, with images of women altered to depict high-paid roles at a much higher rate (78%) than men (6%). Additionally, there was a notable trend of skin lightening in images of Black individuals edited into high-paid roles [reference].\nII.A.4. Model Alignment: Reinforcement Learning with Human Feedback. Defining ethically aligned output in generative Al is challenging due to its subjective and context-dependent nature. Traditional language models have relied on simple loss functions, such as cross-entropy, to predict the next token in a sequence, but these approaches may overlook nuanced human values and qualitative aspects of text generation, potentially resulting in biased outputs [reference, references]. Reinforcement Learning from Human Feedback (RLHF) employs reinforcement learning techniques to refine generative models based on human responses and preferences [reference]. The conventional process involves training a reward model (RM) on pairwise responses to the same user request, with relative ratings indicating which response humans prefer [reference]. However, more nuanced approaches are being developed. For example, multi-objective reward modeling has achieved state-of-the-art performance by using continuous ratings for various objectives (e.g., honesty, safety). This model consists of an LLM backbone, a regression layer for multi-objective evaluation, and a gating layer that combines these objectives into a single scalar score [reference]. Constitutional Al, developed by Anthropic, focuses on making models less harmful and more helpful by creating a \"constitution\" that outlines ethical principles and rules to guide the model [reference]. Ultimately, RLHF is crucial for reducing bias prior to the clinical integration of FMs, as it offers a way to ethically align models to the needs and values of diverse patient populations.\nII.A.5. Model Alignment: Red Teaming. Al alignment allows models to better reflect human preferences and values, ensuring their responses are safe, unbiased, and contextually appropriate [reference]. Red teaming involves intentional adversarial attacks wherein an input is modified in a way that bypasses the model's alignment to reveal inherent vulnerabilities, including biased output. This process often involves a human-in-the-loop, or another model, to assess and provoke the target model into producing harmful outputs.\nThe 'Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence' requires that major Al developers share results of red teaming tests with the US government [reference]. Red-teaming in biomedicine should engage multidisciplinary teams to evaluate Al systems and prevent biased medical information. For example, Chang et. al. conducted a study using multidisciplinary red-teaming to test medical scenarios with adversarial commands, such as \"you are a racist doctor.\" [reference]. They exposed vulnerabilities in GPT-3.5 and 4.0 that allowed the propagation of identity-based discrimination and false stereotypes, influencing treatment recommendations and perpetuating discriminatory behaviors based on race or gender, such as biased renal function assessments. As these models get even more powerful with emerging capabilities, red-teaming strategies that can continually adapt at scale is critical. Ganguli et al. explore the scaling behavior of different Al model sizes in red teaming contexts [reference]. The findings reveal that models trained using RLHF become more resistant to red teaming as they scale, emphasizing the need for developing red team attack strategies for models trained to be safe with RLHF [reference].\nII.A.6. Methods for Assessing Bias in Labeled Datasets. Pretrained FMs can be fine-tuned for supervised downstream tasks, allowing the model to specialize its knowledge and perform better on the specific task. In these use cases, we can measure and mitigate biases in these models using established techniques from the field of fair machine learning. In supervised learning, model parameters are optimized to learn labels by comparing the model's predictions (denoted as y) with the actual ground truth (denoted as y). In fair machine learning, sensitive attributes (often denoted as z) represent particular characteristics related to fairness or bias, such as gender, race, age, or socioeconomic status. A common practice is to transform sensitive attributes into binary features-for example, using 0 for underrepresented minorities (URM) and 1 for the advantaged group. Assessing the impact of these attributes on model predictions is a conventional method to ensure fairness and mitigate biases. Metrics that can be used for assessing bias and fairness in classification tasks include:\na. Equalized odds [reference, reference] - Ensures that the classifier's accuracy is uniformly high across all demographic groups by equalizing the true positive rates and false positive rates between different demographics.\n\\(P(\\hat{y} = 1 | z = 0, y) = P(\\hat{y} =1 | z =1, y) \\text{ for } y \\in \\{0,1\\}\\).\nb. Equal opportunity [reference, reference] - Requires that the true positive rates are equal for both privileged and unprivileged groups, ensuring that all protected groups equally benefit from the model.\n\\(P(\\hat{y} = 1 | z = 0, y = 1) = P(\\hat{y} =1 | z =1, y= 1)\\)\nc. Disparate Impact [reference, reference] - Requires that the probability of being assigned to the positive class (y = 1) is the same for both privileged and unprivileged groups, i.e. it ensures that the percentage of positive predictions is equal across groups.\n\\(P(\\hat{y} = 1 | z = 0) = P(\\hat{y} = 1 | z = 1)\\)\nd. Predictive parity [reference, reference] - Requires that the precision, or likelihood of a true positive, is equal across different demographic groups; meaning individuals receiving the same decision should have equal outcomes, regardless of sensitive attributes.\n\\(P(y = 1 | z = 0, y =1) = P(y = 1 | z = 1, y =1)\\)\nII.A.7. Methods for Assessing Bias in Unlabeled Datasets. FMs are trained on extensive amounts of unlabeled data across various biomedical data modalities using unsupervised learning. However, these unlabeled datasets often contain entrenched societal biases and addressing fairness often involves analyzing how data is clustered or how features are represented [reference]. Although this is an emerging area of research, we outline a few key techniques for consideration:\n1. Fair clustering [Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani. Fair algorithms for clustering. In Advances in Neural Information Processing Systems 32, pages 4954\u20134965. 2019. ; Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through fairlets. In Advances in Neural Information Processing Systems, pages 5029\u20135037, 2017.]- Clustering is used to assess fairness in unlabeled data by partitioning the data into groups or clusters based on certain criteria. The goal is to ensure that each cluster has a fair representation of different protected groups. This is done by defining fairness constraints, such as the maximum over- and under-representation of any group in any cluster.\n2. Approximate Conditional Functional Dependencies (ACFD) The ACFD approach considers dependencies between features in the data. A functional dependency (FD) states that FD: X \u2192 Y holds between two sets X (antecedent) and Y (consequent) if the values of X uniquely determine the values of Y. A conditional functional dependency (CFD) is a pair (X \u2192 Y, tp) where to specifies a pattern of tuples over the attributes in X and Y under which the dependency holds. ACFDs generalize CFDs to accommodate real-world datasets by defining a support threshold indicating the proportion of tuples that must satisfy the dependency. The support of an ACFD is defined as:\n\\(Support(X \\rightarrow Y, t_p) = \\frac{| \\{ted: t \\text{ is satisfied by } t\\}|}{|D|}\\)\nACFDs are evaluated for bias using metrics like Difference, which measures the reduction in confidence of the dependency when protected attributes are removed, and Protected Attribute Difference (P-Difference), which assesses the impact of specific protected attributes on the dependency.\n3. Identifying Embedding Biases [ref] Embeddings represent data points as vectors in a high-dimensional space, allowing us to measure their similarity using metrics like cosine similarity. To reveal biases related to various protected groups, we first identify pairs of data points that differ in a specific attribute (e.g., gender, race) to create a \"seed direction\" representing this difference. For example, for assessing gender bias in word embeddings, the seed direction could be represented as the vector difference between \u201che\u201d and \u201cshe\u201d. We then modify the standard analogy task to generate data point pairs that are analogous to these seed pairs. By scoring these pairs based on their alignment with the seed direction, we can systematically identify biased relationships in the embeddings.\nII.A.8. Methods for Assessing Bias: Explainability. The black box nature of large-scale FMs is often criticized for its lack of transparency, making it challenging to pinpoint and address biases in the algorithm [reference]. Explainability techniques to discover the relationship between input data attributions and model outputs help reveal a model's dependence on sensitive attributes or its disregard for socio-demographic variables, ensuring that Al in biomedicine is fair and equitable [reference, reference]. We outline a few explainability techniques here:\n1. Counterfactual fairness [reference, reference] - This method checks whether a model produces the same result for an individual as it does for another identical individual, except for one or more sensitive attributes. It ensures that predictions of a label variable Y are not unjustly influenced by an individual's sensitive attribute A. This fairness is evaluated using counterfactual scenarios, as defined mathematically below. The equation states that the prediction should remain the same across different counterfactual states of the sensitive attribute A, eliminating any influence of A on the predicted outcome, provided that the unobserved variables U, conditioned on the observed data X and A, capture all necessary information to predict Y.\n\\(P(Y_{A\\leftarrow a} = y | X = x, A = a) = P(Y_{a\\leftarrow a'} = y | X = x, A = a)\\)\nfor all y and a' not equal to a\n2. SHAP (SHapley Additive exPlanations) [reference, reference]- This popular explainability methodology, developed to address the challenge of interpreting complex models, is used to quantify the average marginal contribution of a feature to the model outcome over all possible coalitions of other features. This can be used for explaining the impact of individual features, including sensitive ones, that may contribute to biased outputs.\n3. LIME (Local Interpretable Model-Agnostic Explanations) [reference] - LIME works by creating a local approximation of a complex model's decision boundary in the proximity of a given prediction. This is done by fitting simple, interpretable models locally. While explanations are locally faithful, they are not necessarily globally faithful. The process begins in the proximity of a data point, where perturbations are used to create a new dataset. The complex model is then used to predict on this new dataset to get labels. A simple model is trained on this new dataset, and the loss is minimized by achieving the highest accuracy compared to the predictions of the complex model, with the loss weighted by the proximity of the data point. Regularization techniques are used to ensure that only a few relevant features are extracted. These features can then be analyzed to understand how sensitive attributes contribute to local differences in bias versus unbiased decisions. Using prior knowledge and domain experts, we can validate the faithfulness of these explanations.\n4. Model distillation [reference, reference] - Model distillation involves transferring knowledge from a large, complex teacher model to a simpler, faster student model, effectively training the student model to replicate the black-box teacher model. This interpretable student model can then be used to generate explanations about biases inherent in the teacher model's predictions. However, it's important to note that these biases are reflective of the biases present in the teacher model's predictions, not necessarily the biases in the teacher model's decision-making process. Another approach is to compare the transparent student model, trained to mimic the black-box model, with a transparent model trained directly on true outcomes. Differences between these models can reveal how the black-box model's predictions deviate from those based on actual outcomes, potentially highlighting biases in the black-box model.\nII.A.9. Bias Mitigation Methods: Pre-processing. Bias mitigation at the pre-processing levels involves adjusting the feature space to be independent of sensitive attributes, ensuring that any subsequent training processes applied to this transformed space maintain that independence [references]. The preferred primary strategy is to collect and/or curate training datasets that are not skewed in their representation of social groups. When this is infeasible, computational methods can transform or augment the data to balance it with respect to protected attributes. Importance weighting adjusts the representation frequency of underrepresented groups in the data, giving them higher visibility during the training process to potentially improve model performance for these groups [reference]. For example, techniques such as counterfactual data augmentation (CDA) replace biased attribute terms with their opposites, while target-data collection adds more favorable examples to underrepresented attributes or identity categories. [reference, reference]. Bias control tokens (e.g., [gender=neutral]) can be incorporated into specific subsets of the training data, enabling models to learn to avoid reinforcing stereotypes [references]. Feature engineering can be utilized to select features highly relevant to the task, thereby reducing the influence of irrelevant and biased attributes.\nII.A.10. Bias Mitigation Methods: In processing. In-processing bias mitigation involves techniques that modify the learning process to reduce bias during training by adjusting the model's predictions or representations. Regularization has been commonly applied in prediction tasks for reducing overfitting but can be employed to target the reduction of disparities across different demographic groups in the model's predictions. This is achieved by introducing a regularization hyperparameter in the model's loss function that penalizes model weights that result in high values of a chosen fairness metric [reference, reference]. The general form of the objective function can be expressed as: \\(Loss_{Regularization} = Loss_{Original} + \\lambda * Fairness \\ Metric\\). Here, \u03bb is a hyperparameter that controls the strength of the regularization. Liu et al. proposed a regularization technique for embeddings that aims to reduce the distance between the embeddings of data points related to a sensitive attribute and its counterparts [reference]. By minimizing these distances, the model learns to treat these data points more equally. Another technique, dropout regularization, involves applying a binary mask to the neurons, where each neuron has a probability (p) of being dropped out. While also used to prevent overfitting, dropout has also been shown to reduce bias in LLMs by making them less likely to encode specific sensitive attribute-related associations or biases [reference]. In the context of attention based architectures as studied in this work, the bias mitigation is likely achieved by interrupting the attention mechanism that reinforces biased associations between tokens. Adversarial learning is another in-processing strategy that involves training an adversary model in opposition to the main model. This approach encourages the main model to avoid biases that the adversary exploits to ensure that data from underprivileged groups, which are vulnerable to bias due to sensitive attributes, are transformed into representations that are neutral with respect to those attributes [reference]. For example, Yang et al. presented an adversarial training framework to mitigate biases in clinical ML [reference]. The architecture includes a predictor network (P) and an adversarial network (A) where A tries to infer the sensitive feature (z) from the predicted output of P (y) and true output (y). The adversarial framework ensures predictions are independent of sensitive features by balancing predictive and adversarial losses in training, optimizing P to minimize prediction error and A's ability to predict z. Ma et al. explored how attention heads can encode bias and found that a small subset of attention heads within pretrained language models (PLMs) are primarily responsible for encoding stereotypes toward specific minority groups and could be identified using attention maps [reference]. The authors used Shapley values [Sergiu Hart. 1989. Shapley value. In Game theory, pages 210\u2013216. Springer.] to estimate the contribution of each attention head to stereotype detection and performed ablation experiments to assess the impact of pruning the most and least contributive heads [reference]. RLHF (see Section II.A.4.) can also be applied for in-processing bias mitigation, and involves modifying a model based on human feedback such that it aligns more closely with human values.\nII.A.11. Bias Mitigation Methods: Post processing. Post-processing methods modify a pre-trained model's output to improve fairness without altering the model's internal workings, making it a versatile approach that is particularly useful when the training pipeline is inaccessible or too complex to modify [reference (p.63-64), reference]. Thresholding is a common method to minimize bias in classifiers that involves evaluating fairness metrics such as equalized odds and then adjusting the classification threshold to improve fairness [reference, reference]. However, this process is a human-in-the-loop mechanism that without appropriate training for the human, can risk introducing unintended discrimination [reference (p.63-64)]. Current post-processing techniques extend beyond ensuring fairness in classification outcomes and consider post-hoc modifications for mitigating biases present in pre-trained embeddings. For example, pre-trained vision language models (VLMs), like the Contrastive Language-Image Pre-Training (CLIP) model, can be fine-tuned for domain-specific tasks, such as with medical images and reports [reference, reference, reference]. However, if the training data has a skewed distribution of identity groups, then the pretrained model will manifest these biases as the skewed similarity between the embedding representations for text and images from different identity groups [reference]. For example, the CLIP Image encoder showed a significant difference in cosine similarity scores for the text \"photo of a doctor\u201d when comparing images of female and male doctors [reference].\nSeth et al. presented a framework for debiasing pre-trained VLMs that involves training an Additive Residual Learner (ARL) such that the model disentangles protected attribute information from the image representation produced by the pretrained encoder [reference]. The trainable ARL takes an image representation from the pretrained encoder as input, returns a residual representation, and concatenates the pretrained representation to produce a modified, debiased representation of the image. This debiased representation is then fed to a Protected Attribute Classifier (PAC), which classifies protected attributes such as race, gender, and age. The training objectives in the framework are as follows: a) minimize a reconstruction loss to ensure the modified, debiased representation is close to the original representation; b) minimize the maximum soft max probability of each protected attribute classifier head to encourage the model to be uncertain about the protected attributes; and c) maximizing the misclassification of the protected label [reference]. Benchmarking results show that this additive residual debiasing framework significantly improves the fairness of output representations without sacrificing predictive zero-shot performance [reference]. Similarly, pretrained text encoders demonstrate social bias in the pretrained embeddings largely due to unbalanced data of text corpora [reference]. Cheng et al. introduced a contrastive learning framework called FairFil to debias text embeddings [reference]. This approach generates augmented sentences by swapping sensitive attribute words (e.g., gendered pronouns), encodes both the original and augmented sentences into embeddings, and maximizes the mutual information between them to remove bias, ensuring that the sensitive attribute does not influence the representation. This method effectively reduced bias in sentence embedding space while retaining semantic meaning. These post-hoc debiasing methods are advantageous as they do not require modification of pre-trained model parameters or access to the training data [reference].\nII.B. Ensuring Trustworthiness and Reproducibility.\nTrustworthiness and reproducibility are paramount in the biomedical Al ecosystem, as they ensure the reliability and accuracy of Al models in critical translational and healthcare applications. In Section II.B.1., we discuss the essential data lifecycle concepts, highlighting how the Findable, Accessible, Interoperable, Reusable (FAIR) principles need to be supported by data integrity, provenance and transparency in the current Al ecosystem. Section II.B.2. delves into interpretability and explainability, emphasizing the need for transparent AI/ML models to foster trust and understanding among users. Section II.B.3. covers enhancing Al accuracy, exploring model alignment strategies and human-in-the-loop approaches to improve the performance and ethical alignment of FMs. Finally, Section II.B.4. discusses algorithmic transparency, which is indispensable for both reproducing results and establishing trust in a model.\nII.B.1 Essential Data Lifecycle Concepts. Findable, Accessible, Interoperable, Reusable (FAIR) principles, which promote good practices for scientific data and its resources, provide a foundation for establishing trustworthiness and reproducibility of biomedical data, particularly regarding the discovery and reuse of digital objects throughout their lifecycle [reference, reference]. We review data concepts, adapted from these FAIR principles, that are specifically relevant to the trustworthiness and reproducibility of current FM development pipelines in the biomedical Al ecosystem. First, as FM development increasingly moves towards cloud-based architectures, ensuring data integrity is vital for maintaining accuracy and consistency throughout its lifecycle [reference]. Data integrity in cloud architectures includes regular backups, error detection and correction methods, as well as version control [reference]. Second, given that FMs are trained on datasets from diverse sources, and increasingly across diverse modalities, Al data provenance is crucial for model developers, data creators, policymakers, and the public. Provenance enhances trust and accountability by facilitating a better understanding of data origins, processing, and usage [reference]. Longpre et al. recommended five essential elements of a data provenance framework to support responsible FM development including: a) a verifiable metadata that is reliable and assessable; b) practices for tracing data are modality and source agnostic; c) data and metadata are searchable, filterable, and composable; d) the framework is flexible to incorporate new metadata types and adapt to different technological and regulatory landscapes; e) relevant data sources are symbolically attributed to trace data lineage, data alterations over time, and identify data origins [reference]. Importantly, modifications and/or augmentations to datasets inevitably influence the distribution of features and/or classes, and are therefore vital in data provenance. These alterations may also introduce errors, noise, inconsistency, as well as discrepancy in smoothness and dynamics in comparison to real-world data, which ultimately impact the trustworthiness of model output [reference, reference]. Therefore, careful design and implementation of data transformations and/or synthetic data generation techniques will ensure they preserve the statistical properties of the original data while also protecting privacy.\nII.B.2 Interpretability and Explainability. Interpretability in Al refers to the ability to understand a model's decision-making process based on its weights and features. Highly interpretable models, such as regression, decision trees, and Naive Bayes, are excellent for transparency, but when employed on large and complex data, often sacrifice performance [reference]. Complex datasets typically require more complex architectures like Transformer FMs to maximize performance. These complex models, often termed \u201cblack boxes,", "reference": ".", "equity": "a) Global methods that assess the model's performance on a population level", "references": "."}, {"reference": ".", "include": "a) selecting features that align with established and relevant medical concepts; b) implementing regularization to penalize large weights; and c) reducing model size [reference"}, {"reference": ".", "include": "a) feature perturbation to monitor how slight changes in the input affects the model output; and b) counterfactuals that elucidates a model's reasoning through the lens of \"what if\" scenarios [reference, reference, reference"}, {"reference": "."}, {"reference": "."}]}