{"title": "Building an Ethical and Trustworthy Biomedical Al Ecosystem for the Translational and Clinical\nIntegration of Foundational Models", "authors": ["Baradwaj Simha Sankar", "Destiny Gilliland", "Jack Rincon", "Henning Hermjakob", "Yu Yan", "Irsyad Adam", "Gwyneth Lemaster", "Dean Wang", "Karol Watson", "Alex Bui", "Wei Wang", "Peipei Ping"], "abstract": "Foundational Models (FMs) are emerging as the cornerstone of the biomedical Al ecosystem due to their\nability to represent and contextualize multimodal biomedical data. These capabilities allow FMs to be adapted\nfor various tasks, including biomedical reasoning, hypothesis generation, and clinical decision-making. This\nreview paper examines the foundational components of an ethical and trustworthy AI (ETAI) biomedical\necosystem centered on FMs, highlighting key challenges and solutions. The ETAI biomedical ecosystem is\ndefined by seven key components which collectively integrate FMs into clinical settings: Data Lifecycle\nManagement, Data Processing, Model Development, Model Evaluation, Clinical Translation, Al Governance\nand Regulation, and Stakeholder Engagement. While the potential of biomedical Al is immense, it requires\nheightened ethical vigilance and responsibility. For instance, biases can arise from data, algorithms, and user\ninteractions, necessitating techniques to assess and mitigate bias prior to, during, and after model\ndevelopment. Moreover, interpretability, explainability, and accountability are key to ensuring the\ntrustworthiness of Al systems, while workflow transparency in training, testing, and evaluation is crucial for\nreproducibility. Safeguarding patient privacy and security involves addressing challenges in data access, cloud\ndata privacy, patient re-identification, membership inference attacks, and data memorization. Additionally, Al\ngovernance and regulation are essential for ethical Al use in biomedicine, guided by global standards.\nFurthermore, stakeholder engagement is essential at every stage of the Al pipeline and lifecycle for clinical\ntranslation. By adhering to these principles, we can harness the transformative potential of Al and develop an\nETAI ecosystem.", "sections": [{"title": "Introduction", "content": "A corollary to the rise of \"Big Data\u201d is the development of large scale machine learning models that have the\ncapacity to learn from large datasets (Reference). Foundational models, developed by Stanford's Institute for\nHuman-Centered Artificial Intelligence, are increasingly recognized as a preferred component in the workflow\nfor large-scale Al development, leveraging millions to billions of parameters through self-supervised or\nsemi-supervised learning techniques (Reference). These techniques allow FMs to learn patterns, structures,\nand context within the data without the need for tedious manual annotation. Ultimately, FMs function as\npre-trained backbones, providing foundational architectures for further adaptation and fine-tuning across\ndiverse tasks, from predictive analytics to generative applications (Reference, Reference). For example,\nwell-known FMs such as BERT (Bidirectional Encoder Representations from Transformers) and GPT\n(Generative Pre-trained Transformer) are pretrained on extensive text corpora to develop a comprehensive\nunderstanding of language, significantly enhancing natural language processing capabilities in tasks such as\ntranslation (Reference).\nThe broad applicability of these models streamlines the process of model development, leading to\ncost-effective and scalable Al solutions across many fields (Reference). Model Hubs (such as Hugging Face's\nModel Hub, PyTorch Hub and TensorFlow Hub) provide a repository of pre-trained models that facilitate their\nintegration into task specific Al workflows. This allows for the efficient development of new Al applications by\nadapting existing architectures, rather than necessitating the training of unique models from scratch. However,\nin the realm of biomedical informatics we have seen the development of domain-specific foundational models\nfrom scratch\u2014such as BioLinkBert, HeartBEiT, and scFoundation (Reference, Reference, Reference). Despite\nthe significant overhead in terms of computational power and data handling, typically involving the\norchestration of hundreds of GPUs to process and learn billions of parameters, the strategic investment is\nwarranted. Custom-built foundational models, engineered from the ground up, are pivotal for leveraging the full\nspectrum of Al's capabilities in biomedicine, resulting in enhanced reasoning and understanding for specialized\ntasks (Reference, Reference).\nBiomedical Al technologies that were initially centered on supervised models showed promising capabilities to\ndiagnose, predict, and recommend treatments across a variety of medical modalities and data types, such as\nelectronic health records (EHRs), chest X-rays, and electrocardiograms (Reference). However, there has been\na notable shift towards employing FMs in biomedical Al, initiating an era where their full potential can be\nleveraged to cultivate groundbreaking clinician-Al interactions that significantly enhance medical practice\n(Reference). For instance, clinical language models are able to encode clinical and biomedical language\n[Reference, Reference]. Similarly, FMs trained on electronic medical records (EMRs) comprehensively encode\nthe timelines of patients' disease course [Reference]. FMs developed using single-cell and histopathology\nimages from patient tissues, when correlated with clinical outcomes, facilitate the identification of biomarker\nsignatures and functional states in both health and disease [Reference, Reference, Reference]. Biomedical\nFMs, with the ability to interpret various biomedical data modalities - encompassing multi-omics, (EMRs),\nradiology, histology, medical knowledge bases, and more - are envisioned to produce sophisticated biomedical\ninsights that enhance clinical decision-making, enable precision medicine, and enhance public health\n(Reference, Reference).\nThe emergence of big multimodal data and foundational models shapes our perspective of the evolving Al\necosystem in biomedicine. While the emergence of a biomedical Al ecosystem holds remarkable potential for\nbreakthroughs in disease understanding and treatment, it also necessitates a significant increase in our ethical\nvigilance and responsibility. It is critical that we adopt an ethically governed, co-designed approach that is built\non evidence-based principles and prioritizes the needs of individuals and communities impacted by biomedical\nAl. This approach is essential to ensure that Al innovations contribute positively to healthcare and public\nhealth, reinforcing rather than compromising the necessary safeguards and ethical standards.\nThe main contributions of this paper are as follows:\n1. We outline the essential components of an Al Ecosystem for integrating foundational models (FMs) into\nclinical and/or translational settings.\n2. We examine the current landscape of ethical considerations and ethical practices specific to the\ndevelopment and implementation of FMs, highlighting critical challenges and existing mitigation\nstrategies across three key areas:\nA. Mitigating Bias and Enhancing Fairness\nB. Ensuring Trustworthiness and Reproducibility\nC. Safeguarding Patient Privacy and Security\n3. We examine pivotal government and scholarly publications that chart the present guidelines and future\ndirections for Al stewardship, emphasizing two main components:\nA. Al Governance and Regulation\nB. Stakeholder Engagement\n4. Finally, we discuss a unified perspective of how the principles of ethical and trustworthy Al &\nstewardship in Al integrate into the ecosystem."}, {"title": "II. Al Ecosystem in Biomedicine.", "content": "Al ecosystem is a concept that defines the complex interdependent patterns that connect developers,\nmanufacturers, and users of Al. It provides a structure to develop an ethical and regulatory framework that\npromotes fairness, transparency, and accountability in the development and use of AI/ML systems (Reference).\nThe Al ecosystem can best be defined via 7 key components and is presented in Figure 1 (Reference,\nReference, Reference)."}, {"title": "II. Ethical Considerations in the Al Pipeline for Foundational Models.", "content": "II.A. Mitigating Bias and Enhancing Fairness.\nBiases can arise at any stage of Al pipelines and can perpetuate throughout the system. Foundational models\n(FMs) require the ingestion of massive amounts of data to train billions of parameters in order to perform\neffectively. This scale and complexity introduces significant challenges in minimizing undesirable biases. As a\nresult, biases can perpetuate outdated claims, lead to inaccurate insights, compromise the quality of care for\nmarginalized groups, and/or exacerbate disparities. In this section, we explore the continuum of bias induction\ninto Al, and explore metrics and mitigation strategies to guard against their detrimental effects. In Sections\n2.A.1. to 2.A.2., we examine the sources of bias from data, algorithms, and users. Sections 2.A.3. to 2.A.5.\nfocus on social biases identified in generative Al and the alignment methods being developed to address them.\nIn Sections 2.A.6. to 2.A.7., we delve into data bias mitigation techniques for both unlabeled and labeled\ndatasets. Sections 2.A.8. to 2.A.11. discuss explainability methods for bias mitigation and explore mitigation at\nthree levels: pre-processing, in-processing, and post-processing.\nII.A.1. Bias Originates from the Data. Data bias in machine learning emerges when training data are\nunrepresentative or flawed, either due to biased collection methods or inherent inaccuracies, resulting in\nbiased outputs [reference]. Biases at the data level can exacerbate health disparities or introduce new ones in\nthe application of large-scale Al in biomedicine [reference, reference]. For example, large biorepositories\nsupporting omics datasets predominantly consist of data from individuals of European descent, leading to a\nnotable underrepresentation of racial and ethnic minorities, which impacts precision medicine applications\n[reference,reference]. The bias in the data is transferred to the algorithm, which for example can mistakenly\nlearn health care costs as a deterministic feature instead of relevant disease markers, and further perpetuate\ndisparities in care among racial groups [reference]. Importantly, integrating racial diversity in datasets\nnecessitates recognizing it as social constructs rather than a biological determinant [reference]. This\nperspective is essential because while biomarker variations across socially constructed groups exist, they often\nreflect broader social determinants of health, and may be misconstrued as inherent racial differences. To\naddress this, efforts to diversify datasets must focus on capturing the complex interplay of social,\nenvironmental, and biological factors that disproportionately affect certain communities [reference]. For\ninstance, the American Heart Association's PREVENT tool, developed using data from over 6 million diverse\nadults, incorporates social determinants alongside traditional health metrics without treating race as a\nbiological factor [reference]. Compared to natural data, synthetic datasets are easier to acquire and can\nprovide data to potentially address diversity issues. However, inaccurate, noisy, over-smoothed, and\ninconsistent data compromise the utility of synthetic data. Without integration of fairness metrics and\ndomain-specific expertise, synthetic data can perpetuate societal biases, compromise realism, and undermine\nthe confidence of clinical predictions [reference].\nII.A.2. Additional Bias Mechanisms. Algorithm and user biases can also significantly undermine the fairness\nand effectiveness of Al application [reference]. Algorithmic bias occurs if the underlying algorithms operate on\nprejudiced assumptions or criteria, such as missing data or patient populations not identified by the algorithm\nreference]. We discuss multiple strategies for addressing algorithmic bias below. User bias arises when\noperators inject implicit biases into Al systems, either through the provision of skewed data or biased\ninteractions [reference]. The primary approaches to reducing user biases are non-quantitative/non-technical.\nThese strategies include raising awareness of the problem, reconsidering whether a black box machine\nlearning approach is appropriate for the application, hiring more diverse teams and empowering them to\nchallenge decisions, and engaging stakeholders (see also in Section 3.B.).\nII.A.3. Social Biases in Generative Al. Recent studies highlight the potential of Large Language Models (LLMs)\nin clinical settings for tasks like compiling patient notes and aiding in clinical decision-making, tempered by\nethical concerns [reference, reference, reference]. Omiye et al. examined four major commercial LLMs (Bard,\nChatGPT-3.5, Claude, GPT-4) for their propagation of debunked, race-based medical claims [reference]. They\nfound consistent failures across all models, particularly in relation to kidney function and lung capacity-areas\nhistorically affected by race-based medical practices. These models often perpetuated outdated beliefs\nprevalent among healthcare professionals and relied on biased training data. Navigli et al. in their exploration\nof social biases in LLMs, attributed most bias to the corpora used for training [reference]. They highlight\nsystematic selection biases stemming from sampling and data filtering decisions, which cause these models to\nlearn and amplify existing biases. Srivastava et al. identified a complex relationship between model scale and\nsocial bias in LLMs [reference]. They observed that while model performance on social bias metrics typically\nworsens as the model scale increases, the presence of clear, unambiguous contexts can actually reduce social\nbias at larger scales. Additionally, the authors found that incorporating \"pro-social prefixes\" into tasks\nsignificantly enhanced the models' performance on measures of social bias. Busl\u00f3n et al. discuss the impact of\nsex and gender biases in artificial intelligence and health, highlighting several key challenges: 1) diversity gap\nin clinical trials, 2) underrepresentation of women in STEM and bioinformatics; and 3) inherent Al biases that\nhinder equitable personalized medicine [reference]. The concern for Al gender biases was corroborated by a\nUNESCO study on bias against women in LLMs, presenting clear evidence of gender stereotyping across\nvarious LLMs, emphasizing the need for systematic changes to ensure fairness in Al-generated content\n[reference]. Multimodal generative models have the potential to improve the interpretation of healthcare data.\nFor example, there are promising applications of models that combine vision and language for medical report\ngeneration and visual question answering [reference]. However, multiple factors complicate the adaptation of\nunimodal methodologies to the richer, more complex multimodal domain of vision and language: 1) the\ndisparate expressive capabilities of text and images; 2) the difficulty in manipulating sensitive attributes to\ngenerate counterfactual pairs; 3) challenges in excising sensitive attributes from images; 4) managing biased\ndistributions within vision-language datasets; 5) understanding the nuances of sensitive attributes; and 6)\nensuring unbiased inference outputs [reference]. Text-to-image models like StableDiffusion, OpenAl's DALL-E,\nand Midjourney have been shown to exhibit racial and stereotypical biases in their outputs [reference]. For\ninstance, when prompted to generate images of CEOs, these models predominantly produced images of men,\nreflecting gender bias acquired during training. Saravanan et al. explored social bias in text-to-image\nfoundation models performing image editing tasks [reference]. Their findings revealed significant unintended\ngender alterations, with images of women altered to depict high-paid roles at a much higher rate (78%) than\nmen (6%). Additionally, there was a notable trend of skin lightening in images of Black individuals edited into\nhigh-paid roles [reference].\nII.A.4. Model Alignment: Reinforcement Learning with Human Feedback. Defining ethically aligned output in\ngenerative Al is challenging due to its subjective and context-dependent nature. Traditional language models\nhave relied on simple loss functions, such as cross-entropy, to predict the next token in a sequence, but these\napproaches may overlook nuanced human values and qualitative aspects of text generation, potentially\nresulting in biased outputs [reference, references]. Reinforcement Learning from Human Feedback (RLHF)\nemploys reinforcement learning techniques to refine generative models based on human responses and\npreferences [reference]. The conventional process involves training a reward model (RM) on pairwise\nresponses to the same user request, with relative ratings indicating which response humans prefer [reference].\nHowever, more nuanced approaches are being developed. For example, multi-objective reward modeling has\nachieved state-of-the-art performance by using continuous ratings for various objectives (e.g., honesty, safety).\nThis model consists of an LLM backbone, a regression layer for multi-objective evaluation, and a gating layer\nthat combines these objectives into a single scalar score [reference]. Constitutional Al, developed by\nAnthropic, focuses on making models less harmful and more helpful by creating a \"constitution\" that outlines\nethical principles and rules to guide the model [reference]. Ultimately, RLHF is crucial for reducing bias prior to\nthe clinical integration of FMs, as it offers a way to ethically align models to the needs and values of diverse\npatient populations.\nII.A.5. Model Alignment: Red Teaming. Al alignment allows models to better reflect human preferences and\nvalues, ensuring their responses are safe, unbiased, and contextually appropriate [reference]. Red teaming\ninvolves intentional adversarial attacks wherein an input is modified in a way that bypasses the model's\nalignment to reveal inherent vulnerabilities, including biased output. This process often involves a\nhuman-in-the-loop, or another model, to assess and provoke the target model into producing harmful outputs.\nThe 'Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence'\nrequires that major Al developers share results of red teaming tests with the US government [reference].\nRed-teaming in biomedicine should engage multidisciplinary teams to evaluate Al systems and prevent biased\nmedical information. For example, Chang et. al. conducted a study using multidisciplinary red-teaming to test\nmedical scenarios with adversarial commands, such as \"you are a racist doctor.\" [reference]. They exposed\nvulnerabilities in GPT-3.5 and 4.0 that allowed the propagation of identity-based discrimination and false\nstereotypes, influencing treatment recommendations and perpetuating discriminatory behaviors based on race\nor gender, such as biased renal function assessments. As these models get even more powerful with emerging\ncapabilities, red-teaming strategies that can continually adapt at scale is critical. Ganguli et al. explore the\nscaling behavior of different Al model sizes in red teaming contexts [reference]. The findings reveal that models\ntrained using RLHF become more resistant to red teaming as they scale, emphasizing the need for developing\nred team attack strategies for models trained to be safe with RLHF [reference].\nII.A.6. Methods for Assessing Bias in Labeled Datasets. Pretrained FMs can be fine-tuned for supervised\ndownstream tasks, allowing the model to specialize its knowledge and perform better on the specific task. In\nthese use cases, we can measure and mitigate biases in these models using established techniques from the\nfield of fair machine learning. In supervised learning, model parameters are optimized to learn labels by\ncomparing the model's predictions (denoted as y) with the actual ground truth (denoted as y). In fair machine\nlearning, sensitive attributes (often denoted as z) represent particular characteristics related to fairness or bias,\nsuch as gender, race, age, or socioeconomic status. A common practice is to transform sensitive attributes into\nbinary features-for example, using 0 for underrepresented minorities (URM) and 1 for the advantaged group.\nAssessing the impact of these attributes on model predictions is a conventional method to ensure fairness and\nmitigate biases. Metrics that can be used for assessing bias and fairness in classification tasks include:\na. Equalized odds [reference, reference] - Ensures that the classifier's accuracy is uniformly high across\nall demographic groups by equalizing the true positive rates and false positive rates between different\ndemographics.\n\\(P(\\hat{y} = 1 | z = 0, y) = P(\\hat{y} =1 | z =1, y) \\text{ for } y \\in \\{0,1\\}\\).\nb. Equal opportunity [reference, reference] - Requires that the true positive rates are equal for both\nprivileged and unprivileged groups, ensuring that all protected groups equally benefit from the model.\n\\(P(\\hat{y} = 1 | z = 0, y = 1) = P(\\hat{y} =1 | z =1, y= 1)\\)\nc. Disparate Impact [reference, reference] - Requires that the probability of being assigned to the positive\nclass (y = 1) is the same for both privileged and unprivileged groups, i.e. it ensures that the percentage\nof positive predictions is equal across groups.\n\\(P(\\hat{y} = 1 | z = 0) = P(\\hat{y} = 1 | z = 1)\\)\nd. Predictive parity [reference, reference] - Requires that the precision, or likelihood of a true positive, is\nequal across different demographic groups; meaning individuals receiving the same decision should\nhave equal outcomes, regardless of sensitive attributes.\n\\(P(y = 1 | z = 0, \\hat{y} =1) = P(y = 1 | z = 1, \\hat{y} =1)\\)\nII.A.7. Methods for Assessing Bias in Unlabeled Datasets. FMs are trained on extensive amounts of unlabeled\ndata across various biomedical data modalities using unsupervised learning. However, these unlabeled\ndatasets often contain entrenched societal biases and addressing fairness often involves analyzing how data is\nclustered or how features are represented [reference]. Although this is an emerging area of research, we\noutline a few key techniques for consideration:\n1. Fair clustering [Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani. Fair\nalgorithms for clustering. In Advances in Neural Information Processing Systems 32, pages 4954\u20134965.\n2019. ; Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through\nfairlets. In Advances in Neural Information Processing Systems, pages 5029\u20135037, 2017.]- Clustering\nis used to assess fairness in unlabeled data by partitioning the data into groups or clusters based on\ncertain criteria. The goal is to ensure that each cluster has a fair representation of different protected\ngroups. This is done by defining fairness constraints, such as the maximum over- and\nunder-representation of any group in any cluster.\n2. Approximate Conditional Functional Dependencies (ACFD) The ACFD approach considers\ndependencies between features in the data. A functional dependency (FD) states that FD: X \u2192 Y holds\nbetween two sets X (antecedent) and Y (consequent) if the values of X uniquely determine the values\nof Y. A conditional functional dependency (CFD) is a pair (X \u2192 Y, tp) where to specifies a pattern of\ntuples over the attributes in X and Y under which the dependency holds. ACFDs generalize CFDs to\naccommodate real-world datasets by defining a support threshold indicating the proportion of tuples\nthat must satisfy the dependency. The support of an ACFD is defined as:\n\\(Support(X \\rightarrow Y, tp) = \\frac{\\text{ted: t is satisfied by t|}}{|D|}\\)\nACFDs are evaluated for bias using metrics like Difference, which measures the reduction in\nconfidence of the dependency when protected attributes are removed, and Protected Attribute\nDifference (P-Difference), which assesses the impact of specific protected attributes on the\ndependency.\n3. Identifying Embedding Biases [ref]\nEmbeddings represent data points as vectors in a\nhigh-dimensional space, allowing us to measure their similarity using metrics like cosine similarity. To\nreveal biases related to various protected groups, we first identify pairs of data points that differ in a\nspecific attribute (e.g., gender, race) to create a \"seed direction\" representing this difference. For\nexample, for assessing gender bias in word embeddings, the seed direction could be represented as\nthe vector difference between \u201che\u201d and \u201cshe\u201d. We then modify the standard analogy task to generate\ndata point pairs that are analogous to these seed pairs. By scoring these pairs based on their alignment\nwith the seed direction, we can systematically identify biased relationships in the embeddings.\nII.A.8. Methods for Assessing Bias: Explainability. The black box nature of large-scale FMs is often criticized\nfor its lack of transparency, making it challenging to pinpoint and address biases in the algorithm [reference].\nExplainability techniques to discover the relationship between input data attributions and model outputs help\nreveal a model's dependence on sensitive attributes or its disregard for socio-demographic variables, ensuring\nthat Al in biomedicine is fair and equitable [reference, reference]. We outline a few explainability techniques\nhere:\n1. Counterfactual fairness [reference, reference] - This method checks whether a model produces the\nsame result for an individual as it does for another identical individual, except for one or more sensitive\nattributes. It ensures that predictions of a label variable Y are not unjustly influenced by an individual's\nsensitive attribute A. This fairness is evaluated using counterfactual scenarios, as defined\nmathematically below. The equation states that the prediction should remain the same across different\ncounterfactual states of the sensitive attribute A, eliminating any influence of A on the predicted\noutcome, provided that the unobserved variables U, conditioned on the observed data X and A, capture\nall necessary information to predict Y.\n\\(P(Y_{A\\leftarrow a} = y | X = x, A = a) = P(Y_{a\\leftarrow a'} = y | X = x, A = a)\\)\nfor all y and a' not equal to a\n2. SHAP (SHapley Additive exPlanations) [reference, reference]- This popular explainability methodology,\ndeveloped to address the challenge of interpreting complex models, is used to quantify the average\nmarginal contribution of a feature to the model outcome over all possible coalitions of other features.\nThis can be used for explaining the impact of individual features, including sensitive ones, that may\ncontribute to biased outputs.\n3. LIME (Local Interpretable Model-Agnostic Explanations) [reference] - LIME works by creating a local\napproximation of a complex model's decision boundary in the proximity of a given prediction. This is\ndone by fitting simple, interpretable models locally. While explanations are locally faithful, they are not\nnecessarily globally faithful. The process begins in the proximity of a data point, where perturbations\nare used to create a new dataset. The complex model is then used to predict on this new dataset to get\nlabels. A simple model is trained on this new dataset, and the loss is minimized by achieving the\nhighest accuracy compared to the predictions of the complex model, with the loss weighted by the\nproximity of the data point. Regularization techniques are used to ensure that only a few relevant\nfeatures are extracted. These features can then be analyzed to understand how sensitive attributes\ncontribute to local differences in bias versus unbiased decisions. Using prior knowledge and domain\nexperts, we can validate the faithfulness of these explanations.\n4. Model distillation [reference, reference] - Model distillation involves transferring knowledge from a large,\ncomplex teacher model to a simpler, faster student model, effectively training the student model to\nreplicate the black-box teacher model. This interpretable student model can then be used to generate\nexplanations about biases inherent in the teacher model's predictions. However, it's important to note\nthat these biases are reflective of the biases present in the teacher model's predictions, not necessarily\nthe biases in the teacher model's decision-making process. Another approach is to compare the\ntransparent student model, trained to mimic the black-box model, with a transparent model trained\ndirectly on true outcomes. Differences between these models can reveal how the black-box model's\npredictions deviate from those based on actual outcomes, potentially highlighting biases in the\nblack-box model.\nII.A.9. Bias Mitigation Methods: Pre-processing. Bias mitigation at the pre-processing levels involves adjusting\nthe feature space to be independent of sensitive attributes, ensuring that any subsequent training processes\napplied to this transformed space maintain that independence [references]. The preferred primary strategy is to\ncollect and/or curate training datasets that are not skewed in their representation of social groups. When this is\ninfeasible, computational methods can transform or augment the data to balance it with respect to protected\nattributes. Importance weighting adjusts the representation frequency of underrepresented groups in the data,\ngiving them higher visibility during the training process to potentially improve model performance for these\ngroups [reference]. For example, techniques such as counterfactual data augmentation (CDA) replace biased\nattribute terms with their opposites, while target-data collection adds more favorable examples to\nunderrepresented attributes or identity categories. [reference, reference]. Bias control tokens (e.g.,\n[gender=neutral]) can be incorporated into specific subsets of the training data, enabling models to learn to\navoid reinforcing stereotypes [references]. Feature engineering can be utilized to select features highly\nrelevant to the task, thereby reducing the influence of irrelevant and biased attributes.\nII.A.10. Bias Mitigation Methods: In processing. In-processing bias mitigation involves techniques that modify\nthe learning process to reduce bias during training by adjusting the model's predictions or representations.\nRegularization has been commonly applied in prediction tasks for reducing overfitting but can be employed to\ntarget the reduction of disparities across different demographic groups in the model's predictions. This is\nachieved by introducing a regularization hyperparameter in the model's loss function that penalizes model\nweights that result in high values of a chosen fairness metric [reference, reference]. The general form of the\nobjective function can be expressed as: \\(Loss_{Regularization} = Loss_{Original} + \\lambda * Fairness \\text{Metric}\\). Here, \u03bb is a\nhyperparameter that controls the strength of the regularization. Liu et al. proposed a regularization technique\nfor embeddings that aims to reduce the distance between the embeddings of data points related to a sensitive\nattribute and its counterparts [reference]. By minimizing these distances, the model learns to treat these data\npoints more equally. Another technique, dropout regularization, involves applying a binary mask to the neurons,\nwhere each neuron has a probability (p) of being dropped out. While also used to prevent overfitting, dropout\nhas also been shown to reduce bias in LLMs by making them less likely to encode specific sensitive\nattribute-related associations or biases [reference]. In the context of attention based architectures as studied in\nthis work, the bias mitigation is likely achieved by interrupting the attention mechanism that reinforces biased\nassociations between tokens. Adversarial learning is another in-processing strategy that involves training an\nadversary model in opposition to the main model. This approach encourages the main model to avoid biases\nthat the adversary exploits to ensure that data from underprivileged groups, which are vulnerable to bias due to\nsensitive attributes, are transformed into representations that are neutral with respect to those attributes\nreference]. For example, Yang et al. presented an adversarial training framework to mitigate biases in clinical\nML [reference]. The architecture includes a predictor network (P) and an adversarial network (A) where A tries\nto infer the sensitive feature (z) from the predicted output of P (y) and true output (y). The adversarial\nframework ensures predictions are independent of sensitive features by balancing predictive and adversarial\nlosses in training, optimizing P to minimize prediction error and A's ability to predict z. Ma et al. explored how\nattention heads can encode bias and found that a small subset of attention heads within pretrained language\nmodels (PLMs) are primarily responsible for encoding stereotypes toward specific minority groups and could\nbe identified using attention maps [reference]. The authors used Shapley values [Sergiu Hart. 1989. Shapley\nvalue. In Game theory, pages 210\u2013216. Springer.] to estimate the contribution of each attention head to\nstereotype detection and performed ablation experiments to assess the impact of pruning the most and least\ncontributive heads [reference]. RLHF (see Section II.A.4.) can also be applied for in-processing bias mitigation,\nand involves modifying a model based on human feedback such that it aligns more closely with human values.\nII.A.11. Bias Mitigation Methods: Post processing. Post-processing methods modify a pre-trained model's\noutput to improve fairness without altering the model's internal workings, making it a versatile approach that is\nparticularly useful when the training pipeline is inaccessible or too complex to modify [reference (p.63-64),\nreference]. Thresholding is a common method to minimize bias in classifiers that involves evaluating fairness\nmetrics such as equalized odds and then adjusting the classification threshold to improve fairness [reference,\nreference]. However, this process is a human-in-the-loop mechanism that without appropriate training for the\nhuman, can risk introducing unintended discrimination [reference (p.63-64)]. Current post-processing\ntechniques extend beyond ensuring fairness in classification outcomes and consider post-hoc modifications for\nmitigating biases present in pre-trained embeddings. For example, pre-trained vision language models (VLMs),\nlike the Contrastive Language-Image Pre-Training (CLIP) model, can be fine-tuned for domain-specific tasks,\nsuch as with medical images and reports [reference, reference, reference]. However, if the training data has a\nskewed distribution of identity groups, then the pretrained model will manifest these biases as the skewed\nsimilarity between the embedding representations for text and images from different identity groups [reference].\nFor example, the CLIP Image encoder showed a significant difference in cosine similarity scores for the text\n\"photo of a doctor\u201d when comparing images of female and male doctors [reference].\nSeth et al. presented a framework for debiasing pre-trained VLMs that involves training an Additive Residual\nLearner (ARL) such that the model disentangles protected attribute information from the image representation\nproduced by the pretrained encoder [reference]. The trainable ARL takes an image representation from the\npretrained encoder as input, returns a residual representation, and concatenates the pretrained representation\nto produce a modified, debiased representation of the image. This debiased representation is then fed to a\nProtected Attribute Classifier (PAC), which classifies protected attributes such as race, gender, and age. The\ntraining objectives in the framework are as follows: a) minimize a reconstruction loss to ensure the modified,\ndebiased representation is close to the original representation; b) minimize the maximum soft max probability\nof each protected attribute classifier head to encourage the model to be uncertain about the protected\nattributes; and c) maximizing the misclassification of the protected label [reference]. Benchmarking results\nshow that this additive residual debiasing framework significantly improves the fairness of output\nrepresentations without sacrificing predictive zero-shot performance [reference]. Similarly, pretrained text\nencoders demonstrate social bias in the pretrained embeddings largely due to unbalanced data of text corpora\nreference]. Cheng et al. introduced a contrastive learning framework called FairFil to debias text embeddings\n[reference]. This approach generates augmented sentences by swapping sensitive attribute words (e.g.,\ngendered pronouns), encodes both the original and augmented sentences into embeddings, and maximizes\nthe mutual information between them to remove bias, ensuring that the sensitive attribute does not influence\nthe representation. This method effectively reduced bias in sentence embedding space while retaining\nsemantic meaning. These post-hoc debiasing methods are advantageous as they do not require modification\nof pre-trained model parameters or access to the training data [reference]."}, {"title": "II.B. Ensuring Trustworthiness and Reproducibility.", "content": "Trustworthiness and reproducibility are paramount in the biomedical Al ecosystem, as they ensure the\nreliability and accuracy of Al models in critical translational and healthcare applications. In Section II.B.1., we\ndiscuss the essential data lifecycle concepts, highlighting how the Findable, Accessible, Interoperable,\nReusable (FAIR) principles need to be supported by data integrity, provenance and transparency in the current\nAl ecosystem. Section II.B.2. delves into interpretability and explainability, emphasizing the need for\ntransparent AI/ML models to foster trust and understanding among users. Section II.B.3. covers enhancing Al\naccuracy, exploring model alignment strategies and human-in-the-loop approaches to improve the performance\nand ethical alignment of FMs. Finally, Section II.B.4. discusses algorithmic transparency, which is\nindispensable for both reproducing results and establishing trust in a model.\nII.B.1 Essential Data Lifecycle Concepts. Findable, Accessible, Interoperable, Reusable (FAIR) principles,\nwhich promote good practices for scientific data and its resources, provide a foundation for establishing\ntrustworthiness and reproducibility of biomedical data, particularly regarding the discovery and reuse of digital\nobjects throughout their lifecycle [reference, reference", "lifecycle\nreference": ".", "reference": "."}]}