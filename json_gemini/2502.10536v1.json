{"title": "POLYPATH: Adapting a Large Multimodal Model for Multi-slide Pathology Report Generation", "authors": ["Faruk Ahmed", "Lin Yang", "Tiam Jaroensri", "Andrew Sellergren", "Yossi Matias", "Avinatan Hassidim", "Greg S. Corrado", "Dale R. Webster", "Shravya Shetty", "Shruthi Prabhakara", "Yun Liu", "Daniel Golden", "Ellery Wulczyn", "David F. Steiner"], "abstract": "The interpretation of histopathology cases underlies many important diagnostic and treat-ment decisions in medicine. Notably, this process typically requires pathologists to integrate and summarize findings across multiple slides per case. Existing vision-language capabilities in computational pathology have so far been largely limited to small regions of interest, larger regions at low magnification, or single whole-slide images (WSIs). This limits interpretation of findings that span multiple high-magnification regions across multiple WSIs. By making use of Gemini 1.5 Flash, a large multimodal model (LMM) with a 1-million token context window, we demonstrate the ability to generate bottom-line diagnoses from up to 40,000 768 \u00d7 768 pixel image patches from multiple WSIs at 10X magnification. This is the equivalent of up to 11 hours of video at 1 fps. Expert pathologist evaluations demonstrate that the generated report text is clinically accurate and equivalent to or preferred over the original reporting for 68% (95% CI: [60%, 76%]) of multi-slide examples with up to 5 slides. While performance decreased for examples with 6 or more slides, this study demonstrates the promise of leveraging the long-context capabilities of modern LMMs for the uniquely challenging task of medical report generation where each case can contain thousands of image patches.", "sections": [{"title": "1 Introduction", "content": "Recent applications of vision-language modeling in digital histopathology have been predominantly designed to generate text describing individual regions of interest extracted from a single digitized histopathology image, or Whole Slide Image (WSI). An emerging line of research approaches the more practical clinical use case of slide-level text generation (Ahmed et al., 2024, Chen et al., 2024). However, in the typical clinical use case, there can be multiple biological tissue parts associated with a case, with each part having multiple slides. Pathologists write up a report summarizing their part-level diagnostic findings by microscopically reviewing each of the available slides per part and integrating information across these slides. This many-to-one relationship of slides to clinical descriptions is a recognized challenge for vision-language modeling in this space (Ahmed et al., 2024). The common approach taken in recent literature is to restrict modeling and analysis to single-slide cases or to manually identify a single slide within a case or part that is most representative of the clinical findings in reports (Ahmed et al., 2024, Chen et al., 2024, Guo et al., 2024, Shaikovski et al., 2024, Xu et al., 2024, Zhou et al., 2024). This strategy of selecting representative slides was also adopted in constructing one of the most widely used histopathology datasets, TCGA (Cooper et al., 2018)."}, {"title": "2 Methods", "content": "We build off of the recent data curation effort in Ahmed et al. (2024), where the primary dataset consists of de-identified retrospective data of approximately 350K WSIs with associated pathology reports from a tertiary teaching hospital. The study was reviewed by Advarra IRB (Columbia,"}, {"title": "2.1 Dataset", "content": "Maryland) and deemed exempt from further review as all data are retrospective and de-identified. The dataset is representative of the distribution of surgical and biopsy procedures, and findings read by pathologists in the U.S. in a large hospital system. The vast majority of slides are hematoxylin and eosin (H&E) stained. This dataset was referred to as DS1 in Ahmed et al. (2024).\nAs is typical for pathology reports, each case has an associated report text, corresponding to the final diagnosis (i.e. \"bottom-line\" text). These report texts are structured into part-level sections. In histopathology, a part refers to a specific tissue specimen submitted for pathology review. There may be multiple parts submitted for a given case, and for each part it is typical for multiple slides to be prepared for pathologist review. For each part-level section in the pathology report, there is a label (description of anatomical site and surgical procedure) and a finding (description of diagnostic findings), as illustrated in Figure 1. The label can often include information that is non-inferable from the WSIs alone (e.g., related to specimen location or procedure). For each part, we associate its corresponding report text with the set of WSIs that are available for that part. Since the number of slides per part is a key variable in this study, we define the following dataset subsets consisting of parts with varying number of slides per part:\n1. P1: Parts with a single slide,\n2. P2-5: Parts with 2 to 5 slides,\n3. P6-9: Parts with 6 to 9 slides,\n4. P10+: Parts with 10 or more slides.\nFor this work, we resplit cases in the dataset into train, validation and test sets with the goal of achieving a similar distribution across part-categories within each split. For cases with a part in P1, we used the same train/validation/test splits as in Ahmed et al. (2024). Parts in the remaining (multi-slide) part-categories were split into train/validation/test splits with ratios of approximately 70/20/10. The final part counts per split deviate slightly from the above ratio because we also ensure that each multi-part case has all of its parts in the same split."}, {"title": "2.2 Modeling", "content": "Our modeling approach consists of fine-tuning Gemini 1.5 Flash to generate part-level reports using all the WSIs for a given part as input. We utilized low-rank adaptation (LORA) (Hu et al., 2021), a method for efficiently fine-tuning large pre-trained models by learning low-rank matrices that represent weight-changes while keeping the original model-weights frozen. LoRA was used with rank 16 and applied to all attention and feed-forward layers of the language model. The patch-level image encoder, a version specialized to medical modalities (Yang et al., 2024), was pretrained and frozen. We refer to the entire multi-slide model - frozen image encoder and tuned language model - as POLYPATH. The input WSIs are represented as a sequence of non-overlapping, tissue-containing"}, {"title": "2.3 Evaluation", "content": "We perform automatic evaluations using common NLG metrics (ROUGE-L and METEOR) as well as human expert evaluations by board-certified pathologists. For both evaluations, we prompt the models with the label portion of the report text (indicating tissue type and procedure), and compare the findings output from the prompted models with the corresponding findings in the ground-truth text.\nFor human-evaluations, two U.S. board-certified pathologists evaluated the findings generated by POLYPATH (multi-slide model), SS-RANDOM (single-slide model), SS-LLM (single-slide model), as well as the original report text for a selected subset of 105 P2-5 examples. We did not perform extensive human-evaluation on parts from P6-9 and P10+ due to lower observed NLG metrics (see Section 3.1) for these part-categories and time needed for pathologist review of parts with many slides. We did perform human evaluation for a small subset (n = 16) of P6-9, which confirmed that the lower NLG scores for this subset were indicative of the relatively lower quality according to expert review.\nFor selecting a subset of P2-5 for human evaluation, tissue types were first categorized as common (colorectal, skin, cervix) or less common, and an equal number of common parts and uncommon parts were randomly sampled from the test split. For each part, pathologists were presented with the set of WSIs in a web-based digital pathology viewer along with the four candidate report texts. These texts were randomly ordered and pathologists were blinded to the source to avoid bias in interpretation. As in Ahmed et al. (2024), report texts were rated individually on a five-point scale:\n\u2022 1 - Completely inaccurate,\n\u2022 2 - Partially accurate,\n\u2022 3 - Mostly accurate but clinically significant error or omission,\n\u2022 4 - Mostly accurate without clinically significant error or omission,\n\u2022 5 - Highly accurate."}, {"title": "3 Results", "content": "We computed ROUGE-L and METEOR scores for POLYPATH (multi-slide model), SS-RANDOM and SS-LLM (single-slide models) on all part-categories using the finding from the original report text as ground truth. The results are summarized in Table 2. The metrics indicate consistent improvements for POLYPATH over the single-slide model variants across all part-categories. We also observe that metrics are lower for part-categories with more slides per part, especially for P6-9 and P10+. This is likely due to a combination of such part-categories being more complex to diagnose as well as being relatively less well-represented in our training sets (see Table 1). We also performed the above evaluation without prompting the models with the label, and using the generated label and finding as in Ahmed et al. (2024) for computing NLG metrics (Supplemental Table A.1)."}, {"title": "3.1 NLG Results", "content": "We utilized the pathologist ratings for both individual and paired evaluations of models for the sample of 105 parts from P2-5. We dropped one case because one rater noted that more contextual information was required to reliably rate the associated texts, including the ground truth finding.\nFor individual model evaluations, we computed the average score per model across parts (n = 104) and raters (n = 2). POLYPATH, with an average score of 4.11 out of 5 (95% CI: [3.87, 4.33]), was rated significantly higher (Wilcoxon signed-rank test applied to differences in scores averaged across raters) than the single-slide model variants SS-RANDOM (p = 0.003) and SS-LLM (p < 0.001) with average scores of 3.80 (95% CI: [3.53, 4.07]) and 3.77 (95% CI: [3.49, 4.04]) respectively, while being"}, {"title": "3.2 Pathologist Evaluation", "content": "rated significantly lower than the original report texts (p < 0.001), which averaged a score of 4.58 (95% CI: [4.44, 4.71], Figure 2a).\nComparing NLG metrics with pathologist ratings, we observe that both ROUGE-L and METEOR scores increased with increasing pathologist ratings (Supplemental Figure B.10). However, there was a large spread of NLG scores for any given pathologist rating, highlighting that, although NLG metrics can be conveniently calculated and are correlated with human expert evaluations, NLG metrics are not a substitute for human expert evaluations.\nFor paired comparisons between models, pairs of individual ratings were mapped to categories as described in Supplemental Table A.4. In a comparison between POLYPATH and the original report text, output from POLYPATH was judged to be at least as good as the original report text for 68% (95% CI: [60%, 76%]) of examples (Figure 2b). When we disaggregated this comparison by the severity of the original report text, i.e. normal, mild, significant (Figure 2b), POLYPATH-generated report texts were judged to be at least as good as the original report texts for 80% of parts with normal findings, 57% of parts with mild pathologic findings and 75% of parts with significant pathologic findings (Figure 2b). To characterize the inter-rater variability between the two raters, we include a version of this analysis disaggregated per pathologist reader in Supplemental Figure B.1; the distribution of ratings per pathologist in Supplemental Figure B.3 and Figure B.4; and inter-pathologist confusion matrices for scores for both the original report texts as well as generated report text from POLYPATH in Supplemental Figure B.5. We also conducted the paired comparison described above for SS-RANDOM and SS-LLM (Supplemental Figure B.2a and Figure B.2b), for which the generated report text was rated to be at least as good as the original report text at lower rates of 60% (95% CI: [52%, 69%]) and 61% (95% CI: [52%, 70%]) respectively."}, {"title": "4 Discussion", "content": "In this work, we show that a large multimodal model with long-context capabilities can be used to make progress on the computationally challenging and clinically relevant task of generating report text descriptions from multiple WSIs. We show by means of expert pathologist review that the generated report text for parts with up to 2-5 slides are considered at least equivalent to the part-level report text in the original pathology reports 68% of the time.\nIn the majority of cases, pathologic interpretation requires review of multiple slides. This is especially true for specimens from complex pathologies such as cancer. In our dataset, which is representative of a typical clinical case distribution, at least 63% of parts have more than one slide associated with them (Supplemental Figure B.9). While the current literature predominantly focuses on single-slide capabilities or multi-slide capabilities for a narrow range of anatomical sites, we believe general multi-slide modeling is a necessary step towards generalist diagnostic pathology models trained with supervision from pathology reports covering the wide range of tissue sites, procedures, and pathologies seen in clinical practice. Our experiments also suggest that the application of single-slide models naively may result in either lackluster performance or cascaded errors, further motivating the need for multi-slide modeling. From the clinical perspective, this may be intuitive: tasking multiple independent pathologists with reviewing and writing impressions for individual slides and then tasking a final pathologist with 'combining' the impressions across these slides is suboptimal and fails to incorporate insights that can only be drawn by contextualizing findings across all slides in the case or part.\nIn terms of evaluation, our data comparing absolute human expert ratings with NLG metrics also emphasize the importance of human ratings instead of relying exclusively on NLG comparisons with an original human-written diagnostic report (even when available). Specifically, we find that although NLG metrics were correlated with human ratings, with a monotonic increase in average NLG scores for both ROUGE-L and METEOR with higher human rating, the full spectrum of NLG scores across cases can span nearly the full range of 0-1 values. This likely stems from the fact that the NLG metrics are based on matching words or phrases (with some elementary semantic-matching), and there are often different ways of phrasing a report that corresponds to similar or identical clinical meaning.\nWhile our results for parts with up to 5 slides are promising, the NLG scores for parts with a higher number of slides are significantly lower. We believe this is in part due to relative scarcity of parts with more than 5 slides in our training data for example, while we have ~49k parts with 2-5 slides for training, we only have 7.2k parts with more than 5 slides per part (Table 1). Another limitation is the lack of external validation; while our dataset is curated from a typical clinical setting, all of it comes from the same sites. As a result we have not assessed if the model performance generalizes to cases from different sites or geographical areas.\nWhile we explored the potential application of a long-context LMM for generating report text for parts with multiple slides, clinical reporting is typically done at a case-level, which can involve in-"}, {"title": "5 Conclusion", "content": "In this work we presented a system for generating part-level report text from parts with multiple WSIs. We expect that the combination of ongoing advances in patch-level pathology foundation models and long-context LMMs as well as large-scale digitization of pathology slide archives and reports will enable further development of clinically useful automated pathology interpretation systems."}]}