{"title": "UniSpeaker: A Unified Approach for Multimodality-driven Speaker Generation", "authors": ["Zhengyan Sheng", "Zhihao Du", "Heng Lu", "Shiliang Zhang", "Zhen-Hua Ling"], "abstract": "Recent advancements in personalized speech generation have brought synthetic speech increasingly close to the realism of target speakers' recordings, yet multimodal speaker generation remains on the rise. This paper introduces UniSpeaker, a unified approach for multimodality-driven speaker generation. Specifically, we propose a unified voice aggregator based on KV-Former, applying soft contrastive loss to map diverse voice description modalities into a shared voice space, ensuring that the generated voice aligns more closely with the input descriptions. To evaluate multimodality-driven voice control, we build the first multimodality-based voice control (MVC) benchmark, focusing on voice suitability, voice diversity, and speech quality. UniSpeaker is evaluated across five tasks using the MVC benchmark, and the experimental results demonstrate that UniSpeaker outperforms previous modality-specific models. Speech samples are available at https://UniSpeaker.github.io.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of speech synthesis has seen remarkable progress [Wang et al., 2023a; Du et al., 2024; Ju et al., 2024], enabling the generated speech to closely resemble the actual recordings. However, traditional zero-shot speech synthesis still faces limitations in certain scenarios, such as providing voiceovers for virtual characters, where obtaining ideal reference speech is very difficult or even nonexistent [Guo et al., 2023]. Therefore, the voice control abilities in generative models need to transition from speaker cloning to speaker generation. Compared to cloning voice based on the reference speech, using other more convenient modalities to express the intentions holds great potential for creating the desired voice characteristics.\nRecently, several studies [Shimizu et al., 2024; Zhang et al., 2023; Lu et al., 2021; Sheng et al., 2023] have explored speaker generation based on text prompts or face images. These studies align specific modal representations with speaker embeddings, thereby controlling the voice characteristics using the aligned representations during inference. In addition to the aforementioned absolute voice descriptions, VoxEditor [Sheng et al., 2024] introduces the relative descriptions for voice attributes editing, allowing for more nuanced control over voice characteristics.\nDespite significant progress in these studies, previous methods often explore different voice description modalities and generation approaches independently, typically involving only one extra modality aligned with the reference speech. This leads to two shortcomings: (1) Independent modal alignments hindered collaborative speaker generation across multiple modal descriptions. Actually, the mapping between modalities other than the reference speech and voice characteristics is one-to-many [Leng et al., 2024], meaning the absolute voice description (a single face or a text description) can often correspond to different reasonable voice characteristics. If we combine the absolute voice description and relative voice description, it is evident that the generated speaker will better align with user needs. However, existing models can only process absolute or relative descriptions. (2) Multimodal speech alignment data is scarce and previous methods were trained from scratch on such limited paired multimodal data. This leads to a sparse coverage of the voice space and the limited diversity and consistency of the voice characteristics generated.\nTo address these limitations, we introduce UniSpeaker, a unified speaker generation model that integrates both absolute and relative voice descriptions. As illustrated in Figure 2, UniSpeaker is capable of processing inputs from facial and textual descriptions to generate voice characteristics that meet the expectations set by these voice descriptions. Recognizing the one-to-many issue, wherein users might find the generated voice characteristics unsatisfactory based on the aforementioned absolute descriptions, UniSpeaker enables precise voice attribute editing on the generated speech until the desired voice is attained. To achieve collaborative speaker generation, we propose a unified multimodal voice aggregator (MFA), which aligns these multimodal inputs into a coherent voice space. The MFA is based on the KV-Former architecture, a streamlined variant of the Transformer model, utilizing learnable key-value vectors to develop a shared multimodal voice space, with multimodal representations serving as queries. These key-value vectors encapsulate sufficient voice information, allowing the multimodal voice description to extract the most pertinent information. The output from the MFA is supplied to a subsequent generative model for voice control and aligned with speaker embeddings. In light of the correlation between the voice characteristics of different speakers, soft contrastive learning (SoftCL) is employed during alignment training, which relaxes strict one-to-to contrastive constraints and utilizes intra-modal discriminative information for guidance. Similar with ImageBind, this speech-anchoring mechanism facilitates the emergent alignment of various modalities within the voice space without parallel data across all modalities, which mitigated the impact of data scarcity and ensure the diversity of the voice characteristics.\nIn addition, large-scale speech generation models excel in voice control, but scalable multimodality integration is yet to be explored. We use the open-source CosyVoice [Du et al., 2024] as the backbone for UniSpeaker and apply self-distillation [Anastassiou et al., 2024] to enhance voice disentanglement, maintaining its versatility across tasks.\nDue to the lack of publicly accessible benchmarks for assessing multimodality-driven voice control, we developed a multimodality-based voice control (MVC) benchmark. This benchmark covers five fundamental tasks: face-driven voice conversion (FaceVC), face-driven personalized text-to-speech (FaceTTS), text description-driven voice conversion (TextVC), text description-driven personalized text-to-speech (TextTTS), and attribute-driven voice editing (AVE). Consistent with prior research [Yao et al., 2024], the MVC benchmark evaluates generated speech using multimodal voice descriptions on three parameters: voice suitability, voice diversity, and speech quality. We assessed UniSpeaker with the MVC benchmark, where it outperformed previous modality-specific models in the five fundamental tasks."}, {"title": "2 Relate Work", "content": ""}, {"title": "2.1 Multimodality-driven voice control for speech generation", "content": "Modeling diverse voice characteristics has consistently been a critical focus in the field of speech synthesis. Recent works, such as PromptTTS2 [Leng et al., 2024], Audiobox [Vyas et al., 2023], InstructSpeech[Huang et al., 2024] and others [Guan et al., 2024; Yang et al., 2024; Ji et al., 2024], have explored using text prompts to control the style or emotion of generated speech. However, only a few studies have specifically targeted voice control with text prompt [Shimizu et al., 2024; Zhang et al., 2023]. Text prompt-based style control TTS methods typically convert speech attributes like pitch, energy, duration, and emotion into natural style prompts using LLMs. Since these style prompts primarily reflect prosody and capture minimal speaker individuality, achieving the desired voice control remains challenging.\nIn the field of multimodal voice control, researchers have previously attempted to align different voice description modalities with speaker embeddings using models such as memory networks [Sheng et al., 2023], mixture density networks [Shimizu et al., 2024], and latent diffusion [Yao et al., 2024], as well as loss functions like MSE loss [Lu et al., 2021], cosine similarity loss [Zhang et al., 2023], and perceptual loss [Weng et al., 2023]. However, these alignment methods relied on parallel datasets and were challenging to extend directly to additional modalities. Performance-wise, previous face-based methods [Lee et al., 2023] generally ensured gender accuracy but often produced incongruous voice characteristics, such as generating a youthful voice for an elderly face. Additionally, VoxEditor [Sheng et al., 2024] is limited to performing voice attribute editing on existing source speech, thus offering restricted voice diversity. In response, the proposed UniSpeaker employs a unified voice aggregator to construct a shared voice space that can be easily extended to new modalities, achieving versatile and diverse voice control."}, {"title": "2.2 Large speech generation models", "content": "As speech generation systems [Tan et al., 2022; Kim et al., 2021] have achieved remarkable levels of naturalness and robustness, recent research [Ju et al., 2024; Lee et al., 2024] has shifted focus towards exploring novel generative models, advanced modeling objectives, and larger-scale datasets to pursue voice diversity. When integrating multimodal voice descriptions, it is crucial to preserve the performance of pretrained speech generation models in terms of naturalness, robustness, and prosody. Some representative large-scale speech generation [Wang et al., 2023a; Kim et al., 2024; Chen et al., 2024] models typically leverage a neural codec to convert speech waveforms into discrete acoustic token sequences, along with an autoregressive language model to generate discrete tokens from text. However, the discrete acoustic token sequences entangle content, speaker, and prosodic information in this approach, complicating the alignment of multimodal voice characteristics without disrupting the content and prosody of the generated speech. Recently, CosyVoice [Du et al., 2024] has utilized supervised semantic tokens [Radford et al., 2023] as the modeling objectives for a large language model (LLM). Subsequently, a conditional flow matching model (CFM) generates speech based on semantic tokens, speaker embeddings and mel spectrograms prompt. Since the semantic tokens primarily encompass content and prosodic information, the speaker information included is limited. This facilitates further voice disentanglement and the integration of multimodal voice descriptions, making CosyVoice well-suited as the backbone for the UniSpeaker model proposed in this paper."}, {"title": "3 Methods", "content": "In this section, we first review the backbone CosyVoice, then introduce how multimodal voice descriptions are integrated into a pre-trained speech generation model."}, {"title": "3.1 Preliminaries", "content": "CosyVoice leverages supervised semantic tokens [Radford et al., 2023; Ye et al., 2024] as modeling objectives, utilizing an LLM for text-to-token generation and a CFM for token-to-speech synthesis. Given a dataset $D = \\{x_i, y_i\\}$, where $x$ is a speech sample and $y$ is the corresponding text transcription, the sequence input to the LLM is mainly comprised of $\\{s, Y, C\\}$, where $s$ represents the speaker embeddings of $x$, $Y$ is the text embedding of $y$ and $C$ is the semantic tokens of $x$. The LLM is then trained in an autoregressive manner to minimize the negative log-likelihood of semantic tokens $C$.\nThe core of CFM is to construct a probability density path from a prior distribution to $p_0(X)$ to the data distribution of the Mel-spectrograms $q(X)$. The probability density path is defined by a time-dependent vector field $v_t(X)$,which generates the flow $t$ through an ordinary differential equation (ODE). The flow matching model is trained using optimal-transport conditional flow matching (OT-CFM) [Tong et al., 2023], which can be written as follows,\n$\\mathcal{L}_{OT-CFM} = \\mathbb{E}_{t, p_0(X_0), q(X_1)} [\\| w_t(\\phi_t(X_0, X_1)|X_1) - v_t(\\phi_t(X_0, X_1)|\\theta_{CFM}) \\|_1],$\nwhere\n$\\phi_t(X_0, X_1) = (1 - t)X_0 + tX_1,$\n$\\omega_t(\\phi_t(X_0, X_1)|X_1) = X_1 - X_0.$\nThe speaker embeddings $s$, speech tokens $C$, and masked Mel-spectrogram prompt $X_1$ are also fed into the neural network to match the vector field with learnable parameters $\\theta_{CFM}$,\n$v_t(\\phi_t(X_0, X_1)|\\theta_{CFM}) = NN(\\phi_t^{OT}(\\phi_t(X_0, X_1), t; s, C, X_1).$\nThe supervised semantic tokens contains only a small amount of speaker information, and CosyVoice demonstrates good performance in voice characteristics disentanglement. In our preliminary experiments, we found that although the LLM received speaker embeddings, its impact on voice characteristics was minimal. In contrast, the CFM module plays a decisive role in influencing voice characteristics."}, {"title": "3.2 Multimodal Voice Description Integration", "content": "We incorporate multiple modalities into the CFM model, allowing various inputs to control the voice characteristics of generated speech. As shown in Figure, each modality is first processed by a pre-trained, modality-specific encoder to obtain the corresponding representation. Each kind of representation is then transformed into a latent vector via adaptive average pooling or a multi-layer perceptron. Those vectors across modalities are mapped into a unified voice space through a shared MVA, producing the corresponding speaker embeddings. These speaker embeddings are then fed into the CFM for speech generation.\nMultimodal Voice Aggregator Then global representations of different modalities should be aligned with speaker embeddings within the voice space. Previous methods relied on limited datasets that matched only two modalities for alignment, resulting in a sparse distribution in the voice space and weak generalization capabilities.\nInspired by Q-Former [Li et al., 2023] and the memory mechanism [Sheng et al., 2023; Lee et al., 2021], we propose the KV-Former architecture as a unified multimodal voice aggregator. This architecture integrates learnable key-value vectors into a simplified Transformer, as shown in Figure. The multimodal representations act as queries and perform multi-head cross-attention with the learnable key-value vectors to retrieve the most informative representation in the voice subspace. The formulation of this process is as follows,\n$q = W^qsm, k= W^kf, v = W^vf, a_m = Softmax(\\frac{q k^T}{\\sqrt{d}})$\nwhere $W$ are the projection matrices in attention, $sm \\in \\{sf, sr, st\\}$ represents various state vectors, $f$ are learnable key-value vectors, $d$ is the dimension of $f$, and $am$ is the output of cross attention. In this process, the learnable key-value vectors create an information bottleneck, interacting with the three modalities to build a shared voice space. Additionally, MVA adopts a speech-anchoring mechanism, reference speech is used as input for MVA with a 50% probability. In this way, even without parallel data between all modalities, different modalities achieves emergent alignment in the voice space through shared k-v vectors and joint training, which mitigated the impact of data scarcity and ensure the diversity of voice characteristics. In addition, our module also allows for easy expansion to new modalities by adding the a modality-specific encoder.\nTo integrate multimodal inputs for voice control without losing the general abilities of CFM, we feed the output of MVA to the CFM and adapt the model without changing the CFM weights. The MVA is trained to optimize $\\mathcal{L}_{OT-CFM}$ and Equation (3) is transformed as follows to fit speaker embeddings,\n$\\nu_t(\\phi_t(X_0, X_1)|\\theta_{MVA}) = NN(\\phi_t^{OT}(\\phi_t(X_0, X_1), t; vm, C),$\nwhere $vm \\in \\{vf, vr, vt\\}$ and $vf, vr, vt$ are the outputs of applying MVA to $sf, sr, st$, respectively. In this manner, CFM can integrate multiple modalities for voice control and keep its ability to generate natural and robust speech.\nSoft Contrastive Learning Relying solely on $\\mathcal{L}_{OT-CFM}$ to optimize MVA leads to slow convergence, and the generated speech may exhibit voice discordance with the input voice descriptions. Inspired by previous studies [Gao et al., 2024; Wang et al., 2024], we additionally introduce the SoftCL strategy for speech-anchoring multimodal alignment, including both inter-modal and intra-modal alignment, as shown in Figure For inter-modal alignment, we employ InfoNCE [Radford et al., 2021], which pulls the paired multimodal and speaker embeddings closer together while pushing the unpaired ones apart. In addition, to bring cross-modal similarities closer to the distribution within each modality, intra-modal similarities serve as soft labels. Specifically, given a batch of $N$ multimodal-voice speaker embeddings pairs $\\{(vm,s)\\}_{i=1}^N$, the intra-model self-similarity vector $p_i(sr, sr) = \\{p_{ij}(sr, sr)\\}_{j=1}^N$ can be obtained by:\n$p_{ij}(s_r, s_r) = \\frac{exp\\big(sim(s_i, s_j) /\\tau\\big)}{\\sum_{k=1}^N exp\\big(sim(s_i, s_k) /\\tau\\big)},$\nwhere $\\tau$ is a learnable temperature coefficient, initialized to 0.07, and $sim()$ denotes the dot product used to calculate similarity. Despite intra-model self-similarity, the confidence of positive samples still outweighs that of negatives, potentially overshadowing negatives in cross-modal relation alignment. To address this, we disentangle the negatives in the distribution to boost the relation alignment. For the self-similarity vector $pi(sr,sr) \\in R^{1\\times N}$, the neg-disentangled $p^*(sr, sr) \\in R^{1\\times (N-1)}$ distribution is calculated as follows,\n$p_{ij}^* = \\frac{exp(p_{ij})}{\\sum_{k=1, k\\neq i}^N exp(p_{ik})}.$\nWe also apply the above negative disentanglement to $pi(sr, vm)$, yielding $p^*(sr, vm)$. Then, the intra-modality alignment supervision can be achieved with negative disentanglement as follows,\n$\\mathcal{L}_{INTRA} = \\frac{1}{N} \\sum_{i=1}^N KL\\big(p^*(s_r, s_r) || p^*(s_r, v_m)\\big),$\nwhere $KL$ represents the Kullback-Leibler Divergence. Generally, UniSpeaker is trained to optimize the following loss function,\n$\\mathcal{L} = \\mathcal{L}_{OT-CFM} + \\lambda_1 \\mathcal{L}_{INTRA} + \\lambda_2 \\mathcal{L}_{INTER},$\nwhere $\\mathcal{L}_{INTRA}$ is the InfoNCE loss, $\\lambda_1$ and $\\lambda_2$ are hyperparameters used to balance each loss term.\nSelf-distillation In our preliminary experiments, we observed that CFM usually draws speaker information from semantic tokens, often overlooking speaker information within the face image, due to the cross-modal gap. Therefore, to enhance voice disentanglement before merging multimodal voice descriptions, self-distillation is applied to fine-tune the CFM. Initially, we employ semantic tokens from the original speech, along with a Mel-spectrogram prompt and speaker embeddings from a randomly chosen speaker, which are then inputted into the CFM for voice conversion. Then, given the semantic tokens $C$ of converted speech and speaker embeddings $s$ of source speech, the CFM is fine-tuned to predict the source speech. We removed the masked Mel-spectrogram prompt to improve the voice control by the speaker embeddings, transforming Equation (3) as follows,\n$\\nu_t(\\phi_t(X_0, X_1)|\\theta_{FM}) = NN(\\phi_t^{OT}(\\phi_t(X_0, X_1), t; s, \\hat{C}).$\nIn this way, the voice characteristics of the generated speech is controlled by the speaker embeddings input to the CFM. This allows the integration of multimodal voice description directly into the CFM, simplifying the process without requiring modifications to the LLM."}, {"title": "4 Dataset and Benchmark", "content": "Four modality-specific datasets were used to train the UniSpeaker, including LRS3-TED [Afouras et al., 2018], LibriTTS-P [Kawamura et al., 2024], VCTK-R [Sheng et al., 2024], and inner speaker identity description dataset collected from the internet, totaling about 1000 hours of audio data..\nIn the MVC Benchmark, for face-related evaluation, we randomly selected 600 face images from the test set of LRS3-TED. In terms of textual descriptions, 600 sentences were randomly picked from the validation set and rewritten by a LLM (GPT-3.5-TURBO), ensuring that the meaning of the sentences remained unchanged. For voice attribute editing, 200 sentences were randomly selected from VCTK and edited on all attributes for evaluation. All above samples are unseen during training. The MVC benchmark evaluates the generated speech from three perspectives: voice suitability, voice diversity, and speech quality. 1) Voice suitability evaluates whether the voice characteristics of the generated speech align with the input multimodal voice description. This includes three specific metrics: Speaker Similarity with Target (SST), Speaker Similarity Consistency (SSC), and MOS-Match. 2) Voice diversity evaluates the model's ability to produce a diverse set of voice characteristics based on the descriptions of different speakers, rather than generating very similar ones. A metric named Speaker Similarity Diversity (SSD) is employed for evaluating voice diversity, which measures the speaker similarity between the speech generated from the descriptions of different speakers. 3) Speech quality assesses the robustness and naturalness of the generated speech, using two key metrics: word error rate (WER) and MOS-Nat. We employ an automatic speech recognition model\u00b9 to transcribe the generated speech and compute the WER. MOS-Nat is determined through subjective listening tests for mean opinion scores to evaluate the naturalness of the generated speech. Please refer to Appendix for more details."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experiment Settings", "content": "We trained the UniSpeaker using 4 NVIDIA TESLA V100 32G GPUs for 30K steps. The models were optimized using the AdamW optimizer with a learning rate of 1e-5 and a 10K warmup steps. The weights $\u03bb_1$ and $\u03bb_2$ in Equation (9) were set to 0.05. FaceNet[Schroff et al., 2015], T5[Raffel et al., 2020], and CAM++[Wang et al., 2023b] serve as modality-specific encoders for face, text, and speech, respectively. The speech tokenizer and codec LM were the same as those used in CosyVoice. For TTS, the codec LM accepted only textinputs without speaker embeddings. We compared our UniSpeaker with 11 task-specific expert models in five tasks. We used the official code or pre-trained checkpoints of Imaginary Voice [Lee et al., 2023], FaceVC [Lu et al., 2021], SP-FaceVC [Weng et al., 2023], FVMVC [Sheng et al., 2023], and CosyVoice-Instruct [Du et al., 2024]. For the other methods, we reproduced them according to their respective papers and evaluated them on the same dataset. Please refer to Appendix for more details."}, {"title": "5.2 Evaluation Results", "content": "In this section, we conduct experiments comparing the UniSpeaker with the baselines and all objective and subjective evaluation results are reported in Table 1.\nIn terms of voice suitability, our findings revealed that: 1) Across five tasks, UniSpeaker outperformed previous approaches on all three metrics, except for MOS-Match in the AVE task. While VoxEditor incorporates a complex residual memory network, the performance of our unified and scalable MVA remains competitive in MOS-Match. 2) In terms of face-based voice control, previous methods were generally effective in accurately controlling the gender of the voice characteristics but often exhibited obvious voice inconsistencies in subjective aspects such as age. In contrast, UniSpeaker achieved substantial improvements in both voice-age matching and overall subjective perception. 3) Additionally, we conducted an ABX test, as shown in Figure 3, the voice characteristics generated by UniSpeaker sometimes can match the face image even more closely than those of the actual speaker. We encourage readers to listen to the samples on the demo page. 4) In text control, Cosy Voice-instruct concatenates voice characteristic descriptions with the content prompt in the LLM without utilizing a pre-trained text prompt, resulting in difficulties grasping semantic information effectively and producing ambiguous voice characteristics. In contrast, UniSpeaker achieves excellent semantic-to-voice consistency, where similar semantics generate similar voice characteristics.\nIn terms of voice diversity, it is clear that UniSpeaker significantly outperforms previous methods across 5 tasks. Furthermore, we visualized the speaker embeddings of the generated speech from both SYNTHE-SEES and UniSpeaker systems using t-SNE [Chan et al., 2019], as shown in Figure 4 (a). The figure reveals that the voice space generated by our method is significantly richer, whereas the voice space of the baseline is relatively sparse. This indicates the voice characteristics generated by the baseline for different faces may being very similar, greatly limiting voice diversity.\nIn terms of speech quality, by freezing the CFM during training, UniSpeaker preserve the general abilities of our backbone. Consequently, UniSpeaker surpasses previous methods in overall speech quality, only the MOS-Nat slightly lags behind CosyVoice-Instruct. This lag is due to the CFM occasionally learning noise patterns from the dataset. Conversely, CosyVoice-Instruct only integrate multimodal voice descriptions in the LLM, resulting in minimal impact on speech quality."}, {"title": "5.3 Ablation Study", "content": "Three ablation studies were conducted in our experiments. 1) To verify the effectiveness of MVA, the output of modality-specific encoders was mapped to the global representation, and it was directly fed into the CFM. 2) To assess the effectiveness of SoftCL, we removed the intra-class and inter-class contrastive losses from the output of MVA. 3) To validate the effectiveness of self-distillation, the performance of UniSpeaker and the open-source CosyVoice model (without self-distillation) was compared on TTS and VC tasks. We report the evaluation results for certain tasks in Table 2, with more evaluation results available in the Appendix.\nWe have the following observations: 1) MVA proved beneficial for voice control with a shared multimodal voice space. It utilizes multimodal data for joint modeling through shared k-v vectors, resulting in a uniform distribution of the voice space. This promotes alignment between different modalities and enhances the model's performance in both voice diversity and voice suitability. 2) Removing SoftCL resulted in a decline across various metrics, specifically creating a significant mismatch between the generated voice and the input voice descriptions. 3) Eliminating self-distillation also had notable effects. Experimental results indicated that self-distillation significantly enhanced voice control, particularly in terms of SST. However, due to the limited data used for self-distillation, there was a slight reduction in voice diversity."}, {"title": "5.4 Discussions", "content": "We investigated the impact of different multimodal data scales on the shared voice space. For face-driven voice control, we trained UniSpeaker using various datasets: solely LRS3, and additional datasets of varying sizes. The results, presented in Figure 5, show that increasing the amount of multimodal data improves the performance of FaceVC and FaceTTS, highlighting the benefits of multimodal joint modeling. Furthermore, the influence of additional multimodal data on SSC is less pronounced for SST and SSD, as SSC primarily relies on intra-modal relationships. We randomly selected 8 unseen speakers and sampled 100 different face images from each for FaceTTS. The t-SNE visualization of speaker embeddings extracted from generated speech is presented in Figure 4 (b). We observed that for each speaker, the voice remained consistent across various facial images with different angles and backgrounds. This indicates that UniSpeaker demonstrates strong robustness to noisy information in facial images."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose the UniSpeaker, a speech generation model that leverages multimodal voice description for voice control. Through a unified voice aggregator and designed training strategies, UniSpeaker outperforms previous modality-specific models across five tasks, generating voices that better match the input voice descriptions. In the future, we will explore how to more effectively utilize multiple voice descriptions of different modalities for one speaker simultaneously and apply our method on other more modalities for voice control."}]}