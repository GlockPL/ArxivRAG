{"title": "Mitigating Relative Over-Generalization in Multi-Agent Reinforcement Learning", "authors": ["Ting Zhu", "Yue Jin", "Jeremie Houssineau", "Giovanni Montana"], "abstract": "In decentralized multi-agent reinforcement learning, agents learning in isolation can lead to relative over-generalization (RO), where optimal joint actions are undervalued in favor of suboptimal ones. This hinders effective coordination in cooperative tasks, as agents tend to choose actions that are individually rational but collectively suboptimal. To address this issue, we introduce MaxMax Q-Learning (MMQ), which employs an iterative process of sampling and evaluating potential next states, selecting those with maximal Q-values for learning. This approach refines approximations of ideal state transitions, aligning more closely with the optimal joint policy of collaborating agents. We provide theoretical analysis supporting MMQ's potential and present empirical evaluations across various environments susceptible to RO. Our results demonstrate that MMQ frequently outperforms existing baselines, exhibiting enhanced convergence and sample efficiency.", "sections": [{"title": "1 Introduction", "content": "Cooperative multi-agent reinforcement learning (MARL) has become increasingly important for addressing complex real-world challenges that require coordinated behaviors among multiple agents. Successful applications included playing card games [Brown and Sandholm, 2018], autonomous driving [Shalev-Shwartz et al., 2016, Zhou et al., 2021], unmanned aerial vehicles [Wang et al., 2020], wireless sensor networks [Xu et al., 2020, Sahraoui et al., 2021] and traffic light control [Bazzan, 2009, Zhou et al., 2023]. A dominant framework in MARL is centralized training with decentralized execution (CTDE) [Lowe et al., 2017, Rashid et al., 2018, Son et al., 2019], which often relies on a centralized coordinator to aggregate information, such as local observation or individual actions from other agents during training. While this approach has been widely adopted due to its effectiveness in leveraging global information, it may still face scalability challenges, particularly in environments with large number of agents or complex interactions. Additionally, in scenarios where privacy is a critical concern, CTDE methods that require access to individual agent data could be less desirable. While many CTDE methods do not require exhaustive local information from all agents, the potential for scalability and privacy issues in certain implementations warrants consideration. While inter-agent communication can partially mitigate some of these challenges [Foerster et al., 2016, Zhu et al., 2022], it introduces additional overhead, which can be prohibitive in environments where communication is costly or unreliable.\nConsider large-scale drone swarms for search and rescue or surveillance tasks [Baldazo et al., 2019, Batra et al., 2022]. In a CTDE framework, centrally aggregating data from each drone can be impractical due to network delays, dynamic environments, and potential communication failures."}, {"title": null, "content": "Additionally, communication frameworks may struggle with channel congestion or high packet loss, especially in complex terrain or with electronic interference. Fully decentralized learning presents a promising alternative, where agents rely solely on their experiences without considering the actions or policies of other agents during both training and execution. This approach enables the system to scale effectively and remain resilient to communication issues. However, decentralized approaches come with their own challenges. From an individual agent's perspective, the learning process occurs within a non-stationary MDP, as the transition probabilities change due to the evolving policies of other agents.\nExisting decentralized MARL methods, including optimism strategies in Q-learning [Lauer and Riedmiller, 2000, Matignon et al., 2007, Wei and Luke, 2016], attempt to mitigate these challenges. More recently, Ideal Independent Q-learning (I2Q) [Jiang and Lu, 2022] explicitly models state transitions assuming optimal joint behavior, introducing ideal transition probabilities to address non-stationarity in independent Q-learning.\nAnother critical issue in decentralised MARL is Relative Over-generalisation (RO), where agents prefer suboptimal policies because individual actions appear preferable without coordinated strategies. First defined and studied within the field of cooperative co-evolutionary algorithms [Wiegand, 2004, Panait et al., 2006, Panait, 2007], it has more recently been explored predominantly in centralised MARL contexts [Rashid et al., 2020, Gupta et al., 2021, Shi and Peng, 2022]. RO occurs when agents adapt their actions to limited interactions, often focusing on the exploratory behaviours of other agents. Consequently, agents may favour more robust but less optimal solutions in the absence of coordinated strategies. In decentralised learning settings, this problem has been discussed within simple matrix game scenarios [Wei and Luke, 2016]. RO, exacerbated by non-stationarity, presents a significant challenge as agents make decisions based on fluctuating global rewards without the benefit of coordinated strategies [Matignon et al., 2012, Wei and Luke, 2016]. Although previous implementations of optimistic strategies have shown some efficacy, our empirical results indicate they fall short in cooperative tasks with pronounced RO challenges.\nThis paper introduces MaxMax Q-Learning (MMQ), a novel algorithm designed to address the RO problem in decentralised MARL settings. MMQ aims to mitigate the challenges posed by RO and non-stationarity in decentralised learning environments. The key insight behind MMQ is enabling agents to reason about beneficial experiences that occur infrequently. At its core, MMQ employs two non-parameterised quantile models to capture the range of state transitions, accounting for both environmental factors and the evolving policies of learning agents. These models iteratively sample, evaluate, and select optimal states, refining the approximation of ideal state transitions and facilitating global reward maximisation. The state with the highest Q-value is then selected to update the value function, promoting convergence towards optimal Q-values in the context of other agents' best actions. The MMQ algorithm incorporates two maximum operators in the Bellman update: the first takes the maximum over all possible next states to select the most promising future scenario, and the second takes the maximum over Q-values of state-action pairs to determine the best action in that scenario. MMQ's adaptive nature, which involves continuously updating the ranges of possible next states, enables effective decision-making in dynamic environments.\nThe main contributions of this paper are threefold. First, we introduce MMQ, a novel algorithm that employs quantile models to capture multi-agent dynamics and approximate ideal state transitions through sampling. Second, we provide a theoretical demonstration of MMQ's potential to converge to globally optimal joint policies, assuming perfect knowledge of forward and value functions. Third, we present empirical results showing that MMQ often outperforms or matches established baselines across various cooperative tasks, highlighting its potential for faster convergence, enhanced sample efficiency, and improved reward maximisation in decentralised learning environments.\nThe remainder of this paper is structured as follows: Section 2 discusses related work in MARL and uncertainty quantification. Section 3 provides background on multi-agent Markov Decision Processes and the challenges of relative over-generalization. Section 4 presents the methodology of MaxMax Q-Learning, including its theoretical foundations and implementation details. Section 5 describes our experimental setup and results across various environments. Finally, Section 6"}, {"title": "2 Related work", "content": "Centralised learning methods. Within the centralized training paradigm, RO has been discussed mostly for value factorization methods like QMIX [Rashid et al., 2018]. The monotonic factorization in QMIX cannot represent the dependency of one agent's value on others' policies, making it prone to RO. Proposed solutions include weighting schemes during learning [Rashid et al., 2020], curriculum transfer from simpler tasks [Gupta et al., 2021, Shi and Peng, 2022], and sequential execution policy [Liu et al., 2024]. Soft Q-learning extensions to multi-agent actor-critics [Wei et al., 2018, Lowe et al., 2017] utilise energy policies for global search to mitigate RO. However, unlike our decentralized approach, these methods require a centralized critic with joint action access during training.\nFully decentralized learning. Decentralized approaches in MARL aim to overcome the scalability and privacy issues associated with centralized methods. However, they face unique challenges, particularly in addressing non-stationarity and RO. Existing approaches can be categorized based on their strategies for tackling these issues. Basic independent learning methods like Independent Q-learning (IQL) [Tan, 1993] and independent PPO [Yu et al., 2022] form the foundation of decentralized MARL. However, their simultaneous updates can lead to non-stationarity, potentially compromising convergence. Recent work by Su and Lu [2023] on DPO addresses this by providing monotonic improvement and convergence guarantees. To mitigate negative impacts of uncoordinated learning, several methods promote optimism toward other agents' behaviors. Distributed Q-learning [Lauer and Riedmiller, 2000] selectively updates Q-values based only on positive TD errors. Hysteretic Q-learning [Matignon et al., 2007] uses asymmetric learning rates, while Lenient Q-learning [Wei and Luke, 2016] selectively ignores negative TD errors. These techniques aim to overcome convergence to suboptimal joint actions by dampening unhelpful Q-value changes. Taking a different approach, the recently introduced Ideal Independent Q-learning (I2Q) [Jiang and Lu, 2022] explicitly models ideal cooperative transitions. However, it requires learning an additional utility function over state pairs. Our proposed method, MMQ, builds upon these approaches by encoding uncertainty about decentralized MARL dynamics. We model other agents as sources of heteroscedastic uncertainty with an epistemic flavor, providing a more flexible way to represent optimistic policies. By sampling from possible next states, MMQ avoids the need for heuristic corrections or separate Q-functions, offering a novel solution to the challenges of decentralized MARL.\nUncertainty quantification. Quantifying different sources of uncertainty is crucial in reinforcement learning, particularly in multi-agent settings. Prior work distinguishes between aleatoric uncertainty, arising from environment stochasticity, and epistemic uncertainty, due to insufficient experiences [Osband et al., 2016, Depeweg et al., 2016]. Various methods, including variance networks [Kendall and Gal, 2017, Wu et al., 2021] and ensembles [Lakshminarayanan et al., 2017], have been proposed to model these uncertainties, with applications in single-agent RL [Chua et al., 2018, Sekar et al., 2020]. MARL introduces additional complexity due to the dynamic nature of agent interactions, leading to non-stationarity. This non-stationarity limits an agent's ability to reduce epistemic uncertainty through repeated state visits [Hernandez-Leal et al., 2017] and can be viewed as another form of epistemic uncertainty. Our proposed MMQ algorithm addresses these challenges by using quantile networks to effectively manage two key sources of epistemic uncertainty in multi-agent settings: limited experiential data and evolving strategies of other agents. This approach allows MMQ to better handle the unique uncertainties present in decentralized MARL environments."}, {"title": "3 Background and preliminaries", "content": null}, {"title": "3.1 Multi-agent Markov Decision Process", "content": "Consider a multi-agent Markov Decision Process (MDP) represented by M = (S, A, R, Penv, \u03b3). Within this tuple, S denotes the state space, A is the joint action space, Penv (s'|s, a) and R(s, s') are respectively the environment dynamics and reward function for states s, s' \u2208 S and action a \u2208 A, and \u03b3 is the discount factor. Given N agents, the action space is of the form A = A1 \u00d7\uff65\uff65\uff65 \u00d7 AN with any action a \u2208 A taking the form a = (a1,...,an). At each time step t, an agent indexed by i\u2208 {1, ..., N} selects an individual action, ai. When the N actions are executed, the environment transitions from state s to state s', and every agent receives a global reward, rt. The objective is to maximize the expected return, i.e., \u0395[\u03a3 Ytrt], where T is the time horizon. The individual environment dynamics is defined as\n$P_i(s'|s, a_i) = \\sum_{a_{-i}} P_{env}(s'|s, a) \\pi_{-i}(a_{-i}|s)$"}, {"title": null, "content": "where a\u2212i represents the joint action excluding agent i and \u03c0\u2212i is the joint policy of all other agents. Here, the joint action a inherently depends on ai and a\u2212i. From any individual agent's perspective, the learning process occurs within a non-stationary MDP due to the evolving policy \u03c0i."}, {"title": "3.2 Relative over-generalization through an example", "content": "Consider a two-agent game with the reward structure shown in Table 1. In this game, there are three possible actions: A, B, and C. Agents would receive a joint reward of +3 if they take A together. However, if only one agent takes A, that agent incurs a penalty of -6. Agents end up choosing less optimal actions (B or C) if they perceive the reward for choosing A to be lower, based on their expectations of the other agent's actions. For agent 1, the utility function Q1(\u00b7) is related to the probability that the other agent chooses A, i.e. \u03c02(A). In this case, Q1(A) would be smaller than Q1(B) or Q1 (C) if \u03c02(A) < 1/3. This threshold arises because the expected value of choosing A becomes lower than choosing B or C when the probability of the other agent also choosing A falls below 1/3. Consequently, during initial uniform exploration where \u03c01(A) = \u03c02(A) = 1/3, both agents tend to favour B or C over A, even though A is the globally optimal choice. Thus, with independent learning without considering the other agent's best action, both agents may end up with choosing suboptimal actions and fail to cooperate."}, {"title": "3.3 Independent Q-Learning", "content": "In independent Q-learning [Tan, 1993], each agent i learns a policy independently, treating other agents as part of the environment. The individual Q-function is $Q_i(s, a_i) = E [\\sum_{t=0}^\\infty \\gamma^t r_t | s_0 = s, a_{i,0} = a_i]$ for agent i. Each agent updates its Q-function by minimizing the loss Ep\u2081(s'|s,ai) [(yi \u2212 Qi(s, ai))2], where yi is the target value defined as R(s, s') + \u03b3 maxa' Qi(s', a'). The RO problem arises in this setup as each agent seeks to maximize its own expected return based on experiences where other agents' policies evolve and contain random explorations."}, {"title": "3.4 Ideal transition probabilities", "content": "To address the RO problem in this context, some approaches introduce implicit coordination mechanisms centered on the concept of an ideal transition model [Lauer and Riedmiller, 2000, Matignon et al., 2007, Wei and Luke, 2016, Palmer et al., 2018, Jiang and Lu, 2022]. These methods guide each agent's learning with hypothetical transitions that assume optimal joint behavior, aligning independent learners towards coordination. Let \u03c0\u2217\ni denote the joint policy of other agents, and Q\u2217 the optimal joint Q-function. The optimal joint policy of other agents can be expressed as \u03c0\u2217\u2212i(s, ai) = arg maxa\u2212i Q\u2217 (s, ai, a\u2212i).\nThe concept of ideal transitions refers to hypothetical state transitions that assume other agents are following optimal joint policies. These ideal transition probabilities represent the dynamics that would occur if all agents achieved perfect coordination, and are defined as $P_i^*(s'|s, a_i) = P_{env}(s'|s, a_i, \\pi_{-i}^*(s, a_i))$. Based on these probabilities, the Bellman optimality equation is given by\n$Q_i^*(s, a_i) = E_{s' \\sim P_i^*(s'|s, a_i)}[R(s, s') + \\gamma \\max_{a'} Q_i^*(s', a')]$"}, {"title": null, "content": "where Qi\u2217(s, ai) is the optimal Q-function for agent i.\nAn important theoretical result from Jiang and Lu [2022] establishes that when all agents perform Q-learning based on these ideal transition probabilities, the individual and joint optimality align, that is, maxai Qi(s, ai) = Q\u2217(s, \u03c0\u2217(s)), where Q\u2217(s, a) is the optimal joint Q-function for any action a \u2208 A, and \u03c0\u2217 is the optimal joint policy. However, achieving true ideal transitions is intractable in practice due to the evolving, uncontrolled nature of learning agents. This motivates developing techniques to approximate ideal transitions."}, {"title": "4 MaxMax Q-learning Methodology", "content": null}, {"title": "4.1 Approximation of Bellman optimality equation", "content": "Our methodology aims to approximate ideal transition probabilities, which assume other agents follow optimal joint policies. We focus on deterministic environments and reformulate the Bellman optimality in Eq. (1) to highlight the dependence on the set Ss,ai of possible next states,\n$S_{s, a_i} = \\{ s' = f_{env}(s, a_i, a_{-i}) | a_{-i} \\in \\prod_{j \\neq i} A_j \\}$"}, {"title": null, "content": "where fenv (s, ai, a\u2212i) is the deterministic transition function that maps the current state s and the joint actions of all agents (ai, a\u2212i) to the next state s', Aj is the action space for each agent j and the Cartesian product \u03a0j\u2260i Aj represents all possible combinations of actions by the other agents. It is noted that we only use fenv to define the set Ss,ai here, but do not need to learn this global transition function directly in our algorithm. Encoding the deterministic transitions by a delta function, \u03b4fenv(s,a)(s'), Eq. (1) is rewritten as\n$Q_i(s, a_i) = E_{s'\\sim \\delta_{f_{env}(s,a_i)}(s')}[R(s,s') + \\gamma \\max_{a'} Q_i(s', a')]$\n$ = \\max_{s' \\in S_{s, a_i}^*}\\{R(s, s') + \\gamma \\max_{a'} Q_i(s', a')\\}$"}, {"title": null, "content": "where Sa, is any subset of Ss,ai, including $s'^* = f_{env}(s, a_i, \\pi_{-i}^*(s, a_i))$.\nBy reformulating the Q-value optimization over the set $S_{s, a_i}^*$, our approach allows for targeting the optimal value Qi\u2217(s, ai) under the true coordinated joint behavior, without directly approximating s\u2032\u2217. When there is no information about s\u2032\u2217, the set $S_{s, a_i}^*$ could be set in principle to Ss,ai, if it were known, to ensure the inclusion of s\u2032\u2217. However, this will also make the maximisation over $S_{s, a_i}^*$ in Eq. (3b) more computationally challenging, implying a trade off between reducing the"}, {"title": null, "content": "size of $S_{s, a_i}^*$, and ensuring the inclusion of s\u2032\u2217. We propose a learning procedure that enables each agent to progressively shrink their set of next states $S_{s, a_i}^*$, as all agents explore and accumulate experiences.\nIn practice, at the algorithmic step t, we work with a subset $\\hat{S}_{s, a_i, t}$ which approximates one of the possible subsets $S_{s, a_i}^*$. Since neither Ss,ai nor s\u2032\u2217 are known in practice due to the incomplete information about other agents' policies and the environment dynamics, we cannot guarantee that s\u2032\u2217 \u2208 $\\hat{S}_{s, a_i, t}$ \u2286 Ss,ai holds, but we will show in our performance assessment that s\u2032\u2217 \u2208 $\\hat{S}_{s, a_i, t}$ holds with high probability. Furthermore, direct maximization over $\\hat{S}_{s, a_i, t}$ is challenging as this set is infinite in general.\nTo address this, we resort to Monte Carlo optimization, as in e.g. Robert et al. [1999], by introducing a finite set $M_{s, a_i, t}$ of M points randomly sampled from $\\hat{S}_{s, a_i, t}$. Assuming no approximation error in the predicted bound, that is $\\hat{S}_{s, a_i, t}$ \u2286 Ss,ai, holds, the sample set $M_{s, a_i, t}$ contains only reachable states and it follows that\n$Q_i^*(s, a_i) \\geq \\max_{s' \\in M_{s, a_i, t}}[R(s,s') + \\gamma \\max_{a'} Q_i^*(s', a')]$\n$\\max_{m \\in\\{1,...,M\\}}[R(s,s_m) + \\gamma \\max_{a'} \\max_{a'} Q_i(s_m, a')]$"}, {"title": null, "content": "With the considered approach, there are two natural phases when running the associated algorithms:\n1. With little information to rely on, the agents explore the state space at random and collect diverse trajectories, which improve their understanding of the range of possible next state Ss,ai. In this phase, the estimated sets $\\hat{S}_{s, a_i, t}$ will be close to Ss,ai.\n2. As agents refine their estimates of the set Ss,ai of possible next states and accumulate sufficient reward information, the maximisation in Eq. (4b) yields increasingly stable values, facilitating policy convergence. This, in turns, means that the new trajectories will be more similar and optimised, progressively outnumbering the initial diverse trajectories. This will cause the sets $\\hat{S}_{s, a_i, t}$ to zero in on s\u2032\u2217, hence facilitating the Monte Carlo optimisation in Eq. (4b).\nAn illustration of our sampling and selection process is shown in Figure 1: given a set of possible next state samples, our algorithm selects the state with the highest estimated Q-value for updating, which implicitly indicating the optimal action of other agents. As agents explore more possible actions, the estimated set $\\hat{S}_{s, a_i, t}$ increasingly approximates the true set Ss,ai. Crucially, if the"}, {"title": null, "content": "optimal next state is contained within the estimated set, the equality\n$Q_i^*(s, a_i) = \\max_{s'\\in \\hat{S}_{s,a_i,t}}[R(s, s') + \\gamma \\max_{a'} Q_i^*(s', a')]$\nholds. This property is fundamental to our method, as it implies that through iterative learning and effective sampling, each agent can learn Q-values that closely align with those derived from ideal transition probabilities. In the following section, we analyse the convergence properties of this approach under ideal conditions. The complete algorithm, including implementation details, will be presented in Section 4.3."}, {"title": "4.2 Convergence analysis", "content": "Our combined learning and sampling approach facilitates the gradual convergence of the agents' policies toward the globally-optimal joint policy. This gradual convergence is supported by the insights from the following theorem, which shows the disparity between the optimal Q-values and those learned by the agents is limited by the difference between the best next state in the estimated set and the true best next state. This bounding relationship is crucial, as it indicates that the closer our estimated set of next states is to the actual set composed of all the possible next states, the more accurate the estimations of the agents' optimal Q-functions become.\nIn this section, we further elaborate on this mechanism and provide a formal convergence analysis. This analysis demonstrates how our proposed model-based Q-learning approach, combined with the sampling strategy, effectively facilitates convergence to an optimal global policy. We begin by showing that the difference between the Q-values learned by our approach and the optimal Q-values depends on how well we can estimate the best next state.\nTheorem 4.1. Let Ss,ai be the set of all possible next states as defined in (2) and let \u015c be a non-empty subset of Ss,ai. Let s\u2032\u2217 and \u015d\u2032\u2217 represent the best next states in the optimal and approximate regimes, respectively, that is\ns\u2032\u2217 = arg maxs\u2032\u2208Ss,ai R(s, s\u2032) + \u03b3 maxa Qi(s\u2032, a)\n\u015d\u2032\u2217 = arg maxs\u2032\u2208\u015c R(s, s\u2032) + \u03b3 maxa Qi(s\u2032, a)\nUnder Assumptions A.1-A.3 (see Appendix B), if the Euclidean distance d(s\u2032\u2217, \u015d\u2032\u2217) is at most \u03f5 for all (s,ai), then there exists K > 0 such that |Qi(s, ai) \u2013 Q(s, ai)| \u2264 (1 \u2212 \u03b3)\u22121K\u03f5 for all (s, ai).\nIn the context of our algorithm, we can relate this theorem to our specific implementation. Omitting the algorithm step t from the notations for simplicity, \u015c in our case corresponds to $S_{s, a_i}^M$. This set consists of M states uniformly sampled from $\\hat{S}_{s, a_i}$, which is itself a non-empty subset of Ss, ai. $\\hat{S}_{s,a_i}$ is formed by all possible outcomes predicted by our learned model. \nThis result demonstrates that if the distance between the estimated best next state \u015d\u2032\u2217 and the actual best next state s\u2032\u2217 is arbitrarily small for all state-action pairs (s, ai), then the discrepancy between the learned Q-values Qi(s, ai) and the optimal Q-values Q(s, ai) is bounded. This implies that as we refine our estimation of the optimal next state through iterative sampling and learning, we progressively narrow the gap between the learned Q-values of our agents and the true optimal values.\nTo better analyse the distance d(s\u2032, \u015d\u2032), we introduce a third state s\u2032 as the best next state in $\\hat{S}_{s,a_i}$. This allows us to use the triangle inequality to upper bound the distance as d(s\u2032, \u015d\u2032) \u2264 d(s\u2032, s\u2032) + d(s\u2032, \u015d\u2032), \nThe first term, d(s\u2032, s\u2032), reflects the difference between the optimal next state in Ss,ai and the one in $\\hat{S}_{s,a_i}$. As the size of $\\hat{S}_{s,a_i}$ increases in the first phase of the algorithm, it's more likely to include states closer to s\u2032, thus decreasing d(s\u2032, s\u2032). The second term, d(s\u2032, \u015d\u2032), represents the error"}, {"title": null, "content": "in the Monte Carlo optimization. This error tends to be large initially but decreases in the second phase as $\\hat{S}_{s,a_i}$ shrinks, making it easier to sample states close to s\u2032. This analysis shows how our algorithm progressively improves its estimation of the optimal next state, contributing to the overall convergence of the Q-values.\nTheorem 4.2. Assume that S = R and that $\\hat{S}_{s,a_i}$ is of the form [\u2212u, u] for some u \u2208 (0,\u221e). Consider M i.i.d. samples $s'_1,..., s'_M$ from the uniform distribution on [\u2212u, u]. It holds that\n$E[\\min_{k=1,..., M}|s'* - s'_k|] < \\frac{2u}{M+1}$"}, {"title": null, "content": "This result demonstrates that the Monte Carlo optimization error d(s\u2032, \u015d\u2032) diminishes as the number of samples M increases. Incidentally, the Monte Carlo optimisation error could exhibit exponential dependence as the dimensionality of the state space increases. However, in multi-agent scenarios, partially-observed MDPs typically limit effective dimensionality growth, as agents rely on a restricted view of the overall state space.\nSeemingly, there is an inherent trade-off involved in expanding the state set $\\hat{S}_{s,a_i}$, i.e., expanding u in the above one-dimension case, to cover more possibilities while managing the resultant error. A broader Ss,ai reduces the gap between s\u2032 and the true best state s\u2032\u2217, as it increases the likelihood of encompassing s\u2032\u2217. This action effectively shrinks the error term d(s\u2032, s\u2032). Yet, increasing the size of Ss,ai, i.e., increasing u, also typically increases variability, leading to a larger error d(s\u2032, \u015d\u2032) and necessitating more samples to maintain a given level of precision."}, {"title": "4.3 Implementation details", "content": "To capture the range of possible next states, our implementation utilises two non-parametrised quantile models, $g_l^i$ and $g_u^i$, which employ neural networks to predict the \u03c4l = 0.05 and \u03c4u = 0.95 quantiles for each dimension of the next state. The neural network parameters, denoted by \u03c6i and \u03c8i, are learnt by minimising the quantile loss according to their respective \u03c4 values over samples from individual replay buffer Di:\n$\\mathcal{L}^{(i)} = E_{s, a_i \\sim D_i}[\\mathcal{L}^\\tau(g_l^i(s, a_i; \\phi_i) - s')]$"}, {"title": null, "content": "where $\\mathcal{L}^\\tau(u) = I(u > 0) \\tau u + I(u < 0)(1 - \\tau)u$. For each (s,ai) pair, the two quantile models predict bounds $[g_l^i(s, a_i), g_u^i(s, a_i)]$. We then construct the potential next state set \u015c by including the true s' and M samples drawn from the quantile bounds. We also explored a parametrised multivariate Gaussian model as another method to estimate the possible next states"}, {"title": "5 Experimental results", "content": null}, {"title": "5.1 Environments", "content": "We evaluated the MMQ algorithm in three types of cooperative MARL environments characterized by the need for complex coordination among agents\nDifferential Games We adapted this environment from Jiang and Lu [2022], where N agents move within the range [-1,1]. At each time step, an agent indexed by i selects an action ai \u2208 [\u22121,1]. The state of this agent then transitions to clip{xi + 0.1 \u00d7 ai, \u22121,1}, where xi is the previous state and the clip(y, ymin, ymax) function restricts y within [ymin, ymax]. The global state is the position vector (x1,x2). The reward function, detailed in Appendix A, assigns rewards following each action. A narrow optimal reward region is centred, surrounded by a wide zero-reward area and suboptimal rewards at the edges. This setup can lead to RO problems as agents might prefer staying in larger suboptimal areas.\nMultiple Particle Environment We designed six variants of cooperative navigation tasks with RO rewards as shown in Figure 3. The common goal is for two disk-shaped agents, D1 and D2, to simultaneously reach a disk-shaped target. To encourage coordination, we introduce a penalty for scenarios where only one agent is within a certain distance from the target. Specifically, we define a disk D centered on the target with radius rp and penalize agents if only one is within D. This setup is designed to illustrate the RO problem, where agents might prefer staying outside D rather than risk being the only one inside it. The task difficulty increases as the radius rp decreases. The"}, {"title": null, "content": "reward function, designed to reflect the RO problem, is defined as:\nRin if Di \u2229 D != 0, i = 1,2\n\u03c4CN =\nRout if (D1 \u222a D2) \u2229 D = 0\nRout \u2212 p otherwise.\nThe rewards for the three cases should satisfy Rout \u2212 p < Rout < Rin. Detailed descriptions of Rout and Rin for different settings are provided in Appendix A. In task CN, the penalty for solo entry into the circle is p = 0.2; in task CN+More Penalty, the penalty increases to p = 0.5 for entering the circle alone; in task CN+Heterogeneous Agents (HA), two agents performing the CN task are heterogeneous, having different sizes and velocities; in task CN+Heterogeneous Targets (HT), there are two targets, where entering the circle of target A follows the previous RO design, and entering the circle of target B incurs no RO penalty but offers a reward lower than Rin; in the sub-optimal scenario, agents might only enter the circle of target B; in task Sequential Task, agents must coordinate over a longer period they could either directly reach target A with the same RO reward as before or first reach target B to pick up cargo, then receive a bonus each step (a higher Rin) when they later enter target A together; in task Predator-Prey (PP), two predators (which we control) and one prey, who interact in an environment with two obstacles. The prey, trained using MADDPG [Lowe et al., 2017], is adept at escaping faster than the predators. The predators need to enter the prey's disk together to receive the reward Rin.\nMulti-agent MuJoCo Environment We employ the Half-Cheetah 2x3 scenario from the Multi-agent MuJoCo framework [de Witt et al., 2020]. This environment features two agents, each controlling three joints of the Half-Cheetah robot via torque application. It presents a partial observability setting, with each agent accessing only its local observations. We implement an RO reward structure designed to necessitate high coordination between agents. The reward function ru is defined as: \u22127 if v < v\u0131, \u22129 if v\u0131 \u2264 v < vu, and \u22122, otherwise; where v\u0131 = 0.035/dt, vu = 0.04/dt, and dt = 0.05. This structure rewards agents for moving forward at speeds exceeding vu, penalizes speeds between v\u0131 and vu, and provides a moderate penalty for very low speeds. Agents failing to overcome the RO problem may settle for maintaining low speeds to avoid the harshest penalty."}, {"title": "5.2 Baselines", "content": "Our benchmarks include comparisons with three baseline algorithms: Ideal Independent Q-Learning (I2Q), Independent Deep Deterministic Policy Gradient (IDDPG), and Hysteretic DDPG (HyD-"}, {"title": null, "content": "DPG). We differentiate between two versions of I2Q: the original implementation by Jiang and Lu [2022], which we refer to as I2Q\u2217, involves multiple updates of all network components after every 50 interaction steps. In contrast, our implementation, denoted as I2Q, updates only the critic network multiple times every 50 steps. This distinction allows us to more accurately assess the performance impact of these differing update strategies."}, {"title": "5.3 Experiment Results", "content": "To compare the performance among different baselines, we consider the mean episode return over 8 seeds, where the episode return is defined as the accumulated reward \u03a3Tt=0 r for the whole episode"}]}