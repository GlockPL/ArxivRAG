{"title": "DREAMING IS ALL YOU NEED", "authors": ["Mingze Ni", "Wei Liu"], "abstract": "In classification tasks, achieving a harmonious balance between exploration and precision is of paramount importance. To this end, this research introduces two novel deep learning models, SleepNet and DreamNet, to strike this balance. SleepNet seamlessly integrates supervised learning with unsupervised \"sleep\" stages using pre-trained encoder models. Dedicated neurons within SleepNet are embedded in these unsupervised features, forming intermittent \"sleep\" blocks that facilitate exploratory learning. Building upon the foundation of SleepNet, DreamNet employs full encoder-decoder frameworks to reconstruct the hidden states, mimicking the human \"dreaming\" process. This reconstruction process enables further exploration and refinement of the learned representations. Moreover, the principle ideas of our SleepNet and DreamNet are generic and can be applied to both computer vision and natural language processing downstream tasks. Through extensive empirical evaluations on diverse image and text datasets, SleepNet and DreanNet have demonstrated superior performance compared to state-of-the-art models, showcasing the strengths of unsupervised exploration and supervised precision afforded by our innovative approaches.", "sections": [{"title": "1 Introduction", "content": "In the current digital age, the ability to accurately classify large datasets has become of paramount importance across a myriad of fields, including computer vision (CV) [26, 37, 38, 11], natural language processing (NLP) [30, 33, 8, 4], bioinformatics [21], etc. The blossoming of artificial intelligence and deep learning has greatly facilitated the handling of complex classification tasks. Deep learning's capacity to sift through multitudes of variables, discern patterns, and extract key features has led to impressive breakthroughs in numerous applications, from image recognition and voice recognition to disease prediction [20].\n\nThe groundbreaking convolutional neural networks (ConvNets), such as ResNet[16] and EfficientNet[39], have emerged as dominant architectures in computer vision, with ResNet addressing the vanishing gradient issue through deep residual networks and enabling deeper models without performance loss, while EfficientNet introduced a compound scaling method that scales depth, width, and resolution, enhancing both efficiency and accuracy. These models have set new benchmarks across various datasets and have been pivotal in applications such as autonomous driving and advanced image recognition, reshaping how machines interpret visual data. Meanwhile, the success of pre-trained unsupervised Transformers [41] like ViT [11] for vision tasks and BERT [8] has shown that using primarily standard Transformer layers can achieve significant performance in downstream applications, reaching levels comparable to previous state-of-the-art neural networks and suggesting that Transformers may offer greater scalability across diverse domains.\n\nTransformers have demonstrated superior model capabilities but often suffer from poor generalization when compared to chain-like networks due to a lack of appropriate inductive bias [42]. Recent research has focused on hybrid methods that combine the structures of both to retain their respective advantages [10, 42, 7, 19]. For example, Convolution Vision Transformers (CvT) [42] enhance performance by integrating convolutional token embedding and convolutional transformer blocks with convolutional projection, aiming for improved accuracy and efficiency. Similarly,"}, {"title": "2 Related Work", "content": "This section introduces two topics: (1) chain-like structures and transferomers, and (2) the biological aspects of dreams and sleep for memory consolidation."}, {"title": "2.1 Chain-like Structure and Transformers", "content": "Deep learning has been significantly advanced with the development of supervised learning models like ResNet [16], MobileNet [18], and TextCNN [22], which excel in multiple tasks across image processing, object detection, and natural language processing. These models share a common architecture: chain-like structures where components are sequentially linked, facilitating a systematic enhancement of feature extraction. For instance, ResNet utilizes residual blocks connected by skip connections to tackle the vanishing gradient problem in deep networks, while MobileNet is optimized for mobile environments with streamlined CNN structures. TextCNN revolutionizes text classification by applying convolutional layers directly to text, capturing various n-gram features with global max-pooling. Although highly effective in recognizing patterns from vast labeled datasets, this reliance on extensive labeled data can limit their adaptability and generalization in new, less structured environments.\n\nUnsupervised learning models, such as Variational Autoencoders (VAEs) [24] and Generative Adversarial Networks (GANs) [14], play a key role in understanding the inherent structure and distribution of data without explicit labels,"}, {"title": "2.2 Biological dream", "content": "Sleep and dreams significantly aid in learning and the consolidation of knowledge through various mechanisms. During sleep, especially in the deep stages of REM sleep, the brain actively consolidates new information, transferring it from short-term to long-term memory for better recall later [34]. This period of rest also facilitates synaptic pruning, where the brain refines neural connections to enhance efficiency and retain important information while discarding the less useful. Additionally, REM sleep, which is often associated with vivid dreaming, fosters creativity and problem-solving by enabling the brain to form novel connections between disparate ideas [13]. This stage is also crucial for emotional processing, helping integrate and understand experiences and knowledge on a deeper level. Fig 1 illustrates the role of sleep in memory formation for performance boosting, emphasizing three phases: encoding, consolidation, and recall. During sleep, particularly REM, the brain facilitates memory consolidation by transferring information between"}, {"title": "3 Methodology", "content": "In this section, we provide details of our proposed innovative deep learning architectures, SleepNet and DreamNet."}, {"title": "3.1 Problem Setting", "content": "In general, deep neural networks are constructed by connecting many weight matrices and nonlinear operators. In this paper, we consider a chain-like neural network constructed by stacking similar deep neural blocks, such as Multilayer Perceptron, stacked LSTMs and ResNet. Let $D = {(X_1,Y_1), (X_2,Y_2), ..., (X_n, Y_n)}$ be a dataset consisting of n samples, where x and y represent the input text and its corresponding class, respectively. Given a chain classifier with M layers, $\u0421(\u0445) = (g_M \\circ g_{M-1} \\circ ... \\circ g_2 \\circ g_1) (x)$ maps from the text space X to the k classes, where $g_m(\\cdot)$ is the mth neural blocks. The output of the mth block is $h_m(x) = g_m(h_{m-1}(x))$, and the neural blocks can be similar blocks constructed by fully connected layers, CNN, LSTM, pooling and normalizing layers with the activation functions. To construct a SleepNet, we also need a pre-trained and self-supervised autoencoder model $P = \\theta(\\phi(x))$ where $\\theta(\\cdot)$ and $\\phi(\\cdot)$ are the encoder and decoder, respectively. The hidden state from the latent space is $a = \\phi(x)$."}, {"title": "3.2 SleepNet", "content": "The proposed method, SleepNet, incorporates a pre-trained unsupervised encoder to feed input and hidden states, producing enriched encodings for subsequent layers. As we shall explain in this section, this fusion of information inside a single architecture innovatively borrows the concept of sleep functions in human cognition and applies it to machine learning tasks."}, {"title": "3.2.1 Biological Inspiration: Sleep Consolidation", "content": "Research has highlighted the crucial role of sleep in memory consolidation and the formation of long-term memories [34, 13]. During sleep, the brain actively replays and reinforces the connections between neurons that were activated during waking experiences, leading to the stabilization and integration of newly acquired information into existing knowledge structures [34]. Drawing inspiration from this biological process, we equate sleep to unsupervised learning, suggesting its potential to amplify and consolidate the knowledge gained through supervised tasks.\n\nEncoder-decoder architectures, such as autoencoders, have been widely used in applications like machine translation and image segmentation. In this unsupervised approach, the encoder captures the essential features of the input data and maps them into a lower-dimensional latent space representation while the decoder attempts to reconstruct the original input from this compressed latent representation. The goal is to match the reconstructed output as closely as possible to the initial input without using any labeled data. By incorporating an unsupervised \u201csleep\" phase inspired by the memory consolidation process during biological sleep, we hypothesize that our model can leverage the autoencoder's ability to extract and consolidate meaningful features from the input data, thereby enhancing the performance of the subsequent supervised learning tasks. This integration of unsupervised and supervised learning components within a"}, {"title": "3.2.2 Sleep Connection", "content": "Based on sleep consolidation, the SleepNet is designed to harness such strengths of unsupervised learning into supervised learning models, consolidating them into one cohesive method. Its unique attribute incorporates a self-supervised encoder $\\phi(\\cdot)$, pre-trained on an unlabeled dataset based on the autoencoder framework. The reason for only utilizing the encoder is that a well-trained encoder is expected to transform more features to the input [44], and excluding the decoder will reduce the complexity of the proposed method. We contend that such an encoder setup amplifies feature extraction, easing the learning process.\n\nSleepNet uses a pre-trained encoder $\\phi$ to process hidden states $h_i$, forwarding encoded features to subsequent blocks through a mechanism that we call \"sleep connection.\" This approach harnesses latent features for the supervised learning process. \"Sleep blocks\" interspersed within the model bridge the output of supervised blocks $h_m = g_m(h_{m-1})$ with encoded features $\\phi(h_{m-1})$, enhancing the integration of pre-trained insights into the learning sequence. The \u201csleep block\" $s(\\cdot)$ is mathematically expressed as:\n\n$s_m(h_m) = g_m(h_{m-1}) + \\phi(h_{m-1}), \\tag{1}$\n\nwith $h_{m-1}$ denoting the output from the mth sleep block. The supervised blocks capture local details, whereas encoder $\\phi$ elevates local to non-local information, facilitating enhanced feature fusion.\n\nWe devise two versions of SleepNet, one for computer vision (CV) tasks and the other for natural language processing (NLP) tasks."}, {"title": "3.2.3 SleepNet for CV models", "content": "In computer vision applications, SleepNet begins by processing an input image through convolutional (\u201cCov\") layers to extract preliminary features, which are then normalized (\u201cNorm\") and activated through ReLU functions to prepare them for further processing. The core of SleepNet consists of sleep blocks, each consisting of a sleep connection and chain-like blocks. The sleep connection (Block \u2460 in Fig 2) adjusts the feature dimensions via deconvolutional (\u201cDecov\") and convolutional layers to match the fixed input size required by the typically pre-trained encoder, $\\phi(\\cdot)$. This is essential because initial processing often reduces input dimensionality, potentially mismatching the encoder's specifications. After dimension adjustment and feature enhancement by the encoder, further convolutional and normalization layers refine and compress the features. Simultaneously, the initial output is processed through chain-like blocks for independent feature extraction. Outputs from both pathways are then merged by simple addition, repeating across multiple sleep blocks to progressively enhance features. This integrated workflow in Fig 2 culminates in a dense layer, leading to a softmax classifier for final image classification, effectively combining supervised and unsupervised learning to improve prediction accuracy."}, {"title": "3.2.4 SleepNet for NLP models", "content": "In natural language processing applications, SleepNet initiates its process by embedding input text using layers, such as ELMO [33] or word2Vec [30], which transform raw texts into meaningful vector representations that capture semantic properties. Following embedding, the text progresses through chain-like blocks composed of LSTM units, adept at maintaining context over long sequences. Simultaneously, a sleep connection (Block) in Fig 3) within each sleep block utilizes softmax and argmax functions to prepare sequences for the pre-trained encoder, $\\phi(\\cdot)$, enhancing feature representation. This encoder adjusts and enriches the LSTM-processed data, effectively integrating supervised learning from the LSTM blocks with unsupervised learning from the sleep connection. Outputs from both the sleep connection and LSTM blocks are merged through simple addition, repeating this integration across multiple blocks to refine text representation iteratively. The workflow in Fig 3 culminates in a dense layer followed by a softmax classifier, which categorizes the text into predefined classes, leveraging both learning types to enhance the model's effectiveness in various NLP tasks."}, {"title": "3.3 DreamNet", "content": "Building on top of SleepNet, our DreamNet is devised to simulate biological dreams during sleep. It enhances existing sleep connections with a novel \u201cdream connection\", utilizing a full autoencoder setup. Additionally, DreamNet learns augmented features by reconstructing hidden states using an autoencoder, akin to how humans draw inspiration from recalled dreams."}, {"title": "3.3.1 Dreams in Cognitive Models", "content": "Not only does sleep help the consolidation, but dreams also play a crucial role in brain function by aiding in memory consolidation, emotional processing, and problem-solving [2]. They help integrate new memories with existing knowledge during REM sleep, providing a means for the brain to process and manage emotions effectively [9]. This nightly mental activity also stimulates creative thinking and problem-solving abilities by recombining information in novel ways. Essentially, dreaming acts as a form of neural maintenance, keeping the brain flexible and prepared for new challenges, which can be seen as a form of overnight psychological therapy [3]."}, {"title": "3.3.2 Dream Connection", "content": "We propose a \u201cdream connection\" to enhance the previously proposed \"sleep connection\" through two innovative modifications. Firstly, we replace the encoder $\\phi(\u00b7)$ with a complete pre-trained autoencoder $\\theta(\\phi(x))$. We anticipate that utilizing the full autoencoder will allow for a more comprehensive integration of information into the subsequent block. The mathematical illustration of a \"dream connection\" is as follows:\n\n$s_m(h_m) = g_m(h_{m-1}) + \\theta(\\phi(h_{m-1})), \\tag{2}$\n\nwhere $\\phi(\u00b7)$ and $\\theta(\u00b7)$ are encoder and decoder, respectively. Secondly, the output (\u201cdream\") generated by the autoencoder $\\theta(\\phi(x))$ is not only transferred to the subsequent block but also directed to a parallel block for a deeper analysis of these reconstructed features. In the final stage, the data from the 'dreams' and the chain-like module is combined before being processed through fully connected layers, ultimately leading to a softmax layer for predictions.\n\nDreamNet is expected to achieve better performance by utilizing the pre-trained autoencoder for feature augmentation, which is theoretically and practically different from traditional data augmentation. Specifically, conventional aug-mentation methods, such as RandAugment [6] and TextAttack [31], typically involve crude manipulations on the raw dataset. While these methods can generate similar data, they may also introduce unnecessary noise. In contrast, feature augmentation through \u201cdreaming\" is a more principled approach that leverages the model's architecture. Practically, data augmentation and feature augmentation can be used complementarily within the same model, where data augmentation operates on the raw data, while our proposed feature augmentations with \"dreams\" are based on enhancing the model's internal representations.\n\nLike SleepNet, we design two variations of DreamNets, one for CV tasks and the other for NLP tasks."}, {"title": "3.3.3 DreamNet for CV Models", "content": "DreamNet enhances SleepNet's architecture for computer vision by incorporating a \u201cdream connection\" that employs a comprehensive autoencoder for advanced feature consolidation, as illustrated in Fig 4. Unlike SleepNet, DreamNet introduces \"Dream Blocks\" which not only include standard chain-like blocks for initial feature extraction but also feature an advanced dream connection setup (block 2 in Fig 4). This dream connection uses a full autoencoder that first encodes the feature data to a latent, compressed representation and then decodes it, effectively reconstructing and enhancing the image data to simulate cognitive dreaming processes. This reconstructed output undergoes further refinement in block 3 in Fig 4, where additional convolutional and normalization layers refine these features to ensure robust feature extraction. This continuous cycle of encoding, decoding, and refinement significantly bolsters the model's pattern recognition capabilities. The workflow culminates in a dense layer followed by a softmax classifier, ensuring precise classification by effectively leveraging the enhanced feature set. To give a more valid illustration, we present Fig 6 by plotting the original image and the generated dream-like images."}, {"title": "3.3.4 DreamNet for Textual Models", "content": "In its application to natural language processing tasks, DreamNet expands upon SleepNet by integrating a \"dream connection\" that utilizes a full autoencoder, as detailed in Fig 5. The process begins with the LSTM units, which initially process the text data to capture contextual dependencies. The output from these blocks is then introduced into the dream connection (block \u2461 in Fig 5), where it is first encoded into a latent space and subsequently decoded, enhancing the sequence with deeper insights and contextual understanding. This enriched sequence from the autoencoder is not only refined but also combined with outputs from other processing blocks in 3 in Fig 5. This integration stage involves additional LSTM processing to further enhance sequence quality and integrate the various feature enhancements comprehensively. The final output, a richly enhanced and consolidated feature set, then moves to a dense layer and"}, {"title": "4 Experiments", "content": "We evaluate our models using public datasets and strong baselines. Our code and data are on GitHub\u00b9."}, {"title": "4.1 Experimental Settings", "content": "To rigorously assess the efficacy of SleepNet and DreamNet, we test these models in both CV and NLP tasks. For CV tasks, we utilized datasets CIFAR100[25], ImageNet-tiny [27], and ImageNet 1K [35]. For NLP tasks, we employed datasets AG News [47], IMDB [29], and Yelp [48]. To facilitate a fair and detailed comparison, we varied the number of sleep/dream blocks (M) from 1 to 4. Specifically, SleepNet-1 denotes a network configuration with one sleep block,"}, {"title": "4.2 Augmentation Methods", "content": "Data augmentation techniques are crucial for enhancing model performance in computer vision tasks and are widely applied to state-of-the-art visual models. To ensure robust comparisons, we incorporated two powerful data augmentation methods, namely RandAugment [6] and Mixup [46], into our visual models and the baselines. RandAugment systematically searches for the best augmentation policies by randomly applying a fixed number of distortions, optimizing the augmentation strategy for better generalization. Mixup, on the other hand, creates new training examples by combining pairs of images and their labels, encouraging models to behave linearly in between training examples and improving robustness against adversarial examples.\n\nIn natural language processing (NLP), language models are typically trained with augmentation techniques from methods such as the TextAttack [31] framework, since the original datasets may not be sufficiently large to achieve reasonable performance on their own [5]. Specifically, TextAttack augmentation employs a combination of word swaps, insertions, and substitutions to generate new examples. In our setting, we augmented the text data (for all models in comparison) by editing 10% of the words in each instance per augmentation, effectively doubling the size of the training set with these new variations. This approach ensures that the models are exposed to a variety of linguistic variations, enhancing their ability to generalize and perform well on diverse text inputs."}, {"title": "4.3 Datasets and Metrics", "content": "In this subsection, we describe the datasets and metrics used in our experiments for visual and language tasks, and specify the performance evaluation metrics."}, {"title": "4.4 Baselines", "content": "During the evaluation of our models, we benchmarked their performance against various state-of-the-art models to ensure comprehensive comparisons. For vision-focused tasks, we utilized:"}, {"title": "4.5 Main Results and Analysis", "content": "The main results of the experiments are presented in Tables 3 and 4. In the evaluation of computer vision tasks, vision transformers, particularly MAE-large, demonstrated strong performance, achieving 91.3% on CIFAR100 and 87.1% on ImageNet-tiny. Hybrid models, such as CvT-W24 and CoAtNet-3, also performed impressively across all datasets, highlighting the benefits of integrating convolutional and attention mechanisms. Notably, DreamNet-3MAE-1 achieved the highest accuracy on CIFAR100 (93.4%) and Tiny-ImageNet (89.6%), while DreamNet-4 recorded the best results on ImageNet 1K (89.2%). Additionally, SleepNet consistently outperformed the baselines, securing the second-best results and displaying a performance closely competitive with that of DreamNet.\n\nThe superior performance of DreamNet over SleepNet and other models can be attributed to several factors. The \"dream\" cycles in DreamNet refine and consolidate features learned during training, enhancing the model's ability to generalize to new data. This mechanism allows DreamNet-3 to achieve the highest accuracy on ImageNet 1K (87.8%) and CIFAR100 (92.3%). Additionally, the use of pre-trained self-supervised encoders in both SleepNet and DreamNet provides a strong initialization, further improving their feature extraction capabilities. SleepNet-3MAE-1 set a new benchmark on CIFAR100 with an accuracy of 93.4%, highlighting the significant improvements brought by the \"dream\" mechanism in enhancing learning and generalization.\n\nFor natural language processing tasks, both SleepNet and DreamNet surpassed these baselines. Specifically, DreamNet achieved the highest performance across all datasets, recording 94.4% on AG News with DreamNet-4, 95.1% on IMDB with SleepNet-3XLNet-1, and 97.9% on Yelp with SleepNet-3ROBERTa-1\u00b7 Additionally, SleepNet consistently outperformed other models, securing the second-best results and displaying a performance that was closely competitive with that of DreamNet."}, {"title": "4.6 Ablation Studies", "content": "Since DreamNet is built on top of SleepNet, in our ablation studies, we begin by comparing SleepNet and DreamNet models and analyzing their individual advantages. Next, we shall explore alternative chain-like blocks and pre-trained encoders/autoencoders. Then, we shall test whether the parameters of the pre-trained models should be frozen."}, {"title": "4.6.1 Performance comparison between DreamNet and SleepNet.", "content": "In both CV and NLP tasks, our experiments were documented with results presented in Tables 3 and 4. The findings demonstrate that DreamNet consistently outperformed SleepNet across various datasets. These results highlight DreamNet's consistent edge in both visual and textual data tasks.\n\nThe superior performance of DreamNet can be attributed to several key factors. Firstly, the \"dream\" cycles in DreamNet play a crucial role in refining and consolidating features learned during training. This iterative refinement allows DreamNet to capture more intricate patterns and improve its generalization capabilities. Secondly, DreamNet leverages pre-trained self-supervised encoders, which provide a robust initialization and enhance feature extraction. This strong foundation significantly boosts the model's ability to learn from complex datasets. Additionally, the integration of \"dream\" cycles enables DreamNet to iteratively refine features, leading to superior performance on both CV and NLP tasks. The advanced \"dream\" connection mechanism, as seen in models like DreamNet-3ROBERTa-1, further enhances feature learning and generalization, enabling DreamNet to consistently outperform SleepNet and other baseline models."}, {"title": "4.6.2 Comparisons with different numbers of sleep/dream blocks", "content": "We investigate the impact of varying the number of sleep and dream blocks on the performance of SleepNet and DreamNet. As depicted in Tables 3 and 4, there is a noticeable trend in the performance improvement with the increase in the number of sleep and dream blocks. For CV tasks, models with more blocks generally achieved higher accuracy."}, {"title": "4.6.3 Comparisons with different chain-like models.", "content": "We evaluated the efficacy of various supervised models using three distinct text-based chain-like blocks: two-layer LSTMs, CharCNN, and WordCNN. CharCNN and WordCNN, variants of TextCNN, differ primarily in their tokenization units. For the vision tasks, we selected ResNet18, ResNet34, and ResNet50, which vary in the number of convolutional layers. Fig 7 summarizes their performance on the AG News dataset for text tasks and CIFAR100 for vision tasks. There is a clear trend that shows that the better the performance of the chain-like block, the better the subsequent performance of the proposed models (SleepNet and DreamNet). This correlation suggests that stronger initial performance from a classifier enhances its subsequent integration into SleepNet, likely due to the foundational role of chain-like blocks in the models."}, {"title": "4.6.4 Comparisons with different unsupervised encoders and autoeconders.", "content": "We assessed the performance of SleepNet and DreamNet by utilizing various unsupervised encoders and autoencoders, as depicted in Fig 8. Incorporating more sophisticated encoders consistently improved results: employing RoBERTa-Small with SleepNet increased accuracy from 0.86 to 0.90, and with DreamNet, it reached 0.92. Similarly, ViT-Small improved from 0.70 to 0.78 with SleepNet and to 0.91 with DreamNet. This trend demonstrates that the more advanced the encoder, the better the performance of the proposed methods. The primary reason for this improvement is that sophisticated encoders provide richer feature representations, enhancing the learning process. SleepNet benefits from the additional feature extraction capabilities, while DreamNet's dream cycles further refine and consolidate these features, leading to superior performance."}, {"title": "4.6.5 Comparisons with frozen and unfrozen unsupervised encoders/autoencoders.", "content": "Equally importantly, we examined the effect of freezing versus unfreezing the parameters of unsupervised encoders/autoencoders. This evaluation was carried out across three distinct tasks, employing varied models: TextCNN and BERT-based SleepNet for text classification on the AG News, and ResNet18 coupled with Google's ViT Base for image classification on CIFAR100. Fig 9 demonstrates that consistently across these varied tasks and models, the \"Frozing\" configuration (depicted in blue) outperforms the \u201cNon-Freezing\u201d one (shown in orange). For computer vision tasks, models with frozen encoders generally achieved higher accuracy. For instance, DreamNet with unfrozen MAE encoders reached 93% accuracy on CIFAR100, compared to 86% with frozen encoders. In the NLP domain, a similar trend was observed. On the AG News dataset, SleepNet-3 with unfrozen BERT encoders achieved an accuracy of 88%, outperforming the unfrozen encoder version, which attained 68%. These results demonstrate that frozen encoders generally provide better performance across both CV and NLP tasks.\n\nWe attribute this consistently superior performance of frozen encoders to two major reasons. Firstly, unfreezing the parameters during training may often tend to overfit the model. The overfitting can be traced back to finding an optimal learning rate for such a setup. More specifically, the supervised component of the model, which is not pre-trained, requires a larger learning rate to capture complex patterns effectively. In contrast, the pre-trained unsupervised part necessitates a lower learning rate to avoid drastic changes that can degrade the valuable pre-trained patterns. Balancing this diverse learning rate needs while unfreezing parameters is non-trivial and often leads to overfitting. Secondly, since the unsupervised models contribute additional features to the supervised models, any parameter alteration could modify these supplementary features to fit the specific dataset being processed. Although this might seem beneficial, it could inadvertently filter out some of the generalized, useful latent information that the unsupervised encoder initially captured, thereby limiting the model's overall ability to generalize across diverse datasets."}, {"title": "4.7 Complexity and Qualitative Results", "content": "We conducted our experiments on a RHEL 7.9 system equipped with an Intel(R) Xeon(R) Gold 6238R CPU, an NVIDIA Quadro RTX 5000 GPU, and 88GB RAM. Table 5 presents a comparison of the computational efficiency of different models, evaluated in terms of hours per epoch on the CIFAR100 and ImageNet-tiny datasets. ResNet18 was the most efficient, requiring only 0.45 hours per epoch on CIFAR100 and 6.77 hours per epoch on ImageNet-tiny. SleepNet also demonstrated good efficiency, surpassing ViT-B with 0.8 hours per epoch on CIFAR100 and 8.81 hours per epoch on ImageNet-tiny, compared to ViT-B's 1.04 and 10.60 hours, respectively. This shows that SleepNet is computationally optimized despite its advanced features.\n\nAnalyzing the FLOPs from Tables 3 and 4, ResNet18 had the lowest complexity with 1.8 billion FLOPs. In contrast, DreamNet-4 had 50.5 billion FLOPs, achieving higher accuracy (92.3% on CIFAR100 and 89.9% on ImageNet-tiny). For NLP tasks, DreamNet-4, with 42.1 billion FLOPs, achieved 94.4% accuracy on AG News and 92.3% on IMDB, outperforming models with fewer FLOPs. This illustrates the trade-off between computational complexity and performance, highlighting DreamNet's ability to leverage its higher complexity for superior accuracy.\n\nOverall, SleepNet and DreamNet effectively balance computational efficiency and performance. DreamNet, although more complex, provides notable accuracy improvements, making it suitable for tasks where performance is critical. The integration of sleep and dream blocks in both models demonstrates that added complexity translates into significant performance gains across computer vision and natural language processing tasks."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we introduced SleepNet and DreamNet, two innovative deep learning architectures inspired by cognitive processes in biological brains. These models use novel sleep and dream mechanisms to consolidate and refine features, leading to improved performance in both computer vision and natural language processing tasks. Our experiments show that these models outperform state-of-the-art results, highlighting their effectiveness and potential for general applicability.\n\nOur future work includes optimizing SleepNet and DreamNet for better efficiency and performance, integrating additional cognitive-inspired mechanisms, and applying these models to a broader range of tasks and datasets. We also plan to explore their potential in real-time applications and scalability to more complex tasks, aiming to develop more intelligent and efficient AI systems."}]}