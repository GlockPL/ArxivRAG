{"title": "Differential Adjusted Parity for Learning Fair Representations", "authors": ["Bucher Sahyouni", "Matthew Vowels", "Liqun Chen", "Simon Hadfield"], "abstract": "The development of fair and unbiased machine learning models remains an ongoing objective for researchers in the field of artificial intelligence. We introduce the Differential Adjusted Parity (DAP) loss to produce unbiased informative representations. It utilises a differentiable variant of the adjusted parity metric to create a unified objective function. By combining downstream task classification accuracy and its inconsistency across sensitive feature domains, it provides a single tool to increase performance and mitigate bias. A key element in this approach is the use of soft balanced accuracies. In contrast to previous non-adversarial approaches, DAP does not suffer a degeneracy where the metric is satisfied by performing equally poorly across all sensitive domains. It outperforms several adversarial models on downstream task accuracy and fairness in our analysis. Specifically, it improves the demographic parity, equalized odds and sensitive feature accuracy by as much as 22.5%, 44.1% and 40.1%, respectively, when compared to the best performing adversarial approaches on these metrics. Overall, the DAP loss and its associated metric can play a significant role in creating more fair machine learning models.", "sections": [{"title": "1 Introduction", "content": "As artificial intelligence (AI) and machine learning (ML) models become increasingly prevalent, the need for responsible, fair, and unbiased machine learning is critical [3]. Many machine learning models heavily rely on learned representations which distill complex data into compressed forms, capturing key patterns for efficient learning and prediction [4]. However, they may also encode sensitive attributes like gender or race, thus leading to biased decisions that can perpetuate societal inequities.\nThe recognition of these biases has spurred the development of debiasing techniques. These methods aim to remove sensitive information while maintaining task performance, a balance that is challenging to achieve. Among various approaches, adversarial training has become popular, introducing an adversarial model during training to learn sensitive features. This process, often a min-max game, where the main model competes with the adversarial model making sensitive features more difficult to"}, {"title": "2 Background and Related Work", "content": "Below we will first discuss modern adversarial techniques for debiasing. We will then cover the non-adversarial techniques which are more closely related to this paper. This is followed by a formalisation of the various definitions of fairness found in the literature."}, {"title": "2.1 Adversarial Debiasing", "content": "Adversarially Learned Fair Representations, ALFR (Edwards and Storkey, 2016), is one of the earliest models developed to reduce bias and learn fair informative representations. It utilises an adversary which tries to predict the sensitive feature from the representations. The autoencoder tries to make these predictions as difficult as possible. It employs task and reconstruction losses to ensure the representation are informative for the downstream task, and an additional sensitive feature loss to remove sensitive feature related information.\nThe Conditional Fair Representations, CFair, approach [21] stands as a seminal method aiming for accuracy parity. Within the confines of conventional fair adversarial networks, CFair augments the original adversarial constraints and adopts conditional error constraints.\nLearning Adversarially Fair and Transferable Representation, LAFTR [16], is similar to CFAIR but it uses one adversary instead of two and an L1 instead of a cross entropy loss to debias the representations. It still utilises a global cross-entropy loss for the target variable."}, {"title": "2.2 Non-adversarial Debiasing", "content": "Outside of adversarial learning, Fairness by Compression (FBC) [10] advocates for the use of binary compression to mitigate sensitive elements in representations. They establish that the cross-entropy between $P(z)$ and $Q$ stands as the upper bound for the entropy $H(z)$. In this context, $P(z)$ delineates the distribution of a factorized representation, and $Q$ is utilized to predict $z_i$ based on ${z_0, z_1,..., z_{i-1}}$. The FRC model [17] aims to mitigate the influence of sensitive factors in the data representation by adjusting the correlation between the representation and the sensitive vectors.\nBFA [18] draws inspiration from the correlation coefficient constraints used in FRC. The primary ambition is to minimize the correlation between sensitive information and prediction error (as opposed to minimising correlation between sensitive information and representations in FRC), aiming to maintain high predictive accuracy.\nThe Variational Fair Autoencoder, VFAE [15], comprises of a variational autoencoder (VAE) instead of an autoencoder and employs an addititional maximum mean discrepancy (MMD) loss to ensure less sensitive information, which may be correlated to the target task, leaks into the learned representation. MMD minimises the mismatch in moments between the marginal posterior distributions for the different sensitive features.\nThese techniques exhibit greatly improved training stability compared to the adversarial training approaches. However, they struggle to achieve good task performance (i.e. developing useful representations). It can be observed that many of these techniques experience a degeneracy where"}, {"title": "2.3 Measures of Fairness", "content": "Fairness in machine learning has been extensively studied, and there exist a range of metrics which measure different aspects of it. Our primary contribution is the introduction of a new differentiable fairness metric, thus necessitating a brief overview of commonly used metrics in our model evaluation.\nDemographic parity, also known as statistical parity or group fairness, requires that the selection rate (the rate at which individuals are positively classified) should be the same across all demographic groups. Mathematically, if $Y$ is the predicted label and $A$ denotes the demographic group, demographic parity is defined as:\n$P(\\hat{Y} = 1 | A = 0) = P(\\hat{Y} = 1 | A = 1)$.\nThis implies that the algorithm should be independent of the sensitive attribute $A$ [9], which can be limiting if the attribute is relevant to the outcome [13].\nThe equalized odds fairness metric demands true positive rates and false positive rates to be equal across demographic groups. Mathematically, if $\\hat{Y}$ is the true label:\n$P(Y = 1 | \\hat{Y} = 1, A = 0) = P(Y = 1 | \\hat{Y} = 1, A = 1)$"}, {"title": "3 The Adjusted Parity Metric", "content": "As an initial step towards solving these issues, [19] introduced a parity metric for evaluating domain invariance. This metric accommodates both discrepancies in accuracy across domains and normalised classifier or regressor performance to provide a single unified value for comparing debiasing models.\nThe adjusted parity metric was originally expressed for binary domains as:\n$\\Delta_{adj} = S(1 - 2 \\sigma)$,\nwhere $\\sigma$ represents the standard deviation of the normalised classifier accuracy across the domains, and $S$ denotes the average accuracy over the domains.\nWe extend this definition to an arbitrary number of domains:\n$\\Delta_{adj} = \\frac{S - S_R}{1 - S_R} (1 - \\frac{\\sigma}{\\gamma})$,\nwhere $S_R$ is the baseline accuracy of a random predictor, and $\\gamma$ is the maximum standard deviation across domains. This serves to normalise the metric between [0,1].\nThe introduction of $\\gamma$ in this paper extends the metric to sensitive characteristics with more than two domains. For any even number of domains (N) the value remains at $\\gamma = 0.5$ as in the original formulation. However for an odd number of domains:\n$\\gamma = \\frac{1}{4} \\sqrt{4 - \\frac{1}{N^2}}$"}, {"title": "4 Differential Adjusted Parity", "content": "In order to propose a differentiable variant of the adjusted parity metric from equation 5, we must first rely on a differentiable variant of the accuracy measure $S$. In this paper, we propose the use of \"Soft Balanced Accuracy\". This is a differentiable form of balanced accuracy, which allows us to simultaneously deal with the challenges common in imbalanced problems.\nThe soft accuracy is computed by omitting the \"argmax\" function from a standard classification accuracy metric. In other words, for a vector of predicted class probabilities $P(x)$ given input $x$ and a one-hot encoded label vector $L_x$, the vector of soft True Positive rates for all classes is:\n$TP = \\sum_x P(x) \\odot L_x$,\nwhere $\\odot$ represents the Hadamard product.\nAnalogously, the vector of Soft False Positives (FP) for each class would be computed as the sum of the predicted probabilities for instances that are incorrectly predicted as that. Given the inverted (one-cold) label vector $\\bar{L}$:\n$FP = \\sum_x P(x) \\odot \\bar{L_x}$\nWe can similarly compute the vectors of Soft True Negatives (TN) and Soft False Negatives (FN) as\n$TN = \\sum_x (1 - P(x)) \\odot L_x$,\n$FN = \\sum_x (1 - P(x)) \\odot \\bar{L_x}$.\nAlthough it may be obvious, it is worth pointing out that these soft variants of the TP, TN, FP, and FN are easily differentiable, as they are computed directly from the predicted probabilities. It is also worth pointing out that the classes referred to here are based on the output task, and differ from the sensitive characteristic domains of equation 6.\nGiven the above, we could compute the per-class accuracy vector as\n$Acc = \\frac{(TP + TN)}{(TP + TN + FP + FN)}$\nHowever, this measure can be misleading when classes are imbalanced. For instance, in a dataset where a single class represents 90% of the data, a naive classifier that always predicts the dominant class will have an accuracy of 90%.\nTo address this, we calculate the Soft Balanced Accuracy, which is the average of the per-class recall. In other words\n$S = \\frac{1}{C} \\sum_c \\frac{TP}{|TP + FN|}$,\nwhere $C$ is the number of classes and $|.|_1$ represents the L1 norm. This gives equal weighting to all classes irrespective of their prevalence in the dataset.\nTo return to the definition of $\\Delta_{adj}$ in Section 3: we propose computing this soft balanced accuracy $S$ independently on subsets of the dataset corresponding to each sensitive domain. The mean and standard deviation of $S$ across these domains can then be substituted for $S$ and $\\sigma$ in equation 5 (once again noting that the set of task labels is not the same as the set of sensitive characteristics). By substituting the balanced soft accuracy into the adjusted parity metric, we can obtain a differential"}, {"title": "5 Evaluation protocol", "content": "In our experiments, we've chosen to focus on two widely-acknowledged datasets in fairness research: the Adult dataset and the COMPAS dataset. In both cases we train a network using a combination of our DAP metric and the standard task cross-entropy loss ($L_{ce}$). We also introduce weighting hyperparameters $ \\beta$ and $ \\Omega$ which control the contribution of standard deviation term and $L_{ce}$ respectively."}, {"title": "5.1 Adult Dataset", "content": "The Adult dataset, often referred to as the \"Census Income\" dataset, originates from the UCI Machine Learning Repository [8]. It comprises demographic data extracted from the 1994 Census Bureau database. The primary task for this dataset is binary classification: predicting whether an individual earns more than $50k annually based on attributes like age, occupation, education, and marital status.\nOne notable characteristic of the Adult dataset is its inherent imbalance. Specifically, a substantial proportion of individuals in the dataset have incomes below $50k (around 75.4%). The dataset contains several sensitive attributes such as race and gender. We opt to use gender as the sensitive feature in this evaluation. This is also imbalanced with roughly 67.3% of the data being male. Such imbalances could mislead naive classifiers into an unwanted bias towards the dominant class. Both the gender feature and target income variable are binary. We attempt to eliminate disparities in income predictions across gender groups."}, {"title": "5.2 COMPAS Dataset", "content": "The Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) dataset became notably popular following an investigation by ProPublica in 2016 [7]. COMPAS is a risk assessment tool used in the U.S. legal system to assess the likelihood that a defendant will re-offend. Each instance in the dataset contains 12 features like age, gender, criminal history, and risk scores. The primary task is to predict if an individual will re-offend within two years.\nProPublica's analysis notably highlighted racial disparities in the predictions, where African-American defendants were more likely to be falsely classified as high risk compared to white defendants. We therefore opt to use race as the sensitive feature. The COMPAS dataset is balanced in terms of both the sensitive feature and target variable. The COMPAS dataset one-hot encodes ethnicity into five categories: African American, Asian, Hispanic, Native American, and Other. Studies often reduce this multi-class feature into a binary one distinguishing only between African-American and all other ethnicities, overshadowing the multi-class complexity. To enable comparison against models that do not have multi-class sensitive feature debiasing capability, we perform experiments with this binary simplification. However, we also evaluate our approach with the true multi-class problem. It is important to note that the COMPAS dataset has been heavily cirticised for its use in fairness research due to its inherent measurement biases and errors, its disconnection from real-world criminal justice outcomes and its lack of consideration for the complex normative issues related to fairness, justice and equality [2]. We use it here only to support comparison against previous works."}, {"title": "5.3 Data Preprocessing", "content": "For both datasets, we performed standard preprocessing, mapping categorical features to numerical indices, normalization of continuous variables, and handling missing values by replacing them with -1. We split the datasets into training and test sets in a 175:25 ratio. We also drop redundant features, that are either repeated or with mostly missing values."}, {"title": "5.4 Hyperparameter and Model Training", "content": "We employed a learning rate of 0.005 and 0.01 and a batch size of 64 and 32 for the Adult and COMPAS datasets respectively. The models were trained for 20 epochs.\nFor our hyperparameter sensitivity study, we chose values $\\Omega \\in {0...100}$ and $\\beta \\in {0.1...100}$, resulting in 100 tested combinations of $\\Omega$ and $\\beta$. All models were trained through 5 distinct runs, and we report the median as well as standard deviation of their performance across the runs. Given the stochastic nature of neural network training, this ensured robustness in our findings."}, {"title": "5.5 Evaluation Framework", "content": "To evaluate our models' performance, we trained 2 balanced random forest classifiers to predict the sensitive and target features from the encodings in the testing phase. This allowed us to measure balanced classification accuracies on the task variable and sensitive feature from the embeddings produced during testing. On the fairness front, we obtain fairness metrics like demographic parity"}, {"title": "6 Conclusion", "content": "The results unequivocally position DAP as a highly effective approach for achieving fairness across different datasets, significantly outclassing established models like FAIR, CFAIR, and LAFTR. These adversarial approaches can prove challenging to train but DAP remains stable, even for complex under-explored problems like those with multi-class sensitive features.\nWe formulate a differentiable variant of the adjusted parity metric, which includes only adaptively weighted positive learning signals with no adversarial tension. At its core, this involves the innovative use of the \"Soft Balanced Accuracy\" to provide a metric which is smoothly differentiable and agnostic to dataset biases (either in terms of sensitive characteristics or end-task labels). Unlike other"}]}