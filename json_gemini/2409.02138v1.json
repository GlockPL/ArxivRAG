{"title": "A FINANCIAL TIME SERIES DENOISER BASED ON DIFFUSION MODEL", "authors": ["Zhuohan Wang", "Carmine Ventre"], "abstract": "Financial time series often exhibit low signal-to-noise ratio, posing significant challenges for accurate data interpretation and prediction and ultimately decision making. Generative models have gained attention as powerful tools for simulating and predicting intricate data patterns, with the diffusion model emerging as a particularly effective method. This paper introduces a novel approach utilizing the diffusion model as a denoiser for financial time series in order to improve data predictability and trading performance. By leveraging the forward and reverse processes of the conditional diffusion model to add and remove noise progressively, we reconstruct original data from noisy inputs. Our extensive experiments demonstrate that diffusion model-based denoised time series significantly enhance the performance on downstream future return classification tasks. Moreover, trading signals derived from the denoised data yield more profitable trades with fewer transactions, thereby minimizing transaction costs and increasing overall trading efficiency. Finally, we show that by using classifiers trained on denoised time series, we can recognize the noising state of the market and obtain excess return.", "sections": [{"title": "Introduction", "content": "In the realm of financial data analysis, the issue of low signal-to-noise ratio (SNR) is a critical problem that demands attention [Israel et al., 2020]. A low SNR indicates that the useful information (signal) is overwhelmed by random fluctuations or irrelevant data (noise). Noisy financial data can lead to misinterpretations and erroneous conclusions, potentially resulting in significant financial losses. On the other hand, noise in the data can degrade the performance of machine learning models, making their predictions less reliable and increasing the likelihood of poor decision outcomes.\nRecently, generative models have become a pivotal tool in finance, leveraging their ability to simulate and predict complex data patterns. For example, generative models can be used for data generation [Wiese et al., 2020, Ni et al., 2020, El-Laham and Vyetrenko, 2022], risk management [Marti, 2020] and market simulation [Byrd et al., 2019]. It makes one wonder whether generative models can be used for financial data denoising tasks.\nThe diffusion model [Ho et al., 2020, Song and Ermon, 2019, Song et al., 2020a], as the most popular generative model at present, holds out hope in this context. It contains two data transformation processes, the forward process and reverse process. The forward process keeps adding noise to the original data until it reaches a completely noisy and unrecognizable state, often a prior Gaussian distribution. In the reverse process, the model learns to gradually remove the noise step by step, reconstructing the original data from the noisy version. In the computer vision research field, some recent papers use the diffusion model as a new adversarial purification method, which mitigates the vulnerability of images to adversarial attacks [Nie et al., 2022, Wang et al., 2022]. By leveraging a pre-trained diffusion model, they first add a small amount of noise to data in the forward process, then gradually purify adversarial images by the reverse process. Inspired by the success of the diffusion model in image datasets, we adopt the conditional diffusion model to denoise the financial time series and conduct extensive experiments on the denoised time series in practical trading settings.\nWe summarize our main contributions as follows. (1) To mitigate the low SNR problem, we propose a conditional diffusion model with guidance to obtain denoised stock time series by the noising-denoising procedure in limited steps. The denoised time series shows better performance on the downstream return prediction task. (2) We show that the denoised time series, obtained with our diffusion model, can generate more accurate trading signals on MACD and Bollinger strategies, with better return and fewer trading times. (3) Using classifiers trained on denoised time series, we can obtain the denoised version of the market trend and distinguish whether the observed market trend is noisy. Based on this new information, we design new profitable trading strategies."}, {"title": "Related Work", "content": "We will elaborate on related research in three parts: the theory of diffusion models, the application of diffusion models on time series, and denoising financial time series."}, {"title": "Diffusion Model Theory", "content": "There are two threads of diffusion models, which are eventually unified. The first thread of the diffusion model is inspired by thermodynamics [Sohl-Dickstein et al., 2015], laying the groundwork for diffusion models by proposing the idea of training generative models through gradual noise addition and removal. Ho et al. [2020] proposed the Denoising Diffusion Probabilistic Model (DDPM) in 2020, marking a significant milestone for diffusion models. Their method involves adding noise in a forward process and denoising in a reverse process to generate high-quality images. Subsequent work focuses on how to improve sampling quality and speed [Nichol and Dhariwal, 2021, Song et al., 2020b].\nThe second thread of diffusion models originates from Score-based Generative Models (SGM) [Song and Ermon, 2019] by using Denoising Score Matching (DSM) [Vincent, 2011]. It involves training a model to predict the gradient of the data density (score) by minimizing the denoising score matching loss, which can be used for reconstructing clean data from noisy observations. Song and Ermon [2019] showed that once the score function is learned, samples can be generated by iteratively refining noisy samples using Langevin dynamics.\nThe above DDPM and SGM can be unified under the Stochastic Differential Equation (SDE) framework [Song et al., 2020a]. This framework allows both training and sampling processes to be formulated using SDEs, providing a more elegant and theoretically grounded approach to diffusion modeling. Furthermore, Karras et al. [2022] disentangle the complex design space of the diffusion model in the context of DSM, enabling significant improvements in sampling quality, training cost, and generation speed."}, {"title": "Diffusion Model on Time Series", "content": "The applications of diffusion models on time series can be divided into three categories [Lin et al., 2023], which are forecasting, imputation, and generation. TimeGrad [Rasul et al., 2021] is an auto-regressive time series forecasting model based on DDPM, while ScoreGrad [Yan et al., 2021] is developed on the SDE framework. For time series imputation task, CSDI [Tashiro et al., 2021] uses transformer [Vaswani et al., 2017] as backbone model architecture, and SSSD [Alcaraz and Strodthoff, 2022] uses structured state space for sequence modeling method. For the time series generation task, DiffTime [Coletta et al., 2024] and its variants are proposed for constrained time series generation. D\u00b3VAE [Li et al., 2022] uses a coupled diffusion process for multivariate time series augmentation, then uses a bidirectional VAE with denoising score matching to clear the noise."}, {"title": "Denoising Financial Time Series", "content": "In [Song et al., 2021], padding-based Fourier transform is proposed to eliminate the noise waveform in the frequency domain of financial time series data and recurrent neural networks are used to forecast future stock market index. A hybrid model comprising of wavelet transform and optimized extreme learning machine is proposed in [Yu et al., 2020] to present a stable and precise prediction of financial time series. By using a Denoising AutoEncoder (DAE) architecture, Ma et al. [2022] try to denoise financial time series and demonstrate that denoised labels improve the performances of the downstream learning algorithm."}, {"title": "Score Based Generative Model: Background", "content": "SGM is a self-supervised machine learning method to generate new data samples. In this section, we will briefly introduce the basics of SGM and two recent papers. Then, we show how these are unified under SDEs.\nLet {x} be a dataset from an unknown distribution $P_{data}(x)$. The score of this unknown distribution is defined as $\u2207_x log p(x)$, which is a gradient of the log probability density function for data x. If we know the score, we can start from a random data sample and move it towards the place with higher probability density via gradient ascent. We use a neural network $s_\\theta$ to approximate the score $\u2207_x log p(x)$. The training objective of $s_\\theta$ is to minimize the following objective:"}, {"title": "Score Matching with Langevin Dynamics", "content": "The process of Score Matching with Langevin Dynamics (SMLD) contains two parts. One is to use denoising score matching to estimate score of noised data distribution, the other is to use Langevin dynamics to sample from prior distribution iteratively. Song and Ermon [2019] propose to perturb data with multiple-level noise and train a Noise Conditioned Score Network (NCSN) to estimate scores corresponding to all noise levels. The noising method is defined as $q_\\sigma(x|x) = \\mathcal{N}(x|x, \\sigma^2I)$, where I is the identity matrix. The probability distribution of noised data is $q_\\sigma(x) = \\int P_{data}(x)q_\\sigma(x|x)dx$. They also define a noise sequence ${\\sigma_i}_{i=1}^N$ satisfying $\\sigma_{min} = \\sigma_1 < \\sigma_2 < \\cdot < \\sigma_N = \\sigma_{max}$, where $\\sigma_{min}$ is small enough such that $q_{\\sigma_{min}}(x) \\approx P_{data}(x)$ and $\\sigma_{max}$ is large enough such that $q_{\\sigma_{max}}(x) \\approx \\mathcal{N}(x|0, \\sigma_{max}I)$. To estimate the score of noised data distribution, a conditional score network $s_\\theta(x, \\sigma)$ is trained to satisfy that $\\forall \\sigma \\in {\\sigma_i}_{i=1}^N: s_\\theta(x, \\sigma) \\approx \u2207_x log q_\\sigma(x)$. The training objective of NCSN is a weighted sum of denoising score matching objectives, i.e., finding $\\theta^*$ to minimise"}, {"title": "Denoising Diffusion Probabilistic Model", "content": "Denoising Diffusion Probabilistic Model (DDPM) [Sohl-Dickstein et al., 2015, Ho et al., 2020] can be seen as a hierachical markovian variational autoencoder [Luo, 2022]. Considering a noise sequence $0 < \\beta_1 < \\beta_2 < \\cdot < \\beta_N < 1$ and forward noising process $p(x_i|x_{i-1}) = \\mathcal{N}(x_i|\\sqrt{1 - \\beta_i}x_{i-1}, \\beta_iI)$. Let $a_i = \\prod_{j=1}^{i}(1 - \\beta_j)$, then we have $p_{ai}(x_i|x_0) = \\mathcal{N}(x_i|\\sqrt{a_i}x_0, (1 - a_i)I)$. Similar to SMLD, the perturbed data distribution can be denoted as $p_{a_i}(x) = \\int P_{data}(x)p_{a_i}(x|x)dx$. The noise scale is predetermined to satisfy $p_{a_N}(x) \\approx \\mathcal{N}(0,I)$. The reverse denoising process can be written as $p_\\theta(x_{i-1}|x_i) = \\mathcal{N}(x_{i-1}; \\frac{1}{\\sqrt{1-\\beta_i}}(x_i + \\beta_i s_\\theta(x_i, i)), \\beta_iI)$. The training objective is a sum of weighted evidence lower bound (ELBO), i.e., finding $\\theta^*$ that minimizes:"}, {"title": "Stochastic Differential Equation: A Unified Perspective", "content": "Song et al. [2020a] demonstrate that SMLD and DDPM can be unified under the perspective of SDEs. Let ${x(t)}_{t=0}^T$ be a stochastic diffusion process indexed by a continuous time variable $t \\in [0, T]$. $p_0$ is real data distribution and $p_T$ is tractable prior distribution such that $x_0 \\sim p_0$ and $x_T \\sim p_T$. They denote the probability density function of x(t) by $p_t(x)$ and the transition kernel from x(s) to x(t) by $p_{st}(x(t)|x(s))$ where $0 < s < t < T$. Then, we can use a stochastic differential equation to represent such a forward diffusion process:\n$dx = f(x, t) dt + g(t) dw$\nwhere f(x, t)dt is referred to as the drift item, and g(t)dw is referred to as the diffusion item. w is a standard Wiener process and $dw \\sim \\mathcal{N}(0, I)$. The synthetic data generation process is the reverse process of Eq. (7), which is also an SDE [Anderson, 1982]:\n$dx = [f(x, t) - g^2(t)\u2207_x log p_t(x)] dt + g(t) dw$\nwhere w is a reverse-time Wiener process and $\u2207_x log p_t(x)$ is the score of marginal distribution corresponding to each t. It starts from an initial data point $x_T \\sim p_T$ and gradually denoise it step by step following Eq. (8). Theoretically, if dt is small enough, we can get $x_0 \\sim p_0$. To estimate $\u2207_x log p_t(x)$, the score network $s_\\theta(x, t)$ is trained with objective function:\n$\\Lambda(t)E_t E_{x_0} E_{x_t/x_0} [||s_\\theta (x(t), t) - \u2207_{x(t)} log p_{0t}(x(t)|x(0))||^2]$\nwhere $\\Lambda : [0: T] \\rightarrow \\mathbb{R}^+$ is a positive weight item and $t \\sim U[0, T]$. Eq. (9) is a continuous generalization of Eq. (3) and Eq. (5). In [Song et al., 2020a], the forward process of SMLD and DDPM can be seen as a discrete form of continuous-time SDEs. Typically, the discrete forward process of SMLD is:\n$x_i = x_{i-1} + \\sqrt{\\sigma_i^2 - \\sigma_{i-1}^2}z_{i-1}, i = 1,..., N$.\nWhen $N \\rightarrow \\infty$, the discrete form ${x_i}_{i=1}^N$ becomes continuous form ${x(t)}_{t=0}^T$; and the continuous forward process can be written as Eq. (11):\n$dx = \\frac{d[\\sigma(t)^2]}{dt}\\frac{x}{\\sqrt{\\frac{d[\\sigma(t)^2]}{dt}}} dw$\nwhere $f (x, t) = 0$ and $g(t) = $\\sqrt{\\frac{d[\\sigma(t)^2]}{dt}}$. This is called Variance Exploding (VE) SDE.\nAs for DDPM, the discrete forward process is:\n$x_i = \\sqrt{1 - \\beta_i}x_{i-1} + \\sqrt{\\beta_i}z_{i-1}, i = 1,..., N$\nWhen $N \\rightarrow \\infty$, the continuous form of DDPM forward process can be written as Eq. (13):\n$dx = -\\frac{\\beta(t)}{2}x dt + \\sqrt{\\beta(t)} dw$\nwhere $f (x, t) = -\\frac{\\beta(t)}{2}x$ and $g(t) = $\\sqrt{\\beta(t)}$. This is called Variance Preserving (VP) SDE. By substituting f(x, t) and g(t) in Eq. (8) with f(x, t) and g(t) in Eq. (11) (Eq. (13), respectively), we can get the corresponding backward process of SDE form for SMLD (DDPM, respectively) ."}, {"title": "Methodology", "content": "In this section, we will show how to train the conditional diffusion model by stock time series datasets under the SDE framework shown in Section 3, and how we can get denoised stock time series via trained diffusion model."}, {"title": "Problem Formulation", "content": "Let x and c be two time series of length L, where $x \\in \\mathbb{R}^L$ is the input to our diffusion model and $c \\in \\mathbb{R}^L$ is the model's condition. The size of denoised time series x is also $\\mathbb{R}^L$.\nA neural network $s_\\theta(x, t, c)$ is used to estimate the score of noised data distribution. Here we use the network architecture from CSDI [Tashiro et al., 2021], which is a conditional transformer-based neural network, to train $s_\\theta (x, t, c)$. It takes time series x as input, time series c and sinusoidal-embedded time t as the condition."}, {"title": "Training and Inference", "content": "In the training stage, we follow the training method in [Song et al., 2020a] where a random level of noise $t \\sim U(0, T)$ is added to the input data $x_0$ via forward diffusion process shown in Eq. (7), then the network parameter $\\theta$ is updated by Eq. (9) until convergence. It should be noted that during training the noise level t is uniformly sampled from [0, T], where T = 1 is the maximum noise level. To train conditional diffusion model, we adopt classifier-free guidance method which is explained below. Next, we will show how to use trained conditional diffusion model to denoise a given time series."}, {"title": "Noising-Denoising Procedure", "content": "To denoise time series, we adopt the noising-denoising procedure in the inference stage. Since we aim to denoise the input data instead of generating new data, we should ensure that the denoised data still contains similar moving trends as input data. Therefore, our initial data should be $x_0$ instead of $x_T \\in \\mathcal{N} (0, I)$ in the inference stage. By using a certain noise level $T' < T$, we can obtain a noised data $x_{T'}$. Notice that $T'$ should be large enough so that the noise structure of $x_0$ is removed but not too large to destroy the intrinsic trend. Although $T'$ is often set as a hyperparameter in previous work [Nie et al., 2022, Gao et al., 2024] because it cannot be theoretically determined, we add the extra condition c = $x_0$ in the training and inference stage so that the choice of $T'$ does not have much influence on denoising performance. By letting c = $x_0$, the noised data $x_{T'}$ will always move towards the original $x_0$ no matter what noise level $T'$ we choose. The illustration of training and inference is shown in Figure 1."}, {"title": "Classifier-free Guidance", "content": "How do we inject condition c into the training and inference process? Here we take classifier-free guidance approach [Ho and Salimans, 2022]. It is developed on classifier guidance [Dhariwal and Nichol, 2021], where the conditional score $\u2207_x log p(x|c)$ is calculated by:\n$\u2207_x log p(x|c) = \u2207_x log p(x) + w\u2207_x log p(c|x)$\nwhere $\u2207_x log p(x)$ is unconditional score and $\u2207_x log p(c|x)$ is the gradient of classifier; w is a hyperparameter controlling the guidance strength. Doing so can decrease the diversity of generated data while increasing sampling quality. To avoid training an extra classifier, classifier-free guidance combines the conditional model and unconditional model:\n$\u2207_x log p(x|c) = w\u2207_x log p(x|c) + (1 - w)\u2207_x log p(x)$\nwhere $\u2207_x log p(x|c)$ represents the sampling direction of conditional model and $\u2207_x log p(x)$ represents the sampling direction of unconditional model. Eq. (15) degrades to the score of the unconditional model when w = 0, or becomes the score of the conditional model when w = 1. By tuning w, we can make our model more flexible."}, {"title": "Guidance by two loss function", "content": "To increase the predictability of financial time series, we consider two kinds of loss functions as extra guidance during the inference process. The first loss function is Total Variation (TV) Loss, which is defined as:\n$\\mathcal{L}_{TV}(x) = \\sum_{i=1}^{L-1} |x[i+1] - x[i]|$\nwhere x[i] represents the i-th value of x. TV Loss is a widely used regularization term for image denoising, deblurring, and super-resolution [Allard, 2008]. The main goal of TV Loss is to promote smoothness in the reconstructed data by penalizing rapid changes between neighboring values.\nThe second loss function is Fourier Loss, defined as:\n$\\mathcal{L}_F(x_t, x) = ||FFT(x_t) - Filter(FFT(x), f)||^2$.\nFourier Loss is a technique used to ensure that the frequency domain characteristics of a given signal match a desired target [Gopinathan et al., 2015]. Here $x_t$ is noised data and x is original data. $FFT(\\cdot)$ represents Fast Fourier Transform (FFT) which converts data from time domain to frequency domain. We assume that the noise mostly exists in those frequencies with low amplitude, so we use $Filter(\\cdot)$ to set frequencies whose amplitude is below a certain threshold f to zero. Experimentally, we set the threshold f to be 0.1."}, {"title": "Predictor-Corrector Sampler", "content": "To better improve our solution to reverse SDE, we can use Eq. (8) as the initial solution and employ score-based MCMC approaches to correct the solution at every step, which is called Predictor-Corrector samplers proposed in paper [Song et al., 2020a]. For VE-SDE, the Predictor is a discrete reverse process step:\n$x_{i-1} = x_i + (\\sigma_i^2 - \\sigma_{i-1}^2)\u2207_x log p_i(x_i) + \\sqrt{(\\sigma_i^2 - \\sigma_{i-1}^2)}z_i$, while for VP-SDE, the Predictor is a different discrete reverse process step: $x_{i-1} = [x_i + \\beta_i s_\\theta(x_i, i)] + \\sqrt{\\beta_i}z_i$, where $z_i \\sim \\mathcal{N}(0, I)$. For both VE-SDE and VP-SDE, we choose annealed Langevin MCMC[Song and Ermon, 2019] as our Corrector."}, {"title": "Reduce Sampling Randomness", "content": "Due to the randomness of the reverse denoising process, the denoised time series sometimes might still deviate from the original time series a lot, leading to information loss. To overcome the randomness, we use multiple independent reverse processes to denoise data by s random seeds, then average all the denoised data samples to get the final denoised time series x.\nCompared to other self-supervised learning-based denoisers such as DAE [Ma et al., 2022], our diffusion model-based denoising framework is more flexible due to multiple noising-denoising steps and auxiliary guidance during inference. The complete training and inference algorithm are shown in Algorithms 1 and 2."}, {"title": "Experiments", "content": "In this section, we will first introduce the datasets we use in the following experiments. Then, we show the diffusion model based on the denoised stock price time series in Section 5.2. Next, we try to answer the following three questions in our experiments: (1) Does our diffusion model-based denoiser increase the signal-to-noise ratio of stock time series? (2) Can our diffusion model-based denoiser be used for trading and obtain greater profits? (3) What kind of trading strategies can we develop if we are able to distinguish whether the current market trend is noisy or not?"}, {"title": "Datasets", "content": "We use three stock datasets of 1day (2014.01.01-2023.12.31), 1hour (2023.06.07-2024.04.02) and 5min (2024.02.12-2024.04.02) frequency from Stooq website 1. We only keep the US S&P 500 stocks as our stock pool. For each stock in the stock pool, we use rolling window of size 60 and stride 20 to get stock closing price time series, so that each data sample in the dataset is of length 60. Each dataset is divided into training and testing periods in the proportion of 4 : 1. The number of time series contained in each training dataset is 47,807, 25,985, and 51,794, respectively."}, {"title": "Diffusion Model Based Denoised Time Series", "content": "We use Ori to represent the original closing price time series and EMA for the Exponential Moving Average of Ori, where the decay factor is 0.5. Let the EMA be the diffusion model input $x_0$. The classifier-free guidance scale w is set to be 1. By exploiting the training algorithm and inference algorithm, we can obtain VE-SDE and VP-SDE-based denoised time series. We also consider DAE as the benchmark denoising model [Ma et al., 2022].\nTo better observe the characteristics of these time series, we use the number of directional change events [Guillaume et al., 1997] as a tool. Directional change events focus on discrete points where the price movement is substantial enough to indicate a change in the trend's direction. When the price moves by this threshold in one direction, it triggers a directional change event. We consider this to be a measure of the \"market clock\u201d and a parameter to assess the quality of synthetic time series. In Table 1, we show the average number of directional change events of different time series under various thresholds. We use thresholds taken from [0.01, 0.02, 0.03] for 1day and 1hour datasets, and [0.001, 0.002, 0.003] for 5min dataset. We observe that (1) DAE has the least DC events in most cases; (2) In the 1hour dataset, VE/VP-SDE has more DC events than EMA."}, {"title": "Downstream Classification Task", "content": "To evaluate if the signal-to-noise ratio of data is increased after the denoising process, we measure the predictability of denoised time series in downstream tasks. We set a supervised binary classification task to predict the future log return. It should be pointed out that the labels used here are obtained by corresponding time series, e.g., the label of Ori data is calculated by Ori data, and the label of denoised data is calculated by denoised data. We use an ensemble-tree-based model as our classifier [Ke et al., 2017] due to its robustness and quickness. The input of the classification model is a"}, {"title": "Using the Classifier's Prediction for Trading", "content": "We have demonstrated in Section 5.3 that by using the diffusion model-based denoising method, we can obtain greater performance on the downstream return prediction task. This means that we can predict the denoised market trend when the market is still noisy. In other words, we can tell if the market is noisy or not by comparing the real market trend and our predicted market trend. In this section, we explore how to leverage the information from denoised market trends for trading.\nWe still use GOOG, AAPL, MSFT, and AMZN for experiments for this. Here, we take 1 timestep prediction as an example to explain our experiments. By using the classifier trained in Section 5.3, we can take time series $x_{[t-59, t]}$ as input and obtain a prediction about the market trend on time period [t, t + 1]. When the input is a denoised time series, the output should be the denoised market trend on [t, t + 1], which is called prediction stage. We can also observe the true market trend on [t, t + 1]. By leveraging the information on the prediction stage, we will take positions on [t + 1, t + 2], which is called action stage. We use 1, 5, and 10 timesteps prediction results just as in Section 5.3. We adopt two opposite trading strategies, one is a following-trend strategy, and the other is a countering-trend strategy. For the following-trend strategy, if the prediction result on the prediction stage shows a positive (negative, respectively) trend, then we long (short, respectively) in the action stage. For the countering-trend strategy, if the prediction result in the prediction stage shows a negative (positive, respectively) trend, then we long (short, respectively) in the action stage. We use four metrics to evaluate the performance of different predictors, which are Long-Only Return (LOR), Long-Only Hit Ratio (LOHR), Long-Short Return (LSR) and Long-Short Hit Ratio (LSHR). Hit ratio, also known as the winning rate, measures the proportion of profitable trades to the total number of trades executed.\nThe experiment results are shown in Figure 5. Ori and EMA represent the real market trend observed in the prediction stage, while Ori Pred, EMA Pred, DAE, VE-SDE, and VP-SDE represent the predicted market trend obtained by classifiers. We have two observations from the radar plots. (1) The denoising methods based on the diffusion model perform better by using countering-trend strategy in the 1day and 5min datasets and by using following-trend strategy in the 1hour dataset. (2) The denoising methods based on diffusion model perform comparatively better on 5 and 10 timesteps trading than 1 timestep trading.\nWe explain the observations from the perspective of how much noise remains in the prediction stage. If the real market contains noise in the prediction stage, the price trend in the prediction stage is also noisy and inaccurate. Then, the market will try to recover to the \u201cunnoisy\u201d state in the action stage by countering the current price trend. So, we should take a countering-trend position in the action stage. On the contrary, if the price trend in the prediction stage is not noisy, then the market will likely continue that trend in the action stage, which means we should take the position suggested by following-trend strategy. We believe that the 5min and 1day dataset have more noise than the 1hour dataset. For the 5min dataset, the noise is not smoothed out by the market because the prediction stage is too short. As for the 1day dataset, the prediction stage is so long that new noise may be introduced into the price trend. But for the 1hour dataset, the length of the prediction stage is just enough to balance the market without new noise coming in. The results of DC events can also support this. In Table 1, we find that VE/VP-SDE has relatively more DC events in the 1hour dataset, where the DC events are preserved well after denoising because the 1hour dataset does not contain much noise in the first place. On the other hand, due to Total Variation and Fourier guidance during the inference process, the time series will carry more information about the long trend. Therefore, VE/VP-SDE performs better on longer timesteps trading tasks."}, {"title": "Conclusion", "content": "This study highlights the potential of using the diffusion model as a financial time series denoiser. The denoised time series exhibits better predictability in downstream return classification tasks and generates more profitable trading signals. By effectively distinguishing between noisy and clear market trends using classifiers trained on denoised time series, the diffusion model provides more accurate market predictions and improved trading decisions. The findings suggest that diffusion models can significantly enhance the signal-to-noise ratio of financial time series, thereby improving the reliability of predictive models and trading strategies."}]}