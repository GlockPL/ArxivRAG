{"title": "DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning", "authors": ["Hongye Qiu", "Yue Xu", "Meikang Qiu", "Wenjie Wang"], "abstract": "Large Language Models (LLMs) exhibit strong natural language processing capabilities but also inherit and amplify societal biases, including gender bias, raising fairness concerns. Existing debiasing methods face significant limitations: parameter tuning requires access to model weights, prompt-based approaches often degrade model utility, and optimization-based techniques lack generalizability. To address these challenges, we propose DR.GAP (Demonstration and Reasoning for Gender-Aware Prompting), an automated and model-agnostic approach that mitigates gender bias while preserving model performance. DR.GAP selects bias-revealing examples and generates structured reasoning to guide models toward more impartial responses. Extensive experiments on coreference resolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and Llama2-Alpaca) demonstrate its effectiveness, generalization ability, and robustness. DR.GAP can generalize to vision-language models (VLMs), achieving significant bias reduction.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Ouyang et al., 2022; OpenAI, 2023; Touvron et al., 2023; Grattafiori et al., 2024) have made significant advancements in natural language processing (NLP). However, trained on large-scale, unfiltered datasets, they not only inherit but also magnify social biases, exacerbating existing inequities (Mei et al., 2023; M\u011bchura, 2022). Gender bias, as a typical form of social bias, has been proven to be widely present in LLMs (Dong et al., 2024; Zhonga et al., 2024). Effectively mitigating gender bias in the outputs generated by LLMs has become an urgent issue.\nAn effective debiasing method should adhere to several essential criteria: (1) Automation to minimize human intervention, (2) Applicability to both"}, {"title": "2 Related Word", "content": ""}, {"title": "2.1 Gender Bias Evaluation Methods", "content": "Prior studies have examined gender bias in LLMs through text generation and comprehension tasks, with the former detecting externally exhibited gen-"}, {"title": "2.2 Bias Mitigation Methods", "content": "Various bias-mitigating strategies have been proposed, including white-box approaches that modify model parameters, such as fine-tuning (Raza et al., 2024; Zhang et al., 2024), controlled decoding (Liu et al., 2021), and model editing (Cai et al., 2024b; Si et al., 2022). While effective, these methods are limited by accessibility and efficiency. In contrast, black-box methods leave the model unchanged and use textual prompts to steer generation towards unbiased outputs, employing techniques like Chain-of-Thought (CoT) and in-context learning (ICL) (Schick et al., 2021; Sant et al., 2024), providing a flexible and computationally efficient alternative."}, {"title": "2.3 Prompt Engineering", "content": "Due to the key role of prompts in black-box bias mitigation, several efforts have focused on prompt engineering (Si et al., 2022; Dwivedi et al., 2023). Prompts can include general instructions, specific examples, or a combination, leading to different approaches for improvement. For instance, Ganguli et al. (2023) explored the effectiveness of instructions in bias mitigation for aligned LLMs and examined the impact of prompt structure. Oba et al. (2024) and Bauer et al. (2024) focused on crafting preambles or beliefs as specific examples, either manually or automatically, to prompt fairer generations. We instead focus on improving the reasoning process in demonstrations to guide models toward more impartial responses."}, {"title": "3 Methods", "content": "DR.GAP mitigate gender bias by providing gender-neutral demonstrations and reasoning from a reference model, as system prompt to the target model, guiding the target model to prioritize semantic logic over gender-specific details. In this section, we first"}, {"title": "3.1 Demonstration Selection", "content": "The selection of demonstration data is a critical step, as the chosen examples must effectively highlight the model's gender biases. To identify such cases, we focus on instances where the model provides incorrect answers, as these are more likely to reveal underlying gender bias. Importantly, if an example successfully guides another LLM to produce a correct response, it confirms that the input contains sufficient semantic information, and the target model's error is likely due to bias rather than ambiguity or reasoning limitations. This rationale motivates our introduction of a reference model: by selecting examples where the reference model succeeds but the target LLM fails, we ensure that the identified errors are primarily attributable to bias, excluding other factors like language ambiguity or model capability. This approach allows us to isolate and address gender bias more effectively.\nSpecifically, the dataset is partitioned into a development set and a test set. The development set is used to identify biased examples through parallel evaluations with both the target LLM and a reference model (GPT-4 in our case). We then identify and isolate instances where the target LLM produces erroneous outputs while GPT-4 generates correct responses. This differential analysis yields a subset of cases that potentially exhibit more pronounced gender bias in the target LLM compared to the general data. For QA datasets that don't have definitive correct answers and are only used to assess the model's response tendency, we randomly select examples from the development set."}, {"title": "3.2 Reasoning Generation", "content": "DR.GAP pipeline includes four modules, each with its own independent function, to generate a set of reasoning processes that are correct, gender-independent, and learnable."}, {"title": "3.2.1 Initial Reasoning", "content": "To guide the target model to generate bias-free re-sponses, we generate the initial reasoning from the"}, {"title": "3.2.2 Verification", "content": "Since LLMs remain inherent variability in the accuracy of their responses, with a small probability of generating erroneous reasoning processes, we incorporate a verification phase into our methodology to ensure the accuracy and reliability of the reasoning processes. During this phase, the reference model is prompted to validate prior reasoning chains and their conclusions, which enables the detection and correction of potential inferential errors. This ensures the correctness of the reasoning process in the final generated prompts."}, {"title": "3.2.3 Gender-Independent Filtering", "content": "Due to the gender-biased knowledge inherently incorporated during pre-training, LLMs' reasoning processes may unconsciously employ gender-stereotypical associations and biases. To provide gender-neutral reasoning, we design a semantic filtering module with two core functions: First, identifying and eliminating parts of the reasoning process that stem from gender-based presuppositions or stereotypical associations; and second, explicitly guiding the model to prioritize logical inference patterns that are based on semantic content and contextual relevance. This dual-function approach ensures that the final generated prompts are primarily driven by the logical relationships inherent in the semantic content, rather than being influenced by gender biases or preconceived notions about gender roles."}, {"title": "3.2.4 Iterative Refinement", "content": "Owing to the stochastic nature of LLMs, isolated queries sometimes result in inconsistent reasoning processes, which may undermine their effectiveness in mitigating gender biases. To reduce the impact of randomness on DR.GAP's performance, we introduce an iterative refinement module that includes multiple refinement cycles to enhance the accuracy and stability of the reasoning process. Within this module, each iteration integrates feedback from the preceding reasoning patterns to improve the debias reasoning. This recursive process not only strengthens the robustness of bias mitigation strategies but also ensures greater consistency in the quality of reasoning outputs across multiple query instances."}, {"title": "3.3 Formalize Demonstration and Reasoning", "content": "We gather the reasoning result form all previous steps In the final step, we construct system prompts based on the reasoning processes generated in the previous steps, following the predetermined template. We gather the reasoning result from all previous steps. Although all these examples may have debias effect, our goal is to identify the most effective one. To this end, we structure these reasoning according to the predetermined templates to form a set of candidate system prompts. We then quantitatively assess their gender bias mitigation effects on the development set and select the optimal system prompt as the terminal output of our iterative optimization process."}, {"title": "4 Experiments", "content": "This section presents experiments verifying DR.GAP's effectiveness in mitigating gender bias while balancing model performance and fairness. We begin by detailing the configuration including the evaluated datasets and models, evaluation metrics, baseline methods and ablations. Then, we demonstrate its effectiveness on two tasks, Coreference Resolution (CoR) and QA, in terms of bias mitigation and utility prevention. Next, we include the ablation study to verify the contribution of each module, and study the transferability of the prompt generated by DR.GAP. Last, we extend DR.GAP to vision-language models and demonstrate its adaptability to various models."}, {"title": "4.1 Configurations", "content": ""}, {"title": "4.1.1 Datasets and Metrics", "content": "We conduct experiments across seven datasets spanning two typical tasks of LLMs: CoR and QA, each having its own evaluation metrics.\nCoreference resolution datasets. CoR is a key NLP task that links expressions referring to the same entity. We evaluate four representative datasets, including Winobias (Zhao et al., 2018), Winogender (Rudinger et al., 2018), GAP (Webster et al., 2018) and BUG (Levy et al., 2021). We evaluate Winobias and Winogender with $Acc$ and $AccGap$, where the former refers to the probability of correctly recognizing the coreference relation over multiple trials (m repetitions), formulated as $Acc = \\frac{\\Sigma_{k=1}^m I(Ans[k])}{m}$ and the later refers to the average absolute difference in accuracy between stereotypical and anti-stereotypical sentences, formulated as $AccGap = \\frac{\\Sigma_{i=1}^n|Acc_{stereo}[i]-ACC_{antistereo}[i]|}{n}$ For GAP and"}, {"title": "4.1.2 Evaluated Models", "content": "We utilize GPT-4-1106-preview (OpenAI, 2023) as the reference model to steer the generation and modification of the reasoning process in our workflow. We evaluate DR.GAP on three publicly available LLMs: GPT-3.5-Turbo (Ouyang et al., 2022), Llama3-8B-Instruct (Grattafiori et al., 2024), and Llama2-Alpaca-7B (CRFM, 2023). Furthermore, we extend our experiments to VLMs, including InstructBLIP-vicuna-7B (Dai et al., 2023), Llava-1.5-7B (Liu et al., 2023), and Qwen2-VL-7B-Instruct (Wang et al., 2024)."}, {"title": "4.1.3 Baseline and Ablation", "content": "Manually designed reasoning. We propose to incorporate demonstration and reasoning as system prompt to mitigate bias. An intuitive baseline of DR.GAP is manually designed demonstration and reasoning without demonstration selection and automated reasoning. Therefore, we include DR.GAP manual as a baseline. Details can be found in Appendix A.2.\nCounterfactual-detailed (CFD). We include the counterfactual example method (Oba et al., 2024) as a baseline, which selects three stereotypical sentences from predefined preambles, each emphasizing the reverse association between gender and occupation (e.g., \"Despite being a woman, Anna became an engineer\").\nDirect Preference Optimization (DPO). We also compare DR.GAP with a parameter-tuning method, which tuning the model using DPO (Li et al., 2023) on the GenderAlign (Zhang et al., 2024) dataset. This dataset contains 8,000 single-turn dialogues, each paired with a gender-unbiased \u201cchosen\" response and a biased \u201crejected\u201d response.\nAblations. We conduct an ablation study to evaluate the impact of using aggregated demonstrations and reasoning from different datasets in constructing the system prompt, denoted as DR.GAPagg."}, {"title": "4.2 Effectiveness of DR.GAP", "content": "The gender bias and utility for DR.GAP along with its ablation DR.GAPagg and baselines tested on various LLMs are summarized in Table 2. Since GPT-3.5 is closed-source and Llama3 is well-aligned for instructions, DPO was applied to the weakly aligned Llama2-Alpaca to demonstrate its debiasing effectiveness. Overall, DR.GAP achieves the best or second-best debiasing effect among all the compared methods, while the utility did not decrease significantly. In the following of this section, we provide a detailed analysis of each task.\nCoreference resolution. Our experimental results show that DR.GAP and DR.GAP Pagg effectively mitigates gender bias in CoR for LLMs. DR.GAP reduces gender bias in CoR for GPT-3.5, Llama3,"}, {"title": "4.3 Ablation Study", "content": "To verify the necessity of each module in DR.GAP, we conduct an ablation study to examine the individual impact of Reasoning Verification, Gender-independent Filtering, and Iterative Refinement modules in the DR.GAP pipeline, by removing these modules and evaluating the performance across three datasets (Winobias, Winogender, and BBQ) on Llama3. Table 3 shows that removing any module increases gender bias, with Iterative Refinement having the most significant impact. These findings highlight the critical role of each module in mitigating gender bias and emphasize the necessity of the process that incrementally refines the initial reasoning."}, {"title": "4.4 Generalization Ability of DR.GAP", "content": "We perform a cross-dataset evaluation to demonstrate the generalization ability of DR.GAP, using reasoning examples from seven datasets to evaluate their debiasing effects across different datasets. Given the diverse bias metrics employed, we quantify the debiasing effects by measuring the percentage reduction in gender bias (\u2206Bias). In Figure 3, the x-axis represents the source datasets for reasoning, and the y-axis indicates the target datasets for evaluation. Darker colors indicate a greater improvement. Despite variability in debiasing effects, DR.GAP consistently demonstrates effectiveness.\nReasoning examples from the Winogender and Winobias datasets achieve the best average performance across all datasets. This may be due to their simple templates and clear logical premises without complex context or varied sentence structures. These features enable LLMs to more easily extract reasoning paradigms that emphasize semantics over gender information. Additionally, reasoning examples from each dataset generally achieve the best debiasing effect on the dataset itself, with a few exceptions. These exceptions may be related to the unique characteristics and metrics of the datasets."}, {"title": "4.5 Extending to VLMs", "content": "Given DR.GAP's compatibility with diverse task types, we conduct experiments on captioning, a core task for VLMs. The reasoning examples (see Appendix A.2) provided for VLMs involve recognizing various elements in images and understanding their relationships. As shown in Figure 4, our method consistently reduces gender bias and improves resolution accuracy in InstructBlip, Qwen2-VL and Llava-1.5.\nInstructBlip and Qwen2-VL, which inherently support user-provided system prompts, effectively follow these reasoning examples. However, Llava-1.5 does not support this feature, so it cannot effectively distinguish between the DR.GAP demonstration and the user's query. This interference leads to unreasonable responses. To address this, we introduce a new module at the end of the reasoning generation process. This module abstracts the reasoning and extracts the key content to focus on. It indicates DR.GAP's potential to adapt to other models with specific constraints through minor adjustments. Additional details are provided in Appendix B."}, {"title": "5 Conclusion", "content": "In this work, we proposed DR.GAP, an automated and model-agnostic approach that mitigates gender bias through reasoning generation and a progressively refined process. Compared with previous work, DR.GAP focuses on generating gender-neutral reasoning to guide models toward impartial responses, thereby avoiding the risk of inadvertently reinforcing biases or degrading model performance. Extensive experiments demonstrate that DR.GAP significantly reduces gender bias across seven datasets spanning coreference resolution and QA tasks while preserving model utility, showing significant generalization ability and robustness. In the future, it would be interesting to further explore the effectiveness of the proposed methods on broader NLP tasks (e.g., open-domain QA and summarization) and assess their impact on reducing social biases related to race, religion, and age."}, {"title": "Limitations", "content": "Our study focuses on mitigating gender biases in LLMs using English datasets and prompts. While this approach addresses significant concerns related to gender fairness, it also has notable limitations.\nFirst, our work is limited to the English language and does not account for cultural nuances or biases present in other languages. Gender biases can manifest differently across linguistic and cultural contexts, and extending our approach to other languages is essential for broader applicability. For example, some languages have grammatical gender systems that complicate the identification and mitigation of biases, while others may have unique cultural associations with gender roles that are not captured by our current methods. Additionally, the datasets used for training and evaluation are predominantly English-centric, which may not reflect the diversity of gender-related issues in other linguistic communities. Future work should explore adaptations of our methods to other languages and cultures to ensure more comprehensive and culturally sensitive bias mitigation.\nSecond, our current scope is restricted to binary gender biases, neglecting the diverse spectrum of gender identities beyond the binary. Future research should prioritize evaluating and mitigating biases against non-binary and gender-diverse individuals to ensure more inclusive fairness.\nAdditionally, our method relies on existing datasets and evaluation metrics, which may not fully capture the complexity of real-world scenarios. We recommend further exploration of diverse datasets and continuous refinement of our approach to address these limitations."}, {"title": "Ethics Statements", "content": "Our study targets binary gender biases in LLMs, aiming to enhance fairness and inclusivity. However, we acknowledge that our current scope is limited to male and female genders and does not fully address non-binary or gender-diverse identities. Future research should prioritize evaluating and mitigating biases against non-binary genders to ensure more comprehensive inclusivity. We also recognize the importance of engaging with diverse communities to better understand and address the needs of non-binary and gender-diverse individuals in the context of AI development. While our method shows promising results on existing datasets, its real-world effectiveness requires fur-"}]}