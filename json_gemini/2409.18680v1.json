{"title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models", "authors": ["Yiming Chen", "Xianghu Yue", "Xiaoxue Gao", "Chen Zhang", "Luis Fernando D'Haro", "Robby T. Tan", "Haizhou Li"], "abstract": "Various audio-LLMs (ALLMs) have been explored recently for tackling different audio tasks simultaneously using a single, unified model. While existing evaluations of ALLMs primarily focus on single-audio tasks, real-world applications often involve processing multiple audio streams simultaneously. To bridge this gap, we propose the first multi-audio evaluation (MAE) benchmark that consists of 20 datasets from 11 multi-audio tasks encompassing both speech and sound scenarios. Comprehensive experiments on MAE demonstrate that the existing ALLMs, while being powerful in comprehending primary audio elements in individual audio inputs, struggling to handle multi-audio scenarios. To this end, we propose a novel multi-audio-LLM (MALLM) to capture audio context among multiple similar audios using discriminative learning on our proposed synthetic data. The results demonstrate that the proposed MALLM outperforms all baselines and achieves high data efficiency using synthetic data without requiring human annotations. The proposed MALLM opens the door for ALLMs towards multi-audio processing era and brings us closer to replicating human auditory capabilities in machines. 1", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have become remarkably powerful, driving advancements in various tasks across the field of natural language processing (NLP) (Touvron et al., 2023; Achiam et al., 2023; Team et al., 2023). Recent advancements in LLMs have also led to the development of various powerful audio large language models (ALLMs) (Chu et al., 2023; Huang et al., 2024; Rubenstein et al., 2023), which have achieved impressive results on a range of audio tasks, e.g., automatic speech recognition (Hu et al., 2024), speech synthesis (Gao et al., 2024c), sound event classification (Tang et al., 2024; Yue et al., 2024b).\nHowever, a crucial limitation exists: current ALLM training and evaluation primarily focus on single audio inputs. This is a significant drawback, as various real-world applications, e.g., virtual assistants, often require processing multiple audio streams simultaneously. Additionally, multi-audio processing is essential for effectively implementing few-shot in-context learning, which is a fundamental capability for advanced LLMs. Unlike text-based LLMs, which excel at handling multiple texts (Wang et al., 2024; McKenna et al., 2023), and vision LLMs, which have established benchmarks for processing multiple images (Li et al., 2024; Huang et al., 2023b; Zhao et al., 2024; Li et al., 2023), the audio field lacks systematic evaluations and benchmarks for multi-audio tasks with ALLMs. Although several ALLMs (Chu et al., 2023; Zhan et al., 2024) claim to handle multi-audio contexts, their performance quantification remains unclear. This underscores a significant gap in the audio field, in contrast to the vision and language fields, which already possess established evaluations and benchmarks for their respective tasks.\nTo bridge this gap, we propose the first dedicated multi-audio benchmark for ALLMs, encompassing 11 tasks for both open-ended and close-formed generation across speech and sound domains. Comprehensive experiments on 15 ALLMs reveal that existing open-source ALLMs fall short in multi-audio scenarios despite their strong single-audio processing abilities. For instance, as shown in Fig. 1, while ALLMs can perfectly identify Audiol as clapping and Audio2 as raining respectively in a single-audio scenario, they struggle to determine whether Audiol and Audio2 are the same when tasked with understanding the relationship between two audio inputs. This suggests that current ALLMs are not well-equipped to handle multi-audio tasks, even those that are straightforward for humans. This further underlines the need for advancements in ALLMs to handle multiple audio inputs to enhance human-computer interaction.\nTo this end, we propose an innovative and scalable multi-audio large language model (MALLM) that effectively captures audio contexts essential for reasoning over multiple audios. Inspired by the success of discriminative learning (Clark et al., 2020; Li et al., 2024; Jia et al., 2021), we define a challenging discriminative task that trains the model to discover the subtle differences between two similar audio samples. Furthermore, we introduce a scalable audio pairs synthesis strategy to enable multi-audio processing ability without the need for data collection and human labeling. Comprehensive experiments show that MALLM significantly outperforms existing open-source ALLMs under multi-audio scenarios while maintaining competitiveness under single-audio scenarios.\nOverall, our contributions include: (1) Novel Evaluation Benchmark: We propose the first multi-audio benchmark (MAE) for evaluating the multi-audio processing capabilities of ALLMs, encompassing a diverse range of tasks from both the speech and sound domains. (2) Advanced Multi-Audio LLM: Beyond focusing on the single audio content, our proposed MALLM not only demonstrates remarkable performance across diverse multi-audio tasks but also achieves remarkable data efficiency through an innovative data synthesis strategy. (3) Comprehensive Evaluation: We conduct a comprehensive evaluation on 15 ALLMs across various tasks, providing a solid foundation for future research."}, {"title": "Related Works", "content": "Recently, numerous studies have integrated audio encoders with pre-trained LLMs to develop multimodal ALLMs, serving as general-purpose task solvers for various audio-input tasks across speech and sound domains. Most ALLMs focus on specific audio types, like speech (Zhang et al., 2023; Zhan et al., 2024; Das et al., 2024), or sound (Panagopoulou et al., 2023; Kong et al., 2024; Moon et al., 2023; Han et al., 2023). A few recent models handle multiple types of audio, showing strong capabilities in universal audio understanding (Gong et al., 2023; Chu et al., 2023; Tang et al., 2024). Meanwhile, to effectively benchmark the advancements in ALLMs, new evaluation benchmarks such as AIR-Bench (Yang et al., 2024) and Dynamic-SUPERB (Huang et al., 2023a) have been introduced. However, these benchmarks and existing ALLMs primarily focus on tasks involving single audio inputs, largely overlooking scenarios with multiple audio streams. In contrast, our work introduces the MAE benchmark, specifically designed to assess multi-audio processing capabilities. Notably, MAE includes a variety of tasks across diverse scenarios, encompassing both audio and speech, making it suitable for benchmarking a broad spectrum of ALLMs. Additionally, we develop the MALLM, the first ALLM specifically tailored for multi-audio tasks, demonstrating significant improvements in processing multiple audio streams while maintaining competitive performance on single-audio tasks."}, {"title": "MAE Benchmark", "content": "We propose the first multi-audio benchmark (MAE) for evaluating ALLMs, as illustrated in Fig. 2. The MAE comprehensively benchmarks the multi-audio processing capabilities of ALLMs by including a wide variety of generation tasks from different fields and scenarios. It comprises six speech tasks and five sound tasks, covering both open-ended and close-form generation tasks. Open-ended questions allow models to produce free-form responses without predefined constraints. Conversely, closed-form questions restrict models to a predetermined set of possible outcomes. The MAE is automatically constructed using an advanced text-only LLM from various existing single-audio datasets without the need for further human annotation. Each sample in the MAE includes a combination of two audio contexts and a task instruction."}, {"title": "Speech Tasks", "content": "For speech, we design two open-ended generation tasks from sentence and dialogue levels and four closed-form tasks from word and sentence levels. Speech comparison: This sentence-level, open-ended task requires ALLMs to identify content differences between pairs of speeches. Speech pairs are sourced from ASR datasets with timestamp-level transcription. We segment the speech into spoken words with timestamps and instruct an LLM to reconstruct a subset of these words into a new speech without semantic errors, such as omitting the adjective \"expensive\" in Fig. 2. Then, we combine the original and reconstructed speech or two reconstructed speeches as an evaluation pair. Note that ground truth transcriptions of two speeches can be obtained from the original labels, allowing us to utilize the LLM as an evaluator to label the ALLMS' responses based on the derived transcriptions.\nDialogue response generation: This dialogue-level, open-ended task involves generating a subsequent utterance from the examined ALLMs in a dialogue based on the first two utterances. Similarly, the LLM evaluator assesses the overall quality of the generated dialogue response with respect to the transcriptions of the preceding utterances.\nHotword detection: Hotword detection is defined as a word-level, close-ended task derived from existing ASR datasets. Given a pair of randomly sampled speeches, the ALLMs are asked to detect which speech contains a specific noun that appears only in one of them. This task evaluates the models' precision in recognizing and differentiating specific lexical items within speech contexts.\nIntent identification: This task is defined as a sentence-level, close-ended task. We construct context speech pairs by randomly sampling speeches that either share the same intent or represent different intents from existing intent classification datasets. The ALLMs are tasked with determining whether the paired speeches belong to the same intent category, evaluating their ability to discern and categorize the underlying communicative purposes in speech data.\nKeyword comparison: This task is a word-level, close-ended challenge derived from existing keyword-spotting datasets. We sample pairs of speeches that either share the same keywords or contain different ones. The ALLMs are asked to determine whether the keywords presented in the two speeches are identical.\nSpeech identification: Speech identification is a sentence-level, close-formed task involving context-positive speech pairs and negative pairs with distinct transcriptions. The ALLMs are required to discern whether speech pairs have the same content. Positive pairs are formed by adding different background noises (e.g., coughing and sneezing in Fig. 2) to the same speech, yielding identical content but different backgrounds. Negative pairs consist of randomly selected speeches."}, {"title": "Sound Tasks", "content": "For sound, we design one open-ended and four closed-form tasks to challenge the ALLM comprehension of global and local level audio intricacies. Story generation: This open-ended task tests the ALLMs' global understanding of audio by challenging them to integrate diverse sound events from multiple audios into a coherent narrative. We randomly select two audio samples, each with a distinct event, and task the ALLMs to create a brief narrative incorporating both sounds. The correctness of the story is contingent upon the inclusion of both specified sounds; any omission marks it as incorrect. For instance, as shown in Fig. 2, a story is correct if it includes both the dog barking and"}, {"title": "MALLM", "content": "In this section, we introduce the specifics of our proposed MALLM. Fig. 3 depicts our discriminative ALLM fine-tuning strategy. Specifically, MALLM is trained to describe the subtle distinctions between two similar audios, enhancing its capability to handle multi-audio scenarios. To effectively tackle the challenging discriminative problem, the MALLM is expected to possess a deep comprehension of all input audios and infer both intra-audio and inter-audio relationships. In this way, MALLM can be effectively augmented with reasoning abilities across multiple audios. Furthermore, to improve the data efficiency and scalability, we propose the automatic construction of a synthetic training dataset without additional human intervention. This dataset includes pairs of both synthetic speech and sound pairs with subtle differences, which are used to fine-tune MALLM, expanding its multi-audio processing capabilities.\nSpeech pair synthesis: To automate the construction of speech pairs, we start by randomly selecting a sentence from a text corpus. An advanced text-based LLM is then utilized to generate four variations of the sentence, each containing slight modifications. This is achieved by instructing the LLM with different prompts that direct it to add, delete, or modify a limited number of words in the original sentence and to alter its structure. Detailed"}, {"title": "Experiment", "content": "Throughout the experiments, we use GPT-42 for data synthesis and downstream performance evaluation. For the TTS module in speech pair synthesis,"}, {"title": "Experiment Setup", "content": "Throughout the experiments, we use GPT-42 for data synthesis and downstream performance evaluation. For the TTS module in speech pair synthesis,"}, {"title": "Examined Models", "content": "Our evaluation encompasses a diverse range of ALLMs on the MAE benchmark. For speech-related tasks, we assess LTU-AS (Gong et al., 2023), SALMONN (Tang et al., 2024), Qwen-Audio (Chu et al., 2023), SpeechGPT (Zhang et al., 2023) and AnyGPT (Zhan et al., 2024). In the domain of sound processing, we evaluate LTU (Gong et al., 2024), LTU-AS (Gong et al., 2023), SALMONN (Tang et al., 2024), Qwen-Audio (Chu et al., 2023), NextGPT (Wu et al., 2023), PandaGPT (Su et al., 2023), X-InstructBLIP (Panagopoulou et al., 2023) and Pengi (Deshmukh et al., 2023). In addition, we include advanced proprietary models, Gemini-Flash\u00b3 and Gemini-Pro4, as the performance upper bound of speech tasks, since they cannot handle the sound tasks. Note that the examined ALLMs sometimes fail to follow the instructions, leading to the absence of predicted answers for closed-form tasks. In that case, we default the predicted answer to \"No\" or \"Audiol\", depending on the task context. Since MAE is class-balanced, this strategy effectively equates to a random guessing strategy."}, {"title": "MAE Results", "content": "MAE-Speech: The evaluation results for various ALLMs on the MAE-Speech benchmark (Tab. 2) show that all open-source ALLMs struggle with multi-audio scenarios across all tasks. The highest-performing model, Qwen-Audio, merely achieves an average accuracy of 39.6%. In closed-form tasks, these models tend to collapse to consistently give the same answer for all queries, leading to approximately 50% accuracy. For example, Qwen-Audio always answers \"Yes\" for intent identification. Furthermore, models like LTU-AS frequently fail to adhere to the provided instructions, result-ing in extremely poor performance, particularly in"}, {"title": "Discussion", "content": "Single-audio performance: Alongside multi-audio processing, we also evaluate the single-audio"}, {"title": "Single-audio performance", "content": "Single-audio performance: Alongside multi-audio processing, we also evaluate the single-audio"}, {"title": "Conclusion", "content": "In this paper, we introduce the first multi-audio evaluation benchmark, MAE, to examine the multi-audio processing ability of ALLMs. Extensive evaluations across 15 ALLMs reveal generally unsatisfactory performance among current open-source models in handling multi-audio scenarios. To this end, we develop a simple and effective discriminative training framework that leverages synthetic data. The resulting model, MALLM, significantly surpasses all existing open-source ALLMs without incurring additional costs for human annotation. This work lays a robust foundation for future research aimed at enhancing ALLM capabilities in multi-audio processing. Future work includes introducing more complex multi-audio tasks that involve more than two audios and expanding the MALLM training dataset to cover more scenarios."}, {"title": "Limitations", "content": "In this paper, we propose the first benchmark to evaluate the multi-audio analysis ability of ALLMs. We also develop a novel MALLM, which outperforms existing ALLMs on multi-audio processing. Yet, there are several limitations to this work. Firstly, considering the large performance gap between open-source and proprietary models, we deliberately design the tasks in MAE with a simple nature to obtain meaningful results from open-source ALLMs. Therefore, while MAE poses large challenges to various open-source ALLMs, some tasks are too simple for proprietary ALLMs like Gemini. In future work, we plan to explore more complex tasks, such as speech recognition (Gao et al., 2024b; Yue et al., 2024a), compositional reasoning (Ghosh et al., 2024), speech synthesis (Gao et al., 2024a) and lyrics transcription (Gao et al., 2023b). Secondly, the proposed MAE benchmark currently covers English data. Extending the MAE to the multilingual scenario is also an important future direction to ensure that it's comprehensive and applicable across diverse linguistic contexts. Thirdly, the proposed MALLM is currently trained on a relatively small scale of synthetic data due to computational resource constraints. We aim to further enhance the MALLM with large-scale, diverse training data, enabling its application to more challenging domains like singing (Gupta et al., 2019) and music (Gao et al., 2023a) in future work."}]}