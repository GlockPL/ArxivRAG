{"title": "RopeTP: Global Human Motion Recovery via Integrating Robust Pose Estimation with Diffusion Trajectory Prior", "authors": ["Mingjiang Liang", "Yongkang Cheng", "Hualin Liang", "Shaoli Huang", "Wei Liu"], "abstract": "We present RopeTP, a novel framework that combines Robust pose estimation with a diffusion Trajectory Prior to reconstruct global human motion from videos. At the heart of RopeTP is a hierarchical attention mechanism that significantly improves context awareness, which is essential for accurately inferring the posture of occluded body parts. This is achieved by exploiting the relationships with visible anatomical structures, enhancing the accuracy of local pose estimations. The improved robustness of these local estimations allows for the reconstruction of precise and stable global trajectories. Additionally, RopeTP incorporates a diffusion trajectory model that predicts realistic human motion from local pose sequences. This model ensures that the generated trajectories are not only consistent with observed local actions but also unfold naturally over time, thereby improving the realism and stability of 3D human motion reconstruction. Extensive experimental validation shows that RopeTP surpasses current methods on two benchmark datasets, particularly excelling in scenarios with occlusions. It also outperforms methods that rely on SLAM for initial camera estimates and extensive optimization, delivering more accurate and realistic trajectories.", "sections": [{"title": "1. Introduction", "content": "Imagine how you reason about missing pieces while solving a jigsaw puzzle or how you infer a person's motion trajectory from a single picture. This is analogous to the challenge in computer vision of estimating human shape and pose from images captured by a single camera. Just as you can use surrounding puzzle pieces to deduce the appearance of the missing piece (or exploit the joint positions of human motion to infer their trajectory), our method aims to reconstruct a complete 3D human mesh and precise motion trajectories from images, even when certain body parts are occluded or obscured.\nIn the past, the field heavily relied on parametric models such as SMPL [22], which transformed the complex task of 3D mesh reconstruction into a more manageable parametric regression problem. However, regression-based methods [13, 17, 23] often fail when encountering occlusions, much like how missing puzzle pieces make it difficult to perceive the whole picture.\nThis challenge becomes even more pronounced when body parts are occluded by themselves or external objects, a situation frequently encountered in the real world. Previous methods [13, 17] struggled with this, as they were often overly sensitive to such occlusions. Recent approaches [16, 30] attempted to address this issue by focusing on individual body parts, but these too had their limitations, typically relying excessively on the limited visible information surrounding the occluded areas. Consequently, these methods remain highly sensitive to occlusion issues, as illustrated in the upper half of Figure 1.\nOur work is inspired by the human ability to infer hidden details using contextual cues. Just as you might guess the content of a missing puzzle piece based on adjacent pieces, our method leverages multi-scale visual cues to reconstruct occluded body parts. We introduce a novel architecture that combines a hierarchical attention-guided segmenter with an adaptive contextual part regressor. This framework excels at extracting and synthesizing multi-scale visual information, much like assembling a puzzle while considering both close-up details and the bigger picture. Moreover, monocular camera-based reconstruction methods have long been plagued by the ambiguity of human trajectories, leading to a sharp decline in performance in 3D space. We emulate the human process of inferring motion trajectories based on joint poses by introducing a diffusion generative model, which re-infers global motion trajectories conditioned on the reconstructed joint poses to rectify the ambiguous global information. It's worth noting that this approach is efficient, which distinguishes it from the time-consuming trajectory optimization strategies typically employed in dynamic camera systems [28, 35]. Simultaneously, accurate human pose estimation, as a powerful prior knowledge, can assist the diffusion model in regenerating precise human trajectories. As depicted in the lower half of Figure 1, our method achieves a significant lead over both the static PARE [16] and dynamic GLAMR [35] approaches.\nOur method demonstrates exceptional performance on standard datasets such as 3DPW [29] and Human3.6M [12], as well as on occlusion-specific datasets, showcasing its robustness in handling occlusion complexity. In addition, we validate the precision of our global robust human mesh estimation by comparing it with dynamic camera methods on the EMDB [14] dataset. Extensive qualitative analyses, ablation studies, and video results in the supplementary material underscore the efficacy of our approach in resolving monocular camera trajectory ambiguity and addressing occlusion challenges."}, {"title": "2. Related Work", "content": "Regression-based methods. Regressing human pose and shape parameters from a single RGB image can be categorized into two main approaches: auxiliary regression and direct regression. Auxiliary regression strategies leverage prior knowledge from ancillary domains to facilitate parameter regression. Methods such as HybrIK [19] and IKOL [37] decompose pose parameters into a two-step motion process, while PARE [16] and A-LS [30] predict attention masks for regression. On the other hand, direct regression approaches extract features from a single image for model parameter regression without relying on auxiliary information. HMR [13] employs a CNN network and MLP layers for this purpose, while ROMP [27] maps each 3D pixel channel onto model parameter channels. Building upon HMR, CLIFF [20] utilizes bounding box information and introduces a full-image projection to enhance prediction results. Despite significant advancements in regression-based methods for human pose and shape estimation, their ability to predict global motion trajectories remains limited, constraining their practical applicability in real-world situations.\nOcclusion handling. Addressing occlusion is crucial in 3D human reconstruction. Acquiring occluded data is challenging, leading to artificial occlusions in previous studies [2, 5, 7, 25]. However, this often leads to unsatisfactory real-world outcomes.\nInferring occluded portions involves establishing relationships between discernible cues. VisDB [32] predicts mesh vertex coordinates and visibility labels. PARE [16] and A-LS [30] leverage implicit spatial relationships and adaptive attention mechanisms, respectively. Despite their ability to recover poses obscured by small-scale occlusions, errors involving part depth confusion often emerge in self-occlusion scenarios. BoPR [3] proposes a body-aware reference-based approach to combat depth ambiguity. However, occlusion remains an ill-posed problem. In contrast, our method harnesses attention mechanisms across diverse scales to guide the aggregation of multiple body-aware features, generating plausible results.\nGlobal human motion reconstruction. The task of deriving global human trajectories from a monocular dynamic camera is a complex one. Traditional methods have depended on previously learned human motion distributions to differentiate between human and camera movements. For example, GLAMR [35] forecasts the global trajectory by leveraging a predicted and infilled 3D motion sequence, optimizing it for multiple individuals in a given scene. However, the lack of consideration for camera motion cues in GLAMR can result in noisy trajectories, particularly when the camera is rotating. Alternative techniques like SLAHMR [33] and PACE [1] utilize readily available SLAM algorithms to simultaneously optimize both camera and human movements with the aim of minimizing the negative log likelihood of a learned motion prior [24]. While these traditional methods can produce satisfactory results, their optimization process is computationally intensive, potentially limiting their practical application. In contrast, our proposed method employs a robust pose estimator and learns trajectory prior using diffusion models, providing a more efficient and accurate solution for predicting human trajectories."}, {"title": "3. Method", "content": "The overall structure of RopeTP is illustrated in Figure 2. Its input is an RGB video captured by a monocular camera, and RopeTP's goal is to recover the global robust human mesh. To achieve this objective, we first employ the Rope module to estimate the precise human parameter sequence frame-by-frame, which serves as input to the SMPL model for obtaining 3D human joints \\(\\theta\\in \\mathbb{R}^{N\\times 24 \\times 3}\\). Subsequently, we sample \\(x_t \\sim \\mathcal{N}(0, I)\\) from pure Gaussian noise as the initial noise trajectory and linearly map local joint points to the feature space, which, along with the diffusion time steps, are fed into the TrajDenoiser. The regenerated global trajectory \\(r_o \\in \\mathbb{R}^{N \\times 3}\\) is ultimately obtained through 100 steps of DDIM denoising. The specific implementation details will be disclosed later in the manuscript."}, {"title": "3.1. Overview", "content": "The overall structure of RopeTP is illustrated in Figure 2. Its input is an RGB video captured by a monocular camera, and RopeTP's goal is to recover the global robust human mesh. To achieve this objective, we first employ the Rope module to estimate the precise human parameter sequence frame-by-frame, which serves as input to the SMPL model for obtaining 3D human joints \\(\\theta\\in \\mathbb{R}^{N\\times 24 \\times 3}\\). Subsequently, we sample \\(x_t \\sim \\mathcal{N}(0, I)\\) from pure Gaussian noise as the initial noise trajectory and linearly map local joint points to the feature space, which, along with the diffusion time steps, are fed into the TrajDenoiser. The regenerated global trajectory \\(r_o \\in \\mathbb{R}^{N \\times 3}\\) is ultimately obtained through 100 steps of DDIM denoising. The specific implementation details will be disclosed later in the manuscript.\nPreliminaries: In this section, we provide a brief overview of the human parametric model SMPL, which our work is based upon.\nThe SMPL model is a widely utilized parametric model for human body shape and pose estimation. Our method employs the SMPL parametric model to represent the human body. The model necessitates two essential parameters: pose, denoted as \\(\\theta \\in \\mathbb{R}^{72}\\), and shape, denoted as \\(\\beta \\in \\mathbb{R}^{10}\\). The SMPL model generates a 3D mesh \\(\\mathcal{M}(\\theta, \\beta) \\in \\mathbb{R}^{6890 \\times 3}\\) that represents the human body in a differentiable manner. Specifically, the mesh is defined by a set of vertices that are positioned in 3D space according to the pose and shape parameters. The reconstructed 3D joints are obtained by applying a pre-trained linear regression matrix \\(W\\) to the mesh vertices, resulting in \\(J_{3p} = WM \\in \\mathbb{R}^{J \\times 3}\\), where \\(J = 24\\)."}, {"title": "3.2. Network Architecture of Rope", "content": "Our Rope (Robust Pose Estimation) module draws inspiration from the human ability to infer missing information using contextual clues, much like solving a puzzle with some pieces hidden or missing. This approach is particularly relevant in the challenge of estimating human shape and pose from single-camera images, where occlusions are a common obstacle. In our method, we first define a hierarchical body division strategy that interpret ate human body parts into four levels. Then, as depicted in the Figure 3, we employ the Hierarchical Attention Guided Tokenizer (HAGT) to extract body part features of four levels. Since the human body exhibits different levels of coordination in various activities. By mirroring this inherent structure, we can capture the nuances of human movement more accurately. Moreover, different levels of body coordination provide varying visual cues, which are crucial in scenarios where parts of the body are occluded. Therefore, by dividing the body into hierarchical levels, we can utilize visible segments to infer the posture of occluded parts, akin to completing a puzzle using surrounding pieces. As shown in Figure 4, we categorize the human body into four hierarchical levels of coordination, with a specific focus on handling"}, {"title": "Hierarchical Attention Guided Tokenizer.", "content": "This module is designed to utilize the attention mechanism for extracting hierarchical part feature tokens. The attention mechanism is specifically fine-tuned to be sensitive to occlusions, which allows the framework to concentrate on available visual cues and effectively infer missing information. In our implementation, we pass the feature blocks obtained from the backbone through convolutional layers, resulting in attention maps at each coordinated hierarchy level, denoted as \\(Att_x \\in \\mathbb{R}^{H \\times W \\times J}\\) (where x represents a specific scale level, and J stands for the number of parts at the corresponding scale level). At the same time, to obtain part masks for"}, {"title": "Adaptive Contextual Part Regressor.", "content": "To facilitate interaction with visual cues at various hierarchical levels and uncover appropriate reasoning relationships, we introduce the Adaptive Contextual Part Regressor (ACPR). Initially, it uses WhoBo features as reference dependencies and shares them across tokens of other hierarchical levels to create global-aware token sequences. The intra-hierarchical self-attention layers then adaptively learn part dependencies to optimize token sequences within each hierarchy level. Finally, we input the optimized sequences into cross-hierarchical attention layers to interact with visual cues across different hierarchical levels, reconstructing precise and robust mesh results based on a wealth of visible information."}, {"title": "ISL (Intra-hierarchical Self-Attention Layer).", "content": "First, we expand the WhoBo features to connect them to part feature channels at each hierarchy level. Then, we construct three sets of token sequences at different hierarchy levels, \\(T_x \\in \\mathbb{R}^{N \\times C}\\) (where N represents the number of queries corresponding to the hierarchy level, C denotes the fused feature dimension, and x refers to a specific scale level). Concurrently, we introduce a two-layer Multi-Head Self-Attention layer to encode global-aware part tokens \\(F' \\in \\mathbb{R}^{N \\times 128}\\), enhancing visual cues within each scale level. As a result, our token sequences establish part dependencies within each hierarchy level, improving inference logic and endowing each token with the ability to perceive holistic information."}, {"title": "ICL (Inter-hierarchical Cross-Attention Layer).", "content": "After obtaining the optimized token sequences at three hierarchical levels, we introduce the inter-hierarchical cross-attention layer to enable interaction of visual cues among tokens at different hierarchical levels. This allows features at all hierarchical levels to mutually reinforce each other, resulting in satisfactory and reasonable predictions. Specifically, we first expand the Inter and FulCo tokens to match the 24 joints of the SMPL model, such as the shoulder, arm, and elbow joints, which all correspond to the FulCo arm token. Next, diverging from the traditional cross-attention layer strategy that employs identical K and V values, we use token sequences from the three hierarchical levels as Q, K, and V matrices for the cross-attention layer input. This not only enhances computational efficiency but also more effectively interacts with visual cues across hierarchical levels, providing the model with a more robust feature selection capability. Furthermore, since the visual cues across hierarchical levels are highly redundant, necessary information is not lost while reducing computational demands (see Supplementary Materials for details). The formula is as follows:\n\n\n\nSubsequently, we associate three cross-hierarchical interaction visual cues and project them linearly to the SMPL rotation angles represented in rot6d through an MLP layer, serving as the final output. The camera and shape parameters are directly linked to the features of the three levels and are obtained through projection via an MLP layer.\n\\(Mix = Concat(AttIndep, AttInter, AttFulCo)),\\)\n\\(\\theta = MLP(LN(Mix)),\\)\nBy inputting the regression-obtained rotation angles \\(\\theta \\in \\mathbb{R}^{24 \\times 6}\\), camera parameters \\(cam \\in \\mathbb{R}^{3}\\), and shape parameters \\(\\beta \\in \\mathbb{R}^{10}\\) into the SMPL model, we can acquire 3D mesh points \\(M_{3D} \\in \\mathbb{R}^{6890 \\times 3}\\) and 3D keypoints \\(J_{3D} \\in \\mathbb{R}^{24 \\times 3}\\). Furthermore, by utilizing camera parameters and weak perspective projection, we can also obtain 2D mesh points \\(M_{2D} \\in \\mathbb{R}^{6890 \\times 2}\\) and 2D keypoints \\(J_{2D} \\in \\mathbb{R}^{24 \\times 2}\\)."}, {"title": "3.3. Trajectory Denoiser", "content": "In this chapter, we introduce a diffusion-based trajectory regeneration model. We represent the reconstructed human trajectory sequences from Rope as \\(r_{1:N}\\) and associate them with the corresponding control signal \\(c\\) (here, the encoded joint space positions \\(p\\)), where N denotes the number of frames in the trajectory sequence. We employ a diffusion probability model [11] to regenerate human trajectories, where the diffusion model progressively anneals pure Gaussian noise into the trajectory distribution \\(p(r)\\). Consequently, the model can predict noise from a T-time-step Markov noise process \\(r_{1:N}^T, r_t \\thicksim r_T\\), with \\(r_t^T \\thicksim \\mathcal{N}\\): directly sampled from the original data distribution. The diffusion process is as follows:\n\n\\(q(r_t|r_{t-1}) = \\mathcal{N}(r_t; \\sqrt{1 - \\beta_t}r_{t-1}, \\beta_tI),\\)\n\\(= \\mathcal{N}(r_t; \\sqrt{\\frac{\\alpha_t}{\\alpha_{t-1}}}r_{t-1}, (1 - \\frac{\\alpha_t}{\\alpha_{t-1}})I),\\)\nwhere \\(\\{\\beta_t\\}_{t=1}^T\\) is the variance schedule and \\(\\alpha_t = \\prod_{s=1}^t (1 - \\beta_s)\\). Then the reverse process becomes \\(p_{\\theta}(r_{0:T}) := p(r_T) \\prod_{t=1}^T p_{\\theta}(r_{t-1}|r_t)\\), starting from \\(r_T \\sim \\mathcal{N}(0, I)\\) with noise predictor \\(\\epsilon_{\\theta}\\):\n\\(r_{t-1} = \\frac{1}{\\sqrt{1 - \\beta_t}}(r_t - \\frac{\\beta_t}{\\sqrt{1 - \\alpha_t}}\\epsilon_{\\theta}(r_t, t)) + \\sigma_t z_t,\\)\nwhere \\(z_t \\sim \\mathcal{N}(0, I)\\) and \\(\\sigma_t^2 = \\frac{\\beta_t}{1 - \\alpha_t}\\) means the variance schedule stays constant.\nHowever, unlike the original DDPM [11], we consider the inherent extreme physical constraints of 3D human bodies and replace the predicted noise with the original human trajectory, deviating from image generation. As illustrated in Figure 2, at each step of the denoising process, we reconstruct the original trajectory from pure Gaussian noise, ultimately producing the final generated result through a cyclic process of noise addition and denoising:\n\\(r_o = \\frac{r_t - \\sqrt{1 - \\alpha_t} \\epsilon_{\\theta}(r_t|c)}{\\sqrt{\\alpha_t}},\\)\n\\(r_{t-1} = \\sqrt{\\alpha_{t-1}}r_o + \\sqrt{1 - \\alpha_{t-1}} \\sigma_t \\cdot \\epsilon_{\\theta}^z(r_t|c) + \\sigma_t z_t,\\)\nwhere c represents the joint space positions after being encoded by the conditional signal encoder and \\(\\sigma\\) represents a unified form of the non-Markovian process, where we consistently maintain the DDIM process with a constant value of 0. The regenerated trajectory, combined with joint space coordinates, undergoes optimization in a physics simulation environment [36], resulting in our final output."}, {"title": "3.4. Loss Function", "content": "As Rope module is an end-to-end process trained through explicit supervision, our objective is to minimize the discrepancy between the predicted results and the ground truth labels as much as possible. To capture the prediction errors, we define the following loss functions for our network:\n\\(Loss = L_{SMPL} + L_{3D} + L_{2D} + L_{Att},\\)\n\\(L_{3D} = ||J_{3D} - \\hat{J}_{3D}||,\\)\n\\(L_{2D} = ||J_{2D} - \\hat{J}_{2D}||,\\)\n\\(L_{SMPL} = ||\\theta - \\hat{\\theta}||^2_2 + ||\\beta - \\hat{\\beta}||^2_2,\\)\n\\(L_{Att} = -\\frac{1}{HW} \\sum_{h,w} CrossEntropy(\\sigma(Att_{h,w}), Seg_{h,w}),\\)"}, {"title": "4. Experiments", "content": "Here, we initially conduct a comparative analysis with other advanced methods on two publicly available generic 3D human pose datasets. Subsequently, we provide comprehensive ablation studies to validate the contribution of each module and the effectiveness of our structural design, thereby highlighting the robustness and superiority of our approach."}, {"title": "4.1. Comparison of 3D Human Reconstruction", "content": "We initially conduct a comprehensive comparison with other methods on two publicly available benchmarks, namely 3DPW [29] and Human3.6M [12]. For the 3DPW dataset, we start by training on a combined dataset (COCO, Human3.6M, MPII, and 3DHP) and then fine-tune the network on the 3DPW training dataset. In contrast, for Human3.6M, we exclusively train on the mixed dataset and evaluate according to the official protocol 2. Additionally, we present two network configurations, one with and one without global-aware references from WhoBo, and showcase the corresponding results. Table 1 displays the comparative results of various approaches.\nOur method outperforms nearly all existing techniques in terms of MPJPE and MPVPE. Particularly noteworthy is the significant improvement in MPVPE, an indicator that our reconstructed results more accurately conform to the human body in the original image. Unlike HMR2.0, which is entirely based on a Transformer structure, our method manages to maintain a reconstruction speed close to that of baseline models, PARE and CLIFF, while reducing MPVPE from 81.2 to 78.0, even without a global perception reference (from 81.2 to 80.2). Considering that the latest methods, such as NIKI and PLIKS, have not released open-source training and evaluation models, we compare our approach based on the original performance metrics. Although these new methods exhibit superior performance on the PA-MPJPE metric in the 3DPW dataset, it is important to note that they utilize additional modules (e.g., 3D keypoint priors, posterior distribution sampling). In contrast, our regression-based end-to-end approach is more straightforward and easier to train. HMR2.0, on the other hand, is constrained by its pure ViT architecture, resulting in a slow regression speed that cannot support real-time tasks in practical applications. Supported by subsequent qualitative analysis, the experimental results validate that our approach not only predicts more accurate mesh and body posture, but also exhibits robustness against the instability caused by occlusions."}, {"title": "4.2. Occlusion Robustness", "content": "Table 2 presents the evaluation results on the occlusion datasets 3DPW-OCC and 3DOH [38] for our proposed method, the baseline PARE, CLIFF, and other regression-based methods like BoPR. Specifically, 3DPW-OCC is an occluded subset of the 3DPW dataset, while 3DOH is a challenging dataset proposed by Zhang et al., featuring extensive human body occlusions. All methods are evaluated on 3DPW-OCC without using the 3DPW training dataset. During the 3DOH evaluation, the network is fine-tuned on the 3DPW training set. Both evaluations exclude their respective training sets to ensure a fair comparison of generalization ability on occlusion datasets. PARE and CLIFF are retrained by us, while BoPR is evaluated using the original metrics. Our method outperforms previous ones in terms of MPJPE and PA-MPJPE metrics. Specifically, on the 3DPW-OCC dataset, our method reduces the errors of both metrics by 4.8% and 6.6% respectively compared to the baseline. These quantitative results demonstrate that cross-level attention inference effectively alleviates occlusion challenges, enhancing accuracy and generalization ability in demanding scenarios.\nIn Figure 5, we demonstrate the performance of three methods (our proposed approach, PARE, and CLIFF) on the 3DPW test set and outdoor datasets under occlusion conditions. PARE represents a local regression method based on a single attention-guiding mechanism, while CLIFF is a single-output method based on global feature regression. The results indicate that our approach effectively reconstructs human body meshes in both indoor and outdoor real-world scenarios. Notably, under the occlusion conditions shown in the third row, previous methods struggle to reasonably infer occluded limbs. Relying solely on visible right wrist information, both PARE and CLIFF incorrectly infer the self-occluded right arm as a naturally hanging posture. In contrast, our approach accurately reconstructs the upwardly extended arm posture through a cross-hierarchical inference strategy. Moreover, our approach demonstrates a significant advantage in fitting the mesh to the original image compared to previous methods. This qualitative analysis emphasizes the superiority of the multi-level attention inference approach in addressing challenging occlusion scenarios."}, {"title": "4.3. Diffusion-Based Trajectory Regeneration", "content": "To evaluate the accuracy of our regenerated global trajectories, we introduce an additional EMDB dataset [14],"}, {"title": "4.4. Ablation Study", "content": "Ablation study on Rope, we conduct ablation studies on the 3DPW-TEST and 3DPW-OCC datasets to thoroughly evaluate the impact of various components within our method. We present the results in terms of MPJPE and MPVPE metrics. Table 4 outlines our ablation results, focusing on the significance of the Hierarchical Attention Guided Tokenizer (HAGT) and Adaptive Context Part Regressor (ACPR) modules in handling occlusions. Initially, we incorporate the Transformer structure into the baseline model PARE, resulting in limited improvement due to constrained visual cues at the Indep level. Next, we introduce HAGT to capture multi-level visible information and achieve a substantial enhancement in the MPVPE metric through simple fusion, significantly mitigating occlusion interference. Ultimately, we eliminate the reliance on WhoBo reference information and depend exclusively on the interaction of visual cues from three distinct hierarchical levels for inference. This approach still marginally surpasses the BoPR, which depends on reference features. Furthermore,\nthis cross-hierarchical visual information interaction substantially strengthens the model's robustness against occlusions, leading to a remarkable improvement on the 3DPW-OCC dataset.\nAblation study on TP, Table 5 presents the EMDB evaluation values at each scale level after trajectory generation using the Trajectory Predictor (TP). The results indicate that large-scale features effectively handle occlusion but suffer from larger global displacement errors due to inaccurate end positions. In contrast, small-scale features excel in global metrics but struggle with occlusion (worse MJE).\nFurthermore, we performed additional ablation on the TP module's representation. Our approach, which uses joint location as the guiding condition and regenerates xyz coordinates, outperforms other methods (rot6d representation). The term \"w/o imitation\" refers to the absence of physical simulation optimization. Simulated gravity and friction forces significantly reduce issues like sliding and floating, enhancing global metrics. However, the built-in IK mechanism adjusts global actions, making them subjectively more natural but with a minor impact on metrics."}, {"title": "5. Conclusion", "content": "In summary, our innovative method addresses occlusion challenges in human body mesh reconstruction by introducing a trajectory diffusion generation model for optimizing monocularly-captured motion trajectories. Adaptable to diverse 3D applications, the Rope module employs hierarchical attention-guided segmenters and adaptive context part regressors to capture multi-level visual cues for accurate mesh inference under severe occlusion. Our method outperforms state-of-the-art approaches, particularly in heavily occluded scenes, demonstrating its potential for robust pose estimation and 3D reconstruction. Furthermore, RopeTP regenerates trajectories using a diffusion model, enhancing its applicability to motion capture multimedia applications (refer to Supplementary Video). However, some rough physical problems can also be implemented by referring to knowledge in the field of reinforcement learning and generation [4, 9, 10, 31, 34]."}]}