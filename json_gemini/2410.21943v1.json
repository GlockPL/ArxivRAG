{"title": "Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications", "authors": ["Monica Riedler", "Stefan Langer"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in answering questions, but they lack domain-specific knowledge and are prone to hallucinations. Retrieval Augmented Generation (RAG) is one approach to address these challenges, while multimodal models are emerging as promising AI assistants for processing both text and images. In this paper we describe a series of experiments aimed at determining how to best integrate multimodal models into RAG systems for the industrial domain. The purpose of the experiments is to determine whether including images alongside text from documents within the industrial domain increases RAG performance and to find the optimal configuration for such a multimodal RAG system. Our experiments include two approaches for image processing and retrieval, as well as two LLMs (GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies involve the use of multimodal embeddings and the generation of textual summaries from images. We evaluate our experiments with an LLM-as-a-Judge approach. Our results reveal that multimodal RAG can outperform single-modality RAG settings, although image retrieval poses a greater challenge than text retrieval. Additionally, leveraging textual summaries from images presents a more promising approach compared to the use of multimodal embeddings, providing more opportunities for future advancements.", "sections": [{"title": "1 Introduction", "content": "The release of Large Language Models (LLMs), such as Llama3 (Meta LLaMA Team, 2024) and GPT-4 (OpenAI, 2023a), has significantly advanced the field of Natural Language Processing (NLP), enabling a wide range of applications, including automated content generation and conversational agents. However, LLMs often still lack domain-specific knowledge and are prone to hallucinations (Kandpal et al., 2023; Rawte et al., 2023).\nRetrieval Augmented Generation (RAG) addresses these limitations by combining document retrieval with generative language models.\nRecently, Multimodal Large Language Models (MLLMs) have emerged, extending LLM capabilities to include modalities like images and videos (Zhang et al., 2024; Yin et al., 2023). This development holds significant potential for industrial settings such as manufacturing, engineering, and maintenance, where documents like manuals, software guides, and product brochures frequently combine complex technical text with detailed visuals, such as diagrams, schematics and screenshots. This combination of modalities makes the industrial domain particularly challenging for AI systems, as they must accurately interpret both textual and visual information to provide meaningful insights.\nWhile extensive research has been conducted on text-only RAG systems and their optimization (Gao et al., 2023; Siriwardhana et al., 2023), the application of multimodal RAG to the industrial domain is less documented in academic literature. Existing examples mainly target general-domain datasets (Chen et al., 2022; Lin and Byrne, 2022) and medical applications (Sun et al., 2024; Xia et al., 2024; Zhu et al., 2024).\nIn our paper, we explore the integration of multimodal models into RAG systems for the industrial domain. Specifically, we investigate whether incorporating images alongside text enhances RAG performance and we identify optimal configurations for such systems. We use two MLLMs, GPT-4Vision (OpenAI, 2023b) and LLaVA (Liu et al., 2024), for answer synthesis and evaluate two image processing strategies: multimodal embeddings and textual summaries from images.\nOur research addresses two primary questions: (1) Does the inclusion of both text and images improve the performance of RAG systems in the industrial domain compared to single-modality RAG? (2) How can the performance of multimodal"}, {"title": "2 Related Work", "content": "Multimodal LLMs In recent years, LLMs have shown emergent abilities such as in-context learning, instruction following, and chain-of-thought reasoning (Brown et al., 2020; Ouyang et al., 2022; Wei et al., 2024), making them suitable for various NLP tasks. Multimodal LLMs extend these capabilities to understand and generate multiple modalities, enabling AI assistants to process text, images, videos, and audio. Notable examples include the integration of pre-trained unimodal models into multimodal systems, which consist of a modality encoder, a pre-trained LLM, and a modality generator, connected by projectors to transform inputs and outputs across different modalities (Zhang et al., 2024; Yin et al., 2023).\nRetrieval Augmented Generation RAG has emerged as an effective approach to address the limitations of LLMs in domain-specific question answering. By combining an LLM for answer generation with an external vector database accessed via a retriever, RAG has been effectively applied to various tasks, including question answering, fact verification, and question generation, achieving state-of-the-art results in open-domain question answering (Lewis et al., 2020; Guu et al., 2020; Izacard and Grave, 2021; Borgeaud et al., 2022). Key optimizations to the initial approaches are extensively described by Gao et al. (2023). Despite significant advancements, challenges such as retrieval quality (Ma et al., 2023; Carpineto and Romano, 2012), the reliability of the generation component"}, {"title": "3 Approach", "content": "3.1 Data\nDue to the lack of annotated context-question-answer triplets in the industrial domain, and even more so for a multimodal setting requiring quadruples of text context, image context, question, and answer, we created a manually annotated dataset\u00b9.\nWe used 20 PDF documents from the industrial domain, such as manuals and software documentation for devices like programmable controllers,"}, {"title": "3.2 Experiments", "content": "In this section, we outline the experiments we conducted to investigate two primary questions: (1) Does using both text and image modalities improve performance? (2) What is the optimal configuration for a multimodal RAG system in the industrial domain? We categorize the experiments into three RAG settings: Text-Only RAG, Image-Only RAG, and Multimodal RAG. Additionally, we implemented two reference settings for comparison: (1) a Baseline, where we feed questions directly to an LLM without retrieval, and (2) a Gold Standard Context setting, which serves as an upper bound. We used a prompt to instruct the model to answer based on the retrieved context (Figure 3 in Appendix). To ensure consistency across experiments, we ran all settings with both GPT-4V and LLaVA, including the text-only settings, which do not require multimodal content processing. We report implementation details on vector databases, retrieval, model versions, and hyperparameters in Appendix A.\n3.2.1 Baseline\nOur baseline consists in feeding questions from the test set directly to the LLM, without any retrieval step. This allowed us to test the LLM's internal knowledge and gain insight into its performance on domain-specific industrial questions.\n3.2.2 Text-Only RAG\nIn the Text-Only RAG setting, we used only the texts extracted from the PDF collection. We embedded the text chunks using OpenAI's text-embedding-3-small\u00b2 model and stored them in a vector store. We then performed a similarity search on the vector store for each embedded question to retrieve the most relevant texts, which were concatenated with the user query and passed to the multimodal LLM for answer generation. This setup allowed us to evaluate the performance of text-based retrieval and answer synthesis.\n3.2.3 Image-Only RAG\nIn this setting, we only used images from the PDF documents. For image retrieval, we explored two distinct approaches:\nMultimodal Embeddings We used CLIP (Radford et al., 2021) to jointly embed both images and questions. CLIP was selected for its ability to align image and text modalities in a shared embedding space, which allows to easily compute similarities between different types of data. This alignment is crucial for multimodal retrieval tasks, where understanding the relationship between image content and textual queries is key. We stored the obtained embeddings in a vector store, and performed a similarity search to retrieve the most relevant images based on the embedded query.\nText Embeddings From Image Summaries We summarized the images into text using a multimodal LLM (see Figure 4 in Appendix) and then embedded these summaries using text-embedding-3-small. We employed LangChain's Multi-Vector Retriever\u00b3, which allows to decouple the retrieval and generation sources. Summaries were stored in a vector store, while the original images were stored in a document store, allowing retrieval through textual summaries while preserving the original images for answer generation to reduce potential information loss.\n3.2.4 Multimodal RAG\nFigure 1 provides an overview of our multimodal RAG approaches, where we combined text and image modalities. We reused the two image retrieval methods from the Image-Only RAG setting,\nadding text retrieval. We explored two configurations within this setup:\nMultimodal Embeddings and Separate Vector Stores We embedded images using CLIP and texts using text-embedding-3-small, storing them in separate vector stores. We embedded the query for both modalities and performed a separate similarity search in each store, ensuring both text and image results are retrieved.\nImage Summaries and Combined Vector Store We converted images into text summaries and embedded these, along with text chunks extracted from the PDF documents, using text-embedding-3-small. Both were stored in a single vector store. A similarity search was then performed to retrieve the most relevant documents, whether text or image, based on the query embedding.\n3.2.5 Gold Standard Context Prompting\nIn the Gold Standard Context setting, we directly provided the annotated context from the test set to the LLM along with the question, skipping the retrieval step. This setup serves as an upper bound, demonstrating the performance achievable with perfect retrieval and enabling a direct comparison between the generating models (GPT-4V and LLaVA) independently of retrieval performance."}, {"title": "4 Evaluation Framework", "content": "We evaluate the performance of the RAG pipelines using an LLM-as-a-Judge approach (Chiang et al., 2024; Zhang et al., 2023), developing a custom\nevaluation framework tailored to multimodal data. This framework incorporates metrics similar to those in existing text-only RAG evaluation frameworks, such as RAGAs (Es et al., 2024). However, our framework enables evaluating multimodal content to handle both text and images, ensuring a comprehensive assessment of the RAG system's performance. We make the code for all experiments and the evaluation framework available at the following URL: https://github.com/\nriedlerm/multimodal_rag_for_industry.\nThe framework is designed to be modular and can be used with multiple models as evaluators, including GPT-4V and LLaVA. The core of the framework consists of an evaluation module that includes specialized evaluators for each metric. These evaluators construct prompts for the model, execute the evaluation, and parse the model's output to ensure it meets the required format. The delivered output for each metric includes a binary grade (either 1 or 0) and a reason for the judgment to facilitate easy aggregation and analysis of the results. The final score for each metric is obtained by averaging all binary evaluations over the dataset.\nEvaluation Metrics The framework employs six evaluation metrics: Answer Correctness uses reference-guided pairwise comparison to evaluate the correctness of the generated answer compared to a reference answer and is the only metric relying on the presence of ground truth answers; Answer Relevancy assesses whether the generated answer is relevant to the question; Text Faithfulness mea-"}, {"title": "5 Results", "content": "We summarize the performance of GPT-4V and LLaVA in 9 different settings including single-modality and multimodal RAG approaches in Figure 2. Unlike LLaVA, which was limited to processing a single image per prompt during our experiments, GPT-4V has the capability to handle multiple images and interleaved text-image sequences. We assessed its performance both using one image, for comparison with LLaVA, and multiple images to inspect the effect of using multiple images as context. In settings utilizing image summaries for retrieval, the summarizing model (LLaVA or GPT-4V) is consistently used for answer synthesis. Both models, i.e., LLaVA and GPT-4V, are used for evaluation. This means that the final score is derived from averaging evaluations conducted with both models, regardless of the generator model chosen. This approach helps mitigate self-enhancement bias (Chiang et al., 2024) and avoids single-judge evaluations. We report the full results of our experiments in Appendix D.\n5.1 Single-Modality vs. Multimodal RAG\nIn Figure 2a, we show the Answer Correctness for single-modality and multimodal settings to investigate whether a combination of text and images improves performance. The upper bound results, obtained by prompting with the gold standard context, reveal that using both text and images sig-"}, {"title": "6 Conclusion", "content": "Our research demonstrates the potential of integrating multimodal models into RAG systems for the industrial domain. By incorporating both text and images, we observed significant improvements in performance, particularly when the retrieval process successfully identifies relevant texts and images. Leveraging textual summaries from images provides greater flexibility and optimization opportunities compared to multimodal embeddings. Despite the challenges associated with image retrieval, our findings underscore the importance of multimodal data in enhancing the quality of generated answers. Future work will focus on refining image retrieval, comparing our results with fine-tuning-based approaches, and combining RAG with multimodal LLMs fine-tuned for the industrial domain. Additionally, we plan to evaluate each pipeline step independently on domain-specific data to better identify potential failure points, especially in image retrieval, even though our current analysis focused on end-to-end metrics."}, {"title": "7 Limitations", "content": "While our study demonstrates promising results, several areas require further investigation. First, the lack of publicly available, domain-specific datasets restricts the reproducibility and generalizability of our findings, highlighting the need for future work to develop such resources.\nWhile GPT-4V and LLaVA were effective, they share common LLM limitations, including inaccuracies, hallucinations, and difficulty handling complex multimodal inputs. Additionally, our evaluation relies on LLMs, which may introduce biases. While we mitigated this by using both GPT-4V and LLaVA, human evaluations remain essential for a reliable assessment. New multimodal models, such as GPT-40 (OpenAI, 2024), emerged during our study but could not be included. Continuous evaluations with the latest models are necessary.\nAlthough we focused on the industrial domain, we believe our approach can be applied to other domains as well, since no domain-specific components were used at any stage of our pipeline. Additional experiments can help confirm the applicability of our findings in other fields, such as healthcare or finance."}, {"title": "A Implementation Details", "content": "A.1 Vector Databases and Retrievers\nFor our RAG pipelines, we chose ChromaDB (Chroma, 2022) as our vector database because of its open-source nature, local usability, and seamless integration with LangChain (Chase, 2022), which we used as the framework for our experiments. We employed the Hierarchical Navigable Small World (HNSW) search method (Malkov and Yashunin, 2020) along with L2 similarity search.\nEmbeddings were generated using either CLIP ViT-L/14 via the OpenCLIP implementation (Cherti et al., 2022) for settings with multimodal embeddings, or OpenAI's text-embedding-3-small for text and image summaries.\nFor retrieval, we employed LangChain's VectorStoreRetriever for CLIP embeddings and the MultiVectorRetriever for text and image summaries embedded with text-embedding-3-small.\nA.2 Number of Retrieved Documents\nWe maintained the parameter k, representing the number of documents retrieved during the pipeline's retrieval step, at LangChain's default value of 4 across all experiments to ensure consistency. In settings with separate vector stores and retrievers for text and image embeddings, k was set to 2 for each modality, ensuring a total of 4 retrieved documents. Consequently, in these settings, the top 2 images and the top 2 texts were always retrieved. Conversely, in settings with a single vector store for both texts and textual summaries, the distribution of retrieved texts and summaries varied depending on the relevance of the documents. This variation implies that if the top 4 retrieved documents were all texts, there could be instances where no images were retrieved, and vice versa. We acknowledge that this approach might not be ideal, as the fixed value of k may not always capture the most relevant documents, especially in cases where the relevance distribution between text and images is uneven. However, we adopted this strategy to maintain uniformity and simplicity across all experiments, thereby facilitating a controlled comparison of the retrieval mechanisms.\nA.3 Model Versions and Hyperparameters"}, {"title": "B Prompt Templates", "content": "B.1 RAG Prompt Templates\nQA Prompt:\nYou are an expert Al assistant that answers\nquestions about manuals from the industrial\ndomain.\nYou will be given some context consisting of text\nand/or image(s) that can be photos, screenshots,\ngraphs, charts and other.\nUse this information from both text and image (if\npresent) to provide an answer to the user question.\nUser-provided question:\nText:\nImage:\nImage Summarization Prompt:\nYou are an assistant tasked with summarizing\nimages for retrieval.\nThese summaries will be embedded and used to\nretrieve the raw image.\nGive a concise summary of the image that is\noptimized for retrieval.\nAnswer Correctness Prompt:\nYou are asked to grade the student's answer as\neither correct or incorrect, based on the reference\nanswer.\nIgnore differences in punctuation and phrasing\nbetween the student answer and true answer.\nIt is OK if the student answer contains more\ninformation than the true answer, as long as it does\nnot contain any conflicting statements.\nUSER QUERY:\nREFERENCE ANSWER:\nSTUDENT ANSWER:\nanswer_correctness: Is the student's answer\ncorrect? (YES or NO)\nWrite out in a step by step manner your reasoning\nto be sure that your conclusion is correct by filling\nout the following JSON format with the grade and a\nconcise reason behind the grade:\n{grade: '', 'reason': ''}\nOutput the reason as a string, not as a list.\nThe only allowed grades are YES or NO.\nB.2 Evaluation Prompt Templates\nAnswer Relevancy Prompt:\nEvaluate the following metric:\nanswer_relevancy: Is the answer \"...\" relevant to the\nuser's query \"...\"? (YES or NO)\nWrite out in a step by step manner your reasoning to\nbe sure that your conclusion is correct by filling out\nthe following JSON format with the grade and a\nconcise reason behind the grade:\n{grade: '', 'reason': ''}\nOutput the reason as a string, not as a list.\nThe only allowed grades are YES or NO.\nText Faithfulness Prompt:\nEvaluate the following metric:\ntext_faithfulness: Is the answer faithful to the\ncontext provided by the text, i.e. does it factually align\nwith the context? (YES or NO)\nANSWER:\nTEXT:\nWrite out in a step by step manner your reasoning to\nbe sure that your conclusion is correct by filling out\nthe following JSON format with the grade and a\nconcise reason behind the grade:\n{grade: '', 'reason': ''}\nOutput the reason as a string, not as a list.\nThe only allowed grades are YES or NO."}, {"title": "C Dataset Details", "content": "C.1 Text and Image Extraction\nWe used the PyMuPDF6 library to extract text and images from industrial domain PDFs, creating a structured dataset stored in a parquet file. Each entry represents a page, with text and corresponding images aligned by page. If a page contained multiple images, each image was stored as a separate entry, along with the text of the page.\nC.2 Question and Answer Annotation\nTo generate the RAG test set, we manually annotated 100 question-answer pairs. The questions"}]}