{"title": "Deformable 3D Shape Diffusion Model", "authors": ["Dengsheng Chen", "Jie Hu", "Xiaoming Wei", "Enhua Wu"], "abstract": "The Gaussian diffusion model, initially designed for image generation, has recently been adapted for 3D point cloud generation. However, these adaptations have not fully considered the intrinsic geometric characteristics of 3D shapes, thereby constraining the diffusion model's potential for 3D shape manipulation. To address this limitation, we introduce a novel deformable 3D shape diffusion model that facilitates comprehensive 3D shape manipulation, including point cloud generation, mesh deformation, and facial animation. Our approach innovatively incorporates a differential deformation kernel, which deconstructs the generation of geometric structures into successive non-rigid deformation stages. By leveraging a probabilistic diffusion model to simulate this step-by-step process, our method provides a versatile and efficient solution for a wide range of applications, spanning from graphics rendering to facial expression animation. Empirical evidence highlights the effectiveness of our approach, demonstrating state-of-the-art performance in point cloud generation and competitive results in mesh deformation. Additionally, extensive visual demonstrations reveal the significant potential of our approach for practical applications. Our method presents a unique pathway for advancing 3D shape manipulation and unlocking new opportunities in the realm of virtual reality.", "sections": [{"title": "1. Introduction", "content": "The task of developing a generative model for three-dimensional (3D) shapes, which includes point clouds and meshes, has become a pivotal challenge with numerous applications [14, 29, 36]. The Gaussian diffusion model has demonstrated exceptional performance in image generation tasks. Building on previous studies [22, 39], our research aims to extend the diffusion model to support the generation of 3D shapes.\nTo gain a comprehensive understanding of 3D shape generation as a diffusion process, we interpret the discrete coordinates that comprise the shape as particles within a non-equilibrium thermodynamic system. This system interacts with a heat bath, causing the particle positions to evolve stochastically. Over time, these particles undergo diffusion, gradually dispersing throughout the 3D space-a phenomenon known as the diffusion process. Concurrently, noise is added at each time step to progressively transform the initial particle distribution into a simple noise distribution [22].\nDrawing an analogy, we can link the distribution of coordinates in point clouds or meshes to a noise distribution through the diffusion process. This concept is fundamental to the probabilistic diffusion model that underpins our methodology for 3D shape generation. However, unlike pixel data, 3D data is characterized by the interplay between spatial (coordinate) positions and geometric feature information. Consequently, the introduction of noise to the coordinates not only alters their spatial positions but also disrupts the local geometric structure. Therefore, the traditional diffusion process becomes significantly more challenging to regulate when applied to 3D data, causing meaningful geometric information to dissipate rapidly within a few steps, as illustrated in Fig. 2. This constraint presents substantial hurdles in modeling fine-grained geometric deformations and limits the efficacy of mesh generation techniques.\nTo address the challenges associated with 3D data manipulation, we introduce a novel deformable 3D shape diffusion model. Our model employs a unique Differential Deformation Kernel (DDK) to diffuse a geometric distribution into a predefined template distribution, in contrast to the commonly utilized Gaussian Diffusion Kernel (GDK) in image data. By decomposing the intricate geometric structure into numerous subtle, consecutive samples via the DDK, we aim to capture the gradual deformation characteristics of geometric structures, leveraging the power of the imitation process. This diffusion process enables us to manage 3D data in a geometrically-aware manner.\nTo facilitate fine-grained geometric deformation and restore the original geometric distribution from a given template distribution, we introduce a methodology that inversely simulates the diffusion process. Unlike image reverse diffusion, which primarily models the posterior distribution of data, our approach directly regresses the final timestep structure based on the current one using a neural network. This stepwise regression process allows us to incrementally reconstruct the original geometric structure.\nOur approach represents a significant breakthrough in the development of mesh generation techniques for complex 3D models. Our contributions are summarized as follows:\n\u2022 We propose a novel geometric imitation model for 3D shape manipulation, grounded in the diffusion process of non-equilibrium thermodynamics.\n\u2022 Our model demonstrates state-of-the-art performance in point cloud generation and competitive performance in mesh deformation, as evidenced by experimental results.\n\u2022 Extensive experiments and visualizations highlight the substantial application potential of our method in various fields, including point cloud generation, mesh deformation, graphics rendering, and animation production."}, {"title": "2. Geometric Imitation Models", "content": "This section delves into the geometric imitation model we have developed for the forward and reverse diffusion processes of 3D shapes. We also discuss relevant shape regularizations pertinent to our model.\nSubsequently, we articulate our proposed differential deformation kernel, which facilitates the diffusion of samples within a geometry-conscious reDDMe. Comprehensive descriptions of the training and sampling algorithms we employ are provided."}, {"title": "2.1. Formulation of Diffusion Process", "content": "A point cloud or mesh X can be depicted as a set of n vertices {x_i\\}_{i=1}^n along with associated normals {n_i\\}_{i=1}^n and e edges {e_i\\}_{i=1}^e.\nWe perceive the set of particles X^(0) as the initial state of a dynamic thermodynamic system. With the progression of time, the diffusion process gradually disperses the vertices into a disordered set of points, culminating in a noise distribution. The forward diffusion process is conceptualized as a Markov chain [17]:\nq(x^{(1:T)}|x^{(0)}) = \\prod_{t=1}^{T}q(x^{(t)}|x^{(t-1)}),\nwhere q(x^(t)|x^(t-1)) is the Markov diffusion kernel.\nPrior research [22, 27] has adopted the strategy of considering each vertex x_i as being sampled independently from a point distribution q(x_i). These studies employ a Gaussian diffusion kernel (GDK) that progressively introduces Gaussian noise to the vertices, and the posterior distribution is modeled to reverse this process.\nGiven that 3D data encapsulates both spatial (coordinate) position and feature (geometry) information, introducing noise solely to the coordinates not only displaces the spatial position but also disrupts the local geometric structure. As a result, the Gaussian diffusion kernel is not suitable for diffusing 3D data that adheres to a non-rigid deformation in a well-manifold manner, and it fails to maintain the fine-grained geometric structure. To circumvent this issue and facilitate the diffusion of samples in a geometric-aware reDDMe that guides the model to sample from a geometry distribution rather than a point distribution, we propose a novel Markov diffusion kernel, termed the Differential Deformation Kernel (DDK), denoted by q(x^(t)|x^(t-1), x^(0)) = D(x^(t); x^(t-1), x^(0), \\beta_tI), which will be further elaborated in Sec. 2.3. Considering that the normal vector n_i^(t) can be readily recomputed from the neighboring vertex N_{x_i^(t)}, and that the edge connection e_{ij} persists unaltered throughout the diffusion process, our primary objective is to synthesize a novel set of vertices {x_i\\}_{i=1}^n for X^(t) that demonstrates a cohesive geometric structure.\nTo achieve this, we envision the generation process as the inverse of the diffusion process. Specifically, we initiate by sampling vertices from a rudimentary noise distribution p(x^(T)), and subsequently guide the sampled vertices through the inverse Markov chain, parameterized by an approximate estimation p_\\theta(x^(t-1)|x^(t)), x^(0)), until the target distribution p(x^(0)) is formed.\nWe express the inverse process for generation as:\np_\\theta(x^{(0:T)}) := p(x^{(T)}) \\prod_{t=1}^{T}p_\\theta(x^{(t-1)}|x^{(t)}),\np_\\theta(x^{(t-1)}|x^{(t)}) := F_\\theta(X_{t-1}; x^{(t)}, \\beta_tI)\nHere, F_\\theta denotes an estimated posterior distribution parameterized by \\theta. We refer to this inverse process as Geometric imitation learning, which will be elaborated in Sec. 2.4. The initial distribution p(X^(T)) can be sampled from a standard normal distribution, a unit sphere, or a specific template shape, contingent on the task at hand. Hence, p(X^(T)) is a constant that is independent of \\theta."}, {"title": "2.2. Shape regularization", "content": "Prior to introducing the aforementioned DDK, we first delineate some shape regularizations employed during non-rigid transformation. Unless otherwise specified, we utilize p and q to denote a vertex in point cloud (or mesh) P and Q, respectively, until the conclusion of this sub-section. Given that these constraints are frequently employed in related literature, we provide a succinct description of the impact of each constraint. For a more comprehensive understanding, please refer to [15].\nChamfer Distance Loss Prior to discussing the previously mentioned DDK, we initially establish some shape regularizations utilized in non-rigid transformations. Unless otherwise specified, we employ p and q to denote a vertex in pointcloud (or mesh) P and Q respectively, throughout this subsection. The Chamfer distance loss, which is often used to measure the distance between two point clouds when the corresponding point relationship is undefined, is defined as:\nl_c(P, Q)^2) = \\sum_{p \\in P} \\min_{q \\in Q} || p - q ||^2 + \\sum_{q \\in Q} \\min_{p \\in P} || p - q ||^2.\nWhile effective in driving vertices towards their accurate positions, this loss alone is insufficient for generating a high-quality 3D mesh, as the optimization can easily fall into local minima. The network may produce extreme deformations to favor local consistency, which can be particularly detrimental when the estimated point cloud significantly deviates from the ground truth, resulting in the creation of flying vertices. To address these issues, we suggest the inclusion of additional shape regularizations to capture more intricate information about the underlying geometry.\nNormal Consistency Regularization The normal consistency regularization, a surface normal loss capturing high-order properties, can be expressed as follows:\nl_n (P) = \\sum_{p \\in P} \\sum_{k \\in N(p)} || (p - k, n_p) ||.\nwhere q is the closest vertex to p identified during the Chamfer distance loss calculation, k is the neighboring pixel of p, (,) denotes the inner product of two vectors, and n_p is the observed surface normal.\nThis loss aims to ensure that the edge connecting a vertex with its neighbors remains perpendicular to the observation from the neighboring points. Although this loss may not necessarily be zero except for a planar surface, its optimization can effectively enforce consistency between the normal of a locally fitted tangent plane and the observed surface normal. This approach has proven beneficial in our experiments. Additionally, this normal loss function is fully differentiable, simplifying its optimization.\nLaplacian Regularization The Laplacian term, crucial in preserving local details, ensures that adjacent vertices move in the same direction. We first define a Laplacian coordinate for each vertex p as:\n\\delta_p = p - \\sum_{k \\in N(p)} \\frac{k}{\\left| N(p)\\right|},\nand the Laplacian regularization is defined as:\nl_l(P) = \\sum_{p \\in P} || \\delta_p - \\delta_p ||^2,\nwhere \\delta_p and \\delta_p are the Laplacian coordinates of a vertex before and after a deformation block, respectively. The Laplacian term restricts excessive vertex movement. By promoting the collective movement of neighboring vertices, this term assists in preserving the local mesh structure and preventing self-intersections.\nEdge Length Regularization. To mitigate the problem of flying vertices, which may result in elongated edges within the mesh, we propose an edge length regularization strategy. This regularization penalty is mathematically expressed as:\nl_e(P) = \\sum_{p \\in P} \\sum_{k \\in N(P)} || p - k ||^2.\nIt is important to note that this edge length regularization is exclusively applied to neighboring vertices that are connected via edges.\nPotential Energy Regularization. To tackle the problem of vertex clustering, which can result in an uneven distribution of vertices, we introduce a potential energy regularization. This can be mathematically formulated as:\nl_p (P) = \\sum_{p \\in P} \\sum_{k \\in N(P)} \\frac{1}{1+ || p-k ||}.\nThis regularization differs from the edge length regularization as it not only imposes penalties on vertex clustering to regulate the point distribution, but it also influences all neighboring vertices, irrespective of whether they are interconnected by an edge. This regularization encourages vertices to maintain an optimal distance from each other, ensuring the overall potential energy of the point cloud or mesh remains relatively low, thereby fostering a uniform vertex distribution across the entire mesh."}, {"title": "2.3. Differential Deformation Kernel (DDK)", "content": "To facilitate sample diffusion in a geometrically cognizant manner, we introduce a novel Markov diffusion kernel, termed the Differential Deformation Kernel (DDK):\nq(x^{(t)}|x^{(t-1)}, x^{(0)}) := D(x^{(t)}; x^{(t-1)}, x^{(0)}, \\beta_tI)\nHere, the variance schedule hyper-parameters \\beta_1,\u2026\u2026, \\beta_T dictate the quantum of noise introduced at each timestep. Employing an initial source shape X^(0), we leverage shape regularizations in conjunction with a backpropagation algorithm to execute non-rigid deformation, thereby progressively attaining the target noised shape X^(T). The implementation is elucidated in Algorithm 1.\nWe can formally define the shape regularizations of this gradient-based DDK as follows:\nL(x^{(t)}, x^{(0)}) = \\tau. l(x^{(t)}) + \\lambda_c l_c(x^{(t)}, x^{(0)})\nHere, l(X^(t)) = [l_n(x^(t)), l_l(x^(t)), l_e(x^(t)), l_p(x^(t))], where \\lambda = [\\lambda_n, \\lambda_l, \\lambda_e, \\lambda_p] is a hyper-parameter that modulates the influence of distinct shape regularizations. \\lambda_c is a hyper-parameter that modulates the influence of supervised Chamfer distance.\nBy invoking the chain rule of differentiation, we can compute the offset o_x^{(t-1\\rightarrow t)} for each vertex x_i^(t-1) as follows:\no_x^{(t-1\\rightarrow t)} = \\frac{\\partial L(X^{(t-1)}, x^{(0)})}{\\partial x^{(t-1)}}\nConsequently, the forward diffusion process facilitated by DDK can be expressed as:\nx^{(t)} = x^{(t-1)} + \\eta o_x^{(t-1\\rightarrow t)} + N(0, \\beta_tI).\nwhere N(0, \\beta_tI) represents a Gaussian distribution with zero mean and \\beta_tI variance and \\eta denotes the step size. As is evident, DDK is an approximate Gaussian kernel function that takes into account the plausible geometric structure of the noisy 3D space while incorporating Gaussian noise. A comparative visualization of GDK and DDK is presented in Fig. 2."}, {"title": "2.4. Deformable Imitation Learning", "content": "In order to invert the diffusion process, it is necessary to estimate the posterior distribution p(x^{(t-1)}|x^{(t)}) as delineated in Equation 3. However, instead of approximating the posterior distribution, we introduce a neural network with a learnable parameter \\theta, termed as the Deformable Diffusion Model (DDM), that directly regresses X^(t-1) from X^(t).\nThe aim of training the inverse diffusion process is to enable the model to emulate the non-rigid deformation process as follows:\n\\mathcal{L}(\\theta) = \\sum_{i=1}^{n} || \\varphi_{\\theta}(x_i^{(t-1)}) - x_i^{(t)} ||^2.\nThe point-to-point training objective \\mathcal{L} can be optimized using standard backpropagation algorithms. The learning process is elaborated in Algorithm 2."}, {"title": "2.5. 3D Shape Sampling", "content": "X^(T) is a provided template, which could either be a randomly sampled noise point cloud or a well-structured sphere mesh. With X^(T), we can iteratively sample a high-quality 3D shape X^(0) from a learned geometric distribution model \\varphi_\\theta, as illustrated in Algorithm 3. This enables our algorithm to be implemented in a wider range of scenarios."}, {"title": "2.6. Optimized Initialization of X(T)", "content": "Our experimental results indicate that an optimized initialization of X^(T) can significantly accelerate the diffusion steps required for DDK to converge to X^(T). Consequently, we propose two strategies for selecting an appropriate X^(T) for both point clouds and meshes.\nData-driven Initialization of X(T) for Point Clouds. The diffusion model exhibits a robust expressive capability, enabling the diffusion of X^(0) to a randomly initialized X^(T) directly. Furthermore, the reverse diffusion process can restore X^(0) within several hundred steps, as evidenced in [22]. Nevertheless, an optimized initialization of X^(T) can further reduce the steps required for DDK to converge to X^(T), particularly when generating high-fidelity point clouds with intricate structures.\nTo derive the data-driven X^(T), we iteratively apply DDK to various X^(0) over several thousand steps. This procedure results in the formation of an average shape for the training data, which expedites the imitation learning process, especially for 3D shapes within a single class."}, {"title": "2.7. Equispaced Sampling", "content": "Existing methods [28] can expedite the sampling process under GDK due to the favorable properties of Gaussian distribution. Additionally, we propose an equispaced sampling trick from the geometric perspective to accelerate the reverse diffusion process for DDK.\nOwing to the advantages of DDK in preserving the well-manifold structure, we can employ an equispaced sampling method to generate a subsequence X^s = {x^(T), x^(T-i), x^(T-2i),... x^(0)} from {x^(T), x^(T-1),...,x^(0)}, where i represents the timestep interval between two consecutive samples."}, {"title": "3. Experimental Evaluation", "content": "This section elucidates the superiority of the Deformable Diffusion Model (DDM) in generating point clouds and meshes through three distinct tasks.\nGiven that our methods directly sample a shape from X^(T) without the guidance of a latent vector, conventional metrics for evaluating the quality of point clouds or meshes are inapplicable. Consequently, we primarily assess the quality of various algorithms by visualizing the shapes they generate."}, {"title": "3.1. Generation of High-Resolution Point Clouds", "content": "To procure quantitative indicators, we integrate an additional point feature encoder and concatenate the feature vector with time embeddings, following the approach in [22]. This facilitates conditional point cloud generation and allows us to apply most evaluation metrics for point cloud generation fairly to our methods.\nExperimental Setup. The experiment involves random sampling of point clouds from ShapeNet [3], a dataset comprising 51,127 shapes across 55 categories, as X^(0) at each iteration. The dataset is randomly partitioned into training, testing, and validation sets at ratios of 80%, 15%, and 5% respectively. A well-initialized X^(T) is obtained using a data-driven method with shapes from the training data. Given that point clouds only contain vertices x, there are no shape regularizations, i.e., \\lambda = 0, \\lambda_c = 1.0. We employ DDK for 500 steps in the diffusion process and set i = 50 for equispaced sampling, implying that only the samples at steps 0, 50, 100, and 500 are used for training. During the reverse diffusion sampling process, generating point clouds with the learned DDM takes only 10 steps.\nResults and Discussion. As illustrated in Tab. 1, our proposed DDM exhibits exceptional performance across various evaluation metrics. Notably, DPM3D [22] also treats point cloud generation as a diffusion process but employs GDK instead of DDK for the diffusion process. Despite DPM3D achieving competitive results, it necessitates numerous steps to sample a point cloud, whereas our model requires only 10 steps to generate a comparable shape, underscoring the advantages of DDK in 3D shape generation. Fig. 3 displays the point clouds generated by a simple DDM without a latent vector. The progressive generation results are depicted in Fig. 6.\nExtension to Latent Diffusion Models. Latent diffusion models have been demonstrated to lessen the computational load during the training of diffusion models. Recently, LION [39] also applied this approach to point cloud generation. In this study, we apply DDK in the latent space by initially deforming each shape with DDK, followed by feeding them into the shape encoder network to obtain the latent code. As shown in Tab. 2, DDK can enhance the model's performance in the latent space."}, {"title": "3.2. Generation of Well-Manifold Meshes", "content": "The task of generating meshes is significantly challenging due to the necessity of maintaining the correct topology amongst each vertex. A well-manifold mesh structure is crucial for applications such as graphic rendering. In our experiment, we utilized shapes from ShapeNet [3] and a unit sphere with nearly 5,000 vertices as X^(T). For each sampled X^(0), we employed DDK for 2,000 steps during the diffusion process, setting \\lambda_c = 1.0, \\lambda_e = 0.8, \\lambda_n = 0.01, \\lambda_l = 0.15, \\lambda_p = 0.01. We established i = 50 for equispaced sampling, necessitating 40 steps to sample a mesh. As depicted in Fig. 4, DDM can generate meshes that are comparable to those produced by other state-of-the-art methods in terms of visualization. However, due to the limited resolution of X^(T), it is currently unfeasible to generate meshes with complex geometric structures. Future work will focus on developing a superior X^(T) to enhance the geometric imitation model's representation capability for mesh generation. Fig. 6 illustrates the process of gradually deforming a unit sphere into target shapes using the geometric imitation model."}, {"title": "3.3. Ablation Study on X(T)", "content": "Tab. 3 demonstrates the effect of different initialization methods on X^(T), suggesting that DDK displays robust generalization performance across diverse initialization methods. Nevertheless, a well-initialized X^(T) can significantly decrease the number of steps required for DDK to converge to x^0."}, {"title": "3.4. Broad Applications", "content": "High-Fidelity Rendering. As illustrated in Fig. 5, we render high-fidelity images using Blender without any post-processing. Owing to the well-manifold structure of the generated mesh, light can naturally penetrate the object (e.g., pillow), leading to realistic reflections in the rendered images.\nFacial Expression Animation As previously stated, DDK can capture fine-grained geometric structures. Consequently, we can modify a geometric imitation model to drive a template facial mesh to natural facial meshes with expressive features, as shown in Fig. 6. To the best of our knowledge, we are the first to drive a facial mesh without any landmarks. Unlike previous generation tasks, we adapted a template facial mesh provided by [40] as X^(T). We adopted a smaller step size \\eta and did not adopt equispaced sampling to avoid disrupting the facial topologies. The series of facial meshes that we generated fully demonstrates the potential of geometric imitation models in a variety of future applications."}, {"title": "4. Conclusion", "content": "In this study, we introduced a novel approach for the generation of superior quality point clouds and meshes. Our methodology leverages the recently developed Differential Deformation Kernel (DDK), which facilitates diffusion in a geometrically aware manner. Additionally, we utilize the Geometric Imitation Model (DDM), an innovative technique capable of reversing the diffusion process of three-dimensional shapes. We posit that our methodology has potential for extension to shape generation from images or textual data, presenting significant implications for diverse fields including robotics, gaming, and interactive design."}, {"title": "5. Related Works", "content": "This study delves into a diffusion process closely associated with the Denoising Diffusion Probabilistic Model (DDPM), as outlined in previous work [16]. DDPM, an enhanced variant of the Diffusion Probabilistic Model (DPM), demonstrates superior capabilities in modeling the intricate distributions of natural images, thereby enabling the generation of unconditional images from noise inputs. Recent research [7, 21] has suggested methods for controlling image synthesis using a reference signal, such as class, image, text, or embedding. Expanding on this area, Luo et al. [22] introduced DPM3D, an innovative probabilistic model capable of generating highly realistic point clouds.\nOur methodology views point clouds as samples drawn from a specific distribution. However, rather than attempting to learn the posterior distribution of points, we focus on understanding the geometric distribution through our novel Differential Deformation Kernel (DDK). We employ a probabilistic geometric imitation model and utilize the imitation process to capture the complex deformations of 3D shapes. This results in training and sampling methodologies that differ significantly from previous approaches to traditional DPM."}, {"title": "6. Point transformer network", "content": "Recently, transformer blocks have shown a strong capability to extract features in natural language processes and computer vision. In this work, we use the PointNet++ [25] as the foundational feature extractor framework. We incorporate a transformer block between two consecutive point blocks to enhance the ability to capture global features, as depicted in Fig. 7. Additionally, a time scale shifting module [16] is placed before the feature extraction layers. All other settings, such as the number of channels, are kept the same as in [25].\nIt is important to note that we only feed the vertices to our network for both point clouds and meshes. The different geometric properties are injected through various shape regularizations."}, {"title": "7. Experiments configuration", "content": "We implement DDK mainly depending on the PyTorch3D library, which is an efficient extension toolbox for PyTorch to deal with 3D shapes. We conduct all the experiments on a single NVIDIA A100 GPU chip with 40GB of memory. Comprehensive details of our experimental configurations are available in Tab. 5."}, {"title": "8. Evaluation metrics for pointclouds.", "content": "Following prior works, we use the Chamfer Distance (CD) and the Earth Mover's Distance (EMD) to assess the quality of reconstructed point clouds, following previous work [1]."}]}