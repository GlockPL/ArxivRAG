{"title": "MoBA: A Two-Level Agent System for Efficient Mobile Task Automation", "authors": ["ZICHEN ZHU", "HAO TANG\u2020", "YANSI LI\u2020", "KUNYAO LAN", "YIXUAN JIANG", "HAO ZHOU", "YIXIAO WANG", "SITUO ZHANG", "LIANGTAI SUN", "LU CHEN\u2260", "KAI YU\u2021\u2020"], "abstract": "Current smart assistants on mobile phones are often limited by dependence on APIs of system and third-party applications. At the same time, model-based screen agents struggle with diverse interfaces and complex commands, due to restricted understanding and decision-making abilities. To address these challenges, we propose MOBA, a novel Mobile-phone Agent empowered by multimodal large language models that enhances comprehension and planning capabilities through a sophisticated two-level agent architecture. The high-level Global Agent (GA) interprets user commands, manages history, and plans tasks, while the low-level Local Agent (LA) executes precise actions as function calls based on sub-tasks and memories from GA. By incorporating a double-reflection mechanism, MOBA efficiently handles tasks, even those previously unseen. MOBA demonstrates significant improvements in task execution efficiency and completion rate in real-life evaluations, underscoring the potential of MLLM-empowered mobile assistants.", "sections": [{"title": "1 INTRODUCTION", "content": "Multimodal large language models (MLLMs) have experienced rapid development in recent years, driven by their training on vast amounts of multimodal data. These models [4, 5, 7, 12, 24, 25, 32, 41, 43, 60-62, 69] are not only highly aligned with human preferences in understanding and generating content but also demonstrate emergent abilities such as Chain-of-Thought (CoT) reasoning [51] and In-Context Learning (ICL) [3]. Consequently, MLLMs have been widely adopted in various tasks, including structured rich-content understanding and reasoning [49, 50], multimodal recognition and content creation [6, 10, 26, 33, 53], and numerous industry applications [17, 35, 37].\nThe rise of MLLMs has also revolutionized the development of agent systems. Compared to traditional dialogue or control systems [19, 21], MLLM-based agents possess significantly enhanced capabilities for environment observation, task decomposition, and action decision-making. These agents are becoming increasingly prevalent across embodied robots and virtual environment manipulations. Among these, mobile phone assistants represent a typical and widely used type of agent system. However, unlike agents operating in virtual or artificially constructed test environments, mobile assistants must handle operations in highly dynamic and unpredictable real-world Graphic User Interface (GUI) environments, interpreting complex user instructions, which presents several unique challenges.\nFirstly, mobile interfaces are often cluttered with information that varies across different operating systems, applications, and even with time. Moreover, these interfaces frequently contain task-irrelevant distractions such as pop-up notifications or ads. Secondly, similar tasks may require different execution methods (e.g., setting an alarm might involve scrolling to select or directly entering numbers). Thirdly, user instructions can be ambiguous or colloquial (e.g., \"Do I need to bring an umbrella tomorrow?\" rather than \"Check the rain probability of tomorrow in the weather application.\"), making it harder for the assistant to interpret the user's intent accurately.\nTraditional mobile assistants, which rely on slot-filling techniques and API calls, often fail to handle these complex scenarios effectively. End-to-end pre-trained models [40, 63, 70], while capable in some pre-trained cases, cannot adapt and learn from new environments dynamically. Both methodologies struggle to handle complex scenarios and lack versatility across diverse applications and tasks. In contrast, MLLMs, with strong capabilities in understanding, reasoning, decision-making, and self-reflection, present a promising direction for serving as effective mobile assistants.\nRecent research efforts have introduced several MLLM-based agent systems that utilize the CoT capabilities of MLLMs to plan tasks, simulate actions such as screen clicks, and determine next steps based on execution results [27, 31, 46, 47, 64, 65]. These systems can operate on physical mobile devices and have shown promising results on everyday tasks, demonstrating the powerful reasoning ability and robust generalization of MLLMs. However, several limitations [31, 56] exist in these systems: (1) Agents often successfully complete the overall task but overlook or incorrectly execute finer details (e.g., setting a 7:30 PM silent alarm but forgetting to mute the ringtone). (2) Agents may miss executing necessary but unstated actions based on common sense (e.g., forgetting to click \"Save\" after setting an alarm). (3) Agents may reflect that a task has not been successfully executed but continue to repeat the same actions afterward, ultimately getting stuck in a loop, instead of considering alternative approaches to resolve the issue. (4) Agents may not be able to extract and utilize information (e.g., checking the weather for a trip requires first retrieving the destination and dates from the calendar, which the agent may forget or even skip it)."}, {"title": "To address these challenges, we propose \u041c\u043e\u0432A, an MLLM-empowered mobile phone agent system designed to enhance task execution capabilities in complex and dynamic environments. Our approach introduces several elements:", "content": "Two-Level Agent Structure: Inspired by the human brain's working mechanism, we adopt a two-level agent architecture, with the \"cerebrum\" handling abstraction, planning, and reasoning, and the \"cerebellum\" focusing on execution. Our architecture consists of a Global Agent and a Local Agent. The Global Agent is responsible for high-level task planning and decomposition, while the Local Agent concentrates on selecting target actions for each sub-task. This division of responsibilities facilitates better task management, reduces the complexity of decision-making processes, and enhances overall system efficiency.\nTask Planning and Execution Pipeline: We develop a robust task planning pipeline that includes task decomposition, feasibility assessment, action generation, and result validation. This pipeline leverages both the Global and Local Agents to break down complex tasks into manageable sub-tasks, generate appropriate actions, and verify the success of each step. The use of a double-reflection mechanism not only allows the system to correct errors quickly but also helps prevent ineffective operations before execution.\nMulti-aspect Memory: We incorporate a Memory Module that learns from historical experiences to enhance the agent's adaptability and reduce redundancy. This module stores task execution traces, user preferences, application information and observation, and contextual information, which can be retrieved to inform future decisions. By learning from past successes and failures, the agent can correct and optimize its behavior over time, improving its performance in handling complex tasks.\nWe conduct a thorough evaluation on MOBBENCH, a test set with 50 real-life tasks across a variety of applications and difficulty levels. \u041c\u043eBA achieves the highest milestone score rate of 66.2%, surpassing the second-highest baseline agent system by over 17%. This sets a new standard for mobile assistants by effectively integrating multimodal capabilities, task planning, reflection, and memory mechanisms."}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 LLM Agents", "content": "The development of intelligent agents has been significantly influenced by the advancements in large language models (LLMs) and multimodal large language models (MLLMs). LLM-based agents leverage the autonomy, reactivity, proactiveness, and social ability of these models to perceive external environments and make decisions [55]. Emerging abilities, such as chain-of-thought (CoT) reasoning [48, 51, 68] and in-context learning (ICL) [3, 30]. Recent studies have explored LLM-based approaches for reflection [28, 39, 57, 59], planning [13, 36, 42], and memory mechanisms [15, 23, 29, 66, 67].\nAt the same time, the agents that utilize M/LLMs to interact with the environments are quickly developed. These agents possess significantly enhanced capabilities for environment observation, task decomposition, and action decision-making, which enable M/LLMs to solve complex tasks across social simulations [1, 14, 15, 34], embodied robots [54], software development [35, 37] and virtual assistants [45]."}, {"title": "2.2 GUI Agents", "content": null}, {"title": "2.2.1 Traditional GUI Agents", "content": "Controlling graphical user interface (GUI) screens based on user commands is a complex task that involves both GUI understanding and command interpretation. Early approaches to GUI agents focused on embedding and modular systems. For example, several agents [19, 21] combined natural language and programming"}, {"title": "2.2.2 Advancements with Multimodal Pretrain Models", "content": "The advent of multimodal pretraining models [2, 8, 11, 18, 20, 22, 44] for GUI understanding marked a significant shift in the development of GUI agents. Pretrained agents [40, 63, 70] integrated multimodal information, such as dialogue history, screenshots, and action history, through pretraining. Unlike earlier methods that relied on rigid scripts, these end-to-end models adopted a more human-like approach to interacting with interfaces, enhancing their efficiency in information retrieval and task execution by mapping visual observations and text commands directly into actions."}, {"title": "2.2.3 MLLM-Empowered GUI Agents", "content": "The integration of MLLMs in GUI agents has introduced new opportunities to further enhance their capabilities. With the rise of larger scale models, GUI agents [16, 64, 65] began to leverage advanced reasoning and decision-making processes. These models utilized structural information provided in the view hierarchy (VH) to annotate and locate UI elements, guiding a sequence of atomic actions to achieve specific goals. VH-only agents [52] depend on the structural information to reason and make decisions, which greatly lowers the cost of inference making it suitable for deployment on the device. Image-only agents [9, 46, 47, 58], which employs optical character recognition (OCR), CLIP [38] module, and object detection methods to identify operation targets. This image-only approach is particularly effective when the view hierarchy is inaccessible or noisy, but it may also encounter challenges, e.g. opening a target application by clicking when names are hidden, or logos vary across screens."}, {"title": "3 THE MOBA SYSTEM", "content": null}, {"title": "3.1 Global Agent", "content": null}, {"title": "3.1.1 Plan Module", "content": "The Plan Module is the core component of the Global Agent, responsible for decomposing the current problem into a sequence of sub-tasks until it can be handled one by one with a single action execution by the Local Agent.\nFormally, our goal is to decompose the long-term task t into a sequence of sub-tasks $ST_t = [t_1,..., t_k]$, each of which can be completed by the Action Module in a single step. Each task node $t_i$ is initialized based on the goal description. The inputs include the task t, the environment e, the retrieved memory $MP_t$, and the set of executable actions A. Given the model $MLLM_p$, the set of sub-tasks can be determined as follows:\n$ST_t = MLLM_p (t, e, MP_t, A)$"}, {"title": "3.1.2 Reflection Module", "content": "In the Reflection Module, the objective is to assess the task's execution status $s_t$ based on the task t, previous action $a_t$, environment e, the environment after executing the action e', and the retrieved memory $MV_t$. The model used at this stage is $MLLM_r$, formulated as:\n$s_t = MLLM_r (t, a_t, e, e', MV_t)$\nIf the task execution fails, $MLLM_r$ also generates a Execution Reflection on the current task execution. The retrieved memory includes the LA's observations and thoughts from this previous attempt, aiding $MLLM_p$ in analyzing the reasons for the failure. The completion status and reflections are stored in the Action Node of the Task Memory, which can be retrieved later by querying the Action Node.\nAdditionally, for tasks deemed too complex to be executed directly within the Action Module, the Thought generated by the Action Module is also stored in Memory as Plan Reflection. This reflection process is combined with the target action selection process, as they are highly related and for efficiency consideration."}, {"title": "3.2 Local Agent", "content": null}, {"title": "3.2.1 Action Module", "content": "The Action Module is designed for two functions, first, it detects whether this task can be completed with one action, and later it provides the target action and invokes the controller to execute the specific action if it is feasible, otherwise, the Global Agent is called upon to further decompose the task.\nFormally, in the Action Module, we need to generate the CanComplete flag $f_t \u2208 \\{T, F\\}$ and its corresponding action $a_t \u2208 A$ based on the current task t, environment t, retrieved memory in the Action module $MA_t$, and the provided set of executable actions A. If no such command exists, the model will return a negative flag indicating that the task is too complex and requires further decomposition. Let the model used at the current stage be $MLLM_a$, then the generation of the instruction can be expressed as:\n$f_t, a_t = MLLM_a (t, e, MA_t, A)$ \nAt this stage, various historical information can be incorporated as prompts. Relation-based retrieval elements like the last failed node help minimize repeated errors, while the last successful node and the parent node enhance task comprehension. Content-based retrieval from Task Memory, User Memory, and App Memory provides the agent with insights from past similar tasks.\nFunction $MLLM_a$ also requires generating Observation, Thought, and Message if the task includes user requests that need to be answered in natural language. This information is incorporated into the App Memory and Task Memory to enhance subsequent decision-making processes of M\u043e\u0432\u0410."}, {"title": "3.3 Memory Module", "content": "We categorize Memory into three types based on the historical data generated by agents during task execution: (1) Task Memory, generated from the history of task execution. (2) User Memory, derived from the history of interactions with users. (3) App Memory, learned from the history observations and thoughts of interfaces. All kinds of memories can be initialized by human expert for a warm start up. Integrating these memory types into the execution of each module helps enhance the efficiency and success rate of task execution.\nTask Memory captures detailed information about each task, including its execution status, environmental context, and associated sub-tasks. The design of the memory is informed by the ICE Strategy [36], organizing tasks and sub-tasks in a hierarchical tree format. Task Memory includes the following data types:\nTask Node: Tasks are stored in a tree structure. In the i-th round of the agent's execution, a Task Node $T_i$ is created with the goal $t_i$, where $t_1$ is the user's initial requirement. If the node executes a command, it becomes an Action Node $A_i$. In cases where $t_i$ cannot be achieved, child nodes are generated for the next iteration. Otherwise, the node does not split further and becomes a leaf node.\nRoute: The execution path of a task. Once the task of the root node is completed, for each non-leaf node $t_i$, a route $r_i = [A_{i,1}, A_{i,2},..., A_{i,n}]$ is generated, where $A_{i,x}$ represents the x-th successfully executed Action Node among all sub-tasks of $t_i$.\nRoute History: The routes of all sub-tasks in each completed task are stored in the Route History.\nAction History: Success History stores the collection of all successfully executed Action Nodes. Failure History stores the collection of all unsuccessfully executed Action Nodes with reflections.\nMiscellaneous: Includes Reflection History on failed nodes, Thought History during task execution, and Response History to requests (if the goal involves providing answers).\nUser Memory stores historical interactions between the user and MOBA. It helps MOBA to better grasp the user's needs and infer implicit information that may not be explicitly stated in the current task.\nApp Memory maintains information about each application. It includes both an overall description of functions and detailed observations of previously visited pages and explorations on these pages. This component aids \u041c\u043e\u0432\u0430 in comprehending similar pages by leveraging past experiences as well as locating target applications.\nThe retrieval of memory information includes both relation-based and content-based methods. Relation-based retrieval encompasses relationally close information. Specifically, the last Action Node and the task description of the parent node will be retrieved in the Action Module, which helps the agent to better understand the current task and its stage. Content-based retrieval employs cosine similarity of keys, selecting memories with high similarity. For example, the Task History is retrieved based on the cosine similarity of the task descriptions in the Plan Module, providing the agent with experiences from similar historical tasks. The two approaches can also be combined through weighted retrieval by setting relation distance and content distance, along with their corresponding weights. Finally, the results are ranked based on the aggregated outcome."}, {"title": "3.4 GUI Interface", "content": "We also build an installable application to provide a user-friendly experience. An ASR module will transcript the user's voice commands into text, and a TTS module will read out the response from \u041c\u043e\u0432\u0410. With the wireless ADB, the user is not required to type the command in the computer terminal directly nor connect to the computer through a USB cable."}, {"title": "4 EXPERIMENTS", "content": "To get a thorough comparison between MOBA and other GUI agents about the capability to handle complex user instructions and execute GUI interactions on mobile devices, we conduct the evaluation on a real-life scenario test set naming MOBBENCH."}, {"title": "4.1 The MOBBENCH Test set", "content": "The MOBBENCH comprises a diverse test set of 50 tasks designed to evaluate the performance of MOBA in real-world mobile application scenarios. The test set includes 10 applications widely used in China, each with four tasks of varying difficulty: Easy, Medium, Hard, and Indirect Comprehension, totaling 40 tasks. Easy, Medium, and Hard tasks are categorized by the complexity and steps required to complete. Indirect Comprehension is designed for common cases where the user gives a vague instruction without detailing which application or specific steps are required. The agent is expected to decide target application and find an effective approach. Additionally, there are 10 Cross-Application tasks, which involve interacting with two applications and are more close to Hard level in difficulty. These tasks focus on evaluating the ability of information extraction and retrieval, as well as the awareness of sub-goal completion and application switching.\nCompared with several similar task sets mentioned in other papers [16, 46, 47, 64, 65], which only get a score when it finishes the task completely, we assign several milestone scores for sub-tasks in MOBBENCH. This allows for a more precise process assessment, in the cases where the task is partially finished. We also include a detailed preparation instruction for tasks when a more justice and stable start is needed.\nThree human operators manually performed these tasks on three different mobile phones, recording their execution steps. The actions considered as a step include interactions such as clicks, swipes, opening or closing an app, dismissing non-skippable ads, typing text, or replying to the user. The average number of steps is considered as the human expert baseline."}, {"title": "4.2 Metrics", "content": "Three metrics are designed to better compare the capability of GUI agents thoroughly.\nMilestone Score (MS): Scoring milestones are assigned to several sub-tasks, evenly distributed during the task completion process. Since each task contains 1 to 6 milestones, the agent will get a score as it reaches each milestone. We sum up all milestone scores of 50 tasks as the primary metric.\nComplete Rate (CR): If the agent gets all milestone scores in one task, it is considered as task complete. This is the most common and straightforward metric for GUI agent evaluation.\nExecution Efficiency (EE): We record the effective number of steps for each task and the corresponding milestone scores, that is, the total number of steps executed at the time of getting the last effective milestone score, and calculate the average number of steps required to obtain each effective milestone score. The lower this number, the more efficient the execution; the higher it is, the more it includes ineffective actions."}, {"title": "4.3 Setups", "content": "To provide a comprehensive evaluation, \u041c\u043e\u0432\u0430 is compared against several baselines that illustrate a spectrum of capabilities, from basic manual operations to several sophisticated agent-based automation.\nHuman Baseline as mentioned in \u00a7 4.1 are considered as the optimal solution for each task.\nGPT-40 + Human Baseline utilizes an iterative process where the GPT model [32] provides guidance for manual task execution using screenshots. Each task involves presenting a task and a screenshot to the model, which then suggests the target actions. Human operators interpret the suggestions and execute these actions on the device, continuing until the task is completed or a stopping criterion is met.\nAppAgent [65] uses both view hierarchy and screenshot for planning and choosing target actions. All interactive elements are marked with bounding boxes and a unique index for better grounding performance.\nMobile Agent (v2) [46, 47] uses only visual information from screenshots when making decisions. Target elements are selected with the guidance of OCR and CLIP [38] modules.\nMOBA is evaluated under several settings by disabling the Memory Module or/and Plan Module to assess its performance and the impact of these two modules. We disable the Plan Module by replacing the Global Agent with a"}, {"title": "4.4 Results and Analysis", "content": "The overall experiment results are as listed in Table 4. And for more detailed results categorized by task type please refer to Figure 3."}, {"title": "4.4.1 Human is more adaptive and robust to screen interactions", "content": "While the human baseline is considered the optimal solution for each task, the GPT-40 + Human method achieves performance very close to that of human operators on all metrics. In the evaluation of GPT-40 + Human, the agent only provides textual task descriptions and an initial screenshot, and the GPT-40 generates detailed step-by-step instructions, which are then executed manually by a human operator. The eye-catching performance of GPT-40 + Human can be attributed to several factors: (1) a relatively lenient standard in task execution, allowing human operators to interpret GPT-40's general instructions flexibly; (2) human operators automatically completing tasks such as OCR, target detection, and localization, ensuring more precise actions; (3) GPT-40 provides a global plan, avoiding redundant or missed steps; (4) technical issues (e.g., inability to retrieve XML files or missing information in the files) do not affect task completion."}, {"title": "4.4.2 Performance Comparison", "content": "The performance of MobileAgent is notably higher than that of AppAgent. This improvement is mainly due to the inclusion of both Memory and Reflection modules in MobileAgent, which enhance reasoning capacity and utilize more computational resources, such as tokens. Additionally, MobileAgent keeps a record of all historical actions, allowing it to learn from the entire sequence of operations, whereas AppAgent can only track the most recent action. Furthermore, MobileAgent relies on OCR and CLIP modules for target localization, offering greater flexibility and avoiding the technical limitations that AppAgent faces when dependent on XML files.\nBy adopting a twice-reflection strategy, the ineffective execution steps are slightly reduced, where the sub-tasks that are not able to be completed with a single action are decomposed finer before executed. This gives clearer guidance for the Local Agent to decide the target actions."}, {"title": "4.4.3 Ablation Study", "content": "The lower part of Table 4 presents the results of the ablation study, where we experimented with four different configurations by selectively enabling or disabling the Memory and Plan modules. The results indicate that incorporating both Memory and Plan modules significantly enhances the agent's overall performance.\nThe Plan module alone shows a much stronger effect than the Memory module alone, validating one of the core contributions of this paper-the effectiveness of task decomposition planning. By decomposing tasks into manageable sub-tasks, \u041c\u043e\u0432\u0410 can perform global planning, avoid redundant actions, and minimize overlooked details, effectively managing its historical actions (since in a tree-structured task, previously completed sub-tasks are inherently tracked). Unlike MobileAgent, which focuses solely on the next specific action, MOBA first determines the next abstract task and then plans the specific execution steps, closely mirroring human reasoning patterns and providing a more structured approach."}, {"title": "4.4.4 Take-home Messages", "content": "Combining MLLM-driven planning with human flexibility and adaptive decision-making and target-locating can bridge gaps in current MLLM capabilities, leading to near-optimal performance in complex, dynamic tasks.\nIntegrating comprehensive planning, memory, and reflection capabilities in task-oriented agents significantly enhances their ability to handle complex tasks by optimizing decision-making processes and learning from past actions.\nEffective task decomposition and planning strategies are crucial for improving agent performance, as they enable structured execution and minimize redundant actions.\nCooperating long-term memory capabilities further strengthens the agent's ability to manage complex, cross-application tasks by retaining critical contextual information.\nCombining reflections before and after action execution helps reduce ineffective moves."}, {"title": "5 CASE STUDY", "content": "In this section, we present two case studies to showcase several key features of \u041c\u043e\u0432\u0410."}, {"title": "5.1 Case Study A", "content": "\u041c\u043e\u0432\u0430 demonstrates exceptional proficiency in handling tasks that involve intricate details and multiple sequential steps. This case study focuses on a representative scenario involving a clock application, as shown in Figure 4. The objective is to Create a 6:30 PM alarm with the title 'Work' and use vibration alerts from each Monday to Thursday.\nThe process begins with \u041c\u043e\u0432\u0410 (1) assessing the task feasibility, i.e. whether it can be completed in a single step, as indicated by the CanComplete status. If the task cannot be accomplished in one step, \u041c\u043e\u0432\u0410 (2) decomposes it into smaller, manageable sub-tasks in the Plan Module, as listed in Subgoals. This systematic decomposition ensures that each part of the original command is executed with precision. Following this, \u041c\u043e\u0432\u0410 (3) carries out the first action to solve its first sub-task and (4) reviews and reflects the task completion status indicated by Subgoal_Status and Goal_Status. Once a sub-task is completed, as the output Subgoal_Status is set to True, indicating successful sub-task progress, and MOBA will move to the following sub-task if the sub-task sequence is not empty.\nFor sub-task 2 MOBA further divides it into two distinct steps of adjusting the hour number and minute number separately. These two new sub-tasks are also added to the sub-task sequence. For most agents, this way of thinking might be too cumbersome for they may just adopt a simpler approach: adjusting only the hour number to 18 while ignoring the minute. By breaking down the task, we adopt a Chain-of-Thought (CoT)-like approach that explicitly structures the reasoning process of MOBA. For instance, sub-task 2.2 can be further decomposed into several times of clicking if required.\nIn sub-task 4, \u041c\u043e\u0432\u0430 mis-comprehended the description and selected only Monday and Thursday. During the reflection process, it detects that sub-task 4 has not been completed. Therefore the Plan Module will be invoked to add two more sub-tasks - select Tuesday and select Wednesday.\nIn conclusion, the task decomposition approach invoked by the assessment before action execution and reflection after ensures the accurate execution of a series of detailed steps. The tree-structured task decomposition strategy not"}, {"title": "5.2 Case Study B", "content": "Figure 5 demonstrates how the reflection and Memory Modules support task execution in MOBA. The command is Help me check when will I reach the travel destination tomorrow. \u041c\u043e\u0432\u0430 can accurately interpret user intent from this command and give decomposed sub-tasks.\nFor sub-task 1, \u041c\u043eBA retrieves relevant details from User Memory (e.g., frequent use of Google Calendar), extracts key information (train schedule and destination), and stores it in Task Memory. Leveraging the information stored in App Memory, MOBA is able to locate the next application (China Railway), due to its high relevance to train travel.\nWhen encountering failures, MOBA uses historical experiences to reflect and adapt. During sub-task 3, when \u041c\u043e\u0432\u0430 initially failed to input the train number using the Box_Input function, it reflects on its previous operations and employs a character-by-character input method, successfully completing the task."}, {"title": "6 LIMITATIONS AND FUTURE WORK", "content": "Task Decomposition and Failure Handling. The effectiveness of MOBA largely depends on accurate initial task decomposition. Incorrect decomposition, such as creating sub-tasks that cannot be completed, leads directly to task failure. Currently, while we invoke task decomposition upon failure, there is no mechanism for automatically correcting the initial task plan. If a sub-task cannot be further decomposed or completed, it may lead to a cyclical failure. Moreover, sub-tasks often depend on each other; for example, subsequent actions on screen B can only be executed after completing information retrieval on screen A. If the initial plan wrongly orders the tasks as [B, A], the tasks will not be completed successfully. Although rare in practical tests, such decomposition errors pose significant challenges.\nDependence on View-Hierarchy Information. \u041c\u043e\u0432\u0430 relies view hierarchy (VH) information from XML files to understand mobile interfaces and locate target elements, which assumes the accuracy of these files. However, discrepancies"}, {"title": "7 CONCLUSION", "content": "This paper presented MOBA, an innovative Mobile phone Assistant system empowered by MLLMs. Utilizing a two-level agent structure, comprising a Global Agent and a Local Agent, MOBA effectively understands user commands, plans tasks, and executes actions. The combination of Memory and Plan Modules enhances its ability to learn from previous interactions, improving efficiency and accuracy. Our evaluations demonstrated that MoBA surpasses existing mobile assistants in handling complex tasks, leveraging multi-level memory, task decomposition, and action-validation mechanisms. These features enable precise task execution even with intricate or indirect commands.\nFuture work will focus on improving the performance on image-only scenarios where the view hierarchy is unattainable, deploying an end-side model on mobile phones for faster response and secured privacy. We hope \u041c\u043e\u0432\u0430 illustrates the potential of MLLMs-empowered mobile assistants and provides valuable insights for future works."}, {"title": "B VIEW HIERARCHY PROCESSING", "content": "Given that (1) large models still exhibit limitations in processing visual information and (2) certain elements of the mobile phone interface cannot be obtained through visual means alone, the view hierarchy (VH) plays a crucial role in enabling agents to effectively interpret the mobile interface. However, the XML files representing mobile interfaces contain a substantial amount of redundant information. This redundancy increases token counts and complicates the agent's task of identifying key UI elements.\nTo address this issue, we developed an algorithm designed to filter UI elements. The algorithm consists of four steps: (1) parsing UI elements from the XML file, (2) filtering user-interactable UI elements based on their attributes, and adding them in ascending order of size, unless they exhibit significant overlap with previously added elements, (3) for UI elements containing text, merging the text content with interactive elements if the text is largely contained within those elements, thus enriching the interactive element with explanatory information, and (4) assigning an index to"}]}