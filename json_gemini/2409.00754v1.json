{"title": "Cooperative Path Planning with Asynchronous Multiagent Reinforcement Learning", "authors": ["Jiaming Yin", "Weixiong Rao", "Yu Xiao", "Keshuang Tang"], "abstract": "In this paper, we study the shortest path problem (SPP) with multiple source-destination pairs (MSD), namely MSD-SPP, to minimize average travel time of all shortest paths. The inherent traffic capacity limits within a road network contributes to the competition among vehicles. Multi-agent reinforcement learning (MARL) model cannot offer effective and efficient path planning cooperation due to the asynchronous decision making setting in MSD-SPP, where vehicles (a.k.a agents) cannot simultaneously complete routing actions in the previous time step. To tackle the efficiency issue, we propose to divide an entire road network into multiple sub-graphs and subsequently execute a two-stage process of inter-region and intra-region route planning. To address the asynchronous issue, in the proposed asyn-MARL framework, we first design a global state, which exploits a low-dimensional vector to implicitly represent the joint observations and actions of multi-agents. Then we develop a novel trajectory collection mechanism to decrease the redundancy in training trajectories. Additionally, we design a novel actor network to facilitate the cooperation among vehicles towards the same or close destinations and a reachability graph aimed at preventing infinite loops in routing paths. On both synthetic and real road networks, our evaluation result demonstrates that our approach outperforms state-of-the-art planning approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "The classic shortest path problem (SPP) aims to determine an optimal route in a road network for a given source-destination pair typically with the goal to minimize either travel time or path distance. In a real world, it is common to plan the shortest paths for multiple source-destination pairs (MSD), namely MSD-SPP. The challenge of solving MSD-SPP is that we have to consider the traffic capacity constraints of road networks. When the number of vehicles on a certain road segment exceeds the road capacity constraint, traffic congestion occurs, leading to increased travel time. Given the capacity constraints of road networks, the goal of MSD-SPP is to plan the shortest paths for all source-destination pairs with the optimization objective of minimizing an aggregate metric, such as the average travel time across all paths.\nThough the classic SPP problem has been well studied in the literature, solving MSD-SPP is rather hard. Even a simplified variant of the problem, e.g., the k-Disjoint Shortest Path problem (kDSP), when considering only two source-destination pairs, has been proved to be NP-complete [5]. Here, the kDSP aims to find disjoint shortest paths for k source-destination pairs to alleviate congestion on graph edges. In real-world scenarios where k is large, solving kDSP becomes much harder. It is particularly true since road segments may need to accommodate multiple vehicles simultaneously while subjecting to capacity constraints.\nTo illustrate the challenges inherent in the MSD-SPP problem, we present two baseline approaches: (1) The Entire Path-based Approach independently plans the entire routing path for each individual vehicle. For example, we can exploit a classic algorithm such as Dijkstra's[3] algorithm and A* search algorithm [4] to compute the shortest path for a given source-destination pair. However, when considering multiple source-destination pairs in the context of MSD-SPP, this approach may result in the planning of shortest paths that share certain road segments. When the number of vehicles on these road segments exceeds the capacity constraint, traffic congestion occurs. (2) The Next-hop node-based approach re-plans routes for individual vehicles upon reaching intersections to select the next intersection based on the real-time traffic conditions. Some deep reinforcement learning (DRL) algorithms such as [35], [37] have been proposed to learn such routing policies, which enables adaptive decision-making in response to changing traffic conditions."}, {"title": "2 RELATED WORKS", "content": "This section begins with a preliminary overview of cooperative MARL in Section 2.1, followed by a review of the state-of-the-art for vehicle shortest path planning in Section 2.2.\nNumerous MARL algorithms have been designed to address the scenarios that involve interaction among multiple agents. A straightforward approach is to train each agent independently to maximize their individual rewards by treating other agents as part of the environment [23]. However, in traditional RL, the environment is typically assumed to be stationary, meaning that the dynamics of the environment remain constant over time. However, in MARL, each agent's actions affect the environment, which in turn influences the behavior of other agents. This interaction between agents introduces non-stationarity into the environment, as the environment changes caused by the actions of all involving agents. Due to the non-stationarity of the environment, the Markov assumption that the current state of the environment contains all the information necessary to make decisions about future actions, does not hold in the context of cooperative MARL.[2].\nTo address the non-stationarity issue in MARL, cooperative MARL systems [22], such as centralized critic [24], [28], [25] and value function factorization [29], [30], [31], have been developed. A centralized critic is a mechanism where agents share a common critic or value function that estimates the expected cumulative reward for each agent, taking into account the observations and actions of all agents. For example, Lowe et al. [28] proposes a multi-agent policy gradient algorithm where agents learn a Deep Deterministic Policy Gradient (DDPG) policy [19]. This approach ensures a stationary environment even if the policies of other agents change by using a centralized critic with the joint observations and actions of all agents as input.\nIn the cooperative MARL systems above, a commonly used approach is the so-called centralized training decentralized execution (CTDE) paradigm. It often takes the joint actions and observations of all agents as input to the centralized critic. The CTDE paradigm works wells in the setting of Decentralized Partial Observable Markov Process (Dec-POMDP) [21]. It assumes that all agents perform actions at each time step, indicating the actions of such agents are all synchronous. Yet, in typical vehicle routing tasks, this assumption does not hold and instead agents frequently make decisions asynchronously. Due to the redundancy of joint observations and actions within the training trajectory, several observations may correspond to the same action, or conversely, a single observation may correspond to multiple highly diverse actions. As a result, the CTDE paradigm does not work well in the asynchronous setting.\nTo address this issue, some asynchronous MARL methods are developed.\nExisting works on shortest path planning for vehicle navigation typically aim to plan shortest paths and meanwhile avoid traffic congestion. Depending upon whether or not the route planning cooperates among vehicles, we divide these works into the following categories.\nNon-cooperative route planning approaches compute the SPP for an individual vehicle with no consideration of concurrent planning for other vehicles. The classic approaches, Dijkstra algorithm [3] and A* search algorithm [4], compute the shortest path between a source-destination pair on a road network. Recently, to solve the classic combinatorial optimization problem such as Travel Salesman Problem (TSP) and Vehicle Routing Problem (VRP), the machine learning-based approaches learn an approximation function that maps input road networks to output travel tours [12], [10]. Nevertheless, they do not work well on dynamic road networks. That is, whenever either graph typologies or edge weights of road networks change, these algorithms have to re-plan the shortest paths, leading to high computing overhead.\nWhen a vehicle arrives at an intersection node, DRL-based approaches instead iteratively re-plan the next hop based on current traffic states. As a result, the number of re-plans is just equal to the intermediate nodes within the path between the source-destination pair. For instance, our previous work [35] employs a dueling deep Q-network to determine the shortest path-based vehicle routing on a grid road network. The work [36] exploits a graph convolution network and a deep Q-network to perform the shortest path-based routing on dynamic graphs. Unlike the simple SPP problem, our previous work [37] studies the NP-hard constrained shortest path problem on dynamic graphs.\nAll the works above independently compute or plan the shortest paths for an individual vehicle with no cooperation with other vehicles. Such paths could lead to traffic congestion on certain road segments if an excessive number of vehicles are unfortunately planned onto the same road segments.\nCooperative route planning approaches consider the cooperation among vehicle route planning. The kDSP problem [5], with the aim to find disjoint shortest paths for k source-destination pairs, is NP-complete even with only k = 2 source-destination pairs. The classic Gawron algorithms [8], [9] find an approximation solution to the optimum. Given the traffic demand between intersections, in each iteration, these methods compute the fastest route for each vehicle and then assign a cost to each road segment based on the intensity of traffic. By iteratively moving some traffic to less congested paths and re-computing road costs, they have chance to finally achieve a user equilibrium. However, these methods are computationally expensive due to the iterative steps. Instead of computing all routing paths directly, [45] and [47] develop a RL policy to assign edge weights, and then exploit a softmin function to convert edge weights into flow ratios on graph edges. The two works greatly reduce the solution space from V^(V-1)^E to E where V and E are the numbers of vertices and edges, respectively. Nonetheless, for a large graph size, the training of reinforcement learning policy networks is still hard due to large action space, suffering from poor scalability.\nInstead of computing all shortest paths together, MARL-based methods attempt to learn decentralized routing policies with multiple vehicle or intersection-based agents. Since the action space of a single agent is much smaller than the original solution space, it is feasible for MARL to train such policies efficiently."}, {"title": "3 PROBLEM DEFINITION AND OVERVIEW", "content": "In this section, we first give our problem definition, then give an overview of our solution framework. Table 1 lists the definitions of the symbols.\nDefinition 1. Road Network. We define the road network as a directed graph $G = (V,E)$, where $V$ is a set of nodes, with each node representing a road intersection (we thus, and $E$ is an edge set.\nAn edge $e_{ij} \\in E$ from $v_i$ to $v_j$ indicates the road segment from the intersection $v_i$ to another one $v_j$. The traffic capacity of road segment $e_{ij}$ is defined as $C_{ij}$. Denote the number of vehicles running on the road segment $e_{ij}$ as $n_{ij}$. If this number $n_{ij}$ exceeds $C_{ij}$, for simplicity, we assume that the travel speed along $e_{ij}$ is reduced to a smaller value by a fraction $\\alpha \\frac{C_{ij}}{n_{ij}}$ where $0 < \\alpha < 1$. The greater the number $n_{ij}$ exceeds the capacity $C_{ij}$, the lower the travel speed along the road.\nDefinition 2. Vehicle. Given $L$ vehicles travelling in a road network $G$, each vehicle with a unique ID $l$ is represented with a triplet $L_l = (v_i, v_f, t_i)$, where $v_i$ and $v_f$ represent the travel source and destination intersection nodes, respectively, while $t_i$ indicates the departure time. For simplicity, we assume that all vehicles are travelling at the same speed if no traffic congestion occurs. The routing path of the vehicle consists of a sequence of nodes $p_l = [v_i, ..., v_{id}]$. If $t_d$ is the arrival time at the destination $v_f$, the travel time $t_{L_l} = t_d \u2013 t_i$.\nProblem 1. MSD-SPP. Considering $L$ vehicles navigation through a road network $G$, we formulate a route planning policy as an optimization problem. The objective is to minimize the average travel time of all vehicles while adhering to the traffic capacity constraint of $G$.\n$\\min_\\pi \\sum_{l=1}^{L} t_{L_l}$  (1)\nHere, the traffic speed along the road segment $e_{ij}$ decreases and vehicles suffer from travel time penalties, if the number $n_{ij}$ of vehicles on $e_{ij}$ exceeds its capacity constraint $C_{ij}$.\nOur proposed framework is illustrated in Figure 2. To begin with, we divide the road network into $M$ regions, and transform the vehicle routing from source to destination intersections into the routing from source to destination regions, followed by routing within the destination region towards the destination intersection. More specifically, we formulate MSD-SPP as a two stage process, including inter-region and intra-region planing. Since the number of regions is significantly smaller than the number of intersections, we expect that the computational cost of the two-stage process is much lower than the original one.\nIn terms of road network division, we minimize the number of cutting edges across regions. For each region $R_i$, we denote $E$ to be the set of cutting edges originating from $R_i$, and $V$ to be the set of boundary nodes within the region $R_i$ that are connected by the cutting edges $E$. In Figure 2(a), we have 3 regions. The region $R_1$ involves three cutting edges $E_1 = \\{v_1 \\rightarrow v_4, v_3 \\rightarrow v_5, v_2 \\rightarrow v_5\\}$ and three boundary nodes $V_1 = \\{v_1, v_2, v_3\\}$. We can exploit existing algorithms, such as the classic work METIS [57], to perform this graph division, and tune the number of divided regions mainly depending upon the capacity of region agents. By assuming that an agent can observe the entire region including at most $E_r$ edges, we can roughly compute the number of divided regions by $\\lceil \\frac{E}{E_r}\\rceil$ where $E$ is the total number of edges in the input graph $G$.\nGiven the $M$ regions, we then treat each region as an individual agent, and assume that each agent can observe three following information,"}, {"title": "4 SOLUTION DETAIL", "content": "In this section, we give the details of the three components (the critic, actor, and reachability graph), and then describe the training method of our framework.\nThe key of our critic network is to develop asynchronous trajectory collection in the asynchronous MSD-SPP setting. We first give the general idea as follows. In the asynchronous MSD-SPP setting, some agents take actions and yet others not at the time step t. Whenever some actions are made, the change of the environment always occurs. Thus, we can exploit the developed global state to learn such a change and perform asynchronous trajectory collection as follows.\nMore specifically, whenever an agent $R_i$ takes an action, it inserts its own trajectory data into a separate buffer $D_i$. Denote a transition of a region agent $R_i$ as $(o_{i,qk}^{t}, a_{i,qk}^{t}, s_{i,qk}^{t}, r_{i,qk}^{t}, s_{i,qk}^{t\u2019})$, i.e., given the local state $s_{i,qk}^{t}$ and observation $o_{i,qk}^{t}$ at time step $t$, the agent $R_i$ makes an action $a_{i,qk}^{t}$ and changes the local state to $s_{i,qk}^{t\u2019}$ at time step $t\u2019$ with the reward $r_{i,qk}^{t}$.\nAfter collecting the trajectory data of each agent, we can apply the CTDE paradigm to train the standard MARL model on the collected data. That is, by using the global state $s_t$ and the request observation $o_{i,qk}^{t}$ as the local state $s_{i,qk}^{t}$ of agent $R_i$, the centralized critic exploits an MLP network to estimate the value of $s_{i,qk}^{t}$, i.e., $V(s_{i,qk}^{t}) = MLP([s_t, o_{i,qk}^{t}])$. In this way, with the value $V(s_{i,qk}^{t})$, the critic can evaluate how the action $a_{i,qk}^{t}$ is good or bad to update the policy of the local actor $R_i$.\nTo facilitate vehicle cooperation and avoid congestion caused by planning the same path for vehicles sharing similar or proximate destinations, we propose a novel actor network in Figure 4, consisting of an encoder of the road network, an encoder of plan requests, and an output module.\n(1) The encoder of the road network learns the traffic features of the road network. Recall that the road network observation $o_{i}^{t,G}$ involves the number $|E|$ of $F$-dimensional feature vectors regarding the cutting edges $E$. Thus, for each cutting edge $b_j \\in E$, we denote its feature vector by $(o_{i}^{t,G})_{b_j}$, and feed this vector into multi-layer perceptrons (MLP) to learn a latent embedding vector $e_{i}^{t,b_j}$ for the edge $b_j$. By the concatenation of $|E|$ vectors for the cutting edges $E$, we have the road network state embedding $e_i^{t,G}$\n$e_i^{t,s} = [e_i^{t,b_1},...,e_i^{t,b_{|E|}}]$, where $e_i^{t,b_j} = MLP((o_{i}^{t,G})_{b_j})$ (2)\nwhere $[\\cdot, \\cdot]$ is the concatenate operation and $e_i^{t,b_j} \\in \\mathbb{R}^{D_h}$, $e_i^{t,s} \\in \\mathbb{R}^{|E| \\times D_h}$, and $D_h$ is the dimensionality of the embeddings.\n(2) The encoder of plan requests takes each request vector $o_{i,qk}^{t}$ into an MLP and yields the embedding $e_{i,qk}^{t} \\in \\mathbb{R}^{D_h}$ for the request $q_{i,k}$. To enable the cooperation among those plan requests $Q_i^t$ within the region $R_i$, we learn an embedding for the entire plan requests. Since the number $|Q_i^t|$ differs from the time step $t$, we employ a Gated Recurrent Unit (GRU) [14] cell to represent such requests $Q_i^t$ as follows.\n$h_{k\u2019} = GRU(o_{i,q_k}^{t}\u2019, h_{k\u2019\u22121}), k\u2019 = 1,..., |Q_i^t|$ (3)\nwhere the request vector $o_{i,q_k}^{t}\u2019 \\in \\mathbb{R}^{F\u2019}$ and $h_{k\u2019} \\in \\mathbb{R}^{D_h}$ are input vector and the hidden state of the GRU cell at step $k\u2019$, respectively. Given the sequential GRU model, we take the hidden state of the final GRU cell as the embedding of $Q_i^t$, i.e., $e_i^{t,Q_i^t} = h_{|q_i|} \\in \\mathbb{R}^{D_h}$.\n(3) Now we have three embeddings, i.e., $e_i^{t,s}$, $e_{i,qk}^{t}$ and $e_i^{t,Q_i^t}$.\nWith such embeddings as input, the actor produces a probability distribution on the cutting edges $E$ and chooses the cutting edge with the highest probability. Specifically, the output module first maps the embeddings to a score vector $u_{i,qk}^{t} \\in \\mathbb{R}^{|E|}$ on cutting edges $E$ with an MLP, then employs a softmax function [16] to transform the edge scores into a probability distribution on $E$. Here, a greater edge score corresponds to a higher probability. The intuition of the probability distribution is as follows. With help of the embeddings $e_i^{t,Q_i^t}$ and $e_{i,qk}^{t}$, the actor can identify those requests towards the same or close destinations. If the number of such requests is small, the actor tends to assign a high probability (near 1.0) to the cutting edge which leads to the shortest travel time for the request $q_{i,k}$. Otherwise, the actor assigns an even probability distribution across the cutting edges $E$. In this way, we have chance to avoid planning too many vehicles on the same road segments and thus achieve cooperation among the vehicles.\nSpecifically, we use a non-linear transformation implemented by MLP to achieve the mapping from embeddings to scores. In addition, we apply the invalid action mask [20] with the help of the reachability graph (Section 4.3) to avoid loops in the solution paths. In this way, we compute the score of a cutting edge $b_j$ by $(u_{i,qk}^{t})_{b_j} = (mlp([e_i^{t,s}, e_{i,qk}^{t}, e_i^{t,Q_i^t}]))_j$ if the chosen action $b_j$ is within the reachability graph (that will be given soon) and otherwise $(u_{i,qk}^{t})_{b_j} = -\\infty$. After that, we employ a softmax function [16] to compute the probability of selecting edge $b_j \\in E$ based on the scores,\n$P(a_{i,qk}^{t} = b_j) = \\frac{exp((u_{i,qk}^{t})_{b_j})}{\\sum_{l=1}^{|E|} exp((u_{i,qk}^{t})_l)}$ (4)\nWith the softmax, a greater score regarding a cutting edge $b_j$ leads to a higher probability of being chosen as an action, and thus the probability to select invalid edge becomes zero.\nDue to the lack of a global view, the inter-region routing plan may suffer from the issue of infinite loops. To overcome this issue, we propose to construct a reachability graph for each vehicle $L_l = (v_i, v_f, t_i)$. Firstly, when a vehicle $L_l$ travels from the source $v_i$ to the destination $v_f$, the source region agent processes the plan request and constructs the reachability graph $H_l$ based on the static graph information. The construction of such a reachability graph by the source agent makes sense, because every region agent can observe the static information of an entire road network, such as the GPS coordinates of intersections and the lengths of road segments. After that, when the trip continues, the graph $H_l$ is then sent to a neighbor region agent, which can mask invalid actions that will lead to loops via the received graph $H_l$.\nTo construct a reachability graph $H_l$, the source region first needs to build a connection graph $I$ to represent the potential"}, {"title": "5 EXPERIMENT", "content": "We conduct experiments on two data sets, a synthetic road network and a real-world road network, and perform traffic simulation on a widely used simulator, namely the Simulation of Urban Mobility (SUMO) [58].\n(1) Synthetic data set: Figure 6(a) illustrates the synthetic road network of 4 regions, and each region consists of 25 intersections and 84 road segments. The length of road segments is 100 meters. Each region agent involves 4 cutting edges (i.e., the action space is 4), and the four regions totally 16 cutting edges. We generate traffic flows from the yellow region to the green one, and then randomly choose source and destination intersection nodes within the two regions, respectively. Moreover, we set the maximum speed on each road segment as 13.89 m/s.\n(2) Real data set: We use a traffic data set provided by the previous work [55], which provides the road network data in Koln, Germany. In this data set, we select an area of 100 $km^2$, from 6.166\u00b0E, 50.735\u00b0N to 7.295\u00b0E, 51.840\u00b0N. The selected area contains 2515 intersections and 5784 road segments. Following the data pre-processing technique [56], we remove the isolated road segments and dead-ends and then employ METIS [57] to divide the road network into 12 regions with similar sizes and totally 84 cutting edges. As shown in Figure 6(b), the number of cutting edges per a region ranges from 2 to 11 with an average 7. The road network involves 13 types of road segments. Depending upon the road types, we set the maximum travel speed of each road segment. For example, the maximum speeds of the primary road and secondary road are 19.44 m/s and 13.89 m/s, respectively.\nDuring the simulation, we inject vehicles into a road network every second until the total number of injected vehicles reaches a predefined value (e.g., 200 and 1500 for the synthetic and real road networks, respectively). When the number of vehicles on a road segment exceeds the road capacity limit, we follow the previous work [49] to simulate traffic congestion by decreasing the maximum speed by $a * V_{max}$. Here, the factor a = 0.1 and $V_{max}$ is the original maximum speed of the road segment.\nBy default, we set the capacity limits on the synthetic network as 10, and Section 5.4.5 will evaluate the performance of asyn-MARL under various capacity limits and maximum traffic volumes on the synthetic network. Instead, mainly due to the rather complex road types, we set a relatively high capacity limit 50 to the koln road network. The episode length of experiments is set as 600 seconds and 300 seconds for the two data sets. Table 2 and 3 gives the simulation parameters on the two data sets and the hyper-parameters of the RL model, respectively.\nWe compare our asyn-MARL with three traditional shortest path-based and two MARL-based algorithms."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose a cooperative shortest path planning framework in the asynchronous MSD-SPP setting. To deal with the ineffectiveness and inefficiency issue of existing works, we propose the two-stage routing plans (inter-region and intra-region plans), and formulate the inter-region planning as a decentralized partial observable Markov decision process (Dec-POMDP). The proposed framework asyn-MARL is an actor-critic-based algorithm consisting of a centralized critic improved by asynchronous trajectory collection to address the non-stationary issue of MARL training, a novel actor to cooperate the vehicles with close destinations, and a reachability graph to avoid infinite loops. On both synthetic and real datasets, our evaluation demonstrates that our work asyn-MARL outperforms both the classic shortest path computation algorithms and the recent MARL-based baselines."}]}