{"title": "ADAPT-PRUNER: ADAPTIVE STRUCTURAL PRUNING FOR EFFICIENT SMALL LANGUAGE MODEL TRAINING", "authors": ["Boyao Wang", "Rui Pan", "Shizhe Diao", "Xingyuan Pan", "Jipeng Zhang", "Renjie Pi", "Tong Zhang"], "abstract": "Small language models (SLMs) have attracted considerable attention from both academia and industry\ndue to their broad range of applications in edge devices. To obtain SLMs with strong performance,\nconventional approaches either pre-train the models from scratch, which incurs substantial computa-\ntional costs, or compress/prune existing large language models (LLMs), which results in performance\ndrops and falls short in comparison to pre-training. In this paper, we investigate the family of accel-\neration methods that involve both structured pruning and model training. We found 1) layer-wise\nadaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements\nover existing pruning techniques, 2) adaptive pruning equipped with further training leads to models\ncomparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance\ngain by interleaving pruning with training and only removing a small portion of neurons (~5%)\nat a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms\nconventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7%\nin accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of\nMobileLLM-125M to 600M on the MMLU benchmark with 200\u00d7 fewer tokens via pruning from\nits larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple\nbenchmarks.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) [Kalyan, 2024, OpenAI, 2023] have demonstrated remarkable performance across\na wide range of benchmarks. As their size increases, these models exhibit enhanced capabilities in understanding\nnatural language and solving complex tasks through text generation [Zhao et al., 2023]. However, achieving such\nperformance requires models with billions of parameters, which presents significant challenges for practical deployment.\nThe sheer scale of LLMs leads to high computational costs, making inference both resource-intensive and slow, and\npotentially introducing issues such as increased latency. Consequently, there is a growing demand for methods to\ncompress LLMs [Zhu et al., 2024], aiming to reduce the number of parameters and improve inference speed, all while\npreserving the original model performance. Effective compression techniques hold the potential to create more efficient\nand deployable LLMs.\nSeveral techniques have been proposed to compress LLMs, most of which fall into one of four categories: structured\nand unstructured pruning [Cheng et al., 2024], quantization [Gholami et al., 2022], low-rank factorization [Sainath et al.,\n2013], and knowledge distillation [Gou et al., 2021]. In this paper, we primarily focus on structured pruning, which can\nbe combined together with further training to obtain strong Small Language Models. Structured pruning removes entire\nfilters, layers, or specific model components from neural networks, enabling both compression and realistic acceleration.\nOn top of that, it does not require specialized hardware or library support to achieve these benefits [He and Xiao, 2023]\nin contrast to unstructured pruning."}, {"title": "Related Work", "content": "Pruning Pruning removes weights and modifies the model's architecture. Formally, given a neural network f(x; W),\npruning produces a new model f(x; M \u2299 W), where M \u2208 {0,1}|W| is a binary mask that sets certain parameters to"}, {"title": "Method", "content": "3.1 Adapt-Pruner: Layer-wise Adaptive Pruning aim for Mapping-Preserving\nGiven a large language model M, represented as a sequence of embedded layers with N decoder layers, denoted as LN,\nalong with a final output layer, our method leverages the insight that each decoder layer contributes differently to the\nmodel's overall performance. Furthermore, the contribution of each layer is measured by its importance of maintaining\nthe original functional mapping of the model after pruning.\nSpecifically, Adapt-Pruner compresses the model through multiple iterations, with each iteration comprising two steps:\n1. Evaluating layer importance: Quantitatively computing the importance of each decoder layer and assigning\na corresponding pruning sparsity.\n2. Pruning coupled weights: Grouping the weights within each decoder layer, evaluating the importance of each\ncoupled structure, and pruning the least important structures based on the assigned sparsity"}, {"title": "Assign Sparsity based on Importance", "content": "3.1.1\nLet Li denote the i-th decoder layer, I\u00b9 and S\u00b9 represent the importance and sparsity of the i-th decoder layer, and\nLin and Lout denote the input and output tensors of the i-th decoder layer, respectively. Our goal is to estimate the\nimportance of each decoder layer.\nEstimate Decoder Layer's Importance Our pruning method targets only the multi-head attention and multilayer\nperceptron components within the self-attention layers, leaving the hidden size unchanged. Consequently, the input and\noutput tensors for each decoder layer have identical shapes:\n\u2200i = 0, 1, . . . , N \u2013 1, Shape(Lin) = Shape(Lout)\n= (B, L, H)\nwhere B, L, H denote the batch size, sequence length, and hidden size, respectively. Based on this, we use a function\nthat measures the vector similarity or distance between Lin and Lout to assess the changes in the tensor caused by\neach decoder layer. The greater the similarity or the smaller the distance between Lin and Lout, the less important that\ndecoder layer is. This intuition derives from Li et al. [2024], where it is observed that top self-attention layers have\ndiminished gradient norms and those layers serve similar purposes as identity functions.\nA practical choice for this distance measurement is cosine similarity, where a decoder layer's importance is computed\nas follows:\n\u2200i = 0, 1, . . . , N \u2212 1, I\u00b2 = \u2212cosine_similarity(Lin, L'out)\nThis method can easily be extended to alternative similarity or distance functions, such as Euclidean or Manhattan\ndistance. To ensure consistency, we normalize the decoder layer's importance to the range [-1,1] with a mean of 0, as\nfollows:\nI \u2190 I \u2013 Imean\nI \u2190\nIi\nmax |abs(I)|\nAssign Sparsity After obtaining the importance of each layer, an ad-hoc approach is adopted to link a layer's\nimportance to its sparsity, which decides the number of neurons it will be pruned. Let A being any constants, the\ntargeted sparsity S\u00b2 for each layer i can be:\n\u2200i = 0,1,..., N \u2212 1, S\u00b2 = Sbase \u2013 A \u00b7 I'\nwhere Sbase is the targeted overall sparsity of the model. This formula ensures that each decoder layer's sparsity is\ninversely proportional to its importance, and the averaged sparsity is consistent with the intended overall model sparsity.\nWe call the hyperparameter A as the amplitude of sparsity."}, {"title": "Pruning Weight Groups Inside Decoder Layer", "content": "3.1.2\nTo enable structure-aware pruning in Adapt-Pruner, methods from Ma et al. [2023], Fang et al. [2023] are employed to\nbuild dependency graphs for LLMs, which facilitates automatic identification and extraction of coupled structures in\nLLMs.\nWeight Group Importance With coupled structures and target sparsity defined for each group, the next step is\nselecting weight matrices to prune with minimized performance degradation. For any grouped weight structure\nG = Wk, containing k weight matrices, a calibration dataset D is adopted to assess the relative importance of each\nmatrix. Following [LeCun et al., 1989, Ma et al., 2023], the importance of the i-th weight matrix in layer L is defined\nas:\nIw; = |AL(D)|\n= |Lw; (D) - Lw\u2081=0(D)|\nALT (D)\n1\nWi\nWHW\u2082 + O (\\W\\3)\nWi\n2"}, {"title": "Adapt-Accel: Incremental Pruning with Interleaved Recovery Training", "content": "3.2\nTo achieve the target overall sparsity, our model undergoes multiple rounds of pruning, which inevitably leads to\nperformance degradation. Inspired by neuroplasticity in biological neural networks [Wall\u00f8e et al., 2014], periodic\ninterleaved recovery phases are introduced through post-training after each pruning. The optimal pruning ratios for\ntriggering these recovery phases were determined through ablation studies in Section 4.4.\nIn addition, it is observed that different pruning phases lead to different levels of performance deterioration. Specifically,\npruning in later phases is more likely to remove important neurons. Based on this intuition, a linear growth schedule is\nintroduced for training data allocation: For i-th post-training of T total pruning and post-training iterations, |Di| tokens\nare sampled from |D| total training tokens where\n|Di = 2(i + 1)\n|D|(|D| + 1)"}, {"title": "Experiment", "content": "4\n4.1 Adapt-Pruner as Effective LLM Pruners\nAdapt-Pruner exploits the skewness of importance across layers in a mapping-preserved manner, which allows the\npruning process to automatically identify prunable layers that least affect the functionality of the target LLM.\nSetup To demonstrate the effectiveness of Adapt-Pruner, different pruners are evaluated and compared on Llama-\n3.1-8B [AI@Meta, 2024]. For task-agnostic performance evaluation of the pruned models, zero-shot classification"}, {"title": "Adapt-Accel as Efficient LLM Trainers", "content": "4.2\nBuilt on top of Adapt-Pruner, Adapt-Accel incorporates interleaved pruning and training to accelerate the optimization\nof SLMs, which is shown to be a better strategy compared to past methods [Xia et al., 2024, Sreenivas et al., 2024].\nSetup As the training of language models requires a non-trivial amount of computational resources, a smaller family\nof models are adopted for this section of experiments. MobileLLM [Liu et al., 2024] is a series of SLMs developed by\nMeta for on-device deployment purposes and stands for one of the strongest SLMs on the scale of 125M/350M.\nTo demonstrate the superiority of Adapt-Accel, three benchmarks are employed for evaluation, including BBH [Srivas-\ntava et al., 2022], TruthfulQA [Lin et al., 2021], AGIEval [Zhong et al., 2023], which assess all methods performance in\ndifferent aspects beyond commonsense reasoning. A hybrid dataset with 3.87B tokens is utilized for training, where the\ndata source is available in Table 2.\nFor the interleaved training in Adapt-Accel, a total number of Np = 20 interleavings are adopted in Adapt-Accel,\nwhich leads to a pruning ratio of (125/300)1/Np \u2248 95.7% per training. In other words, the pruning and training will\nbe applied alternatively for 20 times, where each pruning will remove ~ 95.7% of the current models' weights, along\nwith a follow-up recovery training in a |D\u2081| = 2(i + 1)/(|D|\u00b2 + |D|) random samples (without replacement) from the\ntraining set at the i-th iteration."}, {"title": "Adapt-LLMs as Strong LLMs", "content": "4.3\nAdapt-Accel is a favorable tool for fast and flexible customization of model sizes depending on the practical use cases.\nSpecifically, once the large version of LLMs has been obtained from pre-training or other sources, Adapt-Accel can be\nutilized to inherit the capabilities from the target LLM and accelerate the training of its smaller versions. The family of\nSLMs obtained in this fashion, named Adapt-LLMs, not only reduces costs during its training process, but also exhibits\nsignificant performance improvements.\nSetup To provide evidence in support of the claimed strengths of Adapt-LLM, two types of experiments are conducted,\nindividually demonstrating the acceleration and performance benefits of Adapt-LLM. The first branch of experiments\nfocuses on the acceleration aspect of Adapt-Accel, where different sizes of MobileLLMs, ranging from 350M to\n1B, are employed. The second branch of experiments emphasizes performance, where Adapt-Accel is applied to\nQwen-2.5-0.5B [Team, 2024] and Deepseek-R1-Distill-Qwen-1.5B [DeepSeek-AI et al., 2025], proving that the pruned\nAdapt-LLMs can still match or even surpass popular strong open-source models without the heavy cost of pretraining.\nThe same benchmarks and datasets of Section 4.2 are adopted here.\nResults As shown in Table 3, Adapt-LLM pruned from the larger version of MobileLLMs recovers its performance\nacross all model sizes in all benchmarks for models larger than 350M, at a reduced cost of 200\u00d7 less training tokens.\nThis offers strong evidence that Adapt-Accel is a promising acceleration technique especially suitable for customizing\nmodel sizes flexibly.\nTable 5 further demonstrates the performance gain brought by Adapt-Accel via pruning from strong LLMs. It is worth\nnoticing that Deepseek-R1-Distill-Qwen-1.5B [DeepSeek-AI et al., 2025] \u2192 1B leads to a 1B model even stronger\nthan LLaMA-3.2-1B [AI@Meta, 2024] in MMLU, delivering three orders of magnitude cost reduction in terms of\ntraining tokens. This indicates that Adapt-Accel can serve as a favorable tool for inheriting performance from strong\nopen-source LLMs, allowing researchers and engineers with limited computational resources to still keep up with the\nfast iteration speed of state-of-the-art LLMs.\nComputation Cost An 8B-sized model can be pruned adaptively in 2 to 5 minutes on a single NVIDIA A40 GPU, and\nin 15 to 30 minutes on an Intel(R) Xeon(R) Gold 6346 CPU. After applying iterative post-training, the full compression\nprocess takes between 3 and 18 hours. Benchmark evaluation of the compressed models requires an additional 15 to 30\nminutes. In the experiments of Qwen2.5-0.5B \u2192 350M, the training process costs ~ 72 GH200 GPU hours. Compared\nwith the ~ 4608 A100 GPU hours pretraining cost of MobileLLM-350M, it is at least 15\u00d7 more efficient."}, {"title": "Ablation Study: Optimal Interleaving Frequency", "content": "4.4\nInterleaved training is shown to be quite beneficial compared to the traditional prune-then-train paradigm. To further\ninvestigate the optimal frequency for interleaved training during the pruning process, additional experiments are\nconducted on MobileLLM-350M 125M.\nSetup All experiments are conducted on MobileLLM-350M, with 1B tokens sampled from Slimpajama [Soboleva\net al., 2023] and 1/6 random samples in the aforementioned dataset (Table 2). Two types of benchmarks are adopted,\nincluding the commonsense benchmarks in Section 4.1 and MMLU benchmark in Section 4.2. Pruning ratios per training,\nranging from {0.36, 0.90, 0.925, 0.95, 0.975, 0.99}, are searched to decide the optimal value, which corresponds to the\nnumber of interleaves Np \u2208 {1, 9, 12, 20, 38, 96} separately.\nResults As shown in Figure 4, the optimal interleave frequency is around 95% pruning ratio per training, i.e. every\nrecovery training after ~ 5% removal of weights or neurons. It is worth noticing that the performance degrades\nsignificantly when the interleave frequency is too low or too high, implying the occurrence of knowledge loss caused\nby large-portion pruning, or unstable learning due to too-frequent pruning. This phenomenon is quite intriguing as it\nresembles the disappearance of neurons in human brains."}, {"title": "Conclusion", "content": "5\nIn this paper, a novel family of structural pruning methods called AdaptPrune is proposed for LLMs. Adapt-Pruner\nis motivated by 1) the skewness of importance across decoder layers and 2) the goal of preserving the input-output\nmapping for all the layers. These two properties of AdaptPruner lead to significant improvement over conventional\npruning methods.\nOn top of that, Adapt-Pruner gives rise to a novel acceleration paradigm called Adapt-Accel, which is the first\nacceleration approach that combines Adapt-Pruner with interleaved training, a technique shown to provide non-trivial\nperformance gain over the traditional prune-then-train framework. Adapt-Accel provides consistent improvement over\npast structural-pruning-based acceleration methods, including ShearedLLaMA and Minitron.\nAdapt-Accel further enables efficient and flexible customization of model sizes by pruning from larger-sized LLMs.\nSpecifically, it is capable of recovering MobileLLMs' performance from their larger counterparts, and discovers a\n1B-sized model with better performance than LLaMA-3.2-1B in multiple benchmarks."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. Specifically, we expect the proposed\nmethods can greatly reduce the pre-training cost of SLMs, leading to less computational resource consumption, less\ncarbon dioxide emissions, and faster development of SLMs."}, {"title": "Experimental Details", "content": "A\nBasic Setup We extend the LLM-Pruner framework [Ma et al., 2023, Fang et al., 2023] as our baseline, which\nincorporates modules for computing similarity scores and adaptive pruning using PyTorch [Paszke et al., 2019]. Our\nexperiments utilize both NVIDIA GH200 and H100 GPUs, where a single GPU (either GH200 or H100) is utilized for\npruning and evaluation tasks while 4 additional GPUs are employed in parallel for post-training. The metrics and the\nnumber of shots used for each benchmark are available in Table 6.\nExperimental Settings for Adapt-Pruner For Adapt-Pruner, we set A = 0.02 in Equation 6 based on Ablation\nstudy B.2. We evaluate three models with three sparsity choices for each model. All the methods are aligned using\nSlimpajama [Soboleva et al., 2023] as the calibration dataset, which comprises 512 sequences with a maximum length\nof 64 tokens.\nExperimental Settings for Adapt-Accel and Adapt-LLMs For Adapt-Accel and Adapt-LLMs, we keep the same\nchoice of sparsity amplitude A = 0.02 in Equation 6. The global batch size is set to 128, with maximal learning rate of\n2 \u00d7 10-5, and minimal learning rate of 2 \u00d7 10-6. WSD scheduler [Hu et al., 2024] is applied, where the combination of\nlearning rate in all interleaved training iterations forms a WSD scheduler, with 5% warmup steps and 10% linear decay\nsteps at the end. For the ShearedLLaMA [Xia et al., 2024] comparison, we keep these hyperparameters while allocating\n10% of tokens for pruning optimization and 90% for post-training. Similarly, for Minitron comparison [Sreenivas et al.,\n2024], we keep the same settings and fine-tune the teacher model for one epoch on the dataset."}, {"title": "Additional Experimental Results", "content": "B\nB.1 Adapt-Pruner as Strong Pruners\nWe evaluate various structured pruning methods on three LLaMA-series models: LLaMA-3.1-8B, LLaMA-3.2-3B, and\nLLaMA-3.2-1B [AI@Meta, 2024]. For each model, we test sparsity levels of 20%, 40%, and 60% over three trials to\nassess our method's effectiveness across different model sizes.\nAs shown in Table 7, Adapt-Pruner demonstrates significant improvements in commonsense benchmarks, especially at\n40% sparsity level. We conjecture the sparsity of ~60% to be the ceiling of the redundancy information in LLaMA-\nseries models, since further pruning incurs severe performance degradation, with closing gaps across different pruning\nmethods.\nB.2 More Ablation Study\nSensitivity Analysis of Sparsity Amplitude Parameter A We empirically investigate the impact of amplitude\nparameter A in Equation 6 on our algorithm's performance. The amplitude A directly influences the architectural\nsearch space of the compressed model: insufficient amplitude constrains the exploration of potential architectural\nconfigurations, while excessive amplitude can lead to structural imbalances that degrade model performance. To"}]}