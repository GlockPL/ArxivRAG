{"title": "Enhancing Modality Representation and Alignment for Multimodal Cold-start Active Learning", "authors": ["Meng Shen", "Yake Wei", "Jianxiong Yin", "Deepu Rajan", "Di Hu", "Simon See"], "abstract": "Training multimodal models requires a large amount of labeled data. Active learning (AL) aim to reduce labeling costs. Most AL methods employ warm-start approaches, which rely on sufficient labeled data to train a well-calibrated model that can assess the uncertainty and diversity of unlabeled data. However, when assembling a dataset, labeled data are often scarce initially, leading to a cold-start problem. Additionally, most AL methods seldom address multimodal data, highlighting a research gap in this field. Our research addresses these issues by developing a two-stage method for Multi-Modal Cold-Start Active Learning (MMCSAL). Firstly, we observe the modality gap, a significant distance between the centroids of representations from different modalities, when only using cross-modal pairing information as self-supervision signals. This modality gap affects data selection process, as we calculate both uni-modal and cross-modal distances. To address this, we introduce uni-modal prototypes to bridge the modality gap. Secondly, conventional AL methods often falter in multimodal scenarios where alignment between modalities is overlooked. Therefore, we propose enhancing cross-modal alignment through regularization, thereby improving the quality of selected multimodal data pairs in AL. Finally, our experiments demonstrate MMCSAL's efficacy in selecting multimodal data pairs across three multimodal datasets.", "sections": [{"title": "1 Introduction", "content": "Deep multimodal networks process multimodal data pairs consisting of various modalities, harnessing the complementary information across these modalities [4, 37]. However, their effectiveness hinges on the availability of a substantial volume of labeled multimodal data for training. To alleviate the issue of expensive data labeling, active learning (AL) strategies are widely applied to selectively annotate the most significant data samples [25]. Most AL strategies initiate with a randomly selected subset of data, obtaining label annotations for these samples to start the data evaluation and selection, a process known as Warm-start. Nevertheless, in practice, assembling a multimodal dataset often starts with limited or even no labels. This poses a challenge for uncertainty-based, diversity-based, and hybrid AL strategies. These strategies rely on initial labels to construct a model capable of assessing the uncertainty and diversity of the data. A lack of sufficient initial labels"}, {"title": "2 Related Works", "content": "Multimodal Self-supervised Learning constructs semantically rich multimodal representations without the need for human annotations. These methods can be categorized by their learning objectives into contrastive discrimination, clustering, and masked token prediction [39]. Contrastive methods treat paired multimodal samples as positive pairs and unpaired ones as negative pairs, aiming to minimize the representation distance between positive pairs while maximizing it for negative pairs during training. Models trained with contrastive learning, such as CLIP [24], an image-text model, have produced image and text representations with aligned cross-modal semantic information, demonstrating robust zero-shot capabilities. Furthermore, contrastive learning has been successfully applied to various modality combinations, including video-text [34, 38], audio-text [12], and video-audio [16, 22].\nClustering aims to group data with similar semantic information. In the realm of visual SSL, SwAV [6] performs online clustering, assigning multiple instances to prototypes and learning representations by predicting the cluster assignment of one view from another view. PCL [19] combines contrastive learning for extracting useful visual features with cluster assignment prediction to enhance the semantic structure of the representations. In the multimodal domain, XDC [1] uses the clustering assignments of one modality as prediction targets for another, thereby deriving multimodal representations. AV-HuBert [28] performs clustering on unmasked visual and auditory sequences, forcing the masked visual and auditory tokens to predict the clustering results.\nCold-start Active Learning aims to select the initial batch of data samples in circumstances where no labels are available. An effective cold-start AL strategy should identify a subset of samples that are more informative and diverse compared to a randomly chosen set. Due to the absence of labels, SSL has been widely adopted to mitigate the cold-start problem in active learning. ALPS [36] utilizes self-supervised masked language modeling based on BERT [11] to create surprisal embeddings for each sentence sample, identifying more significant samples. PT4AL [35] performs a pretext task before data selection and identifies significant positive correlations between rotation prediction and image classification losses, and between colorization and segmentation losses. It ranks samples based on their pretext task loss, selecting the most challenging ones for the initial cold-start batch. CSVAL [9] uses MoCo-v2 [10] as a SSL strategy to obtain image representations, then generates pseudo labels through K-means to train a classification model. It adopts Dataset Map [29] to select out easy-to-learn samples to form the first batch. These samples are also found to be hard-to-contrast samples as they are hard to be distinguished from others, indicating their typicality. ActiveFT [33] proposes to utilize data representations provided by pretrained models such as DINO [7] to select a subset of data for label annotations and subsequent model fine-tuning. It minimizes the data distribution gap between the selected subset and the entire unlabeled dataset and regularizes the diversity to keep data selection from collapsing to centroid points.\nThese cold-start AL methods are devised for unimodal tasks. In this work, we aim to address the cold-start problem for multimodal AL by designing a suitable MMSSL to improve representation quality and enhancing modality alignment during data selection."}, {"title": "3 Proposed Method", "content": "We illustrate the multimodal cold-start active learning framework in Figure 1 and provide details about our method in Figure 2. In the first stage, we are provided with an unlabeled multimodal dataset $XU = \\{(x_1, x_2)\\}i\u2208[N]$, consisting of N data pairs across modalities $m_1$ and $m_2$. MMSSL is used to train a two-stream multimodal feature encoder, $f_{m1}(x_1)$ and $f_{m2}(x_2)$, for each modality, mapping data samples to representations $r_{m1}$ and $r_{m2}$ in the normalized high-dimensional feature space $IR^C$. The concatenated multimodal representations, $(r_{m1}||r_{m2})$, encapsulate the combined characteristics of both modalities.\nIn the second stage, we allocate a label budget B, typically ranging from 1% to 10% of the total dataset size N, for acquiring labels from human oracles. Using the multimodal representations $RU = \\{\\{r_{m1}||r_{m2}\\}\\}i\u2208[N]$ from the first stage, we apply an AL strategy $S_{AL}$ to select a subset $XS = \\{(x_1, x_2)\\}j\u2208[B]$ from the entire unlabeled dataset $XU$. This budget B is used to assign labels $\\{yj\\}j\u2208[B]$ to the selected subset, forming a labeled subset $XL= \\{(x_1, x_2, y_j)\\} j\u2208 [B]$. The labeled subset is then used to train a downstream task model."}, {"title": "3.2 Multimodal Contrastive Learning with Uni-modal Prototypes", "content": "The objective of MMSSL is to produce high-quality representations for multimodal data that can accurately capture both the uni- and cross-modal relationships. To first learn the cross-modal connections, we can seamlessly adopt cross-modal contrastive learning methods such as CLIP [24]. The idea is to bring the representations of two modalities from the same pair closer and push away those from the unpaired data. Specifically, for a batch of K data pairs, the process involves maximizing the cosine similarity between L2-normalized modality representations for the K paired samples and minimizing it for the $K \u00d7 (K - 1)$ unpaired modality combinations. We optimize the InfoNCE loss [31], with the temperature parameter \u03c4 set to 0.07, as recommended by studies such as [10, 14]:\n$LCross = \\frac{1}{\\left|B\\right|} \\sum_{i \\in [K]} \\left(-\\frac{1}{2} \\log \\frac{\\exp(\\cos(r_{m1}^i, r_{m2}^i)/\u03c4)}{\\sum_{j \\in [K]} \\exp(\\cos(r_{m1}^i, r_{m2}^j)/\u03c4)} - \\frac{1}{2} \\log \\frac{\\exp(\\cos(r_{m2}^i, r_{m1}^i)/\u03c4)}{\\sum_{j \\in [K]} \\exp(\\cos(r_{m2}^i, r_{m1}^j)/\u03c4)}\\right).\\nonumber$\nHowever, measuring the distance between multimodal sample pairs based solely on cross-modal distance may not be sufficiently accurate due to the modality domain gap. As shown in [20], there is a modality gap in representations trained by contrastive loss, affecting the accuracy of distance measurement between samples which is crucial in our data selection. This modality gap phenomenon is also observed in our experiments across three datasets. To reduce this modality gap, we incorporate uni-modal prototypes that reshape the multimodal representations by optimizing the uni-modal prototypical loss within each modality.\nWe construct uni-modal prototypes individually for each modality and visualize our MMSSL method in the left side of Figure 2. Here, we follow the implementation from PCL [19]. Take the first modality $m_1$ as an example, we perform K-means clustering on its uni-modal representations $\\{r_{m1}^i\\}i\u2208[N]$ to produce C clusters, and the centroid of each cluster is referred to as a prototype $p_{m1}$. The same process is performed for the other modality individually. We assign the representation $r_{m1}$ to the prototype $p_{m1}^c^i$ of its cluster $c_i$ as a positive pair. Conversely, we assign it to the prototypes $\\{p_{m1}^c\\}j\u2260i$ from other clusters $\\{c_j\\}j\u2260i$ as negative pairs. Based on this, we define the uni-modal prototypical loss as:\n$LUni = \\frac{1}{\\left|B\\right|} \\sum_{i \\in [K]} \\left(-\\frac{1}{2} \\log \\frac{\\exp(\\cos(r_{m1}^i, p_{m1}^c^i)/\\phi_1)}{\\sum_{j \\in [K]} \\exp(\\cos(r_{m1}^i, p_{m1}^c)/\\phi_1)} - \\frac{1}{2} \\log \\frac{\\exp(\\cos(r_{m2}^i, p_{m2}^c^i)/\\phi_2)}{\\sum_{j \\in [K]} \\exp(\\cos(r_{m2}^i, p_{m2}^c)/\\phi_2)}\\right),\\nonumber$\nwhere \u03c6 estimates and balances the concentrations \u03c6 of formed uni-modal clusters. The value of \u03c6 for each cluster is calculated based"}, {"title": "3.3 Select Data with Good Modality Alignment for Cold-start AL", "content": "After obtaining the learned multimodal representations through our designed MMSSL, we need to conduct multimodal data selection to form the initial batch for cold-start AL. We first provide preliminary studies about ActiveFT [33], which is the current state-of-the-art cold-start AL algorithm. As proposed in ActiveFT, the selected subset XS should have two characteristic: (1) the distribution gap between the representations of the selected subset RS and those of the entire unlabeled dataset RU should be minimized and (2) the diversity of the selected subset RS should be maintained to prevent collapsed selection.\nTo align with these two guidelines, the data samples to be selected with the labelling budget B are modeled as a parametric model OS = $\\{\\theta_j\\}j\u2208[B]$. In our case with two modalities, we model them as OS = $\\{\\theta_{m1}\\theta_{m2}\\}$ j\u2208 [B], where each parameter is concatenated by two uni-modal parameters, and it will be a multimodal representation corresponding to one selected multimodal data sample pair. Assume the model is optimized to satisfy all the characteristics, then the sample $x_j$ will be selected if its representation $(r_{m1}^i||r_{m2}^i)$ is closest to the parameter $(\\theta_{m1}||\\theta_{m2})$, so that we will have a selected subset accordingly. The optimization goal of the data selection parametric model is:\n$\\Theta_{opt}$ = arg min D(RU, OS) \u2013 \u03bbdivR(OS),\nwhere D(RU, OS) is the measurement of distribution gap between the representations of the entire unlabeled dataset and our data selection parametric model, and R(OS) is the diversity of the parameters and \u03bbdiv is a hyper-parameter set as 1.0 in ActiveFT that controls the contribution of the diversity:\nD(RU, OS) = \u2013  \u03a3 \\frac{\\cos(r_{m1}^i, \\theta_{m1}) + \\cos(r_{m2}^i, \\theta_{m2})}{\\tau},\nR(OS) = \u2212  \u03a3 \\log exp  \\frac{\\cos(\\theta_{m1}^j, \\theta_{m1}^k) + \\cos(\\theta_{m2}^j, \\theta_{m2}^k)}{\u03a3\u03c4},\\nonumber$\nWe argue that, when selecting data pairs with multiple modalities, the selected subset XS should have an additional characteristic:"}, {"title": "4 Experiments", "content": "We choose three different multimodal classification datasets:\nKineticsSound [2] is a subset of Kinetics-400 [18]. It consists of 31 action classes which are all correlated to both video and audio signals. To obtain more data pairs for us to perform MMSSL, we include all available video clips of these 31 action classes from Kinetics-400, resulting in 22,588 video clips for training and 3,012 video clips for testing.\nFood101 [32] is an image-text classification dataset for food recipe recognition with 101 kinds of food, where each recipe is composed of a food image and a text recipe description. There are 45,719 sample pairs for training and 15,294 sample pairs for testing.\nVGGSound [8] is a large video-audio dataset with 309 classes where each video clip captures the object that makes the sound. We are only able to download 180,911 clips for training and 14,843 clips for testing. Some of the video clips could not be downloaded due to their unavailability on the YouTube website."}, {"title": "4.2 Baseline", "content": "We compare our method with random selection, four cold-start AL strategies and four warm-start AL methods. Random selects the data samples randomly from the unlabeled data pool.\nWarm-start AL: (1) BADGE [3] extracts the gradient embedding of multimodal classifier and employs K-means++ initialization method to select diverse and informative data samples for label annotations. (2) BMMAL [27] achieves balanced multimodal data selection by modulating the gradient embedding of each uni-modal classifier with modality contribution scores. (3) GCNAL [5] builds"}, {"title": "4.3 Experiment Setting", "content": "Multimodal SSL: We set the temperature \u03c4 in Equations 1, 2 as 0.07 for both cross-modal contrastive learning and uni-modal prototypical learning. The batch sizes are all set as 256 for all three datasets. The numbers of uni-modal prototypes are set to 500, 1000, 5000 for KineticsSound, Food101 and VGGSound, respectively, given their different dataset sizes.\nData Selection: We fix the diversity parameter \u03bbdiv and the temperature \u03c4 in Equation 9 to 1.0 and 0.07, respectively, as in [33]. The alignment parameter \u03bbalign is set to 1.0 by default. We examine the impact of this parameter in our supplementary materials.\nOther Settings: For Food101, we choose pretrained ResNet- 101 [15] as the image backbone and pretrained Bert-base model [11] as the text backbone. For KineticsSound and VGGSound, we use ResNet2P1D-18 [30] as video backbone and ResNet-18 as the audio backbone, modifying the input channel from 3 to 1. We use AdamW [21] as the optimizer. We repeat 10 runs for Food101 and KineticsSound and 5 runs for VGGSound due to its larger data size."}, {"title": "4.4 Supervised Cold-start AL", "content": "We examine the performance of different AL strategies in a cold-start setting where only 1% to 10% labeling budget is available. Table 1 shows the performance comparison using Top-1 Accuracy as the evaluation metric."}, {"title": "4.4.1 Warm-start AL Performs Poorly with Insufficient Labels", "content": "The performance of warm-start AL methods with 1% of labeling data is not shown because they require a random initial subset for model training, which aids in assessing data uncertainty and diversity. As shown in the table, the initially insufficient labeling budget leads to a poorly calibrated model, which fails to guide these warm-start AL methods in selecting informative samples effectively. Consequently, these methods are not as competitive and may even perform worse than random data selection in a cold-start setting with an inadequate label budget."}, {"title": "4.4.2 Diversity-base Cold-start AL Fails", "content": "KGC and K-means are two diversity-based cold-start AL methods that fail under certain conditions. KGC fails because its greedy selection prioritizes outliers that are difficult for the models to learn, as we provide the evidence in our supplementary materials. K-means fails because it selects only typical data samples, the centroids of clusters, without choosing samples with more uncertainty, thereby providing less novel information. As the table reflects, when the labeling budget increases, the performance gap between K-means and other methods widens due to its tendency to select samples based solely on representativeness, neglecting novel knowledge."}, {"title": "4.4.3 Uni-modal Prototypes and Alignment Regularization are Compatible and Improve Performance", "content": "MMCSAL-proto only utilizes uni-modal prototypes without the alignment regularization. It performs better than ActiveFT, demonstrating the effectiveness of these prototypes. As discussed in Sec 3.2 and evidenced in Sec 4.6, our introduced uni-modal prototypes help bridge the modality gap and balance densities of uni-modal representations, making the representations more organized. Therefore, it estimates the distances between multimodal pairs more precisely, leading to a performance improvement. MMCSAL-align only adds the regularization term"}, {"title": "4.5 Semi-supervised Cold-start AL", "content": "We conduct semi-supervised learning in cold-start AL where the initially selected labeled samples are used to fine-tune the pretrained models produced with MMSSL. The results are shown in Table 2. As expected, the semi-supervised learning boosts the downstream task performance by a large margin compared with supervised learning as the model sees more data. In semi-supervised learning, when labeling budget is 10%, the warm-start AL methods start to dominate the performance as they can train a well-calibrated model with sufficient data and guide the model to select more informative samples for future AL iterations. However, when available labeling budget is lower than 10%, warm-start AL methods are not competitive with cold-start AL methods or even close to random data selection. This"}, {"title": "4.6 Modality Gap", "content": "To validate that the uni-modal prototypes effectively bridge the modality gap and balance the distance calculations between uni-modal and cross-modal representations, we assess the modality gap [20] as the Euclidean distance between the centroid representations of each modality. Additionally, we measure the representation densities by calculating the expectation of the average cosine similarity within and between modalities across the entire dataset:\n$S_{mk}^{Uni} = \u03a3  \\frac{\\cos(r_{mk}^i, r_{mk}^j)}{\\left|B\\right|},$ k \u2208 \\{1,2\\},\n$S^{Cross} = \u03a3 \u03a3  \\frac{\\cos(r_{m1}^i, r_{m2}^i)}{\\left|B\\right|},$i\nThese metrics allow us to quantify the effectiveness of the uni-modal prototypes in minimizing the modality gap and enhancing representation similarity assessments. The results are shown in Table 3. Multimodal contrastive learning without uni-modal prototypes shows a modality gap between the centroids of the representations of each modality, along with a discrepancy between uni-and cross-modal similarities. With the integration of uni-modal prototypes, we restructure the modality representations, reducing both the modality gap and the disparities between average uni-modal and cross-modal similarities. Consequently, we can more accurately calculate the distances between two multimodal sample pairs, considering both uni-modal and cross-modal distances."}, {"title": "5 Conclusion", "content": "In this work, we propose a two-stage method to address the cold-start problem in multimodal AL. The introduced uni-modal prototypes bridge the modality gap created by cross-modal contrastive learning. With our MMSSL, the distance estimation among multimodal data pairs becomes more precise, and it benefits data selection. Moreover, we propose to increase the modality alignment for multimodal data pairs to provide more useful modality shared information for downstream multimodal classification tasks. We believe MMCSAL can save labeling budgets for multimodal learning and will explore more multimodal tasks in future work."}, {"title": "6 Implementation Details", "content": "In this section, we provide more details about our implementation of multimodal self-supervised learning and both supervised and semi-supervised cold-start active learning to facilitate reproduction of our reported results."}, {"title": "6.1 Multimodal Self-supervised Learning", "content": "We perform multimodal self-supervised learning with a local batch size of 64 on each GPU card and a global batch size of 256 across four GPU cards. The unimodal representations are vectors with 512 dimensions. The multimodal representations are concatenated unimodal representations. The learning schedulers are cosine sched-ulers with linear warm-up. Note that the unimodal prototypical loss is calculated only after the warm-up period. We use one linear layer as a projector to map the outputs from the feature backbones into the unified 512-dimensional representations.\nFor data augmentation and loading, we follow the settings in BMMAL [27]. For Food101, we apply image data augmentations such as random resize crop, random horizontal flip, grayscale, and color jittering during training. The cropped images with a size of 224 are used as visual inputs. We use Bert's tokenizer to tokenize the textual food recipes and use the tokens as textual inputs. For KineticsSound and VGGSound, we first uniformly sample 10 frames from each video clip. We then randomly sample 3 frames and apply random resize crop and random horizontal flip as video data augmentations. The cropped 3 video frames with a size of 168 are used as video inputs. We resample audio clips to 16 kHz and randomly select 5-second long audio clips for audio data augmentation. We use the spectrum of the audio as audio inputs. The number of Fourier transform points for each frame in the audio is set to 512, the window size is set to 512, and the hop length is set to 159.\nFor Food101, we train for 25 epochs and warm up for 5 epochs. The learning rates are set to 1 \u00d7 10-5 for the text backbone, 1 \u00d7 10-4 for the image backbone, and 1 \u00d7 10-3 for projectors. For KineticsSound and VGGSound, we train for 45 epochs and warm up for 15 epochs. The learning rates are set to 1\u00d710-3 for the video backbone, the audio backbone and the projectors. We use the same AdamW optimizer with a weight decay of 0.02 and betas of 0.9 and 0.95 across the three datasets."}, {"title": "6.2 Supervised Cold-start AL", "content": "We perform the same data augmentation as in MMSSL. For Food101, we train for 15 epochs and warm up for 5 epochs. The learning rates are set to 1 \u00d7 10-5 for the text backbone, 1 \u00d7 10-4 for the image backbone, and 1 \u00d7 10-3 for the projectors. For video-audio datasets, KineticsSound and VGGSound, we train for 45 epochs and warm up for 15 epochs. The learning rates are set to 1 \u00d7 10-3 for the video backbone, the audio backbone and the projectors. We use AdamW optimizer with a weight decay of 0.01 and betas of 0.9 and 0.999 for supervised training."}, {"title": "6.3 Semi-supervised Cold-start AL", "content": "We reduce the number of training epochs and the learning rates to preserve the knowledge learned from the pretraining stage. For Food101, we train for 10 epochs and warm up for 5 epochs. The learning rates are set to 2 \u00d7 10\u22126 for the text backbone, 2 \u00d7 10-5 for"}, {"title": "6.4 Optimize the Parametric Selection Model", "content": "Following the implementation of ActiveFT [33], we use Adam as the optimizer to optimize the parametric selection model with learning objective in Equation 9. To make optimization easier, parameters are initialized using the representations of uniformly selected samples rather than being randomly initialized. We set the learning rate to 1\u00d710-3 and use a cosine scheduler to adjust the rate. The model is optimized over 300 epochs. After optimization, we select the samples that have the smallest unimodal distances to the parameters as our labeled subset."}, {"title": "6.5 Computational Cost", "content": "We present the computational cost for stage 1 and 2 in Table 4 and Table 5 across three different multimodal datasets. In stage 1, GPU memory consumption is higher due to using FAISS [17], which requires 1.5 - 2 GB of memory to accelerate k-means with the GPU. Additionally, the prototypes do not consume a significant amount of memory. For the training process with prototypes in stage 1 and the data selection process in stage 2, our method takes on average 20%-40% more time, which we find acceptable."}, {"title": "7 Analysis of Aalign", "content": "To study the impact of the hyper-parameter align in Equation 9, which controls the contribution of cross-modal alignment in the selected subset, we vary this parameter in our supervised cold-start AL experiments. It should be noted that we always keep \u03bbdiv as 1.0"}, {"title": "8 Data Selection Preference of AL", "content": "We first explain how we derive the results of data preference in Figures 3, 4, 5. We then present results on the data selection preferences of cold-start active learning strategies across three datasets, Food101, KineticsSound and VGGSound."}, {"title": "8.1 Rank the Samples by Confidence", "content": "We perform supervised multimodal classification training with the entire labeled dataset as in [29]. We calculate the confidence score as the average prediction probability across all training epochs:\n$\\mu\u2081 = \\frac{1}{T} \u03a3 p(yixi),$\nwhere T is the number of epochs and p(yi|xi) is the prediction probability for the ground truth label yi. We then rank all data"}, {"title": "8.2 Analyze the Data Preference of AL", "content": "We visualize the preference for data selection of cold-start AL strategies by counting the numbers of confident, ambiguous and uncertain samples in selected subset. The results are presented in Figures"}]}