{"title": "HADACORE: TENSOR CORE ACCELERATED HADAMARD TRANSFORM KERNEL", "authors": ["Krish Agarwal", "Rishi Astra", "Adnan Hoque", "Mudhakar Srivatsa", "Raghu Ganti", "Less Wright", "Sijia Chen"], "abstract": "We present HadaCore, a modified Fast Walsh-Hadamard Transform (FWHT) algorithm optimized for the Tensor Cores present in modern GPU hardware. HadaCore follows the recursive structure of the original FWHT algorithm, achieving the same asymptotic runtime complexity but leveraging a hardware-aware work decomposition that benefits from Tensor Core acceleration. This reduces bottlenecks from compute and data exchange. On Nvidia A100 and H100 GPUs, HadaCore achieves speedups of 1.1\u20131.4x and 1.0\u20131.3x, with a peak gain of 3.5x and 3.6x respectively, when compared to the existing state-of-the-art implementation of the original algorithm. We also show that when using FP16 or BF16, our implementation is numerically accurate, enabling comparable accuracy on MMLU benchmarks when used in an end-to-end Llama3 inference run with quantized (FP8) attention.", "sections": [{"title": "1 Introduction", "content": "QuaRot [1] and SpinQuant [6] both propose methods to increase the numerical accuracy of INT4 and INT8 quantization in LLMs. These methods rotate model weights and activations since a rotation is statistically likely to reduce the magnitude of outliers, as it \u201cdistributes\u201d extreme values among other (less extreme) dimensions, and rotation is also an easily invertible operation using the inverse of a rotation matrix. These methods can also improve FP8 inference accuracy, such as in FlashAttention-3 [7].\nPlacing these rotation matrices into the model runtime introduces extra overhead. Naively, these rotations would be full matrix multiplications. For Llama-2 7B, if we used INT8 quantization, we might expect a theoretical 2x speedup compared to FP16 or BF16 inference, but using FP16 or BF16 matmuls for these rotations could increase the linear layer computation to 110% of the original FP16 or BF16 model (supposing INT8 is half the cost)."}, {"title": "2 Background", "content": "2.1 Hadamard Matrices\nHadamard matrices are square, orthogonal matrices with values exclusively either 1 or -1 ($\\pm \\frac{1}{\\sqrt{d}}$ for a $d$-size Hadamard matrix when normalized). Walsh-Hadamard matrices are power-of-2 size Hadamard matrices which can be constructed recursively. Because of this recursive construction, multiplying an $m \\times n$ activation matrix x with an n-sized Walsh-Hadamard matrix H can be performed in $O(mn \\log(n))$ time as opposed to the $O(mn^2)$ time if general matrix multiplication was used. Furthermore, this algorithm, called the Fast Walsh-Hadamard Transform (FWHT), does not store H in memory, reducing matmul memory bottlenecks."}, {"title": "2.2 Fast Walsh-Hadamard Transform Algorithm", "content": "The Fast Walsh-Hadamard Transform algorithm [4] uses a recursive formula to apply an n-size Walsh-Hadamard transform to a vector/matrix x given the x already transformed by an (n/2)-sized Hadamard transform. This is very related to Sylvester's construction of Hadamard matrices [8], except that the Hadamard transform is applied without materializing the corresponding matrix."}, {"title": "2.3 Parallelization and Running on the GPU", "content": "In the base case of applying a size-2 Hadamard transform, work needs to be done on every pair of adjacent elements. Then in the next iteration, to accomplish a 4 \u00d7 4 Hadamard transform, work needs to be done on every pair of elements separated by 2 (i.e. 0 and 2, 1 and 3, 4 and 6, 5 and 7, etc.). This repeats for higher Hadamard sizes by nature of the algorithm. Importantly, each iteration depends on the output from the previous iteration.\nThe elements accessed in the inner code do not overlap between iterations of the inner and middle loop. This makes it simple to parallelize the 2 inner loops, which together span all m elements of the x vector/matrix, on a GPU. If x is a matrix, we can also parallelize across n rows. However, we cannot parallelize the outermost loop due to each iteration depending on the previous.\nAdditionally, there are synchronization challenges. Suppose that, per iteration, each thread does work on a single pair of elements. That thread will not be working on the same pair in the next iteration, which means it must have access to the results from another thread's work in this iteration to use in the next iteration.\nModern GPUs group threads into warps of 32 threads and further into threadblocks (CTAs) of up to 1024 threads. If each thread processes 2 elements (with the simple parallelization previously explained), we can process up to 64 elements (iteration 5) by exchanging data in the warp and up to 2048 (iteration 11) elements by exchanging within a threadblock through shared memory. If our Hadamard transform size is greater, each thread must process more than 2 elements, or expensive cross-threadblock synchronization and global memory are required."}, {"title": "2.4 fast-hadamard-transform Library", "content": "The Dao AI lab provides an optimized CUDA fast-hadamard-transform library [3] that implements the Fast Walsh-Hadamard Transform algorithm.\nThe library performs a right-Hadamard transform, parallelizing across rows of the input matrix through the launch grid and within each row by using up to 256 threads per row. The library processes 8 elements per thread (using fewer elements per thread was probably not worth the extra data movement and synchronization). We can interpret a row processed by a single threadblock to be shaped as (n_chunk, warps_per_threadblock, threads_per_warp, 8)."}, {"title": "3 Method", "content": "Modern Nvidia GPUs have specialized hardware called Tensor Cores [2], which can compute matrix multiplications on matrix sizes such as 16 \u00d7 16 about 8x faster than CUDA cores (general purpose GPU hardware). Using these Tensor"}, {"title": "3.1 Hadamard Sizes up to 256", "content": "Naively, if we were to integrate the above into the original Fast Walsh-Hadamard Transform algorithm, we could treat a size-16 Hadamard transform as a base case as opposed to size-2 Hadamard being the base case, for which we would use a Tensor Core. Just by using this to replace the first 4 outer iterations of the regular algorithm, this new algorithm takes $C \\frac{mn}{\\log_2(n)}$ time, where C is the latency of the 2 Tensor Core mma instructions.\nAlthough a $16 \\times 16$ by $16 \\times 16$ matmul using Tensor Cores instead of four $mm \\times 2$ by $2 \\times 2$ matmuls uses 2x more floating-point operations for the same result, Tensor Cores have ~8x the FLOPS and reduce the number of iterations thus reducing threadblock/global synchronizations.\nTo extend this, we can rearrange the elements such that applying the base case again achieves the next 4 iterations. First, we can apply the 16-size Hadamard to a 1 \u00d7 256 row vector chunk by reshaping it to 16 \u00d7 16. This effectively applies the 16-size Hadamard to groups of 16 elements in the row. Then to rearrange elements, we can transpose the 16 \u00d7 16 result and apply the 16 \u00d7 16 base case again (now acting on elements 16 apart due to the transpose). Finally, if we transpose the result back and view it again as a 1 \u00d7 256 row vector, we achieve a size-256 Hadamard transform."}, {"title": "3.2 Supporting Sizes Larger than 256", "content": "So far, we would only achieve up to a 256-size Hadamard with the above strategy because every contiguous chunk of h elements must be synced to achieve an h-sized Hadamard. Suppose that for a 1 \u00d7 n row vector we are trying to do an n-size Hadamard transform, where $n = 2^k > 256$. We can first view this as a $\\frac{n}{256} \\times 256$ matrix and apply a 256-size Hadamard transform to $\\frac{n}{256}$ separate chunks with the method described above. To go beyond 256, we can transpose the current result to a 256 \u00d7 $\\frac{n}{256}$ matrix and apply an 256-sized Hadamard transform, again using Tensor Core mmas, and transpose the result back. However, we would lose parallelism if we were to have a single warp process the full 1 \u00d7 n vector, especially for larger sizes like $2^{15}$.\nThe approach we use is to process a full 1 \u00d7 n vector per threadblock, where each threadblock has $warps\\_per\\_block$ warps and each warp processes $num\\_chunks$ chunks of 256 elements. We choose $warps\\_per\\_block$ and $num\\_chunks$ such that 256 \u00b7 $warps\\_per\\_block \\cdot num\\_chunks = n$. Our strategy is the following:"}, {"title": "3.3 Non-Power of 16 Sizes", "content": "The proposed algorithm relies on the desired Hadamard size being a power of 16 rather than a general power of 2, but this issue is easily resolved. If the desired Hadamard size is $d \\neq 16^n$, we can factorize $d = 2^m 16^n$, where 0 < m < 4. The $16^n$-size Hadamard can be achieved with n iterations of applying a 16-size Hadamard with intermediate rearranging as specified above, and the $2^m$ size Hadamard can be achieved in one extra iteration by using an appropriate 16 \u00d7 16 matrix (e.g. for m = 3, have an 8 \u00d7 8 Hadamard repeated on the diagonal). In this way, the algorithm is the same as if the Hadamard size was the next power of 16, except that the last 16 \u00d7 16 matrix is a diagonal tiling of a smaller Hadamard."}, {"title": "3.4 Algorithm Overview", "content": "The original Fast-Walsh Hadamard Transform algorithm achieves a $2^n$-size Hadamard transform given an input already transformed by a $2^{n-1}$-size Hadamard. This is analogous to producing a $2^n$ size Hadamard using the Kronecker product of a $2^{n-1}$-size Hadamard with a size-2 Hadamard. Similarly, by using a 16-size Hadamard per iteration, our algorithm is analogous to using a Kronecker product of a $2^{n-4}$ size Hadamard with a size-16 Hadamard to achieve a $2^n$ size Hadamard transform, without materializing the Hadamard matrix in memory.\nOur algorithm requires $(\\frac{mn}{16}) \\cdot (16 \\cdot 16) [log_{16}(n)] = 16mn[log_{16}(n)] \\geq 16mn \\log_{16}(n) = 4mn \\log_{2}(n)$ floating-point operations. Comparatively, the regular algorithm requires $(\\frac{mn}{2}) \\cdot (2 \\cdot 2) log_{2}(n) = 2mn log_{2}(n)$ floating-point-operations. With at least 2x the floating-point operations, we might expect our algorithm to be slower, but we still expect it to be faster because:\n\u2022 By using Tensor Cores, we have 8x the FLOPS available to us.\n\u2022 By using Tensor Core mma operations, we require less explicit data shuffling, meaning less warp stalls due to synchronization.\n\u2022 This also means we perform far fewer non-Hadamard Transform operations: the fast-hadamard-transform library uses complicated indexing to achieve its warp-level data shuffling, meaning a much higher ALU load; in comparison, our warp-level shuffling is a simple hardware transpose.\n\u2022 Our algorithm is more flexible to varying threadblock sizes, which means we are able to optimize our configurations to achieve higher occupancy compared to the fast-hadamard-transform library (this is especially apparent for a 128-size Hadamard transform as shown in our results section below)."}, {"title": "4 Results", "content": "4.1 Performance Tests\nWe compare HadaCore's performance against the Dao AI lab's fast-hadamard-transform library on both an A100 and an H100. We measure our relative speeds across different Hadamard sizes, and we also vary the element count (analogous to number of input matrix rows). We chose to distinguish element count instead of row count since runtimes for different Hadamard sizes are most similar for the same amount of data rather than the batch size."}, {"title": "4.2 End-To-End Tests", "content": "Beyond basic unit tests that check the output of HadaCore against the output of an explicit Hadamard matrix multiplica-tion, we analyze MMLU [5] accuracy for Llama-3.1 8B to compare implementations that use FP8 attention with and without rotations. These tests validate the usefulness of Hadamard rotations for reducing quantization error as well as demonstrate that HadaCore does not sacrifice numerical accuracy by being faster."}, {"title": "5 Conclusion", "content": "We showcased our speedups achieved by moving the Fast Walsh-Hadamard transform algorithm into a CUDA kernel that leverages Tensor Core acceleration. HadaCore achieves speedups of around 1.1-1.4x and up to 3.5x the speed of the Dao AI CUDA kernel. Further, by running MMLU benchmarks on Llama-3.1 8B with FP8 attention, we show that rotating with HadaCore achieves similar quantization error reduction as the Dao AI CUDA kernel while providing computational acceleration. In the future, we plan to implement a version of HadaCore for OpenAI Triton [9], do more optimizations for the newer H100 Hopper architecture, and experiment with more advanced techniques such as kernel fusion to support fused Hadamard transform and quantization."}, {"title": "B In-Place Rotation", "content": "One simple optimization is modifying the input tensor in-place. If our tensor is 16M elements of FP16 (e.g. 4096\u00d74096), it will fit in ~32MB of cache. However, if we have a separate destination tensor, we need double that, ~64MB. The H100, A100, and L40S have 50MB, 40MB, and 48MB of L2 cache respectively, so for these enterprise GPUs, the source and destination tensors will evict each other's lines from cache. While this analysis should theoretically only apply for sizes larger than half the L2 cache size but less than the full L2 cache size (like 32MB), in practice it might be different depending on the eviction policy and other memory usage."}, {"title": "CBF16 Support", "content": "In addition to FP16, we add BF16 support to our kernel. 16 \u00d7 16 by 16 \u00d7 8 mma is supported for BF16 on modern Nvidia GPUs, so we do not need to modify our strategy for performing the Hadamard transform with Tensor Cores to support BF16. The only difference is that BF16 Tensor Core instructions only allow accumulating the result in FP32, whereas FP16 Tensor Cores allow accumulating back into FP16 (which is what we use). To get around this, we use the FP32 accumulation but then immediately use native FP32 to BF16 type conversion instructions to get back a result in BF16. The added conversion instructions introduce an overhead that makes our BF16 kernel slightly slower than FP16, although we still demonstrate similar speedups as before when comparing our BF16 implementation to the Dao AI Lab's BF16 kernel. Our speedups are shown in the figures below for Nvidia A100 and H100 GPUs."}]}