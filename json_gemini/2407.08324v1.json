{"title": "A Cantor-Kantorovich Metric Between Markov Decision Processes with Application to Transfer Learning", "authors": ["Adrien Banse", "Venkatraman Renganathan", "Rapha\u00ebl M. Jungers"], "abstract": "We extend the notion of Cantor-Kantorovich distance between Markov chains introduced by Banse et al. (2023) in the context of Markov Decision Processes (MDPs). The proposed metric is well-defined and can be efficiently approximated given a finite horizon. Then, we provide numerical evidences that the latter metric can lead to interesting applications in the field of reinforcement learning. In particular, we show that it could be used for forecasting the performance of transfer learning algorithms.", "sections": [{"title": "1. INTRODUCTION", "content": "Research on quantitative notion of behavioural distance between Markov processes by the reinforcement learning community mimics the study of distance between dynamical systems conducted by the control community. Both communities are interested in computing how much processes/dynamical systems differ in terms of their behaviour. Several metrics have been proposed for Markov Chains (MC), including the recent Cantor-Kantorovich metric by Banse et al. (2023) where they applied it for abstraction-based methods. Few metrics like the one in (Banse et al., 2023) are equipped with the availability of algorithms for fast computation making them deployment-ready.\nIt is typical to train a reinforcement learning algorithm in a simpler world modeled as a Markov Decision Process (MDP) and deploy it in a real world setting corresponding to a different MDP. Several Transfer Learning (TL) algorithms have been developed in this paradigm where one transfers a learned policy from one MDP to another in the hope of improving the performance of the latter. Many works have shown numerical evidences that TL algorithms have better performances when the source and target MDPs are similar to each other. This triggered the research community to undertake similar efforts as that of MC for studying the similarity between the MDPs. Interestingly Carroll and Seppi (2005) argued that no single task similarity measure is uniformly superior for TL problems. Despite their observation, it would be beneficial to design a metric between MDPs which does not have computational complexity issues so that it can be used to improve the performance of TL problems.\nThis manuscript extends the range of application of the Cantor-Kantorovich metric proposed by Banse et al. (2023) in the context of TL. Specifically, the main contributions are:\n(1) The Cantor-Kantorovich metric is formulated in the context of MDPs.\n(2) The promising potential of the proposed metric is demonstrated on a transfer learning problem where sources having smaller Cantor-Kantorovich distance with the target are shown to guarantee performance using TL techniques.\nFollowing a short summary of notations and preliminaries, we present the proposed metric between MDPs in Section 2. Subsequently, the application to problems in Transfer Learning domain is demonstrated in Section 3. Finally, the paper is summarised in Section 4 along with the discussion of potential future research directions.\nThe set of real and natural numbers are denoted by R and N respectively. For $N \\in \\mathbb{N}$, we denote by $[N] := \\{0, 1, ..., N\\}$. The cardinality of the set Cis denoted by |C|. The notation $f : X \\rightarrow Y$ denotes that f is a mapping from domain X to range Y. Given a set $S \\subseteq \\mathbb{R}^n$, the Borel set associated with it is denoted"}, {"title": "2. THE CANTOR-KANTOROVICH METRIC IN THE CONTEXT OF MDPS", "content": "In this section, we recall the notions introduced in (Banse et al., 2023) and we show how to use them in the context of MDPs.\n2.1 Preliminaries About Markov Decision Processes\nThe following formalism in inspired from (Abate, 2013).\nDefinition 1. (MDP). A Markov Decision Process is described using a tuple M = (S,U,T, R, \u03bc), where S is the set of states, U is the set of control actions, T is the conditional stochastic kernel that assigns to each state $s \\in S$ and control action $u \\in U$, a probability measure T(\u00b7|s, u) : B(S) \u2192 [0, 1]. Further, R : S \u2192 R denotes the reward function and \u03bc: B(S) \u2192 [0, 1] the initial measure on the states.\nIn this work, we consider that U is a finite set. For each $u \\in U$, we define $t_u(s,\\cdot) := T(\\cdot | s, u)$ for all $s \\in S$. The above definition of MDP leads to the following semantics for a process X (k) over the horizon $k \\in [0, N]$ with $N \\in \\mathbb{N}$. Given a control action sequence $\\textbf{u} = (u_k)_{k \\in [N-1]} \\in U^N$, and an initial state $s_0 \\in S$ sampled according to \u03bc, the semantics of the process X(k) for $k \\in [N \u2013 1]$ is defined as\n$X(k + 1) \\sim T(\\cdot|X(k), u_k)$ with $X(0) = s_0$.\nGiven a MDP M = (S,U,T, R, \u03bc), its trajectory of length N is denoted by $s^N := (s_k)_{k \\in [N-1]}$. In this work, we consider that the control actions associated to this trajectory are determined by a policy p: S \u2192 A. Given a MDP M, a policy p and a horizon N, the probability distribution induced by the MDP M at time N is defined as\n$P^N_p(s^N) = \\mu(s_0) \\prod_{i=0}^{N-2} T_{p(s_i)}(s_i, s_{i+1})$\nfor every sequence $s^N \\in S^N$. For a given policy p, the function $P^N_p$ is therefore a discrete and finite probability distribution of dimension $|S|^N$.\n2.2 A metric between dynamics of two MDPs\nWe now recall the main recent results in (Banse et al., 2023) in the context of MDPs. Consider two MDPS $M_1 = (S_1, U_1, T_1, R_1,\\mu_1)$ and $M_2 = (S_2, U_2, T_2, R_2, \\mu_2)$. We assume that both MDPs are homogeneous (see Definition 3.2 in (Song et al., 2016)), meaning that there exist one-to-one correspondence between their state-spaces (S1, S2), and also between their control spaces (U1,U2). However, for the ease of exposition, we will proceed ahead with a simpler setting where $S_1 = S_2 = S$ and $U_1 = U_2 = U.\nWe are interested in the asymptotic difference between the dynamics of both MDPs under two policies p and q. Given a horizon length N, consider the metric space $(S^N, C)$, equipped with the Cantor metric C. Given two trajectory sequences a, b \u2208 SN, the Cantor metric between a and b is defined as\n$C(a, b) := 2^{-\\inf\\{k: a_k \\neq b_k\\}}$.\nGiven a horizon N and two policies p and q, let $P^N_p$ and $Q^N_q$ be the two probability distributions respectively induced by MDPs M\u2081 and M2 at time N according to (3). The next theorem is an extension of (Banse et al., 2023, Theorem 1), and shows that $K_C (P^N_p, Q^N_q)$ can be computed iteratively.\nTheorem 1. Given a horizon N > 1 and two policies p and q, it holds that\n$K_C (P^{N+1}_p, Q^{N+1}_q) = K_C (P^{N}_p, Q^{N}_q) + 2^{-(N+1)} \\max (\\sum_{s^N \\in S^N} r_{p,q}(s^N, s) - \\sum_{s \\in S} r_{p,q}(s^N))$, with $r_{p,q}(s) = \\min \\{P(s), Q(s)\\}$.\nGiven two policies p and q, we define the metric the dynamics of two MDPs as\n$d(M_1, M_2) := \\lim_{N \\rightarrow \\infty} K_C (P^{N}_p, Q^{N}_q)$.\nIt can be shown that this metric satisfies\n$0 \\le d(M_1, M_2) \u2013 K_C (P^{N}_p, Q^{N}_q) \\le 2^{-N}$,\nwhich allows the user to approximate this metric with a finite horizon N (see (Banse et al., 2023, Theorem 2) for a similar result in the context of Markov Chains). Moreover, this metric can be efficiently approximated using a very similar algorithm as (Banse et al., 2023, Algorithm 1).\nTo give the reader some intuition, the Cantor-Kantorovich metric d captures a discounted difference between the dynamics of the two MDPs because the Cantor distance can be interpreted as a discount factor.\nThe choice of the specific policies p and q depends on the application and the specific example. In Section 3, we propose a choice of policies for forecasting transfer learning performance. The investigation of more general choices is left for further work."}, {"title": "3. APPLICATION TO TRANSFER LEARNING", "content": "In this section, we showcase the possible application of our Cantor-Kantorovich distance to transfer learning with a numerical experiment. We consider a grid-world target MDP which is illustrated in Figure 1. The size of the grid-world is 10 \u00d7 10; the possible control actions are U = {left, right, up, down}; the goal is in position (4,4), and corresponds to a reward of 10; when choosing a direction u, the probability to go in that direction is \u03b4 = 1/2, and the other probabilities are (1 \u2013 \u03b4)/3 = 1/6.\nThe transfer learning experiment goes as follows. We generate 100 other grid-worlds Ms,i, where the only difference with the target is the probability \u03b4, uniformly sampled between 0 and 1. For each source MDP Ms,i:\n(1) We compute an optimal policy ps,i with Q-learning and we save the optimal Q-table Qs,i. Specifically, we used an e-greedy exploration strategy with \u025b = 0.5, and we learned the optimal policy with 4000 episodes of length 100, and with a learning rate of 0.01. We approximated the rewards with 10000 samples, and the discount factor is y = 0.95.\n(2) We compute the Cantor-Kantorovich distance with the target, that is d(MT, Ms,i) using N = 8. We fix the policies pand qas\n$p = q = p_{S,i}$\n(3) We solve the target with Q-learning by initializing Q-values with Qs,i, and we compute the jump-start reward (which describes the increase in the initial performance achievable in the target using the transferred knowledge, before any further learning), that is the difference of reward at the start of the learning process with and without transfer learning.\nThe results of this experiment can be found in Figure 2 and they can be reproduced with the code provided in https://github.com/adrienbanse/MTNSExperiments.\nFirst we can observe that, in the red case (i.e. when the source has a greater chance to take the right direction than the target), the TL technique used here always improves performance as the jumpstart reward is always larger than 1. This is due to the fact that the policy found by the source is always optimal for the target as well. In the second case though, the green dots show that there is a strong correlation between the Cantor-Kantorovich distance and the performance of the transfer.\nThis experiment therefore provides a numerical evidence that sources with a non-optimal policy but with a small Cantor-Kantorovich distance with the target guarantee performance using transfer learning. Although the example discussed here is simple, it is rich enough to demonstrate the effectiveness of our proposed metric. We leave the option of exploring a more involved numerical example with different reward setting, studying other TL algorithms and other nuances to the future study."}, {"title": "4. CONCLUSION & FUTURE OUTLOOK", "content": "In this work, a novel Cantor-Kantorovich metric with reasonable computational complexity was introduced in the context of MDPs. Its applicability to problems in the TL domain was also demonstrated using a simple numerical simulation.\nThere are several promising and potential research directions for the future. For instance, one could aim for improving the upper bound for accuracy of the proposed Cantor-Kantorovich metric with a finite horizon N. Similarly, one could investigate a distance of the form\n$d'(M_1, M_2) = \\alpha d(M_1, M_2) + \\beta d_r(M_1, M_2)$,\nwhere d is a distance between the rewards of M\u2081 and M2. The choice of dr, a and \u03b2 would therefore depend on the application context. For example, one could investigate the distances described in (Gleave et al., 2020). In the same fashion as Carroll and Seppi (2005), one could apply such distances to the same grid-world example as above, but where the goal is moving. Finally, it would be interesting to investigate other performance measures than the jumpstart reward to evaluate the performance of the proposed metric in the transfer learning setting."}]}