{"title": "Experimental Evaluation of Machine Learning Models for\nGoal-oriented Customer Service Chatbot with Pipeline\nArchitecture", "authors": ["Nurul Ain Nabilah Mohd Isa", "Siti Nuraishah Agos Jawaddi", "Azlan Ismail"], "abstract": "Integrating machine learning (ML) into customer service chatbots enhances their ability to under-\nstand and respond to user queries, ultimately improving service performance. However, they may\nappear artificial to some users and affecting customer experience. Hence, meticulous evaluation of ML\nmodels for each pipeline component is crucial for optimizing performance, though differences in func-\ntionalities can lead to unfair comparisons. In this paper, we present a tailored experimental evaluation\napproach for goal-oriented customer service chatbots with pipeline architecture, focusing on three\nkey components: Natural Language Understanding (NLU), dialogue management (DM), and Natu-\nral Language Generation (NLG). Our methodology emphasizes individual assessment to determine\noptimal ML models. Specifically, we focus on optimizing hyperparameters and evaluating candidate\nmodels for NLU (utilizing BERT and LSTM), DM (employing DQN and DDQN), and NLG (leverag-\ning GPT-2 and DialoGPT). The results show that for the NLU component, BERT excelled in intent\ndetection whereas LSTM was superior for slot filling. For the DM component, the DDQN model out-\nperformed DQN by achieving fewer turns, higher rewards, as well as greater success rates. For NLG,\nthe large language model GPT-2 surpassed DialoGPT in BLEU, METEOR, and ROUGE metrics.\nThese findings aim to provide a benchmark for future research in developing and optimizing customer\nservice chatbots, offering valuable insights into model performance and optimal hyperparameters.", "sections": [{"title": "1 Introduction", "content": "Concerns have been raised about the qual-\nity of customer service in e-commerce, particu-\nlarly regarding long waiting times and irrelevant\nresponses in live chat interactions [1]. To overcome\nthese issues, chatbots have been used to sup-\nport customers 24 hours a day. Currently, there\nare two types of chatbot: rule-based and arti-\nficial intelligent (AI)-based [37]. The rule-based"}, {"title": "2 Background", "content": "In this section, we explain three fundamental\nconcepts that drive this research: goal-oriented\ncustomer service chatbots, chatbots with pipeline\narchitecture, and ML models for pipeline architec-\nture."}, {"title": "2.1 Goal-oriented Customer Service\nChatbots", "content": "Goal-oriented chatbots (also known as task-\noriented) are created for specific tasks and\ndesigned to engage in brief conversations within\na limited scope. In contrast, non-goal-oriented\nchatbots excel in mimicking human conversation,\nengaging users in casual chitchat across diverse\ntopics [14]. The goal-oriented chatbot is usually\ndesigned to deal with specific scenarios such as\nthe booking service for hotels or accommodations,\nproduct order placement, scheduling events and\nso on [32]. The chatbot focuses on helping the\nuser achieve a specific goal and does not have the\ncapability to answer questions related to general\nknowledge or trivia questions.\nOn the other hand, integrating chatbots into\ne-services has emerged as a promising strategy to\nenhance customer service nowadays [16]. Gener-\nally, customer service involves offering informa-\ntion and support to a service provider's clients\nto enhance users-providers connection and boost\ncompany revenue, or to give users the necessary\nassistance and information they need [25]. Mean-\nwhile, conversational commerce concurrently pro-\nvides a platform for customers and technology-\ndriven brand representatives to engage in dia-\nlogue and transactions, bridging the gap between\nhumans and computers [35]. As a result, the\npresence of customer service chatbots empowers\ncustomers to receive personalized recommenda-\ntions and interact with brands in a humanized\nenvironment. A survey identified five customer-\nrelated functions employed on a chatbot to achieve\ntwo objectives [39]. For the first objective, the\nfunctions employed are related to three aspects:\ninteraction, entertainment, and problem-solving.\nThe second objective involves functions related\nto trendiness and customization aspects. Besides\nthat, there is also a survey that compiled recent\nstudies that have employed the five customer-\nrelated functions and identified that customer\nservice chatbots can still build positive customer\nrelationships even having limited communication\nwith the customers [16].\nIn line with these findings, our study specif-\nically focuses on developing a customer ser-\nvice chatbot that emphasizes the problem-solving\naspect within the restaurant domain. By narrow-\ning the scope in this way, we aim to ensure that\nthe chatbot effectively addresses customer needs\nwhile maintaining a personalized and engaging\ninteraction."}, {"title": "2.2 Chatbot with Pipeline\nArchitecture", "content": "Goal-oriented chatbots are commonly built using\na pipeline architecture, as outlined in a sur-\nvey in [14]. This architecture comprises four\nmain components: natural language understand-\ning (NLU), dialogue state tracking (DST), dia-\nlogue policy learning (DPL), and natural lan-\nguage generation (NLG). Occasionally, the DST\nand DPL are combined into a single component\nknown as the dialogue manager (DM). Mean-\nwhile, understanding how each component works\nis crucial for designing and building effective chat-\nbots that meet their goals such as providing an\naccurate understanding of user inputs, maintain-\ning coherent conversation states, making informed\ndecisions, as well as, generating natural and rel-\nevant responses. Figure 1 illustrates the dialogue\nflow in the pipeline for a customer service chatbot"}, {"title": "2.3 ML Models for Pipeline\nArchitecture", "content": "The chatbots have evolved from rule-based to\nartificial intelligence (AI)-based by considering\nthe NLP and ML methods in the implemen-\ntation [37]. With the implementation of NLP,\nhumans, and bots are able to interact. Meanwhile,\nwith ML methods, the chatbots are now occu-\npied with additional capabilities: self-learning.\nThe self-learning helps the chatbots to train them-\nselves to recognize keywords and patterns to help\nthem provide responses to the users' queries.\nAdditionally, by utilizing the previous interaction\nwith the users, human intervention is less likely\nrequired to produce responses for future interac-\ntion. On the other hand, deep learning (DL) is a\nsubset of ML methods that is often considered to\nsolve problems involving conversational AI that\nwhich related to extracting the meaning from the\ninput and generating an output as response [43].\nBesides, DL, RL is another subset of ML\nthat has been considered recently to improve\nthe quality of interactions by optimizing dialogue\nstrategies besides enabling the chatbot to han-\ndle complex inputs better through continuous\nlearning [36]. Regardless of the RL-based meth-\nods chosen to develop the DM, it still aims to\nfind the optimal policy that can maximize the\nexpected cumulative reward, however, it struggles\nto effectively explore and learn optimal policies\nin an environment with large state and action\nspaces as it only depends on the lookup table [24]."}, {"title": "3 Related Work", "content": "Current studies that propose experimental eval-\nuation for chatbots commonly exist for three\nmajor reasons: to evaluate the performance of\nthe chatbot from a specific aspect [7, 34], the\nimpact of chatbot approaches toward user expe-\nrience [17, 22, 26, 28], and the suitability of the\nchatbot in conducting particular tasks [4, 45]. As\nfor the first reason, a study evaluated the per-\nformance of the chatbot from its architectural\naspect which involved the author implementing\nthe DM component of the chatbot with three dif-\nferent DRL algorithms and assessing the chatbot's\naccuracy in answering the user questions [34].\nMeanwhile, another study evaluates the natu-\nralness of the conversation between a personal\nchatbot and the user which involves assessing\nthe quality of the responses based on the degree\nof user satisfaction [7]. For the second reason,\nstudies have evaluated five categories of chat-\nbot approaches: conversation types, interaction\nmechanisms [28], chatbot suggestions [26], chat-\nbot modalities [17], and response conditions [22].\nHaugeland et al. [28] had implemented a chat-\nbot with two types of conversation (task-led or\ntopic-led conversation) and two interaction mech-\nanisms (button or free-text) followed by analyzing\nthe impact from a usability perspective. Gao and\nJiang [26] have utilized ParlAI [38] to build four\ntypes of chatbots with different capabilities of pro-\nviding suggestions and evaluating them based on\nthe overall quality, English fluency, and human-\nlikeness. Ciechanowski et al. [17] examined the\nphenomenon of the uncanny valley in user inter-\nactions with various chatbot modalities, such as\ntext-based or avatar, by analyzing psychophysi-\nological factors like heart rate and facial muscle\nactivity. Diederich et al. [22] implement chatbot\nwith two types of response conditions, namely,\ncontrol (have present answers) and treatment\n(have no preset answers) conditions and evaluate\nthe user satisfaction metrics. For the third rea-\nson, a study in [45] has compared the suitability\nof the SOCIO chatbot with the Creately online\ntool in terms of facilitating collaborative modeling\nactivity by assessing four usability aspects (effi-\nciency, effectiveness, satisfaction, and quality).\nMeanwhile, another study evaluated user satisfac-\ntion when using the chatbot-based workshop to\nfacilitate remote learning sessions [4]."}, {"title": "4 Dataset", "content": "The dataset chosen for this study is MultiWOZ\n2.2 [12], which has been retrieved from the Hug-\ngingface website. It is a large-scale, fully anno-\ntated corpus of natural human-human conversa-\ntions that are used for building service agents. It\nis rich in annotations such as goal, metadata, and\ndialogue act, along with user and system utter-\nances. These annotations facilitate the use of ML\ntechniques to develop a chatbot.\nThe dataset is about the user, as a tourist,\nconverses with the system on service-related top-\nics across multiple domains. Each dialogue in the\nMultiWOZ dataset consists of a goal, database,\nand dialogue turns. The goal is defined by the\nservice domain (e.g. hospital, hotel) and the\nslots. The slots are divided into informable,\nrequestable, and book slots. Informable slots rep-\nresent user constraints, requestable slots hold\nadditional information that the user wants to\nobtain, and book slots are used to reserve a place\nrecommended by the system.\nIn the MultiWOZ dialogues, there are a total\nof 13 intents including welcome, inform, request,\nand bye, 24 slots that capture specific information\nin the dialogues like address, area, price range,\nand more, as well as a collection of 1482 data\npoints encompassing various types of information\nsuch as restaurant names, the day of the week,\nand other details relevant to the domains in the\ndataset. Here, each dialogue is a dictionary with\nkeys that also include 'turns'. The 'turns' key\ncontains a list of turns, where each turn is a dictio-\nnary with keys such as 'speaker' and 'utterance',\nwhich represent the speaker of that turn (either\n'0', user, or '1', system) and the text of the utter-\nance, respectively. The uttered exchange between\nthe two speakers can be captured by extracting\nthe 'speaker' and 'utterance' values for each turn.\nThe MultiWOZ dataset has been used in var-\nious studies to fine-tune models for task-oriented\ndialogue systems. This dataset has also been\nused in benchmark tasks such as Dialogue State\nTracking and Dialogue-Context-to-Text Genera-\ntion [58]. It is available in the .json format."}, {"title": "5 Design and Development", "content": "In this section, we present the proposed experi-\nmental evaluation design and describe the devel-\nopment of the chatbot components."}, {"title": "5.1 Experimental Evaluation Design", "content": "In order to comprehensively assess the entire chat-\nbot system in terms of its functionality, we have\ndesigned a two-phase experiment. The first phase\nfocuses on identifying the optimal hyperparame-\nters, while the second phase focuses on identifying\nthe ML that contributes to the best performance\nfor the chatbot.\nFor the first phase, we conducted hyperpa-\nrameter optimization (HPO), a process aimed at\nfinding the right combination of hyperparame-\nters to enhance the performance of ML models\nand yield better results [50]. This phase com-\nprises three key steps: identifying the tool for\nHPO, defining a feasible range of configurations\nfor the hyperparameters, and determining the\noptimal hyperparameters based on the provided\nconfiguration. Conventional methods for conduct-\ning HPO include grid search [30] and random\nsearch [8], known for their ability to navigate\nhigh-dimensional spaces effectively and converge\nquickly, respectively. However, these methods\nfrequently face runtime challenges in produc-\ntion environments and may lack robust perfor-\nmance, issues that Bayesian optimization algo-\nrithms effectively tackle. Several Python libraries\nnow support automated HPO which is based on\nthe Bayesian algorithms, that includes Sequential\nModel-based Algorithm Configuration (SMAC),\nHyperOpt, Optunity, and Optuna. SMAC excels\nin selecting optimal parameters across large con-\nfiguration spaces and reduces optimization time\nby leveraging predictive models of runtime [33].\nHowever, it may struggle with extremely high-dimensional search spaces due to internal model\ncomplexity. HyperOpt, on the other hand, offers\nversatility in optimizing various types of variables\nand supports parallel implementation through\nplatforms like Apache Spark and MongoDB, mak-\ning it advantageous for large-scale problems [9].\nNevertheless, it only provides limited search algo-\nrithms compared to other options (i.e. random\nsearch, Tree-structured Parzen Estimator (\u0422\u0420\u0415),\nand adaptive TPE). In contrast, Optunity's main\nadvantage is its user-friendly design that simplifies\nthe hyperparameter optimization process while\nensuring compatibility and ease of integration\nacross different environments [18]. As Optunity\navoids dependencies on large packages to facili-\ntate use in non-Python environments, this may\nlead to a slower optimization process. Neverthe-\nless, the emergence of Optuna [3] addresses the\ndrawbacks effectively. Optuna not only delivers a\nuser-friendly interface for analyzing optimization\noutcomes but also boasts scalability, accommo-\ndating anything from individual trial optimiza-\ntions to extensive experimentation. It utilized effi-\ncient sampling methods like TPE and Covariance\nMatrix Adaptation Evolution Strategy (\u0421\u041c\u0410-ES), which can handle larger and more complex\nsearch spaces more effectively. Additionally, it can\nhandle various independent sampling and rela-\ntional sampling methods, as well as, depending\non big Python packages such as NumPy for bet-\nter optimization performance and provide more"}, {"title": "5.2 Development of Chatbot\nComponents", "content": "Three key chatbot components need to be devel-\noped to complement the steps taken for the sec-\nond phase of our experimental evaluation design.\nThe components are NLU, DM, and NLG while\nFigure 3 illustrates the interaction between the\ncomponents."}, {"title": "5.2.1 NLU Component", "content": "The first component is the NLU, involved in\ncomprehending user intents by extracting the\nsemantics information from user utterances as\nslots to be processed further for information gath-\nering, question answering, dialogue management,\nrequest fulfillment, and others [56]. This com-\nponent is a fusion of an ML-based model and\na variety of natural language processing meth-\nods, including the tokenization of user speech into\nwords and sub-words, (beginning-inside-outside)\nBIO-tagging to identify types and boundaries\nof entities in a speech, and label encoding to\nconvert intents and slot tags into a numerical\nformat. These techniques are all utilized to inter-\npret user inputs accurately. In this component,\nour attention is primarily on two models, specif-\nically, Long Short-term Memory (LSTM) [29]\nand Bidirectional Encoder Representations from\nTransformers (BERT) [21].\nLSTM is a variant of RNN that has been\ninstrumental in progressing complex language\nprocessing tasks such as language translation [53].\nThe ability of LSTMs to grasp long-term depen-\ndencies in data, bypassing the optimization hur-\ndles typically seen in simpler RNNs, is partic-\nlarly noteworthy. A notable enhancement in\nLSTM technology is the advent of Bi-directional\nLSTMs [27], which process data in both forward\nand backward directions, offering a more holistic\ncomprehension of context. The NLU is suitable\nto be implemented with the Bi-LSTM because of\nits proficiency in managing long-term dependen-\ncies and its bidirectional methodology, enabling it\nto process text bidirectionally and thereby grasp\nthe context and subtleties of user statements. On"}, {"title": "5.2.2 DM Component", "content": "The second component, which is also the chat-\nbot's core function relies on the DM component\nthat helms a decision-making system similar to\nMarkov Decision Processes (MDPs). In this setup,\na DRL agent acts as the decision-maker, choosing\nthe next conversation step based on the current\ndialogue context. The goal is to maximize the\nexpected total reward, aligning with the chatbot's\naim to fulfill the user's intent effectively, just like\nhow optimal policies work in MDPs. Meanwhile,\nthe state tracker or DST maintains an evolving\nrepresentation of the conversation's context in a\nway similar to state transitions in MDPs. It keeps\ntrack of past interactions and updates the per-\nceived context after each user input and chatbot\nresponse, mirroring how states change in an MDP\nafter each action. DQN and DDQN are chosen as\ncandidates for the chatbot's dialogue manager for\ndistinct reasons. DQN offers a compelling choice\ndue to its famous utilisation of a deep convolu-\ntional neural network to represent the Q function\nso it can learn directly from high-dimensional\nsensory inputs. This can be advantageous in pro-\ncessing complex conversational contexts. On the\nother hand, DDQN may mitigate potential overes-\ntimations of action values [2]. This overestimation\nconcern can affect performance in dialogue man-\nagement tasks, so DDQN's usage of two separate\nQ-network copies may provide a more accurate\nand stable decision-making approach during user-\nchatbot interactions.\nFor this study, each DRL agent employs a\nsequential model with two fully connected layers,\nutilising ReLU activation functions. An adjustable\nhidden size parameter sets the first layer's size\nwhile the second is half of this first layer. The\nfinal output layer uses a linear activation function.\nThe model is compiled with Mean Squared Error\n(MSE) loss and uses the Adam optimiser with a\npredefined learning rate.\nThese agents, adaptations of Max Brenner's\nGO-Bot-DRL and a simplified rendition of Miu-Lab's TC-Bot, distinguish themselves primar-\nily through their network configurations: DQN\nemploys a single network for both action selec-\ntion and Q-value estimation while DDQN utilises\na dual-network approach, separating action selec-\ntion and Q-value estimation processes."}, {"title": "5.2.3 NLG Component", "content": "The third component, the NLG is responsible for\nbringing the chatbot's interactions to life. Once\nthe DM component decides on the next action,\nit will be this component's role to transform this\ndecision into a human-like reply. This transfor-\nmation is done by generating text word by word\nwhere each word is selected based on the prob-\nabilities conditioned not only on the preceding\nwords but also on the input from the DM com-\nponent. This generative process, guided by the\nweights within its large language model that has\nbeen fine-tuned to the MultiWOZ dataset, will\nproduce coherent responses to customer queries.\nThe first candidate for the NLG compo-nent is GPT-2, an autoregressive language model\nintroduced by OpenAI in 2019 [52]. It uses\na transformer-based neural network architecture\nwith 12-48 layers, trained on 40 GB of inter-\nnet text data. GPT-2 is able to generate fluent,\ncoherent text that closely matches human writ-ing by predicting the next word in a sequence\nbased on the previous words so fluent, it has\neven raised questions about potential for mali-cious use of large, unchecked language models\nonce released. Meanwhile, the second candidate\nis DialoGPT, an open-domain pre-trained con-versational response generation model released by\nMicrosoft in 2019. It extends the GPT-2 archi-tecture to dialogue modelling by training it on"}, {"title": "6 Experiments and Results", "content": "To evaluate the effectiveness of targeted ML-based\nmodels in a goal-oriented customer service chat-\nbot, we have designed the experiments to first\nfocus on finding optimal hyperparameters for each\ncandidate model using the Optuna tool [3] before\ncomparing the performance of each optimized\nmodel in pairs across the system's three main com-\nponents: NLU, dialogue management, and NLG.\nTwo following research questions (RQs) were pro-\nposed to drive the evaluation process which the\ndetailed answer discussed in Section 7:\n\u2022 RQ1: What are the hyperparameters that sig-nificantly influence the models selected for each\ngoal-oriented customer service chatbot's com-ponent (NLU, DM, NLG) and their optimal\nvalues?\n\u2022 RQ2: Which selected models can give out the\nbest performance for each component of the\ngoal-oriented customer service chatbot?"}, {"title": "6.1 Experiment Setup", "content": "In this section, we describe the configuration\nsettings for each experiment which have been\nrenamed as Experiment 1, Experiment 2, and\nExperiment 3. Each of the experiments is asso-\nciated with the evaluation of the components for\nNLU, dialogue management, and NLG, respec-\ntively.\nFor the NLU component, we aimed to iden-\ntify the best-performing model for intent detection\nand slot filling. We evaluated two candidates: a\njoint LSTM model with an LSTM branch for\nintent detection and dual bi-LSTM branches for\ninform and request slot detection, and a bert-base-uncased BERT model. Training and testing for\nboth models were conducted on user utterances\nsourced from the MultiWOZ restaurant domain\ndataset, employing a 70:30 split ratio for train-ing and testing purposes. Additionally, the models\nare compiled using Adam optimizer with speci-fied learning rates and clipnorm = 1.0. In the\nhyperparameter optimization setup for the NLU\ncandidate models, the parameters optimized for"}, {"title": "6.2 Result Analysis", "content": "In this section, we discuss the result analysis\naccording to Experiments 1, 2, and 3 from two"}, {"title": "6.2.1 Experiment 1: Analysis on NLU\ncomponent", "content": "Figures 4(a) and 4(b) illustrate the parallel coordi-\nnate plot to visualize the correlation between mul-\ntiple hyperparameters within BERT and LSTM\nmodels, respectively.\nBased on our observation, the BERT model\nreached its best performance when the batch\nsize was small and the learning rate closer to\nthe le-5 end of the log scale. These choices fit\nthe small-sized single-domain dataset where high\nlearning rates lead to untimely suboptimal conver-gence and small batch sizes will reduce overfitting.\nMeanwhile, based on the importance score, learn-ing rate(0.93) has a more significant impact on the\nmodel performance compared to batch size(0.07).\nOn the other hand, the LSTM model can per-form at its best with the smallest batch size (i.e.\n16) and the learning rate is below 0.001. How-ever, embedding dimensions and the number of\nLSTM units seem to have no single dominating\nvalue. In terms of importance scores, the learning\nrate (0.47) is the most influential hyperparameter,\nfollowed by batch size (0.34), embedding dimen-sion (0.14), and number of LSTM units (0.05).\nThis underscores the critical role of the learning\nrate in the LSTM's learning process, significantly\nimpacting how quickly and effectively the model\nconverges to an optimal solution. Batch size also\nplays a crucial role by affecting the model's train-ing stability and speed. Although the embedding\ndimension and the number of LSTM units do\ninfluence performance, their impact is less pro-nounced compared to the learning rate and batch\nsize in this optimization scenario.\nFurther evaluation of the performance can be\nreferred to in Figure 5 and Table 4. Figure 5 com-pares the intent detection accuracy of the two\nmodels. The result shows that BERT outperforms\nthe LSTM with a difference of approximately\n0.0338. The average accuracy of the models is\ncalculated as shown in Equation 1, where \u03b1 is\nthe accuracy of intent prediction, \u03b2 is the inform\nslot detection and \u03b3 is the request slot detection.\nThe reason for BERT's superior performance is its\ntransformer-based architecture [21] that processes\nwords in context more effectively than LSTM's\nsequential approach [29] as its pre-training on\nvast text data gives it a deep understanding of\nlanguage nuances. However, BERT's higher accu-racy comes with greater computational demands,\nsomething that should be considered for resource-constrained chatbot deployments. Interestingly, in\nterms of slot filling, LSTM outperforms BERT.\nTable 4 presents a comparison of LSTM and\nBERT models for slot and slot value detection\non a small dataset of 2963 utterances. Overall,\nLSTM performs better than BERT in filling the\n'Inform' and 'Request' slots, with values for pre-cision, recall, and F1-score ranging from 0.9 to\n1.0. Higher precision values indicate high speci-ficity, while higher recall indicates high sensitivity.\nGreater F1-scores reflect robustness, showing that\nthe model can effectively detect these slots despite\nthe limited size of the dataset.\nAverage Accuracy = $\\frac{\\alpha + \\beta + \\gamma}{3}$"}, {"title": "6.2.2 Experiment 2: Analysis on DM\ncomponent", "content": "Figures 6(a) and 6(b) display the hyperparame-ters' correlation of DQN and DDQN, which both\nwere investigated using the same hyperparame-ters. The DQN model is able to reach its best\nperformance when trained with a larger batch size\nand learning rate closer to the upper limit of le-2. Meanwhile, there are no noticeable trends for\nthe effects of hidden layer size and initial epsilon\nvalue on the model. On the other hand, the DDQN\nmodel is able to perform best when the trials with\nthe highest objective values are characterized by\nlower initial epsilon values, especially around the\nrange of 0.1 to 0.2. The remaining parameters\n(learning rate, batch size, and hidden node size) do\nnot show a clear trend or consistent pattern when\nit comes to performance.\nAdditionally, in terms of importance score, the\nhyperparameter that has the highest significant\nimpact on the performance of the DQN model is\nthe learning rate(0.71) followed by batch size(0.18)\nwhile the initial epsilon value(0.06) and hidden\nnode size(0.05) are less likely to affect the model's\nperformance. However, for the DDQN model,\ninitial epsilon value(0.95) influenced the model\nperformance better than the other hyperparam-eters: learning rate(0.02), hidden node size(0.02),\nbatch size(0.01)."}, {"title": "6.2.3 Experiment 3: Analysis on NLG\ncomponent", "content": "Figures 9(a) and 9(b) present the optimized\nhyperparameters for the models selected for the\nNLG component, namely, GPT-2 and DialoGPT.\nThrough the observation, GPT-2 can reach its\nbest performance with the largest batch size of\n16 and learning rates closer to the higher end\nof the tested range, around 1e-4. On the other\nhand, for DialoGPT, a variety of configurations\nlead to higher objective values, with no single\nclear trend dominating the results. This means\nthat the DialoGPT model is somewhat flexible in\nits hyperparameter settings, allowing for a range\nof values to yield satisfactory results when it\ncomes to text generation. Additionally, the learn-ing rate significantly affects the performance of"}, {"title": "7 Discussion", "content": "In this section, we discuss the findings of the eval-uation process based on the proposed RQs. We\naim to provide insights that could help researchers\nselect potential models to be implemented in their\ngoal-oriented customer service chatbots and select\nthe values of hyperparameters for benchmarking\npurposes.\nRecalling the models we selected\nin\nSection 5.1, RQ1 focuses on identifying the\nhyperparameters that most influenced the perfor-mance of the models for each chatbot component\n(NLU, DM, and NLG). For the NLU component,\nwe chose LSTM and BERT models, with the\nlearning rate being the most influential hyperpa-rameter for both. Additionally, LSTM is sensitive\nto changes in batch size. The optimal learning\nrate values for LSTM and BERT are 6.1 \u00d7 10-4\nand 3.5 \u00d7 10-5, respectively, while the optimal\nbatch size for both models is 16. Other optimal\nvalues for LSTM include 256 for the embedding\ndimension and 64 for the LSTM units. For the\nDM component, we selected DQN and DDQN to\ncontrol the conversation flow. DQN's performance\nis significantly affected by changes in the learning\nrate, whereas DDQN is influenced by the initial\nepsilon value. The optimal hyperparameters for\nDQN are a learning rate of 1.365 \u00d7 10\u22123, a batch\nsize of 256, a hidden size of 60, and an initial\nepsilon of 1.0577 \u00d7 10-\u00b9. For DDQN, the optimal\nvalues are a learning rate of 5.1 \u00d7 10\u22124, a batch\nsize of 64, a hidden size of 100, and an initial\nepsilon of 1.5678 \u00d7 10-1. For the NLG compo-nent, we evaluated GPT-2 and DialoGPT models.\nThe performance of both models is sensitive to\nchanges in the learning rate, with optimal values\nof 8.4027 x 10-5 for GPT-2 and 2.561 \u00d7 10-5\n-4\nfor DialoGPT. Additionally, DialoGPT's perfor-mance is significantly influenced by the per-device\ntrain batch size, with an optimal value of 8, while\nthe optimal batch size for GPT-2 is 16.\nSubsequently, after experimenting with the\nchatbot with specific configurations as explained\nin Section 6.1, we addressed RQ2 by identifying\nwhich model provided the best performance for\neach component. For the NLU component, BERT\noutperformed LSTM in intent detection accuracy,\nachieving 96.63% compared to 93.25%. This is\nattributed to BERT's superior contextual process-ing capabilities, despite its higher computational\nresource requirements. However, in slot detection,\nLSTM demonstrated better performance than\nBERT, particularly in the 'Inform' slots. Each\nmodel exhibits unique strengths: BERT is ideal for\napplications prioritizing accuracy in customer ser-vice chatbots where high computational resources\nare acceptable, whereas LSTM is more suit-able for deployments with limited computational\nresources and acceptable accuracy levels in intent\ndetection. For the DM component, DDQN outper-formed DQN, achieving higher success rates, fewer\ndialogue turns, and greater accumulated rewards.\nThis indicates DDQN's enhanced learning effi-ciency and adaptability in managing dialogues,\ndue to its mechanism of reducing Q-value over-estimation. Lastly, GPT-2 excelled in the NLG\ncomponent, surpassing DialoGPT in all evalua-tion metrics (BLEU, METEOR, ROUGE). GPT-2's general-purpose nature and training on diverse\nweb text enable it to generate more accurate,\ncontextually relevant responses, making it the\npreferable model for translating action frames into\nnatural language utterances."}, {"title": "8 Conclusion", "content": "This paper presents an experimental evaluation of\na pipeline-driven", "components": "Natural Language Under-standing (NLU)", "rate": "n3.5 \u00d7 10-5", "size": 16}, {"rate": 6.1, "batch\nsize": 16}, {"rate": "n5.1 \u00d7 10\u22124", "size": 64}, {"rate": 8.4027, "size": "n16) surpassed DialoGPT in BLEU, METEOR,\nand ROUGE metrics when trained with a small\ndataset.\nFuture research can investigate additional\nimportant factors that may affect the optimal\ndeep learning model configurations, such as the\nchoice of optimizer, the utilization of dropout\nand its rates, as well as the exploration"}]}