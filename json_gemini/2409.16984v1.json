{"title": "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "authors": ["P Aditya Sreekar", "Sahil Verma", "Suransh Chopra", "Sarik Ghazarian", "Abhishek Persad", "Narayanan Sadagopan"], "abstract": "Large Language Models (LLMs) are widely used in both industry and academia for various tasks, yet evaluating the consistency of generated text responses continues to be a challenge. Traditional metrics like ROUGE and BLEU show a weak correlation with human judgment. More sophisticated metrics using Natural Language Inference (NLI) have shown improved correlations but are complex to implement, require domain-specific training due to poor cross-domain generalization, and lack explainability. More recently, prompt-based metrics using LLMs as evaluators have emerged; while they are easier to implement, they still lack explainability and depend on task-specific prompts, which limits their generalizability. This work introduces Automated explainable Consistency Evaluation using LLMs (AXCEL), a prompt-based consistency metric which offers explanations for the consistency scores by providing detailed reasoning and pinpointing inconsistent text spans. AXCEL is also a generalizable metric which can be adopted to multiple tasks without changing the prompt. AXCEL outperforms both non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting inconsistencies across summarization by 8.7%, free text generation by 6.2%, and data-to-text conversion tasks by 29.4%. We also evaluate the influence of underlying LLMs on prompt based metric performance and recalibrate the SOTA prompt-based metrics with the latest LLMs for fair comparison. Further, we show that AXCEL demonstrates strong performance using open source LLMs.", "sections": [{"title": "1 Introduction", "content": "Recent advances in natural language processing (NLP) have led to the development of Large Language Models (LLM), which can generate text virtually indistinguishable from that created by humans. These models are adept at interpreting and executing natural language instructions (Ouyang et al., 2022; Rafailov et al., 2023), allowing them to undertake tasks such as summarization and classification directly, without additional training (Brown et al., 2020). This has catalysed their incorporation into a broad spectrum of mainstream applications, including summarization systems, interactive chatbots, and virtual assistants. However, studies (Bang et al., 2023; Raunak et al., 2021) have shown that LLM outputs can contain hallucinations, meaning the output is not consistent or irrelevant to the given context. Identifying these consistency issues is challenging because the errors typically align with the task's overarching structure and theme, making them subtle and difficult to detect. Evaluating the generated text for such issues is crucial to ensure trust and reliability, enabling successful adoption of LLMs across various applications.\nA widely accepted gold-standard approach for evaluating generated text is human evaluation. However, it is labor-intensive, time-consuming, and requires subject-matter expertise, making it expensive for large-scale applications. This underscores the need for automated metrics to efficiently evaluate consistency, defined as the degree to which the generated text aligns with its source. In the context of this paper, consistency and hallucination are inversely related; lower consistency scores correspond to higher degrees of hallucination. Thus, inconsistency and hallucination are used interchangeably. Several metrics have been proposed in the literature for measuring consistency, ranging from simple textual similarity-based metrics to Natural Language Inference (NLI) based methods. Textual similarity-based metrics, such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), and BERTScore (Zhang et al., 2019), require a reference text written by humans and exhibit weak correlation with human evaluation (Liu et al., 2023; Zha et al., 2023). On the other hand, NLI-"}, {"title": "2 Related Work", "content": "Non-Prompt Based Metrics: Non-prompt based metrics primarily consist of textual similarity and NLI based metrics. Textual similarity based metrics rely on a similarity function that computes the similarity between two texts. Methods like ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) use n-gram based lexical graph implementation of the similarity function, whereas, methods like BERTScore (Zhang et al., 2020) and Mover-Score (Zhao et al., 2019) utilize sentence or word embeddings generated by transformers to compute similarity scores. These metrics show poor correlation with human scores (Liu et al., 2023; Zha et al., 2023). On the other hand, NLI based metrics use entailment models, that compute alignment between a text and a context. These methods either use pre-trained NLI models like Summac (Laban et al., 2021) and custom-trained models like AlignScore (Zha et al., 2023). Metrics based on pre-trained models suffer from poor generalization (Mishra et al., 2021), whereas, ones using custom-trained models have good generalizability but requires a custom training data and consistency scores are not explainable.\nPrompt-Based Metrics: The advent of LLMs has led to an influx of consistency metrics that use LLMs as evaluators. These approaches range from zero-shot evaluators (Gao et al., 2023; Chen et al., 2023; Fu et al., 2023; Wang et al., 2023) to those using Chain of Thought (CoT) like G-Eval (Liu et al., 2023). These metrics output a score, but they are black-box in nature. The stochastic nature of LLMs and the lack of reasoning behind the scores make these black-box metrics difficult to interpret and use. SelfCheckGPT (Manakul et al., 2023) also includes a prompt-based variant, which performs sentence level consitency evaluation and averages sentence level scores to obtain a paragraph level consistency score."}, {"title": "3 Methodology", "content": "In this section, consistency is first defined, followed by a detailed discussion of AXCEL's methodology.\n3.1 Consistency\nConsistency of a derived text (DT) is computed with respect to a source text (ST), where DT is generated by any text generation system like LLM and ST acts as context for DT. Consistency is quantified as the ratio of information overlap between DT and ST to the total information in DT. This work aims to emulate the process of human evaluation of consistency, which entails verifying the facts present in DT against the information in ST. To achieve this, the information in a text T, denoted as I(T), is defined as the set of facts, {fi}, present in the text. Further, the overlap of a fact f with a text T is defined as V(f, T), where V is a verification function representing the degree to which f is consistent with information in T. V(\u00b7,\u00b7) values range from 1 to 5, where 1 indicates reference to f does not exist in T or T contradicts f, and 5 indicates f is completely consistent with T. By extension, the consistency of DT with respect to ST, C(DT, ST), is defined as:\n$C (DT, ST) = \\frac{\\Sigma_{f \\in I(DT)} V (f, ST)}{|I (DT)|}$ (1)\nWhere represents the set cardinality operation.\n3.2 AXCEL\nAXCEL computes consistency as defined in Eq. 1. Before delving into AXCEL's prompts, an outline of AXCEL's methodology is provided in Algorithm 1. The methodology can be divided into two steps: fact extraction and verification. In the first step, all the facts from the DT are extracted. Subsequently, consistency of these facts are verified using the ST, along with reasoning for the consistency scores. Although these are two distinct steps, AXCEL employs a single prompt to achieve both, reducing the number of required LLM calls. The prompt combines, which chain-of-thought (CoT) and few-shot prompting techniques, can be broken down into three parts: (1) Instruction Prompt, (2) Few-Shot Exemplars, and (3) Evaluation Query. Fig.1 illustrates the overall workflow. A detailed exploration of each component is provided in the following subsections.\n3.2.1 Instruction Prompt\nThe Instruction Prompt is the CoT component that introduces the task at hand to the LLM. It provides step-by-step instructions to the LLM on how consistency is computed given a pair of (DT, ST). These instructions are set as a system prompt for the LLM. Complete prompt is provided in Appendix A.6\n3.2.2 Few Shot Exemplars\nIn addition to the instruction prompt, a set of few-shot exemplars are provided to enable in-context learning and improve task understanding. These exemplars also enforce the desired formatting of the LLM response, allowing for easier parsing. The"}, {"title": "4 Experiments", "content": "In this section, experimental setup and main results are discussed. Due to space constrains additional results, error analysis of AXCEL's explanations, computational cost and detailed prompts are presented in Appendix A\n4.1 Experimental Setup\nExperimental Design: We assess AXCEL's performance in measuring consistency across three distinct tasks: summarization, free text generation, and converting data to text (Data2Text). For each task, benchmark datasets were identified, along with their corresponding state-of-the-art (SOTA) baselines and evaluation metrics, as detailed in Table 1. The selection of these evaluation metrics is based on previous research pertinent to each task. Detailed description of baseline methods and datasets can be found in Appendix A.3\nImplementation Details: In our study, results are reported across three LLMs: two proprietary models (Claude-3-Haiku, and Claude-3-Sonnet),"}, {"title": "4.2 Quantitative Results", "content": "4.2.1 Summarization\nFor this task, AXCEL is benchmarked on widely used summary evaluation datasets, SummEval (Fabbri et al., 2021), QAGS-CNNDM and QAGS-XSUM (Wang et al., 2020). The datasets contain pairs of source text and machine generated summary, the goal is to score consistency of the summary with respect to the source. Consistency of summaries is evaluated using AXCEL by providing the summary as DT and the source as ST. First, AXCEL is compared against other prompt-based methods in Table 2a. All the prompt-based baselines were re-run using the same underlying LLMs as AXCEL to negate the effect of LLM on the performance. In this setting, AXCEL outperforms all the baseline metrics across all LLMs, not only on average but also at the individual dataset level. Unlike G-Eval, which used different prompts for QAGS and SummEval datasets, AXCEL employs the same prompt while improving on performance, showcasing higher generalizability of AXCEL as a metric. This superior performance of AXCEL is attributed to our prompting strategy for two reasons: (1) Our instructions are more detailed and guide the LLM step-by-step in evaluating consistency at a finer granularity, i.e., fact level, whereas other prompt-based metrics instruct the LLM to evaluate at the entire summary level and provide single line description of the task, making the task more difficult and ambiguous; (2) The use of few-shot exemplars with explanations enhances the LLM's understanding of the task and ability to follow instructions via in-context learning. A detailed structural ablation analysis demonstrating the contribution of each component of AXCEL is presented in Section 4.5.\nTable 2b compares Clade-Sonnet variant of prompt-based metrics against other types of metrics. NLI and prompt-based metrics significantly outperform textual similarity-based methods, demonstrating stronger correlation with human judgment. AlignScore outperforms G-Eval and ChatGPTEval across all LLMs, except G-Eval Claude-Sonnet variant. Notably, AXCEL emerges as the SOTA metric, outperforming the best non-prompt-based metric by 11.8% (66.2 vs 59.2) and the highest-performing baseline prompt-based metric by 8.7% (66.2 vs 60.9).\n4.2.2 Data2Text\nThe dataset for this task, RAGTruth Data2Text (Wu et al., 2023), comprises LLM generated overviews of JSON data. The objective is to detect hallucinations in these generated overviews. Unlike the other two tasks, which involve only textual data, this task contains both structured (JSON) and textual data, making it a unique application of consistency scoring. AXCEL is compared against prompt-based and fine-tuned Llama-13B metrics described in RAGTruth. The prompt-based metric, provided by RAGTruth, is tailored to this dataset, incorporating details on the types of hallucinations present in the data into the prompt itself. In contract, AXCEL uses the same prompt template and relies on LLM's ability to learn this via in-context learning from the in-domain few shot exemplars.\nIn the RAGTruth paper, the F1 score is used to assess the effectiveness of metrics in detecting hallucination; however, this metric is affected by skew within the dataset (68% of generated overviews contain hallucinations). As demonstrated in Table 3, a simple baseline that classifies all generations as hallucinated achieves a very high F1 score of 78.3, indicating the unsuitability of the F1 score for performance evaluation. To overcome this, we propose using ROC-AUC as the metric, under which this baseline scores 50, indicative of random performance. For AXCEL, JSON is provided as ST and the generated overview as DT. Based on AXCEL consistency score, the generated text is marked as hallucinated if consistency score is less than 5 else is marked as not hallucinated. Table 3 compares AXCEL against RAGTruth baselines. AXCEL consistently outperforms the RAGTruth prompt across all model capacities by over 40% on ROC-AUC. It can also be observed that the RAGTruth prompt is"}, {"title": "4.2.3 Free Text Generation", "content": "In this task, hallucinations are detected in free text generations in a zero-resource setting, i.e., without external knowledge. We use WikiBio-GPT3 dataset (Manakul et al., 2023), which includes passages generated by GPT-3. The approach leverages the framework proposed by SelfCheckGPT that computes a hallucination score in a generation (R) from an LLM (L) generated using a prompt (P) without any external knowledge source. It utilizes the principle of self-consistency, where consistency among multiple generations from the LLM are compared. M additional generations, (G1, . . ., GM), are sampled using the same L and P, and each is compared with R for consistency. Next, hallucination score, H, is computed by averaging inconsistency scores as follows:\n$H = \\frac{1}{M} \\Sigma_{i=1}^{M} IC (R,G_i)$ (2)\nWhere IC(R, Gi) is a measure of inconsistency in R with respect to Gi. In the context of AXCEL, IC(R, Gi) is computed as 5 \u2013 C(R, Gi), where 5 represents the upper bound of AXCEL's scoring scale and R is input as DT and Gi as ST.\nThe performance of AXCEL is compared against the prompt-based and NLI-based metrics proposed by SelfCheckGPT. The results in table 4 demonstrate that AXCEL enhances the performance of SelfCheckGPT across all model capacities, showing improvements of 6.2% (83.6 vs 78.7) over its prompt-based metric and 12.8% (83.6 vs 74.1) over its NLI metric. Furthermore, AXCEL reduces the"}, {"title": "4.3 Explainability", "content": "Explainability is essential for a metric to build customer trust by clarifying how the score was derived. Current SOTA metrics like G-Eval and AlignScore lack explainability, outputting only a score without any justification, making them black-box methods. This issue is particularly problematic for prompt based metrics because LLMs are prone to hallucination, and relying solely on scores output by these metrics makes it difficult to identify if there are hallucinations in the scoring. AXCEL alleviates this problem and enhances transparency in evaluation by generating scores accompanied by detailed reasoning. Explanations can be used to verify if the generated consistency scores are justifiable, facilitating in easier detection of hallucinated consistency scores."}, {"title": "4.4 Importance of LLMs in Prompt based Metrics", "content": "LLMs play a crucial role in the effectiveness of prompt-based metrics. To evaluate their impact, we plot the performance of AXCEL as a function of the underlying LLMs across all three tasks, as shown in Figure 2. It can be seen that the performance of AXCEL is a function of the underlying LLM. This pattern holds true for other prompt-based metrics employed in these tasks, with the Claude-Sonnet variant surpassing the performance of all other LLMs, suggesting that improvements in LLMs will likely enhance the performance of these prompt-based metrics in measuring consistency. Therefore, when comparing different prompt-based methods, the effect of the underlying LLM should be taken into consideration."}, {"title": "4.5 Structural Ablation", "content": "To evaluate the individual contributions of AXCEL's components, we conducted a structural ablation study, assessing each component in isolation. Table 5 presents the results of this analysis, performed on the Summeval dataset using Claude-Sonnet as the underlying LLM. When employed independently, AXCEL's CoT component demonstrates an 8.4% improvement over G-Eval's CoT (64.1 vs. 59.1), underscoring the efficacy of our CoT prompt, which provides detailed step-by-step instructions. Similarly, AXCEL's exemplars, which incorporate detailed explanations, outperform those containing only scores by 17.1% (63.3 vs. 53.7), emphasizing the significance of including explanations within the exemplars which facilitates in-context learning. Notably, both AXCEL's CoT and exemplars individually achieve comparable performance (64.1 and 63.3, respectively). The integration of these components yields the optimal performance, with AXCEL attaining an overall score of 66.4."}, {"title": "4.6 Exemplar Ablation Studies", "content": "To understand the impact of exemplars, we conduct ablation studies using Claude-Haiku as the LLM.\n4.6.1 Effect of number of exemplars\nFigure 4 illustrates the impact of number of exemplars used in AXCEL's Claude-Haiku variant on average Spearman correlation across all the summarization datasets. Findings indicate that adding exemplars enhances AXCEL's performance, although the benefits plateau after three exemplars."}, {"title": "4.6.2 In-Domain vs Out-Domain Exemplars", "content": "Exemplars play an important role in making AXCEL a generalizable metric across tasks by facilitating in-context learning, helping AXCEL understand the task better. In this ablation study, the impact of using exemplars from domain different to the test setting is examined. Specifically, experiments are conducted for Free Text Generation, QAGS-XSUM, and Data2Text using exemplars from the Summeval datasets, employing Claude-Haiku as the underlying LLM. These datasets were selected because they different from SummEval, making the exemplars markedly different from the test domain. Table 6 compares AXCEL results using in-domain and out-domain exemplars with other prompt-based metrics. Performance drop is observed when employing out-domain exemplars, which can attributed to increased domain shift between the exemplars and evaluation task. The most significant decline is in data2text, with its JSON inputs causing maximal domain shift. Despite this drop in performance, AXCEL using out-domain exemplars still outperform other prompt-based metrics."}, {"title": "5 Conclusion", "content": "In this paper, we introduce a novel reference-free, prompt-based metric, AXCEL, designed to measure the consistency of responses generated by LLMs. AXCEL combines the CoT and few-shot prompting techniques to guide LLMs through a process that includes fact extraction and verification. Furthermore, AXCEL offers explainability, providing reasoning for consistency scores, which makes trusting the scores easier as compared to the blackbox approaches. It also helps pin-point the span of hallucinated text which can be used as feedback to improve the upstream text generation system.\nAXCEL was tested on three diverse tasks and was shown to outperform or perform as well as the current SOTA approaches. Non-prompt-based metrics like AlignScore and RAGTruth Llama-2-13B rely on domain-specific training data to achieve generalization, requiring costly data annotation. For instance, AlignScore exhibits suboptimal performance on the RAGTruth Data2Text task. Conversely, prompt-based metrics such as GEval and RAGTruth-prompt employ task-specific prompts, limiting their generalizability. AXCEL bridges these approaches by maintaining consistent instruction prompts across tasks and changing only the few shot exemplars, which requires annotating approximately 10 domain-specific instances, significantly less costly than extensive training or fine-tuning. This highlights AXCEL's ability to generalize as a metric across tasks better than its counterparts.\nFurthermore, in this paper, we show that the performance of prompt-based metrics is heavily dependent on the performance of the underlying LLM. With continual improvements in LLMs, the performance of prompt-based metrics is bound to improve. Finally, we show that AXCEL demonstrates strong performance using Llama-3-8B across all three tasks, as seen in tables 2a, 3 and 4; making AXCEL extendable to open source LLMs."}, {"title": "Limitations", "content": "Given that AXCEL involves an LLM, it is prone to hallucinations. As next steps to improve AXCEL, work needs to be done towards a framework that enables us to quantify the amount of hallucination in each of its fact extraction and verification steps. Further, human annotation of explanations generated by AXCEL needs to collected to access their correctness. AXCEL is more cost-efficient than other prompt-based metrics (check Appendix A.5), however, reliance of AXCEL on LLMs makes it more computationally intensive than non-prompt based metrics like AlignScore. Another limitation is the usage of in-domain exemplars, which requires manual annotation for each task and domain separately. Our experiments in Appendix 4.6.2 show that AXCEL outperforms other prompt-based metrics using out-domain exemplars, however, performs worse than AXCEL using in-domain variant. Thus, more work has to be done in designing out-domain exemplars that improve the generalization of AXCEL across domains. Finally, the current setup of AXCEL cannot be adopted for tasks where there are two contexts, such as a question-answering setup where the source text and the question are the two contexts. AXCEL needs to be further developed to be able to handle this type of setup."}, {"title": "A Appendix", "content": "A.1 Kendall-Tau and Pearson Results\nTable 7 report the Kendall-Tau correlation and Pearson correlation numbers for the summarization task. For prompt based metric, we report the numbers using Claude-Sonnet as the LLM, similar to table 2b. We observe that AXCEL is able to outperform or perform as well as all the benchmark methods on Kendall-Tau and Pearson as well.\nA.2 Error Analysis\nTo understand the accuracy of the explanations generated by AXCEL, an error analysis was performed. The QAGS-CNNDM dataset was categorized into error buckets based on the absolute difference between AXCEL's (using Claude-Sonnet) score and the human-annotated score, as shown in Table 8a. The analysis reveals that most instances fall into the \u2264 1 bucket, indicating a high level of agreement between AXCEL's scores and the human annotations. Further investigation was conducted on the instances with a difference greater than 2 (the > 2 bucket), as detailed in Table 8b. This analysis aimed to identify the steps of AXCEL that are prone to errors. The findings suggest that most of the errors were due to incorrect human annotations, rather than issues with AXCEL's performance. Interestingly, the number of errors in the different steps of AXCEL seems to be proportional to the complexity of the task in each step. The fact extraction step, being the easiest, had the least number of errors, while the scoring step, the most complex, had the most errors.\nA.3 Experimental Setup Details\nA.3.1 Dataset Description\nSummEval (Fabbri et al., 2021): This dataset comprises human annotations from both expert judges and crowdsourced workers, evaluating the consistency and three other aspects of summaries generated by natural text summarization models trained on the CNN/DailyMail news corpus (Hermann et al., 2015). Human evaluators assessed the consistency of summaries generated for 100 documents using 16 different summarization models. To benchmark various consistency metrics, the correlation between the human annotations and the generated metric scores is computed at the document level and averaged across all documents.\nQAGS Datasets (Wang et al., 2020): QAGS-CNNDM and QAGS-XSUM datasets comprise human annotations evaluating the factual consistency of model-generated summaries for articles from the CNN/DailyMail (CNNDM) (Hermann et al., 2015) and XSUM (Narayan et al., 2018) corpora. QAGS-CNNDM includes multi-line summaries, while QAGS-XSUM contains single-line summaries, referred to as extreme summaries. Human annotators were tasked with marking each sentence in the summary as either consistent or inconsistent with the source article. QAGS-CNNDM"}]}