{"title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis", "authors": ["Hamed Babaei Giglou", "Jennifer D'Souza", "S\u00f6ren Auer"], "abstract": "In response to the growing complexity and volume of scientific literature, this paper introduces the LLMs4Synthesis framework, designed to enhance the capabilities of Large Language Models (LLMs) in generating high-quality scientific syntheses. This framework addresses the need for rapid, coherent, and contextually rich integration of scientific insights, leveraging both open-source and proprietary LLMs. It also examines the effectiveness of LLMs in evaluating the integrity and reliability of these syntheses, alleviating inadequacies in current quantitative metrics. Our study contributes to this field by developing a novel methodology for processing scientific papers, defining new synthesis types, and establishing nine detailed quality criteria for evaluating syntheses. The integration of LLMs with reinforcement learning and AI feedback is proposed to optimize synthesis quality, ensuring alignment with established criteria. The LLMs4Synthesis framework and its components are made available, promising to enhance both the generation and evaluation processes in scientific research synthesis.", "sections": [{"title": "1 Introduction", "content": "In recent years, the intersection of language processing and scientific research has witnessed major advancements, largely fueled by the capabilities of Large Language Models (LLMs) [9, 15]. These models, including open-source (e.g. BERT [14] or Mistral [22]), or proprietary technologies (e.g. GPT-4 [1]), and their derivatives, have not only redefined the boundaries of text generation and comprehension [40] but have also paved the way for innovative applications in the synthesis of scientific content [17, 34]. To enable the automated and accurate generation of scientific syntheses, which are concise integrations of key insights from multiple scientific articles, this paper introduces the comprehensive LLMs4Synthesis framework. It is designed to enhance open-source LLMs so that they can generate scientific syntheses of comparable quality to those produced by significantly larger proprietary models.\nThe motivation behind this endeavor is driven by the increasing complexity and volume of scientific literature [8, 18, 44], which poses challenges for researchers seeking timely and comprehensive knowledge syntheses. Traditional manual synthesis methods are time-consuming and struggle to keep up with the rapid dissemination of new research. The LLMs4Synthesis framework addresses these challenges by leveraging LLMs to produce concise, coherent, and contextually rich scientific summaries, helping the scientific community stay abreast of expanding knowledge frontiers. Furthermore, next-generation search engines like ORKG Ask [33], Elicit [16], and SciSpace [38] demonstrate how LLMs enhance search capabilities and user interactions. These platforms utilize advanced features such as natural language queries, semantic search, and AI-driven extractions to transform scientific literature access. ORKG Ask is an open platform, allowing for seamless integration of LLMs4Synthesis, enhancing synthesis abilities, and promoting open access and collaboration in the scientific community.\nAccurate evaluation of scientific syntheses is essential to maintain their integrity and reliability. Recent advancements suggest that LLMs can generate these syntheses [16, 33, 34, 38], yet their effectiveness across domains and capability in evaluating them is not well understood. Merely employing existing quantitative metrics, like the ROUGE family [27], commonly applied in similar text generation scenarios, raise concerns about their adequacy [3, 12, 17, 24] in capturing the full extent of the semantic intricacies associated with the quality evaluation of syntheses. Instead, our research examines the effectiveness of LLMs in assessing scientific syntheses, contributing to a significant methodological shift where language understanding tools are increasingly observed as effective evaluators [2, 7]. Moreover, related work [19, 28, 47] shows that using LLMs as evaluators facilitates intricate versatility and detail in assessments across multiple dimensions and scales. Consequently, our study aims to define a comprehensive set of fine-grained evaluation attributes to address critical synthesis issues like irrelevancy, inaccuracy, incompleteness, redundancy, disorganization, incoherence, poor readability, and verbosity, which significantly affect the quality and clarity of syntheses. Additionally, this study explores using LLM evaluator responses as AI feedback in reinforcement learning settings (RLAIF) [5, 11] to enhance the alignment of a model's unsupervised learned parameters [35] with the quality of scientific synthesis generation. This approach moves beyond traditional reliance on human evaluators and ground truth data.\nIn essence, this research aims to equip LLMs with the ability to identify and improve upon their synthesis generation quality, thereby enhancing their learning and output reliability. The study focuses on the following research questions (RQs). RQ1: What features are essential for qualitatively evaluating the informativeness and meaningfulness of a scientific synthesis? RQ2: How can LLMs be fine-tuned to adhere to specific formatting standards like word limits and paragraph structure in scientific syntheses? RQ3: How can the balance be achieved in LLMs between maintaining formatting standards and ensuring semantically informative and meaningful scientific syntheses?\nThis paper makes several contributions. First, we develop a methodology to collect and process scientific papers into a format ready for synthesis using the Open Research Knowledge Graph [4], a multidisciplinary platform that facilitates the comparison of scientific contributions [32]. Second, we introduce new synthesis types -- paper-wise, methodological, and thematic -- that focus on different aspects of the extracted insights. Utilizing Mistral-7B [22] and GPT-4 [1], we generate a large-scale dataset of these syntheses, which is publicly available https://github.com/jd-coderepos/scisynthesis. Third, we establish nine quality criteria for evaluating these syntheses, assessed by both an automated LLM evaluator (GPT-4) and a human-crowdsourced survey. These evaluations provide insights, i.e. at type, domain, LLM levels, in the research community at large, and also help inform the LLMs4Synthesis framework, to enhance LLM scientific synthesis generation task performance. Finally, we propose the LLMs4Synthesis framework, which incorporates RLAIF [5, 11] to optimize LLMs for synthesis generation, ensuring alignment with established quality standards. The framework and its source code are publicly accessible https://github.com/HamedBabaei/LLMs4Synthesis.\nThe rest of the paper is organized as follows. Section 2 details the multidisciplinary ORKG synthesis dataset. Section 3 introduces comprehensive evaluation attributes and assesses synthesis quality through both automatic and human evaluations, providing performance comparisons between proprietary and open-source LLMs. Section 4 elaborates on the LLMs4Synthesis framework, with its evaluations in section 5. Section 6 explores the broader implications of our findings, and section 7 concludes the paper, summarizing our contributions and suggesting future work."}, {"title": "2 The ORKG Scientific Synthesis Dataset", "content": "In this work, we broaden the scope of the dataset collection for scientific synthesis. The primary requirement for building a corpus of scientific syntheses is access to research problems or questions along with their associated papers. Previous efforts involved manually curating this information through a team of human annotators [34]. Here, we systematize this approach by utilizing the Open Research Knowledge Graph (ORKG), a crowdsourcing platform that provides structured research contributions and comparisons. The subsequent paragraphs detail our method for compiling a reliable collection of research problems and corresponding papers to generate a multidisciplinary scientific syntheses corpus. Our corpus surpasses previous work in both multidisciplinarity and size, containing three times as many data samples, as a direct consequence of relying on the ORKG crowdsourced data."}, {"title": "2.1 Synthesis Generation Data Preparation", "content": "The ORKG data source. The ORKG is a web-based service that structures scholarly research contributions into a knowledge graph, using a crowdsourcing approach where users add and semantically describe paper contributions (of which each paper may have one or more). A key feature, Comparisons, allows users to select and compare multiple research contributions in a tabular format [32]. These Comparisons, curated by users, include papers addressing specific research themes. For defining a corpus for scientific synthesis generation, we utilized ORKG Comparisons to extract human-annotated research problems and their associated papers, providing an ideal data source.\nData processing. The ORKG Python package (https://pypi.org/ project/orkg/) was used to collect the data. First, we found all research problems with Comparisons, which produced 1,300 Comparisons for 708 research problems. From these Comparisons, we eliminated those with fewer than five unique papers, which left 495 Comparisons. The minimum threshold of five papers per comparison is a fixed criterion in this study for generating scientific syntheses. This criterion is based on popular search systems like Elicit or ORKG Ask, which typically generate syntheses from the top five results of their respective search engines. Next, we sourced abstracts for each paper in the Comparisons using the open-access platforms Semantic Scholar, Crossref, and CORE. This resulted in 329 Comparisons with five or more papers with abstracts. This step was essential since in this work, paper titles and abstracts are the context from which the syntheses are generated. Finally, for each Comparison, all papers were grouped as all possible collections of five contributions, with each collection representing one sample in the data, for a total of 541 samples. However, it is not uncommon for a Comparison to be tagged in multiple research fields, resulting in duplicated samples that differ only in research field. In such cases, only one Comparison was randomly selected to be kept, yielding a final dataset of 348 samples with each sample as a candidate for scientific synthesis generation.\nIntermediate ORKG scientific synthesis generation dataset. The ORKG scientific synthesis generation dataset including the respective accompanying generated syntheses (details in subsection 2.2) is publicly released at https://github.com/jd-coderepos/ scisynthesis. Each sample in the dataset consists of the following: sample ID, research field, research problem, and the title, abstract, and DOI (if available) of five papers. The research field is selected from the ORKG's own taxonomic schema (https://orkg.org/fields), with users permitted to choose any level within the hierarchy, whereas research problems are entered as free-form text. Due to the flexibility allowed in user inputs, the research problems in the ORKG vary widely in structure and specificity. For example, both \"text classification\" and \"Automated construction of health knowledge graphs from medical records\u201d are included as research problems in our dataset. However, the diverse and subjective nature of these entries, stemming from the dataset's crowdsourced origins, reflects and is representative of user behavior on online search platforms such as Elicit or ORKG Ask. This diversity is a specific strength of our dataset, setting it apart from previous datasets [34] created by a single team of human annotators. Similarly, since users may assign a research field from any level in the hierarchy, they range from broad, high-level fields like \"Chemistry\" to more specified fields like \"Medicinal Chemistry and Pharmaceuticals.\" Therefore, each sample has two research field columns: the original label selected by the user, and a mapping of the selected field to the third level of the ORKG's taxonomy. We chose the third level as representative enough to generalize to specific research field assignments. Note, research fields are not used in synthesis generation but are included as data points to help organize and understand the dataset."}, {"title": "2.2 Scientific Synthesis Generation", "content": "Task description. Inspired by prior work [17], the task of scientific synthesis is defined as a specialized form of multi-document summarization. It involves combining the main insights from multiple research papers-five in this work-into a coherent paragraph that addresses a specific research problem or question.\nThe scientific synthesis generation task encompasses the following five key characteristics. 1) Use of scientific literature: This process involves synthesizing information from the scientific literature, primarily from titles and abstracts of research papers. The task requires summarizing these texts and evaluating their relevance, correctness, and completeness concerning the research problem. 2) Synthesis format: The synthesis should be concisely presented in a single paragraph, limited to 200 words. This limit aligns with the standard guidelines for scientific abstracts as recommended by APA [10], MLA [45], Harvard [43], and Chicago [21] style guides, aiming for brevity and precision in summarizing key information [31, 41]. The format requires distilling and integrating diverse scientific insights into a coherent, comprehensive summary that addresses the research problem directly. This single-paragraph approach emphasizes the need for concise and cohesive communication of complex information. 3) Synthesize vs. summarize: The objective is to synthesize-meaning to combine elements into a coherent whole-rather than merely summarizing each source individually. This involves the integration, cohesion, and coherence of information from multiple sources to produce new insights or understanding in response to the research problem. 4) Referencing source material: Each claim or piece of information in the synthesis must be traceable back to the source material (the abstracts) to ensure the synthesis's accuracy and reliability. 5) Adherence to quality characteristics: Building on approaches from linguistic and semantic quality evaluation studies of generated texts [19], the quality of the synthesis should satisfy the following nine key criteria: relevancy, correctness, completeness, informativeness, integration, cohesion, coherence, readability, and conciseness. These criteria, identified as apt for synthesis quality evaluation, collectively ensure effective communication of the synthesized information. The role and the application of these criteria in practice in this work are introduced in subsection 3.1.\nTo accommodate the diverse information scope typically found in research papers, and to tailor the synthesis to the specific interests of the user, we developed three distinct types of synthesis: 1) Paper-wise synthesis, which provides a general overview; 2) Methodological synthesis, which concentrates on the methods and their pertinent details; and 3) Thematic synthesis, which focuses on identifying and summarizing recurring themes or patterns. Each synthesis type is designed to address a specific research problem while aligning with the intended focus of information. For example, a methodological synthesis will include only details about the methods discussed in the paper abstracts. In contrast, a thematic synthesis will specifically target the repetitive overarching themes present in the research.\nAs alluded to in the Introduction, the goal of this work is to develop a systematic framework for generating scientific syntheses using Large Language Models (LLMs4Synthesis). To our knowledge, this is the first framework to showcase the integration of both generation and evaluation components, aimed at optimizing the downstream synthesis model. While implemented in the context of ORKG Ask, LLMs4Synthesis is designed to be easily adaptable to other similar platforms like Elicit. The remainder of this paper discusses our use of generative AI technology, specifically LLMs, to achieve this goal. We start by describing the LLMs used for scientific synthesis generation and their application in the next paragraph.\nSynthesis generation task instruction, models, and the final ORKG syntheses dataset. This task involved using an LLM to generate a synthesis from the titles and abstracts of five scientific papers, tailored to the synthesis type and research problem.\nPrevious studies on scientific synthesis generation [17, 34], have not extensively discussed how LLMs are prompted for this task. We designed the scientific synthesis generation prompt based on prompt engineering best practices [36] and various tested prompts. Our prompt for synthesis generation is detailed in Table 2. The prompt includes two main parts: a detailed task specification and placeholders for input papers. The first part specifies the task to the LLM, filling in the \"[research-problem]\" placeholder with the research problem from the ORKG synthesis dataset and \"[prompt-type-input-instruction]\" with instructions corresponding to the synthesis type. For example, the methodological synthesis type instruction is: \"The objective of this synthesis is to focus on the methodology. Therefore, compare and integrate the methodologies used in each paper content, emphasizing how they contribute to the research problem.\""}, {"title": "3 Synthesis Quality Evaluation", "content": "To enhance our understanding of the weaknesses of LLMs concerning the synthesis objective and toward realizing potential improvements within the LLMs4Synthesis framework, we carried out a two-part qualitative evaluation of the generated syntheses. This evaluation included: 1) an LLM-based assessment using the most powerful variant of the GPT models, specifically GPT-4-Turbo, and 2) a human survey evaluation of a subsample from the synthesis dataset. Training moderate-sized open-source models effectively we demonstrate with Mistral 7B. We aim to make synthesis generation tasks more accessible and decrease dependence on proprietary models, which, while highly effective as indicated by popular LLM leaderboards, often lack transparency in their pre-training methods, datasets, and parameters. This is a barrier to deeper research insights and wider community participation. Consequently, our work is dedicated to developing methods that equip open-source LLMs to handle essential tasks in scientific language processing.\nAs a first step, we established a comprehensive set of evaluation criteria for the generated syntheses, detailed in the next section."}, {"title": "3.1 Nine Criteria of Synthesis Quality", "content": "Prior work [17] has provided a thorough analysis of the limitations associated with existing quantitative evaluation metrics, such as the ROUGE family [27], traditionally used for text summarization tasks--a broader context for the synthesis objective. The limitations highlighted the necessity to establish evaluation criteria that focus on linguistic and semantic qualitative aspects. In this vein, inspired by a prior study that assessed synthesis quality using just three criteria-comprehensive, trust, and utility--considered crucial for high-quality synthesis [34], this work aims to expand on this foundation. We seek to develop a broader set of criteria for synthesis quality evaluation, incorporating thorough linguistic and semantic assessments, informed by insights from related research on text quality evaluation in summarization [19]. After filtering out unrelated criteria from existing work, we identified nine criteria to evaluate the quality and effectiveness of the synthesized information. Each criterion is presented to the evaluator in the form of a question, as outlined below.\n1. Relevancy: Is the information in the answer relevant to the problem? 2. Correctness: Is the information in the answer a correct representation of the content of the provided abstracts? 3. Completeness: Is the answer a comprehensive encapsulation of the relevant information in the provided abstracts? 4. Informativeness: Is the answer a useful and informative reply to the problem? 5. Integration: Are the sources structurally and linguistically well-integrated, using appropriate markers of provenance/quotation and logical connectors for each reference? In addition, are the sources integrated within a single paragraph? 6. Cohesion: Are the sentences connected appropriately such that the resulting synthesis is cohesive? 7. Coherence: Are the ideas connected in a sound and logical manner? 8. Readability: Does the answer follow appropriate style and structure conventions for academic writing and use language correctly? 9. Conciseness: Is the answer short and clear, without redundant statements? Furthermore, is the synthesis approximately 200 words long?\nOur initial five criteria were crafted to enhance objectivity and precision in evaluating syntheses, expanding on the three criteria used earlier in synthesis evaluation [34]. Specifically, our '3. completeness' corresponds to their 'comprehensive, our '2. correctness' and '5. integration' relate to their 'trust,' and our '1. relevancy' and '4. informativeness' align with their 'utility.' Thus, we expanded the three foundational criteria into five. Our remaining four criteria, three-'6. cohesion,' '7. coherence,' and '8. readability'-are rooted in established text evaluation practices for linguistic quality, which are, for instance, commonly applied in summarization scenarios. The final criterion, \u20189. conciseness,' is unique to this work. It emphasizes both the quality of the synthesized information and adherence to the specified word limit, essential for presenting a succinct representation of synthesized research insights to the reader."}, {"title": "3.2 LLM Evaluation of Synthesis Quality", "content": "Prior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis [6, 17, 19], arguing that they are becoming both the tools and the standards for language assessment. Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing. The evaluation prompt and script are available athttps://github.com/jd-coderepos/ scisynthesis/tree/main/gpt-4%20synthesis-evaluator. The prompt includes all nine evaluation criteria along with their corresponding questions. Each evaluation criteria was assessed based on a rating scale from 1 to 5 as follows: 1. Very Bad, 2. Bad, 3. Moderate, 4. Good, and 5. Very Good. Each rating level was specifically tailored with a description to aid in the assessment of each criterion. For instance, consider the rating scale description for the first criteria i.e. \"1. Relevancy: is the information in the answer relevant to the problem?\" as follows: Rating 1. Very bad: The information provided does not relate to the research problem, showing a lack of understanding or connection to the topic. Rating 2. Bad: The information occasionally relates to the research problem but lacks direct and consistent relevance. Rating 3. Moderate: The information is generally related to the research problem, with occasional lapses in direct relevance. Rating 4. Good: The information is consistently relevant to the research problem, with only minor exceptions. Rating 5. Very good: The synthesis is directly and consistently relevant to the research problem, demonstrating a deep understanding of the topic and its nuances. For the full prompts, including detailed descriptions of the rating scale for all criteria, please refer to the system prompt. Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment. The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported.\n3.2.1 Results. The results are depicted as the purple and green bars in Figure 1. These results per synthesis quality criteria are averaged for the three synthesis types, i.e. paper-wise, methodological, and thematic since the differences between their scores were observed to be minimal. Comparing the two synthesis generators, GPT-4 outperforms Mistral on all characteristics. On the GPT-4-generated syntheses, the highest-scored characteristics are integration, cohesion, coherence, and readability, all of which have average scores of 4.95 or greater, reflecting strong overall structure, logical flow, and ease of comprehension. Informativeness, correctness, and relevancy were also scored highly as indicators of the trust and utility of the answers. Furthermore, the evaluator was conservative in scoring completeness and conciseness suggesting that there might be some redundancy or verbosity in the syntheses. Mistral-7B was evaluated substantially lower than GPT-4. Notably, Mistral and GPT-4 differ significantly in size, with GPT-4 rumored to exceed a billion parameters, which likely contributes to its strong performance in text generation and synthesis tasks. Completeness, correctness, and conciseness received the lowest scores, attributed to the unexpected output format of the syntheses produced by Mistral. In these cases, syntheses were wrongly structured in the shape of original research articles comprising title, abstract, keywords, introduction, results, methodology, and conclusion rather than as a paragraph-format synthesis of multiple other works.\nIn conclusion, both models performed best on readability, integration, cohesion, and coherence, demonstrating the fitness of LLMs to generate clear, logically structured texts. However, there may be a difference in the suitability of each LLM to the varying synthesis types, with GPT-4 performing slightly better on methodological, followed closely by thematic, whereas Mistral performs best on thematic, followed by paper-wise."}, {"title": "3.3 Human Evaluation of Synthesis Quality", "content": "We recruited human evaluators to conduct a survey assessing synthesis quality.\n3.3.1 Survey setup. Prolific (https://www.prolific.com/), a paid crowdworker platform specifically for academic studies, was used to conduct the survey. To reduce effort, we subsampled the syntheses dataset to include a subset of syntheses from the domains of Chemistry, Computer Science, Earth Science, Linguistics, and Sociology. These domains were selected because from surface observations the Mistral syntheses received a wide range of scores by the LLM evaluator (cf. subsection 3.2), and represent scholarly diversity. For each domain, two data samples were selected, each including a research problem, sets of five papers, and six generated syntheses--three each from Mistral-7B and GPT-4, corresponding to the three different synthesis types. The samples were chosen to exhibit distinct performance levels: one with high average evaluation scores and the other with low scores from the LLM evaluator for the Mistral syntheses. This approach aimed to compare the scoring variability between human and LLM evaluators. Furthermore, Mistral, our open-source LLM, is targeted for performance enhancement in synthesis generation through the LLMs4Synthesis framework, which justifies our focus on its generated syntheses' evaluation scores.\nFive surveys were created for each of the five domains, with each survey including both data samples thus a total of 12 syntheses. Each survey started with an introduction screen that defined the task as well as detailed the nine evaluation criteria and the 5-rating scale. In a subsequent screen, participants were shown the research problem and paper titles and abstracts before evaluating the six LLM-generated syntheses on sequential screens. They could revisit the abstracts at any time and optionally provide free text feedback for each characteristic. Links to the original surveys underlying the domain names are: Chemistry, Computer Science, Earth Science, Linguistics, Sociology. Evaluations were obtained from three participants for each domain, involving 15 participants in total. The survey for the human evaluator and the LLM evaluator prompt were closely aligned to ensure nearly identical evaluation setups.\n3.3.2 Survey participant characteristics. Prolific filters were used to screen for eligible individuals, specifically: 1) those fluent in English; 2) those who have completed at least an undergraduate degree, and; 3) those whose subject is in the appropriate domain. Prolific only verifies participants' identity and country of residence; all other demographic details, such as language proficiency and academic specialization, are self-reported.\nOur survey participants, aged 22 to 33, included both undergraduate and graduate students. Detailed demographic information is available online. Participants were paid \u00a39 (GBP) per hour, with an estimated survey duration of 2 hours.\n3.3.3 Survey Results. The results are depicted as the red and blue bars in Figure 1. In line with the LLM evaluator results, overall, GPT-4-generated syntheses consistently outperform those from Mistral across all evaluated characteristics, although some differences are slight. The scores from both LLM and human evaluators are based on the same data subsample used for the Prolific survey. Performance across the evaluation criteria varied. In the case of readability, both generators performed comparably; however, this was Mistral's strongest characteristic while for GPT-4 the average scores for relevancy, correctness, informativeness, and conciseness exceeded that of readability. The most significant difference in model performance is in the characteristics of completeness, informativeness, and integration. Additionally, we can gain an overview of participants' perceptions of the syntheses by examining the optional feedback provided for some texts. Below is a summary of the comments left for each characteristic.\n1. Relevancy. Both LLMs are generally positively received for relevancy. GPT-4's syntheses are more detailed and directly relevant to research problems, while Mistral may generalize findings. Both models need to improve on making connections and providing deeper insights, but GPT-4 slightly outperforms Mistral. 2. Correctness. GPT-4 shows a high degree of correctness, whereas Mistral's performance is variable, with reports of \"multiple serious inaccuracies.\" While GPT-4 reliably captures abstract details, it occasionally misses specific study details or statistical values. Mistral sometimes mislabels studies and includes incorrect information. 3. Completeness. GPT-4 generally covers abstracts better and includes more specific information than Mistral, although both can miss key quantitative details. Mistral has more frequent gaps and occasionally misses entire studies, often being too brief. Both need to enhance their depth and detail, but GPT-4 maintains slightly better completeness. 4. Informativeness. GPT-4 is generally more informative, though more explicit details on methods and influencing factors would be beneficial. Mistral's syntheses tend to be general and lack depth, with both models critiqued for insufficient insight into the research problem. 5. Integration. GPT-4 effectively integrates sources into a structured synthesis, whereas Mistral tends to list sources without transitions. GPT-4 is more consistent in creating a unified narrative, though both could improve paragraph organization. 6. Cohesion. GPT-4 offers more cohesive syntheses with smoother transitions and well-connected ideas, whereas Mistral's work often reads like separate summaries. GPT-4 effectively groups studies thematically, enhancing synthesis cohesion. 7. Coherence. GPT-4 shows higher overall coherence, while Mistral's syntheses can seem disjointed. Both models could strengthen connections between ideas, but GPT-4 is closer to achieving this. 8. Readability. Both models produce clear syntheses, but Mistral's style is slightly preferred for its simplicity. GPT-4 adheres well to stylistic norms but can be overly complex. Despite structural issues, Mistral's writing quality is praised. 9. Conciseness. GPT-4 generally adheres to word limits better, while Mistral often exceeds or falls short of the 200-word target. Both are praised for being non-repetitive.\nFinally, the domain-specific survey evaluations reveal that GPT-4 performs best in Earth Science and Computer Science, while Mistral excels in Earth Science and Chemistry. Both LLMs show weaker performance in Linguistics and Sociology, suggesting a better fit for engineering and hard sciences. For more insights, visit our repository.\nThis first part of the paper has introduced the scientific synthesis task, LLM synthesis generators, the multidisciplinary ORKG dataset, and insights from both an automatic LLM evaluator and a human survey. These elements contribute to defining the LLMs4Synthesis framework, which optimizes open-source LLMs for synthesis generation. The second part evaluates this framework, particularly against Mistral-7B, focusing on improvements in output format and addressing weaknesses in the nine evaluation criteria."}, {"title": "4 The LLMs4Synthesis Framework", "content": "The proposed LLMs4Synthesis adopts a reinforcement learning with Al feedback (RLAIF) [5, 11] paradigm to fine-tune LLM synthesizers for scientific synthesis generation. As illustrated in Figure 2, the first module attempts to construct a standardized dataset. The next step of LLMs4Synthesis is supervised fine-tuning with the aim of learning the task distributions by instructing Mistral-7B as the base LLM and GPT-4 synthesis as the gold standard, attempting to shift the general LLM probability distributions toward scientific domains while maintaining the general knowledge. Finally, LLMs4Synthesis uses reinforcement learning (RL) with Proximal Policy Optimization (PPO) [37] algorithm to leverage human/AI feedback to guide the model's learning process, aiming to enhance the model's performance on synthesis generation by aligning its outputs with human preferences."}, {"title": "4.1 Supervised Finetuning (SFT)", "content": "Supervised fine-tuning (SFT) of LLMs [30] for scientific synthesis involves adapting the model to generate synthesized content from input papers using labeled training data. This process is crucial for customizing a pre-trained model, originally trained on a diverse range of texts, to handle more specific tasks like scientific synthesis generation based on provided research papers. The SFT module in the LLMs4Synthesis framework, illustrated in Figure 2, uses a standardized prompt template that incorporates abstracts and titles from five given papers as input $X_{synthesissPrompt}$, with the synthesis generated by GPT-4 serving as the ground truth $Y_{GPT4-Synthesis}$.\nWe used a Quantized Low-Rank Adapter (QLORA) [13] for SFT to adapt LLM to a synthesis generation task while minimizing computational resources and memory usage. Let's consider $M(W)$ as a pre-trained Mistral-7B LLM, where $W \\in \\mathbb{R}^{d*d}$ represents the model's parameters, where d is the dimension of the LLM hidden state. Adapter layers with low-rank parameterization [20] introduce the transformation based on $W' = AB^T$, where $A \\in \\mathbb{R}^{d*r}$ and $B\\in \\mathbb{R}^{d*r}$ are low-rank matrices with r representative of the rank, which set tor = 8. A is the transformation of the hidden state of LLM, B is the further transform the output of A. Later, A and B quantized via $A_q = quant(A)$ and $B_q = quant(B)$ to reduce the memory footprint using 4-bit quantitation at quant(.). In forward pass generation the model output h is been obtained based on input sequence $x \\in X_{SynthesissPrompt}$ by $h = M(W,x)+Dropout(a*A_q(BM(W,x)))$, Where M(W, x) is the output of M for $x \\in X_{SynthesissPrompt}$, $A_q(BM(W, x))$ is the task-specific adaptation introduced by quantized adapter layer, a is the scaling factor for balancing the contribution the model final output which a = 16, and $Dropout(.)$ denotes the dropout operation which applied to the output of the low-rank adapter with dropout rate of 0.05 to regularize the low-rank adapters, reducing overfitting by randomly dropping parts of the adapter's output during training.\nLater, the AdamW optimizer [29] was employed with a learning rate of $2 * 10^{-4}$, a gradient accumulation step of 4, and a warmup step of 0.03. This configuration was used to optimize both the LLM parameters M(W) and the low-rank adapter parameters $A_q$ and $B_q$ over 5 epochs."}, {"title": "4.2 Modeling Feedback", "content": "Incorporating desired alignment preferences through reward modeling is essential for enhancing the performance of language models. This section presents two reward functions aimed at guiding the generation of syntheses. The first approach employs basic features to align with synthesis format as a paragraph and word limit constraints as structural preferences using feedback", "25": ".", "goals": 1, "Title\", \"Abstract\", \"Conclusion\", etc.) and fabricated author names in the citations. To address this, we used nine regular expressions to identify various reference forms, helping us quantify the adherence to academic styles in LLM-generated texts. The word count limit used a standard space-based splitting method.\nLet's consider S as a synthesis, WC as a word counter function, and PS as a paper structure identifier function that returns 1 if the synthesis has the paper structure and 0 otherwise. The basic features try to adapt human preferences for obtaining an ideal structural format of synthesis S by the following reward function": "n$\n\\begin{aligned"}, "n&\\text { Rbasic }(\\mathrm{S})=\\left\\{\n\\begin{array}{ll}\n-1.5 & \\text { if } \\mathrm{WC}(\\mathrm{S})<50 \\\\\n-1 & \\text { if } \\mathrm{WC}(\\mathrm{S})>200 \\\\\n-2 & \\text { if } \\mathrm{PS}(\\mathrm{S})=1 \\\\\n-0.5 & \\text { if } |\\mathrm{WC}(\\mathrm{S})-200| \\leq 20 \\\\\n2 & \\text { otherwise }\n\\end{array}\n\\right.\n\\end{aligned}\n$\nThe overall objective of this reward function $R_{basic}(S)$ is to encourage syntheses that are of moderate length (close to 200 words) and do not adhere strictly to a rigid paper structure. It penalizes both very short and very long syntheses, as well as those that follow a specific structure while rewarding those that meet the criteria for ideal synthesis length and flexibility. This approach reflects a balance between length, structure, and flexibility, aligning with human preferences for synthesis quality.\n4.2.2 GPT-4 Features. To reward the quality of synthesis based on nine qualitative criteria, we introduce a reward function designed to reflect how closely the provided synthesis is aligned with ideal qualitative standards obtained by GPT-4. We defined the PVF function that asses how close each score is per criteria to the preferred value, which in our case is five (the optimal value that one synthesis could get). The function is given by:\n$PVF_{\\text{score}} (C, p_v) = \\frac{\\sum_{i=1"]}