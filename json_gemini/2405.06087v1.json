{"title": "When Are Combinations of Humans and AI Useful?", "authors": ["Michelle Vaccaro", "Abdullah Almaatouq", "Thomas Malone"], "abstract": "Inspired by the increasing use of AI to augment humans, researchers have studied human-AI systems involving different tasks, systems, and populations. Despite such a large body of work, we lack a broad conceptual understanding of when combinations of humans and AI are better than either alone. Here, we addressed this question by conducting a meta-analysis of over 100 recent experimental studies reporting over 300 effect sizes. First, we found that, on average, human-AI combinations performed significantly worse than the best of humans or AI alone. Second, we found performance losses in tasks that involved making decisions and significantly greater gains in tasks that involved creating content. Finally, when humans outperformed AI alone, we found performance gains in the combination, but when the AI outperformed humans alone we found losses. These findings highlight the heterogeneity of the effects of human-AI collaboration and point to promising avenues for improving human-AI systems.", "sections": [{"title": "Introduction", "content": "People increasingly work with artificial intelligence (AI) tools in fields including medicine, finance, and law, as well as in daily activities such as traveling, shopping, and communicating. These human-AI systems have tremendous potential given the complementary nature of humans and AI \u2013 the general intelligence of humans allows us to reason about diverse problems, and the computational power of AI systems allows them to accomplish specific tasks that people find difficult.\nIn fact, a large body of work suggests that integrating human creativity, intuition, and contextual understanding with Al's speed, scalability, and analytical power can lead to innovative solutions and improved decision-making in areas such as healthcare [1], customer service [2], and scientific research [3]. On the other hand, a growing number of studies reveal that human-AI systems do not necessarily achieve better results when compared to the best of humans or AI alone. Challenges such as communication barriers, trust issues, ethical concerns, and the need for effective coordination between humans and AI systems can hinder the collaborative process [4-9].\nThese seemingly contradictory results raise important questions: When do humans and AI complement each other? And by how much? To address these issues, we conducted a systematic literature review and meta-analysis in which we quantified synergy in human-AI systems and identified factors that explain its presence (or absence) in different settings.\nExtending the concepts of strong and weak synergy from human teams [10, 11], we focused on two types of synergy: (1) strong synergy, where the human-AI group performs better than both the human alone and the AI alone; and (2) weak synergy, where the human-AI group performs better than either the human or the AI alone but not both.\nWhen evaluating human-AI systems, many studies focus on a specific kind of weak synergy where the baseline is the performance of the humans alone [12-15]. This measure can serve important purposes in contexts for which full automation cannot happen for legal, ethical, safety, or other reasons. But when talking about the potential of human-AI systems, most people implicitly assume that the combined system should be better than either alone; otherwise, they would just use the best of the two [16]. Thus they are looking for strong synergy. In light of these considerations, a growing body of work emphasizes evaluating and searching for strong synergy in human-AI systems [4, 8, 17-20].\nTo evaluate this synergy in human-AI systems, we analyzed 370 unique effect sizes from 106 different experiments published between January 2020 and June 2023 that included the performance of the human-only, AI-only, and human-AI systems. On average, we found evidence of weak synergy, meaning that the average human-AI systems performed better than the human alone. But we did not find strong synergy on average, meaning that the average human-AI systems performed worse than either the human alone or the AI alone. So, in practice, if we consider only the performance dimensions the researchers studied, it would have been better to use either a human alone or an AI system alone rather than the human-Al systems studied.\nWhile this overall result may appear discouraging, we also identified specific factors that did or did"}, {"title": "Results", "content": "Our initial literature search yielded 5126 papers, and, per the review process described in the Methods (see section 4.1), we identified 74 that met our inclusion criteria (see Figure S1). These papers reported the results of 106 unique experiments, and many of the experiments had multiple conditions, so we collected a total of 370 unique effect sizes measuring the impact of human-AI collaboration on task performance. Figure S2 highlights the descriptive statistics for the experiments in our analysis. We synthesize these data in a three-level meta-analytic model (see the Methods section 4.2). We also make the materials required to reproduce our results publicly accessible through an Open Science Framework (OSF) repository."}, {"title": "Overall Levels of Human-AI Synergy", "content": "In our primary analyses, we focus on strong human-AI synergy, so we compare the performance of the human-AI systems to a baseline of the human alone or the AI alone, whichever performed best. We found that the human-AI systems performed significantly worse overall than this baseline. The overall pooled effect was negative (g = \u22120.23, p = 0.0047, 95% confidence interval (CI) -0.39 to -0.07) and considered small according to conventional interpretations [21].\nOn the other hand, when we compared the performance of the human-AI systems to a different baseline the humans alone we found substantial evidence of weak human-AI synergy. The human-AI systems performed significantly better than humans alone, and this pooled effect size was positive (g = 0.64, p < 0.0001, 95% CI 0.53 to 0.74) and medium to large in size [21]. Figure 1 displays a forest plot of these effect sizes. In other words, the human-AI systems we analyzed were, on average, better than humans alone but not better than both humans alone and AI alone. For effect sizes that correspond to other types of human-AI synergy, see Figure S3."}, {"title": "Heterogeneity of Human-AI Synergy", "content": "We also found evidence for substantial heterogeneity of effect sizes in our estimations of both strong synergy (I^2 = 97.7%) and weak synergy (I^2 = 93.8%) (see table Table S4 for more details). Our"}, {"title": "Discussion", "content": "Systems that combine human intelligence and AI tools can address multiple issues of societal importance, from how we diagnose disease to how we design complex systems [22-24]. But some studies show how augmenting humans with AI can lead to better outcomes than humans or A\u0399 working alone [24-26], while others show the opposite [4, 7, 9]. These seemingly disparate results raise two important questions: How effective is human-AI collaboration, in general? And under what circumstances does this collaboration lead to performance gains versus losses? Our study analyzes over three years of recent research to provide insights into both of these questions.\nPerformance Losses from Human-AI Collaboration: Regarding the first question, we found that, on average among recent experiments, human-AI systems did not exhibit strong synergy: the human-AI groups performed worse than either the human alone or the AI alone. This result complements the qualitative literature reviews on human-AI collaboration [27-29], which highlight some of the surprising challenges that arise when integrating human and artificial intelligence. For example, people often rely too much on AI systems (\"overreliance\"), using its suggestions as strong guidelines without seeking and processing more information [6, 30, 31]. Other times, however, humans rely too little on AI (\"underreliance\"), ignoring its suggestions because of adverse attitudes toward automation [7, 31, 32].\nInterestingly, we found that, among this same set of experiments, weak synergy (better than humans) did exist in the human-AI systems: the human-AI groups performed better than the humans working alone. Thus, even though, on average, the human-AI combinations did not achieve strong synergy, they did, on average, augment human performance.\nMoreover, the discrepancy between strong and weak synergy in human-AI groups mirrors that found in the literature on synergy in human-only groups. In both cases, substantial empirical evidence points to the existence of weak synergy, but there is relatively spare evidence of strong synergy in task performance [10, 11]. This result can occur, of course, because by definition the baseline for strong synergy is more stringent than that for weak synergy. It may also occur, however, because obtaining strong human-AI synergy requires different forms of human-AI interaction, or because the recent empirical studies were not appropriately designed to elicit strong synergy.\nModerating Effect of Task Type: With the large data set we collected, we also performed additional analyses of factors that influence the effectiveness of human-AI collaboration. We found that the type of task significantly moderated strong synergy in human-AI systems with decision tasks associated with performance losses and creation tasks associated with performance gains.\nWe hypothesize that this advantage for creation tasks occurs because even when creation tasks require the use of creativity, knowledge, or insight for which humans perform better, they often also involve substantial amounts of somewhat routine generation of additional content that AI can perform as well or better than humans. For instance, generating a good artistic image usually requires some creative inspiration about what the image should look like, but it also often requires a fair amount of more routine fleshing out of the details of the image. Similarly, generating many"}, {"title": "Moderating Effect of Relative Human/AI Performance", "content": "kinds of text documents often requires knowledge or insight that humans have and computers do not, but it also often requires filling in boilerplate or routine parts of the text as well.\nWith most of the decision tasks studied in our sample, however, both the human and the AI system make a complete decision, with the humans usually making the final choice. Our results suggest that with these decision tasks, better results might have been obtained if the experimenters had designed processes in which the AI systems did only the parts of the task for which they were clearly better than humans. Only 3 of the 100+ experiments in our analysis explore such processes with a pre-determined delegation of separate sub-tasks to humans and AI. With the 4 effect sizes from these 3 experiments, we found that, on average, strong human-AI synergy (g = 0.22, N.S.) occurred (see Section S3.3 for a more detailed discussion of these experiments).\nModerating Effect of Relative Human/AI Performance: Interestingly, when the AI alone outperformed the human alone, substantial performance losses occurred in the human-AI systems (g = -0.54). When the human outperformed the AI alone, however, performance gains occurred in the human-AI systems (g = 0.46). This finding shows that the human-AI performance cannot be explained with a simple average of the human alone and AI alone. In such a case, strong human-AI synergy could never exist [33].\nMost (>95%) of the human-AI systems in our data set involved humans making the final decisions after receiving input from the AI algorithms. In these cases, one potential explanation of our result is that, when the humans are better than the algorithms overall, they are also better at deciding in which cases to trust their own opinions and in which to rely more on the algorithm's opinions.\nFor example, [34] use an experimental design in which subjects in the human-AI condition saw a problem instance, an AI prediction for that instance, and in some cases, additional descriptions of the accuracy of the AI on this type of instance. The same experimental design, with the same task interface, participant pool, and accuracy of the AI system, was used for three separate tasks: fake hotel review detection, satellite image classification, and bird image classification. For fake hotel review detection, the researchers found that the AI alone achieves an accuracy of 73%, the human alone achieves an accuracy of 55%, and the human-AI system achieves an accuracy of 69%. In this case, we hypothesize that, since the people are less accurate, in general, than the AI algorithms, they are also not good at deciding when to trust the algorithms and when to trust their own judgement, so their participation results in lower overall performance than for the AI algorithm alone.\nOn the other hand, [34] find that, for bird image classification, the AI alone achieves an accuracy of 73%, the human alone achieves an accuracy of 81%, and the human-AI system achieves an accuracy of 90%. Here, the humans alone are more accurate than the AI algorithms alone, so we hypothesize that the humans are good at deciding when to trust their own judgements versus those of the algorithms, and, thus, the overall performance improves over either humans or AI alone.\nSurprisingly Insignificant Moderators: We also investigated other moderators such as the presence of an explanation, the inclusion of the confidence of the AI output, and the type of participant evaluated. These factors have received much attention in recent years [4, 24, 26, 35]. Given our result that, on average across our 300+ effect sizes, they do not impact the effectiveness"}, {"title": "Limitations", "content": "of human-AI collaboration, we think researchers may wish to de-emphasize this line of inquiry and instead shift focus to the significant and less researched moderators we identified: the baseline performance of the human and AI alone, the type of task they perform, and the division of labor between them.\nLimitations: Importantly, we want to highlight some general limitations of our meta-analytic approach to aid with the interpretation of our results. First, our quantitative results apply to the subset of studies we collected through our systematic literature review. To evaluate human-AI synergy, we required that papers report the performance of (1) the human alone, (2) the AI alone, and (3) the human-AI system. We can, however, imagine tasks that a human and / or AI cannot perform alone but can when working with the other. Our analysis does not include such studies.\nSecond, we calculate effect sizes that correspond to different quantitative measures such as task accuracy, error, and quality. By computing Hedges' g, a unitless standardized effect size, we can describe important relations among these experiments in ways that make them comparable across different study designs with different outcome variables [36]. The studies in our data set, though, come from different samples of people some look at doctors [37-39], others at crowdworkers [4, 6, 34], and still others at students [13, 40, 41] and this variation can limit the comparability of the effect size to a degree [36]. Additionally, the measurement error can also vary across experiments. For example, some studies estimate overall accuracy based on the evaluation of as many as 500 distinct images [25] whereas others estimate it based on the evaluation of as few as 15 distinct ones [42]. As is typical for meta-analyses [43], in our three-level model, we weight effect sizes as a function of their variance across participants, so we do not account for this other source of variation in measurement.\nThird, although we did not find evidence of publication biases, it remains possible that they exist, which would impact our literature base and, by extension, our meta-analytic results. However, we expect that if there were a publication bias operating here, it would be a bias to publish studies that showed significant gains from combining humans and AI. And since our overall results showed the opposite, it seems unlikely that they are a result of publication bias.\nFourth, our results only apply to the tasks, processes, and subject pools that researchers have chosen to study, and these configurations may not be representative of the ways human-AI systems are configured in practical uses of AI outside of the laboratory. In other words, even if there is not a publication bias in the studies we analyzed, there might be a research topic selection bias at work.\nFifth, the quality of our analysis depends on the quality of the studies we synthesized. We tried to control for this issue by only including studies published in peer-reviewed publications, but the rigor of the studies may still vary in degree. For example, studies used different attention check mechanisms and performance incentive structures, which can both affect the quality of responses and thus introduce another source of noise into our data.\nFinally, we find a high level of heterogeneity among the effect sizes in our analysis. The moderators we investigate account for some of this heterogeneity, but much remains unexplained. We hypothesize that interaction effects exist between the variables we coded (e.g. explanation and type of AI), but we do not have enough studies to detect such effects. There are also certainly potential moderators"}, {"title": "A Roadmap for Future Work: Finding Human-AI Synergy", "content": "that we did not analyze. For example, researchers mostly used their own experimental platforms and stimuli, which naturally introduce sources of variation between their studies. As the human-AI collaboration literature develops, we hope future work can identify more factors that influence human-AI synergy and assess the interactions among them.\nA Roadmap for Future Work: Finding Human-AI Synergy: Even though our main result suggests that on average - combining humans and AI leads to performance losses, we do not think this means that combining humans and AI is a bad idea. On the contrary, we think it just means that future work needs to focus more specifically on finding effective processes that integrate humans and AI. Our other results suggest promising ways to proceed.\nDevelop Generative AI for Creation Tasks. In our broad sample of recent experiments, the vast majority (about 85%) of the effect sizes were for decision-making tasks in which subjects chose among a predefined set of options. But in these cases we found that the average effect size for strong synergy was significantly negative. On the other hand, only about 10% of the effect sizes researchers studied were for creation tasks those that involved open-ended responses. And in these cases we found that the average effect size for strong synergy was positive and significantly greater than that for decision tasks. This result suggests that studying human-AI synergy for creation tasks many of which can be done with generative AI could be an especially fruitful area for research.\nMuch of the recent work on generative AI with human-subjects, however, tends to focus on attitudes towards the tool [44, 45], interviews or think-alouds with participants [46\u201348], or user experience instead of task performance [49\u201351]. Furthermore, the relatively little work that does evaluate human-AI collaboration according to quantitative performance metrics tends to only report the performance of the human alone and human-AI combination (not the AI alone) [52]. This limitation makes evaluating human-AI synergy difficult as the AI alone may be able to perform the task at a higher quality and speed than the participants involved in the experiment, typically crowdworkers. We thus need studies that further explore human-AI collaboration across diverse tasks while reporting the performance of the human alone, AI alone, and human-AI system.\nDevelop Innovative Processes. Additionally, as discussed in [33], human-AI synergy requires that humans be better at some parts of a task, AI be better at other parts of the task, and the system as a whole be good at appropriately allocating subtasks to whichever partner is best for that subtask. Sometimes that is done by letting the more capable partner decide how to do allocation of subtasks, and sometimes it is done by assigning different subtasks a priori to the most capable partner (see Section S3.3 of the SI for specific examples from experiments in our dataset). In general, to effectively use AI in practice, it may be just as important to design innovative processes for how to combine humans and AI as it is to design innovative technologies [53].\nDevelop More Robust Evaluation Criteria for Human-AI Systems. Many of the experiments in our analysis evaluate performance according to a single measure of overall accuracy, but this measure corresponds to different things depending on the situation, and it omits other important criteria for human-AI systems. For example, as one approaches the upper bound of performance, such as 100% accuracy, the improvements necessary to increase performance usually become more difficult for both"}, {"title": "Methods", "content": "We conducted this meta-analysis in accord with the guidelines from Kitchenham 2004 on systematic reviews, and we follow the standards set forth by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) [55]."}, {"title": "Literature Review", "content": ""}, {"title": "Eligibility Criteria", "content": "Per our pre-registration (see here), we applied the following criteria to select studies that fit our research questions. First, the paper needed to present an original experiment that evaluates some instance in which a human and an AI system work together to perform a task. Second, it needed to report the performance of (1) the human alone, (2) the AI alone, and (3) the human-AI system according to some quantitative measure(s). As such, we excluded studies that reported the performance of the human alone but not the AI alone, and likewise we excluded studies that reported the performance of the AI alone but not the human alone. Following this stipulation, we also excluded purely meta-analyses and literature reviews, theoretical work, qualitative analyses, commentaries, opinions, and simulations. Third, we required the paper to include the experimental design, the number of participants in each condition, and the standard deviation of the outcome in each condition, or enough information to calculate it from other quantities. Finally, we required the paper to be written in English."}, {"title": "Search Strategy", "content": "Given the interdisciplinary nature of human-AI interaction studies, we performed this search in multiple databases covering conferences and journals in the computer sciences, information sciences, and social sciences, as well as other fields. Through consultation with a library specialist in these fields, we decided to target the Association for Computing Machinery Digital Library (ACM DL), Association for Information Systems eLibrary (AISeL), and the Web of Science Core Collection (WoS) for our review. To focus on current forms of artificial intelligence, we limited the search to studies published between January 1, 2020 and June 30, 2023.\nTo develop the search string, we began by distilling the facets of studies that evaluated the performance of a human-AI system. We required the following: (1) a human component, (2) a computer component, (3) a collaboration component, and (4) an experiment component. Given the multidisciplinary nature of our research question, papers published in different venues tended to refer to these components under various names [8, 12, 13, 56, 57]. For broad coverage, we compiled a list of synonyms and abbreviations for each component and then combined these terms with Boolean operations, resulting"}, {"title": "Data Collection and Coding", "content": "in the following search string:\n1.  Human: human OR expert OR participant OR humans OR experts OR participants\n2.  AI: AI OR \"artificial intelligence\" OR ML OR \"machine learning\" OR \"deep learning\"\n3.  Collaboration: collaborate OR assist OR aid OR interact OR help\n4.  Experiment: \"experiment\" OR \"experiments\" OR \"user study\" OR \"user studies\" OR\n\"crowdsourced study\" OR \"crowdsourced studies\" OR \"laboratory study\" OR \"laboratory\nstudies\"\nSearch term: (1) AND (2) AND (3) AND (4) in the abstract. See Table S1 for the exact syntax for\neach database.\nTo further ensure comprehensive coverage, we also conducted a forward and backwards search on all\nstudies we found that meet our inclusion criteria."}, {"title": "Data Collection and Coding", "content": "we\nWe conducted the search in each of these databases in July 2023. To calculate our primary outcome\nof interest the effect of combining human and artificial intelligence on task performance\nrecorded the averages and standard deviations of the task performance of the human alone, the AI\nalone, and the human and AI working with each other, as well as the number of subjects in each of\nthese conditions (see S2.2 for details).\nMany authors reported all of these values directly in the text of the paper. A notable number,\nhowever, reported them indirectly by providing 95% confidence intervals or standard errors instead of\nthe raw standard deviations. For these, we calculated the standard deviations using the appropriate\nformulas [58]. Additionally, multiple papers did not provide the exact numbers needed for such\nformulas, but the authors made the raw data of their study publicly accessible. In these cases, we\ndownload the datasets and computed the averages and standard deviations using Python or R. If\nrelevant data were only presented in the plots of a paper, we contacted the corresponding author to\nask for the numeric values. If the authors did not respond, we used a Web Plot Digitizer [59] to\nconvert plotted values into numerical values. For papers that conducted an experiment that met our\ninclusion criteria but did not report all values need to calculate the effect size, we also contacted the\ncorresponding author directly to ask for the necessary information. If the author did not respond,\nwe could not compute the effect size for the study and did not include it in our analysis.\nWe considered and coded for multiple potential moderators of human-AI performance, namely: (1)\npublication date (2) pre-registration status (3) experimental design (4) data type (5) task type\n(6) task output (7) AI type (8) AI explanation (9) AI confidence (10) participant type and (11)\nperformance metric. See Table S2 for a description of each of these moderator variables. The\nadditional information we recorded served descriptive and exploratory purposes.\nMany papers conducted multiple experiments, contained multiple treatments, or evaluated perfor-\nmance according to multiple measures. In such cases, we assigned a unique experiment identification\nnumber, treatment identification number, and measure identification number to the effect sizes from"}, {"title": "Calculation of Effect Size", "content": "the paper. Note that we defined experiments based on samples of different sets of participants."}, {"title": "Data Analysis", "content": ""}, {"title": "Calculation of Effect Size", "content": "We computed Hedges' g to measure the effect of combining human and artificial intelligence on\ntask performance [60]. For strong synergy, Hedges' g represents the standardized mean difference\nbetween the performance of the human-AI system performance and the baseline, which can be the\nhuman alone or AI alone, whichever one performs better, on average. For weak synergy (better than\nhumans alone), Hedges' g represents the standardized mean difference between the performance of\nthe human-AI system and the baseline of the human alone.\nWe chose Hedges' g as our measure of effect size because it is unitless, so allows us to compare human-\nAI performance across different metrics, and it corrects for upward bias in the raw standardized\nmean difference (Cohen's d) [60]. See SectionS2.3 in the SI for more details about this calculation."}, {"title": "Meta-Analytic Model", "content": "The experiments from the papers we analyzed varied considerably. For example, they evaluated\ndifferent tasks, they recruited participants from different backgrounds, and they employed different\nexperimental designs. Since we expected substantial between-experiment heterogeneity in the true\neffect sizes, for our analysis we used a random-effects model that accounts for variance in effect sizes\nthat comes from both sampling error and \"true\" variability across experiments [61].\nAdditionally, some of the experiments we considered generated multiple dependent effect sizes: they\ncould involve multiple treatment groups, and they could assess performance according to more than\none measure, for example. The more commonly used meta-analytic models assume independence\nof effect sizes, which makes them unsuitable for our analysis [62]. We thus adopted a three-level\nmeta-analytic model in which effect sizes are nested within experiments, so we explicitly took the\ndependencies in our data into account in the model [62, 63]. Furthermore, we used robust variance\nestimate (RVE) methods to compute consistent estimates of the variance of our effect sizes estimates\nand, relatedly, standard errors, confidence intervals, and statistical tests, which account for the\ndependency of sampling errors from overlapping samples that occurred in experiments that compared\nmultiple treatment groups to a single control group [64]. When evaluating the significance of our\nresult, we applied the Knapp and Hartung adjustment and computed a test statistic, standard error,\np-value, and confidence interval based on the t-distribution with degrees of freedom, where denotes\nthe number of effect size clusters (i.e. number of experiments) and denotes the number of coefficients\nin the model. To perform our moderator analyses, we conducted separate meta-regressions for each\nof our moderator variables.\nTo interpret the degree of heterogeneity in our effect sizes, we calculated the popular $I^2$ statistic\nfollowing [65], which quantifies the percentage of variation in effect sizes that is not from sampling"}, {"title": "Bias Tests", "content": "error. Furthermore, to distinguish the sources of this heterogeneity, we also calculated multilevel\nversions of the statistic, following [63]."}, {"title": "Bias Tests", "content": "In the context of our meta-analysis, publication bias may occur if researchers publish experiments\nthat show evidence of significant human-AI synergy more frequently than those that do not. Such\nactions would affect the data we collected and distort our findings. To evaluate this risk, we adopted\nmultiple diagnostic procedures outlined by Viechtbauer and Cheung [66]. First, we created funnel\nplots that graph the observed effect sizes on the x-axis and the corresponding standard errors on the\ny-axis [67]. In the absence of publication bias, we expect the points to fall roughly symmetrically\naround the y-axis. We enhanced these plots with colors indicating the significance level of each\neffect size to help distinguish publication bias from other causes of asymmetry [68]. A lack of effect\nsizes in regions of statistical non-significance points to a greater risk of publication bias. We visually\ninspected and performed Egger's regression test [69] as well as the rank correlation test [70] to\nevaluate the results in the funnel plot. These tests examine the correlation between the observed\neffect sizes and their associated sampling variances. A high correlation indicates asymmetry in the\nfunnel plot, which may stem from publication bias.\nFigure S7 displays the funnel plot of the included effect sizes, and we do not observe significant\nasymmetry or regions of missing data in the plot for our primary outcome, strong synergy. Egger's\nregression did not indicate evidence of publication bias in the sample (\u03b2 = \u22120.67, p = 0.4378), nor\ndid the rank correlation test (\u03c4 = 0.05, p = 0.1211). Taken as a whole, these tests suggest that our\nresults for strong synergy are robust to potential publication bias.\nImportantly, however, we do find potential evidence of publication bias in favor of studies that report\nresults in which the human-AI system outperforms the human alone (weak synergy). In this case,\nEgger's regression does point to publication bias in the sample (\u03b2 = 1.95, p = 0.0016), as does the\nrank correlation test (\u03c4 = 0.19, p < 0.0001).\nThe discrepancy between the symmetry in the funnel plot for strong synergy versus asymmetry in\nthe funnel plot for weak synergy may reflect how many researchers and journals implicitly assume\nan interest in weak synergy, comparing the human-AI system to the human alone."}, {"title": "Sensitivity Analysis", "content": "For our primary analysis, we developed a three-level meta-analytic model that accounted for variance\nin the observed effect sizes (first level), variance between effect sizes from the same experiment\n(second level), and variance between experiments (third level). We then calculated cluster-robust\nstandard errors, confidence intervals, and statistical tests for our effect size estimates in which we\ndefined clusters at the level of the experiment. This model accounts for the dependency in effect sizes\nthat result from evaluating more than one treatment against a common control group and assessing\nperformance according to more than one measure. It does, however, consider the experiments in"}, {"title": "Sensitivity Analysis", "content": "our analysis as independent from each other, even if they come from the same paper. We find this\nassumption plausible because the experiments recruited different sets of participants and entailed\ndifferent tasks or interventions.\nAs a robustness check, though, we performed a sensitivity re-analysis in which we clustered at the\npaper level instead of the experiment level. This multilevel model accounts for variance in the\nobserved effect sizes (first level), variance between effect sizes from the same paper (second level), and\nvariance between papers (third level). We still calculated cluster-robust standard errors, confidence\nintervals, and statistical tests for our effect size estimates in which we defined clusters at the level\nof the experiment because the participant samples only overlapped on the level of the experiment.\nUsing this approach, we found a comparable overall effect size for strong synergy of g = -0.25\n(p = 0.0008) and for weak synergy of g = 0.60 (p < 0.0001).\nWe also evaluated the robustness of our results to outlying and influential data points. To detect\nsuch data, we computed the residuals and Cook's distance for each effect size. We considered residual\nvalues greater or less than three standard deviations from the mean as outliers, and following [71],\nwe considered values greater than 4/N as high influence, where N is the number of data points in\nour analysis. Using this approach, we identified 11 outliers. We performed a sensitivity re-analysis\non a dataset excluding these effect sizes, which resulted in similar effect size for strong synergy\n(g = \u22120.24, p = 0.0010, 95% CI [-0.38, -0.10]).\nAdditionally, we conducted leave-one-out analyses, in which we performed a series of sensitivity\nre-analysis on the different subsets of effect sizes obtained by leaving out one effect size in our\noriginal dataset. We also conducted leave-one-out analyses at the experiment and publication levels.\nThese tests show how each effect size, experiment, and publication affect our overall estimate of\nthe effect of human-AI collaboration on task performance. Summary effect sizes for strong synergy\nranged from -0.28 to \u22120.19 with p < 0.05 in all cases, indicating the robustness of our results to\nany single effect size, experiment, or paper; likewise, summary effect sizes for weak synergy (better\nthan humans alone) ranged from 0.61 to 0.66 with p < 0.05 in all cases, indicating the robustness of\nour results to any single effect size, experiment, or paper.\nLastly, we conducted a sensitivity re-analysis on a dataset that omits the effect sizes we estimated,\neither using WebPlotDigitizer or the information provided by the authors in their paper, and again\nwe found almost identical results (g = \u22120.18, p = 0.0254, 95% CI [\u22120.34, -0.02]).\nWe performed all quantitative analysis with the R statistical programming language, and we primarily\nrelied on the package metafor [72].\nWe compiled the data used in this analysis based on the studies identified in our systematic literature\nreview. We make the data we collected available via the Open Science Framework repository\n(https://osf.io/wrq7c/?view_only=b9e1e86079c048b4bfb03bee6966e560).\nWe share the code used to conduct our analysis via the Open Science Framework repository\n(https://osf.io/wrq7c/?view_only=b9e1e86079c048b4bfb03bee6966e560)."}, {"title": "Types of Human-AI Synergy", "content": "Let H, AI"}]}