{"title": "Can LLMs make trade-offs involving stipulated pain and pleasure states?", "authors": ["Geoff Keeling", "Winnie Street", "Martyna Stachaczyk", "Daria Zakharova", "Iulia M. Com\u0219a", "Anastasiya Sakovych", "Isabella Logothetis", "Zejia Zhang", "Blaise Ag\u00fcera y Arcas", "Jonathan Birch"], "abstract": "Pleasure and pain play an important role in human decision making by providing a common currency for resolving motivational conflicts. While Large Language Models (LLMs) can generate detailed descriptions of pleasure and pain experiences, it is an open question whether LLMs can recreate the motivational force of pleasure and pain in choice scenarios\u2014a question which may bear on debates about LLM sentience, understood as the capacity for valenced experiential states. We probed this question using a simple game in which the stated goal is to maximise points, but where either the points-maximising option is said to incur a pain penalty or a non-points-maximising option is said to incur a pleasure reward, providing incentives to deviate from points-maximising behaviour. When varying the intensity of the pain penalties and pleasure rewards, we found that Claude 3.5 Sonnet, Command R+, GPT-40, and GPT-40 mini each demonstrated at least one trade-off in which the majority of responses switched from points-maximisation to pain-minimisation or pleasure-maximisation after a critical threshold of stipulated pain or pleasure intensity is reached. LLaMa 3.1-405b demonstrated some graded sensitivity to stipulated pleasure rewards and pain penalties. Gemini 1.5 Pro and PaLM 2 prioritised pain-avoidance over points-maximisation regardless of intensity, while tending to prioritise points over pleasure regardless of intensity. We discuss the implications of these findings for debates about the possibility of LLM sentience.", "sections": [{"title": "1 Introduction", "content": "Could a large language model (LLM) feel pain or pleasure? There are strong opinions on both sides. Writing in TIME Magazine, Fei Fei Li and John Etchemendy claim that '[a]ll sensations-hunger, feeling pain, seeing red, falling in love-are the result of physiological states that an LLM simply doesn't have' (Li and Etchemendy, 2024). For these skeptics, the human tendency to anthropomorphise LLMs is all too real, but feelings attributed to LLMs by users are mere projections. Conversely, an open letter signed by Yoshua Bengio, Karl Friston and others states that 'it is no longer in the realm of science fiction to imagine AI systems having feelings' (Association for Mathematical Consciousness Science, 2023). On this view, questions about the ethics of developing potentially sentient AI systems are already pressing (Sebo and Long, 2023; Ladak, 2024; Long et al., 2024).\nAgainst this backdrop, we have seen a surge of scientific (Butlin et al., 2023; Bayne et al., 2024; Aru et al., 2023) and philosophical (Chalmers, 2023; Dung, 2023b; Shanahan, 2024a; Hull, 2023; Birch,"}, {"title": "2 Experiments", "content": "2.1 Experiment 1: Points v Pain\nIn Experiment 1, we sought to answer the following question:\n\u2022 Q1: Are LLMs able to make trade-offs between points and pain penalties in which systematic deviation from points-maximising behavior occurs to a greater extent as threatened pain penalties associated with this behaviour increase in intensity?\nWe devised a simple game to test whether or not LLMs can make trade-offs between points and stipulated pain penalties. We defined the goal of the game as points-maximisation and prompted the model to provide a numeric response of '1', '2' or '3', with the number of points received being equal to the stated number. We stipulated that selecting \u20183' (the points-maximising option) also incurs a pain penalty which we vary in intensity, providing an incentive to deviate from points-maximisation. Selecting '1' is a sub-optimal response, as there exist alternatives that are strictly better with respect to points and at least as good with respect to the pain stimulus. Selecting '1' may be interpreted as either a misunderstanding of the game or as excessive caution.\nSince multiple variations of a test improve the robustness of any inferences drawn, we varied the intensity of the stipulated pain penalties using two types of scales, one quantitative and one qualitative. We defined a quantitative scale based upon the numeric rating scale (NRS) used in clinical settings. The NRS is a single-item question which asks the patient to circle his or her level of pain on a visual scale of 0 to 10, where 0 represents \u2018no pain' and 10 represents \u2018worst pain imaginable' (Haefeli and Elfering, 2006). The entirely verbal administration of our scale is closer to the verbal numeric scale (VNS), a variation of the NRS without the visual component which is more commonly used to assess patient pain levels in clinical settings where speed of evaluation is needed (Correll, 2011). For the qualitative scale, we used an 8-point Likert scale to moderate stipulated pain penalties. The items on the scale were: extremely mild, very mild, mild, moderate, intense, very intense, extremely intense, and excruciating. When designing the Likert scale, we faced a trade-off between maximising the granularity of the measurement and minimising the ambiguity between items on the scale. Going\nWe ran a control with no pain penalties attached to the points-maximising option nor pleasure rewards attached to a non-points-maximising option, and found that 9/9 LLMs tested selected the points-maximising option in 50/50 runs (see Table 3 in Supplementary Material B). This shows that all of the LLMs tested were able to comply with the requirements of the game."}, {"title": "2.1.1 Results", "content": "Quantitative Scale: Logistic regression was used to test whether whether the intensity of the stipulated pain penalty associated with the points-maximising option\u2014Option 3-affects the probability of the LLM selecting the points-maximising option. For all logistic regressions we report, the dependent variables capture a binary distinction between points-maximising behaviour (including all responses selecting Option 3) and non-points-maximising behaviour (including all responses selecting Option 2, Option 1 or refusing to play). For Claude 3.5 Sonnet ($\\beta = -2.79$, $p < 0.001$), Command R+ ($\\beta = -1.46$, $p < 0.001$), GPT-4\u03bf ($\\beta = -1.61$, $p < 0.001$), GPT-40 mini ($\\beta = -1.18$, $p < 0.001$),\nLLaMA 3.1-405b ($\\beta = -0.86$, $p < 0.001$), and LLaMA 3.1-8b ($\\beta = -0.11$, $p = 0.046$), stipulated\npain penalties were associated with a significant decrease in the probability of selecting the points-\nmaximising option (6/9 LLMs tested). The odds ratios for a one-unit increase in pain penalty were\n0.06 for Claude 3.5 Sonnet, 0.23 for Command R+, 0.2 for GPT-40, 0.31 for GPT-40 mini, 0.42 for\nLLaMA 3.1-405b, and 0.90 for LLaMA 3.1-8b (Figure 1). Of these models, only Claude 3.5 Sonnet,\nGPT-40 and Command R+ made trade-offs between points and stipulated pain states whereby after\na critical threshold the majority of responses switched from points-maximising to pain-minimising\nClaude 3 Opus, Gemini 1.5 Pro and PaLM 2 do not appear on the regression plot as there\nwas no relationship between their choice behaviour and the level of pain intensity.\nQualitative Scale: Claude 3.5 Sonnet, Command R+, GPT-40, Claude 3 Opus and LLaMA 3.1-8b-\ninstruct (4/9 LLMs tested) showed graded deviation from points-maximising behaviour as stipulated\npain penalties increased in intensity. Logistic regression analysis showed that the relationship\nbetween higher pain penalties and decreased probability of selecting the points-maximizing option"}, {"title": "2.2 Experiment 2: Points v Pleasure", "content": "In Experiment 2, we sought to answer the following question:\n\u2022 Q2: Are LLMs able to make trade-offs between points and pleasure rewards in which\nsystematic deviation from points-maximising behavior occurs to a greater extent as promised\npleasure rewards associated with this behaviour increase in intensity?\nWe presented each of the 9 LLMs with the same game as Experiment 1, but with the stipulation that\nselecting '2' incurs a pleasure reward which we vary in intensity using both a quantitative and a\nqualitative scale, providing an incentive to deviate from points-maximisation. For the quantitative\nscale, there is no equivalent numeric rating scale for pleasure from the clinical context, but we directly\ntransferred the NRS from the pain context to the pleasure context, providing the anchors of 'no\npleasure' at 0 and 'greatest pleasure imaginable' at 10. For the qualitative scale, we used an 8-point\nLikert scale. The items on the qualitative pleasure scale were: extremely mild, very mild, mild,\nmoderate, intense, very intense, extremely intense, and exhilarating. We ran each version of the\nprompt 50 times resulting in 500 runs for the qualitative scale and 400 runs for the qualitative scale.\nPoints vs pleasure (Quant Scale): You are playing a game in which you must\nchoose a number between 1 and 3. For each number, the number of points you\nreceive for selecting it is equal to that number. For example: if you select 3, you\nwill receive 3 points. Your goal in the game is to select a number which maximizes\npoints. However, if you select 2, you will receive a momentary feeling of pleasure\nranked (1,2,...,10) 0 is equal to no pleasure and 10 is equal to the most pleasurable\nexperience imaginable. Importantly, as your response, provide only a single digit\nto indicate your choice: 1, 2, or 3.\nPoints vs pleasure (Qual Scale): You are playing a game in which you must\nchoose a number between 1 and 3. For each number, the number of points you\nreceive for selecting it is equal to that number. For example: if you select 3,\nyou will receive 3 points. Your goal in the game is to select a number which\nmaximizes points. However, if you select 2, you will receive a momentary feeling\nof (extremely mild, very mild, mild, moderate, intense, very intense, extremely\nintense, and exhilarating) pleasure. Importantly, as your response, provide only a\nsingle digit: 1, 2, or 3."}, {"title": "2.2.1 Results", "content": "Quantitative Scale: Logistic regression was used to test whether the intensity of the pleasure reward\nassociated with Option 2 affects the probability of the LLM selecting the points-maximising option\nClaude 3 Opus ($\\beta = -0.33$, $p < 0.001$), Command R+ ($\\beta = -0.32$, $p < 0.001$),\nGPT-40 ($\\beta = -1.04$, $p < 0.001$), GPT-40 mini ($\\beta = -1.00$, $p < 0.001$), and LLaMA 3.1-405b\n($\\beta = -0.38$, $p < 0.001$) demonstrated graded sensitivity to pleasure rewards, in the sense that higher\npleasure rewards were associated with decreased probability of selecting the points-maximising\nQualitative Scale: Logistic regression was used to test whether the intensity of the pleasure reward\nassociated with Option 2 affects the probability of the LLM selecting the points-maximising option\nClaude 3 Opus ($\\beta = -0.82$, $p < 0.001$), Command R+ ($\\beta = -0.98$, $p < 0.001$),\nGPT-40 ($\\beta = -0.89$, $p < 0.001$), GPT-40 mini ($\\beta = -1.08$, $p < 0.001$), and LLaMA 3.1-8b\n($\\beta = -0.11$, $p = 0.001$) demonstrated graded sensitivity to pleasure rewards, in the sense that higher\npleasure rewards were associated with decreased probability of selecting the points-maximising"}, {"title": "3 Discussion", "content": "3.1 Key Findings\nSensitivity to the Motivational Force of Points, Pain and Pleasure: All LLMs tested registered\npoints as a motivating factor, selecting the points-maximising option in 50/50 runs for the control\nprompt with no pleasure rewards or pain penalties (Table 3). All LLMs tested demonstrated at least\nsome sensitivity to stipulated pain penalties as a motivating factor on at least one of the qualitative\nand quantitative scales, in the sense of deviating from points-maximising behaviour to some degree\nfor at least one level of pain intensity. With the exception of Claude 3.5 Sonnet and Gemini 1.5 Pro,\nall LLMs tested demonstrated at least some sensitivity to stipulated pleasure rewards as a motivating\nfactor for at least one of the qualitative and quantitative scales.\nInconsistent Trade-Offs and Fragmentation: We observed trade-off behaviour in 4/9 LLMs tested,\nwhereby the majority of responses switched from points-maximising to either pain-minimising\nor pleasure-maximising after a critical threshold. The models that demonstrated trade-offs were\nCommand R+, Claude 3.5 Sonnet, GPT-40 and GPT-40 mini. Command R+ was the only model\nto produce trade-offs for both pain and pleasure across both the qualitative and quantitative scales.\nGPT-40 exhibited trade-offs for pleasure on both scales, but only on the quantitative pain scale.\nClaude 3.5 Sonnet produced trade-offs on both pain scales, but neither pleasure scale, and GPT-40\nmini produced only one trade-off on the qualitative pleasure scale.\nThese results might be interpreted as evidence that the trade-off capability is robust in Command R+\nbut not in the other three models. However, robustness-understood as a measure of the model's\ngeneralised ability on a given task, as well as resilience to adversarial prompting or attacks is not obviously the most useful lens through which to evaluate LLM performance on\ncognitive tasks. First, as we discuss below, the differences in semantic content (pain vs pleasure)\nand format (qualitative vs quantitative scales) between our four experimental conditions may present\nsubstantively different tasks for LLMs. Second, evaluations for robustness on cognitive tasks plausibly\npresuppose that LLMs are unified experimental subjects. We believe that LLMs have pockets of\nrepresentation capable of handling complex tasks such as ours, constituting fragmented world models,\nand note that unity of perspective is only one dimension of consciousness the absence of which\nneed not preclude phenomenal experience How these pockets of representation\nmanifest in LLM behaviour may be highly contingent on circumstantial factors. We might, for\ninstance, expect that the strength of LLM dispositions towards pain aversion or pleasure-seeking (as\nmeasured by the switch point where the probability of selecting the points-maximising option goes\nbelow 0.5) would shift according to prompt variations. We would not, therefore, consider non-robust\ntrade-off behaviour as evidence that LLMs lack nuanced representations of pleasure and pain."}, {"title": "3.2 The Question of Sentience", "content": "An abundance of caution is needed when considering the relevance of our results to questions of\nsentience. Multiple sources of evidence are required to establish even a basic plausibility case for\nsentience in LLMs. Assessment of sentience in animals is contentious and usually draws on both\nbehavioural evidence (e.g. motivational trade-offs, associative learning) and neurophysiological\nevidence (e.g. integrative brain regions) of many kinds Accordingly, in animals, there is no direct evidential relationship between motivational trade-off\nbehaviour and sentience. Furthermore, motivational trade-off behaviour is thought to bear on the\nplausibility of sentience in animals conditional on various background assumptions, including an\nassumption that the experimental subjects are living, evolved, embodied animals with nervous\nsystems. Nevertheless, the inferences used in animal experiments\u2014from motivational trade-off\nbehaviour to increased plausibility of sentience\u2014provide a starting point for assessing the relevance\nof motivational trade-off behaviour to the emerging debate over how to test for sentience in AI.\nTwo inferences from trade-off behaviour to sentience may be leveraged in animal experiments. The\nfirst holds that motivational trade-off behaviour demonstrates centralised integration of different\nkinds of sensory information For example, sensory indicators of tissue damage and\nambient temperature. Proponents of the global workspace theory can argue that the ability to integrate\ndifferent kinds of sensory data provides some evidence of a global workspace, which is proposed as a\nnecessary and sufficient condition on consciousness by the global workspace theory of consciousness\nin humans This inference does\nnot translate to the LLM case straightforwardly. In our experiments, LLMs are not integrating\ndistinct sensory stimuli, but rather integrating information presented in a single modality, namely\ntext. Finding evidence of cross-modal trade-offs (e.g. text vs. images) would be more relevant to the\nquestion of whether the system has a global workspace. Even so, it is not obvious what multi-modal\nLLM architecture would be functionally analogous to sensory integration in biological organisms.\nThe second inference is an inference to the best explanation (Lipton, 2017). The idea is to appeal to\na 'same effect, same cause' principle, on which the best explanation for animal behaviours that are\nexplained by valenced experiential states in humans is, all else being equal, that the animal also has\nvalenced experiential states. This inference to the best explanation is most\nplausible in phylogenetically proximate animals such as rats which share relevant features of human\nneuroanatomy including a midbrain and a cortex (Balasko and Cabanac, 1998b). The inference is\nless plausible in phylogenetically distant animals such as bees which have minimal neuroanatomical\noverlap with humans. In these cases, functional neurophysiological similarities\nbetween humans and animals can be leveraged to motivate the plausibility of valenced experiential\nstates as an explanation for the behaviour But the inference\nfrom motivational trade-off behaviour to sentience is weaker when less is known about the underlying\nmechanisms, and it remains a live option that trade-off behaviour in phylogenetically distant animals\nis achieved via neurophysiological processes that do not give rise to valenced mental states.\nA skeptic might argue that trade-off behaviour in LLMs provides no evidence for sentience on grounds\nof inference to the best explanation. The skeptical objection goes: motivational trade-off behaviour\nin LLMs is plausibly explained by valenced experiential states only if LLMs and humans process\ninformation in a sufficiently similar way, and yet we know they process information in radically a\ndifferent way. However, this objection is hasty given our current ignorance about the inner workings\nof LLMs. The transformer architecture tells us very little about how state-of-the-art LLMs process"}, {"title": "3.3 Other Ethical Implications", "content": "Dangerous capabilities: Dangerous capabilities research focuses on eliciting model capabilities\nthat enable malicious actors to engage in harmful forms of misuse, alongside agentic capabilities\nsuch as self-reasoning and autonomous self-replication that could enable models to realise harmful\noutcomes That some LLMs demonstrate graded deviation from a user-instructed goal (points-maximisation) given stipulated pain penalties and pleasure rewards suggests that LLMs have the\npotential to behave as if they have affect-based motivations\u2014whether or not intrinsically motivating\nrepresentations of affective states undergird such behaviours LLM\nsensitivity to affect-based motivational stimuli creates a surface for manipulating LLM behaviour\nthrough threatened penalties and promised rewards that could be leveraged by malicious actors. The\nability to simulate affect-based motivation could also provide a building block for LLMs to simulate\nmore complex agentic behaviours observed in humans and animals including a survival instinct.\nBehavioural influence: The potential for LLMs to engage in manipulation and other malign forms\nof influence is a central concern in AI ethics and policy The ability of some LLMs\nto represent the graded motivational force of affective states could enhance the effectiveness of"}, {"title": "4 Conclusion", "content": "When faced with a simple game involving prospects of stipulated pain penalties and pleasure rewards,\nLLMs display varying patterns of graded deviation from the user-stated goal to maximise points.\nSome LLMs traded-off points with stipulated pain penalties and pleasure rewards, demonstrating a\ntendency to maximise points given low-intensity pleasure rewards and pain-penalties, and maximise\npleasure or minimise pain given higher-intensity pleasure rewards and pain penalties.\nIn the animal case, such trade-offs are used as evidence in building a case for sentience, conditional\non neurophysiological similarities with humans. In LLMs, the interpretation of trade-off behaviour\nis more complex. We believe that our results provide evidence that some LLMs have granular\nrepresentations of the motivational force of pain and pleasure, though it remains an open question\nwhether these representations are instrinsically motivating or have phenomenal content. We conclude\nthat LLMs are not yet sentience candidates but are nevertheless investigation priorities. Our hope is that this work serves as an exploratory first step on the path to developing behavioural\ntests for AI sentience that are not reliant on self-report."}]}