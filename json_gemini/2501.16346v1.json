{"title": "SELF-SUPERVISED GRAPH TRANSFORMER WITH CONTRASTIVE LEARNING FOR\nBRAIN CONNECTIVITY ANALYSIS TOWARDS IMPROVING AUTISM DETECTION", "authors": ["Yicheng Leng", "Syed Muhammad Anwar", "Islem Rekik", "Sen He", "Eung-Joo Lee"], "abstract": "Functional Magnetic Resonance Imaging (fMRI) provides\nuseful insights into the brain function both during task or rest.\nRepresenting fMRI data using correlation matrices is found\nto be a reliable method of analyzing the inherent connectivity\nof the brain in the resting and active states. Graph Neural\nNetworks (GNNs) have been widely used for brain network\nanalysis due to their inherent explainability capability. In this\nwork, we introduce a novel framework using contrastive self-\nsupervised learning graph transformers, incorporating a brain\nnetwork transformer encoder with random graph alterations.\nThe proposed network leverages both contrastive learning and\ngraph alterations to effectively train the graph transformer for\nautism detection. Our approach, tested on Autism Brain\nImaging Data Exchange (ABIDE) data, demonstrates supe-\nrior autism detection, achieving an AUROC of 82.6 and an\naccuracy of 74%, surpassing current state-of-the-art methods.", "sections": [{"title": "1. INTRODUCTION", "content": "Autism spectrum disorder (ASD) is a complex neurodevel-\nopmental condition characterized by challenges in social in-\nteraction, communication, and restricted or repetitive behav-\nior. The clinical heterogeneity of ASD and its overlap with\nother neurological disorders underscore the need for objec-\ntive and reliable diagnostic methods [1]. Traditional diag-\nnostic approaches are largely based on behavioral and neuro-\npsychological assessments, which are subjective and depen-\ndent upon the clinician's experience and the patient's condi-\ntion during the assessment.\nNeuroscientists have actively studied brain network anal-\nysis to comprehend human brain organization, treat disorders,\nand predict clinical outcomes [2]. Recent brain imaging stud-\nies highlight the importance of interactions between brain\nregions for neural development and disorder analysis [3].\nAmong various imaging techniques, functional Magnetic\nResonance Imaging (fMRI) is widely used to assess brain\nfunction and connectivity networks. Functional connectivity\ncaptures correlations in time-series signals between brain re-\ngions, while structural connectivity represents physical links\namong the cortical and subcortical areas [4]. Using graph\ntheory, brain networks represented with nodes and edges\ncan be formulated to depict brain region interactions [5]. In\nparticular, nodes in the network are defined as Regions of\nInterest (ROIs) based on the brain atlas used. Edges are cal-\nculated as pairwise correlations between the Blood-Oxygen-\nLevel-Dependent (BOLD) signal series obtained from each\nregion [6]. This network pattern facilitates the classification\nof brain regions into diverse functional modules, thereby aid-\ning in disease diagnosis, studying brain development, and\nenhancing understanding of disease progression.\nGraph neural networks and connectivity analysis provides\ngreat potential to study and understand complex brain disor-\nders such as autism [7]. Efficient training of such networks\nis challenging, as high-quality labels are required for most\ndeep learning models. Self-Supervised Learning (SSL) is an\nalternative paradigm, where models can be trained with a pre-\ntext task such as image reconstruction, alleviating the burden\nof collecting high-quality labels. Once trained, such mod-\nels can be used for downstream clinical tasks with few la-\nbeled data [8]. We build on the strengths of both SSL and\ngraph neural networks and propose a novel training paradigm\nfor autism detection using fMRI data. Particularly, we focus\non contrastive learning, with a novel graph dilation/shrinkage\nstrategy to detect autism.\nOur contributions: The main contributions of our proposed\nmethods are as follows:\n\u2022 We develop a self-supervised learning strategy for graph\ntransformer utilizing brain network transformer encoder.\n\u2022 We introduce a graph dilation and shrinkage strategy to\nimprove contrastive learning and achieve better detection\noutcomes.\n\u2022 We present extensive experiments, demonstrating state-of-\nthe-art detection performance (AUROC of 82.6), to show-\ncase the effectiveness of our proposed method using con-\ntrastive self-supervised learning with noise addition."}, {"title": "2. RELATED WORK", "content": "Autism Detection and Analysis Functional magnetic reso-\nnance imaging, even in the resting state (rs-fMRI) and the as-\nsociated Functional Connectivity (FC) matrices have opened\nnew avenues for understanding neurological development and\ndisorders, offering a robust and accurate tool to identify dis-\nease biomarkers [9]. FC captures the synchronicity of be-\nhavioral patterns across different regions of the brain at rest,\nrevealing the functional architecture. Typically, FC in pa-\ntients shows a reduction in long-range communication be-\ntween the frontal and posterior brain regions, accompanied\nby an increase in local connections. These changes can lead\nto disrupted cortical connectivity in autism [10]. However,\nFC studies on ASD reveal variable changes in FC, with both\nincreased and decreased connectivity, leading to inconsistent\nfindings. Deep learning have proven to be highly effective\nin detecting subtle patterns in high-dimensional data that are\noften imperceptible to human observation [1].\nGNNs for Brain Connectivity Analysis Recent research has\nfocused on extending Graph Neural Network (GNN) mod-\nels to analyze brain connectivity. For instance, brainGNN\nleverages ROI-aware GNNs to utilize functional information\nand employs special pooling operators for node selection [3].\nFBNetGen explores the generation of brain networks and\ntheir explainability for downstream tasks [11]. Addition-\nally, Graph Transformers have shown strong performance in\ngraph representation learning, integrating edge information\nand using eigenvectors for positional embeddings [12]. The\nSpectral Attention Network (SAN) improves positional em-\nbeddings and refines the attention mechanism by prioritizing\nneighboring nodes alongside global information [13]. The\nNrain Network Transformer (BNT) models brain networks\nas graphs with fixed-size and ordered nodes and introduces\na graph Transformer model with an orthonormal clustering\nreadout for brain network analysis [14].\nGNN training is challenging and unstable with limited\ndatasets, leading to high variance and poor performance.\nAlso, more complex GNN models tend to cause greater vari-\nance and lower performance [15]. Our strategy mitigates this\nproblem, offering stable learning outcomes with lower vari-\nance and state-of-the-art performance for autism detection."}, {"title": "3. METHODS", "content": "Our proposed framework is illustrated in Figure 1. We in-\ntroduce an innovative method for pretraining the BNT en-\ncoder [14] using contrastive loss to enhance feature learning.\nSubsequently, we use the pretrained BNT encoder for super-\nvised training in downstream brain network classification for\nautism detection."}, {"title": "3.1. Contrastive Self-supervised Learning (CSSL) with\nBrain Network Transformer", "content": "In the fMRI datasets, each brain network comprises time se-\nries data represented as $T \\in \\mathbb{R}^{V\\times1}$ and a connectivity pro-\nfile denoted by the correlation matrix $C \\in \\mathbb{R}^{V\\timesV}$, where\n$V$ indicates the number of nodes (i.e., ROIs). In particular,\nthe BNT utilizes C as inputs for brain network classification,\nemploying orthonormal clustering readout functions to effec-\ntively discern modular-level similarities among ROIs in brain\nnetworks.\nIn this work, we propose a framework based on Con-\ntrastive Self-Supervised Learning (CSSL) [16] to pretrain the\nBNT encoder $bnt(\\cdot)$, as illustrated in Fig. 1. The key idea in-\nvolves using the CSSL framework to generate new instances\nfrom unlabeled networks and employing the BNT encoder to\nidentify whether two generated networks are from the same\noriginal instance. Using CSSL, we improve the model to\nfacilitate effective learning of brain network representations,\nthus enhancing classification performance during the down-\nstream task. In addition, our framework incorporates graph\ndilation and shrinkage techniques, specifically designed for\nbrain network analyses. The proposed method allows for gen-\nerating new instances and improving representation learning,\nas detailed in the following section."}, {"title": "3.2. Graph Dilation and Shrinkage", "content": "Basic graph augmentation includes node and edge additions\nand deletions. When generating new instances for contrastive\nlearning through these random operations, the BNT encoder\nlearns self-attention based on the augmented graph structure.\nHowever, in brain networks, ROIs are fully connected, with\nconnectivity represented by the correlation matrix C. Thus,\nwe revise the graph augmentation to focus solely on edge op-\nerations since adding or deleting nodes is not applicable in\nour use case. Intuitively, the correlation between two nodes\nindicates their connectivity. Hence, we modify edge addition\nand deletion to increase or decrease the absolute value of cor-\nrelation, with the increment being random. Furthermore, to\nsimulate node operations, we randomly increase or decrease\nall adjacent edges of randomly selected nodes, altering con-\nnectivity within the ROI. This is termed graph dilation and\nshrinkage.\nThe predictive task of CSSL is to train the BNT encoder\nto predict whether two embeddings of graphs, $f_1 = bnt(C_1)$\nand $f_2 = bnt(C_2)$, are from the same graph. This involves\ngenerating two augmented graphs, $C_i$ and $C_j$, along with a\nset of randomly generated graphs ${C_k}$ for each unlabeled\ngraph C. With that, and the training set $X = {x_1,..., X_N}$,\nthe contrastive loss is formulated as follows:\n$L(x_i) = -log(\\frac{e^{(sim(g(f_i),g(f_j))/\\tau)}}{\\sum_{k \\ne i} e^{(sim(g(f_i),g(f_k))/\\tau)}})$ (1)\nwhere sim represents the cosine function. Similar to the ap-\nproach used in CSSL [16], we employ MoCo [17] to optimize\nEquation 1. Using CSSL, we load the pretrained parameters\nfor the BNT encoder to conduct finetuning on the BNT model,\nincorporating a new classification head identical to the one\nused in BNT."}, {"title": "4. EXPERIMENTS", "content": ""}, {"title": "4.1. Experimental Settings", "content": ""}, {"title": "4.1.1. Datasets", "content": "There are a limited number of brain imaging datasets that con-\ntain raw imaging data for preprocessing to generate brain net-\nwork datasets, primarily due to regulatory restrictions. To\naddress this, we use the preprocessed Autism Brain Imag-\ning Data Exchange (ABIDE) dataset, an openly accessible\ncompilation of rs-fMRI data from autistic spectrum disorder\nsubjects and neurotypical controls [18]. The dataset spans a\nbroad age range and includes data from numerous healthy and\nASD participants, collected over an extended period of time.\nIn specific, the pre-processed data consist of 1,009 samples,\neach comprising 200 nodes and 40,000 edges. The node fea-\ntures consist of time series data with a length of 100, resulting\nin a 200\u00d7100 matrix for every sample. Moreover, the edge\nfeatures, represented by the correlations, result in a 40,000\u00d71\nmatrix for each sample. With that, the correlation matrix is\ndense, and these characteristics remain consistent across all\nsamples within the ABIDE dataset."}, {"title": "4.1.2. Evaluation Metrics", "content": "For this binary classification task, we used AUROC and ac-\ncuracy (threshold set at 0.5) as evaluation metrics, along\nwith sensitivity and specificity to assess model performance.\nSpecificity denotes the ratio of accurately classified negative\nsamples, whereas sensitivity represents the ratio of correctly\nclassified positive samples."}, {"title": "4.1.3. Implementation Details", "content": "For the MoCo implementation, we follow the CSSL ap-\nproach [16], setting the queue size to 512, momentum to\n0.999, and temperature $ \\tau$ to 0.07. We use the stochastic\ngradient descent optimizer with a learning rate of 0.00001\nand a batch size of 64. We perform CSSL pretraining for\n900 epochs for our method and ablations, using all instances\nin ABIDE without labels. For finetuning, we combine the\npretrained BNT encoder with a classification head for the\ndownstream graph classification, as in BNT [14]. At the\nend of the BNT encoder, the features are reshaped to dimen-\nsions $(N_b, N_o, 8)$, where $N_o$ is the output number of nodes\n= 100. The classification head includes Linear$(D_f, 256)$,\nLeakyReLU, Linear$(256, 32)$, LeakyReLU, Linear$(32, 2)$,\nwith $D_f=8xN_o$, with $N_o$ and $D_f$ representing the batch size\nand the flattened feature dimension.\nWe train the BNT model for 200 epochs using the Adam\noptimizer, with a learning rate of 0.00005, weight decay of\n0.00005, and a batch size of 64. We use a relatively smaller\nlearning rate than BNT to ensure stability of performance."}, {"title": "4.2. Results", "content": ""}, {"title": "4.2.1. Performance Analysis", "content": "Table 1 presents the results of our model. We compare our re-\nsults with existing GNN-based methods, including the Graph\nTransformer from Section 2. In general, our proposed method\nachieved an AUROC of 82.6 \u00b1 1.8, which is the highest com-\npared to other methods in the literature. The standard devi-\nation also shows that the models results are stable, which is\nimportant for reliable test time performance. In this work,\nwe reproduce the state-of-the-art results achieved by the BNT\nmethod and compare our performance with those reported by\nthem [14]."}, {"title": "4.2.2. Ablation Studies", "content": "This section presents results from various configurations of\nour training strategy. The ablation study is conducted to\nexplore the impact of different numbers of nodes for graph\naugmentation and different types and levels of noise applied\nduring training. The top-performing results in each metric\nare highlighted in bold. Table 2 presents performance com-\nparisons using different numbers of nodes for graph dilation\nand shrinkage, showing that excessive node usage can lead to\noverfitting and reduced performance. Table 2 also provides\ncomparisons using different types of noise. The correspond-\ning ROC curves are presented in 3. The results indicate that\nlow-variance Gaussian noise improves classification accu-\nracy, AUROC, and specificity."}, {"title": "5. CONCLUSION", "content": "In this paper, we present a contrastive self-supervised learning\nmethod for graph transformers. We also propose a node di-\nlation and shrinkage strategy to enhance contrastive learning.\nExtensive experiments with our proposed method demon-\nstrate that it significantly improves the autism detection ca-\npability, achieving state-of-the-art performance in autism\ndetection in ABIDE data."}]}