{"title": "Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models", "authors": ["Yi-Lun Lee", "Yi-Hsuan Tsai", "Wei-Chen Chiu"], "abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To address this, recent approaches apply contrastive decoding to calibrate the model's response via contrasting output distributions with original and visually distorted samples, demonstrating promising hallucination mitigation in a training-free manner. However, the potential of changing information in visual inputs is not well-explored, so a deeper investigation into the behaviors of visual contrastive decoding is of great interest. In this paper, we first explore various methods for contrastive decoding to change visual contents, including image down-sampling and editing. Downsampling images reduces the detailed textual information while editing yields new contents in images, providing new aspects as visual contrastive samples. To further study benefits by using different contrastive samples, we analyze probability-level metrics, including entropy and distribution distance. Interestingly, the effect of these samples in mitigating hallucinations varies a lot across LVLMs and benchmarks. Based on our analysis, we propose a simple yet effective method to combine contrastive samples, offering a practical solution for applying contrastive decoding across various scenarios. Extensive experiments are conducted to validate the proposed fusion method among different benchmarks. Code is available at https://github.com/YiLunLee/VCD_Analysis.", "sections": [{"title": "1. Introduction", "content": "Recent large vision-language models (LVLMs) [2, 5, 18] have shown strong capabilities of yielding detailed and plausible responses correlated to the visual inputs. However, stemming from LLMs, LVLMs also suffer from \"hallucination\", where the generated responses are fluent and semantically coherent but may conflict with input visual contents, leading to a risk of producing fabri-"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Hallucination Mitigation in LVLMs", "content": "With advancements in LLMs, various LVLMs incorporate visual encoders like CLIP [20] with LLMs (e.g., LLaMA [23], Vincuna [29], or Qwen [1]) to yield reasonable and detailed responses related to visual contents, including but not limited to InstructBLIP [5], LLaVA-1.5 [18], Qwen-VL [2], MiniGPT-4 [31], and MPLG-Owl2 [26]. Despite the promising performance, these LVLMs still suffer from hallucinations, where the generated responses are inaccurately grounded with image contents. To address this, several approaches are proposed from different aspects: learning with human feedback and preference methods [19, 22, 28] either conduct reinforcement learning with the human feedback [22] or leverage Direct Preference Optimization (DPO) framework [21] to fine-tune LVLMs with DPO datasets generated by GPT-4v [28] or CLIP Ranking [19]; finetuning methods [10, 17] first introduce newly collected hallucination-calibration datasets (e.g., LRV-Instruct [17] and M-HalDetect [10]) and fine-tune LVLMs with them to prevent hallucinations; post-hoc methods [27, 30] includes post-processing calibration with external models [27] or training post-hoc revisors to correct hallucinated responses [30].\nHowever, most of the above-mentioned approaches have practical drawbacks: they require significant manual effort to create hallucination-specific datasets and involve fine-tuning LVLMs or training post-hoc revisors. In contrast, some studies focus on adjusting decoding strategies, offering a more practical solution without additional train-"}, {"title": "2.2. Contrastive Decoding", "content": "Contrastive decoding [14] is oriented from mitigating the hallucination of LLMs, leveraging the contrast between expert and amateur models to amplify good expert behavior and diminish undesired amateur behavior. To extend contrastive decoding for LVLMs, several approaches [4, 7, 13, 24, 32] are proposed with different designs of contrastive samples prone to producing hallucinations. As a pioneer, VCD [13] increases the uncertainty of visual inputs by adding Gaussian noise in a diffusion manner, producing more hallucinated contents. M3ID [7] directly contrasts the predictions from the original visual inputs with those of pre-trained LLMs without input images, which induce hallucinations from language priors. IBD [32] finetunes LVLM to be image-biased LVLM via amplification of attention on visual tokens and then contrast with conventional LVLM, which is likely text-biased to mitigate the hallucination. ICD [24] adjusts the instruction for the Q-Former [5] with deceptive prompts to destroy the alignment between visual and text embeddings, leading to more likelihoods of producing hallucinations. HALC [4] selects different field of views (FOVs) based on decoding probabilities and performs contrastive decoding of FOV pairs to find the optimal outputs matching the visual input.\nDespite the impressive performance, the potential of using visually changed samples in LVLMs remains underexplored. To this end, we first explore different manipulations of image contents for creating visually changed samples and investigate the prediction-level metrics on LVLMs' response and revision behaviors of contrastive decoding. Moreover, based on these observations, we propose an effective fusion method to leverage the strengths of different visually changed samples."}, {"title": "3. Visual Contrastive Decoding", "content": null}, {"title": "3.1. Preliminaries", "content": "Decoding process of LVLMs. We first introduce the generation paradigm of LVLMs, involving an LVLM parameterized by \\( \\theta \\), visual input v, and textual query x. Then LVLM considers the contextual information from the visual input v to yield relevant responses y to the textuary query x, where the responses are generated auto-regressively via sampling from the distribution conditioned on visual input v and textual query x. The auto-regressive generation process can be formulated as follows:\n\\( y_t \\sim P_\\theta(y_t|v,x,y_{<t}) = \\text{softmax}(\\text{logit}_\\theta(y_t|v, x, y_{<t})) \\).  (1)\nHere, \\( y_t \\) denotes the token at time step t, and \\( y_{<t} \\) represents the sequence of generated tokens up to the time step (t-1). However, during the decoding process, the hallucination response may occur in several situations: 1) the visual contextual information is captured unsuccessfully, 2) LLM priors suppress the information from image embeddings, and 3) statistics from pre-trained datasets affect the responses with the similar scene learned previously.\nContrastive decoding with visually changed samples. To mitigate hallucinations, several approaches [7, 13, 14, 24] are proposed to adjust the decoding process in a contrastive manner. Specifically, samples that are changed with less visual information are introduced to produce a hallucination response due to insufficient visual contents. Then, these hallucinated contents are subtracted from the original response during the contrastive decoding process to prevent LVLMs from producing hallucinations. That is, the original logits of the next token are substrated by those of contrastive samples, where the formula can be denoted as:\n\\( P_{cd}(y_t|v, v_{cd}, x, y_{<t}) = \\text{softmax}[(1+\\alpha)\\text{logit}_\\theta(y_t|v, x, y_{<t})\n-\\alpha (\\text{logit}_\\theta (y_t| v_{cd}, x, y_{<t}))] \\), (2)\nwhere \\( v_{cd} \\) is the visually changed sample and \\( \\alpha \\) is the weight to control the effect of differences between the two distributions."}, {"title": "3.2. Visually Changed Samples in Different Aspects", "content": "The high-level concept behind contrastive decoding is to create visually changed (or distorted) samples serving as contrastive samples, which are more likely to produce hallucinated contents due to the reduced image information. Previous works [7, 13] have explored two ways, adding diffusion noise to the image or simply removing the image. Inspired by these methods, we further explore two distinct types: image downsampling and editing. The illustration of visually changed samples is shown in Figure 2.\nDiffusion noise. As a pioneer in applying contrastive decoding to multimodal scenarios, VCD [13] proposes applying a diffusion process following [11] to add Gaussian noise \\( \\mathcal{N} \\) onto the original input image \\( v_0 \\). During the diffusion process, the diffusion function q incrementally adds a small amount of Gaussian noise for N steps, producing distorted image sequences \\( {v_1, v_2, ..., v_N } \\). The diffusion process can be formed:\n\\( q(v_n|v_{n-1}) = \\mathcal{N}(v_n; \\sqrt{1-\\gamma} v_{n-1}, \\gamma I), \\)\n\\( q(v_N|v_0) = \\prod_{n=1}^{N} q(v_n|v_{n-1}), \\) (3)"}, {"title": "4. Analysis on Visual Contrastive Encoding", "content": "Although prior studies (e.g., VCD [13] and M3ID [7]) have demonstrated impressive improvement on LVLMs via contrastive decoding with visually changed samples (e.g., adding noise and removing the image, respectively), The effectiveness of using different visual CD methods to mitigate the hallucination issue is not well explored yet. As mentioned in Sec. 1, we observe that on different benchmarks (POPE [15] and MME [8]), the performance gain brought from different visually changed samples varies across LVLMs and tasks.\nWe hypothesize that applying different visually changed samples yields varying influences on mitigating hallucinations. Thus, we conduct several analysis on the prediction-level statistics to understand behaviors of using each visually changed sample, including entropy/confidence, probability distribution distance, revised outcomes' behaviors,"}, {"title": "4.1. Benchmarks and LVLMs", "content": "Benchmarks. POPE [15], the Polling-based Object Probing Evaluation, aims to assess the object hallucination. This benchmark consists of Yes-No question-answering pairs related to the corresponding image, querying the LVLM to answer whether a specific object exists in the image or not. The ratio of Yes and No questions is balanced (i.e., 50% vs 50%). Within this benchmark, there are three different difficulties of queried non-existent objects, including random, popular, and adversarial. In the random setting, the non-existent objects are selected from all the objects in datasets randomly, while the popular setting selects non-existent objects from the high-frequency pool. For the adversarial setting, the non-existent objects are selected from the co-occurrence objects related to the queried image scene.\nMME [8], a more challenging benchmark for assessing hallucination from multiple perspectives. It contains 14 Yes-No question-answering tasks, including four coarse-grained perception tasks, six fine-grained perception tasks, and four cognition-focus tasks.\nLVLM backbones. We follow prior works [13] to evaluate the prediction tendency and behaviors of different visually changed samples using InstructBLIP [5], LLaVA-1.5 [18], and Qwen-VL [2]. Specifically, these LVLMs are comprised of three components: image encoder, visual-textual interface, and language decoder. The main difference among these three LVLMs is the design of the visual-textual interface: InstructBLIP uses the Q-Former (a transformer) to extract task-relevant image features related to the textual query; LLaVA-1.5 directly projects visual embedding into language embedding with multi-layer perception (MLP), and Qwen-VL utilizes a position-aware vision-language adapter (a cross-attention layer) to compress image features. These differences in the visual-textual inter-"}, {"title": "4.2. Analysis with Entropy", "content": "We investigate the behavior of each visually changed sample in terms of entropy (i.e., confidence). It is acknowledged that entropy represents the uncertainty of the prediction. Previous work [13] demonstrates that visual uncertainty amplifies the language priors and statistical bias, which leads to hallucinations. The entropy can serve as an indicator to determine the extent of inducing hallucinations given different visually changed samples. To delve into more concrete observations using entropy, we visualize the entropy distribution of the first token in the Yes-No question, as shown in Figure 7. Visually changed samples have distinct properties of entropy across different LVLMs and benchmarks, which reflects the design of manipulation on visual inputs. We draw several observations:\n1) No image method only contrasts with the response from the pre-trained LLM without image information, producing more noisy and unpredictable outputs.\n2) Downsample and diffusion noise methods have similar entropy distributions, indicating a similar effect of losing some image information to increase the uncertainty of output predictions.\n3) Different from other visually changed samples, the image editing method provides a more specific modification related to the textual instruction extracted from the textual query of LVLMs, where the editing goal is to insert the queried objects into the image. Hence, the entropy is very low in comparison with others on POPE, which only focuses on object existence. In contrast, the MME benchmark contains various perception and cognition tasks; the editing difficulty of the queried fine-grained contents increases, and thus, edited images may fail to be consistent with the original images."}, {"title": "4.3. Analysis with Probability Distribution Distance", "content": "In addition to the entropy, we further measure the probability distribution distance (PDD) between the original and CD predictions, which is also investigated in [7]. Here, we mainly use the Hellinger distance to estimate the distance between probability distributions, defined as \\( H(p,q) = \\frac{1}{\\sqrt{2}} \\sum_{i=1}^{k} (\\sqrt{p_i} - \\sqrt{q_i})^2 \\), where p = \\( (p_i)_{i\\in k} \\) and q = \\( (q_i)_{i\\in |k|} \\) are discrete probability distributions. Intuitively, when the distance increases, the prediction distribution of the original visual input and visually changed sample differs more, leading to a high risk of obtaining hallucination results from visually changed samples. According to the results shown in Figure 9, we draw several observations:\n1) No image method presents distinct distributions of PDD with InstructBLLIP and LLaVA-1.5. This phenomenon indicates that the language priors learned from LVLMs are different.\n2) Diffusion noise method has PDD distribution centralized at lower PDD values, demonstrating that even the uncertainty of visual inputs is enhanced with added noises, the prediction distribution is still similar to the original input. Downsample method also has lower PDD values, where the overall distribution is smoother, meaning that parts of downsampled samples change their predictions due to the lost of high-frequency details.\n3) Different from others, image editing method obtains the PDD distribution with higher values on POPE but lower values on MME. As mentioned previously, editing a"}, {"title": "4.4. Revision Behaviors", "content": "To investigate the behavior of each contrastive sample, we focus on three metrics: revise-correct samples, revise-wrong samples, and the predicting tendency on the POPE and MME benchmarks. Revise-correct samples indicate that the contrastive decoding revises the original response with the correct answers, while revise-wrong samples present the opposite case. Predicting tendency is the tendency of LVLMs to answer Yes-No Questions with or without contrastive decoding, including yes, no, and others. The results are shown in Figure 5. We draw several observations:\n1) As shown in Figure 5 (a) and (c), the results present that the contrastive decoding with different visually changed samples adjusts both correct and incorrect answers, where the revise-correct samples are more than revise-wrong ones. This phenomenon reveals that: although these CD methods appear to improve the overall performance, it is because that the number of revise-correct samples outweighs the number of revise-wrong samples."}, {"title": "4.5. Pairwise Overlap of Rectified Answers", "content": "Based on the above observation of revision behaviors, we investigate the complementary properties among different CD methods. Specifically, we calculate the pairwise overlap of rectified answers among different visually changed samples. That is, the Jaccard similarity coefficient (known as IoU) is computed in the revise-correct set using any of the two CD methods. As shown in Figure 12, a lower score means a higher complement between two CD methods. Interestingly, results show that most pairwise scores are low, demonstrating the distinct effect of each CD method to rectify answers. This evidence motivates us to design a combined approach that leverages the strengths of each contrastive sample."}, {"title": "5. Fusion of Visual Contrastive Decoding", "content": "According to the observations studied above, we find that one visually changed sample may exhibit better performance under some settings due to its distinct behaviors and output distributions. Instead of manually selecting the best contrastive decoding method when encountering new tasks or datasets, we propose to apply all the visually changed samples and fuse their influence, which can be a more general approach to mitigate hallucinations in practice."}, {"title": "5.1. Naive Fusion", "content": "An intuitive approach to fuse the effect of different visually changed samples is to combine them with the same weight. According to (2), the decoding with fusion of several visually changed samples can be derived as:\n\\( P_{cd}(y_t|v, v_{cd}, x, y_{<t}) = \\text{softmax}[(1+\\alpha)\\text{logit}_\\theta(y_t|v, x, y_{<t})\n-\\alpha \\sum_{i=1}^{k} \\text{logit}_\\theta (y_t| v_{cd_i}, x, y_{<t})] \\), (7)\nwhere the \\( {v_{cd_1}, v_{cd_2}, ..., v_{cd_k} } \\) is the set of different visually changed samples."}, {"title": "5.2. Entropy-weighted Fusion", "content": "As studied above, when the entropy is higher, the LVLM is less confident in its response. Visually changed samples with a higher entropy can be more likely to yield the hallucinated response, which is more suitable for serving as a contrastive sample. Therefore, we fuse the different types of visually changed samples with the corresponding entropy of outputs as the weight. We then reformulate (2) as:\n\\( P_{cd}(y_t|v, v_{cd}, x, y_{<t}) = \\text{softmax}[\\text{logit}_\\theta(y_t|v, x, y_{<t})\n- \\sum_{i=1}^{k} e_i (\\text{logit}_\\theta (y_t|v, x, y_{<t}) - \\text{logit}_\\theta (y_t| v_{cd_i}, x, y_{<t}))] \\), (8)\nwhere \\( e_i \\) is the entropy of output distribution for a visually changed sample \\( v_{cd_i} \\). Note that, a similar formulation can be also derived for the case of using probability distribution distance, in which we will include details in the supplementary material."}, {"title": "5.3. Quantitative Results", "content": "We compare our fusion methods with other contrastive decoding methods with the single visually changed sample in Table 1. We demonstrate the accuracy under several settings with different LVLMs (i.e., InstructBLIP [5], LLaVA-1.5 [18], and Qwen-VL [2]) and benchmarks (POPE [15]"}, {"title": "5.4. Combinations of Fusion Methods", "content": "We have shown that entropy-weighted fusion with all visually changed samples is more robust to hallucinations than solely applying a specific visually changed sample; however, one question may emerge: \"Do we need to fuse all designed visually changed samples?\u201d. To further study this, we conduct an ablation study of different combinations of fusion methods in Table 2, where the combination ranges from fusing two to four visually changed samples. We find that fusion with more visually changed samples yields better overall performance, demonstrating that it is more general method than selecting specific combinations to be deployed for practical applications on novel data and tasks. More results are provided in the supplementary materials."}, {"title": "6. Conclusions", "content": "In this paper, we delve into visual contrastive decoding with various visually changed samples to mitigate hallucinations for large vision-language models. We first explore new visually changed samples from different aspects, including downsampling and image editing. We conduct thorough prediction-level analysis to investigate the prediction behaviors of visually changed samples, concluding that different visually changed images exhibit significantly varied suitability across different LVLMs and datasets. To this end, we propose an entropy-weighted fusion method to leverage the complementary property from visually changed samples based on the entropy of predictions, which is proportional to the likelihood of hallucinations. Experimental results showcase the efficacy of our fusion method to mitigate hallucinations of LVLMs."}, {"title": "7. Details of Visually Contrastive Decoding", "content": null}, {"title": "7.1. Plausibility Constraints", "content": "Based on the contrastive decoding process from (2) in the main paper, it may penalize the entire LVLM's responses via contrast samples. However, either original or visually changed samples may still generate words related to linguistic standards and common sense. Therefore, inaccurate punishment of these words may cause an unpredictable and even failed response. To tackle this issue, we follow previous works [13, 14] to introduce the plausibility constraint, which avoids the indiscriminate penalization caused by directly applying contrastive decoding to entire responses. The plausibility constraint is triggered by the confidence related to candidate tokens of original visual inputs, which can be formulated as:\n\\( V_{cond}(Y_{<t}) = \\{y_t \\in V : \\newline P_\\theta(y_t|v, x, y_{<t}) \\ge \\beta \\max_w P_\\theta(w|v, x, y_{<t})\\}, \\newline P_{cd}(y_t|v, v_{cd}, x, y_{<t}) = 0 \\text{ if } y_t \\notin V_{cond}(Y_{<t}), \\) (9)\nwhere \\( V_{cond} \\) is the output of LVLMs and \\( \\beta \\) in [0, 1] is a hyperparameter that controls how aggressive to keep only high-probability tokens (e.g., a higher value indicates a more aggressive manner). Therefore, the probability of other candidate tokens below the constraints is replaced with 0. With the plausibility constraint, contrastive decoding influences the next token predicting behaviors only when the LVLMs are not confident with their prediction. We set \\( \\beta = 0.2 \\) by default."}, {"title": "8. More Analysis on LVLMS", "content": "We provide more analysis of visually changed samples on LVLMs, including more LVLMs (e.g., Qwen-VL [2]), more metrics (e.g., confidence of predictions), and details of POPE benchmarks (e.g., random, popular tasks). In this section, we mainly investigate the behaviors of visually changed samples from the perspective of different LVLMs, in which the conclusions are drawn similarly to the main paper."}, {"title": "8.1. Analysis with Entropy and Confidence", "content": "We have discussed the investigation of entropy with InstructBLIP [5] and LLaVA-1.5 [18] in the main paper, which presents the different behaviors of visually changed samples. Here we provide the comparison of entropy with another LVLM, Qwen-VL, as shown in Figure 7. We also visualize the confidence distribution in Figure 8, where the confidence is estimated from the highest probability of the next-token prediction. The confidence is correlated with the entropy, where the lower confidence induces the higher entropy. We draw several observations:\n1) Within the same benchmark (i.e., POPE [15], which focuses on object hallucination and has tasks of three difficulties), the distributions of entropy and confidence are similar across different LVLMs.\n2) With similar concepts of changing images (i.e., reducing visual information), the downsample and diffusion noise methods have similar behaviors regarding entropy and confidence on the LLaVA-1.5 and Qwen-VL, while obtaining distinct behaviors on InstructBLIP. The reason could be the different architecture of visual-textual interfaces in LVLMs, where InstructBLIP leverages Q-Former to extract visual features while LLaVA-1.5 and Qwen-VL directly project the image embedding to text embedding via multi-layer perception (MLP) and cross-attention layer, respectively.\nBased on the above observations, we argue that the behaviors of visually changed samples vary across LVLMs and benchmarks."}, {"title": "8.2. Analysis with Probability Distribution Distance", "content": "We investigate the probability distribution distance (PDD) of visually changed samples across different POPE tasks and LVLMs, which present the similarity of output predictions to original ones, as shown in Figure 9. We find that different visually changed samples have varying PDD distributions across different LVLMs:\n1) For InstructBLIP, the influence of each visually changed sample is distinct, where the prediction of image editing and no image methods are centralized at high PDD value, while downsample and diffusion noise methods have the opposite distributions.\n2) For LLaVA-1.5, the downsample and diffusion noise methods have similar distributions centralized at lower PDD values, while the image editing method has a distribution with higher PDD values. Different from InstructBLIP, the no image method has a much lower PDD value, which could result from the difference in answering tendency (as shown in Figure 10).\n3) However, for the Qwen-VL, the PDD distributions of each visually changed sample are much similar and smoother, meaning that the responses produced by visually changed samples are unpredictable. That is, we cannot determine the impact of downsample or diffusion noise meth-"}, {"title": "8.3. Revision Behaviors", "content": "We provide the statistics of revision behaviors and answering tendencies on different benchmarks, as shown in Figure 10 and 11. We find that the answering tendencies of visually changed samples on LVLMs are very different. The InstructBLIP tends to answer \"yes\" while LLaVA-1.5 is prone to answering \u201cno\u201d for the visually changed samples. Moreover, we also find that no image method obtains valid answers (\"yes\" or \"no\") on LLaVA-1.5 and Qwen-VL, which differs from the behavior on InstructBLIP. This evidence demonstrates the answering tendency varies across different LVLMs, resulting in different behaviors of contrastive decoding with visually changed samples. Such a conclusion can also be obtained from the observation of revision behaviors, which shows that different visually changed samples present different trends in revising answers of different LVLMs (e.g., image editing on POPE-A achieves the least revise-wrong on InstructBLIP but the highest one on Qwen-VL)."}, {"title": "8.4. Pairwise Overlap of Rectified Answers", "content": "We visualize the pairwise overlap of rectified answers of LVLMs in Figure 12, and draw several observations:\n1) For InstructBLIP and LLaVA-1.5, all pairs of CDs have low IoUs, demonstrating the rectified samples resulting from visually changed samples are different and non-overlapping. Moreover, the IoU of rectified samples from applying downsample and diffusion noise methods is rather higher than other pairs, indicating a similar type of changes on images could lead to similar outcomes.\n2) For Qwen-VL, the results are different from those on the other two LVLMs. On the POPE benchmark, the overlap between downsample and diffusion noise is higher. Meanwhile, on the MME benchmark, the overlaps among downsample, diffusion noise, and no image methods exhibit significantly higher IoUs. On the contrary, the image editing method shows notably low IoUs with the other methods. The evidence suggests that the reduction of visual information may have a similar influence on predictions of Qwen-VL, whereas the editing of contents impacts its predictions differently.\nFrom these observations, we conclude that for the different LVLMs, the behaviors of visually changed samples could vary, encouraging us to develop a fusion method to leverage the strengths of different visually changed samples to mitigate hallucinations."}, {"title": "9. More Ablation Studies", "content": "To validate our designs for the fusion method of visually changed samples, we conduct several ablation studies, including the combination of fusion, the choice of fusion weights, and the influence of the extent of visual changes."}, {"title": "9.1. Combinations of Fusion", "content": "We provide results of exhaustive combinations of entropy-weighted fusion methods ranging from involving two to four visually changed samples, as shown in Table 3. In general, combining more visually changed samples obtains better overall performance. With careful selection, the combination of two visually changed samples (e.g., no image (B) + downsampling (C)) can reach a comparable performance with the combination of all samples. However, it could be difficult to accurately select the optimal combinations when deploying on real-world AI agents. In contrast, our fusion method leveraging all the strengths of different visually changed samples serves as a more general and effective solution to mitigate hallucinations of LVLMs."}, {"title": "9.2. Ablation of Metric-Weighted Fusions", "content": "As mentioned in Sec. 5.2 of the main paper, when the entropy is higher, the LVLM is more likely to yield the hallucinated response, which is more suitable for serving as a contrastive sample. Here, we investigate using different probability-level metrics as a weight for the fusion of visually changed samples. To be more general, we reformulate Eq. (8) of the main paper as:\n\\( P_{cd}(y_t|v, v_{cd}, x, y_{<t}) = \\text{softmax}[\\text{logit}_\\theta(y_t|v, x, y_{<t})\n- \\sum_{i=1}^{k} w_i (\\text{logit}_\\theta (y_t|v, x, y_{<t}) - \\text{logit}_\\theta (y_t| v_{cd_i}, x, y_{<t}))] \\), (10)\nwhere \\( w_i \\) is the weight of output distribution for a visually changed sample \\( v_{cd_i} \\), which can be metrics we have discussed before, including entropy \\( e_i \\), confidence \\( c_i \\), unconfidence \\( u_i = \\frac{1}{c_i} \\), and probability distribution distance \\( d_i \\).\nThe results are shown in Table 4. We find that the entropy-weighted fusion reaches the best performance on overall accuracy, verifying our motivation to combine visually changed samples with entropy as weight. In addition, the unconfidence-weighted (denoted as unconf-weighted) fusion also reaches the best performance, which has a higher performance on MME and a lower performance on POPE, demonstrating that different fusion methods have varying preferences for weighting visually changed samples."}, {"title": "9.3. Ablation of Extent of Visual Changes", "content": "We investigate the influence of the distortion extent of visually changed samples on different benchmarks from the"}]}