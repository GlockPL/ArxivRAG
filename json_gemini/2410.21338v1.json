{"title": "FINTEAMEXPERTS: ROLE-SPECIALIZED MOES FOR FINANCIAL ANALYSIS", "authors": ["Yue Yu", "Prayag Tiwari"], "abstract": "Large Language Models (LLMs), such as ChatGPT, Phi3 and Llama-3, are leading a significant\nleap in AI, as they can generalize knowledge from their training to new tasks without fine-tuning.\nHowever, their application in the financial domain remains relatively limited. The financial field is\ninherently complex, requiring a deep understanding across various perspectives, from macro, micro\neconomic trend to quantitative analysis.\nMotivated by this complexity, a mixture of expert LLMs tailored to specific financial domains could\noffer a more comprehensive understanding for intricate financial tasks. In this paper, we present the\nFinTeamExperts, a role-specialized LLM framework structured as a Mixture of Experts (MOES) for\nfinancial analysis. The framework simulates a collaborative team setting by training each model\nto specialize in distinct roles: Macro Analysts, Micro analysts, and Quantitative Analysts. This\nrole-specific specialization enhances the model's ability to integrate their domain-specific expertise.\nWe achieve this by training three 8-billion parameter models on different corpus, each dedicated to\nexcelling in specific finance-related roles. We then instruct-tune FinTeamExperts on downstream\ntasks to align with practical financial tasks. The experimental results show that FinTeamExperts\noutperform all models of the same size and larger on three out of four datasets. On the fourth dataset,\nwhich presents a more complex task, FinTeamExperts still surpass all models of the same size. This\nhighlights the success of our role-based specialization approach and the continued training approach\nfor FinTeamExperts.", "sections": [{"title": "1 Introduction", "content": "Traditional machine learning methods, such as Support Vector Machines (SVMs) [1], gradient-boosted trees [2], and\nlogistic regression [3], have limitations when it comes to understanding, reasoning, and generalizing in language-specific\ntasks. This limitation was overcome with the introduction of transformer architecture [4], which relies solely on attention\nmechanisms, removing the need for recurrence and convolutions. By training on large datasets using autoregressive\ntechniques, Large Language Models are able to capture context and semantic dependencies in language. They are\nhighly effective in translating across languages, processing large volumes of data, delivering quick responses with\nminimal delay, and can be fine-tuned to handle specific tasks and domains.\nLLMs are increasingly used in the finance industry. They improve customer service with chatbots, help summarize\ninformation, recommend relevant knowledge, and automate tasks like filling out forms. LLMs also support risk\nmanagement by analyzing market and credit risks and detecting anomalies or sentiment analysis. In investment, they\nassist with quantitative analysis, and in document processing, they help ensure compliance with regulations. However,\nresearch in financial Large Language Models (FinLLMs) remains limited. The earlier model, FinBERT[5], adapts\nBERT[6] for financial sentiment analysis by pre-training on financial documents and fine-tuning with a sentiment-\nspecific dataset, achieving superior results but struggling with tasks beyond sentiment analysis. FinMA[7] fine-tuned\nLlama with financial instructions, showing competitive performance but not surpassing larger models like GPT-4."}, {"title": "2 Methodologies", "content": "In this section, we present the framework, outline the roles within the team, explore the adaptation of learning processes,\nand discuss the architectures of MOES.\nThe FinTeamExperts architecture consists of two stages designed for financial analysis using a MoE approach. In stage\none of continue autoregressive training, LLMs are fine-tuned with different datasets (marked as D1, D2, D3 in Figure 1)\nto create specialized models, referred to as Experts. Each expert model is trained on its respective dataset, allowing it\nto specialize in a specific domain or task relevant to financial analysis. This phase focuses on autoregressive training,\nwhere the experts continue with predicting the next token in the given corpus.\nIn stage two of transfer learning, the trained experts are applied together to new financial tasks. The input is passed\nthrough a gating mechanism, which selects the most appropriate experts for the task at hand. The outputs from these\nselected experts are combined via a weighted manner and processed by hidden layers and a feedforward network, which\nrefine the aggregated information. This setup enables the model to generalize and make downstream predictions for\nfinancial applications, such as trend analysis or risk assessment."}, {"title": "2.1 Framework Formulation", "content": "The core of the MOE is the routing mechanism of inputs to Experts. We set up a group of experts, corresponding to\nmacro experts, micro experts, and quant experts. A hard gating mechanism is used to select one expert from each group,\nand their outputs are then weighted via another soft gating mechanism. The routing gate can be expressed as $g(x_i)$\nrepresenting a probability distribution over the experts.\n\n$g(x_i) = Softmax(W_gX_i)$\n\nwhere $x_i$ is the input token and $W_g$ is the gating network weight matrix, which learns to route inputs to appropriate\nexperts.\n\n$y_i = \\sum_{j=1}^{K} g_j(x_i) E_j(X_i)$\n\nWe define $y_i$ as the output for the input token $x_i$, which is a weighted sum of the selected experts' outputs. $E_j(x_i)$ is\nthe output of expert j for input $x_i$, where each expert processes the input differently. K is the total number of experts\navailable, for instance, three in our study. Typically, only a subset of these experts is used for each input but we have it\nas a weighted contribution.\nWe define the total loss for the MOE model as $L_{task}$, including both the task-specific loss and the regularization term.\nThe task-specific loss (e.g., cross-entropy) is used for training the model on the primary task.\n\n$L_{MOE} = L_{task} + \\lambda \\cdot H(g(x))$\n\nwhere $\\lambda$ is a regularization coefficient controlling the impact of the entropy term. $H(g(x))$ is the entropy of the gate\nfunction output, used to encourage balanced utilization of experts."}, {"title": "2.2 Teamed Roles", "content": "To optimize performance and achieve strategic goals, FinTeamExperts focus on three pivotal roles within the investment\nsetting:\nMacro Analysts: Macro analysts study broader economic trends, geopolitical events, and global market movements to\ninform investment strategies. Their insights are crucial for understanding the larger economic context in which specific\ninvestments operate, enabling more informed decision-making.\nMicro Analyst: Portfolio managers are responsible for making investment decisions and managing a portfolio of\nassets. They analyze market trends, assess risk, and determine the best investment strategies to maximize returns while\nadhering to the company's risk tolerance and investment objectives. Their role is vital in driving the overall investment\nstrategy and ensuring alignment with the company's financial goals.\nQuantitative Analysts: Quants develop mathematical models and algorithms to analyze financial data and identify\ntrading opportunities. They work on creating predictive models, optimizing trading strategies, and automating trading"}, {"title": "2.3 Adapting LLM Knowledge", "content": "We continue training the LLM checkpoints (such as GPT-2 and LLaMA-3-8B) using typical autoregressive learning\nwith the next token prediction as the learning objective, defined as:\n\n$P(x_1, x_2,...,x_T) = \\prod_{t=1}^{T} P(x_t | x_1, x_2, ..., x_{t-1})$\n\nwhere $x_1, x_2,...,x_T$ are the tokens from the corpus. The model generates each token $x_t$ conditioned on the previous\ntokens.\nAdapting large language models to specialize in finance-domain tasks involves curating a comprehensive financial\ncorpus and implementing role-specific training. The initial phase includes pretraining the model on a curated dataset of\nmarket reports, economic analyses, and financial statements, building a robust understanding of financial terminology\nand concepts. Each model within FinTeamExperts is then trained to specialize in one of the three roles, developing\nexpertise in macroeconomic analysis, asset management, or statistical trading techniques. This specialized training\nenables the models to integrate their expertise, forming a comprehensive financial analysis tool."}, {"title": "2.4 Mixture-of-Experts Architecture", "content": "The FinTeamExperts framework leverages the Mixture of Experts (MOEs) architecture to optimize performance in\nfinance-related tasks. MOEs utilize multiple expert models, each specializing in different aspects of financial analysis,\nand dynamically route queries to the most relevant expert. This architecture allows for efficient resource allocation and\nenhances the model's ability to handle diverse and complex financial scenarios.\nWe employ Dynamic Routing: Queries are dynamically routed to the most appropriate expert model based on the\nspecific financial task, improving accuracy and efficiency.\nSpecialized Training: Each expert model undergoes specialized training focused on its designated role (Macro Analysts,\nPortfolio Managers, Quantitative Analysts), ensuring depth of knowledge and proficiency in specific areas.\nHierarchical Expertise: The architecture supports hierarchical expertise, where higher-level experts oversee and refine\nthe outputs of lower-level models, ensuring coherent and high-quality analysis. This MOEs-based methodology allows\nFinTeamExperts to leverage the strengths of individual expert models while maintaining flexibility and adaptability in\naddressing a wide range of financial tasks. The integration of these innovations demonstrates the potential of advanced\nLLMs in transforming financial analysis and decision-making."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Role-specific Continue Training Datasets", "content": "In this study, we utilize three categories of datasets to continue pretrain models for financial roles: Macro of financial\nNews, Financial Statements and Reports, and Market and Transactional Data. Each category serves a distinct role\nin equipping the model with the necessary knowledge to support the varied analytical needs of macroeconomic,\nmicroeconomic, and quantitative investment strategies.\nFinancial News Datasets: This category includes datasets that capture the sentiment and content of financial news\narticles, providing valuable insights into market trends and investor sentiment. One notable resource is the Thomson\nReuters News Analytics (TRNA) dataset, which offers sentiment data derived from a vast archive of news articles.\nThese datasets are crucial for developing models that can interpret the influence of global events and market news on\ninvestment decisions.\nFinancial Statements and Reports Datasets: This category encompasses structured financial data extracted from\ncorporate reports such as balance sheets, income statements, and cash flow statements. The SEC EDGAR database 2"}, {"title": "3.2 Downstream Datasets", "content": "FPB dataset [9] contains 4,840 sentences from English-language financial news articles, each labeled by sentiment.\nSentences are categorized based on the level of agreement among 5 to 8 annotators, providing a clear indication of\nconsensus in sentiment labeling."}, {"title": "3.3 Settings", "content": "We build our expert model, on top of GPT-3-Large (with less than 1B parameters), noted as E-1B, and LLaMA-3-8B,\nnoted as E-8B.\nE-1B consists of 24 layers, each with 16 attention heads and a hidden size of 1,536, while E-8B comprises 36 layers, 32\nattention heads, and a hidden size of 4,096. Using pretrained weights as the backbone for both models, we further train\nthem on their respective corpora as specified in section 3.1.\nFor E-1B, we use a learning rate of $1 \\times 10^{-4}$ with 500 warmup steps, a cosine learning rate scheduler, and the Adam\noptimizer [15], with $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, and a weight decay of 0.1. The model is trained for one epoch.\nFor E-8B, we use a learning rate of $1 \\times 10^{-4}$ with 1,000 warmup steps, a cosine scheduler, and the Adam optimizer\nwith $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, and an epsilon value of $1 \\times 10^{-6}$. We apply a weight decay of 0.1 and train the model for\none epoch.\nBoth models are trained using 16-bit mixed precision on four 24GB A10G GPUs with fully sharded data parallelism,\nimplemented via the Accelerate library6."}, {"title": "3.4 Reseults", "content": "The FinTeamExpert models, particularly the 3\u00d78B version, achieve the best performance across all sentiment tasks, with\nthe highest scores in FPB (90.5), FiQA-SA (81.0), and FOMC (65.5). This suggests that increasing model size and using\nan ensemble of models enhances performance in financial sentiment analysis. Even the smaller FinTeamExpert (3\u00d71B)\nperforms competitively, especially in FPB (89.3) and FOMC (64.2), demonstrating that well-structured ensembles of\nsmaller models can rival larger models in specialized tasks.\nIn comparison, FinMA-Full (7B) performs well on FPB (87.0) and FiQA-SA (79.0), though it falls slightly behind the\nFinTeamExpert models. On the other hand, GPT-4, while a powerful general-purpose model, lags behind in financial\ntasks with a lower FPB score (78.0). Large models like BLOOM (176B) and BloombergGPT (50B) underperform in\nFPB, highlighting that model size alone does not guarantee effectiveness without domain-specific adaptation."}, {"title": "3.5 Ablation Study", "content": "The ablation study, as shown in Figure 2, reveals how removing individual roles\u2014Macro, Micro, or Quant\u2014impacts\nmodel performance across different tasks. The results clearly show that the full FinTeamExperts, without any role\nremoved, consistently outperforms all ablated versions, particularly in complex tasks like FPB. This highlights the\ncritical importance of each role in achieving top performance. For InFiQA-SA, dropping any role yields similar\nperformance, suggesting that no single role is indispensable for this task. However, in FPB, the complete model far\nexceeds any version with a role removed, underscoring the collaboration required for optimal results. For FOMC,\ndroping Micro lead to lower performance, indicating the importance of micro knowledge of this task. Overall, these\nfindings reinforce the value of having all roles intact for the most challenging tasks."}, {"title": "4 Related Works", "content": ""}, {"title": "4.1 Financial LLMs", "content": "A survey regarding FinLLMs [16] shows that research in this area remains limited.\nFinBERT [5] adapts BERT for financial sentiment analysis in two steps. First, it is pre-trained on financial documents\nusing a subset of Reuters' TRC2 dataset, enhancing its understanding of financial terminology. Second, a dense layer is\nadded to the last hidden state of the classification token (CLS) and fine-tuned with the Financial PhraseBank (FPB)\ndataset. FinBERT achieves outstanding results in financial sentiment analysis, surpassing state-of-the-art models, but is\nlimited to this task and performs poorly on others.\nBloombergGPT [13], a 50-billion parameter model based on BLOOM's architecture, is one of the first decoder-only\nLLMs trained specifically for finance. Trained from scratch on 363 billion tokens from financial documents and 345\nbillion from general datasets, it predicts the next token in documents without fine-tuning on instructions. However, its\nresults lag behind those of other models, including some much smaller ones, as detailed in [17].\nFinMA [7] curated an instruction dataset for a financial LLM and fine-tuned it on Llama, resulting in strong performance\namong similar-sized LLMs, though not surpassing larger models like GPT-4."}, {"title": "4.2 Mixture of Experts", "content": "Mixture of Experts (MoEs), derived from Gaussian Mixture models, are employed to boost the performance of large\nlanguage models (LLMs) without increasing computational resource requirements.\nSwithTransformer [18], places MoE layers after the multi-head attention mechanism in each transformer block to select\nthe feedforward layers. Unlike LLMs where all parameters are used for every input, SwitchTransformer activates only a\nsmall subset of experts for each input, significantly reducing computational costs. A routing network determines which\nexpert should handle each input, ensuring that only the most relevant parts of the model are utilized for each task.\nGLaM [19], a model with 1.2 trillion parameters, is approximately 7 times larger than GPT-3, yet it achieves this scale\nwith only a third of the energy consumption required to train GPT-3. GLaM's architecture integrates MoE layers with\nTransformer layers. For each input token, a gating module selects the two most relevant experts, and a weighted average\nof their outputs is passed to the next Transformer layer. The gating network, which is trained to identify the optimal\nexperts for each token in the input sequence, ensures efficient use of computational resources while maintaining high\nperformance.\nST-MoE [20], is a sparse Transformer model with 269B parameters, offering performance at a computational cost similar\nto that of a dense 32B parameter encoder-decoder Transformer. The model recommends a top-2 routing mechanism,\nwith each input token routed to the two most relevant experts, ensuring computational efficiency. A capacity factor of\n1.25 is recommended, which controls the number of tokens processed by each expert, and this factor can be adjusted\nduring evaluation to meet changing memory or computational requirements. Additionally, quality improvements are\nachieved through dense layer stacking and the introduction of a multiplicative bias."}, {"title": "5 Conclusion", "content": "In this paper, we introduced FinTeamExperts, a novel framework of role-specialized large language models (LLMs)\ndesigned as a Mixture of Experts (MOEs) to excel in financial analysis tasks. By mimicking a real-world team\nsetting, each model in FinTeamExperts specializes in one of three critical roles: Macro Analysts, Portfolio Managers,\nand Quantitative Analysts. This specialization allows the models to integrate their expertise effectively, forming a\ncomprehensive and robust financial analysis tool.\nOur approach includes several innovative contributions. Firstly, we are the first to implement role-based teams of LLMs\nas MOEs, aiming to mimic practical implementation scenarios within the finance domain. This method leverages the\nstrengths of individual expert models while maintaining flexibility and adaptability in handling a wide range of financial\ntasks. Secondly, we introduced advancements in the MOEs architecture, such as dynamic routing, specialized training,\nand hierarchical expertise, which significantly enhance the model's performance in downstream financial tasks.\nThrough instruct-tuning and rigorous experiments, we demonstrated that FinTeamExperts outperform existing LLMs in\nfinance-related tasks, underscoring the effectiveness of our pretraining methodology and specialized training strategies.\nThese contributions showcase the potential of advanced LLMs in transforming financial analysis and decision-making,\npaving the way for more sophisticated and practical AI applications in the finance industry.\nThere are several directions for future exploration and enhancement. First, expanding the diversity of role-based teams\nwithin the MOEs framework could further refine task-specific expertise, particularly by incorporating specialized\nmodels trained on emerging financial topics, such as ESG (Environmental, Social, and Governance) criteria or digital\nasset analytics. Additionally, investigating the effects of cross-domain transfer learning may yield insights into how\nfinancial LLMs can benefit from knowledge in adjacent fields, such as legal or regulatory compliance, which often\nintersect with financial analysis. Another promising direction is the optimization of dynamic routing strategies to allow\nfor more granular control over model selection based on real-time task complexity and data characteristics. This could\ninvolve developing adaptive routing algorithms that leverage reinforcement learning or other self-learning techniques,\nallowing the MOEs framework to continuously improve task assignment efficiency and performance."}]}