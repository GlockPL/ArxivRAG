{"title": "An Introduction to Quantum Reinforcement Learning (QRL)", "authors": ["Samuel Yen-Chi Chen"], "abstract": "Recent advancements in quantum computing (QC) and machine learning (ML) have sparked considerable interest in the integration of these two cutting-edge fields. Among the various ML techniques, reinforcement learning (RL) stands out for its ability to address complex sequential decision-making problems. RL has already demonstrated substantial success in the classical ML community. Now, the emerging field of Quantum Reinforcement Learning (QRL) seeks to enhance RL algorithms by incorporating principles from quantum computing. This paper offers an introduction to this exciting area for the broader AI and ML community.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum computing (QC) offers the potential for substantial computational advantages in specific problems compared to classical computers [1]. Despite the current limitations of quantum devices, such as noise and imperfections, significant efforts are being made to achieve quantum advantages. One prominent area of focus is quantum machine learning (QML), which leverages quantum computing principles to enhance machine learning tasks. Most QML algorithms rely on a hybrid quantum-classical paradigm, which divides the computational task into two components: quantum computers handle the parts that benefit from quantum computation, while classical computers process the parts they excel at.\nVariational quantum algorithms (VQAs) [2] form the foundation of current quantum machine learning (QML) approaches. QML has demonstrated success in various machine learning tasks, including classification [3]\u2013[6], sequential learning [7], [8], natural language processing [9]\u2013[12], and reinforcement learning [13]\u2013[19]. Among these areas, quantum reinforcement learning (QRL) is an emerging field where researchers are exploring the application of quantum computing principles to enhance the performance of reinforcement learning agents. This article provides an introduction to the concepts and recent developments in QRL."}, {"title": "II. QUANTUM NEURAL NETWORKS", "content": "A. Quantum Computing\nA qubit represents the fundamental unit of quantum information processing. Unlike a classical bit, which is restricted to holding a state of either 0 or 1, a qubit can simultaneously encapsulate the information of both 0 and 1 due to the principle of superposition. A single qubit quantum state can be expressed as $|\\Psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle$, where $|0\\rangle = [1,0]^T$ and $|1\\rangle = [0,1]^T$ are column vectors, and $\\alpha$ and $\\beta$ are complex numbers. In an n-qubit system, the state vector has a length of $2^n$. Quantum gates U are utilized to transform a quantum state, represented as $|\\Psi\\rangle$, to another state $|\\Psi'\\rangle$ through the operation $|\\Psi'\\rangle = U |\\Psi\\rangle$. These quantum gates are unitary transformations that satisfy the condition $UU^\\dagger = U^\\dagger U = I_{2^n \\times 2^n}$, where n denotes the number of qubits. It has been demonstrated that a small set of basic quantum gates is sufficient for universal quantum computation. One such set includes single-qubit gates $H, \\sigma_x, \\sigma_y, \\sigma_z, R_x(\\theta) = e^{-i\\theta \\sigma_x/2}, R_y(\\theta) = e^{-i\\theta \\sigma_y/2}, R_z(\\theta) = e^{-i\\theta \\sigma_z/2}$, and the two-qubit gate CNOT. In quantum machine learning (QML), rotation gates $R_x$, $R_y$, and $R_z$ are particularly crucial as their rotation angles can be treated as trainable or learnable parameters subject to optimization. For quantum operations on multi-qubit systems, the unitary transformation can be constructed via the tensor product of individual single-qubit or two-qubit operations, $U = U_1 \\otimes U_2 \\cdots U_k$. At the final stage of a quantum circuit, a procedure known as measurement is performed. A single execution of a quantum circuit generates a binary string. This procedure can be repeated multiple times to determine the probabilities of different computational bases (e.g., $|0,..., 0\\rangle$, ..., $|1,..., 1\\rangle$) or to calculate expectation values (e.g., Pauli X, Y, and Z).\nB. Variational Quantum Circuits\nVariational quantum circuits (VQCs), also referred to as parameterized quantum circuits (PQCs), represent a specialized class of quantum circuits with trainable parameters. VQCs are extensively utilized within the current hybrid quantum-classical computing framework [2] and have demonstrated specific types of quantum advantages [20]\u2013[22]. There are three fundamental components in a VQC: encoding circuit, variational circuit and the final measurements. As shown in Figure 1, the encoding circuit $U(x)$ transforms the initial"}, {"title": "III. QUANTUM REINFORCEMENT LEARNING", "content": "A. Reinforcement Learning\nReinforcement Learning (RL) is a pivotal paradigm within machine learning, where an autonomous entity known as the agent learns to make decisions through iterative interactions with its environment [25]. The agent operates within a defined environment, represented as $\\mathcal{E}$, over discrete time steps. At each time step t, the agent receives state or observation information, denoted as $s_t$, from the environment $\\mathcal{E}$. Based on this information, the agent selects an action $a_t$ from a set of permissible actions $\\mathcal{A}$, guided by its policy $\\pi$. The policy $\\pi$ acts as a function that maps the current state or observation $s_t$ to the corresponding action $a_t$. Notably, the policy can be stochastic, indicating that for a given state $s_t$, the action $a_t$ is determined by a probability distribution $\\pi(a_t|s_t)$.\nUpon executing action $a_t$, the agent transitions to the subsequent state $s_{t+1}$ and receives a scalar reward $r_t$. This cycle continues until the agent reaches a terminal state or fulfills a specified stopping condition, such as a maximum number of steps. We define an episode as the sequence beginning from an initial state, following the described process, and concluding either at the terminal state or upon meeting the stopping criterion. The use of quantum neural networks for learning policy or value functions is referred to as quantum reinforcement learning (QRL). The idea of QRL is illustrated in Figure 2. For a comprehensive review of current QRL domain, refer to the review article [18].\nB. Quantum Deep Q-learning\nQ-learning [25] is a fundamental model-free RL algorithm. It learns the optimal action-value function and operates off-policy. The process begins with the random initialization of $Q(s, a)$ for all states $s \\in \\mathcal{S}$ and actions $a \\in \\mathcal{A}$, stored in a Q-table. The $Q^*(s, a)$ estimates are updated using the Bellman equation:\n$Q (s_t, a_t) \\leftarrow Q (s_t, a_t) + \\alpha \\Big[r_t + \\gamma \\max_a Q (s_{t+1}, a) - Q (s_t, a_t)\\Big]$.\n\nThe conventional Q-learning approach offers the optimal action-value function but is impractical for problems requiring extensive memory, especially with high-dimensional state (s) or action (a) spaces. In environments with continuous states, storing Q(s, a) in a table is inefficient or impossible. To address this challenge, neural networks (NNs) are used to represent $Q^*(s,a)$, $\\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}$, leading to deep Q-learning. The network in this technique is known as a deep Q-network (DQN) [26]. To enhance the stability of DQN, techniques such as experience replay and the use of a target network are employed [26]. Experience replay stores experiences as transition tuples $s_t, a_t, r_t, s_{t+1}$ in a memory or buffer. After gathering sufficient experiences, the agent randomly samples a batch to compute the loss and update DQN parameters. Additionally, to reduce correlation between target and prediction, a target network, which is a duplicate of the DQN, is used. The DQN parameters $\\theta$ are updated iteratively, while the target network parameters $\\theta^-$ are updated periodically. The DQN training is done via minimizing the mean square error (MSE) loss function:\n$L(\\theta) = E \\Big[(r_t + \\max_{a'} Q (s_{t+1}, a'; \\theta^-) - Q (s_t, a_t; \\theta))^2\\Big]$\n\nOther loss functions such as Huber loss or mean absolute error (MAE) can also be used. The first VQC-based QRL is described in the work [13] in which a VQC is designed to solve environments with discrete observations such as the"}, {"title": "C. Quantum Policy Gradient Methods", "content": "In contrast to value-based RL algorithms, such as Q-learning, which depend on learning a value function to guide decision-making at each time step, policy gradient methods aim to optimize a policy function directly. This policy function $\\pi(a|s;\\theta)$ is parameterized by $\\theta$. The parameters $\\theta$ are adjusted using gradient ascent on the expected total return, $\\mathbb{E}[R_t]$. A prominent example of a policy gradient algorithm is the REINFORCE algorithm [27]. The policy function $\\pi(a|s; \\theta)$ can be implemented using a VQC, where the rotation parameters serve as $\\theta$. In [28], the authors employ the REINFORCE algorithm to train a VQC-based policy. Their results demonstrate that VQC-based policies can achieve performance comparable to or exceeding that of classical DNNs on several standard benchmarks. In the traditional REINFORCE algorithm, parameter updates for $\\theta$ are based on the gradient $\\nabla_{\\theta} \\log \\pi (a_t|s_t;\\theta) R_t$, which provides an unbiased estimate of $\\nabla_{\\theta} \\mathbb{E} [R_t]$. However, this gradient estimate can exhibit high variance, which may lead to difficulties or instability during training. To address this issue and reduce variance while preserving unbiasedness, a baseline term can be subtracted from the return. This baseline, $b_t(s_t)$, is a learned function of the state $s_t$. The update rule then becomes $\\nabla_{\\theta}\\log \\pi (a_t|s_t; \\theta) (R_t \u2013 b_t (s_t))$. A typical choice for the baseline $b_t(s_t)$ in RL is an estimate of the value function $V^*(s_t)$. Employing this baseline generally leads to a reduction in the variance of the policy gradient estimate [25]. The term $R_t - b_t = Q(s_t, a_t) - V(s_t)$ represents the advantage $A(s_t, a_t)$ of taking action $a_t$ in state $s_t$. This advantage can be viewed as a measure of how favorable or unfavorable action $a_t$ is compared to the average value of the state $s_t$. This method is referred to as the advantage actor-critic (A2C) approach, where the policy serves as the actor and the value function $V$ acts as the critic [25]. Similar to traditional policy gradient methods, the A2C algorithm can be implemented using VQCs. In [29], the authors utilize VQCs to construct both the actor (policy function) and the critic (value function). Their study demonstrates that, for comparable numbers of model parameters, a hybrid approach\u2014where classical neural networks post-process the outputs from the VQC\u2014achieves superior performance across the tested environments. The asynchronous advantage actor-critic (A3C) algorithm [30] is an enhanced variant of the A2C method that utilizes multiple concurrent actors to learn the policy through parallel processing. This approach involves deploying several agents across several instances of the environment, enabling them to experience a wide range of states simultaneously. By reducing the correlation between states or observations, this method improves the numerical stability of on-policy RL algorithms like actor-critic [30]. Moreover, asynchronous training eliminates the need for extensive replay memory, which helps in reducing memory usage [30]. A3C achieves high sample efficiency and robust learning performance, making it a favored choice in RL. In the context of quantum RL, asynchronous or distributed training can further boost sampling efficiency and leverage the capabilities of multiple quantum computers or quantum processing units (QPUs). In [31], the authors extend the A3C framework to quantum settings, showing that VQC actors and critics can outperform classical models when the sizes of the models are comparable."}, {"title": "D. Quantum RL with Evolutionary Optimization", "content": "One of the significant challenges in current QML applications is the limitation of quantum computers or quantum simulation software in processing input dimensions. These systems can only handle inputs up to a certain level, which is insufficient for encoding larger vectors. In QRL, this constraint means the observation vector that the quantum agent can process from the environment is severely restricted. To address this issue, various dimensional reduction methods have been proposed. Among these, a hybrid quantum-classical approach that incorporates a classical learnable model with a VQC has shown promising results. In the work by Chen et al. [14], a quantum-inspired classical model based on a specific type of tensor network, known as a matrix product state (MPS), is integrated with a VQC to function as a learnable compressor [4] (see Figure 4). The hybrid architecture MPS-VQC, including the tensor network and VQC, is randomly initialized, and the entire model is trained in an end-to-end manner. Although gradient-based methods have achieved considerable success in RL, several challenges remain. Notably, these methods can become trapped in local minima or fail to converge to the optimal solution, particularly in sparse RL environments where the agent frequently receives zero rewards during episodes. Evolutionary optimization techniques have been proposed to address these challenges in classical RL and have demon-"}, {"title": "E. Quantum RL with Recurrent Policies", "content": "The previously mentioned quantum RL methods primarily utilize various VQCs without incorporating recurrent structures. However, recurrent connections are essential in classical machine learning for retaining memory of past time steps. Certain RL tasks necessitate that agents have the capability to remember information from previous time steps to select optimal actions. For instance, environments with partial observability often require agents to make decisions based not only on information from the current time step but also on information accumulated from the past. In classical ML, recurrent neural networks (RNNs), such as long short-term memory (LSTM) [33], have been proposed to solve tasks with temporal dependencies. The quantum version of LSTM (QLSTM) has been designed by replacing classical neural networks with VQCs [7]. It has been shown that QLSTM can outperform classical LSTM in several time-series prediction tasks when the model sizes are similar [7]. To address RL environments with partial observability or those requiring temporal memories, QRL agents utilizing QLSTM as the value or policy function have been proposed in [17]. It has been demonstrated that QLSTM-based value or policy functions enable QRL agents to outperform classical LSTM models with a similar number of parameters.\nWhile the QLSTM-based models achieve significant results in several benchmarks, there is at least one major challenge preventing such models from wide applications. The training of RNNs, both in quantum and classical, requires significant computational resources due to the requirement of performing backpropagation-through-time (BPTT). One might question whether it is possible to leverage the capabilities of QLSTM without the need for gradient calculations with respect to"}, {"title": "F. Quantum RL with Fast Weight Programmers", "content": "An alternative approach for developing a QRL model that can memorize temporal or sequential dependencies without utilizing quantum RNNs is the Quantum Fast Weight Programmers (QFWP). The idea of Fast Weight Programmers (FWP) was originally proposed in the work of Schmidhuber [34], [35]. In this sequential learning model, two distinct neural networks (NN) are utilized: the slow programmer and the fast programmer. Here, the NN weights act as the model/agent's program. The core concept of FWP involves the slow programmer generating updates or changes to the fast programmer's NN weights based on observations at each time-step. This reprogramming process quickly redirects the fast programmer's attention to salient information within the incoming data stream. Notably, the slow programmer does not completely overwrite the fast programmer but instead applies updates or changes. This approach allows the fast programmer to incorporate previous observations, enabling a simple feed-forward NN to manage sequential prediction or control without the high computational demands of recurrent neural networks (RNNs). The idea of FWP can be further extended into the hybrid quantum-classical regime as described in the work [36]. In the work [36], classical neural networks are used to construct the slow networks, which generate values to update the parameters of the fast networks, implemented as a VQC."}, {"title": "G. Quantum RL with Quantum Architecture Search", "content": "While QRL has demonstrated effectiveness across various problem domains, the design of successful architectures is far from trivial. Developing VQC architectures tailored to specific problems requires substantial effort and expertise. The field of quantum architecture search (QAS) focuses on developing methods to identify high-performing quantum circuits for specific tasks. A QAS problem is formulated by specifying a particular goal (e.g., total returns in RL) and the constraints of the quantum device (e.g., maximum number of quantum operations, set of allowed quantum gates). QAS has been explored in the context of QRL. For instance, in [37], evolutionary algorithms are employed to search for high-performing circuits. The authors define a set of candidate VQC blocks, including entangling blocks, data-encoding blocks, variational blocks, and measurement blocks. The objective of the evolutionary search is to determine an optimal sequence of these blocks, given a constraint on the maximum number of circuit blocks. While this approach has shown effectiveness in the evaluated cases, scalability issues may arise as the search space expands. Differentiable quantum architecture search (DiffQAS) methods, as proposed in [38], draw inspiration from differentiable neural architecture search in classical deep learning to identify effective quantum circuits for RL. In [39], the authors apply DiffQAS to quantum deep Q-learning. They parameterize a probability distribution $P(k, \\alpha)$ for circuit architecture k using $\\alpha$. During training, mini-batches of VQCs are sampled, and the weighted loss is calculated based on the distribution $P(k, \\alpha)$. Both the architecture parameter $\\alpha$ and the quantum circuit parameters $\\theta$ are updated using conventional gradient-based methods. In [40], the authors extend the DiffQAS framework to asynchronous QRL. This extension allows multiple parallel instances (a single instance is shown in Figure 6) to optimize their own structural weights (denoted as w in Figure 6) alongside the VQC parameters. The gradients of these structural weights and quantum circuit parameters are shared across instances to enhance the training process."}, {"title": "IV. QUANTUM RL APPLICATIONS AND CHALLENGES", "content": "QRL can be extended to multi-agent settings and applied in fields like wireless communication and autonomous control systems [41]. Additionally, as discussed in Section III-G, QAS involves sequential decision-making and can be addressed through RL. In [42], a QRL approach is developed to discover quantum circuit architectures that generate desired quantum states. In the NISQ era, a major challenge for QML applications is the limited quantum resources, which complicates both the training and inference phases. In [43], [44], the authors propose a method using a QNN to generate classical NN weights. For an N-qubit QNN, measuring the expectation values of individual qubits provides up to N values. However, collecting the probabilities of all computational basis states $|00\u2026\u20260\\rangle,\u2026\u2026\u2026,|11\u00b7\u00b7\u00b71\\rangle$ yields $2^N$ values. These values can be rescaled and used as NN weights. Thus, for an NN with M weights, only $[\\log_2 M]$ qubits are needed to generate the weights. Numerical simulations demonstrate that the quantum circuit can efficiently generate NN weights, achieving inference performance comparable to conventional training. Future research could further explore the trainability challenges in QRL models highlighted by Sequeira et al. [45], which are key to enhancing their practical performance."}, {"title": "V. CONCLUSION AND OUTLOOK", "content": "This paper introduces the concept of quantum reinforcement learning (QRL), where variational quantum circuits (VQCs) are used as policy and value functions. It also explores advanced constructs, including quantum recurrent policies, quantum fast weight programmers, and QRL with differentiable quantum architectures. QRL holds the potential to offer quantum advantages in various sequential decision-making tasks."}]}